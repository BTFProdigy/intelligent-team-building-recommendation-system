Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1284?1293,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Looking at Unbalanced Specialized Comparable Corpora
for Bilingual Lexicon Extraction
Emmanuel Morin and Amir Hazem
Universit?e de Nantes, LINA UMR CNRS 6241
2 rue de la houssini`ere, BP 92208, 44322 Nantes Cedex 03, France
{emmanuel.morin,amir.hazem}@univ-nantes.fr
Abstract
The main work in bilingual lexicon ex-
traction from comparable corpora is based
on the implicit hypothesis that corpora are
balanced. However, the historical context-
based projection method dedicated to this
task is relatively insensitive to the sizes
of each part of the comparable corpus.
Within this context, we have carried out
a study on the influence of unbalanced
specialized comparable corpora on the
quality of bilingual terminology extraction
through different experiments. Moreover,
we have introduced a regression model
that boosts the observations of word co-
occurrences used in the context-based pro-
jection method. Our results show that the
use of unbalanced specialized comparable
corpora induces a significant gain in the
quality of extracted lexicons.
1 Introduction
The bilingual lexicon extraction task from bilin-
gual corpora was initially addressed by using par-
allel corpora (i.e. a corpus that contains source
texts and their translation). However, despite
good results in the compilation of bilingual lex-
icons, parallel corpora are scarce resources, es-
pecially for technical domains and for language
pairs not involving English. For these reasons,
research in bilingual lexicon extraction has fo-
cused on another kind of bilingual corpora com-
prised of texts sharing common features such as
domain, genre, sampling period, etc. without hav-
ing a source text/target text relationship (McEnery
and Xiao, 2007). These corpora, well known now
as comparable corpora, have also initially been
introduced as non-parallel corpora (Fung, 1995;
Rapp, 1995), and non-aligned corpora (Tanaka
and Iwasaki, 1996). According to Fung and Che-
ung (2004), who range bilingual corpora from par-
allel corpora to quasi-comparable corpora going
through comparable corpora, there is a continuum
from parallel to comparable corpora (i.e. a kind of
filiation).
The bilingual lexicon extraction task from com-
parable corpora inherits this filiation. For instance,
the historical context-based projection method
(Fung, 1995; Rapp, 1995), known as the standard
approach, dedicated to this task seems implicitly
to lead to work with balanced comparable corpora
in the same way as for parallel corpora (i.e. each
part of the corpus is composed of the same amount
of data).
In this paper we want to show that the assump-
tion that comparable corpora should be balanced
for bilingual lexicon extraction task is unfounded.
Moreover, this assumption is prejudicial for spe-
cialized comparable corpora, especially when in-
volving the English language for which many doc-
uments are available due the prevailing position
of this language as a standard for international
scientific publications. Within this context, our
main contribution consists in a re-reading of the
standard approach putting emphasis on the un-
founded assumption of the balance of the spe-
cialized comparable corpora. In specialized do-
mains, the comparable corpora are traditionally of
small size (around 1 million words) in comparison
with comparable corpus-based general language
(up to 100 million words). Consequently, the ob-
servations of word co-occurrences which is the ba-
sis of the standard approach are unreliable. To
make them more reliable, our second contribution
is to contrast different regression models in order
to boost the observations of word co-occurrences.
This strategy allows to improve the quality of ex-
tracted bilingual lexicons from comparable cor-
pora.
1284
2 Bilingual Lexicon Extraction
In this section, we first describe the standard ap-
proach that deals with the task of bilingual lexi-
con extraction from comparable corpora. We then
present an extension of this approach based on re-
gression models. Finally, we discuss works related
to this study.
2.1 Standard Approach
The main work in bilingual lexicon extraction
from comparable corpora is based on lexical con-
text analysis and relies on the simple observation
that a word and its translation tend to appear in
the same lexical contexts. The basis of this obser-
vation consists in the identification of ?first-order
affinities? for each source and target language:
?First-order affinities describe what other words
are likely to be found in the immediate vicinity
of a given word? (Grefenstette, 1994, p. 279).
These affinities can be represented by context vec-
tors, and each vector element represents a word
which occurs within the window of the word to
be translated (e.g. a seven-word window approxi-
mates syntactic dependencies). In order to empha-
size significant words in the context vector and to
reduce word-frequency effects, the context vectors
are normalized according to an association mea-
sure. Then, the translation is obtained by compar-
ing the source context vector to each translation
candidate vector after having translated each ele-
ment of the source vector with a general dictio-
nary.
The implementation of the standard approach
can be carried out by applying the following
three steps (Rapp, 1999; Chiao and Zweigenbaum,
2002; D?ejean et al, 2002; Morin et al, 2007;
Laroche and Langlais, 2010, among others):
Computing context vectors We collect all the
words in the context of each word i and count
their occurrence frequency in a window of
n words around i. For each word i of the
source and the target languages, we obtain
a context vector v
i
which gathers the set of
co-occurrence words j associated with the
number of times that j and i occur together
cooc(i, j). In order to identify specific words
in the lexical context and to reduce word-
frequency effects, we normalize context vec-
tors using an association score such as Mu-
tual Information, Log-likelihood, or the dis-
counted log-odds (LO) (Evert, 2005) (see
equation 1 and Table 1 where N = a + b +
c + d).
Transferring context vectors Using a bilingual
dictionary, we translate the elements of the
source context vector. If the bilingual dictio-
nary provides several translations for an ele-
ment, we consider all of them but weight the
different translations according to their fre-
quency in the target language.
Finding candidate translations For a word to be
translated, we compute the similarity be-
tween the translated context vector and all
target vectors through vector distance mea-
sures such as Jaccard or Cosine (see equa-
tion 2 where assoc
i
j
stands for ?association
score?, v
k
is the transferred context vector of
the word k to translate, and v
l
is the con-
text vector of the word l in the target lan-
guage). Finally, the candidate translations of
a word are the target words ranked following
the similarity score.
j ?j
i a = cooc(i, j) b = cooc(i,?j)
?i c = cooc(?i, j) d = cooc(?i,?j)
Table 1: Contingency table
LO(i, j) = log
(a +
1
2
) ? (d +
1
2
)
(b +
1
2
) ? (c +
1
2
)
(1)
Cosine
v
k
v
l
=
?
t
assoc
l
t
assoc
k
t
?
?
t
assoc
l
t
2
?
?
t
assoc
k
t
2
(2)
This approach is sensitive to the choice of pa-
rameters such as the size of the context, the choice
of the association and similarity measures. The
most complete study about the influence of these
parameters on the quality of word alignment has
been carried out by Laroche and Langlais (2010).
The standard approach is used by most re-
searchers so far (Rapp, 1995; Fung, 1998; Pe-
ters and Picchi, 1998; Rapp, 1999; Chiao and
Zweigenbaum, 2002; D?ejean et al, 2002; Gaussier
et al, 2004; Morin et al, 2007; Laroche and
Langlais, 2010; Prochasson and Fung, 2011;
1285
References Domain Languages Source/Target Sizes
Tanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million words
Fung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of data
Rapp (1999) Newspaper GE/EN 135/163 million words
Chiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 words
D?ejean et al (2002) Medical GE/EN 100,000/100,000 words
Morin et al (2007) Medical FR/JP 693,666/807,287 words
Otero (2007) European Parliament SP/EN 14/17 million words
Ismail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentences
Bouamor et al (2013) Financial FR/EN 402,486/756,840 words
- Medical FR/EN 396,524/524,805 words
Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction
Bouamor et al, 2013, among others) with the im-
plicit hypothesis that comparable corpora are bal-
anced. As McEnery and Xiao (2007, p. 21) ob-
serve, a specialized comparable corpus is built
as balanced by analogy with a parallel corpus:
?Therefore, in relation to parallel corpora, it is
more likely for comparable corpora to be designed
as general balanced corpora.?. For instance, Ta-
ble 2 describes the comparable corpora used in the
main work dedicated to bilingual lexicon extrac-
tion for which the ratio between the size of the
source and the target texts is comprised between
1 and 1.8.
In fact, the assumption that words which have
the same meaning in different languages should
have the same lexical context distributions does
not involve working with balanced comparable
corpora. To our knowledge, no attention
1
has
been paid to the problem of using unbalanced
comparable corpora for bilingual lexicon extrac-
tion. Since the context vectors are computed from
each part of the comparable corpus rather than
through the parts of the comparable corpora, the
standard approach is relatively insensitive to dif-
ferences in corpus sizes. The only precaution for
using the standard approach with unbalanced cor-
pora is to normalize the association measure (for
instance, this can be done by dividing each entry
of a given context vector by the sum of its associ-
ation scores).
2.2 Prediction Model
Since comparable corpora are usually small in spe-
cialized domains (see Table 2), the discrimina-
1
We only found mention of this aspect in Diab and Finch
(2000, p. 1501) ?In principle, we do not have to have the
same size corpora in order for the approach to work?.
tive power of context vectors (i.e. the observa-
tions of word co-occurrences) is reduced. One
way to deal with this problem is to re-estimate
co-occurrence counts by a prediction function
(Hazem and Morin, 2013). This consists in as-
signing to each observed co-occurrence count of
a small comparable corpora, a new value learned
beforehand from a large training corpus.
In order to make co-occurrence counts more
discriminant and in the same way as Hazem
and Morin (2013), one strategy consists in ad-
dressing this problem through regression: given
training corpora of small and large size (abun-
dant in the general domain), we predict word co-
occurrence counts in order to make them more
reliable. We then apply the resulting regression
function to each word co-occurrence count as a
pre-processing step of the standard approach. Our
work differs from Hazem and Morin (2013) in two
ways. First, while they experienced the linear re-
gression model, we propose to contrast different
regression models. Second, we apply regression
to unbalanced comparable corpora and study the
impact of prediction when applied to the source
texts, the target texts and both source and target
texts of the used comparable corpora.
We use regression analysis to describe the rela-
tionship between word co-occurrence counts in a
large corpus (the response variable) and word co-
occurrence counts in a small corpus (the predictor
variable). As most regression models have already
been described in great detail (Christensen, 1997;
Agresti, 2007), the derivation of most models is
only briefly introduced in this work.
As we can not claim that the prediction of word
co-occurrence counts is a linear problem, we con-
sider in addition to the simple linear regression
1286
model (Lin), a generalized linear model which is
the logistic regression model (Logit) and non lin-
ear regression models such as polynomial regres-
sion model (Poly
n
) of order n. Given an input
vector x ? R
m
, where x
1
,...,x
m
represent fea-
tures, we find a prediction y? ? R
m
for the co-
occurrence count of a couple of words y ? R us-
ing one of the regression models presented below:
y?
Lin
= ?
0
+ ?
1
x (3)
y?
Logit
=
1
1 + exp(?(?
0
+ ?
1
x))
(4)
y?
Poly
n
= ?
0
+ ?
1
x + ?
2
x
2
+ ... + ?
n
x
n
(5)
where ?
i
are the parameters to estimate.
Let us denote by f the regression function and
by cooc(w
i
, w
j
) the co-occurrence count of the
words w
i
and w
j
. The resulting predicted value of
cooc(w
i
, w
j
), noted ?cooc(w
i
, w
j
) is given by the
following equation:
?cooc(w
i
, w
j
) = f(cooc(w
i
, w
j
)) (6)
2.3 Related Work
In the past few years, several contributions have
been proposed to improve each step of the stan-
dard approach.
Prochasson et al (2009) enhance the represen-
tativeness of the context vector by strengthening
the context words that happen to be transliterated
words and scientific compound words in the target
language. Ismail and Manandhar (2010) also sug-
gest that context vectors should be based on the
most important contextually relevant words (in-
domain terms), and thus propose a method for fil-
tering the noise of the context vectors. In another
way, Rubino and Linar`es (2011) improve the con-
text words based on the hypothesis that a word and
its candidate translations share thematic similari-
ties. Yu and Tsujii (2009) and Otero (2007) pro-
pose, for their part, to replace the window-based
method by a syntax-based method in order to im-
prove the representation of the lexical context.
To improve the transfer context vectors step,
and increase the number of elements of translated
context vectors, Chiao and Zweigenbaum (2003)
and Morin and Prochasson (2011) combine a stan-
dard general language dictionary with a special-
ized dictionary, whereas D?ejean et al (2002) use
the hierarchical properties of a specialized the-
saurus. Koehn and Knight (2002) automatically
induce the initial seed bilingual dictionary by us-
ing identical spelling features such as cognates
and similar contexts. As regards the problem of
words ambiguities, Bouamor et al (2013) carried
out word sense disambiguation process only in
the target language whereas Gaussier et al (2004)
solve the problem through the source and target
languages by using approaches based on CCA
(Canonical Correlation Analysis) and multilingual
PLSA (Probabilistic Latent Semantic Analysis).
The rank of candidate translations can be im-
proved by integrating different heuristics. For in-
stance, Chiao and Zweigenbaum (2002) introduce
a heuristic based on word distribution symme-
try. From the ranked list of candidate translations,
the standard approach is applied in the reverse
direction to find the source counterparts of the
first target candidate translations. And then only
the target candidate translations that had the ini-
tial source word among the first reverse candidate
translations are kept. Laroche and Langlais (2010)
suggest a heuristic based on the graphic similarity
between source and target terms. Here, candidate
translations which are cognates of the word to be
translated are ranked first among the list of trans-
lation candidates.
3 Linguistic Resources
In this section, we outline the different textual re-
sources used for our experiments: the comparable
corpora, the bilingual dictionary and the terminol-
ogy reference lists.
3.1 Specialized Comparable Corpora
For our experiments, we used two specialized
French/English comparable corpora:
Breast cancer corpus This comparable corpus is
composed of documents collected from the
Elsevier website
2
. The documents were taken
from the medical domain within the sub-
domain of ?breast cancer?. We have auto-
matically selected the documents published
between 2001 and 2008 where the title or the
keywords contain the term cancer du sein in
French and breast cancer in English. We col-
lected 130 French documents (about 530,000
words) and 1,640 English documents (about
2
http://www.elsevier.com
1287
7.4 million words). We split the English doc-
uments into 14 parts each containing about
530,000 words.
Diabetes corpus The documents making up the
French part of the comparable corpus have
been craweled from the web using three
keywords: diab`ete (diabetes), alimentation
(food), and ob?esit?e (obesity). After a man-
ual selection, we only kept the documents
which were relative to the medical domain.
As a result, 65 French documents were ex-
tracted (about 257,000 words). The English
part has been extracted from the medical
website PubMed
3
using the keywords: dia-
betes, nutrition and feeding. We only kept
the free fulltext available documents. As a re-
sult, 2,339 English documents were extracted
(about 3,5 million words). We also split the
English documents into 14 parts each con-
taining about 250,000 words.
The French and English documents were then
normalised through the following linguistic pre-
processing steps: tokenisation, part-of-speech tag-
ging, and lemmatisation. These steps were car-
ried out using the TTC TermSuite
4
that applies
the same method to several languages including
French and English. Finally, the function words
were removed and the words occurring less than
twice in the French part and in each English part
were discarded. Table 3 shows the number of dis-
tinct words (# words) after these steps. It also
indicates the comparability degree in percentage
(comp.) between the French part and each English
part of each comparable corpus. The comparabil-
ity measure (Li and Gaussier, 2010) is based on
the expectation of finding the translation for each
word in the corpus and gives a good idea about
how two corpora are comparable. We can notice
that all the comparable corpora have a high degree
of comparability with a better comparability of the
breast cancer corpora as opposed to the diabetes
corpora. In the remainder of this article, [breast
cancer corpus i] for instance stands for the breast
cancer comparable corpus composed of the unique
French part and the English part i (i ? [1, 14]).
3.2 Bilingual Dictionary
The bilingual dictionary used in our experiments
is the French/English dictionary ELRA-M0033
3
http://www.ncbi.nlm.nih.gov/pubmed/
4
http://code.google.com/p/ttc-project
Breast cancer Diabetes
# words (comp.) # words (comp.)
French
Part 1 7,376 4,982
English
Part 1 8,214 (79.2) 5,181 (75.2)
Part 2 7,788 (78.8) 5,446 (75.9)
Part 3 8,370 (78.8) 5,610 (76.6)
Part 4 7,992 (79.3) 5,426 (74.8)
Part 5 7,958 (78.7) 5,610 (75.0)
Part 6 8,230 (79.1) 5,719 (73.6)
Part 7 8,035 (78.3) 5,362 (75.6)
Part 8 8,008 (78.8) 5,432 (74.6)
Part 9 8,334 (79.6) 5,398 (74.2)
Part 10 7,978 (79.1) 5,059 (75.6)
Part 11 8,373 (79.4) 5,264 (74.9)
Part 12 8,065 (78.9) 4,644 (73.4)
Part 13 7,847 (80.0) 5,369 (74.8)
Part 14 8,457 (78.9) 5,669 (74.8)
Table 3: Number of distinct words (# words) and
degree of comparability (comp.) for each compa-
rable corpora
available from the ELRA catalogue
5
. This re-
source is a general language dictionary which con-
tains only a few terms related to the medical do-
main.
3.3 Terminology Reference Lists
To evaluate the quality of terminology extrac-
tion, we built a bilingual terminology reference
list for each comparable corpus. We selected
all French/English single words from the UMLS
6
meta-thesaurus. We kept only i) the French sin-
gle words which occur more than four times in the
French part and ii) the English single words which
occur more than four times in each English part
i
7
. As a result of filtering, 169 French/English
single words were extracted for the breast can-
cer corpus and 244 French/English single words
were extracted for the diabetes corpus. It should
be noted that the evaluation of terminology ex-
traction using specialized comparable corpora of-
5
http://www.elra.info/
6
http://www.nlm.nih.gov/research/umls
7
The threshold sets to four is required to build a bilin-
gual terminology reference list composed of about a hundred
words. This value is very low to obtain representative context
vectors. For instance, Prochasson and Fung (2011) showed
that the standard approach is not relevant for infrequent words
(since the context vectors are very unrepresentative i.e. poor
in information).
1288
Breast cancer corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1
Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6
Diabetes corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3
Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9
Table 4: Results (MAP %) of the standard approach using the balanced and unbalanced comparable
corpora
ten relies on lists of a small size: 95 single
words in Chiao and Zweigenbaum (2002), 100 in
Morin et al (2007), 125 and 79 in Bouamor et
al. (2013).
4 Experiments and Results
In this section, we present experiments to evaluate
the influence of comparable corpus size and pre-
diction models on the quality of bilingual termi-
nology extraction.
We present the results obtained for the terms be-
longing to the reference list for English to French
direction measured in terms of the Mean Average
Precision (MAP) (Manning et al, 2008) as fol-
lows:
MAP (Ref) =
1
|Ref |
|Ref |
?
i=1
1
r
i
(7)
where |Ref | is the number of terms of the refer-
ence list and r
i
the rank of the correct candidate
translation i.
4.1 Standard Approach Evaluation
In order to evaluate the influence of corpus size on
the bilingual terminology extraction task, two ex-
periments have been carried out using the standard
approach. We first performed an experiment using
each comparable corpus independently of the oth-
ers (we refer to these corpora as balanced corpora).
We then conducted a second experiment where we
varied the size of the English part of the compara-
ble corpus, from 530,000 to 7.4 million words for
the breast cancer corpus in 530,000 words steps,
and from 250,000 to 3.5 million words for the di-
abetes corpus in 250,000 words steps (we refer to
these corpora as unbalanced corpora). In the ex-
periments reported here, the size of the context
window w was set to 3 (i.e. a seven-word window
that approximates syntactic dependencies), the re-
tained association and similarity measures were
the discounted log-odds and the Cosine (see Sec-
tion 2.1). The results shown were those that give
the best performance for the comparable corpora
used individually.
Table 4 shows the results of the standard ap-
proach on the balanced and the unbalanced breast
cancer and diabetes comparable corpora. Each
column corresponds to the English part i (i ?
[1, 14]) of a given comparable corpus. The first
line presents the results for each individual com-
parable corpus and the second line presents the re-
sults for the cumulative comparable corpus. For
instance, the column 3 indicates theMAP obtained
by using a comparable corpus that is composed i)
only of [breast cancer corpus 3] (MAP of 21.0%),
and ii) of [breast cancer corpus 1, 2 and 3] (MAP
of 34.7%).
As a preliminary remark, we can notice that the
results differ noticeably according to the compa-
rable corpus used individually (MAP variation be-
tween 21.0% and 29.6% for the breast cancer cor-
pora and between 10.5% and 16.5% for the dia-
betes corpora). We can also note that the MAP
of all the unbalanced comparable corpora is al-
ways higher than any individual comparable cor-
pus. Overall, starting with a MAP of 26.1% as
provided by the balanced [breast cancer corpus 1],
we are able to increase it to 42.3% with the un-
balanced [breast cancer corpus 12] (the variation
observed for some unbalanced corpora such as
[diabetes corpus 12, 13 and 14] can be explained
by the fact that adding more data in the source
language increases the error rate of the translation
phase of the standard approach, which leads to the
introduction of additional noise in the translated
context vectors).
1289
Balanced breast cancer corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
No prediction 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1
Source
pred
26.5 26.0 23.0 30.0 25.4 30.1 28.3 29.4 32.1 24.9 24.4 30.5 30.1 29.0
Target
pred
19.5 20.0 17.2 23.4 19.9 23.1 21.4 21.6 24.1 19.3 18.1 26.6 24.3 22.6
Source
pred
+ Target
pred
23.9 21.9 20.5 25.8 23.5 25.3 24.1 26.1 27.4 22.5 21.0 25.6 28.5 24.6
Balanced diabetes corpus
1 2 3 4 5 6 7 8 9 10 11 12 13 14
No prediction 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3
Source
pred
13.9 14.3 12.6 15.5 14.9 10.9 17.6 11.1 14.0 14.2 16.4 13.3 13.5 15.7
Target
pred
09.8 09.0 08.3 11.9 10.1 08.0 15.9 07.3 10.8 10.0 10.1 08.8 10.8 10.2
Source
pred
+ Target
pred
10.9 11.0 09.0 13.6 11.8 08.6 15.4 07.7 12.8 11.5 11.9 10.5 11.7 11.8
Table 5: Results (MAP %) of the standard approach using the Lin regression model on the balanced
breast cancer and diabetes corpora (comparison of predicting the source side, the target side and both
sides of the comparable corpora)
4.2 Prediction Evaluation
The aim of this experiment is two-fold: first, we
want to evaluate the usefulness of predicting word
co-occurrence counts and second, we want to find
out whether it is more appropriate to apply predic-
tion to the source side, the target side or both sides
of the bilingual comparable corpora.
Breast cancer Diabetes
No prediction 29.6 16.5
Lin 30.5 17.6
Poly
2
30.6 17.5
Poly
3
30.4 17.6
Logit 22.3 13.6
Table 6: Results (MAP %) of the standard ap-
proach using different regression models on the
balanced breast cancer and diabetes corpora
4.2.1 Regression Models Comparison
We contrast the prediction models presented in
Section 2.2 to findout which is the most appropri-
ate model to use as a pre-processing step of the
standard approach. We chose the balanced corpora
where the standard approach has shown the best
results in the previous experiment, namely [breast
cancer corpus 12] and [diabetes corpus 7].
Table 6 shows a comparison between the
standard approach without prediction noted No
prediction and the standard approach with pre-
diction models. We contrast the simple linear re-
gression model (Lin) with the second and the third
order polynomial regressions (Poly
2
and Poly
3
)
and the logistic regression model (Logit). We
can notice that except for the Logit model, all the
regression models outperform the baseline (No
prediction). Also, as we can see, the results
obtained with the linear and polynomial regres-
sions are very close. This suggests that both linear
and polynomial regressions are suitable as a pre-
processing step of the standard approach, while
the logistic regression seems to be inappropriate
according to the results shown in Table 6.
That said, the gain of regression models is not
significant. This may be due to the regression pa-
rameters that have been learned from a training
corpus of the general domain. Another reason that
could explain these results is the prediction pro-
cess. We applied the same regression function
to all co-occurrence counts while learning mod-
els for low and high frequencies should have been
more appropriate. In the light of the above results,
we believe that prediction can be beneficial to our
task.
4.2.2 Source versus Target Prediction
Table 5 shows a comparison between the standard
approach without prediction noted No prediction
and the standard approach based on the predic-
tion of the source side noted Source
pred
, the tar-
get side noted Target
pred
and both sides noted
Source
pred
+Target
pred
. If prediction can not re-
place a large amount of data, it aims at increasing
co-occurrence counts as if large amounts of data
were at our disposal. In this case, applying pre-
diction to the source side may simulate a config-
uration of using unbalanced comparable corpora
where the source side is n times bigger than the
target side. Predicting the target side only, may
1290
1 2 3 4 5 6 7 8 9 10 11 12 13 140
5
10
15
20
25
30
35
40
45
50
[English-i]-French breast cancer corpus
M
AP
(%
)
Balanced
Balanced+Prediction
Unbalanced
Unbalanced+Prediction
(a)
1 2 3 4 5 6 7 8 9 10 11 12 13 140
5
10
15
20
25
30
35
[English-i]-French diabetes corpus
M
AP
(%
)
Balanced
Balanced+Prediction
Unbalanced
Unbalanced+Prediction
(b)
Figure 1: Results (MAP %) of the standard approach using the best configurations of the prediction
models (Lin for Balanced + Prediction and Poly
2
for Unbalanced + Prediction) on the breast
cancer and the diabetes corpora
leads us to the opposite configuration where the
target side is n times bigger than the source side.
Finally, predicting both sides may simulate a large
comparable corpora on both sides. In this experi-
ment, we chose to use the linear regression model
(Lin) for the prediction part. That said, the other
regression models have shown the same behavior
as Lin.
We can see that the best results are obtained by
the Source
pred
approach for both comparable cor-
pora. We can also notice that predicting the tar-
get side and both sides of the comparable corpora
degrades the results. It is not surprising that pre-
dicting the target side only leads to lower results,
since it is well known that a better characterization
of a word to translate (given from the source side)
leads to better results. We can deduce from Ta-
ble 5 that source prediction is the most appropriate
configuration to improve the quality of extracted
lexicons. This configuration which simulates the
use of unbalanced corpora leads us to think that
using prediction with unbalanced comparable cor-
pora should also increase the performance of the
standard approach. This assumption is evaluated
in the next Subsection.
4.3 Predicting Unbalanced Corpora
In this last experiment we contrast the standard
approach applied to the balanced and unbalanced
corpora noted Balanced and Unbalanced with
the standard approach combined with the predic-
tion model noted Balanced + Prediction and
Unbalanced + Prediction.
Figure 1(a) illustrates the results of the exper-
iments conducted on the breast cancer corpus.
We can see that the Unbalanced approach sig-
nificantly outperforms the baseline (Balanced).
The big difference between the Balanced and
the Unbalanced approaches would indicate that
the latter is optimal. We can also notice that the
prediction model applied to the balanced corpus
(Balanced + Prediction) slightly outperforms
the baseline while the Unbalanced+Prediction
approach significantly outperforms the three other
approaches (moreover the variation observed with
the Unbalanced approach are lower than the
Unbalanced + Prediction approach). Overall,
the prediction increases the performance of the
standard approach especially for unbalanced cor-
pora.
The results of the experiments conducted on
the diabetes corpus are shown in Figure 1(b). As
for the previous experiment, we can see that the
Unbalanced approach significantly outperforms
the Balanced approach. This confirms the unbal-
anced hypothesis and would motivate the use of
unbalanced corpora when they are available. We
can also notice that the Balanced + Prediction
approach slightly outperforms the baseline while
the Unbalanced+Prediction approach gives the
best results. Here also, the prediction increases the
performance of the standard approach especially
for unbalanced corpora. It is clear that in addi-
tion to the benefit of using unbalanced comparable
1291
corpora, prediction shows a positive impact on the
performance of the standard approach.
5 Conclusion
In this paper, we have studied how an unbalanced
specialized comparable corpus could influence the
quality of the bilingual lexicon extraction. This as-
pect represents a significant interest when working
with specialized comparable corpora for which the
quantity of the data collected may differ depend-
ing on the languages involved, especially when in-
volving the English language as many scientific
documents are available. More precisely, our dif-
ferent experiments show that using an unbalanced
specialized comparable corpus always improves
the quality of word translations. Thus, the MAP
goes up from 29.6% (best result on the balanced
corpora) to 42.3% (best result on the unbalanced
corpora) in the breast cancer domain, and from
16.5% to 26.0% in the diabetes domain. Addition-
ally, these results can be improved by using a pre-
diction model of the word co-occurrence counts.
Here, the MAP goes up from 42.3% (best result
on the unbalanced corpora) to 46.9% (best result
on the unbalanced corpora with prediction) in the
breast cancer domain, and from 26.0% to 29.8%
in the diabetes domain. We hope that this study
will pave the way for using specialized unbalanced
comparable corpora for bilingual lexicon extrac-
tion.
Acknowledgments
This work is supported by the French National Re-
search Agency under grant ANR-12-CORD-0020.
References
Alan Agresti. 2007. An Introduction to Categorical
Data Analysis (2nd ed.). Wiley & Sons, Inc., Hobo-
ken, New Jersey.
Dhouha Bouamor, Nasredine Semmar, and Pierre
Zweigenbaum. 2013. Context vector disambigua-
tion for bilingual lexicon extraction from compa-
rable corpora. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL?13), pages 759?764, Sofia, Bulgaria.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING?02), pages 1208?1212, Tapei,
Taiwan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The Effect of a General Lexicon in Corpus-Based
Identification of French-English Medical Word
Translations. In The New Navigators: from Profes-
sionals to Patients, Actes Medical Informatics Eu-
rope, pages 397?402.
Ronald Christensen. 1997. Log-Linear Models and
Logistic Regression. Springer-Verlag, Berlin.
Herv?e D?ejean, Fatia Sadat, and
?
Eric Gaussier. 2002.
An approach based on multilingual thesauri and
model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING?02), pages
218?224, Tapei, Taiwan.
Mona T. Diab and Steve Finch. 2000. A Statistical
Word-Level Translation Model for Comparable Cor-
pora. In Proceedings of the 6th International Con-
ference on Computer-Assisted Information Retrieval
(RIAO?00), pages 1500?1501, Paris, France.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Universit?at Stuttgart, Germany.
Pascale Fung and Percy Cheung. 2004. Multi-
level bootstrapping for extracting parallel sentences
from a quasi-comparable corpus. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics (COLING?04), pages 1051?1057,
Geneva, Switzerland.
Pascale Fung and Kathleen McKeown. 1997. Finding
Terminology Translations from Non-parallel Cor-
pora. In Proceedings of the 5th Annual Workshop
on Very Large Corpora (VLC?97), pages 192?202,
Hong Kong.
Pascale Fung. 1995. Compiling Bilingual Lexicon
Entries from a non-Parallel English-Chinese Cor-
pus. In Proceedings of the 3rd Annual Workshop
on Very Large Corpora (VLC?95), pages 173?183,
Cambridge, MA, USA.
Pascale Fung. 1998. A Statistical View on Bilin-
gual Lexicon Extraction: From Parallel Corpora to
Non-parallel Corpora. In David Farwell, Laurie
Gerber, and Eduard Hovy, editors, Proceedings of
the 3rd Conference of the Association for Machine
Translation in the Americas (AMTA?98), pages 1?
16, Langhorne, PA, USA.
?
Eric Gaussier, Jean-Michel Renders, Irena Matveeva,
Cyril Goutte, and Herv?e D?ejean. 2004. A
Geometric View on Bilingual Lexicon Extraction
from Comparable Corpora. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL?04), pages 526?533,
Barcelona, Spain.
Gregory Grefenstette. 1994. Corpus-Derived First,
Second and Third-Order Word Affinities. In Pro-
ceedings of the 6th Congress of the European As-
sociation for Lexicography (EURALEX?94), pages
279?290, Amsterdam, The Netherlands.
1292
Amir Hazem and Emmanuel Morin. 2013. Word
co-occurrence counts prediction for bilingual ter-
minology extraction from comparable corpora. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing (IJCNLP?13),
pages 1392?1400, Nagoya, Japan.
Azniah Ismail and Suresh Manandhar. 2010. Bilingual
lexicon extraction from comparable corpora using
in-domain terms. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING?10), pages 481?489, Beijing, China.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 Workshop on Unsuper-
vised Lexical Acquisition (ULA?02), pages 9?16,
Philadelphia, PA, USA.
Audrey Laroche and Philippe Langlais. 2010. Revis-
iting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics (COLING?10), pages
617?625, Beijing, China.
Bo Li and
?
Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (COLING?10), pages 644?652, Beijing, China.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Anthony McEnery and Zhonghua Xiao. 2007. Paral-
lel and comparable corpora: What are they up to?
In Gunilla Anderman and Margaret Rogers, editors,
Incorporating Corpora: Translation and the Lin-
guist, Multilingual Matters, chapter 2, pages 18?31.
Clevedon, UK.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora (BUCC?11), pages 27?34, Portland,
OR, USA.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual Terminology
Mining ? Using Brain, not brawn comparable cor-
pora. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?07), pages 664?671, Prague, Czech Republic.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable english and spanish corpora.
In Proceedings of the 11th Conference on Machine
Translation Summit (MT Summit XI), pages 191?
198, Copenhagen, Denmark.
Carol Peters and Eugenio Picchi. 1998. Cross-
language information retrieval: A system for com-
parable corpus querying. In Gregory Grefenstette,
editor, Cross-language information retrieval, chap-
ter 7, pages 81?90. Kluwer Academic Publishers.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compa-
rable Documents. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?11), pages 1327?1335, Portland, OR,
USA.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexicon
extraction from small comparable corpora. In Pro-
ceedings of the 12th Conference on Machine Trans-
lation Summit (MT Summit XII), pages 284?291, Ot-
tawa, Canada.
Reinhard Rapp. 1995. Identify Word Translations in
Non-Parallel Texts. In Proceedings of the 35th An-
nual Meeting of the Association for Computational
Linguistics (ACL?95), pages 320?322, Boston, MA,
USA.
Reinhard Rapp. 1999. Automatic Identification of
Word Translations from Unrelated English and Ger-
man Corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?99), pages 519?526, College Park,
MD, USA.
Rapha?el Rubino and Georges Linar`es. 2011. A multi-
view approach for term translation spotting. In Pro-
ceedings of the 12th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing?11), pages 29?40, Tokyo, Japan.
Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction
of Lexical Translations from Non-Aligned Corpora.
In Proceedings of the 16th International Conference
on Computational Linguistics (COLING?96), pages
580?585, Copenhagen, Denmark.
Kun Yu and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of the
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT?09),
pages 121?124, Boulder, CO, USA.
1293
Bilingual Lexicon Extraction from Comparable Corpora as Metasearch
Amir Hazem and Emmanuel Morin
Universite? de Nantes,
LINA - UMR CNRS 6241
2 rue de la Houssinie`re,
BP 92208 44322 Nantes Cedex 03
amir.hazem@univ-nantes.fr
emmanuel.morin@univ-nantes.fr
Sebastian Pen?a Saldarriaga
1100 rue Notre-Dame Ouest,
Montre?al, Que?bec,
Canada H3C 1K3
spena@synchromedia.ca
Abstract
In this article we present a novel way of look-
ing at the problem of automatic acquisition
of pairs of translationally equivalent words
from comparable corpora. We first present
the standard and extended approaches tradi-
tionally dedicated to this task. We then re-
interpret the extended method, and motivate a
novel model to reformulate this approach in-
spired by the metasearch engines in informa-
tion retrieval. The empirical results show that
performances of our model are always better
than the baseline obtained with the extended
approach and also competitive with the stan-
dard approach.
1 Introduction
Bilingual lexicon extraction from comparable cor-
pora has received considerable attention since the
1990s (Rapp, 1995; Fung, 1998; Fung and Lo,
1998; Peters and Picchi, 1998; Rapp, 1999; Chiao
and Zweigenbaum, 2002a; De?jean et al, 2002;
Gaussier et al, 2004; Morin et al, 2007; Laroche
and Langlais, 2010, among others). This attention
has been motivated by the scarcity of parallel cor-
pora, especially for countries with only one official
language and for language pairs not involving En-
glish. Furthermore, as a parallel corpus is com-
prised of a pair of texts (a source text and a translated
text), the vocabulary appearing in the translated text
is highly influenced by the source text, especially in
technical domains. Consequently, comparable cor-
pora are considered by human translators to be more
trustworthy than parallel corpora (Bowker and Pear-
son, 2002). Comparable corpora are clearly of use
in the enrichment of bilingual dictionaries and the-
sauri (Chiao and Zweigenbaum, 2002b; De?jean et
al., 2002), and in the improvement of cross-language
information retrieval (Peters and Picchi, 1998).
According to (Fung, 1998), bilingual lexicon
extraction from comparable corpora can be ap-
proached as a problem of information retrieval (IR).
In this representation, the query would be the word
to be translated, and the documents to be found
would be the candidate translations of this word. In
the same way that as documents found, the candi-
date translations are ranked according to their rele-
vance (i.e. a document that best matches the query).
More precisely, in the standard approach dedicated
to bilingual lexicon extraction from comparable cor-
pora, a word to be translated is represented by a
vector context composed of the words that appear
in its lexical context. The candidate translations
for a word are obtained by comparing the translated
source context vector with the target context vectors
through a general bilingual dictionary. Using this
approach, good results on single word terms (SWTs)
can be obtained from large corpora of several million
words, with an accuracy of about 80% for the top 10-
20 proposed candidates (Fung and McKeown, 1997;
Rapp, 1999). Cao and Li (2002) have achieved 91%
accuracy for the top three candidates using the Web
as a comparable corpus. Results drop to 60% for
SWTs using specialized small size language cor-
pora (Chiao and Zweigenbaum, 2002a; De?jean and
Gaussier, 2002; Morin et al, 2007).
In order to avoid the insufficient coverage of the
bilingual dictionary required for the translation of
source context vectors, an extended approach has
35
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 35?43,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
been proposed (De?jean et al, 2002; Daille and
Morin, 2005). This approach can be seen as a query
reformulation process in IR for which similar words
are substituted for the word to be translated. These
similar words share the same lexical environments
as the word to be translated without appearing with
it. With the extended approach, (De?jean et al, 2002)
obtained for single French-English words 43% and
51% precision out of the ten and twenty first candi-
dates applied to a medical corpus of 100 000 words
(respectively 44% and 57% with the standard ap-
proach) and 79% and 84% precision on the ten and
twenty first candidates applied to a social science
corpus of 8 million words (respectively 35% and
42% with the standard approach). Within this con-
text, we want to show how metasearch engines can
be used for bilingual lexicon extraction from spe-
cialized comparable corpora. In particular, we will
focus on the use of different strategies to take full
advantage of similar words.
The remainder of this paper is organized as fol-
lows. Section 2 presents the standard and extended
approaches based on lexical context vectors dedi-
cated to word alignment from comparable corpora.
Section 3 describes our metasearch approach that
can be viewed as the combination of different search
engines. Section 4 describes the different linguistic
resources used in our experiments and evaluates the
contribution of the metasearch approach on the qual-
ity of bilingual terminology extraction through dif-
ferent experiments. Finally, Section 5 presents our
conclusions.
2 Related Work
In this section, we first describe the standard ap-
proach dedicated to word alignment from compara-
ble corpora. We then present an extension of this
approach.
2.1 Standard Approach
The main work in bilingual lexicon extraction from
comparable corpora is based on lexical context anal-
ysis and relies on the simple observation that a word
and its translation tend to appear in the same lexi-
cal contexts. The basis of this observation consists
in the identification of first-order affinities for each
source and target language: First-order affinities de-
scribe what other words are likely to be found in
the immediate vicinity of a given word (Grefenstette,
1994a, p. 279). These affinities can be represented
by context vectors, and each vector element repre-
sents a word which occurs within the window of
the word to be translated (for instance a seven-word
window approximates syntactical dependencies).
The implementation of this approach can be car-
ried out by applying the following four steps (Rapp,
1995; Fung and McKeown, 1997):
Context characterization
All the lexical units in the context of each lexical
unit i are collected, and their frequency in a window
of n words around i extracted. For each lexical unit
i of the source and the target languages, we obtain a
context vector i where each entry, ij , of the vector is
given by a function of the co-occurrences of units j
and i. Usually, association measures such as the mu-
tual information (Fano, 1961) or the log-likelihood
(Dunning, 1993) are used to define vector entries.
Vector transfer
The lexical units of the context vector i are trans-
lated using a bilingual dictionary. Whenever the
bilingual dictionary provides several translations for
a lexical unit, all the entries are considered but
weighted according to their frequency in the target
language. Lexical units with no entry in the dictio-
nary are discarded.
Target language vector matching
A similarity measure, sim(i, t), is used to score
each lexical unit, t, in the target language with re-
spect to the translated context vector, i. Usual mea-
sures of vector similarity include the cosine similar-
ity (Salton and Lesk, 1968) or the weighted jaccard
index (WJ) (Grefenstette, 1994b) for instance.
Candidate translation
The candidate translations of a lexical unit are the
target lexical units ranked following the similarity
score.
2.2 Extended Approach
The main shortcoming of the standard approach is
that its performance greatly relies on the coverage of
the bilingual dictionary. When the context vectors
36
mot Identify k
similar vectors
Source language Target language
mot word
mot word
mot word
mot word
mot word
...
...
word
word
word
Match vectors
in target language ...
Bilingual dictionary
Figure 1: Illustration of the extended approach.
are well translated, the translation retrieval rate in
the target language improves.
Although, the coverage of the bilingual dictionary
can be extended by using specialized dictionaries
or multilingual thesauri (Chiao and Zweigenbaum,
2003; De?jean et al, 2002), translation of context
vectors remains the core of the approach.
In order to be less dependent on the coverage
of the bilingual dictionary, De?jean and Gaussier
(2002) have proposed an extension to the standard
approach. The basic intuition of this approach is
that words sharing the same meaning will share
the same environments. The approach is based
on the identification of second-order affinities in
the source language: Second-order affinities show
which words share the same environments. Words
sharing second-order affinities need never appear
together themselves, but their environments are sim-
ilar (Grefenstette, 1994a, p. 280).
Generally speaking, a bilingual dictionary is a
bridge between two languages established by its en-
tries. The extended approach is based on this ob-
servation and avoids explicit translation of vectors
as shown in Figure 1. The implementation of this
extended approach can be carried out in four steps
where the first and last steps are identical to the stan-
dard approach (De?jean and Gaussier, 2002; Daille
and Morin, 2005):
Reformulation in the target language
For a lexical unit i to be translated, we identify
the k-nearest lexical units (k nlu), among the dic-
tionary entries corresponding to words in the source
language, according to sim(i, s). Each nlu is trans-
lated via the bilingual dictionary, and the vector in
the target language, s, corresponding to the transla-
tion is selected. If the bilingual dictionary provides
several translations for a given unit, s is given by
the union of the vectors corresponding to the trans-
lations. It is worth noting that the context vectors are
not translated directly, thus reducing the influence of
the dictionary.
Vector matching against reformulations
The similarity measure, sim(s, t), is used to score
each lexical unit, t, in the target language with re-
spect to the k nlu. The final score assigned to each
unit, t, in the target language is given by:
sim(i, t) = ?
s?kNLU
sim(i, s)? sim(s, t) (1)
An alternate scoring function has been proposed
by Daille and Morin (2005). The authors computed
the centroid vector of the k nlu, then scored target
units with respect to the centroid.
3 The Metasearch Approach
3.1 Motivations
The approach proposed by De?jean and Gaussier
(2002) implicitly introduces the problem of select-
ing a good k. Generally, the best choice of k depends
on the data. Although several heuristic techniques,
like cross-validation, can be used to select a good
value of k, it is usually defined empirically.
The application of the extended approach (EA) to
our data showed that the method is unstable with
respect to k. In fact, for values of k over 20, the
precision drops significantly. Furthermore, we can-
not ensure result stability within particular ranges of
37
values. Therefore, the value of k should be carefully
tuned.
Starting from the intuition that each nearest lexi-
cal unit (nlu) contributes to the characterization of a
lexical unit to be translated, our proposition aims at
providing an algorithm that gives a better precision
while ensuring higher stability with respect to the
number of nlu. Pushing the analogy of IR style ap-
proaches (Fung and Lo, 1998) a step further, we pro-
pose a novel way of looking at the problem of word
translation from comparable corpora that is concep-
tually simple: a metasearch problem.
In information retrieval, metasearch is the prob-
lem of combining different ranked lists, returned
by multiple search engines in response to a given
query, in such a way as to optimize the performance
of the combined ranking (Aslam and Montague,
2001). Since the k nlu result in k distinct rankings,
metasearch provides an appropriate framework for
exploiting information conveyed by the rankings.
In our model, we consider each list of a given nlu
as a response of a search engine independently from
the others. After collecting all the lists of the se-
lected nlu?s, we combine them to obtain the final
similarity score. It is worth noting that all the lists
are normalized to maximize in such a way the con-
tribution of each nlu. A good candidate is the one
that obtains the highest similarity score which is cal-
culated with respect to the selected k. If a given can-
didate has a high frequency in the corpus, it may be
similar not only to the selected nearest lexical units
(k), but also to other lexical units of the dictionary. If
the candidate is close to the selected nlu?s and also
close to other lexical units, we consider it as a po-
tential noise (the more neighbours a candidate has,
the more it?s likely to be considered as noise). We
thus weight the similarity score of a candidate by
taking into account this information. We compare
the distribution of the candidate with the k nlu and
also with all its neighbours. This leads us to sup-
pose that a good candidate should be closer to the
selected nlu?s than the rest of its neighbours, if it?s
not the case there is more chances for this candidate
to be a wrong translation.
3.2 Proposed Approach
In the following we will describe our extension
to the method proposed by De?jean and Gaussier
(2002). The notational conventions adopted are re-
viewed in Table 1. Elaborations of definitions will
be given when the notation is introduced. In all our
experiments both terms and lexical units are single
words.
Symbol Definition
l a list of a given lexical unit.
k the number of selected nearest lex-
ical units (lists).
freq(w, k) the number of lists (k) in which a
term appears.
n all the neighbours of a given term.
u all the lexical units of the dictio-
nary.
wl a term of a given list l.
s(wl) the score of the term w in the list l.
maxl the maximum score of a given list
l.
maxAll the maximum score of all the lists.
snorm(wl) the normalized score of term w in
the list l.
s(w) the final score of a term w.
?w the regulation parameter of the
term w.
Table 1: Notational conventions.
The first step of our method is to collect each
list of each nlu. The size of the list has its impor-
tance because it determines how many candidates
are close to a given nlu. We noticed from our ex-
periments that, if we choose lists with small sizes,
we should lose information and if we choose lists
with large sizes, we could keep more information
than necessary and this should be a potential noise,
so we consider that a good size of each list should
be between 100 and 200 terms according to our ex-
periments.
After collecting the lists, the second step is to nor-
malize the scores. Let us consider the equation 2 :
snorm(wl) = s(wl)? maxlmaxAll (2)
We justify this by a rationale derived from two
observations. First, scores in different rankings are
compatible since they are based on the same simi-
larity measure (i.e., on the same scale). The second
observations follows from the first: if max (l) 
38
max (m), then the system is more confident about
the scores of the list l than m.
Using scores as fusion criteria, we compute the
similarity score of a candidate by summing its scores
from each list of the selected nlu?s :
s(w) = ?w ?
?k
l=1 snorm(wl)?n
l=1 snorm(wl)
(3)
the weight ? is given by :
?w = freq(w, k)? (u? (k ? freq(w, k)))(u? freq(w, n)) (4)
The aim of this parameter is to give more con-
fidence to a term that occurs more often with the
selected nearest neighbours (k) than the rest of its
neighbours. We can not affirm that the best candi-
date is the one that follows this idea, but we can nev-
ertheless suppose that candidates that appear with a
high number of lexical units are less confident and
have higher chances to be wrong candidates (we can
consider those candidates as noise). So, ? allows us
to regulate the similarity score, it is used as a confi-
dent weight or a regulation parameter. We will refer
to this model as the multiple source (MS) model. We
also use our model without using ? and refer to it by
(LC), this allows us to show the impact of ? in our
results.
4 Experiments and Results
4.1 Linguistic Resources
We have selected the documents from the Elsevier
website1 in order to obtain a French-English spe-
cialized comparable corpus. The documents were
taken from the medical domain within the sub-
domain of ?breast cancer?. We have automatically
selected the documents published between 2001 and
2008 where the title or the keywords contain the
term ?cancer du sein? in French and ?breast can-
cer? in English. We thus collected 130 documents
in French and 118 in English and about 530,000
words for each language. The documents compris-
ing the French/English specialized comparable cor-
pus have been normalized through the following lin-
guistic pre-processing steps: tokenisation, part-of-
1www.elsevier.com
speech tagging, and lemmatisation. Next, the func-
tion words were removed and the words occurring
less than twice (i.e. hapax) in the French and the
English parts were discarded. Finally, the compara-
ble corpus comprised about 7,400 distinct words in
French and 8,200 in English.
The French-English bilingual dictionary required
for the translation phase was composed of dictionar-
ies that are freely available on the Web. It contains,
after linguistic pre-processing steps, 22,300 French
single words belonging to the general language with
an average of 1.6 translations per entry.
In bilingual terminology extraction from special-
ized comparable corpora, the terminology refer-
ence list required to evaluate the performance of
the alignment programs are often composed of 100
single-word terms (SWTs) (180 SWTs in (De?jean
and Gaussier, 2002), 95 SWTs in (Chiao and
Zweigenbaum, 2002a), and 100 SWTs in (Daille
and Morin, 2005)). To build our reference list,
we selected 400 French/English SWTs from the
UMLS2 meta-thesaurus and the Grand dictionnaire
terminologique3. We kept only the French/English
pair of SWTs which occur more than five times in
each part of the comparable corpus. As a result of
filtering, 122 French/English SWTs were extracted.
4.2 Experimental Setup
Three major parameters need to be set to the ex-
tended approach, namely the similarity measure, the
association measure defining the entry vectors and
the size of the window used to build the context vec-
tors. Laroche and Langlais (2010) carried out a com-
plete study about the influence of these parameters
on the quality of bilingual alignment.
As similarity measure, we chose to use the
weighted jaccard index:
sim(i, j) =
?
t min (it, jt)?
t max (it, jt)
(5)
The entries of the context vectors were deter-
mined by the log-likelihood (Dunning, 1993), and
we used a seven-word window since it approximates
syntactic dependencies. Other combinations of pa-
rameters were assessed but the previous parameters
turned out to give the best performance.
2http://www.nlm.nih.gov/research/umls
3http://www.granddictionnaire.com/
39
4.3 Results
To evaluate the performance of our method, we use
as a baseline, the extended approach (EA) proposed
by De?jean and Gaussier (2002). We compare this
baseline to the two metasearch strategies defined
in Section 3: the metasearch model without the
regulation parameter ? (LC); and the one which is
weighted by theta (MS). We also provide results ob-
tained with the standard approach (SA).
We first investigate the stability of the metasearch
strategies with respect to the number of nlu consid-
ered. Figure 2 show the precision at Top 20 as a
function of k.
1 10 20 30 40 50 600
10
20
30
40
50
60
70
Number of NLU
Pre
cisi
on
att
op
20
LC
MS
EA
SA
Figure 2: Precision at top 20 as a function of the number
of nlu.
In order to evaluate the contribution of the param-
eter ?, we chose to evaluate the metasearch method
starting from k = 4, this explains why the precision
is extremely low for low values of k. We further
considered that less than four occurrences of a term
in the whole lexical units lists can be considered as
noise. On the other side, we started from k = 1 for
the extended approach since it makes no use of the
parameter ?. Figure 2 shows that extended approach
reaches its best performance at k = 7 with a preci-
sion of 40.98%. Then, after k = 15 the precision
starts steadily decreasing as the value of k increases.
The metasearch strategy based only on similarity
scores shows better results than the baseline. For
every value of k ? 10, the LC model outperform the
extended approach. The best precision (48.36%) is
obtained at k = 14, and the curve corresponding to
the LC model remains above the baseline regardless
of the increasing value of the parameter k. The curve
corresponding to the MS model is always above the
(EA) for every value of k ? 10. The MS model
consistently improves the precision, and achieves its
best performance (60.65%) at k = 21.
We can notice from Figure 2 that the LC and MS
models outperform the baseline (EA). More impor-
tantly, these models exhibit a better stability of the
precision with respect to the k-nearest lexical units.
Although the performance decrease as the value of k
increases, it does not decrease as fast as in the base-
line approach.
For the sake of comparability, we also provide
results obtained with the standard approach (SA)
(56.55%) represented by a straight line as it is not
dependent on k. As we can see, the metasearch
approach (MS) outperforms the standard approach
for values of k bertween 20 and 30 and for greater
values of k the precision remains more or less al-
most the same as the standard approach (SA). Thus,
the metasearch model (MS) can be considered as a
competitive approach regarding to its results as it is
shown in the figure 2.
Finally, Figure 3 shows the contribution of each
nlu taken independently from the others. This con-
firms our intuition that each nlu contribute to the
characterization of a lexical unit to be translated, and
supports our idea that their combination can improve
the performances.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
5
10
15
20
25
lexical units
Prec
isio
nat
top
20
Figure 3: Precision at top 20 for each of the 20 nlu. The
precision is computed by taking the each nlu indepen-
dently from the others.
Figure 3 shows the top 20 of each nlu. Notice
40
that the nlu are ordered from the most similar to the
lexical unit to be translated to the less similar, and
that each one of the nearest lexical units contains
information that it is worth taking into account.
Although each nlu can only translate few terms,
by using the metasearch idea we are able to improve
the retrieval of translation equivalents. The main
idea of the metasearch paradigm is to take into ac-
count the information conveyed by all the k nlu, us-
ing either similarity scores, their behaviour with all
the neighbours, in order to improve the performance
of the alignment process.
Although significant improvements can be ob-
tained with the metasearch models (comparatively
to the EA and SA approach), especially concerning
precision stability with respect to the k nlu, we be-
lieve that we need to address the estimation of k be-
forehand. Rather than fixing the same k for all the
units to be translated, there is the possibility to adapt
an optimal value of k to each lexical unit, according
to some criteria which have to be determined.
Approachs Top 5 Top 10 Top 15 Top 20
SA 37.70 45.08 52.45 56.55
EA 21.31 31.14 36.88 40.98
MS 40.98 54.91 56.55 60.65
Table 2: Precision(%) at top 5, 10, 15, 20 for SA, EA and
MS.
Finally, we present in table 2 a comparison be-
tween SA, EA and MS for the top 5, 10, 15 and 20.
By choosing the best configuration of each method,
we can note that our method outperforms the others
in each top. In addition, for the top 10 our preci-
sion is very close to the precision of the standard ap-
proach (SA) at the top 20. we consider these results
as encouraging for future work.
4.4 Discussion
Our experiments show that the parameter k remains
the core of both EA and MS approaches. A good
selection of the nearest lexical units of a term guar-
antee to find the good translation. It is important to
say that EA and MS which are based on the k nlu?s
depends on the coverage of the terms to be trans-
lated. Indeed, these approaches face three cases :
firstly, if the frequency of the word to be translated
is high and the frequency of the good translation in
the target language is low, this means that the nearest
lexical units of the candidate word and its translation
are unbalanced. This leads us to face a lot of noise
because of the high frequency of the source word
that is over-represented by its nlu?s comparing to the
target word which is under-represented. Secondly,
we consider the inverse situation, which is: low fre-
quency of the source word and high frequency of
the target translation, here as well, we have both the
source and the target words that are unbalanced re-
garding to the selected nearest lexical units. The
third case, represents more or less the same distri-
bution of the frequencies of source candidate and
target good translation. This can be considered as
the most appropriate case to find the good transla-
tion by applying the approaches based on the nlu?s
(EA or MS). Our experiments show that our method
works well in all the cases by using the parameter ?
which regulate the similarity score by taken into ac-
count the distribution of the candidate according to
both : selected nlu?s and all its neighbours. In re-
sume, words to be translated as represented in case
one and two give more difficulties to be translated
because of their unbalanced distribution which leads
to an unbalanced nlu?s. Future works should con-
firm the possibility to adapt an optimal value of k
to each candidate to be translated, according to its
distribution with respect to its neighbours.
5 Conclusion
We have presented a novel way of looking at the
problem of bilingual lexical extraction from compa-
rable corpora based on the idea of metasearch en-
gines. We believe that our model is simple and
sound. Regarding the empirical results of our propo-
sition, performances of the multiple source model
on our dataset was better than the baseline proposed
by De?jean and Gaussier (2002), and also outper-
forms the standard approach for a certain range of
k. We believe that the most significant result is that
a new approach to finding single word translations
has been shown to be competitive. We hope that
this new paradigm can lead to insights that would
be unclear in other models. Preliminary tests in this
perspective show that using an appropriate value of
k for each word can improve the performance of the
lexical extraction process. Dealing with this prob-
41
lem is an interesting line for future research.
6 Acknowledgments
The research leading to these results has received
funding from the French National Research Agency
under grant ANR-08-CORD-013.
References
Javed A. Aslam and Mark Montague. 2001. Models for
Metasearch. In SIGIR ?01, proceedings of the 24th
Annual SIGIR Conference, pages 276?284.
Lynne Bowker and Jennifer Pearson. 2002. Working
with Specialized Language: A Practical Guide to Us-
ing Corpora. Routledge, London/New York.
Yunbo Cao and Hang Li. 2002. Base Noun Phrase Trans-
lation Using Web Data and the EM Algorithm. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 127?
133, Tapei, Taiwan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002a.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING?02), pages 1208?1212, Tapei, Tai-
wan.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002b.
Looking for French-English Translations in Compara-
ble Medical Corpora. Journal of the American Society
for Information Science, 8:150?154.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The
Effect of a General Lexicon in Corpus-Based Identifi-
cation of French-English Medical Word Translations.
In Robert Baud, Marius Fieschi, Pierre Le Beux, and
Patrick Ruch, editors, The New Navigators: from Pro-
fessionals to Patients, Actes Medical Informatics Eu-
rope, volume 95 of Studies in Health Technology and
Informatics, pages 397?402, Amsterdam. IOS Press.
Be?atrice Daille and Emmanuel Morin. 2005. French-
English Terminology Extraction from Comparable
Corpora. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing (IJ-
CLNP?05), pages 707?718, Jeju Island, Korea.
Herve? De?jean and E?ric Gaussier. 2002. Une nouvelle ap-
proche a` l?extraction de lexiques bilingues a` partir de
corpus comparables. Lexicometrica, Alignement lexi-
cal dans les corpus multilingues, pages 1?22.
Herve? De?jean, Fatia Sadat, and E?ric Gaussier. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics (COLING?02), pages 218?
224, Tapei, Taiwan.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61?74.
Robert M. Fano. 1961. Transmission of Information:
A Statistical Theory of Communications. MIT Press,
Cambridge, MA, USA.
Pascale Fung and Yuen Yee Lo. 1998. An ir approach for
translating new words from nonparallel, comparable
texts. In Proceedings of the 17th international con-
ference on Computational linguistics (COLING?98),
pages 414?420.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing Terminology Translations from Non-parallel Cor-
pora. In Proceedings of the 5th Annual Workshop on
Very Large Corpora (VLC?97), pages 192?202, Hong
Kong.
Pascale Fung. 1998. A Statistical View on Bilingual Lex-
icon Extraction: From ParallelCorpora to Non-parallel
Corpora. In David Farwell, Laurie Gerber, and Eduard
Hovy, editors, Proceedings of the 3rd Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA?98), pages 1?16, Langhorne, PA, USA.
E?ric Gaussier, Jean-Michel Renders, Irena Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A Geomet-
ric View on Bilingual Lexicon Extraction from Com-
parable Corpora. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?04), pages 526?533, Barcelona, Spain.
Gregory Grefenstette. 1994a. Corpus-Derived First, Sec-
ond and Third-Order Word Affinities. In Proceedings
of the 6th Congress of the European Association for
Lexicography (EURALEX?94), pages 279?290, Ams-
terdam, The Netherlands.
Gregory Grefenstette. 1994b. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publisher,
Boston, MA, USA.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference on
Computational Linguistics (COLING?10), pages 617?
625, Beijing, China.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining ?
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), pages 664?
671, Prague, Czech Republic.
Carol Peters and Eugenio Picchi. 1998. Cross-language
information retrieval: A system for comparable cor-
pus querying. In Gregory Grefenstette, editor, Cross-
language information retrieval, chapter 7, pages 81?
90. Kluwer Academic Publishers.
42
Reinhard Rapp. 1995. Identify Word Translations in
Non-Parallel Texts. In Proceedings of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?95), pages 320?322, Boston, MA, USA.
Reinhard Rapp. 1999. Automatic Identification of Word
Translations from Unrelated English and German Cor-
pora. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL?99),
pages 519?526, College Park, MD, USA.
Gerard Salton and Michael E. Lesk. 1968. Computer
evaluation of indexing and text processing. Jour-
nal of the Association for Computational Machinery,
15(1):8?36.
43
JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 61?68,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Participation du LINA ? DEFT 2012
Florian Boudin Amir Hazem Nicolas Hernandez Prajol Shrestha
Universit? de Nantes
pr?nom.nom@univ-nantes.fr
R?SUM?
Cet article pr?sente la participation de l??quipe TALN du LINA au d?fi fouille de textes (DEFT)
2012. D?velopp? sp?cifiquement pour la seconde piste du d?fi, notre syst?me combine les sorties
de trois diff?rentes m?thodes d?extraction de mots cl?s. Notre syst?me s?est class? ? la 2i e`me place
sur un total de 9 syst?mes avec une f-mesure de 21,3%.
ABSTRACT
LINA at DEFT 2012
This article presents the participation of the TALN group at LINA to the d?fi fouille de textes
(DEFT) 2012. Developed specifically for the second task, our system combines the outputs of
three different keyword extraction methods. Our system ranked 2nd out of 9 systems with a
f-measure of 21,3%.
MOTS-CL?S : extraction de mots cl?s, deft 2012, combinaison de m?thodes.
KEYWORDS: keyword extraction, deft 2012, combining methods.
1 Introduction
L?indexation automatique consiste ? identifier un ensemble de mots cl?s (e.g. mots, termes) qui
d?crit le contenu d?un document. Les mots cl?s peuvent ensuite ?tre utilis?s, entre autres, pour
faciliter la recherche d?information ou la navigation dans les collections de documents. L??dition
2012 du d?fi fouille de textes (DEFT) porte sur l?extraction automatique de mots cl?s ? partir
d?articles scientifiques parus dans le domaine des Sciences Humaines et Sociales (SHS).
L?objectif du d?fi est de retrouver, ? partir du contenu des documents (i.e. articles scientifiques),
les mots cl?s qui ont pu ?tre choisis par les auteurs. Deux diff?rentes pistes ont ?t? propos?es.
La premi?re piste consiste ? identifier dans une terminologie, les mots cl?s qui ont ?t? assign?s
aux documents. Cette terminologie regroupe l?ensemble des mots cl?s utilis?s dans la collection.
La seconde piste, de prime abord plus complexe, consiste ? extraire les mots cl?s directement ?
partir du contenu des documents. Cet article d?crit notre participation ? la seconde piste du d?fi.
Le reste de cet article est organis? comme suit. La section 2 d?crit l?ensemble de donn?es utilis?
pour la campagne d??valuation. La section 3 pr?sente les diff?rentes m?thodes que nous avons
d?velopp?es sp?cifiquement pour la seconde piste du d?fi. Nous d?crivons ensuite en section
4 nos r?sultats exp?rimentaux avant de pr?senter les m?thodes que nous avons test?es et qui
61
qui ont eu un impact nul ou n?gatif sur les r?sultats. La section 6 conclut cet article et donne
quelques perspectives de travaux futurs.
2 Description de la campagne DEFT 2012
L?ensemble de documents utilis? pour le d?fi 2012 est constitu? de 234 articles scientifiques parus
dans le domaine des SHS. Ces articles ont ?t? publi?s entre 2001 et 2008 dans quatre revues
diff?rentes. L?ensemble d?apprentissage contient 60% des documents (soit 141 articles), et celui
de test contient les 40% restants (soit 93 articles). La r?partition des quatre diff?rentes revues
dans les deux ensembles est uniforme.
Du point de vue technique, les articles sont au format XML. Ils sont structur?s en deux parties : le
r?sum? et le corps de l?article. Chaque article contient ?galement le nombre de mots cl?s indexant
son contenu. Les mots cl?s assign?s ? chaque article sont disponibles pour chacun des articles de
l?ensemble d?entra?nement.
Les syst?mes participant au d?fi sont ?valu?s ? l?aide des mesures classiques de pr?cision, rappel
et f-mesure. Pour chaque article, les mots cl?s g?n?r?s par les syst?mes sont compar?s aux mots
cl?s de r?f?rence (assign?s par les auteurs). Afin de limiter les probl?mes li?s aux diff?rentes
variations orthographiques, plusieurs traitements de normalisation (i.e. normalisation de la casse
et lemmatisation) sont appliqu?s au pr?alable aux mots cl?s. Chaque participant peut soumettre
jusqu?? trois ex?cutions par piste.
La liste ci-dessous pr?sente quelques unes des difficult?s que nous avons identifi?es dans les
articles de l?ensemble d?entra?nement.
? Articles diff?rents ayant le m?me r?sum?, e.g. les articles as_2002_007048ar et as_2002_
007053ar.
? Contenu des articles dans des langues diff?rentes et/ou m?lang?es, e.g. fran?ais et anglais
dans ttr_2008_037494ar, espagnol dans meta_2005_019927ar.
? Contenu des articles tr?s bruit? avec des probl?mes de ponctuation, de caract?res unicodes et
de segmentation en paragraphes, e.g. ci-dessous un extrait de l?article meta_2005_019840ar.
<p>Ce langage est au c.ur des pr?occupations des juristes , qui
nous rappellent r?guli?rement </p>
<p>que le droit est affaire de mots. Et cela dans tout l.univers
du droit , vers quelque c?t? que l.on se</p>
<p>tourne , dans le monde juridique anglophone - o?, pour
Mellinkoff (1963& amp;#x00A0 ;: vii), &amp;#x00A0;The law is a</p>
<p></p>
<p>dans son ensemble , la technique juridique aboutit , pour la
plus grande part , ? une question de</p>
<p>terminologie&amp;# x00A0;. Chacun pourra le v?rifier par la
consultation d.ouvrages parmi les plus r?cents et</p>
62
3 Approches
Les diff?rentes m?thodes que nous avons d?velopp?es utilisent le mot comme unit? principale.
Nous avons donc appliqu? un ensemble commun de pr?-traitements aux documents : segmenta-
tion en phrases, d?coupage en mots et ?tiquetage morpho-syntaxique. L?information structurelle
pr?sente dans chacun des documents (i.e. r?sum?, corps de l?article et paragraphes) est pr?serv?e.
Chaque paragraphe est segment? en phrases en utilisant la m?thode PUNKT de d?tection de
changement de phrases (Kiss et Strunk, 2006) mise en ?uvre dans la bo?te ? outils NLTK (Bird et
Loper, 2004). La tokenisation des phrases est effectu?e avec un outil d?velopp? en interne utili-
sant le lexique des formes fl?chies du fran?ais (lefff)1 pour l?indentification des unit?s lexicales
complexes (e.g. mots compos?s). L??tiquetage morpho-syntaxique est obtenu ? l?aide du Stanford
POS Tagger (Toutanova et al, 2003)2 entrain?e sur le French Treebank (Abeill? et al, 2003).
3.1 Syst?me 1
Ce syst?me est bas? sur du T F? IDF et trois r?gles issues du corpus d?apprentissage. La principale
question qui se pose ici est : qu?est ce qu?un mot cl? ? ou autrement dit, qu?est ce qui fait qu?un
terme a plus de chances d??tre un mot cl? qu?un autre ?
En analysant les documents du corpus d?apprentissage, nous avons relev? trois particularit?s li?es
aux mots cl?s. La premi?re concerne leur localisation dans les documents. Chaque document ?tant
divis? en deux parties qui sont : le r?sum? (ABSTRACT) et le corps du document (BODY), nous
nous sommes donc int?ress?s ? la position des mots cl?s par rapport ? ce d?coupage. Nous avons
pu constater qu?un terme apparaissant ? la fois dans le r?sum? et dans le corps du document avait
plus de chances d??tre un mot cl?. Ainsi, nous avons utilis? cette information comme premi?re
r?gle de notre syst?me (nous appellerons cette r?gle : R1). Deux strat?gies utilisant cette r?gle
ont ?t? adopt?es, la premi?re consiste ? ne s?lectionner que des termes qui apparaissent ? la
fois dans le r?sum? et dans le corps du document (nous appellerons cette strat?gie : S1), la
deuxi?me consiste ? donner la priorit? aux termes respectant la strat?gie S1 en utilisant une
pond?ration par un param?tre ? fix? empiriquement (nous appellerons cette strat?gie : S2).
Les diff?rents tests conduits ont montr? que l?utilisation de la strat?gie S1 donnait de meilleurs
r?sultats que l?utilisation de la strat?gie S2. Intuitivement, nous aurions tendance ? penser le
contraire (la strat?gie S2 devrait ?tre meilleur que S1), car ?liminer des termes n?apparaissant
que dans le r?sum? ou que dans le corps du document nous ferait sans doute perdre des mots
cl?s. L?explication et que la strat?gie S1 corrige sans doute les faiblesses de notre syst?me qui
renverrait plus de faux positifs que de vrais n?gatifs.
La deuxi?me particularit? relative aux n-grammes, d?coule de la question suivante : est ce qu?un
terme simple (1-gramme) a plus de chances d??tre un mot cl? qu?un terme compos? (n-grammes
avec n > 1) ? De part le corpus d?apprentissage nous avons pu constater qu?il y avait 70% de
termes simples et 30% de termes compos?s. Ainsi, nous avons voulu donner une plus grande
importance aux termes simples extraits par notre syst?me. De la m?me mani?re que pour la
strat?gie S2, nous avons introduit un param?tre de pond?ration ? afin de prioriser les termes
simples (nous appellerons cette r?gle R2).
1
http://www.labri.fr/perso/clement/lefff/2Nous utilison la version 3.1.0 avec les param?tres par d?faut.
63
La troisi?me particularit? rel?ve de la simple observation que la quasi totalit? des mots cl?s
?taient soit des noms, soit des adjectifs. ? partir de cette constatation, nous avons introduit une
troisi?me r?gle (R3) qui filtre les verbes.
3.2 Syst?me 2
Ce syst?me repose sur l?exploitation d?un existant ? savoir l?approche KEA (Keyphrase Extraction
Algorithm) de (Witten et al, 1999). KEA permet d?une part de mod?liser les expressions significa-
tives (compos?es d?un ou plusieurs mots) du contenu de textes ? l?aide de textes et d?expressions
cl?s associ?es et d?autre part d?extraire les expressions cl?s d?une collection de textes ? l?aide
d?une mod?lisation construite a priori. L?approche utilise un classifieur bay?sien na?f pour calculer
un score de probabilit? de chaque expression cl? candidate. La construction requiert un ensemble
d?expressions cl?s class?es positivement pour chaque texte du corpus d?apprentissage. L?extraction
se r?alise sur un corpus de domaine similaire au domaine du corpus d?apprentissage.
Les phases de mod?lisation ou d?extraction des expressions cl?s fonctionnent toutes deux ? la
suite de deux phases ?l?mentaires : l?extraction de candidats et le calcul de traits descriptifs
des candidats. Les candidats s?obtiennent par extraction de n-grammes de taille pr?d?finie ne
d?butant pas et ne finissant pas par un mot outil.
Les traits utilis?s pour d?crire chaque candidat au sein d?un document sont les suivants : le
T F ? IDF (mesure de sp?cificit? du candidat pour le document), la position de la premi?re
occurrence (pourcentage du texte pr?c?dent l?occurrence), le nombre de mots qui compose le
candidat. Les candidats ayant un haut T F ? IDF , apparaissant au d?but d?un texte et comptant
le plus de mots sont ainsi consid?r?s comme ?tant de bons descripteurs du contenu d?un texte.
Les derni?res ?volutions de KEA permettent d?exploiter des lexiques contr?l?s de type th?saurus
dans la construction de la mod?lisation (Medelyan et Witten, 2006).
Nous n?avons pas exploit? de ressources ext?rieures de type lexiques contr?l?s dans la construction
de notre mod?lisation. Nous avons utilis? la version 5.0 de l?impl?mentation de KEA3 disponible
sous licence GNU ; en pratique nous avons utilis? les fonctionnalit?s d?extraction ?libre? pr?sentes
d?s la version 3.0. Les fonctionnalit?s d?velopp?es ult?rieurement concernent l?exploitation de
lexiques contr?l?s. Nos candidats ?taient au maximum de taille 5. Nous avons exploit? le corpus
d?apprentissage fourni pour la seconde piste pour construire notre mod?lisation. Chaque texte
(r?sum? et corps) a ?t? consid?r? comme une unit? documentaire.
L?approche KEA est facilement portable ? diff?rentes langues du fait qu?elle n?cessite peu de
ressources. En particulier elle ne requiert pas une pr?-analyse syntaxique pour s?lectionner des
candidats. Nous avons n?anmoins port? une certaine attention ? nos traitements pr?liminaires
et nous avons constat? qu?une pr?-segmentation en token mots ainsi que l?utilisation d?une liste
multilingue de mots outils augmentaient la qualit? de l?extraction des expressions cl?s lorsque
nous ?valuions l?approche par validation crois?e sur le corpus d?apprentissage. Concernant la
liste des mots outils, nous avons fusionn? les listes fournies par KEA pour le fran?ais, l?anglais et
l?espagnol. Nous l?avons compl?t?e des formes des mots outils pouvant subir une ?lision du e final
en fran?ais (e.g. ?le? s?est vu compl?t? de la forme ?l??, de m?me pour ?lorsque? avec ?lorsqu??. . .).
Ces formes ?taient en effet reconnues par notre segmenteur en mots.
3
http://www.nzdl.org/Kea
64
3.3 Syst?me 3
Ce syst?me est bas? sur une approche par classification supervis?e. La t?che d?extraction de mots
cl?s est ici consid?r?e comme une t?che de classification binaire. La premi?re ?tape consiste ?
g?n?rer tous les mots cl?s candidats ? partir du document. Pour ce faire, nous commen?ons par
extraire tous les n-grammes de mots jusqu?? n = 4. Des contraintes syntaxiques sont ensuite
utilis?es pour filtrer les candidats. Ainsi, seuls les n-grammes compos?s uniquement de noms,
d?adjectifs et de mots outils (except? en premier/dernier mot du n-gramme) sont gard?s.
Pour chaque candidat, nous calculons les traits suivants :
? Poids T F ? IDF
? Nombre de mots du n-gramme
? Patron syntaxique du n-gramme (e.g. ?Nom Adjectif?)
? Position relative de la premi?re occurence dans le document
? Section(s) o? apparait le n-gramme (r?sum?, corps ou les deux)
? Nombre de documents de la collection dans lesquels le n-gramme apparait
? Score de saillance dans l?arbre de d?pendances de coh?sion lexicale du texte (voir ci-dessous)
Nous construisons ce que nous appelons un ?arbre de d?pendances de coh?sion lexicale? selon
une approche d?crite par Choi ? la section 6.3.1. de sa th?se (Choi, 2002). Une d?pendance
est pr?suppos?e exister entre deux phrases cons?cutives si celles-ci ont des mots en commun ;
l?hypoth?se est de consid?rer la seconde phrase comme une ?laboration de la premi?re. En
pratique, notre algorithme ne reconna?t pas syst?matiquement une relation de d?pendance
entre deux phrases cons?cutives qui partagent des mots en commun. En effet notre algorithme
recherche, pour chaque phrase du texte, la phrase la plus haute dans la cha?ne de d?pendance
de la phrase pr?c?dente avec laquelle elle partage des mots en commun. L?arbre est construit
en prenant le texte dans son ensemble (r?sum? et corps) pr?alablement lemmatis?. Un score de
saillance est calcul? pour chaque phrase en fonction du nombre de ses d?pendances (directes
et transitives) normalis? par le nombre de d?pendances maximal qu?une phrase peut avoir sur
le texte donn?. Chaque expression candidate h?rite alors du score de la phrase o? apparait sa
premi?re occurence.
Nous utilisons la combinaison par vote de trois algorithmes de classification disponibles dans la
boite ? outils Weka (Hall et al, 2009) : NaiveBayes, J48 et RandomForest. Les mots-cl?s candidats
sont ensuite tri?s selon leurs scores de pr?diction.
3.4 Combinaison des syst?mes
Les trois syst?mes que nous avons d?velopp?s utilisent diff?rentes m?thodes pour capturer
l?importance d?un mot cl? par rapport ? un document. Une combinaison des sorties de ces
derniers est donc pertinente.
Nous disposons pour chaque document, de trois listes pond?r?es de mots cl?s. La m?thode la
plus simple consisterait ? utiliser la somme des scores des trois syst?mes. Cependant, les scores
calcul?s par chacun des syst?mes ne sont pas directement comparables. ? la place du score, nous
utilisons pour chaque mot cl? candidat, l?inverse de son rang dans la liste ordonn?e.
Deux strat?gies de combinaison ont ?t? utilis?es. La premi?re, COMBI1 consiste ? assigner la
65
somme de l?inverse des rangs d?un mot cl? dans les listes ordonn?es des trois syst?mes. Pour la
seconde strat?gie, COMBI2, nous ne consid?rons que les mots cl?s apparaissant dans les sorties
des trois syst?mes. L?id?e est de filtrer les mots cl?s consid?r?s comme important par seulement
un ou deux des trois syst?mes.
4 R?sultats
Nous pr?sentons dans cette section les r?sultats officiels de la campagne DEFT 2012. Nous
avons soumis trois ex?cutions pour chacune des deux pistes. Pour la premi?re piste, nous avons
simplement utilis? le Syst?me 1 (d?crit dans la section 3.1) et filtr? les mots cl?s candidats ? l?aide
de la terminologie. Le nombre de mots cl?s retourn?s est fix? ? 7 pour la premi?re ex?cution et ?
6 pour les deux autres. Les trois configurations utilisent la r?gle R3.
La premi?re ex?cution utilise la r?gle R2 (? = 0,6). La seconde ex?cution utilise la r?gle R2
(? = 0, 65). La troisi?me ex?cution utilise la r?gle R1 avec la strat?gie S2 (?= 0, 65) et la r?gle
R2 (? = 0,65). Pour la seconde piste, nous avons soumis les ex?cutions de deux combinaisons
(COMBI1 et COMBI2) ainsi que du syst?me 3 (d?crit dans la section 3.3). Le nombre de mots cl?s
retourn?s est fix? ? 130% du nombre de mots cl?s de r?f?rence pour COMBI1 et COMBI2 et ?
110% pour le syst?me 3. Ces nombres permettent d?obtenir les meilleurs r?sultats sur l?ensemble
d?entra?nement.
La table 1 pr?sente les r?sultats de nos trois ex?cutions pour la premi?re piste. Les r?sultats
obtenus par les trois ex?cutions sont moins bons que ceux obtenus sur l?ensemble d?entrainement
(f-mesure=0,44 pour la premi?re ex?cution). Nous constatons que la variation du rappel sur les
trois ex?cutions est faible. La chute de la pr?cision pour la troisi?me ex?cution s?explique par
l?application de la r?gle R1 qui limite le nombre de candidats possibles.
Syst?me Pr?cision Rappel f-mesure
1 0,3812 0,4004 0,3906
2 0,3759 0,3948 0,3851
3 0,3343 0,4097 0,3682
TAB. 1 ? R?sultats de nos trois ex?cutions pour la premi?re piste.
La table 2 montre les r?sultats de nos trois ex?cutions pour la seconde piste. Nous pouvons voir
que la performance de COMBI2 est largement en dessous de COMBI1. Nous avions constat? le
ph?nom?ne inverse sur les donn?es d?entra?nement. Ceci est du au fait que le nombre de mots
cl?s retourn?s par COMBI2 peut dans certains cas ?tre inf?rieur au seuil que nous avons fix?. En
effet, l?intersection des listes des 100 meilleurs mots cl?s candidats de chaque syst?me est tr?s
restrainte pour quelque uns des documents de l?ensemble de test. Nous constatons que les scores
du syst?me 3, ayant obtenu les meilleurs r?sultats sur l?ensemble d?entra?nement parmis nos trois
syst?mes, sont faibles en comparaison des deux combinaisons. Ce r?sultat semble indiquer un
probl?me de sur-entra?nement et illustre bien l?utilit? de la combinaison.
La table 3 pr?sente, pour chacune des deux pistes, le classement des diff?rentes ?quipes sur la
base de la meilleure soumission. Notre soumission est class?e au rang 5 sur 10 pour la premi?re
66
Syst?me Pr?cision Rappel f-mesure
COMBI1 0,1949 0,2355 0,2133
COMBI2 0,1788 0,2128 0,1943
Syst?me 3 0,1643 0,1880 0,1753
TAB. 2 ? R?sultats de nos trois ex?cutions pour la seconde piste.
piste et au rang 2 sur 9 pour la seconde piste. Les r?sultats obtenus par l??quipe 16 sont bien
au dessus de toutes les autres ?quipes et montrent qu?une marge de progression importante est
possible pour notre syst?me.
Rang Piste 1 Piste 2
1 ?quipe 16 (0,9488) ?quipe 16 (0,5874)
2 ?quipe 05 (0,7475) ?quipe 06 (0,2133)
3 ?quipe 04 (0,4417) ?quipe 05 (0,2087)
4 ?quipe 02 (0,3985) ?quipe 02 (0,1921)
5 ?quipe 06 (0,3906) ?quipe 01 (0,1901)
6 ?quipe 01 (0,2737) ?quipe 13 (0,1632)
7 ?quipe 13 (0,1378) ?quipe 04 (0,1270)
8 ?quipe 17 (0,1079) ?quipe 17 (0,0895)
9 ?quipe 03 (0,0857) ?quipe 03 (0,0785)
10 ?quipe 18 (0,0428) -
TAB. 3 ? Classement de DEFT 2012 sur la base de la meilleure soumission de chaque ?quipe pour
chacune des deux pistes. Notre classement est indiqu? en gras (?quipe 06).
5 Ce qui n?a pas march?
Nous d?crivons ici les m?thodes qui ont eu un impact nul ou n?gatif sur les r?sultats.
Traits ayant un impact n?gatif sur la performance du syst?me 3 : la dispersion d?un mot cl?
dans le document, mots appartenant ? des phrases contenant des citations, noms des auteurs les
plus cit?s dans le document (sp?cifique aux articles commen?ant par ?as?).
Suppression de la redondance : nous avons constat? un niveau de redondance important
des mots cl?s dans les sorties de nos syst?mes. Par exemple, les mots cl?s ?jardins collectifs?,
?jardins? et ?collectifs? sont tous les trois pr?sents dans le top 10, ce qui fait baisser le rappel.
Plusieurs strat?gies ont ?t? exp?riment?es pour supprimer cette redondance (e.g. suppression
d?un n-gramme si tous les mots qui le composent sont ?galement pr?sents parmi les 10 meilleurs
candidats). Une d?gradation des r?sultats est cependant observ?e indiquant que la strat?gie ?
adopter est d?pendante des documents.
Mod?le de pond?ration ? base de graphe : nous avons impl?ment? l?approche propos?e
dans (Mihalcea et Tarau, 2004). Il s?agit de repr?senter chaque document sous la forme d?un
67
graphe de mots connect?s par des relations de co-occurrences. Des algorithmes de centralit? sont
ensuite appliqu?s pour extraire les mots les plus caract?ristiques. Les r?sultats obtenus par cette
m?thode sont inf?rieurs ? ceux obtenus ? l?aide d?une pond?ration par la mesure T F ? IDF .
6 Conclusions
Nous avons d?crit la participation du LINA ? DEFT 2012. Notre syst?me est le r?sultat de la
combinaison des sorties de trois diff?rentes m?thodes d?extraction de mots cl?s. Les r?sultats
obtenus par ce dernier sont toujours meilleurs que ceux obtenus par chacune des trois m?thodes
individuellement. Pour la seconde piste, notre syst?me s?est class? ? la 2i e`me place sur un total de
9 syst?mes avec une f-mesure de 21,3%.
La strat?gie que nous avons employ?e pour combiner les sorties des diff?rentes m?thodes n?est
cependant pas optimale. Nous envisageons d??tendre ce travail en proposant d?autres strat?gies
comme par exemple l?utilisation d?un meta-classifieur.
R?f?rences
ABEILL?, A., CL?MENT, L. et TOUSSENEL, F. (2003). Building a treebank for French. Treebanks :
building and using parsed corpora, pages 165?188.
BIRD, S. et LOPER, E. (2004). NLTK : The natural language toolkit. In ACL, Barcelone, Espagne.
CHOI, F. Y. Y. (2002). Content-based Text Navigation. Th?se de doctorat, Department of Computer
Science, University of Manchester.
HALL, M., FRANK, E., HOLMES, G., PFAHRINGER, B., REUTEMANN, P. et WITTEN, I. (2009). The weka
data mining software : an update. ACM SIGKDD Explorations Newsletter, 11(1):10?18.
KISS, T. et STRUNK, J. (2006). Unsupervised multilingual sentence boundary detection. Compu-
tational Linguistics, 32(4):485?525.
MEDELYAN, O. et WITTEN, I. H. (2006). Thesaurus based automatic keyphrase indexing. In
Proceedings of the 6th ACM/IEEE-CS joint conference on Digital libraries, JCDL ?06, pages 296?297,
New York, NY, USA. ACM.
MIHALCEA, R. et TARAU, P. (2004). Textrank : Bringing order into texts. In LIN, D. et WU,
D., ?diteurs : Proceedings of EMNLP 2004, pages 404?411, Barcelona, Spain. Association for
Computational Linguistics.
TOUTANOVA, K., KLEIN, D., MANNING, C. et SINGER, Y. (2003). Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the 3rd Conference of the North American
Chapter of the ACL (NAACL 2003), pages 173?180. Association for Computational Linguistics.
WITTEN, I. H., PAYNTER, G. W., FRANK, E., GUTWIN, C. et NEVILL-MANNING, C. G. (1999). Kea :
Practical automatic keyphrase extraction. CoRR, cs.DL/9902007.
68
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 24?33,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A Comparison of Smoothing Techniques for Bilingual Lexicon Extraction
from Comparable Corpora
Amir Hazem and Emmanuel Morin
Laboratore d?Informatique de Nantes-Atlantique (LINA)
Universite? de Nantes, 44322 Nantes Cedex 3, France
{Amir.Hazem, Emmanuel.Morin}@univ-nantes.fr
Abstract
Smoothing is a central issue in lan-
guage modeling and a prior step in dif-
ferent natural language processing (NLP)
tasks. However, less attention has been
given to it for bilingual lexicon extrac-
tion from comparable corpora. If a first
work to improve the extraction of low
frequency words showed significant im-
provement while using distance-based av-
eraging (Pekar et al, 2006), no investi-
gation of the many smoothing techniques
has been carried out so far. In this pa-
per, we present a study of some widely-
used smoothing algorithms for language
n-gram modeling (Laplace, Good-Turing,
Kneser-Ney...). Our main contribution is
to investigate how the different smoothing
techniques affect the performance of the
standard approach (Fung, 1998) tradition-
ally used for bilingual lexicon extraction.
We show that using smoothing as a pre-
processing step of the standard approach
increases its performance significantly.
1 Introduction
Cooccurrences play an important role in many
corpus based approaches in the field of natural-
language processing (Dagan et al, 1993). They
represent the observable evidence that can be
distilled from a corpus and are employed for a
variety of applications such as machine transla-
tion (Brown et al, 1992), information retrieval
(Maarek and Smadja, 1989), word sense disam-
biguation (Brown et al, 1991), etc. In bilingual
lexicon extraction from comparable corpora,
frequency counts for word pairs often serve as
a basis for distributional methods, such as the
standard approach (Fung, 1998) which compares
the cooccurrence profile of a given source word, a
vector of association scores for its translated cooc-
currences (Fano, 1961; Dunning, 1993), with the
profiles of all words of the target language. The
distance between two such vectors is interpreted
as an indicator of their semantic similarity and
their translational relation. If using association
measures to extract word translation equivalents
has shown a better performance than using a
raw cooccurrence model, the latter remains the
core of any statistical generalisation (Evert, 2005).
As has been known, words and other type-rich
linguistic populations do not contain instances
of all types in the population, even the largest
samples (Zipf, 1949; Evert and Baroni, 2007).
Therefore, the number and distribution of types
in the available sample are not reliable estimators
(Evert and Baroni, 2007), especially for small
comparable corpora. The literature suggests two
major approaches for solving the data sparseness
problem: smoothing and class-based methods.
Smoothing techniques (Good, 1953) are often
used to better estimate probabilities when there
is insufficient data to estimate probabilities ac-
curately. They tend to make distributions more
uniform, by adjusting low probabilities such as
zero probabilities upward, and high probabilities
downward. Generally, smoothing methods not
only prevent zero probabilities, but they also
attempt to improve the accuracy of the model as a
whole (Chen and Goodman, 1999). Class-based
models (Pereira et al, 1993) use classes of similar
words to distinguish between unseen cooccur-
rences. The relationship between given words is
modeled by analogy with other words that are
in some sense similar to the given ones. Hence,
class-based models provide an alternative to the
independence assumption on the cooccurrence
of given words w1 and w2: the more frequent
w2 is, the higher estimate of P (w2|w1) will be,
regardless of w1.
24
Starting from the observation that smoothing es-
timates ignore the expected degree of association
between words (assign the same estimate for all
unseen cooccurrences) and that class-based mod-
els may not structure and generalize word cooc-
currence to class cooccurrence patterns without
losing too much information, (Dagan et al, 1993)
proposed an alternative to these latter approaches
to estimate the probabilities of unseen cooccur-
rences. They presented a method that makes
analogies between each specific unseen cooccur-
rence and other cooccurrences that contain similar
words. The analogies are based on the assump-
tion that similar word cooccurrences have simi-
lar values of mutual information. Their method
has shown significant improvement for both: word
sense disambiguation in machine translation and
data recovery tasks. (Pekar et al, 2006) em-
ployed the nearest neighbor variety of the previ-
ous approach to extract translation equivalents for
low frequency words from comparable corpora.
They used a distance-based averaging technique
for smoothing (Dagan et al, 1999). Their method
yielded a significant improvement in relation to
low frequency words.
Starting from the assumption that smoothing
improves the accuracy of the model as a whole
(Chen and Goodman, 1999), we believe that
smoothed context vectors should lead to bet-
ter performance for bilingual terminology extrac-
tion from comparable corpora. In this work we
carry out an empirical comparison of the most
widely-used smoothing techniques, including ad-
ditive smoothing (Lidstone, 1920), Good-Turing
estimate (Good, 1953), Jelinek-Mercer (Mercer,
1980), Katz (Katz, 1987) and kneser-Ney smooth-
ing (Kneser and Ney, 1995). Unlike (Pekar et al,
2006), the present work does not investigate un-
seen words. We only concentrate on observed
cooccurrences. We believe it constitutes the most
systematic comparison made so far with differ-
ent smoothing techniques for aligning translation
equivalents from comparable corpora. We show
that using smoothing as a pre-processing step of
the standard approach, leads to significant im-
provement even without considering unseen cooc-
currences.
In the remainder of this paper, we present in
Section 2, the different smoothing techniques. The
steps of the standard approach and our extended
method are then described in Section 3. Section
4 describes the experimental setup and our re-
sources. Section 5 presents the experiments and
comments on several results. We finally discuss
the results in Section 6 and conclude in Section 7.
2 Smoothing Techniques
Smoothing describes techniques for adjusting the
maximum likelihood estimate of probabilities to
reduce more accurate probabilities. The smooth-
ing techniques tend to make distributions more
uniform. In this section we present the most
widely used techniques.
2.1 Additive Smoothing
The Laplace estimator or the additive smoothing
(Lidstone, 1920; Johnson, 1932; Jeffreys, 1948)
is one of the simplest types of smoothing. Its
principle is to estimate probabilities P assuming
that each unseen word type actually occurred once.
Then, if we have N events and V possible words
instead of :
P(w) = occ(w)N (1)
We estimate:
Paddone(w) =
occ(w) + 1
N + V (2)
Applying Laplace estimation to word?s cooc-
currence suppose that : if two words cooccur to-
gether n times in a corpus, they can cooccur to-
gether (n + 1) times. According to the maximum
likelihood estimation (MLE):
P(wi+1|wi) =
C(wi,wi+1)
C(wi)
(3)
Laplace smoothing:
P?(wi+1|wi) =
C(wi,wi+1) + 1
C(wi) + V
(4)
Several disadvantages emanate from this
method:
1. The probability of frequent n-grams is under-
estimated.
2. The probability of rare or unseen n-grams is
overestimated.
25
3. All the unseen n-grams are smoothed in the
same way.
4. Too much probability mass is shifted towards
unseen n-grams.
One improvement is to use smaller added count
following the equation below:
P?(wi+1|wi) =
? + C(wi,wi+1)
?|V|+ C(wi)
(5)
with ? ?]0, 1].
2.2 Good-Turing Estimator
The Good-Turing estimator (Good, 1953) pro-
vides another way to smooth probabilities. It states
that for any n-gram that occurs r times, we should
pretend that it occurs r? times. The Good-Turing
estimators use the count of things you have seen
once to help estimate the count of things you have
never seen. In order to compute the frequency of
words, we need to compute Nc, the number of
events that occur c times (assuming that all items
are binomially distributed). Let Nr be the num-
ber of items that occur r times. Nr can be used to
provide a better estimate of r, given the binomial
distribution. The adjusted frequency r? is then:
r? = (r + 1)Nr+1Nr
(6)
2.3 Jelinek-Mercer Smoothing
As one alternative to missing n-grams, useful in-
formation can be provided by the corresponding
(n-1)-gram probability estimate. A simple method
for combining the information from lower-order
n-gram in estimating higher-order probabilities is
linear interpolation (Mercer, 1980). The equation
of linear interpolation is given below:
Pint(wi+1|wi) = ?P(wi+1|wi) + (1? ?)P(wi) (7)
? is the confidence weight for the longer n-
gram. In general, ? is learned from a held-out
corpus. It is useful to interpolate higher-order n-
gram models with lower-order n-gram models, be-
cause when there is insufficient data to estimate a
probability in the higher order model, the lower-
order model can often provide useful information.
Instead of the cooccurrence counts, we used the
Good-Turing estimator in the linear interpolation
as follows:
c?int(wi+1|wi) = ?c
?(wi+1|wi) + (1? ?)P(wi) (8)
2.4 Katz Smoothing
(Katz, 1987) extends the intuitions of Good-
Turing estimate by adding the combination of
higher-order models with lower-order models. For
a bigram wii?1 with count r = c(wii?1), its cor-
rected count is given by the equation:
ckatz(w
i
i?1) =
{
r? if r > 0
?(wi?1)PML(wi) if r = 0 (9)
and:
?(wi?1) =
1?
?
wi:c(w
i
i?1)>0
Pkatz(wii?1)
1?
?
wi:c(w
i
i?1)>0
PML(wi?1)
(10)
According to (Katz, 1987), the general dis-
counted estimate c? of Good-Turing is not used forall counts c. Large counts where c > k for somethreshold k are assumed to be reliable. (Katz,1987) suggests k = 5. Thus, we define c? = cfor c > k, and:
c? =
(c + 1)Nc+1Nc ? c
(k+1)Nk+1
N1
1?
(k+1)Nk+1
N1
(11)
2.5 Kneser-Ney Smoothing
Kneser-Ney have introduced an extension of ab-
solute discounting (Kneser and Ney, 1995). The
estimate of the higher-order distribution is created
by subtracting a fixed discount D from each non-
zero count. The difference with the absolute dis-
counting smoothing resides in the estimate of the
lower-order distribution as shown in the following
equation:
r =
?
?
?
?
?
Max(c(wii?n+1)?D,0)
?
wi
c(wii?n+1)
if c(wii?n+1) > 0
?(wi?1i?n+1)Pkn(wi|w
i?1
i?n+2) if c(wii?n+1) = 0
(12)
where r = Pkn(wi|wi?1i?n+1) and ?(wi?1i?n+1) is
chosen to make the distribution sum to 1 (Chen
and Goodman, 1999).
3 Methods
In this section we first introduce the different steps
of the standard approach, then we present our ex-
tended approach that makes use of smoothing as a
new step in the process of the standard approach.
26
3.1 Standard Approach
The main idea for identifying translations of terms
in comparable corpora relies on the distributional
hypothesis 1 that has been extended to the bilin-
gual scenario (Fung, 1998; Rapp, 1999). If many
variants of the standard approach have been pro-
posed (Chiao and Zweigenbaum, 2002; Herve?
De?jean and Gaussier, 2002; Morin et al, 2007;
Gamallo, 2008)[among others], they mainly differ
in the way they implement each step and define its
parameters. The standard approach can be carried
out as follows:
Step 1 For a source word to translate wsi , we first
build its context vector vwsi . The vector vwsicontains all the words that cooccur with wsi
within windows of n words. Lets denote by
cooc(wsi , wsj ) the cooccurrence value of wsi
and a given word of its context wsj . The pro-
cess of building context vectors is repeated
for all the words of the target language.
Step 2 An association measure such as the point-
wise mutual information (Fano, 1961), the
log-likelihood (Dunning, 1993) or the dis-
counted odds-ratio (Laroche and Langlais,
2010) is used to score the strength of corre-
lation between a word and all the words of its
context vector.
Step 3 The context vector vwsi is projected intothe target language vtwsi . Each wordwsj of vwsiis translated with the help of a bilingual dic-
tionary D. If wsj is not present in D, wsj is
discarded. Whenever the bilingual dictionary
provides several translations for a word, all
the entries are considered but weighted ac-
cording to their frequency in the target lan-
guage (Morin et al, 2007).
Step 4 A similarity measure is used to score each
target word wti , in the target language with
respect to the translated context vector, vtwsi .Usual measures of vector similarity include
the cosine similarity (Salton and Lesk, 1968)
or the weighted Jaccard index (WJ) (Grefen-
stette, 1994) for instance. The candidate
translations of the word wsi are the target
words ranked following the similarity score.
1words with similar meaning tend to occur in similar con-
texts
3.2 Extended Approach
We aim at investigating the impact of differ-
ent smoothing techniques for the task of bilin-
gual terminology extraction from comparable cor-
pora. Starting from the assumption that word
cooccurrences are not reliable especially for small
corpora (Zipf, 1949; Evert and Baroni, 2007)
and that smoothing is usually used to counter-
act this problem, we apply smoothing as a pre-
processing step of the standard approach. Each
cooc(wsi , wsj ) is smoothed according to the tech-
niques described in Section 2. The smoothed
cooccurrence cooc?(wsi , wsj ) is then used for cal-
culating the association measure between wsi and
wsj and so on (steps 2, 3 and 4 of the standard ap-
proach are unchanged). We chose not to study
the prediction of unseen cooccurrences. The lat-
ter has been carried out successfully by (Pekar
et al, 2006). We concentrate on the evaluation
of smoothing techniques of known cooccurrences
and their effect according to different association
and similarity measures.
4 Experimental Setup
In order to evaluate the smoothing techniques, sev-
eral resources and parameters are needed. We
present hereafter the experiment data and the pa-
rameters of the standard approach.
4.1 Corpus Data
The experiments have been carried out on two
English-French comparable corpora. A special-
ized corpus of 530,000 words from the medical
domain within the sub-domain of ?breast cancer?
and a specialize corpus from the domain of ?wind-
energy? of 300,000 words. The two bilingual cor-
pora have been normalized through the follow-
ing linguistic pre-processing steps: tokenization,
part-of-speech tagging, and lemmatization. The
function words have been removed and the words
occurring once (i.e. hapax) in the French and
the English parts have been discarded. For the
breast cancer corpus, we have selected the doc-
uments from the Elsevier website2 in order to
obtain an English-French specialized comparable
corpora. We have automatically selected the doc-
uments published between 2001 and 2008 where
the title or the keywords contain the term ?cancer
du sein? in French and ?breast cancer? in English.
We collected 130 documents in French and 118 in
2www.elsevier.com
27
English. As summarised in Table 1, The compara-
ble corpora comprised about 6631 distinct words
in French and 8221 in English. For the wind en-
ergy corpus, we used the Babook crawler (Groc,
2011) to collect documents in French and English
from the web. We could only obtain 50 documents
in French and 65 in English. As the documents
were collected from different websites according
to some keywords of the domain, this corpus is
more noisy and not well structured compared to
the breast cancer corpus. The wind-energy corpus
comprised about 5606 distinct words in French
and 6081 in English.
Breast cancer Wind energy
TokensS 527,705 307,996
TokensT 531,626 314,551
|S| 8,221 6,081
|T | 6,631 5,606
Table 1: Corpus size
4.2 Dictionary
In our experiments we used the French-English
bilingual dictionary ELRA-M0033 of about
200,000 entries3. It contains, after linguistic pre-
processing steps and projection on both corpora
fewer than 4000 single words. The details are
given in Table 2.
Breast cancer Wind energy
|ELRAS | 3,573 3,459
|ELRAT | 3,670 3,326
Table 2: Dictionary coverage
4.3 Reference Lists
In bilingual terminology extraction from special-
ized comparable corpora, the terminology refer-
ence list required to evaluate the performance
of the alignment programs is often composed of
100 single-word terms (SWTs) (180 SWTs in
(Herve? De?jean and Gaussier, 2002), 95 SWTs in
(Chiao and Zweigenbaum, 2002), and 100 SWTs
in (Daille and Morin, 2005)). To build our ref-
erence lists, we selected only the French/English
pair of SWTs which occur more than five times in
each part of the comparable corpus. As a result
3ELRA dictionary has been created by Sciper in the Tech-
nolangue/Euradic project
of filtering, 321 French/English SWTs were ex-
tracted (from the UMLS4 meta-thesaurus.) for the
breast cancer corpus, and 100 pairs for the wind-
energy corpus.
4.4 Evaluation Measure
Three major parameters need to be set to the
standard approach, namely the similarity measure,
the association measure defining the entry vec-
tors and the size of the window used to build the
context vectors. (Laroche and Langlais, 2010)
carried out a complete study of the influence of
these parameters on the quality of bilingual align-
ment. As a similarity measure, we chose to use
Weighted Jaccard Index (Grefenstette, 1994) and
Cosine similarity (Salton and Lesk, 1968). The en-
tries of the context vectors were determined by the
log-likelihood (Dunning, 1993), mutual informa-
tion (Fano, 1961) and the discounted Odds-ratio
(Laroche and Langlais, 2010). We also chose a 7-
window size. Other combinations of parameters
were assessed but the previous parameters turned
out to give the best performance. We note that
?Top k? means that the correct translation of a
given word is present in the k first candidates of
the list returned by the standard approach. We use
also the mean average precision MAP (Manning
et al, 2008) which represents the quality of the
system.
MAP (Q) = 1
|Q|
|Q|?
i=1
1
mi
k?
mi=1
P (Rik) (13)
where |Q| is the number of terms to be trans-
lated, mi is the number of reference translations
for the ith term (always 1 in our case), and P (Rik)
is 0 if the reference translation is not found for the
ith term or 1/r if it is (r is the rank of the reference
translation in the translation candidates).
4.5 Baseline
The baseline in our experiments is the standard
approach (Fung, 1998) without any smoothing of
the data. The standard approach is often used for
comparison (Pekar et al, 2006; Gamallo, 2008;
Prochasson and Morin, 2009), etc.
4.6 Training Data Set
Some smoothing techniques such as the Good-
Turing estimators need a large training corpus to
4http://www.nlm.nih.gov/research/umls
28
estimate the adjusted cooccurrences. For that pur-
pose, we chose a training general corpus of 10 mil-
lion words. We selected the documents published
in 1994 from the ?Los Angeles Times/Le Monde?
newspapers.
5 Experiments and Results
We conducted a set of three experiments on two
specialized comparable corpora. We carried out a
comparison between the standard approach (SA)
and the smoothing techniques presented in Sec-
tion 2 namely : additive smoothing (Add1), Good-
Turing smoothing (GT), the Jelinek-Mercer tech-
nique (JM), the Katz-Backoff (Katz) and kneser-
Ney smoothing (Kney). Experiment 1 shows the
results for the breast cancer corpus. Experiment 2
shows the results for the wind energy corpus and
finally experiment 3 presents a comparison of the
best configurations on both corpora.
5.1 Experiment 1
Table 3 shows the results of the experiments on
the breast cancer corpus. The first observation
concerns the standard approach (SA). The best
results are obtained using the Log-Jac parame-
ters with a MAP = 27.9%. We can also no-
tice that for this configuration, only the Addi-
tive smoothing significantly improves the perfor-
mance of the standard approach with a MAP =
30.6%. The other smoothing techniques even de-
grade the results. The second observation con-
cerns the Odds-Cos parameters where none of
the smoothing techniques significantly improved
the performance of the baseline (SA). Although
Good-Turing and Katz-Backoff smoothing give
slightly better results with respectively a MAP =
25.2 % and MAP = 25.3 %, these results are not
significant. The most notable result concerns the
PMI-COS parameters. We can notice that four of
the five smoothing techniques improve the perfor-
mance of the baseline. The best smoothing is the
Jelinek-Mercer technique which reaches a MAP =
29.5% and improves the Top1 precision of 6% and
the Top10 precision of 10.3%.
5.2 Experiment 2
Table 4 shows the results of the experiments on
the wind energy corpus. Generally the results
exhibit the same behaviour as the previous ex-
periment. The best results of the standard ap-
proach are obtained using the Log-Jac parameters
SA Add1 GT JM Katz Kney
P1 15.5 17.1 18.7 21.5 18.7 05.3
PM
I-C
osP5 31.1 32.7 32.0 38.3 33.9 13.4
P10 34.5 37.0 37.0 44.8 38.0 15.2
MAP 22.6 24.8 25.6 29.5 25.9 09.1
P1 15.8 16.1 16.8 14.6 17.1 09.0
Od
ds-
Co
s
P5 34.8 33.6 34.2 33.0 33.9 19.6
P10 40.4 41.7 39.8 38.3 40.1 25.2
MAP 24.8 24.4 25.2 23.3 25.3 14.1
P1 20.2 22.4 14.6 14.6 14.6 16.2
Lo
g-J
acP5 35.8 40.5 27.7 26.7 26.7 29.9
P10 42.6 44.2 34.2 33.3 33.0 33.9
MAP 27.9 30.6 21.4 21.2 21.2 22.9
Table 3: Results of the experiments on the ?Breast
cancer? corpus (except the Odds-Cos configura-
tion, the improvements indicate a significance at
the 0.05 level using the Student t-test).
SA Add1 GT JM Katz Kney
P1 07.0 14.0 14.0 21.0 16.0 09.0
PM
I-C
osP5 27.0 32.0 31.0 37.0 30.0 17.0
P10 37.0 42.0 43.0 51.0 44.0 28.0
MAP 17.8 23.6 22.9 30.1 24.2 14.1
P1 12.0 17.0 12.0 12.0 12.0 06.0
Od
ds-
Co
s
P5 31.0 35.0 31.0 32.0 28.0 16.0
P10 38.0 44.0 36.0 39.0 35.0 21.0
MAP 21.8 26.5 19.8 20.8 19.7 11.1
P1 17.0 22.0 13.0 13.0 13.0 14.0
Lo
g-J
acP5 36.0 38.0 27.0 27.0 27.0 29.0
P10 42.0 50.0 37.0 38.0 38.0 39.0
MAP 25.7 29.7 20.5 21.3 21.3 22.9
Table 4: Results of the experiments on the ?Wind
Energy? corpus (except the Odds-Cos configura-
tion, the improvements indicate a significance at
the 0.05 level using the Student t-test).
with a MAP = 25.7%. Here also, only the Ad-
ditive smoothing significantly improves the per-
formance of the standard approach with a MAP
= 39.7%. The other smoothing techniques also
degrade the results. About the Odds-Cos param-
eters, except the additive smoothing, here again
none of the smoothing techniques significantly im-
proved the performance of the baseline. Finally
the most remarkable result still concerns the PMI-
COS parameters where the same four of the five
smoothing techniques improve the performance of
the baseline. The best smoothing is the Jelinek-
Mercer technique which reaches a MAP = 30.1%
and improves the Top1 and and the Top10 preci-
sions by 14.0%.
29
5.3 Experiment 3
In this experiment, we would like to investigate
whether the smoothing techniques are more effi-
cient for frequent translation equivalents or less
frequent ones. For that purpose, we split the breast
cancer reference list of 321 entries into two sets
of translation pairs. A set of 133 frequent pairs
named : High-test set and a set of 188 less fre-
quent pairs called Low-test set. The initial refer-
ence list of 321 pairs is the Full-test set. We con-
sider frequent pairs those of a frequency higher
than 100. We chose to analyse the two configu-
rations that provided the best performance that is :
Log-Jac and Pmi-Cos parameters according to the
Full-test, High-test and Low-test sets.
Figure 1 shows the results using the Log-
Jac configuration. We can see that the additive
smoothing always outperforms the standard ap-
proach for all the test sets. The other smoothing
techniques are always under the baseline and be-
have approximately the same way. Figure 2 shows
the results using the PMI-COS configuration. We
can see that except the Kneser-Ney smoothing, all
the smoothing techniques outperform the standard
approach for all the test sets. We can also notice
that the Jelinek-Mercer smoothing improves more
notably the High-test set.
6 Discussion
Smoothing techniques are often evaluated on their
ability to predict unseen n-grams. In our exper-
iments we only focused on smoothing observed
cooccurrences of context vectors. Hence, the pre-
vious evaluations of smoothing techniques may
not always be consistent with our findings. This
is for example the case for the additive smooth-
ing technique. The latter which is described as
a poor estimator in statistical NLP, turns out to
perform well when associated with the Log-Jac
parameters. This is because we did not consider
unseen cooccurences which are over estimated by
the Add-one smoothing. Obviously, we can imag-
ine that adding one to all unobserved cooccur-
rences would not make sense and would certainly
degrade the results. Except the add-one smooth-
ing, none of the other algorithms reached good
results when associated to the Log-Jac configu-
ration. This is certainly related to the properties
of the log-likelihood association measure. Addi-
tive smoothing has been used to address the prob-
lem of rare words aligning to too many words
(Moore, 2004). At each iteration of the standard
Expectation-Maximization (EM) procedure all the
translation probability estimates are smoothed by
adding virtual counts to uniform probability dis-
tribution over all target words. Here also additive
smoothing has shown interesting results. Accord-
ing to these findings, we can consider the addi-
tive smoothing as an appropriate technique for our
task.
Concerning the Odds-Cos parameters, although
there have been slight improvements in the add-
one algorithm, smoothing techniques have shown
disappointing results. Here again the Odds-ratio
association measure seems to be incompatible
with re-estimating small cooccurrences. More in-
vestigations are certainly needed to highlight the
reasons for this poor performance. It seems that
smoothing techniques based on discounting does
not fit well with association measures based on
contingency table. The most noticeable improve-
ment concerns the PMI-Cos configurations. Ex-
cept Kneser-Ney smoothing, all the other tech-
niques showed better performance than the stan-
dard approach. According to the results, point-
wise mutual information performs better with
smoothing techniques especially with the linear
interpolation of Jelinek-Mercer method that com-
bines high-order (cooccurrences) and low-order
(unigrams) counts of the Good-Turing estima-
tions. Jelinek-Mercer smoothing counteracts the
disadvantage of the point-wise mutual information
which consists of over estimating less frequent
words. This latter weakness is corrected first by
the Good-Turing estimators and then by consider-
ing the low order counts. The best performance
was obtained with ? = 0.5.
Smoothing techniques attempt to improve the
accuracy of the model as a whole. This particu-
larity has been confirmed by the third experiment
where we noticed the smoothing improvements for
both reference lists, that is the High-test and Low-
test sets. This latter experiment has shown that
smoothing observed cooccurrences is useful for all
frequency ranges. The difference of precision be-
tween the two test lists can be explained by the fact
that less frequent words are harder to translate.
In statistical NLP, smoothing techniques for n-
gram models have been addressed in a number
of studies (Chen and Goodman, 1999). The ad-
30
1 5 10 15 20 25 30 35 40 45 5010
20
30
40
50
60
70
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(a)
1 5 10 15 20 25 30 35 40 45 5020
30
40
50
60
70
80
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(b)
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(c)
Figure 1: A set of three figures on the breast cancer corpus for the Log-Jac configuration : (a) Full-test
set ; (b) High-test set; and (c) Low-test set.
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
60
70
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(a)
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
60
70
80
90
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(b)
1 5 10 15 20 25 30 35 40 45 500
10
20
30
40
50
Top
Prec
ision
(%)
SAAdd1GTJMKatzKney
(c)
Figure 2: A set of three figures on the breast cancer corpus for the PMI-COS configuration : (a) Full-test
set ; (b) High-test set; and (c) Low-test set.
ditive smoothing that performs rather poorly has
shown good results in our evaluation. The Good-
Turing estimate which is not used in isolation
forms the basis of later techniques such as Back-
off or Jelinek-Mercer smoothing, two techniques
that generally work well. The good performance
of Katz and JM on the PMI-Cos configura-
tion was expected. The reason is that these two
methods have used the Good-Turing estimators
which also achieved good performances in our
experiments. Concerning the Kneser-Ney algo-
rithm, surprisingly this performed poorly in our
experiments while it is known to be one of the
best smoothing techniques. Discounting a fixed
amount in all counts of observed cooccurrences
degrades the results in our data set. We also im-
plemented the modified Knener-ney method (not
presented in this paper) but this also performed
poorly. We conclude that discounting is not an
appropriate method for observed cooccurrences.
Especially for point-wise mutual information that
over-estimates low frequencies, hense, discount-
ing low cooccurrences will increase this over-
estimation.
7 Conclusion
In this paper, we have described and compared
the most widely-used smoothing techniques for
the task of bilingual lexicon extraction from com-
parable corpora. Regarding the empirical results
of our proposition, performance of smoothing on
our dataset was better than the baseline for the
Add-One smoothing combined with the Log-Jac
parameters and all smoothing techniques except
the Kneser-ney for the Pmi-Cos parameters. Our
findings thus lend support to the hypothesis that
a re-estimation process of word cooccurrence in a
small specialized comparable corpora is an appro-
priate way to improve the accuracy of the standard
approach.
31
Acknowledgments
The research leading to these results has re-
ceived funding from the French National Research
Agency under grant ANR-12-CORD-0020.
References
Brown, P. F., Pietra, S. D., Pietra, V. J. D., and Mercer,
R. L. (1991). Word-sense disambiguation using sta-
tistical methods. In Proceedings of the 29th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?91), pages 264?270, California, USA.
Brown, P. F., Pietra, V. J. D., de Souza, P. V., Lai, J. C.,
and Mercer, R. L. (1992). Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467?479.
Chen, S. F. and Goodman, J. (1999). An empirical
study of smoothing techniques for language model-
ing. Computer Speech & Language, 13(4):359?393.
Chiao, Y.-C. and Zweigenbaum, P. (2002). Look-
ing for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING?02), pages 1208?1212, Tapei,
Taiwan.
Dagan, I., Lee, L., and Pereira, F. C. N. (1999).
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Dagan, I., Marcus, S., and Markovitch, S. (1993). Con-
textual word similarity and estimation from sparse
data. In Proceedings of the 31ST Annual Meet-
ing of the Association for Computational Linguistics
(ACL?93), pages 164?171, Ohio, USA.
Daille, B. and Morin, E. (2005). French-English Ter-
minology Extraction from Comparable Corpora. In
Proceedings of the 2nd International Joint Confer-
ence on Natural Language Processing (IJCLNP?05),
pages 707?718, Jeju Island, Korea.
Dunning, T. (1993). Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1):61?74.
Evert, S. (2005). The statistics of word cooccurrences :
word pairs and collocations. PhD thesis, University
of Stuttgart, Holzgartenstr. 16, 70174 Stuttgart.
Evert, S. and Baroni, M. (2007). zipfr: Word frequency
modeling in r. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?07), Prague, Czech Republic.
Fano, R. M. (1961). Transmission of Information: A
Statistical Theory of Communications. MIT Press,
Cambridge, MA, USA.
Fung, P. (1998). A Statistical View on Bilingual Lex-
icon Extraction: From Parallel Corpora to Non-
parallel Corpora. In Proceedings of the 3rd Confer-
ence of the Association for Machine Translation in
the Americas (AMTA?98), pages 1?16, Langhorne,
PA, USA.
Gamallo, O. (2008). Evaluating two different meth-
ods for the task of extracting bilingual lexicons from
comparable corpora. In Proceedings of LREC 2008
Workshop on Comparable Corpora (LREC?08),
pages 19?26, Marrakech, Marroco.
Good, I. J. (1953). The population frequencies of
species and the estimation of population parameters.
Biometrika, 40:16?264.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publisher,
Boston, MA, USA.
Groc, C. D. (2011). Babouk : Focused Web Crawl-
ing for Corpus Compilation and Automatic Termi-
nology Extraction. In Proceedings of The IEEE-
WICACM International Conferences on Web Intel-
ligence, pages 497?498, Lyon, France.
Herve? De?jean and Gaussier, E?. (2002). Une nouvelle
approche a` l?extraction de lexiques bilingues a` partir
de corpus comparables. Lexicometrica, Alignement
lexical dans les corpus multilingues, pages 1?22.
Jeffreys, H. (1948). Theory of Probability. Clarendon
Press, Oxford. 2nd edn Section 3.23.
Johnson, W. (1932). Probability: the deductive and in-
ductive problems. Mind, 41(164):409?423.
Katz, S. M. (1987). Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech and Signal Processing, 35(3):400?401.
Kneser, R. and Ney, H. (1995). Improved backing-
off for M-gram language modeling. In Proceedings
of the 20th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP?95), pages
181?184, Michigan, USA.
Laroche, A. and Langlais, P. (2010). Revisit-
ing Context-based Projection Methods for Term-
Translation Spotting in Comparable Corpora. In
Proceedings of the 23rd International Conference
on Computational Linguistics (COLING?10), pages
617?625, Beijing, China.
Lidstone, G. J. (1920). Note on the general case of the
bayes-laplace formula for inductive or a posteriori
probabilities. Transactions of the Faculty of Actuar-
ies, 8:182?192.
Maarek, Y. S. and Smadja, F. A. (1989). Full text
indexing based on lexical relations an application:
Software libraries. In SIGIR, pages 198?206, Mas-
sachusetts, USA.
32
Manning, D. C., Raghavan, P., and Schu?tze, H. (2008).
Introduction to information retrieval. Cambridge
University Press.
Mercer, L. ; Jelinek, F. (1980). Interpolated estimation
of markov source parameters from sparse data. In
Workshop on pattern recognition in Practice, Ams-
terdam, The Netherlands.
Moore, R. C. (2004). Improving ibm word alignment
model 1. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?04), pages 518?525, Barcelona, Spain.
Morin, E., Daille, B., Takeuchi, K., and Kageura, K.
(2007). Bilingual Terminology Mining ? Using
Brain, not brawn comparable corpora. In Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), pages 664?
671, Prague, Czech Republic.
Pekar, V., Mitkov, R., Blagoev, D., and Mulloni,
A. (2006). Finding translations for low-frequency
words in comparable corpora. Machine Translation,
20(4):247?266.
Pereira, F. C. N., Tishby, N., and Lee, L. (1993). Dis-
tributional clustering of english words. In Proceed-
ings of the 31ST Annual Meeting of the Association
for Computational Linguistics (ACL?93), pages 183?
190, Ohio, USA.
Prochasson, E. and Morin, E. (2009). Anchor points
for bilingual extraction from small specialized com-
parable corpora. TAL, 50(1):283?304.
Rapp, R. (1999). Automatic Identification of Word
Translations from Unrelated English and German
Corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?99), pages 519?526, College Park, MD, USA.
Salton, G. and Lesk, M. E. (1968). Computer evalua-
tion of indexing and text processing. Journal of the
Association for Computational Machinery, 15(1):8?
36.
Zipf, G. K. (1949). Human Behaviour and the Princi-
ple of Least Effort: an Introduction to Human Ecol-
ogy. Addison-Wesley.
33
