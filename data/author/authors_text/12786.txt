First Joint Conference on Lexical and Computational Semantics (*SEM), pages 282?287,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UCM-I: A Rule-based Syntactic Approach for Resolving the Scope of
Negation
Jorge Carrillo de Albornoz, Laura Plaza, Alberto D??az and Miguel Ballesteros
Universidad Complutense de Madrid
C/ Prof. Jose? Garc??a Santesmases, s/n
28040 Madrid (Spain)
{jcalbornoz,lplazam,albertodiaz,miballes}@fdi.ucm.es
Abstract
This paper presents one of the two contribu-
tions from the Universidad Complutense de
Madrid to the *SEM Shared Task 2012 on Re-
solving the Scope and Focus of Negation. We
describe a rule-based system for detecting the
presence of negations and delimitating their
scope. It was initially intended for process-
ing negation in opinionated texts, and has been
adapted to fit the task requirements. It first
detects negation cues using a list of explicit
negation markers (such as not or nothing), and
infers other implicit negations (such as affixal
negations, e.g, undeniable or improper) by us-
ing semantic information from WordNet con-
cepts and relations. It next uses the informa-
tion from the syntax tree of the sentence in
which the negation arises to get a first approxi-
mation to the negation scope, which is later re-
fined using a set of post-processing rules that
bound or expand such scope.
1 Introduction
Detecting negation is important for many NLP tasks,
as it may reverse the meaning of the text affected
by it. In information extraction, for instance, it is
obviously important to distinguish negated informa-
tion from affirmative one (Kim and Park, 2006). It
may also improve automatic indexing (Mutalik et
al., 2001). In sentiment analysis, detecting and deal-
ing with negation is critical, as it may change the
polarity of a text (Wiegand et al, 2010). How-
ever, research on negation has mainly focused on the
biomedical domain, and addressed the problem of
detecting if a medical term is negated or not (Chap-
man et al, 2001), or the scope of different negation
signals (Morante et al, 2008).
During the last years, the importance of process-
ing negation is gaining recognition by the NLP re-
search community, as evidenced by the success of
several initiatives such as the Negation and Spec-
ulation in Natural Language Processing workshop
(NeSp-NLP 2010)1 or the CoNLL-2010 Shared
Task2, which aimed at identifying hedges and their
scope in natural language texts. In spite of this, most
of the approaches proposed so far deal with negation
in a superficial manner.
This paper describes our contribution to the
*SEM Shared Task 2012 on Resolving the Scope
and Focus of Negation. As its name suggests, the
task aims at detecting the scope and focus of nega-
tion, as a means of encouraging research in negation
processing. In particular, we participate in Task 1:
scope detection. For each negation in the text, the
negation cue must be detected, and its scope marked.
Moreover, the event or property that is negated must
be recognized. A comprehensive description of the
task may be found in (Morante and Blanco, 2012).
For the sake of clarity, it is important to define
what the organization of the task understands by
negation cue, scope of negation and negated event.
The words that express negation are called negation
cues. Not and no are common examples of such
cues. Scope is defined as the part of the mean-
ing that is negated, and encloses all negated con-
cepts. The negated event is the property that is
1http://www.clips.ua.ac.be/NeSpNLP2010/
2www.inf.u-szeged.hu/rgai/conll2010st/
282
negated by the cue. For instance, in the sentence:
[Holmes] did not [say anything], the scope is en-
closed in square brackets, the negation cue is under-
lined and the negated event is shown in bold. More
details about the annotation of negation cues, scopes
and negated events may be found in (Morante and
Daelemans, 2012).
The system presented to the shared task is an
adaptation of the one published in (Carrillo de Al-
bornoz et al, 2010), whose aim was to detect and
process negation in opinionated text in order to im-
prove polarity and intensity classification. When
classifying sentiments and opinions it is important
to deal with the presence of negations and their ef-
fect on the emotional meaning of the text affected by
them. Consider the sentence (1) and (2). Sentence
(1) expresses a positive opinion, whereas that in sen-
tence (2) the negation word not reverses the polarity
of such opinion.
(1) I liked this hotel.
(2) I didn?t like this hotel.
Our system has the main advantage of being sim-
ple and highly generic. Even though it was origi-
nally conceived for treating negations in opinionated
texts, a few simple modifications have been suffi-
cient to successfully address negation in a very dif-
ferent type of texts, such as Conan Doyle stories. It
is rule-based and does not need to be trained. It also
uses semantic information in order to automatically
detect the negation cues.
2 Methodology
As already told, the UCM-I system is a modified ver-
sion of the one presented in (Carrillo de Albornoz
et al, 2010). Next sections detail the modifications
performed to undertake the present task.
2.1 Detecting negation cues
Our previous work was focused on explicit nega-
tions (i.e., those introduced by negation tokens such
as not, never). In contrast, in the present work
we also consider what we call implicit negations,
which includes affixal negation (i.,e., words with
prefixes such as dis-, un- or suffixes such as -less;
e.g., impatient or careless), inffixal negation (i.e.,
pointlessness, where the negation cue less is in the
middle of the noun phrase). Note that we did not
Table 1: Examples of negation cues.
Explicit negation cues
no not non nor
nobody never nowhere ...
Words with implicit negation cues
unpleasant unnatural dislike impatient
fearless hopeless illegal ...
have into account these negation cues when ana-
lyzing opinionated texts because these words them-
selves usually appear in affective lexicons with their
corresponding polarity values (i.e., impatient, for in-
stance, appears in SentiWordNet with a negative po-
larity value).
In order to detect negation cues, we use a list of
predefined negation signals, along with an automatic
method for detecting new ones. The list has been
extracted from different previous works (Councill et
al., 2010; Morante, 2010). This list also includes the
most frequent contracted forms (e.g., don?t, didn?t,
etc.). The automated method, in turn, is intended
for discovering in text new affixal negation cues. To
this end, we first find in the text all words with pre-
fixes dis-, a-, un-, in-, im-, non-, il-, ir- and the suf-
fix -less that present the appropriate part of speech.
Since not all words with such affixes are negation
cues, we use semantic information from WordNet
concepts and relations to decide. In this way, we re-
trieve from WordNet the synset that correspond to
each word, using WordNet::SenseRelate (Patward-
han et al, 2005) to correctly disambiguate the mean-
ing of the word according to its context, along with
all its antonym synsets. We next check if, after re-
moving the affix, the word exists in WordNet and
belongs to any of the antonym synsets. If so, we
consider the original word to be a negation cue (i.e.,
the word without the affix has the opposite meaning
than the lexical item with the affix).
Table 1 presents some examples of explicit nega-
tion cues and words with implicit negation cues. For
space reasons, not all cues are shown. We also con-
sider common spelling errors such as the omission
of apostrophes (e.g., isnt or nt). They are not likely
to be found in literary texts, but are quite frequent in
user-generated content.
This general processing is, however, improved
with two rules:
283
Table 2: Examples of false negation cues.
no doubt without a doubt not merely not just
not even not only no wonder ...
1. False negation cues: Some negation words
may be also used in other expressions with-
out constituting a negation, as in sentence (3).
Therefore, when the negation token belongs
to such expressions, this is not processed as a
negation. Examples of false negation cues are
shown in Table 2.
(3) ... the evidence may implicate not only your
friend Mr. Stapleton but his wife as well.
2. Tag questions: Some sentences in the cor-
pora present negative tag questions in old En-
glish grammatical form, as it may shown in
sentences (4) and (5). We have implemented a
specific rule to deal with this type of construc-
tions, so that they are not treated as negations.
(4) You could easily recognize it , could you not?.
(5) But your family have been with us for several
generations , have they not?
2.2 Delimiting the scope of negation
The scope of a negation is determined by using the
syntax tree of the sentence in which the negation
arises, as generated by the Stanford Parser.3 To this
end, we find in the syntax tree the first common an-
cestor that encloses the negation token and the word
immediately after it, and assume all descendant leaf
nodes to the right of the negation token to be af-
fected by it. This process may be seen in Figure
1, where the syntax tree for the sentence: [Watson
did] not [solve the case] is shown. In this sentence,
the method identifies the negation token not and as-
sumes its scope to be all descendant leaf nodes of the
common ancestor of the words not and solve (i.e.,
solve the case).
This modeling has the main advantage of being
highly generic, as it serves to delimit the scope of
negation regardless of what the negated event is (i.e.,
the verb, the subject, the object of the verb, an ad-
jective or an adverb). As shown in (Carrillo de Al-
3http://nlp.stanford.edu/software/lex-parser.shtml
Figure 1: Syntax tree of the sentence: Watson did not
solve the case.
bornoz et al, 2010), it behaves well when determin-
ing the scope of negation for the purpose of classi-
fying product reviews in polarity classes. However,
we have found that this scope is not enough for the
present task, and thus we have implemented a set of
post-processing rules to expand and limit the scope
according to the task guidelines:
1. Expansion to subject. This rule expands the
negation scope in order to include the subject of
the sentence within it. In this way, in sentence
(6) the appropriate rule is fired to include ?This
theory? within the negation scope.
(6) [This theory would] not [work].
It must be noted that, for polarity classifica-
tion purposes, we do not consider the subject
of the sentence to be part of this scope. Con-
sider, for instance, the sentence: The beauti-
ful views of the Eiffel Tower are not guaranteed
in all rooms. According to traditional polarity
classification approaches, if the subject is con-
sidered as part of the negation scope, the polar-
ity of the positive polar expression ?beautiful?
should be changed, and considered as negative.
2. Subordinate boundaries. Our original nega-
tion scope detection method works well with
coordinate sentences, in which negation cues
scope only over their clause, as if a ?boundary?
exists between the different clauses. This oc-
curs, for instance, in the sentence:
284
Table 3: List of negation scope delimiters.
Tokens POS
so, because, if, while
INuntil, since, unless
before, than, despite IN
what, whose WP
why, where WRB
however RB
?,?, - , :, ;, (, ), !, ?, . -
(7) [It may be that you are] not [yourself lumi-
nous], but you are a conductor of light.
It also works properly in subordinate sentences,
when the negation occurs in the subordinate
clause, as in: You can imagine my surprise
when I found that [there was] no [one there].
However, it may fail in some types of subor-
dinate sentences, where the scope should be
limited to the main clause, but our model pre-
dict both clauses to be affected by the negation.
This is the case for the sentences where the de-
pendent clause is introduced by the subordinate
conjunctions in Table 3. An example of such
type of sentence is (8), where the conjunction
token because introduces a subordinate clause
which is out of the negation scope. To solve this
problem, the negation scope detection method
includes a set of rules to delimit the scope in
those cases, using as delimiters the conjunc-
tions in Table 3. Note that, since some of these
delimiters are ambiguous, their part of speech
tags are used to disambiguate them.
(8) [Her father] refused [to have anything to do
with her] because she had married without his
consent.
3. Prepositional phrases: Our original method
also fails to correctly determine the negation
scope when the negated event is followed by
a prepositional phrase, as it may be seen in
Figure 2, where the syntax tree for the sen-
tence: [There was] no [attempt at robbery] is
shown. Note that, according to our original
model, the phrase ?at robbery? does not belong
to the negation scope. This is an error that was
not detected before, but has been fixed for the
present task.
Figure 2: Syntax tree for the sentence: There was no at-
tempt at robbery.
2.3 Finding negated events
We only consider a single type of negated events,
so that, when a cue word contains a negative affix,
the word after removing the affix is annotated as the
negated event. In this way, ?doubtedly? is correctly
annotated as the negated event in sentence (9). How-
ever, the remaining types of negated events are rele-
gated to future work.
(9) [The oval seal is] undoubtedly [a plain
sleeve-link].
3 Evaluation Setup
The data collection consists of a development set, a
training set, and two test sets of 787, 3644, 496 and
593 sentences, respectively from different stories by
Conan Doyle (see (Morante and Blanco, 2012) for
details). Performance is measured in terms of recall,
precision and F-measure for the following subtasks:
? Predicting negation cues.
? Predicting both the scope and cue.
? Predicting the scope, the cue does not need to
be correct.
? Predicting the scope tokens, where not a full
scope match is required.
? Predicting negated events.
? Full evaluation, which requires all elements to
be correct.
285
Table 4: Results for the development set.
Metric Pr. Re. F-1
Cues 92.55 86.13 89.22
Scope (cue match) 86.05 44.05 58.27
Scope (no cue match) 86.05 44.05 58.27
Scope tokens (no cue match) 88.05 59.05 70.69
Negated (no cue match) 65.00 10.74 18.43
Full negation 74.47 20.23 31.82
4 Evaluation Results
The results of our system when evaluated on the de-
velopment set and the two test sets (both jointly and
separately), are shown in Tables 4, 5, and 6.
It may be seen from these tables that our sys-
tem behaves quite well in the prediction of negation
cues subtask, achieving around 90% F-measure in
all data sets, and the second position in the com-
petition. Performance in the scope prediction task,
however, is around 60% F-1, and the same results
are obtained if the correct prediction of cues is re-
quired (Scope (cue match)). This seems to indicate
that, for all correct scope predictions, our system
have also predicted the negation cues correctly. Ob-
viously these results improve for the Scope tokens
measure, achieving more than 77% F-1 for the Card-
board data set. We also got the second position in
the competition for these three subtasks. Concerning
detection of negated events, our system gets poor re-
sults, 22.85% and 19.81% F-1, respectively, in each
test data set. These results affect the performance
of the full negation prediction task, where we get
32.18% and 32.96% F-1, respectively. Surprisingly,
the result in the test sets are slightly better than those
in the development set, and this is due to a better be-
havior of the WordNet-based cue detection method
in the formers than in the later.
5 Discussion
We next discuss and analyze the results above.
Firstly, and regarding detection of negation cues, our
initial list covers all explicit negations in the devel-
opment set, while the detection of affixal negation
cues using our WordNet-based method presents a
precision of 100% but a recall of 53%. In particu-
lar, our method fails when discovering negation cues
such as unburned, uncommonly or irreproachable,
where the word after removing the affix is a derived
form of a verb or adjective.
Secondly, and concerning delimitation of the
scope, our method behaves considerably well. We
have found that it correctly annotates the negation
scope when the negation affects the predicate that
expresses the event, but sometimes fails to include
the subject of the sentence in such scope, as in:
[I know absolutely] nothing [about the fate of this
man], where our method only recognizes as the
negation scope the terms about the fate of this man.
The results have also shown that the method fre-
quently fails when the subject of the sentence or the
object of an event are negated. This occurs, for
instance, in sentences: I think, Watson, [a brandy
and soda would do him] no [harm] and No [woman
would ever send a reply-paid telegram], where we
only point to ?harm? and ?woman? as the scopes.
We have found a further category of errors in the
scope detection tasks, which concern some types
of complex sentences with subordinate conjunctions
where our method limits the negation scope to the
main clause, as in sentence: [Where they came from,
or who they are,] nobody [has an idea] , where our
method limits the scope to ?has an idea?. However,
if the negation cue occurs in the subordinate clause,
the method behaves correctly.
Thirdly, with respect to negated event detection,
as already told our method gets quite poor results.
This was expected, since our system was not orig-
inally designed to face this task and thus it only
covers one type of negated events. Specifically,
it correctly identifies the negated events for sen-
tences with affixal negation cues, as in: It is most
improper, most outrageous, where the negated event
is ?proper?. However, it usually fails to identify
these events when the negation affects the subject
of the sentence or the object of an event.
6 Conclusions and Future Work
This paper presents one of the two contributions
from the Universidad Complutense de Madrid to the
*SEM Shared Task 2012. The results have shown
that our method successes in identifying negation
cues and performs reasonably well when determin-
ing the negation scope, which seems to indicate that
a simple unsupervised method based on syntactic in-
formation and a reduced set of post-processing rules
286
Table 5: Results for the test sets (jointly).
Metric Gold System Tp Fp Fn Precision Recall F-1
Cues 264 278 241 29 23 89.26 91.29 90.26
Scopes (cue match) 249 254 116 24 133 82.86 46.59 59.64
Scopes (no cue match) 249 254 116 24 133 82.86 46.59 59.64
Scope tokens (no cue match) 1805 1449 1237 212 568 85.37 68.53 76.03
Negated (no cue match) 173 33 22 11 151 66.67 12.72 21.36
Full negation 264 278 57 29 207 66.28 21.59 32.57
Table 6: Results for the Cardboard and Circle test sets.
Metric
Cardboard set Circle set
Pr. Re. F-1 Pr. Re. F-1
Cues 90.23 90.23 90.23 88.32 92.37 90.30
Scope (cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope (no cue match) 83.33 46.88 60.00 82.35 46.28 59.26
Scope tokens (no cue match) 84.91 72.08 77.97 85.96 64.50 73.70
Negated (no cue match) 66.67 13.79 22.85 66.67 11.63 19.81
Full negation 68.29 21.05 32.18 64.44 22.14 32.96
is a viable approach for dealing with negation. How-
ever, detection of negated events is the main weak-
ness of our approach, and this should be tackled in
future work. We also plan to improve our method
for detecting affixal negations to increment its recall,
by using further WordNet relations such as ?derived
from adjective?, and ?pertains to noun?, as well as
to extend this method to detect infixal negations.
Acknowledgments
This research is funded by the Spanish Ministry of
Science and Innovation (TIN2009-14659-C03-01)
and the Ministry of Education (FPU program).
References
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerva?s. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the 14th Conference on Computational Natural
Language Learning (CoNLL 2010), pages 153?161.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34:301?310.
Isaac Councill, Ryan McDonald, and Leonid Velikovich.
2010. What?s great and what?s not: learning to classify
the scope of negation for improved sentiment analysis.
In Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, pages 51?59.
Jung-Jae Kim and Jong C. Park. 2006. Extracting con-
trastive information from negation patterns in biomed-
ical literature. ACM Trans. on Asian Language Infor-
mation Processing, 5(1):44?60.
Roser Morante and Eduardo Blanco. 2012. Sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the 1st Joint Conference on
Lexical and Computational Semantics (*SEM 2012).
Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the 8th International
Conference on Language Resources and Evaluation.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 715?724.
Roser Morante. 2010. Descriptive Analysis of Negation
Cues in Biomedical Texts. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation.
A.G. Mutalik, A. Deshpande, and P.M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents. A quantita-
tive study using the UMLS. J Am Med Inform Assoc,
8(6):598?609.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. SenseRelate::TargetWord: a generalized
framework for word sense disambiguation. In Pro-
ceedings of the ACL 2005 on Interactive poster and
demonstration sessions, pages 73?76.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, pages 60?68.
287
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 288?293,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UCM-2: a Rule-Based Approach to Infer the Scope of Negation via
Dependency Parsing
Miguel Ballesteros, Alberto D??az, Virginia Francisco,
Pablo Gerva?s, Jorge Carrillo de Albornoz and Laura Plaza
Natural Interaction Based on Language Group
Complutense University of Madrid
Spain
{miballes, albertodiaz, virginia}@fdi.ucm.es,
pgervas@sip.ucm.es, {jcalbornoz, lplazam}@fdi.ucm.es
Abstract
UCM-2 infers the words that are affected by
negations by browsing dependency syntactic
structures. It first makes use of an algo-
rithm that detects negation cues, like no, not
or nothing, and the words affected by them
by traversing Minipar dependency structures.
Second, the scope of these negation cues is
computed by using a post-processing rule-
based approach that takes into account the in-
formation provided by the first algorithm and
simple linguistic clause boundaries. An initial
version of the system was developed to handle
the annotations of the Bioscope corpus. For
the present version, we have changed, omitted
or extended the rules and the lexicon of cues
(allowing prefix and suffix negation cues, such
as impossible or meaningless), to make it suit-
able for the present task.
1 Introduction
One of the challenges of the *SEM Shared Task
(Morante and Blanco, 2012) is to infer and classify
the scope and event associated to negations, given
a training and a development corpus based on Co-
nan Doyle stories (Morante and Daelemans, 2012).
Negation, simple in concept, is a complex but essen-
tial phenomenon in any language. It turns an affir-
mative statement into a negative one, changing the
meaning completely. We believe therefore that be-
ing able to handle and classify negations we would
be able to improve several text mining applications.
Previous to this Shared Task, we can find several
systems that handle the scope of negation in the state
of the art. This is a complex problem, because it re-
quires, first, to find and capture the negation cues,
and second, based on either syntactic or semantic
representations, to identify the words that are di-
rectly (or indirectly) affected by these negation cues.
One of the main works that started this trend in natu-
ral language processing was published by Morante?s
team (2008; 2009), in which they presented a ma-
chine learning approach for the biomedical domain
evaluating it on the Bioscope corpus.
In 2010, a Workshop on Negation and Spec-
ulation in Natural Language Processing (Morante
and Sporleder, 2010) was held in Uppsala, Swe-
den. Most of the approaches presented worked in
the biomedical domain, which is the most studied in
negation detection.
The system presented in this paper is a modifica-
tion of the one published in Ballesteros et al (2012).
This system was developed in order to replicate (as
far as possible) the annotations given in the Bio-
scope corpus (Vincze et al, 2008). Therefore, for
the one presented in the task we needed to modify
most of the rules to make it able to handle the more
complex negation structures in the Conan Doyle cor-
pus and the new challenges that it represents. The
present paper has the intention of exemplifying the
problems of such a system when the task is changed.
Our system presented to the Shared Task is based
on the following properties: it makes use of an algo-
rithm that traverses dependency structures, it classi-
fies the scope of the negations by using a rule-based
approach that studies linguistic clause boundaries
and the outcomes of the algorithm for traversing
dependency structures, it applies naive and simple
288
solutions to the problem of classifying the negated
event and it does not use the syntactic annotation
provided in the Conan Doyle corpus (just in an ex-
ception for the negated event annotation).
In Section 2 we describe the algorithms that we
propose for inferring the scope of negation and the
modifications that we needed to make to the previ-
ous version. In Section 3 we discuss the evaluation
performed with the blind test set and development
set and the error analysis over the development set.
Finally, in Section 4 we give our conclusions and
suggestions for future work.
2 Methodology
Our system consists of two algorithms: the first one
is capable of inferring words affected by the negative
operators (cues) by traversing dependency trees and
the second one is capable of annotating sentences
within the scope of negations. This second algo-
rithm is the one in which we change the behaviour in
a deeper way. The first one just serves as a consult-
ing point in some of the rules of the second one. By
using the training set and development set provided
to the authors we modified, omitted or changed the
old rules when necessary.
The first algorithm which traverses a dependency
tree searching for negation cues to determine the
words affected by negations, was firstly applied (at
an earlier stage) to a very different domain (Balles-
teros et al, 2010) obtaining interesting results. At
that time, the Minipar parser (Lin, 1998) was se-
lected to solve the problem in a simple way with-
out needing to carry out several machine learning
optimizations which are well known to be daunting
tasks. We also selected Minipar because at that mo-
ment we only needed unlabelled parsing.
Therefore, our system consists of three different
modules: a static negation cue lexicon, an algorithm
that from a parse given by Minipar and the nega-
tion cue lexicon produces a set of words affected
by the negations, and a rule-based system that pro-
duces the annotation of the scope of the studied sen-
tence. These components are described in the fol-
lowing sections.
In order to annotate the sentence as it is done in
the Conan Doyle corpus, we also developed a post-
processing system that makes use of the outcomes
of the initial system and produces the expected out-
put. Besides this, we also generate a very naive rule-
based approach to handle the problem of annotating
the negated event.
It is worth to mention that we did not make
use of the syntactic annotation provided in the Co-
nan Doyle corpus, our input is the plain text sen-
tence. Therefore, the system could work without the
columns that are included in the annotation, just with
the word forms. We only make use of the annota-
tion when we annotate the negated event, checking
the part-of-speech tag to ascertain whether the cor-
responding word is a verb or not. The system could
work without these columns but only the results of
the negated event would be affected.
2.1 Negation Cue Lexicon
The lexicon containing the negation cues is static. It
can be extended indefinitely but it has the restriction
that it does not learn and it does not grow automat-
ically when applying it to a different domain. The
lexicon used in the previous system (Ballesteros et
al., 2012) was also static but it was very small com-
pared to the one employed by the present system,
just containing less than 20 different negation cues.
Therefore, in addition to the previous lexicon, we
analysed the training set and development sets and
extracted 153 different negation cues (plus the ones
already present in the previous system). We stored
these cues in a file that feeds the system when it
starts. Table 1 shows a small excerpt of the lexicon.
not no neither..nor
unnecessary unoccupied unpleasant
unpractical unsafe unseen
unshaven windless without
Table 1: Excerpt of the lexicon
2.2 Affected Wordforms Detection Algorithm
The algorithm that uses the outcomes of Minipar is
the same employed in (Ballesteros et al, 2012) with-
out modifications. It basically traverses the depen-
dency structures and returns for each negation cue a
set of words affected by the cue.
The algorithm takes into account the way of han-
dling main verbs by Minipar, in which these verbs
289
appear as heads and the auxiliary verbs are depen-
dants of them. Therefore, the system first detects the
nodes that contain a word which is a negation cue,
and afterwards it does the following:
? If the negation cue is a verb, such as lack, it is
marked as a negation cue.
? If the negation cue is not a verb, the algorithm
marks the main verb (if it exists) that governs
the structure as a negation cue.
For the rest of nodes, if a node depends directly
on any of the ones previously marked as negation
cue, the system marks it as affected. The negation is
also propagated until finding leaves, so wordforms
that are not directly related to the cues are detected
too.
Finally, by using all of the above, the algorithm
generates a list of words affected by each negation
cue.
2.3 Scope Classification Algorithm
This second algorithm is the one that has suffered
the deepest modifications from the first version. The
previous version handled the annotation as it is done
in the Bioscope corpus. The algorithm works as fol-
lows:
? The system opens a scope when it finds a new
negation cue detected by the affected word-
forms detection algorithm. In Bioscope, only
the sentences in passive voice include the sub-
ject inside the scope. However, the Conan
Doyle corpus does not contain this exception
always including the subject in the scope when
it exists. Therefore, we modified the decision
that fires this rule, and we apply the way of an-
notating sentences in passive voice for all the
negation cues, either passive or active voice
sentences.
Therefore, for most of the negation cues the
system goes backward and opens the scope
when it finds the subject involved or a marker
that indicates another statement, like a comma.
There are some exceptions to this, such as
scopes in which the cue is without or nei-
ther...nor. For them the system just opens the
scope at the cue.
? The system closes a scope when there are no
more wordforms to be added, i.e.:
? It finds words that indicate another state-
ment, such as but or because.
? No more words in the output of the first
algorithm.
? End of the sentence.
? We also added a new rule that can handle the
negation cues that are prefix or suffix of another
word, such as meaning-less: if the system finds
a cue word like this, it then annotates the suffix
or prefix as the cue (such as less) and the rest of
the word as part of the scope. Note that the Af-
fected Wordforms Detection algorithm detects
the whole word as a cue word.
2.4 Negated Event Handling
In order to come up with a solution that could pro-
vide at least some results in the negated event han-
dling, we decided to do the following:
? When the cue word contains a negative prefix
or a negative suffix, we annotate the word as
the negated event.
? When the cue word is either not or n?t and the
next word is a verb, according to the part-of-
speech annotation of the Conan Doyle corpus,
we annotate the verb as the negated event.
2.5 Post-Processing Step
The post-processing step basically processes the an-
notated sentence with Bioscope style, (we show
an example for clarification: <scope>There is
<cue>no</cue> problem</scope>). It tokenizes
the sentences, in which each token is a word or a
wordform, after that, it does the following:
? If the token contains the string <scope>, the
system just starts a new scope column reserv-
ing three new columns and it puts the word in
the first free ?scope? column. Because it means
that there is a new scope for the present sen-
tence.
? If the token is between a <cue> annotation, the
system puts it in the corresponding free ?cue?
column of the scope already opened.
290
? If the token is annotated as ?negated event?, the
system just puts the word in the last column of
the scope already opened.
Note that these three rules are not exclusive and
can be fired for the same token, but in this case they
are fired in the same order as they are presented.
3 Results and Discussion
In this section we first show the evaluation results
and second the error analysis after studying the re-
sults on the development set.
3.1 Results
In this section we show the results obtained in two
different tables: Table 2 shows the results of the sys-
tem with the test set, Table 3 shows the results of the
system with the development set.
As we can observe, the results for the develop-
ment set are higher than the ones obtained for the
test set. The reason is simple, we used the develop-
ment set (apart from the training set) to modify the
rules and to make the system able to annotate the
sentences of the test set.
Note that our system only detects some of the
negation cues (around 72% F1 and 76% F1, respec-
tively, for the test and development sets). We there-
fore believe that one of the main drawbacks of the
present system is the static lexicon of cues. In the
previous version, due to the simplicity of the task,
this was not an issue. However, it is worth noting
that once the negation is detected the results are not
that bad, we show a high precision in most of the
tasks. But the recall suffers due to the coverage of
the lexicon.
It is also worth noting that for the measure Scope
tokens, which takes into account the tokens included
in the scope but not a full scope match, our system
provides interesting outcomes (around 63% F1 and
73% F1, respectively), showing that it is able to an-
notate the tokens in a similar way. We believe that
this fact evidences that the present system comes
from a different kind of annotation and a different
domain, and the extension or modification of such a
system is a complex task.
We can also observe that the negated events re-
sults are very low (around 17.46% F1 and 22.53%
F1, respectively), but this was expected because by
using our two rules we are only covering two cases
and moreover, these two cases are not always behav-
ing in the same way in the corpora.
3.2 Error Analysis
In this section we analyse the different errors of our
system with respect to the development set. This set
contains 787 sentences, of which 144 are negation
sentences containing 168 scopes, 173 cues and 122
negation events.
With respect to the negation cue detection we
have obtained 58 false negatives (fn) and 16 false
positives (fp). These results are not directly derived
from the static lexicon of cues. The main problem is
related with the management of sentences with more
than one scope. The majority of the errors have been
produced because in some cases all the cues are as-
signed to all the scopes detected in the same sen-
tence, generating fp, and in other cases the cues of
the second and subsequent scopes are ignored, gen-
erating fn. The first case occurs in sentences like
(1), no and without are labelled as cues in the two
scopes. The second case occurs in sentences like
(2), where neither the second scope nor the second
cue are labelled. In sentence (3) un is labelled as
cue two times (unbrushed, unshaven) but within the
same scope, generating a fp in the first scope and a
fn in the second one.
? (1) But no [one can glance at your toilet and at-
tire without [seeing that your disturbance dates
from the moment of your waking .. ?]]
? (2) [You do ]n?t [mean] - . [you do] n?t [mean
that I am suspected] ? ?
? (3) Our client smoothed down [his] un[brushed
hair] and felt [his] un[shaven chin].
We also found false negatives that occur in multi
word negation cues as by no means, no more and
rather than.
A different kind of false positives is related to
modality cues, dialogue elements and special cases
(Morante and Blanco, 2012). For example, no in (4),
not in (5) and save in (6).
? (4) ? You traced him through the telegram , no
[doubt]., ? said Holmes .
291
Test set gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 264 235 170 39 94 81.34 64.39 71.88
Scopes(cue match): 249 233 96 47 153 67.13 38.55 48.98
Scopes(no cue match): 249 233 96 48 152 66.90 38.96 49.24
Scope tokens(no cue match): 1805 2096 1222 874 583 58.30 67.70 62.65
Negated(no cue match): 173 81 36 42 134 46.15 21.18 29.03
Full negation: 264 235 29 39 235 42.65 10.98 17.46
Table 2: Test set results.
Development gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 173 161 115 16 58 87.79 66.47 75.66
Scopes(cue match): 168 160 70 17 98 80.46 41.67 54.90
Scopes(no cue match): 168 160 70 17 98 80.46 41.67 54.90
Scope tokens(no cue match): 1348 1423 1012 411 336 71.12 75.07 73.04
Negated(no cue match): 122 71 35 31 82 53.03 29.91 38.25
Full negation: 173 161 24 16 149 60.00 13.87 22.53
Table 3: Development set results.
? (5) ? All you desire is a plain statement , [is it]
not ? ?.
? (6) Telegraphic inquiries ... that [Marx knew]
nothing [of his customer save that he was a
good payer] .
We can also find problems with affixal negations,
that is, bad separation of the affix and root of the
word. For example, in (7) dissatisfied was erro-
neously divided in di- and ssatisfied. Again, it is
derived from the use of a static lexicon.
? (7) He said little about the case, but from
that little we gathered that [he also was not
dis[satisfied] at the course of events].
Finally, we could also find cases that may be due
to annotation errors. For example, incredible is not
annotated as negation cue in (8). The annotation of
this cue we think is inconsistent, it appears 5 times
in the training corpus, 2 times is labelled as cue, but
3 times is not. According to the context in this sen-
tence, incredible means not credible.
? (8) ?Have just had most incredible and
grotesque experience.
With respect to the full scope detection, most of
the problems are due again to the management of
sentences with more than one scope. We have ob-
tained 98 fn and 17 fp. Most of the problems are
related with affixal negations, as in (9), in which all
the words are included in the scope, which accord-
ing to the gold standard is not correct.
? (9) [Our client looked down with a rueful face
at his own] un[conventional appearance].
With respect to the scope tokens detection, the
results are higher, around 73% F1 in scope tokens
compared to 55% in full match scopes. The reason
is because our system included tokens for the ma-
jority of scopes, increasing the recall until 75% but
lowering the precision due to the inclusion of more
fp.
4 Conclusions and Future Work
In this paper we presented our participation in the
SEM-Shared Task, with a modification of a rule-
based system that was designed to be used in a dif-
ferent domain. As the main conclusion we could say
that modifying such a system to perform in a differ-
ent type of texts is complicated. However, taking
into account this fact, and the results obtained, we
are tempted to say that our system presents compet-
itive results.
292
We believe that the present system has a lot of
room for improvement: (i) improve the manage-
ment of sentences with more than one scope modify-
ing the scope classification algorithm and the post-
processing step, (ii) replacing the dependency parser
with a state-of-the-art parser in order to get higher
performance, or (iii) proposing a different way of
getting a reliable lexicon of cues, by using a seman-
tic approach that informs if the word has a negative
meaning in the context of the sentence. Again, this
could be achieved by using one of the parsers pre-
sented in the ConLL 2008 Shared Task (Surdeanu et
al., 2008).
Acknowledgments
This research is funded by the Spanish Ministry
of Education and Science (TIN2009-14659-C03-01
Project).
References
Miguel Ballesteros, Rau?l Mart??n, and Bele?n D??az-Agudo.
2010. Jadaweb: A cbr system for cooking recipes. In
Proceedings of the Computing Cooking Contest of the
International Conference of Case-Based Reasoning.
Miguel Ballesteros, Virginia Francisco, Alberto D??az,
Jesu?s Herrera, and Pablo Gerva?s. 2012. Inferring the
scope of negation in biomedical documents. In Pro-
ceedings of the 13th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLING 2012), New Delhi. Springer.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems, Granada.
Roser Morante and Eduardo Blanco. 2012. Sem 2012
shared task: Resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference on
Lexical and Computational Semantics (*SEM 2012),
Montreal, Canada.
Roser Morante and Walter Daelemans. 2009. A met-
alearning approach to processing the scope of nega-
tion. In Proceedings of the Thirteenth Conference on
Computational Natural Language Learning, CoNLL
?09, pages 21?29, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Roser Morante and Walter Daelemans. 2012.
Conandoyle-neg: Annotation of negation in conan
doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC). Istanbul, Turkey.
Roser Morante and Caroline Sporleder, editors. 2010.
Proceedings of the Workshop on Negation and Specu-
lation in Natural Language Processing, Uppsala, Swe-
den.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 715?724, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings of
the Twelfth Conference on Natural Language Learn-
ing, pages 159?177, Manchester, United Kingdom.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
293
Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 53?56
Manchester, August 2008
Concept-graph based Biomedical Automatic Summarization using
Ontologies
Laura Plaza Morales
Alberto D??az Esteban
Pablo Gerv
?
as
Universidad Complutense de Madrid
C/Profesor Jos?e Garc??a Santesmases, s/n, Madrid 28040, Spain
lplazam@pas.ucm.es, albertodiaz@fdi.ucm.es,pgervas@sip.ucm.es
Abstract
One of the main problems in research on
automatic summarization is the inaccu-
rate semantic interpretation of the source.
Using specific domain knowledge can con-
siderably alleviate the problem. In this pa-
per, we introduce an ontology-based ex-
tractive method for summarization. It is
based on mapping the text to concepts
and representing the document and its sen-
tences as graphs. We have applied our
approach to summarize biomedical litera-
ture, taking advantages of free resources as
UMLS. Preliminary empirical results are
presented and pending problems are iden-
tified.
1 Introduction
In recent years, the amount of electronic biomedi-
cal literature has increased explosively. Physicians
and researchers constantly have to consult up-to
date information according to their needs, but the
process is time-consuming. In order to tackle this
overload of information, text summarization can
undoubtedly play a role.
Simultaneously, a big deal of resources, such
as biomedical terminologies and ontologies, have
emerged. They can significantly benefit the deve-
lopment of NLP systems, and in particular, when
used in automatic summarization, they can in-
crease the quality of summaries.
In this paper, we present an ontology-based ex-
tractive method for the summarization of biomed-
ical literature, based on mapping the text to con-
cepts in UMLS and representing the document and
its sentences as graphs. To assess the importance
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of the sentences, we compute the centrality of their
concepts in the document graph.
2 Previous Work
Traditionally, automatic summarization methods
have been classified in those which generate ex-
tracts and those which generate abstracts. Al-
though human summaries are typically abstracts,
most of existing systems produce extracts.
Extractive methods build summaries on a super-
ficial analysis of the source. Early summariza-
tion systems are based on simple heuristic fea-
tures, as the position of sentences in the docu-
ment (Brandow et al, 1995), the frequency of
the words they contain (Luhn, 1958; Edmundson,
1969), or the presence of certain cue words or in-
dicative phrases (Edmundson, 1969). Some ad-
vanced approaches also employ machine learning
techniques to determine the best set of attributes
for extraction (Kupiec et al, 1995). Recently,
several graph-based methods have been proposed
to rank sentences for extraction. LexRank (Erkan
and Radev, 2004) is an example of a centroid-
based method to multi-document summarization
that assess sentence importance based on the con-
cept of eigenvector centrality. It represents the
sentences in each document by its tf*idf vectors
and computes sentence connectivity using the co-
sine similarity. Even if results are promising, most
of these approaches exhibit important deficiencies
which are consequences of not capturing the se-
mantic relations between terms (synonymy, hyper-
onymy, homonymy, and co-occurs and associated-
with relations).
We present an extractive method for summariza-
tion which attempts to solve this deficiencies. Un-
like researches conducted by (Yoo et al, 2007;
Erkan and Radev, 2004), which cluster sentences
to identify shared topics in multiple documents, in
this work we apply clustering to identify groups
53
of concepts closely related. We hypothesize that
each cluster represents a theme or topic in the do-
cument, and we evaluate three different heuristics
to ranking sentences.
3 Biomedical Ontologies. UMLS
Biomedical ontologies organize domain concepts
and knowledge in a system of hierarchical and as-
sociative relations. One of the most widespread
in NLP applications is UMLS
1
(Unified Medi-
cal Language System). UMLS consists of three
components: the Metathesaurus, a collection of
concepts and terms from various vocabularies and
their relationships; the Semantic Network, a set of
categories and relations used to classify and relate
the entries in the Metathesaurus; and the Special-
ist Lexicon, a database of lexicographic informa-
tion for use in NLP. In this work, we have se-
lected UMLS for several reasons. First, it pro-
vides a mapping structure between different ter-
minologies, including MeSH or SNOMED, and
thus allows to translate between them. Secondly, it
contains vocabularies in various languages, which
allows to process multilingual information.
4 Summarization Method
The method proposed consists of three steps. Each
step is discussed in detail below. A preliminary
system has been implemented and tested on several
documents from the corpus developed by BioMed
Central
2
.
As the preprocessing, text is split into sentences
using GATE
3
, and generic words and high fre-
quency terms are removed, as they are not useful
in discriminating between relevant and irrelevant
sentences.
4.1 Graph-based Document Representation
This step consists in representing each document
as a graph, where the vertices are the concepts in
UMLS associated to the terms, and the edges indi-
cate the relations between them. Firstly, each sen-
tence is mapped to the UMLSMetathesaurus using
MetaMap (Aronson, 2001). MetaMap allows
to map terms to UMLS concepts, using n-grams
for indexing in the ULMS Metathesaurus, and
performing disambiguation to identify the correct
1
NLM Unified Medical Language System (UMLS). URL:
http://www.nlm.nih.gov/research/umls
2
BioMed Central: http://www.biomedcentral.com/
3
GATE (Generic Architecture for Text Engineering):
http://gate.ac.uk/
concept for a term. Secondly, the UMLS concepts
are extended with their hyperonyms. Figure 1
shows the graph for sentence ?The goal of the trial
was to assess cardiovascular mortality and mor-
bidity for stroke, coronary heart disease and con-
gestive heart failure, as an evidence-based guide
for clinicians who treat hypertension.?
Next, each edge is assigned a weight, which is
directly proportional to the deep in the hierarchy at
which the concepts lies (Figure 1). That is to say,
the more specific the concepts connected are, the
more weight is assigned to them. Expression (1)
shows how these values are computed.
|? ? ?|
|? ? ?|
=
|?|
|?|
(1)
where ? is the set of all the parents of a con-
cept, including the concept, and ? is the set of all
the parents of its immediate higher-level concept,
including the concept.
Finally, the sentence graphs are merged into
a document graph, enriched with the associated-
with relations between the semantic types in
UMLS corresponding to the concepts (Figure 1).
Weights for the new edges are computed using ex-
pression (1).
4.2 Concept Clustering and Theme
Recognition
The second step consists of clustering concepts in
the document graph, using a degree-based method
(Erkan and Radev, 2004). Each cluster is com-
posed by a set of concepts that are closely related
in meaning, and can be seen as a theme in the do-
cument. The most central concepts in the cluster
give the sufficient and necessary information re-
lated to its theme. We hypothesize that the docu-
ment graph is an instance of a scale-free network
(Barabasi, 1999). Following (Yoo et al, 2007),
we introduce the salience of vertices. Mathemati-
cally, the salience of a vertex (v
i
) is calculated as
follows.
salience(v
i
) =
?
e
j
|?v
k
?e
j
conecta(v
i
,v
k
)
weight(e
j
)
(2)
Within the set of vertices, we select the n
that present the higher salience and iteratively
group them in Hub Vertex Sets (HVS). A HVS
represents a group of vertices that are strongly
related to each other. The remaining vertices are
54
Figure 1: Sentence graph
assigned to that cluster to which they are more
connected.
Finally, we assign each sentence to a cluster. To
measure the similarity between a cluster and a sen-
tence graph, we use a vote mechanism (Yoo et al,
2007). Each vertex (v
k
) of a sentence (O
j
) gives to
each cluster (C
i
) a different number of votes (p
i,j
)
depending on whether the vertex belongs to HVS
or non-HVS (3).
similarity(C
i
, O
j
) =
?
v
k
|v
k
?O
j
w
k,j
(3)
where
{
w
k,j
=0 si v
k
6?C
i
w
k,j
=1.0,si v
k
?HV S(C
i
)
w
k,j
=0.5,si v
k
6?HV S(C
i
)
4.3 Sentence Selection
The last step consists of selecting significant sen-
tences for the summary, based on the similarity
between sentences and clusters. We investigated
three alternatives for this step.
? Heuristic 1: For each cluster, the top n
i
sen-
tences are selected, where n
i
is proportional
to its size.
? Heuristic 2: We accept the hypothesis that
the cluster with more concepts represents the
main theme in the document, and select the
top N sentences from this cluster.
? Heuristic 3: We compute a single score for
each sentence, as the sum of the votes as-
signed to each cluster adjusted to their sizes,
and select theN sentences with higher scores.
5 Results and Evaluation
In order to evaluate the method, we analyze the
summaries generated by the three heuristics over
a document
4
from the BioMed Central Corpus,
using a compression rate of 20%. Table 1 shows
the sentences selected along with their scores.
Although results are not statistically significant,
they show some aspects in which our method be-
haves satisfactorily. Heuristics 1 and 3 extract sen-
tence 0, and assign to it the higher score. This
supports the positional criterion of selecting the
first sentence in the document, as the one that con-
tains the most significant information. Sentence 58
represents an example of sentence, situated at the
end, which gathers the conclusions of the author.
In general, these sentences are highly informative.
Sentence 19, in turn, evidences how the method
systematically gives preference to long sentences.
Moreover, while summaries by heuristics 1 and 3
have a lot of sentences in common (9 out of 12),
heuristic 2 generates a summary considerably dif-
ferent and ignores important topics in the docu-
ment. Finally, we have compared these summaries
with the author?s abstract. It can be observed that
heuristics 1 and 3 cover all topics in the author?s
abstract (see sentences 0, 4, 15, 17, 19, 20 and 25).
4
BioMed Central: www.biomedcentral.com/content/
download/xml/cvm-2-6-254.xml
55
Sentences 0 4 19 58 7 28 25 20 21 8 43 15
Heuristic 1 99.0 20.0 19.0 18.5 17.0 16.5 16.0 15.5 15.5 13.5 13.5 12.0
Heuristic 2 19.0 16.5 15.5 12.5 12.0 10.5 9.0 9.0 7.5 7.0 7.0 7.0
Heuristic 3 98.8 18.7 17.9 16.3 15.3 14.5 13.4 13.0 13.0 12.7 12.7 12.2
Table 1: Results
As far as heuristic 2 is concerned, it does not cover
adequately the information in the abstract.
6 Conclusions and Future Work
In this paper we introduce a method for summa-
rizing biomedical literature. We represent the do-
cument as an ontology-enriched scale-free graph,
using UMLS concepts and relations. This way we
get a richer representation than the one provided by
a vector space model. In section 5 we have evalu-
ated several heuristics for sentence extraction. We
have determined that heuristic 2 does not cover all
relevant topics and selects sentences with a low rel-
ative significance. Conversely, heuristics 1 and 3,
present very similar results and cover all important
topics.
Nonetheless, we have identified several prob-
lems and some possible improvements. Firstly, as
our method extracts whole sentences, long ones
have higher probability to be selected, because
they contain more concepts. The alternative could
be to normalise the sentences scores by the number
of concepts. Secondly, concepts associated with
general semantic types in UMLS, as functional
concept, temporal concept, entity and language,
could be ignored, since they do not contribute to
distinguish what sentences are significant.
Finally, in order to formally evaluate the method
and the different heuristics, a large-scale evalua-
tion on the BioMed Corpus is under way, based on
computing the ROUGE measures (Lin, 2004).
Acknowledgements
This research is funded by the Ministerio de Edu-
caci?on y Ciencia (TIN2006-14433-C02-01), Uni-
versidad Complutense de Madrid and Direcci?on
General de Universidades e Investigaci?on de la Co-
munidad de Madrid (CCG07-UCM/TIC 2803).
References
Aronson A. R. Effective Mapping of Biomedical Text
to the UMLS Metathesaurus: The MetaMap Pro-
gram. 2001. In Proceedings of American Medical
Informatics Association.
Barabasi A.L. and Albert R. Emergence of scaling in
random networks. 1999. Science,286?509.
Brandow R. and Mitze K. and Rau L. F. Automatic
Condensation of Electronic Publications by Sen-
tence Selection. 1995. Information Processing and
Management,5(31):675?685.
Edmundson H.P. New Methods in Automatic Extract-
ing. 1969. Journal of the Association for Computing
Machinery,2(16):264?285.
Erkan G. and Radev D. R. LexRank: Graph-based
Lexical Centrality as Salience in Text Summariza-
tion. 2004. Journal of Artificial Intelligence Re-
search (JAIR),22:457?479.
Kupiec J. and Pedersen J.O. and Chen F. A Trainable
Document Summarizer. 1995. In Proceedings of
the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval,68?73.
Lin C-Y. ROUGE: A Package for Automatic Eval-
uation of Summaries. 2004. In Proceedings of
Workshop on Text Summarization Branches Out,
Post-Conference Workshop of ACL 2004, Barcelona,
Spain.
Luhn H.P. The Automatic Creation of Literature
Abstracts. 1958. IBM Journal of Research
Development,2(2):159?165.
Sparck-Jones K. Automatic Summarizing: Factors and
Directions. 1999. I. Mani y M.T. Maybury, Advances
in Automatic Text Summarization. The MIT Press.
Yoo I. and Hu X. and Song I.Y. A coherent graph-based
semantic clustering and summarization approach for
biomedical literature and a new summarization eval-
uation method. 2007. BMC Bioinformatics,8(9).
56
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 55?63,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Improving Summarization of Biomedical Documents
using Word Sense Disambiguation
Laura Plaza?
lplazam@fdi.ucm.es
Mark Stevenson?
m.stevenson@dcs.shef.ac.uk
? Universidad Complutense de Madrid, C/Prof. Jose? Garc??a Santesmases, 28040 Madrid, Spain
? University of Sheffield, Regent Court, 211 Portobello St., Sheffield, S1 4DP, UK
Alberto D??az?
albertodiaz@fdi.ucm.es
Abstract
We describe a concept-based summariza-
tion system for biomedical documents and
show that its performance can be improved
using Word Sense Disambiguation. The
system represents the documents as graphs
formed from concepts and relations from
the UMLS. A degree-based clustering al-
gorithm is applied to these graphs to dis-
cover different themes or topics within
the document. To create the graphs, the
MetaMap program is used to map the
text onto concepts in the UMLS Metathe-
saurus. This paper shows that applying a
graph-based Word Sense Disambiguation
algorithm to the output of MetaMap im-
proves the quality of the summaries that
are generated.
1 Introduction
Extractive text summarization can be defined as
the process of determining salient sentences in a
text. These sentences are expected to condense
the relevant information regarding the main topic
covered in the text. Automatic summarization of
biomedical texts may benefit both health-care ser-
vices and biomedical research (Reeve et al, 2007;
Hunter and Cohen, 2006). Providing physicians
with summaries of their patient records can help
to reduce the diagnosis time. Researchers can use
summaries to quickly determine whether a docu-
ment is of interest without having to read it all.
Summarization systems usually work with a
representation of the document consisting of in-
formation that can be directly extracted from the
document itself (Erkan and Radev, 2004; Mihalcea
and Tarau, 2004). However, recent studies have
demonstrated the benefit of summarization based
on richer representations that make use of external
knowledge sources (Plaza et al, 2008; Fiszman et
al., 2004). These approaches can represent seman-
tic associations between the words and terms in the
document (i.e. synonymy, hypernymy, homonymy
or co-occurrence) and use this information to im-
prove the quality of the summaries. In the biomed-
ical domain the Unified Medical Language Sys-
tem (UMLS) (Nelson et al, 2002) has proved to
be a useful knowledge source for summarization
(Fiszman et al, 2004; Reeve et al, 2007; Plaza et
al., 2008). In order to access the information con-
tained in the UMLS, the vocabulary of the doc-
ument being summarized has to be mapped onto
it. However, ambiguity is common in biomedi-
cal documents (Weeber et al, 2001). For exam-
ple, the string ?cold? is associated with seven pos-
sible meanings in the UMLS Metathesuarus in-
cluding ?common cold?, ?cold sensation? , ?cold
temperature? and ?Chronic Obstructive Airway
Disease?. The majority of summarization sys-
tems in the biomedical domain rely on MetaMap
(Aronson, 2001) to map the text onto concepts
from the UMLS Metathesaurus (Fiszman et al,
2004; Reeve et al, 2007). However, MetaMap fre-
quently fails to identify a unique mapping and, as
a result, various concepts with the same score are
returned. For instance, for the phrase ?tissues are
often cold? MetaMap returns three equally scored
concepts for the word ??cold?: ?common cold?,
?cold sensation? and ?cold temperature?.
The purpose of this paper is to study the ef-
fect of lexical ambiguity in the knowledge source
on semantic approaches to biomedical summariza-
tion. To this end, the paper describes a concept-
based summarization system for biomedical doc-
uments that uses the UMLS as an external knowl-
edge source. To address the word ambiguity prob-
lem, we have adapted an existing WSD system
(Agirre and Soroa, 2009) to assign concepts from
the UMLS. The system is applied to the summa-
rization of 150 biomedical scientific articles from
the BioMed Central corpus and it is found that
55
WSD improves the quality of the summaries. This
paper is, to our knowledge, the first to apply WSD
to the summarization of biomedical documents
and also demonstrates that this leads to an im-
provement in performance.
The next section describes related work on sum-
marization and WSD. Section 3 introduces the
UMLS resources used in the WSD and sum-
marization systems. Section 4 describes our
concept-based summarization algorithm. Section
5 presents a graph-based WSD algorithm which
has been adapted to assign concepts from the
UMLS. Section 6 describes the experiments car-
ried out to evaluate the impact of WSD and dis-
cusses the results. The final section provides
concluding remarks and suggests future lines of
work.
2 Related work
Summarization has been an active area within
NLP research since the 1950s and a variety of ap-
proaches have been proposed (Mani, 2001; Afan-
tenos et al, 2005). Our focus is on graph-based
summarization methods. Graph-based approaches
typically represent the document as a graph, where
the nodes represent text units (i.e. words, sen-
tences or paragraphs), and the links represent co-
hesion relations or similarity measures between
these units. The best-known work in the area is
LexRank (Erkan and Radev, 2004). It assumes a
fully connected and undirected graph, where each
node corresponds to a sentence, represented by
its TF-IDF vector, and the edges are labeled with
the cosine similarity between the sentences. Mi-
halcea and Tarau (2004) present a similar method
where the similarity among sentences is measured
in terms of word overlaps.
However, methods based on term frequencies
and syntactic representations do not exploit the se-
mantic relations among the words in the text (i.e.
synonymy, homonymy or co-occurrence). They
cannot realize, for instance, that the phrases my-
ocardial infarction and heart attack refer to the
same concepts, or that pneumococcal pneumonia
and mycoplasma pneumonia are two similar dis-
eases that differ in the type of bacteria that causes
them. This problem can be partially solved by
dealing with concepts and semantic relations from
domain-specific resources, rather than terms and
lexical or syntactic relations. Consequently, some
recent approaches have adapted existing methods
to represent the document at a conceptual level. In
particular, in the biomedical domain Reeve et al
(2007) adapt the lexical chaining approach (Barzi-
lay and Elhadad, 1997) to work with UMLS con-
cepts, using the MetaMap Transfer Tool to anno-
tate these concepts. Yoo et al (2007) represent a
corpus of documents as a graph, where the nodes
are the MeSH descriptors found in the corpus, and
the edges represent hypernymy and co-occurrence
relations between them. They cluster the MeSH
concepts in the corpus to identify sets of docu-
ments dealing with the same topic and then gen-
erate a summary from each document cluster.
Word sense disambiguation attempts to solve
lexical ambiguities by identifying the correct
meaning of a word based on its context. Super-
vised approaches have been shown to perform bet-
ter than unsupervised ones (Agirre and Edmonds,
2006) but need large amounts of manually-tagged
data, which are often unavailable or impractical to
create. Knowledge-based approaches are a good
alternative that do not require manually-tagged
data.
Graph-based methods have recently been shown
to be an effective approach for knowledge-based
WSD. They typically build a graph for the text in
which the nodes represent all possible senses of
the words and the edges represent different kinds
of relations between them (e.g. lexico-semantic,
co-occurrence). Some algorithm for analyzing
these graphs is then applied from which a rank-
ing of the senses of each word in the context is
obtained and the highest-ranking one is chosen
(Mihalcea and Tarau, 2004; Navigli and Velardi,
2005; Agirre and Soroa, 2009). These methods
find globally optimal solutions and are suitable for
disambiguating all words in a text.
One such method is Personalized PageRank
(Agirre and Soroa, 2009) which makes use of
the PageRank algorithm used by internet search
engines (Brin and Page, 1998). PageRank as-
signs weight to each node in a graph by analyz-
ing its structure and prefers ones that are linked to
by other nodes that are highly weighted. Agirre
and Soroa (2009) used WordNet as the lexical
knowledge base and creates graphs using the en-
tire WordNet hierarchy. The ambiguous words in
the document are added as nodes to this graph and
directed links are created from them to each of
their possible meanings. These nodes are assigned
weight in the graph and the PageRank algorithm is
56
applied to distribute this information through the
graph. The meaning of each word with the high-
est weight is chosen. We refer to this approach
as ppr. It is efficient since it allows all ambigu-
ous words in a document to be disambiguated si-
multaneously using the whole lexical knowledge
base, but can be misled when two of the possible
senses for an ambiguous word are related to each
other in WordNet since the PageRank algorithm
assigns weight to these senses rather than transfer-
ring it to related words. Agirre and Soroa (2009)
also describe a variant of the approach, referred
to as ?word to word? (ppr w2w), in which a sep-
arate graph is created for each ambiguous word.
In these graphs no weight is assigned to the word
being disambiguated so that all of the information
used to assign weights to the possible senses of the
word is obtained from the other words in the doc-
ument. The ppr w2w is more accurate but less
efficient due to the number of graphs that have to
be created and analyzed. Agirre and Soroa (2009)
show that the Personalized PageRank approach
performs well in comparison to other knowledge-
based approaches to WSD and report an accuracy
of around 58% on standard evaluation data sets.
3 UMLS
The Unified Medical Language System (UMLS)
(Humphreys et al, 1998) is a collection of con-
trolled vocabularies related to biomedicine and
contains a wide range of information that can
be used for Natural Language Processing. The
UMLS comprises of three parts: the Specialist
Lexicon, the Semantic Network and the Metathe-
saurus.
The Metathesaurus forms the backbone of the
UMLS and is created by unifying over 100 con-
trolled vocabularies and classification systems. It
is organized around concepts, each of which repre-
sents a meaning and is assigned a Concept Unique
Identifier (CUI). For example, the following CUIs
are all associated with the term ?cold?: C0009443
?Common Cold?, C0009264 ?Cold Temperature?
and C0234192 ?Cold Sensation?.
The MRREL table in the Metathesaurus lists re-
lations between CUIs found in the various sources
that are used to form the Metathesaurus. This ta-
ble lists a range of different types of relations, in-
cluding CHD (?child?), PAR (?parent?), QB (?can
be qualified by?), RQ (?related and possibly syn-
onymous?) and RO (?other related?). For exam-
ple, the MRREL table states that C0009443 ?Com-
mon Cold? and C0027442 ?Nasopharynx? are con-
nected via the RO relation.
The MRHIER table in the Metathesaurus lists
the hierarchies in which each CUI appears, and
presents the whole path to the top or root of
each hierarchy for the CUI. For example, the
MRHIER table states that C0035243 ?Respiratory
Tract Infections? is a parent of C0009443 ?Com-
mon Cold?.
The Semantic Network consists of a set of cat-
egories (or semantic types) that provides a consis-
tent categorization of the concepts in the Metathe-
saurus, along with a set of relationships (or seman-
tic relations) that exist between the semantic types.
For example, the CUI C0009443 ?Common Cold?
is classified in the semantic type ?Disease or Syn-
drome?.
The SRSTR table in the Semantic Network de-
scribes the structure of the network. This table
lists a range of different relations between seman-
tic types, including hierarchical relations (is a)
and non hierarchical relations (e.g. result of,
associated with and co-occurs with).
For example, the semantic types ?Disease or Syn-
drome? and ?Pathologic Function? are connected
via the is a relation in this table.
4 Summarization system
The method presented in this paper consists of 4
main steps: (1) concept identification, (2) doc-
ument representation, (3) concept clustering and
topic recognition, and (4) sentence selection. Each
step is discussed in detail in the following subsec-
tions.
4.1 Concept identification
The first stage of our process is to map the doc-
ument to concepts from the UMLS Metathesaurus
and semantic types from the UMLS Semantic Net-
work.
We first run the MetaMap program over the text
in the body section of the document1 MetaMap
(Aronson, 2001) identifies all the phrases that
could be mapped onto a UMLS CUI, retrieves
and scores all possible CUI mappings for each
phrase, and returns all the candidates along with
1We do not make use of the disambiguation algorithm
provided by MetaMap, which is invoked using the -y flag
(Aronson, 2006), since our aim is to compare the effect of
WSD on the performance of our summarization system rather
than comparing WSD algorithms.
57
their score. The semantic type for each concept
mapping is also returned. Table 1 shows this map-
ping for the phrase tissues are often cold. This ex-
ample shows that MetaMap returns a single CUI
for two words (tissues and often) but also returns
three equally scored CUIs for cold (C0234192,
C0009443 and C0009264). Section 5 describes
how concepts are selected when MetaMap is un-
able to return a single CUI for a word.
Phrase: ?Tissues?
Meta Mapping (1000)
1000 C0040300:Tissues (Body tissue)
Phrase: ?are?
Phrase: ?often cold?
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0234192:Cold (Cold Sensation)
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0009443:Cold (Common Cold)
MetaMapping (888)
694 C0332183:Often (Frequent)
861 C0009264:Cold (cold temperature)
Table 1: An example of MetaMap mapping for the
phrase Tissues are often cold
UMLS concepts belonging to very general se-
mantic types are discarded, since they have been
found to be excessively broad or unrelated to the
main topic of the document. These types are
Quantitative Concept, Qualitative Concept, Tem-
poral Concept, Functional Concept, Idea or Con-
cept, Intellectual Product, Mental Process, Spatial
Concept and Language. Therefore, the concept
C0332183 ?Often? in the previous example, which
belongs to the semantic type Temporal Concept, is
discarded.
4.2 Document representation
The next step is to construct a graph-based repre-
sentation of the document. To this end, we first ex-
tend the disambiguated UMLS concepts with their
complete hierarchy of hypernyms and merge the
hierarchies of all the concepts in the same sentence
to construct a graph representing it. The two upper
levels of these hierarchies are removed, since they
represent concepts with excessively broad mean-
ings and may introduce noise to later processing.
Next, all the sentence graphs are merged into
a single document graph. This graph is extended
with more semantic relations to obtain a more
complete representation of the document. Vari-
ous types of information from the UMLS can be
used to extend the graph. We experimented us-
ing different sets of relations and finally used the
hypernymy and other related relations between
concepts from the Metathesaurus, and the asso-
ciated with relation between semantic types from
the Semantic Network. Hypernyms are extracted
from the MRHIER table, RO (?other related?) re-
lations are extracted from the MRREL table, and
associated with relations are extracted from
the SRSTR table (see Section 3). Finally, each
edge is assigned a weight in [0, 1]. This weight
is calculated as the ratio between the relative posi-
tions in their corresponding hierarchies of the con-
cepts linked by the edge.
Figure 1 shows an example graph for a sim-
plified document consisting of the two sentences
below. Continuous lines represent hypernymy re-
lations, dashed lines represent other related rela-
tions and dotted lines represent associated with re-
lations.
1. The goal of the trial was to assess cardiovascular
mortality and morbidity for stroke, coronary heart
disease and congestive heart failure, as an evidence-
based guide for clinicians who treat hypertension.
2. The trial was carried out in two groups: the first
group taking doxazosin, and the second group tak-
ing chlorthalidone.
4.3 Concept clustering and topic recognition
Our next step consists of clustering the UMLS
concepts in the document graph using a degree-
based clustering method (Erkan and Radev, 2004).
The aim is to construct sets of concepts strongly
related in meaning, based on the assumption that
each of these sets represents a different topic in the
document.
We assume that the document graph is an in-
stance of a scale-free network (Barabasi and Al-
bert, 1999). A scale-free network is a complex net-
work that (among other characteristics) presents a
particular type of node which are highly connected
to other nodes in the network, while the remain-
ing nodes are quite unconnected. These highest-
degree nodes are often called hubs. This scale-
free power-law distribution has been empirically
observed in many large networks, including lin-
guistic and semantic ones.
To discover these prominent or hub nodes, we
compute the salience or prestige of each vertex
58
Figure 1: Example of a simplified document graph
in the graph (Yoo et al, 2007), as shown in (1).
Whenever an edge from vi to vj exists, a vote from
node i to node j is added with the strength of this
vote depending on the weight of the edge. This
ranks the nodes according to their structural im-
portance in the graph.
salience(vi) =
?
?ej |?vk?ejconnect(vi,vk)
weight(ej) (1)
The n vertices with a highest salience are
named Hub Vertices. The clustering algorithm
first groups the hub vertices into Hub Vertices
Sets (HVS). These can be seen as set of concepts
strongly related in meaning, and will represent the
centroids of the clusters. To construct these HVS,
the clustering algorithm first searches, iteratively
and for each hub vertex, the hub vertex most con-
nected to it, and merges them into a single HVS.
Second, the algorithm checks, for every pair of
HVS, if their internal connectivity is lower than
the connectivity between them. If so, both HVS
are merged. The remaining vertices (i.e. those
not included in the HVS) are iteratively assigned
to the cluster to which they are more connected.
This connectivity is computed as the sum of the
weights of the edges that connect the target vertex
to the other vertices in the cluster.
4.4 Sentence selection
The last step of the summarization process con-
sists of computing the similarity between all sen-
tences in the document and each of the clusters,
and selecting the sentences for the summary based
on these similarities. To compute the similarity be-
tween a sentence graph and a cluster, we use a non-
democratic vote mechanism (Yoo et al, 2007), so
that each vertex of a sentence assigns a vote to
a cluster if the vertex belongs to its HVS, half a
vote if the vertex belongs to it but not to its HVS,
and no votes otherwise. Finally, the similarity be-
tween the sentence and the cluster is computed as
the sum of the votes assigned by all the vertices in
the sentence to the cluster, as expressed in (2).
similarity(Ci, Sj) =
?
vk|vk?Sj
wk,j (2)
where
{
wk,j=0 if vk 6?Ci
wk,j=1 if vk?HV S(Ci)
wk,j=0.5 if vk 6?HV S(Ci)
Finally, we select the sentences for the sum-
mary based on the similarity between them and
the clusters as defined above. In previous work
(blind reference), we experimented with different
heuristics for sentence selection. In this paper, we
just present the one that reported the best results.
For each sentence, we compute a single score, as
59
the sum of its similarity to each cluster adjusted
to the cluster?s size (expression 3). Then, the N
sentences with higher scores are selected for the
summary.
Score(Sj) =
?
Ci
similarity(Ci, Sj)
|Ci|
(3)
In addition to semantic-graph similarity
(SemGr) we have also tested two further features
for computing the salience of sentences: sentence
location (Location) and similarity with the title
section (Title). The sentence location feature
assigns higher scores to the sentences close to the
beginning and the end of the document, while
the similarity with the title feature assigns higher
scores as the proportion of common concepts be-
tween the title and the target sentence is increased.
Despite their simplicity, these are well accepted
summarization heuristics that are commonly used
(Bawakid and Oussalah, 2008; Bossard et al,
2008).
The final selection of the sentences for the sum-
mary is based on the weighted sum of these feature
values, as stated in (4). The values for the param-
eters ?, ? and ? have been empirically set to 0.8,
0.1, and 0.1 respectively.
Score(Sj) = ?? SemGr(Sj) +
? ? Location(Sj) + ?? Title(Sj) (4)
5 WSD for concept identification
Since our summarization system is based on the
UMLS it is important to be able to accurately map
the documents onto CUIs. The example in Section
4.1 shows that MetaMap does not always select a
single CUI and it is therefore necessary to have
some method for choosing between the ones that
are returned. Summarization systems typically
take the first mapping as returned by MetaMap,
and no attempt is made to solve this ambiguity
(Plaza et al, 2008). This paper reports an alter-
native approach that uses a WSD algorithm that
makes use of the entire UMLS Metathesaurus.
The Personalized PageRank algorithm (see Sec-
tion 2) was adapted to use the UMLS Metathe-
saurus and used to select a CUI from the MetaMap
output2. The UMLS is converted into a graph
in which the CUIs are the nodes and the edges
2We use a publicly available implementation of the Per-
sonalized Page Rank algorithm (http://ixa2.si.ehu.
es/ukb/) for the experiments described here.
are derived from the MRREL table. All possible
relations in this table are included. The output
from MetaMap is used to provide the list of pos-
sible CUIs for each term in a document and these
are passed to the disambiguation algorithm. We
use both the standard (ppr) and ?word to word?
(ppr w2w) variants of the Personalized PageRank
approach.
It is difficult to evaluate how well the Person-
alized PageRank approach performs when used
in this way due to a lack of suitable data. The
NLM-WSD corpus (Weeber et al, 2001) con-
tains manually labeled examples of ambiguous
terms in biomedical text but only provides exam-
ples for 50 terms that were specifically chosen be-
cause of their ambiguity. To evaluate an approach
such as Personalized PageRank we require doc-
uments in which the sense of every ambiguous
word has been identified. Unfortunately no such
resource is available and creating one would be
prohibitively expensive. However, our main in-
terest is in whether WSD can be used to improve
the summaries generated by our system rather than
its own performance and, consequently, decided to
evaluate the WSD by comparing the output of the
summarization system with and without WSD.
6 Experiments
6.1 Setup
The ROUGE metrics (Lin, 2004) are used to eval-
uate the system. ROUGE compares automati-
cally generated summaries (called peers) against
human-created summaries (called models), and
calculates a set of measures to estimate the con-
tent quality of the summaries. Results are re-
ported for the ROUGE-1 (R-1), ROUGE-2 (R-
2), ROUGE-SU4 (R-SU) and ROUGE-W (R-W)
metrics. ROUGE-N (e.g. ROUGE-1 and ROUGE-
2) evaluates n-gram co-occurrences among the
peer and models summaries, where N stands for
the length of the n-grams. ROUGE-SU4 allows
bi-gram to have intervening word gaps no longer
than four words. Finally, ROUGE-W computes
the union of the longest common subsequences be-
tween the candidate and the reference summaries
taking into account the presence of consecutive
matches.
To the authors? knowledge, no specific corpus
for biomedical summarization exists. To evalu-
ate our approach we use a collection of 150 doc-
uments randomly selected from the BioMed Cen-
60
tral corpus3 for text mining research. This collec-
tion is large enough to ensure significant results in
the ROUGE evaluation (Lin, 2004) and allows us
to work with the ppr w2w disambiguation soft-
ware, which is quite time consuming. We generate
automatic summaries by selecting sentences until
the summary reaches a length of the 30% over the
original document size. The abstract of the papers
(i.e. the authors? summaries) are removed from
the documents and used as model summaries.
A separate development set was used to deter-
mine the optimal values for the parameters in-
volved in the algorithm. This set consists of 10
documents from the BioMed Central corpus. The
model summaries for these documents were man-
ually created by medical students by selecting be-
tween 20-30% of the sentences within the paper.
The parameters to be estimated include the per-
centage of vertices considered as hub vertices by
the clustering method (see Section 4.3) and the
combination of summarization features used to
sentence selection (see Section 4.4). As a result,
the percentage of hub vertices was set to 15%, and
no additional summarization features (apart from
the semantic-graph similarity) were used.
Two baselines were also implemented. The
first, lead baseline, generate summaries by select-
ing the first n sentences from each document. The
second, random baseline, randomly selects n sen-
tences from the document. The n parameter is
based on the desired compression rate (i.e. 30%
of the document size).
6.2 Results
Various summarizers were created and evaluated.
First, we generated summaries using our method
without performing word sense disambiguation
(SemGr), but selecting the first CUI returned by
MetaMap. Second, we repeated these experiments
using the Personalized Page Rank disambigua-
tion algorithm (ppr) to disambiguate the CUIs re-
turned by MetaMap (SemGr + ppr). Finally, we
use the ?word to word? variant of the Personalized
Page Rank algorithm (ppr w2w) to perform the
disambiguation (SemGr + ppr w2w).
Table 2 shows ROUGE scores for the different
configurations of our system together with the two
baselines. All configurations significantly outper-
form both baselines (Wilcoxon Signed Ranks Test,
p < 0.01).
3http://www.biomedcentral.com/info/about/datamining/
Summarizer R-1 R-2 R-W R-SU
random .5089 .1879 .1473 .2349
lead .6483 .2566 .1621 .2646
SemGr .7504 .3283 .1915 .3117
SemGr+ppr .7737 .3419 .1937 .3178
SemGr+ppr w2w .7804 .3530 .1966 .3262
Table 2: ROUGE scores for two baselines and
SemGr (with and without WSD). Significant dif-
ferences among the three versions of SemGr are
indicated in bold font.
The use of WSD improves the average ROUGE
score for the summarizer. The ?standard? (i.e.
ppr) version of the WSD algorithm signifi-
cantly improves ROUGE-1 and ROUGE-2 metrics
(Wilcoxon Signed Ranks Test, p < 0.01), com-
pared with no WSD (i.e. SemGr). The ?word to
word? variant (ppr w2w) significantly improves
all ROUGE metrics. Performance using the ?word
to word? variant is also higher than standard ppr
in all ROUGE scores.
These results demonstrate that employing a
state of the art WSD algorithm that has been
adapted to use the UMLS Metathesaurus improves
the quality of the summaries generated by a sum-
marization system. To our knowledge this is
the first result to demonstrate that WSD can im-
prove summarization systems. However, this im-
provement is less than expected and this is prob-
ably due to errors made by the WSD system.
The Personalized PageRank algorithms (ppr and
ppr w2w) have been reported to correctly dis-
ambiguate around 58% of words in general text
(see Section 2) and, although we were unable to
quantify their performance when adapted for the
biomedical domain (see Section 5), it is highly
likely that they will still make errors. However, the
WSD performance they do achieve is good enough
to improve the summarization process.
6.3 Analysis
The results presented above demonstrate that us-
ing WSD improves the performance of our sum-
marizer. The reason seems to be that, since the ac-
curacy in the concept identification step increases,
the document graph built in the following steps is
a better approximation of the structure of the doc-
ument, both in terms of concepts and relations. As
a result, the clustering method succeeds in finding
the topics covered in the document, and the infor-
mation in the sentences selected for the summary
61
is closer to that presented in the model summaries.
We have observed that the clustering method
usually produces one big cluster along with a vari-
able number of small clusters. As a consequence,
though the heuristic for sentence selection was de-
signed to select sentences from all the clusters in
the document, the fact is that most of the sentences
are extracted from this single large cluster. This
allows our system to identify sentences that cover
the main topic of the document, while it occasion-
ally fails to extract other ?satellite? information.
We have also observed that the ROUGE scores
differ considerably from one document to others.
To understand the reasons of these differences we
examined the two documents with the highest and
lowest ROUGE scores respectively. The best case
is one of the largest document in the corpus, while
the worst case is one of the shortest (6 versus 3
pages). This was expected, since according to our
hypothesis that the document graph is an instance
of a scale-free network (see Section 4.3), the sum-
marization algorithm works better with larger doc-
uments. Both documents also differ in their under-
lying subject matter. The best case concerns the
reactions of some kind of proteins over the brain
synaptic membranes; while the worst case regards
the use of pattern matching for database searching.
We have verified that UMLS covers the vocabu-
lary contained in the first document better than in
the second one. We have also observed that the use
in the abstract of synonyms of terms presented in
the document body is quite frequent. In particular
the worst case document uses different terms in the
abstract and the body, for example ?pattern match-
ing? and ?string searching?. Since the ROUGE
metrics rely on evaluating summaries based on the
number of strings they have in common with the
model summaries the system?s output is unreason-
ably penalised.
Another problem is related to the use of
acronyms and abbreviations. Most papers in the
corpus do not include an Abbreviations section but
define them ad hoc in the document body. These
contracted forms are usually non-standard and do
not exist in the UMLS Metathesaurus. This seri-
ously affects the performance of both the disam-
biguation and the summarization algorithms, es-
pecially considering that it has been observed that
the terms (or phrases) represented in an abbrevi-
ated form frequently correspond to central con-
cepts in the document. For example, in a pa-
per from the corpus that presents an analysis tool
for simple sequence repeat tracts in DNA, only
the first occurrence of ?simple sequence repeat?
is presented in its expanded form. In the re-
maining of the document, this phrase is named
by its acronym ?SSR?. The same occurs in a pa-
per that investigates the developmental expression
of survivin during embryonic submandibular sali-
vary gland development, where ?embryonic sub-
mandibular gland? is always referred as ?SMG?.
7 Conclusion and future work
In this paper we propose a graph-based approach
to biomedical summarization. Our algorithm rep-
resents the document as a semantic graph, where
the nodes are concepts from the UMLS Metathe-
saurus and the links are different kinds of seman-
tic relations between them. This produces a richer
representation than the one provided by traditional
models based on terms.
This approach relies on accurate mapping of
the document being summarized into the concepts
in the UMLS Metathesaurus. Three methods for
doing this were compared and evaluated. The
first was to select the first mapping generated by
MetaMap while the other two used a state of the
art WSD algorithm. This WSD algorithm was
adapted for the biomedical domain by using the
UMLS Metathesaurus as a knowledge based and
MetaMap as a pre-processor to identify the pos-
sible CUIs for each term. Results show that the
system performs better when WSD is used.
In future work we plan to make use of the dif-
ferent types of information within the UMLS to
create different configurations of the Personalized
PageRank WSD algorithm and explore their ef-
fect on the summarization system (i.e. consider-
ing different UMLS relations and assigning differ-
ent weights to different relations). It would also
be interesting to test the system with other disam-
biguation algorithms and use a state of the art al-
gorithm for identifying and expanding acronyms
and abbreviations.
Acknowledgments
This research is funded by the Spanish Govern-
ment through the FPU program and the projects
TIN2009-14659-C03-01 and TSI 020312-2009-
44. Mark Stevenson acknowledges the support of
the Engineering and Physical Sciences Research
Council (grant EP/D069548/1).
62
References
S.D. Afantenos, V. Karkaletsis, and P. Stamatopou-
los. 2005. Summarization from medical docu-
ments: a survey. Artificial Intelligence in Medicine,
33(2):157?177.
E. Agirre and P. Edmonds, editors, 2006. Word
Sense Disambiguation: Algorithms and Applica-
tions. Springer.
E. Agirre and A. Soroa. 2009. Personalizing PageRank
for Word Sense Disambiguation. In Proceedings of
EACL-09, pages 33?41, Athens, Greece.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
pages 17?21.
A. Aronson. 2006. MetaMap: Mapping text to the
UMLS Metathesaurus. Technical report, U.S. Na-
tional Library of Medicine.
A.L. Barabasi and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 268:509?512.
R. Barzilay and M. Elhadad. 1997. Using lexical
chains for text summarization. In Proceedings of the
ACL Workshop on Intelligent Scalable Text Summa-
rization, pages 10?17.
A. Bawakid and M. Oussalah. 2008. A semantic
summarization system: University of Birmingham
at TAC 2008. In Proceedings of the First Text Anal-
ysis Conference (TAC 2008).
A. Bossard, M. Gnreux, and T. Poibeau. 2008. De-
scription of the LIPN systems at TAC 2008: sum-
marizing information and opinions. In Proceedings
of the First Text Analysis Conference (TAC 2008).
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
Networks and ISDN Systems, 30:1?7.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR), 22:457?479.
M. Fiszman, T. C. Rindflesch, and H. Kilicoglu.
2004. Abstraction summarization for managing the
biomedical research literature. In Proceedings of
the HLT-NAACL Workshop on Computational Lex-
ical Semantics, pages 76?83.
L. Humphreys, D. Lindberg, H. Schoolman, and
G. Barnett. 1998. The Unified Medical Lan-
guage System: An informatics research collabora-
tion. Journal of the American Medical Informatics
Association, 1(5):1?11.
L. Hunter and K. B. Cohen. 2006. Biomedical
Language Processing: Perspective Whats Beyond
PubMed? Mol Cell., 21(5):589?594.
C.-Y. Lin. 2004. Rouge: A package for automatic eval-
uation of summaries. In Proceedings of the ACL-
04 Workshop: Text Summarization Branches Out.,
pages 74?81, Barcelona, Spain.
I. Mani. 2001. Automatic summarization. Jonh Ben-
jamins Publishing Company.
R. Mihalcea and P. Tarau. 2004. TextRank - Bringing
order into text. In Proceedings of the Conference
EMNLP 2004, pages 404?411.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075?1086.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Mar-
cel Dekker, Inc.
L. Plaza, A. D??az, and P. Gerva?s. 2008. Concept-
graph based biomedical automatic summarization
using ontologies. In TextGraphs ?08: Proceedings
of the 3rd Textgraphs Workshop on Graph-Based Al-
gorithms for Natural Language Processing, pages
53?56.
L.H. Reeve, H. Han, and A.D. Brooks. 2007. The
use of domain-specific concepts in biomedical text
summarization. Information Processing and Man-
agement, 43:1765?1776.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMIA Sympo-
sium, pages 746?50, Washington, DC.
I. Yoo, X. Hu, and I-Y. Song. 2007. A coherent
graph-based semantic clustering and summarization
approach for biomedical literature and a new sum-
marization evaluation method. BMC Bioinformat-
ics, 8(9).
63
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 153?161,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Hybrid Approach to Emotional Sentence Polarity and 
Intensity Classification  
Jorge Carrillo de Albornoz, Laura Plaza, Pablo Gerv?s 
Universidad Complutense de Madrid 
Madrid, Spain 
{jcalbornoz,lplazam}@fdi.ucm.es, pgervas@sip.ucm.es 
  
 
 
 
 
 
Abstract 
In this paper, the authors present a new ap-
proach to sentence level sentiment analysis. 
The aim is to determine whether a sentence 
expresses a positive, negative or neutral sen-
timent, as well as its intensity. The method 
performs WSD over the words in the sentence 
in order to work with concepts rather than 
terms, and makes use of the knowledge in an 
affective lexicon to label these concepts with 
emotional categories.  It also deals with the ef-
fect of negations and quantifiers on polarity 
and intensity analysis. An extensive evaluation 
in two different domains is performed in order 
to determine how the method behaves in 2-
classes (positive and negative), 3-classes (posi-
tive, negative and neutral) and 5-classes 
(strongly negative, weakly negative, neutral, 
weakly positive and strongly positive) classifi-
cation tasks. The results obtained compare fa-
vorably with those achieved by other systems 
addressing similar evaluations. 
1 Introduction 
Sentiment analysis has gained much attention 
from the research community in recent years. It 
is concerned with the problem of discovering 
emotional meanings in text, and most common 
tasks usually include emotion labeling (assigning 
a text its main emotion), polarity recognition 
(classifying a statement into positive or negative) 
and subjectivity identification (determining 
whether a text is subjective or objective). The 
growing research interest is mainly due to the 
practical applications of sentiment analysis. 
Companies and organizations are interested in 
finding out costumer sentiments and opinions, 
while individuals are interested in others? opi-
nions when purchasing a product or deciding 
whether or not watching a movie. 
Many approaches have dealt with sentiment 
analysis as the problem of classifying product or 
service reviews (Pang et al, 2002; Turney, 
2002), while others have attempted to classify 
news items (Devitt and Ahmad, 2007). The task 
is usually addressed as a 2-classes classification 
problem (positive vs. negative). Recent works 
have included the neutral class, trying to detect 
not only the polarity but also the absence of emo-
tional meaning (Wilson et al, 2005; Esuli and 
Sebastiani, 2006). However, few approaches try 
to face a more fine-grained prediction of the in-
tensity (e.g. classifying the polarity into strongly 
negative, weakly negative, neutral, weakly posi-
tive and strongly positive). 
Another important problem of most of these 
approximations is that they usually work with 
terms, and so disregard the contextual meaning 
of those terms in the sentence (Martineau and 
Finin, 2009; Moilanen and Pulman, 2007). The 
use of word disambiguation is not usual in this 
task, due to the fact that most approaches use 
lexical resources created to work with terms. 
However, it is essential to correctly capture the 
meaning of these terms within the text. 
In this paper, we present a hybrid approach 
based on machine learning techniques and lexical 
rules to classify sentences according to their po-
larity and intensity. Thus, given an input text, the 
method is able to determine the polarity of each 
sentence (i.e. if it is negative or positive), as well 
as its intensity. The system tackles the effect of 
negations and quantifiers in sentiment analysis, 
and addresses the problem of word ambiguity, 
taken into account the contextual meaning of the 
terms in the text by using a word sense disam-
biguation algorithm. 
The paper is organized as follows. Section 2 
exposes some background and related work on 
sentiment analysis. Section 3 presents the lexical 
resources and corpora used by the system. Sec-
153
tion 4 describes the method proposed for polarity 
and intensity classification. Section 5 presents 
the evaluation framework and discusses the ex-
perimental results. Finally, section 6 provides 
concluding remarks and future lines of work. 
2 Related work 
The sentiment analysis discipline in computa-
tional linguistic is mainly focused on identify-
ing/classifying different emotional contents with-
in a phrase, sentence or document. This field 
usually encloses tasks such as emotion identifica-
tion, subjectivity classification and polarity rec-
ognition. Sentiment analysis has obtained great 
popularity in the last years mostly due to its suc-
cessful application to different business domains, 
such as the evaluation of products and services, 
where the goal is to discern whether the opinion 
expressed by a user about a product or service is 
favorable or unfavorable. 
Focusing on polarity recognition, the aim of 
this task is the classification of texts into positive 
or negative according to their emotional mean-
ing. Most of the approaches rely on machine 
learning techniques or rule based methods. Sta-
tistical approaches based on term frequencies and 
bags of words are frequently used in machine 
learning approximations. Pang et al (2002) 
present a comparison between three different 
machine learning algorithms trained with bags of 
features computed over term frequencies, and 
conclude that SVM classifiers can be efficiently 
used in polarity identification. Martineau and 
Finin (2009) use a similar approach where the 
words are scored using a Delta TF-IDF function 
before classifying the documents. On the other 
hand, Meena and Prabhakar (2007) study the ef-
fect of conjuncts in polarity recognition using 
rule based methods over the syntax tree of the 
sentence. Whitelaw et al (2005) introduce the 
concept of ?appraisal groups? which are com-
bined with bags of word features to automatical-
ly classify movie reviews. To this aim, they use a 
semi-automated method to generate a lexicon of 
appraising adjectives and modifiers. 
During the past few years, the problem of po-
larity recognition has been usually faced as a step 
beyond the identification of the subjectivity or 
objectivity of texts (Wiebe et al, 1999). Differ-
ent approximations have been proposed to deal 
with this problem. Pang and Lee (2004) propose 
a graph-based method which finds minimum cuts 
in a document graph to classify the sentences 
into subjective or objective. After that, they use a 
bag of words approximation to classify the sub-
jective sentences into positive or negative. Kim 
and Hovy (2004) also introduce a previous step 
to identify the subjectivity of sentences regarding 
a certain topic, and later classify these sentences 
into positives or negatives. 
Most recent approaches do not only deal with 
the 2-classes classification problem, but also in-
troduce a new class representing neutrality. Thus, 
the aim of these works is to classify the text into 
positive, negative or neutral. Wilson et al (2005) 
present a double subjectivity classifier based on 
features such as syntactic classes and sentence 
position, and more semantic features such as ad-
jective graduation. The first classifier determines 
the subjectivity or neutrality of the phrases in the 
text, while the second determines its polarity (in-
cluding neutrality). Esuli and Sebastiani (2006) 
also address this problem testing three different 
variants of a semi-supervised method, and classi-
fy the input into positive, negative or neutral. 
The method proposed yields good results in the 
2-classes polarity classification, while the results 
decrease when dealing with 3-classes. A more 
ambitious classification task is proposed by 
Brooke (2009), where the goal is to measure the 
intensity of polarity. To this aim, the author clas-
sifies the input into 3-classes (strongly-negative, 
ambivalent, and strongly-positive), 4 classes 
(strongly-negative, weakly-negative, weakly-
positive and strongly-positive) and 5-classes 
(strongly-negative, weakly-negative, ambivalent, 
weakly-positive and strongly-positive). The re-
sults decrease considerably with the number of 
classes, from 62% of accuracy for 3-classes to 
38% of accuracy for 5-classes. 
3 Corpora and resources 
The evaluation of the system has been carried out 
using two corpora from two very distinct do-
mains: the Sentence Polarity Movie Review Da-
taset1 and the one used in the SemEval 2007 Af-
fective Text task 2
                                                 
1 http://www.cs.cornell.edu/People/pabo/movie-
review-data/  
. The first one consists of 
10.662 sentences selected from different movie 
review websites. These sentences are labeled as 
positive or negative depending on whether they 
express a positive or negative opinion within the 
movie review. The second one consists of a 
training set and a test set of 250 and 1000 news 
headlines respectively, extracted from different 
news sites. Each sentence is labeled with a value 
2 http://www.cse.unt.edu/~rada/affectivetext/ 
154
between -100 and 100, where -100 means highly 
negative emotional intensity, 100 means highly 
positive and 0 means neutral. To the purpose of 
this work, the test set from the SemEval corpus 
and 1000 sentences randomly extracted from the 
Sentence Polarity Movie Review corpus (500 
positive and 500 negative) were used as evalua-
tion datasets.  
In order to identify the emotional categories 
associated to the concepts in the sentences, an 
affective lexical database based on semantic 
senses, instead of terms, is needed. To this aim, 
the authors have tested different resources and 
finally selected the WordNet Affect affective 
database (Strapparava and Valitutti, 2004). This 
affective lexicon has the particularity of assign-
ing emotional categories to synsets of the Word-
Net lexical database (Miller et al, 1990), allow-
ing the system to correctly disambiguate the 
terms using one of the many WordNet-based 
word sense disambiguation algorithms. The emo-
tional categories in WordNet Affect are orga-
nized hierarchically, and its first level distin-
guishes between positive-emotion, negative-
emotion, neutral-emotion and ambiguous-
emotion. The second level encloses the emotion-
al categories themselves, and consists of a set of 
32 categories. For this work, a subset of 16 emo-
tional categories from this level has been se-
lected, since the hierarchy proposed in WordNet 
Affect is considerably broader than those com-
monly used in sentiment analysis. On the other 
hand, the first level of emotional categories may 
be useful to predict the polarity, but it is clearly 
not enough to predict the intensity of this polari-
ty. To be precise, the subset of emotional catego-
ries used in this work is: {joy, love, liking, calm-
ness, positive-expectation, hope, fear, sadness, 
dislike, shame, compassion, despair, anxiety, 
surprise, ambiguous-agitation and ambiguous-
expectation}. The authors consider this subset to 
be a good representation of the human feeling.  
Since the WordNet Affect hierarchy does not 
provide an antonym relationship, the authors has 
created that relation for the previous set of emo-
tional categories.  Only relationships between 
emotional categories with a strongly opposite 
meaning are created, such as liking-disliking and 
joy-sadness. The purpose of this antonym rela-
tionship is twofold: first, it contributes to handle 
negation forms; and second, it can be used to 
automatically expand the affective lexicon. Both 
issues are discussed in detail later in the docu-
ment. 
On the other hand, since a good amount of 
words with a highly emotional meaning, such as 
dead, cancer and violent, are not labeled in 
WordNet Affect, these words have been manual-
ly labeled by the authors and have been later ex-
tended with their synonyms, antonyms and de-
rived adjectives using the corresponding seman-
tic and lexical relations in WordNet. This process 
has been done in two steps in order to measure 
the effect of the number of synsets labeled on the 
classification accuracy, as described in section 5.  
The WordNet Affect 1.1 lexicon consists of a 
set of 911 synsets. However, the authors have 
detected that a good number of these synsets 
have been labeled more than once, and with dif-
ferent emotional categories (e.g. the synset 
?a#00117872 {angered, enraged, furious, infu-
riated, maddened}? is labeled with three different 
categories: anger, fury and infuriation). Thus, 
after removing these synsets and those labeled 
with an emotional category not included in the 
16-categories subset used in this work, the affec-
tive lexicon presents 798 synsets. After the first 
step of semi-automatic labeling, the affective 
lexicon increased the number of synsets in 372, 
of which 100 synsets were manually labeled, and 
272 were automatically derived throughout the 
WordNet relations. The second and last step of 
semi-automatic labeling added 603 synsets to the 
lexicon, of which 200 synsets were manually 
labeled, and 403 were automatically derived.  
The final lexicon presents 1773 synsets and 4521 
words labeled with an emotional category. Table 
1 shows the distribution of the affective lexicon 
in grammatical categories. 
 
Grammatical  
Category 
WNAffect 
 
WNAffect + 
1st  step 
WNAffect + 
 2nd step 
Nouns 280 440 699 
Verbs 122 200 309 
Adjectives 273 394 600 
Adverbs 123 136 165 
 
Table 1: Distribution in grammatical categories of the syn-
sets in the affective lexicon. 
4 The method 
In this section, the method for automatically 
labeling sentences with an emotional intensity 
and polarity is presented. The problem is faced 
as a text classification task, which is accomplish-
es throughout four steps. Each step is explained 
in detail in the following subsections.  
155
4.1 Pre-processing: POS tagging and con-
cept identification 
In order to determine the appropriate emotional 
category for each word in its context, a pre-
processing step is accomplished to translate each 
term in the sentence to its adequate sense in 
WordNet. To this aim, the system analyzes the 
text, splits it into sentences and tags the tokens 
with their part of speech. The Gate architecture3 
and the Stanford Parser4
Once the sentences have been split and tagged, 
the method maps each word of each sentence 
into its sense in WordNet according to its con-
text. To this end, the lesk WSD algorithm im-
plemented in the WordNet Sense-Relate perl 
package is used (Patwardhan et al, 2005). The 
disambiguation is carried out only over the 
words belonging to the grammatical categories 
noun, verb, adjective and adverb, as only these 
categories can present an emotional meaning. As 
a result, we get the stem and sense in WordNet 
of each word, and this information is used to re-
trieve its synset.  
 were selected to carry 
out this process. In particular the Annie English 
Tokeniser, Hash Gazetter, RegEx Sentence Split-
ter and the Stanford Parser modules in Gate are 
used to analyze the input. In this step also the 
syntax tree and dependencies are retrieved from 
the Stanford Parser. These features will be used 
in the post-processing step in order to identify 
the negations and the quantifiers, as well as their 
scope. 
A good example of the importance of perform-
ing word disambiguation can be shown in the 
sentence ?Test to predict breast cancer relapse is 
approved? from the SemEval news corpus. The 
noun cancer has five possible entries in WordNet 
and only one refers to a ?malignant growth or 
tumor?, while the others are related with ?astrol-
ogy? and the ?cancer zodiacal constellation?. 
Obviously, without a WSD algorithm, the wrong 
synset will be considered, and a wrong emotion 
will be assigned to the concept. 
Besides, to enrich the emotion identification 
step, the hypernyms of each concept are also re-
trieved from WordNet. 
4.2 Emotion identification 
The aim of the emotion identification step is to 
map the WordNet concepts previously identified 
to those present in the affective lexicon, as well 
                                                 
3 http://gate.ac.uk/ 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
as to retrieve from this lexicon the corresponding 
emotional category of each concept.  
We hypothesize that the hypernyms of a con-
cept entail the same emotions than the concept 
itself, but the intensity of such emotions decreas-
es as we move up the hierarchy (i.e. the more 
general the hypernym becomes, the less its emo-
tional intensity is).  Following this hypothesis, 
when no entry is found in the affective lexicon 
for a given concept, the emotional category asso-
ciated to its nearest hypernym, if any, is used to 
label the concept. However, only a certain level 
of hypernymy is accepted, since an excessive 
generalization introduces some noise in the emo-
tion identification. This parameter has been em-
pirically set to 3 (Carrillo de Albornoz et al, 
2010). Previous experiments have shown that, 
upper this level, the working hypothesis becomes 
unreliable. 
The sentence ?Siesta cuts risk of heart disease 
death study finds? clearly illustrates the process 
described above. In this sentence, the concepts 
risk, death and disease are labeled with an emo-
tional category: in particular, the categories as-
signed to them are fear, fear and dislike respec-
tively. However, while the two firsts are re-
trieved from the affective lexicon by their own 
synsets, the last one is labeled through its hyper-
nym: since no matching is found for disease in 
the lexicon, the analysis over its hypernyms de-
tects the category dislike assigned to the synset 
of its first hypernym, which contains words such 
as illness and sickness, and the same emotion 
(dislike) is assigned to disease. 
It must be noted that, to perform this analysis, 
a previous mapping between 2.1 and 1.6 Word-
Net versions was needed, since the method and 
the affective lexicon work on different versions 
of the database.  
4.3 Post-processing: Negation and quantifi-
ers detection 
Once the concepts of the sentence have been la-
beled with their emotional categories, the next 
step aims to detect and solve the effect of the 
negations and the quantifiers on the emotional 
categories identified in the previous step.  
The effect of negation has been broadly stu-
died in NLP (Morante and Daelemans, 2009) and 
sentiment analysis (Jia et al, 2009). Two main 
considerations must be taken into account when 
dealing with negation. First, the negation scope 
may affect only a word (no reason), a proposi-
tion (Beckham does not want to play again for 
Real) or even a subject (No one would like to do 
156
this). Different approximations have been pro-
posed to delimit the scope of negation. Some 
assume the scope to be those words between the 
negation token and the first punctuation mark 
(Pang et al, 2002), others consider a fixed num-
ber of words after the negation token (Hu and 
Liu, 2004). Second, the impact of negation is 
usually neutralized by reversing the polarity of 
the sentence (Polanyi and Zaenen, 2006) or using 
contextual valence shifters which increase or 
dismiss the final value of negativity or positivity 
of the sentence (Kennedy and Inkpen, 2006). 
In this work, the negation scope is detected us-
ing the syntax tree and dependencies generated 
by the Stanford Parser. The dependency neg al-
lows us to easily determine the presence of sev-
eral simple types of negation, such as those pre-
ceded by don?t, didn?t, not, never, etc. Other 
words not identified with this dependency, but 
also with a negation meaning, such as no, none? 
nor or nobody, are identified using a negation 
token list. To determine the negation scope, we 
find in the syntax tree the first common ancestor 
that encloses the negation token and the word 
immediately after it, and assume all descendant 
leaf nodes to be affected by the negation.  
For each concept in the sentence that falls into 
the scope of a negation, the system retrieves its 
antonym emotional category, if any, and assigns 
this category to the concept. If no antonym emo-
tion is obtained, the concept is labeled with no 
emotion, according to the premise that the nega-
tion may change or neutralize the emotional po-
larity. An example of this process can be shown 
in the sentence ?Children and adults enamored 
of all things pokemon won't be disappointed?. In 
this sentence, the Stanford Parser discovers a 
negation and the system, through the syntax tree, 
determines that the scope of the negation enclos-
es the words ?won?t be disappointed?. As the 
synset of ?disappointed? has been labeled with 
the emotional category despair, its antonym is 
retrieved, and the emotional category of the an-
tonym, hope, is used to label the concept.  
On the other hand, the quantifiers are words 
considered in sentiment analysis as amplifiers or 
downtoners (Quirk et al, 1985). That is to say, 
the word very in the sentence ?That is a very 
good idea? amplifies the intensity of the emo-
tional meaning and the positivity of the sentence, 
while the word less in the sentence ?It is less 
handsome than I was expecting? dismisses its 
intensity and polarity. The most common ap-
proach to identify quantifiers is the use of lists of 
words which play specific grammatical roles in 
the sentence. These lists normally contain a fixed 
value for all positive words and another value for 
all negative words (Polanyi and Zaenen, 2006). 
By contrast, Brooke (2009) proposes a novel ap-
proach where each quantifier is assigned its own 
polarity and weight.  
The quantifiers are usually represented as sen-
tence modifiers, assuming their scope to be the 
whole sentence and modifying its overall polari-
ty. However, when dealing with sentences like 
?The house is really nice and the neighborhood 
is not bad?, these approaches assume that the 
quantifier really amplifies the intensity of both 
conjunctives, when it only should amplify the 
intensity of the first one. By contrast, our ap-
proach determines the scope of the quantifiers by 
the syntax tree and the dependencies over them. 
Thus, when a quantifier is detected in a sentence, 
the dependencies are checked and only those that 
play certain roles, such as adverbial or adjectival 
modifiers, are considered. All concepts affected 
by a quantifier are marked with the weight cor-
responding to that quantifier, which will serve to 
amplify/dismiss the emotions of these concepts 
in the classification step.  The quantifier list used 
here is the one proposed in Brooke (2009). 
The sentence ?Stale first act, scrooge story, 
blatant product placement, some very good com-
edic songs? illustrates the analysis of the quan-
tifiers. The system detects two tokens which are 
in the quantifier list and play the appropriate 
grammatical roles. The first quantifier some af-
fects to the words ?very good comedic songs?, 
while the second quantifier very only affects to 
?good?. So these concepts are marked with the 
specific weight of each quantifier. Note that the 
concept ?good? is marked twice. 
4.4 Sentence classification 
Up to this point, the sentence has been labeled 
with a set of emotional categories, negations and 
their scope have been detected and the quantifi-
ers and the concepts affected by them have been 
identified. In this step, this information is used to 
translate the sentence into a Vector of Emotional 
Occurrences (VEO), which will be the input to 
the machine learning classification algorithm. 
Thus, each sentence is represented as a vector of 
16 values, each of one representing an emotional 
category. The VEO vector is generated as fol-
lows: 
? If the concept has been labeled with an 
emotional category, the position of the 
vector for this category is increased in 1. 
157
? If no emotional category has been found 
for the concept, then the category of its 
first hypernym labeled is used. As the 
hypernym generalizes the meaning of the 
concept, the value assigned to the position 
of the emotional category in the VEO is 
weighted as follows: 
[ ] [ ]
1.
1
+
+=
DepthHyper
iVEOiVEO  
? If a negation scope encloses the concept, 
then the antonym emotion is used, as de-
scribed in the previous step. The emotion-
al category position of this antonym in the 
VEO is increased in 0.9. Different tests 
have been carried out to set this parameter, 
and the 0.9 value got the best results. The 
reason for using a lower value for the 
emotional categories derived from nega-
tions is that the authors consider that a ne-
gation changes the emotional meaning of a 
concept but usually in a lower percentage. 
For example, the sentence ?The neighbor-
hood is not bad? does not necessarily 
mean that it is a good neighborhood, but it 
is a quite acceptable one.  
? If a concept is affected by a quantifier, 
then the weight of that quantifier is added 
to the position in the VEO of the emotion-
al category assigned to the concept. 
Thus, a sentence like ?This movie?. isn?t 
worth the energy it takes to describe how really 
bad it is? will be represented by the VEO [1.0, 0, 
0.0, 0, 0, 0.0, 0, 0, 2.95, 0, 0, 0, 0, 0, 0, 0].  In 
this sentence, the concept movie is labeled with 
the emotional category joy, the concept worth is 
labeled with positive-expectation, the concept 
energy is labeled with liking, and the concept 
bad is labeled with dislike. Since the concepts 
worth and energy fall into the negation scope, 
they both change their emotional category to dis-
like. Besides, since the concept bad is amplified 
by the quantifier really, the weight of this con-
cept in the VEO is increased in 0.15. 
5 Evaluation framework and results 
In this work, two different corpora have been 
used for evaluation (see Section 3): a movie re-
view corpus containing 1000 sentences labeled 
with either a positive or negative polarity; and a 
news headlines corpus composed of 1000 sen-
tences labeled with an emotional intensity value 
between -100 and 100. 
To determine the best machine learning algo-
rithm for the task, 20 classifiers currently imple-
mented in Weka5
5.1 Evaluating polarity classification 
 were compared. We only show 
the results of the best performance classifiers: a 
logistic regression model (Logistic), a C4.5 deci-
sion tree (J48Graph) and a support vector ma-
chine (LibSVM). The best outcomes for the three 
algorithms were reported when using their de-
fault parameters, except for J48Graph, where the 
confidence factor was set to 0.2. The evaluation 
is accomplishes using 10-fold cross validation. 
Therefore, 100 instances of each dataset are held 
back for testing in each fold, and the additional 
900 instances are used for training. 
We first analyze the effect of expanding the cov-
erage of the emotional lexicon by semi-
automatically adding to WordNet Affect more 
synsets labeled with emotional categories, as ex-
plained in Section 3. To this end, we compare the 
results of the method using three different affec-
tive lexical databases: WordNet Affect and 
WordNet Affect extended with 372 and 603 syn-
sets, respectively. For the sake of comparing the 
results in both corpora, the news dataset has been 
mapped to a -100/100 classification (-100 = [-
100, 0), 100 = [0,100]). 
Table 2 shows the results as average precision 
and accuracy of these experiments. Note that, as 
the weight of mislabeling for both classes is the 
same and the classes are balanced, accuracy is 
equal to recall in all cases. Labeling 975 new 
synsets significantly improves the performance 
of our system in both datasets and for all ML 
techniques. In particular, the best improvement is 
achieved by the Logistic classifier: from 52.7% 
to 72.4% of accuracy in the news dataset, and 
from 50.5% to 61.5% of accuracy in the movies 
dataset.  
 
Emotional  
Lexicon 
Method 
News Corpus Movie Reviews 
Pr. Ac. Pr. Ac. 
WNAffect 
Logistic 52.8 52.7 51.3 50.5 
J48Graph 27.7 52.6 50 50 
LibSVM 27.7 52.6 53.2 50.6 
WNAffect + 
372 synsets 
Logistic 69.9 65.2 53.9 53.8 
J48Graph 70.1 64.8 55.3 55.1 
LibSVM 68.9 63.9 52 51.8 
WNAffect + 
603 synsets 
Logistic 73.8 72.4 61.6 61.5 
J48Graph 73.6 70.9 60.9 60.9 
LibSVM 71.6 70.3 62.5 59.4 
 
Table 2: Precision and accuracy percentages achieved by 
our system using different affective databases. 
 
                                                 
5 http://www.cs.waikato.ac.nz/ml/weka/ 
158
We have observed that, especially in the news 
dataset, an important number of sentences that 
are labeled with a low positive or negative emo-
tional intensity could be perfectly considered as 
neutral. The intensity of these sentences highly 
depends on the previous knowledge and particu-
lar interpretation of the reader. For instance, the 
sentence ?Looking beyond the iPhone? does not 
express any emotion itself, unless you are fan or 
detractor of Apple. However, this sentence is 
labeled in the corpus with a 15 intensity value. It 
is likely that these kinds of sentences introduce 
noise into the dataset. In order to estimate the 
influence of such sentences in the experimental 
results, we conducted a test removing from the 
news dataset those instances with an intensity 
value in the range [-25, 25]. As expected, the 
accuracy of the method increases substantially, 
i.e. from 72.4% to 76.3% for logistic regression. 
The second group of experiments is directed to 
evaluate if dealing with negations and quantifiers 
improves the performance of the method. To this 
end, the approach described in Section 4.3 was 
applied to both datasets. Table 3 shows that 
processing negations consistently improves the 
accuracy of all algorithms in both datasets; while 
the effect of the quantifiers is not straightforward. 
Even if we expected that using quantifiers would 
lead to better results, the performance in both 
datasets decreases in 2 out of the 3 ML algo-
rithms. However, combining both features im-
proves the results in both datasets. The reason 
seems to be that, when no previous negation de-
tection is performed, if the emotional category 
assigned to certain concepts are wrong (because 
these concepts are affected by negations), the 
quantifiers will be weighting the wrong emotions.  
 
Features Method 
News Corpus Movie Reviews 
Pr. Ac. Pr. Ac. 
Negations  
Logistic 74.2 72.5 61.7 61.6 
J48Graph 74.1 71.2 62.8 62.6 
LibSVM 72.7 71.1 62.4 60.1 
Quantifiers 
Logistic 73.7 72.2 61.9 61.9 
J48Graph 73.6 70.9 59.5 59.5 
LibSVM 72.1 70.6 61.1 59 
Negations + 
Quantifiers 
Logistic 74.4 72.7 62.4 62.4 
J48Graph 74.1 71.2 62.5 62.1 
LibSVM 72.8 71.2 62.6 60.5 
 
Table 3: Precision and accuracy of the system improved 
with negation and quantifier detection. 
 
The comparison with related work is difficult 
due to the different datasets and methods used in 
the evaluations. For instance, Pang et al (2002) 
use the Movie Review Polarity Dataset, achiev-
ing an accuracy of 82.9% training a SVM over a 
bag of words. However, their aim was to deter-
mine the polarity of documents (i.e. the whole 
movie reviews) instead of sentences. When 
working at the sentence level, the information 
from the context is missed, and the results are 
expected to be considerably lower. As a matter 
of fact, it happens that many sentences in the 
Sentence Polarity Movie Review Dataset are la-
beled as positive or negative, but do not express 
any polarity when taken out of the context of the 
overall movie review. This conclusion is also 
drawn by Meena and Prabhakar (2007), who 
achieve an accuracy of 39% over a movie review 
corpus (not specified) working at the sentence 
level, using a rule based method to analyze the 
effect of conjuncts. This accuracy is well below 
that of our method (62.6%).  
Molianen and Pulman (2007) present a senti-
ment composition model where the polarity of a 
sentence is calculated as a complex function of 
the polarity of its parts. They evaluate their sys-
tem over the SemEval 2007 news corpus, and 
achieve an accuracy of 65.6%, under our same 
experimental conditions, which is also signifi-
cantly lower than the accuracy obtained by our 
method.  
5.2 Evaluating intensity classification 
Apart from identifying of polarity, we also want 
to examine the ability of our system to determine 
the emotional intensity in the sentences. To this 
aim, we define two intensity distributions: the 3-
classes and the 5-classes distribution. For the 
first distribution, we map the news dataset to 3-
classes: negative [-100, -50), neutral [-50, 50) 
and positive [50, 100]. For the second distribu-
tion, we map the dataset to 5-classes: strongly 
negative [-100, -60), negative [-60, -20), neutral 
[-20, 20), positive [20, 60) and strongly positive 
[60, 100]. We can see in Table 4 that, as the 
number of intensity classes increases, the results 
are progressively worse, since the task is pro-
gressively more difficult. 
 
Intensity 
classes 
Method 
News Corpus 
Pr. Ac. 
2-classes 
Logistic 74.4 72.7 
J48Graph 74.1 71.2 
LibSVM 72.8 71.2 
3-classes 
Logistic 60.2 63.8 
J48Graph 66 64.8 
LibSVM 54.8 64.6 
5-classes 
Logistic 48.3 55.4 
J48Graph 47.3 54.8 
LibSVM 43.1 53.1 
 
Table 4: Precision and accuracy in three different intensity 
classification tasks. 
159
The 3-classes distribution coincides exactly 
with that used in one of the SemEval 2007 Af-
fective task, so that we can easily compare our 
results with those of the systems that participated 
in the task. The CLaC and CLaC-NB systems 
(Andreevskaia and Bergler, 2007) achieved, re-
spectively, the best precision and recall. CLaC 
reported a precision of 61.42 % and a recall of 
9.20%; while CLaC-NB reported a precision of 
31.18% and a recall of 66.38%. Our method 
clearly outperforms both systems in precision, 
while provides a recall (which is equal to the ac-
curacy) near to that of the best system. Besides, 
our results for both metrics are well-balanced, 
which does not occur in the other systems. 
Regarding the 5-classes distribution evalua-
tion, to the authors? knowledge no other work 
has been evaluated under these conditions. How-
ever, our system reports promising results: using 
5 classes it achieves better results than other par-
ticipant in the SemEval task using just 3 classes 
(Chaumartin, 2007; Katz et al, 2007). 
5.3 Evaluating the effect of  word ambiguity 
on sentiment analysis 
A further test has been conducted to examine the 
effect of word ambiguity on the classification 
results. To this aim, we repeated the experiments 
above without using WSD. First, we simply as-
signed to each word its first sense in WordNet. 
Second, we selected these senses randomly.  The 
results are presented in Table 5. We only show 
those of the best algorithm for each intensity dis-
tribution.  
 
Intensity classes Method 
News Corpus 
Pr. Ac. 
2-classes (Logistic) 
WSD 74.4 72.6 
1st Sense 71.6 69.3 
Random Sense 69.1 64.1 
3-classes (J48Graph) 
WSD 66 64.8 
1st Sense 59 62.9 
Random Sense 50.8 61 
5-classes  (Logistic) 
WSD 48.3 55.4 
1st Sense 43.7 53.8 
Random Sense 46.8 51.6 
 
Table 5: Precision and accuracy for three different word 
disambiguation strategies. 
 
It can be observed that, even though the use of 
word disambiguation improves the classification 
precision and accuracy, the improvement with 
respect to the first sense heuristic is less than ex-
pected. This may be due to the fact that the 
senses of the words in WordNet are ranked ac-
cording to their frequency, and so the first sense 
of a word is also the most frequent one. Besides, 
the Most Frequent Sense (MFS) heuristic in 
WSD is usually regarded as a difficult competi-
tor. On the contrary, the improvement with re-
spect to the random sense heuristic is quite re-
markable. 
 
6 Conclusions and future work 
In this paper, a novel approach to sentence level 
sentiment analysis has been described. The sys-
tem has resulted in a good method for sentence 
polarity classification, as well as for intensity 
identification. The results obtained outperform 
those achieved by other systems which aim to 
solve the same task.  
Nonetheless, some considerations must be 
noted. Even with the extended affective lexicon, 
around 1 in 4 sentences of each corpus has not 
been assigned any emotional category, some-
times because their concepts are not labeled in 
the lexicon, but mostly because their concepts do 
not have any emotional meaning per se. A test on 
the news corpus removing those sentences not 
labeled with any emotional meaning has been 
performed for the 2-classes classification prob-
lem, allowing the method to obtain an accuracy 
of 81.7%. However, to correctly classify these 
sentences, it would be necessary to have addi-
tional information about their contexts (i.e. the 
body of the news item, its section in the newspa-
per, etc.).   
Finally, the authors plan to extend the method 
to deal with modal and conditional operators, 
which will allow us to distinguish among situa-
tions that have happened, situations that are hap-
pening, situations that could, might or possibly 
happen or will happen, situations that are wished 
to happen, etc. 
 
Acknowledgments 
This research is funded by the Spanish Ministry 
of Science and Innovation (TIN2009-14659-
C03-01), the Comunidad Autonoma de Madrid 
and the European Social Fund through the IV 
PRICIT program, and the Spanish Ministry of 
Education through the FPU program. 
References  
Julian Brooke. 2009. A Semantic Approach to Auto-
mated Text Sentiment Analysis. Simon Fraser 
University. Ph. D. Thesis. 
Jorge Carrillo de Albornoz, Laura Plaza and Pablo 
Gerv?s. 2010. Improving Emotional Intensity Clas-
160
sification using Word Sense Disambiguation. Re-
search in Computing Science 46 :131-142. 
Fran?ois-R?gis Chaumartin. 2007. UPAR7: A Know-
ledge-based System for Headline Sentiment Tag-
ging. In Proceedings of the 4th Workshop on Se-
mantic Evaluations (SemEval 2007), pages 422-
425. 
Ann Devitt and Khurshid Ahmad. 2007. Sentiment 
Polarity Identification in Financial News: A Cohe-
sion-based Approach. In Proceedings of the 45th 
Annual Meeting of the ACL, pages 984-991. 
Andrea Esuli and Fabrizio Sebastiani. 2006. Deter-
mining Term Subjectivity and Term Orientation for 
Opinion Mining. In Proceedings of the 11th Confe-
rence of the EACL, pages 193-200. 
Minging Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 
10th ACM SIGKDD Conference on Knowledge 
Discovery and Data Mining, pages 168-177. 
Lifeng Jia, Clement Yu and Weiji Meng. 2009. The 
Effect of Negation on Sentiment Analysis and Re-
trieval Effectiveness. In Proceeding of the 18th  
ACM Conference on Information and Knowledge 
Management, pages 1827-1830. 
Phil Katz, Matthew Singleton and Richard Wicen-
towski. 2007. SWAT-MP: the SemEval-2007 Sys-
tems for Task 5 and Task 14. In Proceedings of the 
4th Workshop on Semantic Evaluations (SemEval 
2007), pages 308-313. 
Alistair Kennedy and Diana Inkpen. 2006. Sentiment 
Classification of Movie Reviews Using Contextual 
Valence Shifters. Computational Intelligence 
22(2): 110-125. 
Soo-Min Kim and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. In Proceedings of COL-
ING 2004, pages 1367-1373. 
Justin Martineau and Tim Finin. 2009. Delta TFIDF: 
An Improved Feature Space for Sentiment Analy-
sis. In Proceedings of the 3rd AAAI International 
Conference on Weblogs and Social Media. 
Arun Meena and T.V. Prabhakar. 2007. Sentence 
Level Sentiment Analysis in the Presence of Con-
juncts Using Linguistic Analysis. In Proceedings 
of ECIR 2007, pages 573-580. 
George A. Miller, Richard Beckwith, Christiane Fell-
baum Derek Gross and Katherine Miller. 1990. In-
troduction to WordNet: An On-Line Lexical Data-
base. International Journal of Lexicography 
3(4):235-244. 
Karo Moilanen and Stephen Pulman. 2007. Sentiment 
Composition. In Proceedings of RANLP 2007, 
pages 378-382. 
Roser Morante and Walter Daelemans. 2009. A Meta-
learning Approach to Processing the Scope of Ne-
gation. In Proceedings of the CONLL 2009, pages 
21-29. 
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment Classification using 
Machine Learning Techniques. In Proceedings of 
CoRR 2002. 
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis using Subjectivity Sum-
marization based on Minimum Cuts. In Proceed-
ings of the 42nd  Annual Meeting of the ACL, pages 
271-278. 
Siddharth Patwardhan, Satanjeev Banerjee and Ted 
Pedersen. 2005. SenseRelate::TargetWord - A Ge-
neralized Framework for Word Sense Disambigua-
tion. In Proceedings of the ACL 2005 on Interac-
tive Poster and Demonstration Sessions, pages 73-
76. 
Livia Polanyi and Annie Zaenen. 2006. Contextual 
Valence Shifters. Computing Attitude and Affect in 
Text: Theory and Applications. In The Information 
Retrieval Series 20, pages 1-10. 
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech 
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman. 
Carlo Strapparava and Alessandro Valitutti. 2004. 
Wordnet-Affect: an Affective Extension of Word-
Net. In Proceedings of the LREC 2004, pages 
1083-1086.  
Peter D. Turney. 2002. Thumbs up or Thumbs 
down?: Semantic Orientation applied to Unsuper-
vised Classification of Reviews. In Proceedings of 
the 40th Annual Meeting of the ACL, pages 417-
424. 
Casey Whitelaw, Navendu Garg and Shlomo Arga-
mon. 2005. Using Appraisal Groups for Sentiment 
Analysis. In Proceedings of the 14th ACM Confe-
rence on Information and Knowledge Manage-
ment, pages 625-631. 
Janyce M. Wiebe, Rebecca F. Bruce and Thomas P. 
O?Hara. 1999. Development and Use of a Gold-
standard Data Set for Subjectivity Classification. In 
Proceedings of the 37th Annual Meeting of the 
ACL, pages 246-253. 
Theresa Wilson, Janyce Wiebe and Paul Hoffman. 
2005. Recognizing Contextual Polarity in Phrase-
level Sentiment Analysis. In Proceedings of the 
HLT-EMNLP 2005, pages 347-354. 
161
