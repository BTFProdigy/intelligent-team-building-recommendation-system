Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 165?172,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Mining a comparable text corpus for a Vietnamese - French  
statistical machine translation system 
 
 
Thi-Ngoc-Diep Do *,**, Viet-Bac Le *, Brigitte Bigi*,  
Laurent Besacier*, Eric Castelli** 
*LIG Laboratory, CNRS/UMR-5217, Grenoble, France 
** MICA Center, CNRS/UMI-2954, Hanoi, Vietnam 
thi-ngoc-diep.do@imag.fr 
 
 
 
 
Abstract 
This paper presents our first attempt at con-
structing a Vietnamese-French statistical 
machine translation system. Since Vietnam-
ese is an under-resourced language, we con-
centrate on building a large Vietnamese-
French parallel corpus. A document align-
ment method based on publication date, spe-
cial words and sentence alignment result is 
proposed. The paper also presents an appli-
cation of the obtained parallel corpus to the 
construction of a Vietnamese-French statis-
tical machine translation system, where the 
use of different units for Vietnamese (sylla-
bles, words, or their combinations) is dis-
cussed. 
1 Introduction 
Over the past fifty years of development, ma-
chine translation (MT) has obtained good results 
when applied to several pairs of languages such 
as English, French, German, Japanese, etc. How-
ever, for under-resourced languages, it still re-
mains a big gap. For instance, although 
Vietnamese is the 14th widely-used language in 
the world, research on MT for Vietnamese is 
very rare.  
The earliest MT system for Vietnamese is the 
system from the Logos Corporation, developed 
as an English-Vietnamese system for translating 
aircraft manuals during the 1970s (Hutchins, 
2001). Until now, in Vietnam, there are only four 
research groups working on MT for Vietnamese-
English (Ho, 2005). However the results are still 
modest. 
MT research on Vietnamese-French occurs 
even more rarely. Doan (2001) proposed a trans-
lation module for Vietnamese within ITS3, a 
multilingual MT system based on the classical 
analysis-transfer-generation approach. Nguyen 
(2006) worked on Vietnamese language and 
Vietnamese-French text alignment. But no com-
plete MT system for this pair of languages has 
been published so far.  
There are many approaches for MT: rule-based 
(direct translation, interlingua-based, transfer-
based), corpus-based (statistical, example-based) 
as well as hybrid approaches. We focus on build-
ing a Vietnamese-French statistical machine 
translation (SMT) system. Such an approach re-
quires a parallel bilingual corpus for source and 
target languages. Using this corpus, we build a 
statistical translation model for source/target lan-
guages and a statistical language model for target 
language. Then the two models and a search 
module are used to decode the best translation 
(Brown et al, 1993; Koehn et al, 2003). 
Thus, the first task is to build a large parallel 
bilingual text corpus. This corpus can be de-
scribed as a set of bilingual sentence pairs. At the 
moment, such a large parallel corpus for Viet-
namese-French is unavailable. (Nguyen, 2006) 
presents a Vietnamese-French parallel corpus of 
law and economics documents. Our SMT system 
was trained using Vietnamese-French news cor-
pus created by mining a comparable bilingual 
text corpus from the Web.  
Section 2 presents the general methodology of 
mining a comparable text corpus. We present an 
overview of document alignment methods and 
sentence alignment methods, and discuss the 
document alignment method we utilized, which 
is based on publishing date, special words, and 
sentence alignment results. Section 3 describes 
our experiments in automatically mining a multi-
lingual news website to create a Vietnamese-
French parallel text corpus. Section 4 presents 
165
our application to rapidly build Vietnamese-
French SMT systems using the obtained parallel 
corpus, where the use of different units for Viet-
namese (syllables, words, or their combination) is 
discussed. Section 5 concludes and discusses fu-
ture work. 
2 Mining a comparable text corpus  
In (Munteanu and Daniel Marcu, 2006), the au-
thors present a method for extracting parallel 
sub-sentential fragments from comparable bilin-
gual corpora. However this method is in need of 
an initial parallel bilingual corpus, which is not 
available for the pair of language Vietnamese-
French (in the news domain). 
The overall process of mining a bilingual text 
corpus which is used in a SMT system typically 
takes five following steps (Koehn, 2005): raw 
data collection, document alignment, sentence 
splitting, tokenization and sentence alignment. 
This section presents the two main steps: docu-
ment alignment and sentence alignment. We also 
discuss the proposed document alignment 
method. 
2.1 Document alignment 
Let S1 be set of documents in language L1; let S2 
be set of documents in language L2. Extracting 
parallel documents or aligning documents from 
the two sets S1, S2 can be seen as finding the 
translation document D2 (in the set S2) of a 
document D1 (in the set S1). We call this pair of 
documents D1-D2 a parallel document pair 
(PDP). 
For collecting bilingual text data for the two 
sets S1, S2, the Web is an ideal source as it is 
large, free and available (Kilgarriff and Grefen-
stette, 2003). For this kind of data, various meth-
ods to align documents have been proposed. 
Documents can be simply aligned based on the 
anchor link, the clue in URL (Kraaij et al, 2003) 
or the web page structure (Resnik and Smith, 
2003). However, this information is not always 
available or trustworthy. The titles of documents 
D1, D2 can also be used (Yang and Li, 2002), but 
sometimes they are completely different. 
Another useful source of information is invari-
ant words, such as named entities, dates, and 
numbers, which are often common in news data. 
We call these words special words. (Patry and 
Langlais, 2005) used numbers, punctuation, and 
entity names to measure the parallelism between 
two documents. The order of this information in 
document is used as an important criterion. How-
ever, this order is not always respected in a PDP 
(see an example in Table 1). 
 
French document Vietnamese document 
      Selon l'Administration 
nationale du tourisme, les 
voyageurs en provenance de 
l'Asie du Nord-Est (Japon, 
R?publique de Cor?e,...) 
repr?sentent 33%, de l'Eu-
rope, 16%, de l'Am?rique 
du Nord, 13%, d'Australie 
et de Nouvelle-Z?lande, 6%. 
     En outre, depuis le d?but 
de cette ann?e, environ 2,8 
millions de touristes ?tran-
gers ont fait le tour du Viet-
nam, 78% d'eux sont venus 
par avion.  
     Cela t?moigne d'un af-
flux des touristes riches au 
Vietnam.? 
     Trong s? g?n 2,8 tri?u 
l??t kh?ch qu?c t? ??n Vi?t 
Nam t? ??u n?m ??n nay, 
l??ng kh?ch ??n b?ng 
???ng h?ng kh?ng v?n 
chi?m ch? ??o v?i kho?ng 
78%.  
     ?i?u n?y cho th?y, d?ng 
kh?ch du l?ch ch?t l??ng 
cao ??n Vi?t Nam t?ng 
nhanh. 
     Theo th?ng k? th? kh?ch 
qu?c t? v?o Vi?t Nam cho 
th?y kh?ch ??ng B?c ? 
(Nh?t B?n, H?n Qu?c) 
chi?m t?i 33%, ch?u ?u 
chi?m 16%, B?c M? 13%, 
?xtr?ylia v? Niu Dil?n 
chi?m 6%.? 
Table 1. An example of a French-Vietnamese 
parallel document pair in our corpus. 
2.2 Sentence alignment 
From a PDP D1-D2, the sentence alignment 
process identifies parallel sentence pairs (PSPs) 
between two documents D1 and D2. For each 
D1-D2, we have a set SenAlignmentD1-D2 of 
PSPs. 
SenAlignmentD1-D2 = {?sen1-sen2?| sen1 is 
zero/one/many sentence(s) in document D1, 
sen2 is zero/one/many sentence(s) in docu-
ment D2, sen1-sen2 is considered as a 
PSP}. 
We call a PSP sen1-sen2 alignment type m:n 
when sen1 contains m consecutive sentences and 
sen2 contains n consecutive sentences. 
Several automatic sentence alignment ap-
proaches have been proposed based on sentence 
length (Brown et al, 1991) and lexical informa-
tion (Kay and Roscheisen, 1993). A hybrid ap-
proach is presented in (Gale and Church, 1993) 
whose basic hypothesis is that ?longer sentences 
in one language tend to be translated into longer 
sentences in the other language, and shorter sen-
tences tend to be translated into shorter sen-
tences?. Some toolkits such as Hunalign1 and 
Vanilla2 implement these approaches. However, 
they tend to work best when documents D1, D2 
contain few sentence deletions and insertions, 
and mainly contain PSPs of type 1:1. 
                                                          
1
 http://mokk.bme.hu/resources/hunalign 
2
 http://nl.ijs.si/telri/Vanilla/ 
166
Ma (2006) provides an open source software 
called Champollion1 to solve this limitation. 
Champollion permits alignment type m:n (m, n = 
0,1,2,3,4), so the length of sentence does not play 
an important role. Champollion uses also lexical 
information (lexemes, stop words, bilingual dic-
tionary, etc.) to align sentences. Champollion can 
easily be adapted to new pairs of languages. 
Available language pairs in Champollion are 
English-Arabic and English-Chinese (Ma, 2006).  
2.3 Our document alignment method 
Figure 1 describes our methodology for docu-
ment alignment. For each document D1 in the set 
S1, we find the aligned document D2 in the set 
S2.  
We propose to use publishing date, special 
words, and the results of sentence alignment to 
discover PDPs. First, the publishing date is used 
to reduce the number of possible documents D2. 
Then we use a filter based on special words con-
tained in the documents to determine the candi-
date documents D2. Finally, we eliminate 
candidates in D2 based on the combination of 
document length information and lexical infor-
mation, which are extracted from the results of 
sentence alignment. 
 
 
Figure 1. Our document alignment scheme. 
2.3.1 The first filter: publishing date 
We assume that the document D2 is translated 
and published at most n days after the publishing 
date of the original document. We do not know 
whether D1 or D2 is the original document, so 
                                                          
1
 http://champollion.sourceforge.net 
we assume that D2 is published n days before or 
after D1. After filtering by publishing date crite-
rion, we obtain a subset S2? containing possible 
documents D2. 
2.3.2 The second filter: special words  
In our case, the special words are numbers and 
named entities. Not only numbers (0-9) but also 
attached symbols (?$?, ?%?, ???, ?,?, ?.??) are 
extracted from documents, for example: 
?12.000$?; ?13,45?; ?50%?;? Named entities 
are specified by one or several words in which 
the first letter of each word is upper case, e.g. 
?Paris?, ?Nations Unies? in French.  
While named entities in language L1 are usu-
ally translated into the corresponding names in 
language L2, in some cases the named entities in 
L1 (such as personal names or organization 
names) do not change in L2. In particular, many 
Vietnamese personal names are translated into 
other languages by removal of diacritical marks 
(see examples in Table 2). 
 
 French Vietnamese Vietnamese 
-Removed 
diacritic 
Nations 
Unies 
Li?n H?p 
Qu?c 
Lien Hop 
Quoc 
Changed 
France Ph?p Phap 
ASEAN ASEAN ASEAN 
Nong Duc 
Manh 
N?ng ??c 
M?nh 
Nong Duc 
Manh 
Not 
changed 
Dien Bien ?i?n Bi?n Dien Bien 
Table 2. Some examples of named entities in 
French-Vietnamese. 
 
All special words are extracted from document 
D1. This gives a list of special words w1,w2,?wn. 
For each special word, we search in the set S2? 
documents D2 which contain this special word. 
For each word, we obtain a list of documents D2. 
The document D2 which has the biggest number 
of appearance in all lists is chosen. It is the 
document containing the highest number of spe-
cial words. We can find zero, one or several 
documents which are satisfactory. We call this 
set of documents set S2?? (see in Figure 2). 
The way that we use special words is different 
from the way used in (Patry and Langlais, 2005). 
We do not use punctuation as special words. We 
use the attached symbols (?$?, ?%?, ???, ?) with 
the number. Furthermore, in our method, the or-
der of special words in documents is not impor-
tant, and if a special word appears several times 
in a document, it does not affect the result. 
 
S1 S2 
Filter by publishing date  
(?n days) 
S2?
S2??
?S2?  D2 },ent{SenAlignm D2-D1 ?
Filter by special words 
 (numbers+ named entities) 
Align sentences 
Filter SenAlignment 
(use ?, ?) 
sen2}-{sen1D2}-{D1 +
D1 D2
167
  
Figure 2. Using special words to filter documents 
D2. 
2.3.3 The third filter: sentence alignments  
As mentioned in section 2.3.2, for each document 
D1, we discover a set S2??, which contains zero, 
one or several documents D2. When we continue 
to align sentences for each PDP D1-D2, we get a 
lot of low quality PSPs. The results of sentence 
alignment allow us to further filter the documents 
D2. 
After aligning sentences, we have a set of 
PSPs, SenAlignmentD1-D2, for each PDP D1-D2. 
We add two rules to filter documents D2.  
When D1-D2 is not a true PDP, it is hard to 
find out PSPs. So we note the number of PSPs in 
the set SenAlignmentD1-D2 by 
card(SenAlignmentD1-D2). The number of sentence 
pairs which can not find their alignment partner 
(when sen1 or sen2 is ?null?) is noted by 
nbr_omitted(SenAlignmentD1-D2).  
When ?>)ignmentcard(SenAl
)mentd(SenAlignnbr_omitte
D2-D1
D2-D1
 , this 
PDP D1-D2 will be eliminated. 
This first rule also deals with the problem of 
document length, sentence deletions and sentence 
insertions. 
The second rule makes use of lexical informa-
tion. For each PSP, we add two scores xL1 and xL2 
for sen1 and sen2.  
i
i
Li
seninwordsofnumber
seninwordstranslatedofnumber
x
????
?????
=
 
Translated words are words having translation 
equivalents in the other sentence. In this rule, we 
do not take into account the stop words. Table 3 
shows an example for calculating two scores xL1 
and xL2  for a PSP.  
In the second rule, when all PSPs in Se-
nAlignmentD1-D2 have two scores xL1 and xL2 that 
are both smaller than ?, this PDP D1-D2 will be 
eliminated. This rule removes the low quality 
PDP which creates a set of low quality PSPs. 
sen1 (in French) : ils ont ?chang? leurs opinions pour 
parvenir ? la signature de documents constituant la base 
du d?veloppement et de l' intensification de la coop?ra-
tion en ?conomie en commerce et en investissement ainsi 
que celles dans la culture le sport et le tourisme entre les 
deux pays 
sen2 (in Vietnamese) : hai b?n ?? ti?n_h?nh trao_??i ?? 
k?_k?t c?c v?n_b?n l?m c?_s? cho vi?c m?_r?ng v? 
t?ng_c??ng quan_h? h?p_t?c kinh_t? th??ng_m?i 
??u_t? v?n_ho? th?_thao v? du_l?ch gi?a hai n??c 
Translated words :  
??chan-
ger:trao_??i? ;?base:c?_s??,?intensification:t?ng_c??n
g? ;?coop?ration:h?p_t?c?,??conomie:kinh_t?? ; inves-
tissement:??u_t??,?sport:th?_thao? ; ?tou-
risme :du_l?ch? ; ?pays:n??c? 
Number of non-stop words in sen1 19  
Number of non-stop words in sen2 21 
Number of translated words 9 
xL1 = 9/19=0.47 ; xL2 = 9/21=0.43 
Table 3. Example for calculating two scores xL1 
and xL2. 
 
After using three filters based on information 
of publishing date, special words, and the results 
of sentence alignment, we have a corpus of 
PDPs, and also a corpus of corresponding PSPs. 
To ensure the quality of output PSPs, we can 
continue to filter PSPs. For example, we can keep 
only the PSPs whose scores (xL1 and xL2) are 
higher than a threshold. 
3 Experiments 
3.1 Characteristics of Vietnamese 
The basic unit of the Vietnamese language is syl-
lable. In writing, syllables are separated by a 
white space. One word corresponds to one or 
more syllables (Nguyen, 2006). Table 4 presents 
an example of a Vietnamese sentence segmented 
into syllables and words.  
 
Vietnamese sentence: Th?nh ph? hy v?ng s? ??n nh?n 
kho?ng 3 tri?u kh?ch du l?ch n??c ngo?i trong n?m nay 
Segmentation in syllables: Th?nh | ph? | hy | v?ng | s? | 
??n | nh?n | kho?ng | 3 | tri?u | kh?ch | du | l?ch | n??c | 
ngo?i | trong | n?m | nay 
Segmentation in words: Th?nh_ph? | hy_v?ng | s? | 
??n_nh?n | kho?ng | 3 | tri?u | kh?ch_du_l?ch | 
n??c_ngo?i | trong | n?m | nay 
Corresponding English sentence: The city is expected to 
receive 3 million foreign tourists this year 
Table 4. An example of a Vietnamese sentence 
segmented into syllables and words. 
 
In Vietnamese, words do not change their 
form. Instead of conjugation for verb, noun or 
adjective, Vietnamese language uses additional 
words, such as ?nh?ng?, ?c?c? to express the plu-
D1 
Extract  
special words 
w1?wn  
 find w1 in S2?   doc1, doc3, doc5 
 find w2 in S2?   doc3, doc4, doc5 
 find w3 in S2?   doc3, doc5 
 ?                     ? 
doc1: 1 time 
doc3: 3 times 
doc4: 1 time 
doc5: 3 times 
 Count 
Choose 
the max 
S2?? 
{doc3, 
doc5} 
168
ral; ????, ?s?? to express the past tense and the 
future. The syntactic functions are also deter-
mined by the order of words in the sentence 
(Nguyen, 2006).  
3.2 Data collecting  
In order to build a Vietnamese-French parallel 
text corpus, we applied our proposed methodol-
ogy to mine a comparable text corpus from a 
Vietnamese daily news website, the Vietnam 
News Agency1 (VNA). This website contains 
news articles written in four languages (Vietnam-
ese, English, French, and Spanish) and divided in 
9 categories including ?Politics - Diplomacy?, 
?Society - Education?, ?Business - Finance?, 
?Culture - Sports?, ?Science - Technology?, 
?Health?, ?Environment?, ?Asian corner? and 
?World?. However, not all of the Vietnamese 
articles have been translated into the other three 
languages. The distribution of the amount of data 
in four languages is shown in figure 3. 
 
 
Figure 3. Distribution of the amount of data for 
each language on VNA website. 
 
Each document (i.e., article) can be obtained 
via a permanent URL link from VNA. To date, 
we have obtained about 121,000 documents in 
four languages, which are gathered from 12 April 
2006 to 14 August 2008; each document con-
tains, on average, 10 sentences, with around 30 
words per sentence. 
3.3 Data pre-processing 
We splitted the collected data into 2 sets. The 
development set, designated SDEV, contained 
1000 documents, was used to tune the mining 
system parameters. The rest of data, designated 
STRAIN, was used as a training set, where the esti-
mated parameters were applied to build the entire 
corpus. We applied the following pre-process to 
each set SDEV and STRAIN: 
1. Extract contents from documents. 
                                                          
1
 http://www.vnagency.com.vn/ 
2. Classify documents by language (using 
TextCat2, an n-gram based language identi-
fication). 
3. Process and clean both Vietnamese and 
French documents by using the CLIPS-Text-
Tk toolkit (LE et al, 2003): convert html to 
text file, convert character code, segment 
sentence, segment word. The resulting clean 
corpora are S1 (for French) and S2 (for 
Vietnamese). 
3.4  Parameters estimation 
Our proposed document alignment method was 
applied to the sets S1 and S2 extracted from the 
set SDEV. To filter by publishing date, we as-
sumed that n=2.  
The second filter was implemented on the set 
S1 and the new set S2* which was created by re-
moving diacritical marks from the set S2 (in the 
case of Vietnamese).  
The sentence alignment process was imple-
mented by using data from sets S1, S2 and the 
Champollion toolkit. We adapted Champollion to 
Vietnamese-French by changing some parame-
ters: the ratio of French word to Vietnamese 
translation word is set to 1.2, penalty for align-
ment type 1-1 is set to 1, for type 0-1 to 0.8, for 
type 2-1, 1-2 and 2-2 to 0.75, and we did not use 
the other types (see more in (Ma, 2006)). After 
using two filters, the result data is shown in Table 
5. The true PDPs were manually extracted. 
 
SDEV - Number of documents: 1000 
- Number of French documents: 173 
- Number of Vietnamese documents: 348 
- Number of true PDPs: 129 
S2?? - Number of found PDPs: 379 
- Number of hits PDPs: 129 
- Precision = 34.04% , Recall = 100% 
Table 5. Result data after using two filters. 
  
The third filter was applied in which ? was set 
to (0.4, 0.5, 0.6, 0.7) and ? was set to (0.1, 0.15, 
0.2, 0.25, 0.3, 0.35, 0.4). The precision and recall 
were calculated according to our true PDPs and 
the F-measure (F1 score) was estimated. 
F-measure 
    ?   
?  0.1 0.15 0.2 0.25 0.3 0.35 0.4 
0.4 0.69 0.71 0.71 0.60 0.48 0.36 0.21 
0.5 0.76 0.79 0.77 0.65 0.52 0.39 0.23 
0.6 0.77 0.83 0.82 0.70 0.56 0.41 0.26 
0.7 0.75 0.84 0.83 0.73 0.59 0.44 0.27 
Table 6. Filter result with different values of ? 
and ? on the SDEV. 
                                                          
2
 http://www.let.rug.nl/~vannoord/TextCat/ 
169
From the results mentioned in Table 6, we 
chose ?=0.7 and ?=0.15. 
3.5 Mining the entire corpus 
We applied the same methodology with the pa-
rameters estimated in section 3.4 to the set 
STRAIN. The obtained corpus is presented in Table 
7.  
 
STRAIN - Number of documents: 120,218 
- Number of French documents: 20,884 
- Number of Vietnamese documents: 
54,406 
Entire 
corpus 
- Number of PDPs: 12,108 
- Number of PSPs: 50,322 
Table 7. The obtained corpus from STRAIN. 
4 Application: a Vietnamese - French 
statistical machine translation system  
With the obtained parallel corpus, we attempted 
to rapidly build a SMT system for Vietnamese-
French. The system was built using the Moses 
toolkit1. The Moses toolkit contains all of the 
components needed to train both the translation 
model and the language model. It also contains 
tools for tuning these models using minimum 
error rate training and for evaluating the transla-
tion result using the BLEU score (Koehn et al, 
2007). 
4.1 Preparing data 
From the entire corpus, we chose 50 PDPs (351 
PSPs) for developing (Dev), 50 PDPs (384 PSPs) 
for testing (Tst), with the rest PDPs (49,587 
PSPs) reserved for training (Trn).  
Concerning the developing and testing PSPs, 
we manually verified and eliminated low quality 
PSPs, which produced 198 good quality PSPs for 
developing and 210 good quality PSPs for test-
ing. The data used to create the language model 
were extracted from 49,587 PSPs of the training 
set. 
4.2 Baseline system  
We built translation systems in two translation 
directions: French to Vietnamese (FV) and 
Vietnamese to French (VF). The Vietnamese 
data were segmented into either words or sylla-
bles. So we first have four translation systems. 
We removed sentences longer than 100 
words/syllables from the training and develop-
                                                          
1
 http://www.statmt.org/moses/ 
ment sets according to the Moses condition (so 
the number of PSPs used in the training set dif-
fers slightly between systems). All words found 
are implicitly added to the vocabulary. 
 
System Direction Vietnamese is 
segmented into Nbr of PSPs 
S1FV FV 
S1VF VF 
Syllable 
Training: 47,081 
Developing: 198 
Testing:        210 
S2FV FV 
S2VF VF 
Word 
Training: 48,864 
Developing: 198 
Testing:        210 
 
System Set - 
Language 
Nbr. of vocab  
(K) 
Nbr. of running 
words/syllables 
(K) 
Fr 38.6 1783.6 Trn Vn 21.9 2190.2 
Fr 1.8 6.3 Dev Vn 1.2 6.9 
Fr 1.9 6.4 
S1FV 
S1VF 
 
Tst Vn 1.3 7.1 
Fr 39.7 1893 Trn Vn 33.4 1629 
Fr 1.8 6.3 Dev Vn 1.5 4.8 
Fr 1.9 6.3 
S2FV 
S2VF 
 
Tst Vn 1.6 4.9 
Table 8. Our four translation systems. 
 
We obtained the performance results for those 
systems in Table 9. In the case of the systems 
where Vietnamese was segmented into words, 
the Vietnamese sentences were changed back to 
syllable representation before calculating the 
BLEU scores, so that all the BLEU scores evalu-
ated can be compared to each other. 
 
 S1FV S1VF S2FV S2VF 
BLEU  0.40 0.31 0.40 0.30 
Table 9. Evaluation of SMTs on the Tst set. 
 
The BLEU scores for French to Vietnamese 
translation direction are around 0.40 and the 
BLEU scores for Vietnamese to French transla-
tion direction are around 0.31, which is encour-
aging as a first result. Moreover, only one 
reference was used to estimate BLEU scores in 
our experiments. It is also interesting to note that 
segmenting Vietnamese sentences into words or 
syllables does not significantly change the per-
formance for both translation directions. An ex-
ample of translation from four systems is 
presented in Table 10.  
 
 
 
170
Given a pair of parallel sentences 
FR: selon le d?partement de gestion des travailleurs 
? l' ?tranger le qatar est un march? prometteur et 
n?cessite une grande quantit? de travailleurs ?tran-
gers 
VNsyl : theo c?c qu?n l? lao ??ng ngo?i n??c cata 
l? th? tr??ng ??y ti?m n?ng v? c? nhu c?u l?n lao 
??ng n??c ngo?i 
VNword : theo c?c qu?n_l? lao_??ng ngo?i n??c 
cata l? th?_tr??ng ??y ti?m_n?ng v? c? nhu_c?u l?n 
lao_??ng n??c_ngo?i 
S1FV Input: FR              Reference: VNsyl 
Output: theo c?c qu?n l? lao ??ng ? n??c 
ngo?i ph?a cata l? m?t th? tr??ng ??y ti?m 
n?ng v? c?n m?t l??ng l?n lao ??ng n??c 
ngo?i 
S2FR Input: FR              Reference: VNword 
Output: theo th?ng_k? c?a c?c qu?n_l? 
lao_??ng ngo?i n??c cata l? m?t 
th?_tr??ng ??y ti?m_n?ng v? c?n c? s? l?n 
l??ng lao_??ng n??c_ngo?i 
S1VF Input: VNsyl         Reference: FR 
Output: selon le d?partement de gestion 
des travailleurs ?trangers cata ?tait un mar-
ch? plein de potentialit?s et aux besoins 
importants travailleurs ?trangers 
S2VF Input: VNword      Reference: FR 
Output : selon le d?partement de gestion 
des travailleurs ?trangers cata march? plein 
de potentialit?s et la grande travailleurs 
?trangers 
Table 10 : Example of translation from systems. 
4.3 Combining word- and syllable-based 
systems 
We performed another experiment on combining 
syllable and word units on the Vietnamese side. 
We carried out the experiment on the Vietnamese 
to French translation direction only. In fact, the 
Moses toolkit supports the combination of 
phrase-tables. The phrase-tables of the system 
S1VF (Tsyl) and system S2VF (Tword) were used. 
Another phrase-table (Tword*) was created from 
the Tword, in which all words in the phrase table 
were changed back into syllable representation 
(in this latter case, the word segmentation infor-
mation was used during the alignment process 
and the phrase table construction, while the unit 
kept at the end remains the syllable). The combi-
nations of these three phrase-tables were also 
created (by simple concatenation of the phrase 
tables). The Vietnamese input for this experiment 
was either in word or in syllable representation. 
As usual, the developing set was used for tuning 
the log-linear weights and the testing set was 
used to estimate the BLEU score. The obtained 
results are presented in Table 11. Some perform-
ances are marked as X since those combinations 
of input and phrase table do not make sense (for 
instance the combination of input in words and 
syllable-based phrase table). 
 
Input in syllable Input in word Phrase-tables 
used Dev Tst Dev Tst 
Tsyl 0.35 0.31 X X 
Tword X X 0.35 0.30 
Tword* 0.37 0.31 X X 
Tsyl + Tword 0.35 0.31 0.36 0.30 
Tsyl + Tword* 0.38 0.32 X X 
Tword + Tword* 0.37 0.30 0.36 0.30 
Table 11: The BLEU scores obtained from com-
bination of phrase-tables on Dev set and Tst set 
(Vietnamese to French machine translation). 
 
These results show that the performance can 
be improved by combining information from 
word and syllable representations of Vietnamese. 
(BLEU improvement from 0.35 to 0.38 on the 
Dev set and from 0.31 to 0.32 on the Tst set). In 
the future, we will analyze more the combination 
of syllable and word units for Vietnamese MT 
and we will investigate the use of confusion net-
works as an MT input, which have the advantage 
to keep both segmentations (word, syllable) into 
a same structure. 
4.4 Comparing with Google Translate1 
Google Translate system has recently supported 
Vietnamese. In most cases, it uses English as an 
intermediary language. For the first comparative 
evaluation, some simple tests were carried out. 
Two sets of data were used: in domain data set 
(the Tst set in section 4.2) and out of domain data 
set. The latter was obtained from a Vietnamese-
French bilingual website2 which is not a news 
website. After pre-processing and aligning manu-
ally, we obtained 100 PSPs in the out of domain 
data set. In these tests, the Vietnamese data were 
segmented into syllables. Both data sets were 
inputted to our translation systems (S1FV, S1VF) 
and the Google Translate system. The outputs of 
Google Translate system were post-processed 
(lowercased) and then the BLEU scores were 
estimated. Table 12 presents the results of these 
tests. While our system is logically better for in 
domain data set, it is also slightly better than 
Google for out of domain data set.  
 
                                                          
1
 http://translate.google.com 
2
 http://www.ambafrance-vn.org 
171
BLEU score  Direction Our system Google 
FV 0.40  0.25 In domain 
(210 PSPs) VF 0.31  0.16 
FV 0.25 0.24 Out of domain 
(100 PSPs) VF 0.20 0.16 
Table 12: Comparing with Google Translate. 
5 Conclusions and perspectives 
In this paper, we have presented our work on 
mining a comparable Vietnamese-French corpus 
and our first attempts at Vietnamese-French 
SMT. The paper has presented our document 
alignment method, which is based on publication 
date, special words and sentence alignment re-
sult. The proposed method is applied to Vietnam-
ese and French news data collected from VNA. 
For Vietnamese and French data, we obtained 
around 12,100 parallel document pairs and 
50,300 parallel sentence pairs. This is our first 
Vietnamese-French parallel bilingual corpus. We 
have built SMT systems using Moses. The BLEU 
scores for French to Vietnamese translation sys-
tems and Vietnamese to French translation sys-
tems were 0.40 and 0.31 in turn. Moreover, 
combining information from word and syllable 
representations of Vietnamese can be useful to 
improve the performance of Vietnamese MT sys-
tem.  
In the future, we will attempt to increase the 
corpus size (by using unsupervised SMT for in-
stance) and investigate further the use of different 
Vietnamese lexical units (syllable, word) in a MT 
system.  
References  
Brown, Peter F., Jennifer C. Lai and Robert L. Mer-
cer. 1991. Aligning sentences in parallel corpora. 
Proceedings of 47th Annual Meeting of the Asso-
ciation for Computational Linguistics. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics. 
Vol. 19, no. 2. 
Doan, Nguyen Hai. 2001. Generation of Vietnamese 
for French-Vietnamese and English-Vietnamese 
Machine Translation. ACL, Proceedings of the 8th 
European workshop on Natural Language Genera-
tion. 
Gale, William A. and Kenneth W. Church. 1993. A 
program for aligning sentences in bilingual cor-
pora. Proceedings of the 29th annual meeting on 
Association for Computational Linguistics. 
Ho, Tu Bao. 2005. Current Status of Machine Trans-
lation Research in Vietnam Towards Asian wide 
multi language machine translation project. Viet-
namese Language and Speech Processing Work-
shop. 
Hutchins, W.John. 2001. Machine translation over 
fifty years. Histoire, epistemologie, langage: HEL, 
ISSN 0750-8069, Vol. 23, N? 1, 2001 , pages. 7-32.  
Kay, Martin and Martin Roscheisen. 1993. Text - 
translation alignment. Association for Computa-
tional Linguistics. 
Kilgarriff, Adam and Gregory Grefenstette. 2003. 
Introduction to the Special Issue on the Web as 
Corpus. Computational Linguistics, volume 29. 
Koehn, Philipp, Franz Josef Och and Daniel Marcu. 
2003. Statistical phrase-based translation. Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics on Human 
Language Technology - Volume 1. 
Koehn, Philipp. 2005. Europarl: A Parallel Corpus 
for Statistical Machine Translation. Machine 
Translation Summit.  
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Richard Zens, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, Wade Shen and 
Christine Moran. 2007. Moses: Open Source Tool-
kit for Statistical Machine Translation. Proceedings 
of the ACL.  
Kraaij, Wessel, Jian-Yun Nie and Michel Simard. 
2003. Embedding web-based statistical translation 
models in cross-language information retrieval. 
Computational Linguistics,  Volume 29 ,  Issue 3. 
LE, Viet Bac, Brigitte Bigi, Laurent Besacier and Eric 
Castelli. 2003. Using the Web for fast language 
model construction in minority languages. Eu-
rospeech'03. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel 
Text Sentence Aligner. LREC: Fifth International 
Conference on Language Resources and Evalua-
tion.  
Munteanu, Dragos Stefan and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
non-parallel corpora . 44th annual meeting of the 
Association for Computational Linguistics 
Nguyen, Thi Minh Huyen. 2006. Outils et ressources 
linguistiques pour l'alignement de textes multilin-
gues fran?ais-vietnamiens. Th?se pr?sent?e pour 
l?obtention du titre de Docteur de l?Universit? Hen-
ri Poincar?, Nancy 1 en Informatique.  
Patry, Alexandre and Philippe Langlais. 2005. Para-
docs: un syst?me d?identification automatique de 
documents parall?les. 12e Conference sur le Trai-
tement Automatique des Langues Naturelles. 
Dourdan, France.  
Resnik, Philip and Noah A. Smith. 2003. The Web as 
a Parallel Corpus. Computational Linguistics.  
Yang, Christopher C. and Kar Wing Li. 2002. Mining 
English/Chinese Parallel Documents from the 
World Wide Web. Proceedings of the 11th Interna-
tional World Wide Web Conference, Honolulu, 
USA.  
172
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 186?191,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Multimodal Annotation of Conversational Data
P. Blache1, R. Bertrand1, B. Bigi1, E. Bruno3, E. Cela6, R. Espesser1, G. Ferr?4, M. Guardiola1, D. Hirst1,
E.-P. Magro6, J.-C. Martin2, C. Meunier1, M.-A. Morel6, E. Murisasco3, I Nesterenko1, P. Nocera5,
B. Pallaud1, L. Pr?vot1, B. Priego-Valverde1, J. Seinturier3, N. Tan2, M. Tellier1, S. Rauzy1
(1) LPL-CNRS-Universit? de Provence (2) LIMSI-CNRS-Universit? Paris Sud
(3) LSIS-CNRS-Universit? de Toulon (4) LLING-Universit? de Nantes
(5) LIA-Universit? d?Avignon (6) RFC-Universit? Paris 3
blache@lpl-aix.fr
Abstract
We propose in this paper a broad-coverage
approach for multimodal annotation of
conversational data. Large annotation pro-
jects addressing the question of multimo-
dal annotation bring together many dif-
ferent kinds of information from different
domains, with different levels of granula-
rity. We present in this paper the first re-
sults of the OTIM project aiming at deve-
loping conventions and tools for multimo-
dal annotation.
1 Introduction
We present in this paper the first results of the
OTIM1 project aiming at developing conventions
and tools for multimodal annotation. We show
here how such an approach can be applied in the
annotation of a large conversational speech cor-
pus.
Before entering into more details, let us men-
tion that our data, tools and conventions are des-
cribed and freely downlodable from our website
(http ://www.lpl-aix.fr/ otim/).
The annotation process relies on several tools
and conventions, most of them elaborated within
the framework of the project. In particular, we pro-
pose a generic transcription convention, called En-
riched Orthographic Trancription, making it pos-
sible to annotate all specific pronunciation and
speech event, facilitating signal alignment. Dif-
ferent tools have been used in order to prepare
or directly annotate the transcription : grapheme-
phoneme converter, signal alignment, syllabifica-
tion, prosodic analysis, morpho-syntactic analysis,
chunking, etc. Our ambition is to propose a large
corpus, providing rich annotations in all the dif-
1OTIM stands for Outils pour le Traitement de l?Informa-
tion Multimodale (Tools for Multimodal Annotation). This
project in funded by the French ANR agency.
ferent linguistic domains, from prosody to gesture.
We describe in the following our first results.
2 Annotations
We present in this section some of the annota-
tions of a large conversational corpus, called CID
(Corpus of Interactional Data, see (Bertrand08)),
consisting in 8 dialogues, with audio and video si-
gnal, each lasting 1 hour.
Transcription : The transcription process is
done following specific conventions derived from
that of the GARS (Blanche-Benveniste87). The
result is what we call an enriched orthographic
construction, from which two derived transcrip-
tions are generated automatically : the standard or-
thographic transcription (the list of orthographic
tokens) and a specific transcription from which
the phonetic tokens are obtained to be used by the
grapheme-phoneme converter.
From the phoneme sequence and the audio si-
gnal, the aligner outputs for each phoneme its
time localization. This aligner (Brun04) is HMM-
based, it uses a set of 10 macro-classes of vowel
(7 oral and 3 nasal), 2 semi-vowels and 15 conso-
nants. Finally, from the time aligned phoneme se-
quence plus the EOT, the orthographic tokens is
time-aligned.
Syllables : The corpus was automatically seg-
mented in syllables. Sub-syllabic constituents (on-
set, nucleus and coda) are then identified as well
as the syllable structure (V, CV, CCV, etc.). Sylla-
bic position is specified in the case of polysyllabic
words.
Prosodic phrasing : Prosodic phrasing refers
to the structuring of speech material in terms of
boundaries and groupings. Our annotation scheme
supposes the distinction between two levels of
phrasing : the level of accentual phrases (AP, (Jun,
2002)) and the higher level of intonational phrases
186
(IP). Mean annotation time for IPs and APs was
30 minutes per minute.
Prominence : The prominence status of a syl-
lable distinguishes between accentuability (the
possibility for syllable to be prominent) and pro-
minence (at the perception level). In French the
first and last full syllables (not containing a
schwa) of a polysyllabic word can be prominent,
though this actual realization depends on spea-
kers choices. Accentuability annotation is auto-
matic while prominence annotation is manual and
perceptually based.
Tonal layer : Given a lack of consensus on the
inventory of tonal accents in French, we choose to
integrate in our annotation scheme three types of
tonal events : a/ underlying tones (for an eventual
FrenchToBI annotation) ; b/ surface tones (anno-
tated in terms of MOMel-Intsint protocol Hirst et
al 2000) ; c/ melodic contours (perceptually anno-
tated pitch movements in terms of their form and
function). The interest to have both manual and
automatic INTSINT annotations is that it allows
the study of their links.
Hand gestures : The formal model we use for
the annotation of hand gestures is adapted from
the specification files created by Kipp (2004) and
from the MUMIN coding scheme (Allwood et al,
2005). Among the main gesture types, we anno-
tate iconics, metaphoric, deictics, beats, emblems,
butterworths or adaptors.
We used the Anvil tool (Kipp, 2004) for the ma-
nual annotations. We created a specification files
taking into account the different information types
and the addition of new values adapted to the
CID corpus description (e.g. we added a separate
track Symmetry). For each hand, the scheme has 10
tracks. We allowed the possibility of a gesture per-
taining to several semiotic types using a boolean
notation. A gesture phrase (i.e. the whole gesture)
can be decomposed into several gesture phases i.e.
the different parts of a gesture such as the prepara-
tion, the stroke (the climax of the gesture), the hold
and the retraction (when the hands return to their
rest position) (McNeill, 1992). The scheme also
enables to annotate gesture lemmas (Kipp, 2004),
the shape and orientation of the hand during the
stroke, the gesture space, and contact. We added
the three tracks to code the hand trajectory, ges-
ture velocity and gesture amplitude.
Discourse and Interaction : Our discourse an-
notation scheme relies on multidimensional fra-
meworks such as DIT++ (Bunt, 2009) and is com-
patible with the guidelines defined by the Semantic
Annotation Framework (Dialogue Act) working
group of ISO TC37/4.
Discourse units include information about their
producer, have a form (clause, fragment, dis-
fluency, non-verbal), a content and a communi-
cative function. The same span of raw data may
be covered by several discourse units playing dif-
ferent communicative functions. Two discourse
units may even have exactly the same temporal ex-
tension, due to the multifonctionality that cannot
be avoided (Bunt, 2009).
Compared to standard dialogue act annotation
frameworks, three main additions are proposed :
rhetorical function, reported speech and humor.
Our rhetorical layer is an adaptation of an exis-
ting schema developed for monologic written data
in the context of the ANNODIS project.
Disfluencies : Disfluencies are organized
around an interruption point, which can occur al-
most anywhere in the production. Disfluencies can
be prosodic (lenghtenings, silent and filled pauses,
etc.), or lexicalized. In this case, they appear as a
word or a phrase truncation, that can be comple-
ted. We distinguish three parts in a disfluency (see
(Shriberg, 1994), (Blanche-Benveniste87)) :
? Reparandum : what precedes the interruption
point. This part is mandatory in all disfluen-
cies. We indicate there the nature of the inter-
rupted unit (word or phrase), and the type of
the truncated word (lexical or grammatical) ;
? Break interval. It is optional, some disfluen-
cies do not bear any specific event there.
? Reparans : the part following the break, repai-
ring the reparandum. We indicate there type
of the repair (no restart, word restart, determi-
ner restart, phrase restart, etc.), and its func-
tion (continuation, repair without change, re-
pair with change, etc.).
3 Quantitative information
We give in this section some indication about
the state of development of the CID annotation.
Hand gestures : 75 minutes involving 6 spea-
kers have been annotated, yielding a total number
of 1477 gestures. The onset and offset of gestures
correspond to the video frames, starting from and
187
going back to a rest position.
Face and gaze : At the present time, head move-
ments, gaze directions and facial expressions have
been coded in 15 minutes of speech yielding a to-
tal number of 1144 movements, directions and ex-
pressions, to the exclusion of gesture phases. The
onset and offset of each tag are determined in the
way as for hand gestures.
Body Posture : Our annotation scheme consi-
ders, on top of chest movements at trunk level,
attributes relevant to sitting positions (due to the
specificity of our corpus). It is based on the Pos-
ture Scoring System (Bull, 1987) and the Annota-
tion Scheme for Conversational Gestures (Kipp et
al., 2007). Our scheme covers four body parts :
arms, shoulders, trunk and legs. Seven dimensions
at arm level and six dimensions at leg level, as well
as their related reference points we take in fixing
the spatial location, are encoded.
Moreover, we added two dimensions to describe
respectively the arm posture in the sagittal plane
and the palm orientation of the forearm and the
hand. Finally, we added three dimensions for leg
posture : height, orientation and the way in which
the legs are crossed in sitting position.
We annotated postures on 15 minutes of the cor-
pus involving one pair of speakers, leading to 855
tags with respect to 15 different spatial location
dimensions of arms, shoulder, trunk and legs.
Annotation Time (min.) Units
Transcript 480 -
Hands 75 1477
Face 15 634
Gaze 15 510
Posture 15 855
R. Speech 180
Com. Function 6 229
Disfluencies At the moment, this annotation is
fully manual (we just developed a tool helping the
process in identifying disfluencies, but it has not
yet been evaluated). Annotating this phenomenon
requires 15mns for 1 minute of the corpus. The
following table illustrates the fact that disfluen-
cies are speaker-dependent in terms of quantity
and type. These figures also shows that disfluen-
cies affect lexicalized words as well as grammati-
cal ones.
Speaker_1 Speaker_1
Total number of words 1,434 1,304
Disfluent grammatical words 17 54
Disfluent lexicalized words 18 92
Truncated words 7 12
Truncated phrases 26 134
Transcription and phonemes The following
table recaps the main figures about the different
specific phenomena annotated in the EOT. To the
best of our knowledge, these data are the first of
this type obtained on a large corpus. This informa-
tion is still to be analyzed.
Phenomenon Number
Elision 11,058
Word truncation 1,732
Standard liaison missing 160
Unusual liaison 49
Non-standard phonetic realization 2,812
Laugh seq. 2,111
Laughing speech seq. 367
Single laugh IPU 844
Overlaps > 150 ms 4,150
Syntax We used the stochastic parser developed
at the LPL (Blache&Rauzy, 2008) to automaticaly
generate morppho-syntactic and syntactic annota-
tions. The parser has been adapted it in order to ac-
count for the specificities of speech analysis. First,
the system implements a segmentation technique,
identifying large syntactic units that can be consi-
dered as the equivalent of sentences in written
texts. This technique distinguishes between strong
and weak or soft punctuation marks. A second mo-
dification concerns the lexical frequencies used by
the parser model in order to capture phenomena
proper to conversational data.
The categories and chunks counts for the whole
corpus are summarized in the following figure :
Category Count Group Count
adverb 15123 AP 3634
adjective 4585 NP 13107
auxiliary 3057 PP 7041
determiner 9427 AdvP 15040
conjunction 9390 VPn 22925
interjection 5068 VP 1323
preposition 8693 Total 63070
pronoun 25199
noun 13419 Soft Pct 9689
verb 20436 Strong Pct 14459
Total 114397 Total 24148
4 Evaluations
Prosodic annotation : Prosodic annotation of
1 dialogue has been done by 2 experts. The
annotators worked separately using Praat. Inter-
transcriber agreement studies were done for the
annotation of higher prosodic units. First anno-
tator marked 3,159 and second annotator 2,855
188
Intonational Phrases. Mean percentage of inter-
transcriber agreement was 91.4% and mean
kappa-statistics 0.79, which stands for a quite sub-
stantial agreement.
Gesture : We performed a measure of inter-
reliability for three independent coders for Gesture
Space. The measure is based on Cohen?s correc-
ted kappa coefficient for the validation of coding
schemes (Carletta96).
Three coders have annotated three minutes for
GestureSpace including GestureRegion and Ges-
tureCoordinates. The kappa values indicated that
the agreement is high for GestureRegion of right
hand (kappa = 0.649) and left hand (kappa =
0.674). However it is low for GestureCoordinates
of right hand (k= 0.257) and left hand (k= 0.592).
Such low agreement of GestureCoordinates might
be due to several factors. First, the number of ca-
tegorical values is important.
Second, three minutes might be limited in terms
of data to run a kappa measure. Third, GestureRe-
gion affects GestureCoordinates : if the coders di-
sagree about GestureRegion, they are likely to also
annotate GestureCoordinates in a different way.
For instance, it was decided that no coordinate
would be selected for a gesture in the center-center
region, whereas there is a coordinate value for ges-
tures occurring in other parts of the GestureRe-
gion. This means that whenever coders disagree
between the center-center or center region, the an-
notation of the coordinates cannot be congruent.
5 Information representation
5.1 XML encoding
Our approach consists in first precisely define
the organization of annotations in terms of typed-
feature structures. We obtain an abstract descrip-
tion from which we automatically generate a for-
mal schema in XML. All the annotations are then
encoded following this schema.
Our XML schema, besides a basic encoding of
data following AIF, encode all information concer-
ning the organization as well as the constraints on
the structures. In the same way as TFS are used
as a tree description language in theories such as
HPSG, the XML schema generated from our TFS
representation also plays the same role with res-
pect to the XML annotation data file. On the one
hand, basic data are encoded with AIF, on the
other hand, the XML schema encode all higher
level information. Both components (basic data +
structural constraints) guarantee against informa-
tion loss that otherwise occurs when translating
from one coding format to another (for example
from Anvil to Praat).
5.2 Querying
To ease the multimodal exploitation of the data,
our objective is to provide a set of operators dedi-
cated to concurrent querying on hierarchical an-
notation. Concurrent querying consists in que-
rying annotations belonging to two or more mo-
dalities or even in querying the relationships bet-
ween modalities. For instance, we want to be able
to express queries over gestures and intonation
contours (what kind of intonational contour does
the speaker use when he looks at the listener ?).
We also want to be able to query temporal relation-
ships (in terms of anticipation, synchronization or
delay) between both gesture strokes and lexical af-
filiates.
Our proposal is to define these operators as an
extension of XQuery. From the XML encoding
and the temporal alignment of annotated data, it
will possible to express queries to find patterns and
to navigate in the structure. We also want to en-
able a user to check predicates on parts of the cor-
pus using classical criteria on values, annotations
and existing relationships (temporal or structural
ones corresponding to inclusions or overlaps bet-
ween annotations). First, we shall rely on one of
our previous proposal called MSXD (MultiStruc-
tured XML Document). It is a XML-compatible
model designed to describe and query concurrent
hierarchical structures defined over the same tex-
tual data which supports Allen?s relations.
6 Conclusion
Multimodal annotation is often reduced to
the encoding of gesture, eventually accompa-
nied with another level of linguistic information
(e.g. morpho-syntax). We reported in this paper a
broad-coverage approach, aiming at encoding all
the linguistic domains into a unique framework.
We developed for this a set of conventions and
tools making it possible to bring together and align
all these different pieces of information. The result
is the CID (Corpus of Interactional Data), the first
large corpus of conversational data bearing rich
annotations on all the linguistic domains.
189
References
Allen J. (1999) Time and time again : The many way to re-
present time. International Journal of Intelligent Systems,
6(4)
Allwood, J., Cerrato, L., Dybkjaer, L., Jokinen, K., Navar-
retta, C., Paggio, P. (2005) The MUMIN Multimodal Co-
ding Scheme, NorFA yearbook 2005.
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P.
F. Patel-Schneider (2003) The Description Logic Hand-
book : Theory, Implementation, Applications. Cambridge
University Press.
Bertrand, R., Blache, P., Espesser, R., Ferr?, G., Meunier, C.,
Priego-Valverde, B., Rauzy, S. (2008) ?Le CID - Corpus
of Interactional Data - Annotation et Exploitation Multi-
modale de Parole Conversationnelle?, in revue Traitement
Automatique des Langues, 49 :3.
Bigi, C. Meunier, I. Nesterenko, R. Bertrand 2010. ?Syllable
Boundaries Automatic Detection in Spontaneous Speech?,
in proceedings of LREC 2010.
Blache P. and Rauzy S. 2008. ?Influence de la qualit? de
l??tiquetage sur le chunking : une corr?lation d?pendant de
la taille des chunks?. in proceedings of TALN 2008 (Avi-
gnon, France), pp. 290-299.
Blache P., R. Bertrand, and G. Ferr? 2009. ?Creating and
Exploiting Multimodal Annotated Corpora : The ToMA
Project?. In Multimodal Corpora : From Models of Natu-
ral Interaction to Systems and Applications, Springer.
Blanche-Benveniste C. & C. Jeanjean (1987) Le fran?ais
parl?. Transcription et ?dition, Didier Erudition.
Blanche-Benveniste C. 1987. ?Syntaxe, choix du lexique et
lieux de bafouillage?, in DRLAV 36-37
Browman C. P. and L. Goldstein. 1989. ?Articulatory ges-
tures as phonological units?. In Phonology 6, 201-252
Brun A., Cerisara C., Fohr D., Illina I., Langlois D., Mella O.
& Smaili K. (2004- ?Ants : Le syst?l?me de transcription
automatique du Loria?, Actes des XXV Journ?es d?Etudes
sur la Parole, F?s.
E. Bruno, E. Murisasco (2006) Describing and Querying hie-
rarchical structures defined over the same textual data, in
Proceedings of the ACM Symposium on Document Engi-
neering (DocEng 2006).
Bull, P. (1987) Posture and Gesture, Pergamon Press.
Bunt H. 2009. ?Multifunctionality and multidimensional
dialogue semantics.? In Proceedings of DiaHolmia?09,
SEMDIAL.
B?rki A., C. Gendrot, G. Gravier & al.(2008) ?Alignement
automatique et analyse phon?tique : comparaison de dif-
f?rents syst?mes pour l?analyse du schwa?, in revue TAL
,49 :3
Carletta, J. (1996) ?Assessing agreement on classification
tasks : The kappa statistic?, in Computational Linguistics
22.
Corlett, E. N., Wilson,John R. Manenica. I. (1986) ?Influence
Parameters and Assessment Methods for Evaluating Body
Postures?, in Ergonomics of Working Postures : Models,
Methods and Cases , Proceedings of the First International
Occupational Ergonomics Symposium.
Di Cristo & Hirst D. (1996) ?Vers une typologie des unites in-
tonatives du fran?ais?, XXI?me JEP, 219-222, 1996, Avi-
gnon, France
Di Cristo A. & Di Cristo P. (2001) ?Syntaix, une approche
m?trique-autosegmentale de la prosodie?, in revue Traite-
ment Automatique des Langues, 42 :1.
Dipper S., M. Goetze and S. Skopeteas (eds.) 2007. Informa-
tion Structure in Cross-Linguistic Corpora : Annotation
Guidelines, Working Papers of the SFB 632, 7 :07
FGNet Second Foresight Report (2004) Face
and Gesture Recognition Working Group.
http ://www.mmk.ei.tum.de/ waf/fgnet-intern/3rd-
fgnet-foresight-workshop.pdf
Gendner V. et al 2003. ?PEAS, the first instantiation of a
comparative framework for evaluating parsers of French?.
in Research Notes of EACL 2003 (Budapest, Hungaria).
Hawkins S. and N. Nguyen 2003. ?Effects on word re-
cognition of syllable-onset cues to syllable-coda voicing?,
in Papers in Laboratory Phonology VI. Cambridge Univ.
Press.
Hirst, D., Di Cristo, A., Espesser, R. 2000. ?Levels of des-
cription and levels of representation in the analysis of in-
tonation?, in Prosody : Theory and Experiment, Kluwer.
Hirst, D.J. (2005) ?Form and function in the representation
of speech prosody?, in K.Hirose, D.J.Hirst & Y.Sagisaka
(eds) Quantitative prosody modeling for natural speech
description and generation (Speech Communication 46 :3-
4.
Hirst, D.J. (2007) ?A Praat plugin for Momel and INTSINT
with improved algorithms for modelling and coding into-
nation?, in Proceedings of the XVIth International Confe-
rence of Phonetic Sciences.
Hirst, D. (2007), Plugin Momel-Intsint. Inter-
net : http ://uk.groups.yahoo.com/group/praat-
users/files/Daniel_Hirst/plugin_momel-intsint.zip,
Boersma, Weenink, 2007.
Jun, S.-A., Fougeron, C. 2002. ?Realizations of accentual
phrase in French intonation?, in Probus 14.
Kendon, A. (1980) ?Gesticulation and Speech : Two Aspects
of the Porcess of Utterance?, in M.R. Key (ed.), The Re-
lationship of Verbal and Nonverbal Communication, The
Hague : Mouton.
Kita, S., Ozyurek, A. (2003) ?What does cross-linguistic va-
riation in semantic coordination of speech and gesture re-
veal ? Evidence for an interface representation of spatial
thinking and speaking?, in Journal of Memory and Lan-
guage, 48.
Kipp, M. (2004). Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation. Boca
Raton, Florida, Dissertation.com.
Kipp, M., Neff, M., Albrecht, I. (2007). An annotation
scheme for conversational gestures : how to economically
capture timing and form. Language Resources and Eva-
luation, 41(3).
Koiso H., Horiuchi Y., Ichikawa A. & Den Y.(1998) ?An ana-
lysis of turn-taking and backchannels based on prosodic
and syntactic features in Japanese map task dialogs?, in
Language and Speech, 41.
McNeill, D. (1992). Hand and Mind. What Gestures Re-
veal about Thought, Chicago : The University of Chicago
Press.
McNeill, D. (2005). Gesture and Thought, Chicago, London :
The University of Chicago Press.
Milborrow S., F. Nicolls. (2008). Locating Facial Features
with an Extended Active Shape Model. ECCV (4).
Nesterenko I. (2006) ?Corpus du parler russe spontan? : an-
notations et observations sur la distribution des fronti?res
prosodiques?, in revue TIPA, 25.
190
Paroubek P. et al 2006. ?Data Annotations and Measures in
EASY the Evaluation Campaign for Parsers in French?. in
proceedings of the 5th international Conference on Lan-
guage Resources and Evaluation 2006 (Genoa, Italy), pp.
314-320.
Pierrehumbert & Beckman (1988) Japanese Tone Structure.
Coll. Linguistic Inquiry Monographs, 15. Cambridge,
MA, USA : The MIT Press.
Platzer, W., Kahle W. (2004) Color Atlas and Textbook of
Human Anatomy, Thieme. Project MuDis. Technische
Universitat Munchen. http ://www9.cs.tum.edu/research
Scherer, K.R., Ekman, P. (1982) Handbook of methods in
nonverbal behavior research. Cambridge University Press.
Shriberg E. 1994. Preliminaries to a theory of speech dis-
fluencies. PhD Thesis, University of California, Berkeley
Wallhoff F., M. Ablassmeier, and G. Rigoll. (2006) ?Mul-
timodal Face Detection, Head Orientation and Eye Gaze
Tracking?, in proceedings of International Conference on
Multisensor Fusion and Integration (MFI).
White, T. D., Folkens, P. A. (1991) Human Osteology. San
Diego : Academic Press, Inc.
191
JEP-TALN-RECITAL 2012, Atelier DEGELS 2012: D?fi GEste Langue des Signes, pages 85?92,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
SPPAS : un outil ? user-friendly ? pour l?alignement texte/son
Brigitte Bigi
Laboratoire Parole et Langage, CNRS & Aix-Marseille Universit?,
5 avenue Pasteur, BP80975, 13604 Aix-en-Provence France
brigitte.bigilpl-aix.fr
R?SUM?
Cet article pr?sente SPPAS, le nouvel outil du LPL pour l?alignement texte/son. La seg-
mentation s?op?re en 4 ?tapes successives dans un processus enti?rement automatique ou
semi-automatique, ? partir d?un fichier audio et d?une transcription. Le r?sultat comprend la
segmentation en unit?s inter-pausales, en mots, en syllabes et en phon?mes. La version actuelle
propose un ensemble de ressources qui permettent le traitement du fran?ais, de l?anglais, de
l?italien et du chinois. L?ajout de nouvelles langues est facilit?e par la simplicit? de l?architecture
de l?outil et le respect des formats de fichiers les plus usuels. L?outil b?n?ficie en outre d?une
documentation en ligne et d?une interface graphique afin d?en faciliter l?accessibilit? aux
non-informaticiens. Enfin, SPPAS n?utilise et ne contient que des ressources et programmes sous
licence libre GPL.
ABSTRACT
SPPAS : a tool to perform text/speech alignement
This paper presents SPPAS, a new tool dedicated to phonetic alignments, from the LPL
laboratory. SPPAS produces automatically or semi-automatically annotations which include
utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its
transcription. SPPAS is currently implemented for French, English, Italian and Chinese. There
is a very simple procedure to add other languages in SPPAS : it is just needed to add related
resources in the appropriate directories. SPPAS can be used by a large community of users :
accessibility and portability are important aspects in its development. The tools and resources
will all be distributed with a GPL license.
MOTS-CL?S : segmentation, phon?tisation, alignement, syllabation.
KEYWORDS: segmentation, phonetization, alignement, syllabification.
1 Introduction
De nombreux d?veloppements de logiciels sont effectu?s dans les laboratoires de recherche
comme support ? la recherche ou aboutissement d?une recherche. Ces d?veloppements sont sou-
vent innovants et int?ressent rapidement d?autres entit?s que le laboratoire. Il se pose alors la
question des choix pour permettre et pour accompagner leur valorisation, pour augmenter leur
visibilit? et leur capacit? ? susciter des collaborations. La portabilit? et l?accessibilit? du logiciel
85
en sont des points cl?s. La portabilit?, car choisir une plate-forme pour un programme revient ?
en restreindre l?audience. L?accessibilit? aux contenus et aux fonctions du logiciel s?av?re ?gale-
ment essentielle pour sa diffusion large ? diff?rentes communaut?s d?utilisateurs, car un logiciel
est ? la fois un objet scientifique mais aussi potentiellement un objet de transfert de technologie.
De nombreuses bo?tes ? outils pour r?aliser diff?rents niveaux de segmentations de la parole et
l?apprentissage des mod?les sous-jacents sont mis ? disposition sur le web. Elles b?n?ficient par-
fois d?une large documentation, d?une communaut? d?utilisateurs, de tutoriaux et de forums ac-
tifs. Des ressources (dictionnaires, mod?les) sont ?galement disponibles pour quelques langues.
Pourtant, lorsqu?il s?agit d?effectuer des alignements texte/son, la plupart des phon?ticiens choi-
sissent de le faire manuellement m?me si plusieurs heures sont souvent n?cessaire pour n?aligner
qu?une seule minute de signal. Les raisons principalement ?voqu?es concernent le fait qu?au-
cun outil n?est ? la fois disponible librement, utilisable de fa?on simple et ergonomique, multi-
plateforme et, bien s?r, qui prend en charge la langue que veut traiter l?utilisateur. Ainsi, bien
qu?elles soient tr?s utilis?es par les informaticiens, des bo?tes ? outils telles que, par exemple,
HTK (Young, 1994), Sphinx (Carnegie Mellon University, 2011) ou Julius (Lee et al, 2001),
ne b?n?ficient toujours pas d?un d?veloppement qui permette une accessibilit? ? une commu-
naut? plus large d?utilisateurs, en particulier, ? des utilisateurs non-informaticiens. HTK (Hidden
Markov Toolkit), en effet, requiert un niveau de connaissances techniques tr?s important ? la
fois pour son installation et pour son utilisation. Par ailleurs, HTK n?cessite de s?enregistrer et il
est propos? sous une licence qui limite les termes de sa diffusion (? The Licensed Software either
in whole or in part can not be distributed or sub-licensed to any third party in any form. ?). En
outre, la derni?re version (3.4.1) date de 2005. Malgr? cela, HTK est largement utilis? et ses
formats de donn?es ont ?t? largement repris par d?autres outils. Contrairement ? HTK, Sphinx
et Julius sont diffus?s sous licence GPL. ? ce titre, ils peuvent ?tre re-distribu?s par des tiers,
et ils sont r?guli?rement mis ? jour. Par rapport ? Sphinx, Julius offre toutefois l?avantage de
pouvoir utiliser des mod?les et dictionnaires au format HTK et de s?installer tr?s facilement.
D?velopper un outil d?alignement automatique, s?appuyant uniquement sur des ressources li-
bres (outils et donn?es) et regroupant les crit?res n?cessaire ? son accessibilit? ? des non-
informaticiens n?est pas uniquement un d?fi technique. On suppose en effet que si tel ?tait le
cas, cet outil existerait depuis longtemps ! Quelques outils sont toutefois d?j? disponibles. P2FA
(Yuan et Liberman, 2008) est un programme python multi-plate-forme qui permet de simplifier
l?utilisation d?HTK pour l?alignement. De m?me, EasyAlign (Goldman, 2011) repose sur HTK,
pour l?alignement automatique. Il se pr?sente sous la forme d?un plugin pour le logiciel Praat
(Boersma et Weenink, 2009), tr?s utilis? pour l?annotation phon?tique. EasyAlign (Goldman,
2011) offre l?avantage d??tre simple ? utiliser et propose une segmentation semi-automatique
en Unit?s Inter-Pausales (IPUs), mots, syllabes et phon?mes pour 5 langues, mais il ne fonc-
tionne que sous Windows. Dans (Cangemi et al, 2010), les auteurs proposent les ressources
pour l?italien et un logiciel d?alignement (licence GPL), ?galement seulement pour Windows.
L?outil pr?sent? dans cet article s?appelle SPPAS, acronyme de ? SPeech Phonetization Align-
ment and Syllabification ?. L?article en pr?sente d?abord une vue d?ensemble puis d?crit les 4
modules principaux : la segmentation en unit?s inter-pausales, la phon?tisation, l?alignement et
la syllabation. Enfin, une ?valuation de la phon?tisation est propos?e.
86
2 SPPAS : vue d?ensemble
SPPAS peut ?tre utilis? de diverses fa?ons. La mani?re la plus simple d?utiliser SPPAS consiste
? utiliser le programme sppas.command sous un syst?me Unix, ou sppas.py sous Windows, qui
lance l?interface graphique (voir figure 1). La division de SPPAS en diff?rentes ?tapes (ou mo-
dules) permet une utilisation semi-automatique. Chacune des ?tapes de SPPAS peut ?tre lanc?e
puis le r?sultat corrig? manuellement avant de lancer l??tape suivante. Pour des utilisateurs
avertis, SPPAS peut aussi ?tre utilis? en ligne de commande : soit avec le programme g?n?ral
sppas.py, soit ?tape par ?tape (un ensemble d?outils est disponible dans le r?pertoite tools).
Un des points importants pour favoriser la diffusion destin?e ? une large communaut? concerne
la licence. SPPAS n?utilise que des ressources et des outils d?pos?s sous licence GPL. SPPAS
peut ainsi ?tre distribu? sous les termes de cette licence libre. Par ailleurs, pour des raisons de
compatibilit?, SPPAS manipule des fichiers TextGrid (format natif de Praat).
SPPAS permet de traiter diff?rentes langues avec la m?me approche car la connaissance linguis-
tique est plac?e dans les ressources et non dans les algorithmes. Actuellement, cet outil peut
traiter des donn?es en anglais, fran?ais, italien ou chinois. Ajouter une nouvelle langue L dans
SPPAS consiste ? ajouter les ressources n?cessaires ? chacun des modules, ? savoir :
1. pour la phon?tisation : un dictionnaire au format HTK, dans dict/L.dict,
2. pour l?alignement : un mod?le acoustique (au format standard HTK-ASCII, appris ? partir
de fichiers audio ?chantillon?s ? 16000Hz), dans models/models-L,
3. pour la syllabation : un fichier de r?gles, dans syll/syllConfig-L.txt.
Si ces fichiers sont plac?s dans les r?pertoires appropri?s et respectent la convention de nom-
mage et le format requis, la nouvelle langue sera prise en compte automatiquement.
FIGURE 1 ? SPPAS - Version 1.4
87
Afin d?assurer la portabilit?, SPPAS est d?velopp? dans le langage python. En plus d??tre multi-
plateforme, le langage python offre l?avantage d??tre orient? objet. Il permet ainsi un d?veloppe-
ment modulaire int?ressant compte-tenu des objectifs du logiciel. En outre, python est inter-
pr?t?, il ne n?cessite donc pas l??tape de compilation qui peut ?tre difficile pour un non in-
formaticien. L?interface graphique de SPPAS est d?velopp?e ? l?aide de la librairie wxPython,
?galement tr?s facile ? installer.
Actuellement, SPPAS a notamment permis au LPL de participer ? la campagne d??valuation
Evalita 2011, pour la t?che d?alignement forc? de dialogues en map-task, en italien (Bigi, 2012).
SPPAS a ?galement ?t? choisi pour traiter les donn?es du corpus AixOx, enregistr?es dans le
cadre du projet Amennpro, sur les phrases lues en fran?ais, par des locuteurs fran?ais ou des
apprenants anglophones (Herment et al, 2012). Il est par ailleurs r?guli?rement utilis? au LPL
pour r?aliser diff?rentes t?ches d?alignement.
Sur la figure 1, on voit une ?tape suppl?mentaire nomm?e ? Momel and INTSINT ?. Elle impl?-
mente la mod?lisation automatique de la m?lodie propos?e dans (Hirst et Espesser, 1993).
3 Les modules de SPPAS
3.1 Segmentation en IPUs
Cette segmentation consiste ? aligner les macro-unit?s d?un texte (segments, phrases, etc) avec
le son qui lui correspond. C?est un probl?me de recherche ouvert car, ? notre connaissance, seul
EasyAlign propose un tel algorithme. SPPAS utilise les pauses indiqu?es (manuellement) dans
la transcription. L?algorithme s?appuie sur la recherche des pauses dans le signal et leur aligne-
ment avec les unit?s propos?es dans la transcription (en supposant qu?une pause s?pare chaque
unit?). Pour une dur?e fix?e de pause et une dur?e fix?e des segments de parole, une recherche
dichotomique permet d?ajuster le volume pour trouver le bon nombre d?unit?s. Selon que le
nombre d?unit?s trouv?es est inf?rieur ou sup?rieur au nombre souhait? d?unit?s, la recherche
est relanc?e avec des valeurs de dur?es de pauses et de dur?e des unit?s plus ?lev?es ou moins
?lev?es. La recherche s?arr?te lorsque les 3 param?tres sont fix?s correctement, c?est-?-dire qu?ils
permettent de trouver le bon nombre d?unit?s. Cet alorithme a ?t? appliqu? ? un corpus de
lecture de mots et au corpus AixOx (Herment et al, 2012) de lecture de petits paragraphes (3-6
phrases). La figure 2 montre une segmentation de ce dernier. SPPAS a ainsi permis ainsi un gain
de temps substantiel.
FIGURE 2 ? Segmentation en IPUs
88
3.2 Phon?tisation
La phon?tisation, aussi appel?e conversion graph?me-phon?me, consiste ? repr?senter les unit?s
(mots, syllabes) d?un texte par des symboles phon?tiques. Il existe deux familles d?approches
dans les m?thodes de phon?tisation : celles reposant sur des r?gles (propos?es par des experts
et/ou apprises sur corpus) et celles s?appuyant uniquement sur un dictionnaire. SPPAS impl?-
mente cette derni?re approche. SPPAS propose aussi un plugin, nomm? ESPPAS, qui permet
d?utiliser l?approche ? base de r?gles pour le fran?ais.
Approche ? base de dictionnaire : Il n?y a pas d?algorithme sp?cifique dans cette approche.
Le principe r?side simplement ? consulter le dictionnaire pour en extraire la prononciation de
chaque entr?e observ?e. Les deux situations suivantes peuvent survenir :
? une entr?e peut se prononcer de diff?rentes mani?res. C?est le cas notamment des homo-
graphes h?t?rophones, mais aussi des accents r?gionaux ou des ph?nom?nes de r?ductions
propres ? l?oral. Dans ce cas, SPPAS ne choisit pas a priori la prononciation. Toutes les varian-
tes pr?sentes dans le dictionnaire sont cumul?es dans la phon?tisation.
? une entr?e peut ?tre absente du dictionnaire. SPPAS peut soit la remplacer par le symbole
UNK, soit produire une phon?tisation automatique. Dans ce cas, l?algorithme repose sur une
recherche ? longest matching ?, ind?pendante de la langue. Il cherche, de gauche ? droite, les
segments les plus longs dans le dictionnaire et recompose la phon?tisation des segments pour
cr?er la phon?tisation du mot absent.
Par exemple, pour les mots ? je ? et ? suis ? le dictionnaire propose :
je [je] jj suis [suis] ss yy ii
je(2) [je] jj eu suis(2) [suis] ss yy ii zz
je(3) [je] ch suis(3) [suis] ss uu ii
suis(3) [suis] yy ii
Pour l??nonc? ? je suis ?, SPPAS propose alors la phon?tisation : ? jj|jj.eu|ch
ss.yy.ii|ss.yy.ii.zz|ss.uu.ii|yy.ii ? dans laquelle les espaces s?parent les mots, les points s?-
parent les phon?mes et les barres verticales s?parent les variantes. L?utilisateur peut laisser la
phon?tisation telle quelle (processus enti?rement automatique) : c?est l?aligneur qui choisira
la phon?tisation. La phrase phon?tis?e sera l?une des combinaisons possibles des variantes
propos?es par SPPAS pour chaque mot. Dans un processus semi-automatique, l?utilisateur
peut choisir la phon?tisation appropri?e (ou la modifier) manuellement. Pour des raisons de
compatibilit?, SPPAS utilise des dictionnaires au m?me format que ceux d?HTK. C?est un format
ASCII ?ditable ; il peuvent donc ?tre facilement modifi?s avec un ?diteur de texte.
Approche ? base de r?gles : Dans le cadre de notre ?tude, notre choix s?est port? sur l?outil
LIA_Phon (Bechet, 2001), pour deux raisons. La premi?re parce qu?il est diffus? sous licence GPL,
donc facilement accessible et par ailleurs, suffisamment bien document?, facile d?utilisation et
multi-plateformes. La seconde car il est connu pour produire une phon?tisation de qualit?.
En dehors de l??tape de transcription graph?me-phon?me, g?n?ralement trait?e par une ap-
proche ? base de r?gles, de nombreux traitements linguistiques sont n?cessaires afin de lever
les ambigu?t?s d?oralisation du texte ?crit (formatage du texte, homographes h?t?rophones, li-
aisons, phon?tisation des noms propres, sigles ou emprunts ? des langues ?trang?res, etc). Les
outils inclus dans le LIA_Phon peuvent se d?composer en trois modules : les outils de formatage
et d??tiquetage, les outils de phon?tisation et les outils d?exploitation des textes phon?tis?s. Dans
89
la pr?sente ?tude, nous faisons appel aux deux premiers modules. Le plugin ? ESPPAS ? (pour
Enriched-SPPAS) encapsule le LIA_Phon pour l?utiliser facilement dans SPPAS.
3.3 Alignement en phon?mes et en mots
L?alignement en phon?mes consiste ? d?terminer la localisation temporelle de chacun des
phon?mes d?une unit?. SPPAS fait appel ? Julius pour r?aliser l?alignement. Julius est essen-
tiellement d?di? ? la reconnaissance automatique de la parole. Il est distribu? sous licence GPL,
avec des versions ex?cutables simples ? installer. Pour r?aliser l?alignement, Julius a besoin d?une
grammaire et d?un mod?le acoustique. La grammaire contient la (ou les) prononciation(s) de
chaque mot et l?indication des transitions entre les mots. L?alignement requiert aussi un mod-
?le acoustique qui doit ?tre au format HTK-ASCII, appris ? partir de fichiers audio en 16000hz.
Dans une premi?re ?tape, Julius s?lectionne la phon?tisation et la segmentation en phon?mes
est effectu?e lors d?une seconde ?tape. La segmentation en mots est d?duite de cette derni?re.
La table 1 synth?tise les informations relatives aux ressources incluses dans SPPAS. Il con-
tient le nombre d?entr?es du dictionnaire et la quantit? de donn?es utilis?es pour l?appren-
tissage des mod?les acoustiques. Les ressources de l?anglais proviennent du projet VoxForge
(http ://www.voxforge.org), avec le dictionnaire du CMU.
Langue Dictionnaire Mod?le Acoustique
Fran?ais 348k, 305k variantes 7h30 CID et 30min AixOx, triphones
Italien 390k, 5k variantes 3h30 CLIPS dialogues map-task, triphones
Chinois simplifi? 353 syllabes 1h36 phrases lues, monophones
TABLE 1 ? Ressources de SPPAS - Version 1.4
3.4 Syllabation
SPPAS encapsule le syllabeur du LPL (Bigi et al, 2010). Il consiste ? d?finir un ensemble de r?-
gles de segmentation entre phon?mes. Il repose sur les deux principes suivants : 1/ une syllabe
contient une seule voyelle ; 2/ une pause est une fronti?re de syllabe. Ces deux principes r?su-
ment le probl?me de syllabation en la recherche de fronti?res de syllabes entre deux voyelles.
Les phon?mes sont alors regroup?s en classes et des r?gles de segmentation entre ces classes
sont ?tablies, comme dans l?exemple suivant :
Transcription et donc on mange sur la baignoire donc c?est c?est ?a
Phon?mes e d O? k O? m A? Z s y ? l A b e n w A ? d O? k s e s e s A
Classes V O V O V N V F F V L L V O V N G V L O V O F V F V F V
Syllabes e . dO? . kO? . mA?Z . sy? . lA . be . nwA? . d O? k . se . se . sA
Le programme utilise un fichier de configuration qui d?crit la liste des phon?mes et leur classe,
ainsi que la liste de toutes les r?gles. Il peut ?tre facilement modifi?, ce qui rend l?outil applicable
? d?autres langues. SPPAS inclut les fichiers de configuration pour la syllabation du fran?ais et
de l?italien (ce dernier n?a pas ?t? ?valu?).
90
4 ?valuations
Nous pr?sentons ici des ?valuations que nous avons r?alis?es sur la phon?tisation du fran?ais.
Les ?valuations sur la phon?tisation et l?alignement de l?italien sont pr?sent?es dans (Bigi, 2012).
Nous avons d?abord construit un corpus, nomm? ? MARC-Fr - Manual Alignments Reference
Corpus for French ?, enti?rement phon?tis? et algn? manuellement par un expert phon?ticien.
Il est d?pos? sous licence GPL sur la forge SLDR 1. Il est compos? de 3 corpus :
? 143 secondes d?extraits du CID, corpus conversationnel d?crit dans (Bertrand et al, 2008)
? 137 secondes d?extraits du corpus AixOx, corpus de lecture d?crit dans (Herment et al, 2012),
? 134 secondes d?un extrait d?Yves Cochet lors d?un d?bat ? l?Assembl?e nationale portant sur
le ? Grenelle II de l?environnement ? d?crit dans (Bigi et al, 2011).
La transcription est en orthographe standard et contient les pauses pleines, les pauses per?ues,
les rires, les bruits, les amorces et les r?p?titions. D?autres r?sultats avec diff?rents enrichisse-
ments de la transcription sont propos?s dans (Bigi et al, 2012)). Les ?valuations sont effectu?es
avec l?outil Sclite (NIST, 2009). Il calcule le taux d?erreurs de la phon?tisation (Err) qui somme
les erreurs de substitution (Sub), de suppression (Del) et d?insertion (Ins). Les r?sultats sont
pr?sent?s dans le tableau 2. La phon?tisation du CID est meilleure en utilisant SPPAS, tandis
que pour les deux autres corpus, la phon?tisation est meilleure en utilisant le LIA_Phon. Cepen-
dant, puisque SPPAS utilise une approche ? base de dictionnaire qui d?pend ?norm?ment des
ressources dont il dispose, il b?n?ficie d?une marge de progression assez importante. Le dictio-
nnaire pourrait en effet ?tre am?lior? en v?rifiant manuellement les entr?es. Il faudrait aussi
am?liorer le mod?le acoustique, en ajoutant des donn?es d?apprentissage.
Sub Del Ins Err
CID SPPAS-dico 3,6 2,1 7,6 13,2
LIA_Phon 2,7 1,4 10,3 14,4
AixOx SPPAS-dico 3,1 2,4 2,9 8,4
LIA_Phon 1,4 2,3 2,9 6,5
Grenelle SPPAS-dico 1,7 1,7 4,1 7,4
LIA_Phon 1,0 1,2 4,1 6,2
TABLE 2 ? Pourcentages d?erreurs de la phon?tisation
5 Perspectives
SPPAS est un outil qui permet d?aligner automatiquement textes et sons. Sa particularit? vient
du fait qu?il s?adresse ? une communaut? tr?s large d?utilisateurs. De nombreux efforts ont ?t?
r?alis?s en ce sens lors de son d?veloppement : portabilit?, accessibilit?, modularit?, licence libre,
etc. Les d?veloppements ? venir suivent 3 directions : la premi?re consiste valoriser la version
actuelle (documentation, tutoriel, d?p?t dans une forge, packaging, etc), le deuxi?me consiste
en l?ajout de nouveaux modules (d?tection de pitch, tokenizer multilingue), la troisi?me est
l?ajout de nouvelles ressources pour la prise en charge de nouvelles langues (et/ou la cr?ation
d?un mod?le multilingue).
1. Speech Language Data Repository, http ://www.sldr.fr
91
R?f?rences
BECHET, F. (2001). LIA_PHON - un syst?me complet de phon?tisation de textes. Traitement
Automatique des Langues, 42(1).
BERTRAND, R., BLACHE, P., ESPESSER, R., FERR?, G., MEUNIER, C., PRIEGO-VALVERDE, B. et RAUZY, S.
(2008). Le CID - Corpus of Interactional Data. Traitement Automatique des Langues, 49(3):105?
134.
BIGI, B. (2012). The SPPAS participation to Evalita 2011. In EVALITA 2011 : Workshop on
Evaluation of NLP and Speech Tools for Italian, Rome, Italie.
BIGI, B., MEUNIER, C., NESTERENKO, I. et BERTRAND, R. (2010). Automatic detection of syllable
boundaries in spontaneous speech. In Language Resource and Evaluation Conference, pages
3285?3292, La Valetta, Malta.
BIGI, B., PORTES, C., STEUCKARDT, A. et TELLIER, M. (2011). Multimodal annotations and catego-
rization for political debates. In In ICMI Workshop on Multimodal Corpora for Machine learning
(ICMI-MMC), Alicante, Espagne.
BIGI, B., P?RI, P. et BERTRAND, R. (2012). Orthographic Transcription : Which Enrichment is
required for Phonetization ? In The eighth international conference on Language Resources and
Evaluation, Istanbul (Turkey).
BOERSMA, P. et WEENINK, D. (2009). Praat : doing phonetics by computer,
http ://www.praat.org.
CANGEMI, F., CUTUGNO, F., LUDUSAN, B., SEPPI, D. et COMPERNOLLE, D.-V. (2010). Automatic
speech segmentation for italian (assi) : tools, models, evaluation, and applications. In 7th AISV
Conference, Lecce, Italie.
CARNEGIE MELLON UNIVERSITY (2011). CMUSphinx : Open Source Toolkit For Speech Recogni-
tion. http ://cmusphinx.sourceforge.net.
GOLDMAN, J.-P. (2011). EasyAlign : an automatic phonetic alignment tool under Praat. In
InterSpeech, Florence, Italie.
HERMENT, S., LOUKINA, A., TORTEL, A., HIRST, D. et BIGI, B. (2012). A multi-layered learners
corpus : automatic annotation. In 4th International Conference on Corpus Linguistics Language,
corpora and applications : diversity and change, Ja?n (Espagne).
HIRST, D. J. et ESPESSER, R. (1993). Automatic modelling of fundamental frequency using a
quadratic spline function. Travaux de l?Institut de Phon?tique d?Aix, 15:75?85.
LEE, A., KAWAHARA, T. et SHIKANO., K. (2001). Julius ? an open source real-time large vocab-
ulary recognition engine.". In European Conference on Speech Communication and Technology,
pages 1691?1694.
NIST (2009). Speech recognition scoring toolkit, http ://www.itl.nist.gov/iad/mig/tools/.
YOUNG, S. (1994). The HTK Hidden Markov Model Toolkit : Design and Philosophy. Entropic
Cambridge Research Laboratory, Ltd, 2:2?44.
YUAN, J. et LIBERMAN, M. (2008). Speaker identification on the scotus corpus. In Acoustics.
92
Proceedings of the SIGDIAL 2013 Conference, pages 87?91,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A quantitative view of feedback lexical markers in conversational French
Laurent Pre?vot Brigitte Bigi
Aix Marseille Universite? & CNRS
Laboratoire Parole et Langage
Aix-en-Provence (France)
firstname.lastname@lpl-aix.fr
Roxane Bertrand
Abstract
This paper presents a quantitative descrip-
tion of the lexical items used for linguis-
tic feedback in the Corpus of Interactional
Data (CID). The paper includes the raw
figures for feedback lexical item as well
as more detailed figures concerning inter-
individual variability. This effort is a first
step before a broader analysis including
more discourse situations and featuring
communicative function annotation.
Index Terms: Feedback, Backchannel, Corpus,
French Language
1 Objectives
Conversational feedback is mostly performed
through short utterances such as yeah, mh, okay
not produced by the main speaker but by one of
the other participants of a conversation. Such ut-
terances are among the most frequent in conver-
sational data (Stolcke et al, 2000). They also
have been described in psycho-linguistic models
of communication as a crucial communicative tool
for achieving coordination or alignment in dia-
logue (Clark, 1996).
The general objective of the project (ANR
CoFee: Conversational Feedback)1(Pre?vot and
Bertrand, 2012) in which this work takes place
is to propose a fine grained model of the
form/function relationship concerning feedback
behaviors in conversation. The present study is
first exploration aiming at knowing better the dis-
tribution of these items in one of our corpus. More
precisely, we would to verify how much inter-
individual variability we will face in further study
and whether we can identify a structure in this
variability (e.g speaker profiles). Second, we tried
1See the project website: http://cofee.hypotheses.org
to check there some strong trends in terms of evo-
lution of use of these items in the course of the
conversation. This later point was not conclusive
and is not developed in this paper.
Some data-intensive works exist for English
(Gravano et al, 2012), Japanese (Kamiya et al,
2010; Misu et al, 2011) or Swedish (Allwood et
al., 1992; Cerrato, 2007; Neiberg et al, 2013) but
not on many other languages such as French for
example. On French, the work of (Muller and
Pre?vot, 2003; Muller and Pre?vot, 2009) concerned
a smaller scale (A hour corpus) and very specific
task. (Bertrand et al, 2007) was focussed on the
feedback inviting cues and also on a smaller scale
(2 ? 15 minutes). They showed that particular
pitch contours and discursive markers play a sys-
tematic role as inviting-cues both for vocal and
gestural back-channels.
The paper is structured as follow. Section 2
presents the conversational corpus used for this
study, then section 3 presents how this corpus has
been processed. Section 4 is related to general fig-
ures for the feedback lexical items, followed by
more detailed information about inter-individual
variability (section 5).
2 The corpus
The Corpus of Interactional Data (CID) (Bertrand
et al, 2008; Blache et al, 2009)2 is an audio-video
recording of 8 hours of spontaneous French dia-
logues, 1 hour of recording per session. Each di-
alogue involved two participants of the same gen-
der. One of the following two topics of conver-
sation was suggested to participants: conflicts in
their professional environment or unusual situa-
tions in which participants may have found them-
selves. It features a nearly free conversational
style with only a single theme proposed to the par-
ticipants at the beginning of the experiment. This
2http://www.sldr.org/sldr000027/en
87
corpus is fully transcribed and forced-aligned at
phone level. Moreover, it has been annotated with
various linguistic information (Prosodic Phrasing,
Discourse units, Syntactic tags, ...) (Blache et al,
2010) which will allow us later to take advantage
of these levels of analysis.
Numerous studies have been carried out in pre-
pared speech. However, conversational speech
refers to a more informal activity, in which par-
ticipants have constantly to manage and negotiate
turn-taking, topic changes (among other things)
without any preparation. As a consequence, nu-
merous phenomena appear such as hesitations, re-
peats, backchannels, etc. Phonetic phenomena
such as non-standard elision, reduction phenom-
ena, truncated words, and more generally, non-
standard pronunciations are also very frequent.
All these phenomena can impact on the phoneti-
zation, then on alignment.
3 Processing the corpus
The transcription process is done following spe-
cific conventions derived from that of the GARS
(Blanche-Benveniste and Jeanjean, 1987). The
result is what we call an enriched orthographic
transcription (EOT), from which two derived tran-
scriptions are generated automatically : the stan-
dard orthographic transcription (the list of ortho-
graphic tokens) and a specific transcription from
which the phonetic tokens are obtained to be
used by the grapheme-phoneme converter. From
the phoneme sequence and the audio signal, the
aligner outputs for each phoneme its time localiza-
tion. This corpus has been processed with several
aligners. The first and main one (Brun et al, 2004)
is HMM-based, it uses a set of 10 macro-classes
of vowel (7 oral and 3 nasal), 2 semi-vowels and
15 consonants. Finally, from the time aligned
phoneme sequence plus the EOT, the orthographic
tokens is time-aligned.
The alignment for this paper is another ver-
sion that has been carried out using SPPAS3 (Bigi,
2012). SPPAS is a tool to produce automatic anno-
tations which include utterance, word, syllabic and
phonemic segmentations from a recorded speech
sound and its transcription.
Alignment of items of the list given in (1) were
then manually verified. Largest errors were cor-
rected to obtain reliable alignments.
DM prononciations are the standard ones except
3http://www.lpl-aix.fr/?bigi/sppas/
for a few cases. There are only two items with
non standard cases that are over 2 occurrences:
sampa: m.w.e.) that is an hybrid between mh
and ouais, and sampa w.a.l.a, a reduction of
v.w.a.l.a voila`.
The extraction themselves have been realized
by the authors with a Python script and all the
statistical analyses and plots have been produced
with R statistical analysis tool.
4 Descriptive statistics for the lexical
markers used in feedback
All the lexical items of the list given in (1) were
automatically extracted and categorized into two
categories: (i) Isolated items are items or sequence
of items surrounded by pauses of at least 200 ms
and not including any extra material than the items
of this list ; (ii) Initial items (or sequence items)
are located in front of some other items (but there
is no other material within the sequence). Most
of these items also occur in final or even sur-
rounded positions but we did not consider these
cases since they do are not clearly related to feed-
back. More precisely surrounded items are mostly
consisting in breaks of disfluencies or genuinely
integrated construction (e.g j?e?tais d?accord avec
lui / I agreed with him). Final ones can play a
role in eliciting feedback or sometimes bring some
kind of closure at the end of the utterance (what
has been described as Pivot Ending in (Gravano et
al., 2012)).
(1) ah (ah), bon (well), ben (well), euh (err,
uh), mh (mh), ouais (yeah), oui (yes), non
(no), d?accord (agreed), OK (okay), voila`
(that?s it, right)
Strictly speaking, the list (1) is not exhaustive.
However, other items are already in the thin part
of the distribution?s tail. Moreover, some of the
items such as euh / err are not necessarily related
to feedback. However, by crossing lexical values
with position we expect to get close enough the
full set of tokens involved in feedback. For exam-
ple, initial euh not followed by a feedback related
item will not be included in the final dataset. This
is also an objective of the present work to identify
these situations.
The different markers exhibit very different fig-
ures with regard to their location as it can be seen
in 1. While some are specialized in isolated feed-
back such as the continuer mh which is most of the
88
time backchanneled, others are found at the begin-
ning of utterances such as euh, ah. The later makes
sense since euh is also a filled pause.
ah 
ah_
oua
is 
ah_
oui ben bon 
dacc
ord euh mh 
mh_
mh non
 
oua
is 
oua
is_o
uais
 
oui voila
 0
100
200
300
400
500
initialisolated
Figure 1: Distribution of isolated vs. initial posi-
tion for the most frequent lexical items
In total 197 different combinations of the ba-
sic markers were identified. The most frequent
are the simple repetitions of items such as ouais
(up to nine times) or mh. There are also more
complex structures as exhibited in (2) that seem
to mix two kinds of items: base ones and mod-
ifiers (ah, euh). The base ones seem by default
to carry general purpose communicative functions
as described in (Bunt, 2009; Bunt, 2012) while the
others can also be produced alone but are generally
dealing specific dimension such as turn-taking, at-
titude expression or time management.
(2) a. ah ouais d?accord ok (ah yeah right
okay)
b. voila` oui non (that?s it yes no)
With regard to duration, the data is rather messy
concerning the very long items. There are extreme
lengthening on these units. Aside that and the filler
uh that exhibit a wide spread, the other items are
not produced with huge variations. Monosyllabic
remain well centered around 150-250 ms while di-
syllabic and repeated items are distributed in the
250-500 ms range. This is important for our next
step in which automatic acoustic analysis of these
items will be performed.
l
l
ll
l
l
lll
l
l
ll
l
l
l
l
l l
l
l l
l
l
l
l
ll
l
ll
l
l
l
ll
l
l
l
llll
l
l
l
ll lll
l
ll
l
l
l
l
ll ll
llll
l
ll
l
l
ll
l
l
ll
l
l
ll
l
l
l l
ah 
ah_
oua
is 
ah_
oui ben bon 
dacc
ord euh mh 
mh_
mh non
 
oua
is 
oua
is_o
uais
 
oui voila
 
0.0
0.5
1.0
1.5
2.0
Figure 2: Duration (in seconds) of each lexical
type
5 Inter-individual variability
Inter-individual variation is a big issue on the way
to the generalizability. We would like to under-
stand some of the feedback producing profiles.
Our intuitions coming from familiarity of the data
is that there are strong variation but they corre-
spond to a few different speaking styles. In fu-
ture work, we would like to see in a second step
whether we can identify and characterize these
styles.
l
150
200
250
300
350
400
450
Figure 3: Number of feedback items per speaker
Figure 3 illustrates the total figures of feedback
per speaker. As expected variation is huge, from
132 to 425 but with in fact with few outliers with
a nice batch of speaker in the 200 ? 300 range.
The wider spread of the distribution in the high
range comes from two factors. First of all, there
are participants producing a high quantity of feed-
89
back items. They produce a massive amount of
light backchannels (mh, ouais) compared to low-
quantity feedback producers. The later also pro-
duce feedback during the long pauses of the main
speaker but they produce much less overlapping
backchannels. This should be double checked
with a specific measure (adding overlapping as a
factor). However, a second effect seems important
for at least one speaker (the outlier): the amount
to time holding the floor. In fact the speaker pro-
ducing the most feedback did so because she was
rarely the main speaker.
In order to get a global idea of the different uses
of these items, Figure 5 represents the proportion
of each item per speaker. As expected, the varia-
tion is important but one can spot some tendencies.
For examples for the most frequent items, the rank
seems to preserved across speakers.
CID_
AB 
CID_
AC 
CID_
AG 
CID_
AP 
CID_
BX 
CID_
CM 
CID_
EB 
CID_
IM 
CID_
LJ 
CID_
LL 
CID_
MB 
CID_
MG 
CID_
ML 
CID_
NH 
CID_
SR 
CID_
YM 
voilaouiouais_ouaisouaisnonmh_mhmheuhdaccordbonbenah_ouiah_ouaisah
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Distribution of the lexical items
Based on their feedback profile (proportion of
use of each items as illustrated in Figure 5), we
attempted to cluster the participants as showed in
5. While the lower parts of the dendrogram are
hard to interpret the higher part matches well with
the impression acquired by listening to the corpus
(no backchannels and rather formal feedback vs.
lots of backchannels and very colloquial style).
6 Current and Future Work
About this first batch of analyses, we will com-
plete the analysis of the evolution during the con-
versation. More precisely, we will go at the
individual level looking for time-based changes
AB
AG
IM ML
YM
BX
AP EB LJ NH
CM
SR
AC MB
LL MG0.1
0.2
0.3
0.4
0.5
Dendrogram of  diana(x = distSpForm)
diana (*, "NA")distSpForm
Heig
ht
Figure 5: Dendrogram of the participants cluster
based on their feedback profile
in their profiles as well as looking at the pairs
for tracking potential convergence effect either in
terms of distribution of lexical marker types or in
their duration.
In parallel to this work, we are launching in-
dependent prosodic and kinesic analyses of the
forms, as well as a discourse analysis of the func-
tions. Moreover the work is being extended by
adding two corpora in the study in order to allow
for a better situation generalisability: A French
MapTask; and a third corpus consisting in a less
cooperative situation. The idea is later to bring
together the observations from the different levels
in order to propose a multidimensional model for
feedback in French dialogues.
Those are steps toward more extensive studies
in the spirit of (Gravano et al, 2012) or (Neiberg
et al, 2013) on French language and in which we
hope to address more directly the issue of dis-
course situation generalisability.
Acknowledgment
This work has been realized with the support of
the ANR (Grant Number: ANR-12-JCJC-JSH2-
006-01) and exploited aligned data produced in
the framework of the ANR project (Grant Number
ANR-08-BLAN-0239). We would like to thank all
the members of these two projects.
90
References
J. Allwood, J. Nivre, and E. Ahlsen. 1992. On the se-
mantics and pragmatics of linguistic feedback. Jour-
nal of Semantics, 9.
R. Bertrand, G. Ferre?, P. Blache, R. Espesser, and
S. Rauzy. 2007. Backchannels revisited from a mul-
timodal perspective. In Proceedings of Auditory-
visual Speech Processing. Citeseer.
R. Bertrand, P. Blache, R. Espesser, G. Ferre?, C. Me-
unier, B. Priego-Valverde, and S. Rauzy. 2008.
Le cid-corpus of interactional data-annotation et ex-
ploitation multimodale de parole conversationnelle.
Traitement Automatique des Langues, 49(3):1?30.
B. Bigi. 2012. SPPAS: a tool for the phonetic segmen-
tation of speech. In Language Resource and Evalu-
ation Conference, pages 1748?1755, ISBN 978?2?
9517408?7?7, Istanbul (Turkey).
P. Blache, R. Bertrand, and G. Ferre?. 2009. Creating
and exploiting multimodal annotated corpora: the
toma project. Multimodal corpora, pages 38?53.
P. Blache, R. Bertrand, B. Bigi, E. Bruno, E. Cela,
R. Espesser, G. Ferre?, M. Guardiola, D. Hirst,
E. Muriasco, J.-C. Martin, C. Meunier, M.-A. Morel,
I. Nesterenko, P. Nocera, B. Palaud, L. Pre?vot,
B. Priego-Valverde, J. Seinturier, N. Tan, M. Tel-
lier, and S. Rauzy. 2010. Multimodal annotation
of conversational data. In Proceedings of Linguistic
Annotation Workshop.
C. Blanche-Benveniste and C. Jeanjean. 1987. Le
franc?ais parle?. Edition et transcription. Paris, Di-
dier Erudition.
A. Brun, C. Cerisara, D. Fohr, I. Illina, D. Langlois,
O. Mella, and K. Sma??li. 2004. Ants: le syste`me de
transcription automatique du loria. In Actes des XXV
Journe?es d?Etudes sur la Parole, Fe`s, Morocco.
H. Bunt. 2009. Multifunctionality and multidimen-
sional dialogue act annotation. In Proceedings of
DiaHolmia, SEMDIAL.
H. Bunt. 2012. The semantics of feedback. In
16th Workshop on the Semantics and Pragmatics of
Dialogue (SEMDIAL 2012), pages 118?127, Paris
(France).
L. Cerrato. 2007. Investigating Communicative Feed-
back Phenomena across Languages and Modalities.
Ph.D. thesis.
H.H. Clark. 1996. Using language. Cambridge: Cam-
bridge University Press.
A. Gravano, J. Hirschberg, and S?. Ben?us?. 2012. Af-
firmative cue words in task-oriented dialogue. Com-
putational Linguistics, 38(1):1?39.
Y. Kamiya, T. Ohno, and S. Matsubara. 2010. Coher-
ent back-channel feedback tagging of in-car spoken
dialogue corpus. In Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 205?208. Association for Com-
putational Linguistics.
T. Misu, E. Mizukami, Y. Shiga, S. Kawamoto,
H. Kawai, and S. Nakamura. 2011. Toward con-
struction of spoken dialogue system that evokes
users? spontaneous backchannels. In Proceedings
of the SIGDIAL 2011 Conference, pages 259?265.
Association for Computational Linguistics.
P. Muller and L. Pre?vot. 2003. An empirical study
of acknowledgement structures. In Proceedings od
Diabruck, 7th workshop on semantics and pragmat-
ics of dialogue, Saarbrucken.
P. Muller and L. Pre?vot. 2009. Grounding information
in route explanation dialogues. In Spatial Language
and Dialogue. Oxford University Press.
D. Neiberg, G. Salvi, and J. Gustafson. 2013. Semi-
supervised methods for exploring the acoustics of
simple productive feedback. Speech Communica-
tion.
L. Pre?vot and R. Bertrand. 2012. Cofee-toward a mul-
tidimensional analysis of conversational feedback,
the case of french language. In Proceedings of the
Workshop on Feedback Behaviors. (poster).
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
91
