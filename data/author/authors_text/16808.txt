Proceedings of the ACL Student Research Workshop, pages 31?37,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Survey on parsing three dependency representations for English
Angelina Ivanova Stephan Oepen Lilja ?vrelid
University of Oslo, Department of Informatics
{angelii |oe |liljao }@ifi.uio.no
Abstract
In this paper we focus on practical is-
sues of data representation for dependency
parsing. We carry out an experimental
comparison of (a) three syntactic depen-
dency schemes; (b) three data-driven de-
pendency parsers; and (c) the influence of
two different approaches to lexical cate-
gory disambiguation (aka tagging) prior to
parsing. Comparing parsing accuracies in
various setups, we study the interactions
of these three aspects and analyze which
configurations are easier to learn for a de-
pendency parser.
1 Introduction
Dependency parsing is one of the mainstream re-
search areas in natural language processing. De-
pendency representations are useful for a number
of NLP applications, for example, machine trans-
lation (Ding and Palmer, 2005), information ex-
traction (Yakushiji et al, 2006), analysis of ty-
pologically diverse languages (Bunt et al, 2010)
and parser stacking (?vrelid et al, 2009). There
were several shared tasks organized on depen-
dency parsing (CoNLL 2006?2007) and labeled
dependencies (CoNLL 2008?2009) and there were
a number of attempts to compare various depen-
dencies intrinsically, e.g. (Miyao et al, 2007), and
extrinsically, e.g. (Wu et al, 2012).
In this paper we focus on practical issues of data
representation for dependency parsing. The cen-
tral aspects of our discussion are (a) three depen-
dency formats: two ?classic? representations for
dependency parsing, namely, Stanford Basic (SB)
and CoNLL Syntactic Dependencies (CD), and
bilexical dependencies from the HPSG English
Resource Grammar (ERG), so-called DELPH-IN
Syntactic Derivation Tree (DT), proposed recently
by Ivanova et al (2012); (b) three state-of-the art
statistical parsers: Malt (Nivre et al, 2007), MST
(McDonald et al, 2005) and the parser of Bohnet
and Nivre (2012); (c) two approaches to word-
category disambiguation, e.g. exploiting common
PTB tags and using supertags (i.e. specialized
ERG lexical types).
We parse the formats and compare accuracies
in all configurations in order to determine how
parsers, dependency representations and grammat-
ical tagging methods interact with each other in
application to automatic syntactic analysis.
SB and CD are derived automatically from
phrase structures of Penn Treebank to accommo-
date the needs of fast and accurate dependency
parsing, whereas DT is rooted in the formal gram-
mar theory HPSG and is independent from any
specific treebank. For DT we gain more expres-
sivity from the underlying linguistic theory, which
challenges parsing with statistical tools. The struc-
tural analysis of the schemes in Ivanova et al
(2012) leads to the hypothesis that CD and DT
are more similar to each other than SB to DT.
We recompute similarities on a larger treebank and
check whether parsing results reflect them.
The paper has the following structure: an
overview of related work is presented in Sec-
tion 2; treebanks, tagsets, dependency schemes
and parsers used in the experiments are introduced
in Section 3; analysis of parsing results is dis-
cussed in Section 4; conclusions and future work
are outlined in Section 5.
2 Related work
Schwartz et al (2012) investigate which depen-
dency representations of several syntactic struc-
tures are easier to parse with supervised ver-
sions of the Klein and Manning (2004) parser,
ClearParser (Choi and Nicolov, 2009), MST
Parser, Malt and the Easy First Non-directional
parser (Goldberg and Elhadad, 2010). The results
imply that all parsers consistently perform better
when (a) coordination has one of the conjuncts as
the head rather than the coordinating conjunction;
31
A , B and C A , B and C A, B and C
Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats
(b) the noun phrase is headed by the noun rather
than by determiner; (c) prepositions or subordinat-
ing conjunctions, rather than their NP or clause ar-
guments, serve as the head in prepositional phrase
or subordinated clauses. Therefore we can expect
(a) Malt and MST to have fewer errors on coor-
dination structures parsing SB and CD than pars-
ing DT, because SB and CD choose the first con-
junct as the head and DT chooses the coordinating
conjunction as the head; (b,c) no significant dif-
ferences for the errors on noun and prepositional
phrases, because all three schemes have the noun
as the head of the noun phrase and the preposition
as the head of the prepositional phrase.
Miwa et al (2010) present intristic and extris-
tic (event-extraction task) evaluation of six parsers
(GDep, Bikel, Stanford, Charniak-Johnson, C&C
and Enju parser) on three dependency formats
(Stanford Dependencies, CoNLL-X, and Enju
PAS). Intristic evaluation results show that all
parsers have the highest accuracies with the
CoNLL-X format.
3 Data and software
3.1 Treebanks
For the experiments in this paper we used the Penn
Treebank (Marcus et al, 1993) and the Deep-
Bank (Flickinger et al, 2012). The latter is com-
prised of roughly 82% of the sentences of the first
16 sections of the Penn Treebank annotated with
full HPSG analyses from the English Resource
Grammar (ERG). The DeepBank annotations are
created on top of the raw text of the PTB. Due to
imperfections of the automatic tokenization, there
are some token mismatches between DeepBank
and PTB. We had to filter out such sentences to
have consistent number of tokens in the DT, SB
and CD formats. For our experiments we had
available a training set of 22209 sentences and a
test set of 1759 sentences (from Section 15).
3.2 Parsers
In the experiments described in Section 4 we used
parsers that adopt different approaches and imple-
ment various algorithms.
Malt (Nivre et al, 2007): transition-based de-
pendency parser with local learning and greedy
search.
MST (McDonald et al, 2005): graph-based
dependency parser with global near-exhaustive
search.
Bohnet and Nivre (2012) parser: transition-
based dependency parser with joint tagger that im-
plements global learning and beam search.
3.3 Dependency schemes
In this work we extract DeepBank data in the form
of bilexical syntactic dependencies, DELPH-IN
Syntactic Derivation Tree (DT) format. We ob-
tain the exact same sentences in Stanford Basic
(SB) format from the automatic conversion of the
PTB with the Stanford parser (de Marneffe et al,
2006) and in the CoNLL Syntactic Dependencies
(CD) representation using the LTH Constituent-
to-Dependency Conversion Tool for Penn-style
Treebanks (Johansson and Nugues, 2007).
SB and CD represent the way to convert PTB
to bilexical dependencies; in contrast, DT is
grounded in linguistic theory and captures deci-
sions taken in the grammar. Figure 1 demonstrates
the differences between the formats on the coor-
dination structure. According to Schwartz et al
(2012), analysis of coordination in SB and CD is
easier for a statistical parser to learn; however, as
we will see in section 4.3, DT has more expressive
power distinguishing structural ambiguities illus-
trated by the classic example old men and women.
3.4 Part-of-speech tags
We experimented with two tag sets: PTB tags and
lexical types of the ERG grammar - supertags.
PTB tags determine the part of speech (PoS)
and some morphological features, such as num-
ber for nouns, degree of comparison for adjectives
and adverbs, tense and agreement with person and
number of subject for verbs, etc.
Supertags are composed of part-of-speech, va-
lency in the form of an ordered sequence of
complements, and annotations that encompass
category-internal subdivisions, e.g. mass vs. count
vs. proper nouns, intersective vs. scopal adverbs,
32
or referential vs. expletive pronouns. Example of
a supertag: v np is le (verb ?is? that takes noun
phrase as a complement).
There are 48 tags in the PTB tagset and 1091
supertags in the set of lexical types of the ERG.
The state-of-the-art accuracy of PoS-tagging on
in-domain test data using gold-standard tokeniza-
tion is roughly 97% for the PTB tagset and ap-
proximately 95% for the ERG supertags (Ytrest?l,
2011). Supertagging for the ERG grammar is an
ongoing research effort and an off-the-shelf su-
pertagger for the ERG is not currently available.
4 Experiments
In this section we give a detailed analysis of pars-
ing into SB, CD and DT dependencies with Malt,
MST and the Bohnet and Nivre (2012) parser.
4.1 Setup
For Malt and MST we perform the experiments
on gold PoS tags, whereas the Bohnet and Nivre
(2012) parser predicts PoS tags during testing.
Prior to each experiment with Malt, we used
MaltOptimizer to obtain settings and a feature
model; for MST we exploited default configura-
tion; for the Bohnet and Nivre (2012) parser we
set the beam parameter to 80 and otherwise em-
ployed the default setup.
With regards to evaluation metrics we use la-
belled attachment score (LAS), unlabeled attach-
ment score (UAS) and label accuracy (LACC) ex-
cluding punctuation. Our results cannot be di-
rectly compared to the state-of-the-art scores on
the Penn Treebank because we train on sections
0-13 and test on section 15 of WSJ. Also our re-
sults are not strictly inter-comparable because the
setups we are using are different.
4.2 Discussion
The results that we are going to analyze are pre-
sented in Tables 1 and 2. Statistical significance
was assessed using Dan Bikel?s parsing evaluation
comparator1 at the 0.001 significance level. We
inspect three different aspects in the interpretation
of these results: parser, dependency format and
tagset. Below we will look at these three angles
in detail.
From the parser perspective Malt and MST are
not very different in the traditional setup with gold
1http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
PTB tags (Table 1, Gold PTB tags). The Bohnet
and Nivre (2012) parser outperforms Malt on CD
and DT and MST on SB, CD and DT with PTB
tags even though it does not receive gold PTB tags
during test phase but predicts them (Table 2, Pre-
dicted PTB tags). This is explained by the fact that
the Bohnet and Nivre (2012) parser implements a
novel approach to parsing: beam-search algorithm
with global structure learning.
MST ?loses? more than Malt when parsing SB
with gold supertags (Table 1, Gold supertags).
This parser exploits context features ?POS tag of
each intervening word between head and depen-
dent? (McDonald et al, 2006). Due to the far
larger size of the supertag set compared to the PTB
tagset, such features are sparse and have low fre-
quencies. This leads to the lower scores of pars-
ing accuracy for MST. For the Bohnet and Nivre
(2012) parser the complexity of supertag predic-
tion has significant negative influence on the at-
tachment and labeling accuracies (Table 2, Pre-
dicted supertags). The addition of gold PTB tags
as a feature lifts the performance of the Bohnet
and Nivre (2012) parser to the level of perfor-
mance of Malt and MST on CD with gold su-
pertags and Malt on SB with gold supertags (com-
pare Table 2, Predicted supertags + gold PTB, and
Table 1, Gold supertags).
Both Malt and MST benefit slightly from the
combination of gold PTB tags and gold supertags
(Table 1, Gold PTB tags + gold supertags). For
the Bohnet and Nivre (2012) parser we also ob-
serve small rise of accuracy when gold supertags
are provided as a feature for prediction of PTB
tags (compare Predicted PTB tags and Predicted
PTB tags + gold supertags sections of Table 2).
The parsers have different running times: it
takes minutes to run an experiment with Malt,
about 2 hours with MST and up to a day with the
Bohnet and Nivre (2012) parser.
From the point of view of the dependency for-
mat, SB has the highest LACC and CD is first-rate
on UAS for all three parsers in most of the con-
figurations (Tables 1 and 2). This means that SB
is easier to label and CD is easier to parse struc-
turally. DT appears to be a more difficult target
format because it is both hard to label and attach
in most configurations. It is not an unexpected re-
sult, since SB and CD are both derived from PTB
phrase-structure trees and are oriented to ease de-
pendency parsing task. DT is not custom-designed
33
Gold PTB tags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 89.21 88.59 90.95 90.88 93.58 92.79
CD 88.74 88.72 91.89 92.01 91.29 91.34
DT 85.97 86.36 89.22 90.01 88.73 89.22
Gold supertags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 87.76 85.25 90.63 88.56 92.38 90.29
CD 88.22 87.27 91.17 90.41 91.30 90.74
DT 89.92 89.58 90.96 90.56 92.50 92.64
Gold PTB tags + gold supertags
LAS UAS LACC
Malt MST Malt MST Malt MST
SB 90.321 89.431 91.901 91.842 94.481 93.261
CD 89.591 89.372 92.431 92.772 92.321 92.072
DT 90.691 91.192 91.831 92.332 93.101 93.692
Table 1: Parsing results of Malt and MST on
Stanford Basic (SB), CoNLL Syntactic De-
pendencies (CD) and DELPH-IN Syntactic
Derivation Tree (DT) formats. Punctuation is
excluded from the scoring. Gold PTB tags:
Malt and MST are trained and tested on gold
PTB tags. Gold supertags: Malt and MST
are trained and tested on gold supertags. Gold
PTB tags + gold supertags: Malt and MST are
trained on gold PTB tags and gold supertags.
1 denotes a feature model in which gold PTB
tags function as PoS and gold supertags act
as additional features (in CPOSTAG field); 2
stands for the feature model which exploits
gold supertags as PoS and uses gold PTB tags
as extra features (in CPOSTAG field).
Predicted PTB tags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 89.56 92.36 93.30
CD 89.77 93.01 92.10
DT 88.26 91.63 90.72
Predicted supertags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 85.41 89.38 90.17
CD 86.73 90.73 89.72
DT 85.76 89.50 88.56
Pred. PTB tags + gold supertags
LAS UAS LACC
Bohnet and Nivre (2012)
SB 90.32 93.01 93.85
CD 90.55 93.56 92.79
DT 91.51 92.99 93.88
Pred. supertags + gold PTB
LAS UAS LACC
Bohnet and Nivre (2012)
SB 87.20 90.07 91.81
CD 87.79 91.47 90.62
DT 86.31 89.80 89.17
Table 2: Parsing results of the Bohnet
and Nivre (2012) parser on Stanford Ba-
sic (SB), CoNLL Syntactic Dependencies
(CD) and DELPH-IN Syntactic Deriva-
tion Tree (DT) formats. Parser is trained
on gold-standard data. Punctuation is ex-
cluded from the scoring. Predicted PTB:
parser predicts PTB tags during the test
phase. Predicted supertags: parser pre-
dicts supertags during the test phase. Pre-
dicted PTB + gold supertags: parser re-
ceives gold supertags as feature and pre-
dicts PTB tags during the test phase. Pre-
dicted supertags + gold PTB: parser re-
ceives PTB tags as feature and predicts
supertags during test phase.
34
to dependency parsing and is independent from
parsing questions in this sense. Unlike SB and
CD, it is linguistically informed by the underlying,
full-fledged HPSG grammar.
The Jaccard similarity on our training set is 0.57
for SB and CD, 0.564 for CD and DT, and 0.388
for SB and DT. These similarity values show that
CD and DT are structurally closer to each other
than SB and DT. Contrary to our expectations, the
accuracy scores of parsers do not suggest that CD
and DT are particularly similar to each other in
terms of parsing.
Inspecting the aspect of tagset we conclude that
traditional PTB tags are compatible with SB and
CD but do not fit the DT scheme well, while ERG
supertags are specific to the ERG framework and
do not seem to be appropriate for SB and CD. Nei-
ther of these findings seem surprising, as PTB tags
were developed as part of the treebank from which
CD and SB are derived; whereas ERG supertags
are closely related to the HPSG syntactic struc-
tures captured in DT. PTB tags were designed to
simplify PoS-tagging whereas supertags were de-
veloped to capture information that is required to
analyze syntax of HPSG.
For each PTB tag we collected corresponding
supertags from the gold-standard training set. For
open word classes such as nouns, adjectives, ad-
verbs and verbs the relation between PTB tags
and supertags is many-to-many. Unique one-to-
many correspondence holds only for possessive
wh-pronoun and punctuation.
Thus, supertags do not provide extra level of
detalization for PTB tags, but PTB tags and su-
pertags are complementary. As discussed in sec-
tion 3.4, they contain bits of information that are
different. For this reason their combination re-
sults in slight increase of accuracy for all three
parsers on all dependency formats (Table 1, Gold
PTB tags + gold supertags, and Table 2, Predicted
PTB + gold supertags and Predicted supertags +
gold PTB). The Bohnet and Nivre (2012) parser
predicts supertags with an average accuracy of
89.73% which is significantly lower than state-of-
the-art 95% (Ytrest?l, 2011).
When we consider punctuation in the evalua-
tion, all scores raise significantly for DT and at
the same time decrease for SB and CD for all three
parsers. This is explained by the fact that punctu-
ation in DT is always attached to the nearest token
which is easy to learn for a statistical parser.
4.3 Error analysis
Using the CoNLL-07 evaluation script2 on our test
set, for each parser we obtained the error rate dis-
tribution over CPOSTAG on SB, CD and DT.
VBP, VBZ and VBG. VBP (verb, non-3rd
person singular present), VBZ (verb, 3rd per-
son singular present) and VBG (verb, gerund or
present participle) are the PTB tags that have error
rates in 10 highest error rates list for each parser
(Malt, MST and the Bohnet and Nivre (2012)
parser) with each dependency format (SB, CD
and DT) and with each PoS tag set (PTB PoS
and supertags) when PTB tags are included as
CPOSTAG feature. We automatically collected all
sentences that contain 1) attachment errors, 2) la-
bel errors, 3) attachment and label errors for VBP,
VBZ and VBG made by Malt parser on DT format
with PTB PoS. For each of these three lexical cat-
egories we manually analyzed a random sample
of sentences with errors and their corresponding
gold-standard versions.
In many cases such errors are related to the root
of the sentence when the verb is either treated as
complement or adjunct instead of having a root
status or vice versa. Errors with these groups of
verbs mostly occur in the complex sentences that
contain several verbs. Sentences with coordina-
tion are particularly difficult for the correct attach-
ment and labeling of the VBP (see Figure 2 for an
example).
Coordination. The error rate of Malt, MST and
the Bohnet and Nivre (2012) parser for the coor-
dination is not so high for SB and CD ( 1% and
2% correspondingly with MaltParser, PTB tags)
whereas for DT the error rate on the CPOSTAGS
is especially high (26% with MaltParser, PTB
tags). It means that there are many errors on
incoming dependency arcs for coordinating con-
junctions when parsing DT. On outgoing arcs
parsers also make more mistakes on DT than on
SB and CD. This is related to the difference in
choice of annotation principle (see Figure 1). As
it was shown in (Schwartz et al, 2012), it is harder
to parse coordination headed by coordinating con-
junction.
Although the approach used in DT is harder for
parser to learn, it has some advantages: using SB
and CD annotations, we cannot distinguish the two
cases illustrated with the sentences (a) and (b):
2http://nextens.uvt.nl/depparse-wiki/
SoftwarePage#scoring
35
VBP VBD VBD
The figures show that spending rose 0.1 % in the third quarter <. . .> and was up 3.8 % from a year ago .
root
SB-HD
VP-VP
HD-CMP
MRK-NH
root
SP-HD
HD-CMP Cl-CL
MRK-NH
Figure 2: The gold-standard (in green above the sentence) and the incorrect Malt?s (in red below the
sentence) analyses of the utterance from the DeepBank in DT format with PTB PoS tags
a) The fight is putting a tight squeeze on prof-
its of many, threatening to drive the small-
est ones out of business and straining rela-
tions between the national fast-food chains
and their franchisees.
b) Proceeds from the sale will be used for re-
modelling and reforbishing projects, as well
as for the planned MGM Grand hotel/casino
and theme park.
In the sentence a) ?the national fast-food? refers
only to the conjunct ?chains?, while in the sen-
tence b) ?the planned? refers to both conjuncts and
?MGM Grand? refers only to the first conjunct.
The Bohnet and Nivre (2012) parser succeeds in
finding the correct conjucts (shown in bold font)
on DT and makes mistakes on SB and CD in some
difficult cases like the following ones:
a) <. . .> investors hoard gold and help under-
pin its price <. . .>
b) Then take the expected return and subtract
one standard deviation.
CD and SB wrongly suggest ?gold? and ?help? to
be conjoined in the first sentence and ?return? and
?deviation? in the second.
5 Conclusions and future work
In this survey we gave a comparative experi-
mental overview of (i) parsing three dependency
schemes, viz., Stanford Basic (SB), CoNLL Syn-
tactic Dependencies (CD) and DELPH-IN Syn-
tactic Derivation Tree (DT), (ii) with three lead-
ing dependency parsers, viz., Malt, MST and the
Bohnet and Nivre (2012) parser (iii) exploiting
two different tagsets, viz., PTB tags and supertags.
From the parser perspective, the Bohnet and
Nivre (2012) parser performs better than Malt and
MST not only on conventional formats but also on
the new representation, although this parser solves
a harder task than Malt and MST.
From the dependency format perspective, DT
appeares to be a more difficult target dependency
representation than SB and CD. This suggests that
the expressivity that we gain from the grammar
theory (e.g. for coordination) is harder to learn
with state-of-the-art dependency parsers. CD and
DT are structurally closer to each other than SB
and DT; however, we did not observe sound evi-
dence of a correlation between structural similar-
ity of CD and DT and their parsing accuracies
Regarding the tagset aspect, it is natural that
PTB tags are good for SB and CD, whereas the
more fine-grained set of supertags fits DT bet-
ter. PTB tags and supertags are complementary,
and for all three parsers we observe slight benefits
from being supplied with both types of tags.
As future work we would like to run more ex-
periments with predicted supertags. In the absence
of a specialized supertagger, we can follow the
pipeline of (Ytrest?l, 2011) who reached the state-
of-the-art supertagging accuracy of 95%.
Another area of our interest is an extrinsic eval-
uation of SB, CD and DT, e.g. applied to semantic
role labeling and question-answering in order to
find out if the usage of the DT format grounded
in the computational grammar theory is beneficial
for such tasks.
Acknowledgments
The authors would like to thank Rebecca Dridan,
Joakim Nivre, Bernd Bohnet, Gertjan van Noord
and Jelke Bloem for interesting discussions and
the two anonymous reviewers for comments on
the work. Experimentation was made possible
through access to the high-performance comput-
ing resources at the University of Oslo.
36
References
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In
EMNLP-CoNLL, pages 1455?1465. ACL.
Harry Bunt, Paola Merlo, and Joakim Nivre, editors.
2010. Trends in Parsing Technology. Springer Ver-
lag, Stanford.
Jinho D Choi and Nicolas Nicolov. 2009. K-best, lo-
cally pruned, transition-based dependency parsing
using robust risk minimization. Recent Advances in
Natural Language Processing V, pages 205?216.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 541?548, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012.
DeepBank: a Dynamically Annotated Treebank of
the Wall Street Journal. In Proceedings of the
Eleventh International Workshop on Treebanks and
Linguistic Theories, pages 85?96. Edies Colibri.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 742?750, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom?
a contrastive study of syntacto-semantic dependen-
cies. In Proceedings of the Sixth Linguistic Annota-
tion Workshop, pages 2?11, Jeju, Republic of Korea,
July. Association for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: mod-
els of dependency and constituency. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330, June.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, HLT ?05, pages 523?530, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun ichi Tsujii. 2010. Evaluating dependency repre-
sentations for event extraction. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 779?787.
Tsinghua University Press.
Yusuke Miyao, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Towards framework-independent evaluation of deep
linguistic parsers. In Ann Copestake, editor, Pro-
ceedings of the GEAF 2007 Workshop, CSLI Studies
in Computational Linguistics Online, page 21 pages.
CSLI Publications.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Lilja ?vrelid, Jonas Kuhn, and Kathrin Spreyer. 2009.
Cross-framework parser stacking for data-driven de-
pendency parsing. TAL, 50(3):109?138.
Roy Schwartz, Omri Abend, and Ari Rappoport. 2012.
Learnability-based syntactic annotation design. In
Proc. of the 24th International Conference on Com-
putational Linguistics (Coling 2012), Mumbai, In-
dia, December. Coling 2012 Organizing Committee.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2012. A Compara-
tive Study of Target Dependency Structures for Sta-
tistical Machine Translation. In ACL (2), pages 100?
104. The Association for Computer Linguistics.
Akane Yakushiji, Yusuke Miyao, Tomoko Ohta, Yuka
Tateisi, and Jun?ichi Tsujii. 2006. Automatic con-
struction of predicate-argument structure patterns
for biomedical information extraction. In Proceed-
ings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?06,
pages 284?292, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Gisle Ytrest?l. 2011. Cuteforce: deep deterministic
HPSG parsing. In Proceedings of the 12th Interna-
tional Conference on Parsing Technologies, IWPT
?11, pages 186?197, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
37
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,
Dublin, Ireland, August 23-24, 2014.
SemEval 2014 Task 8:
Broad-Coverage Semantic Dependency Parsing
Stephan Oepen
??
, Marco Kuhlmann
?
, Yusuke Miyao
?
, Daniel Zeman
?
,
Dan Flickinger
?
, Jan Haji
?
c
?
, Angelina Ivanova
?
, and Yi Zhang
?
?
University of Oslo, Department of Informatics
?
Potsdam University, Department of Linguistics
?
Link?ping University, Department of Computer and Information Science
?
National Institute of Informatics, Tokyo
?
Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
?
Stanford University, Center for the Study of Language and Information
?
Nuance Communications Aachen GmbH
sdp-organizers@emmtee.net
Abstract
Task 8 at SemEval 2014 defines Broad-
Coverage Semantic Dependency Pars-
ing (SDP) as the problem of recovering
sentence-internal predicate?argument rela-
tionships for all content words, i.e. the se-
mantic structure constituting the relational
core of sentence meaning. In this task
description, we position the problem in
comparison to other sub-tasks in compu-
tational language analysis, introduce the se-
mantic dependency target representations
used, reflect on high-level commonalities
and differences between these representa-
tions, and summarize the task setup, partic-
ipating systems, and main results.
1 Background and Motivation
Syntactic dependency parsing has seen great ad-
vances in the past decade, in part owing to rela-
tively broad consensus on target representations,
and in part reflecting the successful execution of a
series of shared tasks at the annual Conference for
Natural Language Learning (CoNLL; Buchholz &
Marsi, 2006; Nivre et al., 2007; inter alios). From
this very active research area accurate and efficient
syntactic parsers have developed for a wide range
of natural languages. However, the predominant
data structure in dependency parsing to date are
trees, in the formal sense that every node in the de-
pendency graph is reachable from a distinguished
root node by exactly one directed path.
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and the
proceedings footer are added by the organizers: http://
creativecommons.org/licenses/by/4.0/.
Unfortunately, tree-oriented parsers are ill-suited
for producing meaning representations, i.e. mov-
ing from the analysis of grammatical structure to
sentence semantics. Even if syntactic parsing ar-
guably can be limited to tree structures, this is not
the case in semantic analysis, where a node will
often be the argument of multiple predicates (i.e.
have more than one incoming arc), and it will often
be desirable to leave nodes corresponding to se-
mantically vacuous word classes unattached (with
no incoming arcs).
Thus, Task 8 at SemEval 2014, Broad-Coverage
Semantic Dependency Parsing (SDP 2014),
1
seeks
to stimulate the dependency parsing community
to move towards more general graph processing,
to thus enable a more direct analysis of Who did
What to Whom? For English, there exist several
independent annotations of sentence meaning over
the venerable Wall Street Journal (WSJ) text of the
Penn Treebank (PTB; Marcus et al., 1993). These
resources constitute parallel semantic annotations
over the same common text, but to date they have
not been related to each other and, in fact, have
hardly been applied for training and testing of data-
driven parsers. In this task, we have used three
different such target representations for bi-lexical
semantic dependencies, as demonstrated in Figure 1
below for the WSJ sentence:
(1) A similar technique is almost impossible to apply to
other crops, such as cotton, soybeans, and rice.
Semantically, technique arguably is dependent on
the determiner (the quantificational locus), the mod-
ifier similar, and the predicate apply. Conversely,
the predicative copula, infinitival to, and the vac-
1
See http://alt.qcri.org/semeval2014/
task8/ for further technical details, information on how to
obtain the data, and official results.
63
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
A1 A2
(a) Partial semantic dependencies in PropBank and NomBank.
A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.
top
ARG2 ARG3 ARG1
ARG2mwe _and_cARG1ARG1
BV
ARG1 implicit_conjARG1
(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice
top
ARG1
ARG2
ARG1
ARG2
ARG2
ARG1
ARG1 ARG1 ARG1ARG1
ARG1
ARG2
ARG1
ARG2
ARG1
ARG2
ARG1 ARG1 ARG1 ARG2
(c) Enju Predicate?Argument Structures (PAS).
A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .
RSTR
PAT
EXT
PAT
ACT
RSTR
ADDR
ADDR
ADDR
ADDR
APPS.m
APPS.m
CONJ.m
CONJ.m CONJ.m
top
(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).
Figure 1: Sample semantic dependency graphs for Example (1).
uous preposition marking the deep object of ap-
ply can be argued to not have a semantic contri-
bution of their own. Besides calling for node re-
entrancies and partial connectivity, semantic depen-
dency graphs may also exhibit higher degrees of
non-projectivity than is typical of syntactic depen-
dency trees.
In addition to its relation to syntactic dependency
parsing, the task also has some overlap with Se-
mantic Role Labeling (SRL; Gildea & Jurafsky,
2002). In much previous work, however, target
representations typically draw on resources like
PropBank and NomBank (Palmer et al., 2005; Mey-
ers et al., 2004), which are limited to argument
identification and labeling for verbal and nominal
predicates. A plethora of semantic phenomena?
for example negation and other scopal embedding,
comparatives, possessives, various types of modi-
fication, and even conjunction?typically remain
unanalyzed in SRL. Thus, its target representations
are partial to a degree that can prohibit seman-
tic downstream processing, for example inference-
based techniques. In contrast, we require parsers
to identify all semantic dependencies, i.e. compute
a representation that integrates all content words in
one structure. Another difference to common inter-
pretations of SRL is that the SDP 2014 task defini-
tion does not encompass predicate disambiguation,
a design decision in part owed to our goal to focus
on parsing-oriented, i.e. structural, analysis, and in
part to lacking consensus on sense inventories for
all content words.
Finally, a third closely related area of much cur-
rent interest is often dubbed ?semantic parsing?,
which Kate and Wong (2010) define as ?the task of
mapping natural language sentences into complete
formal meaning representations which a computer
can execute for some domain-specific application.?
In contrast to most work in this tradition, our SDP
target representations aim to be task- and domain-
independent, though at least part of this general-
ity comes at the expense of ?completeness? in the
above sense; i.e. there are aspects of sentence mean-
ing that arguably remain implicit.
2 Target Representations
We use three distinct target representations for se-
mantic dependencies. As is evident in our run-
ning example (Figure 1), showing what are called
the DM, PAS, and PCEDT semantic dependencies,
there are contentful differences among these anno-
tations, and there is of course not one obvious (or
even objective) truth. In the following paragraphs,
64
we provide some background on the ?pedigree? and
linguistic characterization of these representations.
DM: DELPH-IN MRS-Derived Bi-Lexical De-
pendencies These semantic dependency graphs
originate in a manual re-annotation of Sections 00?
21 of the WSJ Corpus with syntactico-semantic
analyses derived from the LinGO English Re-
source Grammar (ERG; Flickinger, 2000). Among
other layers of linguistic annotation, this resource?
dubbed DeepBank by Flickinger et al. (2012)?
includes underspecified logical-form meaning rep-
resentations in the framework of Minimal Recur-
sion Semantics (MRS; Copestake et al., 2005).
Our DM target representations are derived through
a two-step ?lossy? conversion of MRSs, first to
variable-free Elementary Dependency Structures
(EDS; Oepen & L?nning, 2006), then to ?pure?
bi-lexical form?projecting some construction se-
mantics onto word-to-word dependencies (Ivanova
et al., 2012). In preparing our gold-standard
DM graphs from DeepBank, the same conversion
pipeline was used as in the system submission of
Miyao et al. (2014). For this target representa-
tion, top nodes designate the highest-scoping (non-
quantifier) predicate in the graph, e.g. the (scopal)
degree adverb almost in Figure 1.
2
PAS: Enju Predicate-Argument Structures
The Enju parsing system is an HPSG-based parser
for English.
3
The grammar and the disambigua-
tion model of this parser are derived from the Enju
HPSG treebank, which is automatically converted
from the phrase structure and predicate?argument
structure annotations of the PTB. The PAS data
set is extracted from the WSJ portion of the Enju
HPSG treebank. While the Enju treebank is an-
notated with full HPSG-style structures, only its
predicate?argument structures are converted into
the SDP data format for use in this task. Top
nodes in this representation denote semantic heads.
Again, the system description of Miyao et al. (2014)
provides more technical detail on the conversion.
PCEDT: Prague Tectogrammatical Bi-Lexical
Dependencies The Prague Czech-English De-
pendency Treebank (PCEDT; Haji
?
c et al., 2012)
4
is a set of parallel dependency trees over the WSJ
2
Note, however, that non-scopal adverbs act as mere in-
tersective modifiers, e.g. loudly is a predicate in DM, but the
main verb provides the top node in structures like Abrams
sang loudly.
3
See http://kmcs.nii.ac.jp/enju/.
4
See http://ufal.mff.cuni.cz/pcedt2.0/.
id form lemma pos top pred arg1 arg2
#20200002
1 Ms. Ms. NNP ? + _ _
2 Haag Haag NNP ? ? compound ARG1
3 plays play VBZ + + _ _
4 Elianti Elianti NNP ? ? _ ARG2
5 . . . ? ? _ _
Table 1: Tabular SDP data format (showing DM).
texts from the PTB, and their Czech translations.
Similarly to other treebanks in the Prague family,
there are two layers of syntactic annotation: an-
alytical (a-trees) and tectogrammatical (t-trees).
PCEDT bi-lexical dependencies in this task have
been extracted from the t-trees. The specifics of
the PCEDT representations are best observed in the
procedure that converts the original PCEDT data to
the SDP data format; see Miyao et al. (2014). Top
nodes are derived from t-tree roots; i.e. they mostly
correspond to main verbs. In case of coordinate
clauses, there are multiple top nodes per sentence.
3 Graph Representation
The SDP target representations can be character-
ized as labeled, directed graphs. Formally, a se-
mantic dependency graph for a sentence x =
x
1
, . . . , x
n
is a structure G = (V,E, `
V
, `
E
) where
V = {1, . . . , n} is a set of nodes (which are in
one-to-one correspondence with the tokens of the
sentence); E ? V ? V is a set of edges; and `
V
and `
E
are mappings that assign labels (from some
finite alphabet) to nodes and edges, respectively.
More specifically for this task, the label `
V
(i) of a
node i is a tuple consisting of four components: its
word form, lemma, part of speech, and a Boolean
flag indicating whether the corresponding token
represents a top predicate for the specific sentence.
The label `
E
(i? j) of an edge i? j is a seman-
tic relation that holds between i and j. The exact
definition of what constitutes a top node and what
semantic relations are available differs among our
three target representations, but note that top nodes
can have incoming edges.
All data provided for the task uses a column-
based file format (dubbed the SDP data format)
similar to the one of the 2009 CoNLL Shared Task
(Haji
?
c et al., 2009). As in that task, we assume gold-
standard sentence and token segmentation. For
ease of reference, each sentence is prefixed by a
line with just a unique identifier, using the scheme
2SSDDIII, with a constant leading 2, two-digit sec-
tion code, two-digit document code (within each
65
section), and three-digit item number (within each
document). For example, identifier 20200002 de-
notes the second sentence in the first file of PTB
Section 02, the classic Ms. Haag plays Elianti. The
annotation of this sentence is shown in Table 1.
With one exception, our fields (i.e. columns in
the tab-separated matrix) are a subset of the CoNLL
2009 inventory: (1) id, (2) form, (3) lemma, and
(4) pos characterize the current token, with token
identifiers starting from 1 within each sentence. Be-
sides the lemma and part-of-speech information, in
the closed track of our task, there is no explicit
analysis of syntax. Across the three target represen-
tations in the task, fields (1) and (2) are aligned and
uniform, i.e. all representations annotate exactly
the same text. On the other hand, fields (3) and (4)
are representation-specific, i.e. there are different
conventions for lemmatization, and part-of-speech
assignments can vary (but all representations use
the same PTB inventory of PoS tags).
The bi-lexical semantic dependency graph over
tokens is represented by two or more columns start-
ing with the obligatory, binary-valued fields (5)
top and (6) pred. A positive value in the top
column indicates that the node corresponding to
this token is a top node (see Section 2 below). The
pred column is a simplification of the correspond-
ing field in earlier tasks, indicating whether or not
this token represents a predicate, i.e. a node with
outgoing dependency edges. With these minor dif-
ferences to the CoNLL tradition, our file format can
represent general, directed graphs, with designated
top nodes. For example, there can be singleton
nodes not connected to other parts of the graph,
and in principle there can be multiple tops, or a
non-predicate top node.
To designate predicate?argument relations, there
are as many additional columns as there are pred-
icates in the graph (i.e. tokens marked + in the
pred column); these additional columns are called
(7) arg1, (8) arg2, etc. These colums contain
argument roles relative to the i-th predicate, i.e. a
non-empty value in column arg1 indicates that
the current token is an argument of the (linearly)
first predicate in the sentence. In this format, graph
reentrancies will lead to a token receiving argument
roles for multiple predicates (i.e. non-empty arg
i
values in the same row). All tokens of the same sen-
tence must always have all argument columns filled
in, even on non-predicate words; in other words,
all lines making up one block of tokens will have
the same number n of fields, but n can differ across
DM PAS PCEDT
(1) # labels 51 42 68
(2) % singletons 22.62 4.49 35.79
(3) # edge density 0.96 1.02 0.99
(4) %
g
trees 2.35 1.30 56.58
(5) %
g
projective 3.05 1.71 53.29
(6) %
g
fragmented 6.71 0.23 0.56
(7) %
n
reentrancies 27.35 29.40 9.27
(8) %
g
topless 0.28 0.02 0.00
(9) # top nodes 0.9972 0.9998 1.1237
(10) %
n
non-top roots 44.71 55.92 4.36
Table 2: Contrastive high-level graph statistics.
sentences, depending on the count of graph nodes.
4 Data Sets
All three target representations are annotations of
the same text, Sections 00?21 of the WSJ Cor-
pus. For this task, we have synchronized these
resources at the sentence and tokenization levels
and excluded from the SDP 2014 training and test-
ing data any sentences for which (a) one or more of
the treebanks lacked a gold-standard analysis; (b) a
one-to-one alignment of tokens could not be estab-
lished across all three representations; or (c) at least
one of the graphs was cyclic. Of the 43,746 sen-
tences in these 22 first sections of WSJ text, Deep-
Bank lacks analyses for close to 15%, and the Enju
Treebank has gaps for a little more than four per-
cent. Some 500 sentences show tokenization mis-
matches, most owing to DeepBank correcting PTB
idiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and
?U.S., .?, and introducing a few new ones (Fares
et al., 2013). Finally, 232 of the graphs obtained
through the above conversions were cyclic. In total,
we were left with 34,004 sentences (or 745,543
tokens) as training data (Sections 00?20), and 1348
testing sentences (29,808 tokens), from Section 21.
Quantitative Comparison As a first attempt at
contrasting our three target representations, Table 2
shows some high-level statistics of the graphs com-
prising the training data.
5
In terms of distinctions
5
These statistics are obtained using the ?official? SDP
toolkit. We refer to nodes that have neither incoming nor
outgoing edges and are not marked as top nodes as singletons;
these nodes are ignored in subsequent statistics, e.g. when
determining the proportion of edges per node (3) or the per-
centages of rooted trees (4) and fragmented graphs (6). The
notation ?%
n
? denotes (non-singleton) node percentages, and
?%
g
? percentages over all graphs. We consider a root node any
(non-singleton) node that has no incoming edges; reentrant
nodes have at least two incoming edges. Following Sagae and
Tsujii (2008), we consider a graph projective when there are
no crossing edges (in a left-to-right rendering of nodes) and no
roots are ?covered?, i.e. for any root j there is no edge i? k
66
Directed Undirected
DM PAS PCEDT DM PAS PCEDT
DM ? .6425 .2612 ? .6719 .5675
PAS .6688 ? .2963 .6993 ? .5490
PCEDT .2636 .2963 ? .5743 .5630 ?
Table 3: Pairwise F
1
similarities, including punctu-
ation (upper right diagonals) or not (lower left).
drawn in dependency labels (1), there are clear dif-
ferences between the representations, with PCEDT
appearing linguistically most fine-grained, and PAS
showing the smallest label inventory. Unattached
singleton nodes (2) in our setup correspond to
tokens analyzed as semantically vacuous, which
(as seen in Figure 1) include most punctuation
marks in PCEDT and DM, but not PAS. Further-
more, PCEDT (unlike the other two) analyzes some
high-frequency determiners as semantically vacu-
ous. Conversely, PAS on average has more edges
per (non-singleton) nodes than the other two (3),
which likely reflects its approach to the analysis of
functional words (see below).
Judging from both the percentage of actual trees
(4), the proportions of projective graphs (5), and the
proportions of reentrant nodes (7), PCEDT is much
more ?tree-oriented? than the other two, which at
least in part reflects its approach to the analysis
of modifiers and determiners (again, see below).
We view the small percentages of graphs without
at least one top node (8) and of graphs with at
least two non-singleton components that are not
interconnected (6) as tentative indicators of general
well-formedness. Intuitively, there should always
be a ?top? predicate, and the whole graph should
?hang together?. Only DM exhibits non-trivial (if
small) degrees of topless and fragmented graphs,
and these may indicate imperfections in the Deep-
Bank annotations or room for improvement in the
conversion from full MRSs to bi-lexical dependen-
cies, but possibly also exceptions to our intuitions
about semantic dependency graphs.
Finally, in Table 3 we seek to quantify pairwise
structural similarity between the three representa-
tions in terms of unlabeled dependency F
1
(dubbed
UF in Section 5 below). We provide four variants
of this metric, (a) taking into account the direc-
tionality of edges or not and (b) including edges
involving punctuation marks or not. On this view,
DM and PAS are structurally much closer to each
other than either of the two is to PCEDT, even more
such that i < j < k.
so when discarding punctuation. While relaxing
the comparison to ignore edge directionality also
increases similarity scores for this pair, the effect
is much more pronounced when comparing either
to PCEDT. This suggests that directionality of se-
mantic dependencies is a major source of diversion
between DM and PAS on the one hand, and PCEDT
on the other hand.
Linguistic Comparison Among other aspects,
Ivanova et al. (2012) categorize a range of syntac-
tic and semantic dependency annotation schemes
according to the role that functional elements take.
In Figure 1 and the discussion of Table 2 above, we
already observed that PAS differs from the other
representations in integrating into the graph aux-
iliaries, the infinitival marker, the case-marking
preposition introducing the argument of apply (to),
and most punctuation marks;
6
while these (and
other functional elements, e.g. complementizers)
are analyzed as semantically vacuous in DM and
PCEDT, they function as predicates in PAS, though
do not always serve as ?local? top nodes (i.e. the se-
mantic head of the corresponding sub-graph): For
example, the infinitival marker in Figure 1 takes the
verb as its argument, but the ?upstairs? predicate
impossible links directly to the verb, rather than to
the infinitival marker as an intermediate.
At the same time, DM and PAS pattern alike
in their approach to modifiers, e.g. attributive ad-
jectives, adverbs, and prepositional phrases. Un-
like in PCEDT (or common syntactic dependency
schemes), these are analyzed as semantic predi-
cates and, thus, contribute to higher degrees of
node reentrancy and non-top (structural) roots.
Roughly the same holds for determiners, but here
our PCEDT projection of Prague tectogrammatical
trees onto bi-lexical dependencies leaves ?vanilla?
articles (like a and the) as singleton nodes.
The analysis of coordination is distinct in the
three representations, as also evident in Figure 1.
By design, DM opts for what is often called
the Mel?
?
cukian analysis of coordinate structures
(Mel?
?
cuk, 1988), with a chain of dependencies
rooted at the first conjunct (which is thus consid-
ered the head, ?standing in? for the structure at
large); in the DM approach, coordinating conjunc-
tions are not integrated with the graph but rather
contribute different types of dependencies. In PAS,
the final coordinating conjunction is the head of the
6
In all formats, punctuation marks like dashes, colons, and
sometimes commas can be contentful, i.e. at times occur as
both predicates, arguments, and top nodes.
67
employee stock investment plans
compound compound compound
employee stock investment plans
ARG1
ARG1
ARG1
employee stock investment plans
ACT
PAT REG
Figure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .
structure and each coordinating conjunction (or in-
tervening punctuation mark that acts like one) is a
two-place predicate, taking left and right conjuncts
as its arguments. Conversely, in PCEDT the last
coordinating conjunction takes all conjuncts as its
arguments (in case there is no overt conjunction, a
punctuation mark is used instead); additional con-
junctions or punctuation marks are not connected
to the graph.
7
A linguistic difference between our representa-
tions that highlights variable granularities of anal-
ysis and, relatedly, diverging views on the scope
of the problem can be observed in Figure 2. Much
noun phrase?internal structure is not made explicit
in the PTB, and the Enju Treebank from which
our PAS representation derives predates the brack-
eting work of Vadas and Curran (2007). In the
four-way nominal compounding example of Fig-
ure 2, thus, PAS arrives at a strictly left-branching
tree, and there is no attempt at interpreting seman-
tic roles among the members of the compound ei-
ther; PCEDT, on the other hand, annotates both the
actual compound-internal bracketing and the as-
signment of roles, e.g. making stock the PAT(ient)
of investment. In this spirit, the PCEDT annota-
tions could be directly paraphrased along the lines
of plans by employees for investment in stocks. In
a middle position between the other two, DM dis-
ambiguates the bracketing but, by design, merely
assigns an underspecified, construction-specific de-
pendency type; its compound dependency, then,
is to be interpreted as the most general type of de-
pendency that can hold between the elements of
this construction (i.e. to a first approximation either
an argument role or a relation parallel to a prepo-
sition, as in the above paraphrase). The DM and
PCEDT annotations of this specific example hap-
pen to diverge in their bracketing decisions, where
the DM analysis corresponds to [...] investments
in stock for employees, i.e. grouping the concept
7
As detailed by Miyao et al. (2014), individual con-
juncts can be (and usually are) arguments of other predicates,
whereas the topmost conjunction only has incoming edges in
nested coordinate structures. Similarly, a ?shared? modifier of
the coordinate structure as a whole would take as its argument
the local top node of the coordination in DM or PAS (i.e. the
first conjunct or final conjunction, respectively), whereas it
would depend as an argument on all conjuncts in PCEDT.
employee stock (in contrast to ?common stock?).
Without context and expert knowledge, these de-
cisions are hard to call, and indeed there has been
much previous work seeking to identify and anno-
tate the relations that hold between members of a
nominal compound (see Nakov, 2013, for a recent
overview). To what degree the bracketing and role
disambiguation in this example are determined by
the linguistic signal (rather than by context and
world knowledge, say) can be debated, and thus the
observed differences among our representations in
this example relate to the classic contrast between
?sentence? (or ?conventional?) meaning, on the one
hand, and ?speaker? (or ?occasion?) meaning, on
the other hand (Quine, 1960; Grice, 1968). In
turn, we acknowledge different plausible points of
view about which level of semantic representation
should be the target representation for data-driven
parsing (i.e. structural analysis guided by the gram-
matical system), and which refinements like the
above could be construed as part of a subsequent
task of interpretation.
5 Task Setup
Training data for the task, providing all columns in
the file format sketched in Section 3 above, together
with a first version of the SDP toolkit?including
graph input, basic statistics, and scoring?were
released to candidate participants in early Decem-
ber 2013. In mid-January, a minor update to the
training data and optional syntactic ?companion?
analyses (see below) were provided, and in early
February the description and evaluation of a sim-
ple baseline system (using tree approximations and
the parser of Bohnet, 2010). Towards the end of
March, an input-only version of the test data was
released, with just columns (1) to (4) pre-filled; par-
ticipants then had one week to run their systems on
these inputs, fill in columns (5), (6), and upwards,
and submit their results (from up to two different
runs) for scoring. Upon completion of the testing
phase, we have shared the gold-standard test data,
official scores, and system results for all submis-
sions with participants and are currently preparing
all data for general release through the Linguistic
Data Consortium.
68
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Peking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05
Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42
Copenhagen-
80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01
Malm?
Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19
Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82
Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01
DM PAS PCEDT
LF LP LR LF LM LP LR LF LM LP LR LF LM
Priberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68
CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12
Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82
Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42
Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60
In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30
Table 4: Results of the closed (top) and open tracks (bottom). For each system, the second column (LF)
indicates the averaged LF score across all target representations), which was used to rank the systems.
Evaluation Systems participating in the task
were evaluated based on the accuracy with which
they can produce semantic dependency graphs for
previously unseen text, measured relative to the
gold-standard testing data. The key measures for
this evaluation were labeled and unlabeled preci-
sion and recall with respect to predicted dependen-
cies (predicate?role?argument triples) and labeled
and unlabeled exact match with respect to complete
graphs. In both contexts, identification of the top
node(s) of a graph was considered as the identifi-
cation of additional, ?virtual? dependencies from
an artificial root node (at position 0). Below we
abbreviate these metrics as (a) labeled precision,
recall, and F
1
: LP, LR, LF; (b) unlabeled precision,
recall, and F
1
: UP, UR, UF; and (c) labeled and
unlabeled exact match: LM, UM.
The ?official? ranking of participating systems, in
both the closed and the open tracks, is determined
based on the arithmetic mean of the labeled depen-
dency F
1
scores (i.e. the geometric mean of labeled
precision and labeled recall) on the three target rep-
resentations (DM, PAS, and PCEDT). Thus, to be
considered for the final ranking, a system had to
submit semantic dependencies for all three target
representations.
Closed vs. Open Tracks The task was sub-
divided into a closed track and an open track, where
systems in the closed track could only be trained
on the gold-standard semantic dependencies dis-
tributed for the task. Systems in the open track, on
the other hand, could use additional resources, such
as a syntactic parser, for example?provided that
they make sure to not use any tools or resources
that encompass knowledge of the gold-standard
syntactic or semantic analyses of the SDP 2014
test data, i.e. were directly or indirectly trained or
otherwise derived from WSJ Section 21.
This restriction implies that typical off-the-shelf
syntactic parsers had to be re-trained, as many data-
driven parsers for English include this section of
the PTB in their default training data. To simplify
participation in the open track, the organizers pre-
pared ready-to-use ?companion? syntactic analyses,
sentence- and token-aligned to the SDP data, in
two formats, viz. PTB-style phrase structure trees
obtained from the parser of Petrov et al. (2006) and
Stanford Basic syntactic dependencies (de Marn-
effe et al., 2006) produced by the parser of Bohnet
and Nivre (2012).
6 Submissions and Results
From 36 teams who had registered for the task,
test runs were submitted for nine systems. Each
team submitted one or two test runs per track. In
total, there were ten runs submitted to the closed
track and nine runs to the open track. Three teams
submitted to both the closed and the open track.
The main results are summarized and ranked in
Table 4. The ranking is based on the average LF
score across all three target representations, which
is given in the LF column. In cases where a team
submitted two runs to a track, only the highest-
ranked score is included in the table.
69
Team Track Approach Resources
Link?ping C extension of Eisner?s algorithm for DAGs, edge-factored
structured perceptron
?
Potsdam C & O graph-to-tree transformation, Mate companion
Priberam C & O model with second-order features, decoding with dual decom-
position, MIRA
companion
Turku O cascade of SVM classifiers (dependency recognition, label
classification, top recognition)
companion,
syntactic n-grams,
word2vec
Alpage C & O transition-based parsing for DAGs, logistic regression, struc-
tured perceptron
companion,
Brown clusters
Peking C transition-based parsing for DAGs, graph-to-tree transforma-
tion, parser ensemble
?
CMU O edge classification by logistic regression, edge-factored struc-
tured SVM
companion
Copenhagen-Malm? C graph-to-tree transformation, Mate ?
In-House O existing parsers developed by the organizers grammars
Table 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).
In the closed track, the average LF scores across
target representations range from 85.91 to 72.20.
Comparing the results for different target represen-
tations, the average LF scores across systems are
85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.
The scores for labeled exact match show a much
larger variation across both target representations
and systems.
8
In the open track, we see very similar trends.
The average LF scores across target representations
range from 86.27 to 75.89 and the corresponding
scores across systems are 88.64 for PAS, 84.95
for DM, and 67.52 for PCEDT. While these scores
are consistently higher than in the closed track,
the differences are small. In fact, for each of the
three teams that submitted to both tracks (Alpage,
Potsdam, and Priberam) improvements due to the
use of additional resources in the open track do not
exceed two points LF.
7 Overview of Approaches
Table 5 shows a summary of the systems that sub-
mitted final results. Most of the systems took
a strategy to use some algorithm to process (re-
stricted types of) graph structures, and apply ma-
chine learning like structured perceptrons. The
methods for processing graph structures are clas-
sified into three types. One is to transform graphs
into trees in the preprocessing stage, and apply con-
ventional dependency parsing systems (e.g. Mate;
Bohnet, 2010) to the converted trees. Some sys-
tems simply output the result of dependency pars-
ing (which means they inherently lose some depen-
8
Please see the task web page at the address indicated
above for full labeled and unlabeled scores.
dencies), while the others apply post-processing
to recover non-tree structures. The second strat-
egy is to use a parsing algorithm that can directly
generate graph structures (in the spirit of Sagae &
Tsujii, 2008; Titov et al., 2009). In many cases
such algorithms generate restricted types of graph
structures, but these restrictions appear feasible for
our target representations. The last approach is
more machine learning?oriented; they apply classi-
fiers or scoring methods (e.g. edge-factored scores),
and find the highest-scoring structures by some de-
coding method.
It is difficult to tell which approach is the best;
actually, the top three systems in the closed and
open tracks selected very different approaches. A
possible conclusion is that exploiting existing sys-
tems or techniques for dependency parsing was
successful; for example, Peking built an ensemble
of existing transition-based and graph-based depen-
dency parsers, and Priberam extended an existing
dependency parser. As we indicated in the task de-
scription, a novel feature of this task is that we have
to compute graph structures, and cannot assume
well-known properties like projectivity and lack of
reentrancies. However, many of the participants
found that our representations are mostly tree-like,
and this fact motivated them to apply methods that
have been well studied in the field of syntactic de-
pendency parsing.
Finally, we observe that three teams participated
in both the closed and open tracks, and all of them
reported that adding external resources improved
accuracy by a little more than one point. Systems
with (only) open submissions extensively use syn-
tactic features (e.g. dependency paths) from exter-
nal resources, and they are shown effective even
70
with simple machine learning models. Pre-existing,
tree-oriented dependency parsers are relatively ef-
fective, especially when combined with graph-to-
tree transformation. Comparing across our three
target representations, system scores show a ten-
dency PAS> DM> PCEDT, which can be taken as
a tentative indicator of relative levels of ?parsabil-
ity?. As suggested in Section 4, this variation most
likely correlates at least in part with diverging de-
sign decisions, e.g. the inclusion of relatively local
and deterministic dependencies involving function
words in PAS, or the decision to annotate contex-
tually determined speaker meaning (rather than
?mere? sentence meaning) in at least some construc-
tions in PCEDT.
8 Conclusions and Outlook
We have described the motivation, design, and out-
comes of the SDP 2014 task on semantic depen-
dency parsing, i.e. retrieving bi-lexical predicate?
argument relations between all content words
within an English sentence. We have converted to
a common format three existing annotations (DM,
PAS, and PCEDT) over the same text and have put
this to use for the first time in training and testing
data-driven semantic dependency parsers. Building
on strong community interest already to date and
our belief that graph-oriented dependency parsing
will further gain importance in the years to come,
we are preparing a similar (slightly modified) task
for SemEval 2015. Candidate modifications and
extensions will include cross-domain testing and
evaluation at the level of ?complete? predications
(in contrast to more lenient per-dependency F
1
used
this year). As optional new sub-tasks, we plan on
offering cross-linguistic variation and predicate (i.e.
semantic frame) disambiguation for at least some of
the target representations. To further probe the role
of syntax in the recovery of semantic dependency
relations, we will make available to participants
a wider selection of syntactic analyses, as well as
add a third (idealized) ?gold? track, where syntactic
dependencies are provided directly from available
syntactic annotations of the underlying treebanks.
Acknowledgements
We are grateful to ?eljko Agi
?
c and Bernd Bohnet
for consultation and assistance in preparing our
baseline and companion parses, to the Linguistic
Data Consortium (LDC) for support in distributing
the SDP data to participants, as well as to Emily M.
Bender and two anonymous reviewers for feedback
on this manuscript. Data preparation was supported
through access to the ABEL high-performance com-
puting facilities at the University of Oslo, and we
acknowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. Part of this
work has been supported by the infrastructural fund-
ing by the Ministry of Education, Youth and Sports
of the Czech Republic (CEP ID LM2010013).
References
Bohnet, B. (2010). Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (p. 89 ? 97). Beijing, China.
Bohnet, B., & Nivre, J. (2012). A transition-based
system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 1455 ? 1465). Jeju
Island, Korea.
Buchholz, S., & Marsi, E. (2006). CoNLL-X shared
task on multilingual dependency parsing. In Pro-
ceedings of the 10th Conference on Natural Lan-
guage Learning (p. 149 ? 164). New York, NY,
USA.
Copestake, A., Flickinger, D., Pollard, C., & Sag, I. A.
(2005). Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4),
281 ? 332.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D.
(2006). Generating typed dependency parses from
phrase structure parses. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 449 ? 454). Genoa, Italy.
Fares, M., Oepen, S., & Zhang, Y. (2013). Machine
learning for high-quality tokenization. Replicating
variable tokenization schemes. In Computational lin-
guistics and intelligent text processing (p. 231 ? 244).
Springer.
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1), 15 ? 28.
Flickinger, D., Zhang, Y., & Kordoni, V. (2012). Deep-
Bank. A dynamically annotated treebank of the Wall
Street Journal. In Proceedings of the 11th Interna-
tional Workshop on Treebanks and Linguistic Theo-
ries (p. 85 ? 96). Lisbon, Portugal: Edi??es Colibri.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling
of semantic roles. Computational Linguistics, 28,
71
245 ? 288.
Grice, H. P. (1968). Utterer?s meaning, sentence-
meaning, and word-meaning. Foundations of Lan-
guage, 4(3), 225 ? 242.
Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,
Mart?, M. A., M?rquez, L., . . . Zhang, Y. (2009).
The CoNLL-2009 Shared Task. syntactic and seman-
tic dependencies in multiple languages. In Proceed-
ings of the 13th Conference on Natural Language
Learning (p. 1 ? 18). Boulder, CO, USA.
Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,
O., Cinkov?, S., . . . ?abokrtsk?, Z. (2012). An-
nouncing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(p. 3153 ? 3160). Istanbul, Turkey.
Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.
(2012). Who did what to whom? A contrastive study
of syntacto-semantic dependencies. In Proceedings
of the Sixth Linguistic Annotation Workshop (p. 2 ?
11). Jeju, Republic of Korea.
Kate, R. J., & Wong, Y. W. (2010). Semantic pars-
ing. The task, the state of the art and the future. In
Tutorial abstracts of the 20th Meeting of the Associ-
ation for Computational Linguistics (p. 6). Uppsala,
Sweden.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A.
(1993). Building a large annotated corpora of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19, 313 ? 330.
Mel?
?
cuk, I. (1988). Dependency syntax. Theory and
practice. Albany, NY, USA: SUNY Press.
Meyers, A., Reeves, R., Macleod, C., Szekely, R.,
Zielinska, V., Young, B., & Grishman, R. (2004).
Annotating noun argument structure for NomBank.
In Proceedings of the 4th International Conference
on Language Resources and Evaluation (p. 803 ?
806). Lisbon, Portugal.
Miyao, Y., Oepen, S., & Zeman, D. (2014). In-house:
An ensemble of pre-existing off-the-shelf parsers. In
Proceedings of the 8th International Workshop on
Semantic Evaluation. Dublin, Ireland.
Nakov, P. (2013). On the interpretation of noun com-
pounds: Syntax, semantics, and entailment. Natural
Language Engineering, 19(3), 291 ? 330.
Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,
J., Riedel, S., & Yuret, D. (2007). The CoNLL 2007
shared task on dependency parsing. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Natural Language Learning (p. 915 ? 932). Prague,
Czech Republic.
Oepen, S., & L?nning, J. T. (2006). Discriminant-
based MRS banking. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (p. 1250 ? 1255). Genoa, Italy.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The
Proposition Bank. A corpus annotated with semantic
roles. Computational Linguistics, 31(1), 71 ? 106.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Meeting of the Association for Computational
Linguistics (p. 433 ? 440). Sydney, Australia.
Quine, W. V. O. (1960). Word and object. Cambridge,
MA, USA: MIT press.
Sagae, K., & Tsujii, J. (2008). Shift-reduce depen-
dency DAG parsing. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (p. 753 ? 760). Manchester, UK.
Titov, I., Henderson, J., Merlo, P., & Musillo, G.
(2009). Online graph planarisation for synchronous
parsing of semantic and syntactic dependencies. In
Proceedings of the 21st International Joint Confer-
ence on Artifical Intelligence (p. 1562 ? 1567).
Vadas, D., & Curran, J. (2007). Adding Noun Phrase
Structure to the Penn Treebank. In Proceedings of
the 45th Meeting of the Association for Computa-
tional Linguistics (p. 240 ? 247). Prague, Czech Re-
public.
72
Proceedings of the 3rd Workshop on the People?s Web Meets NLP, ACL 2012, pages 34?43,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting Context-Rich Entailment Rules from Wikipedia Revision History
Elena Cabrio
INRIA
2004, route de Lucioles BP93
06902 Sophia Antipolis, France.
elena.cabrio@inria.fr
Bernardo Magnini
FBK
Via Sommarive 18
38100 Povo-Trento, Italy.
magnini@fbk.eu
Angelina Ivanova
University of Oslo
Gaustadalle?en 23B
Ole-Johan Dahls hus
N-0373 Oslo, Norway.
angelii@ifi.uio.no
Abstract
Recent work on Textual Entailment has shown
a crucial role of knowledge to support entail-
ment inferences. However, it has also been
demonstrated that currently available entail-
ment rules are still far from being optimal. We
propose a methodology for the automatic ac-
quisition of large scale context-rich entailment
rules from Wikipedia revisions, taking advan-
tage of the syntactic structure of entailment
pairs to define the more appropriate linguis-
tic constraints for the rule to be successfully
applicable. We report on rule acquisition ex-
periments on Wikipedia, showing that it en-
ables the creation of an innovative (i.e. ac-
quired rules are not present in other available
resources) and good quality rule repository.
1 Introduction
Entailment rules have been introduced to provide
pieces of knowledge that may support entailment
judgments (Dagan et al, 2009) with some degree of
confidence. More specifically, an entailment rule is
defined (Szpektor et al, 2007) as a directional rela-
tion between two sides of a pattern, corresponding
to text fragments with variables (typically phrases
or parse sub-trees). The left-hand side (LHS) of
the pattern entails the right-hand side (RHS) of the
same pattern under the same variable instantiation.
Given the Text-Hypothesis pair (T-H) in Example 1:
Example 1.
T: Dr. Thomas Bond established a hospital in Philadel-
phia for the reception and cure of poor sick persons.
H: Dr. Bond created a medical institution for sick people.
a (directional) lexical rule like:
1) LHS: hospital? RHS: medical institution
probability: 0.8
brings to a TE system (aimed at recognizing that
a particular target meaning can be inferred from
different text variants in several NLP application,
e.g. Question Answering or Information Extraction)
the knowledge that the word hospital in Text can
be aligned, or transformed, into the word medical
institution in the Hypothesis, with a probability 0.8
that this operation preserves the entailment relation
among T and H. Similar considerations apply for
more complex rules involving verbs, as:
2) LHS: X establish Y ? RHS: X create Y
probability: 0.8
where the variables may be instantiated by any tex-
tual element with a specified syntactic relation with
the verb. Both kinds of rules are typically ac-
quired either from structured sources (e.g. WordNet
(Fellbaum, 1998)), or from unstructured sources ac-
cording for instance to distributional properties (e.g.
DIRT (Lin and Pantel, 2001)). Entailment rules
should typically be applied only in specific contexts,
defined in (Szpektor et al, 2007) as relevant con-
texts. Some existing paraphrase and entailment ac-
quisition algorithms add constraints to the learned
rules (e.g. (Sekine, 2005), (Callison-Burch, 2008)),
but most do not. Because of a lack of an adequate
representation of the linguistic context in which the
34
rules can be successfully applied, their concrete use
reflects this limitation. For instance, rule 2 (ex-
tracted from DIRT) fails if applied to ?The mathe-
matician established the validity of the conjecture?,
where the sense of establish is not a synonym of
create (but of prove, demonstrate), decreasing sys-
tem?s precision. Moreover, these rules often suffer
from lack of directionality, and from low accuracy
(i.e. the strength of association of the two sides of
the rule is often weak, and not well defined). Such
observations are also in line with the discussion on
ablation tests carried out at the last RTE evaluation
campaigns (Bentivogli et al, 2010).
Additional constraints specifying the variable
types are therefore required to correctly instantiate
them. In this work, we propose to take advantage
of Collaboratively Constructed Semantic Resources
(CSRs) (namely, Wikipedia) to mine information
useful to context-rich entailment rule acquisition.
More specifically, we take advantage of material ob-
tained through Wikipedia revisions, which provides
at the same time real textual variations from which
we may extrapolate the relevant syntactic context,
and several simplifications with respect to alterna-
tive resources. We consider T-H pairs where T is a
revision of a Wikipedia sentence and H is the origi-
nal sentence, as the revision is considered more in-
formative then the revised sentence.
We demonstrate the feasibility of the proposed
approach for the acquisition of context-rich rules
from Wikipedia revision pairs, focusing on two case
studies, i.e. the acquisition of entailment rules for
causality and for temporal expressions. Both phe-
nomena are highly frequent in TE pairs, and for both
there are no available resources yet. The result of
our experiments consists in a repository that can be
used by TE systems, and that can be easily extended
to entailment rules for other phenomena.
The paper is organized as follows. Section 2
reports on previous work, highlighting the speci-
ficity of our work. Section 3 motivates and de-
scribes the general principles underlying our ac-
quisition methodology. Section 4 describes in de-
tails the steps for context-rich rules acquisition from
Wikipedia pairs. Section 5 reports about the experi-
ments on causality and temporal expressions and the
obtained results. Finally, Section 6 concludes the pa-
per and suggests directions for future improvements.
2 Related work
The use of Wikipedia revision history in NLP tasks
has been previously investigated by a few works.
In (Zanzotto and Pennacchiotti, 2010), two versions
of Wikipedia and semi-supervised machine learning
methods are used to extract large TE data sets sim-
ilar to the ones provided for the RTE challenges.
(Yatskar et al, 2010) focus on using edit histories
in Simple English Wikipedia to extract lexical sim-
plifications. Nelken and Yamangil (2008) compare
different versions of the same document to collect
users? editorial choices, for automated text correc-
tion, sentence compression and text summarization
systems. (Max and Wisniewski, 2010) use the revi-
sion history of French Wikipedia to create a corpus
of natural rewritings, including spelling corrections,
reformulations, and other local text transformations.
In (Dutrey et al, 2011), a subpart of this corpus is
analyzed to define a typology of local modifications.
Because of its high coverage, Wikipedia is used
by the TE community for lexical-semantic rules ac-
quisition, named entity recognition, geographical in-
formation1 (e.g. (Mehdad et al, 2009), (Mirkin et
al., 2009), (Iftene and Moruz, 2010)), i.e. to provide
TE systems with world and background knowledge.
However, so far it has only been used as source of
factual knowledge, while in our work the focus is on
the acquisition of more complex rules, concerning
for instance spatial or temporal expressions.
The interest of the research community in produc-
ing specific methods to collect inference and para-
phrase pairs is proven by a number of works in the
field, which are relevant to the proposed approach.
As for paraphrase, Sekine?s Paraphrase Database
(Sekine, 2005) is collected using an unsupervised
method, and focuses on phrases connecting two
Named Entities. In the Microsoft Research Para-
phrase Corpus2, pairs of sentences are extracted
from news sources on the web, and manually an-
notated. As for rule repositories collected using dis-
tributional properties, DIRT (Discovery of Inference
Rules from Text)3 is a collection of inference rules
1http://www.aclweb.org/aclwiki/index.
php?title=RTE_Knowledge_Resources
2http://research.microsoft.com/en-us/
downloads
3http://www.aclweb.org/aclwiki/index.
php?title=DIRT_Paraphrase_Collection
35
(Lin and Pantel, 2001), obtained extracting binary
relations between a verb and an object-noun (or a
small clause) from dependency trees. Barzilay and
Lee (2003) present an approach for generating sen-
tence level paraphrases, learning structurally simi-
lar patterns of expression from data and identifying
paraphrasing pairs among them using a comparable
corpus. Since the data sets cited so far are para-
phrase collections, rules are bidirectional, while one
of the peculiarities of the entailment relation is the
directionality, addressed in our work.
Aharon et al (2010) presented FRED, an algo-
rithm for generating entailment rules between pred-
icates from FrameNet. Moreover, the TEASE col-
lection of entailment rules (Szpektor et al, 2004)
consists of 136 templates provided as input, plus
all the learned templates. Their web-based extrac-
tion algorithm is applied to acquire verb-based ex-
pressions. No directionality of the pairs is specified,
but additional guessing mechanisms it are proposed.
In (Szpektor and Dagan, 2008), two approaches for
unsupervised learning of unary rules (i.e. between
templates with a single variable) are investigated.
In (Zhao et al, 2009), a pivot approach for ex-
tracting paraphrase patterns from bilingual paral-
lel corpora is presented, while in (Callison-Burch,
2008) the quality of paraphrase extraction from par-
allel corpora is improved by requiring that phrases
and their paraphrases have the same syntactic type.
Our approach is different from theirs in many re-
spects: their goal is paraphrase extraction, while we
are extracting directional entailment rules; as textual
resources for pattern extraction they use parallel cor-
pora (using patterns in another language as pivots),
while we rely on monolingual Wikipedia revisions
(taking benefit from its increasing size); the para-
phrases they extract are more similar to DIRT, while
our approach allows to focus on the acquisition of
rules for specific phenomena frequent in entailment
pairs, and not covered by other resources.
3 General methodology
The general approach we have implemented is based
on the idea that, given a seed word, we extract all
the entailment rules from Wikipedia revision pairs
where the seed word appears as the head of the rule
either in T or H. The head is the non-variable part
of the rule on which the other parts depend (i.e. the
word establish is the head of rule 2).
Entailment judgment. A Wikipedia revision may
be consistent with the original sentence, bringing to
an entailment relation, or it may introduce inconsis-
tency, expressing a contradiction w.r.t. the original
sentence. We manually checked a sample of revision
pairs (?200), and we found out that in about 95%
of the revisions entailment is preserved, in line with
(Zanzotto and Pennacchiotti, 2010). We assume this
one as the default case in our experiments.
Monothematic pairs. The capability of automatic
extraction of entailment rules is affected by the com-
plexity of the pairs from which we extract the rules.
In our experiments we take advantage of revision
pairs with minimal difference between T and H, and
we assume that for such pairs we have only one rule
to extract. Under this perspective, T-H pairs derived
from Wikipedia revisions have strong similarity with
monothematic pairs (i.e. pairs where the entailment
judgment is due to only one linguistic phenomenon,
as suggested in (Bentivogli et al, 2010)). Section
4.2 describes the algorithm for filtering out revision
pairs with more than one phenomenon.
Directionality. A Wikipedia revision, in principle,
may be interpreted as either T entailing H, or as H
entailing T. However, through a manual inspection
of a revision sample (?200 pairs), it came out that
in most of the cases the meaning of the revised sen-
tence (T) entails the meaning of the original one (H).
Given such observation, for our experiments (Sec-
tions 4 and 5) we assume that for all revision pairs,
the revised sentence (T) entails the original one (H).
Context of a rule. We have defined the notion of
context of a rule R as a set of morpho-syntactic con-
straints C over the application of R in a specific T-H
pair. Ideally, the set of such constraints should be
the minimal set of constraints over R such that the
proportion of successful applications of R is max-
imized (e.g. the precision-recall mean is highest).
Intuitively, given an entailment rule, in absence of
constraints we have the highest recall (the rule is al-
ways applied when the LHS is activated in T and
the RHS is activated in H), although we may find
cases of wrong application of the rule (i.e. low preci-
sion). On the other side, as syntactic constraints are
36
required (e.g. the subject of a verb has to be a noun)
the number of successful applications increases, al-
though we may find cases where the constraints pre-
vent the correct application (e.g. low recall).
In the absence of a data set where we can em-
pirically estimate precision and recall of rule appli-
cation, we have approximated the ideal context on
the basis of linguistic intuitions. More specifically,
for different syntactic heads of the rules, we define
the most appropriate syntactic constraints through a
search algorithm over the syntactic tree produced on
T and H (see Section 4.4 for a detailed explanation).
4 Entailment rules acquisition
In the next sections, the steps for the acquisition of
rules from Wikipedia pairs are described in detail.
4.1 Step 1: preprocessing Wikipedia dumps
We downloaded two dumps of the English
Wikipedia (one dated 6.03.2009, Wiki 09, and
one dated 12.03.2010, Wiki 10).4 We used the
script WikiExtractor.py5 to extract plain text from
Wikipedia pages, discarding any other information
or annotation, but keeping the reference to the orig-
inal document. For our goal, we consider only non-
identical documents present in both Wiki 09 and Wiki
10 (i.e. 1,540,870 documents).
4.2 Step 2: extraction of entailment pairs
For both Wiki 09 and Wiki 10 each document has
been sentence-splitted, and the sentences of the two
versions have been aligned to create pairs. To mea-
sure the similarity between the sentences in each
pair, we adopted the Position Independent Word Er-
ror Rate (PER) (Tillmann et al, 1997), a metric
based on the calculation of the number of words
which differ between a pair of sentences (diff func-
tion in (1)). Such measure is based on Levenshtein
distance, but works at word level, and allows for re-
ordering of words and sequences of words between
the two texts (e.g. a translated text s and a reference
translation r). It is expressed by the formula:
PER(s, r) = diff(s,r)+diff(r,s)?r? (1)
4http://en.wikipedia.org/wiki/Wikipedia:
Database_download
5http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
Pairs are clustered according to different thresholds:
? Pairs composed by identical sentences were
discarded; if only one word was different in the
two sentences, we checked if it was a typo cor-
rection using (Damerau, 1964) distance. If that
was the case, we discarded such pairs as well.
? Pairs in which one of the sentences contains the
other one, meaning that the users added some
information to the new version, without modi-
fying the old one (set a: 1,547,415 pairs).
? Pairs composed by very similar sentences,
where users carried out minor editing (PER <
0.2) (set b: 1,053,114 pairs). We filtered out
pairs where differences were correction of mis-
spelling and typos, and two-word sentences.
? Pairs composed by sentences where major edit-
ing was carried out (0.2 < PER < 0.6), but still
describe the same event (set c: 2,566,364).
? Pairs in which the similarity between sentences
is low (PER > 0.6) were discarded.
To extract entailment rules, we consider only the
pairs contained in set b. For each pair, we intuitively
set the sentence extracted from Wiki 10 as the Text,
since we assume that it contains more (and more
precise) information w.r.t. the sentence extracted
from Wiki 09. We set the sentence extracted from
Wiki 09 as the Hypothesis (see Examples 2 and 3).
Example 2.
T: The Oxford Companion to Philosophy says ?there is
no single defining position that all anarchists hold [...]?
H: According to the Oxford Companion to Philosophy
?there is no single defining position that all anarchists
hold [...] ?
Example 3.
T: Bicycles are used by all socio-economic groups be-
cause of their convenience [...].
H: Bicycles are used by all socio-economic groups due to
their convenience [...].
4.3 Step 3: extraction of entailment rules
Pairs in set b are collected in a data set, and pro-
cessed with the Stanford parser (Klein and Manning,
37
2003); chunks are extracted from each pair using
the script chunklink.pl.6 The assumption underlying
our approach is that the difference between T and
H (i.e. the editing made by the user on a specific
structure) can be extracted from such pairs and
identified as an entailment rule. The rule extraction
algorithm was implemented to this purpose. In
details, for each sentence pair the algorithm itera-
tively compares the chunks of T and H to extract
the ones that differ. It can be the case that several
chunks of H are identical to a given chunk of T, as in:
T:<NP>[The DT][Oxford NNP][Companion NNP]
</NP><PP>[to TO]</PP> <NP>[Philosophy NNP]
</NP><VP>[says VBZ]</VP>...
H:<PP>[According VBG]</PP><PP>[to TO]</PP>
<NP>[the DT][Oxford NNP][Companion NNP]</NP>
<PP>[to TO]</PP><NP>[Philosophy NNP]</NP>...
Therefore, to decide for instance which chunk
<PP>[to TO]</PP> from H corresponds to the
identical chunk in T, the algorithm checks if the
previous chunks are equal as well. If this is the
case, such chunks are matched. In the example
above, the second chunk <PP>to</PP> from H
is considered as a good match because previous
chunks in T and H are equal as well (<NP>the
Oxford Companion</NP>). If the previous
chunks in T and H are not equal, the algorithm
keeps on searching. If such match is not found, the
algorithm goes back to the first matching chunk
and couples the chunk from T with it. Rules are
created setting the unmatched chunks from T as
the left-hand side of the rule, and the unmatched
chunks from H as the right-hand side of the rule.
Two consecutive chunks (different in T and H) are
considered part of the same rule. For instance, from
Examples 2 and 3:
2) <LHS> says </LHS>
<RHS> according to </RHS>
3) <LHS> because of </LHS>
<RHS> due to </RHS>
On the contrary, two non consecutive chunks gener-
ate two different entailment rules.
6http://ilk.uvt.nl/team/sabine/
chunklink/README.html
4.4 Step 4: rule expansion with minimal
context
As introduced before, our work aims at providing
precise and context-rich entailment rules, to maxi-
mize their correct application to RTE pairs. So far,
rules extracted by the rule extraction algorithm (Sec-
tion 4.3) are too general with respect to our goal.
To add the minimum context to each rule (as dis-
cussed in Section 3), we implemented a rule expan-
sion algorithm: both the file with the syntactic rep-
resentation of the pairs (obtained with the Stanford
parser), and the file with the rules extracted at Step 3
are provided as input. For every pair, and separately
for T and H, the words isolated in the corresponding
rule are matched in the syntactic tree of that sen-
tence, and the common subsumer node is detected.
Different strategies are applied to expand the rule,
according to linguistic criteria. In details, if the
common subsumer node is i) a Noun Phrase (NP)
node, the rule is left as it is; ii) a Prepositional
Phrase node (PP), all the terminal nodes of the
subtree below PP are extracted; iii) a clause intro-
duced by a subordinating conjunction (SBAR), all
the terminal nodes of the subtree below SBAR are
extracted; iv) an adjectival node (ADJP), all the
terminal nodes of the tree below the ADJP node
are extracted; v) a Verbal Phrase node (VP), the
dependency tree under the VP node is extracted.
For Example 3 (see Figure 1), the LHS of the rule
because of is matched in the syntactic tree of T and
the prepositional phrase (PP) is identified as com-
mon subsumer node. All the terminal nodes and the
PoS of the tree below PP are then extracted. The
same is done for the RHS of the rule, where the com-
mon subsumer node is an adjectival phrase (ADJP).
5 Experiments and results
In the previous section, we described the steps
carried out to acquire context-rich entailment rules
from Wikipedia revisions. To show the applicability
of the adopted methodology, we have performed
two experiments focusing, respectively, on entail-
ment rules for causality and temporal expressions.
In particular, as case studies we chose two seeds:
the conjunction because to derive rules for causality,
and the preposition before for temporal expressions.
38
(a) LHS rule (b) RHS rule
Figure 1: Rule expansion with minimal context (Example 3)
causality (because) temporal exp. (before)
(PP(RB because)(IN of)(NP(JJ)(NNS))? (SBAR(IN before)(S))?
(ADJP(JJ due)(PP(TO to)(NP(JJ)(NNS)))) (ADVP(RB prior)(PP(TO to)(S)
e.g.: because of contractual conflicts ? due to contractual conflicts e.g.: before recording them ? prior to recording them
(SBAR(IN because)(S))? (VP(PP(IN on)(NP(DT the) (ADVP(RB prior)(PP(TO to)(NP(DT)(NN))))?
(NNS grounds)))(SBAR (IN that)(S) (SBAR(IN before)(NP(DT)(NN)))
e.g.: because it penalized people ? on the grounds that it penalized people e.g.: prior to the crash ? before the crash
(PP(RB because)(IN of)(NP(DT)(NN)))? (PP(IN as)(NP (SBAR(IN until)(NP(CD)))?
(NP(DT a)(NN result))(PP(IN of)(NP(DT)(NN))))) (SBAR(IN before)(NP(CD)))
e.g.: because of an investigation ? as a result of an investigation e.g.: until 1819 ? before 1819
Table 1: Sample of extracted entailment rules.
Accordingly, we extracted from set b only the pairs
containing one of these two seeds (either in T or
in H) and we built two separate data sets for our
experiments. We run the rule extraction algorithm,
and then we filtered again the rules acquired, to
collect only those containing one of the two seeds
(either in the LHS or in the RHS). This second
filtering has been done because there could be pairs
in which either because or before are present, but
the differences in T and H do not concern those
seeds. The algorithm for rule expansion has then
been applied to the selected rules to add the minimal
context. The resulting rule for Example 3 is:
<rule ruleid="23" docid="844" pairid="15">
<LHS> (PP
(RB 8 because) (IN 9 of)(NP
(PRP 10 their)
(NN 11 convenience))) </LHS>
<RHS> (ADJP
(JJ 8 due)(PP
(TO 9 to) (NP
(PRP 10 their)
(NN 11 convenience)))) </RHS>
</rule>
To create entailment rules balancing high-
precision with their recall (Section 3), when the
words of the context added to the rule in Step 4
are identical we substitute them with their PoS. For
Example 3, the rule is generalized as follows:
<rule ruleid="23" docid="844" pairid="15">
<LHS> (PP
(RB because) (IN of)(NP
(PRP)
(NN))) </LHS>
<RHS> (ADJP
(JJ due)(PP
(TO to) (NP
(PRP)
(NN)))) </RHS>
</rule>
The intuition underlying the generalization phase is
to allow a more frequent application of the rule,
while keeping some constraints on the allowed con-
text. The application of the rule from Example 3 is
39
allowed if the subtrees below the seed words are the
same (the rule can be applied in another T-H pair as,
e.g. because of his status? due to his status).
Contradictions (e.g. antonyms and semantic op-
positions) are generally very infrequent, but in cer-
tain cases they can have high impact (one of the most
frequent rule collected for temporal expression is be-
fore S? after S). For this reason, we used WordNet
(Fellbaum, 1998) to identify and filter antonyms out
during the generalization phase. We also checked
for awkward inconsistencies due to mistakes of the
algorithm on noisy Wikipedia data (e.g. rules with
the same seed word in both the LHS and the RHS),
and we automatically filtered them out. Table 1 re-
ports a sample of rules extracted for each seed word.
Statistics about the resulting data sets, i.e. the num-
ber of acquired rules both before and after the gener-
alization phase are shown in Table 2. Identical rules
are collapsed into a unique one, but the value of their
frequency is kept in the header of that rule. Such in-
dex can then be used to estimate the correctness of
the rule and, according to our intuition, the probabil-
ity that the rule preserves the entailment relation.7
causality temporal exp.
# rules before gen. 1671 813
# rules after gen. 977 457
rules frequency ? 2 66 27
Table 2: Resulting sets of entailment rules
5.1 Evaluation
Due to the sparseness of the phenomena under con-
sideration (i.e. causality and temporal expressions)
in RTE data sets, evaluating the acquired rules on
such data does not provide interesting results.
For this reason, (following (Zhao et al, 2009),
(Callison-Burch, 2008), (Szpektor et al, 2004)), we
opted for a manual analysis of a sample of 100
rules per set, including all the rules whose fre-
quency is ?2 (Table 2), plus a random set of rules
with frequency equal to 1. Two annotators with
skills in linguistics annotated such rules according
7It is difficult to compare our results with related work, since
such phenomena are not covered by other resources. The cor-
rect comparison would be with the subset of e.g. DIRT para-
phrases dealing with causality and temporal relations, if any.
to five possible values (rules have been presented
with the sentence pairs from which they have been
acquired): entailment=yes (YES), i.e. correctness of
the rule; entailment=more-phenomena (+PHEN), i.e.
the rule is correct, but more than one phenomenon
is involved, see Section 5.2; entailment=unknown
(UNK), i.e. there is no entailment between the LHS
and the RHS of the rule, often because the editing
changed the semantics of the proposition; entail-
ment=unknown:reverse entailment (REV), wrong
directionality, i.e. the RHS of the rule entails the
LHS; entailment=error (ERR), i.e. the rule is wrong,
either because the editing in Wiki10 was done to cor-
rect mistakes, or because the rule is not well-formed
due to mistakes produced by our algorithm.
The inter-annotator agreement has been calcu-
lated, counting when judges agree on the assigned
value. It amounts to 80% on the sample of rules
for causality, and to 77% on the sample of rules for
temporal expressions. The highest inter-annotator
agreement is for correct entailment rules, whereas
the lowest agreement rates are for unknown and er-
ror judgments. This is due to the fact that detecting
correct rules is straightforward, while it is less clear
whether to consider a wrong rule as well-formed but
with an unknown judgment, or to consider it as not
appropriate (i.e. error). Table 3 shows the outcomes
of the analysis of the two sets of rules, as resulting
after a reconciliation phase carried out by the an-
notators. Such results, provided both for the whole
samples8 and for the rules whose frequency is ?2
only, are discussed in the next section.
YES +PHEN UNK REV ERR
caus.
all 67 2 13 8 10
fr?2 80.3 0 16.7 1.5 1.5
temp.
all 36 6 23 7 28
fr?2 52 3.7 37 7.3 0
Table 3: Accuracy (%) of the extracted sets of rules.
5.2 Discussion and error analysis
Due to the amount of noisy data present in
Wikipedia, on average 19% of the collected rules
8We are aware of the fact that including all the most frequent
rules in the sample biases the results upwards, but our choice is
motivated by the fact that we aim at verifying that with redun-
dancy the accuracy is actually improved.
40
include editing done by the users for spelling and
typos corrections, or are just spam (Table 3). To dis-
card such cases, spell-checkers or dictionary-based
filters should be used to improve our filtering tech-
niques. Moreover, to select only reliable rules we
consider making use of their frequency in the data to
estimate the confidence that a certain rule maintains
the entailment. The accuracy of the rules occurring
more than once is indeed much higher than the ac-
curacy estimated on the whole sample. Also the per-
centage of incorrect rules is strongly reduced when
considering redundant rules. Our assumption about
the directionality of entailment rules extracted from
Wikipedia versions is also verified (less than 10% of
the rules per set are tagged as reverse-entailment).
However, since the acquisition procedure privi-
leges precision, only a few rules appear very fre-
quently (Table 2), and this can be due to the con-
straints defined for the context extraction. This fact
motivates also the lower precision of the rules for
temporal expressions, where 73% of the sample we
analyzed involved rules with frequency equal to 1.
Moreover, in most of the rules annotated as un-
known, the editing of Wiki10 changed the semantics
of the pair, e.g. before 1990 ? 1893, or when x
produced? because x produced. Further strategies
to empirically estimate precision and recall of rule
application should be experimented as future work.
Indeed, several rules appearing only once represent
correct rules, and should not be discarded a priori.
Finally, the idea of using only very similar pairs to
extract entailment rules is based on the assumption
that such rules should concern one phenomenon at a
time (Bentivogli et al, 2010). Despite the strategies
adopted to avoid multiple phenomena per rule, in
about 10% of the cases two phenomena (e.g lexical
and syntactic) are collapsed on consecutive tokens,
making it complex to separate them automatically:
e.g. in because of his divorce settlement cost? due
to the cost of his divorces settlement, the causative
(because of x? due to x) and the argument realiza-
tion (x cost? cost of x) rules should be separated.
6 Conclusion and future work
We have presented a methodology for the automatic
acquisition of entailment rules from Wikipedia re-
vision pairs. The main benefits are the follow-
ing: i) potential large-scale acquisition, given the in-
creasing size of Wikipedia revisions; ii) new cover-
age, because Wikipedia revisions contain linguistic
phenomena (e.g. causality, temporal expressions),
which are not covered by existing resources: as a
consequence, the coverage of current TE systems
can be significantly extended; iii) quality: we intro-
duce the notion of context of a rule as the minimal
set of syntactic features maximizing its successful
application, and we have implemented it as a search
over the syntactic representation of revision pairs.
Results obtained on two experimental acquisi-
tions on causality and temporal expressions (seeds
because and before) show both good quality and
coverage of the extracted rules. The obtained re-
sources9: i) cover entailment and paraphrasing as-
pects not represented in other similar sets of rules,
ii) can be easily extended by applying the algorithms
to automatically collect rules for other phenomena
relevant to inference; and iii) are periodically up-
dated, as Wikipedia revisions change continuously.
We consider such aspects as part of our future work.
These results encourage us to further improve the
approach, considering a number of directions. First,
we plan to improve our filtering techniques to ex-
clude revision pairs containing more than one phe-
nomenon considering the syntactic structure of the
sentence. Moreover, we are planning to carry out
more extended evaluations, according to two pos-
sible strategies: i) applying the instance-based ap-
proach (Szpektor et al, 2007) on the Penn Treebank
data (i.e. for each PTB sentence that contains the
LHS of an entailment rule from our set, a pair sen-
tence will be generated by replacing the LHS of the
rule with its RHS. Human judges will then judge
each pair); ii) integrating the extracted rules into
existing TE systems. However, this evaluation has
to be carefully designed, as the ablation tests car-
ried on at the RTE challenges show. In particular,
as RTE tasks are moving towards real applications
(e.g. summarization) we think that knowledge re-
flecting real textual variations produced by humans
(as opposed to knowledge derived from linguistic re-
sources) may introduce interesting and novel hints.
9Available at http://www.aclweb.org/aclwiki/
index.php?title=Textual_Entailment_
Resource_Pool. We encourage its integration into TE
systems, to obtain feedback on its utility in TE tasks.
41
Acknowledgments
This work has been partially supported by the EC-
funded project EXCITEMENT (FP7 ICT-287923).
References
Roni Ben Aharon, Idan Szpektor, Ido Dagan. 2010. Gen-
erating Entailment Rules from FrameNet. Proceedings
of the ACL 2010 Conference Short Papers. July 11-16.
Uppsala, Sweden.
Regina Barzilay, Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. Proceedings of the HLT-
NAACL. May 27-June 1. Edmonton, Canada.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T. Dang,
Danilo Giampiccolo. 2010. The Sixth PASCAL Rec-
ognizing Textual Entailment Challenge. Proceedings
of the TAC 2010 Workshop on TE. November 15-16.
Gaithersburg, Maryland.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. Proceedings of the Seventh
conference on International Language Resources and
Evaluation. May 19-21. Malta.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP2008) October
25-27. Honolulu, Hawaii.
Ido Dagan, Bill Dolan, Bernardo Magnini, Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing (JNLE). Special Issue 04, volume 15, i-xvii. Cam-
bridge University Press.
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7 (3), pages 171?176. ACM, New York, NY,
USA.
Camille Dutrey, Houda Bouamor, Delphine Bernhard and
Aurelien Max 2011. Local modifications and para-
phrases in Wikipedia?s revision history. SEPLN jour-
nal (Revista de Procesamiento del Lenguaje Natural),
46:51-58.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Language, Speech and Communi-
cation. MIT Press.
Adrian Iftene, Mihai-Alex Moruz. 2010. UAIC Partici-
pation at RTE-6. Proceedings of the TAC 2010 Work-
shop on TE. November 15-16. Gaithersburg, Mary-
land.
Dan Klein, Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics.
July 7-12. Sapporo, Japan.
Dekang Lin, Patrick Pantel. 2001. Discovery of Infer-
ence Rules for Question Answering. Natural Language
Engineering 7(4):343-360.
Rowan Nairn, Cleo Condoravdi, Lauri Karttunen. 2006.
Computing relative polarity for textual inference. In-
ference in Computational Semantics (ICoS-5). April
20-21. Buxton, UK.
Aurelien Max, Guillaume Wisniewski. 2010. Mining
naturally-occurring corrections and paraphrases from
wikipedia?s revision history. Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation. May 19-21. Valletta, Malta.
Yashar Mehdad, Matteo Negri, Elena Cabrio,
Milen Kouylekov, Bernardo Magnini. 2009. Using
Lexical Resources in a Distance-Based Approach to
RTE. Proceedings of the TAC 2009 Workshop on TE.
November 17. Gaithersburg, Maryland.
Shachar Mirkin, Roy Bar-Haim, Jonathan Beran, Ido Da-
gan, Eyal Shnarch, Asher Stern, Idan Szpektor. 2009.
Addressing Discourse and Document Structure in the
RTE Search Task. Proceedings of the TAC 2009 Work-
shop on TE. November 17. Gaithersburg, Maryland.
Rani Nelken, Elif Yamangil. 2008. Mining Wikipedia?s
Article Revision History for Training Computational
Linguistics Algorithms. Proceedings of the AAAI
Workshop on Wikipedia and Artificial Intelligence.
July 13-14, Chicago, Illinois.
Satoshi Sekine. 2005. Automatic Paraphrase Discovery
based on Context and Kwywords between NE Pairs.
Proceedings of the International Workshop on Para-
phrasing (IWP-05). October 14. Jeju Island, South
Korea.
Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition of
Entailment Relations. Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing. July 25-26. Barcelona, Spain.
Idan Szpektor, Ido Dagan. 2008. Learning Entailment
Rules for Unary Templates. Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008). August 18-22. Manchester, UK.
Idan Szpektor, Eyal Shnarch, Ido Dagan. 2007.
Instance-based Evaluation of Entailment Rule Acqui-
sition. Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics. June 23-
30. Prague, Czech Republic.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, Hassan Sawaf. 1997. Accelerated DP
based search for statistical translation. Proceedings
42
of the European Conf. on Speech Communication and
Technology, pages 26672670. September. Rhodes,
Greece.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, Lillian Lee. 2010. For the sake of simplicity:
Unsupervised extraction of lexical simplifications from
Wikipedia. Proceedings of the NAACL, pp. 365-368,
2010. Short paper. June 1-6. Los Angeles, USA.
Fabio Massimo Zanzotto, Marco Pennacchiotti. 2010.
Expanding textual entailment corpora from Wikipedia
using co-training. Proceedings of the COLING-
Workshop on The Peoples Web Meets NLP: Collabo-
ratively Constructed Semantic Resources. August 28.
Beijing, China.
Shiqi Zhao, Haifeng Wang, Ting Liu, Sheng Li. 2009.
Extracting Paraphrase Patterns from Bilingual Paral-
lel Corpora. Journal of Natural Language Engineer-
ing, 15 (4): 503:526.
43
