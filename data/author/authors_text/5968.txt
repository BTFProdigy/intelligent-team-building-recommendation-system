Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 907?914, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Semi-Supervised Feature Clustering Algorithm
with Application to Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
In this paper we investigate an applica-
tion of feature clustering for word sense
disambiguation, and propose a semi-
supervised feature clustering algorithm.
Compared with other feature clustering
methods (ex. supervised feature cluster-
ing), it can infer the distribution of class
labels over (unseen) features unavailable
in training data (labeled data) by the use of
the distribution of class labels over (seen)
features available in training data. Thus,
it can deal with both seen and unseen fea-
tures in feature clustering process. Our ex-
perimental results show that feature clus-
tering can aggressively reduce the dimen-
sionality of feature space, while still main-
taining state of the art sense disambigua-
tion accuracy. Furthermore, when com-
bined with a semi-supervised WSD algo-
rithm, semi-supervised feature clustering
outperforms other dimensionality reduc-
tion techniques, which indicates that using
unlabeled data in learning process helps to
improve the performance of feature clus-
tering and sense disambiguation.
1 Introduction
This paper deals with word sense disambiguation
(WSD) problem, which is to assign an appropriate
sense to an occurrence of a word in a given context.
Many corpus based statistical methods have been
proposed to solve this problem, including supervised
learning algorithms (Leacock et al, 1998; Towel and
Voorheest, 1998), weakly supervised learning algo-
rithms (Dagan and Itai, 1994; Li and Li, 2004; Mi-
halcea, 2004; Niu et al, 2005; Park et al, 2000;
Yarowsky, 1995), unsupervised learning algorithms
(or word sense discrimination) (Pedersen and Bruce,
1997; Schu?tze, 1998), and knowledge based algo-
rithms (Lesk, 1986; McCarthy et al, 2004).
In general, the most common approaches start by
evaluating the co-occurrence matrix of features ver-
sus contexts of instances of ambiguous word, given
sense-tagged training data for this target word. As
a result, contexts are usually represented in a high-
dimensional sparse feature space, which is far from
optimal for many classification algorithms. Further-
more, processing data lying in high-dimensional fea-
ture space requires large amount of memory and
CPU time, which limits the scalability of WSD
model to very large datasets or incorporation of
WSD model into natural language processing sys-
tems.
Standard dimentionality reduction techniques in-
clude (1) supervised feature selection and super-
vised feature clustering when given labeled data, (2)
unsupervised feature selection, latent semantic in-
dexing, and unsupervised feature clustering when
only unlabeled data is available. Supervised fea-
ture selection improves the performance of an ex-
amplar based learning algorithm over SENSEVAL-
2 data (Mihalcea, 2002), Naive Bayes and deci-
sion tree over SENSEVAL-1 and SENSEVAL-2 data
(Lee and Ng, 2002), but feature selection does not
improve SVM and Adaboost over SENSEVAL-1
and SENSEVAL-2 data (Lee and Ng, 2002) for
word sense disambiguation. Latent semantic in-
dexing (LSI) studied in (Schu?tze, 1998) improves
the performance of sense discrimination, while un-
supervised feature selection also improves the per-
formance of word sense discrimination (Niu et al,
2004). But little work is done on using feature clus-
tering to conduct dimensionality reduction for WSD.
This paper will describe an application of feature
907
clustering technique to WSD task.
Feature clustering has been extensively studied
for the benefit of text categorization and document
clustering. In the context of text categorization, su-
pervised feature clustering algorithms (Baker and
McCallum, 1998; Bekkerman et al, 2003; Slonim
and Tishby, 2001) usually cluster words into groups
based on the distribution of class labels over fea-
tures, which can compress the feature space much
more aggressively while still maintaining state of
the art classification accuracy. In the context of
document clustering, unsupervised feature cluster-
ing algorithms (Dhillon, 2001; Dhillon et al, 2002;
Dhillon et al, 2003; El-Yaniv and Souroujon, 2001;
Slonim and Tishby, 2000) perform word clustering
by the use of word-document co-occurrence matrix,
which can improve the performance of document
clustering by clustering documents over word clus-
ters.
Supervised feature clustering algorithm groups
features into clusters based on the distribution of
class labels over features. But it can not group un-
seen features (features that do not occur in labeled
data) into meaningful clusters since there are no
class labels associated with these unseen features.
On the other hand, while given labeled data, un-
supervised feature clustering method can not uti-
lize class label information to guide feature cluster-
ing procedure. While, as a promising classification
strategy, semi-supervised learning methods (Zhou et
al., 2003; Zhu and Ghahramani, 2002; Zhu et al,
2003) usually utilize all the features occurring in la-
beled data and unlabeled data. So in this paper we
propose a semi-supervised feature clustering algo-
rithm to overcome this problem. Firstly, we try to
induce class labels for unseen features based on the
similarity among seen features and unseen features.
Then all the features (including seen features and
unseen features) are clustered based on the distrib-
ution of class labels over them.
This paper is organized as follows. First, we
will formulate a feature clustering based WSD prob-
lem in section 2. Then in section 3 we will de-
scribe a semi-supervised feature clustering algo-
rithm. Section 4 will provide experimental results
of various dimensionality reduction techniques with
combination of state of the art WSD algorithms on
SENSEVAL-3 data. Section 5 will provide a review
of related work on feature clustering. Finally we will
conclude our work and suggest possible improve-
ment in section 6.
2 Problem Setup
Let X = {xi}ni=1 be a set of contexts of occur-
rences of an ambiguous word w, where xi repre-
sents the context of the i-th occurrence, and n is
the total number of this word?s occurrences. Let
S = {sj}cj=1 denote the sense tag set of w. The first
l examples xg(1 ? g ? l) are labeled as yg (yg ? S)
and other u (l+u = n) examples xh(l+1 ? h ? n)
are unlabeled. The goal is to predict the sense of w
in context xh by the use of label information of xg
and similarity information among examples in X .
We use F? to represent feature clustering result
into NF? clusters when F is a set of features. After
feature clustering, any context xi in X can be repre-
sented as a vector over feature clusters F? . Then we
can use supervised methods (ex. SVM) (Lee and
Ng, 2002) or semi-supervised methods (ex. label
propagation algorithm) (Niu et al, 2005) to perform
sense disambiguation on unlabeled instances of tar-
get word.
3 Semi-Supervised Feature Clustering
Algorithm
In supervised feature clustering process, F consists
of features occurring in the first l labeled examples,
which can be denoted as FL. But in the setting of
transductive learning, semi-supervised learning al-
gorithms will utilize not only the features in labeled
examples (FL), but also unseen features in unlabeled
examples (denoted as FL). FL consists of the fea-
tures that occur in unlabeled data, but never appear
in labeled data.
Supervised feature clustering algorithm usually
performs clustering analysis over feature-class ma-
trix, where each entry (i, j) in this matrix is the num-
ber of times of the i-th feature co-occurring with the
j-th class. Therefore it can not group features in FL
into meaningful clusters since there are no class la-
bels associated with these features. We overcome
this problem by firstly inducing class labels for un-
seen features based on the similarity among features
in FL and FL, then clustering all the features (in-
cluding FL and FL) based on the distribution of class
908
labels over them.
This semi-supervised feature clustering algorithm
is defined as follows:
Input:
Feature set F = FL
?FL (the first |FL| features
in F belong to FL, and the remaining |FL| features
belong to FL), context set X , the label information
of xg(1 ? g ? l), NF? (the number of clusters in F? );
Output:
Clustering solution F? ;
Algorithm:
1. Construct |F | ? |X| feature-example matrix
MF,X , where entry MF,Xi,j is the number of times of
fi co-occurring with example xj (1 ? j ? n).
2. Form |F | ? |F | affinity matrix W defined by
Wij = exp(?
d2ij
?2 ) if i 6= j and Wii = 0 (1 ?
i, j ? |F |), where dij is the distance (ex. Euclid-
ean distance) between fi (the i-th row in MF,X ) and
fj (the j-th row in MF,X ), and ? is used to control
the weight Wij .
3. Construct |FL| ? |S| feature-class matrix
Y FL,S , where the entry Y FL,Si,j is the number of
times of feature fi (fi ? FL) co-occurring with
sense sj .
4. Obtain hard label matrix for features in FL
(denoted as Y FL,Shard ) based on Y FL,S , where entry
Y F,Shard i,j = 1 if the hard label of fi is sj , otherwise
zero. Obtain hard labels for features in FL using
a classifier based on W and Y FL,Shard . In this paper
we use label propagation (LP) algorithm (Zhu and
Ghahramani, 2002) to get hard labels for FL.
5. Construct |F | ? |S| feature-class matrix Y F,Shard,
where entry Y F,Shard i,j = 1 if the hard label of fi is
sj , otherwise zero.
6. Construct the matrix L = D?1/2WD?1/2 in
which D is a diagonal matrix with its (i, i)-element
equal to the sum of the i-th row of W .
7. Label each feature in F as soft label Y? F,Si , the
i-th row of Y? F,S , where Y? F,S = (I ? ?L)?1Y F,Shard.
8. Obtain the feature clustering solution F? by
clustering the rows of Y? F,Si into NF? groups. In
this paper we use sequential information bottleneck
(sIB) algorithm (Slonim and Tishby, 2000) to per-
form clustering analysis.
End
Step 3 ? 5 are the process to obtain hard la-
bels for features in F , while the operation in step 6
and 7 is a local and global consistency based semi-
supervised learning (LGC) algorithm (Zhou et al,
2003) that smooth the classification result of LP al-
gorithm to acquire a soft label for each feature.
At first sight, this semi-supervised feature cluster-
ing algorithm seems to make little sense. Since we
run feature clustering in step 8, why not use LP algo-
rithm to obtain soft label matrix Y FL,S for features
in FL by the use of Y FL,S and W , then just apply
sIB directly to soft label matrix Y? F,S (constructed
by catenating Y FL,S and Y FL,S)?
The reason for using LGC algorithm to acquire
soft labels for features in F is that in the context
of transductive learning, the size of labeled data is
rather small, which is much less than that of un-
labeled data. This makes it difficult to obtain re-
liable estimation of class label?s distribution over
features from only labeled data. This motivates
us to use raw information (hard labels of features
in FL) from labeled data to estimate hard labels
of features in FL. Then LGC algorithm is used
to smooth the classification result of LP algorithm
based on the assumption that a good classification
should change slowly on the coherent structure ag-
gregated by a large amount of unlabeled data. This
operation makes our algorithm more robust to the
noise in feature-class matrix Y FL,S that is estimated
from labeled data.
In this paper, ? is set as the average distance be-
tween labeled examples from different classes, and
NF? = |F |/10. Latent semantic indexing technique
(LSI) is used to perform factor analysis in MF,X be-
fore calculating the distance between features in step
2.
4 Experiments and Results
4.1 Experiment Design
For empirical study of dimensionality reduction
techniques on WSD task, we evaluated five dimen-
sionality reduction algorithms on the data in English
lexical sample (ELS) task of SENSEVAL-3 (Mihal-
cea et al, 2004)(including all the 57 English words
) 1: supervised feature clustering (SuFC) (Baker and
McCallum, 1998; Bekkerman et al, 2003; Slonim
1Available at http://www.senseval.org/senseval3
909
and Tishby, 2001), iterative double clustering (IDC)
(El-Yaniv and Souroujon, 2001), semi-supervised
feature clustering (SemiFC) (our algorithm), super-
vised feature selection (SuFS) (Forman, 2003), and
latent semantic indexing (LSI) (Deerwester et. al.,
1990) 2.
We used sIB algorithm 3 to cluster features in
FL into groups based on the distribution of class la-
bels associated with each feature. This procedure
can be considered as our re-implementation of su-
pervised feature clustering. After feature clustering,
examples can be represented as vectors over feature
clusters.
IDC is an extension of double clustering method
(DC) (Slonim and Tishby, 2000), which performs it-
erations of DC. In the transductive version of IDC,
they cluster features in F as distributions over class
labels (given by the labeled data) during the first
stage of the IDC first iteration. This phase results in
feature clusters F? . Then they continue as usual; that
is, in the second phase of the first IDC iteration they
group X into NX? clusters, where X is represented
as distribution over F? . Subsequent IDC iterations
use all the unlabeled data. This IDC algorithm can
result in two clustering solutions: F? and X? . Follow-
ing (El-Yaniv and Souroujon, 2001), the number of
iterations is set as 15, and NX? = |S| (the number of
senses of target word) in our re-implementation of
IDC. After performing IDC, examples can be repre-
sented as vectors over feature clusters F? .
Supervised feature selection has been extensively
studied for text categorization task (Forman, 2003).
Information gain (IG) is one of state of the art cri-
teria for feature selection, which measures the de-
crease in entropy when the feature is given vs. ab-
sent. In this paper, we calculate IG score for each
feature in FL, then select top |F |/10 features with
highest scores to form reduced feature set. Then
examples can be represented as vectors over the re-
duced feature set.
LSI is an unsupervised factor analysis technique
based on Singular Value Decomposition of a |X| ?
|F | example-feature matrix. The underlying tech-
nique for LSI is to find an orthogonal basis for the
2Following (Baker and McCallum, 1998), we use LSI as a
representative method for unsupervised dimensionality reduc-
tion.
3Available at http://www.cs.huji.ac.il/?noamm/
feature-example space for which the axes lie along
the dimensions of maximum variance. After using
LSI on the example-feature matrix, we can get vec-
tor representation for each example in X in reduced
feature space.
For each ambiguous word in ELS task of
SENSEVAL-3, we used three types of features to
capture contextual information: part-of-speech of
neighboring words with position information, un-
ordered single words in topical context, and local
collocations (as same as the feature set used in (Lee
and Ng, 2002) except that we did not use syntactic
relations). We removed the features with occurrence
frequency (counted in both training set and test set)
less than 3 times.
We ran these five algorithms for each ambiguous
word to reduce the dimensionality of feature space
from |F | to |F |/10 no matter which training data is
used (ex. full SENSEVAL-3 training data or sam-
pled SENSEVAL-3 training data). Then we can ob-
tain new vector representation of X in new feature
space acquired by SuFC, IDC, SemiFC, and LSI or
reduced feature set by SuFS.
Then we used SVM 4 and LP algorithm to per-
form sense disambiguation on vectors in dimension-
ality reduced feature space. SVM and LP were eval-
uated using accuracy 5 (fine-grained score) on test
set of SENSEVAL-3. For LP algorithm, the test set
in SENSEVAL-3 data was also used as unlabeled
data in tranductive learning process.
We investigated two distance measures for LP: co-
sine similarity and Jensen-Shannon (JS) divergence
(Lin, 1991). Cosine similarity measures the angle
between two feature vectors, while JS divergence
measures the distance between two probability dis-
tributions if each feature vector is considered as
probability distribution over features.
For sense disambiguation on SENSEVAL-3 data,
we constructed connected graphs for LP algorithm
following (Niu et al, 2005): two instances u, v will
be connected by an edge if u is among v?s k nearest
neighbors, or if v is among u?s k nearest neighbors
4We used SV M light with linear kernel function, available
at http://svmlight.joachims.org/.
5If there are multiple sense tags for an instance in training
set or test set, then only the first tag is considered as correct
answer. Furthermore, if the answer of the instance in test set is
?U?, then this instance will be removed from test set.
910
as measured by cosine or JS distance measure. k is
5 in later experiments.
4.2 Experiments on Full SENSEVAL-3 Data
In this experiment, we took the training set in
SENSEVAL-3 as labeled data, and the test set as un-
labeled data. In other words, all of dimensionality
reduction methods and classifiers can use the label
information in training set, but can not access the
label information in test set. We evaluated differ-
ent sense disambiguation processes using test set in
SENSEVAL-3.
We use features with occurrence frequency no less
than 3 in training set and test set as feature set F for
each ambiguous word. F consists of two disjoint
subsets: FL and FL. FL consists of features occur-
ring in training set of target word in SENSEVAL-3,
while FL consists of features that occur in test set,
but never appear in training set.
Table 1 lists accuracies of SVM and LP
without or with dimensionality reduction on full
SENSEVAL-3 data. From this table, we have some
findings as follows:
(1) If without dimensionality reduction, the best
performance of sense disambiguation is 70.3%
(LPJS), while if using dimensionality reduction,
the best two systems can achieve 69.8% (SuFS +
LPJS) and 69.0% (SemiFC + LPJS) accuracies.
It seems that feature selection and feature clustering
can significantly reduce the dimensionality of fea-
ture space while losing only about 1.0% accuracy.
(2) Furthermore, LPJS algorithm performs bet-
ter than SVM when combined with the same dimen-
sionality reduction technique (except IDC). Notice
that LP algorithm uses unlabelled data during its dis-
ambiguation phase while SVM doesn?t. This indi-
cates that using unlabeled data helps to improve the
performance of sense disambiguation.
(3) When using LP algorithm for sense disam-
biguation, SemiFC performs better than other fea-
ture clustering algorithms, such as SuFC, IDC.
This indicates that clustering seen and unseen fea-
tures can satisfy the requirement of semi-supervised
learning algorithm, which does help the classifica-
tion process.
(4) When using SuFC, IDC, SuFS, or SemiFC for
dimensionality reduction, the performance of sense
disambiguation is always better than that using LSI
as dimensionality reduction method. SuFC, IDC,
SuFS, and SemiFC use label information to guide
feature clustering or feature selection, while LSI is
an unsupervised factor analysis method that can con-
duct dimensionality reduction without the use of la-
bel information from labeled data. This indicates
that using label information in dimensionality re-
duction procedure can cluster features into better
groups or select better feature subsets, which results
in better representation of contexts in reduced fea-
ture space.
4.3 Additional Experiments on Sampled
SENSEVAL-3 Data
For investigating the performance of various dimen-
sionality reduction techniques with very small train-
ing data, we ran them with only lw examples from
training set of each word in SENSEVAL-3 as la-
beled data. The remaining training examples and
all the test examples were used as unlabeled data
for SemiFC or LP algorithm. Finally we evaluated
different sense disambiguation processes using test
set in SENSEVAL-3. For each labeled set size lw,
we performed 20 trials. In each trial, we randomly
sampled lw labeled examples for each word from
training set. If any sense was absent from the sam-
pled labeled set, we redid the sampling. lw is set as
Nw,train ? 10%, where Nw,train is the number of
examples in training set of word w. Other settings
of this experiment is as same as that of previous one
in section 4.2.
In this experiment, feature set F is as same as that
in section 4.2. FL consists of features occurring in
sampled training set of target word in SENSEVAL-
3, while FL consists of features that occur in unla-
beled data (including unselected training data and all
the test set), but never appear in labeled data (sam-
pled training set).
Table 2 lists accuracies of SVM and LP with-
out or with dimensionality reduction on sampled
SENSEVAL-3 training data 6. From this table, we
have some findings as follows:
(1) If without dimensionality reduction, the best
performance of sense disambiguation is 54.9%
(LPJS), while if using dimensionality reduction, the
6We can not obtain the results of IDC over 20 trials since it
costs about 50 hours for each trial (Pentium 1.4 GHz CPU/1.0
GB memory).
911
Table 1: This table lists the accuracies of SVM and LP without or with dimensionality reduction on full
SENSEVAL-3 data. There is no result for LSI + LPJS , since the vectors obtained by LSI may contain
negative values, which prohibits the application of JS divergence for measuring the distance between these
vectors.
Without With various dimensionality
dimensionality reduction techniques
Classifier reduction SuFC IDC SuFS LSI SemiFC
SVM 69.7% 66.4% 65.1% 65.2% 59.1% 64.0%
LPcosine 68.4% 66.7% 64.9% 66.0% 60.7% 67.6%
LPJS 70.3% 67.2% 64.0% 69.8% - 69.0%
Table 2: This table lists the accuracies of SVM and LP without or with dimensionality reduction on sam-
pled SENSEVAL-3 training data. For each classifier, we performed paired t-test between the system using
SemiFC for dimensionality reduction and any other system with or without dimensionality reduction. ? (or
?) means p-value ? 0.01, while > (or <) means p-value falling into (0.01, 0.05]. Both ? (or ?) and >
(or <) indicate that the performance of current WSD system is significantly better (or worse) than that using
SemiFC for dimensionality reduction, when given same classifier.
Without With various dimensionality
dimensionality reduction techniques
Classifier reduction SuFC SuFS LSI SemiFC
SVM 53.4?1.1% (?) 50.4?1.1% (?) 52.2?1.2% (>) 49.8?0.8% (?) 51.5?1.0%
LPcosine 54.4?1.2% (?) 49.5?1.1% (?) 51.1?1.0% (?) 49.8?1.0% (?) 52.9?1.0%
LPJS 54.9?1.1% (?) 52.0?0.9% (?) 52.5?1.0% (?) - 54.1?1.2%
best performance of sense disambiguation is 54.1%
(SemiFC + LPJS). Feature clustering can signif-
icantly reduce the dimensionality of feature space
while losing only 0.8% accuracy.
(2) LPJS algorithm performs better than SVM
when combined with most of dimensionality reduc-
tion techniques. This result confirmed our previous
conclusion that using unlabeled data can improve
the sense disambiguation process. Furthermore,
SemiFC performs significantly better than SuFC and
SuFS when using LP as the classifier for sense dis-
ambiguation. The reason is that when given very
few labeled examples, the distribution of class labels
over features can not be reliably estimated, which
deteriorates the performance of SuFC or SuFS. But
SemiFC uses only raw label information (hard label
of each feature) estimated from labeled data, which
makes it robust to the noise in very small labeled
data.
(3) SuFC, SuFS and SemiFC perform better than
LSI no matter which classifier is used for sense dis-
ambiguation. This observation confirmed our previ-
ous conclusion that using label information to guide
dimensionality reduction process can result in bet-
ter representation of contexts in feature subspace,
which further improves the results of sense disam-
biguation.
5 Related Work
Feature clustering has been extensively studied for
the benefit of text categorization and document clus-
tering, which can be categorized as supervised fea-
ture clustering, semi-supervised feature clustering,
and unsupervised feature clustering.
Supervised feature clustering algorithms (Baker
and McCallum, 1998; Bekkerman et al, 2003;
Slonim and Tishby, 2001) usually cluster words into
groups based on the distribution of class labels over
features. Baker and McCallum (1998) apply super-
vised feature clustering based on distributional clus-
tering for text categorization, which can compress
the feature space much more aggressively while still
912
maintaining state of the art classification accuracy.
Slonim and Tishby (2001) and Bekkerman et. al.
(2003) apply information bottleneck method to find
word clusters. They present similar results with the
work by Baker and McCallum (1998). Slonim and
Tishby (2001) goes further to show that when the
training sample is small, word clusters can yield sig-
nificant improvement in classification accuracy.
Unsupervised feature clustering algorithms
(Dhillon, 2001; Dhillon et al, 2002; Dhillon et al,
2003; El-Yaniv and Souroujon, 2001; Slonim and
Tishby, 2000) perform word clustering by the use
of word-document co-occurrence matrix, which do
not utilize class labels to guide clustering process.
Slonim and Tishby (2000), El-Yaniv and Souroujon
(2001) and Dhillon et. al. (2003) show that word
clusters can improve the performance of document
clustering.
El-Yaniv and Souroujon (2001) present an itera-
tive double clustering (IDC) algorithm, which per-
forms iterations of double clustering (Slonim and
Tishby, 2000). Furthermore, they extend IDC algo-
rithm for semi-supervised learning when given both
labeled and unlabeled data.
Our algorithm belongs to the family of semi-
supervised feature clustering techniques, which can
utilize both labeled and unlabeled data to perform
feature clustering.
Supervised feature clustering can not group un-
seen features (features that do not occur in labeled
data) into meaningful clusters since there are no
class labels associated with these unseen features.
Our algorithm can overcome this problem by induc-
ing class labels for unseen features based on the sim-
ilarity among seen features and unseen features, then
clustering all the features (including both seen fea-
tures and unseen features) based on the distribution
of class labels over them.
Compared with the semi-supervised version of
IDC algorithm, our algorithm is more efficient, since
we perform feature clustering without iterations.
The difference between our algorithm and unsu-
pervised feature clustering is that our algorithm de-
pends on both labeled and unlabeled data, but unsu-
pervised feature clustering requires only unlabeled
data.
O?Hara et. al. (2004) use semantic class-
based collocations to augment traditional word-
based collocations for supervised WSD. Three sep-
arate sources of word relatedness are used for
these collocations: 1) WordNet hypernym rela-
tions; 2) cluster-based word similarity classes; and
3) dictionary definition analysis. Their system
achieved 56.6% fine-grained score on ELS task of
SENSEVAL-3. In contrast with their work, our data-
driven method for feature clustering based WSD
does not require external knowledge resource. Fur-
thermore, our SemiFC+LPJS method can achieve
69.0% fine-grained score on the same dataset, which
shows the effectiveness of our method.
6 Conclusion
In this paper we have investigated feature clustering
techniques for WSD, which usually group features
into clusters based on the distribution of class labels
over features. We propose a semi-supervised fea-
ture clustering algorithm to satisfy the requirement
of semi-supervised classification algorithms for di-
mensionality reduction in feature space. Our ex-
perimental results on SENSEVAL-3 data show that
feature clustering can aggressively reduce the di-
mensionality of feature space while still maintaining
state of the art sense disambiguation accuracy. Fur-
thermore, when combined with a semi-supervised
WSD algorithm, semi-supervised feature cluster-
ing outperforms supervised feature clustering and
other dimensionality reduction techniques. Our ad-
ditional experiments on sampled SENSEVAL-3 data
indicate that our semi-supervised feature clustering
method is robust to the noise in small labeled data,
which achieves better performance than supervised
feature clustering.
In the future, we may extend our work by using
more datasets to empirically evaluate this feature
clustering algorithm. This semi-supervised feature
clustering framework is quite general, which can be
applied to other NLP tasks, ex. text categorization.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments.
Z.Y. Niu is supported by A*STAR Graduate Schol-
arship.
References
Baker L. & McCallum A.. 1998. Distributional Clus-
tering of Words for Text Classification. ACM SIGIR
913
1998.
Bekkerman, R., El-Yaniv, R., Tishby, N., & Winter, Y..
2003. Distributional Word Clusters vs. Words for
Text Categorization. Journal of Machine Learning Re-
search, Vol. 3: 1183-1208.
Dagan, I. & Itai A.. 1994. Word Sense Disambigua-
tion Using A Second Language Monolingual Corpus.
Computational Linguistics, Vol. 20(4), pp. 563-596.
Deerwester, S.C., Dumais, S.T., Landauer, T.K., Furnas,
G.W., & Harshman, R.A.. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
of Information Science, Vol. 41(6), pp. 391-407.
Dhillon I.. 2001. Co-Clustering Documents and Words
Using Bipartite Spectral Graph Partitioning. ACM
SIGKDD 2001.
Dhillon I., Mallela S., & Kumar R.. 2002. Enhanced
Word Clustering for Hierarchical Text Classification.
ACM SIGKDD 2002.
Dhillon I., Mallela S., & Modha, D.. 2003. Information-
Theoretic Co-Clustering. ACM SIGKDD 2003.
El-Yaniv, R., & Souroujon, O.. 2001. Iterative Dou-
ble Clustering for Unsupervised and Semi-Supervised
Learning. NIPS 2001.
Forman, G.. 2003. An Extensive Empirical Study of Fea-
ture Selection Metrics for Text Classification. Journal
of Machine Learning Research 3(Mar):1289?1305.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24:1, 147?
165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. EMNLP 2002, (pp. 41-
48).
Lesk M.. 1986. Automated Word Sense Disambiguation
Using Machine Readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. ACM SIGDOC
1986.
Li, H. & Li, C.. 2004. Word Translation Disambiguation
Using Bilingual Bootstrapping. Computational Lin-
guistics, 30(1), 1-22.
Lin, J. 1991. Divergence Measures Based on the Shan-
non Entropy. IEEE Transactions on Information The-
ory, 37:1, 145?150.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J..
2004. Finding Predominant Word Senses in Untagged
Text. ACL 2004.
Mihalcea R.. 2002. Instance Based Learning with Au-
tomatic Feature Selection Applied to Word Sense Dis-
ambiguation. COLING 2002.
Mihalcea R.. 2004. Co-Training and Self-Training for
Word Sense Disambiguation. CoNLL 2004.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004. The
SENSEVAL-3 English Lexical Sample Task. SENSE-
VAL 2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2004. Learning Word
Senses With Feature Selection and Order Identification
Capabilities. ACL 2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2005. Word Sense
Disambiguation Using Label Propagation Based Semi-
Supervised Learning. ACL 2005.
O?Hara, T., Bruce, R., Donner, J., & Wiebe, J..
2004. Class-Based Collocations for Word-Sense Dis-
ambiguation. SENSEVAL 2004.
Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000. Word
Sense Disambiguation by Learning from Unlabeled
Data. ACL 2000.
Pedersen. T., & Bruce, R.. 1997. Distinguishing Word
Senses in Untagged Text. EMNLP 1997.
Schu?tze, H.. 1998. Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:1, 97?123.
Slonim, N. & Tishby, N.. 2000. Document Clustering
Using Word Clusters via the Information Bottleneck
Method. ACM SIGIR 2000.
Slonim, N. & Tishby, N.. 2001. The Power of Word
Clusters for Text Classification. The 23rd European
Colloquium on Information Retrieval Research.
Towel, G. & Voorheest, E.M.. 1998. Disambiguating
Highly Ambiguous Words. Computational Linguis-
tics, 24:1, 125?145.
Yarowsky, D.. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. ACL 1995,
pp. 189-196.
Zhou D., Bousquet, O., Lal, T.N., Weston, J., &
Scho?lkopf, B.. 2003. Learning with Local and Global
Consistency. NIPS 16,pp. 321-328.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003. Semi-
Supervised Learning Using Gaussian Fields and Har-
monic Functions. ICML 2003.
914
  	
   	

	 	
	  	 	

Unsupervised Feature Selection for Relation Extraction
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised re-
lation extraction algorithm, which in-
duces relations between entity pairs by
grouping them into a ?natural? num-
ber of clusters based on the similarity
of their contexts. Stability-based crite-
rion is used to automatically estimate
the number of clusters. For removing
noisy feature words in clustering proce-
dure, feature selection is conducted by
optimizing a trace based criterion sub-
ject to some constraint in an unsuper-
vised manner. After relation clustering
procedure, we employ a discriminative
category matching (DCM) to find typi-
cal and discriminative words to repre-
sent different relations. Experimental
results show the effectiveness of our al-
gorithm.
1 Introduction
Relation extraction is the task of finding rela-
tionships between two entities from text contents.
There has been considerable work on supervised
learning of relation patterns, using corpora which
have been annotated to indicate the information to
be extracted (e.g. (Califf and Mooney, 1999; Ze-
lenko et al, 2002)). A range of extraction mod-
els have been used, including both symbolic rules
and statistical rules such as HMMs or Kernels.
These methods have been particularly success-
ful in some specific domains. However, manu-
ally tagging of large amounts of training data is
very time-consuming; furthermore, it is difficult
for one extraction system to be ported across dif-
ferent domains.
Due to the limitation of supervised methods,
some weakly supervised (or semi-supervised) ap-
proaches have been suggested (Brin, 1998; Eu-
gene and Luis, 2000; Sudo et al, 2003). One
common characteristic of these algorithms is that
they need to pre-define some initial seeds for any
particular relation, then bootstrap from the seeds
to acquire the relation. However, it is not easy
to select representative seeds for obtaining good
results.
Hasegawa, et al put forward an unsuper-
vised approach for relation extraction from large
text corpora (Hasegawa et al, 2004). First, they
adopted a hierarchical clustering method to clus-
ter the contexts of entity pairs. Second, after con-
text clustering, they selected the most frequent
words in the contexts to represent the relation
that holds between the entities. However, the ap-
proach exists its limitation. Firstly, the similar-
ity threshold for the clusters, like the appropriate
number of clusters, is somewhat difficult to pre-
defined. Secondly, the representative words se-
lected by frequency tends to obscure the clusters.
For solving the above problems, we present a
novel unsupervised method based on model or-
der selection and discriminative label identifica-
tion. For achieving model order identification,
stability-based criterion is used to automatically
estimate the number of clusters. For removing
noisy feature words in clustering procedure, fea-
ture selection is conducted by optimizing a trace
based criterion subject to some constraint in an
262
unsupervised manner. Furthermore, after relation
clustering, we employ a discriminative category
matching (DCM) to find typical and discrimina-
tive words to represent different relations types.
2 Proposed Method
Feature selection for relation extraction is the task
of finding important contextual words which will
help to discriminate relation types. Unlike su-
pervised learning, where class labels can guide
feature search, in unsupervised learning, it is ex-
pected to define a criterion to assess the impor-
tance of the feature subsets. Due to the interplay
between feature selection and clustering solution,
we should define an objective function to evaluate
both feature subset and model order.
In this paper, the model selection capability is
achieved by resampling based stability analysis,
which has been successfully applied to several un-
supervised learning problems (e.g. (Levine and
Domany, 2001), (Lange et al, 2002), (Roth and
Lange et al, 2003), (Niu et al, 2004)). We extend
the cluster validation strategy further to address
both feature selection and model order identifica-
tion.
Table 1 presents our model selection algorithm.
The objective function MFk,k is relevant with
both feature subset and model order. Clustering
solution that is stable against resampling will give
rise to a local optimum of MFk,k, which indicates
both important feature subset and the true cluster
number.
2.1 Entropy-based Feature Ranking
Let P = {p1, p2, ...pN} be a set of local context
vectors of co-occurrences of entity pair E1 and
E2. Here, the context includes the words occur-
ring between, before and after the entity pair. Let
W = {w1, w2, ..., wM} represent all the words
occurred in P . To select a subset of important
features from W , words are first ranked accord-
ing to their importance on clustering. The im-
portance can be assessed by the entropy criterion.
Entropy-based feature ranking is based on the as-
sumption that a feature is irrelevant if the presence
of it obscures the separability of data set(Dash et
al., 2000).
We assume pn, 1 ? n ? N , lies in feature
space W , and the dimension of feature space is
Table 1: Model Selection Algorithm for Relation Extrac-
tion
Input: Corpus D tagged with Entities(E1, E2);
Output: Feature subset and Model Order (number of
relation types);
1. Collect the contexts of all entity pairs in the document
corpus D, namely P ;
2. Rank features using entropy-based method described
in section 2.1;
3. Set the range (Kl,Kh) for the possible number of
relation clusters;
4. Set estimated model order k = Kl;
5. Conduct feature selection using the algorithm pre-
sented in section 2.2;
6. Record F?k,k and the score of the merit of both of
them, namely MF,k;
7. If k < Kh, k = k + 1, go to step 5; otherwise, go to
Step 7;
8. Select k and feature subset F?k which maximizes the
score of the merit MF,k;
M . Then the similarity between i-th data point
pi and j-th data point pj is given by the equa-
tion: Si,j = exp(?? ? Di,j), where Di,j is the
Euclidean distance between pi and pj , and ? is a
positive constant, its value is ? ln 0.5D , where D is
the average distance among the data points. Then
the entropy of data set P with N data points is
defined as:
E = ?
N?
i=1
N?
j=1
(Si,j logSi,j + (1? Si,j) log(1? Si,j))
(1)
For ranking of features, the importance of each
word I(wk) is defined as entropy of the data af-
ter discarding feature wk. It is calculated in this
way: remove each word in turn from the feature
space and calculate E of the data in the new fea-
ture space using the Equation 1. Based on the
observation that a feature is the least important if
the removal of it results in minimum E, we can
obtain the rankings of the features.
2.2 Feature Subset Selection and Model
Order Identification
In this paper, for each specified cluster number,
firstly we perform K-means clustering analysis on
each feature subset and adopts a scattering cri-
terion ?Invariant Criterion? to select an optimal
feature subset F from the feature subset space.
Here, trace(P?1W PB) is used to compare the clus-
ter quality for different feature subsets 1, which
1trace(P?1W PB) is trace of a matrix which is the sum
of its diagonal elements. PW is the within-cluster scatter
263
Table 2: Unsupervised Algorithm for Evaluation of Fea-
ture Subset and Model Order
Function: criterion(F, k, P, q)
Input: feature subset F , cluster number k, entity pairs
set P , and sampling frequency q;
Output: the score of the merit of F and k;
1. With the cluster number k as input, perform k-means
clustering analysis on pairs set PF ;
2. Construct connectivity matrix CF,k based on above
clustering solution on full pairs set PF ;
3. Use a random predictor ?k to assign uniformly drawn
labels to each entity pair in PF ;
4. Construct connectivity matrix CF,?k based on above
clustering solution on full pairs set PF ;
5. Construct q sub sets of the full pairs set, by randomly
selecting ?N of the N original pairs, 0 ? ? ? 1;
6. For each sub set, perform the clustering analysis in
Step 2, 3, 4, and result C?F,k, C?F,?k ;
7. Compute MF,k to evaluate the merit of k using Equa-
tion 3;
8. Return MF,k;
measures the ratio of between-cluster to within-
cluster scatter. The higher the trace(P?1W PB), the
higher the cluster quality.
To improve searching efficiency, features are
first ranked according to their importance. As-
sume Wr = {f1, ..., fM} is the sorted feature list.
The task of searching can be seen in the feature
subset space: {(f1, ..., fk),1 ? k ? M}.
Then the selected feature subset F is eval-
uated with the cluster number using the ob-
jective function, which can be formulated as:
F?k = argmaxF?Wr{criterion(F, k)}, subject
to coverage(P, F ) ? ? 2. Here, F?k is the opti-
mal feature subset, F and k are the feature subset
and the value of cluster number under evaluation,
and the criterion is set up based on resampling-
based stability, as Table 2 shows.
Let P? be a subset sampled from full entity
pairs set P with size ?|P | (? set as 0.9 in this
paper.), C(C?) be |P | ? |P |(|P?| ? |P?|) con-
nectivity matrix based on the clustering results on
P (P?). Each entry cij(c?ij) of C(C?) is calculated
in the following: if the entity pair pi ? P (P?),
pj ? P (P?) belong to the same cluster, then
cij(c?ij) equals 1, else 0. Then the stability is de-
matrix as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xj ? mj)
t
and PB is the between-cluster scatter matrix as: PB =?c
j=1(mj ?m)(mj ?m)t, where m is the total mean vec-
tor and mj is the mean vector for jth cluster and (Xj?mj)t
is the matrix transpose of the column vector (Xj ?mj).
2let coverage(P, F ) be the coverage rate of the feature
set F with respect to P . In practice, we set ? = 0.9.
fined in Equation 2:
M(C?, C) =
?
i,j 1{C?i,j = Ci,j = 1, pi ? P?, pj ? P?}?
i,j 1{Ci,j = 1, pi ? P?, pj ? P?}
(2)
Intuitively, M(C?, C) denotes the consistency
between the clustering results on C? and C. The
assumption is that if the cluster number k is actu-
ally the ?natural? number of relation types, then
clustering results on subsets P? generated by
sampling should be similar to the clustering re-
sult on full entity pair set P . Obviously, the above
function satisfies 0 ? M ? 1.
It is noticed that M(C?, C) tends to decrease
when increasing the value of k. Therefore for
avoiding the bias that small value of k is to be
selected as cluster number, we use the cluster
validity of a random predictor ?k to normalize
M(C?, C). The random predictor ?k achieved
the stability value by assigning uniformly drawn
labels to objects, that is, splitting the data into k
clusters randomly. Furthermore, for each k, we
tried q times. So, in the step 7 of the algorithm
of Table 2, the objective function M(C?F,k, CF,k)
can be normalized as equations 3:
MnormF,k = 1q
q?
i=1
M(C?iF,k, CF,k)?
1
q
q?
i=1
M(C?iF,?k , CF,?k )
(3)
Normalizing M(C?, C) by the stability of the
random predictor can yield values independent of
k.
After the number of optimal clusters and the
feature subset has been chosen, we adopted the
K-means algorithm for the clustering phase. The
output of context clustering is a set of context
clusters, each of them is supposed to denote one
relation type.
2.3 Discriminative Feature identification
For labelling each relation type, we use DCM
(discriminative category matching) scheme to
identify discriminative label, which is also used
in document classification (Gabriel et al, 2002)
and weights the importance of a feature based on
their distribution. In this scheme, a feature is not
important if the feature appears in many clusters
and is evenly distributed in these clusters, other-
wise it will be assigned higher importance.
To weight a feature fi within a category, we
take into account the following information:
264
Table 3: Three domains of entity pairs: frequency distribution for different relation types
PER-ORG # of pairs:786 ORG-GPE # of pairs:262 ORG-ORG # of pairs:580
Relation types Percentage Relation types Percentage Relation types Percentage
Management 36.39% Based-In 46.56% Member 27.76%
General-staff 29.90% Located 35.11% Subsidiary 19.83%
Member 19.34% Member 11.07% Part-Of 18.79%
Owner 4.45% Affiliate-Partner 3.44% Affiliate-Partner 17.93%
Located 3.28% Part-Of 2.29% Owner 8.79%
Client 1.91% Owner 1.53% Client 2.59%
Other 1.91% Management 2.59%
Affiliate-Partner 1.53% Other 1.21%
Founder 0.76% Other 0.52%
? The relative importance of fi within a cluster is de-
fined as: WCi,k = log2(pfi,k+1)log2(Nk+1) , where pfi,k is the
number of those entity pairs which contain feature fi
in cluster k. Nk is the total number of term pairs in
cluster k.
? The relative importance of fi across clusters is given
by: CCi = log N?maxk?Ci{WCi,k}?N
k=1 WCi,k
? 1logN , where Ci
is the set of clusters which contain feature fi. N is the
total number of clusters.
Here, WCi,k and CCi are designed to capture
both local information within a cluster and global
information about the feature distribution across
clusters respectively. Combining both WCi,k and
CCi we define the weight Wi,k of fi in cluster k
as: Wi,k = WC
2
i,k?CC2i?
WC2i,k+CC2i
? ?2, 0 ? Wi,k ? 1.
3 Experiments and Results
3.1 Data
We constructed three subsets for domains PER-
ORG, ORG-GPE and ORG-ORG respectively
from ACE corpus3 The details of these subsets
are given in Table 3, which are broken down by
different relation types. To verify our proposed
method, we only extracted those pairs of entity
mentions which have been tagged relation types.
And the relation type tags were used as ground
truth classes to evaluate.
3.2 Evaluation method for clustering result
Since there was no relation type tags for each
cluster in our clustering results, we adopted a
permutation procedure to assign different rela-
tion type tags to only min(|EC|,|TC|) clusters,
where |EC| is the estimated number of clusters,
and |TC| is the number of ground truth classes
3http://www.ldc.upenn.edu/Projects/ACE/
(relation types). This procedure aims to find an
one-to-one mapping function ? from the TC to
EC. To perform the mapping, we construct a
contingency table T , where each entry ti,j gives
the number of the instances that belong to both
the i-th cluster and j-th ground truth class. Then
the mapping procedure can be formulated as:?? =
argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the index
of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we
can define the evaluation measure as follows:
Accuracy(P ) =
?
j t??(j),j?
i,j ti,j
. Intuitively, it reflects
the accuracy of the clustering result.
3.3 Evaluation method for relation labelling
For evaluation of the relation labeling, we need
to explore the relatedness between the identified
labels and the pre-defined relation names. To do
this, we use one information-content based mea-
sure (Lin, 1997), which is provided in Wordnet-
Similarity package (Pedersen et al, 2004) to eval-
uate the similarity between two concepts in Word-
net. Intuitively, the relatedness between two con-
cepts in Wordnet is captured by the information
content of their lowest common subsumer (lcs)
and the information content of the two concepts
themselves , which can be formalized as follows:
Relatednesslin(c1, c2) = 2?IC(lcs(c1,c2))IC(c1)+IC(c2) . This
measure depends upon the corpus to estimate in-
formation content. We carried out the experi-
ments using the British National Corpus (BNC)
as the source of information content.
3.4 Experiments and Results
For comparison of the effect of the outer and
within contexts of entity pairs, we used five dif-
265
Table 4: Automatically determined the number of relation types using different feature ranking methods.
Domain Context
Window
Size
# of real
relation
types
Model Or-
der Base-
line
Model
Order with
?2
Model
Order with
Freq
Model Or-
der with
Entropy
PER-ORG 0-5-0 9 7 7 7 7
2-5-2 9 8 6 7 8
0-10-0 9 8 6 8 8
2-10-2 9 6 7 6 8
5-10-5 9 5 5 6 7
ORG-GPE 0-5-0 6 3 3 3 4
2-5-2 6 2 3 4 4
0-10-0 6 6 4 5 6
2-10-2 6 4 3 4 5
5-10-5 6 2 3 3 3
ORG-ORG 0-5-0 9 7 7 7 7
2-5-2 9 7 5 6 7
0-10-0 9 9 8 9 9
2-10-2 9 6 6 6 7
5-10-5 9 8 5 7 9
ferent settings of context window size (WINpre-
WINmid-WINpost) for each domain.
Table 4 shows the results of model order iden-
tification without feature selection (Baseline) and
with feature selection based on different feature
ranking criterion( ?2 , Frequency and Entropy).
The results show that the model order identifica-
tion algorithm with feature selection based on en-
tropy achieve best results: estimate cluster num-
bers which are very close to the true values. In ad-
dition, we can find that with the context setting, 0-
10-0, the estimated number of the clusters is equal
or close to the ground truth value. It demonstrates
that the intervening words less than 10 are appro-
priate features to reflect the structure behind the
contexts, while the intervening words less than 5
are not enough to infer the structure. For the con-
textual words beyond (before or after) the enti-
ties, they tend to be noisy features for the relation
estimation, as can be seen that the performance
deteriorates when taking them into consideration,
especially for the case without feature selection.
Table 5 gives a comparison of the aver-
age accuracy over five different context win-
dow size settings for different clustering settings.
For each domain, we conducted five cluster-
ing procedures: Hasegawa?s method, RLBaseline,
RLFS?2 , RLFSFreq and RLFSEntropy. For
Hasegawa?s method (Hasegawa et al, 2004), we
set the cluster number to be identical with the
number of ground truth classes. For RLBaseline,
we use the estimated cluster number to clus-
ter contexts without feature selection. For
RLFS?2 ,RLFSFreq and RLFSEntropy, we use
the selected feature subset and the estimated clus-
ter number to cluster the contexts, where the fea-
ture subset comes from ?2, frequency and entropy
criterion respectively. Comparing the average ac-
curacy of these clustering methods, we can find
that the performance of feature selection methods
is better than or comparable with the baseline sys-
tem without feature selection. Furthermore, it is
noted that RLFSEntropy achieves the highest av-
erage accuracy in three domains, which indicates
that entropy based feature pre-ranking provides
useful heuristic information for the selection of
important feature subset.
Table 6 gives the automatically estimated labels
for relation types for the domain PER-ORG. We
select two features as labels of each relation type
according to their DCM scores and calculate the
average (and maximum) relatedness between our
selected labels (E) and the predefined labels (H).
Following the same strategy, we also extracted re-
lation labels (T) from the ground truth classes and
provided the relatedness between T and H. From
the column of relatedness (E-H), we can see that it
is not easy to find the hand-tagged relation labels
exactly, furthermore, the identified labels from the
ground-truth classes are either not always compa-
rable to the pre-defined labels in most cases (T-
H). The reason may be that the pre-defined rela-
tion names tend to be some abstract labels over
the features, e.g., ?management? vs. ?president?,
266
Table 5: Performance of the clustering algorithms over three domains: the average accuracy over 5 different context window
size.
Domain Hasegawa?s
method
RLBaseline RLFS?2 RLFSFreq RLFSEntropy
PER-ORG 32.4% 34.3% 33.9% 36.6% 41.3%
ORG-GPE 43.7% 47.4% 47.1% 48.4% 50.6%
ORG-ORG 26.5% 36.2% 36.0% 38.7% 42.4%
Table 6: Relation Labelling using DCM strategy for the domain PER-ORG. Here, (T) denotes the identified relation labels
from ground truth classes. (E) is the identified relation labels from our estimated clusters. ?Ave (T-H)? denotes the average
relatedness between (T) and (H). ?Max (T-H)? denotes the maximum relatedness between (T) and (H).
Hand-tagged La-
bel (H)
Identified Label
(T)
Identified Label
(E)
Ave
(T-H)
Max
(T-H)
Ave
(E-H)
Max
(E-H)
Ave
(E-T)
Max
(E-T)
management head,president president,control 0.3703 0.4515 0.3148 0.3406 0.7443 1.0000
general-staff work,fire work,charge 0.6254 0.7823 0.6411 0.7823 0.6900 1.0000
member join,communist become,join 0.394 0.4519 0.1681 0.3360 0.3366 1.0000
owner bond,bought belong,house 0.1351 0.2702 0.0804 0.1608 0.2489 0.4978
located appear,include lobby,appear 0.0000 0.0000 0.1606 0.3213 0.2500 1.0000
client hire,reader bought,consult 0.4378 0.8755 0.0000 0.0000 0.1417 0.5666
affiliate-partner affiliate,associate assist,affiliate 0.9118 1.0000 0.5000 1.0000 0.5000 1.0000
founder form,found invest,set 0.1516 0.3048 0.3437 0.6875 0.4376 0.6932
?head? or ?control?; ?member? vs. ?join?, ?be-
come?, etc., while the abstract words and the fea-
tures are located far away in Wordnet. Table 6
also lists the relatedness between (E) and (T). We
can see that the labels are comparable by their
maximum relatedness(E-T).
4 Conclusion and Future work
In this paper, we presented an unsupervised ap-
proach for relation extraction from corpus. The
advantages of the proposed approach includes
that it doesn?t need any manual labelling of the re-
lation instances, it can identify an important fea-
ture subset and the number of the context clusters
automatically, and it can avoid extracting those
common words as characterization of relations.
References
Mary Elaine Califf and Raymond J.Mooney. 1999. Rela-
tional Learning of Pattern-Match Rules for Information
Extraction, AAAI99.
Sergey Brin. 1998. Extracting patterns and relations from
world wide web. In Proc. of WebDB?98. pages 172-183.
Kiyoshi Sudo, Satoshi Sekine and Ralph Grishman. 2003.
An Improved Extraction Pattern Representation Model
for Automatic IE Pattern Acquisition. Proceedings of ACL
2003; Sapporo, Japan.
Eugene Agichtein and Luis Gravano. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections, In
Proc. of the 5th ACM International Conference on Digi-
tal Libraries (ACMDL?00).
Takaaki Hasegawa, Satoshi Sekine and Ralph Grishman.
2004. Discovering Relations among Named Entities from
Large Corpora, ACL2004. Barcelona, Spain.
Dmitry Zelenko, Chinatsu Aone and Anthony Richardella.
2002. Kernel Methods for Relation Extraction,
EMNLP2002. Philadelphia.
Lange,T., Braun,M.,Roth, V., and Buhmann,J.M.. 2002.
Stability-Based Model Selection, Advances in Neural In-
formation Processing Systems 15.
Levine,E. and Domany,E.. 2001. Resampling Method
for Unsupervised Estimation of Cluster Calidity, Neural
Computation, Vol.13, 2573-2593.
Zhengyu Niu, Donghong Ji and Chew Lim Tan. 2004. Doc-
ument Clustering Based on Cluster Validation, CIKM?04.
November 8-13, 2004, Washington, DC, USA.
Volker Roth and Tilman Lange. 2003. Feature Selection in
Clustering Problems, NIPS2003 workshop.
Manoranjan Dash and Huan Liu. 2000. Feature Selection
for Clustering, Proceedings of Pacific-Asia Conference
on Knowledge Discovery and Data Mining.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu and Hongjun
Lu. 2002. Discriminative Category Matching: Effi-
cient Text Classification for Huge Document Collections,
ICDM2002. December 09-12, 2002, Japan.
D.Lin. 1997. Using syntactic dependency as a local context
to resolve word sense ambiguity. In Proceedings of the
35th Annual Meeting of ACL,. Madrid, July 1997.
Ted Pedersen, Siddharth Patwardhan and Jason Michelizzi.
2004. WordNet::Similarity-Measuring the Relatedness of
Concepts, AAAI2004.
267
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 25?28,
New York, June 2006. c?2006 Association for Computational Linguistics
Semi-supervised Relation Extraction with Label Propagation
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
To overcome the problem of not hav-
ing enough manually labeled relation in-
stances for supervised relation extraction
methods, in this paper we propose a label
propagation (LP) based semi-supervised
learning algorithm for relation extraction
task to learn from both labeled and unla-
beled data. Evaluation on the ACE corpus
showed when only a few labeled examples
are available, our LP based relation extrac-
tion can achieve better performance than
SVM and another bootstrapping method.
1 Introduction
Relation extraction is the task of finding relation-
ships between two entities from text. For the task,
many machine learning methods have been pro-
posed, including supervised methods (Miller et al,
2000; Zelenko et al, 2002; Culotta and Soresen,
2004; Kambhatla, 2004; Zhou et al, 2005), semi-
supervised methods (Brin, 1998; Agichtein and Gra-
vano, 2000; Zhang, 2004), and unsupervised method
(Hasegawa et al, 2004).
Supervised relation extraction achieves good per-
formance, but it requires a large amount of manu-
ally labeled relation instances. Unsupervised meth-
ods do not need the definition of relation types and
manually labeled data, but it is difficult to evaluate
the clustering result since there is no relation type
label for each instance in clusters. Therefore, semi-
supervised learning has received attention, which
can minimize corpus annotation requirement.
Current works on semi-supervised resolution for
relation extraction task mostly use the bootstrap-
ping algorithm, which is based on a local consis-
tency assumption: examples close to labeled ex-
amples within the same class will have the same
labels. Such methods ignore considering the simi-
larity between unlabeled examples and do not per-
form classification from a global consistency view-
point, which may fail to exploit appropriate mani-
fold structure in data when training data is limited.
The objective of this paper is to present a label
propagation based semi-supervised learning algo-
rithm (LP algorithm) (Zhu and Ghahramani, 2002)
for Relation Extraction task. This algorithm works
by representing labeled and unlabeled examples as
vertices in a connected graph, then propagating the
label information from any vertex to nearby vertices
through weighted edges iteratively, finally inferring
the labels of unlabeled examples after the propaga-
tion process converges. Through the label propaga-
tion process, our method can make the best of the
information of labeled and unlabeled examples to re-
alize a global consistency assumption: similar ex-
amples should have similar labels. In other words,
the labels of unlabeled examples are determined by
considering not only the similarity between labeled
and unlabeled examples, but also the similarity be-
tween unlabeled examples.
2 The Proposed Method
2.1 Problem Definition
Let X = {xi}ni=1 be a set of contexts of occurrences
of all entity pairs, where xi represents the contexts
of the i-th occurrence, and n is the total number of
occurrences of all entity pairs. The first l examples
are labeled as yg ( yg ? {rj}Rj=1, rj denotes relation
type and R is the total number of relation types).
And the remaining u(u = n? l) examples are unla-
beled.
Intuitively, if two occurrences of entity pairs have
25
the similar contexts, they tend to hold the same re-
lation type. Based on this assumption, we create a
graph where the vertices are all the occurrences of
entity pairs, both labeled and unlabeled. The edge
between vertices represents their similarity. Then
the task of relation extraction can be formulated as
a form of propagation on a graph, where a vertex?s
label propagates to neighboring vertices according
to their proximity. Here, the graph is connected with
the weights: Wij = exp(? s
2
ij
?2 ), where sij is the sim-
ilarity between xi and xj calculated by some simi-
larity measures. In this paper,two similarity mea-
sures are investigated, i.e. Cosine similarity measure
and Jensen-Shannon (JS) divergence (Lin, 1991).
And we set ? as the average similarity between la-
beled examples from different classes.
2.2 Label Propagation Algorithm
Given such a graph with labeled and unlabeled ver-
tices, we investigate the label propagation algorithm
(Zhu and Ghahramani, 2002) to help us propagate
the label information of any vertex in the graph
to nearby vertices through weighted edges until a
global stable stage is achieved.
Define a n ? n probabilistic transition matrix T
Tij = P (j ? i) = wij?n
k=1 wkj
, where Tij is the prob-
ability to jump from vertex xj to vertex xi. Also de-
fine a n?R label matrix Y , where Yij representing
the probabilities of vertex yi to have the label rj .
Then the label propagation algorithm consists the
following main steps:
Step1: Initialization Firstly, set the iteration in-
dex t = 0. Then let Y 0 be the initial soft labels at-
tached to each vertex and Y 0L be the top l rows of Y 0,
which is consistent with the labeling in labeled data
(Y 0ij = 1 if yi is label rj and 0 otherwise ). Let Y 0U
be the remaining u rows corresponding to unlabeled
data points and its initialization can be arbitrary.
Step 2: Propagate the label by Y t+1 = TY t,
where T is the row-normalized matrix of T , i.e.
Tij = Tij/
?
k Tik, which can maintain the class
probability interpretation.
Step 3: Clamp the labeled data, i.e., replace the
top l row of Y t+1 with Y 0L . In this step, the labeled
data is clamped to replenish the label sources from
these labeled data. Thus the labeled data act like
sources to push out labels through unlabeled data.
Table 1: Frequency of Relation SubTypes in the ACE training
and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
Step 4: Repeat from step 2 until Y converges.
Step 5: Assign xh(l + 1 ? h ? n) with a label:
yh = argmaxjYhj .
3 Experiments and Results
3.1 Data
Our proposed graph-based method is evaluated on
the ACE corpus 1, which contains 519 files from
sources including broadcast, newswire, and news-
paper. A break-down of the tagged data by different
relation subtypes is given in Table 1.
3.2 Features
We extract the following lexical and syntactic fea-
tures from two entity mentions, and the contexts be-
fore, between and after the entity pairs. Especially,
we set the mid-context window as everything be-
tween the two entities and the pre- and post- context
as up to two words before and after the correspond-
ing entity. Most of these features are computed from
the parse trees derived from Charniak Parser (Char-
niak, 1999) and the Chunklink script 2 written by
Sabine Buchholz from Tilburg University.
1 http://www.ldc.upenn.edu/Projects/ACE/
2Software available at http://ilk.uvt.nl/?sabine/chunklink/
26
Table 2: Performance of Relation Detection: SVM and LP algorithm with different size of labeled data. The LP algorithm is
performed with two similarity measures: Cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 35.9 32.6 34.4 58.3 56.1 57.1 58.5 58.7 58.5
10% 51.3 41.5 45.9 64.5 57.5 60.7 64.6 62.0 63.2
25% 67.1 52.9 59.1 68.7 59.0 63.4 68.9 63.7 66.1
50% 74.0 57.8 64.9 69.9 61.8 65.6 70.1 64.1 66.9
75% 77.6 59.4 67.2 71.8 63.4 67.3 72.4 64.8 68.3
100% 79.8 62.9 70.3 73.9 66.9 70.2 74.2 68.2 71.1
Table 3: Performance of Relation Classification on Relation Subtype: SVM and LP algorithm with different size of labeled data.
The LP algorithm is performed with two similarity measures: Cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 31.6 26.1 28.6 39.6 37.5 38.5 40.1 38.0 39.0
10% 39.1 32.7 35.6 45.9 39.6 42.5 46.2 41.6 43.7
25% 49.8 35.0 41.1 51.0 44.5 47.3 52.3 46.0 48.9
50% 52.5 41.3 46.2 54.1 48.6 51.2 54.9 50.8 52.7
75% 58.7 46.7 52.0 56.0 52.0 53.9 56.1 52.6 54.3
100% 60.8 48.9 54.2 56.2 52.3 54.1 56.3 52.9 54.6
Words: Surface tokens of the two entities and
three context windows.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZATION,
FACILITY, LOCATION and GPE.
POS: Part-Of-Speech tags corresponding to all
tokens in the two entities and three context windows.
Chunking features: Chunk tag information and
Grammatical function of the two entities and three
context windows. IOB-chains of the heads of the
two entities are also considered. IOB-chain notes
the syntactic categories of all the constituents on the
path from the root node to this leaf node of tree.
We combine the above features with their position
information in the context to form the context vec-
tor. Before that, we filter out low frequency features
which appeared only once in the entire set.
3.3 Experimental Evaluation
3.3.1 Relation Detection
We collect all entity mention pairs which co-occur
in the same sentence from the training and devtest
corpus into two set C1 and C2 respectively. The set
C1 includes annotated training data AC1 and un-
related data UC1. We randomly sample l examples
from AC1 as labeled data and add a ?NONE? class
into labeled data for the case where the two entity
mentions are not related. The data of the ?NONE?
Table 4: Comparison of performance on individual relation
type of Zhang (2004)?s method and our method. For Zhang
(2004)?s method, feature sampling probability is set to 0.3 and
agreement threshold is set to 9 out of 10.
Bootstrapping LPJS
Rel-Type P R F P R F
ROLE 78.5 69.7 73.8 81.0 74.7 77.7
PART 65.6 34.1 44.9 70.1 41.6 52.2
AT 61.0 84.8 70.9 74.2 79.1 76.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0
NEAR undef 0 undef 13.7 12.5 13.0
class is resulted by sampling l examples from UC1.
Moreover, we combine the rest examples of C1 and
the whole set C2 as unlabeled data.
Given labeled and unlabeled data,we can perform
LP algorithm to detect possible relations, which
are those entity pairs that are not classified to the
?NONE? class but to the other 24 subtype classes.
In addition,we conduct experiments with different
sampling set size l, including 1% ? Ntrain,10% ?
Ntrain,25%?Ntrain,50%?Ntrain,75%?Ntrain,
100% ? Ntrain (Ntrain = |AC1|). If any major
subtype was absent from the sampled labeled set,we
redo the sampling. For each size,we perform 20 tri-
als and calculate an average of 20 random trials.
3.3.2 SVM vs. LP
Table 2 reports the performance of relation detec-
tion by using SVM and LP with different sizes of
27
labled data. For SVM, we use LIBSVM tool with
linear kernel function 3. And the same sampled la-
beled data used in LP is used to train SVM mod-
els. From Table 2, we see that both LPCosine and
LPJS achieve higher Recall than SVM. Especially,
with small labeled dataset (percentage of labeled
data ? 25%), this merit is more distinct. When
the percentage of labeled data increases from 50%
to 100%, LPCosine is still comparable to SVM in F-
measure while LPJS achieves better F-measure than
SVM. On the other hand, LPJS consistently outper-
forms LPCosine.
Table 3 reports the performance of relation classi-
fication, where the performance describes the aver-
age values over major relation subtypes. From Table
3, we see that LPCosine and LPJS outperform SVM
by F-measure in almost all settings of labeled data,
which is due to the increase of Recall. With smaller
labeled dataset, the gap between LP and SVM is
larger. On the other hand, LPJS divergence consis-
tently outperforms LPCosine.
3.3.3 LP vs. Bootstrapping
In (Zhang, 2004), they perform relation classifi-
cation on ACE corpus with bootstrapping on top of
SVM. To compare with their proposed Bootstrapped
SVM algorithm, we use the same feature stream set-
ting and randomly selected 100 instances from the
training data as the size of initial labeled data.
Table 4 lists the performance on individual rela-
tion type. We can find that LP algorithm achieves
6.8% performance improvement compared with the
(Zhang, 2004)?s bootstrapped SVM algorithm aver-
age on all five relation types. Notice that perfor-
mance reported on relation type ?NEAR? is low, be-
cause it occurs rarely in both training and test data.
4 Conclusion and Future work
This paper approaches the task of semi-supervised
relation extraction on Label Propagation algorithm.
Our results demonstrate that, when only very few
labeled examples are available, this manifold learn-
ing based algorithm can achieve better performance
than supervised learning method (SVM) and boot-
strapping based method, which can contribute to
3LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
minimize corpus annotation requirement. In the fu-
ture we would like to investigate how to select more
useful feature stream and whether feature selection
method can improve the performance of our graph-
based semi-supervised relation extraction.
References
Agichtein E. and Gravano L. 2000. Snowball: Extracting Rela-
tions from large Plain-Text Collections, In Proceeding of the
5th ACM International Conference on Digital Libraries.
Brin Sergey. 1998. Extracting patterns and relations from
world wide web. In Proceeding of WebDB Workshop at 6th
International Conference on Extending Database Technol-
ogy. pages 172-183.
Charniak E. 1999. A Maximum-entropy-inspired parser. Tech-
nical Report CS-99-12. Computer Science Department,
Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels for
relation extraction, In Proceedings of 42th ACL conference.
Hasegawa T., Sekine S. and Grishman R. 2004. Discover-
ing Relations among Named Entities from Large Corpora,
In Proceeding of Conference ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and semantic
features with Maximum Entropy Models for extracting rela-
tions, In Proceedings of 42th ACL conference. Spain.
Lin,J. 1991. Divergence Measures Based on the Shannon En-
tropy. IEEE Transactions on Information Theory. 37:1,145-
150.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000. A novel
use of statistical parsing to extract information from text.
In Proceedings of 6th Applied Natural Language Processing
Conference 29 April-4 may 2000, Seattle USA.
Yarowsky D. 1995. Unsupervised Word Sense Disambiguation
Rivaling Supervised Methods. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational Linguis-
tics. pp.189-196.
Zelenko D., Aone C. and Richardella A. 2002. Kernel Meth-
ods for Relation Extraction, In Proceedings of the EMNLP
Conference. Philadelphia.
Zhang Zhu. 2004. Weakly-supervised relation classification for
Information Extraction, In proceedings of ACM 13th con-
ference on Information and Knowledge Management. 8-13
Nov 2004. Washington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min. 2005.
Combining lexical, syntactic and semantic features with
Maximum Entropy Models for extracting relations, In pro-
ceedings of 43th ACL conference. USA.
Zhu Xiaojin and Ghahramani Zoubin. 2002. Learning from
Labeled and Unlabeled Data with Label Propagation. CMU
CALD tech report CMU-CALD-02-107.
28
Learning Word Senses With Feature Selection and Order Identification
Capabilities
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised word sense
learning algorithm, which induces senses of target
word by grouping its occurrences into a ?natural?
number of clusters based on the similarity of their
contexts. For removing noisy words in feature set,
feature selection is conducted by optimizing a clus-
ter validation criterion subject to some constraint in
an unsupervised manner. Gaussian mixture model
and Minimum Description Length criterion are used
to estimate cluster structure and cluster number.
Experimental results show that our algorithm can
find important feature subset, estimate model or-
der (cluster number) and achieve better performance
than another algorithm which requires cluster num-
ber to be provided.
1 Introduction
Sense disambiguation is essential for many lan-
guage applications such as machine translation, in-
formation retrieval, and speech processing (Ide and
Ve?ronis, 1998). Almost all of sense disambigua-
tion methods are heavily dependant on manually
compiled lexical resources. However these lexical
resources often miss domain specific word senses,
even many new words are not included inside.
Learning word senses from free text will help us
dispense of outside knowledge source for defining
sense by only discriminating senses of words. An-
other application of word sense learning is to help
enriching or even constructing semantic lexicons
(Widdows, 2003).
The solution of word sense learning is closely re-
lated to the interpretation of word senses. Different
interpretations of word senses result in different so-
lutions to word sense learning.
One interpretation strategy is to treat a word sense
as a set of synonyms like synset in WordNet. The
committee based word sense discovery algorithm
(Pantel and Lin, 2002) followed this strategy, which
treated senses as clusters of words occurring in sim-
ilar contexts. Their algorithm initially discovered
tight clusters called committees by grouping top
n words similar with target word using average-
link clustering. Then the target word was assigned
to committees if the similarity between them was
above a given threshold. Each committee that the
target word belonged to was interpreted as one of
its senses.
There are two difficulties with this committee
based sense learning. The first difficulty is about
derivation of feature vectors. A feature for target
word here consists of a contextual content word and
its grammatical relationship with target word. Ac-
quisition of grammatical relationship depends on
the output of a syntactic parser. But for some lan-
guages, ex. Chinese, the performance of syntactic
parsing is still a problem. The second difficulty with
this solution is that two parameters are required to
be provided, which control the number of commit-
tees and the number of senses of target word.
Another interpretation strategy is to treat a word
sense as a group of similar contexts of target word.
The context group discrimination (CGD) algorithm
presented in (Schu?tze, 1998) adopted this strategy.
Firstly, their algorithm selected important contex-
tual words using ?2 or local frequency criterion.
With the ?2 based criterion, those contextual words
whose occurrence depended on whether the am-
biguous word occurred were chosen as features.
When using local frequency criterion, their algo-
rithm selected top n most frequent contextual words
as features. Then each context of occurrences of
target word was represented by second order co-
occurrence based context vector. Singular value de-
composition (SVD) was conducted to reduce the di-
mensionality of context vectors. Then the reduced
context vectors were grouped into a pre-defined
number of clusters whose centroids corresponded to
senses of target word.
Some observations can be made about their fea-
ture selection and clustering procedure. One ob-
servation is that their feature selection uses only
first order information although the second order co-
occurrence data is available. The other observation
is about their clustering procedure. Similar with
committee based sense discovery algorithm, their
clustering procedure also requires the predefinition
of cluster number. Their method can capture both
coarse-gained and fine-grained sense distinction as
the predefined cluster number varies. But from a
point of statistical view, there should exist a parti-
tioning of data at which the most reliable, ?natural?
sense clusters appear.
In this paper, we follow the second order repre-
sentation method for contexts of target word, since
it is supposed to be less sparse and more robust than
first order information (Schu?tze, 1998). We intro-
duce a cluster validation based unsupervised fea-
ture wrapper to remove noises in contextual words,
which works by measuring the consistency between
cluster structures estimated from disjoint data sub-
sets in selected feature space. It is based on the
assumption that if selected feature subset is impor-
tant and complete, cluster structure estimated from
data subset in this feature space should be stable
and robust against random sampling. After deter-
mination of important contextual words, we use a
Gaussian mixture model (GMM) based clustering
algorithm (Bouman et al, 1998) to estimate cluster
structure and cluster number by minimizing Min-
imum Description Length (MDL) criterion (Ris-
sanen, 1978). We construct several subsets from
widely used benchmark corpus as test data. Experi-
mental results show that our algorithm (FSGMM )
can find important feature subset, estimate cluster
number and achieve better performance compared
with CGD algorithm.
This paper is organized as follows. In section
2 we will introduce our word sense learning al-
gorithm, which incorporates unsupervised feature
selection and model order identification technique.
Then we will give out the experimental results of
our algorithm and discuss some findings from these
results in section 3. Section 4 will be devoted to
a brief review of related efforts on word sense dis-
crimination. In section 5 we will conclude our work
and suggest some possible improvements.
2 Learning Procedure
2.1 Feature selection
Feature selection for word sense learning is to find
important contextual words which help to discrim-
inate senses of target word without using class la-
bels in data set. This problem can be generalized
as selecting important feature subset in an unsuper-
vised manner. Many unsupervised feature selection
algorithms have been presented, which can be cate-
gorized as feature filter (Dash et al, 2002; Talav-
era, 1999) and feature wrapper (Dy and Brodley,
2000; Law et al, 2002; Mitra et al, 2002; Modha
and Spangler, 2003).
In this paper we propose a cluster valida-
tion based unsupervised feature subset evaluation
method. Cluster validation has been used to solve
model order identification problem (Lange et al,
2002; Levine and Domany, 2001). Table 1 gives
out our feature subset evaluation algorithm. If some
features in feature subset are noises, the estimated
cluster structure on data subset in selected feature
space is not stable, which is more likely to be the
artifact of random splitting. Then the consistency
between cluster structures estimated from disjoint
data subsets will be lower. Otherwise the estimated
cluster structures should be more consistent. Here
we assume that splitting does not eliminate some of
the underlying modes in data set.
For comparison of different clustering structures,
predictors are constructed based on these clustering
solutions, then we use these predictors to classify
the same data subset. The agreement between class
memberships computed by different predictors can
be used as the measure of consistency between clus-
ter structures. We use the stability measure (Lange
et al, 2002) (given in Table 1) to assess the agree-
ment between class memberships.
For each occurrence, one strategy is to construct
its second order context vector by summing the vec-
tors of contextual words, then let the feature selec-
tion procedure start to work on these second order
contextual vectors to select features. However, since
the sense associated with a word?s occurrence is al-
ways determined by very few feature words in its
contexts, it is always the case that there exist more
noisy words than the real features in the contexts.
So, simply summing the contextual word?s vectors
together may result in noise-dominated second or-
der context vectors.
To deal with this problem, we extend the feature
selection procedure further to the construction of
second order context vectors: to select better feature
words in contexts to construct better second order
context vectors enabling better feature selection.
Since the sense associated with a word?s occur-
rence is always determined by some feature words
in its contexts, it is reasonable to suppose that the
selected features should cover most of occurrences.
Formally, let coverage(D,T ) be the coverage rate
of the feature set T with respect to a set of con-
texts D, i.e., the ratio of the number of the occur-
rences with at least one feature in their local con-
texts against the total number of occurrences, then
we assume that coverage(D,T ) ? ? . In practice,
we set ? = 0.9.
This assumption also helps to avoid the bias to-
ward the selection of fewer features, since with
fewer features, there are more occurrences without
features in contexts, and their context vectors will
be zero valued, which tends to result in more stable
cluster structure.
Let D be a set of local contexts of occurrences of
target word, then D = {di}Ni=1, where di represents
local context of the i-th occurrence, and N is the
total number of this word?s occurrences.
W is used to denote bag of words occurring in
context set D, then W = {wi}Mi=1, where wi de-
notes a word occurring in D, and M is the total
number of different contextual words.
Let V denote a M ? M second-order co-
occurrence symmetric matrix. Suppose that the i-th
, 1 ? i ? M , row in the second order matrix corre-
sponds to word wi and the j-th , 1 ? j ? M , col-
umn corresponds to word wj , then the entry speci-
fied by i-th row and j-th column records the number
of times that word wi occurs close to wj in corpus.
We use v(wi) to represent the word vector of con-
textual word wi, which is the i-th row in matrix V .
HT is a weight matrix of contextual word subset
T , T ? W . Then each entry hi,j represents the
weight of word wj in di, wj ? T , 1 ? i ? N . We
use binary term weighting method to derive context
vectors: hi,j = 1 if word wj occurs in di, otherwise
zero.
Let CT = {cTi }Ni=1 be a set of context vectors in
feature space T , where cTi is the context vector of
the i-th occurrence. cTi is defined as:
cTi =
?
j
(hi,jv(wj)), wj ? T, 1 ? i ? N. (1)
The feature subset selection in word set W can be
formulated as:
T? = argmax
T
{criterion(T,H, V, q)}, T ? W, (2)
subject to coverage(D,T ) ? ? , where T? is the op-
timal feature subset, criterion is the cluster valida-
tion based evaluation function (the function in Ta-
ble 1), q is the resampling frequency for estimate
of stability, and coverage(D,T ) is the proportion
of contexts with occurrences of features in T . This
constrained optimization results in a solution which
maximizes the criterion and meets the given con-
straint at the same time. In this paper we use se-
quential greedy forward floating search (Pudil et al,
1994) in sorted word list based on ?2 or local fre-
quency criterion. We set l = 1, m = 1, where l is
plus step, and m is take-away step.
2.2 Clustering with order identification
After feature selection, we employ a Gaussian mix-
ture modelling algorithm, Cluster (Bouman et al,
Table 1: Unsupervised Feature Subset Evaluation Algorithm.
Intuitively, for a given feature subset T , we iteratively split data
set into disjoint halves, and compute the agreement of cluster-
ing solutions estimated from these sets using stability measure.
The average of stability over q resampling is the estimation of
the score of T .
Function criterion(T , H , V , q)
Input parameter: feature subset T , weight matrix H ,
second order co-occurrence matrix V , resampling
frequency q;
(1) ST = 0;
(2) For i = 1 to q do
(2.1) Randomly split CT into disjoint halves, denoted
as CTA and CTB ;
(2.2) Estimate GMM parameter and cluster number on CTA
using Cluster, and the parameter set is denoted as ??A;
The solution ??A can be used to construct a predictor
?A;
(2.3) Estimate GMM parameter and cluster number on CTB
using Cluster, and the parameter set is denoted as ??B ,
The solution ??B can be used to construct a predictor
?B ;
(2.4) Classify CTB using ?A and ?B ;
The class labels assigned by ?A and ?B are denoted
as LA and LB ;
(2.5) ST+ = maxpi 1|CTB |
?
i 1{pi(LA(cTBi)) = LB(cTBi)},
where pi denotes possible permutation relating indices
between LA and LB , and cTBi ? CTB ;
(3) ST = 1qST ;
(4) Return ST ;
1998), to estimate cluster structure and cluster num-
ber. Let Y = {yn}Nn=1 be a set of M dimen-
sional vectors to be modelled by GMM. Assuming
that this model has K subclasses, let pik denote the
prior probability of subclass k, ?k denote the M di-
mensional mean vector for subclass k, Rk denote
the M ?M dimensional covariance matrix for sub-
class k, 1 ? k ? K. The subclass label for pixel
yn is represented by xn. MDL criterion is used
for GMM parameter estimation and order identifi-
cation, which is given by:
MDL(K, ?) = ?
N?
n=1
log (pyn|xn(yn|?)) +
1
2L log (NM),
(3)
pyn|xn(yn|?) =
K?
k=1
pyn|xn(yn|k, ?)pik, (4)
L = K(1 +M + (M + 1)M2 )? 1, (5)
The log likelihood measures the goodness of fit of
a model to data sample, while the second term pe-
nalizes complex model. This estimator works by at-
tempting to find a model order with minimum code
length to describe the data sample Y and parameter
set ?.
If the cluster number is fixed, the estimation of
GMM parameter can be solved using EM algorithm
to address this type of incomplete data problem
(Dempster et al, 1977). The initialization of mix-
ture parameter ?(1) is given by:
pi(1)k =
1
Ko (6)
?(1)k = yn, where n = b(k? 1)(N ? 1)/(Ko? 1)c+1 (7)
R(1)k =
1
N ?
N
n=1ynytn (8)
Ko is a given initial subclass number.
Then EM algorithm is used to estimate model pa-
rameters by minimizing MDL:
E-step: re-estimate the expectations based on pre-
vious iteration:
pxn|yn(k|yn, ?(i)) =
pyn|xn(yn|k, ?(i))pik?K
l=1(pyn|xn(yn|l, ?(i))pil)
, (9)
M-step: estimate the model parameter ?(i) to
maximize the log-likelihood in MDL:
Nk =
N?
n=1
pxn|yn(k|yn, ?(i)) (10)
pik = NkN (11)
?k =
1
Nk
N?
n=1
ynpxn|yn(k|yn, ?(i)) (12)
Rk = 1Nk
N?
n=1
(yn ? ?k)(yn ? ?k)tpxn|yn(k|yn, ?(i))
(13)
pyn|xn(yn|k, ?(i)) =
1
(2pi)M/2 |Rk|
?1/2 exp{?} (14)
? = ?12(yn ? ?k)
tR?1k (yn ? ?k) (15)
The EM iteration is terminated when the change
of MDL(K, ?) is less than ?:
? = 1100(1 +M +
(M + 1)M
2 )log(NM) (16)
For inferring the cluster number, EM algorithm
is applied for each value of K, 1 ? K ? Ko, and
the value K? which minimizes the value of MDL
is chosen as the correct cluster number. To make
this process more efficient, two cluster pair l and m
are selected to minimize the change in MDL crite-
ria when reducing K to K ? 1. These two clusters
l and m are then merged. The resulting parameter
set is chosen as an initial condition for EM iteration
with K ? 1 subclasses. This operation will avoid a
complete minimization with respect to pi, ?, and R
for each value of K.
Table 2: Four ambiguous words, their senses and frequency
distribution of each sense.
Word Sense Percentage
hard not easy (difficult) 82.8%
(adjective) not soft (metaphoric) 9.6%
not soft (physical) 7.6%
interest money paid for the use of money 52.4%
a share in a company or business 20.4%
readiness to give attention 14%
advantage, advancement or favor 9.4%
activity that one gives attention to 3.6%
causing attention to be given to 0.2%
line product 56%
(noun) telephone connection 10.6%
written or spoken text 9.8%
cord 8.6%
division 8.2%
formation 6.8%
serve supply with food 42.6%
(verb) hold an office 33.6%
function as something 16%
provide a service 7.8%
3 Experiments and Evaluation
3.1 Test data
We constructed four datasets from hand-tagged cor-
pus 1 by randomly selecting 500 instances for each
ambiguous word - ?hard?, ?interest?, ?line?, and
?serve?. The details of these datasets are given in
Table 2. Our preprocessing included lowering the
upper case characters, ignoring all words that con-
tain digits or non alpha-numeric characters, remov-
ing words from a stop word list, and filtering out
low frequency words which appeared only once in
entire set. We did not use stemming procedure.
The sense tags were removed when they were used
by FSGMM and CGD. In evaluation procedure,
these sense tags were used as ground truth classes.
A second order co-occurrence matrix for English
words was constructed using English version of
Xinhua News (Jan. 1998-Dec. 1999). The win-
dow size for counting second order co-occurrence
was 50 words.
3.2 Evaluation method for feature selection
For evaluation of feature selection, we used mutual
information between feature subset and class label
set to assess the importance of selected feature sub-
set. Our assessment measure is defined as:
M(T ) = 1|T |
?
w?T
?
l?L
p(w, l)log p(w, l)p(w)p(l) , (17)
where T is the feature subset to be evaluated, T ?
W , L is class label set, p(w, l) is the joint distri-
bution of two variables w and l, p(w) and p(l) are
marginal probabilities. p(w, l) is estimated based
1http://www.d.umn.edu/?tpederse/data.html
on contingency table of contextual word set W and
class label set L. Intuitively, if M(T1) > M(T2),
T1 is more important than T2 since T1 contains more
information about L.
3.3 Evaluation method for clustering result
When assessing the agreement between clustering
result and hand-tagged senses (ground truth classes)
in benchmark data, we encountered the difficulty
that there was no sense tag for each cluster.
In (Lange et al, 2002), they defined a permu-
tation procedure for calculating the agreement be-
tween two cluster memberships assigned by differ-
ent unsupervised learners. In this paper, we applied
their method to assign different sense tags to only
min(|U |, |C|) clusters by maximizing the accuracy,
where |U | is the number of clusters, and |C| is the
number of ground truth classes. The underlying as-
sumption here is that each cluster is considered as
a class, and for any two clusters, they do not share
same class labels. At most |C| clusters are assigned
sense tags, since there are only |C| classes in bench-
mark data.
Given the contingency table Q between clusters
and ground truth classes, each entry Qi,j gives the
number of occurrences which fall into both the i-
th cluster and the j-th ground truth class. If |U | <
|C|, we constructed empty clusters so that |U | =
|C|. Let ? represent a one-to-one mapping function
from C to U . It means that ?(j1) 6= ?(j2) if j1 6=
j2 and vice versa, 1 ? j1, j2 ? |C|. Then ?(j)
is the index of the cluster associated with the j-th
class. Searching a mapping function to maximize
the accuracy of U can be formulated as:
?? = argmax
?
|C|?
j=1
Q?(j),j . (18)
Then the accuracy of solution U is given by
Accuracy(U) =
?
j Q??(j),j?
i,j Qi,j
. (19)
In fact,
?
i,j Qi,j is equal to N , the number of
occurrences of target word in test set.
3.4 Experiments and results
For each dataset, we tested following procedures:
CGDterm:We implemented the context group
discrimination algorithm. Top max(|W | ?
20%, 100) words in contextual word list was se-
lected as features using frequency or ?2 based rank-
ing. Then k-means clustering2 was performed on
context vector matrix using normalized Euclidean
distance. K-means clustering was repeated 5 times
2We used k-means function in statistics toolbox of Matlab.
and the partition with best quality was chosen as fi-
nal result. The number of clusters used by k-means
was set to be identical with the number of ground
truth classes. We tested CGDterm using various
word vector weighting methods when deriving con-
text vectors, ex. binary, idf , tf ? idf .
CGDSV D: The context vector matrix was de-
rived using same method in CGDterm. Then k-
means clustering was conducted on latent seman-
tic space transformed from context vector matrix,
using normalized Euclidean distance. Specifically,
context vectors were reduced to 100 dimensions us-
ing SVD. If the dimension of context vector was
less than 100, all of latent semantic vectors with
non-zero eigenvalue were used for subsequent clus-
tering. We also tested it using different weighting
methods, ex. binary, idf , tf ? idf .
FSGMM : We performed cluster validation
based feature selection in feature set used by CGD.
Then Cluster algorithm was used to group target
word?s instances using Euclidean distance measure.
? was set as 0.90 in feature subset search procedure.
The random splitting frequency is set as 10 for es-
timation of the score of feature subset. The initial
subclass number was 20 and full covariance matrix
was used for parameter estimation of each subclass.
For investigating the effect of different context
window size on the performance of three proce-
dures, we tested these procedures using various con-
text window sizes: ?1, ?5, ?15, ?25, and all of
contextual words. The average length of sentences
in 4 datasets is 32 words before preprocessing. Per-
formance on each dataset was assessed by equation
19.
The scores of feature subsets selected by
FSGMM and CGD are listed in Table 3 and
4. The average accuracy of three procedures with
different feature ranking and weighting method is
given in Table 5. Each figure is the average over 5
different context window size and 4 datasets. We
give out the detailed results of these three proce-
dures in Figure 1. Several results should be noted
specifically:
From Table 3 and 4, we can find that FSGMM
achieved better score on mutual information (MI)
measure than CGD over 35 out of total 40 cases.
This is the evidence that our feature selection pro-
cedure can remove noise and retain important fea-
tures.
As it was shown in Table 5, with both ?2 and
freq based feature ranking, FSGMM algorithm
performed better than CGDterm and CGDSV D if
we used average accuracy to evaluate their per-
formance. Specifically, with ?2 based feature
ranking, FSGMM attained 55.4% average accu-
racy, while the best average accuracy of CGDterm
and CGDSV D were 40.9% and 51.3% respec-
tively. With freq based feature ranking, FSGMM
achieved 51.2% average accuracy, while the best av-
erage accuracy of CGDterm and CGDSV D were
45.1% and 50.2%.
The automatically estimated cluster numbers by
FSGMM over 4 datasets are given in Table 6.
The estimated cluster number was 2 ? 4 for ?hard?,
3 ? 6 for ?interest?, 3 ? 6 for ?line?, and 2 ? 4
for ?serve?. It is noted that the estimated cluster
number was less than the number of ground truth
classes in most cases. There are some reasons for
this phenomenon. First, the data is not balanced,
which may lead to that some important features can-
not be retrieved. For example, the fourth sense of
?serve?, and the sixth sense of ?line?, their corre-
sponding features are not up to the selection criteria.
Second, some senses can not be distinguished using
only bag-of-words information, and their difference
lies in syntactic information held by features. For
example, the third sense and the sixth sense of ?in-
terest? may be distinguished by syntactic relation of
feature words, while the bag of feature words occur-
ring in their context are similar. Third, some senses
are determined by global topics, rather than local
contexts. For example, according to global topics, it
may be easier to distinguish the first and the second
sense of ?interest?.
Figure 2 shows the average accuracy over three
procedures in Figure 1 as a function of context
window size for 4 datasets. For ?hard?, the per-
formance dropped as window size increased, and
the best accuracy(77.0%) was achieved at win-
dow size 1. For ?interest?, sense discrimination
did not benefit from large window size and the
best accuracy(40.1%) was achieved at window size
5. For ?line?, accuracy dropped when increas-
ing window size and the best accuracy(50.2%) was
achieved at window size 1. For ?serve?, the per-
formance benefitted from large window size and the
best accuracy(46.8%) was achieved at window size
15.
In (Leacock et al, 1998), they used Bayesian ap-
proach for sense disambiguation of three ambiguous
words, ?hard?, ?line?, and ?serve?, based on cues
from topical and local context. They observed that
local context was more reliable than topical context
as an indicator of senses for this verb and adjective,
but slightly less reliable for this noun. Compared
with their conclusion, we can find that our result
is consistent with it for ?hard?. But there is some
differences for verb ?serve? and noun ?line?. For
Table 3: Mutual information between feature subset and class
label with ?2 based feature ranking.
Word Cont. Size of MI Size of MI
wind. feature ?10?2 feature ?10?2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4018 80 0.4300
15 100 0.1362 80 0.1416
25 133 0.0997 102 0.1003
all 145 0.0937 107 0.0890
interest 1 64 1.9697 55 2.0639
5 100 0.3234 89 0.3355
15 157 0.1558 124 0.1531
25 190 0.1230 138 0.1267
all 200 0.1163 140 0.1191
line 1 39 4.2089 32 4.6456
5 100 0.4628 84 0.4871
15 183 0.1488 128 0.1429
25 263 0.1016 163 0.0962
all 351 0.0730 192 0.0743
serve 1 22 6.8169 20 6.7043
5 100 0.5057 85 0.5227
15 188 0.2078 164 0.2094
25 255 0.1503 225 0.1536
all 320 0.1149 244 0.1260
Table 4: Mutual information between feature subset and class
label with freq based feature ranking.
Word Cont. Size of MI Size of MI
wind. feature ?10?2 feature ?10?2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4194 80 0.4832
15 100 0.1647 80 0.1774
25 133 0.1150 102 0.1259
all 145 0.1064 107 0.1269
interest 1 64 1.9697 55 2.7051
5 100 0.6015 89 0.8309
15 157 0.2526 124 0.3495
25 190 0.1928 138 0.2982
all 200 0.1811 140 0.2699
line 1 39 4.2089 32 4.4606
5 100 0.6895 84 0.7816
15 183 0.2301 128 0.2929
25 263 0.1498 163 0.2181
all 351 0.1059 192 0.1630
serve 1 22 6.8169 20 7.0021
5 100 0.7045 85 0.8422
15 188 0.2763 164 0.3418
25 255 0.1901 225 0.2734
all 320 0.1490 244 0.2309
?serve?, the possible reason is that we do not use
position of local word and part of speech informa-
tion, which may deteriorate the performance when
local context(? 5 words) is used. For ?line?, the
reason might come from the feature subset, which
is not good enough to provide improvement when
Table 5: Average accuracy of three procedures with various
settings over 4 datasets.
Algorithm Feature Feature Average
ranking weighting accuracy
method method
FSGMM ?2 binary 0.554
CGDterm ?2 binary 0.404
CGDterm ?2 idf 0.407
CGDterm ?2 tf ? idf 0.409
CGDSVD ?2 binary 0.513
CGDSVD ?2 idf 0.512
CGDSVD ?2 tf ? idf 0.508
FSGMM freq binary 0.512
CGDterm freq binary 0.451
CGDterm freq idf 0.437
CGDterm freq tf ? idf 0.447
CGDSVD freq binary 0.502
CGDSVD freq idf 0.498
CGDSVD freq tf ? idf 0.485
Table 6: Automatically determined mixture component num-
ber.
Word Context Model Model
window order order
size with ?2 with freq
hard 1 3 4
5 2 2
15 2 3
25 2 3
all 2 3
interest 1 5 4
5 3 4
15 4 6
25 4 6
all 3 4
line 1 5 6
5 4 3
15 5 4
25 5 4
all 3 4
serve 1 3 3
5 3 4
15 3 3
25 3 3
all 2 4
context window size is no less than 5.
4 Related Work
Besides the two works (Pantel and Lin, 2002;
Schu?tze, 1998), there are other related efforts on
word sense discrimination (Dorow and Widdows,
2003; Fukumoto and Suzuki, 1999; Pedersen and
Bruce, 1997).
In (Pedersen and Bruce, 1997), they described an
experimental comparison of three clustering algo-
rithms for word sense discrimination. Their feature
sets included morphology of target word, part of
speech of contextual words, absence or presence of
particular contextual words, and collocation of fre-
0 1 5 15 25 all0.4
0.5
0.6
0.7
0.8
0.9
Hard dataset
Acc
ura
cy
0 1 5 15 25 all0.2
0.3
0.4
0.5
0.6
Acc
ura
cy
Interest dataset
0 1 5 15 25 all0.2
0.3
0.4
0.5
0.6
0.7
Line dataset
Acc
ura
cy
0 1 5 15 25 all0.3
0.35
0.4
0.45
0.5
0.55
0.6
Serve dataset
Acc
ura
cy
Figure 1: Results for three procedures over 4 datases. The
horizontal axis corresponds to the context window size. Solid
line represents the result of FSGMM + binary, dashed line
denotes the result of CGDSVD + idf , and dotted line is the
result of CGDterm + idf . Square marker denotes ?2 based
feature ranking, while cross marker denotes freq based feature
ranking.
0 1 5 15 25 all0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Ave
rage
 Acc
urac
y
Hard datasetInterest datasetLine datasetServe dataset
Figure 2: Average accuracy over three procedures in Figure
1 as a function of context window size (horizontal axis) for 4
datasets.
quent words. Then occurrences of target word were
grouped into a pre-defined number of clusters. Sim-
ilar with many other algorithms, their algorithm also
required the cluster number to be provided.
In (Fukumoto and Suzuki, 1999), a term weight
learning algorithm was proposed for verb sense dis-
ambiguation, which can automatically extract nouns
co-occurring with verbs and identify the number of
senses of an ambiguous verb. The weakness of their
method is to assume that nouns co-occurring with
verbs are disambiguated in advance and the number
of senses of target verb is no less than two.
The algorithm in (Dorow and Widdows, 2003)
represented target noun word, its neighbors and
their relationships using a graph in which each node
denoted a noun and two nodes had an edge between
them if they co-occurred with more than a given
number of times. Then senses of target word were
iteratively learned by clustering the local graph of
similar words around target word. Their algorithm
required a threshold as input, which controlled the
number of senses.
5 Conclusion and Future Work
Our word sense learning algorithm combined two
novel ingredients: feature selection and order iden-
tification. Feature selection was formalized as a
constrained optimization problem, the output of
which was a set of important features to determine
word senses. Both cluster structure and cluster num-
ber were estimated by minimizing a MDL crite-
rion. Experimental results showed that our algo-
rithm can retrieve important features, estimate clus-
ter number automatically, and achieve better per-
formance in terms of average accuracy than CGD
algorithm which required cluster number as input.
Our word sense learning algorithm is unsupervised
in two folds: no requirement of sense tagged data,
and no requirement of predefinition of sense num-
ber, which enables the automatic discovery of word
senses from free text.
In our algorithm, we treat bag of words in lo-
cal contexts as features. It has been shown that
local collocations and morphology of target word
play important roles in word sense disambiguation
or discrimination (Leacock et al, 1998; Widdows,
2003). It is necessary to incorporate these more
structural information to improve the performance
of word sense learning.
References
Bouman, C. A., Shapiro, M., Cook, G. W., Atkins,
C. B., & Cheng, H. (1998) Cluster: An
Unsupervsied Algorithm for Modeling Gaus-
sian Mixtures. http://dynamo.ecn.purdue.edu/
?bouman/software/cluster/.
Dash, M., Choi, K., Scheuermann, P., & Liu, H. (2002)
Feature Selection for Clustering - A Filter Solution.
Proc. of IEEE Int. Conf. on Data Mining(pp. 115?
122).
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977)
Maximum likelihood from incomplete data using the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(B).
Dorow, B, & Widdows, D. (2003) Discovering Corpus-
Specific Word Senses. Proc. of the 10th Conf. of the
European Chapter of the Association for Computa-
tional Linguistics, Conference Companion (research
notes and demos)(pp.79?82).
Dy, J. G., & Brodley, C. E. (2000) Feature Subset Selec-
tion and Order Identification for Unsupervised Learn-
ing. Proc. of the 17th Int. Conf. on Machine Learn-
ing(pp. 247?254).
Fukumoto, F., & Suzuki, Y. (1999) Word Sense Disam-
biguation in Untagged Text Based on Term Weight
Learning. Proc. of the 9th Conf. of European Chapter
of the Association for Computational Linguistics(pp.
209?216).
Ide, N., & Ve?ronis, J. (1998) Word Sense Disambigua-
tion: The State of the Art. Computational Linguistics,
24:1, 1?41.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M. (2002)
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Law, M. H., Figueiredo, M., & Jain, A. K. (2002) Fea-
ture Selection in Mixture-Based Clustering. Advances
in Neural Information Processing Systems 15.
Leacock, C., Chodorow, M., & Miller A. G. (1998) Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24:1, 147?
165.
Levine, E., & Domany, E. (2001) Resampling Method
for Unsupervised Estimation of Cluster Validity. Neu-
ral Computation, Vol. 13, 2573?2593.
Mitra, P., Murthy, A. C., & Pal, K. S. (2002) Unsu-
pervised Feature Selection Using Feature Similarity.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 24:4, 301?312.
Modha, D. S., & Spangler, W. S. (2003) Feature Weight-
ing in k-Means Clustering. Machine Learning, 52:3,
217?237.
Pantel, P. & Lin, D. K. (2002) Discovering Word Senses
from Text. Proc. of ACM SIGKDD Conf. on Knowl-
edge Discovery and Data Mining(pp. 613-619).
Pedersen, T., & Bruce, R. (1997) Distinguishing Word
Senses in Untagged Text. Proceedings of the 2nd
Conference on Empirical Methods in Natural Lan-
guage Processing(pp. 197?207).
Pudil, P., Novovicova, J., & Kittler, J. (1994) Floating
Search Methods in Feature Selection. Pattern Recog-
nigion Letters, Vol. 15, 1119-1125.
Rissanen, J. (1978) Modeling by Shortest Data Descrip-
tion. Automatica, Vol. 14, 465?471.
Schu?tze, H. (1998) Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:1, 97?123.
Talavera, L. (1999) Feature Selection as a Preprocessing
Step for Hierarchical Clustering. Proc. of the 16th Int.
Conf. on Machine Learning(pp. 389?397).
Widdows, D. (2003) Unsupervised methods for devel-
oping taxonomies by combining syntactic and statisti-
cal information. Proc. of the Human Language Tech-
nology / Conference of the North American Chapter
of the Association for Computational Linguistics(pp.
276?283).
Proceedings of the 43rd Annual Meeting of the ACL, pages 395?402,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Word Sense Disambiguation Using Label Propagation Based
Semi-Supervised Learning
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
Shortage of manually sense-tagged data is
an obstacle to supervised word sense dis-
ambiguation methods. In this paper we in-
vestigate a label propagation based semi-
supervised learning algorithm for WSD,
which combines labeled and unlabeled
data in learning process to fully realize
a global consistency assumption: simi-
lar examples should have similar labels.
Our experimental results on benchmark
corpora indicate that it consistently out-
performs SVM when only very few la-
beled examples are available, and its per-
formance is also better than monolingual
bootstrapping, and comparable to bilin-
gual bootstrapping.
1 Introduction
In this paper, we address the problem of word sense
disambiguation (WSD), which is to assign an appro-
priate sense to an occurrence of a word in a given
context. Many methods have been proposed to deal
with this problem, including supervised learning al-
gorithms (Leacock et al, 1998), semi-supervised
learning algorithms (Yarowsky, 1995), and unsuper-
vised learning algorithms (Schu?tze, 1998).
Supervised sense disambiguation has been very
successful, but it requires a lot of manually sense-
tagged data and can not utilize raw unannotated data
that can be cheaply acquired. Fully unsupervised
methods do not need the definition of senses and
manually sense-tagged data, but their sense cluster-
ing results can not be directly used in many NLP
tasks since there is no sense tag for each instance in
clusters. Considering both the availability of a large
amount of unlabelled data and direct use of word
senses, semi-supervised learning methods have re-
ceived great attention recently.
Semi-supervised methods for WSD are character-
ized in terms of exploiting unlabeled data in learning
procedure with the requirement of predefined sense
inventory for target words. They roughly fall into
three categories according to what is used for su-
pervision in learning process: (1) using external re-
sources, e.g., thesaurus or lexicons, to disambiguate
word senses or automatically generate sense-tagged
corpus, (Lesk, 1986; Lin, 1997; McCarthy et al,
2004; Seo et al, 2004; Yarowsky, 1992), (2) exploit-
ing the differences between mapping of words to
senses in different languages by the use of bilingual
corpora (e.g. parallel corpora or untagged monolin-
gual corpora in two languages) (Brown et al, 1991;
Dagan and Itai, 1994; Diab and Resnik, 2002; Li and
Li, 2004; Ng et al, 2003), (3) bootstrapping sense-
tagged seed examples to overcome the bottleneck of
acquisition of large sense-tagged data (Hearst, 1991;
Karov and Edelman, 1998; Mihalcea, 2004; Park et
al., 2000; Yarowsky, 1995).
As a commonly used semi-supervised learning
method for WSD, bootstrapping algorithm works
by iteratively classifying unlabeled examples and
adding confidently classified examples into labeled
dataset using a model learned from augmented la-
beled dataset in previous iteration. It can be found
that the affinity information among unlabeled ex-
amples is not fully explored in this bootstrapping
process. Bootstrapping is based on a local consis-
tency assumption: examples close to labeled exam-
ples within same class will have same labels, which
is also the assumption underlying many supervised
learning algorithms, such as kNN.
Recently a promising family of semi-supervised
learning algorithms are introduced, which can ef-
fectively combine unlabeled data with labeled data
395
in learning process by exploiting cluster structure
in data (Belkin and Niyogi, 2002; Blum et al,
2004; Chapelle et al, 1991; Szummer and Jaakkola,
2001; Zhu and Ghahramani, 2002; Zhu et al, 2003).
Here we investigate a label propagation based semi-
supervised learning algorithm (LP algorithm) (Zhu
and Ghahramani, 2002) for WSD, which works by
representing labeled and unlabeled examples as ver-
tices in a connected graph, then iteratively propagat-
ing label information from any vertex to nearby ver-
tices through weighted edges, finally inferring the
labels of unlabeled examples after this propagation
process converges.
Compared with bootstrapping, LP algorithm is
based on a global consistency assumption. Intu-
itively, if there is at least one labeled example in each
cluster that consists of similar examples, then unla-
beled examples will have the same labels as labeled
examples in the same cluster by propagating the la-
bel information of any example to nearby examples
according to their proximity.
This paper is organized as follows. First, we will
formulate WSD problem in the context of semi-
supervised learning in section 2. Then in section
3 we will describe LP algorithm and discuss the
difference between a supervised learning algorithm
(SVM), bootstrapping algorithm and LP algorithm.
Section 4 will provide experimental results of LP al-
gorithm on widely used benchmark corpora. Finally
we will conclude our work and suggest possible im-
provement in section 5.
2 Problem Setup
Let X = {xi}ni=1 be a set of contexts of occur-
rences of an ambiguous word w, where xi repre-
sents the context of the i-th occurrence, and n is
the total number of this word?s occurrences. Let
S = {sj}cj=1 denote the sense tag set of w. The first
l examples xg(1 ? g ? l) are labeled as yg (yg ? S)
and other u (l+u = n) examples xh(l+1 ? h ? n)
are unlabeled. The goal is to predict the sense of w
in context xh by the use of label information of xg
and similarity information among examples in X .
The cluster structure in X can be represented as a
connected graph, where each vertex corresponds to
an example, and the edge between any two examples
xi and xj is weighted so that the closer the vertices
in some distance measure, the larger the weight as-
sociated with this edge. The weights are defined as
follows: Wij = exp(?
d2ij
?2 ) if i 6= j and Wii = 0
(1 ? i, j ? n), where dij is the distance (ex. Euclid-
ean distance) between xi and xj , and ? is used to
control the weight Wij .
3 Semi-supervised Learning Method
3.1 Label Propagation Algorithm
In LP algorithm (Zhu and Ghahramani, 2002), label
information of any vertex in a graph is propagated
to nearby vertices through weighted edges until a
global stable stage is achieved. Larger edge weights
allow labels to travel through easier. Thus the closer
the examples, more likely they have similar labels
(the global consistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 ? Nn?c represent initial soft labels at-
tached to vertices, where Y 0ij = 1 if yi is sj and 0
otherwise. Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent with the
labeling in labeled data, and the initialization of Y 0U
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set ? as the aver-
age distance between labeled examples from differ-
ent classes.
Define n ? n probability transition matrix Tij =
P (j ? i) = Wij?n
k=1 Wkj
, where Tij is the probability
to jump from example xj to example xi.
Compute the row-normalized matrix T by T ij =
Tij/
?n
k=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
396
?2 ?1 0 1 2 3 4
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
labeled +1
unlabeled
labeled ?1
(a) Dataset with Two?Moon Pattern (b) SVM 
(c) Bootstrapping (d) Ideal Classification 
A 8
A 9
B8 
B9
A10 
B10 
A0 
B0 
Figure 1: Classification result on two-moon pattern dataset.
(a) Two-moon pattern dataset with two labeled points, (b) clas-
sification result by SVM, (c) labeling procedure of bootstrap-
ping algorithm, (d) ideal classification.
Then LP algorithm is defined as follows:
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Y t+1 = TY t;
3. Clamp labeled data by replacing the top l row
of Y t+1 with Y 0L . Repeat from step 2 until Y t con-
verges;
4. Assign xh(l + 1 ? h ? n) with a label sj? ,
where j? = argmaxjYhj .
This algorithm has been shown to converge to
a unique solution, which is Y?U = limt?? Y tU =
(I ? T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of Y 0U is not im-
portant, since Y 0U does not affect the estimation of
Y?U . I is u ? u identity matrix. T uu and T ul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
3.2 Comparison between SVM, Bootstrapping
and LP
For WSD, SVM is one of the state of the art super-
vised learning algorithms (Mihalcea et al, 2004),
while bootstrapping is one of the state of the art
semi-supervised learning algorithms (Li and Li,
2004; Yarowsky, 1995). For comparing LP with
SVM and bootstrapping, let us consider a dataset
with two-moon pattern shown in Figure 1(a). The
upper moon consists of 9 points, while the lower
moon consists of 13 points. There is only one la-
beled point in each moon, and other 20 points are un-
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
?2 ?1 0 1 2 3
?2
?1
0
1
2
(a) Minimum Spanning Tree (b) t=1 
(c) t=7 (d) t=10
(e) t=12 (f) t=100
B 
A 
C 
Figure 2: Classification result of LP on two-moon pattern
dataset. (a) Minimum spanning tree of this dataset. The conver-
gence process of LP algorithm with t varying from 1 to 100 is
shown from (b) to (f).
labeled. The distance metric is Euclidian distance.
We can see that the points in one moon should be
more similar to each other than the points across the
moons.
Figure 1(b) shows the classification result of
SVM. Vertical line denotes classification hyper-
plane, which has the maximum separating margin
with respect to the labeled points in two classes. We
can see that SVM does not work well when labeled
data can not reveal the structure (two moon pattern)
in each class. The reason is that the classification
hyperplane was learned only from labeled data. In
other words, the coherent structure (two-moon pat-
tern) in unlabeled data was not explored when infer-
ring class boundary.
Figure 1(c) shows bootstrapping procedure using
kNN (k=1) as base classifier with user-specified pa-
rameter b = 1 (the number of added examples from
unlabeled data into classified data for each class in
each iteration). Termination condition is that the dis-
tance between labeled and unlabeled points is more
than inter-class distance (the distance between A0
and B0). Each arrow in Figure 1(c) represents
one classification operation in each iteration for each
class. After eight iterations, A1 ? A8 were tagged
397
as +1, and B1 ? B8 were tagged as ?1, while
A9 ? A10 and B9 ? B10 were still untagged. Then
at the ninth iteration, A9 was tagged as +1 since the
label of A9 was determined only by labeled points in
kNN model: A9 is closer to any point in {A0 ? A8}
than to any point in {B0 ? B8}, regardless of the
intrinsic structure in data: A9 ? A10 and B9 ? B10
are closer to points in lower moon than to points in
upper moon. In other words, bootstrapping method
uses the unlabeled data under a local consistency
based strategy. This is the reason that two points A9
and A10 are misclassified (shown in Figure 1(c)).
From above analysis we see that both SVM and
bootstrapping are based on a local consistency as-
sumption.
Finally we ran LP on a connected graph-minimum
spanning tree generated for this dataset, shown in
Figure 2(a). A, B, C represent three points, and
the edge A ? B connects the two moons. Figure
2(b)- 2(f) shows the convergence process of LP with
t increasing from 1 to 100. When t = 1, label in-
formation of labeled data was pushed to only nearby
points. After seven iteration steps (t = 7), point B
in upper moon was misclassified as ?1 since it first
received label information from point A through the
edge connecting two moons. After another three it-
eration steps (t=10), this misclassified point was re-
tagged as +1. The reason of this self-correcting be-
havior is that with the push of label information from
nearby points, the value of YB,+1 became higher
than YB,?1. In other words, the weight of edge
B ? C is larger than that of edge B ? A, which
makes it easier for +1 label of point C to travel to
point B. Finally, when t ? 12 LP converged to a
fixed point, which achieved the ideal classification
result.
4 Experiments and Results
4.1 Experiment Design
For empirical comparison with SVM and bootstrap-
ping, we evaluated LP on widely used benchmark
corpora - ?interest?, ?line? 1 and the data in English
lexical sample task of SENSEVAL-3 (including all
57 English words ) 2.
1Available at http://www.d.umn.edu/?tpederse/data.html
2Available at http://www.senseval.org/senseval3
Table 1: The upper two tables summarize accuracies (aver-
aged over 20 trials) and paired t-test results of SVM and LP on
SENSEVAL-3 corpus with percentage of training set increasing
from 1% to 100%. The lower table lists the official result of
baseline (using most frequent sense heuristics) and top 3 sys-
tems in ELS task of SENSEVAL-3.
Percentage SVM LPcosine LPJS
1% 24.9?2.7% 27.5?1.1% 28.1?1.1%
10% 53.4?1.1% 54.4?1.2% 54.9?1.1%
25% 62.3?0.7% 62.3?0.7% 63.3?0.9%
50% 66.6?0.5% 65.7?0.5% 66.9?0.6%
75% 68.7?0.4% 67.3?0.4% 68.7?0.3%
100% 69.7% 68.4% 70.3%
Percentage SVM vs. LPcosine SVM vs. LPJS
p-value Sign. p-value Sign.
1% 8.7e-004 ? 8.5e-005 ?
10% 1.9e-006 ? 1.0e-008 ?
25% 9.2e-001 ? 3.0e-006 ?
50% 1.9e-006 ? 6.2e-002 ?
75% 7.4e-013 ? 7.1e-001 ?
100% - - - -
Systems Baseline htsa3 IRST-Kernels nusels
Accuracy 55.2% 72.9% 72.6% 72.4%
We used three types of features to capture con-
textual information: part-of-speech of neighboring
words with position information, unordered sin-
gle words in topical context, and local collocations
(as same as the feature set used in (Lee and Ng,
2002) except that we did not use syntactic relations).
For SVM, we did not perform feature selection on
SENSEVAL-3 data since feature selection deterio-
rates its performance (Lee and Ng, 2002). When
running LP on the three datasets, we removed the
features with occurrence frequency (counted in both
training set and test set) less than 3 times.
We investigated two distance measures for LP: co-
sine similarity and Jensen-Shannon (JS) divergence
(Lin, 1991).
For the three datasets, we constructed connected
graphs following (Zhu et al, 2003): two instances
u, v will be connected by an edge if u is among v?s
k nearest neighbors, or if v is among u?s k nearest
neighbors as measured by cosine or JS distance mea-
sure. For ?interest? and ?line? corpora, k is 10 (fol-
lowing (Zhu et al, 2003)), while for SENSEVAL-3
data, k is 5 since the size of dataset for each word
in SENSEVAL-3 is much less than that of ?interest?
and ?line? datasets.
398
4.2 Experiment 1: LP vs. SVM
In this experiment, we evaluated LP and SVM
3 on the data of English lexical sample task in
SENSEVAL-3. We used l examples from training
set as labeled data, and the remaining training ex-
amples and all the test examples as unlabeled data.
For each labeled set size l, we performed 20 trials.
In each trial, we randomly sampled l labeled exam-
ples for each word from training set. If any sense
was absent from the sampled labeled set, we redid
the sampling. We conducted experiments with dif-
ferent values of l, including 1%?Nw,train, 10%?
Nw,train, 25%?Nw,train, 50%?Nw,train, 75%?
Nw,train, 100%?Nw,train (Nw,train is the number
of examples in training set of word w). SVM and LP
were evaluated using accuracy 4 (fine-grained score)
on test set of SENSEVAL-3.
We conducted paired t-test on the accuracy fig-
ures for each value of l. Paired t-test is not run when
percentage= 100%, since there is only one paired
accuracy figure. Paired t-test is usually used to esti-
mate the difference in means between normal pop-
ulations based on a set of random paired observa-
tions. {?, ?}, {<, >}, and ? correspond to p-
value ? 0.01, (0.01, 0.05], and > 0.05 respectively.
? (or ?) means that the performance of LP is sig-
nificantly better (or significantly worse) than SVM.
< (or >) means that the performance of LP is better
(or worse) than SVM.?means that the performance
of LP is almost as same as SVM.
Table 1 reports the average accuracies and paired
t-test results of SVM and LP with different sizes
of labled data. It also lists the official results of
baseline method and top 3 systems in ELS task of
SENSEVAL-3.
From Table 1, we see that with small labeled
dataset (percentage of labeled data ? 10%), LP per-
forms significantly better than SVM. When the per-
centage of labeled data increases from 50% to 75%,
the performance of LPJS and SVM become almost
same, while LPcosine performs significantly worse
than SVM.
3we used linear SVM light, available at
http://svmlight.joachims.org/.
4If there are multiple sense tags for an instance in training
set or test set, then only the first tag is considered as correct
answer. Furthermore, if the answer of the instance in test set is
?U?, then this instance will be removed from test set.
Table 2: Accuracies from (Li and Li, 2004) and average ac-
curacies of LP with c ? b labeled examples on ?interest? and
?line? corpora. Major is a baseline method in which they al-
ways choose the most frequent sense. MB-D denotes monolin-
gual bootstrapping with decision list as base classifier, MB-B
represents monolingual bootstrapping with ensemble of Naive
Bayes as base classifier, and BB is bilingual bootstrapping with
ensemble of Naive Bayes as base classifier.
Ambiguous Accuracies from (Li and Li, 2004)
words Major MB-D MB-B BB
interest 54.6% 54.7% 69.3% 75.5%
line 53.5% 55.6% 54.1% 62.7%
Ambiguous Our results
words #labeled examples LPcosine LPJS
interest 4?15=60 80.2?2.0% 79.8?2.0%
line 6?15=90 60.3?4.5% 59.4?3.9%
4.3 Experiment 2: LP vs. Bootstrapping
Li and Li (2004) used ?interest? and ?line? corpora
as test data. For the word ?interest?, they used its
four major senses. For comparison with their re-
sults, we took reduced ?interest? corpus (constructed
by retaining four major senses) and complete ?line?
corpus as evaluation data. In their algorithm, c is
the number of senses of ambiguous word, and b
(b = 15) is the number of examples added into clas-
sified data for each class in each iteration of boot-
strapping. c ? b can be considered as the size of
initial labeled data in their bootstrapping algorithm.
We ran LP with 20 trials on reduced ?interest? cor-
pus and complete ?line? corpus. In each trial, we
randomly sampled b labeled examples for each sense
of ?interest? or ?line? as labeled data. The rest
served as both unlabeled data and test data.
Table 2 summarizes the average accuracies of LP
on the two corpora. It also lists the accuracies of
monolingual bootstrapping algorithm (MB), bilin-
gual bootstrapping algorithm (BB) on ?interest? and
?line? corpora. We can see that LP performs much
better than MB-D and MB-B on both ?interest? and
?line? corpora, while the performance of LP is com-
parable to BB on these two corpora.
4.4 An Example: Word ?use?
For investigating the reason for LP to outperform
SVM and monolingual bootstrapping, we used the
data of word ?use? in English lexical sample task of
SENSEVAL-3 as an example (totally 26 examples
in training set and 14 examples in test set). For data
399
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
?0.4 ?0.2 0 0.2 0.4 0.6
?0.5
0
0.5
(a) Initial Setting (b) Ground?truth
(c) SVM (d) Bootstrapping
(e) Bootstrapping (f) LP
B A 
C 
Figure 3: Comparison of sense disambiguation results be-
tween SVM, monolingual bootstrapping and LP on word ?use?.
(a) only one labeled example for each sense of word ?use?
as training data before sense disambiguation (? and ? denote
the unlabeled examples in SENSEVAL-3 training set and test
set respectively, and other five symbols (+, ?, ?, ?, and ?)
represent the labeled examples with different sense tags sam-
pled from SENSEVAL-3 training set.), (b) ground-truth re-
sult, (c) classification result on SENSEVAL-3 test set by SVM
(accuracy= 314 = 21.4%), (d) classified data after bootstrap-
ping, (e) classification result on SENSEVAL-3 training set and
test set by 1NN (accuracy= 614 = 42.9% ), (f) classifica-
tion result on SENSEVAL-3 training set and test set by LP
(accuracy= 1014 = 71.4% ).
visualization, we conducted unsupervised nonlinear
dimensionality reduction5 on these 40 feature vec-
tors with 210 dimensions. Figure 3 (a) shows the
dimensionality reduced vectors in two-dimensional
space. We randomly sampled only one labeled ex-
ample for each sense of word ?use? as labeled data.
The remaining data in training set and test set served
as unlabeled data for bootstrapping and LP. All of
these three algorithms are evaluated using accuracy
on test set.
From Figure 3 (c) we can see that SVM misclassi-
5We used Isomap to perform dimensionality reduction by
computing two-dimensional, 39-nearest-neighbor-preserving
embedding of 210-dimensional input. Isomap is available at
http://isomap.stanford.edu/.
fied many examples from class + into class ? since
using only features occurring in training set can not
reveal the intrinsic structure in full dataset.
For comparison, we implemented monolingual
bootstrapping with kNN (k=1) as base classifier.
The parameter b is set as 1. Only b unlabeled ex-
amples nearest to labeled examples and with the
distance less than dinter?class (the minimum dis-
tance between labeled examples with different sense
tags) will be added into classified data in each itera-
tion till no such unlabeled examples can be found.
Firstly we ran this monolingual bootstrapping on
this dataset to augment initial labeled data. The re-
sulting classified data is shown in Figure 3 (d). Then
a 1NN model was learned on this classified data and
we used this model to perform classification on the
remaining unlabeled data. Figure 3 (e) reports the
final classification result by this 1NN model. We can
see that bootstrapping does not perform well since it
is susceptible to small noise in dataset. For example,
in Figure 3 (d), the unlabeled example B 6 happened
to be closest to labeled example A, then 1NN model
tagged example B with label ?. But the correct label
of B should be + as shown in Figure 3 (b). This
error caused misclassification of other unlabeled ex-
amples that should have label +.
In LP, the label information of example C can
travel to B through unlabeled data. Then example A
will compete with C and other unlabeled examples
around B when determining the label of B. In other
words, the labels of unlabeled examples are deter-
mined not only by nearby labeled examples, but also
by nearby unlabeled examples. Using this classifi-
cation strategy achieves better performance than the
local consistency based strategy adopted by SVM
and bootstrapping.
4.5 Experiment 3: LPcosine vs. LPJS
Table 3 summarizes the performance comparison
between LPcosine and LPJS on three datasets. We
can see that on SENSEVAL-3 corpus, LPJS per-
6In the two-dimensional space, example B is not the closest
example to A. The reason is that: (1) A is not close to most
of nearby examples around B, and B is not close to most of
nearby examples around A; (2) we used Isomap to maximally
preserve the neighborhood information between any example
and all other examples, which caused the loss of neighborhood
information between a few example pairs for obtaining a glob-
ally optimal solution.
400
Table 3: Performance comparison between LPcosine and
LPJS and the results of three model selection criteria are re-
ported in following two tables. In the lower table, < (or >)
means that the average value of function H(Qcosine) is lower
(or higher) than H(QJS), and it will result in selecting cosine
(or JS) as distance measure. Qcosine (or QJS) represents a ma-
trix using cosine similarity (or JS divergence). ? and ? denote
correct and wrong prediction results respectively, while ?means
that any prediction is acceptable.
LPcosine vs. LPJS
Data p-value Significance
SENSEVAL-3 (1%) 1.1e-003 ?
SENSEVAL-3 (10%) 8.9e-005 ?
SENSEVAL-3 (25%) 9.0e-009 ?
SENSEVAL-3 (50%) 3.2e-010 ?
SENSEVAL-3 (75%) 7.7e-013 ?
SENSEVAL-3 (100%) - -
interest 3.3e-002 >
line 8.1e-002 ?
H(D) H(W ) H(YU )
Data cos. vs. JS cos. vs. JS cos. vs. JS
SENSEVAL-3 (1%) > (?) > (?) < (?)
SENSEVAL-3 (10%) < (?) > (?) < (?)
SENSEVAL-3 (25%) < (?) > (?) < (?)
SENSEVAL-3 (50%) > (?) > (?) > (?)
SENSEVAL-3 (75%) > (?) > (?) > (?)
SENSEVAL-3 (100%) < (?) > (?) < (?)
interest < (?) > (?) < (?)
line > (?) > (?) > (?)
forms significantly better than LPcosine, but their
performance is almost comparable on ?interest? and
?line? corpora. This observation motivates us to au-
tomatically select a distance measure that will boost
the performance of LP on a given dataset.
Cross-validation on labeled data is not feasi-
ble due to the setting of semi-supervised learning
(l ? u). In (Zhu and Ghahramani, 2002; Zhu et
al., 2003), they suggested a label entropy criterion
H(YU ) for model selection, where Y is the label
matrix learned by their semi-supervised algorithms.
The intuition behind their method is that good para-
meters should result in confident labeling. Entropy
on matrix W (H(W )) is a commonly used measure
for unsupervised feature selection (Dash and Liu,
2000), which can be considered here. Another pos-
sible criterion for model selection is to measure the
entropy of c ? c inter-class distance matrix D cal-
culated on labeled data (denoted as H(D)), where
Di,j represents the average distance between the i-
th class and the j-th class. We will investigate three
criteria, H(D), H(W ) and H(YU ), for model se-
lection. The distance measure can be automatically
selected by minimizing the average value of function
H(D), H(W ) or H(YU ) over 20 trials.
Let Q be the M ?N matrix. Function H(Q) can
measure the entropy of matrix Q, which is defined
as (Dash and Liu, 2000):
Si,j = exp (?? ?Qi,j), (1)
H(Q) = ?
M?
i=1
N?
j=1
(Si,j logSi,j + (1? Si,j) log (1? Si,j)),
(2)
where ? is positive constant. The possible value of ?
is? ln 0.5I? , where I? =
1
MN
?
i,j Qi,j . S is introduced
for normalization of matrix Q. For SENSEVAL-
3 data, we calculated an overall average score of
H(Q) by ?w
Nw,test?
w Nw,test
H(Qw). Nw,test is the
number of examples in test set of word w. H(D),
H(W ) and H(YU ) can be obtained by replacing Q
with D, W and YU respectively.
Table 3 reports the automatic prediction results
of these three criteria.
From Table 3, we can see that using H(W )
can consistently select the optimal distance measure
when the performance gap between LPcosine and
LPJS is very large (denoted by? or?). But H(D)
and H(YU ) fail to find the optimal distance measure
when only very few labeled examples are available
(percentage of labeled data ? 10%).
H(W ) measures the separability of matrix W .
Higher value of H(W ) means that distance mea-
sure decreases the separability of examples in full
dataset. Then the boundary between clusters is ob-
scured, which makes it difficult for LP to locate this
boundary. Therefore higher value of H(W ) results
in worse performance of LP.
When labeled dataset is small, the distances be-
tween classes can not be reliably estimated, which
results in unreliable indication of the separability
of examples in full dataset. This is the reason that
H(D) performs poorly on SENSEVAL-3 corpus
when the percentage of labeled data is less than 25%.
For H(YU ), small labeled dataset can not reveal
intrinsic structure in data, which may bias the esti-
mation of YU . Then labeling confidence (H(YU ))
can not properly indicate the performance of LP.
This may interpret the poor performance of H(YU )
on SENSEVAL-3 data when percentage ? 25%.
401
5 Conclusion
In this paper we have investigated a label propaga-
tion based semi-supervised learning algorithm for
WSD, which fully realizes a global consistency as-
sumption: similar examples should have similar la-
bels. In learning process, the labels of unlabeled ex-
amples are determined not only by nearby labeled
examples, but also by nearby unlabeled examples.
Compared with semi-supervised WSD methods in
the first and second categories, our corpus based
method does not need external resources, includ-
ing WordNet, bilingual lexicon, aligned parallel cor-
pora. Our analysis and experimental results demon-
strate the potential of this cluster assumption based
algorithm. It achieves better performance than SVM
when only very few labeled examples are avail-
able, and its performance is also better than mono-
lingual bootstrapping and comparable to bilingual
bootstrapping. Finally we suggest an entropy based
method to automatically identify a distance measure
that can boost the performance of LP algorithm on a
given dataset.
It has been shown that one sense per discourse
property can improve the performance of bootstrap-
ping algorithm (Li and Li, 2004; Yarowsky, 1995).
This heuristics can be integrated into LP algorithm
by setting weight Wi,j = 1 if the i-th and j-th in-
stances are in the same discourse.
In the future we may extend the evaluation of LP
algorithm and related cluster assumption based al-
gorithms using more benchmark data for WSD. An-
other direction is to use feature clustering technique
to deal with data sparseness and noisy feature prob-
lem.
Acknowledgements We would like to thank
anonymous reviewers for their helpful comments.
Z.Y. Niu is supported by A*STAR Graduate Schol-
arship.
References
Belkin, M., & Niyogi, P.. 2002. Using Manifold Structure for Partially Labeled
Classification. NIPS 15.
Blum, A., Lafferty, J., Rwebangira, R., & Reddy, R.. 2004. Semi-Supervised
Learning Using Randomized Mincuts. ICML-2004.
Brown P., Stephen, D.P., Vincent, D.P., & Robert, Mercer.. 1991. Word Sense
Disambiguation Using Statistical Methods. ACL-1991.
Chapelle, O., Weston, J., & Scho?lkopf, B. 2002. Cluster Kernels for Semi-
supervised Learning. NIPS 15.
Dagan, I. & Itai A.. 1994. Word Sense Disambiguation Using A Second Lan-
guage Monolingual Corpus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Dash, M., & Liu, H.. 2000. Feature Selection for Clustering. PAKDD(pp. 110?
121).
Diab, M., & Resnik. P.. 2002. An Unsupervised Method for Word Sense Tagging
Using Parallel Corpora. ACL-2002(pp. 255?262).
Hearst, M.. 1991. Noun Homograph Disambiguation using Local Context in
Large Text Corpora. Proceedings of the 7th Annual Conference of the UW
Centre for the New OED and Text Research: Using Corpora, 24:1, 1?41.
Karov, Y. & Edelman, S.. 1998. Similarity-Based Word Sense Disambiguation.
Computational Linguistics, 24(1): 41-59.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998. Using Corpus Statistics and
WordNet Relations for Sense Identification. Computational Linguistics, 24:1,
147?165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Evaluation of Knowledge Sources and
Learning Algorithms for Word Sense Disambiguation. EMNLP-2002, (pp.
41-48).
Lesk M.. 1986. Automated Word Sense Disambiguation Using Machine Read-
able Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. Pro-
ceedings of the ACM SIGDOC Conference.
Li, H. & Li, C.. 2004. Word Translation Disambiguation Using Bilingual Boot-
strapping. Computational Linguistics, 30(1), 1-22.
Lin, D.K.. 1997. Using Syntactic Dependency as Local Context to Resolve Word
Sense Ambiguity. ACL-1997.
Lin, J. 1991. Divergence Measures Based on the Shannon Entropy. IEEE Trans-
actions on Information Theory, 37:1, 145?150.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J.. 2004. Finding Predominant
Word Senses in Untagged Text. ACL-2004.
Mihalcea R.. 2004. Co-training and Self-training for Word Sense Disambigua-
tion. CoNLL-2004.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004. The SENSEVAL-3 English
Lexical Sample Task. SENSEVAL-2004.
Ng, H.T., Wang, B., & Chan, Y.S.. 2003. Exploiting Parallel Texts for Word
Sense Disambiguation: An Empirical Study. ACL-2003, pp. 455-462.
Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000. Word Sense Disambiguation by
Learning from Unlabeled Data. ACL-2000.
Schu?tze, H.. 1998. Automatic Word Sense Discrimination. Computational Lin-
guistics, 24:1, 97?123.
Seo, H.C., Chung, H.J., Rim, H.C., Myaeng. S.H., & Kim, S.H.. 2004. Unsu-
pervised Word Sense Disambiguation Using WordNet Relatives. Computer,
Speech and Language, 18:3, 253?273.
Szummer, M., & Jaakkola, T.. 2001. Partially Labeled Classification with Markov
Random Walks. NIPS 14.
Yarowsky, D.. 1995. Unsupervised Word Sense Disambiguation Rivaling Super-
vised Methods. ACL-1995, pp. 189-196.
Yarowsky, D.. 1992. Word Sense Disambiguation Using Statistical Models of
Roget?s Categories Trained on Large Corpora. COLING-1992, pp. 454-460.
Zhu, X. & Ghahramani, Z.. 2002. Learning from Labeled and Unlabeled Data
with Label Propagation. CMU CALD tech report CMU-CALD-02-107.
Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003. Semi-Supervised Learning Using
Gaussian Fields and Harmonic Functions. ICML-2003.
402
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 129?136,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Relation Extraction Using Label Propagation Based Semi-supervised
Learning
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
Shortage of manually labeled data is an
obstacle to supervised relation extraction
methods. In this paper we investigate a
graph based semi-supervised learning al-
gorithm, a label propagation (LP) algo-
rithm, for relation extraction. It represents
labeled and unlabeled examples and their
distances as the nodes and the weights of
edges of a graph, and tries to obtain a la-
beling function to satisfy two constraints:
1) it should be fixed on the labeled nodes,
2) it should be smooth on the whole graph.
Experiment results on the ACE corpus
showed that this LP algorithm achieves
better performance than SVM when only
very few labeled examples are available,
and it also performs better than bootstrap-
ping for the relation extraction task.
1 Introduction
Relation extraction is the task of detecting and
classifying relationships between two entities from
text. Many machine learning methods have been
proposed to address this problem, e.g., supervised
learning algorithms (Miller et al, 2000; Zelenko et
al., 2002; Culotta and Soresen, 2004; Kambhatla,
2004; Zhou et al, 2005), semi-supervised learn-
ing algorithms (Brin, 1998; Agichtein and Gravano,
2000; Zhang, 2004), and unsupervised learning al-
gorithms (Hasegawa et al, 2004).
Supervised methods for relation extraction per-
form well on the ACE Data, but they require a large
amount of manually labeled relation instances. Un-
supervised methods do not need the definition of
relation types and manually labeled data, but they
cannot detect relations between entity pairs and its
result cannot be directly used in many NLP tasks
since there is no relation type label attached to
each instance in clustering result. Considering both
the availability of a large amount of untagged cor-
pora and direct usage of extracted relations, semi-
supervised learning methods has received great at-
tention.
DIPRE (Dual Iterative Pattern Relation Expan-
sion) (Brin, 1998) is a bootstrapping-based sys-
tem that used a pattern matching system as clas-
sifier to exploit the duality between sets of pat-
terns and relations. Snowball (Agichtein and Gra-
vano, 2000) is another system that used bootstrap-
ping techniques for extracting relations from un-
structured text. Snowball shares much in common
with DIPRE, including the employment of the boot-
strapping framework as well as the use of pattern
matching to extract new candidate relations. The
third system approaches relation classification prob-
lem with bootstrapping on top of SVM, proposed by
Zhang (2004). This system focuses on the ACE sub-
problem, RDC, and extracts various lexical and syn-
tactic features for the classification task. However,
Zhang (2004)?s method doesn?t actually ?detect? re-
laitons but only performs relation classification be-
tween two entities given that they are known to be
related.
Bootstrapping works by iteratively classifying un-
labeled examples and adding confidently classified
examples into labeled data using a model learned
from augmented labeled data in previous iteration. It
129
can be found that the affinity information among un-
labeled examples is not fully explored in this boot-
strapping process.
Recently a promising family of semi-supervised
learning algorithm is introduced, which can effec-
tively combine unlabeled data with labeled data in
learning process by exploiting manifold structure
(cluster structure) in data (Belkin and Niyogi, 2002;
Blum and Chawla, 2001; Blum et al, 2004; Zhu
and Ghahramani, 2002; Zhu et al, 2003). These
graph-based semi-supervised methods usually de-
fine a graph where the nodes represent labeled and
unlabeled examples in a dataset, and edges (may be
weighted) reflect the similarity of examples. Then
one wants a labeling function to satisfy two con-
straints at the same time: 1) it should be close to the
given labels on the labeled nodes, and 2) it should be
smooth on the whole graph. This can be expressed
in a regularization framework where the first term
is a loss function, and the second term is a regu-
larizer. These methods differ from traditional semi-
supervised learning methods in that they use graph
structure to smooth the labeling function.
To the best of our knowledge, no work has been
done on using graph based semi-supervised learning
algorithms for relation extraction. Here we inves-
tigate a label propagation algorithm (LP) (Zhu and
Ghahramani, 2002) for relation extraction task. This
algorithm works by representing labeled and unla-
beled examples as vertices in a connected graph,
then propagating the label information from any ver-
tex to nearby vertices through weighted edges itera-
tively, finally inferring the labels of unlabeled exam-
ples after the propagation process converges. In this
paper we focus on the ACE RDC task1.
The rest of this paper is organized as follows. Sec-
tion 2 presents related work. Section 3 formulates
relation extraction problem in the context of semi-
supervised learning and describes our proposed ap-
proach. Then we provide experimental results of our
proposed method and compare with a popular su-
pervised learning algorithm (SVM) and bootstrap-
ping algorithm in Section 4. Finally we conclude
our work in section 5.
1 http://www.ldc.upenn.edu/Projects/ACE/, Three tasks of
ACE program: Entity Detection and Tracking (EDT), Rela-
tion Detection and Characterization (RDC), and Event Detec-
tion and Characterization (EDC)
2 The Proposed Method
2.1 Problem Definition
The problem of relation extraction is to assign an ap-
propriate relation type to an occurrence of two entity
pairs in a given context. It can be represented as fol-
lows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 denote the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity mention pairs. In this pa-
per, we set the mid-context window as the words be-
tween the two entity mentions and the pre- and post-
context as up to two words before and after the cor-
responding entity mention.
Let X = {xi}ni=1 be a set of contexts of occur-
rences of all the entity mention pairs, where xi rep-
resents the contexts of the i-th occurrence, and n is
the total number of occurrences. The first l exam-
ples (or contexts) are labeled as yg ( yg ? {rj}Rj=1,
rj denotes relation type and R is the total number of
relation types). The remaining u(u = n ? l) exam-
ples are unlabeled.
Intuitively, if two occurrences of entity mention
pairs have the similarity context, they tend to hold
the same relation type. Based on the assumption, we
define a graph where the vertices represent the con-
texts of labeled and unlabeled occurrences of entity
mention pairs, and the edge between any two ver-
tices xi and xj is weighted so that the closer the ver-
tices in some distance measure, the larger the weight
associated with this edge. Hence, the weights are de-
fined as follows:
Wij = exp(?
s2ij
?2 ) (2)
where sij is the similarity between xi and xj calcu-
lated by some similarity measures, e.g., cosine sim-
ilarity, and ? is used to scale the weights. In this
paper, we set ? as the average similarity between la-
beled examples from different classes.
2.2 A Label Propagation Algorithm
In the LP algorithm, the label information of any
vertex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage is
achieved. Larger edge weights allow labels to travel
130
through easier. Thus the closer the examples are, the
more likely they have similar labels.
We define soft label as a vector that is a proba-
bilistic distribution over all the classes. In the la-
bel propagation process, the soft label of each initial
labeled example is clamped in each iteration to re-
plenish label sources from these labeled data. Thus
the labeled data act like sources to push out labels
through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. Hopefully, the val-
ues of Wij across different classes would be as small
as possible and the values of Wij within the same
class would be as large as possible. This will make
label propagation to stay within the same class. This
label propagation process will make the labeling
function smooth on the graph.
Define an n? n probabilistic transition matrix T
Tij = P (j ? i) = wij?n
k=1 wkj
(3)
where Tij is the probability to jump from vertex xj
to vertex xi. We define a n ? R label matrix Y ,
where Yij representing the probabilities of vertex yi
to have the label rj .
Then the label propagation algorithm consists the
following main steps:
Step1 : Initialization
? Set the iteration index t = 0;
? Let Y 0 be the initial soft labels attached to
each vertex, where Y 0ij = 1 if yi is label rj
and 0 otherwise.
? Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent
with the labeling in labeled data and the
initialization of Y 0U can be arbitrary.
Step 2 : Propagate the labels of any vertex to
nearby vertices by Y t+1 = TY t , where
T is the row-normalized matrix of T , i.e.
Tij = Tij/
?
k Tik, which can maintain the
class probability interpretation.
Step 3 : Clamp the labeled data, that is, replace the
top l row of Y t+1 with Y 0L .
Step 4 : Repeat from step 2 until Y converges.
Step 5 : Assign xh(l + 1 ? h ? n) with a label:
yh = argmaxjYhj .
The above algorithm can ensure that the labeled
data YL never changes since it is clamped in Step 3.
Actually we are interested in only YU . This algo-
rithm has been shown to converge to a unique solu-
tion Y?U = limt?? Y tU = (I ? T?uu)?1T?ulY 0L (Zhu
and Ghahramani, 2002). Here, T?uu and T?ul are ac-
quired by splitting matrix T? after the l-th row and
the l-th column into 4 sub-matrices. And I is u? u
identity matrix. We can see that the initialization of
Y 0U in this solution is not important, since Y 0U does
not affect the estimation of Y?U .
3 Experiments and Results
3.1 Feature Set
Following (Zhang, 2004), we used lexical and syn-
tactic features in the contexts of entity pairs, which
are extracted and computed from the parse trees de-
rived from Charniak Parser (Charniak, 1999) and the
Chunklink script 2 written by Sabine Buchholz from
Tilburg University.
Words: Surface tokens of the two entities and
words in the three contexts.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZA-
TION, FACILITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all tokens in the two entities and words in
the three contexts.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two enti-
ties and words in the three contexts. The
?0? tag means that the word is not in any
chunk. The ?I-XP? tag means that this
word is inside an XP chunk. The ?B-XP?
by default means that the word is at the
beginning of an XP chunk.
? Grammatical function of the two enti-
ties and words in the three contexts. The
2Software available at http://ilk.uvt.nl/?sabine/chunklink/
131
last word in each chunk is its head, and
the function of the head is the function of
the whole chunk. ?NP-SBJ? means a NP
chunk as the subject of the sentence. The
other words in a chunk that are not the
head have ?NOFUNC? as their function.
? IOB-chains of the heads of the two enti-
ties. So-called IOB-chain, noting the syn-
tactic categories of all the constituents on
the path from the root node to this leaf
node of tree.
The position information is also specified in the
description of each feature above. For example,
word features with position information include:
1) WE1 (WE2): all words in e1 (e2)
2) WHE1 (WHE2): head word of e1 (e2)
3) WMNULL: no words in Cmid
4) WMFL: the only word in Cmid
5) WMF, WML, WM2, WM3, ...: first word, last
word, second word, third word, ...in Cmid when at
least two words in Cmid
6) WEL1, WEL2, ...: first word, second word, ...
before e1
7) WER1, WER2, ...: first word, second word, ...
after e2
We combine the above lexical and syntactic features
with their position information in the contexts to
form context vectors. Before that, we filter out low
frequency features which appeared only once in the
dataset.
3.2 Similarity Measures
The similarity sij between two occurrences of entity
pairs is important to the performance of the LP al-
gorithm. In this paper, we investigated two similar-
ity measures, cosine similarity measure and Jensen-
Shannon (JS) divergence (Lin, 1991). Cosine sim-
ilarity is commonly used semantic distance, which
measures the angle between two feature vectors. JS
divergence has ever been used as distance measure
for document clustering, which outperforms cosine
similarity based document clustering (Slonim et al,
2002). JS divergence measures the distance between
two probability distributions if feature vector is con-
sidered as probability distribution over features. JS
divergence is defined as follows:
Table 1: Frequency of Relation SubTypes in the ACE training
and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
JS(q, r) = 12 [DKL(q?p?) +DKL(r?p?)] (4)
DKL(q?p?) =
?
y
q(y)(log q(y)p?(y) ) (5)
DKL(r?p?) =
?
y
r(y)(log r(y)p?(y) ) (6)
where p? = 12(q + r) and JS(q, r) represents JS
divergence between probability distribution q(y) and
r(y) (y is a random variable), which is defined in
terms of KL-divergence.
3.3 Experimental Evaluation
3.3.1 Experiment Setup
We evaluated this label propagation based rela-
tion extraction method for relation subtype detection
and characterization task on the official ACE 2003
corpus. It contains 519 files from sources including
broadcast, newswire, and newspaper. We dealt with
only intra-sentence explicit relations and assumed
that all entities have been detected beforehand in the
EDT sub-task of ACE. Table 1 lists the types and
subtypes of relations for the ACE Relation Detection
and Characterization (RDC) task, along with their
132
Table 2: The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes.
The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 35.9 32.6 34.4 58.3 56.1 57.1 58.5 58.7 58.5
10% 51.3 41.5 45.9 64.5 57.5 60.7 64.6 62.0 63.2
25% 67.1 52.9 59.1 68.7 59.0 63.4 68.9 63.7 66.1
50% 74.0 57.8 64.9 69.9 61.8 65.6 70.1 64.1 66.9
75% 77.6 59.4 67.2 71.8 63.4 67.3 72.4 64.8 68.3
100% 79.8 62.9 70.3 73.9 66.9 70.2 74.2 68.2 71.1
Table 3: The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classification
on relation subtypes. The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.
SVM LPCosine LPJS
Percentage P R F P R F P R F
1% 31.6 26.1 28.6 39.6 37.5 38.5 40.1 38.0 39.0
10% 39.1 32.7 35.6 45.9 39.6 42.5 46.2 41.6 43.7
25% 49.8 35.0 41.1 51.0 44.5 47.3 52.3 46.0 48.9
50% 52.5 41.3 46.2 54.1 48.6 51.2 54.9 50.8 52.7
75% 58.7 46.7 52.0 56.0 52.0 53.9 56.1 52.6 54.3
100% 60.8 48.9 54.2 56.2 52.3 54.1 56.3 52.9 54.6
frequency of occurrence in the ACE training set and
test set. We constructed labeled data by randomly
sampling some examples from ACE training data
and additionally sampling examples with the same
size from the pool of unrelated entity pairs for the
?NONE? class. We used the remaining examples in
the ACE training set and the whole ACE test set as
unlabeled data. The testing set was used for final
evaluation.
3.3.2 LP vs. SVM
Support Vector Machine (SVM) is a state of the
art technique for relation extraction task. In this ex-
periment, we use LIBSVM tool 3 with linear kernel
function.
For comparison between SVM and LP, we ran
SVM and LP with different sizes of labeled data
and evaluate their performance on unlabeled data
using precision, recall and F-measure. Firstly, we
ran SVM or LP algorithm to detect possible rela-
tions from unlabeled data. If an entity mention pair
is classified not to the ?NONE? class but to the other
24 subtype classes, then it has a relation. Then con-
struct labeled datasets with different sampling set
size l, including 1%?Ntrain, 10%?Ntrain, 25%?
Ntrain, 50%?Ntrain, 75%?Ntrain, 100%?Ntrain
(Ntrain is the number of examples in the ACE train-
3LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
ing set). If any relation subtype was absent from the
sampled labeled set, we redid the sampling. For each
size, we performed 20 trials and calculated average
scores on test set over these 20 random trials.
Table 2 reports the performance of SVM and LP
with different sizes of labled data for relation detec-
tion task. We used the same sampled labeled data in
LP as the training data for SVM model.
From Table 2, we see that both LPCosine and
LPJS achieve higher Recall than SVM. Specifically,
with small labeled dataset (percentage of labeled
data ? 25%), the performance improvement by LP
is significant. When the percentage of labeled data
increases from 50% to 100%, LPCosine is still com-
parable to SVM in F-measure while LPJS achieves
slightly better F-measure than SVM. On the other
hand, LPJS consistently outperforms LPCosine.
Table 3 reports the performance of relation clas-
sification by using SVM and LP with different sizes
of labled data. And the performance describes the
average values of Precision, Recall and F-measure
over major relation subtypes.
From Table 3, we see that LPCosine and LPJS out-
perform SVM by F-measure in almost all settings
of labeled data, which is due to the increase of Re-
call. With smaller labeled dataset (percentage of la-
beled data ? 50%), the gap between LP and SVM
is larger. When the percentage of labeled data in-
133
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
1% 10% 25% 50% 75% 100%
Percentage of Labeled Examples
F-m
ea
su
re SVM
LP_Cosine
LP_JS
 
Figure 1: Comparison of the performance of SVM
and LP with different sizes of labeled data
creases from 75% to 100%, the performance of LP
algorithm is still comparable to SVM. On the other
hand, the LP algorithm based on JS divergence con-
sistently outperforms the LP algorithm based on Co-
sine similarity. Figure 1 visualizes the accuracy of
three algorithms.
As shown in Figure 1, the gap between SVM
curve and LPJS curves is large when the percentage
of labeled data is relatively low.
3.3.3 An Example
In Figure 2, we selected 25 instances in train-
ing set and 15 instances in test set from the ACE
corpus,which covered five relation types. Using
Isomap tool 4, the 40 instances with 229 feature di-
mensions are visualized in a two-dimensional space
as the figure. We randomly sampled only one la-
beled example for each relation type from the 25
training examples as labeled data. Figure 2(a) and
2(b) show the initial state and ground truth result re-
spectively. Figure 2(c) reports the classification re-
sult on test set by SVM (accuracy = 415 = 26.7%),
and Figure 2(d) gives the classification result on both
training set and test set by LP (accuracy = 1115 =
73.3%).
Comparing Figure 2(b) and Figure 2(c), we find
that many examples are misclassified from class ?
to other class symbols. This may be caused that
SVMs method ignores the intrinsic structure in data.
For Figure 2(d), the labels of unlabeled examples
are determined not only by nearby labeled examples,
but also by nearby unlabeled examples, so using LP
4The tool is available at http://isomap.stanford.edu/.
     




	
      




	

     




	
      




	

Figure 2: An example: comparison of SVM and LP
algorithm on a data set from ACE corpus. ? and
4 denote the unlabeled examples in training set and
test set respectively, and other symbols (?,?,2,+
and 5) represent the labeled examples with respec-
tive relation type sampled from training set.
strategy achieves better performance than the local
consistency based SVM strategy when the size of
labeled data is quite small.
3.3.4 LP vs. Bootstrapping
In (Zhang, 2004), they perform relation classifi-
cation on ACE corpus with bootstrapping on top of
SVM. To compare with their proposed Bootstrapped
SVM algorithm, we use the same feature stream set-
ting and randomly selected 100 instances from the
training data as the size of initial labeled data.
Table 4 lists the performance of the bootstrapped
SVM method from (Zhang, 2004) and LP method
with 100 seed labeled examples for relation type
classification task. We can see that LP algorithm
outperforms the bootstrapped SVM algorithm on
four relation type classification tasks, and perform
comparably on the relation ?SOC? classification
task.
4 Discussion
In this paper,we have investigated a graph-based
semi-supervised learning approach for relation ex-
traction problem. Experimental results showed that
the LP algorithm performs better than SVM and
134
Table 4: Comparison of the performance of the bootstrapped SVM method from (Zhang, 2004) and LP method with 100 seed
labeled examples for relation type classification task.
Bootstrapping LPJS
Relation type P R F P R F
ROLE 78.5 69.7 73.8 81.0 74.7 77.7
PART 65.6 34.1 44.9 70.1 41.6 52.2
AT 61.0 84.8 70.9 74.2 79.1 76.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0
NEAR ? ? ? 13.7 12.5 13.0
Table 5: Comparison of the performance of previous methods on ACE RDC task.
Relation Dectection Relation Detection and Classification
on Types on Subtypes
Method P R F P R F P R F
Culotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -
Kambhatla (2004) Feature based, Maxi-
mum Entropy
- - - - - - 63.5 45.2 52.8
Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5
bootstrapping. We have some findings from these
results:
The LP based relation extraction method can use
the graph structure to smooth the labels of unlabeled
examples. Therefore, the labels of unlabeled exam-
ples are determined not only by the nearby labeled
examples, but also by nearby unlabeled examples.
For supervised methods, e.g., SVM, very few la-
beled examples are not enough to reveal the struc-
ture of each class. Therefore they can not perform
well, since the classification hyperplane was learned
only from few labeled data and the coherent struc-
ture in unlabeled data was not explored when in-
ferring class boundary. Hence, our LP-based semi-
supervised method achieves better performance on
both relation detection and classification when only
few labeled data is available. Bootstrapping
Currently most of works on the RDC task of
ACE focused on supervised learning methods Cu-
lotta and Soresen (2004; Kambhatla (2004; Zhou
et al (2005). Table 5 lists a comparison on re-
lation detection and classification of these meth-
ods. Zhou et al (2005) reported the best result as
63.1%/49.5%/55.5% in Precision/Recall/F-measure
on the relation subtype classification using feature
based method, which outperforms tree kernel based
method by Culotta and Soresen (2004). Compared
with Zhou et al?s method, the performance of LP al-
gorithm is slightly lower. It may be due to that we
used a much simpler feature set. Our work in this
paper focuses on the investigation of a graph based
semi-supervised learning algorithm for relation ex-
traction. In the future, we would like to use more ef-
fective feature sets Zhou et al (2005) or kernel based
similarity measure with LP for relation extraction.
5 Conclusion and Future Work
This paper approaches the problem of semi-
supervised relation extraction using a label propaga-
tion algorithm. It represents labeled and unlabeled
examples and their distances as the nodes and the
weights of edges of a graph, and tries to obtain a
labeling function to satisfy two constraints: 1) it
should be fixed on the labeled nodes, 2) it should
be smooth on the whole graph. In the classifica-
tion process, the labels of unlabeled examples are
determined not only by nearby labeled examples,
but also by nearby unlabeled examples. Our exper-
imental results demonstrated that this graph based
algorithm can achieve better performance than SVM
when only very few labeled examples are available,
and also outperforms the bootstrapping method for
relation extraction task.
In the future, we would like to investigate more
effective feature set or use feature selection to im-
prove the performance of this graph-based semi-
supervised relation extraction method.
135
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries (ACMDL?00).
Belkin M. and Niyogi P.. 2002. Using Manifold Struc-
ture for Partially Labeled Classification. Advances in
Neural Infomation Processing Systems 15.
Blum A. and Chawla S. 2001. Learning from Labeled
and Unlabeled Data Using Graph Mincuts. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning.
Blum A., Lafferty J., Rwebangira R. and Reddy R. 2004.
Semi-Supervised Learning Using Randomized Min-
cuts. In Proceedings of the 21th International Confer-
ence on Machine Learning..
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proceedings of WebDB Work-
shop at 6th International Conference on Extending
Database Technology (WebDB?98). pages 172-183.
Charniak E. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In Proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Hasegawa T., Sekine S. and Grishman R. 2004. Dis-
covering Relations among Named Entities from Large
Corpora, In Proceeding of Conference ACL2004.
Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In Proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics.. 21-26 July 2004. Barcelona, Spain.
Lin J. 1991. Divergence Measures Based on the Shan-
non Entropy. IEEE Transactions on Information The-
ory. Vol 37, No.1, 145-150.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In Proceedings of 6th Applied Natural Lan-
guage Processing Conference 29 April-4 may 2000,
Seattle USA.
Slonim, N., Friedman, N., and Tishby, N. 2002. Un-
supervised Document Classification Using Sequential
Information Maximization. In Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Yarowsky D. 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics. pp.189-196.
Zelenko D., Aone C. and Richardella A. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In Proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction. In Proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
Zhu Xiaojin and Ghahramani Zoubin. 2002. Learning
from Labeled and Unlabeled Data with Label Propa-
gation. CMU CALD tech report CMU-CALD-02-107.
Zhu Xiaojin, Ghahramani Zoubin, and Lafferty J. 2003.
Semi-Supervised Learning Using Gaussian Fields and
Harmonic Functions. In Proceedings of the 20th Inter-
national Conference on Machine Learning.
136
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 89?96,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Relation Disambiguation Using Spectral Clustering
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
This paper presents an unsupervised learn-
ing approach to disambiguate various rela-
tions between name entities by use of vari-
ous lexical and syntactic features from the
contexts. It works by calculating eigen-
vectors of an adjacency graph?s Laplacian
to recover a submanifold of data from a
high dimensionality space and then per-
forming cluster number estimation on the
eigenvectors. Experiment results on ACE
corpora show that this spectral cluster-
ing based approach outperforms the other
clustering methods.
1 Introduction
In this paper, we address the task of relation extrac-
tion, which is to find relationships between name en-
tities in a given context. Many methods have been
proposed to deal with this task, including supervised
learning algorithms (Miller et al, 2000; Zelenko et
al., 2002; Culotta and Soresen, 2004; Kambhatla,
2004; Zhou et al, 2005), semi-supervised learn-
ing algorithms (Brin, 1998; Agichtein and Gravano,
2000; Zhang, 2004), and unsupervised learning al-
gorithm (Hasegawa et al, 2004).
Among these methods, supervised learning is usu-
ally more preferred when a large amount of la-
beled training data is available. However, it is
time-consuming and labor-intensive to manually tag
a large amount of training data. Semi-supervised
learning methods have been put forward to mini-
mize the corpus annotation requirement. Most of
semi-supervised methods employ the bootstrapping
framework, which only need to pre-define some ini-
tial seeds for any particular relation, and then boot-
strap from the seeds to acquire the relation. How-
ever, it is often quite difficult to enumerate all class
labels in the initial seeds and decide an ?optimal?
number of them.
Compared with supervised and semi-supervised
methods, Hasegawa et al (2004)?s unsupervised ap-
proach for relation extraction can overcome the dif-
ficulties on requirement of a large amount of labeled
data and enumeration of all class labels. Hasegawa
et al (2004)?s method is to use a hierarchical cluster-
ing method to cluster pairs of named entities accord-
ing to the similarity of context words intervening be-
tween the named entities. However, the drawback of
hierarchical clustering is that it required providing
cluster number by users. Furthermore, clustering is
performed in original high dimensional space, which
may induce non-convex clusters hard to identified.
This paper presents a novel application of spec-
tral clustering technique to unsupervised relation ex-
traction problem. It works by calculating eigenvec-
tors of an adjacency graph?s Laplacian to recover a
submanifold of data from a high dimensional space,
and then performing cluster number estimation on
a transformed space defined by the first few eigen-
vectors. This method may help us find non-convex
clusters. It also does not need to pre-define the num-
ber of the context clusters or pre-specify the simi-
larity threshold for the clusters as Hasegawa et al
(2004)?s method.
The rest of this paper is organized as follows. Sec-
tion 2 formulates unsupervised relation extraction
and presents how to apply the spectral clustering
89
technique to resolve the task. Then section 3 reports
experiments and results. Finally we will give a con-
clusion about our work in section 4.
2 Unsupervised Relation Extraction
Problem
Assume that two occurrences of entity pairs with
similar contexts, are tend to hold the same relation
type. Thus unsupervised relation extraction prob-
lem can be formulated as partitioning collections of
entity pairs into clusters according to the similarity
of contexts, with each cluster containing only entity
pairs labeled by the same relation type. And then, in
each cluster, the most representative words are iden-
tified from the contexts of entity pairs to induce the
label of relation type. Here, we only focus on the
clustering subtask and do not address the relation
type labeling subtask.
In the next subsections we will describe our pro-
posed method for unsupervised relation extraction,
which includes: 1) Collect the context vectors in
which the entity mention pairs co-occur; 2) Cluster
these Context vectors.
2.1 Context Vector and Feature Design
Let X = {xi}ni=1 be the set of context vectors of oc-
currences of all entity mention pairs, where xi repre-
sents the context vector of the i-th occurrence, and n
is the total number of occurrences of all entity men-
tion pairs.
Each occurrence of entity mention pairs can be
denoted as follows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 represents the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity mention pairs respectively.
We extracted features from e1, e2, Cpre, Cmid,
Cpost to construct context vectors, which are com-
puted from the parse trees derived from Charniak
Parser (Charniak, 1999) and the Chunklink script 1
written by Sabine Buchholz from Tilburg University.
Words: Words in the two entities and three context
windows.
1 Software available at http://ilk.uvt.nl/ sabine/chunklink/
Entity Type: the entity type of both entities, which
can be PERSON, ORGANIZATION, FACIL-
ITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all words in the two entities and three con-
text windows.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two enti-
ties and three context windows. The ?0?
tag means that the word is outside of any
chunk. The ?I-XP? tag means that this
word is inside an XP chunk. The ?B-XP?
by default means that the word is at the
beginning of an XP chunk.
? Grammatical function of the two entities
and three context windows. The last word
in each chunk is its head, and the function
of the head is the function of the whole
chunk. ?NP-SBJ? means a NP chunk as
the subject of the sentence. The other
words in a chunk that are not the head have
?NOFUNC? as their function.
? IOB-chains of the heads of the two enti-
ties. So-called IOB-chain, noting the syn-
tactic categories of all the constituents on
the path from the root node to this leaf
node of tree.
We combine the above lexical and syntactic fea-
tures with their position information in the context
to form the context vector. Before that, we filter out
low frequency features which appeared only once in
the entire set.
2.2 Context Clustering
Once the context vectors of entity pairs are prepared,
we come to the second stage of our method: cluster
these context vectors automatically.
In recent years, spectral clustering technique has
received more and more attention as a powerful ap-
proach to a range of clustering problems. Among
the efforts on spectral clustering techniques (Weiss,
1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,
2001; Zha et al, 2001), we adopt a modified version
90
Table 1: Context Clustering with Spectral-based Clustering
technique.
Input: A set of context vectors X = {x1, x2, ..., xn},
X ? <n?d;
Output: Clustered data and number of clusters;
1. Construct an affinity matrix by Aij = exp(? s
2
ij
?2 ) if i 6=j, 0 if i = j. Here, sij is the similarity between xi and
xj calculated by Cosine similarity measure. and the free
distance parameter ?2 is used to scale the weights;
2. Normalize the affinity matrix A to create the matrix L =
D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)
element is the sum of A?s ith row;
3. Set q = 2;
4. Compute q eigenvectors of L with greatest eigenvalues.
Arrange them in a matrix Y .
5. Perform elongated K-means with q + 1 centers on Y ,
initializing the (q + 1)-th mean in the origin;
6. If the q+1-th cluster contains any data points, then there
must be at least an extra cluster; set q = q + 1 and go
back to step 4. Otherwise, algorithm stops and outputs
clustered data and number of clusters.
(Sanguinetti et al, 2005) of the algorithm by Ng et
al. (2001) because it can provide us model order se-
lection capability.
Since we do not know how many relation types
in advance and do not have any labeled relation
training examples at hand, the problem of model
order selection arises, i.e. estimating the ?opti-
mal? number of clusters. Formally, let k be the
model order, we need to find k in Equation: k =
argmaxk{criterion(k)}. Here, the criterion is de-
fined on the result of spectral clustering.
Table 1 shows the details of the whole algorithm
for context clustering, which contains two main
stages: 1) Transformation of Clustering Space (Step
1-4); 2) Clustering in the transformed space using
Elongated K-means algorithm (Step 5-6).
2.3 Transformation of Clustering Space
We represent each context vector of entity pair as a
node in an undirected graph. Each edge (i,j) in the
graph is assigned a weight that reflects the similarity
between two context vectors i and j. Hence, the re-
lation extraction task for entity pairs can be defined
as a partition of the graph so that entity pairs that
are more similar to each other, e.g. labeled by the
same relation type, belong to the same cluster. As a
relaxation of such NP-hard discrete graph partition-
ing problem, spectral clustering technique computes
eigenvalues and eigenvectors of a Laplacian matrix
related to the given graph, and construct data clus-
ters based on such spectral information.
Thus the starting point of context clustering is to
construct an affinity matrix A from the data, which
is an n ? n matrix encoding the distances between
the various points. The affinity matrix is then nor-
malized to form a matrix L by conjugating with the
the diagonal matrix D?1/2 which has as entries the
square roots of the sum of the rows of A. This is to
take into account the different spread of the various
clusters (points belonging to more rarified clusters
will have lower sums of the corresponding row of
A). It is straightforward to prove that L is positive
definite and has eigenvalues smaller or equal to 1,
with equality holding in at least one case.
Let K be the true number of clusters present in
the dataset. If K is known beforehand, the first K
eigenvectors of L will be computed and arranged as
columns in a matrix Y . Each row of Y corresponds
to a context vector of entity pair, and the above pro-
cess can be considered as transforming the original
context vectors in a d-dimensional space to new con-
text vectors in the K-dimensional space. Therefore,
the rows of Y will cluster upon mutually orthogonal
points on the K dimensional sphere,rather than on
the coordinate axes.
2.4 The Elongated K-means algorithm
As the step 5 of Table 1 shows, the result of elon-
gated K-means algorithm is used to detect whether
the number of clusters selected q is less than the true
number K, and allows one to iteratively obtain the
number of clusters.
Consider the case when the number of clusters q
is less than the true cluster number K present in the
dataset. In such situation, taking the first q < K
eigenvectors, we will be selecting a q-dimensional
subspace in the clustering space. As the rows of the
K eigenvectors clustered along mutually orthogo-
nal vectors, their projections in a lower dimensional
space will cluster along radial directions. Therefore,
the general picture will be of q clusters elongated in
the radial direction, with possibly some clusters very
near the origin (when the subspace is orthogonal to
some of the discarded eigenvectors).
Hence, the K-means algorithm is modified as
the elongated K-means algorithm to downweight
distances along radial directions and penalize dis-
91
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(a) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(b) 
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
(c) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(d) 
Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Three
elongated clusters in the 2D clustering space using
Spectral clustering: two dominant eigenvectors; (d)
The clustering result using Spectral-based clustering
(?2=0.05). (4,? and + denote examples in different
clusters)
tances along transversal directions. The elongated
K-means algorithm computes the distance of point
x from the center ci as follows:
? If the center is not very near the origin, cTi ci > ? (? is a
parameter to be fixed by the user), the distances are cal-
culated as: edist(x, ci) = (x ? ci)TM(x ? ci), where
M = 1? (Iq ?
cicTi
cTi ci
) + ? cic
T
i
cTi ci
, ? is the sharpness param-
eter that controls the elongation (the smaller, the more
elongated the clusters) 2.
? If the center is very near the origin,cTi ci < ?, the dis-
tances are measured using the Euclidean distance.
In each iteration of procedure in Table 1, elon-
gated K-means is initialized with q centers corre-
sponding to data points in different clusters and one
center in the origin. The algorithm then will drag the
center in the origin towards one of the clusters not
accounted for. Compute another eigenvector (thus
increasing the dimension of the clustering space to
q + 1) and repeat the procedure. Eventually, when
one reach as many eigenvectors as the number of
clusters present in the data, no points will be as-
signed to the center at the origin, leaving the cluster
empty. This is the signal to terminate the algorithm.
2.5 An example
Figure 1 visualized the clustering result of three cir-
cle dataset using K-means and Spectral-based clus-
tering. From Figure 1(b), we can see that K-means
can not separate the non-convex clusters in three cir-
cle dataset successfully since it is prone to local min-
imal. For spectral-based clustering, as the algorithm
described, initially, we took the two eigenvectors of
L with largest eigenvalues, which gave us a two-
dimensional clustering space. Then to ensure that
the two centers are initialized in different clusters,
one center is set as the point that is the farthest from
the origin, while the other is set as the point that
simultaneously farthest the first center and the ori-
gin. Figure 1(c) shows the three elongated clusters in
the 2D clustering space and the corresponding clus-
tering result of dataset is visualized in Figure 1(d),
which exploits manifold structure (cluster structure)
in data.
2 In this paper, the sharpness parameter ? is set to 0.2
92
Table 2: Frequency of Major Relation SubTypes in the ACE
training and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
3 Experiments and Results
3.1 Data Setting
Our proposed unsupervised relation extraction is
evaluated on ACE 2003 corpus, which contains 519
files from sources including broadcast, newswire,
and newspaper. We only deal with intra-sentence
explicit relations and assumed that all entities have
been detected beforehand in the EDT sub-task of
ACE. To verify our proposed method, we only col-
lect those pairs of entity mentions which have been
tagged relation types in the given corpus. Then the
relation type tags were removed to test the unsuper-
vised relation disambiguation. During the evalua-
tion procedure, the relation type tags were used as
ground truth classes. A break-down of the data by
24 relation subtypes is given in Table 2.
3.2 Evaluation method for clustering result
When assessing the agreement between clustering
result and manually annotated relation types (ground
truth classes), we would encounter the problem that
there was no relation type tags for each cluster in our
clustering results.
To resolve the problem, we construct a contin-
gency table T , where each entry ti,j gives the num-
ber of the instances that belong to both the i-th es-
timated cluster and j-th ground truth class. More-
over, to ensure that any two clusters do not share
the same labels of relation types, we adopt a per-
mutation procedure to find an one-to-one mapping
function ? from the ground truth classes (relation
types) TC to the estimated clustering result EC.
There are at most |TC| clusters which are assigned
relation type tags. And if the number of the esti-
mated clusters is less than the number of the ground
truth clusters, empty clusters should be added so that
|EC| = |TC| and the one-to-one mapping can be
performed, which can be formulated as the function:
?? = argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the in-
dex of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we adopt
Precision, Recall and F-measure to evaluate the
clustering result.
3.3 Experimental Design
We perform our unsupervised relation extraction on
the devtest set of ACE corpus and evaluate the al-
gorithm on relation subtype level. Firstly, we ob-
serve the influence of various variables, including
Distance Parameter ?2, Different Features, Context
Window Size. Secondly, to verify the effectiveness
of our method, we further compare it with other two
unsupervised methods.
3.3.1 Choice of Distance Parameter ?2
We simply search over ?2 and pick the value
that finds the best aligned set of clusters on the
transformed space. Here, the scattering criterion
trace(P?1W PB) is used to compare the cluster qual-
ity for different value of ?2 3, which measures the ra-
tio of between-cluster to within-cluster scatter. The
higher the trace(P?1W PB), the higher the cluster
quality.
In Table 3 and Table 4, with different settings of
feature set and context window size, we find out the
3 trace(P?1W PB) is trace of a matrix which is the sum of
its diagonal elements. PW is the within-cluster scatter matrix
as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xi ? mj)
t and PB
is the between-cluster scatter matrix as: PB =
?c
j=1(mj ?
m)(mj ? m)t, where m is the total mean vector and mj is
the mean vector for jth cluster and (Xj ? mj)t is the matrix
transpose of the column vector (Xj ?mj).
93
Table 3: Contribution of Different Features
Features ?2 cluster number trace value Precison Recall F-measure
Words 0.021 15 2.369 41.6% 30.2% 34.9%
+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%
+POS 0.017 18 3.206 37.8% 46.9% 41.8%
+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%
Table 4: Different Context Window Size Setting
Context Window Size ?2 cluster number trace value Precision Recall F-measure
0 0.016 18 3.576 37.6% 48.1% 42.2%
2 0.015 19 3.900 43.5% 49.4% 46.3%
5 0.020 21 2.225 29.3% 34.7% 31.7%
corresponding value of ?2 and cluster number which
maximize the trace value in searching for a range of
value ?2.
3.3.2 Contribution of Different Features
As the previous section presented, we incorporate
various lexical and syntactic features to extract rela-
tion. To measure the contribution of different fea-
tures, we report the performance by gradually in-
creasing the feature set, as Table 3 shows.
Table 3 shows that all of the four categories of fea-
tures contribute to the improvement of performance
more or less. Firstly,the addition of entity type fea-
ture is very useful, which improves F-measure by
6.6%. Secondly, adding POS features can increase
F-measure score but do not improve very much.
Thirdly, chunking features also show their great use-
fulness with increasing Precision/Recall/F-measure
by 5.7%/2.5%/4.5%.
We combine all these features to do all other eval-
uations in our experiments.
3.3.3 Setting of Context Window Size
We have mentioned in Section 2 that the context
vectors of entity pairs are derived from the contexts
before, between and after the entity mention pairs.
Hence, we have to specify the three context window
size first. In this paper, we set the mid-context win-
dow as everything between the two entity mentions.
For the pre- and post- context windows, we could
have different choices. For example, if we specify
the outer context window size as 2, then it means that
the pre-context (post-context)) includes two words
before (after) the first (second) entity.
For comparison of the effect of the outer context
of entity mention pairs, we conducted three different
Table 5: Performance of our proposed method (Spectral-
based clustering) compared with other unsupervised methods:
((Hasegawa et al, 2004))?s clustering method and K-means
clustering.
Precision Recall F-measure
Hasegawa?s Method1 38.7% 29.8% 33.7%
Hasegawa?s Method2 37.9% 36.0% 36.9%
Kmeans 34.3% 40.2% 36.8%
Our Proposed Method 43.5% 49.4% 46.3%
settings of context window size (0, 2, 5) as Table 4
shows. From this table we can find that with the con-
text window size setting, 2, the algorithm achieves
the best performance of 43.5%/49.4%/46.3% in
Precision/Recall/F-measure. With the context win-
dow size setting, 5, the performance becomes worse
because extending the context too much may include
more features, but at the same time, the noise also
increases.
3.3.4 Comparison with other Unsupervised
methods
In (Hasegawa et al, 2004), they preformed un-
supervised relation extraction based on hierarchical
clustering and they only used word features between
entity mention pairs to construct context vectors. We
reported the clustering results using the same clus-
tering strategy as Hasegawa et al (2004) proposed.
In Table 5, Hasegawa?s Method1 means the test used
the word feature as Hasegawa et al (2004) while
Hasegawa?s Method2 means the test used the same
feature set as our method. In both tests, we specified
the cluster number as the number of ground truth
classes.
We also approached the relation extraction prob-
lem using the standard clustering technique, K-
94
means, where we adopted the same feature set de-
fined in our proposed method to cluster the con-
text vectors of entity mention pairs and pre-specified
the cluster number as the number of ground truth
classes.
Table 5 reports the performance of our proposed
method comparing with the other two unsupervised
methods. Table 5 shows our proposed spectral based
method clearly outperforms the other two unsuper-
vised methods by 12.5% and 9.5% in F-measure re-
spectively. Moreover, the incorporation of various
lexical and syntactic features into Hasegawa et al
(2004)?s method2 makes it outperform Hasegawa et
al. (2004)?s method1 which only uses word feature.
3.4 Discussion
In this paper, we have shown that the modified spec-
tral clustering technique, with various lexical and
syntactic features derived from the context of entity
pairs, performed well on the unsupervised relation
extraction problem. Our experiments show that by
the choice of the distance parameter ?2, we can esti-
mate the cluster number which provides the tightest
clusters. We notice that the estimated cluster num-
ber is less than the number of ground truth classes
in most cases. The reason for this phenomenon may
be that some relation types can not be easily distin-
guished using the context information only. For ex-
ample, the relation subtypes ?Located?, ?Based-In?
and ?Residence? are difficult to disambiguate even
for human experts to differentiate.
The results also show that various lexical and
syntactic features contain useful information for the
task. Especially, although we did not concern the
dependency tree and full parse tree information as
other supervised methods (Miller et al, 2000; Cu-
lotta and Soresen, 2004; Kambhatla, 2004; Zhou et
al., 2005), the incorporation of simple features, such
as words and chunking information, still can provide
complement information for capturing the character-
istics of entity pairs. This perhaps dues to the fact
that two entity mentions are close to each other in
most of relations defined in ACE. Another observa-
tion from the result is that extending the outer con-
text window of entity mention pairs too much may
not improve the performance since the process may
incorporate more noise information and affect the
clustering result.
As regards the clustering technique, the spectral-
based clustering performs better than direct cluster-
ing, K-means. Since the spectral-based algorithm
works in a transformed space of low dimension-
ality, data can be easily clustered so that the al-
gorithm can be implemented with better efficiency
and speed. And the performance using spectral-
based clustering can be improved due to the reason
that spectral-based clustering overcomes the draw-
back of K-means (prone to local minima) and may
find non-convex clusters consistent with human in-
tuition.
Generally, from the point of view of unsu-
pervised resolution for relation extraction, our
approach already achieves best performance of
43.5%/49.4%/46.3% in Precision/Recall/F-measure
compared with other clustering methods.
4 Conclusion and Future work
In this paper, we approach unsupervised relation ex-
traction problem by using spectral-based clustering
technique with diverse lexical and syntactic features
derived from context. The advantage of our method
is that it doesn?t need any manually labeled relation
instances, and pre-definition the number of the con-
text clusters. Experiment results on the ACE corpus
show that our method achieves better performance
than other unsupervised methods, i.e.Hasegawa et
al. (2004)?s method and Kmeans-based method.
Currently we combine various lexical and syn-
tactic features to construct context vectors for clus-
tering. In the future we will further explore other
semantic information to assist the relation extrac-
tion problem. Moreover, instead of cosine similar-
ity measure to calculate the distance between con-
text vectors, we will try other distributional similar-
ity measures to see whether the performance of re-
lation extraction can be improved. In addition, if we
can find an effective unsupervised way to filter out
unrelated entity pairs in advance, it would make our
proposed method more practical.
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proc. of the 5th ACM International Conference on
Digital Libraries (ACMDL?00).
95
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proc. of WebDB Workshop at
6th International Conference on Extending Database
Technology (WebDB?98). pages 172-183.
Charniak E.. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12.. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Defense Advanced Research Projects Agency. 1995.
Proceedings of the Sixth Message Understanding Con-
ference (MUC-6) Morgan Kaufmann Publishers, Inc.
Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.
2004. Discovering Relations among Named Enti-
ties from Large Corpora, Proceeding of Conference
ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Kannan R., Vempala S., and Vetta A.. 2000. On cluster-
ing: Good,bad and spectral. In Proceedings of the 41st
Foundations of Computer Science. pages 367-380.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In proceedings of 6th Applied Natural Lan-
guage Processing Conference. 29 April-4 may 2000,
Seattle USA.
Ng Andrew.Y, Jordan M., and Weiss Y.. 2001. On spec-
tral clustering: Analysis and an algorithm. In Pro-
ceedings of Advances in Neural Information Process-
ing Systems. pages 849-856.
Sanguinetti G., Laidler J. and Lawrence N.. 2005. Au-
tomatic determination of the number of clusters us-
ing spectral algorithms.In: IEEE Machine Learning
for Signal Processing. 28-30 Sept 2005, Mystic, Con-
necticut, USA.
Shi J. and Malik.J. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence. 22(8):888-905.
Weiss Yair. 1999. Segmentation using eigenvectors: A
unifying view. ICCV(2). pp.975-982.
Zelenko D., Aone C. and Richardella A.. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001. Spec-
tral Relaxation for k-means clustering. In Neural In-
formation Processing Systems (NIPS2001). pages
1057-1064, 2001.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction, In proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
96
Optimizing Feature Set for Chinese Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This article describes the implementation of I2R
word sense disambiguation system (I2R ?WSD)
that participated in one senseval3 task: Chinese lex-
ical sample task. Our core algorithm is a supervised
Naive Bayes classifier. This classifier utilizes an op-
timal feature set, which is determined by maximiz-
ing the cross validated accuracy of NB classifier on
training data. The optimal feature set includes part-
of-speech with position information in local con-
text, and bag of words in topical context.
1 Introduction
Word sense disambiguation (WSD) is to assign ap-
propriate meaning to a given ambiguous word in
a text. Corpus based method is one of the suc-
cessful lines of research on WSD. Many supervised
learning algorithms have been applied for WSD,
ex. Bayesian learning (Leacock et al, 1998), ex-
emplar based learning (Ng and Lee, 1996), decision
list (Yarowsky, 2000), neural network (Towel and
Voorheest, 1998), maximum entropy method (Dang
et al, 2002), etc.. In this paper, we employ Naive
Bayes classifier to perform WSD.
Resolving the ambiguity of words usually relies
on the contexts of their occurrences. The feature
set used for context representation consists of lo-
cal and topical features. Local features include part
of speech tags of words within local context, mor-
phological information of target word, local collo-
cations, and syntactic relations between contextual
words and target word, etc.. Topical features are
bag of words occurred within topical context. Con-
textual features play an important role in providing
discrimination information for classifiers in WSD.
In other words, an informative feature set will help
classifiers to accurately disambiguate word senses,
but an uninformative feature set will deteriorate the
performance of classifiers. In this paper, we opti-
mize feature set by maximizing the cross validated
accuracy of Naive Bayes classifier on sense tagged
training data.
2 Naive Bayes Classifier
Let C = {c1, c2, ..., cL} represent class labels,
F = {f1, f2, ..., fM} be a set of features. The
value of fj , 1 ? j ? M , is 1 if fj is present in
the context of target word, otherwise 0. In classi-
fication process, the Naive Bayes classifier tries to
find the class that maximizes P (ci|F ), the proba-
bility of class ci given feature set F , 1 ? i ? L.
Assuming the independence between features, the
classification procedure can be formulated as:
i? = arg max
1?i?L
p(ci)
?M
j=1 p(fj |ci)?M
j=1 p(fj)
, (1)
where p(ci), p(fj |ci) and p(fj) are estimated using
maximum likelihood method. To avoid the effects
of zero counts when estimating p(fj |ci), the zero
counts of p(fj |ci) are replaced with p(ci)/N , where
N is the number of training examples.
3 Feature Set
For Chinese WSD, there are two strategies to extract
contextual information. One is based on Chinese
characters, the other is to utilize Chinese words and
related morphological or syntactic information. In
our system, context representation is based on Chi-
nese words, since words are less ambiguous than
characters.
We use two types of features for Chinese WSD:
local features and topical features. All of these fea-
tures are acquired from data at senseval3 without
utilization of any other knowledge resource.
3.1 Local features
Two sets of local features are investigated, which
are represented by LocalA and LocalB. Let nl de-
note the local context window size.
LocalA contains only part of speech tags
with position information: POS?nl , ...,
POS?1, POS0, POS+1, ..., POS+nl , where
POS?i (POS+i) is the part of speech (POS) of the
i-th words to the left (right) of target word w, and
POS0 is the POS of w.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
LocalB enriches the local context by including
the following features: local words with position in-
formation (W?nl , ..., W?1, W+1, ..., W+nl), bigram
templates ((W?nl , W?(nl?1)), ..., (W?1, W+1),
..., (W+(nl?1), W+nl)), local words with POS tags(W POS) (position information is not considered),
and part of speech tags with position information.
All of these POS tags, words, and bigrams are
gathered and each of them contributed as one fea-
ture. For a training or test example, the value of
some feature is 1 if it occurred in local context, oth-
erwise it is 0. In this paper, we investigate two val-
ues of nl for LocalA and LocalB, 1 and 2, which
results in four feature sets.
3.2 Topical features
We consider all Chinese words within a context
window size nt as topical features. For each training
or test example, senseval3 data provides one sen-
tence as the context of ambiguous word. In sense-
val3 Chinese training data, all contextual sentences
are segmented into words and tagged with part of
speech.
Words which contain non-Chinese character are
removed, and remaining words occurred within
context window size nt are gathered. Each remain-
ing word is considered as one feature. The value of
topical feature is 1 if it occurred within window size
nt, otherwise it is 0.
In later experiment, we set different values for nt,
ex. 1, 2, 3, 4, 5, 10, 20, 30, 40, 50. Our experimen-
tal result indicated that the accuracy of sense dis-
ambiguation is related to the value of nt. For differ-
ent ambiguous words, the value of nt which yields
best disambiguation accuracy is different. It is de-
sirable to determine an optimal value, n?t, for each
ambiguous word by maximizing the cross validated
accuracy.
4 Data Set
In Chinese lexical sample task, training data con-
sists of 793 sense-tagged examples for 20 ambigu-
ous Chinese words. Test data consists of 380 un-
tagged examples for the same 20 target words. Ta-
ble 1 shows the details of training data and test data.
5 Criterion for Evaluation of Feature Sets
In this paper, five fold cross validation method was
employed to estimate the accuracy of our classi-
fier, which was the criterion for evaluation of fea-
ture sets. All of the sense tagged examples of some
target word in senseval3 training data were shuf-
fled and divided into five equal folds. We used four
folds as training set and the remaining fold as test
set. This procedure was repeated five times under
different division between training set and test set.
The average accuracy over five runs is defined as the
accuracy of our classifier.
6 Evaluation of Feature Sets
Four feature sets were investigated:
FEATUREA1: LocalA with nl = 1, and topical
feature within optimal context window size n?t;
FEATUREA2: LocalA with nl = 2, and topical
feature within optimal context window size n?t;
FEATUREB1: LocalB with nl = 1, and topical
feature within optimal context window size n?t;
FEATUREB2: LocalB with nl = 2, and topical
feature within optimal context window size n?t.
We performed training and test procedure using
exactly same training and test set for each feature
set. For each word, the optimal value of topical con-
text window size n?t was determined by selecting a
minimal value of nt which maximized the cross val-
idated accuracy.
Table 2 summarizes the results of Naive Bayes
classifier using four feature sets evaluated on sen-
seval3 Chinese training data. Figure 1 shows the
accuracy of Naive Bayes classifier as a function of
topical context window size on four nouns and three
verbs. Several results should be noted specifically:
If overall accuracy over 20 Chinese charac-
ters is used as evaluation criterion for feature
set, the four feature sets can be sorted as fol-
lows: FEATUREA1 > FEATUREA2 ?
FEATUREB1 > FEATUREB2. This indi-
cated that simply increasing local window size or
enriching feature set by incorporating bigram tem-
plates, local word with position information, and lo-
cal words with POS tags did not improve the perfor-
mance of sense disambiguation.
In table 2, it showed that with FEATUREA1, the
optimal topical context window size was less than
10 words for 13 out of 20 target words. Figure
1 showed that for most of nouns and verbs, Naive
Bayes classifier achieved best disambiguation accu-
racy with small topical context window size (<10
words). This gives the evidence that for most of
Chinese words, including nouns and verbs, the near
distance context is more important than the long dis-
tance context for sense disambiguation.
7 Experimental Result
The empirical study in section 6 showed that FEA-
TUREA1 performed best among all the feature sets.
A Naive Bayes classifier with FEATUREA1 as fea-
ture set was learned from all the senseval3 Chinese
training data for each target word. Then we used
Table 1: Details of training data and test data in Chinese lexical sample task.
POS occurred # senses occurred
Ambiguous word in training data # training examples in training data # test examples
ba3wo4 n v vn 31 4 15
bao1 n nr q v 76 8 36
cai2liao4 n 20 2 10
chong1ji1 v vn 28 3 13
chuan1 v 28 3 14
di4fang1 b n 36 4 17
fen1zi3 n 36 2 16
huo2dong4 a v vn 36 5 16
lao3 Ng a an d j 57 6 26
lu4 n nr q 57 6 28
mei2you3 d v 30 3 15
qi3lai2 v 40 4 20
qian2 n nr 40 4 20
ri4zi5 n 48 3 21
shao3 Ng a ad j v 42 5 20
tu1chu1 a ad v 30 3 15
yan2jiu1 n v vn 30 3 15
yun4dong4 n nz v vn 54 3 27
zou3 v vn 49 5 24
zuo4 v 25 3 12
this classifier to determine the senses of occurrences
of target words in test data. The official result of
I2R?WSD system in Chinese lexical sample task
is listed below:
Precision: 60.40% (229.00 correct of 379.00 at-
tempted).
Recall: 60.40% (229.00 correct of 379.00 in to-
tal).
Attempted: 100.00% (379.00 attempted of
379.00 in total).
8 Conclusion
In this paper, we described the implementation of
I2R ? WSD system that participated in one sen-
seval3 task: Chinese lexical sample task. An op-
timal feature set was selected by maximizing the
cross validated accuracy of supervised Naive Bayes
classifier on sense-tagged data. The senses of occur-
rences of target words in test data were determined
using Naive Bayes classifier with optimal feature
set learned from training data. Our system achieved
60.40% precision and recall in Chinese lexical sam-
ple task.
References
Dang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D.
(2002) Simple Features for Chinese Word Sense
Disambiguation. In Proc. of COLING.
Leacock, C., Chodorow, M., & Miller G. A. (1998)
Using Corpus Statistics and WordNet Relations
for Sense Identification. Computational Linguis-
tics, 24:1, 147?165.
Mooney, R. J. (1996) Comparative Experiments on
Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. In Proc.
of EMNLP, pp. 82-91, Philadelphia, PA.
Ng, H. T., & Lee H. B. (1996) Integrating Multi-
ple Knowledge Sources to Disambiguate Word
Sense: An Exemplar-Based Approach. In Proc.
of ACL, pp. 40-47.
Pedersen, T. (2001) A Decision Tree of Bigrams is
an Accurate Predictor of Word Sense. In Proc. of
NAACL.
Towel, G., & Voorheest, E. M. (1998) Disambiguat-
ing Highly Ambiguous Words. Computational
Linguistics, 24:1, 125?146.
Yarowsky, D. (2000) Hierarchical Decision Lists
for Word Sense Disambiguation. Computers and
the Humanities, 34(1-2), 179?186.
Table 2: Accuracy of Naive Bayes classifier with different feature sets on Senseval3 Chinese training data.
FEATUREA1 FEATUREA2 FEATUREB1 FEATUREB2
Ambiguous word n?t Accuracy n?t Accuracy n?t Accuracy n?t Accuracy
ba3wo4 5 30.0 4 23.3 4 30.0 3 30.0
bao1 2 30.7 20 34.0 2 33.3 20 32.0
cai2liao4 2 85.0 2 80.0 2 75.0 2 60.0
chong1ji1 20 40.0 3 40.0 30 36.0 1 28.0
chuan1 3 72.0 5 68.0 3 56.0 5 64.0
di4fang1 2 74.3 1 62.9 1 71.4 1 65.7
fen1zi3 20 91.4 50 91.4 20 88.6 20 85.7
huo2dong4 5 40.0 20 51.4 10 42.9 4 40.0
lao3 3 49.1 4 47.3 3 52.7 20 52.7
lu4 1 83.6 2 78.2 2 81.8 1 76.4
mei2you3 20 50.0 20 47.9 4 43.3 3 50.0
qi3lai2 4 75.0 1 75.0 1 80.0 1 77.5
qian2 3 57.5 4 57.5 3 60.0 5 57.5
ri4zi5 4 62.2 4 57.8 10 55.6 4 55.6
shao3 4 45.0 3 50.0 10 42.5 20 50.0
tu1chu1 10 83.3 10 80.0 10 80.0 10 76.7
yan2jiu1 20 43.3 20 46.7 10 50.0 20 36.7
yun4dong4 10 64.0 10 66.0 10 62.0 10 58.0
zou3 5 44.4 5 44.4 4 51.1 4 51.1
zuo4 20 64.0 30 60.0 20 64.0 20 64.0
Overall 57.7 56.9 57.0 55.1
0 1 2 3 4 5 10 20 30 40 500.4
0.5
0.6
0.7
0.8
0.9
1
nt
Ac
cu
rac
y
0 1 2 3 4 5 10 20 30 40 500.3
0.4
0.5
0.6
0.7
0.8
nt
Ac
cu
rac
y
chuan1 
qi3lai2
zuo4   
cai2liao4
fen1zi3  
qian2    
ri4zi5   
Figure 1: Accuracy of Naive Bayes classifier with the optimal feature set FEATUREA1 on four nouns (top
figure) and three verbs (bottom figure). The horizontal axis represents the topical context window size.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 415?422,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Partially Supervised Sense Disambiguation by Learning Sense Number
from Tagged and Untagged Corpora
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
{zniu, dhji}@i2r.a-star.edu.sg
Chew Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
Supervised and semi-supervised sense dis-
ambiguation methods will mis-tag the in-
stances of a target word if the senses of
these instances are not defined in sense in-
ventories or there are no tagged instances
for these senses in training data. Here we
used a model order identification method
to avoid the misclassification of the in-
stances with undefined senses by discov-
ering new senses from mixed data (tagged
and untagged corpora). This algorithm
tries to obtain a natural partition of the
mixed data by maximizing a stability cri-
terion defined on the classification result
from an extended label propagation al-
gorithm over all the possible values of
the number of senses (or sense number,
model order). Experimental results on
SENSEVAL-3 data indicate that it outper-
forms SVM, a one-class partially super-
vised classification algorithm, and a clus-
tering based model order identification al-
gorithm when the tagged data is incom-
plete.
1 Introduction
In this paper, we address the problem of partially
supervised word sense disambiguation, which is
to disambiguate the senses of occurrences of a tar-
get word in untagged texts when given incomplete
tagged corpus 1.
Word sense disambiguation can be defined as
associating a target word in a text or discourse
1?incomplete tagged corpus? means that tagged corpus
does not include the instances of some senses for the target
word, while these senses may occur in untagged texts.
with a definition or meaning. Many corpus based
methods have been proposed to deal with the sense
disambiguation problem when given definition for
each possible sense of a target word or a tagged
corpus with the instances of each possible sense,
e.g., supervised sense disambiguation (Leacock et
al., 1998), and semi-supervised sense disambigua-
tion (Yarowsky, 1995).
Supervised methods usually rely on the infor-
mation from previously sense tagged corpora to
determine the senses of words in unseen texts.
Semi-supervised methods for WSD are charac-
terized in terms of exploiting unlabeled data in
the learning procedure with the need of prede-
fined sense inventories for target words. The in-
formation for semi-supervised sense disambigua-
tion is usually obtained from bilingual corpora
(e.g. parallel corpora or untagged monolingual
corpora in two languages) (Brown et al, 1991; Da-
gan and Itai, 1994), or sense-tagged seed examples
(Yarowsky, 1995).
Some observations can be made on the previous
supervised and semi-supervised methods. They
always rely on hand-crafted lexicons (e.g., Word-
Net) as sense inventories. But these resources may
miss domain-specific senses, which leads to in-
complete sense tagged corpus. Therefore, sense
taggers trained on the incomplete tagged corpus
will misclassify some instances if the senses of
these instances are not defined in sense invento-
ries. For example, one performs WSD in informa-
tion technology related texts using WordNet 2 as
sense inventory. When disambiguating the word
?boot? in the phrase ?boot sector?, the sense tag-
ger will assign this instance with one of the senses
of ?boot? listed in WordNet. But the correct sense
2Online version of WordNet is available at
http://wordnet.princeton.edu/cgi-bin/webwn2.0
415
?loading operating system into memory? is not in-
cluded in WordNet. Therefore, this instance will
be associated with an incorrect sense.
So, in this work, we would like to study the
problem of partially supervised sense disambigua-
tion with an incomplete sense tagged corpus.
Specifically, given an incomplete sense-tagged
corpus and a large amount of untagged examples
for a target word 3, we are interested in (1) label-
ing the instances in the untagged corpus with sense
tags occurring in the tagged corpus; (2) trying to
find undefined senses (or new senses) of the target
word 4 from the untagged corpus, which will be
represented by instances from the untagged cor-
pus.
We propose an automatic method to estimate
the number of senses (or sense number, model or-
der) of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification result over all
the possible values of sense number. At the same
time, we can obtain a classification of the mixed
data with the optimal number of groups. If the es-
timated sense number in the mixed data is equal
to the sense number of the target word in tagged
corpus, then there is no new sense in untagged
corpus. Otherwise new senses will be represented
by groups in which there is no instance from the
tagged corpus.
This partially supervised sense disambiguation
algorithm may help enriching manually compiled
lexicons by inducing new senses from untagged
corpora.
This paper is organized as follows. First, a
model order identification algorithm will be pre-
sented for partially supervised sense disambigua-
tion in section 2. Section 3 will provide experi-
mental results of this algorithm for sense disam-
biguation on SENSEVAL-3 data. Then related
work on partially supervised classification will be
summarized in section 4. Finally we will conclude
our work and suggest possible improvements in
section 5.
2 Partially Supervised Word Sense
Disambiguation
The partially supervised sense disambiguation
problem can be generalized as a model order iden-
3Untagged data usually includes the occurrences of all the
possible senses of the target word
4?undefined senses? are the senses that do not appear in
tagged corpus.
tification problem. We try to estimate the sense
number of a target word in mixed data (tagged cor-
pus+untagged corpus) by maximizing a stability
criterion defined on classification results over all
the possible values of sense number. If the esti-
mated sense number in the mixed data is equal to
the sense number in the tagged corpus, then there
is no new sense in the untagged corpus. Other-
wise new senses will be represented by clusters in
which there is no instance from the tagged corpus.
The stability criterion assesses the agreement be-
tween classification results on full mixed data and
sampled mixed data. A partially supervised clas-
sification algorithm is used to classify the full or
sampled mixed data into a given number of classes
before the stability assessment, which will be pre-
sented in section 2.1. Then we will provide the
details of the model order identification procedure
in section 2.2.
2.1 An Extended Label Propagation
Algorithm
Table 1: Extended label propagation algorithm.
Function: ELP(DL, DU , k, Y 0DL+DU )Input: labeled examples DL, unlabeled
examples DU , model order k, initial
labeling matrix Y 0DL+DU ;Output: the labeling matrix YDU on DU ;
1 If k < kXL then
YDU =NULL;
2 Else if k = kXL then
Run plain label propagation algorithm
on DU with YDU as output;
3 Else then
3.1 Estimate the size of tagged data set
of new classes;
3.2 Generate tagged examples from DU
for (kXL + 1)-th to k-th new classes;
3.3 Run plain label propagation algorithm
on DU with augmented tagged dataset
as labeled data;
3.4 YDU is the output from plain label
propagation algorithm;
End if
4 Return YDU ;
Let XL+U = {xi}ni=1 be a set of contexts of
occurrences of an ambiguous word w, where xi
represents the context of the i-th occurrence, and n
is the total number of this word?s occurrences. Let
416
SL = {sj}cj=1 denote the sense tag set of w in XL,
where XL denotes the first l examples xg(1 ? g ?
l) that are labeled as yg (yg ? SL). Let XU denote
other u (l + u = n) examples xh(l + 1 ? h ? n)
that are unlabeled.
Let Y 0XL+U ? N |XL+U |?|SL| represent initialsoft labels attached to tagged instances, where
Y 0XL+U ,ij = 1 if yi is sj and 0 otherwise. Let Y 0XL
be the top l rows of Y 0XL+U and Y 0XU be the remain-
ing u rows. Y 0XL is consistent with the labeling inlabeled data, and the initialization of Y 0XU can bearbitrary.
Let k denote the possible value of the number
of senses in mixed data XL+U , and kXL be the
number of senses in initial tagged data XL. Note
that kXL = |SL|, and k ? kXL .
The classification algorithm in the order identi-
fication process should be able to accept labeled
data DL 5, unlabeled data DU 6 and model order k
as input, and assign a class label or a cluster index
to each instance in DU as output. Previous super-
vised or semi-supervised algorithms (e.g. SVM,
label propagation algorithm (Zhu and Ghahra-
mani, 2002)) cannot classify the examples in DU
into k groups if k > kXL . The semi-supervised k-
means clustering algorithm (Wagstaff et al, 2001)
may be used to perform clustering analysis on
mixed data, but its efficiency is a problem for clus-
tering analysis on a very large dataset since multi-
ple restarts are usually required to avoid local op-
tima and multiple iterations will be run in each
clustering process for optimizing a clustering so-
lution.
In this work, we propose an alternative method,
an extended label propagation algorithm (ELP),
which can classify the examples in DU into k
groups. If the value of k is equal to kXL , then
ELP is identical with the plain label propagation
algorithm (LP) (Zhu and Ghahramani, 2002). Oth-
erwise, if the value of k is greater than kXL , we
perform classification by the following steps:
(1) estimate the dataset size of each new class as
sizenew class by identifying the examples of new
classes using the ?Spy? technique 7 and assuming
5DL may be the dataset XL or a subset sampled from XL.
6DU may be the dataset XU or a subset sampled from
XU .
7The ?Spy? technique was proposed in (Liu et al, 2003).
Our re-implementation of this technique consists of three
steps: (1) sample a small subset DsL with the size 15%?|DL|from DL; (2) train a classifier with tagged data DL ? DsL;(3) classify DU and DsL, and then select some examples from
DU as the dataset of new classes, which have the classifica-
that new classes are equally distributed;
(2) D?L = DL, D?U = DU ;
(3) remove tagged examples of the m-th new
class (kXL + 1 ? m ? k) from D?L 8 and train a
classifier on this labeled dataset without the m-th
class;
(4) the classifier is then used to classify the ex-
amples in D?U ;
(5) the least confidently unlabeled point
xclass m ? D
?
U , together with its label m, is added
to the labeled data D?L = D?L + xclass m, and
D?U = D
?
U ? xclass m;
(6) steps (3) to (5) are repeated for each new
class till the augmented tagged data set is large
enough (here we try to select sizenew class/4 ex-
amples with their sense tags as tagged data for
each new class);
(7) use plain LP algorithm to classify remaining
unlabeled data D?U with D?L as labeled data.
Table 1 shows this extended label propagation
algorithm.
Next we will provide the details of the plain la-
bel propagation algorithm.
Define Wij = exp(?d
2
ij
?2 ) if i 6= j and Wii = 0(1 ? i, j ? |DL + DU |), where dij is the distance
(e.g., Euclidean distance) between the example xi
and xj , and ? is used to control the weight Wij .
Define |DL + DU | ? |DL + DU | probability
transition matrix Tij = P (j ? i) = Wij?n
k=1 Wkj
,
where Tij is the probability to jump from example
xj to example xi.
Compute the row-normalized matrix T by
T ij = Tij/
?n
k=1 Tik.
The classification solution is obtained by
YDU = (I ? T uu)?1T ulY 0DL . I is |DU | ? |DU |
identity matrix. T uu and T ul are acquired by split-
ting matrix T after the |DL|-th row and the |DL|-th
column into 4 sub-matrices.
2.2 Model Order Identification Procedure
For achieving the model order identification (or
sense number estimation) ability, we use a clus-
ter validation based criterion (Levine and Domany,
2001) to infer the optimal number of senses of w
in XL+U .
tion confidence less than the average of that in DsL. Classifi-cation confidence of the example xi is defined as the absolute
value of the difference between two maximum values from
the i-th row in labeling matrix.
8Initially there are no tagged examples for the m-th class
in D?L. Therefore we do not need to remove tagged examples
for this new class, and then directly train a classifier with D?L.
417
Table 2: Model order evaluation algorithm.
Function: CV(XL+U , k, q, Y 0XL+U )
Input: data set XL+U , model order k,
and sampling frequency q;
Output: the score of the merit of k;
1 Run the extended label propagation
algorithm with XL, XU , k and Y 0XL+U ;
2 Construct connectivity matrix Ck based
on above classification solution on XU ;
3 Use a random predictor ?k to assign
uniformly drawn labels to each vector
in XU ;
4 Construct connectivity matrix C?k using
above classification solution on XU ;
5 For ? = 1 to q do
5.1 Randomly sample a subset X?L+U with
the size ?|XL+U | from XL+U , 0 < ? < 1;
5.2 Run the extended label propagation
algorithm with X?L, X?U , k and Y 0?;
5.3 Construct connectivity matrix C?k using
above classification solution on X?U ;
5.4 Use ?k to assign uniformly drawn labels
to each vector in X?U ;
5.5 Construct connectivity matrix C??k usingabove classification solution on X?U ;
Endfor
6 Evaluate the merit of k using following
formula:
Mk = 1q
?
?(M(C?k , Ck) ? M(C??k , C?k)),
where M(C?, C) is given by equation (2);
7 Return Mk;
Then this model order identification procedure
can be formulated as:
k?XL+U = argmaxKmin?k?Kmax{CV (XL+U , k, q, Y 0XL+U )}.(1)
k?XL+U is the estimated sense number in XL+U ,
Kmin (or Kmax) is the minimum (or maximum)
value of sense number, and k is the possible value
of sense number in XL+U . Note that k ? kXL .
Then we set Kmin = kXL . Kmax may be set as a
value greater than the possible ground-truth value.
CV is a cluster validation based evaluation func-
tion. Table 2 shows the details of this function.
We set q, the resampling frequency for estimation
of stability score, as 20. ? is set as 0.90. The ran-
dom predictor assigns uniformly distributed class
labels to each instance in a given dataset. We
run this CV procedure for each value of k. The
value of k that maximizes this function will be se-
lected as the estimation of sense number. At the
same time, we can obtain a partition of XL+U with
k?XL+U groups.
The function M(C?, C) in Table 2 is given by
(Levine and Domany, 2001):
M(C?, C) =
?
i,j 1{C
?
i,j = Ci,j = 1, xi, xj ? X?U}
?
i,j 1{Ci,j = 1, xi, xj ? X
?
U}
,
(2)
where X?U is the untagged data in X?L+U , X?L+U
is a subset with the size ?|XL+U | (0 < ? < 1)
sampled from XL+U , C or C? is |XU | ? |XU | or
|X?U | ? |X
?
U | connectivity matrix based on classi-
fication solutions computed on XU or X?U respec-
tively. The connectivity matrix C is defined as:
Ci,j = 1 if xi and xj belong to the same cluster,
otherwise Ci,j = 0. C? is calculated in the same
way.
M(C?, C) measures the proportion of example
pairs in each group computed on XU that are also
assigned into the same group by the classification
solution on X?U . Clearly, 0 ? M ? 1. Intu-
itively, if the value of k is identical with the true
value of sense number, then classification results
on the different subsets generated by sampling
should be similar with that on the full dataset. In
the other words, the classification solution with the
true model order as parameter is robust against re-
sampling, which gives rise to a local optimum of
M(C?, C).
In this algorithm, we normalize M(C?k , Ck) by
the equation in step 6 of Table 2, which makes
our objective function different from the figure of
merit (equation ( 2)) proposed in (Levine and Do-
many, 2001). The reason to normalize M(C?k , Ck)
is that M(C?k , Ck) tends to decrease when increas-
ing the value of k (Lange et al, 2002). Therefore
for avoiding the bias that the smaller value of k
is to be selected as the model order, we use the
cluster validity of a random predictor to normalize
M(C?k , Ck).
If k?XL+U is equal to kXL , then there is no new
sense in XU . Otherwise (k?XL+U > kXL) new
senses of w may be represented by the groups in
which there is no instance from XL.
3 Experiments and Results
3.1 Experiment Design
We evaluated the ELP based model order iden-
tification algorithm on the data in English lexi-
cal sample task of SENSEVAL-3 (including all
418
Table 3: Description of The percentage of official
training data used as tagged data when instances
with different sense sets are removed from official
training data.
The percentage of official
training data used as tagged data
Ssubset = {s1} 42.8%
Ssubset = {s2} 76.7%
Ssubset = {s3} 89.1%
Ssubset = {s1, s2} 19.6%
Ssubset = {s1, s3} 32.0%
Ssubset = {s2, s3} 65.9%
the 57 English words ) 9, and further empirically
compared it with other state of the art classifi-
cation methods, including SVM 10 (the state of
the art method for supervised word sense disam-
biguation (Mihalcea et al, 2004)), a one-class par-
tially supervised classification algorithm (Liu et
al., 2003) 11, and a semi-supervised k-means clus-
tering based model order identification algorithm.
The data for English lexical samples task in
SENSEVAL-3 consists of 7860 examples as offi-
cial training data, and 3944 examples as official
test data for 57 English words. The number of
senses of each English word varies from 3 to 11.
We evaluated these four algorithms with differ-
ent sizes of incomplete tagged data. Given offi-
cial training data of the word w, we constructed
incomplete tagged data XL by removing the all
the tagged instances from official training data that
have sense tags from Ssubset, where Ssubset is a
subset of the ground-truth sense set S for w, and S
consists of the sense tags in official training set for
w. The removed training data and official test data
of w were used as XU . Note that SL = S?Ssubset.
Then we ran these four algorithm for each target
word w with XL as tagged data and XU as un-
tagged data, and evaluated their performance us-
ing the accuracy on official test data of all the 57
words. We conducted six experiments for each tar-
get word w by setting Ssubset as {s1}, {s2}, {s3},
{s1, s2}, {s1, s3}, or {s2, s3}, where si is the i-th
most frequent sense of w. Ssubset cannot be set as
{s4} since some words have only three senses. Ta-
ble 3 lists the percentage of official training data
used as tagged data (the number of examples in in-
9Available at http://www.senseval.org/senseval3
10we used a linear SV M light, available at
http://svmlight.joachims.org/.
11Available at http://www.cs.uic.edu/?liub/LPU/LPU-
download.html
complete tagged data divided by the number of ex-
amples in official training data) when we removed
the instances with sense tags from Ssubset for all
the 57 words. If Ssubset = {s3}, then most of
sense tagged examples are still included in tagged
data. If Ssubset = {s1, s2}, then there are very few
tagged examples in tagged data. If no instances are
removed from official training data, then the value
of percentage is 100%.
Given an incomplete tagged corpus for a target
word, SVM does not have the ability to find the
new senses from untagged corpus. Therefore it la-
bels all the instances in the untagged corpus with
sense tags from SL.
Given a set of positive examples for a class and
a set of unlabeled examples, the one-class partially
supervised classification algorithm, LPU (Learn-
ing from Positive and Unlabeled examples) (Liu
et al, 2003), learns a classifier in four steps:
Step 1: Identify a small set of reliable negative
examples from unlabeled examples by the use of a
classifier.
Step 2: Build a classifier using positive ex-
amples and automatically selected negative exam-
ples.
Step 3: Iteratively run previous two steps until
no unlabeled examples are classified as negative
ones or the unlabeled set is null.
Step 4: Select a good classifier from the set of
classifiers constructed above.
For comparison, LPU 12 was run to perform
classification on XU for each class in XL. The
label of each instance in XU was determined by
maximizing the classification score from LPU out-
put for each class. If the maximum score of an
instance is negative, then this instance will be la-
beled as a new class. Note that LPU classifies
XL+U into kXL + 1 groups in most of cases.
The clustering based partially supervised sense
disambiguation algorithm was implemented by re-
placing ELP with a semi-supervised k-means clus-
tering algorithm (Wagstaff et al, 2001) in the
model order identification procedure. The label
information in labeled data was used to guide the
semi-supervised clustering on XL+U . Firstly, the
labeled data may be used to determine initial clus-
ter centroids. If the cluster number is greater
12The three parameters in LPU were set as follows: ?-s1
spy -s2 svm -c 1?. It means that we used the spy technique for
step 1 in LPU, the SVM algorithm for step 2, and selected the
first or the last classifier as the final classifier. It is identical
with the algorithm ?Spy+SVM IS? in Liu et al (2003).
419
than kXL , the initial centroids of clusters for new
classes will be assigned as randomly selected in-
stances. Secondly, in the clustering process, the
instances with the same class label will stay in
the same cluster, while the instances with different
class labels will belong to different clusters. For
better clustering solution, this clustering process
will be restarted three times. Clustering process
will be terminated when clustering solution con-
verges or the number of iteration steps is more than
30. Kmin = kXL = |SL|, Kmax = Kmin + m. m
is set as 4.
We used Jensen-Shannon (JS) divergence (Lin,
1991) as distance measure for semi-supervised
clustering and ELP, since plain LP with JS diver-
gence achieves better performance than that with
cosine similarity on SENSEVAL-3 data (Niu et al,
2005).
For the LP process in ELP algorithm, we con-
structed connected graphs as follows: two in-
stances u, v will be connected by an edge if u is
among v?s 10 nearest neighbors, or if v is among
u?s 10 nearest neighbors as measured by cosine or
JS distance measure (following (Zhu and Ghahra-
mani, 2002)).
We used three types of features to capture the
information in all the contextual sentences of tar-
get words in SENSEVAL-3 data for all the four
algorithms: part-of-speech of neighboring words
with position information, words in topical con-
text without position information (after removing
stop words), and local collocations (as same as the
feature set used in (Lee and Ng, 2002) except that
we did not use syntactic relations). We removed
the features with occurrence frequency (counted
in both training set and test set) less than 3 times.
If the estimated sense number is more than the
sense number in the initial tagged corpus XL, then
the results from order identification based meth-
ods will consist of the instances from clusters of
unknown classes. When assessing the agreement
between these classification results and the known
results on official test set, we will encounter the
problem that there is no sense tag for each instance
in unknown classes. Slonim and Tishby (2000)
proposed to assign documents in each cluster with
the most dominant class label in that cluster, and
then conducted evaluation on these labeled docu-
ments. Here we will follow their method for as-
signing sense tags to unknown classes from LPU,
clustering based order identification process, and
ELP based order identification process. We as-
signed the instances from unknown classes with
the dominant sense tag in that cluster. The result
from LPU always includes only one cluster of the
unknown class. We also assigned the instances
from the unknown class with the dominant sense
tag in that cluster. When all instances have their
sense tags, we evaluated the their results using the
accuracy on official test set.
3.2 Results on Sense Disambiguation
Table 4 summarizes the accuracy of SVM, LPU,
the semi-supervised k-means clustering algorithm
with correct sense number |S| or estimated sense
number k?XL+U as input, and the ELP algorithm
with correct sense number |S| or estimated sense
number k?XL+U as input using various incomplete
tagged data. The last row in Table 4 lists the av-
erage accuracy of each algorithm over the six ex-
perimental settings. Using |S| as input means that
we do not perform order identification procedure,
while using k?XL+U as input is to perform order
identification and obtain the classification results
on XU at the same time.
We can see that ELP based method outperforms
clustering based method in terms of average accu-
racy under the same experiment setting, and these
two methods outperforms SVM and LPU. More-
over, using the correct sense number as input helps
to improve the overall performance of both clus-
tering based method and ELP based method.
Comparing the performance of the same sys-
tem with different sizes of tagged data (from the
first experiment to the third experiment, and from
the fourth experiment to the sixth experiment), we
can see that the performance was improved when
given more labeled data. Furthermore, ELP based
method outperforms other methods in terms of ac-
curacy when rare senses (e.g. s3) are missing in
the tagged data. It seems that ELP based method
has the ability to find rare senses with the use of
tagged and untagged corpora.
LPU algorithm can deal with only one-class
classification problem. Therefore the labeled data
of other classes cannot be used when determining
the positive labeled data for current class. ELP
can use the labeled data of all the known classes to
determine the seeds of unknown classes. It may
explain why LPU?s performance is worse than
ELP based sense disambiguation although LPU
can correctly estimate the sense number in XL+U
420
Table 4: This table summarizes the accuracy of SVM, LPU, the semi-supervised k-means clustering al-
gorithm with correct sense number |S| or estimated sense number k?XL+U as input, and the ELP algorithm
with correct sense number |S| or estimated sense number k?XL+U as input on the official test data of ELS
task in SENSEVAL-3 when given various incomplete tagged corpora.
Clustering algorithm ELP algorithm Clustering algorithm ELP algorithm
SVM LPU with |S| as input with |S| as input with k?XL+U as input with k?XL+U as input
Ssubset =
{s1} 30.6% 22.3% 43.9% 47.8% 40.0% 38.7%
Ssubset =
{s2} 59.7% 54.6% 44.0% 62.4% 48.5% 62.6%
Ssubset =
{s3} 67.0% 53.4% 48.7% 67.2% 52.4% 69.1%
Ssubset =
{s1, s2} 14.6% 13.1% 44.4% 40.2% 35.6% 33.0%
Ssubset =
{s1, s3} 25.7% 21.1% 48.5% 37.9% 39.8% 31.0%
Ssubset =
{s2, s3} 56.2% 53.1% 47.3% 59.4% 46.6% 58.7%
Average accuracy 42.3% 36.3% 46.1% 52.5% 43.8% 48.9%
Table 5: These two tables provide the mean and
standard deviation of absolute values of the differ-
ence between ground-truth results |S| and sense
numbers estimated by clustering or ELP based or-
der identification procedure respectively.
Clustering based method ELP based method
Ssubset =
{s1} 1.3?1.1 2.2?1.1
Ssubset =
{s2} 2.4?0.9 2.4?0.9
Ssubset =
{s3} 2.6?0.7 2.6?0.7
Ssubset =
{s1, s2} 1.2?0.6 1.6?0.5
Ssubset =
{s1, s3} 1.4?0.6 1.8?0.4
Ssubset =
{s2, s3} 1.8?0.5 1.8?0.5
when only one sense is missing in XL.
When very few labeled examples are avail-
able, the noise in labeled data makes it difficult
to learn the classification score (each entry in
YDU ). Therefore using the classification confi-
dence criterion may lead to poor performance of
seed selection for unknown classes if the classifi-
cation score is not accurate. It may explain why
ELP based method does not outperform cluster-
ing based method with small labeled data (e.g.,
Ssubset = {s1}).
3.3 Results on Sense Number Estimation
Table 5 provides the mean and standard devia-
tion of absolute difference values between ground-
truth results |S| and sense numbers estimated by
clustering or ELP based order identification pro-
cedures respectively. For example, if the ground
truth sense number of the word w is kw, and the es-
timated value is k?w, then the absolute value of the
difference between these two values is |kw ? k?w|.
Therefore we can have this value for each word.
Then we calculated the mean and deviation on this
array of absolute values. LPU does not have the
order identification capability since it always as-
sumes that there is at least one new class in un-
labeled data, and does not further differentiate the
instances from these new classes. Therefore we do
not provide the order identification results of LPU.
From the results in Table 5, we can see that esti-
mated sense numbers are closer to ground truth re-
sults when given less labeled data for clustering or
ELP based methods. Moreover, clustering based
method performs better than ELP based method in
terms of order identification when given less la-
beled data (e.g., Ssubset = {s1}). It seems that
ELP is not robust to the noise in small labeled data,
compared with the semi-supervised k-means clus-
tering algorithm.
4 Related Work
The work closest to ours is partially supervised
classification or building classifiers using positive
examples and unlabeled examples, which has been
studied in machine learning community (Denis et
al., 2002; Liu et al, 2003; Manevitz and Yousef,
2001; Yu et al, 2002). However, they cannot
421
group negative examples into meaningful clusters.
In contrast, our algorithm can find the occurrence
of negative examples and further group these neg-
ative examples into a ?natural? number of clusters.
Semi-supervised clustering (Wagstaff et al, 2001)
may be used to perform classification by the use
of labeled and unlabeled examples, but it encoun-
ters the same problem of partially supervised clas-
sification that model order cannot be automatically
estimated.
Levine and Domany (2001) and Lange et al
(2002) proposed cluster validation based criteria
for cluster number estimation. However, they
showed the application of the cluster validation
method only for unsupervised learning. Our work
can be considered as an extension of their methods
in the setting of partially supervised learning.
In natural language processing community, the
work that is closely related to ours is word sense
discrimination which can induce senses by group-
ing occurrences of a word into clusters (Schu?tze,
1998). If it is considered as unsupervised meth-
ods to solve sense disambiguation problem, then
our method employs partially supervised learning
technique to deal with sense disambiguation prob-
lem by use of tagged and untagged texts.
5 Conclusions
In this paper, we present an order identification
based partially supervised classification algorithm
and investigate its application to partially super-
vised word sense disambiguation problem. Exper-
imental results on SENSEVAL-3 data indicate that
our ELP based model order identification algo-
rithm achieves better performance than other state
of the art classification algorithms, e.g., SVM,
a one-class partially supervised algorithm (LPU),
and a semi-supervised k-means clustering based
model order identification algorithm.
References
Brown P., Stephen, D.P., Vincent, D.P., & Robert, Mer-
cer.. 1991. Word Sense Disambiguation Using Sta-
tistical Methods. Proceedings of ACL.
Dagan, I. & Itai A.. 1994. Word Sense Disambigua-
tion Using A Second Language Monolingual Cor-
pus. Computational Linguistics, Vol. 20(4), pp. 563-
596.
Denis, F., Gilleron, R., & Tommasi, M.. 2002. Text
Classification from Positive and Unlabeled Exam-
ples. Proceedings of the 9th International Confer-
ence on Information Processing and Management of
Uncertainty in Knowledge-Based Systems.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M.
2002. Stability-Based Model Selection. NIPS 15.
Leacock, C., Miller, G.A. & Chodorow, M.. 1998.
Using Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24:1, 147?165.
Lee, Y.K. & Ng, H.T.. 2002. An Empirical Eval-
uation of Knowledge Sources and Learning Algo-
rithms for Word Sense Disambiguation. Proceed-
ings of EMNLP, (pp. 41-48).
Levine, E., & Domany, E. 2001. Resampling Method
for Unsupervised Estimation of Cluster Validity.
Neural Computation, Vol. 13, 2573?2593.
Lin, J. 1991. Divergence Measures Based on the
Shannon Entropy. IEEE Transactions on Informa-
tion Theory, 37:1, 145?150.
Liu, B., Dai, Y., Li, X., Lee, W.S., & Yu, P.. 2003.
Building Text Classifiers Using Positive and Unla-
beled Examples. Proceedings of IEEE ICDM.
Manevitz, L.M., & Yousef, M.. 2001. One Class
SVMs for Document Classification. Journal of Ma-
chine Learning, 2, 139-154.
Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004.
The SENSEVAL-3 English Lexical Sample Task.
SENSEVAL-2004.
Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2005. Word Sense
Disambiguation Using Label Propagation Based
Semi-Supervised Learning. Proceedings of ACL.
Schu?tze, H.. 1998. Automatic Word Sense Discrimi-
nation. Computational Linguistics, 24:1, 97?123.
Wagstaff, K., Cardie, C., Rogers, S., & Schroedl, S..
2001. Constrained K-Means Clustering with Back-
ground Knowledge. Proceedings of ICML.
Yarowsky, D.. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. Pro-
ceedings of ACL.
Yu, H., Han, J., & Chang, K. C.-C.. 2002. PEBL: Pos-
itive example based learning for web page classifi-
cation using SVM. Proceedings of ACM SIGKDD.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
422
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 568?575,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Relation Disambiguation with Order Identification
Capabilities
Jinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu1
1Institute for Infocomm Research 2Department of Computer Science
21 Heng Mui Keng Terrace National University of Singapore
119613 Singapore 117543 Singapore
{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
Abstract
We present an unsupervised learning ap-
proach to disambiguate various relations
between name entities by use of various
lexical and syntactic features from the
contexts. It works by calculating eigen-
vectors of an adjacency graph?s Lapla-
cian to recover a submanifold of data
from a high dimensionality space and
then performing cluster number estima-
tion on the eigenvectors. This method
can address two difficulties encoutered
in Hasegawa et al (2004)?s hierarchical
clustering: no consideration of manifold
structure in data, and requirement to pro-
vide cluster number by users. Experiment
results on ACE corpora show that this
spectral clustering based approach outper-
forms Hasegawa et al (2004)?s hierarchi-
cal clustering method and a plain k-means
clustering method.
1 Introduction
The task of relation extraction is to identify vari-
ous semantic relations between name entities from
text. Prior work on automatic relation extraction
come in three kinds: supervised learning algorithms
(Miller et al, 2000; Zelenko et al, 2002; Culotta
and Soresen, 2004; Kambhatla, 2004; Zhou et al,
2005), semi-supervised learning algorithms (Brin,
1998; Agichtein and Gravano, 2000; Zhang, 2004),
and unsupervised learning algorithm (Hasegawa et
al., 2004).
Among these methods, supervised learning is usu-
ally more preferred when a large amount of la-
beled training data is available. However, it is
time-consuming and labor-intensive to manually tag
a large amount of training data. Semi-supervised
learning methods have been put forward to mini-
mize the corpus annotation requirement. Most of
semi-supervised methods employ the bootstrapping
framework, which only need to pre-define some ini-
tial seeds for any particular relation, and then boot-
strap from the seeds to acquire the relation. How-
ever, it is often quite difficult to enumerate all class
labels in the initial seeds and decide an ?optimal?
number of them.
Compared with supervised and semi-supervised
methods, Hasegawa et al (2004)?s unsupervised ap-
proach for relation extraction can overcome the dif-
ficulties on requirement of a large amount of labeled
data and enumeration of all class labels. Hasegawa
et al (2004)?s method is to use a hierarchical cluster-
ing method to cluster pairs of named entities accord-
ing to the similarity of context words intervening be-
tween the named entities. However, the drawback of
hierarchical clustering is that it required providing
cluster number by users. Furthermore, clustering is
performed in original high dimensional space, which
may induce non-convex clusters hard to identified.
This paper presents a novel application of spec-
tral clustering technique to unsupervised relation ex-
traction problem. It works by calculating eigenvec-
tors of an adjacency graph?s Laplacian to recover a
submanifold of data from a high dimensional space,
and then performing cluster number estimation on
a transformed space defined by the first few eigen-
vectors. This method may help us find non-convex
clusters. It also does not need to pre-define the num-
ber of the context clusters or pre-specify the simi-
larity threshold for the clusters as Hasegawa et al
568
(2004)?s method.
The rest of this paper is organized as follows. Sec-
tion 2 formulates unsupervised relation extraction
and presents how to apply the spectral clustering
technique to resolve the task. Then section 3 reports
experiments and results. Finally we will give a con-
clusion about our work in section 4.
2 Unsupervised Relation Extraction
Problem
Assume that two occurrences of entity pairs with
similar contexts, are tend to hold the same relation
type. Thus unsupervised relation extraction prob-
lem can be formulated as partitioning collections of
entity pairs into clusters according to the similarity
of contexts, with each cluster containing only entity
pairs labeled by the same relation type. And then, in
each cluster, the most representative words are iden-
tified from the contexts of entity pairs to induce the
label of relation type. Here, we only focus on the
clustering subtask and do not address the relation
type labeling subtask.
In the next subsections we will describe our pro-
posed method for unsupervised relation extraction,
which includes: 1) Collect the context vectors in
which the entity mention pairs co-occur; 2) Cluster
these Context vectors.
2.1 Context Vector and Feature Design
Let X = {xi}ni=1 be the set of context vectors of oc-
currences of all entity mention pairs, where xi repre-
sents the context vector of the i-th occurrence, and n
is the total number of occurrences of all entity pairs.
Each occurrence of entity mention pairs can be
denoted as follows:
R ? (Cpre, e1, Cmid, e2, Cpost) (1)
where e1 and e2 represents the entity mentions, and
Cpre,Cmid,and Cpost are the contexts before, be-
tween and after the entity pairs respectively.
We extracted features from e1, e2, Cpre, Cmid,
Cpost to construct context vectors, which are com-
puted from the parse trees derived from Charniak
Parser (Charniak, 1999) and the Chunklink script 1
written by Sabine Buchholz from Tilburg University.
1 Software available at http://ilk.uvt.nl/ sabine/chunklink/
Words: Words in the two entities and three context
windows.
Entity Type: the entity type of both entity men-
tions, which can be PERSON, ORGANIZA-
TION, FACILITY, LOCATION and GPE.
POS features: Part-Of-Speech tags corresponding
to all words in the two entities and three con-
text windows.
Chunking features: This category of features are
extracted from the chunklink representation,
which includes:
? Chunk tag information of the two entities and
three context windows. The ?0? tag means that
the word is outside of any chunk. The ?I-XP? tag
means that this word is inside an XP chunk. The
?B-XP? by default means that the word is at the be-
ginning of an XP chunk.
? Grammatical function of the two entities and
three context windows. The last word in each chunk
is its head, and the function of the head is the func-
tion of the whole chunk. ?NP-SBJ? means a NP
chunk as the subject of the sentence. The other
words in a chunk that are not the head have ?NO-
FUNC? as their function.
? IOB-chains of the heads of the two entities. So-
called IOB-chain, noting the syntactic categories of
all the constituents on the path from the root node
to this leaf node of tree.
We combine the above lexical and syntactic fea-
tures with their position information in the context
to form the context vector. Before that, we filter out
low frequency features which appeared only once in
the entire set.
2.2 Context Clustering
Once the context vectors of entity pairs are prepared,
we come to the second stage of our method: cluster
these context vectors automatically.
In recent years, spectral clustering technique has
received more and more attention as a powerful ap-
proach to a range of clustering problems. Among
the efforts on spectral clustering techniques (Weiss,
1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,
2001; Zha et al, 2001), we adopt a modified version
(Sanguinetti et al, 2005) of the algorithm by Ng et
al. (2001) because it can provide us model order se-
lection capability.
Since we do not know how many relation types
in advance and do not have any labeled relation
569
Table 1: Context Clustering with Spectral-based Clustering
technique.
Input: A set of context vectors X = {x1, x2, ..., xn},
X ? <n?d;
Output: Clustered data and number of clusters;
1. Construct an affinity matrix by Aij = exp(? s
2
ij
?2 ) if i 6=j, 0 if i = j. Here, sij is the similarity between xi and
xj calculated by Cosine similarity measure. and the free
distance parameter ?2 is used to scale the weights;
2. Normalize the affinity matrix A to create the matrix L =
D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)
element is the sum of A?s ith row;
3. Set q = 2;
4. Compute q eigenvectors of L with greatest eigenvalues.
Arrange them in a matrix Y .
5. Perform elongated K-means with q + 1 centers on Y ,
initializing the (q + 1)-th mean in the origin;
6. If the q+1-th cluster contains any data points, then there
must be at least an extra cluster; set q = q + 1 and go
back to step 4. Otherwise, algorithm stops and outputs
clustered data and number of clusters.
training examples at hand, the problem of model
order selection arises, i.e. estimating the ?opti-
mal? number of clusters. Formally, let k be the
model order, we need to find k in Equation: k =
argmaxk{criterion(k)}. Here, the criterion is de-
fined on the result of spectral clustering.
Table 1 shows the details of the whole algorithm
for context clustering, which contains two main
stages: 1) Transformation of Clustering Space (Step
1-4); 2) Clustering in the transformed space using
Elongated K-means algorithm (Step 5-6).
2.3 Transformation of Clustering Space
We represent each context vector of entity pair as a
node in an undirected graph. Each edge (i,j) in the
graph is assigned a weight that reflects the similarity
between two context vectors i and j. Hence, the re-
lation extraction task for entity pairs can be defined
as a partition of the graph so that entity pairs that
are more similar to each other, e.g. labeled by the
same relation type, belong to the same cluster. As a
relaxation of such NP-hard discrete graph partition-
ing problem, spectral clustering technique computes
eigenvalues and eigenvectors of a Laplacian matrix
related to the given graph, and construct data clus-
ters based on such spectral information.
Thus the starting point of context clustering is to
construct an affinity matrix A from the data, which
is an n ? n matrix encoding the distances between
the various points. The affinity matrix is then nor-
malized to form a matrix L by conjugating with the
the diagonal matrix D?1/2 which has as entries the
square roots of the sum of the rows of A. This is to
take into account the different spread of the various
clusters (points belonging to more rarified clusters
will have lower sums of the corresponding row of
A). It is straightforward to prove that L is positive
definite and has eigenvalues smaller or equal to 1,
with equality holding in at least one case.
Let K be the true number of clusters present in
the dataset. If K is known beforehand, the first K
eigenvectors of L will be computed and arranged as
columns in a matrix Y . Each row of Y corresponds
to a context vector of entity pair, and the above pro-
cess can be considered as transforming the original
context vectors in a d-dimensional space to new con-
text vectors in the K-dimensional space. Therefore,
the rows of Y will cluster upon mutually orthogonal
points on the K dimensional sphere,rather than on
the coordinate axes.
2.4 The Elongated K-means algorithm
As the step 5 of Table 1 shows, the result of elon-
gated K-means algorithm is used to detect whether
the number of clusters selected q is less than the true
number K, and allows one to iteratively obtain the
number of clusters.
Consider the case when the number of clusters q
is less than the true cluster number K present in the
dataset. In such situation, taking the first q < K
eigenvectors, we will be selecting a q-dimensional
subspace in the clustering space. As the rows of the
K eigenvectors clustered along mutually orthogo-
nal vectors, their projections in a lower dimensional
space will cluster along radial directions. Therefore,
the general picture will be of q clusters elongated in
the radial direction, with possibly some clusters very
near the origin (when the subspace is orthogonal to
some of the discarded eigenvectors).
Hence, the K-means algorithm is modified as
the elongated K-means algorithm to downweight
distances along radial directions and penalize dis-
tances along transversal directions. The elongated
K-means algorithm computes the distance of point
x from the center ci as follows:
? If the center is not very near the origin, cTi ci > ? (? is a
parameter to be fixed by the user), the distances are cal-
570
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(a) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(b) 
0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
(c) 
-4 -3 -2 -1 0 1 2 3 4-4
-3
-2
-1
0
1
2
3
4
(d) 
Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Three
elongated clusters in the 2D clustering space using
Spectral clustering: two dominant eigenvectors; (d)
The clustering result using Spectral-based clustering
(?2=0.05). (4,? and + denote examples in different
clusters)
culated as: edist(x, ci) = (x ? ci)TM(x ? ci), where
M = 1? (Iq ?
cicTi
cTi ci
) + ? cic
T
i
cTi ci
, ? is the sharpness param-
eter that controls the elongation (the smaller, the more
elongated the clusters) 2.
? If the center is very near the origin,cTi ci < ?, the dis-
tances are measured using the Euclidean distance.
In each iteration of procedure in Table 1, elon-
gated K-means is initialized with q centers corre-
sponding to data points in different clusters and one
center in the origin. The algorithm then will drag the
center in the origin towards one of the clusters not
accounted for. Compute another eigenvector (thus
increasing the dimension of the clustering space to
q + 1) and repeat the procedure. Eventually, when
one reach as many eigenvectors as the number of
clusters present in the data, no points will be as-
signed to the center at the origin, leaving the cluster
empty. This is the signal to terminate the algorithm.
2.5 An example
Figure 1 visualized the clustering result of three cir-
cle dataset using K-means and Spectral-based clus-
tering. From Figure 1(b), we can see that K-means
can not separate the non-convex clusters in three cir-
cle dataset successfully since it is prone to local min-
imal. For spectral-based clustering, as the algorithm
described, initially, we took the two eigenvectors of
L with largest eigenvalues, which gave us a two-
dimensional clustering space. Then to ensure that
the two centers are initialized in different clusters,
one center is set as the point that is the farthest from
the origin, while the other is set as the point that
simultaneously farthest the first center and the ori-
gin. Figure 1(c) shows the three elongated clusters in
the 2D clustering space and the corresponding clus-
tering result of dataset is visualized in Figure 1(d),
which exploits manifold structure (cluster structure)
in data.
3 Experiments and Results
3.1 Data Setting
Our proposed unsupervised relation extraction is
evaluated on ACE corpus, which contains 519 files
from sources including broadcast, newswire, and
newspaper. We only deal with intra-sentence ex-
plicit relations and assumed that all entities have
2 In this paper, the sharpness parameter ? is set to 0.2
571
Table 2: Frequency of Major Relation SubTypes in the ACE
training and devtest corpus.
Type SubType Training Devtest
ROLE General-Staff 550 149
Management 677 122
Citizen-Of 127 24
Founder 11 5
Owner 146 15
Affiliate-Partner 111 15
Member 460 145
Client 67 13
Other 15 7
PART Part-Of 490 103
Subsidiary 85 19
Other 2 1
AT Located 975 192
Based-In 187 64
Residence 154 54
SOC Other-Professional 195 25
Other-Personal 60 10
Parent 68 24
Spouse 21 4
Associate 49 7
Other-Relative 23 10
Sibling 7 4
GrandParent 6 1
NEAR Relative-Location 88 32
been detected beforehand in the EDT sub-task of
ACE. To verify our proposed method, we only col-
lect those pairs of entity mentions which have been
tagged relation types in the given corpus. Then the
relation type tags were removed to test the unsuper-
vised relation disambiguation. During the evalua-
tion procedure, the relation type tags were used as
ground truth classes. A break-down of the data by
24 relation subtypes is given in Table 2.
3.2 Evaluation method for clustering result
When assessing the agreement between clustering
result and manually annotated relation types (ground
truth classes), we would encounter the problem that
there was no relation type tags for each cluster in our
clustering results.
To resolve the problem, we construct a contin-
gency table T , where each entry ti,j gives the num-
ber of the instances that belong to both the i-th es-
timated cluster and j-th ground truth class. More-
over, to ensure that any two clusters do not share
the same labels of relation types, we adopt a per-
mutation procedure to find an one-to-one mapping
function ? from the ground truth classes (relation
types) TC to the estimated clustering result EC.
There are at most |TC| clusters which are assigned
relation type tags. And if the number of the esti-
mated clusters is less than the number of the ground
truth clusters, empty clusters should be added so that
|EC| = |TC| and the one-to-one mapping can be
performed, which can be formulated as the function:
?? = argmax?
?|TC|
j=1 t?(j),j , where ?(j) is the in-
dex of the estimated cluster associated with the j-th
class.
Given the result of one-to-one mapping, we adopt
Precision, Recall and F-measure to evaluate the
clustering result.
3.3 Experimental Design
We perform our unsupervised relation extraction on
the devtest set of ACE corpus and evaluate the al-
gorithm on relation subtype level. Firstly, we ob-
serve the influence of various variables, including
Distance Parameter ?2, Different Features, Context
Window Size. Secondly, to verify the effectiveness
of our method, we further compare it with super-
vised method based on SVM and other two unsuper-
vised methods.
3.3.1 Choice of Distance Parameter ?2
We simply search over ?2 and pick the value
that finds the best aligned set of clusters on the
transformed space. Here, the scattering criterion
trace(P?1W PB) is used to compare the cluster qual-
ity for different value of ?2 3, which measures the ra-
tio of between-cluster to within-cluster scatter. The
higher the trace(P?1W PB), the higher the cluster
quality.
In Table 3 and Table 4, with different settings of
feature set and context window size, we find out the
corresponding value of ?2 and cluster number which
maximize the trace value in searching for a range of
value ?2.
3.3.2 Contribution of Different Features
As the previous section presented, we incorporate
various lexical and syntactic features to extract rela-
3 trace(P?1W PB) is trace of a matrix which is the sum of
its diagonal elements. PW is the within-cluster scatter matrix
as: PW =
?c
j=1
?
Xi??j (Xi ? mj)(Xi ? mj)
t and PB
is the between-cluster scatter matrix as: PB =
?c
j=1(mj ?
m)(mj ? m)t, where m is the total mean vector and mj is
the mean vector for jth cluster and (Xj ? mj)t is the matrix
transpose of the column vector (Xj ?mj).
572
Table 3: Contribution of Different Features
Features ?2 cluster number trace value Precison Recall F-measure
Words 0.021 15 2.369 41.6% 30.2% 34.9%
+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%
+POS 0.017 18 3.206 37.8% 46.9% 41.8%
+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%
Table 4: Different Context Window Size Setting
Context Window Size ?2 cluster number trace value Precision Recall F-measure
0 0.016 18 3.576 37.6% 48.1% 42.2%
2 0.015 19 3.900 43.5% 49.4% 46.3%
5 0.020 21 2.225 29.3% 34.7% 31.7%
tion. To measure the contribution of different fea-
tures, we report the performance by gradually in-
creasing the feature set, as Table 3 shows.
Table 3 shows that all of the four categories of fea-
tures contribute to the improvement of performance
more or less. Firstly,the addition of entity type fea-
ture is very useful, which improves F-measure by
6.6%. Secondly, adding POS features can increase
F-measure score but do not improve very much.
Thirdly, chunking features also show their great use-
fulness with increasing Precision/Recall/F-measure
by 5.7%/2.5%/4.5%.
We combine all these features to do all other eval-
uations in our experiments.
3.3.3 Setting of Context Window Size
We have mentioned in Section 2 that the context
vectors of entity pairs are derived from the contexts
before, between and after the entity mention pairs.
Hence, we have to specify the three context window
size first. In this paper, we set the mid-context win-
dow as everything between the two entity mentions.
For the pre- and post- context windows, we could
have different choices. For example, if we specify
the outer context window size as 2, then it means that
the pre-context (post-context)) includes two words
before (after) the first (second) entity.
For comparison of the effect of the outer context
of entity mention pairs, we conducted three different
settings of context window size (0, 2, 5) as Table 4
shows. From this table we can find that with the con-
text window size setting, 2, the algorithm achieves
the best performance of 43.5%/49.4%/46.3% in
Precision/Recall/F-measure. With the context win-
dow size setting, 5, the performance becomes worse
Table 5: Performance of our proposed method (Spectral-
based clustering) compared with supervised method (SVM) and
unsupervised methods((Hasegawa et al, 2004))?s method and
K-means clustering.
Precision Recall F-measure
SVM 61.2% 49.6% 54.8%
Hasegawa?s Method1 38.7% 29.8% 33.7%
Hasegawa?s Method2 37.9% 36.0% 36.9%
Kmeans 34.3% 40.2% 36.8%
Our Proposed Method 43.5% 49.4% 46.3%
because extending the context too much may include
more features, but at the same time, the noise also
increases.
3.3.4 Comparison with Supervised methods
and other Unsupervised methods
To explore the effectiveness of our unsupervised
method compared to supervised method, we perform
SVM technique with the same feature set defined in
our proposed method. The LIBSVM tool is used in
this test 4. The kernel function we used is linear
and SVM models are trained using the training set
of ACE corpus.
In (Hasegawa et al, 2004), they preformed un-
supervised relation extraction based on hierarchical
clustering and they only used word features between
entity mention pairs to construct context vectors. We
reported the clustering results using the same clus-
tering strategy as Hasegawa et al (2004) proposed.
In Table 5, Hasegawa?s Method1 means the test used
the word feature as Hasegawa et al (2004) while
Hasegawa?s Method2 means the test used the same
feature set as our method. In both tests, we specified
4 LIBSVM : a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/ cjlin/libsvm. It
supports multi-class classification.
573
Table 6: Comparison of the existing efforts on ACE RDC task.
Relation Dectection Relation Classification
on Types on Subtypes
Method P R F P R F P R F
Culotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -
Kambhatla (2004) Feature based, Maxi-
mum Entropy
- - - - - - 63.5 45.2 52.8
Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5
the cluster number as the number of ground truth
classes.
We also approached the relation extraction prob-
lem using the standard clustering technique, K-
means, where we adopted the same feature set de-
fined in our proposed method to cluster the con-
text vectors of entity mention pairs and pre-specified
the cluster number as the number of ground truth
classes.
Table 5 reports the performance of our pro-
posed method comparing with SVM-based super-
vised method and the other two unsupervised meth-
ods. As the result shows, SVM-based method by us-
ing the same feature set in our proposed method can
achieve 61.2%/49.6%/54.8% in Precision/Recall/F-
measure. Table 5 also shows our proposed spec-
tral based method clearly outperforms the other
two unsupervised methods by 12.5% and 9.5% in
F-measure respectively. Moreover, the incorpora-
tion of various lexical and syntactic features into
Hasegawa et al (2004)?s method2 makes it outper-
form Hasegawa et al (2004)?s method1 which only
uses word feature.
3.4 Discussion
In this paper, we have shown that the modified spec-
tral clustering technique, with various lexical and
syntactic features derived from the context of en-
tity pairs, performed well on the unsupervised re-
lation disambiguation problem. Our experiments
show that by the choice of the distance parameter
?2, we can estimate the cluster number which pro-
vides the tightest clusters. We notice that the es-
timated cluster number is less than the number of
ground truth classes in most cases. The reason for
this phenomenon may be that some relation types
can not be easily distinguished using the context in-
formation only. For example, the relation subtypes
?Located?, ?Based-In? and ?Residence? are difficult
to disambiguate even for human experts to differen-
tiate.
The results also show that various lexical and
syntactic features contain useful information for the
task. Especially, although we did not concern the
dependency tree and full parse tree information as
other supervised methods (Miller et al, 2000; Cu-
lotta and Soresen, 2004; Kambhatla, 2004; Zhou et
al., 2005), the incorporation of simple features, such
as words and chunking information, still can provide
complement information for capturing the charac-
teristics of entity pairs. Another observation from
the result is that extending the outer context window
of entity mention pairs too much may not improve
the performance since the process may incorporate
more noise information and affect the clustering re-
sult.
As regards the clustering technique, the spectral-
based clustering performs better than direct cluster-
ing, K-means. Since the spectral-based algorithm
works in a transformed space of low dimension-
ality, data can be easily clustered so that the al-
gorithm can be implemented with better efficiency
and speed. And the performance using spectral-
based clustering can be improved due to the reason
that spectral-based clustering overcomes the draw-
back of K-means (prone to local minima) and may
find non-convex clusters consistent with human in-
tuition.
Currently most of works on the RDC task of ACE
focused on supervised learning methods. Table 6
lists a comparison of these methods on relation de-
tection and relation classification. Zhou et al (2005)
reported the best result as 63.1%/49.5%/55.5% in
Precision/Recall/F-measure on the extraction of
ACE relation subtypes using feature based method,
which outperforms tree kernel based method by
Culotta and Soresen (2004). Although our unsu-
pervised method still can not outperform these su-
574
pervised methods, from the point of view of un-
supervised resolution for relation extraction, our
approach already achieves best performance of
43.5%/49.4%/46.3% in Precision/Recall/F-measure
compared with other clustering methods.
4 Conclusion and Future work
In this paper, we approach unsupervised relation dis-
ambiguation problem by using spectral-based clus-
tering technique with diverse lexical and syntactic
features derived from context. The advantage of our
method is that it doesn?t need any manually labeled
relation instances, and pre-definition the number of
the context clusters. Experiment results on the ACE
corpus show that our method achieves better perfor-
mance than other unsupervised methods.
Currently we combine various lexical and syn-
tactic features to construct context vectors for clus-
tering. In the future we will further explore other
semantic information to assist the relation extrac-
tion problem. Moreover, instead of cosine similar-
ity measure to calculate the distance between con-
text vectors, we will try other distributional similar-
ity measures to see whether the performance of re-
lation extraction can be improved. In addition, if we
can find an effective unsupervised way to filter out
unrelated entity pairs in advance, it would make our
proposed method more practical.
References
Agichtein E. and Gravano L.. 2000. Snowball: Ex-
tracting Relations from large Plain-Text Collections,
In Proc. of the 5th ACM International Conference on
Digital Libraries (ACMDL?00).
Brin Sergey. 1998. Extracting patterns and relations
from world wide web. In Proc. of WebDB Workshop at
6th International Conference on Extending Database
Technology (WebDB?98). pages 172-183.
Charniak E.. 1999. A Maximum-entropy-inspired parser.
Technical Report CS-99-12.. Computer Science De-
partment, Brown University.
Culotta A. and Soresen J. 2004. Dependency tree kernels
for relation extraction, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Defense Advanced Research Projects Agency. 1995.
Proceedings of the Sixth Message Understanding Con-
ference (MUC-6) Morgan Kaufmann Publishers, Inc.
Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.
2004. Discovering Relations among Named Enti-
ties from Large Corpora, Proceeding of Conference
ACL2004. Barcelona, Spain.
Kambhatla N. 2004. Combining lexical, syntactic and
semantic features with Maximum Entropy Models for
extracting relations, In proceedings of 42th Annual
Meeting of the Association for Computational Linguis-
tics. 21-26 July 2004. Barcelona, Spain.
Kannan R., Vempala S., and Vetta A.. 2000. On cluster-
ing: Good,bad and spectral. In Proceedings of the 41st
Foundations of Computer Science. pages 367-380.
Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.
A novel use of statistical parsing to extract information
from text. In proceedings of 6th Applied Natural Lan-
guage Processing Conference. 29 April-4 may 2000,
Seattle USA.
Ng Andrew.Y, Jordan M., and Weiss Y.. 2001. On spec-
tral clustering: Analysis and an algorithm. In Pro-
ceedings of Advances in Neural Information Process-
ing Systems. pages 849-856.
Sanguinetti G., Laidler J. and Lawrence N.. 2005. Au-
tomatic determination of the number of clusters us-
ing spectral algorithms.In: IEEE Machine Learning
for Signal Processing. 28-30 Sept 2005, Mystic, Con-
necticut, USA.
Shi J. and Malik.J. 2000. Normalized cuts and image
segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence. 22(8):888-905.
Weiss Yair. 1999. Segmentation using eigenvectors: A
unifying view. ICCV(2). pp.975-982.
Zelenko D., Aone C. and Richardella A.. 2002. Ker-
nel Methods for Relation Extraction, Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Philadelphia.
Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001. Spec-
tral Relaxation for k-means clustering. In Neural In-
formation Processing Systems (NIPS2001). pages
1057-1064, 2001.
Zhang Zhu. 2004. Weakly-supervised relation classifi-
cation for Information Extraction, In proceedings of
ACM 13th conference on Information and Knowledge
Management (CIKM?2004). 8-13 Nov 2004. Wash-
ington D.C.,USA.
Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.
2005. Exploring Various Knowledge in Relation Ex-
traction, In proceedings of 43th Annual Meeting of the
Association for Computational Linguistics. USA.
575
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 177?182,
Prague, June 2007. c?2007 Association for Computational Linguistics
I2R: Three Systems for Word Sense Discrimination, Chinese Word Sense
Disambiguation, and English Word Sense Disambiguation
Zheng-Yu Niu, Dong-Hong Ji
Institute for Infocomm Research
21 Heng Mui Keng Terrace
119613 Singapore
niu zy@hotmail.com
dhji@i2r.a-star.edu.sg
Chew-Lim Tan
Department of Computer Science
National University of Singapore
3 Science Drive 2
117543 Singapore
tancl@comp.nus.edu.sg
Abstract
This paper describes the implementation
of our three systems at SemEval-2007, for
task 2 (word sense discrimination), task 5
(Chinese word sense disambiguation), and
the first subtask in task 17 (English word
sense disambiguation). For task 2, we ap-
plied a cluster validation method to esti-
mate the number of senses of a target word
in untagged data, and then grouped the in-
stances of this target word into the esti-
mated number of clusters. For both task 5
and task 17, We used the label propagation
algorithm as the classifier for sense disam-
biguation. Our system at task 2 achieved
63.9% F-score under unsupervised evalua-
tion, and 71.9% supervised recall with su-
pervised evaluation. For task 5, our sys-
tem obtained 71.2% micro-average preci-
sion and 74.7% macro-average precision.
For the lexical sample subtask for task
17, our system achieved 86.4% coarse-
grained precision and recall.
1 Introduction
SemEval-2007 launches totally 18 tasks for evalua-
tion exercise, covering word sense disambiguation,
word sense discrimination, semantic role labeling,
and sense disambiguation for information retrieval,
and other topics in NLP. We participated three tasks
in SemEval-2007, which are task 2 (Evaluating
Word Sense Induction and Discrimination Systems),
task 5 (Multilingual Chinese-English Lexical Sam-
ple Task) and the first subtask at task 17 (English
Lexical Sample, English Semantic Role Labeling
and English All-Words Tasks).
The goal for SemEval-2007 task 2 (Evaluat-
ing Word Sense Induction and Discrimination Sys-
tems)(Agirre and Soroa, 2007) is to automatically
discriminate the senses of English target words by
the use of only untagged data. Here we address this
word sense discrimination problem by (1) estimat-
ing the number of word senses of a target word in
untagged data using a stability criterion, and then (2)
grouping the instances of this target word into the
estimated number of clusters according to the simi-
larity of contexts of the instances. No sense-tagged
data is used to help the clustering process.
The goal of task 5 (Chinese Word Sense Disam-
biguation) is to create a framework for the evaluation
of word sense disambiguation in Chinese-English
machine translation systems. Each participates of
this task will be provided with sense tagged train-
ing data and untagged test data for 40 Chinese pol-
ysemous words. The ?sense tags? for the ambigu-
ous Chinese target words are given in the form of
their English translations. Here we used a semi-
supervised classification algorithm (label propaga-
tion algorithm) (Niu, et al, 2005) to address this
Chinese word sense disambiguation problem.
The lexical sample subtask of task 17 (English
Word Sense Disambiguation) provides sense-tagged
training data and untagged test data for 35 nouns and
65 verbs. This data includes, for each target word:
OntoNotes sense tags (these are groupings of Word-
Net senses that are more coarse-grained than tradi-
177
tional WN entries), as well as the sense inventory for
these lemmas. Here we used only the training data
supplied in this subtask for sense disambiguation in
test set. The label propagation algorithm (Niu, et al,
2005) was used to perform sense disambiguation by
the use of both training data and test data.
This paper will be organized as follows. First, we
will provide the feature set used for task 2, task 5
and task 17 in section 2. Secondly, we will present
the word sense discrimination method used for task
2 in section 3. Then, we will give the label propa-
gation algorithm for task 5 and task 17 in section 4.
Section 5 will provide the description of data sets at
task 2, task 5 and task 17. Then, we will present the
experimental results of our systems at the three tasks
in section 6. Finally we will give a conclusion of our
work in section 7.
2 Feature Set
In task 2, task 5 and task 17, we used three types of
features to capture contextual information: part-of-
speech of neighboring words (no more than three-
word distance) with position information, unordered
single words in topical context (all the contextual
sentences), and local collocations (including 11 col-
locations). The feature set used here is as same as
the feature set used in (Lee and Ng, 2002) except
that we did not use syntactic relations.
3 The Word Sense Discrimination Method
for Task 2
Word sense discrimination is to automatically dis-
criminate the senses of target words by the use of
only untagged data. So we can employ clustering
algorithms to address this problem. Another prob-
lem is that there is no sense inventories for target
words. So the clustering algorithms should have the
ability to automatically estimate the sense number
of a target word.
Here we used the sequential Information Bottle-
neck algorithm (sIB) (Slonim, et al, 2002) to esti-
mate cluster structure, which measures the similarity
of contexts of instances of target words according to
the similarity of their contextual feature conditional
distribution. But sIB requires the number of clus-
ters as input. So we used a cluster validation method
to automatically estimate the sense number of a tar-
Table 1: Sense number estimation procedure for
word sense discrimination.
1 Set lower bound Kmin and upper bound Kmax
for sense number k;
2 Set k = Kmin;
3 Conduct the cluster validation process
presented in Table 2 to evaluate the merit of k;
4 Record k and the value of Mk;
5 Set k = k + 1. If k ? Kmax, go to step 3,
otherwise go to step 6;
6 Choose the value k? that maximizes Mk,
where k? is the estimated sense number.
get word before clustering analysis. Cluster valida-
tion (or stability based approach)is a commonly used
method to the problem of model order identification
(or cluster number estimation) (Lange, et al, 2002;
Levine and Domany, 2001). The assumption of this
method is that if the model order is identical with the
true value, then the cluster structure estimated from
the data is stable against resampling, otherwise, it is
more likely to be the artifact of sampled data.
3.1 The Sense Number Estimation Procedure
Table 1 presents the sense number estimation pro-
cedure. Kmin was set as 2, and Kmax was set as 5 in
our system. The evaluation function Mk (described
in Table 2) is relevant with the sense number k. q
is set as 20 here. Clustering solution which is stable
against resampling will give rise to a local optimum
of Mk, which indicates the true value of sense num-
ber. In the cluster validation procedure, we used the
sIB algorithm to perform clustering analysis (de-
scribed in section 3.2).
The function M(C?, C) in Table 2 is given by
(Levine and Domany, 2001):
M(C?, C) =
?
i,j 1{C
?
i,j = Ci,j = 1, di ? D?, dj ? D?}?
i,j 1{Ci,j = 1, di ? D?, dj ? D?}
,
(1)
where D? is a subset with size ?|D| sampled from
full data set D, C and C? are |D|? |D| connectivity
matrixes based on clustering solutions computed on
D and D? respectively, and 0 ? ? ? 1. The con-
nectivity matrix C is defined as: Ci,j = 1 if di and
dj belong to the same cluster, otherwise Ci,j = 0.
C? is calculated in the same way. ? is set as 0.90 in
this paper.
178
Table 2: The cluster validation method for evalua-
tion of values of sense number k.
Function: Cluster Validation(k, D, q)
Input: cluster number k, data set D,
and sampling frequency q;
Output: the score of the merit of k;
1 Perform clustering analysis using sIB on
data set D with k as input;
2 Construct connectivity matrix Ck based on
above clustering solution on D;
3 Use a random predictor ?k to assign
uniformly drawn labels to instances in D;
4 Construct connectivity matrix C?k
using above clustering solution on D;
5 For ? = 1 to q do
5.1 Randomly sample a subset (D?) with size
?|D| from D, 0 ? ? ? 1;
5.2 Perform clustering analysis using sIB on
(D?) with k as input;
5.3 Construct connectivity matrix C?k using
above clustering solution on (D?);
5.4 Use ?k to assign uniformly drawn labels
to instances in (D?);
5.5 Construct connectivity matrix C??kusing above clustering solution on (D?);
Endfor
6 Evaluate the merit of k using following
objective function:
Mk = 1q
?
? M(C?k , Ck) ? 1q
?
? M(C??k , C?k),
where M(C?, C) is given by equation (1);
7 Return Mk;
M(C?, C) measures the proportion of document
pairs in each cluster computed on D that are also as-
signed into the same cluster by clustering solution
on D?. Clearly, 0 ? M ? 1. Intuitively, if clus-
ter number k is identical with the true value, then
clustering results on different subsets generated by
sampling should be similar with that on full data set,
which gives rise to a local optimum of M(C?, C).
In our algorithm, we normalize M(C?F,k, CF,k)using the equation in step 6 of Table 2, which
makes our objective function different from the fig-
ure of merit (equation ( 1)) proposed in (Levine
and Domany, 2001). The reason to normalize
M(C?F,k, CF,k) is that M(C?F,k, CF,k) tends to de-
crease when increasing the value of k. Therefore for
avoiding the bias that smaller value of k is to be se-
lected as cluster number, we use the cluster validity
of a random predictor to normalize M(C?F,k, CF,k).
3.2 The sIB Clustering Algorithm
Here we used the sIB algorithm (Slonim, et al,
2002) to estimate cluster structure, which measures
the similarity of contexts of instances according to
the similarity of their feature conditional distribu-
tion. sIB is a simplified ?hard? variant of informa-
tion bottleneck method (Tishby, et al, 1999).
Let d represent a document, and w represent a fea-
ture word, d ? D, w ? F . Given the joint distri-
bution p(d,w), the document clustering problem is
formulated as looking for a compact representation
T for D, which preserves as much information as
possible about F . T is the document clustering so-
lution. For solving this optimization problem, sIB
algorithm was proposed in (Slonim, et al, 2002),
which found a local maximum of I(T, F ) by: given
an initial partition T , iteratively drawing a d ? D
out of its cluster t(d), t ? T , and merging it into
tnew such that tnew = argmaxt?Td(d, t). d(d, t) is
the change of I(T, F ) due to merging d into cluster
tnew, which is given by
d(d, t) = (p(d) + p(t))JS(p(w|d), p(w|t)). (2)
JS(p, q) is the Jensen-Shannon divergence, which
is defined as
JS(p, q) = pipDKL(p?p) + piqDKL(q?p), (3)
DKL(p?p) =
?
y
plog pp, (4)
DKL(q?p) =
?
y
qlog qp, (5)
{p, q} ? {p(w|d), p(w|t)}, (6)
{pip, piq} ? {
p(d)
p(d) + p(t) ,
p(t)
p(d) + p(t)}, (7)
p = pipp(w|d) + piqp(w|t). (8)
179
4 The Label Propagation Algorithm for
Task 5 and Task 17
In the label propagation algorithm (LP) (Zhu and
Ghahramani, 2002), label information of any ver-
tex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage
is achieved. Larger edge weights allow labels to
travel through easier. Thus the closer the examples,
more likely they have similar labels (the global con-
sistency assumption).
In label propagation process, the soft label of each
initial labeled example is clamped in each iteration
to replenish label sources from these labeled data.
Thus the labeled data act like sources to push out la-
bels through unlabeled data. With this push from la-
beled examples, the class boundaries will be pushed
through edges with large weights and settle in gaps
along edges with small weights. If the data structure
fits the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
Let Y 0 ? Nn?c represent initial soft labels at-
tached to vertices, where Y 0ij = 1 if yi is sj and 0
otherwise. Let Y 0L be the top l rows of Y 0 and Y 0U
be the remaining u rows. Y 0L is consistent with the
labeling in labeled data, and the initialization of Y 0U
can be arbitrary.
Optimally we expect that the value of Wij across
different classes is as small as possible and the value
of Wij within same class is as large as possible.
This will make label propagation to stay within same
class. In later experiments, we set ? as the aver-
age distance between labeled examples from differ-
ent classes.
Define n ? n probability transition matrix Tij =
P (j ? i) = Wij?n
k=1 Wkj
, where Tij is the probability
to jump from example xj to example xi.
Compute the row-normalized matrix T by T ij =
Tij/
?n
k=1 Tik. This normalization is to maintain
the class probability interpretation of Y .
Then LP algorithm is defined as follows:
1. Initially set t=0, where t is iteration index;
2. Propagate the label by Y t+1 = TY t;
3. Clamp labeled data by replacing the top l row
of Y t+1 with Y 0L . Repeat from step 2 until Y t con-
verges;
4. Assign xh(l + 1 ? h ? n) with a label sj? ,
where j? = argmaxjYhj .
This algorithm has been shown to converge to
a unique solution, which is Y?U = limt?? Y tU =
(I ? T uu)?1T ulY 0L (Zhu and Ghahramani, 2002).
We can see that this solution can be obtained with-
out iteration and the initialization of Y 0U is not im-
portant, since Y 0U does not affect the estimation of
Y?U . I is u ? u identity matrix. T uu and T ul are
acquired by splitting matrix T after the l-th row and
the l-th column into 4 sub-matrices.
For task 5 and 17, we constructed connected
graphs as follows: two instances u, v will be con-
nected by an edge if u is among v?s k nearest neigh-
bors, or if v is among u?s k nearest neighbors as mea-
sured by cosine or JS distance measure. k is set 10
in our system implementation.
5 Data Sets of Task 2, Task 5 and Task 17
The test data for task 2 includes totally 27132 un-
tagged instances for 100 ambiguous English words.
There is no training data for task 2.
There are 40 ambiguous Chinese words in task
5. The training data for this task consists of 2686
instances, while the test data includes 935 instances.
There are 100 ambiguous English words in the
first subtask of task 17. The training data for this
task consists of 22281 instances, while the test data
includes 4851 instances.
6 Experimental Results of Our Systems at
Task 2, Task 5 and Task 17
Table 3: The best/worst/average F-score of all the
systems at task 2 and the F-score of our system at
task 2 for all target words, nouns and verbs with un-
supervised evaluation.
All words Nouns Verbs
Best 78.7% 80.8% 76.3%
Worst 56.1% 65.8% 45.1%
Average 65.4% 69.0% 61.4%
Our system 63.9% 68.0% 59.3%
Table 3 lists the best/worst/average F-score of all
the systems at task 2 and the F-score of our system
at task 2 for all target words, nouns and verbs with
180
Table 4: The best/worst/average supervised recall of
all the systems at task 2 and the supervised recall of
our system at task 2 for all target words, nouns and
verbs with supervised evaluation.
All words Nouns Verbs
Best 81.6% 86.8% 75.7%
Worst 78.5% 81.4% 75.2%
Average 79.6% 83.0% 75.7%
Our system 81.6% 86.8% 75.7%
Table 5: The best/worst/average micro-average pre-
cision and macro-average precision of all the sys-
tems at task 5 and the micro-average precision and
macro-average precision of our system at task 5.
Micro-average Macro-average
Best 71.7% 74.9%
Worst 33.7% 39.6%
Average 58.5% 62.7%
Our system 71.2% 74.7%
unsupervised evaluation. Our system obtained the
fourth place among six systems with unsupervised
evaluation. Table 4 shows the best/worst/average
supervised recall of all the systems at task 2 and the
supervised recall of our system at task 2 for all tar-
get words, nouns and verbs with supervised evalu-
ation. Our system is ranked as the first among six
systems with supervised evaluation. Table 7 lists
the estimated sense numbers by our system for all
the words at task 2. The average of all the estimated
sense numbers is 3.1, while the average of all the
ground-truth sense numbers is 3.6 if we consider the
sense inventories provided in task 17 as the answer.
It seems that our estimated sense numbers are close
to the ground-truth ones.
Table 5 provides the best/worst/average micro-
average precision and macro-average precision of all
the systems at task 5 and the micro-average preci-
sion and macro-average precision of our system at
task 5. Our system obtained the second place among
six systems for task 5.
Table 6 shows the best/worst/average coarse-
grained score (precision) of all the systems the lexi-
cal sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sample
Table 6: The best/worst/average coarse-grained
score (precision) of all the systems at the lexical
sample subtask of task 17 and the coarse-grained
score (precision) of our system at the lexical sam-
ple subtask of task 17.
Coarse-grained score (precision)
Best 88.7%
Worst 52.1%
Average 70.0%
Our system 86.4%
subtask of task 17. The attempted rate of all the sys-
tems is 100%. So the precision value is equal to the
recall value for all the systems. Here we listed only
the precision for the 13 systems at this subtask. Our
system is ranked as the third one among 13 systems.
7 Conclusion
In this paper, we described the implementation of
our I2R systems that participated in task 2, task 5,
and task 17 at SemEval-2007. Our systems achieved
63.9% F-score and 81.6% supervised recall for task
2, 71.2% micro-average precision and 74.7% macro-
average precision for task 5, and 86.4% coarse-
grained precision and recall for the lexical sample
subtask of task 17. The performance of our system
is very good under supervised evaluation. It may
be explained by that our system has the ability to
find some minor senses so that it can outperforms
the baseline system that always uses the most fre-
quent sense as the answer.
References
Agirre E. , & Soroa A. 2007. SemEval-2007 Task 2:
Evaluating Word Sense Induction and Discrimination
Systems. Proceedings of SemEval-2007, Association
for Computational Linguistics.
Lange, T., Braun, M., Roth, V., & Buhmann, J. M. 2002.
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Lee, Y.K., & Ng, H.T. 2002. An Empirical Evalua-
tion of Knowledge Sources and Learning Algorithms
for Word Sense Disambiguation. Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, (pp. 41-48).
181
Levine, E., & Domany, E. 2001. Resampling Method for
Unsupervised Estimation of Cluster Validity. Neural
Computation, Vol. 13, 2573?2593.
Niu, Z.Y., Ji, D.H., & Tan, C.L. 2005. Word Sense
Disambiguation Using Label Propagation Based Semi-
Supervised Learning. Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics.
Slonim, N., Friedman, N., & Tishby, N. 2002. Un-
supervised Document Classification Using Sequential
Information Maximization. Proceedings of the 25th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Tishby, N., Pereira, F., & Bialek, W. (1999) The Infor-
mation Bottleneck Method. Proc. of the 37th Allerton
Conference on Communication, Control and Comput-
ing.
Zhu, X. & Ghahramani, Z.. 2002. Learning from La-
beled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Table 7: The estimated sense numbers by our system
for all the words at task 2.
explain 2 move 3
position 3 express 4
buy 2 begin 2
hope 3 prepare 3
feel 5 policy 2
hold 2 attempt 2
work 5 recall 3
people 4 find 2
system 2 join 2
bill 2 build 2
hour 5 base 3
value 4 management 2
job 5 turn 4
rush 2 kill 2
ask 2 area 5
approve 4 affect 4
capital 4 keep 5
purchase 2 improve 2
propose 2 do 2
see 3 drug 5
president 3 come 5
power 3 disclose 4
effect 2 avoid 3
part 5 plant 2
exchange 4 share 2
state 2 carrier 2
care 5 complete 2
promise 3 maintain 3
estimate 2 development 4
rate 2 space 5
say 2 raise 3
remove 5 future 3
grant 4 network 3
remember 3 announce 5
cause 2 start 3
point 5 order 2
occur 4 defense 5
authority 3 set 3
regard 2 chance 2
go 3 produce 2
allow 4 negotiate 2
describe 2 enjoy 4
prove 3 exist 4
claim 4 replace 3
fix 2 examine 3
end 5 lead 3
receive 3 source 2
complain 3 report 2
need 2 believe 2
condition 2 contribute 3
182
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 46?54,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Exploiting Heterogeneous Treebanks for Parsing
Zheng-Yu Niu, Haifeng Wang, Hua Wu
Toshiba (China) Research and Development Center
5/F., Tower W2, Oriental Plaza, Beijing, 100738, China
{niuzhengyu,wanghaifeng,wuhua}@rdc.toshiba.com.cn
Abstract
We address the issue of using heteroge-
neous treebanks for parsing by breaking
it down into two sub-problems, convert-
ing grammar formalisms of the treebanks
to the same one, and parsing on these
homogeneous treebanks. First we pro-
pose to employ an iteratively trained tar-
get grammar parser to perform grammar
formalism conversion, eliminating prede-
fined heuristic rules as required in previ-
ous methods. Then we provide two strate-
gies to refine conversion results, and adopt
a corpus weighting technique for parsing
on homogeneous treebanks. Results on the
Penn Treebank show that our conversion
method achieves 42% error reduction over
the previous best result. Evaluation on
the Penn Chinese Treebank indicates that a
converted dependency treebank helps con-
stituency parsing and the use of unlabeled
data by self-training further increases pars-
ing f-score to 85.2%, resulting in 6% error
reduction over the previous best result.
1 Introduction
The last few decades have seen the emergence of
multiple treebanks annotated with different gram-
mar formalisms, motivated by the diversity of lan-
guages and linguistic theories, which is crucial to
the success of statistical parsing (Abeille et al,
2000; Brants et al, 1999; Bohmova et al, 2003;
Han et al, 2002; Kurohashi and Nagao, 1998;
Marcus et al, 1993; Moreno et al, 2003; Xue et
al., 2005). Availability of multiple treebanks cre-
ates a scenario where we have a treebank anno-
tated with one grammar formalism, and another
treebank annotated with another grammar formal-
ism that we are interested in. We call the first
a source treebank, and the second a target tree-
bank. We thus encounter a problem of how to
use these heterogeneous treebanks for target gram-
mar parsing. Here heterogeneous treebanks refer
to two or more treebanks with different grammar
formalisms, e.g., one treebank annotated with de-
pendency structure (DS) and the other annotated
with phrase structure (PS).
It is important to acquire additional labeled data
for the target grammar parsing through exploita-
tion of existing source treebanks since there is of-
ten a shortage of labeled data. However, to our
knowledge, there is no previous study on this is-
sue.
Recently there have been some works on us-
ing multiple treebanks for domain adaptation of
parsers, where these treebanks have the same
grammar formalism (McClosky et al, 2006b;
Roark and Bacchiani, 2003). Other related works
focus on converting one grammar formalism of a
treebank to another and then conducting studies on
the converted treebank (Collins et al, 1999; Forst,
2003; Wang et al, 1994; Watkinson and Manand-
har, 2001). These works were done either on mul-
tiple treebanks with the same grammar formalism
or on only one converted treebank. We see that
their scenarios are different from ours as we work
with multiple heterogeneous treebanks.
For the use of heterogeneous treebanks1, we
propose a two-step solution: (1) converting the
grammar formalism of the source treebank to the
target one, (2) refining converted trees and using
them as additional training data to build a target
grammar parser.
For grammar formalism conversion, we choose
the DS to PS direction for the convenience of the
comparison with existing works (Xia and Palmer,
2001; Xia et al, 2008). Specifically, we assume
that the source grammar formalism is dependency
1Here we assume the existence of two treebanks.
46
grammar, and the target grammar formalism is
phrase structure grammar.
Previous methods for DS to PS conversion
(Collins et al, 1999; Covington, 1994; Xia and
Palmer, 2001; Xia et al, 2008) often rely on pre-
defined heuristic rules to eliminate converison am-
biguity, e.g., minimal projection for dependents,
lowest attachment position for dependents, and the
selection of conversion rules that add fewer num-
ber of nodes to the converted tree. In addition, the
validity of these heuristic rules often depends on
their target grammars. To eliminate the heuristic
rules as required in previous methods, we propose
to use an existing target grammar parser (trained
on the target treebank) to generate N-best parses
for each sentence in the source treebank as conver-
sion candidates, and then select the parse consis-
tent with the structure of the source tree as the con-
verted tree. Furthermore, we attempt to use con-
verted trees as additional training data to retrain
the parser for better conversion candidates. The
procedure of tree conversion and parser retraining
will be run iteratively until a stopping condition is
satisfied.
Since some converted trees might be imper-
fect from the perspective of the target grammar,
we provide two strategies to refine conversion re-
sults: (1) pruning low-quality trees from the con-
verted treebank, (2) interpolating the scores from
the source grammar and the target grammar to se-
lect better converted trees. Finally we adopt a cor-
pus weighting technique to get an optimal combi-
nation of the converted treebank and the existing
target treebank for parser training.
We have evaluated our conversion algorithm on
a dependency structure treebank (produced from
the Penn Treebank) for comparison with previous
work (Xia et al, 2008). We also have investi-
gated our two-step solution on two existing tree-
banks, the Penn Chinese Treebank (CTB) (Xue et
al., 2005) and the Chinese Dependency Treebank
(CDT)2 (Liu et al, 2006). Evaluation on WSJ data
demonstrates that it is feasible to use a parser for
grammar formalism conversion and the conversion
benefits from converted trees used for parser re-
training. Our conversion method achieves 93.8%
f-score on dependency trees produced from WSJ
section 22, resulting in 42% error reduction over
the previous best result for DS to PS conversion.
Results on CTB show that score interpolation is
2Available at http://ir.hit.edu.cn/.
more effective than instance pruning for the use
of converted treebanks for parsing and converted
CDT helps parsing on CTB. When coupled with
self-training technique, a reranking parser with
CTB and converted CDT as labeled data achieves
85.2% f-score on CTB test set, an absolute 1.0%
improvement (6% error reduction) over the previ-
ous best result for Chinese parsing.
The rest of this paper is organized as follows. In
Section 2, we first describe a parser based method
for DS to PS conversion, and then we discuss pos-
sible strategies to refine conversion results, and
finally we adopt the corpus weighting technique
for parsing on homogeneous treebanks. Section
3 provides experimental results of grammar for-
malism conversion on a dependency treebank pro-
duced from the Penn Treebank. In Section 4, we
evaluate our two-step solution on two existing het-
erogeneous Chinese treebanks. Section 5 reviews
related work and Section 6 concludes this work.
2 Our Two-Step Solution
2.1 Grammar Formalism Conversion
Previous DS to PS conversion methods built a
converted tree by iteratively attaching nodes and
edges to the tree with the help of conversion
rules and heuristic rules, based on current head-
dependent pair from a source dependency tree and
the structure of the built tree (Collins et al, 1999;
Covington, 1994; Xia and Palmer, 2001; Xia et
al., 2008). Some observations can be made on
these methods: (1) for each head-dependent pair,
only one locally optimal conversion was kept dur-
ing tree-building process, at the risk of pruning
globally optimal conversions, (2) heuristic rules
are required to deal with the problem that one
head-dependent pair might have multiple conver-
sion candidates, and these heuristic rules are usu-
ally hand-crafted to reflect the structural prefer-
ence in their target grammars. To overcome these
limitations, we propose to employ a parser to gen-
erate N-best parses as conversion candidates and
then use the structural information of source trees
to select the best parse as a converted tree.
We formulate our conversion method as fol-
lows.
Let CDS be a source treebank annotated with
DS and CPS be a target treebank annotated with
PS. Our goal is to convert the grammar formalism
of CDS to that of CPS .
We first train a constituency parser on CPS
47
Input: CPS , CDS , Q, and a constituency parser Output: Converted trees CDSPS
1. Initialize:
? Set CDS,0PS as null, DevScore=0, q=0;
? Split CPS into training set CPS,train and development set CPS,dev;
? Train the parser on CPS,train and denote it by Pq?1;
2. Repeat:
? Use Pq?1 to generate N-best PS parses for each sentence in CDS , and convert PS to DS for each parse;
? For each sentence in CDS Do
? t?=argmaxtScore(xi,t), and select the t?-th parse as a converted tree for this sentence;
? Let CDS,qPS represent these converted trees, and let Ctrain=CPS,train
?CDS,qPS ;
? Train the parser on Ctrain, and denote the updated parser by Pq;
? Let DevScoreq be the f-score of Pq on CPS,dev;
? If DevScoreq > DevScore Then DevScore=DevScoreq, and CDSPS =CDS,qPS ;
? Else break;
? q++;
Until q > Q
Table 1: Our algorithm for DS to PS conversion.
(90% trees in CPS as training set CPS,train, and
other trees as development set CPS,dev) and then
let the parser generate N-best parses for each sen-
tence in CDS .
Let n be the number of sentences (or trees) in
CDS and ni be the number of N-best parses gen-
erated by the parser for the i-th (1 ? i ? n) sen-
tence in CDS . Let xi,t be the t-th (1 ? t ? ni)
parse for the i-th sentence. Let yi be the tree of the
i-th (1 ? i ? n) sentence in CDS .
To evaluate the quality of xi,t as a conversion
candidate for yi, we convert xi,t to a dependency
tree (denoted as xDSi,t ) and then use unlabeled de-
pendency f-score to measure the similarity be-
tween xDSi,t and yi. Let Score(xi,t) denote the
unlabeled dependency f-score of xDSi,t against yi.
Then we determine the converted tree for yi by
maximizing Score(xi,t) over the N-best parses.
The conversion from PS to DS works as fol-
lows:
Step 1. Use a head percolation table to find the
head of each constituent in xi,t.
Step 2. Make the head of each non-head child
depend on the head of the head child for each con-
stituent.
Unlabeled dependency f-score is a harmonic
mean of unlabeled dependency precision and unla-
beled dependency recall. Precision measures how
many head-dependent word pairs found in xDSi,t
are correct and recall is the percentage of head-
dependent word pairs defined in the gold-standard
tree that are found in xDSi,t . Here we do not take
dependency tags into consideration for evaluation
since they cannot be obtained without more so-
phisticated rules.
To improve the quality of N-best parses, we at-
tempt to use the converted trees as additional train-
ing data to retrain the parser. The procedure of
tree conversion and parser retraining can be run it-
eratively until a termination condition is satisfied.
Here we use the parser?s f-score on CPS,dev as a
termination criterion. If the update of training data
hurts the performance on CPS,dev, then we stop
the iteration.
Table 1 shows this DS to PS conversion algo-
rithm. Q is an upper limit of the number of loops,
and Q ? 0.
2.2 Target Grammar Parsing
Through grammar formalism conversion, we have
successfully turned the problem of using hetero-
geneous treebanks for parsing into the problem of
parsing on homogeneous treebanks. Before using
converted source treebank for parsing, we present
two strategies to refine conversion results.
Instance Pruning For some sentences in
CDS , the parser might fail to generate high qual-
ity N-best parses, resulting in inferior converted
trees. To clean the converted treebank, we can re-
move the converted trees with low unlabeled de-
pendency f-scores (defined in Section 2.1) before
using the converted treebank for parser training
48
Figure 1: A parse tree in CTB for a sentence of
/?.<world> ?<every> I<country> <
?<people> ?<all> r<with> 81<eyes>
? ?<cast> ? l<Hong Kong>0with
/People from all over the world are cast-
ing their eyes on Hong Kong0as its English
translation.
because these trees are/misleading0training in-
stances. The number of removed trees will be de-
termined by cross validation on development set.
Score Interpolation Unlabeled dependency
f-scores used in Section 2.1 measure the quality of
converted trees from the perspective of the source
grammar only. In extreme cases, the top best
parses in the N-best list are good conversion can-
didates but we might select a parse ranked quite
low in the N-best list since there might be con-
flicts of syntactic structure definition between the
source grammar and the target grammar.
Figure 1 shows an example for illustration of
a conflict between the grammar of CDT and
that of CTB. According to Chinese head percola-
tion tables used in the PS to DS conversion tool
/Penn2Malt03 and Charniak?s parser4, the head
of VP-2 is the word /r0(a preposition, with
/BA0as its POS tag in CTB), and the head of
IP-OBJ is ??0. Therefore the word /?
?0depends on the word/r0. But according
to the annotation scheme in CDT (Liu et al, 2006),
the word/r0is a dependent of the word/?
?0. The conflicts between the two grammars
may lead to the problem that the selected parses
based on the information of the source grammar
might not be preferred from the perspective of the
3Available at http://w3.msi.vxu.se/?nivre/.
4Available at http://www.cs.brown.edu/?ec/.
target grammar.
Therefore we modified the selection metric in
Section 2.1 by interpolating two scores, the prob-
ability of a conversion candidate from the parser
and its unlabeled dependency f-score, shown as
follows:
S?core(xi,t) = ??Prob(xi,t)+(1??)?Score(xi,t). (1)
The intuition behind this equation is that converted
trees should be preferred from the perspective of
both the source grammar and the target grammar.
Here 0 ? ? ? 1. Prob(xi,t) is a probability pro-
duced by the parser for xi,t (0 ? Prob(xi,t) ? 1).
The value of ? will be tuned by cross validation on
development set.
After grammar formalism conversion, the prob-
lem now we face has been limited to how to build
parsing models on multiple homogeneous tree-
bank. A possible solution is to simply concate-
nate the two treebanks as training data. However
this method may lead to a problem that if the size
of CPS is significantly less than that of converted
CDS , converted CDS may weaken the effect CPS
might have. One possible solution is to reduce the
weight of examples from converted CDS in parser
training. Corpus weighting is exactly such an ap-
proach, with the weight tuned on development set,
that will be used for parsing on homogeneous tree-
banks in this paper.
3 Experiments of Grammar Formalism
Conversion
3.1 Evaluation on WSJ section 22
Xia et al (2008) used WSJ section 19 from the
Penn Treebank to extract DS to PS conversion
rules and then produced dependency trees from
WSJ section 22 for evaluation of their DS to PS
conversion algorithm. They showed that their
conversion algorithm outperformed existing meth-
ods on the WSJ data. For comparison with their
work, we conducted experiments in the same set-
ting as theirs: using WSJ section 19 (1844 sen-
tences) as CPS , producing dependency trees from
WSJ section 22 (1700 sentences) as CDS5, and
using labeled bracketing f-scores from the tool
/EVALB0on WSJ section 22 for performance
evaluation.
5We used the tool/Penn2Malt0to produce dependency
structures from the Penn Treebank, which was also used for
PS to DS conversion in our conversion algorithm.
49
All the sentences
DevScore LR LP F
Models (%) (%) (%) (%)
The best result of
Xia et al (2008) - 90.7 88.1 89.4
Q-0-method 86.8 92.2 92.8 92.5
Q-10-method 88.0 93.4 94.1 93.8
Table 2: Comparison with the work of Xia et al
(2008) on WSJ section 22.
All the sentences
DevScore LR LP F
Models (%) (%) (%) (%)
Q-0-method 91.0 91.6 92.5 92.1
Q-10-method 91.6 93.1 94.1 93.6
Table 3: Results of our algorithm on WSJ section
2?18 and 20?22.
We employed Charniak?s maximum entropy in-
spired parser (Charniak, 2000) to generate N-best
(N=200) parses. Xia et al (2008) used POS
tag information, dependency structures and depen-
dency tags in test set for conversion. Similarly, we
used POS tag information in the test set to restrict
search space of the parser for generation of better
N-best parses.
We evaluated two variants of our DS to PS con-
version algorithm:
Q-0-method: We set the value of Q as 0 for a
baseline method.
Q-10-method: We set the value of Q as 10 to
see whether it is helpful for conversion to retrain
the parser on converted trees.
Table 2 shows the results of our conversion al-
gorithm on WSJ section 22. In the experiment
of Q-10-method, DevScore reached the highest
value of 88.0% when q was 1. Then we used
CDS,1PS as the conversion result. Finally Q-10-
method achieved an f-score of 93.8% on WSJ sec-
tion 22, an absolute 4.4% improvement (42% er-
ror reduction) over the best result of Xia et al
(2008). Moreover, Q-10-method outperformed Q-
0-method on the same test set. These results indi-
cate that it is feasible to use a parser for DS to PS
conversion and the conversion benefits from the
use of converted trees for parser retraining.
3.2 Evaluation on WSJ section 2?18 and
20?22
In this experiment we evaluated our conversion al-
gorithm on a larger test set, WSJ section 2?18 and
20?22 (totally 39688 sentences). Here we also
used WSJ section 19 as CPS . Other settings for
All the sentences
LR LP F
Training data (%) (%) (%)
1? CTB + CDTPS 84.7 85.1 84.9
2? CTB + CDTPS 85.1 85.6 85.3
5? CTB + CDTPS 85.0 85.5 85.3
10? CTB + CDTPS 85.3 85.8 85.6
20? CTB + CDTPS 85.1 85.3 85.2
50? CTB + CDTPS 84.9 85.3 85.1
Table 4: Results of the generative parser on the de-
velopment set, when trained with various weight-
ing of CTB training set and CDTPS .
this experiment are as same as that in Section 3.1,
except that here we used a larger test set.
Table 3 provides the f-scores of our method with
Q equal to 0 or 10 on WSJ section 2?18 and
20?22.
With Q-10-method, DevScore reached the high-
est value of 91.6% when q was 1. Finally Q-
10-method achieved an f-score of 93.6% on WSJ
section 2?18 and 20?22, better than that of Q-0-
method and comparable with that of Q-10-method
in Section 3.1. It confirms our previous finding
that the conversion benefits from the use of con-
verted trees for parser retraining.
4 Experiments of Parsing
We investigated our two-step solution on two ex-
isting treebanks, CDT and CTB, and we used CDT
as the source treebank and CTB as the target tree-
bank.
CDT consists of 60k Chinese sentences, anno-
tated with POS tag information and dependency
structure information (including 28 POS tags, and
24 dependency tags) (Liu et al, 2006). We did not
use POS tag information as inputs to the parser in
our conversion method due to the difficulty of con-
version from CDT POS tags to CTB POS tags.
We used a standard split of CTB for perfor-
mance evaluation, articles 1-270 and 400-1151 as
training set, articles 301-325 as development set,
and articles 271-300 as test set.
We used Charniak?s maximum entropy inspired
parser and their reranker (Charniak and Johnson,
2005) for target grammar parsing, called a gener-
ative parser (GP) and a reranking parser (RP) re-
spectively. We reported ParseVal measures from
the EVALB tool.
50
All the sentences
LR LP F
Models Training data (%) (%) (%)
GP CTB 79.9 82.2 81.0
RP CTB 82.0 84.6 83.3
GP 10? CTB + CDTPS 80.4 82.7 81.5
RP 10? CTB + CDTPS 82.8 84.7 83.8
Table 5: Results of the generative parser (GP) and
the reranking parser (RP) on the test set, when
trained on only CTB training set or an optimal
combination of CTB training set and CDTPS .
4.1 Results of a Baseline Method to Use CDT
We used our conversion algorithm6 to convert the
grammar formalism of CDT to that of CTB. Let
CDTPS denote the converted CDT by our method.
The average unlabeled dependency f-score of trees
in CDTPS was 74.4%, and their average index in
200-best list was 48.
We tried the corpus weighting method when
combining CDTPS with CTB training set (abbre-
viated as CTB for simplicity) as training data, by
gradually increasing the weight (including 1, 2, 5,
10, 20, 50) of CTB to optimize parsing perfor-
mance on the development set. Table 4 presents
the results of the generative parser with various
weights of CTB on the development set. Consid-
ering the performance on the development set, we
decided to give CTB a relative weight of 10.
Finally we evaluated two parsing models, the
generative parser and the reranking parser, on the
test set, with results shown in Table 5. When
trained on CTB only, the generative parser and the
reranking parser achieved f-scores of 81.0% and
83.3%. The use of CDTPS as additional training
data increased f-scores of the two models to 81.5%
and 83.8%.
4.2 Results of Two Strategies for a Better Use
of CDT
4.2.1 Instance Pruning
We used unlabeled dependency f-score of each
converted tree as the criterion to rank trees in
CDTPS and then kept only the top M trees
with high f-scores as training data for pars-
ing, resulting in a corpus CDTPSM . M var-
ied from 100%?|CDTPS | to 10%?|CDTPS |
with 10%?|CDTPS | as the interval. |CDTPS |
6The setting for our conversion algorithm in this experi-
ment was as same as that in Section 3.1. In addition, we used
CTB training set as CPS,train, and CTB development set as
CPS,dev .
All the sentences
LR LP F
Models Training data (%) (%) (%)
GP CTB + CDTPS? 81.4 82.8 82.1
RP CTB + CDTPS? 83.0 85.4 84.2
Table 6: Results of the generative parser and the
reranking parser on the test set, when trained on
an optimal combination of CTB training set and
converted CDT.
is the number of trees in CDTPS . Then
we tuned the value of M by optimizing the
parser?s performance on the development set with
10?CTB+CDTPSM as training data. Finally the op-
timal value of M was 100%?|CDT|. It indicates
that even removing very few converted trees hurts
the parsing performance. A possible reason is that
most of non-perfect parses can provide useful syn-
tactic structure information for building parsing
models.
4.2.2 Score Interpolation
We used ?Score(xi,t)7 to replace Score(xi,t) in
our conversion algorithm and then ran the updated
algorithm on CDT. Let CDTPS? denote the con-
verted CDT by this updated conversion algorithm.
The values of ? (varying from 0.0 to 1.0 with 0.1
as the interval) and the CTB weight (including 1,
2, 5, 10, 20, 50) were simultaneously tuned on the
development set8. Finally we decided that the op-
timal value of ? was 0.4 and the optimal weight of
CTB was 1, which brought the best performance
on the development set (an f-score of 86.1%). In
comparison with the results in Section 4.1, the
average index of converted trees in 200-best list
increased to 2, and their average unlabeled depen-
dency f-score dropped to 65.4%. It indicates that
structures of converted trees become more consis-
tent with the target grammar, as indicated by the
increase of average index of converted trees, fur-
ther away from the source grammar.
Table 6 provides f-scores of the generative
parser and the reranker on the test set, when
trained on CTB and CDTPS? . We see that the
performance of the reranking parser increased to
7Before calculating S?core(xi,t), we normal-
ized the values of Prob(xi,t) for each N-best list
by (1) Prob(xi,t)=Prob(xi,t)-Min(Prob(xi,?)),
(2)Prob(xi,t)=Prob(xi,t)/Max(Prob(xi,?)), resulting
in that their maximum value was 1 and their minimum value
was 0.
8Due to space constraint, we do not show f-scores of the
parser with different values of ? and the CTB weight.
51
All the sentences
LR LP F
Models Training data (%) (%) (%)
Self-trained GP 10?T+10?D+P 83.0 84.5 83.7
Updated RP CTB+CDTPS? 84.3 86.1 85.2
Table 7: Results of the self-trained gen-
erative parser and updated reranking parser
on the test set. 10?T+10?D+P stands for
10?CTB+10?CDTPS? +PDC.
84.2% f-score, better than the result of the rerank-
ing parser with CTB and CDTPS as training data
(shown in Table 5). It indicates that the use of
probability information from the parser for tree
conversion helps target grammar parsing.
4.3 Using Unlabeled Data for Parsing
Recent studies on parsing indicate that the use of
unlabeled data by self-training can help parsing
on the WSJ data, even when labeled data is rel-
atively large (McClosky et al, 2006a; Reichart
and Rappoport, 2007). It motivates us to em-
ploy self-training technique for Chinese parsing.
We used the POS tagged People Daily corpus9
(Jan. 1998?Jun. 1998, and Jan. 2000?Dec.
2000) (PDC) as unlabeled data for parsing. First
we removed the sentences with less than 3 words
or more than 40 words from PDC to ease pars-
ing, resulting in 820k sentences. Then we ran the
reranking parser in Section 4.2.2 on PDC and used
the parses on PDC as additional training data for
the generative parser. Here we tried the corpus
weighting technique for an optimal combination
of CTB, CDTPS? and parsed PDC, and chose the
relative weight of both CTB and CDTPS? as 10
by cross validation on the development set. Fi-
nally we retrained the generative parser on CTB,
CDTPS? and parsed PDC. Furthermore, we used
this self-trained generative parser as a base parser
to retrain the reranker on CTB and CDTPS? .
Table 7 shows the performance of self-trained
generative parser and updated reranker on the test
set, with CTB and CDTPS? as labeled data. We see
that the use of unlabeled data by self-training fur-
ther increased the reranking parser?s performance
from 84.2% to 85.2%. Our results on Chinese data
confirm previous findings on English data shown
in (McClosky et al, 2006a; Reichart and Rap-
poport, 2007).
9Available at http://icl.pku.edu.cn/.
4.4 Comparison with Previous Studies for
Chinese Parsing
Table 8 and 9 present the results of previous stud-
ies on CTB. All the works in Table 8 used CTB
articles 1-270 as labeled data. In Table 9, Petrov
and Klein (2007) trained their model on CTB ar-
ticles 1-270 and 400-1151, and Burkett and Klein
(2008) used the same CTB articles and parse trees
of their English translation (from the English Chi-
nese Translation Treebank) as training data. Com-
paring our result in Table 6 with that of Petrov
and Klein (2007), we see that CDTPS? helps pars-
ing on CTB, which brought 0.9% f-score improve-
ment. Moreover, the use of unlabeled data further
boosted the parsing performance to 85.2%, an ab-
solute 1.0% improvement over the previous best
result presented in Burkett and Klein (2008).
5 Related Work
Recently there have been some studies address-
ing how to use treebanks with same grammar for-
malism for domain adaptation of parsers. Roark
and Bachiani (2003) presented count merging and
model interpolation techniques for domain adap-
tation of parsers. They showed that their sys-
tem with count merging achieved a higher perfor-
mance when in-domain data was weighted more
heavily than out-of-domain data. McClosky et al
(2006b) used self-training and corpus weighting to
adapt their parser trained on WSJ corpus to Brown
corpus. Their results indicated that both unla-
beled in-domain data and labeled out-of-domain
data can help domain adaptation. In comparison
with these works, we conduct our study in a dif-
ferent setting where we work with multiple het-
erogeneous treebanks.
Grammar formalism conversion makes it possi-
ble to reuse existing source treebanks for the study
of target grammar parsing. Wang et al (1994)
employed a parser to help conversion of a tree-
bank from a simple phrase structure to a more in-
formative phrase structure and then used this con-
verted treebank to train their parser. Collins et al
(1999) performed statistical constituency parsing
of Czech on a treebank that was converted from
the Prague Dependency Treebank under the guid-
ance of conversion rules and heuristic rules, e.g.,
one level of projection for any category, minimal
projection for any dependents, and fixed position
of attachment. Xia and Palmer (2001) adopted bet-
ter heuristic rules to build converted trees, which
52
? 40 words All the sentences
LR LP F LR LP F
Models (%) (%) (%) (%) (%) (%)
Bikel & Chiang (2000) 76.8 77.8 77.3 - - -
Chiang & Bikel (2002) 78.8 81.1 79.9 - - -
Levy & Manning (2003) 79.2 78.4 78.8 - - -
Bikel?s thesis (2004) 78.0 81.2 79.6 - - -
Xiong et. al. (2005) 78.7 80.1 79.4 - - -
Chen et. al. (2005) 81.0 81.7 81.2 76.3 79.2 77.7
Wang et. al. (2006) 79.2 81.1 80.1 76.2 78.0 77.1
Table 8: Results of previous studies on CTB with CTB articles 1-270 as labeled data.
? 40 words All the sentences
LR LP F LR LP F
Models (%) (%) (%) (%) (%) (%)
Petrov & Klein (2007) 85.7 86.9 86.3 81.9 84.8 83.3
Burkett & Klein (2008) - - - - - 84.2
Table 9: Results of previous studies on CTB with more labeled data.
reflected the structural preference in their target
grammar. For acquisition of better conversion
rules, Xia et al (2008) proposed to automati-
cally extract conversion rules from a target tree-
bank. Moreover, they presented two strategies to
solve the problem that there might be multiple
conversion rules matching the same input depen-
dency tree pattern: (1) choosing the most frequent
rules, (2) preferring rules that add fewer number
of nodes and attach the subtree lower.
In comparison with the works of Wang et al
(1994) and Collins et al (1999), we went fur-
ther by combining the converted treebank with the
existing target treebank for parsing. In compar-
ison with previous conversion methods (Collins
et al, 1999; Covington, 1994; Xia and Palmer,
2001; Xia et al, 2008) in which for each head-
dependent pair, only one locally optimal conver-
sion was kept during tree-building process, we
employed a parser to generate globally optimal
syntactic structures, eliminating heuristic rules for
conversion. In addition, we used converted trees to
retrain the parser for better conversion candidates,
while Wang et al (1994) did not exploit the use of
converted trees for parser retraining.
6 Conclusion
We have proposed a two-step solution to deal with
the issue of using heterogeneous treebanks for
parsing. First we present a parser based method
to convert grammar formalisms of the treebanks to
the same one, without applying predefined heuris-
tic rules, thus turning the original problem into the
problem of parsing on homogeneous treebanks.
Then we present two strategies, instance pruning
and score interpolation, to refine conversion re-
sults. Finally we adopt the corpus weighting tech-
nique to combine the converted source treebank
with the existing target treebank for parser train-
ing.
The study on the WSJ data shows the benefits of
our parser based approach for grammar formalism
conversion. Moreover, experimental results on the
Penn Chinese Treebank indicate that a converted
dependency treebank helps constituency parsing,
and it is better to exploit probability information
produced by the parser through score interpolation
than to prune low quality trees for the use of the
converted treebank.
Future work includes further investigation of
our conversion method for other pairs of grammar
formalisms, e.g., from the grammar formalism of
the Penn Treebank to more deep linguistic formal-
ism like CCG, HPSG, or LFG.
References
Anne Abeille, Lionel Clement and Francois Toussenel. 2000.
Building a Treebank for French. In Proceedings of LREC
2000, pages 87-94.
Daniel Bikel and David Chiang. 2000. Two Statistical Pars-
ing Models Applied to the Chinese Treebank. In Proceed-
ings of the Second SIGHAN workshop, pages 1-6.
Daniel Bikel. 2004. On the Parameter Space of Generative
Lexicalized Statistical Parsing Models. Ph.D. thesis, Uni-
versity of Pennsylvania.
Alena Bohmova, Jan Hajic, Eva Hajicova and Barbora
Vidova-Hladka. 2003. The Prague Dependency Tree-
bank: A Three-Level Annotation Scenario. Treebanks:
53
Building and Using Annotated Corpora. Kluwer Aca-
demic Publishers, pages 103-127.
Thorsten Brants, Wojciech Skut and Hans Uszkoreit. 1999.
Syntactic Annotation of a German Newspaper Corpus. In
Proceedings of the ATALA Treebank Workshop, pages 69-
76.
David Burkett and Dan Klein. 2008. Two Languages are
Better than One (for Syntactic Parsing). In Proceedings of
EMNLP 2008, pages 877-886.
Eugene Charniak. 2000. A Maximum Entropy Inspired
Parser. In Proceedings of NAACL 2000, pages 132-139.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine
N-Best Parsing and MaxEnt Discriminative Reranking. In
Proceedings of ACL 2005, pages 173-180.
Ying Chen, Hongling Sun and Dan Jurafsky. 2005. A Cor-
rigendum to Sun and Jurafsky (2004) Shallow Semantic
Parsing of Chinese. University of Colorado at Boulder
CSLR Tech Report TR-CSLR-2005-01.
David Chiang and Daniel M. Bikel. 2002. Recovering La-
tent Information in Treebanks. In Proceedings of COL-
ING 2002, pages 1-7.
Micheal Collins, Lance Ramshaw, Jan Hajic and Christoph
Tillmann. 1999. A Statistical Parser for Czech. In Pro-
ceedings of ACL 1999, pages 505-512.
Micheal Covington. 1994. GB Theory as Dependency
Grammar. Research Report AI-1992-03.
Martin Forst. 2003. Treebank Conversion - Establishing
a Testsuite for a Broad-Coverage LFG from the TIGER
Treebank. In Proceedings of LINC at EACL 2003, pages
25-32.
Chunghye Han, Narae Han, Eonsuk Ko and Martha Palmer.
2002. Development and Evaluation of a Korean Treebank
and its Application to NLP. In Proceedings of LREC 2002,
pages 1635-1642.
Sadao Kurohashi and Makato Nagao. 1998. Building a
Japanese Parsed Corpus While Improving the Parsing Sys-
tem. In Proceedings of LREC 1998, pages 719-724.
Roger Levy and Christopher Manning. 2003. Is It Harder to
Parse Chinese, or the Chinese Treebank? In Proceedings
of ACL 2003, pages 439-446.
Ting Liu, Jinshan Ma and Sheng Li. 2006. Building a Depen-
dency Treebank for Improving Chinese Parser. Journal of
Chinese Language and Computing, 16(4):207-224.
Mitchell P. Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated Cor-
pus of English: The Penn Treebank. Computational Lin-
guistics, 19(2):313-330.
David McClosky, Eugene Charniak and Mark Johnson.
2006a. Effective Self-Training for Parsing. In Proceed-
ings of NAACL 2006, pages 152-159.
David McClosky, Eugene Charniak and Mark Johnson.
2006b. Reranking and Self-Training for Parser Adapta-
tion. In Proceedings of COLING/ACL 2006, pages 337-
344.
Antonio Moreno, Susana Lopez, Fernando Sanchez and
Ralph Grishman. 2003. Developing a Syntactic Anno-
tation Scheme and Tools for a Spanish Treebank. Tree-
banks: Building and Using Annotated Corpora. Kluwer
Academic Publishers, pages 149-163.
Slav Petrov and Dan Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of HLT/NAACL
2007, pages 404-411.
Roi Reichart and Ari Rappoport. 2007. Self-Training for En-
hancement and Domain Adaptation of Statistical Parsers
Trained on Small Datasets. In Proceedings of ACL 2007,
pages 616-623.
Brian Roark and Michiel Bacchiani. 2003. Supervised and
Unsupervised PCFG Adaptation to Novel Domains. In
Proceedings of HLT/NAACL 2003, pages 126-133.
Jong-Nae Wang, Jing-Shin Chang and Keh-Yih Su. 1994.
An Automatic Treebank Conversion Algorithm for Corpus
Sharing. In Proceedings of ACL 1994, pages 248-254.
Mengqiu Wang, Kenji Sagae and Teruko Mitamura. 2006. A
Fast, Accurate Deterministic Parser for Chinese. In Pro-
ceedings of COLING/ACL 2006, pages 425-432.
Stephen Watkinson and Suresh Manandhar. 2001. Translat-
ing Treebank Annotation for Evaluation. In Proceedings
of ACL Workshop on Evaluation Methodologies for Lan-
guage and Dialogue Systems, pages 1-8.
Fei Xia and Martha Palmer. 2001. Converting Dependency
Structures to Phrase Structures. In Proceedings of HLT
2001, pages 1-5.
Fei Xia, Rajesh Bhatt, Owen Rambow, Martha Palmer
and Dipti Misra. Sharma. 2008. Towards a Multi-
Representational Treebank. In Proceedings of the 7th In-
ternational Workshop on Treebanks and Linguistic Theo-
ries, pages 159-170.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin and
Yueliang Qian. 2005. Parsing the Penn Chinese Tree-
bank with Semantic Knowledge. In Proceedings of IJC-
NLP 2005, pages 70-81.
Nianwen Xue, Fei Xia, Fu-Dong Chiou and Martha Palmer.
2005. The Penn Chinese TreeBank: Phrase Structure An-
notation of a Large Corpus. Natural Language Engineer-
ing, 11(2):207-238.
54
Coling 2010: Poster Volume, pages 1507?1514,
Beijing, August 2010
Predicting Discourse Connectives for Implicit Discourse Relation
Recognition
Zhi-Min Zhou and Yu Xu
East China Normal University
51091201052@ecnu.cn
Zheng-Yu Niu
Toshiba China R&D Center
zhengyu.niu@gmail.com
Man Lan and Jian Su
Institute for Infocomm Research
sujian@i2r.a-star.edu.sg
Chew Lim Tan
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Existing works indicate that the absence
of explicit discourse connectives makes
it difficult to recognize implicit discourse
relations. In this paper we attempt to
overcome this difficulty for implicit rela-
tion recognition by automatically insert-
ing discourse connectives between argu-
ments with the use of a language model.
Then we propose two algorithms to lever-
age the information of these predicted
connectives. One is to use these pre-
dicted implicit connectives as additional
features in a supervised model. The other
is to perform implicit relation recognition
based only on these predicted connectives.
Results on Penn Discourse Treebank 2.0
show that predicted discourse connectives
help implicit relation recognition and the
first algorithm can achieve an absolute av-
erage f-score improvement of 3% over a
state of the art baseline system.
1 Introduction
Discourse relation analysis is to automatically
identify discourse relations (e.g., explanation re-
lation) that hold between arbitrary spans of text.
This analysis may be a part of many natural lan-
guage processing systems, e.g., text summariza-
tion system, question answering system. If there
are discourse connectives between textual units
to explicitly mark their relations, the recognition
task on these texts is defined as explicit discourse
relation recognition. Otherwise it is defined as im-
plicit discourse relation recognition.
Previous study indicates that the presence of
discourse connectives between textual units can
greatly help relation recognition. In Penn Dis-
course Treebank (PDTB) corpus (Prasad et al,
2008), the most general senses, i.e., Comparison
(Comp.), Contingency (Cont.), Temporal (Temp.)
and Expansion (Exp.), can be disambiguated in
explicit relations with more than 90% f-scores
based only on the discourse connectives explicitly
used to signal the relation (Pitler and Nenkova.,
2009b).
However, for implicit relations, there are no
connectives to explicitly mark the relations, which
makes the recognition task quite difficult. Some of
existing works attempt to perform relation recog-
nition without hand-annotated corpora (Marcu
and Echihabi, 2002), (Sporleder and Lascarides,
2008) and (Blair-Goldensohn, 2007). They use
unambiguous patterns such as [Arg1, but Arg2]
to create synthetic examples of implicit relations
and then use [Arg1, Arg2] as an training example
of an implicit relation. Another research line is
to exploit various linguistically informed features
under the framework of supervised models, (Pitler
et al, 2009a) and (Lin et al, 2009), e.g., polarity
features, semantic classes, tense, production rules
of parse trees of arguments, etc.
Our study on PDTB test data shows that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we simply mapped the ground
truth implicit connective of each test instance to
its most frequent sense. It indicates the impor-
tance of connective information for implicit rela-
tion recognition. However, so far there is no previ-
ous study attempting to use such kind of connec-
tive information for implicit relation. One possi-
1507
ble reason is that implicit connectives do not ex-
ist in unannotated real texts. Another evidence
of the importance of connectives for implicit re-
lations is shown in PDTB annotation. The PDTB
annotation consists of inserting a connective ex-
pression that best conveys the inferred relation by
the readers. Connectives inserted in this way to
express inferred relations are called implicit con-
nectives, which do not exist in real texts. These
evidences inspire us to consider two interesting re-
search questions:
(1) Can we automatically predict implicit connec-
tives between arguments?
(2) How to use the predicted implicit connectives
to build an automatic discourse relation analysis
system?
In this paper we address these two questions as
follows: (1) We insert appropriate discourse con-
nectives between two textual units with the use of
a language model. Here we train the language
model on large amount of raw corpora without
the use of any hand-annotated data. (2) Then we
present two algorithms to use these predicted con-
nectives for implicit relation recognition. One is
to use these connectives as additional features in a
supervised model. The other is to perform relation
recognition based only on these connectives.
We performed evaluation of the two algorithms
and a baseline system on PDTB 2.0 corpus. Ex-
perimental results showed that using predicted
discourse connectives as additional features can
significantly improve the performance of implicit
discourse relation recognition. Specifically, the
first algorithm achieved an absolute average f-
score improvement of 3% over a state of the art
baseline system.
The rest of this paper is organized as follows.
Section 2 describes the two algorithms for implicit
discourse relation recognition. Section 3 presents
experiments and results on PDTB data. Section
4 reviews related work. Section 5 concludes this
work.
2 Our Algorithms for Implicit Discourse
Relation Recognition
2.1 Prediction of implicit connectives
Explicit discourse relations are easily identifiable
due to the presence of discourse connectives be-
tween arguments. (Pitler and Nenkova., 2009b)
showed that in PDTB corpus, the most general
senses, i.e., Comparison (Comp.), Contingency
(Cont.), Temporal (Temp.) and Expansion (Exp.),
can be disambiguated in explicit relations with
more than 90% f-scores based only on discourse
connectives.
But for implicit relations, there are no connec-
tives to explicitly mark the relations, which makes
the recognition task quite difficult. PDTB data
provides implicit connectives that are inserted be-
tween paragraph-internal adjacent sentence pairs
not marked by any of explicit connectives. The
availability of ground-truth implicit connectives
makes it possible to evaluate the contribution of
these connectives for implicit relation recognition.
Our initial study on PDTB data show that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we obtained the sense of each
test example by mapping each ground truth im-
plicit connective to its most frequent sense. We
see that connective information is an important
knowledge source for implicit relation recogni-
tion. However these implicit connectives do not
exist in real texts. In this paper we overcome this
difficulty by inserting a connective between two
arguments with the use of a language model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two ar-
guments, denoted as Arg1 and Arg2. Typically,
there are two possible positions for most of im-
plicit connectives1, i.e., the position before Arg1
and the position between Arg1 and Arg2. Given a
set of possible implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as PPL(Sci,j). According
1For parallel connectives, e.g., if . . . then. . . , the two con-
nectives will take the two arguments together, so there is only
one possible combination for connectives and arguments.
1508
to the value of PPL(Sci,j) (the lower the better),
we can rank these sentences and select the con-
nectives in top N sentences as implicit connec-
tives for this argument pair. The language model
may be trained on large amount of unannotated
corpora that can be cheaply acquired, e.g., North
American News corpus.
2.2 Using predicted implicit connectives as
additional features
We predict implicit connectives on both training
set and test set. Then we can use the predicted
implicit connectives as additional features for su-
pervised implicit relation recognition. Previous
works exploited various linguistically informed
features under the framework of supervised mod-
els. In this paper, we include 9 types of features
in our system due to their superior performance
in previous studies, e.g., polarity features, seman-
tic classes of verbs, contextual sense, modality,
inquirer tags of words, first-last words of argu-
ments, cross-argument word pairs, ever used in
(Pitler et al, 2009a), production rules of parse
trees of arguments used in (Lin et al, 2009), and
intra-argument word pairs inspired by the work of
(Saito et al, 2006).
Here we provide the details of the 9 features,
shown as follows:
Verbs: Similar to the work in (Pitler et al,
2009a), the verb features consist of the number of
pairs of verbs in Arg1 and Arg2 if they are from
the same class based on their highest Levin verb
class level (Dorr, 2001). In addition, the average
length of verb phrase and the part of speech tags
of main verb are also included as verb features.
Context: If the immediately preceding (or fol-
lowing) relation is an explicit, its relation and
sense are used as features. Moreover, we use an-
other feature to indicate if Arg1 leads a paragraph.
Polarity: We use the number of positive,
negated positive, negative and neutral words in ar-
guments and their cross product as features. For
negated positives, we locate the negated words in
text span and then define the closely behind posi-
tive word as negated positive.
Modality: We look for modal words including
their various tenses or abbreviation forms in both
arguments. Then we generate a feature to indicate
the presence or absence of modal words in both
arguments and their cross product.
Inquirer Tags: Inquirer Tags extracted from
General Inquirer lexicon (Stone et al, 1966) con-
tains positive or negative classification of words.
In fact, its fine-grained categories, such as Fall
versus Rise, or Pleasure versus Pain, can indi-
cate the relation between two words, especially
for verbs. So we choose the presence or absence
of 21 pair categories with complementary relation
in Inquirer Tags as features. We also include their
cross production as features.
FirstLastFirst3: We choose the first and last
words of each argument as features, as well as the
pair of first words, the pair of last words, and the
first 3 words in each argument. In addition, we ap-
ply Porter?s Stemmer (Porter, 1980) to each word
before preparation of these features.
Production Rule: According to (Lin et al,
2009), we extract all the possible production rules
from arguments, and check whether the rules ap-
pear in Arg1, Arg2 and both arguments. We re-
move the rules occurring less than 5 times in train-
ing data.
Cross-argument Word Pairs: We perform the
Porter?s stemming (Porter, 1980), and then group
all words from Arg1 and Arg2 into two sets W1
and W2 respectively. Then we generate any possi-
ble word pair (wi, wj) (wi ? W1, wj ? W2). We
remove the word pairs with less than 5 times.
Intra-argument Word Pairs: Let
Q1 = (q1, q2, . . . , qn) be the word se-
quence of Arg1. The intra-argument word
pairs for Arg1 is defined as WP1 =
((q1, q2), (q1, q3), . . . , (q1, qn), (q2, q3), . . . ,
(qn?1, qn)). We extract all the intra-argument
word pairs from Arg1 and Arg2 and remove word
pairs appearing less than 5 times in training data.
2.3 Relation recognition based only on
predicted implicit connectives
After the prediction of implicit connectives, we
can address the implicit relation recognition task
with the methods for explicit relation recogni-
tion due to the presence of implicit connectives,
e.g., sense classification based only on connec-
tives (Pitler and Nenkova., 2009b). The work of
(Pitler and Nenkova., 2009b) showed that most
1509
of connectives are unambiguous and it is possible
to obtain high performance in prediction of dis-
course sense due to the simple mapping relation
between connectives and senses. Given two ex-
amples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey Comparison and Contingency sense
respectively. In most cases, we can easily recog-
nize the relation sense by the appearance of dis-
course connective since it can be interpreted in
only one way. That means, the ambiguity of the
mapping between sense and connective is quite
few.
We count the frequency of sense tags for each
possible connective on PDTB training data for im-
plicit relation. Then we build a sense recognition
model by simply mapping each connective to its
most frequent sense. Here we do not perform con-
nective prediction on training data. During test-
ing, we use the language model to insert implicit
connectives into each test argument pair. Then we
perform relation recognition by mapping each im-
plicit connective to its most frequent sense.
3 Experiments and Results
3.1 Experiments
3.1.1 Data sets
In this work we used the PDTB 2.0 corpus for
evaluation of our algorithms. Following the work
of (Pitler et al, 2009a), we used sections 2-20 as
training set, sections 21-22 as test set, and sec-
tions 0-1 as development set for parameter opti-
mization. For comparison with the work of (Pitler
et al, 2009a), we ran four binary classification
tasks to identify each of the main relations (Cont.,
Comp., Exp., and Temp.) from the rest. For each
relation, we used equal numbers of positive and
negative examples as training data2. The negative
examples were chosen at random from sections 2-
20. We used all the instances in sections 21 and
22 as test set, so the test set is representative of
2Here the numbers of training and test instances for Ex-
pansion relation are different from those in (Pitler et al,
2009a). The reason is that we do not include instances of
EntRel as positive examples.
the natural distribution. The numbers of positive
and negative instances for each sense in different
data sets are listed in Table 1.
Table 1: Statistics of positive and negative sam-
ples in training, development and test sets for each
relation.
Relation Train Dev Test
Pos/Neg Pos/Neg Pos/Neg
Comp. 1927/1927 191/997 146/912
Cont. 3375/3375 292/896 276/782
Exp. 6052/6052 651/537 556/502
Temp. 730/730 54/1134 67/991
In this work we used LibSVM toolkit to con-
struct four linear SVM models for a baseline sys-
tem and the system in Section 2.2.
3.1.2 A baseline system
We first built a baseline system, which used 9
types of features listed in Section 2.2.
We tuned the numbers of firstLastFirst3, cross-
argument word pair, intra-argument word pair on
development set. Finally we set the frequency
threshold at 3, 5 and 5 respectively.
3.1.3 Prediction of implicit connectives
To predict implicit connectives, we adopt the
following two steps:(1) train a language model;
(2) select top N implicit connectives.
Step 1: We used SRILM toolkit to train the lan-
guage models on three benchmark news corpora,
i.e., New York part in the BLLIP North Ameri-
can News, Xin and Ltw parts of English Gigaword
(4th Edition). We also tried different values for
n in n-gram model. The parameters were tuned
on the development set to optimize the accuracy
of prediction. In this work we chose 3-gram lan-
guage model trained on NY corpus.
Step 2: We combined each instance?s Arg1 and
Arg2 with connectives extract from PDTB2 (100
in all). There are two types of connectives, sin-
gle connective (e.g. because and but) and paral-
lel connective (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of pos-
sible implicit connectives {ci}, for single connec-
tive {ci}, we constructed two synthetic sentences,
ci+Arg1+Arg2 and Arg1+ci+Arg2. In case of
1510
parallel connective, we constructed one synthetic
sentence like ci1+Arg1+ci2+Arg2.
As a result, we can get 198 synthetic sentences
for each argument pair. Then we converted all
words to lower cases and used the language model
trained in the above step to calculate perplexity
on sentence level. The perplexity scores were
ranked from low to high. For example, we got the
perplexity (ppl) for two sentences as follows:
(1) but this is an old story, we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 652.837
(2) this is an old story, but we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 583.514
We considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is, the
presence and absence of the specific connective.
According to the value of PPL(Sci,j) (the
lower the better), we selected the connectives in
top N sentences as implicit connectives for this
argument pair. In order to get the optimal N value,
we tried various values of N on development set
and selected the minimum value of N so that the
ground-truth connectives appeared in top N con-
nectives. The final N value is set to 60 based on
the trade-off between performance and efficiency.
3.1.4 Using predicted connectives as
additional features
This system combines the predicted implicit
connectives as additional features and the 9 types
of features in an supervised framework. The 9
types of features are listed as shown in Section 2.2
and tuned on development set.
We combined predicted connectives with the
best subset features from the development data set
with respect to f-score. In our experiment of se-
lecting best subset features, single features rather
than the combination of several features achieved
much higher scores. So we combine single fea-
tures with predicted connectives as final features.
3.1.5 Using only predicted connectives for
implicit relation recognition
We built two variants for the algorithm in Sec-
tion 2.3. One is to use the data for explicit re-
lations in PDTB sections 2-20 as training data.
The other is to use the data for implicit relations
in PDTB sections 2-20 as training data. Given
training data, we obtained the most frequent sense
for each connective appearing in the training data.
Then given test data, we recognized the sense of
each argument pair by mapping each predicted
connective to its most frequent sense. In this
work we conducted another experiment to see the
upper-bound performance of this algorithm. Here
we performed recognition based on ground-truth
implicit connectives and used the data for implicit
relations as training data.
3.2 Results
3.2.1 Result of baseline system
Table 2 summarizes the best performance
achieved by the baseline system in compari-
son with previous state-of-the-art performance
achieved in (Pitler et al, 2009a). The first two
lines in the table show their best results using sin-
gle feature and using combined feature subset. It
indicates that the performance of using combined
feature subset is higher than that using single fea-
ture alone.
From this table, we can find that our base-
line system has a comparable result on Contin-
gency and Temporal. On Comparison, our system
achieved a better performance around 9% f-score
higher than their best result. However, for Expan-
sion, they expanded both training and testing sets
by including EntRel relation as positive examples,
which makes it impossible to perform direct com-
parison. Generally, our baseline system is reason-
able and thus the consequent experiments on it are
reliable.
3.2.2 Result of algorithm 1: using predicted
connectives as additional features
Table 3 summarizes the best performance
achieved by the baseline system and the first al-
gorithm (i.e., baseline + Language Model) on test
set. The second and third column show the best
performance achieved by the baseline system and
1511
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test
set.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Using the best single feature (Pitler et al, 2009a) 21.01(52.59) 36.75(62.44) 71.29(59.23) 15.93(61.20)
Using the best feature subset (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
the first algorithm using predicted connectives as
additional features.
Table 3: Performance comparison of the algo-
rithm in Section 2.2 with the baseline system on
test set.
Rela- Features Baseline Baseline+LM
tion F1 (Acc) F1 (Acc)
Comp. Production Rule 30.72(78.26) 31.08(68.15)
Context 24.66(42.25) 27.64(53.97)
InquirerTags 23.31(73.25) 27.87(55.48)
Polarity 21.11(40.64) 23.64(52.36)
Modality 17.25(80.06) 26.17(55.20)
Verbs 25.00(53.50) 31.79(58.22)
Cont. Prodcution Rule 45.38(40.17) 47.16(48.96)
Context 37.61(44.70) 34.74(48.87)
Polarity 35.57(50.00) 43.33(33.74)
InquirerTags 38.04(41.49) 42.22(36.11)
Modality 32.18(66.54) 35.26(55.58)
Verbs 40.44(54.06) 42.04(32.23)
Exp. Context 48.34(54.54) 68.32(53.02)
FirstLastFirst3 65.95(57.94) 68.94(53.59)
InquirerTags 61.29(52.84) 68.49(53.21)
Modality 64.36(56.14) 68.9(52.55)
Polarity 49.95(50.38) 68.62(53.40)
Verbs 52.95(53.31) 70.11(54.54)
Temp. Context 13.52(64.93) 16.99(79.68)
FirstLastFirst3 15.75(66.64) 19.70(64.56)
InquirerTags 8.51(83.74) 19.20(56.24)
Modality 16.46(29.96) 19.97(54.54)
Polarity 16.29(51.42) 20.30(55.48)
Verbs 13.88(54.25) 13.53(61.34)
From this table, we found that this additional
feature obtained from language model showed
significant improvements in almost four relations.
Specifically, the top two improvements are on Ex-
pansion and Temporal relations, which improved
4.16% and 3.84% in f-score respectively. Al-
though on Comparison relation there is only a
slight improvement (+1.07%), our two best sys-
tems both got around 10% improvements of f-
score over a state-of-the-art system in (Pitler et al,
2009a). As a whole, the first algorithm achieved
3% improvement of f-score over a state of the art
baseline system. All these results indicate that
predicted implicit connectives can help improve
the performance.
3.2.3 Result of algorithm 2: using only
predicted connectives for implicit
relation recognition
Table 4 summarizes the best performance
achieved by the second algorithm in comparison
with the baseline system on test set.
The experiment showed that the baseline sys-
tem using just gold-truth implicit connectives can
achieve an f-score of 91.8% for implicit relation
recognition. It once again proved that implicit
connectives make significant contributions for im-
plicit relation recognition. This also encourages
our future work on finding the most suitable con-
nectives for implicit relation recognition.
From this table, we found that, using only pre-
dicted implicit connectives achieved an compara-
ble performance to (Pitler et al, 2009a), although
it was still a bit lower than our best baseline. But
we should bear in mind that this algorithm only
uses 4 features for implicit relation recognition
and these 4 features are easy computable and fast
run, which makes the system more practical in ap-
plication. Furthermore, compared with other al-
gorithms which require hand-annotated data for
training, the performance of this second algorithm
is acceptable if we take into account that no la-
beled data is used for model training.
3.3 Analysis
Experimental results on PDTB showed that using
the predicted implicit connectives significantly
improves the performance of implicit discourse
relation recognition. Our first algorithm achieves
an average f-score improvement of 3% over a
state of the art baseline system. Specifically, for
the relations: Comp., Cont., Exp., Temp., our
first algorithm can achieve 1.07%, 1.78%, 4.16%,
3.84% f-score improvements over a state of the
art baseline system. Since (Pitler et al, 2009a)
1512
Table 4: Performance comparison of the algorithm in Section 2.3 with the baseline system on test set.
System Comp. vs. Other Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
Our algorithm with training data for explicit relation 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97)
Our algorithm with training data for implicit relation 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51)
Sense recognition using gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07)
used different selection of instances for Expan-
sion sense3, we cannot make a direct compari-
son. However, we achieve the best f-score around
70%, which provide 5% improvements over our
baseline system. On the other hand, the second
proposed algorithm using only predicted connec-
tives still achieves promising results for each rela-
tion. Specifically, the model for the Comparison
relation achieves an f-score of 26.02% (5% over
the previous work in (Pitler et al, 2009a)). Fur-
thermore, the models for Contingency and Tem-
poral relation achieve 35.72% and 13.76% f-score
respectively, which are comparable to the previ-
ous work in (Pitler et al, 2009a). The model for
Expansion relation obtains an f-score of 64.95%,
which is only 1% less than our baseline system
which consists of ten thousands of features.
4 Related Work
Existing works on automatic recognition of dis-
course relations can be grouped into two cat-
egories according to whether they used hand-
annotated corpora.
One research line is to perform relation recog-
nition without hand-annotated corpora.
(Marcu and Echihabi, 2002) used a pattern-
based approach to extract instances of discourse
relations such as Contrast and Elaboration from
unlabeled corpora. Then they used word-pairs be-
tween two arguments as features for building clas-
sification models and tested their model on artifi-
cial data for implicit relations.
There are other efforts that attempt to extend the
work of (Marcu and Echihabi, 2002). (Saito et al,
2006) followed the method of (Marcu and Echi-
habi, 2002) and conducted experiments with com-
bination of cross-argument word pairs and phrasal
3They expanded the Expansion data set by adding ran-
domly selected EntRel instances by 50%, which is consid-
ered to significantly change data distribution.
patterns as features to recognize implicit relations
between adjacent sentences in a Japanese corpus.
They showed that phrasal patterns extracted from
a text span pair provide useful evidence in the re-
lation classification. (Sporleder and Lascarides,
2008) discovered that Marcu and Echihabi?s mod-
els do not perform as well on implicit relations as
one might expect from the test accuracies on syn-
thetic data. (Blair-Goldensohn, 2007) extended
the work of (Marcu and Echihabi, 2002) by re-
fining the training and classification process using
parameter optimization, topic segmentation and
syntactic parsing.
(Lapata and Lascarides, 2004) dealt with tem-
poral links between main and subordinate clauses
by inferring the temporal markers linking them.
They extracted clause pairs with explicit temporal
markers from BLLIP corpus as training data.
Another research line is to use human-
annotated corpora as training data, e.g., the RST
Bank (Carlson et al, 2001) used by (Soricut and
Marcu, 2003), adhoc annotations used by (?),
(Baldridge and Lascarides, 2005), and the Graph-
Bank (Wolf et al, 2005) used by (Wellner et al,
2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2008) bene-
fits the researchers with a large discourse anno-
tated corpora, using a comprehensive scheme for
both implicit and explicit relations. (Pitler et al,
2009a) performed implicit relation classification
on the second version of the PDTB. They used
several linguistically informed features, such as
word polarity, verb classes, and word pairs, show-
ing performance increases over a random classi-
fication baseline. (Lin et al, 2009) presented an
implicit discourse relation classifier in PDTB with
the use of contextual relations, constituent Parse
Features, dependency parse features and cross-
argument word pairs.
1513
In comparison with existing works, we investi-
gated a new knowledge source, implicit connec-
tives, for implicit relation recognition. Moreover,
our two models can exploit both labeled and un-
labeled data by training a language model on un-
labeled data and then using this language model
to generate implicit connectives for recognition
models trained on labeled data.
5 Conclusions
In this paper we use a language model to auto-
matically generate implicit connectives and then
present two methods to use these connectives for
recognition of implicit relations. One method is to
use these predicted implicit connectives as addi-
tional features in a supervised model and the other
is to perform implicit relation recognition based
only on these predicted connectives. Results on
Penn Discourse Treebank 2.0 show that predicted
discourse connectives help implicit relation recog-
nition and the first algorithm achieves an absolute
average f-score improvement of 3% over a state of
the art baseline system.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, Col-
lege Park, MD,2001.
R. Girju. 2003. Automatic detection of causal rela-
tions for question answering. In ACL 2003 Work-
shops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
EMNLP.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th ACL.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th ACL.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. 1980. An algorithm for suffix stripping. In
Program, vol. 14, no. 3, pp.130-137.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. Sentence Level Discourse
Parsing using Syntactic and Lexical Information.
Proceedings of HLT/NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
1514
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 476?485,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit
Discourse Relation Recognition
Man Lan and Yu Xu
Department of Computer Science and Technology
East China Normal University
Shanghai, P.R.China
mlan@cs.ecnu.edu.cn
51101201049@ecnu.cn
Zheng-Yu Niu
Baidu Inc.
Beijing, P.R.China
niuzhengyu@baidu.com
Abstract
To overcome the shortage of labeled data
for implicit discourse relation recogni-
tion, previous works attempted to auto-
matically generate training data by remov-
ing explicit discourse connectives from
sentences and then built models on these
synthetic implicit examples. However, a
previous study (Sporleder and Lascarides,
2008) showed that models trained on these
synthetic data do not generalize very well
to natural (i.e. genuine) implicit discourse
data. In this work we revisit this issue and
present a multi-task learning based system
which can effectively use synthetic data
for implicit discourse relation recognition.
Results on PDTB data show that under the
multi-task learning framework our models
with the use of the prediction of explicit
discourse connectives as auxiliary learn-
ing tasks, can achieve an averaged F1 im-
provement of 5.86% over baseline models.
1 Introduction
The task of implicit discourse relation recognition
is to identify the type of discourse relation (a.k.a.
rhetorical relation) hold between two spans of
text, where there is no discourse connective (a.k.a.
discourse marker, e.g., but, and) in context to ex-
plicitly mark their discourse relation (e.g., Con-
trast or Explanation). It can be of great benefit
to many downstream NLP applications, such as
question answering (QA) (Verberne et al, 2007),
information extraction (IE) (Cimiano et al, 2005),
and machine translation (MT), etc. This task is
quite challenging due to two reasons. First, with-
out discourse connective in text, the task is quite
difficult in itself. Second, implicit discourse rela-
tion is quite frequent in text. For example, almost
half the sentences in the British National Corpus
held implicit discourse relations (Sporleder and
Lascarides, 2008). Therefore, the task of implicit
discourse relation recognition is the key to im-
proving end-to-end discourse parser performance.
To overcome the shortage of manually anno-
tated training data, (Marcu and Echihabi, 2002)
proposed a pattern-based approach to automat-
ically generate training data from raw corpora.
This line of research was followed by (Sporleder
and Lascarides, 2008) and (Blair-Goldensohn,
2007). In these works, sentences containing cer-
tain words or phrases (e.g. but, although) were
selected out from raw corpora using a pattern-
based approach and then these words or phrases
were removed from these sentences. Thus the
resulting sentences were used as synthetic train-
ing examples for implicit discourse relation recog-
nition. Since there is ambiguity of a word or
phrase serving for discourse connective (i.e., the
ambiguity between discourse and non-discourse
usage or the ambiguity between two or more dis-
course relations if the word or phrase is used as a
discourse connective), the synthetic implicit data
would contain a lot of noises. Later, with the re-
lease of manually annotated corpus, such as Penn
Discourse Treebank 2.0 (PDTB) (Prasad et al,
2008), recent studies performed implicit discourse
relation recognition on natural (i.e., genuine) im-
plicit discourse data (Pitler et al, 2009) (Lin et al,
2009) (Wang et al, 2010) with the use of linguis-
tically informed features and machine learning al-
gorithms.
(Sporleder and Lascarides, 2008) conducted a
study of the pattern-based approach presented by
(Marcu and Echihabi, 2002) and showed that the
model built on synthetical implicit data has not
generalize well on natural implicit data. They
found some evidence that this behavior is largely
independent of the classifiers used and seems to
lie in the data itself (e.g., marked and unmarked
examples may be too dissimilar linguistically and
476
removing unambiguous markers in the automatic
labelling process may lead to a meaning shift in
the examples). We state that in some cases it is
true while in other cases it may not always be so.
A simple example is given here:
(E1) a. We can?t win.
b. [but] We must keep trying.
We may find that in this example whether the in-
sertion or the removal of connective but would
not lead to a redundant or missing information be-
tween the above two sentences. That is, discourse
connectives can be inserted between or removed
from two sentences without changing the seman-
tic relations between them in some cases. An-
other similar observation is in the annotation pro-
cedure of PDTB. To label implicit discourse re-
lation, annotators inserted connective which can
best express the relation between sentences with-
out any redundancy1. We see that there should
be some linguistical similarities between explicit
and implicit discourse examples. Therefore, the
first question arises: can we exploit this kind of
linguistic similarity between explicit and implicit
discourse examples to improve implicit discourse
relation recognition?
In this paper, we propose a multi-task learning
based method to improve the performance of im-
plicit discourse relation recognition (as main task)
with the help of relevant auxiliary tasks. Specif-
ically, the main task is to recognize the implicit
discourse relations based on genuine implicit dis-
course data and the auxiliary task is to recognize
the implicit discourse relations based on synthetic
implicit discourse data. According to the princi-
ple of multi-task learning, the learning model can
be optimized by the shared part of the main task
and the auxiliary tasks without bring unnecessary
noise. That means, the model can learn from syn-
thetic implicit data while it would not bring unnec-
essary noise from synthetic implicit data.
Although (Sporleder and Lascarides, 2008) did
not mention, we speculate that another possible
reason for the reported worse performance may
result from noises in synthetic implicit discourse
data. These synthetic data can be generated from
two sources: (1) raw corpora with the use of
pattern-based approach in (Marcu and Echihabi,
1According to the PDTB Annotation Manual (PDTB-
Group, 2008), if the insertion of connective leads to ?redun-
dancy?, the relation is annotated as Alternative lexicalizations
(AltLex), not implicit.
2002) and (Sporleder and Lascarides, 2008), and
(2) manually annotated explicit data with the re-
moval of explicit discourse connectives. Obvi-
ously, the data generated from the second source
is cleaner and more reliable than that from the
first source. Therefore, the second question to ad-
dress in this work is: whether synthetic implicit
discourse data generated from explicit discourse
data source (i.e., the second source) can lead to
a better performance than that from raw corpora
(i.e., the first source)? To answer this question,
we will make a comparison of synthetic discourse
data generated from two corpora, i.e., the BILLIP
corpus and the explicit discourse data annotated in
PDTB.
The rest of this paper is organized as follows.
Section 2 reviews related work on implicit dis-
course relation classification and multi-task learn-
ing. Section 3 presents our proposed multi-task
learning method for implicit discourse relation
classification. Section 4 provides the implemen-
tation technique details of the proposed multi-task
method. Section 5 presents experiments and dis-
cusses results. Section 6 concludes this work.
2 Related Work
2.1 Implicit discourse relation classification
2.1.1 Unsupervised approaches
Due to the lack of benchmark data for implicit
discourse relation analysis, earlier work used un-
labeled data to generate synthetic implicit dis-
course data. For example, (Marcu and Echi-
habi, 2002) proposed an unsupervised method
to recognize four discourse relations, i.e., Con-
trast, Explanation-evidence, Condition and Elab-
oration. They first used unambiguous pattern to
extract explicit discourse examples from raw cor-
pus. Then they generated synthetic implicit dis-
course data by removing explicit discourse con-
nectives from sentences extracted. In their work,
they collected word pairs from synthetic data set
as features and used machine learning method to
classify implicit discourse relation. Based on this
work, several researchers have extended the work
to improve the performance of relation classifica-
tion. For example, (Saito et al, 2006) showed that
the use of phrasal patterns as additional features
can help a word-pair based system for discourse
relation prediction on a Japanese corpus. Further-
more, (Blair-Goldensohn, 2007) improved previ-
ous work with the use of parameter optimization,
477
topic segmentation and syntactic parsing. How-
ever, (Sporleder and Lascarides, 2008) showed
that the training model built on a synthetic data
set, like the work of (Marcu and Echihabi, 2002),
may not be a good strategy since the linguistic dis-
similarity between explicit and implicit data may
hurt the performance of a model on natural data
when being trained on synthetic data.
2.1.2 Supervised approaches
This line of research work approaches this relation
prediction problem by recasting it as a classifica-
tion problem. (Soricut and Marcu, 2003) parsed
the discourse structures of sentences on RST Bank
data set (Carlson et al, 2001) which is annotated
based on Rhetorical Structure Theory (Mann and
Thompson, 1988). (Wellner et al, 2006) pre-
sented a study of discourse relation disambigua-
tion on GraphBank (Wolf et al, 2005). Recently,
(Pitler et al, 2009) (Lin et al, 2009) and (Wang
et al, 2010) conducted discourse relation study on
PDTB (Prasad et al, 2008) which has been widely
used in this field.
2.1.3 Semi-supervised approaches
Research work in this category exploited both la-
beled and unlabeled data for discourse relation
prediction. (Hernault et al, 2010) presented a
semi-supervised method based on the analysis of
co-occurring features in labeled and unlabeled
data. Very recently, (Hernault et al, 2011) in-
troduced a semi-supervised work using structure
learning method for discourse relation classifica-
tion, which is quite relevant to our work. However,
they performed discourse relation classification on
both explicit and implicit data. And their work is
different from our work in many aspects, such as,
feature sets, auxiliary task, auxiliary data, class la-
bels, learning framework, and so on. Furthermore,
there is no explicit conclusion or evidence in their
work to address the two questions raised in Sec-
tion 1.
Unlike their previous work, our previous work
(Zhou et al, 2010) presented a method to predict
the missing connective based on a language model
trained on an unannotated corpus. The predicted
connective was then used as a feature to classify
the implicit relation.
2.2 Multi-task learning
Multi-task learning is a kind of machine learning
method, which learns a main task together with
other related auxiliary tasks at the same time, us-
ing a shared representation. This often leads to
a better model for the main task, because it al-
lows the learner to use the commonality among
the tasks. Many multi-task learning methods have
been proposed in recent years, (Ando and Zhang,
2005a), (Argyriou et al, 2008), (Jebara, 2004),
(Bonilla et al, 2008), (Evgeniou and Pontil, 2004),
(Baxter, 2000), (Caruana, 1997), (Thrun, 1996).
One group uses task relations as regularization
terms in the objective function to be optimized.
For example, in (Evgeniou and Pontil, 2004) the
regularization terms make the parameters of mod-
els closer for similar tasks. Another group is pro-
posed to find the common structure from data and
then utilize the learned structure for multi-task
learning (Argyriou et al, 2008) (Ando and Zhang,
2005b).
3 Multi-task Learning for Discourse
Relation Prediction
3.1 Motivation
The idea of using multi-task learning for implicit
discourse relation classification is motivated by
the observations that we have made on implicit
discourse relation.
On one hand, since building a hand-annotated
implicit discourse relation corpus is costly and
time consuming, most previous work attempted to
use synthetic implicit discourse examples as train-
ing data. However, (Sporleder and Lascarides,
2008) found that the model trained on synthetic
implicit data has not performed as well as expected
in natural implicit data. They stated that the reason
is linguistic dissimilarity between explicit and im-
plicit discourse data. This indicates that straightly
using synthetic implicit data as training data may
not be helpful.
On the other hand, as shown in Section 1, we
observe that in some cases explicit discourse rela-
tion and implicit discourse relation can express the
same meaning with or without a discourse connec-
tive. This indicates that in certain degree they must
be similar to each other. If it is true, the synthetic
implicit relations are expected to be helpful for im-
plicit discourse relation classification. Therefore,
what we have to do is to find a way to train a model
which has the capabilities to learn from their sim-
ilarity and to ignore their dissimilarity as well.
To solve it, we propose a multi-task learn-
ing method for implicit discourse relation classi-
478
fication, where the classification model seeks the
shared part through jointly learning main task and
multiple auxiliary tasks. As a result, the model can
be optimized by the similar shared part without
bringing noise in the dissimilar part. Specifically,
in this work, we use alternating structure optimiza-
tion (ASO) (Ando and Zhang, 2005a) to construct
the multi-task learning framework. ASO has been
shown to be useful in a semi-supervised learning
configuration for several NLP applications, such
as, text chunking (Ando and Zhang, 2005b) and
text classification (Ando and Zhang, 2005a).
3.2 Multi-task learning and ASO
Generally, multi-task learning(MTL) considers m
prediction problems indexed by ? ? {1, ...,m},
each with n? samples (X?i , Y ?i ) for i ? {1, ...n?}
(Xi are input feature vectors and Yi are corre-
sponding classification labels) and assumes that
there exists a common predictive structure shared
by these m problems. Generally, the joint linear
model for MTL is to predict problem ? in the fol-
lowing form:
f?(?, X) = wT? X + vT? ?X,??T = I, (1)
where I is the identity matrix,w? and v? are weight
vectors specific to each problem ?, and ? is the
structure matrix shared by all the m predictors.
The main goal of MTL is to learn a common good
feature map ?X for all the m problems. Several
MTL methods have been presented to learn ?X
for all the m problems. In this work, we adopt the
ASO method.
Specifically, the ASO method adopted singu-
lar value decomposition (SVD) to obtain ? and
m predictors that minimize the empirical risk
summed over all the m problems. Thus, the prob-
lem of optimization becomes the minimization of
the joint empirical risk written as:
m?
?=1
( n??
i=1
L(f?(?, X?i ), Yi)
n?
+ ?||W?||2
)
(2)
where loss function L(.) quantifies the difference
between the prediction f(Xi) and the true out-
put Yi for each predictor, and ? is a regulariza-
tion parameter for square regularization to control
the model complexity. To minimize the empirical
risk, ASO repeats the following alternating opti-
mization procedure until a convergence criterion
is met:
1) Fix (?, V?), and find m predictors f? that
minimize the above joint empirical risk.
2) Fix m predictors f?, and find (?, V?) that
minimizes the above joint empirical risk.
3.3 Auxiliary tasks
There are two main principles to create auxiliary
tasks. First, the auxiliary tasks should be auto-
matically labeled in order to reduce the cost of
manual labeling. Second, since the MTL model
learns from the shared part of main task and aux-
iliary tasks, the auxiliary tasks should be quite rel-
evant/similar to the main task. It is generally be-
lieved that the more the auxiliary tasks are relevant
to the main task, the more the main task can ben-
efit from the auxiliary tasks. Following these two
principles, we create the auxiliary tasks by gener-
ating automatically labeled data as follows.
Previous work (Marcu and Echihabi, 2002) and
(Sporleder and Lascarides, 2008) adopted prede-
fined pattern-based approach to generate synthetic
labeled data, where each predefined pattern has
one discourse relation label. In contrast, we adopt
an automatic approach to generate synthetic la-
beled data, where each discourse connective be-
tween two texts serves as their relation label. The
reason lies in the very strong connection between
discourse connectives and discourse relations. For
example, the connective but always indicates a
contrast relation between two texts. And (Pitler et
al., 2008) proved that using only connective itself,
the accuracy of explicit discourse relation classifi-
cation is over 93%.
To build the mapping between discourse con-
nective and discourse relation, for each connec-
tive, we count the times it appears in each relation
and regard the relation in which it appears most
frequently as its most relevant relation. Based on
this mapping between connective and relation, we
extract the synthetic labeled data containing the
connective as training data for auxiliary tasks.
For example, and appears 3, 000 times in PDTB
as a discourse connective. Among them, it is man-
ually annotated as an Expansion relation for 2, 938
times. So we regard the Expansion relation as its
most relevant relation and generate a mapping pat-
tern like: ?and ? Expansion?. Then we extract
all sentences which contain discourse ?and? and
remove this connective ?and? from sentences to
generate synthetic implicit data. The resulting sen-
tences are used in auxiliary task and automatically
479
marked as Expansion relation.
4 Implementation Details of Multi-task
Learning Method
4.1 Data sets for main and auxiliary tasks
To examine whether there is a difference in syn-
thetic implicit data generated from unannotated
and annotated corpus, we use two corpora. One
is a hand-annotated explicit discourse corpus, i.e.,
the explicit discourse relations in PDTB, denoted
as exp. Another is an unannotated corpus, i.e.,
BLLIP (David McClosky and Johnson., 2008).
4.1.1 Penn Discourse Treebank
PDTB (Prasad et al, 2008) is the largest hand-
annotated corpus of discourse relation so far. It
contains 2, 312 Wall Street Journal (WSJ) articles.
The sense label of discourse relations is hierarchi-
cally with three levels, i.e., class, type and sub-
type. The top level contains four major seman-
tic classes: Comparison (denoted as Comp.), Con-
tingency (Cont.), Expansion (Exp.) and Temporal
(Temp.). For each class, a set of types is used to
refine relation sense. The set of subtypes is to fur-
ther specify the semantic contribution of each ar-
gument. In this paper, we focus on the top level
(class) and the second level (type) relations be-
cause the subtype relations are too fine-grained
and only appear in some relations.
Both explicit and implicit discourse relations
are labeled in PDTB. In our experiment, the im-
plicit discourse relations are used in the main task
and for evaluation. While the explicit discourse
relations are used in the auxiliary task. A detailed
description of the data sources for different tasks
is given below.
Data set for main task Following previous
work in (Pitler et al, 2009) and (Zhou et al, 2010),
the implicit relations in sections 2-20 are used as
training data for the main task (denoted as imp)
and the implicit relations in sections 21-22 are
for evaluation. Table 1 shows the distribution of
implicit relations. There are too few training in-
stances for six second level relations (indicated by
* in Table 1), so we removed these six relations in
our experiments.
Data set for auxiliary task All explicit in-
stances in sections 00-24 in PDTB, i.e., 18, 459
instances, are used for auxiliary task (denoted as
exp). Following the method described in Section
3.3, we build the mapping patterns between con-
Top level Second level train test
Temp 736 83
Synchrony 203 28
Asynchronous 532 55
Cont 3333 279
Cause 3270 272
Pragmatic Cause* 64 7
Condition* 1 0
Pragmatic condition* 1 0
Comp 1939 152
Contrast 1607 134
Pragmatic contrast* 4 0
Concession 183 17
Pragmatic concession* 1 0
Exp 6316 567
Conjunction 2872 208
Instantiation 1063 119
Restatement 2405 213
Alternative 147 9
Exception* 0 0
List 338 12
Table 1: Distribution of implicit discourse rela-
tions in the top and second level of PDTB
nectives and relations in PDTB and generate syn-
thetic labeled data by removing the connectives.
According to the most relevant relation sense of
connective removed, the resulting instances are
grouped into different data sets.
4.1.2 BLLIP
BLLIP North American News Text (Complete) is
used as unlabeled data source to generate syn-
thetic labeled data. In comparison with the syn-
thetic labeled data generated from the explicit re-
lations in PDTB, the synthetic labeled data from
BLLIP contains more noise. This is because the
former data is manually annotated whether a word
serves as discourse connective or not, while the
latter does not manually disambiguate two types
of ambiguity, i.e., whether a word serves as dis-
course connective or not, and the type of discourse
relation if it is a discourse connective. Finally, we
extract 26, 412 instances from BLLIP (denoted as
BLLIP) and use them for auxiliary task.
4.2 Feature representation
For both main task and auxiliary tasks, we adopt
the following three feature types. These features
are chosen due to their superior performance in
previous work (Pitler et al, 2009) and our previ-
ous work (Zhou et al, 2010).
Verbs: Following (Pitler et al, 2009), we ex-
tract the pairs of verbs from both text spans. The
number of verb pairs which have the same highest
480
Levin verb class levels (Levin, 1993) is counted
as a feature. Besides, the average length of verb
phrases in each argument is included as a feature.
In addition, the part of speech tags of the main
verbs (e.g., base form, past tense, 3rd person sin-
gular present, etc.) in each argument, i.e., MD,
VB, VBD, VBG, VBN, VBP, VBZ, are recorded
as features, where we simply use the first verb in
each argument as the main verb.
Polarity: This feature records the number of
positive, negated positive, negative and neutral
words in both arguments and their cross product
as well. For negated positives, we first locate the
negated words in text span and then define the
closely behind positive word as negated positive.
The polarity of each word in arguments is de-
rived from Multi-perspective Question Answering
Opinion Corpus (MPQA) (Wilson et al, 2009).
Modality: We examine six modal words (i.e.,
can, may, must, need, shall, will) including their
various tenses or abbreviation forms in both argu-
ments. This feature records the presence or ab-
sence of modal words in both arguments and their
cross product.
4.3 Classifiers used multi-task learning
We extract the above linguistically informed fea-
tures from two synthetic implicit data sets (i.e.,
BLLIP and exp) to learn the auxiliary classifier and
from the natural implicit data set (i.e., imp) to learn
the main classifier. Under the ASO-based multi-
task learning framework, the model of main task
learns from the shared part of main task and aux-
iliary tasks. Specifically, we adopt multiple binary
classification to build model for main task. That
is, for each discourse relation, we build a binary
classifier.
5 Experiments and Results
5.1 Experiments
Although previous work has been done on PDTB
(Pitler et al, 2009) and (Lin et al, 2009), we can-
not make a direct comparison with them because
various experimental conditions, such as, differ-
ent classification strategies (multi-class classifica-
tion, multiple binary classification), different data
preparation (feature extraction and selection), dif-
ferent benchmark data collections (different sec-
tions for training and test, different levels of dis-
course relations), different classifiers with various
parameters (MaxEnt, Na??ve Bayes, SVM, etc) and
even different evaluation methods (F1, accuracy)
have been adopted by different researchers.
Therefore, to address the two questions raised in
Section 1 and to make the comparison reliable and
reasonable, we performed experiments on the top
and second level of PDTB using single task learn-
ing and multi-task learning, respectively. The sys-
tems using single task learning serve as baseline
systems. Under the single task learning, various
combinations of exp and BLLIP data are incorpo-
rated with imp data for the implicit discourse rela-
tion classification task.
We hypothesize that synthetical implicit data
would contribute to the main task, i.e., the implicit
discourse relation classification. Specifically, the
natural implicit data (i.e., imp) are used to create
main task and the synthetical implicit data (exp or
BLLIP) are used to create auxiliary tasks for the
purpose of optimizing the objective functions of
main task. If the hypothesis is correct, the perfor-
mance of main task would be improved by auxil-
iary tasks created from synthetical implicit data.
Thus in the experiments of multi-task learning,
only natural implicit examples (i.e., imp) data are
used for main task training while different combi-
nations of synthetical implicit examples (exp and
BLLIP) are used for auxiliary task training.
We adopt precision, recall and their combina-
tion F1 for performance evaluation. We also per-
form one-tailed t-test to validate if there is signif-
icant difference between two methods in terms of
F1 performance analysis.
5.2 Results
Table 2 summarizes the experimental results under
single and multi-task learning on the top level of
four PDTB relations with respect to different com-
binations of synthetic implicit data. For each rela-
tion, the first three rows indicate the results of us-
ing different single training data under single task
learning and the last three rows indicate the results
using different combinations of training data un-
der single task and multi-task learning. The best
F1 for every relation is shown in bold font. From
this table, we can find that on four relations, our
multi-task learning systems achieved the best per-
formance using the combination of exp and BLLIP
synthetic data.
Table 3 summarizes the best single task and the
best multi-task learning results on the second level
of PDTB. For four relations, i.e., Synchrony, Con-
481
Single-task Multi-task
Level 1 class Data P R F1 Data Data P R F1
(main) (aux)
Comp. imp 21.43 37.50 27.27 - - - - -
BLLIP 12.68 53.29 20.48 - - - - -
exp 15.25 50.66 23.44 - - - - -
imp + exp 16.94 40.13 23.83 imp exp 22.94 49.34 30.90
imp + BLLIP 13.56 44.08 20.74 imp BLLIP 20.47 63.16 30.92
imp + exp + BLLIP 14.54 38.16 21.05 imp exp + BLLIP 23.47 48.03 31.53
Cont. imp 37.65 43.73 40.46 - - - - -
BLLIP 33.72 31.18 32.40 - - - - -
exp 35.24 26.52 30.27 - - - - -
imp + exp 39.00 13.98 20.58 imp exp 39.94 45.52 42.55
imp + BLLIP 37.30 24.73 29.74 imp BLLIP 37.80 63.80 47.47
imp + exp + BLLIP 39.37 31.18 34.80 imp exp + BLLIP 35.90 70.25 47.52
Exp. imp 56.59 66.67 61.21 - - - - -
BLLIP 53.29 40.04 45.72 - - - - -
exp 57.97 58.38 58.17 - - - - -
imp + exp 57.32 65.61 61.18 imp exp 59.14 67.90 63.22
imp + BLLIP 56.28 65.61 60.59 imp BLLIP 53.80 99.82 69.92
imp + exp + BLLIP 55.81 65.26 60.16 imp exp + BLLIP 53.90 99.82 70.01
Temp. imp 16.46 63.86 26.17 - - - - -
BLLIP 17.31 43.37 24.74 - - - - -
exp 15.46 36.14 21.66 - - - - -
imp + exp 15.35 39.76 22.15 imp exp 18.60 63.86 28.80
imp + BLLIP 14.74 33.73 20.51 imp BLLIP 18.12 67.47 28.57
imp + exp + BLLIP 15.94 39.76 22.76 imp exp + BLLIP 19.08 65.06 29.51
Table 2: Performance of precision, recall and F1 for 4 Level 1 relation classes. ?-? indicates N.A.
Single-task Multi-task
Level 2 type Data P R F1 Data Data P R F1
(main) (aux)
Asynchronous imp 11.36 74.55 19.71 imp exp + BLLIP 23.08 21.82 22.43
Synchrony imp - - - imp exp + BLLIP - - -
Cause imp 36.38 64.34 46.48 imp exp + BLLIP 36.01 67.65 47.00
Contrast imp 20.07 42.54 27.27 imp exp + BLLIP 20.70 52.99 29.77
Concession imp - - - imp exp + BLLIP - - -
Conjunction imp 26.35 63.46 37.24 imp exp + BLLIP 26.29 73.56 38.73
Instantiation imp 22.78 53.78 32.00 imp exp + BLLIP 22.55 57.98 32.47
Restatement imp 23.11 67.61 34.45 imp exp + BLLIP 26.93 53.99 35.94
Alternative imp - - - imp exp + BLLIP - - -
List imp - - - imp exp + BLLIP - - -
Table 3: Performance of precision, recall and F1 for 10 Level 2 relation types. ?-? indicates 0.00.
cession, Alternative and List, the classifier labels
no instances due to the small percentages for these
four types.
Table 4 summarizes the one-tailed t-test results
on the top level of PDTB between the best single
task learning system (i.e., imp) and three multi-
task learning systems (imp:exp+BLLIP indicates
that imp is used for main task and the combi-
nation of exp and BLLIP are for auxiliary task).
The systems with insignificant performance differ-
ences are grouped into one set and ?>? and ?>>?
denote better than at significance level 0.01 and
0.001 respectively.
5.3 Discussion
From Table 2 to Table 4, several findings can be
found as follows.
We can see that the multi-task learning sys-
tems perform consistently better than the single
task learning systems for the prediction of implicit
discourse relations. Our best multi-task learning
system achieves an averaged F1 improvement of
5.86% over the best single task learning system on
the top level of PDTB relations. Specifically, for
482
Class One-tailed t-test results
Comp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)
Cont. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)
Exp. (imp:exp+BLLIP, imp:BLLIP) >> (imp:exp) > (imp)
Temp. (imp:exp+BLLIP, imp:exp, imp:BLLIP) >> (imp)
Table 4: Statistical significance tests results.
the relations Comp., Cont., Exp., Temp., our best
multi-task learning system achieve 4.26%, 7.06%,
8.8% and 3.34% F1 improvements over the best
single task learning system. It indicates that using
synthetic implicit data as auxiliary task greatly im-
proves the performance of the main task. This is
confirmed by the following t-tests in Table 4.
In contrast to the performance of multi-task
learning, the performance of the best single task
learning system has been achieved on natural im-
plicit discourse data alone. This finding is con-
sistent with (Sporleder and Lascarides, 2008). It
indicates that under single task learning, directly
adding synthetic implicit data to increase the num-
ber of training data cannot be helpful to implicit
discourse relation classification. The possible rea-
sons result from (1) the different nature of implicit
and explicit discourse data in linguistics and (2)
the noise brought from synthetic implicit data.
Based on the above analysis, we state that it is
the way of utilizing synthetic implicit data that is
important for implicit discourse relation classifica-
tion.
Although all three multi-task learning systems
outperformed single task learning systems, we
find that the two synthetic implicit data sets have
not been shown a universally consistent perfor-
mance on four top level PDTB relations. On one
hand, for the relations Comp. and Temp., the per-
formance of the two synthetic implicit data sets
alone and their combination are comparable to
each other and there is no significant difference
between them. On the other hand, for the rela-
tions Cont. and Exp., the performance of exp data
is inferior to that of BLLIP and their combination.
This is contrary to our original expectation that exp
data which has been manually annotated for dis-
course connective disambiguation should outper-
form BLLIP which contains a lot of noise. This
finding indicates that under the multi-task learn-
ing, it may not be worthy of using manually anno-
tated corpus to generate auxiliary data. It is quite
promising since it can provide benefits to reducing
the cost of human efforts on corpus annotation.
5.4 Ambiguity Analysis
Although our experiments show that synthetic im-
plicit data can help implicit discourse relation clas-
sification under multi-task learning framework,
the overall performance is still quite low (44.64%
in F1). Therefore, we analyze the types of ambi-
guity in relations and connectives in order to mo-
tivate possible future work.
5.4.1 Ambiguity of implicit relation
Without explicit discourse connective, the implicit
discourse relation instance can be understood in
two or more different ways. Given the example
E2 in PDTB, the PDTB annotators explain it as
Contingency or Expansion relation and manually
insert corresponding implicit connective for one
thing or because to express its relation.
(E2) Arg1:Now the stage is set for the battle to
play out
Arg2:The anti-programmers are getting
some helpful thunder from Congress
Connective1:because
Sense1:Contingency.Cause.Reason
Connective2:for one thing
Sense2:Expansion.Instantiation
(wsj 0118)
Thus the ambiguity of implicit discourse rela-
tions makes this task difficult in itself.
5.4.2 Ambiguity of discourse connectives
As we mentioned before, even given an explicit
discourse connective in text, its discourse rela-
tion still can be explained in two or more differ-
ent ways. And for different connectives, the am-
biguity of relation senses is quite different. That
is, the most frequent sense is not always the only
sense that a connective expresses. In example E3,
?since? is explained by annotators to express Tem-
poral or Contingency relation.
(E3) Arg1:MiniScribe has been on the rocks
Arg2:since it disclosed early this year that
its earnings reports for 1988 weren?t accu-
rate.
483
Sense1:Temporal.Asynchronous.Succession
Sense2:Contingency.Cause.Reason
(wsj 0003)
In PDTB, ?since? appears 184 times in explicit
discourse relations. It expresses Temporal relation
for 80 times, Contingency relation for 94 times
and both Temporal and Contingency for 10 time
(like example E3). Therefore, although we use its
most frequent sense, i.e., Contingency, to automat-
ically extract sentences and label them, almost less
than half of them actually express Temporal rela-
tion. Thus the ambiguity of discourse connectives
is another source which has brought noise to data
when we generate synthetical implicit discourse
relation.
6 Conclusions
In this paper, we present a multi-task learning
method to improve implicit discourse relation
classification by leveraging synthetic implicit dis-
course data. Results on PDTB show that under
the framework of multi-task learning, using syn-
thetic discourse data as auxiliary task significantly
improves the performance of main task. Our best
multi-task learning system achieves an averaged
F1 improvement of 5.86% over the best single task
learning system on the top level of PDTB rela-
tions. Specifically, for the relations Comp., Cont.,
Exp., Temp., our best multi-task learning system
achieves 4.26%, 7.06%, 8.8%, and 3.34% F1 im-
provements over a state of the art baseline system.
This indicates that it is the way of utilizing syn-
thetic discourse examples that is important for im-
plicit discourse relation classification.
Acknowledgements
This research is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500), Doctoral Fund of Ministry of
Education of China (No. 20090076120029) and
Shanghai Knowledge Service Platform Project
(No. ZF1213).
References
R.K. Ando and T. Zhang. 2005a. A framework for
learning predictive structures from multiple tasks
and unlabeled data. The Journal of Machine Learn-
ing Research, 6:1817?1853.
R.K. Ando and T. Zhang. 2005b. A high-performance
semi-supervised learning method for text chunking.
pages 1?9. Association for Computational Linguis-
tics. Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
A. Argyriou, C.A. Micchelli, M. Pontil, and Y. Ying.
2008. A spectral regularization framework for
multi-task structure learning. Advances in Neural
Information Processing Systems, 20:2532.
J. Baxter. 2000. A model of inductive bias learning. J.
Artif. Intell. Res. (JAIR), 12:149?198.
S.J. Blair-Goldensohn. 2007. Long-answer question
answering and rhetorical-semantic relations. Ph.D.
thesis.
E. Bonilla, K.M. Chai, and C. Williams. 2008. Multi-
task gaussian process prediction. Advances in Neu-
ral Information Processing Systems, 20(October).
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. pages 1?10. As-
sociation for Computational Linguistics. Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue-Volume 16.
R. Caruana. 1997. Multitask learning. Machine
Learning, 28(1):41?75.
P. Cimiano, U. Reyle, and J. Saric. 2005. Ontology-
driven discourse analysis for information extraction.
Data and Knowledge Engineering, 55(1):59?83.
Eugene Charniak David McClosky and Mark Johnson.
2008. Bllip north american news text, complete.
T. Evgeniou and M. Pontil. 2004. Regularized multi?
task learning. pages 109?117. ACM. Proceedings
of the tenth ACM SIGKDD international conference
on Knowledge discovery and data mining.
H. Hernault, D. Bollegala, and M. Ishizuka. 2010. A
semi-supervised approach to improve classification
of infrequent discourse relations using feature vector
extension. pages 399?409. Association for Compu-
tational Linguistics. Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing.
H. Hernault, D. Bollegala, and M. Ishizuka. 2011.
Semi-supervised discourse relation classification
with structural learning. In Proceedings of the 12th
international conference on Computational linguis-
tics and intelligent text processing - Volume Part
I, CICLing?11, pages 340?352, Berlin, Heidelberg.
Springer-Verlag.
T. Jebara. 2004. Multi-task feature and kernel se-
lection for svms. page 55. ACM. Proceedings of
the twenty-first international conference on Machine
learning.
B. Levin. 1993. English verb classes and alternations:
A preliminary investigation, volume 348. University
of Chicago press Chicago, IL:.
484
Z. Lin, M.Y. Kan, and H.T. Ng. 2009. Recogniz-
ing implicit discourse relations in the penn discourse
treebank. pages 343?351. Association for Compu-
tational Linguistics. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text-Interdisciplinary Journal for the
Study of Discourse, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An unsupervised
approach to recognizing discourse relations. pages
368?375. Association for Computational Linguis-
tics. Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics.
PDTB-Group. 2008. The penn discourse treebank 2.0
annotation manual. Technical report, Institute for
Research in Cognitive Science, University of Penn-
sylvania.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. Citeseer. Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING 2008), Manchester, UK, August.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. pages 683?691. Association for Computational
Linguistics. Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.
M. Saito, K. Yamamoto, and S. Sekine. 2006. Us-
ing phrasal patterns to identify discourse relations.
pages 133?136. Association for Computational Lin-
guistics. Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers on XX.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. pages 149?156. Association for Computational
Linguistics. Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14(03):369?416.
S. Thrun. 1996. Is learning the n-th thing any easier
than learning the first? Advances in Neural Infor-
mation Processing Systems, pages 640?646.
S. Verberne, L. Boves, N. Oostdijk, and P.A. Coppen.
2007. Evaluating discourse-based answer extraction
for why-question answering. pages 735?736. ACM.
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval.
W.T. Wang, J. Su, and C.L. Tan. 2010. Kernel based
discourse relation recognition with temporal order-
ing information. pages 710?719. Association for
Computational Linguistics. Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using multi-
ple knowledge sources. pages 117?125. Association
for Computational Linguistics. Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Rec-
ognizing contextual polarity: An exploration of fea-
tures for phrase-level sentiment analysis. Computa-
tional Linguistics, 35(3):399?433.
F. Wolf, E. Gibson, A. Fisher, and M. Knight. 2005.
The discourse graphbank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
Z.M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, and C.L.
Tan. 2010. Predicting discourse connectives for im-
plicit discourse relation recognition. pages 1507?
1514. Association for Computational Linguistics.
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
485
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139?146,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The Effects of Discourse Connectives Prediction on Implicit Discourse
Relation Recognition
Zhi Min Zhou?, Man Lan?,?, Zheng Yu Niu?, Yu Xu?, Jian Su?
?East China Normal University, Shanghai, PRC.
?Baidu.com Inc., Beijing, PRC.
?Institute for Infocomm Research, Singapore.
51091201052@ecnu.cn, lanman.sg@gmail.com
Abstract
Implicit discourse relation recognition is
difficult due to the absence of explicit
discourse connectives between arbitrary
spans of text. In this paper, we use lan-
guage models to predict the discourse con-
nectives between the arguments pair. We
present two methods to apply the pre-
dicted connectives to implicit discourse
relation recognition. One is to use the
sense frequency of the specific connec-
tives in a supervised framework. The
other is to directly use the presence of the
predicted connectives in an unsupervised
way. Results on PDTB2 show that using
language model to predict the connectives
can achieve comparable F-scores to the
previous state-of-art method. Our method
is quite promising in that not only it has
a very small number of features but also
once a language model based on other re-
sources is trained it can be more adaptive
to other languages and domains.
1 Introduction
Discourse relation analysis involves identifying
the discourse relations (e.g., the comparison re-
lation) between arbitrary spans of text, where
the discourse connectives (e.g., ?however?, ?be-
cause?) may or may not explicitly exist in the text.
This analysis is one important application both as
an end in itself and as an intermediate step in var-
ious downstream NLP applications, such as text
summarization, question answering etc.
As discussed in (Pitler and Nenkova., 2009b),
although explicit discourse connectives may have
two types of ambiguity, i.e., one is discourse or
non-discourse usage (?once? can be either a tem-
poral connective or a word meaning ?formerly?),
the other is discourse relation sense ambiguity
(?since? can serve as either a temporal or causal
connective), their study shows that for explicit
discourse relations in Penn Discourse Treebank
(PDTB) corpus, the most general 4 senses, i.e.,
Comparison (Comp.), Contingency (Cont.), Tem-
poral (Temp.) and Expansion (Exp.), can be eas-
ily addressed by the presence of discourse con-
nectives and a simple method only considering the
sense frequency of connectives can achieve more
than 93% accuracy. This indicates the importance
of connectives for discourse relation recognition.
However, with implicit discourse relation
recognition, there is no connective between the
textual arguments, which results in a very difficult
task. In recent years, a multitude of efforts have
been employed to solve this task. One approach
is to exploit various linguistically informed fea-
tures extracted from human-annotated corpora in
a supervised framework (Pitler et al, 2009a) and
(Lin et al, 2009). Another approach is to perform
recognition without human-annotated corpora by
creating synthetic examples of implicit relations in
an unsupervised way (Marcu and Echihabi, 2002).
Moreover, our initial study on PDTB implicit
relation data shows that the averaged F-score for
the most general 4 senses can reach 91.8% when
we obtain the sense of test examples by map-
ping each implicit connective to its most frequent
sense (i.e., sense recognition using gold-truth im-
plicit connectives). This high F-score performance
again proves that the connectives are very crucial
source for implicit relation recognition.
In this paper, we present a new method to ad-
dress the problem of recognizing implicit dis-
course relation. This method is inspired by the
above observations, especially the two gold-truth
results, which reveals that discourse connectives
are very important signals for discourse relation
recognition. Our basic idea is to recover the im-
plicit connectives (not present in real text) be-
tween two spans of text with the use of a language
139
model trained on large amount of raw data without
any human-annotation. Then we use these pre-
dicted connectives to generate feature vectors in
two ways for implicit discourse relation recogni-
tion. One is to use the sense frequency of the spe-
cific connectives in a supervised framework. The
other is to directly use the presence of the pre-
dicted connectives in an unsupervised way.
We performed evaluation on explicit and im-
plicit relation data sets in the PDTB 2 corpus. Ex-
perimental results showed that the two methods
achieved comparable F-scores to the state-of-art
methods. It indicates that the method using lan-
guage model to predict connectives is very useful
in solving this task.
The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our methods for implicit discourse relation
recognition. Section 4 presents experiments and
results. Section 5 offers some conclusions.
2 Related Work
Existing works on automatic recognition of im-
plicit discourse relations fall into two categories
according to whether the method is supervised or
unsupervised.
Some works perform relation recognition with
supervised methods on human-annotated corpora,
for example, the RST Bank (Carlson et al, 2001)
used by (Soricut and Marcu, 2003), adhoc anno-
tations used by (Girju, 2003) and (Baldridge and
Lascarides, 2005), and the GraphBank (Wolf et al,
2005) used by (Wellner et al, 2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2006) has sig-
nificantly expanded the discourse-annotated cor-
pora available to researchers, using a comprehen-
sive scheme for both implicit and explicit rela-
tions. (Pitler et al, 2009a) performed implicit re-
lation classification on the second version of the
PDTB. They used several linguistically informed
features, such as word polarity, verb classes, and
word pairs, showing performance increases over a
random classification baseline. (Lin et al, 2009)
presented an implicit discourse relation classifier
in PDTB with the use of contextual relations, con-
stituent Parse Features, dependency parse features
and cross-argument word pairs. Although both of
two methods achieved the state of the art perfor-
mance for automatical recognition of implicit dis-
course relations, due to lack of human-annotated
corpora, their approaches are not very useful in the
real word.
Another line of research is to use the unsuper-
vised methods on unhuman-annotated corpus.
(Marcu and Echihabi, 2002) used several pat-
terns to extract instances of discourse relations
such as contrast and elaboration from unlabeled
corpora. Then they used word-pairs between argu-
ments as features for building classification mod-
els and tested their model on artificial data for im-
plicit relations.
Subsequently other studies attempt to ex-
tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered that
Marcu and Echihabi?s models do not perform as
well on implicit relations as one might expect
from the test accuracy on synthetic data. (Gold-
ensohn, 2007) extended the work of (Marcu and
Echihabi, 2002) by refining the training and clas-
sification process using parameter optimization,
topic segmentation and syntactic parsing. (Saito
et al, 2006) followed the method of (Marcu and
Echihabi, 2002) and conducted experiments with
a combination of cross-argument word pairs and
phrasal patterns as features to recognize implicit
relations between adjacent sentences in a Japanese
corpus.
Previous work showed that with the use of some
patterns, structures, or the pairs of words, rela-
tion classification can be performed using unsu-
pervised methods.
In contrast to existing work, we investigated a
new knowledge source, i.e., implicit connectives
predicted using a language model, for implicit re-
lation recognition. Moreover, this method can
be applied in both supervised and unsupervised
ways by generating features on labeled and unla-
beled training data and then performing implicit
discourse connectives recognition.
3 Methodology
3.1 Predicting implicit connectives via a
language model
Previous work (Pitler and Nenkova., 2009b)
showed that with the presence of discourse con-
nectives, explicit discourse relations in PDTB can
be easily identified with more than 90% F-score.
Our initial study on PDTB human-annotated im-
plicit relation data shows that the averaged F-score
for the most general 4 senses can reach 91.8%
when we simply map each implicit connective to
140
its most frequent sense. These high F-scores indi-
cate that the connectives are very crucial source of
information for both explicit and implicit relation
recognition. However, for implicit relations, there
are no explicitly discourse connectives in real text.
This built-in absence makes the implicit relation
recognition task quite difficult. In this work we
overcome this difficulty by inserting connectives
into the two arguments with the use of a language
model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two
arguments, denoted as Arg1 and Arg2. Typi-
cally, there are two possible positions for most
of implicit connectives, i.e., the position before
Arg1 and the position between Arg1 and Arg2.
Given a set of implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as Ppl(Sci,j). According to
the value of Ppl(Sci,j) (the lower the better), we
can rank these sentences and select the connec-
tives in top N sentences as implicit connectives
for this argument pair. Here the language model
may be trained on any large amount of unanno-
tated corpora that can be cheaply acquired. Typi-
cally, a large corpora with the same domain as the
test data will be used for training language model.
Therefore, we chose news corpora, such as North
American News Corpora.
After that, we use the top N predicted connec-
tives to generate different feature vectors and per-
form the classification in two ways. One is to use
the sense frequency of predicted connectives in a
supervised framework. The other is to directly use
the presence of the predicted connectives in an un-
supervised way. The two approaches are described
as follows.
3.2 Using sense frequency of predicted
discourse connectives as features
After the above procedure, we get a sorted set of
predicted discourse connectives. Due to the pres-
ence of an implicit connective, the implicit dis-
course relation recognition task can be addressed
with the methods for explicit relation recognition,
e.g., sense classification based only on connectives
(Pitler et al, 2009a). Inspired by their work, the
first approach is to use sense frequency of pre-
dicted discourse connectives as features. We take
the connective with the lowest perplexity value
(i.e., top 1 connective) as the real connective for
the arguments pair. Then we count the sense
frequency of this connective on the training set.
Figure 1 illustrates the procedure of generating
predicted discourse connective from a language
model and calculating its sense frequency from
training data. Here the calculation of sense fre-
quency of connective is based on the annotated
training data which has labeled discourse rela-
tions, thus this method is a supervised one.
Figure 1: Procedure of generating a predicted dis-
course connective and its sense frequency from the
training set and a language model.
Then we can directly use the sense frequency
to generate a 4-feature vector to perform the clas-
sification. For example, the sense frequency of
the connective but in the most general 4 senses
can be counted from training set as 691, 6, 49,
2, respectively. For a given pair of arguments,
if but is predicted as the top 1 connective based
on a language model, a 4-dimension feature vec-
tor (691, 6, 49, 2) is generated for this pair and
used for training and test procedure. Figure 2
and 3 show the training and test procedure for this
method.
Figure 2: Training procedure for the first ap-
proach.
141
Figure 3: Test procedure for the first approach.
3.3 Using presence or absence of predicted
discourse connective as features
(Pitler et al, 2008) showed that most connectives
are unambiguous and it is possible to obtain high-
accuracy in prediction of discourse senses due to
the simple mapping relation between connectives
and senses. Given two examples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey the Comparison and Contingency
senses respectively. In most cases, we can easily
recognize the relation sense by the appearance of
a discourse connective since it can be interpreted
in only one way. That means the ambiguity of
the mapping between sense and connective is quite
low. Therefore, the second approach is to use only
the presence of the top N predicted discourse con-
nectives to generate a feature vector for a given
pair of arguments.
4 Experiment
4.1 Data sets
We used PDTB as our data set to perform the eval-
uation of our methods. The corpus contains anno-
tations of explicit and implicit discourse relations.
The first evaluation is performed on the annotated
implicit data set. Following the work of (Pitler et
al., 2009a), we used sections 2-20 as the training
set, sections 21-22 as the test set and sections 0-
1 as the development set for parameter optimiza-
tion (e.g., N value). The second evaluation is per-
formed on the annotated explicit data set. We fol-
low the method used in (Sporleder and Lascarides,
2008) to remove the discourse connective from the
explicit instances and consider these processed in-
stances as implicit ones.
We constructed four binary classifiers to recog-
nize each main senses (i.e., Cont., Cont., Exp.,
Temp.) from the rest. For each sense we used
equal numbers of positive and negative instances
in training set. The negative instances were cho-
sen at random from the rest of training set. For
both evaluations all instances in sections 21-22
were used as test set. Table 1 lists the numbers
of positive and negative instances for each sense
in training, development and test sets of implicit
and explicit relation data sets.
4.2 Evaluation and classifier
To evaluate the performance of above systems, we
used two widely-used measures, F-score ( i.e., F1)
and accuracy. In addition, in this work we used
the LIBSVM toolkit to construct four linear SVM
classifiers for each sense.
4.3 Preprocessing
We used the SRILM toolkit to build a language
model and calculated the perplexity value for each
training and test sample. The steps are described
as follows. First, since perplexity is an intrin-
sic score to measure the similarity between train-
ing and test samples, in order to fit the restric-
tion of perplexity we chose 3 widely-used cor-
pora in the Newswire domain to train the language
model, i.e., (1) the New York part of BLLIP North
American News Text (Complete), (2) the Xin and
(3) the Ltw parts of the English Gigaword Fourth
Edition. For the BLLIP corpus with 1,796,386
automatically parsed English sentences, we con-
verted the parsed sentences into original textual
data. Some punctuation marks such as commas,
periods, minuses, right/left parentheses are con-
verted into their original form. For the Xin and
Ltw parts, we only used the Sentence Detector
toolkit in OpenNLP to split each sentence. Finally
we constructed 3-, 4- and 5-grams language mod-
els from these three corpora. Table 2 lists statis-
tics of different n-grams in the different language
models and different corpora.
Next, for each instance we combined its Arg1
and Arg2 with connectives obtained from PDTB.
There are two types of connectives, single con-
nectives (e.g. ?because? and ?but?) and paral-
142
Table 1: Statistics of positive and negative instances for each sense in training, development and test sets
of implicit and explicit relation data sets.
Implicit Explicit
Comp. Cont. Exp. Temp. Comp. Cont. Exp. Temp.
Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663
Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247
Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124
Table 2: Statistics of different n-grams in the dif-
ferent language models and different corpora.
n-gram BLLIP - Gigaword- Gigaword-
New York Xin Ltw
1-gram 1638156 2068538 2276491
2-grams 26156851 23961796 33504873
3-grams 80876435 77799100 101855639
4-grams 127142452 134410879 159791916
5-grams 146454530 168166195 183794771
lel connectives (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of
possible implicit connectives {ci}, for a single
connective ci, we constructed two synthetic sen-
tences, ci+Arg1+Arg2 and Arg1+ci+Arg2. In case
of parallel connectives, we constructed one syn-
thetic sentence like ci,1+Arg1+ci,2+Arg2.
As a result, we obtain 198 synthetic sentences
(|ci| ? 2 for single connective or |ci| for parallel
connective) for each pair of arguments. Then we
converted all words to lower cases and used the
language model trained in the above step to calcu-
late its perplexity (the lower the better) value on
sentence level. The sentences were ranked from
low to high according to their perplexity scores.
For example, given a sentence with arguments pair
as follows:
Arg1: it increased its loan-loss reserves by $93
million after reviewing its loan portfolio,
Arg2: before the loan-loss addition it had operat-
ing profit of $10 million for the quarter.
we got the perplexity (Ppl) values for this argu-
ments pair in combination with two connectives
(but and by comparison) in two positions as fol-
lows:
1. but + Arg1 + Arg2: Ppl= 349.622
2. Arg1 + but + Arg2: Ppl= 399.339
3. by comparison + Arg1 + Arg2: Ppl= 472.206
4. Arg1 + by comparison + Arg2: Ppl= 543.051
In our second approach described in Section
3.3, we considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is,
the presence or absence of the specific connective.
According to the value of Ppl(Sci,j), we tried var-
ious N values on development set to get the opti-
mal N value.
4.4 Results
Table 3 summarizes the best performance
achieved using gold-truth implicit connectives,
the previous state-of-art performance achieved
by (Pitler et al, 2009a) and our approaches.
The first line shows the result by mapping the
gold-truth implicit connectives directly to the
relation?s sense. The second line presents the best
result of (Pitler et al, 2009a). One thing worth
mentioning here is that for the Expansion relation,
(Pitler et al, 2009a) expanded both training and
test sets by including EntRel relation as positive
examples, which makes it impossible to perform
direct comparison. The third and fourth lines
show the best results using our first approach,
where the sense frequency is counted on explicit
and implicit training set respectively. The last line
shows the best result of our second approach only
considering the presence of top N connectives.
Table 4 summarizes the best performance using
gold-truth explicit connectives reported in (Pitler
and Nenkova., 2009b) and our two approaches.
Figure 4 shows the curves of averaged F-scores
on implicit connective classification with differ-
ent n-gram language models. From this figure we
can see that all 4-grams language models achieved
around 0.5% better averaged F-score than 3-grams
models. And except for Ltw corpus, other 5-grams
models achieved lower averaged F-score than 4-
grams models. Specially the 5-grams result of
New York corpus is much lower than its 3-grams
result.
Figure 5 shows the averaged F-scores of dif-
ferent top N on the New York corpus with 3-,
4- and 5-grams language models. The essential
143
Table 3: Best result of implicit relations compared with state-of-art methods.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using
gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)
Best result in (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)
Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)
Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)
Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)
Table 4: Best result of explicit relation conversion to implicit relation compared with results using the
same method.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using gold-truth
explicit connectives in (Pitler et al, 2009a) N/A N/A N/A N/A N/A(93.67)
Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)
Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)
0 10 20 30 40 50 60 70 80 90 100110120130140150160170180190200
30.0
30.5
31.0
31.5
32.0
32.5
33.0
33.5
34.0
34.5
 
 NY 3-gram
 NY 4-gram
 NY 5-gram
Top N value
A
v
e
r
a
g
e
d
 
F
-
S
c
o
r
e
Figure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with different
top N values.
trend of these curves cannot be summarized in
one sentence. But we can see that the best aver-
aged F-scores mostly appeared in the range from
100 ? 160. For 4-grams and 5-grams models, the
system achieved the top averaged F-scores when
N = 20 as well.
4.5 Discussion
Experimental results on PDTB showed that using
predicted connectives achieved the comparable F-
scores of the state-of-art method.
From Table 3 we can find that our results are
closely to the best performance of previous state-
of-art methods in terms of averaged F-score. On
the Comparison sense, our first approach has an
improvement of more than 4% F-score on the pre-
vious state-of-art method (Pitler et al, 2009a). As
we mentioned before, for the Expansion sense,
they included EntRel relation to expand the train-
ing set and test set, which makes it impossible to
perform a direct comparison. Since the positive in-
stances size has been increased by 50%, they may
achieve a higher F-score than our approach. For
other relations, our best performance is slightly
lower than theirs. While bearing in mind that our
approach only uses a very small amount of fea-
tures for implicit relation recognition. Compared
144
3-gram 4-gram 5-gram
31.0
31.2
31.4
31.6
31.8
32.0
32.2
32.4
32.6
 
 New York
 Xin
 Ltw
n-gram
A
v
e
r
a
g
e
d
 
F
-
s
c
o
r
e
Figure 4: Curves of averaged F-score on implicit
connective classification with n-Gram language
model.
with other approaches involving thousands of fea-
tures, our method is quite promising.
From Table 4 we observe comparable averaged
F-score (39.98% F-score) on explicit relation data
set to that on implicit relation data set. Previ-
ously, (Sporleder and Lascarides, 2008) also used
the same conversion method to perform implicit
relation recognition on different corpora and their
best result is around 33.69% F-score. Although
the two results cannot be compared directly due to
different data sets, the magnitude of performance
quantities is comparable and reliable.
By comparing with the above different systems,
we find several useful observations. First, our
method using predicted implicit connectives via a
language model can help the task of implicit dis-
course relation recognition. The results are com-
parable to the previous state-of-art studies. Sec-
ond, our method has a lot of advantages, i.e., a
very small amount of features (several or no more
than 200 vs. ten thousand), easy computation
(only based on the trained language model vs. us-
ing a lot of NLP tools to extract a large amount of
linguistically informed features) and fast running,
which makes it more practical in real world appli-
cation. Furthermore, since the language model can
be trained on many corpora whether annotated or
unannotated, this method is more adaptive to other
languages and domains.
5 Conclusions
In this paper we have presented an approach to
implicit discourse relation recognition using pre-
dicted implicit connectives via a language model.
The predicted connectives have been used for im-
plicit relation recognition in two ways, i.e., super-
vised and unsupervised framework. Results on the
Penn Discourse Treebank 2.0 show that the pre-
dicted discourse connectives can help implicit re-
lation recognition and the two algorithms achieve
comparable F-scores with the state-of-art method.
In addition, this method is quite promising due to
its simple, easy to retrieve, fast run and increased
adaptivity to other languages and domains.
Acknowledgments
We thank the reviewers for their helpful com-
ments and Jonathan Ginzburg for his mentor-
ing. This work is supported by grants from
National Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, College
Park, MD,2001.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL 2003 Workshops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics.
145
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.
Lee, A. Joshi. 2008. Easily Identifiable Dis-
course Relations. Coling 2008: Companion vol-
ume: Posters.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. Proceedings of the COLING/ACL Work-
shop on Sentiment and Subjectivity in Text.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. Proceedings of the Human Language Technol-
ogy and North American Association for Computa-
tional Linguistics Conference.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
146
