Robustness Issues in a Data-Driven Spoken Language Understanding
System
Yulan He and Steve Young
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, England
{yh213, sjy}@eng.cam.ac.uk
Abstract
Robustness is a key requirement in spoken lan-
guage understanding (SLU) systems. Human
speech is often ungrammatical and ill-formed,
and there will frequently be a mismatch be-
tween training and test data. This paper dis-
cusses robustness and adaptation issues in a
statistically-based SLU system which is en-
tirely data-driven. To test robustness, the sys-
tem has been tested on data from the Air Travel
Information Service (ATIS) domain which has
been artificially corrupted with varying levels
of additive noise. Although the speech recog-
nition performance degraded steadily, the sys-
tem did not fail catastrophically. Indeed, the
rate at which the end-to-end performance of
the complete system degraded was significantly
slower than that of the actual recognition com-
ponent. In a second set of experiments, the
ability to rapidly adapt the core understanding
component of the system to a different appli-
cation within the same broad domain has been
tested. Using only a small amount of training
data, experiments have shown that a semantic
parser based on the Hidden Vector State (HVS)
model originally trained on the ATIS corpus
can be straightforwardly adapted to the some-
what different DARPA Communicator task us-
ing standard adaptation algorithms. The paper
concludes by suggesting that the results pre-
sented provide initial support to the claim that
an SLU system which is statistically-based and
trained entirely from data is intrinsically robust
and can be readily adapted to new applications.
1 Introduction
Spoken language is highly variable as different people
use different words and sentence structures to convey the
same meaning. Also, many utterances are grammatically-
incorrect or ill-formed. It thus remains an open issue as to
how to provide robustness for large populations of non-
expert users in spoken dialogue systems. The key compo-
nent of a spoken language understanding (SLU) system is
the semantic parser, which translates the users? utterances
into semantic representations. Traditionally, most seman-
tic parser systems have been built using hand-crafted se-
mantic grammar rules and so-called robust parsing (Ward
and Issar, 1996; Seneff, 1992; Dowding et al, 1994) is
used to handle the ill-formed user input in which word
patterns corresponding to semantic tokens are used to fill
slots in different semantic frames in parallel. The frame
with the highest score then yields the selected semantic
representation.
Formally speaking, the robustness of language (recog-
nition, parsing, etc.) is a measure of the ability of hu-
man speakers to communicate despite incomplete infor-
mation, ambiguity, and the constant element of surprise
(Briscoe, 1996). In this paper, two aspects of SLU sys-
tem performance are investigated: noise robustness and
adaptability to different applications. For the former, we
expect that an SLU system should maintain acceptable
performance when given noisy input speech data. This
requires, the understanding components of the SLU sys-
tem to be able to correctly interpret the meaning of an
utterance even when faced with recognition errors. For
the latter, the SLU system should be readily adaptable to
a different application using a relatively small set (e.g.
less than 100) of adaptation utterances.
The rest of the paper is organized as follows. An
overview of our data-driven SLU system is outlined in
section 2. Experimental results on performance under a
range of SNRs are then presented in section 3. Section 4
discusses adaptation of the HVS model to new applica-
tions. Finally, section 5 concludes the paper.
2 System Overview
Spoken language understanding (SLU) aims to interpret
the meanings of users? utterances and respond reason-
ably to what users have said. A typical architecture of
an SLU system is given in Fig. 1, which consists of a
speech recognizer, a semantic parser, and a dialog act de-
coder. Within a statistical framework, the SLU problem
can be factored into three stages. First the speech recog-
nizer recognizes the underlying word string W from each
input acoustic signal A, i.e.
W? = argmax
W
P (W |A) = argmax
W
P (A|W )P (W ) (1)
then the semantic parser maps the recognized word string
W? into a set of semantic concepts C
C? = argmax
C
P (C|W? ) (2)
and finally the dialogue act decoder infers the user?s dia-
log acts or goals by solving
G?u = argmax
Gu
P (Gu|C?) (3)
Dialog Act
Decoder
Semantic
Parser
Speech
Recognizer
Acoustic Signal Words Concepts User?s Dialog Acts
PSfrag replacements
A W C Gu
Figure 1: Typical structure of a spoken language under-
standing system.
The sequential decoding described above is suboptimal
since the solution at each stage depends on an exact so-
lution to the previous stage. To reduce the effect of this
approximation, a word lattice or N -best word hypotheses
can be retained instead of the single best string W? as the
output of the speech recognizer. The semantic parse re-
sults may then be incorporated with the output from the
speech recognizer to rescore the N -best list as below.
C?, W? ? argmax
C,W?LN
P (A|W )P (W )P (C|W )
? argmax
C,W?LN
P (A|W )P (W )?P (C|W )? (4)
where P (A|W ) is the acoustic probability from the
first pass, P (W ) is the language modelling likelihood,
P (C|W ) is the semantic parse score, LN denotes the N -
best list, ? is a semantic parse scale factor, and ? is a
grammar scale factor.
In the system described in this paper, each of these
stages is modelled separately. We use a standard HTK-
based (HTK, 2004) Hidden Markov Model (HMM) rec-
ognizer for recognition, the Hidden Vector State (HVS)
model for semantic parsing (He and Young, 2003b), and
Tree-Augmented Naive Bayes networks (TAN) (Fried-
man et al, 1997) for dialog act decoding.
The speech recognizer comprises 14 mixture Gaus-
sian HMM state-clustered cross-word triphones aug-
mented by using heteroscedastic linear discriminant anal-
ysis (HLDA) (Kumar, 1997). Incremental speaker adap-
tation based on the maximum likelihood linear regression
(MLLR) method (Gales and Woodland, 1996) was per-
formed during the test with updating being performed in
batches of five utterances per speaker.
The Hidden Vector State (HVS) model (He and Young,
2003b) is a hierarchical semantic parser which associates
each state of a push-down automata with the state of a
HMM. State transitions are factored into separate stack
pop and push operations and then constrained to give
a tractable search space. The result is a model which
is complex enough to capture hierarchical structure but
which can be trained automatically from unannotated
data.
CITY DATE
SS
SE
TOLOC ON
RETURN
SS
DUMMY
SS DUMMY
SS SS
Dallassent_start I want to return to
RETURN
SS
RETURN
TOLOC
CITY
SS
RETURN
TOLOC
RETURN
RETURN
SS
ON
DATE
Thursday sent_end
SE
SS
ON
on
Figure 2: Example of a parse tree and its vector state
equivalent.
Let each state at time t be denoted by a vector of Dt
semantic concept labels (tags) ct = [ct[1], ct[2], ..ct[Dt]]
where ct[1] is the preterminal concept and ct[Dt] is the
root concept (SS in Figure 2). Given a word sequence
W , concept vector sequence C and a sequence of stack
pop operations N , the joint probability of P (W,C, N)
can be decomposed as
P (W,C, N) =
T
?
t=1
P (nt|ct?1) ?
P (ct[1]|ct[2 ? ? ?Dt]) ? P (wt|ct) (5)
where ct at word position t is a vector of Dt semantic
concept labels (tags), nt is the vector stack shift operation
and takes values in the range 0, ? ? ? , Dt?1 where Dt?1 is
the stack size at word position t ? 1, and ct[1] = cwt is
the new preterminal semantic tag assigned to word wt at
word position t.
Thus, the HVS model consists of three types of proba-
bilistic move:
1. popping semantic tags off the stack;
2. pushing a pre-terminal semantic tag onto the stack;
3. generating the next word.
The dialog act decoder was implemented using the
Tree-Augmented Naive Bayes (TAN) algorithm (Fried-
man et al, 1997), which is an extension of Naive Bayes
Networks. One TAN was used for each dialogue act or
goal Gu, the semantic concepts Ci which serve as input
to its corresponding TAN were selected based on the mu-
tual information (MI) between the goal and the concept.
Naive Bayes networks assume all the concepts are con-
ditionally independent given the value of the goal. TAN
networks relax this independence assumption by adding
dependencies between concepts based on the conditional
mutual information (CMI) between concepts given the
goal. The goal prior probability P (Gu) and the condi-
tional probability of each semantic concept Ci given the
goal Gu, P (Ci|Gu) are learned from the training data.
Dialogue act detection is done by picking the goal with
the highest posterior probability of Gu given the particu-
lar instance of concepts C1 ? ? ?Cn, P (Gu|C1 ? ? ?Cn).
3 Noise Robustness
The ATIS corpus which contains air travel information
data (Dahl et al, 1994) has been chosen for the SLU sys-
tem development and evaluation. ATIS was developed
in the DARPA sponsored spoken language understanding
programme conducted from 1990 to 1995 and it provides
a convenient and well-documented standard for measur-
ing the end-to-end performance of an SLU system. How-
ever, since the ATIS corpus contains only clean speech,
corrupted test data has been generated by adding samples
of background noise to the clean test data at the waveform
level.
3.1 Experimental Setup
The experimental setup used to evaluate the SLU system
was similar to that described in (He and Young, 2003a).
As mentioned in section 2, the SLU system consists of
three main components, a standard HTK-based HMM
recognizer, the HVS semantic parser, and the TAN dia-
logue act (DA) decoder. Each of the three major compo-
nents are trained separately. The acoustic speech signal
in the ATIS training data is modelled by extracting 39
features every 10ms: 12 cepstra, energy, and their first
and second derivatives. This data is then used to train the
speaker-independent, continuous speech recognizer. The
HVS semantic parser is trained on the unannotated utter-
ances using EM constrained by the domain-specific lex-
ical class information and the dominance relations built
into the abstract annotations (He and Young, 2003b). In
the case of ATIS, the lexical classes can be extracted au-
tomatically from the relational database, whilst abstract
semantic annotations for each utterance are automatically
derived from the accompanying SQL queries of the train-
ing utterances. The dialogue act decoder is trained using
the main topics or goals and the key semantic concepts
extracted automatically from the reference SQL queries
Performance is measured at both the component and
the system level. For the former, the recognizer is eval-
uated by word error rate, the parser by concept slot re-
trieval rate using an F-measure metric (Goel and Byrne,
1999), and the dialog act decoder by detection rate. The
overall system performance is measured using the stan-
dard NIST ?query answer? rate.
In the expriments reported here, car noise from the
NOISEX-92 (Varga et al, 1992) database was added to
the ATIS-3 NOV93 and DEC94 test sets. In order to ob-
tain different SNRs, the noise was scaled accordingly be-
fore adding to the speech signal.
3.2 Experimental Results
Robust spoken language understanding components
should be able to compensate for the weakness of the
speech recognizer. That is, ideally they should be capable
of generating the correct meaning of an utterance even
if it is recognized wrongly by a speech recognizer. At
minimum, the performance of the understanding compo-
nents should degrade gracefully as recognition accuracy
degrades.
Figure 3 gives the system performance on the cor-
rupted test data with additive noise ranging from 25dB to
10dB SNR. The label ?clean? in the X-axis denotes the
original clean speech data without additive noise. Note
that the recognition results on the corrupted test data
were obtained directly using the original clean speech
HMM models without retraining for the noisy condi-
tions. The upper portion of Figure 3 shows the end-to-
end performance in terms of query answer error rate for
the NOV93 and DEC94 test sets. For easy reference,
WER is also shown. The individual component perfor-
mance, F-measure for the HVS semantic parser and di-
alogue act (DA) detection accuracy for the DA decoder,
are illustrated in the lower portion of Figure 3. For each
test set, the performance on the rescored word hypothe-
ses is given as well. This incorporates the semantic parse
scores into the acoustic and language modelling likeli-
hoods to rescore the 25-best word lists from the speech
recognizer.
It can be observed that the system gives fairly stable
performance at high SNRs and then the recognition accu-
racy degrades rapidly in the presence of increasing noise.
At 20dB SNR, the WER for the NOV93 test set increases
by 1.6 times relative to clean whilst the query answer
error rate increases by only 1.3 times. On decreasing
the SNR to 15dB, the system performance degrades sig-
nificantly. The WER increases by 3.1 times relative to
clean but the query answer error rate increases by only
1.7 times. Similar figures were obtained for the DEC94
test set.
The above suggests that the end-to-end performance
measured in terms of answer error rate degrades more
slowly compared to the recognizer WER as the noise
level increases. This demonstrates that the statistically-
based understanding components of the SLU system, the
semantic parser and the dialogue act decoder, are rela-
tively robust to degrading recognition performance.
Regarding the individual component performance, the
dialogue act detection accuracy appears to be less sensi-
tive to decreasing SNR. This is probably a consequence
of the fact that the Bayesian networks are set up to re-
spond to only the presence or absence of semantic con-
cepts or slots, regardless of the actual values assigned to
them. In another words, the performance of the dialogue
act decoder is not affected by the mis-recognition of indi-
vidual words, but only by a failure to detect the presence
of a semantic concept. It can also be observed from Fig-
ure 3 that the F-measure needs to be better than 85% in
order to achieve acceptable end-to-end performance.
4 Adaptation to New Applications
Statistical model adaptation techniques are widely used
to reduce the mismatch between training and test or to
adapt a well-trained model to a novel domain. Com-
monly used techniques can be classified into two cat-
egories, Bayesian adaptation which uses a maximum a
posteriori (MAP) probability criteria (Gauvain and Lee,
1994) and transformation-based approaches such as max-
imum likelihood linear regression (MLLR) (Gales and
Woodland, 1996), which uses a maximum likelihood
(ML) criteria. In recent years, MAP adaptation has been
successfully applied to n-gram language models (Bac-
chiani and Roark, 2003) and lexicalized PCFG models
(Roark and Bacchiani, 2003). Luo et al have proposed
transformation-based approaches based on the Markov
transform (Luo et al, 1999) and the Householder trans-
form (Luo, 2000), to adapt statistical parsers. However,
the optimisation processes for the latter are complex and
it is not clear how general they are.
Since MAP adaptation is straightforward and has been
applied successfully to PCFG parsers, it has been selected
for investigation in this paper. Since one of the special
forms of MAP adaptation is interpolation between the in-
domain and out-of-domain models, it is natural to also
consider the use of non-linear interpolation and hence this
has been studied as well 1.
1Experiments using linear interpolation have also been con-
ducted but it was found that the results are worse than those
4.1 MAP Adaptation
Bayesian adaptation reestimates model parameters di-
rectly using adaptation data. It can be implemented via
maximum a posteriori (MAP) estimation. Assuming that
model parameters are denoted by ?, then given observa-
tion samples Y , the MAP estimate is obtained as
?MAP = argmax
?
P (?|Y ) = argmax
?
P (Y |?)P (?)
(6)
where P (Y |?) is the likelihood of the adaptation data Y
and model parameters ? are random vectors described by
their probabilistic mass function (pmf) P (?), also called
the prior distribution.
In the case of HVS model adaptation, the objective is to
estimate probabilities of discrete distributions over vector
state stack shift operations and output word generation.
Assuming that they can be modelled under the multino-
mial distribution, for mathematical tractability, the con-
jugate prior, the Dirichlet density, is normally used. As-
sume a parser model P (W,C) for a word sequence W
and semantic concept sequence C exists with J compo-
nent distributions Pj each of dimension K, then given
some adaptation data Wl, the MAP estimate of the kth
component of Pj , P?j(k), is
P?j(k) =
?j
?j + ?
P?j(k) +
?
?j + ?
Pj(k) (7)
where ?j =
?K
k=1 ?j(k) in which ?j(k) is defined as the
total count of the events associated with the kth compo-
nent of Pj summed across the decoding of all adaptation
utterances Wl, ? is the prior weighting parameter, Pj(k)
is the probability of the original unadapted model, and
P?j(k) is the empirical distribution of the adaptation data,
which is defined as
P?j(k) =
?j(k)
?K
i=1 ?j(i)
(8)
As discussed in section 2, the HVS model consists of
three types of probabilistic move. The MAP adaptation
technique can be applied to the HVS model by adapting
each of these three component distributions individually.
4.2 Log-Linear Interpolation
Log-linear interpolation has been applied to language
model adaptation and has been shown to be equivalent
to a constrained minimum Kullback-Leibler distance op-
timisation problem(Klakow, 1998).
Following the notation introduced in section 4.1, where
Pj(k) is the probability of the original unadapted model,
and P?j(k) is the empirical distribution of the adaptation
obtained using MAP adaptation or log-linear interpolation.
clean 25dB 20dB 15dB 10dB
3.5
8.5
13.5
18.5
23.5
28.5
33.5
38.5
43.5
Speech to Noise Ratio ? SNR (NOV93 Test Set)
Sp
ok
en
 L
an
gu
ag
e 
Un
de
rs
ta
nd
in
g 
Er
ro
r R
at
e 
(%
)
WER                        
WER with Rescoring         
Answer Error               
Answer Error with Rescoring
(a) NOV93 End-to-End Performance
clean 25dB 20dB 15dB 10dB
2.5
7.5
12.5
17.5
22.5
27.5
32.5
Speech to Noise Ratio ? SNR (DEC94 Test Set)
Sp
ok
en
 L
an
gu
ag
e 
Un
de
rs
ta
nd
in
g 
Er
ro
r R
at
e 
(%
)
WER                        
WER with Rescoring         
Answer Error               
Answer Error with Rescoring
(c) DEC94 End-to-End Performance
clean 25dB 20dB 15dB 10dB
0.7
0.75
0.8
0.85
0.9
0.95
Speech to Noise Ratio ? SNR (NOV93 Test Set)
F?
m
ea
su
re
 a
nd
 D
A 
De
te
ct
io
n 
Ac
cu
ra
cy
F?measure                           
F?measure with Rescoring            
DA Detection Accuracy               
DA Detection Accuracy with Rescoring
(b) NOV93 Component Performance
clean 25dB 20dB 15dB 10dB
0.82
0.84
0.86
0.88
0.9
0.92
Speech to Noise Ratio ? SNR (DEC94 Test Set)
F?
m
ea
su
re
 a
nd
 D
A 
De
te
ct
io
n 
Ac
cu
ra
cy
F?measure                           
F?measure with Rescoring            
DA Detection Accuracy               
DA Detection Accuracy with Rescoring
(d) DEC94 Component Performance
Figure 3: SLU system performance vs SNR.
data, denote the final adapted model probability as P?j(k).
It is assumed that the Kullback-Leibler distance of the
adapted model to the unadapted and empirically deter-
mined model is
D(P?j(k) ? Pj(k)) = d1 (9)
D(P?j(k) ? P?j(k)) = d2 (10)
Given an additional model probability P?j(k) whose
distance to P?j(k) should be kept small, and introducing
Lagrange multipliers ??1 and ?
?
2 to ensure that constraints
9 and 10 are satistifed, yields
D = D(P?j(k) ? P?j(k))+?
?
1(D(P?j(k) ? Pj(k))?d1)
+ ??2(D(P?j(k) ? P?j(k)) ? d2) (11)
Minimizing D with respect to P?j(k) yields the required
distribution.
With some manipulation and redefinition of the La-
grange Multipliers, it can be shown that
P?j(k) =
1
Z?
Pj(k)?1 P?j(k)?2 (12)
where P?j(k) has been assumed to be a uniform distribu-
tion which is then absorbed into the normalization term
Z?.
The computation of Z? is very expensive and can usu-
ally be dropped without significant loss in performance
(Martin et al, 2000). For the other parameters, ?1 and
?2, the generalized iterative scaling algorithm or the sim-
plex method can be employed to estimate their optimal
settings.
4.3 Experiments
To test the portability of the statistical parser, the initial
experiments reported here are focussed on assessing the
adaptability of the HVS model when it is tested in a do-
main which covers broadly similar concepts, but com-
prises rather different speaking styles. To this end, the
flight information subset of the DARPA Communicator
Travel task has been used as the target domain (CUD-
ata, 2004). By limiting the test in this way, we ensure
that the dimensionalities of the HVS model parameters
remain the same and no new semantic concepts are intro-
duced by the adaptation training data.
The baseline HVS parser was trained on the ATIS
corpus using 4978 utterances selected from the context-
independent (Class A) training data in the ATIS-2 and
ATIS-3 corpora. The vocabulary size of the ATIS training
corpus is 611 and there are altogether 110 semantic con-
cepts defined. The parser model was then adapted using
utterances relating to flight reservation from the DARPA
Communicator data. Although the latter bears similari-
ties to the ATIS data, it contains utterances of a different
style and is often more complex. For example, Commu-
nicator contains utterances on multiple flight legs, infor-
mation which is not available in ATIS.
To compare the adapted ATIS parser with an in-domain
Communicator parser, a HVS model was trained from
scratch using 10682 Communicator training utterances.
The vocabulary size of the in-domain Communicator
training data is 505 and a total of 99 semantic concepts
have been defined. For all tests, a set of 1017 Communi-
cator test utterances was used.
Table 1 lists the recall, precision, and F-measure re-
sults obtained when tested on the 1017 utterance DARPA
Communicator test set. The baseline is the unadapted
HVS parser trained on the ATIS corpus only. The in-
domain results are obtained using the HVS parser trained
solely on the 10682 DARPA training data. The other rows
of the table give the parser performance using MAP and
log-linear interpolation based adaptation of the baseline
model using 50 randomly selected adaptation utterances.
System Recall Precision F-measure
Baseline 79.81% 87.14% 83.31%
In-domain 87.18% 91.89% 89.47%
MAP 86.74% 91.07% 88.85%
Log-Linear 86.25% 92.35% 89.20%
Table 1: Performance comparison of adaptation using
MAP or log-linear interpolation.
Since we do not yet have a reference database for the
DARPA Communicator task, it is not possible to conduct
the end-to-end performance evaluation as in section 3.
However, the experimental results in section 3.2 indi-
cate that the F-measure needs to exceed 85% to give ac-
ceptable end-to-end performance (see Figure 3). There-
fore, it can be inferred from Table 1 that the unadapted
ATIS parser model would perform very badly in the new
Communicator application whereas the adapted models
would give performance close to that of a fully trained
in-domain model.
Figure 4 shows the parser performance versus the num-
ber of adaptation utterances used. It can be observed that
when there are only a few adaptation utterances, MAP
adaptation performs significantly better than log-linear
interpolation. However above 25 adaptation utterances,
the converse is true. The parser performance saturates
when the number of adaptation utterances reaches 50 for
both techniques and the best performance overall is given
by the parser adapted using log-linear interpolation. The
performance of both models however degrades when the
number of adaptation utterances exceeds 100, possibly
due to model overtraining. For this particular application,
we conclude that just 50 adaptation utterances would be
sufficient to adapt the baseline model to give comparable
results to the in-domain Communicator model.
0 25 50 75 100 125 150
0.82
0.84
0.86
0.88
0.9
Adaptation Training Utterance Number
F?
m
ea
su
re
MAP       
Log?Linear
Figure 4: F-measure vs amount of adaptation training
data.
5 Conclusions
The spoken language understanding (SLU) system dis-
cussed in this paper is entirely statistically based. The
recogniser uses a HMM-based acoustic model and an n-
gram language model, the semantic parser uses a hid-
den vector state model and the dialogue act decoder uses
Bayesian networks. The system is trained entirely from
data and there are no heuristic rules. One of the major
claims motivating the design of this type of system is
that its fully-statistical framework makes it intrinsically
robust and readily adaptable to new applications. The
aim of this paper has been to investigate this claim ex-
perimentally via two sets of experiments using a system
trained on the ATIS corpus.
In the first set of experiments, the acoustic test data
was corrupted with varying levels of additive car noise.
The end-to-end system performance was then measured
along with the individual component performances. It
was found that although the addition of noise had a sub-
stantial effect on the word error rate, its relative influ-
ence on both the semantic parser slot/value retrieval rate
and the dialogue act detection accuracy was somewhat
less. Overall, the end-to-end error rate degraded rela-
tively more slowly than word error rate and perhaps most
importantly of all, there was no catastrophic failure point
at which the system effectively stops working, a situation
not uncommon in current rule-based systems.
In the second set of experiments, the ability of the se-
mantic decoder component to be adapted to another ap-
plication was investigated. In order, to limit the issues to
parameter mismatch problems, the new application cho-
sen (Communicator) covered essentially the same set of
concepts but was a rather different corpus with different
user speaking styles and different syntactic forms. Over-
all, we found that moving a system trained on ATIS to
this new application resulted in a 6% absolute drop in F-
measure on concept accuracy (i.e. a 62% relative increase
in parser error) and by extrapolation with the results in
the ATIS domain, we infer that this would make the non-
adapted system essentially unusable in the new applica-
tion. However, when adaptation was applied using only
50 adaptation sentences, the loss of concept accuracy was
mostly restored. Specifically, using log-linear adapta-
tion, the out-of-domain F-measure of 83.3% was restored
to 89.2% which is close to the in-domain F-measure of
89.5%.
Although these tests are preliminary and are based on
off-line corpora, the results do give positive support to
the initial claim made for statistically-based spoken lan-
guage systems, i.e. that they are robust and they are read-
ily adaptable to new or changing applications.
Acknowledgements
The authors would like to thank Mark Gales for providing
the software to generate the corrupted speech data with
additive noise.
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Proc. of the IEEE Intl.
Conf. on Acoustics, Speech and Signal Processing,
Hong Kong, Apr.
T. Briscoe. 1996. Robust parsing. In R. Cole, J. Mariani,
H. Uszkoreit, A. Zaenen, and V. Zue, editors, Survey
of the State of the Art of Human Language Technology,
chapter 3.7. Cambridge University Press, Cambridge,
England.
CUData, 2004. DARPA Communicator Travel
Data. University of Colorado at Boulder.
http://communicator.colorado.edu/phoenix.
D.A. Dahl, M. Bates, M. Brown, K. Hunicke-Smith,
D. Pallett, C. Pao, A. Rudnicky, and L. Shriberg. 1994.
Expanding the scope of the ATIS task: the ATIS-3
corpus. In ARPA Human Language Technology Work-
shop, Princeton, NJ, Mar.
J. Dowding, R. Moore, F. Andry, and D. Moran.
1994. Interleaving syntax and semantics in an efficient
bottom-up parser. In Proc. of the 32nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 110?116, Las Cruces, New Maxico, June.
N. Friedman, D. Geiger, and M. Goldszmidt. 1997.
Bayesian network classifiers. Machine Learning,
29(2):131?163.
M.J. Gales and P.C. Woodland. 1996. Mean and variance
adaptation within the MLLR framework. Computer
Speech and Language, 10:249?264, Oct.
J.L. Gauvain and C.-H. Lee. 1994. Maximum a poste-
riori estimation for multivariate Gaussian mixture ob-
servations of Markov chains. IEEE Trans. on Speech
and Audio Processing, 2(2):291?298.
V. Goel and W. Byrne. 1999. Task dependent loss
functions in speech recognition: Application to named
entity extraction. In ESCA ETRW Workshop on Ac-
cessing Information from Spoken Audio, pages 49?53,
Cambridge, UK.
Yulan He and Steve Young. 2003a. A data-driven spoken
language understanding system. In IEEE Automatic
Speech Recognition and Understanding Workshop, St.
Thomas, U.S. Virgin Islands, Dec.
Yulan He and Steve Young. 2003b. Hidden vector state
model for hierarchical semantic parsing. In Proc. of
the IEEE Intl. Conf. on Acoustics, Speech and Signal
Processing, Hong Kong, Apr.
HTK, 2004. Hidden Markov Model Toolkit (HTK)
3.2. Cambridge University Engineering Department.
http://htk.eng.cam.ac.uk/.
D. Klakow. 1998. Log-linear interpolation of language
models. In Proc. of Intl. Conf. on Spoken Language
Processing, Sydney, Australia, Nov.
N. Kumar. 1997. Investigation of Silicon Auditory Mod-
els and Generalization of Linear Discriminant analysis
for Improved Speech Recognition. Ph.D. thesis, Johns
Hopkins University, Baltimore MD.
X. Luo, S. Roukos, and T. Ward. 1999. Unsupervised
adaptation of statistical parsers based on Markov trans-
form. In IEEE Automatic Speech Recognition and Un-
derstanding Workshop, Keystone, Colorado, Dec.
X. Luo. 2000. Parser adaptation via householder trans-
form. In Proc. of the IEEE Intl. Conf. on Acoustics,
Speech and Signal Processing, Istanbul, Turkey, June.
S. Martin, A. Kellner, and T. Portele. 2000. Interpolation
of stochastic grammar and word bigram models in nat-
ural language understanding. In Proc. of Intl. Conf. on
Spoken Language Processing, Beijing, China, Oct.
B. Roark and M. Bacchiani. 2003. Supervised and unsu-
pervised PCFG adaptation to novel domains. In Pro-
ceedings of the joint meeting of the North American
Chapter of the Association for Computational Linguis-
tics and the Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
S. Seneff. 1992. Robust parsing for spoken language
systems. In Proc. of the IEEE Intl. Conf. on Acoustics,
Speech and Signal Processing, San Francisco.
A.P. Varga, H.J.M. Steeneken, M. Tomlinson, and
D. Jones. 1992. The NOISEX-92 study on the ef-
fect of additive noise on automatic speech recognition.
Technical report, DRA Speech Research Unit.
W. Ward and S. Issar. 1996. Recent improvements in the
CMU spoken language understanding system. In Proc.
of the ARPA Human Language Technology Workshop,
pages 213?216. Morgan Kaufman Publishers, Inc.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1113?1120
Manchester, August 2008
A Hybrid Generative/Discriminative Framework to Train a Semantic
Parser from an Un-annotated Corpus
Deyu Zhou and Yulan He
Information Research Centre
The University of Reading
Reading, RG6 6BX, UK
d.zhou@rdg.ac.uk, y.he@rdg.ac.uk
Abstract
We propose a hybrid genera-
tive/discriminative framework for se-
mantic parsing which combines the hidden
vector state (HVS) model and the hidden
Markov support vector machines (HM-
SVMs). The HVS model is an extension of
the basic discrete Markov model in which
context is encoded as a stack-oriented
state vector. The HM-SVMs combine the
advantages of the hidden Markov models
and the support vector machines. By
employing a modified K-means clustering
method, a small set of most representative
sentences can be automatically selected
from an un-annotated corpus. These
sentences together with their abstract an-
notations are used to train an HVS model
which could be subsequently applied on
the whole corpus to generate semantic
parsing results. The most confident
semantic parsing results are selected to
generate a fully-annotated corpus which is
used to train the HM-SVMs. The proposed
framework has been tested on the DARPA
Communicator Data. Experimental results
show that an improvement over the base-
line HVS parser has been observed using
the hybrid framework. When compared
with the HM-SVMs trained from the fully-
annotated corpus, the hybrid framework
gave a comparable performance with only
a small set of lightly annotated sentences.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
Semantic parsing maps the natural language sen-
tences to complete formal meaning representa-
tions. Traditionally, research in the field of se-
mantic parsing can be divided into two categories:
rule-based approaches and statistical approaches.
Based on hand-crafted semantic grammar rules,
rule-based approaches fill slots in semantic frames
using word pattern and semantic tokens (Dowding
et al, 1994; Ward and Issar, 1994). Such rule-
based approaches are typically domain-specific
and often fragile. Statistical approaches are gen-
erally based on stochastic models. They can be
further categorized into three types: generative
approaches, discriminative approaches and a hy-
brid of the two. Generative approaches learn the
joint probability model, P (W,C), of input sen-
tence W and its semantic tag sequence C, com-
pute P (C|W ) using the Bayes rule, and then take
the most probable tag sequence C. The hidden
Morkov model (HMM), being a generative model,
has been predominantly used in statistical seman-
tic parsing. It models sequential dependencies by
treating a semantic parse sequence as a Markov
chain, which leads to an efficient dynamic pro-
gramming formulation for inference and learning.
The hidden vector state (HVS) model (He and
Young, 2005) is a discrete HMM model in which
each HMM state represents the state of a push-
down automaton with a finite stack size. State
transitions are factored into separate stack pop
and push operations constrained to give a tractable
search space. The result is a model which is
complex enough to capture hierarchical structure
but which can be trained automatically from only
lightly annotated data. Discriminative approaches
directly model posterior probability P (C|W ) and
1113
learn mappings from W to C. One representa-
tive example is support vector machines (SVMs)
(Vapnik, 1995). More recently, the hidden Markov
support vector machines (HM-SVMs) (Altun et al,
2003) have been proposed which combine the flex-
ibility of kernel methods with the idea of HMMs to
predict a label sequence given an input sequence.
However, HM-SVMs require full annotated cor-
pora for training which are difficult to obtain in
practical applications. On the other hand, the HVS
model can be easily trained from only lightly an-
notated corpora. It is thus interesting to explore the
feasibility to combine the advantages of the HVS
model and the HM-SVMs.
We propose a hybrid generative/discriminative
framework here where a modified K-means clus-
tering method is first applied to select a small
set of the most representative sentences automat-
ically from an un-annotated corpus. These sen-
tences together with their abstract annotations are
used to train an HVS model which could be sub-
sequently applied on the whole corpus to generate
semantic parsing results. The most confident se-
mantic parsing results are selected to generate a
fully-annotated corpus which is used to train the
HM-SVMs. Experimental results show that an im-
provement over the baseline HVS parser has been
achieved using the hybrid framework. When com-
pared with the HM-SVMs trained from the fully-
annotated corpus, the hybrid framework gave a
comparable performance with only a small set of
lighted annotated sentences.
The rest of this paper is organized as follows.
Section 2 reviews other proposed hybrid gener-
ative/discriminative frameworks in recent years.
Section 3 briefly describes the HVS model and
the HM-SVMs followed by the presentation of the
proposed hybrid framework. In Section 4, exper-
imental setup and results are discussed. Finally,
Section 5 concludes the paper.
2 Related Work
Combination of generative and discriminative
models for data classification has recently attracted
much interests in the machine learning community.
It has been shown theoretically and experimentally
in (Jaakkola and Haussler, 1998; Ng and Jordan,
2002; Bouchard and Triggs, 2004) that the hy-
brid model combines the complementary powers
of the both models. The first extensive study on hy-
brid models were discussed in (Jaakkola and Haus-
sler, 1998) where discriminative features were ex-
tracted using generative models and were later
used in discriminative models. More recently, the
HM-SVMs (Altun et al, 2003) have been proposed
which incorporate kernel methods into HMMs to
predict a label sequence given an input sequence.
There have also been several studies on explor-
ing the hybrid generative/discriminative frame-
works which combine the generative and discrim-
inative models in a pipelined way. One exam-
ple is the hybrid framework proposed in (Abou-
Moustafa et al, 2004) for sequential data classi-
fication. The framework employs HMMs to map
the variable length sequential data into a fixed size
P -dimensional vector that can be classified us-
ing any discriminative model. Experiments were
conducted on the NIST database for handwritten
digits and results showed a better recognition rate
than that of standard HMMs. Another example is
the hybrid generative/discriminative approach pro-
posed in (Holub et al, 2008) for detecting and
classifying object categories in the machine ver-
sion domain. In this approach, ?Fisher Kernels?
were used to retain most of the desirable prop-
erties of generative methods and a discriminative
setting was used to increase the classification per-
formance. Experimental results showed signifi-
cant performance improvement over the generative
counterpart.
3 Methodologies
This section first introduces the hidden vector
state (HVS) model and the hidden Markov sup-
port vector machines (HM-SVMs) followed by
the presentation of the proposed hybrid genera-
tive/discriminative framework.
3.1 Hidden Vector State Model
Given a model and an observed word sequence
W = (w
1
? ? ?w
T
), semantic parsing can be
viewed as a pattern recognition problem and the
most likely semantic representation can be found
through statistical decoding. If assuming that the
hidden data take the form of a semantic parse tree
C then the model should be a push-down automata
which can generate the pair ?W,C? through some
canonical sequence of moves D = (d
1
? ? ? d
T
).
That is,
P (W,C) =
T
?
t=1
P (d
t
|d
t?1
? ? ? d
1
) (1)
1114
For the general case of an unconstrained hierarchi-
cal model, D will consist of three types of proba-
bilistic move:
1. popping semantic category labels off the
stack;
2. pushing one or more non-terminal semantic
category label onto the stack;
3. generating the next word.
When considering a constrained form of au-
tomata where the stack is finite depth and ?W,C?
is built by repeatedly popping 0 to n labels off
the stack, pushing exactly one new label onto the
stack and then generating the next word, it defines
the Hidden Vector State (HVS) model in which
conventional grammar rules are replaced by three
probability tables.
Given a word sequence W , concept vector se-
quence C and a sequence of stack pop operations
N , the joint probability of P (W,C,N) can be de-
composed as
P (W,C,N) =
T
?
t=1
P (n
t
|c
t?1
)P (c
t
[1]|c
t
[2 ? ? ?D
t
])
P (w
t
|c
t
) (2)
where c
t
, the vector state at word position t, is a
vector of D
t
semantic concept labels (tags), i.e.
c
t
= [c
t
[1], c
t
[2], ..c
t
[D
t
]] where c
t
[1] is the preter-
minal concept label and c
t
[D
t
] is the root concept
label, n
t
is the vector stack shift operation at word
position t and take values in the range 0, . . . , D
t?1
and c
t
[1] = c
w
t
is the new preterminal semantic
tag assigned to word w
t
at word position t.
3.2 Hidden Markov Support Vector
Machines
To learn a function that assigns to a sequence of
words W = (w
1
? ? ?w
T
), w
i
? W, i = 1, . . . T a
sequence of semantic tags C = c
1
c
2
. . . c
T
, c
i
?
C, i = 1, . . . T , a common approach is to deter-
mine a discriminant function F : W?C ? R that
assigns a score to every input W ? W := W
?
and
every semantic tag sequence C ? C := C
?
, where
W
?
denotes the Kleene closure of W. In order to
obtain a prediction f(W ) ? C, the function is max-
imized with respect to f(W ) = argmax
C?C
F (W,C).
In particular, the function F (W,C) is assumed
to be linear in some combined feature represen-
tation of W and C in HM-SVMs (Altun et al,
2003), F (W,C) := ?w,?(W,C)?. Given a set
of training data (W
i
, C
i
), i = 1, . . . N , the param-
eters w are adjusted so that the true semantic tag
sequence C
i
scores higher than all other tag se-
quences C ? C
i
:= C\C
i
with a large margin. To
achieve the goal, the following optimization prob-
lem is solved instead.
min
?
i
?R,w?F
Cons
?
i
?
i
+
1
2
?w?
2
(3)
s.t. ?w,?(W,C
i
)? ? ?w,?(W,C)? ? 1? ?
i
,
?i = 1, . . . N and C ? C\C
i
where ?
i
is non-negative slack variables allowing
one to increase the global margin by paying a lo-
cal penalty on some outlying examples, and Cons
dictates the desired trade off between margin size
and outliers. To solve the equation 3, the dual of
the equation is solved instead. The solution w
?
can
be written as
w
?
=
N
?
i=1
?
C?C
?
i
(C)?(W
i
, C), (4)
where ?
i
(C) is the Lagrange multiplier of the con-
straint associated with example i and C
i
.
3.3 A Hybrid Generative/Discriminative
Framework for Semantic Parsing
The framework of combining the HVS model and
the HM-SVMs is illustrated in Figure 1. It consists
of three main stages, Representative Sentences Se-
lection, Fully Annotated Corpus Generation, and
HM-SVM Training and Testing. Each of them is
discussed in details below.
? Representative Sentences Selection. Given an
un-annotated corpus, the modified K-means
clustering algorithm is first employed to se-
lect the most representative sentences for an-
notation. This is to avoid annotating the
whole corpus and hopefully the model trained
from the subset of the original corpus would
still give a similar performance when com-
pared with the model trained from the full
corpus. The modified K-means clustering al-
gorithm is described in Figure 3.3.
Initially, k different sentences are randomly
selected as the initial centroids. Then, each
sentence s
i
in the training data is assigned to
one of the k clusters based on the similarity
measurement which will be discussed later.
1115
HM-SVM Training and Testing
Fully Annotated Corpus Generation
Representative Sentence Selection
Un-annotated 
corpus
Test Data(Sentences)
HVS 
model
Parsing results filtering
HVS trainingHVS parsing
Clustering Annotating
HM-SVM Training
Classification
Results
Sentences and 
their annotations
Fully annotated 
corpus
Sentences and their 
parsing sequences
Representative 
sentences
HM-SVM 
Classifier
Figure 1: The hybrid generative/discriminative framework for semantic parsing.
After that, the centroids of the k clusters are
recalculated. The above process repeats until
there are no further changes in the centroids.
The similarity between two sentences is cal-
culated based on sequence alignment. Sup-
pose a = a
1
a
2
? ? ? a
n
and b = b
1
b
2
? ? ? b
m
are
the two word sequences of length of n and m,
Sim(i, j) is defined as the score of the op-
timal alignment between the initial segment
from a
1
to a
i
of a and the initial segment from
b
1
to b
j
of b, where Sim(i, j) is recursively
calculated as follows:
Sim(i, 0) = 0, i = 1, 2, ...n
Sim(0, j) = 0, j = 1, 2, ...m
Sim(i, j) = max
?
?
?
?
?
?
?
?
?
?
?
0,
Sim(i? 1, j ? 1) + s(a
i
, b
j
),
Sim(i? 1, j) + s(a
i
,
?
?
?
),
Sim(i, j ? 1) + s(
?
?
?
, b
j
)
Here s(a
i
, b
j
) is the score of aligning a
i
with
b
j
and is defined as:
s(a
i
, b
j
) = log
[
p(a
i
, b
j
)
p(a
i
)? p(b
j
)
]
(5)
where, p(a
i
) denotes the occurrence probabil-
ity of the word a
i
and p(a
i
, b
j
) denotes the
probability that a
i
and b
j
appear at the same
position in two aligned sequences.
To ensure that content words containing key
information are weighted more heavily than
the less relevant words such as function
words, a score matrix can then be built and
dynamic programming is used to find the
largest score between two sequences. The
distance between the two sentences is de-
fined as the negation of the similarity between
them.
After generating k clusters, the centroid in
each of the clusters is selected as the represen-
tative sentence for annotation. This results in
an exactly k sentences being selected. There
are however two ways to construct the anno-
tated corpus depending on the neighborhood
threshold value d. When d = 1, the anno-
tated corpus only contains k sentences. When
d < 1, both the centroid and some of its near-
est neighboring sentences in each cluster will
receive the same abstract annotation. Thus,
the annotated corpus will contain more than
k sentences. It has to be noted that in both
cases, only k sentences (centroids) need to be
annotated.
? Fully Annotated Corpus Generation. An
HVS model is trained from the annotated cor-
pus constructed in the previous stage which
is then applied to the original un-annotated
corpus to generate a semantic tag sequence
for each sentence. The generated semantic
tag sequences essentially define the explicit
word/tag alignments which could serve as the
full annotation required by HM-SVMs train-
ing. However, the HVS model does not guar-
antee to parse the sentences with 100% accu-
racy. Based on a user-defined parse probabil-
1116
Input: A set of sentences S = {s
i
, i = 1, . . . , N}, a distance threshold ?, a neighborhood threshold d
Output: A set of representative sentences R = {r
j
, j = 1, . . . ,M}, and a set of the centroids of the
generated clusters Cent
Algorithm:
1. For each s
i
? S, set Flag
i
= 1. Initialize R and Cent to be empty sets.
2. Select sentences from S with Flag equal to 1 , then reconstruct
?
S = {s
j
|Flag
j
= 1, j =
1, . . . ,
?
N},
?
N is the number of sentences with Flag equal to 1 in S.
3. Randomly select k different sentences c
k
from
?
S, the default value of k is 1000. Construct k
clusters C = {c
l
}, l = 1, . . . , k. Set NumOfFlag = ?
?
S?
4. Loop for each sentences s
i
?
?
S
Loop for each cluster c
l
Calculate the distance between s
i
and the centroid of c
l
. Dist
il
= Distance(s
i
, c
l
).
If Dist
il
< ?, then c
l
= c
l
?
s
i
, set Flag
i
= 0, ExitLoop.
EndLoop
If Flag
i
6= 0, then find the cluster l
?
= argmin
l
{Dist
il
, l = 1, . . . , k}, c
l
?
= c
l
?
?
s
i
EndLoop
If ?{s
i
|s
i
?
?
S, F lag
i
= 1}? <NumOfFlag, then set NumOfFlag = ?{s
i
|s
i
?
?
S, F lag
i
= 1}?,
go to Step 4.
Cent = Cent
?
Cent
l
, l = 1, . . . , k, ?c
l
? 6= 0.
5. If NumOfFlag > 0 then Go to step 2.
Else
R = R
?
Cent.
Construct ?Cent? clusters
?
C = {c
l
}, l = 1, . . . , ?Cent?.
Loop for each sentences s
i
? S
Find the cluster l
?
= argmin
l
{Dist
il
, l = 1, . . . , ?Cent?}, c
?
l
= c
?
l
?
s
i
If Dist
il
?
< d, then R = R
?
s
i
EndLoop
EndIf
Figure 2: A modified K-means clustering method.
ity threshold, the most confident parse results
are selected for the construction of the fully
annotated corpus.
? HM-SVMs Training and Testing. Given the
fully annotated corpus generated in the previ-
ous stage, the HM-SVMs can then be trained
which could later be used to derive the seman-
tic tag sequences for the test data.
4 Experiment
Experiments have been conducted on the DARPA
Communicator data (CUData, 2004) which are
available to the public as open source download.
The data contain utterance transcriptions and the
semantic parse results from the rule-based Phoenix
parser
1
. The DARPA Communicator data were
collected in 461 days. From these, 46 days were
randomly selected for use as test set data and the
remainder were used for training. After cleaning
up the data, the training set consist of 12702 utter-
ances while the test set contains 1178 utterances.
1
http://communicator.colorado.edu/phoenix
The abstract annotation used for training and the
reference annotation needed for testing were de-
rived by hand correcting the Phoenix parse results.
For example, for the sentence ?Show me flights
from Boston to New York?, the abstract annota-
tion would be
FLIGHT(FROMLOC(CITY) TOLOC(CITY)).
Such an annotation need only list a set of valid se-
mantic concepts and the dominance relationships
between them without considering the actual real-
ized concept sequence or attempting to identify ex-
plicit word/concept pairs. Thus, it avoids the need
for expensive tree-bank style annotations.
To evaluate the performance of the model, a ref-
erence frame structure was derived for every test
set sentence consisting of slot/value pairs. An ex-
ample of a reference frame is:
Show me flights from Boston to New York.
Frame: FLIGHT
Slots: FROMLOC.CITY = Boston
TOLOC.CITY = New York
Performance was then measured in terms of
F -measure on slot/value pairs, which combines
1117
Table 1: Feature templates used in HM-SVMs. w
i
is the current word, and w
1
, . . . , w
n
is the entire
sentence.
Current word w
i
Previous word w
i?1
Word two back w
i?2
Next word w
i+1
Word two ahead w
i+2
Bigram features w
i?2
, w
i?1
w
i?1
, w
i
w
i
, w
i+1
w
i+1
, w
i+2
Table 2: The number of representative sentences
vs the different settings of ? and d.
H
H
H
H
H
H
d
?
0.5 0.6 0.7 0.8 0.9
1 350 663 1068 1743 2763
0.6 6878 7596 9810 9640 11872
the precision (P) and recall (R) values with equal
weight and is defined as F = (P +R)/2PR.
In all the subsequent experiments, the open
source SVM
hmm
(Tsochantaridis et al, 2005)
2
has been used to train and test the HM-SVMs. The
features used in the HM-SVMs are listed in Ta-
ble 1.
4.1 Comparison between HVS and
HM-SVMs
In the modified K-means clustering algorithm de-
scribed in Figure 3.3, the number of representative
sentences depends on both the distance threshold ?
and the neighborhood threshold d. Table 2 shows
the number of representative sentences obtained by
varying ? and d.
First, a set of experiments were conducted to
compare the performance of the HVS model with
the HM-SVMs only without incorporating the hy-
brid framework. Based on the different values of
d and ?, we constructed different sets of the anno-
tated corpus. For example, when ? = 0.5, there
are 350 clusters generated from a total of 12702
sentences in the un-annotated corpus. The centroid
from each of the cluster is then selected for annota-
tion. These 350 sentences were annotated with ab-
stract annotation for the HVS model training. And
they were also fully annotated by providing word-
level annotations for HM-SVMs training.
2
http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
0.5 0.6 0.7 0.8 0.90.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
?
F?
me
asu
re 
 
 
HM?SVM
HVS (d = 1)
HVS (d = 0.6)
Figure 3: Comparisons of the performance of HVS
and HM-SVMs on different ?.
Since only abstract annotations need to be pro-
vided for the HVS model training, it is possible to
automatically enlarge the annotated corpus by in-
cluding some of the neighboring sentences if the
same annotation of the centroid can be assigned to
them. This is controlled by varying the neighbor-
hood threshold d. If d = 1, then the annotated
corpus only contains 350 sentences with their ab-
stract annotations. If varying d to 0.6, then for each
cluster, some of the neighboring sentences also re-
ceive the same abstract annotation as that of the
centroid, thus the annotated corpus is enlarged to
contain 6878 sentences.
The performance comparison of the HVS and
the HM-SVMs is shown in Figure 3. It can be
observed that in general, HM-SVMs outperforms
HVS. This is not surprising as HM-SVMs was
trained from the fully annotated corpus. The HVS
model based on d = 0.6 achieved better perfor-
mance over the one based on d = 1 since the en-
larged annotated corpus was used for training. The
best performance given by HM-SVMs is 92.5%
of F-measure when ? = 0.9 and 2793 annotated
sentences were used for training, while the HVS
model gave an F-measure of 86.9%.
Though HM-SVMs outperforms HVS by 5.5%,
it should be noted that the time consumed for
preparing the fully annotated corpus for HM-
SVMs is far more than the time spent for abstract
annotation for HVS as shown in Figure 4. When
? = 0.5, annotating 350 sentences with the ex-
plicit word/semantic tag mappings took about 17.5
hours while abstract annotation only took about 3
hours. When ? = 0.9, the time spent on fully an-
notating 2763 sentences is almost six times that of
abstract annotation.
1118
0
20
40
60
80
100
120
140
1 2 3 4 5
alpha
ho
urs

Abstract Annotation Word level Annotation
Figure 4: Comparison of time Consuming in
preparing training data for the HVS model and the
HM-SVMs.
4.2 Comparison between HVS and the
Hybrid Framework with Clustering
Figure 5 shows the performance comparison be-
tween the HVS model and the hybrid framework
by varying ?. It can be observed that when the size
of the annotated corpus is small, the HVS model
outperforms the hybrid framework. However, with
increased number of annotated sentences, the hy-
brid framework achieves the better performance.
For both the HVS model and the hybrid frame-
work, improved performance is observed by train-
ing the model/framework from the augmented an-
notated corpus with the neighboring sentences au-
tomatically added in (cf. Figure 5(a) and (b)).
We notice from Figure 5(a) that when the num-
ber of annotated sentences increases from 1743 to
2763, the performances of both the HVS model
and the hybrid framework decrease. By analyz-
ing the clustering results generated from the mod-
ified K-means algorithm, we found that some of
the clusters formed actually contain those rare sen-
tences and they represent the outliers of the orig-
inal training set. This therefore leads to the de-
creased performance of the HVS model and the
hybrid framework.
With only 2763 annotated sentences, the hybrid
framework trained under d = 0.6 achieves 88.5%
in F-measure which results in a relative error re-
duction of 12% when compared with the HVS
model where the F-measure value of 87.0% was
obtained.
4.3 Comparison between HVS and the
Hybrid Framework without Clustering
Experiments have also been conducted to compare
the performance of the HVS model and the hy-
0.5 0.6 0.7 0.8 0.90.75
0.760.77
0.780.79
0.80.81
0.820.83
0.840.85
0.860.87
0.880.89
0.9
?
F?m
eas
ure
 
 
HVS
Hybrid framework
(a) d = 1.
0.5 0.6 0.7 0.8 0.90.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
?
F?m
eas
ure
 
 
 
HVS
Hybrid framework
(b) d = 0.6.
Figure 5: Comparison of the performance of the
HVS model and the hybrid framework
brid framework without employing the modified
K-means clustering algorithm to automatically se-
lect the most representative sentences for annota-
tion. That is, the whole corpus of 12702 sentences
were provided with abstract annotations. Both
the HVS model and the hybrid framework were
trained from the training set which was formed by
randomly selecting the annotated sentences from
the original corpus. Figure 6 illustrates the perfor-
mance of the HVS model and the hybrid frame-
work versus the varying sizes of the training data.
Here 10-fold cross validation was performed and
the F-measure value at each point of the curve in
Figure 6 was calculated by averaging the perfor-
mance of the 10 experiments each time with dif-
ferent training set of the same size.
It can be observed that the performance of both
the HVS model and the proposed hybrid frame-
work increases with the increased size of the train-
ing data. The hybrid framework outperforms the
HVS model when the size of the training data is
beyond 6000. The improvement is more substan-
tial by incorporating more training data.
The best performance achieved by the HVS
model and the proposed hybrid framework is listed
1119
1 2 3 4 5 6 7 8 9 10 11 120.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
0.9
0.91
Number of Sentences in the Training data (*1000)
F?m
eas
ure
 
 
HVS
Hybrid framework
Figure 6: Performance of the HVS model and the
hybrid framework vs the size of the training data.
in Table 3. It can be observed that the hybrid
framework gives the relative error reduction of
22% when compared with the performance of the
HVS model where only 87.97% was achieved.
Table 3: Performance comparison between HVS
and the hybrid framework.
Measurement HVS Hybrid Framework
Recall 87.81% 90.99 %
Precision 88.13% 90.25%
F-measure 87.97% 90.62%
It should also be noted that the best performance
of 87.97% in F-measure was obtained when the
HVS model was trained on the whole annotated
corpus of 12702 sentences. By employing the clus-
tering method to select the most representative sen-
tences, the hybrid framework trained with less than
one fourth of the original training data (2763 an-
notated sentences) achieves 88.53% in F-measure,
which is better than that of the HVS model.
5 Conclusions
This paper presented a hybrid framework by com-
bining the HVS model and the HM-SVMs for se-
mantic parsing. Experimental results show that
22% relative error reduction in F-measure was ob-
tained using the hybrid framework on the DARPA
Communicator Data when compared with the per-
formance of the HVS model. Furthermore, em-
ploying the modified K-means clustering algo-
rithm to automatically select the most representa-
tive sentences for annotation greatly reduces the
human effort for annotation. With only 2763
annotated sentences, the hybrid framework gives
the better performance compared with the HVS
model trained on the full 12702 annotated sen-
tences. Also, the hybrid framework gives the com-
parable performance with that of the HM-SVMs
but without the use of the expensive word-level an-
notations.
References
Abou-Moustafa, K.T., C.Y. Suen, and M. Cheriet.
2004. A generative-discriminative hybrid for se-
quential data classification. In Acoustics, Speech,
and Signal Processing, 2004 (ICASSP ?04), vol-
ume 5, pages 805?808.
Altun, Y., I. Tsochantaridis, and T. Hofmann. 2003.
Hidden markov support vector machines. In Inter-
national Conference in Machine Learning, pages 3?
10.
Bouchard, Guillaume and Bill Triggs. 2004. The trade-
off between generative and discriminative classifiers.
In Proc. of COMPSTAT 2004, pages 721?728.
CUData. 2004. Darpa communicator travel data.
university of colorado at boulder. Avaiable from
http://communicator.colorado.edu/phoenix.
Dowding, J., R. Moore, F. Andry, and D. Moran. 1994.
Interleaving syntax and semantics in an efficient
bottom-up parser. In Proc. of the 32th Annual Meet-
ing of the Association for Computational Linguistics,
pages 110?116, Las Cruces, New Mexico, USA.
He, Yulan and Steve Young. 2005. Semantic process-
ing using the hidden vector state model. Computer
Speech and Language, 19(1):85?106.
Holub, Alex D., Max Welling, and Pietro Perona1.
2008. Hybrid generative-discriminative visual cat-
egorization. International Journal of Computer Vi-
sion, 77:239?258.
Jaakkola, T. and D. Haussler. 1998. Exploiting genera-
tive models in discriminative classifiers. In Proc. of
Advances in Neural Information Processing 11.
Ng, A. and M. Jordan. 2002. On generative vs. dis-
criminative classifiers: A comparison of logistic re-
gression and naive bayes. In Proc. of Advances in
Neural Information Processing 15, pages 841?848.
Tsochantaridis, Ioannis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent output
variables. J. Mach. Learn. Res., 6:1453?1484.
Vapnik, Vladimir N. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Ward, W. and S. Issar. 1994. Recent improvements in
the cmu spoken language understanding system. In
Proc. of the workshop on Human Language Technol-
ogy, pages 213?216, Plainsboro, New Jerey, USA.
1120
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 98?99,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting Protein-Protein Interaction based on Discriminative Training of
the Hidden Vector State Model
Deyu Zhou and Yulan He
Informatics Research Centre, The University of Reading, Reading RG6 6BX, UK
Email:d.zhou@reading.ac.uk, y.he@reading.ac.uk
1 Introduction
The knowledge about gene clusters and protein in-
teractions is important for biological researchers
to unveil the mechanism of life. However, large
quantity of the knowledge often hides in the liter-
ature, such as journal articles, reports, books and
so on. Many approaches focusing on extracting in-
formation from unstructured text, such as pattern
matching, shallow and deep parsing, have been pro-
posed especially for extracting protein-protein inter-
actions (Zhou and He, 2008).
A semantic parser based on the Hidden Vector
State (HVS) model for extracting protein-protein in-
teractions is presented in (Zhou et al, 2008). The
HVS model is an extension of the basic discrete
Markov model in which context is encoded as a
stack-oriented state vector. Maximum Likelihood
estimation (MLE) is used to derive the parameters
of the HVS model. In this paper, we propose a dis-
criminative approach based on parse error measure
to train the HVS model. To adjust the HVS model to
achieve minimum parse error rate, the generalized
probabilistic descent (GPD) algorithm (Kuo et al,
2002) is used. Experiments have been conducted on
the GENIA corpus. The results demonstrate mod-
est improvements when the discriminatively trained
HVS model outperforms its MLE trained counter-
part by 2.5% in F-measure on the GENIA corpus.
2 Methodologies
The Hidden Vector State (HVS) model (He and
Young, 2005) is a discrete Hidden Markov Model
(HMM) in which each HMM state represents the
state of a push-down automaton with a finite stack
size.
Normally, MLE is used for generative probabil-
ity model training in which only the correct model
needs to be updated during training. It is be-
lieved that improvement can be achieved by train-
ing the generative model based on a discriminative
optimization criteria (Klein and Manning, 2002) in
which the training procedure is designed to maxi-
mize the conditional probability of the parses given
the sentences in the training corpus. That is, not only
the likelihood for the correct model should be in-
creased but also the likelihood for the incorrect mod-
els should be decreased.Assuming the most likely semantic parse tree
C? = Cj and there are altogether M semantic parse
hypotheses for a particular sentence W , a parse er-
ror measure (Juang et al, 1993; Chou et al, 1993;
Chen and Soong, 1994) can be defined as
d(W ) = ? logP (W,Cj) + log[
1
M ? 1
?
i,i6=j
P (W,Ci)? ]
1
? (1)
where ? is a positive number and is used to se-
lect competing semantic parses. When ? = 1,
the competing semantic parse term is the average
of all the competing semantic parse scores. When
? ? ?, the competing semantic parse term be-
comes max
i.i6=j
P (W,Ci) which is the score for the top
competing semantic parse. By varying the value of
?, we can take all the competing semantic parses into
consideration. d(W ) > 0 implies classification er-
ror and d(W ) ? 0 implies correct decision.
The sigmoid function can be used to normalize
d(W ) in a smooth zero-one range and the loss func-
tion is thus defined as (Juang et al, 1993):
`(W ) = sigmoid(d(W )) (2)
98
where
sigmoid(x) = 1
1 + e??x
(3)
Here, ? is a constant which controls the slope of the
sigmoid function.
The update formula is given by:
?k+1 = ?k ? ?k?`(Wi, ?k) (4)
where ?k is the step size.
Using the definition of `(Wi, ?k) and after work-
ing out the mathematics, we get the update formu-
lae 5, 6, 7,
(
logP (n|c?)
)? = logP (n|c?)? ??`(di)(1? `(di))
?
??I(Cj , n, c?) +
?
i,i6=j
I(Ci, n, c?)
P (Wi, Ci, ?)??
i,i6=j P (Wi, Ci, ?)?
?
? (5)
(logP (c[1]|c[2..D]))? = logP (c[1]|c[2..D])? ??`(di)(1? `(di))
?
??I(Cj , c[1], c[2..D]) +
?
i,i6=j
I(Ci, c[1], c[2..D])
P (Wi, Ci, ?)??
i,i6=j P (Wi, Ci, ?)?
?
?
(6)
(logP (w|c))? = logP (w|c)? ??`(di)(1? `(di))
?
??I(Cj , w, c) +
?
i,i6=j
I(Ci, w, c)
P (Wi, Ci, ?)??
i,i6=j P (Wi, Ci, ?)?
?
? (7)
where I(Ci, n, c?) denotes the number of times
the operation of popping up n semantic tags at
the current vector state c? in the Ci parse tree,
I(Ci, c[1], c[2..D]) denotes the number of times the
operation of pushing the semantic tag c[1] at the cur-
rent vector state c[2..D] in the Ci parse tree and
I(Ci, w, c) denotes the number of times of emitting
the word w at the state c in the parse tree Ci.
3 Experimental Setup and Results
GENIA (Kim et al, 2003) is a collection of 2000 re-
search abstracts selected from the search results of
MEDLINE database using keywords (MESH terms)
?human, blood cells and transcription factors?. All
these abstracts were then split into sentences and
those containing more than two protein names and
at least one interaction keyword were kept. Alto-
gether 3533 sentences were left and 2500 sentences
were sampled to build our data set.
The results using MLE and discriminative train-
ing are listed in Table 1. Discriminative training
improves on the MLE by relatively 2.5% where N
Table 1: Performance comparison of MLE versus Dis-
criminative training
Measurement GENIA
MLE Discriminative
Recall 61.78% 64.59%
Precision 61.16% 61.51%
F-measure 61.47% 63.01%
and I are set to 5 and 200 individually. Here N de-
notes the number of semantic parse hypotheses and
I denotes the the number of sentences in the training
data.
References
J.K. Chen and F.K. Soong. 1994. An n-best candidates-
based discriminative training for speech recognition
applications. IEEE Transactions on Speech and Audio
Processing, 2:206 ? 216.
W. Chou, C.H. Lee, and B.H. Juang. 1993. Minimum
error rate training based on n-best string models. In
Acoustics, Speech, and Signal Processing, IEEE Inter-
national Conference on ICASSP ?93, volume 2, pages
652 ? 655.
Y. He and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and
Language, 19(1):85?106.
B.H. Juang, W. Chou, and C.H. Lee. 1993. Statistical
and discriminative methods for speech recognition. In
Rubio, editor, Speech Recognition and Understanding,
NATO ASI Series, Berlin. Springer-Verlag.
JD. Kim, T. Ohta, Y. Tateisi, and J Tsujii. 2003. GE-
NIA corpus?semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl 1):i180?2.
D. Klein and C. D. Manning. 2002. Conditional struc-
ture versus conditional estimation in nlp models. In
Proc. the ACL-02 conference on Empirical methods in
natural language processing, pages 9?16, University
of Pennsylvania, PA.
H.-K.J. Kuo, E. Fosle-Lussier, H. Jiang, and C.H. Lee.
2002. Discriminative training of language models
for speech recognition. In Acoustics, Speech, and
Signal Processing, IEEE International Conference on
ICASSP ?02, volume 1, pages 325 ? 328.
Deyu Zhou and Yulan He. 2008. Extracting Interac-
tions between Proteins from the Literature. Journal
of Biomedical Informatics, 41:393?407.
Deyu Zhou, Yulan He, and Chee Keong Kwoh. 2008.
Extracting Protein-Protein Interactions from the Liter-
ature using the Hidden Vector State Model. Interna-
tional Journal of Bioinformatics Research and Appli-
cations, 4(1):64?80.
99
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 123?131,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatically Extracting Polarity-Bearing Topics for Cross-Domain
Sentiment Classification
Yulan He Chenghua Lin? Harith Alani
Knowledge Media Institute, The Open University
Milton Keynes MK7 6AA, UK
{y.he,h.alani}@open.ac.uk
? School of Engineering, Computing and Mathematics
University of Exeter, Exeter EX4 4QF, UK
cl322@exeter.ac.uk
Abstract
Joint sentiment-topic (JST) model was previ-
ously proposed to detect sentiment and topic
simultaneously from text. The only super-
vision required by JST model learning is
domain-independent polarity word priors. In
this paper, we modify the JST model by in-
corporating word polarity priors through mod-
ifying the topic-word Dirichlet priors. We
study the polarity-bearing topics extracted by
JST and show that by augmenting the original
feature space with polarity-bearing topics, the
in-domain supervised classifiers learned from
augmented feature representation achieve the
state-of-the-art performance of 95% on the
movie review data and an average of 90% on
the multi-domain sentiment dataset. Further-
more, using feature augmentation and selec-
tion according to the information gain criteria
for cross-domain sentiment classification, our
proposed approach performs either better or
comparably compared to previous approaches.
Nevertheless, our approach is much simpler
and does not require difficult parameter tun-
ing.
1 Introduction
Given a piece of text, sentiment classification aims
to determine whether the semantic orientation of the
text is positive, negative or neutral. Machine learn-
ing approaches to this problem (?; ?; ?; ?; ?; ?) typ-
ically assume that classification models are trained
and tested using data drawn from some fixed distri-
bution. However, in many practical cases, we may
have plentiful labeled examples in the source do-
main, but very few or no labeled examples in the
target domain with a different distribution. For ex-
ample, we may have many labeled books reviews,
but we are interested in detecting the polarity of
electronics reviews. Reviews for different produces
might have widely different vocabularies, thus clas-
sifiers trained on one domain often fail to produce
satisfactory results when shifting to another do-
main. This has motivated much research on sen-
timent transfer learning which transfers knowledge
from a source task or domain to a different but re-
lated task or domain (?; ?; ?; ?).
Joint sentiment-topic (JST) model (?; ?) was ex-
tended from the latent Dirichlet alocation (LDA)
model (?) to detect sentiment and topic simultane-
ously from text. The only supervision required by
JST learning is domain-independent polarity word
prior information. With prior polarity words ex-
tracted from both the MPQA subjectivity lexicon1
and the appraisal lexicon2, the JST model achieves
a sentiment classification accuracy of 74% on the
movie review data3 and 71% on the multi-domain
sentiment dataset4. Moreover, it is also able to ex-
tract coherent and informative topics grouped under
different sentiment. The fact that the JST model
does not required any labeled documents for training
makes it desirable for domain adaptation in senti-
ment classification. Many existing approaches solve
the sentiment transfer problem by associating words
1http://www.cs.pitt.edu/mpqa/
2http://lingcog.iit.edu/arc/appraisal_
lexicon_2007b.tar.gz
3http://www.cs.cornell.edu/people/pabo/
movie-review-data
4http://www.cs.jhu.edu/?mdredze/
datasets/sentiment/index2.html
123
from different domains which indicate the same sen-
timent (?; ?). Such an association mapping problem
can be naturally solved by the posterior inference in
the JST model. Indeed, the polarity-bearing topics
extracted by JST essentially capture sentiment asso-
ciations among words from different domains which
effectively overcome the data distribution difference
between source and target domains.
The previously proposed JST model uses the sen-
timent prior information in the Gibbs sampling in-
ference step that a sentiment label will only be sam-
pled if the current word token has no prior sentiment
as defined in a sentiment lexicon. This in fact im-
plies a different generative process where many of
the word prior sentiment labels are observed. The
model is no longer ?latent?. We propose an alter-
native approach by incorporating word prior polar-
ity information through modifying the topic-word
Dirichlet priors. This essentially creates an informed
prior distribution for the sentiment labels and would
allow the model to actually be latent and would be
consistent with the generative story.
We study the polarity-bearing topics extracted by
the JST model and show that by augmenting the
original feature space with polarity-bearing topics,
the performance of in-domain supervised classifiers
learned from augmented feature representation im-
proves substantially, reaching the state-of-the-art re-
sults of 95% on the movie review data and an aver-
age of 90% on the multi-domain sentiment dataset.
Furthermore, using simple feature augmentation,
our proposed approach outperforms the structural
correspondence learning (SCL) (?) algorithm and
achieves comparable results to the recently proposed
spectral feature alignment (SFA) method (?). Never-
theless, our approach is much simpler and does not
require difficult parameter tuning.
We proceed with a review of related work on
sentiment domain adaptation. We then briefly de-
scribe the JST model and present another approach
to incorporate word prior polarity information into
JST learning. We subsequently show that words
from different domains can indeed be grouped un-
der the same polarity-bearing topic through an illus-
tration of example topic words extracted by JST be-
fore proposing a domain adaptation approach based
on JST. We verify our proposed approach by con-
ducting experiments on both the movie review data
and the multi-domain sentiment dataset. Finally, we
conclude our work and outline future directions.
2 Related Work
There has been significant amount of work on algo-
rithms for domain adaptation in NLP. Earlier work
treats the source domain data as ?prior knowledge?
and uses maximum a posterior (MAP) estimation to
learn a model for the target domain data under this
prior distribution (?). Chelba and Acero (?) also
uses the source domain data to estimate prior dis-
tribution but in the context of a maximum entropy
(ME) model. The ME model has later been studied
in (?) for domain adaptation where a mixture model
is defined to learn differences between domains.
Other approaches rely on unlabeled data in the
target domain to overcome feature distribution dif-
ferences between domains. Motivated by the alter-
nating structural optimization (ASO) algorithm (?)
for multi-task learning, Blitzer et al (?) proposed
structural correspondence learning (SCL) for do-
main adaptation in sentiment classification. Given
labeled data from a source domain and unlabeled
data from target domain, SCL selects a set of pivot
features to link the source and target domains where
pivots are selected based on their common frequency
in both domains and also their mutual information
with the source labels.
There has also been research in exploring care-
ful structuring of features for domain adaptation.
Daume? (?) proposed a kernel-mapping function
which maps both source and target domains data to
a high-dimensional feature space so that data points
from the same domain are twice as similar as those
from different domains. Dai et al(?) proposed trans-
lated learning which uses a language model to link
the class labels to the features in the source spaces,
which in turn is translated to the features in the
target spaces. Dai et al (?) further proposed us-
ing spectral learning theory to learn an eigen fea-
ture representation from a task graph representing
features, instances and class labels. In a similar
vein, Pan et al (?) proposed the spectral feature
alignment (SFA) algorithm where some domain-
independent words are used as a bridge to con-
struct a bipartite graph to model the co-occurrence
relationship between domain-specific words and
domain-independent words. Feature clusters are
124
generated by co-align domain-specific and domain-
independent words.
Graph-based approach has also been studied in
(?) where a graph is built with nodes denoting
documents and edges denoting content similarity
between documents. The sentiment score of each
unlabeled documents is recursively calculated until
convergence from its neighbors the actual labels of
source domain documents and pseudo-labels of tar-
get document documents. This approach was later
extended by simultaneously considering relations
between documents and words from both source and
target domains (?).
More recently, Seah et al (?) addressed the issue
when the predictive distribution of class label given
input data of the domains differs and proposed Pre-
dictive Distribution Matching SVM learn a robust
classifier in the target domain by leveraging the la-
beled data from only the relevant regions of multiple
sources.
3 Joint Sentiment-Topic (JST) Model
Assume that we have a corpus with a collection ofD
documents denoted by C = {d1, d2, ..., dD}; each
document in the corpus is a sequence of Nd words
denoted by d = (w1, w2, ..., wNd), and each word
in the document is an item from a vocabulary index
with V distinct terms denoted by {1, 2, ..., V }. Also,
let S be the number of distinct sentiment labels, and
T be the total number of topics. The generative
process in JST which corresponds to the graphical
model shown in Figure ??(a) is as follows:
? For each document d, choose a distribution
pid ? Dir(?).
? For each sentiment label l under document d,
choose a distribution ?d,l ? Dir(?).
? For each word wi in document d
? choose a sentiment label li ? Mult(pid),
? choose a topic zi ? Mult(?d,li),
? choose a word wi from ?lizi , a Multino-
mial distribution over words conditioned
on topic zi and sentiment label li.
Gibbs sampling was used to estimate the posterior
distribution by sequentially sampling each variable
of interest, zt and lt here, from the distribution over
w
 
!
 
z"
NdS*T
# $
D
l
S
(a) JST model.
w
 
!
 
z"
NdS*T
# $
D
l
SS
!
S
(b) Modified JST model.
Figure 1: JST model and its modified version.
that variable given the current values of all other
variables and data. Letting the superscript ?t de-
note a quantity that excludes data from tth position,
the conditional posterior for zt and lt by marginaliz-
ing out the random variables ?, ?, and pi is
P (zt = j, lt = k|w, z?t, l?t, ?, ?, ?) ?
N?twt,j,k + ?
N?tj,k + V ?
?
N?tj,k,d + ?j,k
N?tk,d +
?
j ?j,k
?
N?tk,d + ?
N?td + S?
. (1)
where Nwt,j,k is the number of times word wt ap-
peared in topic j and with sentiment label k, Nj,k
is the number of times words assigned to topic j
and sentiment label k, Nj,k,d is the number of times
a word from document d has been associated with
topic j and sentiment label k, Nk,d is the number of
times sentiment label k has been assigned to some
word tokens in document d, andNd is the total num-
ber of words in the document collection.
In the modified JST model as shown in Fig-
ure ??(b), we add an additional dependency link of
? on the matrix ? of size S?V which we use to en-
code word prior sentiment information into the JST
model. For each word w ? {1, ..., V }, if w is found
in the sentiment lexicon, for each l ? {1, ..., S}, the
element ?lw is updated as follows
?lw =
{
1 if S(w) = l
0 otherwise
, (2)
where the function S(w) returns the prior sentiment
label of w in a sentiment lexicon, i.e. neutral, posi-
125
Book DVD Book Elec. Book Kitch. DVD Elec. DVD Kitch. Elec. Kitch.
P
os
.
recommend funni interest pictur interest qualiti concert sound movi recommend sound pleas
highli cool topic clear success easili rock listen stori highli excel look
easi entertain knowledg paper polit servic favorit bass classic perfect satisfi worth
depth awesom follow color clearli stainless sing amaz fun great perform materi
strong worth easi accur popular safe talent acoust charact qulati comfort profession
N
eg
.
mysteri cop abus problem bore return bore poorli horror cabinet tomtom elimin
fbi shock question poor tediou heavi plot low alien break region regardless
investig prison mislead design cheat stick stupid replac scari install error cheapli
death escap point case crazi defect stori avoid evil drop code plain
report dirti disagre flaw hell mess terribl crap dead gap dumb incorrect
Table 1: Extracted polarity words by JST on the combined data sets.
tive or negative.
The matrix ? can be considered as a transforma-
tion matrix which modifies the Dirichlet priors ? of
size S ? T ? V , so that the word prior polarity can
be captured. For example, the word ?excellent? with
index i in the vocabulary has a positive polarity. The
corresponding row vector in ? is [0, 1, 0] with its el-
ements representing neutral, positive, and negative.
For each topic j, multiplying ?li with ?lji, only the
value of ?lposji is retained, and ?lneuji and ?lnegji
are set to 0. Thus, the word ?excellent? can only
be drawn from the positive topic word distributions
generated from a Dirichlet distribution with param-
eter ?lpos .
4 Polarity Words Extracted by JST
The JST model allows clustering different terms
which share similar sentiment. In this section, we
study the polarity-bearing topics extracted by JST.
We combined reviews from the source and target
domains and discarded document labels in both do-
mains. There are a total of six different combi-
nations. We then run JST on the combined data
sets and listed some of the topic words extracted as
shown in Table ??. Words in each cell are grouped
under one topic and the upper half of the table shows
topic words under the positive sentiment label while
the lower half shows topic words under the negative
sentiment label.
We can see that JST appears to better capture sen-
timent association distribution in the source and tar-
get domains. For example, in the DVD+Elec. set,
words from the DVD domain describe a rock con-
cert DVD while words from the Electronics domain
are likely relevant to stereo amplifiers and receivers,
and yet they are grouped under the same topic by the
JST model. Checking the word coverage in each do-
main reveals that for example ?bass? seldom appears
in the DVD domain, but appears more often in the
Electronics domain. Likewise, in the Book+Kitch.
set, ?stainless? rarely appears in the Book domain
and ?interest? does not occur often in the Kitchen
domain and they are grouped under the same topic.
These observations motivate us to explore polarity-
bearing topics extracted by JST for cross-domain
sentiment classification since grouping words from
different domains but bearing similar sentiment has
the effect of overcoming the data distribution differ-
ence of two domains.
5 Domain Adaptation using JST
Given input data x and a class label y, labeled pat-
terns of one domain can be drawn from the joint
distribution P (x, y) = P (y|x)P (x). Domain adap-
tation usually assume that data distribution are dif-
ferent in source and target domains, i.e., Ps(x) 6=
Pt(x). The task of domain adaptation is to predict
the label yti corresponding to x
t
i in the target domain.
We assume that we are given two sets of training
data, Ds and Dt, the source domain and target do-
main data sets, respectively. In the multiclass clas-
sification problem, the source domain data consist
of labeled instances, Ds = {(xsn; y
s
n) ? X ? Y :
1 ? n ? N s}, where X is the input space and Y
is a finite set of class labels. No class label is given
in the target domain, Dt = {xtn ? X : 1 ? n ?
N t, N t  N s}. Algorithm ?? shows how to per-
form domain adaptation using the JST model. The
source and target domain data are first merged with
document labels discarded. A JST model is then
126
learned from the merged corpus to generate polarity-
bearing topics for each document. The original doc-
uments in the source domain are augmented with
those polarity-bearing topics as shown in Step 4 of
Algorithm ??, where li zi denotes a combination of
sentiment label li and topic zi for word wi. Finally,
feature selection is performed according to the infor-
mation gain criteria and a classifier is then trained
from the source domain using the new document
representations. The target domain documents are
also encoded in a similar way with polarity-bearing
topics added into their feature representations.
Algorithm 1 Domain adaptation using JST.
Input: The source domain data Ds = {(xsn; y
s
n) ? X ?
Y : 1 ? n ? Ns}, the target domain data, Dt =
{xtn ? X : 1 ? n ? N
t, N t  Ns}
Output: A sentiment classifier for the target domain Dt
1: Merge Ds and Dt with document labels discarded,
D = {(xsn, 1 ? n ? N
s;xtn, 1 ? n ? N
t}
2: Train a JST model on D
3: for each document xsn = (w1, w2, ..., wm) ? D
s do
4: Augment document with polarity-bearing topics
generated from JST,
xs
?
n = (w1, w2, ..., wm, l1 z1, l2 z2, ..., lm zm)
5: Add {xs
?
n ; y
s
n} into a document pool B
6: end for
7: Perform feature selection using IG on B
8: Return a classifier, trained on B
As discussed in Section ?? that the JST model di-
rectly models P (l|d), the probability of sentiment
label given document, and hence document polar-
ity can be classified accordingly. Since JST model
learning does not require the availability of docu-
ment labels, it is possible to augment the source do-
main data by adding most confident pseudo-labeled
documents from the target domain by the JST model
as shown in Algorithm ??.
6 Experiments
We evaluate our proposed approach on the two
datasets, the movie review (MR) data and the multi-
domain sentiment (MDS) dataset. The movie re-
view data consist of 1000 positive and 1000 neg-
ative movie reviews drawn from the IMDB movie
archive while the multi-domain sentiment dataset
contains four different types of product reviews ex-
tracted from Amazon.com including Book, DVD,
Electronics and Kitchen appliances. Each category
Algorithm 2 Adding pseudo-labeled documents.
Input: The target domain data, Dt = {xtn ? X :
1 ? n ? N t, N t  N s}, document sentiment
classification threshold ?
Output: A labeled document pool B
1: Train a JST model parameterized by ? on Dt
2: for each document xtn ? D
t do
3: Infer its sentiment class label from JST as
ln = arg maxs P (l|xtn; ?)
4: if P (ln|xtn; ?) > ? then
5: Add labeled sample (xtn, ln) into a docu-
ment pool B
6: end if
7: end for
of product reviews comprises of 1000 positive and
1000 negative reviews and is considered as a do-
main. Preprocessing was performed on both of the
datasets by removing punctuation, numbers, non-
alphabet characters and stopwords. The MPQA sub-
jectivity lexicon is used as a sentiment lexicon in our
experiments.
6.1 Experimental Setup
While the original JST model can produce reason-
able results with a simple symmetric Dirichlet prior,
here we use asymmetric prior ? over the topic pro-
portions which is learned directly from data using a
fixed-point iteration method (?).
In our experiment, ? was updated every 25 itera-
tions during the Gibbs sampling procedure. In terms
of other priors, we set symmetric prior ? = 0.01 and
? = (0.05?L)/S, where L is the average document
length, and the value of 0.05 on average allocates 5%
of probability mass for mixing.
6.2 Supervised Sentiment Classification
We performed 5-fold cross validation for the per-
formance evaluation of supervised sentiment clas-
sification. Results reported in this section are av-
eraged over 10 such runs. We have tested several
classifiers including Na??ve Bayes (NB) and support
vector machines (SVMs) from WEKA5, and maxi-
mum entropy (ME) from MALLET6. All parameters
are set to their default values except the Gaussian
5http://www.cs.waikato.ac.nz/ml/weka/
6http://mallet.cs.umass.edu/
127
prior variance is set to 0.1 for the ME model train-
ing. The results show that ME consistently outper-
forms NB and SVM on average. Thus, we only re-
port results from ME trained on document vectors
with each term weighted according to its frequency.
85
90
95
100
ccu
rac
y (%
)
Movie Review Book DVD Electronics Kitchen
75
80
1 5 10 15 30 50 100 150 200
Acc
ura
cy (
%)
No. of Topics
Figure 2: Classification accuracy vs. no. of topics.
The only parameter we need to set is the number
of topics T . It has to be noted that the actual num-
ber of feature clusters is 3 ? T . For example, when
T is set to 5, there are 5 topic groups under each
of the positive, negative, or neutral sentiment labels
and hence there are altogether 15 feature clusters.
The generated topics for each document from the
JST model were simply added into its bag-of-words
(BOW) feature representation prior to model train-
ing. Figure ?? shows the classification results on the
five different domains by varying the number of top-
ics from 1 to 200. It can be observed that the best
classification accuracy is obtained when the number
of topics is set to 1 (or 3 feature clusters). Increas-
ing the number of topics results in the decrease of
accuracy though it stabilizes after 15 topics. Never-
theless, when the number of topics is set to 15, us-
ing JST feature augmentation still outperforms ME
without feature augmentation (the baseline model)
in all of the domains. It is worth pointing out that
the JST model with single topic becomes the stan-
dard LDA model with only three sentiment topics.
Nevertheless, we have proposed an effective way to
incorporate domain-independent word polarity prior
information into model learning. As will be shown
later in Table ?? that the JST model with word po-
larity priors incorporated performs significantly bet-
ter than the LDA model without incorporating such
prior information.
For comparison purpose, we also run the LDA
model and augmented the BOW features with the
Method MR
MDS
Book DVD Elec. Kitch.
Baseline 82.53 79.96 81.32 83.61 85.82
LDA 83.76 84.32 85.62 85.4 87.68
JST 94.98 89.95 91.7 88.25 89.85
[YE10] 91.78 82.75 82.85 84.55 87.9
[LI10] - 79.49 81.65 83.64 85.65
Table 2: Supervised sentiment classification accuracy.
generated topics in a similar way. The best accu-
racy was obtained when the number of topics is set
to 15 in the LDA model. Table ?? shows the clas-
sification accuracy results with or without feature
augmentation. We have performed significance test
and found that LDA performs statistically signifi-
cant better than Baseline according to a paired t-test
with p < 0.005 for the Kitchen domain and with
p < 0.001 for all the other domains. JST performs
statistically significant better than both Baseline and
LDA with p < 0.001.
We also compare our method with other recently
proposed approaches. Yessenalina et al (?) ex-
plored different methods to automatically generate
annotator rationales to improve sentiment classifica-
tion accuracy. Our method using JST feature aug-
mentation consistently performs better than their ap-
proach (denoted as [YE10] in Table ??). They fur-
ther proposed a two-level structured model (?) for
document-level sentiment classification. The best
accuracy obtained on the MR data is 93.22% with
the model being initialized with sentence-level hu-
man annotations, which is still worse than ours. Li
et al (?) adopted a two-stage process by first clas-
sifying sentences as personal views and impersonal
views and then using an ensemble method to per-
form sentiment classification. Their method (de-
noted as [LI10] in Table ??) performs worse than ei-
ther LDA or JST feature augmentation. To the best
of our knowledge, the results achieved using JST
feature augmentation are the state-of-the-art for both
the MR and the MDS datasets.
6.3 Domain Adaptation
We conducted domain adaptation experiments on
the MDS dataset comprising of four different do-
mains, Book (B), DVD (D), Electronics (E), and
Kitchen appliances (K). We randomly split each do-
128
main data into a training set of 1,600 instances and a
test set of 400 instances. A classifier trained on the
training set of one domain is tested on the test set of
a different domain. We preformed 5 random splits
and report the results averaged over 5 such runs.
Comparison with Baseline Models
We compare our proposed approaches with two
baseline models. The first one (denoted as ?Base? in
Table ??) is an ME classifier trained without adapta-
tion. LDA results were generated from an ME clas-
sifier trained on document vectors augmented with
topics generated from the LDA model. The number
of topics was set to 15. JST results were obtained
in a similar way except that we used the polarity-
bearing topics generated from the JST model. We
also tested with adding pseudo-labeled examples
from the JST model into the source domain for ME
classifier training (following Algorithm ??), denoted
as ?JST-PL? in Table ??. The document sentiment
classification probability threshold ? was set to 0.8.
Finally, we performed feature selection by selecting
the top 2000 features according to the information
gain criteria (?JST-IG?)7.
There are altogether 12 cross-domain sentiment
classification tasks. We showed the adaptation loss
results in Table ?? where the result for each domain
and for each method is averaged over all three pos-
sible adaptation tasks by varying the source domain.
The adaptation loss is calculated with respect to the
in-domain gold standard classification result. For
example, the in-domain goal standard for the Book
domain is 79.96%. For adapting from DVD to Book,
baseline achieves 72.25% and JST gives 76.45%.
The adaptation loss is 7.71 for baseline and 3.51 for
JST.
It can be observed from Table ?? that LDA only
improves slightly compared to the baseline with an
error reduction of 11%. JST further reduces the er-
ror due to transfer by 27%. Adding pseudo-labeled
examples gives a slightly better performance com-
pared to JST with an error reduction of 36%. With
feature selection, JST-IG outperforms all the other
approaches with a relative error reduction of 53%.
7Both values of 0.8 and 2000 were set arbitrarily after an ini-
tial run on some held-out data; they were not tuned to optimize
test performance.
Domain Base LDA JST JST-PL JST-IG
Book 10.8 9.4 7.2 6.3 5.2
DVD 8.3 6.1 4.8 4.4 2.9
Electr. 7.9 7.7 6.3 5.4 3.9
Kitch. 7.6 7.6 6.9 6.1 4.4
Average 8.6 7.7 6.3 5.5 4.1
Table 3: Adaptation loss with respect to the in-domain
gold standard. The last row shows the average loss over
all the four domains.
Parameter Sensitivity
There is only one parameters to be set in the JST-
IG approach, the number of topics. We plot the clas-
sification accuracy versus different topic numbers in
Figure ?? with the number of topics varying between
1 and 200, corresponding to feature clusters varying
between 3 and 600. It can be observed that for the
relatively larger Book and DVD data sets, the accu-
racies peaked at topic number 10, whereas for the
relatively smaller Electronics and Kitchen data sets,
the best performance was obtained at topic number
50. Increasing topic numbers results in the decrease
of classification accuracy. Manually examining the
extracted polarity topics from JST reveals that when
the topic number is small, each topic cluster contains
well-mixed words from different domains. How-
ever, when the topic number is large, words under
each topic cluster tend to be dominated by a single
domain.
Comparison with Existing Approaches
We compare in Figure ?? our proposed approach
with two other domain adaptation algorithms for
sentiment classification, SCL and SFA. Each set of
bars represent a cross-domain sentiment classifica-
tion task. The thick horizontal lines are in-domain
sentiment classification accuracies. It is worth not-
ing that our in-domain results are slightly different
from those reported in (?; ?) due to different ran-
dom splits. Our proposed JST-IG approach outper-
forms SCL in average and achieves comparable re-
sults to SFA. While SCL requires the construction of
a reasonable number of auxiliary tasks that are use-
ful to model ?pivots? and ?non-pivots?, SFA relies
on a good selection of domain-independent features
for the construction of bipartite feature graph before
running spectral clustering to derive feature clusters.
129
70
75
80
85
ura
cy (%
)
D >B E >B K >B B >D E >D K >D
60
65
1 5 10 15 30 50 100 150 200
Acc
ura
cy (%
)
No. of topics
(a) Adapted to Book and DVD data sets.
80
85
ura
cy (%
)
B >E D >E K >E B >K D >K E >K
70
75
1 5 10 15 30 50 100 150 200
Acc
ura
cy (%
)
No. of topics
(b) Adapted to Electronics and Kitchen data sets.
Figure 3: Classification accuracy vs. no. of topics.
On the contrary, our proposed approach based on
the JST model is much simpler and yet still achieves
comparable results.
7 Conclusions
In this paper, we have studied polarity-bearing top-
ics generated from the JST model and shown that by
augmenting the original feature space with polarity-
bearing topics, the in-domain supervised classi-
fiers learned from augmented feature representation
achieve the state-of-the-art performance on both the
movie review data and the multi-domain sentiment
dataset. Furthermore, using feature augmentation
and selection according to the information gain cri-
teria for cross-domain sentiment classification, our
proposed approach outperforms SCL and gives sim-
ilar results as SFA. Nevertheless, our approach is
much simpler and does not require difficult parame-
ter tuning.
There are several directions we would like to ex-
plore in the future. First, polarity-bearing topics
generated by the JST model were simply added into
the original feature space of documents, it is worth
investigating attaching different weight to each topic
79.96 81.32
75
80
85
ura
cy (%
)
baseline SCL MI SFA JST IG
65
70
D >B E >B K >B B >D E >D K >D
Acc
ura
cy (%
)
(a) Adapted to Book and DVD data sets.
83.61 85.82
80
85
90
ura
cy (%
)
baseline SCL MI SFA JST IG
65
70
75
B >E D >E K >E B >K D >K E >K
Acc
ura
cy (%
)
(b) Adapted to Electronics and Kitchen data sets.
Figure 4: Comparison with existing approaches.
maybe in proportional to the posterior probability of
sentiment label and topic given a word estimated by
the JST model. Second, it might be interesting to
study the effect of introducing a tradeoff parameter
to balance the effect of original and new features.
Finally, our experimental results show that adding
pseudo-labeled examples by the JST model does not
appear to be effective. We could possibly explore in-
stance weight strategies (?) on both pseudo-labeled
examples and source domain training examples in
order to improve the adaptation performance.
Acknowledgements
This work was supported in part by the EC-FP7
projects ROBUST (grant number 257859).
References
R.K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. The Journal of Machine Learning Re-
search, 6:1817?1853.
A. Aue and M. Gamon. 2005. Customizing sentiment
classifiers to new domains: a case study. In Proceed-
ings of Recent Advances in Natural Language Process-
ing (RANLP).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
130
2003. Latent Dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies,
bollywood, boom-boxes and blenders: Domain adap-
tation for sentiment classification. In ACL, page 440?
447.
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy classifier: Little data can help a lot. In
EMNLP.
W. Dai, Y. Chen, G.R. Xue, Q. Yang, and Y. Yu. 2008.
Translated learning: Transfer learning across different
feature spaces. In NIPS, pages 353?360.
W. Dai, O. Jin, G.R. Xue, Q. Yang, and Y. Yu. 2009.
Eigentransfer: a unified framework for transfer learn-
ing. In ICML, pages 193?200.
H. Daume? III and D. Marcu. 2006. Domain adaptation
for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26(1):101?126.
H. Daume?. 2007. Frustratingly easy domain adaptation.
In ACL, pages 256?263.
J. Jiang and C.X. Zhai. 2007. Instance weighting for
domain adaptation in NLP. In ACL, pages 264?271.
A. Kennedy and D. Inkpen. 2006. Sentiment clas-
sification of movie reviews using contextual valence
shifters. Computational Intelligence, 22(2):110?125.
S. Li, C.R. Huang, G. Zhou, and S.Y.M. Lee. 2010.
Employing personal/impersonal views in supervised
and semi-supervised sentiment classification. In ACL,
pages 414?423.
C. Lin and Y. He. 2009. Joint sentiment/topic model for
sentiment analysis. In Proceedings of the 18th ACM
international conference on Information and knowl-
edge management (CIKM), pages 375?384.
C. Lin, Y. He, and R. Everson. 2010. A Compara-
tive Study of Bayesian Models for Unsupervised Sen-
timent Detection. In Proceedings of the 14th Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 144?152.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In ACL, pages 432?
439.
T. Minka. 2003. Estimating a Dirichlet distribution.
Technical report.
S.J. Pan, X. Ni, J.T. Sun, Q. Yang, and Z. Chen. 2010.
Cross-domain sentiment classification via spectral fea-
ture alignment. In Proceedings of the 19th interna-
tional conference on World Wide Web (WWW), pages
751?760.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL, page 271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In EMNLP, pages 79?86.
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
NAACL-HLT, pages 126?133.
C.W. Seah, I. Tsang, Y.S. Ong, and K.K. Lee. 2010. Pre-
dictive Distribution Matching SVM for Multi-domain
Learning. In ECML-PKDD, pages 231?247.
Casey Whitelaw, Navendu Garg, and Shlomo Argamon.
2005. Using appraisal groups for sentiment analysis.
In Proceedings of the ACM international conference
on Information and Knowledge Management (CIKM),
pages 625?631.
Q. Wu, S. Tan, and X. Cheng. 2009. Graph ranking for
sentiment transfer. In ACL-IJCNLP, pages 317?320.
Q. Wu, S. Tan, X. Cheng, and M. Duan. 2010. MIEA:
a Mutual Iterative Enhancement Approach for Cross-
Domain Sentiment Classification. In COLING, page
1327-1335.
A. Yessenalina, Y. Choi, and C. Cardie. 2010a. Auto-
matically generating annotator rationales to improve
sentiment classification. In ACL, pages 336?341.
A. Yessenalina, Y. Yue, and C. Cardie. 2010b. Multi-
Level Structured Models for Document-Level Senti-
ment Classification. In EMNLP, pages 1046?1056.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding re-
dundant features for CRFs-based sentence sentiment
classification. In EMNLP, pages 117?126.
131
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 58?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Empirical Study on Uncertainty Identification in Social Media Context
Zhongyu Wei1, Junwen Chen1, Wei Gao2,
Binyang Li1, Lanjun Zhou1, Yulan He3, Kam-Fai Wong1
1The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
3School of Engineering & Applied Science, Aston University, Birmingham, UK
{zywei,jwchen,byli,ljzhou,kfwong}@se.cuhk.edu.hk
wgao@qf.org.qa, y.he@cantab.net
Abstract
Uncertainty text detection is important
to many social-media-based applications
since more and more users utilize social
media platforms (e.g., Twitter, Facebook,
etc.) as information source to produce
or derive interpretations based on them.
However, existing uncertainty cues are in-
effective in social media context because
of its specific characteristics. In this pa-
per, we propose a variant of annotation
scheme for uncertainty identification and
construct the first uncertainty corpus based
on tweets. We then conduct experiments
on the generated tweets corpus to study the
effectiveness of different types of features
for uncertainty text identification.
1 Introduction
Social media is not only a social network tool for
people to communicate but also plays an important
role as information source with more and more
users searching and browsing news on it. People
also utilize information from social media for de-
veloping various applications, such as earthquake
warning systems (Sakaki et al, 2010) and fresh
webpage discovery (Dong et al, 2010). How-
ever, due to its casual and word-of-mouth pecu-
liarities, the quality of information in social me-
dia in terms of factuality becomes a premier con-
cern. Chances are there for uncertain information
or even rumors flooding in such a context of free
form. We analyzed a tweet dataset which includes
326,747 posts (Details are given in Section 3) col-
lected during 2011 London Riots, and result re-
veals that at least 18.91% of these tweets bear un-
certainty characteristics1. Therefore, distinguish-
ing uncertain statements from factual ones is cru-
cial for users to synthesize social media informa-
tion to produce or derive reliable interpretations,
1The preliminary study was done based on a manually de-
fined uncertainty cue-phrase list. Tweets containing at least
one hedge cue were treated as uncertain.
and this is expected helpful for applications like
credibility analysis (Castillo et al, 2011) and ru-
mor detection (Qazvinian et al, 2011) based on
social media.
Although uncertainty has been studied theoret-
ically for a long time as a grammatical phenom-
ena (Seifert and Welte, 1987), the computational
treatment of uncertainty is a newly emerging area
of research. Szarvas et al (2012) pointed out that
?Uncertainty - in its most general sense - can be
interpreted as lack of information: the receiver of
the information (i.e., the hearer or the reader) can-
not be certain about some pieces of information?.
In recent years, the identification of uncertainty
in formal text, e.g., biomedical text, reviews or
newswire, has attracted lots of attention (Kilicoglu
and Bergler, 2008; Medlock and Briscoe, 2007;
Szarvas, 2008; Light et al, 2004). However, un-
certainty identification in social media context is
rarely explored.
Previous research shows that uncertainty identi-
fication is domain dependent as the usage of hedge
cues varies widely in different domains (Morante
and Sporleder, 2012). Therefore, the employment
of existing out-of-domain corpus to social media
context is ineffective. Furthermore, compared to
the existing uncertainty corpus, the expression of
uncertainty in social media is fairly different from
that in formal text in a sense that people usu-
ally raise questions or refer to external informa-
tion when making uncertain statements. But, nei-
ther of the uncertainty expressions can be repre-
sented based on the existing types of uncertainty
defined in the literature. Therefore, a different un-
certainty classification scheme is needed in social
media context.
In this paper, we propose a novel uncertainty
classification scheme and construct the first uncer-
tainty corpus based on social media data ? tweets
in specific here. And then we conduct experi-
ments for uncertainty post identification and study
the effectiveness of different categories of features
based on the generated corpus.58
2 Related work
We introduce some popular uncertainty corpora
and methods for uncertainty identification.
2.1 Uncertainty corpus
Several text corpora from various domains have
been annotated over the past few years at different
levels (e.g., expression, event, relation, sentence)
with information related to uncertainty.
Sauri and Pustejovsky (2009) presented a cor-
pus annotated with information about the factu-
ality of events, namely Factbank, which is con-
structed based on TimeBank2 containing 3,123 an-
notated sentences from 208 news documents with
8 different levels of uncertainty defined.
Vincze et al (2008) constructed the BioSocpe
corpus, which consists of medical and biological
texts annotated for negation, uncertainty and their
linguistic scope. This corpus contains 20,924 sen-
tences.
Ganter et al (2009) generated Wikipedia
Weasels Corpus, where Weasel tags in Wikipedia
articles is adopted readily as labels for uncertainty
annotation. It contains 168,923 unique sentences
with 437 weasel tags in total.
Although several uncertainty corpora exist,
there is not a uniform set of standard for uncer-
tainty annotation. Szarvas et al (2012) normal-
ized the annotation of the three corpora aforemen-
tioned. However, the context of these corpora
is different from that of social media. Typically,
these documents annotated are grammatically cor-
rect, carefully punctuated, formally structured and
logically expressed.
2.2 Uncertainty identification
Previous work on uncertainty identification fo-
cused on classifying sentences into uncertain
or definite categories. Existing approaches are
mainly based on supervised methods (Light et
al., 2004; Medlock and Briscoe, 2007; Medlock,
2008; Szarvas, 2008) using the annotated corpus
with different types of features including Part-Of-
Speech (POS) tags, stems, n-grams, etc..
Classification of uncertain sentences was con-
solidated as a task in the 2010 edition of CoNLL
shared task on learning to detect hedge cues
and their scope in natural language text (Farkas
et al, 2010). The best system for Wikipedia
data (Georgescul, 2010) employed Support Vector
Machine (SVM), and the best system for biolog-
ical data (Tang et al, 2010) adopted Conditional
2http://www.timeml.org/site/timebank/
timebank.html
Random Fields (CRF).
In our work, we conduct an empirical study of
uncertainty identification on tweets dataset and ex-
plore the effectiveness of different types of fea-
tures (i.e., content-based, user-based and Twitter-
specific) from social media context.
3 Uncertainty corpus for microblogs
3.1 Types of uncertainty in microblogs
Traditionally, uncertainty can be divided into
two categories, namely Epistemic and Hypothet-
ical (Kiefer, 2005). For Epistemic, there are two
sub-classes Possible and Probable. For Hypotheti-
cal, there are four sub-classes including Investiga-
tion, Condition, Doxastic andDynamic. The detail
of the classification is described as below (Kiefer,
2005):
Epistemic: On the basis of our world knowledge
we cannot decide at the moment whether the
statement is true or false.
Hypothetical: This type of uncertainty includes
four sub-classes:
? Doxastic: Expresses the speaker?s be-
liefs and hypotheses.
? Investigation: Proposition under inves-
tigation.
? Condition: Proposition under condi-
tion.
? Dynamic: Contains deontic, disposi-
tional, circumstantial and buletic modal-
ity.
Compared to the existing uncertainty corpora,
social media authors enjoy free form of writing.
In order to study the difference, we annotated a
small set of 827 randomly sampled tweets accord-
ing to the scheme of uncertainty types above, in
which we found 65 uncertain tweets. And then,
we manually identified all the possible uncertain
tweets, and found 246 really uncertain ones out of
these 827 tweets, which means that 181 uncertain
tweets are missing based on this scheme. We have
the following three salient observations:
? Firstly, there is no tweet found with the type of
Investigation. We find people seldom use words
like ?examine? or ?test? (indicative words of In-
vestigation category) when posting tweets. Once
they do this, the statement should be considered
as highly certain. For example, @dobibid I have
tested the link, it is fake!
? Secondly, people frequently raise questions
about some specific topics for confirmation which
expresses uncertainty. For example, @ITVCentral59
Can you confirm that Birmingham children?s hos-
pital has/hasn?t been attacked by rioters?
? Thirdly, people tend to post message with exter-
nal information (e.g., story from friends) which re-
veals uncertainty. For example, Friend who works
at the children?s hospital in Birmingham says the
riot police are protecting it.
Based on these observations, we propose a vari-
ant of uncertainty types in social media context
by eliminating the category of Investigation and
adding the category of Question and External un-
der Hypothetical, as shown in Table 3.1. Note
that our proposed scheme is based on Kiefer?s
work (2005) which was previously extended to
normalize uncertainty corpora in different genres
by Szarvas et al (2012). But we did not try these
extended schema for specific genres since even the
most general one (Kiefer, 2005) was proved un-
suitable for social media context.
3.2 Annotation result
The dataset we annotated was collected from Twit-
ter using Streaming API during summer riots
in London during August 6-13 2011, including
326,747 tweets in total. Search criteria include
hashtags like #ukriots, #londonriots, #prayforlon-
don, and so on. We further extracted the tweets
relating to seven significant events during the riot
identified by UK newspaper The Guardian from
this set of tweets. We annotated all the 4,743 ex-
tracted tweets for the seven events3.
Two annotators were trained to annotate the
dataset independently. Given a collection of
tweets T = {t1, t2, t3...tn}, the annotation task is
to label each tweet ti as either uncertain or cer-
tain. Uncertainty assertions are to be identified
in terms of the judgements about the author?s in-
tended meaning rather than the presence of uncer-
tain cue-phrase. For those tweets annotated as un-
certain, sub-class labels are also required accord-
ing to the classification indicated in Table 3.1 (i.e.,
multi-label is allowed).
The Kappa coefficient (Carletta, 1996) indi-
cating inter-annotator agreement was 0.9073 for
the certain/uncertain binary classification and was
0.8271 for fine-grained annotation. The conflict
labels from the two annotators were resolved by a
third annotator. Annotation result is displayed in
Table 3.2, where 926 out of 4,743 tweets are la-
beled as uncertain accounting for 19.52%. Ques-
tion is the uncertainty category with most tweets,
followed by External. Only 21 tweets are labeled
3http://www.guardian.co.uk/
uk/interactive/2011/dec/07/
london-riots-twitter
Tweet# 4743
Uncertainty# 926
Epistemic Possible# 16Probable# 129
Hypothetical
Condition# 71
Doxastic# 48
Dynamic# 21
External# 208
Question# 488
Table 2: Statistics of annotation result
as Dynamic and all of them are buletic modal-
ity4 which shares similarity with Doxastic. There-
fore, we consider Dynamic together with Domes-
tic in the error analysis for simplicity. During
the preliminary annotation, we found that uncer-
tainty cue-phrase is a good indicator for uncer-
tainty tweets since tweets labeled as uncertain al-
ways contain at least one cue-phrase. Therefore,
annotators are also required identify cue-phrases
which trigger the sense of uncertainty in the tweet.
All cue-phrases appearing more than twice are col-
lected to form a uncertainty cue-phrase list.
4 Experiment and evaluation
We aim to identify those uncertainty tweets from
tweet collection automatically based on machine
learning approaches. In addition to n-gram fea-
tures, we also explore the effectiveness of three
categories of social media specific features includ-
ing content-based, user-based and Twitter-specific
ones. The description of the three categories of
features is shown in Table 4. Since the length of
tweet is relatively short, we therefore did not carry
out stopwords removal or stemming.
Our preliminary experiments showed that com-
bining unigrams with bigrams and trigrams gave
better performance than using any one or two of
these three features. Therefore, we just report the
result based on the combination of them as n-gram
features. Five-fold cross validation is used for
evaluation. Precision, recall and F-1 score of un-
certainty category are used as the metrics.
4.1 Overall performance
The overall performance of different approaches
is shown in Table 4.1. We used uncertainty cue-
phrase matching approach as baseline, denoted
by CP. For CP, we labeled tweets containing at
least one entry in uncertainty cue-phrase list (de-
scribed in Section 3) as uncertain. All the other
approaches are supervised methods using SVM
based on different feature sets. n-gram stands for
n-gram feature set, C means content-based feature
set, U denotes user-based feature set, T represents
4Proposition expresses plans, intentions or desires.60
Category Subtype Cue Phrase Example
Epistemic Possible, etc. may, etc. It may be raining.Probable likely, etc. It is probably raining.
Hypothetical
Condition if, etc. If it rains, we?ll stay in.
Doxastic believe, etc. He believes that the Earth is flat.
Dynamic hope, etc. fake picture of the london eye on fire... i hope
External someone said, etc. Someone said that London zoo was attacked.
Question seriously?, etc. Birmingham riots are moving to the children hospital?! seriously?
Table 1: Classification of uncertainty in social media context
Category Name Description
Content-based
Length Length of the tweet
Cue Phrase Whether the tweet contains a uncertainty cue
OOV Ratio Ratio of words out of vocabulary
Twitter-specific
URL Whether the tweet contains a URL
URL Count Frequency of URLs in corpus
Retweet Count How many times has this tweet been retweeted
Hashtag Whether the tweet contains a hashtag
Hashtag Count Number of Hashtag in tweets
Reply Is the current tweet a reply tweet
Rtweet Is the current tweet a retweet tweet
User-based
Follower Count Number of follower the user owns
List Count Number of list the users owns
Friend Count Number of friends the user owns
Favorites Count Number of favorites the user owns
Tweet Count Number of tweets the user published
Verified Whether the user is verified
Table 3: Feature list for uncertainty classification
Approach Precision Recall F-1
CP 0.3732 0.9589 0.5373
SVMn?gram 0.7278 0.8259 0.7737
SVMn?gram+C 0.8010 0.8260 0.8133
SVMn?gram+U 0.7708 0.8271 0.7979
SVMn?gram+T 0.7578 0.8266 0.7907
SVMn?gram+ALL 0.8162 0.8269 0.8215
SVMn?gram+Cue Phrase 0.7989 0.8266 0.8125SVMn?gram+Length 0.7372 0.8216 0.7715SVMn?gram+OOV Ratio 0.7414 0.8233 0.7802
Table 4: Result of uncertainty tweets identification
Twitter-specific feature set and ALL is the combi-
nation of C, U and T.
Table 4.1 shows that CP achieves the best recall
but its precision is the lowest. The learning based
methods with different feature sets give some sim-
ilar recalls. Compared to CP, SVMn?gram in-
creases the F-1 score by 43.9% due to the salient
improvement on precision and small drop of re-
call. The performance improves in terms of pre-
cision and F-1 score when the feature set is ex-
panded by adding C, U or T onto n-gram, where
+C brings the highest gain, and SVMn?gram+ALL
performs best in terms of precision and F-1 score.
We then study the effectiveness of the three
content-based features, and result shows that the
presence of uncertain cue-phrase is most indica-
tive for uncertainty tweet identification.
4.2 Error analysis
We analyze the prediction errors based on
SVMn?gram+ALL. The distribution of errors in
terms of different types of uncertainty is shown
Type Poss. Prob. D.&D. Cond. Que. Ext.
Total# 16 129 69 71 488 208
Error# 11 20 18 11 84 40
% 0.69 0.16 0.26 0.15 0.17 0.23
Table 5: Error distributions
in Table 4.2. Our method performs worst on the
type of Possible and on the combination of Dy-
namic and Doxastic because these two types have
the least number of samples in the corpus and the
classifier tends to be undertrained without enough
samples.
5 Conclusion and future work
In this paper, we propose a variant of classification
scheme for uncertainty identification in social me-
dia and construct the first uncertainty corpus based
on tweets. We perform uncertainty identification
experiments on the generated dataset to explore
the effectiveness of different types of features. Re-
sult shows that the three categories of social media
specific features can improve uncertainty identifi-
cation. Furthermore, content-based features bring
the highest improvement among the three and the
presence of uncertain cue-phrase contributes most
for content-based features.
In future, we will explore to use uncertainty
identification for social media applications.
6 Acknowledgement
This work is partially supported by General Re-
search Fund of Hong Kong (No. 417112).61
References
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249?254.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th International Conference
on World Wide Web, pages 675?684.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 331?340. ACM.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The conll-
2010 shared task: learning to detect hedges and their
scope in natural language text. In Proceedings of
the 14th Conference on Computational Natural Lan-
guage Learning?Shared Task, pages 1?12. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009, pages 173?
176. Association for Computational Linguistics.
Maria Georgescul. 2010. A hedgehop over a max-
margin framework using hedge cues. In Proceed-
ings of the 14th Conference on Computational Natu-
ral Language Learning?Shared Task, pages 26?31.
Association for Computational Linguistics.
Ferenc Kiefer. 2005. Lehetoseg es szuk-
segszeruseg[Possibility and necessity]. Tinta Kiado,
Budapest.
H. Kilicoglu and S. Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a
linguistically motivated perspective. BMC bioinfor-
matics, 9(Suppl 11):S10.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings
of BioLink 2004 workshop on linking biological lit-
erature, ontologies and databases: tools for users,
pages 17?24.
B. Medlock and T. Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific litera-
ture. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
992?999.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636?654.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational Linguistics, 38(2):223?260.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589?1599.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, pages 851?860. ACM.
R. Saur?? and J. Pustejovsky. 2009. Factbank: A cor-
pus annotated with event factuality. Language Re-
sources and Evaluation, 43(3):227?268.
Stephan Seifert and Werner Welte. 1987. A basic bib-
liography on negation in natural language, volume
313. Gunter Narr Verlag.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335?367.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the 14th Conference on Compu-
tational Natural Language Learning?Shared Task,
pages 13?17. Association for Computational Lin-
guistics.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and
J. Csirik. 2008. The bioscope corpus: biomedical
texts annotated for uncertainty, negation and their
scopes. BMC bioinformatics, 9(Suppl 11):S9.
62
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 618?624,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Automatic Labelling of Topic Models Learned from Twitter by
Summarisation
Amparo Elizabeth Cano Basave
?
Yulan He
?
Ruifeng Xu
?
?
Knowledge Media Institute, Open University, UK
?
School of Engineering and Applied Science, Aston University, UK
?
Key Laboratory of Network Oriented Intelligent Computation
Shenzhen Graduate School, Harbin Institute of Technology, China
amparo.cano@open.ac.uk, y.he@cantab.net, xuruifeng@hitsz.edu.cn
Abstract
Latent topics derived by topic models such
as Latent Dirichlet Allocation (LDA) are
the result of hidden thematic structures
which provide further insights into the
data. The automatic labelling of such
topics derived from social media poses
however new challenges since topics may
characterise novel events happening in the
real world. Existing automatic topic la-
belling approaches which depend on exter-
nal knowledge sources become less appli-
cable here since relevant articles/concepts
of the extracted topics may not exist in ex-
ternal sources. In this paper we propose
to address the problem of automatic la-
belling of latent topics learned from Twit-
ter as a summarisation problem. We in-
troduce a framework which apply sum-
marisation algorithms to generate topic la-
bels. These algorithms are independent
of external sources and only rely on the
identification of dominant terms in doc-
uments related to the latent topic. We
compare the efficiency of existing state
of the art summarisation algorithms. Our
results suggest that summarisation algo-
rithms generate better topic labels which
capture event-related context compared to
the top-n terms returned by LDA.
1 Introduction
Topic model based algorithms applied to social
media data have become a mainstream technique
in performing various tasks including sentiment
analysis (He, 2012) and event detection (Zhao et
al., 2012; Diao et al, 2012). However, one of
the main challenges is the task of understanding
the semantics of a topic. This task has been ap-
proached by investigating methodologies for iden-
tifying meaningful topics through semantic coher-
ence (Aletras and Stevenson, 2013; Mimno et al,
2011; Newman et al, 2010) and for characterising
the semantic content of a topic through automatic
labelling techniques (Hulpus et al, 2013; Lau et
al., 2011; Mei et al, 2007). In this paper we focus
on the latter.
Our research task of automatic labelling a topic
consists on selecting a set of words that best de-
scribes the semantics of the terms involved in this
topic. The most generic approach to automatic la-
belling has been to use as primitive labels the top-
n words in a topic distribution learned by a topic
model such as LDA (Griffiths and Steyvers, 2004;
Blei et al, 2003). Such top words are usually
ranked using the marginal probabilities P (w
i
|t
j
)
associated with each word w
i
for a given topic t
j
.
This task can be illustrated by considering the fol-
lowing topic derived from social media related to
Education:
school protest student fee choic motherlod
tuition teacher anger polic
where the top 10 words ranked by P (w
i
|t
j
) for
this topic are listed. Therefore the task is to find
the top-n terms which are more representative of
the given topic. In this example, the topic certainly
relates to a student protest as revealed by the top
3 terms which can be used as a good label for this
topic.
However previous work has shown that top
terms are not enough for interpreting the coherent
meaning of a topic (Mei et al, 2007). More re-
cent approaches have explored the use of external
sources (e.g. Wikipedia, WordNet) for supporting
the automatic labelling of topics by deriving can-
didate labels by means of lexical (Lau et al, 2011;
Magatti et al, 2009; Mei et al, 2007) or graph-
based (Hulpus et al, 2013) algorithms applied on
these sources.
Mei et al (2007) proposed an unsupervised
probabilistic methodology to automatically assign
a label to a topic model. Their proposed approach
618
was defined as an optimisation problem involving
the minimisation of the KL divergence between a
given topic and the candidate labels while max-
imising the mutual information between these two
word distributions. Lau et al (2010) proposed to
label topics by selecting top-n terms to label the
overall topic based on different ranking mecha-
nisms including pointwise mutual information and
conditional probabilities.
Methods relying on external sources for auto-
matic labelling of topics include the work by Ma-
gatti et al (2009) which derived candidate topic
labels for topics induced by LDA using the hi-
erarchy obtained from the Google Directory ser-
vice and expanded through the use of the OpenOf-
fice English Thesaurus. Lau et al (2011) gen-
erated label candidates for a topic based on top-
ranking topic terms and titles of Wikipedia arti-
cles. They then built a Support Vector Regres-
sion (SVR) model for ranking the label candidates.
More recently, Hulpus et al (2013) proposed to
make use of a structured data source (DBpedia)
and employed graph centrality measures to gener-
ate semantic concept labels which can characterise
the content of a topic.
Most previous topic labelling approaches focus
on topics derived from well formatted and static
documents. However in contrast to this type of
content, the labelling of topics derived from tweets
presents different challenges. In nature microp-
ost content is sparse and present ill-formed words.
Moreover, the use of Twitter as the ?what?s-
happening-right now? tool, introduces new event-
dependent relations between words which might
not have a counter part in existing knowledge
sources (e.g. Wikipedia). Our original interest in
labelling topics stems from work in topic model
based event extraction from social media, in par-
ticular from tweets (Shen et al, 2013; Diao et
al., 2012). As opposed to previous approaches,
the research presented in this paper addresses the
labelling of topics exposing event-related content
that might not have a counter part on existing ex-
ternal sources. Based on the observation that a
short summary of a collection of documents can
serve as a label characterising the collection, we
propose to generate topic label candidates based
on the summarisation of a topic?s relevant docu-
ments. Our contributions are two-fold:
- We propose a novel approach for topics la-
belling that relies on term relevance of documents
relating to a topic; and
- We show that summarisation algorithms,
which are independent of extenal sources, can be
used with success to label topics, presenting a
higher perfomance than the top-n terms baseline.
2 Methodology
We propose to approach the topic labelling prob-
lem as a multi-document summarisation task. The
following describes our proposed framework to
characterise documents relevant to a topic.
2.1 Preliminaries
Given a set of documents the problem to be solved
by topic modelling is the posterior inference of the
variables, which determine the hidden thematic
structures that best explain an observed set of doc-
uments. Focusing on the Latent Dirichlet Alloca-
tion (LDA) model (Blei et al, 2003; Griffiths and
Steyvers, 2004), let D be a corpus of documents
denoted as D = {d
1
,d
2
, ..,d
D
}; where each doc-
ument consists of a sequence ofN
d
words denoted
by d = (w
1
, w
2
, .., w
N
d
); and each word in a
document is an item from a vocabulary index of
V different terms denoted by {1, 2, .., V }. Given
D documents containing K topics expressed over
V unique words, LDA generative process is de-
scribed as follows:
- For each topic k ? {1, ...K} draw ?
k
?
Dirichlet(?),
- For each document d ? {1..D}:
? draw ?
d
? Dirichlet(?);
? For each word n ? {1..N
d
} in document d:
? draw a topic z
d,n
? Multinomial(?
d
);
? draw a word w
d,n
? Multinomial(?
z
d,n
).
where ?
k
is the word distribution for topic k,
and ?
d
is the distribution of topics in document
d. Topics are interpreted using the top N terms
ranked based on the marginal probability p(w
i
|t
j
).
2.2 Automatic Labelling of Topic Models
Given K topics over the document collection D,
the topic labelling task consists on discovering a
sequence of words for each topic k ? K. We pro-
pose to generate topic label candidates by sum-
marising topic relevant documents. Such docu-
ments can be derived using both the observed data
from the corpus D and the inferred topic model
variables. In particular, the prominent topic of a
document d can be found by
k
d
= argmax
k?K
p(k|d) (1)
619
Therefore given a topic k, a set of C documents
related to this topic can be obtained via equation
1.
Given the set of documents C relevant to topic k,
we proposed to generate a label of a desired length
x from the summarisation of C.
2.3 Topic Labelling by Summarisation
We compare different summarisation algorithms
based on their ability to provide a good label to a
given topic. In particular we investigate the use of
lexical features by comparing three different well-
known multi-document summarisation algorithms
against the top-n topic terms baseline. These al-
gorithms include:
Sum Basic (SB) This is a frequency based sum-
marisation algorithm (Nenkova and Vanderwende,
2005), which computes initial word probabilities
for words in a text. It then weights each sen-
tence in the text (in our case a micropost) by
computing the average probability of the words in
the sentence. In each iteration it picks the high-
est weighted document and from it the highest
weighted word. It uses an update function which
penalises words which have already been picked.
Hybrid TFIDF (TFIDF) It is similar to SB,
however rather than computing the initial word
probabilities based on word frequencies it weights
terms based on TFIDF. In this case the document
frequency is computed as the number of times a
word appears in a micropost from the collection
C. Following the same procedure as SB it returns
the top x weighted terms.
Maximal Marginal Relevance (MMR) This is a
relevance based ranking algorithm (Carbonell and
Goldstein, 1998), which avoids redundancy in the
documents used for generating a summary. It mea-
sures the degree of dissimilarity between the docu-
ments considered and previously selected ones al-
ready in the ranked list.
Text Rank (TR) This is a graph-based sum-
mariser method (Mihalcea and Tarau, 2004) where
each word is a vertex. The relevance of a vertex
(term) to the graph is computed based on global
information recursively drawn from the whole
graph. It uses the PageRank algorithm (Brin and
Page, 1998) to recursively change the weight of
the vertices. The final score of a word is there-
fore not only dependent on the terms immediately
connected to it but also on how these terms con-
nect to others. To assign the weight of an edge
between two terms, TextRank computes word co-
occurrence in windows of N words (in our case
N = 10). Once a final score is calculated for each
vertex of the graph, TextRank sorts the terms in
a reverse order and provided the top T vertices in
the ranking. Each of these algorithms produces a
label of a desired length x for a given topic k.
3 Experimental Setup
3.1 Dataset
Our Twitter Corpus (TW) was collected between
November 2010 and January 2011. TW comprises
over 1 million tweets. We used the OpenCalais?
document categorisation service
1
to generate cate-
gorical sets. In particular, we considered four dif-
ferent categories which contain many real-world
events, namely: War and Conflict (War), Disaster
and Accident (DisAc), Education (Edu) and Law
and Crime (LawCri). The final TW dataset after
removing retweets and short microposts (less than
5 words after removing stopwords) contains 7000
tweets in each category.
We preprocessed TW by first removing: punc-
tuation, numbers, non-alphabet characters, stop
words, user mentions, and URL links. We then
performed Porter stemming (Porter, 1980) in order
to reduce the vocabulary size. Finally to address
the issue of data sparseness in the TW dataset, we
removed words with a frequency lower than 5.
3.2 Generating the Gold Standard
Evaluation of automatic topic labelling often re-
lied on human assessment which requires heavy
manual effort (Lau et al, 2011; Hulpus et al,
2013). However performing human evaluations of
Social Media test sets comprising thousands of in-
puts become a difficult task. This is due to both
the corpus size, the diversity of event-related top-
ics and the limited availability of domain experts.
To alleviate this issue here, we followed the distri-
bution similarity approach, which has been widely
applied in the automatic generation of gold stan-
dards (GSs) for summary evaluations (Donaway et
al., 2000; Lin et al, 2006; Louis and Nenkova,
2009; Louis and Nenkova, 2013). This approach
compares two corpora, one for which no GS labels
exist, against a reference corpus for which a GS
exists. In our case these corpora correspond to the
TW and a Newswire dataset (NW). Since previous
1
OpenCalais service, http://www.opencalais.com
620
research has shown that headlines are good indi-
cators of the main focus of a text, both in struc-
ture and content, and that they can act as a human
produced abstract (Nenkova, 2005), we used head-
lines as the GS labels of NW.
The News Corpus (NW) was collected during
the same period of time as the TW corpus. NW
consists of a collection of news articles crawled
from traditional news media (BBC, CNN, and
New York Times) comprising over 77,000 articles
which include supplemental metadata (e.g. head-
line, author, publishing date). We also used the
OpenCalais? document categorisation service to
automatically label news articles and considered
the same four topical categories, (War, DisAc,
Edu and LawCri). The same preprocessing steps
were performed on NW.
Therefore, following a similarity alignment ap-
proach we performed the steps oulined in Algo-
rithm 1 for generating the GS topic labels of a topic
in TW.
Algorithm 1 GS for Topic Labels
Input: LDA topics for TW, and the LDA topics for NW for
category c.
Output: Gold standard topic label for each of the LDA top-
ics for TW.
1: for each topic i ? {1, 2, ..., 100} from TW do
2: for each topic j ? {1, 2..., 100} from NW do
3: Compute the Cosine similarity between word dis-
tributions of topic t
i
and topic t
j
.
4: end for
5: Select topic j which has the highest similarity to i and
whose similarity measure is greater than a threshold
(in this case 0.7)
6: end for
7: for each of the extracted topic pairs (t
i
? t
j
) do
8: Collect relevant news articles C
j
NW
of topic t
j
from
the NW set.
9: Extract the headlines of news articles from C
j
NW
and
select the top x most frequent words as the gold stan-
dard label for topic t
i
in the TW set
10: end for
These steps can be outlined as follows:1) We
ran LDA on TW and NW separately for each cate-
gory with the number of topics set to 100; 2) We
then aligned the Twitter topics and Newswire top-
ics by the similarity measurement of word distri-
butions of these topics (Ercan and Cicekli, 2008;
Haghighi and Vanderwende, 2009; Wang et al,
2009; Delort and Alfonseca, 2012); 3) Finally to
generate the GS label for each aligned topic pair
(t
i
? t
j
), we extracted the headlines of the news
articles relevant to t
j
and selected the top x most
frequent words (after stop word removal and stem-
ming). The generated label was used as the gold
standard label for the corresponding Twitter topic
t
i
in the topic pair.
4 Experimental Results
We compared the results of the summarisation
techniques with the top terms (TT) of a topic as
our baseline. These TT set corresponds to the
top x terms ranked based on the probability of
the word given the topic (p(w|k)) from the topic
model. We evaluated these summarisation ap-
proaches with the ROUGE-1 method (Lin, 2004),
a widely used summarisation evaluation metric
that correlates well with human evaluation (Liu
and Liu, 2008). This method measures the over-
lap of words between the generated summary and
a reference, in our case the GS generated from the
NW dataset.
The evaluation was performed at x =
{1, .., 10}. Figure 1 presents the ROUGE-1 per-
formance of the summarisation approaches as the
lengthx of the generated topic label increases. We
can see in all four categories that the SB and
TFIDF approaches provide a better summarisa-
tion coverage as the length of the topic label in-
creases. In particular, in both the Education
and Law & Crime categories, both SB and
TFIDF outperforms TT and TR by a large margin.
The obtained ROUGE-1 performance is within the
same range of performance previously reported on
Social Media summarisation (Inouye and Kalita,
2011; Nichols et al, 2012; Ren et al, 2013).
Table 1 presents average results for ROUGE-
1 in the four categories. Particularly the SB
and TFIDF summarisation techniques consis-
tently outperform the TT baseline across all four
categories. SB gives the best results in three cate-
gories except War.
ROUGE-1
TT SB TFIDF MMR TR
War 0.162 0.184 0.192 0.154 0.141
DisAc 0.134 0.194 0.160 0.132 0.124
Edu 0.106 0.240 0.187 0.104 0.023
LawCri 0.035 0.159 0.149 0.034 0.115
Table 1: Average ROUGE-1 for topic labels at x =
{1..10}, generated from the TW dataset.
The generated labels with summarisation at x =
5 are presented in Table 2, where GS represents the
label generated from the Newswire headlines.
Different summarisation techniques reveal
words which do not appear in the top terms but
621
0.05
0.10
0.15
0.20
0.25
2.5 5.0 7.5 10.0
x
Ro
uge
War_Conflict
0.10
0.15
0.20
0.25
2.5 5.0 7.5 10.0
x
Ro
uge
Disaster_Accident
0.1
0.2
2.5 5.0 7.5 10.0
x
Ro
uge
Education
0.00
0.05
0.10
0.15
0.20
2.5 5.0 7.5 10.0
x
Ro
uge
Law_Crime
Tw
itte
r To
pic
s
variable
TT
SB
TFIDF
TR
MMR
Figure 1: Performance in ROUGE for Twitter-derived topic labels, where x is the number of terms in the
generated label
which are relevant to the information clustered
by the topic. In this way, the labels generated for
topics belonging to different categories generally
extend the information provided by the top terms.
For example in Table 2, the DisAc headline is
characteristic of the New Zealand?s Pike River?s
coal mine blast accident, which is an event
occurred in November 2010.
Although the top 5 terms set from the LDA topic
extracted from TW (listed under TT) does capture
relevant information related to the event, it does
not provide information regarding the blast. In this
sense the topic label generated by SB more accu-
rately describes this event.
We can also notice that the GS labels generated
from Newswire media presented in Table 2 appear
on their own, to be good labels for the TW topics.
However as we described in the introduction we
want to avoid relaying on external sources for the
derivation of topic labels.
This experiment shows that frequency based
summarisation techniques outperform graph-
based and relevance based summarisation
techniques for generating topic labels that im-
prove upon the top-terms baseline, without relying
on external sources. This is an attractive property
for automatically generating topic labels for
tweets where their event-related content might not
have a counter part on existing external sources.
5 Conclusions and Future Work
In this paper we proposed a novel alternative to
topic labelling which do not rely on external data
sources. To the best of out knowledge no existing
work has been formally studied for automatic la-
belling through summarisation. This experiment
shows that existing summarisation techniques can
be exploited to provide a better label of a topic,
extending in this way a topic?s information by pro-
War DisAc
GS protest brief polic
afghanistan attack world
leader bomb obama
pakistan
mine zealand rescu miner
coal fire blast kill man dis-
ast
TT polic offic milit recent
mosqu
mine coal pike river
zealand
SB terror war polic arrest offic mine coal explos river pike
TFIDF polic war arrest offic terror mine coal pike safeti
zealand
MMR recent milit arrest attack
target
trap zealand coal mine ex-
plos
TR war world peac terror hope mine zealand plan fire fda
Edu LawCri
GS school protest student fee
choic motherlod tuition
teacher anger polic
man charg murder arrest
polic brief woman attack
inquiri found
TT student univers protest oc-
cupi plan
man law child deal jail
SB student univers school
protest educ
man arrest law kill judg
TFIDF student univers protest
plan colleg
man arrest law judg kill
MMR nation colleg protest stu-
dent occupi
found kid wife student jail
TR student tuition fee group
hit
man law child deal jail
Table 2: Labelling examples for topics generated
from the TW Dataset. GS represents the gold-
standard generated from the relevant Newswire
dataset. All terms are Porter stemmed as described
in subsection 3.1
viding a richer context than top-terms. These re-
sults show that there is room to further improve
upon existing summarisation techniques to cater
for generating candidate labels.
Acknowledgments
This work was supported by the EPRSC grant
EP/J020427/1, the EU-FP7 project SENSE4US
(grant no. 611242), and the Shenzhen Interna-
tional Cooperation Research Funding (grant num-
ber GJHZ20120613110641217).
622
References
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics.
In Proceedings of the 10th International Conference
on Computational Semantics (IWCS 2013) ? Long
Papers, pages 13?22, Potsdam, Germany, March.
Association for Computational Linguistics.
David Meir Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. In J. Mach. Learn.
Res. 3, pages 993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine*
1. In Computer networks and ISDN systems, vol-
ume 30, pages 107?117.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?98, pages 335?336, New York,
NY, USA. ACM.
Jean-Yves Delort and Enrique Alfonseca. 2012. Dual-
sum: A topic-model based approach for update sum-
marization. In Proceedings of the 13th Conference
of the European Chapter of the Association for Com-
putational Linguistics, EACL ?12, pages 214?223,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 536?544, Jeju Island, Korea,
July. Association for Computational Linguistics.
Robert L. Donaway, Kevin W. Drummey, and Laura A.
Mather. 2000. A comparison of rankings produced
by summarization evaluation measures. In Proceed-
ings of the 2000 NAACL-ANLP Workshop on Au-
tomatic Summarization, NAACL-ANLP-AutoSum
?00, pages 69?78, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Gonenc Ercan and Ilyas Cicekli. 2008. Lexical co-
hesion based topic modeling for summarization. In
Proceedings of the 9th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing?08, pages 582?592, Berlin, Hei-
delberg. Springer-Verlag.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL ?09, pages 362?370,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Yulan He. 2012. Incorporating sentiment prior
knowledge for weakly supervised sentiment analy-
sis. ACM Transactions on Asian Language Infor-
mation Processing, 11(2):4:1?4:19, June.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based
topic labelling using dbpedia. In Proceedings of the
sixth ACM international conference on Web search
and data mining, WSDM ?13, pages 465?474, New
York, NY, USA. ACM.
David Inouye and Jugal K. Kalita. 2011. Comparing
twitter summarization algorithms for multiple post
summaries. In SocialCom/PASSAT, pages 298?306.
IEEE.
Jey Han Lau, David Newman, Karimi Sarvnaz, and
Timothy Baldwin. 2010. Best Topic Word Selec-
tion for Topic Labelling. CoLing.
Jey Han Lau, Karl Grieser, David Newman, and Tim-
othy Baldwin. 2011. Automatic labelling of topic
models. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 1536?1545, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and
Jian-Yun Nie. 2006. An information-theoretic
approach to automatic evaluation of summaries.
In Proceedings of the Main Conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ?06, pages 463?
470, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Feifan Liu and Yang Liu. 2008. Correlation between
rouge and human evaluation of extractive meeting
summaries. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics on Human Language Technologies: Short
Papers, HLT-Short ?08, pages 201?204, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization with-
out human models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1 - Volume 1, EMNLP
?09, pages 306?314, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
623
standard. Computational Linguistics, 39(2):267?
300.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic labeling of top-
ics. In Proceedings of the 2009 Ninth International
Conference on Intelligent Systems Design and Appli-
cations, ISDA ?09, pages 1227?1232, Washington,
DC, USA. IEEE Computer Society.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery
and data mining, KDD ?07, pages 490?499, New
York, NY, USA. ACM.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing Order into Texts. In Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?04, pages 404?411, Barcelona, Spain. As-
sociation for Computational Linguistics.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 262?272, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization. Microsoft Re-
search, Redmond, Washington, Tech. Rep. MSR-TR-
2005-101.
Ani Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document un-
derstanding conference. In Proceedings of the 20th
National Conference on Artificial Intelligence - Vol-
ume 3, AAAI?05, pages 1436?1441. AAAI Press.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 100?108, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In
Proceedings of the 2012 ACM International Confer-
ence on Intelligent User Interfaces, IUI ?12, pages
189?198, New York, NY, USA. ACM.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Zhaochun Ren, Shangsong Liang, Edgar Meij, and
Maarten de Rijke. 2013. Personalized time-aware
tweets summarization. In Proceedings of the 36th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
?13, pages 513?522, New York, NY, USA. ACM.
Chao Shen, Fei Liu, Fuliang Weng, and Tao Li. 2013.
A participant-based approach for event summariza-
tion using twitter streams. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?13, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xin Zhao, Baihan Shu, Jing Jiang, Yang Song, Hongfei
Yan, and Xiaoming Li. 2012. Identifying event-
related bursts via social media activities. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1466?1477, Jeju Island, Korea, July. Association for
Computational Linguistics.
624
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 700?705,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Simple Bayesian Modelling Approach to Event Extraction from Twitter
Deyu Zhou
??
Liangyu Chen
?
Yulan He
?
?
School of Computer Science and Engineering, Key Laboratory of Computer Network
and Information Integration, Ministry of Education, Southeast University, China
?
State Key Laboratory for Novel Software Technology, Nanjing University, China
?
School of Engineering and Applied Science, Aston University, UK
d.zhou@seu.edu.cn, cly1cn@126.com, y.he@cantab.net
Abstract
With the proliferation of social media
sites, social streams have proven to con-
tain the most up-to-date information on
current events. Therefore, it is crucial to
extract events from the social streams such
as tweets. However, it is not straight-
forward to adapt the existing event ex-
traction systems since texts in social me-
dia are fragmented and noisy. In this pa-
per we propose a simple and yet effec-
tive Bayesian model, called Latent Event
Model (LEM), to extract structured rep-
resentation of events from social media.
LEM is fully unsupervised and does not
require annotated data for training. We
evaluate LEM on a Twitter corpus. Ex-
perimental results show that the proposed
model achieves 83% in F-measure, and
outperforms the state-of-the-art baseline
by over 7%.
1 Introduction
Event extraction is to automatically identify events
from text with information about what happened,
when, where, to whom, and why. Previous work in
event extraction has focused largely on news ar-
ticles, as the newswire texts have been the best
source of information on current events (Hogen-
boom et al, 2011). Approaches for event ex-
traction include knowledge-based (Piskorski et al,
2007; Tanev et al, 2008), data-driven (Piskorski
et al, 2008) and a combination of the above two
categories (Grishman et al, 2005). Knowledge-
based approaches often rely on linguistic and lexi-
cographic patterns which represent expert domain
knowledge for particular event types. They lack
the flexibility of porting to new domains since ex-
traction patterns often need to be re-defined. Data-
driven approaches require large annotated data to
train statistical models that approximate linguistic
phenomena. Nevertheless, it is expensive to obtain
annotated data in practice.
With the increasing popularity of social media,
social networking sites such as Twitter have be-
come an important source of event information.
As reported in (Petrovic et al, 2013), even 1% of
the public stream of Twitter contains around 95%
of all the events reported in the newswire. Never-
theless, the social stream data such as Twitter data
pose new challenges. Social media messages are
often short and evolve rapidly over time. As such,
it is not possible to know the event types a priori
and hence violates the use of existing event extrac-
tion approaches.
Approaches to event extraction from Twitter
make use of a graphical model to extract canonical
entertainment events from tweets by aggregating
information across multiple messages (Benson et
al., 2011). In (Liu et al, 2012), social events in-
volving two persons are extracted from multiple
similar tweets using a factor graph by harvesting
the redundancy in tweets. Ritter et al (2012) pre-
sented a system called TwiCal which extracts an
open-domain calendar of significant events repre-
sented by a 4-tuple set including a named entity,
event phrase, calendar date, and event type from
Twitter.
In our work here, we notice a very important
property in social media data that the same event
could be referenced by high volume messages.
This property allows us resort to statistical mod-
els that can group similar events based on the co-
occurrence patterns of their event elements. Here,
event elements include named entities such as per-
son, company, organization, date/time, location,
and the relations among them. We can treat an
event as a latent variable and model the genera-
tion of an event as a joint distribution of its indi-
vidual event elements. We thus propose a Latent
Event Model (LEM) which can automatically de-
tect events from social media without the use of
labeled data.
700
Latent 
Event
Model
Post-
processing
Tweets Pre-processing
POS
Tagging
Named Entity 
Recognition Stemming
Temporal Resolution
Extracted Events
Name Entity Time Location Key words
...
Amy
Winehouse
2011/0
7/23 London
Die, 
Death, ..
Space Shuttle 
Atlantis
2011/0
7/08
Kennedy 
Space Center Land ...
Figure 1: The proposed framework for event extraction from tweets.
Our work is similar to TwiCal in the sense that
we also focus on the extraction of structured repre-
sentation of events from Twitter. However, TwiCal
relies on a supervised sequence labeler trained
on tweets annotated with event mentions for the
identification of event-related phrases. We pro-
pose a simple Bayesian modelling approach which
is able to directly extract event-related keywords
from tweets without supervised learning. Also,
TwiCal uses G
2
test to choose an entity y with
the strongest association with a date d to form a
binary tuple ?y, d? to represent an event. On the
contrary, the structured representation of events
can be directly extracted from the output of our
LEM model. We have conducted experiments on
a Twitter corpus and the results show that our pro-
posed approach outperforms TwiCal, the state-of-
the-art open event extraction system, by 7.7% in
F-measure.
2 Methodology
Events extracted in our proposed framework are
represented as a 4-tuple ?y, d, l, k?, where y stands
for a non-location named entity, d for a date, l for a
location, and k for an event-related keyword. Each
event mentioned in tweets can be closely depicted
by this representation. It should be noted that for
some events, one or more elements in their corre-
sponding tuples might be absent as their related in-
formation is not available in tweets. As illustrated
in Figure 1, our proposed framework consists of
three main steps, pre-processing, event extraction
based on the LEM model and post-processing.
The details of our proposed framework are de-
scribed below.
2.1 Pre-processing
Tweets are pre-processed by time expression
recognition, named entity recognition, POS tag-
ging and stemming.
Time Expression Recognition. Twitter users
might represent the same date in various forms.
For example, ?tomorrow?, ?next Monday?, ? Au-
gust 23th? in tweets might all refer to the same
day, depending on the date that users wrote the
tweets. To resolve the ambiguity of the time ex-
pressions, SUTime
1
(Chang and Manning, 2012)
is employed, which takes text and a reference date
as input and outputs a more accurate date which
the time expression refers to.
Named Entity Recognition. Named entity
recognition (NER) is a crucial step since the
results would directly impact the final extracted
4-tuple ?y, d, l, k?. It is not easy to accurately
identify named entities in the Twitter data since
tweets contain a lot of misspellings and abbrevi-
ations. However, it is often observed that events
mentioned in tweets are also reported in news
articles in the same period (Petrovic et al, 2013).
Therefore, named entities mentioned in tweets are
likely to appear in news articles as well. We thus
perform named entity recognition in the following
way. First, a traditional NER tool such as the
Stanford Named Entity Recognizer
2
is used to
identify named entities from the news articles
crawled from BBC and CNN during the same
period that the tweets were published. The recog-
nised named entities from news are then used to
build a dictionary. Named entities from tweets
are extracted by looking up the dictionary through
fuzzy matching. We have also used a named
entity tagger trained specifically on the Twitter
data
3
(Ritter et al, 2011) to directly extract named
entities from tweets. However, as will be shown
in Section 3 that using our constructed dictionary
for named entity extraction gives better results.
We distinguish between location entities, denoted
as l, and non-location entities such as person or
organization, denoted as y.
1
http://nlp.stanford.edu/software/
sutime.shtml
2
http://nlp.stanford.edu/software/
CRF-NER.shtml
3
http://github.com/aritter/twitter-nlp
701
Finally, we use a POS tagger
4
trained on
tweets (Gimpel et al, 2011) to perform POS tag-
ging on the tweets data and apart from the pre-
viously recognised named entities, only words
tagged with nouns, verbs or adjectives are kept.
These remaining words are subsequently stemmed
and words occurred less than 3 times are filtered.
After the pre-processing step, non-location enti-
ties y, locations l, dates d and candidate keywords
of the tweets are collected as the input to the LEM
model for event extraction.
2.2 Event Extraction using the Latent Event
Model (LEM)
We propose an unsupervised latent variable model,
called the Latent Event Model (LEM), to extract
events from tweets. The graphical model of LEM
is shown in Figure 2.
MN
y
?
?
d l k
e
E? ? ? ?
? ? ? ?
Figure 2: Laten Event Model (LEM).
In this model, we assume that each tweet mes-
sage m ? {1..M} is assigned to one event in-
stance e, while e is modeled as a joint distribution
over the named entities y, the date/time d when
the event occurred, the location l where the event
occurred and the event-related keywords k. This
assumption essentially encourages events that in-
volve the same named entities, occur at the same
time and in the same location and have similar
keyword to be assigned with the same event.
The generative process of LEM is shown below.
? Draw the event distribution pi
e
?
Dirichlet(?)
? For each event e ? {1..E}, draw multino-
mial distributions ?
e
? Dirichlet(?),?
e
?
Dirichlet(?),?
e
? Dirichlet(?),?
e
?
Dirichlet(?).
4
http://www.ark.cs.cmu.edu/TweetNLP
? For each tweet w
? Choose an event e ? Multinomial(pi),
? For each named entity occur in tweet
w, choose a named entity y ?
Multinomial(?
e
),
? For each date occur in tweet w, choose
a date d ? Multinomial(?
e
),
? For each location occur in tweet w,
choose a location l ? Multinomial(?
e
),
? For other words in tweet w, choose a
word k ? Multinomial(?
e
).
We use Collapsed Gibbs Sampling (Griffiths
and Steyvers, 2004) to infer the parameters of the
model and the latent class assignments for events,
given observed data D and the total likelihood.
Gibbs sampling allows us repeatedly sample from
a Markov chain whose stationary distribution is
the posterior of e
m
from the distribution over that
variable given the current values of all other vari-
ables and the data. Such samples can be used to
empirically estimate the target distribution. Let-
ting the subscript ?m denote a quantity that ex-
cludes data from mth tweet , the conditional pos-
terior for e
m
is:
P (e
m
= t|e
?m
,y,d, l,z,?) ?
n
?m
t
+ ?
M + E?
?
Y?
y=1
?
n
(m)
t,y
b=1
(n
t,y
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+ Y ?)
?
D?
d=1
?
n
(m)
t,d
b=1
(n
t,d
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+D?)
?
L?
l=1
?
n
(m)
t,l
b=1
(n
t,l
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+ L?)
?
V?
k=1
?
n
(m)
t,k
b=1
(n
t,k
? b+ ?)
?
n
(m)
t
b=1
(n
t
? b+ V ?)
where n
t
is the number of tweets that have been
assigned to the event t; M is the total number of
tweets, n
t,y
is the number of times named entity y
has been associated with event t; n
t,d
is the num-
ber of times dates d has been associated with event
t; n
t,l
is the number of times locations l has been
assigned with event t; n
t,k
is the number of times
keyword k has associated with event t, counts with
(m) notation denote the counts relating to tweet
m only. Y,D,L, V are the total numbers of dis-
tinct named entities, dates, locations, and words
appeared in the whole Twitter corpus respectively.
E is the total number of events which needs to be
set.
Once the class assignments for all events are
known, we can easily estimate the model param-
eters {pi,?,?,?,?}. We set the hyperparame-
ters ? = ? = ? = ? = ? = 0.5 and run Gibbs
702
sampler for 10,000 iterations and stop the iteration
once the log-likelihood of the training data con-
verges under the learned model. Finally we select
an entity, a date, a location, and the top 2 keywords
of the highest probability of every event to form a
4-tuple as the representation of that event.
2.3 Post-processing
To improve the precision of event extraction, we
remove the least confident event element from the
4-tuples using the following rule. If P (element)
is less than
1
?
P (S), where P (S) is the sum of
probabilities of the other three elements and ? is a
threshold value and is set to 5 empirically, the ele-
ment will be removed from the extracted results.
3 Experiments
In this section, we first describe the Twitter corpus
used in our experiments and then present how we
build a baseline based on the previously proposed
TwiCal system (Ritter et al, 2012), the state-of-
the-art open event extraction system on tweets. Fi-
nally, we present our experimental results.
3.1 Dataset
We use the First Story Detection (FSD)
dataset (Petrovic et al, 2013) in our experi-
ment. It consists of 2,499 tweets which are
manually annotated with the corresponding event
instances resulting in a total of 27 events. The
tweets were published between 7th July and 12th
September 2011. These events cover a range of
categories, from celebrity news to accidents, and
from natural disasters to science discoveries. It
should be noted here that some event elements
such as location is not always available in the
tweets. Automatically inferring geolocation of the
tweets is a challenging task and will be considered
in our future work. For the tweets without time
expressions, we used the tweets? publication dates
as a default. The number of tweets for each event
ranges from 2 to around 1000. We believe that in
reality, events which are mentioned in very few
tweets are less likely to be significant. Therefore,
the dataset was filtered by removing the events
which are mentioned in less than 10 tweets. This
results in a final dataset containing 2468 tweets
annotated with 21 events.
3.2 Baseline construction
The baseline we chose is TwiCal (Ritter et al,
2012). The events extracted in the baseline are
represented as a 3-tuple ?y, d, k?
5
, where y stands
for a non-location named entity, d for a date and
k for an event phrase. We re-implemented the
system and evaluate the performance of the base-
line on the correctness of the exacted three ele-
ments excluding the location element. In the base-
line approach, the tuple ?y, d, k? are extracted in
the following ways. Firstly, a named entity rec-
ognizer (Ritter et al, 2011) is employed to iden-
tify named entities. The TempEx (Mani and Wil-
son, 2000) is used to resolve temporal expressions.
For each date, the baseline approach chose the en-
tity y with the strongest association with the date
and form the binary tuple ?y, d? to represent an
event. An event phrase extractor trained on an-
notated tweets is required to extract event-related
phrases. Due to the difficulties of re-implementing
the sequence labeler without knowing the actual
features set and the annotated training data, we as-
sume all the event-related phrases are identified
correctly and simply use the event trigger words
annotated in the FSD corpus as k to form the event
3-tuples. It is worth noting that the F-measure re-
ported for the event phrase extraction is only 64%
in the baseline approach (Ritter et al, 2012).
3.3 Evaluation Metric
To evaluate the performance of the propose ap-
proach, we use precison, recall, and F ?
measure as in general information extraction sys-
tems (Makhoul et al, 1999). For the 4-tuple
?y, d, l, k?, the precision is calculated based on the
following criteria:
1. Do the entity y, location l and date d that we
have extracted refer to the same event?
2. Are the keywords k in accord with the event
that other extracted elements y, l, d refer to
and are they informative enough to tell us
what happened?
If the extracted representation does not contain
keywords, its precision is calculated by check-
ing the criteria 1. If the extracted representation
contains keywords, its precision is calculated by
checking both criteria 1 and 2.
3.4 Experimental Results
The number of events, E, in the LEM model
is set to 25. The performance of the proposed
5
TwiCal also groups event instances into event types such
as ?Sport? or ?Politics? using LinkLDA which is not consid-
ered here.
703
Method Tuple Evaluated Precision Recall F-measure
Baseline ?y, d, k? 75% 76.19% 75.59%
Proposed ?y, d, l? 96% 80.95% 87.83%
Proposed ?y, d, l, k? 92% 76.19% 83.35%
Table 1: Comparison of the performance of event
extraction on the FSD dataset.
Method Tuple Evaluated Precision Recall F-measure
TW-NER ?y, d, l? 88% 76.19% 80.35%
TW-NER ?y, d, l, k? 84% 76.19% 79.90%
NW-NER ?y, d, l? 96% 80.95% 87.83%
NW-NER ?y, d, l, k? 92% 76.19% 83.35%
Table 2: Comparison of the performance of event
extraction using different NER method.
framework is presented in Table 1. The base-
line re-implemented here can only output 3-tuples
?y, d, k? and we simply use the gold standard event
trigger words to assign to k. Still, we observe
that compared to the baseline approach, the per-
formance of our proposed framework evaluated on
the 4-tuple achieves nearly 17% improvement on
precision. The overall improvement on F-measure
is around 7.76%.
3.5 Impact of Named Entity Recognition
We experimented with two approaches for named
entity recognition (NER) in preprocessing. One
is to use the NER tool trained specifically on the
Twitter data (Ritter et al, 2011), denoted as ?TW-
NER? in Table 2. The other uses the traditional
Stanford NER to extract named entities from news
articles published in the same period and then
perform fuzzy matching to identify named enti-
ties from tweets. The latter method is denoted
as ?NW-NER? in Table 2. It can be observed
from Table 2 that by using NW-NER, the per-
formance of event extraction system is improved
significantly by 7.5% and 3% respectively on F-
measure when evaluated on 3-tuples (without key-
words) or 4-tuples (with keywords).
3.6 Impact of the Number of Events E
We need to set the number of events E in the
LEM model. Figure 3 shows the performance of
event extraction versus different value of E. It can
be observed that the performance of the proposed
framework improves with the increase of the value
ofE until it reaches 25, which is close to the actual
number of events in our data. If further increasing
E, we notice more balanced precision/recall val-
ues and a relatively stable F-measure. This shows
that our LEM model is less sensitive to the num-
ber of events E so long as E is set to a relatively
larger value.
10 15 20 25 30 35 40
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
p
e
r
f
o
r
m
a
n
c
e
E
 Precision
 Recall
 F-meature
Figure 3: The performance of the proposed frame-
work with different number of events E.
4 Conclusions and Future Work
In this paper we have proposed an unsupervised
Bayesian model, called the Latent Event Model
(LEM), to extract the structured representation of
events from social media data. Instead of em-
ploying labeled corpora for training, the proposed
model only requires the identification of named
entities, locations and time expressions. After that,
the model can automatically extract events which
involving a named entity at certain time, location,
and with event-related keywords based on the co-
occurrence patterns of the event elements. Our
proposed model has been evaluated on the FSD
corpus. Experimental results show our proposed
framework outperforms the state-of-the-art base-
line by over 7% in F-measure. In future work,
we plan to investigate inferring geolocations au-
tomatically from tweets. We also intend to study
a better method to infer date more accurately from
tweets and explore efficient ranking strategies to
rank evens extracted for a better presentation of
results.
Acknowledgments
This work was funded by the National Natural
Science Foundation of China (61103077), Ph.D.
Programs Foundation of Ministry of Education
of China for Young Faculties (20100092120031),
Scientific Research Foundation for the Returned
Overseas Chinese Scholars, State Education Min-
istry, the Fundamental Research Funds for the
Central Universities, and the UK?s EPSRC grant
EP/L010690/1.
704
References
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
389?398, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Angel X. Chang and Christopher D. Manning. 2012.
Sutime: A library for recognizing and normaliz-
ing time expressions. In 8th International Confer-
ence on Language Resources and Evaluation (LREC
2012).
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of ACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the Na-
tional Academy of Sciences 101 (Suppl. 1), page
5228C5235.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description.
In ACE 05 Evaluation Workshop.
Frederik Hogenboom, Flavius Frasincar, Uzay Kay-
mak, and Franciska de Jong. 2011. An overview of
event extraction from text. In Workshop on Detec-
tion, Representation, and Exploitation of Events in
the Semantic Web (DeRiVE 2011) at Tenth Interna-
tional Semantic Web Conference (ISWC2011), pages
48?57.
Xiaohua Liu, Xiangyang Zhou, Zhongyang Fu, Furu
Wei, and Ming Zhou. 2012. Exacting social events
for tweets using a factor graph. In Proceedings of
the Twenty-Sixth AAAI Conference on Artificial In-
telligence, pages 1692?1698.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proceedings of DARPA
Broadcast News Workshop.
Inderjeet Mani and George Wilson. 2000. Robust tem-
poral processing of news. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 69?76, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sasa Petrovic, Miles Osborne, Richard McCreadie,
Craig Macdonald, Iadh Ounis, and Luke Shrimpton.
2013. Can twitter replace newswire for breaking
news? In Proceedings of ICWSM?13.
J. Piskorski, H. Tanev, and P. Oezden Wennerberg.
2007. Extracting violent events from on-line news
for ontology population. In Business Information
Systems, pages 287?300.
J. Piskorski, H. Tanev, M. Atkinson, and E. Van
Der Goot. 2008. Cluster-centric approach to news
event extraction. In International Conference on
New Trends in Multimedia and Network Information
Systems, pages 276?290.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter.
In Proceedings of the 18th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, KDD ?12, pages 1104?1112, New
York, NY, USA. ACM.
H. Tanev, J. Piskorski, and M. Atkinson. 2008. Real-
time news event extraction for global crisis monitor-
ing. In 13th International Conference on Applica-
tions of Natural Language to Information Systems
(NLDB), pages 207?218.
705
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 37?42,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Real-Time Detection, Tracking, and Monitoring of Automatically
Discovered Events in Social Media
Miles Osborne
?
Edinburgh
Sean Moran
Edinburgh
Richard McCreadie
Glasgow
Alexander Von Lunen
Loughborough
Martin Sykora
Loughborough
Elizabeth Cano
Aston
Neil Ireson
Sheffield
Craig Macdonald
Glasgow
Iadh Ounis
Glasgow
Yulan He
Aston
Tom Jackson
Loughborough
Fabio Ciravegna
Sheffield
Ann O?Brien
Loughborough
Abstract
We introduce ReDites, a system for real-
time event detection, tracking, monitoring
and visualisation. It is designed to as-
sist Information Analysts in understand-
ing and exploring complex events as they
unfold in the world. Events are automat-
ically detected from the Twitter stream.
Then those that are categorised as be-
ing security-relevant are tracked, geolo-
cated, summarised and visualised for the
end-user. Furthermore, the system tracks
changes in emotions over events, sig-
nalling possible flashpoints or abatement.
We demonstrate the capabilities of ReD-
ites using an extended use case from the
September 2013 Westgate shooting inci-
dent. Through an evaluation of system la-
tencies, we also show that enriched events
are made available for users to explore
within seconds of that event occurring.
1 Introduction and Challenges
Social Media (and especially Twitter) has become
an extremely important source of real-time infor-
mation about a massive number of topics, ranging
from the mundane (what I had for breakfast) to the
profound (the assassination of Osama Bin Laden).
?
Corresponding author: miles@inf.ed.ac.uk
Detecting events of interest, interpreting and mon-
itoring them has clear economic, security and hu-
manitarian importance.
The use of social media message streams for
event detection poses a number of opportunities
and challenges as these streams are: very high in
volume, often contain duplicated, incomplete, im-
precise and incorrect information, are written in
informal style (i.e. short, unedited and conver-
sational), generally concern the short-term zeit-
geist; and finally relate to unbounded domains.
These characteristics mean that while massive and
timely information sources are available, domain-
relevant information may be mentioned very infre-
quently. The scientific challenge is therefore the
detection of the signal within that noise. This chal-
lenge is exacerbated by the typical requirement
that documents must be processed in (near) real-
time, such that events can be promptly acted upon.
The ReDites system meets these requirements
and performs event detection, tracking, summari-
sation, categorisation and visualisation. To the
best of our understanding, it is the first published,
large-scale, (near) real-time Topic Detection and
Tracking system that is tailored to the needs of in-
formation analysts in the security sector. Novel as-
pects of ReDites include the first large-scale treat-
ment of spuriously discovered events and tailoring
the event stream to the security domain.
37
Figure 1: System Diagram
2 Related Work
A variety of event exploration systems have previ-
ously been proposed within the literature. For in-
stance, Trend Miner
1
enables the plotting of term
times series, drawn from Social Media (Preot?iuc-
Pietro and Cohn, 2013). It has a summarisation
component and is also multilingual. In contrast,
our system is focussed instead upon documents
(Tweets) and is more strongly driven by real-
time considerations. The Social Sensor (Aiello et
al., 2013) system facilitates the tracking of pre-
defined events using social streams.
In contrast, we track all automatically discov-
ered events we find in the stream. The Twitci-
dent (Abel et al., 2012) project deals with user-
driven searching through Social Media with re-
spect to crisis management. However, unlike
ReDites, these crises are not automatically dis-
covered. The LRA Crisis Tracker
2
has a similar
purpose as ReDites. However, while LRA uses
crowdsourcing, our ReDites system is fully auto-
matic.
3 System Overview and Architecture
Figure 1 gives a high-level system description.
The system itself is loosely coupled, with ser-
vices from different development teams coordi-
nating via a Thrift interface. An important as-
pect of this decoupled design is that it enables ge-
ographically dispersed teams to coordinate with
each other. Event processing is comprised of the
following main 4 steps:
1) New events are detected. An event is described
by the first Tweet that we find discussing it and
is defined as something that is captured within a
single Tweet (Petrovic et al., 2010).
1
http://www.trendminer-project.eu/
2
http://www.lracrisistracker.com/
2) When an event is first discovered it may initially
have little information associated with it. Further-
more, events evolve over time. Hence, the sec-
ond step involves tracking the event ? finding new
posts relating to it as they appear and maintaining
a concise updated summary of them.
3) Not all events are of interest to our intended
audience, so we organise them. In particular, we
determine whether an event is security-related (or
otherwise), geolocate it, and detect how prominent
emotions relating to that event evolve.
4) Finally, we visualise the produced stream of
summarised, categorised and geolocated events
for the analyst(s), enabling them to better make
sense of the mass of raw information present
within the original Twitter stream.
Section 6 further describes these four steps.
4 Data and Statistics
For the deployment of ReDites, we use the Twit-
ter 1% streaming sample. This provides approx-
imately four million Tweets per day, at a rate of
about 50 Tweets a second. Table 1 gives some
illustrative statistics on a sample of data from
September 2013 to give a feel for the rate of data
and generated events we produce. Table 2 gives
timing information, corresponding with the major
components of our system: time to process and
time to transfer to the next component, which is
usually a service on another machine on the in-
ternet. The latency of each step is measured in
seconds over a 1000 event sample. ?Transfer? la-
tencies is the time between one step completing
and the output arriving at the next step to be pro-
cessed (Thrift transfer time). Variance is the aver-
age deviation from the mean latency over the event
sample.
When processing the live stream, we ingest data
at an average rate of 50 Tweets per second and de-
tect an event (having geolocated and filtered out
non-English or spam Tweets) with a per-Tweet la-
tency of 0.6?0.55 seconds. Figure 2 gives laten-
cies for the various major components of the sys-
tem. All processing uses commodity machines.
5 The Westgate Shopping Mall Attack
As an illustrative example of a complex recent
event, we considered a terrorist attack on the 21st
of September, 2013.
3
This event is used to demon-
strate how our system can be used to understand it.
3
https://en.wikipedia.org/wiki/Westgate shopping mall shooting
38
Measure Event Detection Tracking and Summ Emotion Ident Security Class
Detection Transfer Ranking Summ Transfer Ident Transfer Class
Latency (sec.) 0.6226 0.7929 2.2892 0.0409 0.0519 0.2881 0.1032 0.1765
Variance (sec.) 0.5518 0.2987 1.3079 0.0114 0.0264 0.1593 0.0195 0.0610
Table 2: Event exploration timing and timing variance (seconds)
Data Rate
Tweets 35 Million
Detected events 533k
Categorised (security-related) events 5795
Table 1: Data statistics, 1st September - 30th
September 2013
In summary, a shopping Mall in Kenya was at-
tacked from the 21st of September until the 24th
of September. This event was covered by tradi-
tional newswire, by victims caught up in it as well
as by terrorist sympathisers, often in real-time.
As we later show, even though we only operate
over 1% of the Twitter Stream, we are still able to
find many (sub) events connected with this attack.
There were 6657 mentions of Westgate in Septem-
ber 2013 in our 1% of sample Tweets.
6 Major Components
6.1 Event Detection
Building upon an established Topic Detection and
Tracking (TDT) methodology, which assumes that
each new event corresponds with seeing some
novel document. the event detection component
uses a hashing approach that finds novel events
4
in constant time (Petrovic et al., 2010). To make
it scale and process thousands of documents each
second, it can optionally be distributed over a clus-
ter of machines (via Storm
5
) (McCreadie et al.,
2013). The system favours recall over precision
and has been shown to have high recall, but a low
precision (Petrovic et al., 2013). Given that we are
presenting discovered events to a user and we do
not want to overload them with false positives, we
need to take steps to increase precision (ie present
fewer false positives).
We use a content categoriser to determine
whether a discovered event is worth reporting.
Using more than 100k automatically discovered
events from the Summer of 2011, we created a
training set and manually labelled each Tweet:
4
An event is defined as something happening at a given
time and place. Operationally, this means something that can
be described within a Tweet.
5
http://storm.incubator.apache.org/
was it content bearing (what you might want to
read about in traditional newswire) or irrelevant
/ not useful. With this labelled data, we used
a Passive-Aggressive algorithm to build a con-
tent classifier. Features were simply unigrams in
Tweets. This dramatically improves precision, to
70%, with a drop in recall to 25% (when tested
on 73k unseen events, manually labelled by two
annotators). We can change the precision-recall
boundary as needed by adjusting the associated
decision boundary. We do not consider non-
English language Tweets in this work and they are
filtered out (Lui and Baldwin, 2012).
Geolocation is important, as we are particu-
larly interested in events that occur at a spe-
cific location. We therefore additionally geolo-
cate any Tweets that were not originally ge-
olocated. To geotag those Tweets that do not
have any geo-location information we use the
Tweet text and additional Tweet metadata (lan-
guage, city/state/country name, user description
etc), to learn a L
1
penalised least squares regres-
sor (LASSO) to predict the latitude and longitude.
The model is learnt on nearly 20 million geo-
located Tweets collected from 2010-2014. Exper-
iments on a held-out test dataset show we can lo-
calise Tweets to within a mean distance of 433 km
of their true location. This performance is based
on the prediction of individual tweet location and
not, as in most previous work, on the location of a
user who is represented by a set of tweets. Further-
more we are not restricted to a single, well-defined
area (such as London) and we also evaluate over a
very large set of unfiltered tweets.
Turning to the Westgate example, the first men-
tion of it in our data was at 10:02 UTC. There were
57 mentions of Westgate in discovered events,
of which 42 mentioned Kenya and 44 mentioned
Nairobi. The first mention itself in Twitter was at
09:38 UTC. We declared it an event (having seen
enough evidence and post-processing it) less than
one second later:
Westgate under siege. Armed thugs. Gun-
shots reported. Called the managers, phones are
off/busy. Are cops on the way?
We also captured numerous informative sub-
39
events covering different aspects and sides of the
central Westgate siege event, four of these are il-
lustrated below:
Post Time Tweet
10:05am RT @ItsMainaKageni: My friend Ruhila Adatia
passed away together with her unborn child. Please
keep her family and new husband in your thou
10:13am RT howden africa: Kenya police firing tear gas and
warning shots at Kenyan onlookers. Crowd getting
angry #westgate
10:10am RT @BreakingNews: Live video: Local news cov-
erage of explosions, gunfire as smoke billows from
Nairobi, Kenya, mall - @KTNKenya
10:10am ?Purportedly official Twitter account for al-Shabaab
Tweeting on the Kenyan massacre HSM Press
(http://t.co/XnCz9BulGj)
6.2 Tracking and Summarisation
The second component of the event exploration
system is Tracking and Summarisation (TaS). The
aim of this component is to use the underlying
Tweet stream to produce an overview for each
event produced by the event detection stage, up-
dating this overview as the event evolves. Track-
ing events is important when dealing with live, on-
going disasters, since new information can rapidly
emerge over time.
TaS takes as input a Tweet representing an event
and emits a list of Tweets summarising that event
in more detail. TaS is comprised of two dis-
tinct sub-components, namely: real-time tracking;
and event summarisation. The real-time track-
ing component maintains a sliding window of
Tweets from the underlying Tweet stream. As
an event arrives, the most informative terms con-
tained
6
form a search query that is used to retrieve
new Tweets about the event. For example, tak-
ing the Tweet about the Westgate terrorist attack
used in the previous section as input on September
21st 2013 at 10:15am, the real-time tracking sub-
component retrieved the following related Tweets
from the Twitter Spritzer (1%) steam
7
(only 5/100
are shown):
ID Post Time Tweet Score
1 10:05am Westgate under siege. Armed thugs. Gun-
shots reported. Called the managers, phones are
off/busy. Are cops on the way?
123.7
2 10:13am DO NOT go to Westgate Mall. Gunshots and
mayhem, keep away until further notice.
22.9
3 10:13am RT DO NOT go to Westgate Mall. Gunshots
and mayhem, keep away until further notice.
22.9
4 10:10am Good people please avoid Westgate Mall. @Po-
liceKE @IGkimaiyo please act ASAP, reports
of armed attack at #WestgateMall
22.2
5 10:07am RT @steve enzo: @kirimiachieng these thugs
won?t let us be
11.5
6
Nouns, adjectives, verbs and cardinal numbers
7
https://dev.twitter.com/docs/streaming-
apis/streams/public
The second TaS sub-component is event sum-
marisation. This sub-component takes as input the
Tweet ranking produced above and performs ex-
tractive summarisation (Nenkova and McKeown,
2012) upon it, i.e. it selects a subset of the ranked
Tweets to form a summary of the event. The goals
of event summarisation are two-fold. First, to re-
move any Tweets from the above ranking that are
not relevant to the event (e.g. Tweet 5 in the exam-
ple above). Indeed when an event is first detected,
there may be few relevant Tweets yet posted. The
second goal is to remove redundancy from within
the selected Tweets, such as Tweets 2 and 3 in the
above example, thereby focussing the produced
summary on novelty. To tackle the first of these
goals, we leverage the score distribution of Tweets
within the ranking to identify those Tweets that are
likely background noise. When an event is first
detected, few relevant Tweets will be retrieved,
hence the mean score over the Tweets is indicative
of non-relevant Tweets. Tweets within the rank-
ing whose scores diverge from the mean score in
the positive direction are likely to be on-topic. We
therefore, make an include/exclude decision for
each Tweet t in the ranking R:
include(t, R) =
?
?
?
?
?
?
?
?
?
?
?
1 if score(t)? SD(R) > 0
and |SD(R)? score(t)| >
? ?
1
|R|
?
t
?
?R
|SD(R)? score(t
?
)|
0 otherwise
(1)
where SD(R) is the standard deviation of scores
inR, score(t) is the retrieval score for Tweet t and
? is a threshold parameter that describes the mag-
nitude of the divergence from the mean score that
a Tweet must have before it is included within the
summary. Then, to tackle the issue of redundancy,
we select Tweets in a greedy time-ordered man-
ner (earliest first). A similarity (cosine) threshold
between the current Tweet and each Tweet previ-
ously selected is used to remove those that are tex-
tually similar, resulting in the following extractive
summary:
ID Post Time Tweet Score
1 10:05am Westgate under siege. Armed thugs.
Gunshots reported. Called the man-
agers, phones are off/busy. Are cops
on the way?
123.7
2 10:13am DO NOT go to Westgate Mall. Gun-
shots and mayhem, keep away until
further notice.
22.9
4 10:10am Good people please avoid Westgate
Mall. @PoliceKE @IGkimaiyo please
act ASAP, reports of armed attack at
#WestgateMall
22.2
Finally, the TaS component can be used to track
40
events over time. In this case, instead of tak-
ing a new event as input from the event detec-
tion component, a previously summarised event
can be used as a surrogate. For instance, a user
might identify an event that they want to track.
The real-time search sub-component retrieves new
Tweets about the event posted since that event was
last summarised. The event summarisation sub-
component then removes non-relevant and redun-
dant Tweets with respect to those contained within
the previous summary, producing a new updated
summary.
6.3 Organising Discovered Events
The events we discover are not targeted at infor-
mation analysts. For example, they contain sports
updates, business acquisitions as well as those that
are genuinely relevant and can bear various opin-
ions and degrees of emotional expression. We
therefore take steps to filter and organise them for
our intended audience: we predict whether they
have a specific security-focus and finally predict
an emotional label for events (which can be useful
when judging changing viewpoints on events and
highlighting extreme emotions that could possibly
motivate further incidents).
6.3.1 Security-Related Event Detection
We are particularly interested in security-related
events such as violent events, natural disasters, or
emergency situations. Given a lack of in-domain
labelled data, we resort to a weakly supervised
Bayesian modelling approach based on the previ-
ously proposed Violence Detection Model (VDM)
(Cano et al., 2013) for identifying security events.
In order to differentiate between security and
non-security related events, we extract words re-
lating to security events from existing knowledge
sources such as DBpedia and incorporate them as
priors into the VDM model learning. It should be
noted that such a word lexicon only provides ini-
tial prior knowledge into the model. The model
will automatically discover new words describing
security-related events.
We trained the VDM model on a randomly
sampled 10,581 Tweets from the TREC Mi-
croblog 2011 corpus (McCreadie et al., 2012)
and tested the model on 1,759 manually labelled
Tweets which consist of roughly the same num-
ber of security-related and non-security related
Tweets. Our results show that the VDM model
achieved 85.8% in F-measure for the identification
of security-related Tweets, which is not far from
the F-measure of 90% obtained using the super-
vised Naive Bayes classifier despite using no la-
belled data in the model.
Here, we derived word priors from a total
of 32,174 documents from DBpedia and ex-
tracted 1,612 security-related words and 1,345
non-security-related words based on the measure-
ment of relative word entropy. We then trained
the VDM model by setting the topic number to
50 and using 7,613 event Tweets extracted from
the Tweets collected during July-August 2011 and
September 2013 in addition to 10,581 Tweets from
the TREC Microblog 2011 corpus. In the afore-
mentioned Westgate example, we classify 24% of
Tweets as security-related out of a total of 7,772
summary Tweets extracted by the TaS component.
Some of the security-related Tweets are listed be-
low
8
:
ID Post Time Tweet
1 9:46am Like Bin Laden kind of explosion?
?@The realBIGmeat:
There is an explosion at westgate!?
2 10:08am RT @SmritiVidyarthi: DO NOT go to Westgate
Mall. Gunshots and mayhem, keep away till further no-
tice.
3 10:10am RT @juliegichuru: Good people please avoid
Westgate. @PoliceKE @IGkimaiyo please act
ASAP, reports of armed attack at #WestgateMall.
4 10:13am there has bn shooting @ Westgate which is suspected
to b of gangs.......there is tension rt nw....
6.3.2 Emotion
Security-related events can be fraught, with emo-
tionally charged posts possibly evolving over time,
reflecting ongoing changes in underlying events.
Eight basic emotions, as identified in the psychol-
ogy literature (see (Sykora et al., 2013a) for a de-
tailed review of this literature) are covered, specif-
ically; anger, confusion, disgust, fear, happiness,
sadness, shame and surprise. Extreme values ?as
well as their evolution? can be useful to an ana-
lyst (Sykora et al., 2013b). We detect enotions in
Tweets and support faceted browing. The emotion
component assigns labels to Tweets representing
these emotions. It is based upon a manually con-
structed ontology, which captures the relationships
between these emotions and terms (Sykora et al.,
2013a).
We sampled the summarised Tweets of the
Westgate attack, starting from the event detection
and following the messages over a course of seven
days. In the relevant Tweets, we detected that
8
Note some Tweets happen on following days.
41
8.6% had emotive terms in them, which is in line
with the aforementioned literature. Some example
expressions of emotion include:
Time Tweet Emotions
03:34 -) Ya so were those gunshots outside Fear
of gables?! I?m terrified ?
06:27 -) I?m so impressed @ d way. Kenyans r handling d siege. Surprise
14:32 -) All you xenophobic idiots spewing anti-Muslim Fear
bullshit need to -get in one of these donation lines Disgust
and see how wrong you ?
For Westgate, the emotions of sadness, fear and
surprise dominated. Very early on the emotions of
fear and sadness were expressed, as Twitter users
were terrified by the event and saddened by the
loss of lives. Sadness and fear were ? over time ?
the emotions that were stated most frequently and
constantly, with expressions of surprise, as users
were shocked about what was going on, and some
happiness relating to when people managed to
escape or were rescued from the mall. Generally
speaking, factual statements in the Tweets were
more prominent than emotive ones. This coincides
with the emotive Tweets that represented fear and
surprise in the beginning, as it was not clear what
had happened and Twitter users were upset and
tried to get factual information about the event.
6.4 Visual Analytics
The visualisation component is designed to facili-
tate the understanding and exploration of detected
events. It offers faceted browsing and multiple vi-
sualisation tools to allow an information analyst
to gain a rapid understanding of ongoing events.
An analyst can constrain the detected events us-
ing information both from the original Tweets (e.g.
hashtags, locations, user details) and from the up-
dated summaries derived by ReDites. The ana-
lyst can also view events using facet values, loca-
tions/keywords in topic maps and time/keywords
in multiple timelines. By combining informa-
tion dimensions, the analyst can determine pat-
terns across dimensions to determine if an event
should be acted upon ? e.g the analyst can choose
to view Tweets, which summarise highly emotive
events, concerning middle eastern countries.
7 Discussion
We have presented ReDites, the first published
system that carries out large-scale event detection,
tracking summarisation and visualisation for the
security sector. Events are automatically identified
and those that are relevant to information analysts
are quickly made available for ongoing monitor-
ing. We showed how the system could be used
to help understand a complex, large-scale security
event. Although our system is initially specialised
to the security sector, it is easy to repurpose it to
other domains, such as natural disasters or smart
cities. Key aspects of our approach include scala-
bility and a rapid response to incoming data.
Acknowledgements
This work was funded by EPSRC grant
EP/L010690/1. MO also acknowledges sup-
port from grant ERC Advanced Fellowship
249520 GRAMPLUS.
References
F. Abel, C. Hauff, G.-J. Houben, R. Stronkman, and K. T.
Semantics + filtering + search = twitcident. exploring in-
formation in social web streams. In Proc. of HT, 2012.
L. M. Aiello et al. L. Aiello, G. Petkos, C. Martin, D. Corney,
S. Papadopoulos, R. Skraba, A. Goker, Y. Kompatsiaris,
A. Jaimes Sensing trending topics in Twitter. Transac-
tions on Multimedia Journal, 2012.
A.E. Cano, Y. He, K. Liu, and J. Zhao. A weakly supervised
bayesian model for violence detection in social media. In
Proc. of IJCNLP, 2013.
M. Lui and T. Baldwin. Langid.py: An off-the-shelf lan-
guage identification tool. In Proc. of ACL, 2012.
R. McCreadie, C. Macdonald, I. Ounis, M. Osborne, and S.
Petrovic. Scalable distributed event detection for twitter.
In Proc. of Big Data, 2013.
R. McCreadie, I. Soboroff, J. Lin, C. Macdonald, I. Ounis and
D. McCullough. On building a reusable Twitter corpus. In
Proc. of SIGIR, 2012.
A. Nenkova and K. McKeown. A survey of text summariza-
tion techniques. In Mining Text Data Journal, 2012.
S. Petrovic, M. Osborne, and V. Lavrenko. Streaming first
story detection with application to Twitter. In Proc. of
NAACL, 2010.
S. Petrovic, M. Osborne, R. McCreadie, C. Macdonald, I.
Ounis, and L. Shrimpton. Can Twitter replace newswire
for breaking news? In Proc. of WSM, 2012.
D. Preot?iuc-Pietro and T. Cohn. A temporal model of text pe-
riodicities using gaussian processes. In Proc. of EMNLP,
2012.
M. D. Sykora, T. W. Jackson, A. O?Brien, and S. Elayan.
Emotive ontology: Extracting fine-grained emotions from
terse, informal messages. Computer Science and Informa-
tion Systems Journal, 2013.
M. D. Sykora, T. W. Jackson, A. O?Brien, and S. Elayan.
National security and social media monitoring. In Proc.
of EISIC, 2013.
42
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 144?152,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Comparative Study of Bayesian Models for Unsupervised Sentiment
Detection
Chenghua Lin
School of Engineering,
Computing and Mathematics
University of Exeter
Exeter, EX4 4QF, UK.
cl322@exeter.ac.uk
Yulan He
Knowledge Media Institute
The Open University
Milton Keynes
MK7 6AA, UK
Y.He@open.ac.uk
Richard Everson
School of Engineering,
Computing and Mathematics
University of Exeter
Exeter, EX4 4QF, UK.
R.E.Everson@exeter.ac.uk
Abstract
This paper presents a comparative study
of three closely related Bayesian mod-
els for unsupervised document level senti-
ment classification, namely, the latent sen-
timent model (LSM), the joint sentiment-
topic (JST) model, and the Reverse-JST
model. Extensive experiments have been
conducted on two corpora, the movie re-
view dataset and the multi-domain senti-
ment dataset. It has been found that while
all the three models achieve either bet-
ter or comparable performance on these
two corpora when compared to the exist-
ing unsupervised sentiment classification
approaches, both JST and Reverse-JST are
able to extract sentiment-oriented topics.
In addition, Reverse-JST always performs
worse than JST suggesting that the JST
model is more appropriate for joint senti-
ment topic detection.
1 Introduction
With the explosion of web 2.0, various types of
social media such as blogs, discussion forums and
peer-to-peer networks present a wealth of infor-
mation that can be very helpful in assessing the
general public?s sentiments and opinions towards
products and services. Recent surveys have re-
vealed that opinion-rich resources like online re-
views are having greater economic impact on both
consumers and companies compared to the tradi-
tional media (Pang and Lee, 2008). Driven by
the demand of gleaning insights of such great
amounts of user-generated data, work on new
methodologies for automated sentiment analysis
has bloomed splendidly.
Compared to the traditional topic-based text
classification, sentiment classification is deemed
to be more challenging as sentiment is often em-
bodied in subtle linguistic mechanisms such as
the use of sarcasm or incorporated with highly
domain-specific information. Although the task
of identifying the overall sentiment polarity of
a document has been well studied, most of the
work is highly domain dependent and favoured
in supervised learning (Pang et al, 2002; Pang
and Lee, 2004; Whitelaw et al, 2005; Kennedy
and Inkpen, 2006; McDonald et al, 2007), re-
quiring annotated corpora for every possible do-
main of interest, which is impractical for real
applications. Also, it is well-known that senti-
ment classifiers trained on one domain often fail
to produce satisfactory results when shifted to an-
other domain, since sentiment expression can be
quite different in different domains (Aue and Ga-
mon, 2005). Moreover, aside from the diversity
of genres and large-scale size of Web corpora,
user-generated contents evolve rapidly over time,
which demands much more efficient algorithms
for sentiment analysis than the current approaches
can offer. These observations have thus motivated
the problem of using unsupervised approaches for
domain-independent joint sentiment topic detec-
tion.
Some recent research efforts have been made to
adapt sentiment classifiers trained on one domain
to another domain (Aue and Gamon, 2005; Blitzer
et al, 2007; Li and Zong, 2008; Andreevskaia
and Bergler, 2008). However, the adaption perfor-
mance of these lines of work pretty much depends
on the distribution similarity between the source
and target domain, and considerable effort is still
required to obtain labelled data for training.
Intuitively, sentiment polarities are dependent
on contextual information, such as topics or do-
mains. In this regard, some recent work (Mei et
al., 2007; Titov and McDonald, 2008a) has tried to
model both sentiment and topics. However, these
two models either require postprocessing to calcu-
late the positive/negative coverage in a document
for polarity identification (Mei et al, 2007) or re-
144
quire some kind of supervised setting in which
review text should contain ratings for aspects of
interest (Titov and McDonald, 2008a). More re-
cently, Dasgupta and Ng (2009) proposed an unsu-
pervised sentiment classification algorithm by in-
tegrating user feedbacks into a spectral clustering
algorithm. Features induced for each dimension of
spectral clustering can be considered as sentiment-
oriented topics. Nevertheless, human judgement
of identifying the most important dimensions dur-
ing spectral clustering is required.
Lin and He (2009) proposed a joint sentiment-
topic (JST) model for unsupervised joint senti-
ment topic detection. They assumed that top-
ics are generated dependent on sentiment distri-
butions and then words are generated conditioned
on sentiment-topic pairs. While this is a reason-
able design choice, one may argue that the re-
verse is also true that sentiments may vary ac-
cording to topics. Thus in this paper, we studied
the reverse dependence of the JST model called
Reverse-JST, in which sentiments are generated
dependent on topic distributions in the modelling
process. We also note that, when the topic num-
ber is set to 1, both JST and reversed-JST es-
sentially become a simple latent Dirichlet aloca-
tion (LDA) model with only S (number of sen-
timent label) topics, each of which corresponds
to a sentiment label. We called it latent senti-
ment model (LSM) in this paper. Extensive ex-
periments have been conducted on the movie re-
view (MR)1 (Pang et al, 2002) and multi-domain
sentiment (MDS)2 (Blitzer et al, 2007) datasets
to compare the performance of LSM, JST and
Reverse-JST. Results show that all these three
models are able to give either better or compara-
ble performance compared to the existing unsu-
pervised sentiment classification approaches. In
addition, both JST and reverse-JST are able to ex-
tract sentiment-oriented topics. Furthermore, the
fact that reverse-JST always performs worse than
JST suggests that the JST model is more appropri-
ate for joint sentiment topic detection.
The rest of the paper is organized as follows.
Section 2 presents related work. Section 3 de-
scribes the LSM, JST and Reserver-JST models.
Experimental setup and results on the MR and
MDS datasets are discussed in Section 4 and 5 re-
1http://www.cs.cornell.edu/people/
pabo/movie-review-data
2http://www.cs.jhu.edu/
?
mdredze/
datasets/sentiment/index2.html
spectively. Finally, Section 6 concludes the paper
and outlines the future work.
2 Related Work
As opposed to the work (Pang et al, 2002; Pang
and Lee, 2004; Whitelaw et al, 2005; Kennedy
and Inkpen, 2006) that only focused on senti-
ment classification in one particular domain, re-
cent research attempts have been made to address
the problem of sentiment classification across do-
mains. Aue and Gamon (2005) explored vari-
ous strategies for customizing sentiment classifiers
to new domains, where the training is based on
a small number of labelled examples and large
amounts of unlabelled in-domain data. However,
their experiments achieved only limited success,
with most of the classification accuracy below
80%. In the same vein, some more recent work
focused on domain adaption for sentiment classi-
fiers. Blitzer et al (2007) used the structural corre-
spondence learning (SCL) algorithm with mutual
information. Li and Zong (2008) combined multi-
ple single classifiers trained on individual domains
using SVMs. However, the adaption performance
in (Blitzer et al, 2007) depends on the selection of
pivot features that used to link the source and tar-
get domains; whereas the approach of Li and Zong
(2008) heavily relies on labelled data from all the
domains to train the integrated classifier and thus
lack the flexibility to adapt the trained classifier to
domains where label information is not available.
Recent years have also seen increasing interests
in modelling both sentiment and topics simultane-
ously. The topic-sentiment mixture (TSM) model
(Mei et al, 2007) can jointly model sentiment and
topics by constructing an extra background com-
ponent and two additional sentiment subtopics on
top of the probabilistic latent semantic indexing
(pLSI) (Hofmann, 1999). However, TSM may
suffer from the problem of overfitting the data
which is known as a deficiency of pLSI, and post-
processing is also required in order to calculate
the sentiment prediction for a document. The
multi-aspect sentiment (MAS) model (Titov and
McDonald, 2008a), which is extended from the
multi-grain latent Dirichlet alocation (MG-LDA)
model (Titov and McDonald, 2008b), allows sen-
timent text aggregation for sentiment summary of
each rating aspect extracted from MG-LDA. One
drawback of MAS is that it requires that every as-
pect is rated at least in some documents, which
145
is practically infeasible. More recently, Dasgupta
and Ng (2009) proposed an unsupervised sen-
timent classification algorithm where user feed-
backs are provided on the spectral clustering pro-
cess in an interactive manner to ensure that text are
clustered along the sentiment dimension. Features
induced for each dimension of spectral cluster-
ing can be considered as sentiment-oriented top-
ics. Nevertheless, human judgement of identify-
ing the most important dimensions during spectral
clustering is required.
Among various efforts for improving senti-
ment detection accuracy, one direction is to in-
corporate prior information or subjectivity lexi-
con (i.e., words bearing positive or negative sen-
timent) into the sentiment model. Such sen-
timent lexicons can be acquired from domain-
independent sources in many different ways, from
manually built appraisal groups (Whitelaw et
al., 2005), to semi-automatically (Abbasi et al,
2008) and fully automatically (Kaji and Kitsure-
gawa, 2006) constructed lexicons. When incor-
porating lexical knowledge as prior information
into a sentiment-topic model, Andreevskaia and
Bergler (2008) integrated the lexicon-based and
corpus-based approaches for sentence-level sen-
timent annotation across different domains; Li et
al. (2009) employed lexical prior knowledge for
semi-supervised sentiment classification based on
non-negative matrix tri-factorization, where the
domain-independent prior knowledge was incor-
porated in conjunction with domain-dependent un-
labelled data and a few labelled documents. How-
ever, this approach performed worse than the JST
model on the movie review data even with 40%
labelled documents as will be shown in Section 5.
3 Latent Sentiment-Topic Models
This section describes three closely related
Bayesian models for unsupervised sentiment clas-
sification, the latent sentiment model (LSM), the
joint sentiment-topic (JST) model, and the joint
topic sentiment model by reversing the generative
process of sentiment and topics in the JST model,
called Reverse-JST.
3.1 Latent Sentiment Model (LSM)
The LSM model, as shown in Figure 1(a), can be
treated as a special case of LDA where a mixture
of only three sentiment labels are modelled, i.e.
positive, negative and neutral.
Assuming that we have a total number of S sen-
timent labels3; a corpus with a collection of D
documents is denoted by C = {d1, d2, ..., dD};
each document in the corpus is a sequence of Nd
words denoted by d = (w1, w2, ..., wNd), and
each word in the document is an item from a vo-
cabulary index with V distinct terms denoted by
{1, 2, ..., V }. The procedure of generating a word
in LSM starts by firstly choosing a distribution
over three sentiment labels for a document. Fol-
lowing that, one picks up a sentiment label from
the sentiment label distribution and finally draws a
word according to the sentiment label-word distri-
bution.
The joint probability of words and sentiment la-
bel assignment in LSM can be factored into two
terms:
P (w, l) = P (w|l)P (l|d). (1)
Letting the superscript ?t denote a quantity that
excludes data from the tth position, the conditional
posterior for lt by marginalizing out the random
variables ? and pi is
P (lt = k|w, l?t, ?,?) ?
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
, (2)
where Nwt,k is the number of times word wt has
associated with sentiment label k; Nk is the the
number of times words in the corpus assigned to
sentiment label k; Nk,d is the number of times
sentiment label k has been assigned to some word
tokens in document d; Nd is the total number of
words in the document collection.
Gibbs sampling is used to estimate the poste-
rior distribution of LSM, as well as the JST and
Reverse-JST models that will be discussed in the
following two sections.
3.2 Joint Sentiment-Topic Model (JST)
In contrast to LSM that only models document
sentiment, the JST model (Lin and He, 2009)
can detect sentiment and topic simultaneously, by
modelling each document with S (number of sen-
timent labels) topic-document distributions. It
should be noted that when the topic number is
set to 1, JST effectively becomes the LSM model
with only three topics corresponding to each of the
3For all the three models, i.e., LSM, JST and Reverse-
JST, we set the sentiment label number S to 3 representing
the positive, negative and neutral polarities, respectively.
146
w


l

N d D
S
(a)


w




z

l
N d D
S
S * T
(b)
wExploring English Lexicon Knowledge for Chinese Sentiment Analysis
Yulan He Harith Alani
Knowledge Media Institute
The Open University
Milton Keynes MK6 6AA, UK
{y.he, h.alani}@open.ac.uk
Deyu Zhou
School of Computer Science and Engineering
Southeast University
Nanjing, China
d.zhou@seu.edu.cn
Abstract
This paper presents a weakly-supervised
method for Chinese sentiment analysis
by incorporating lexical prior knowledge
obtained from English sentiment lexi-
cons through machine translation. A
mechanism is introduced to incorpo-
rate the prior information about polarity-
bearing words obtained from existing
sentiment lexicons into latent Dirichlet
allocation (LDA) where sentiment labels
are considered as topics. Experiments
on Chinese product reviews on mobile
phones, digital cameras, MP3 players,
and monitors demonstrate the feasibil-
ity and effectiveness of the proposed ap-
proach and show that the weakly su-
pervised LDA model performs as well
as supervised classifiers such as Naive
Bayes and Support vector Machines with
an average of 83% accuracy achieved
over a total of 5484 review documents.
Moreover, the LDA model is able to
extract highly domain-salient polarity
words from text.
1 Introduction
Sentiment analysis aims to understand subjec-
tive information such as opinions, attitudes, and
feelings expressed in text. It has become a hot
topic in recent years because of the explosion in
availability of people?s attitudes and opinions ex-
pressed in social media including blogs, discus-
sion forums, tweets, etc. Research in sentiment
analysis has mainly focused on the English lan-
guage. There have been few studies in sentiment
analysis in other languages due to the lack of re-
sources, such as subjectivity lexicons consisting
of a list of words marked with their respective
polarity (positive, negative or neutral) and manu-
ally labeled subjectivity corpora with documents
labeled with their polarity.
Pilot studies on cross-lingual sentiment anal-
ysis utilize machine translation to perform senti-
ment analysis on the English translation of for-
eign language text (Banea et al, 2008; Bautin
et al, 2008; Wan, 2009). The major problem
is that they cannot be generalized well when
there is a domain mismatch between the source
and target languages. There have also been in-
creasing interests in exploiting bootstrapping-
style approaches for weakly-supervised senti-
ment classification in languages other than En-
glish (Zagibalov and Carroll, 2008b; Zagibalov
and Carroll, 2008a; Qiu et al, 2009). Other
approaches use ensemble techniques by either
combining lexicon-based and corpus-based algo-
rithms (Tan et al, 2008) or combining sentiment
classification outputs from different experimen-
tal settings (Wan, 2008). Nevertheless, all these
approaches are either complex or require careful
tuning of domain and data specific parameters.
This paper proposes a weakly-supervised ap-
proach for Chinese sentiment classification by
incorporating language-specific lexical knowl-
edge obtained from available English senti-
ment lexicons through machine translation. Un-
like other cross-lingual sentiment classification
methods which often require labeled corpora for
training and therefore hinder their applicability
for cross-domain sentiment analysis, the pro-
posed approach does not require labeled docu-
ments. Moreover, as opposed to existing weakly-
supervised sentiment classification approaches
which are rather complex, slow, and require care-
ful parameter tuning, the proposed approach is
simple and computationally efficient; rendering
more suitable for online and real-time sentiment
classification from the Web.
Our experimental results on the Chinese re-
views of four different product types show that
the LDA model performs as well as the super-
vised classifiers such as Naive Bayes and Sup-
port Vector Machines trained from labeled cor-
pora. Although this paper primarily studies sen-
timent analysis in Chinese, the proposed ap-
proach is applicable to any other language so
long as a machine translation engine is available
between the selected language and English.
The remainder of the paper is organized as
follows. Related work on cross-lingual senti-
ment classification and weakly-supervised sen-
timent classification in languages other than En-
glish are discussed in Section 2. The proposed
mechanism of incorporating prior word polarity
knowledge into the LDA model is introduced in
Section 3. The experimental setup and results of
sentiment classification on the Chinese reviews
of four different products are presented in Sec-
tion 4 and 5 respectively. Finally, Section 6 con-
cludes the paper.
2 Related Work
Pilot studies on cross-lingual sentiment analysis
rely on English corpora for subjectivity classifi-
cation in other languages. For example, Mihal-
cea et al (2007) make use of a bilingual lexicon
and a manually translated parallel text to gener-
ate the resources to build subjectivity classifiers
based on Support Vector Machines (SVMs) and
Naive Bayes (NB) in a new language; Banea et
al. (2008) use machine translation to produce a
corpus in a new language and train SVMs and
NB for subjectivity classification in the new lan-
guage. Bautin et al (2008) also utilize machine
translation to perform sentiment analysis on the
English translation of a foreign language text.
More recently, Wan (2009) proposed a co-
training approach to tackle the problem of cross-
lingual sentiment classification by leveraging an
available English corpus for Chinese sentiment
classification. Similar to the approach proposed
in (Banea et al, 2008), Wan?s method also uses
machine translation to produced a labeled Chi-
nese review corpus from the available labeled
English review data. However, in order to allevi-
ate the language gap problem that the underlying
distributions between the source and target lan-
guage are different, Wan builds two SVM classi-
fiers, one based on English features and the other
based on Chinese features, and uses a bootstrap-
ping method based on co-training to iteratively
improve classifiers until convergence.
The major problem of the aforementioned
cross-lingual sentiment analysis algorithms is
that they all utilize supervised learning to train
sentiment classifiers from annotated English cor-
pora (or the translated target language corpora
generated by machine translation). As such, they
cannot be generalized well when there is a do-
main mismatch between the source and target
language. For example, For example, the word
?compact? might express positive polarity when
used to describe a digital camera, but it could
have negative orientation if it is used to describe
a hotel room. Thus, classifiers trained on one
domain often fail to produce satisfactory results
when shifting to another domain.
Recent efforts have also been made for
weakly-supervised sentiment classification in
Chinese. Zagibalov and Carroll (2008b) starts
with a one-word sentiment seed vocabulary and
use iterative retraining to gradually enlarge the
seed vocabulary by adding more sentiment-
bearing lexical items based on their relative fre-
quency in both the positive and negative parts
of the current training data. Sentiment direction
of a document is then determined by the sum
of sentiment scores of all the sentiment-bearing
lexical items found in the document. The prob-
lem with this approach is that there is no princi-
pal way to set the optimal number of iterations.
They then suggested an iteration control method
in (Zagibalov and Carroll, 2008a) where itera-
tive training stops when there is no change to the
classification of any document over the previous
two iterations. However, this does not necessar-
ily correlate to the best classification accuracy.
Similar to (Zagibalov and Carroll, 2008b),
Qiu et al (2009) also uses a lexicon-based iter-
ative process as the first phase to iteratively en-
large an initial sentiment dictionary. But instead
of using a one-word seed dictionary as in (Za-
gibalov and Carroll, 2008b), they started with a
much larger HowNet Chinese sentiment dictio-
nary1 as the initial lexicon. Documents classified
by the first phase are taken as the training set to
train the SVMs which are subsequently used to
revise the results produced by the first phase.
Other researchers investigated ensemble tech-
niques for weakly-supervised sentiment classifi-
cation. Tan et al (2008) proposed a combination
of lexicon-based and corpus-based approaches
that first labels some examples from a give do-
main using a sentiment lexicon and then trains
a supervised classifier based on the labeled ones
from the first stage. Wan (2008) combined sen-
timent scores calculated from Chinese product
reviews using the Chinese HowNet sentiment
dictionary and from the English translation of
Chinese reviews using the English MPQA sub-
jectivity lexicon2. Various weighting strategies
were explored to combine sentiment classifica-
tion outputs from different experimental settings
in order to improve classification accuracy.
Nevertheless, all these weakly-supervised
sentiment classification approaches are rather
complex and require either iterative training or
careful tuning of domain and data specific pa-
rameters, and hence unsuitable for online and
real-time sentiment analysis in practical applica-
tions.
3 Incorporating Prior Word Polarity
Knowledge into LDA
Unlike existing approaches, we view sentiment
classification as a generative problem that when
an author writes a review document, he/she first
decides on the overall sentiment or polarity (pos-
itive, negative, or neutral) of a document, then
for each sentiment, decides on the words to be
used. We use LDA to model a mixture of only
three topics or sentiment labels, i.e. positive,
negative and neutral.
Assuming that we have a total number of S
sentiment labels; a corpus with a collection of D
1http://www.keenage.com/download/
sentiment.rar
2http://www.cs.pitt.edu/mpqa/
documents is denoted by C = {d1, d2, ..., dD};
each document in the corpus is a sequence of Nd
words denoted by d = (w1, w2, ..., wNd), and
each word in the document is an item from a vo-
cabulary index with V distinct terms denoted by
{1, 2, ..., V }. The generative process is as fol-
lows:
? Choose distributions ? ? Dir(?).
? For each document d ? [1, D], choose dis-
tributions pid ? Dir(?).
? For each of the Nd word posi-
tion wt, choose a sentiment label
lt ? Multinomial(pid), and then choose a
word wt ?Multinomial(?lt).
The joint probability of words and sentiment
label assignment in LDA can be factored into
two terms:
P (w, l) = P (w|l)P (l|d). (1)
Letting the superscript ?t denote a quantity that
excludes data from the tth position, the condi-
tional posterior for lt by marginalizing out the
random variables ? and pi is
P (lt = k|w, l?t, ?,?) ?
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
, (2)
where Nwt,k is the number of times word wt has
associated with sentiment label k; Nk is the the
number of times words in the corpus assigned to
sentiment label k; Nk,d is the number of times
sentiment label k has been assigned to some
word tokens in document d; Nd is the total num-
ber of words in the document collection.
Each words in documents can either bear pos-
itive polarity (lt = 1), or negative polarity (lt =
2), or is neutral (lt = 0). We now show how
to incorporate polarized words in sentiment lex-
icons as prior information in the Gibbs sampling
process. Let
Qt,k =
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
(3)
We can then modify the Gibbs sampling equa-
tion as follows:
P (lt = k|w, l?t, ?,?) ?
{
1I(k = S(wt))?Qt,k if S(wt) is defined
Qt,k otherwise
(4)
where the function S(wt) returns the prior senti-
ment label of wt in a sentiment lexicon and it is
defined if word wt is found in the sentiment lex-
icon. 1I(k = S(wt)) is an indicator function that
takes on value 1 if k = S(wt) and 0 otherwise.
Equation 4 in fact applies a hard constraint
that when a word is found in a sentiment lexi-
con, its sampled sentiment label is restricted to
be the same as its prior sentiment label defined
in the lexicon. This constraint can be relaxed by
introducing a parameter to control the strength of
the constraint such that when wordwt is found in
the sentiment lexicon, Equation 4 becomes
P (lt = k|w, l?t, ?,?) ?
(1? ?)?Qt,k + ?? 1I(k = S(wt))?Qt,k
(5)
where 0 ? ? ? 1. When ? = 1, the hard con-
straint will be applied; when ? = 0, Equation 5
is reduced to the original unconstrained Gibbs
sampling as defined in Equation 2.
While sentiment prior information is incor-
porated by modifying conditional probabilities
used in Gibbs sampling here, it is also possible to
explore other mechanisms to define expectation
or posterior constraints, for example, using the
generalized expectation criteria (McCallum et
al., 2007) to express preferences on expectations
of sentiment labels of those lexicon words. We
leave the exploitation of other mechanisms of in-
corporating prior knowledge into model training
as future work.
The document sentiment is classified based on
P (l|d), the probability of sentiment label given
document, which can be directly obtained from
the document-sentiment distribution. We de-
fine that a document d is classified as positive
if P (lpos|d) > P (lneg|d), and vice versa.
Table 2: Data statistics of the four Chinese prod-
uct reviews corpora.
No. of Reviews Vocab
Corpus positive Negative Size
Mobile 1159 1158 8945
DigiCam 853 852 5716
MP3 390 389 4324
Monitor 341 342 4712
4 Experimental Setup
We conducted experiments on the four corpora3
which were derived from product reviews har-
vested from the website IT1684 with each cor-
responding to different types of product reviews
including mobile phones, digital cameras, MP3
players, and monitors. All the reviews were
tagged by their authors as either positive or neg-
ative overall. The statistics of the four corpora
are shown in Table 2.
We explored three widely used English sen-
timent lexicons in our experiments, namely the
MPQA subjectivity lexicon, the appraisal lexi-
con5, and the SentiWordNet6 (Esuli and Sebas-
tiani, 2006). For all these lexicons, we only ex-
tracted words bearing positive or negative polar-
ities and discarded words bearing neutral polar-
ity. For SentiWordNet, as it consists of words
marked with positive and negative orientation
scores ranging from 0 to 1, we extracted a subset
of 8,780 opinionated words, by selecting those
whose orientation strength is above a threshold
of 0.6.
We used Google translator toolkit7 to translate
these three English lexicons into Chinese. After
translation, duplicate entries, words that failed to
translate, and words with contradictory polarities
were removed. For comparison, we also tested a
Chinese sentiment lexicon, NTU Sentiment Dic-
tionary (NTUSD)8 (Ku and Chen, 2007) which
3http://www.informatics.sussex.ac.uk/
users/tz21/dataZH.tar.gz
4http://product.it168.com
5http://lingcog.iit.edu/arc/
appraisal_lexicon_2007b.tar.gz
6http://sentiwordnet.isti.cnr.it/
7http://translate.google.com
8http://nlg18.csie.ntu.edu.tw:
Table 1: Matched polarity words statistics (positive/negative).
Lexicon
Chinese English
Mobile DigiCam MP3 Monitors Mobile DigiCam MP3 Monitors
(a)MPQA 261/253 183/174 162/135 169/147 293/331 220/241 201/153 210/174
(b)Appraisal 279/165 206/127 180/104 198/105 392/271 330/206 304/153 324/157
(c)SentiWN 304/365 222/276 202/213 222/236 394/497 306/397 276/310 313/331
(d)NTUSD 338/319 263/242 239/167 277/241 ?
(a)+(c) 425/465 307/337 274/268 296/289 516/607 400/468 356/345 396/381
(a)+(b)+(c) 495/481 364/353 312/280 344/302 624/634 496/482 447/356 494/389
(a)+(c)+(d) 586/608 429/452 382/336 421/410 ?
was automatically generated by enlarging an ini-
tial manually created seed vocabulary by con-
sulting two thesauri, tong2yi4ci2ci2lin2 and the
Academia Sinica Bilingual Ontological Word-
Net 3.
Chinese word segmentation was performed on
the four corpora using the conditional random
fields based Chinese Word Segmenter9. The to-
tal numbers of matched polarity words in each
corpus using different lexicon are shown in Ta-
ble 1 with the left half showing the statistics
against the Chinese lexicons (the original En-
glish lexicons have been translated into Chinese)
and the right half listing the statistics against the
English lexicons. We did not translate the Chi-
nese lexicon NTUSD into English since we fo-
cused on Chinese sentiment classification here.
It can be easily seen from the table that in gen-
eral the matched positive words outnumbered the
matched negative words using any single lexi-
con except SentiWordNet. But the combination
of the lexicons results in more matched polarity
words and thus gives more balanced number of
positive and negative words. We also observed
the increasing number of the matched polarity
words on the translated English corpora com-
pared to their original Chinese corpora. How-
ever, as will be discussed in Section 5.2 that the
increasing number of the matched polarity words
does not necessarily lead to the improvement of
the sentiment classification accuracy.
We modified GibbsLDA++ package10 for the
model implementation and only used hard con-
8080/opinion/pub1.html
9http://nlp.stanford.edu/software/
stanford-chinese-segmenter-2008-05-21.
tar.gz
10http://gibbslda.sourceforge.net/
straints as defined in Equation 4 in our experi-
ments. The word prior polarity information was
also utilized during the initialization stage that
if a word can be found in a sentiment lexicon,
the word token is assigned with its correspond-
ing sentiment label. Otherwise, a sentiment label
is randomly sampled for the word. Symmetric
Dirichlet prior ? was used for sentiment-word
distribution and was set to 0.01, while asym-
metric Dirichlet prior ? was used for document-
sentiment distribution and was set to 0.01 for
positive and neutral sentiment labels, and 0.05
for negative sentiment label.
5 Experimental Results
This section presents the experimental results
obtained under two different settings: LDA
model with translated English lexicons tested on
the original Chinese product review corpora; and
LDA model with original English lexicons tested
on the translated product review corpora.
5.1 Results with Different Sentiment
Lexicons
Table 3 gives the classification accuracy results
using the LDA model with prior sentiment la-
bel information provided by different sentiment
lexicons. Since we did not use any labeled in-
formation, the accuracies were averaged over 5
runs and on the whole corpora. For comparison
purposes, we have also implemented a baseline
model which simply assigns a score +1 and -1
to any matched positive and negative word re-
spectively based on a sentiment lexicon. A re-
view document is then classified as either posi-
tive or negative according to the aggregated sen-
timent scores. The baseline results were shown
in brackets in Table 3 .
Table 3: Sentiment classification accuracy (%) by LDA, numbers in brackets are baseline results.
Lexicon Mobile DigiCam MP3 Monitors Average
(a)MPQA 82.00 (63.53) 80.93 (67.59) 78.31 (68.42) 81.41 (64.86) 80.66 (66.10)
(b)Appraisal 71.95 (56.28) 80.46 (60.54) 77.28 (61.36) 80.67 (57.98) 77.59 (59.04)
(c)SentiWN 81.10 (62.45) 78.52 (57.13) 79.08 (64.57) 75.55 (55.34) 78.56 (59.87)
(d)NTUSD 82.61 (71.21) 78.70 (68.23) 78.69 (75.87) 84.63 (74.96) 81.16 (72.57)
(a)+(c) 81.18 (65.95) 78.70 (65.18) 83.83 (67.52) 80.53 (62.08) 81.06 (65.18)
(a)+(b)+(c) 81.48 (62.84) 80.22 (65.88) 80.23 (65.60) 78.62 (61.35) 80.14 (63.92)
(a)+(c)+(d) 82.48 (69.96) 84.33 (69.58) 83.70 (71.12) 82.72 (65.59) 83.31 (69.06)
Naive Bayes 86.52 82.27 82.64 86.21 84.41
SVMs 84.49 82.04 79.43 83.87 82.46
It can be observed from Table 3 that the
LDA model performs significantly better than
the baseline model. The improvement ranges be-
tween 9% and 19% and this roughly corresponds
to how much the model learned from the data.
We can thus speculate that LDA is indeed able to
learn the sentiment-word distributions from data.
Translated English sentiment lexicons per-
form comparably with the Chinese sentiment
lexicon NTUSD. As for the individual lexicon,
using MPQA subjectivity lexicon gives the best
result among all the English lexicons on all the
corpora except the MP3 corpus where MPQA
performs slightly worse than SentiWordNet. The
combination of MPQA and SentiWordNet per-
forms significantly better than other lexicons on
the MP3 corpus, with almost 5% improvement
compared to the second best result. We also
notice that the combination of all the three En-
glish lexicons does not lead to the improvement
of classification accuracy which implies that the
quality of a sentiment lexicon is indeed impor-
tant to sentiment classification. The above re-
sults suggest that in the absence of any Chinese
sentiment lexicon, MPQA subjectivity lexicon
appears to be the best candidate to be used to
provide sentiment prior information to the LDA
model for Chinese sentiment classification.
We also conducted experiments by includ-
ing the Chinese sentiment lexicon NTUSD and
found that the combination of MPQA, Senti-
WordNet, and NTUSD gives the best overall
classification accuracy with 83.31% achieved.
For comparison purposes, we list the 10-fold
cross validation results obtained using the super-
vised classifiers, Naive Bayes and SVMs, trained
on the labeled corpora as previously reported in
(Zagibalov and Carroll, 2008a). It can be ob-
served that using only English lexicons (the com-
bination of MPQA and SentiWordNet), we ob-
tain better results than both NB and SVMs on
the MP3 corpus. With an additional inclusion
of NTUSD, LDA outperforms NB and SVMs
on both DigiCam and MP3. Furthermore, LDA
gives a better overall accuracy when compared
to SVMs. Thus, we may conclude that the un-
supervised LDA model performs as well as the
supervised classifiers such as NB and SVMs on
the Chinese product review corpora.
5.2 Results with Translated Corpora
We ran a second set of experiments on the trans-
lated Chinese product review corpora using the
original English sentiment lexicons. Both the
translated corpora and the sentiment lexicons
have gone through stopword removal and stem-
ming in order to reduce the vocabulary size and
thereby alleviate data sparseness problem. It can
be observed from Figure 1 that in general senti-
ment classification on the original Chinese cor-
pora using the translated English sentiment lex-
icons gives better results than classifying on the
translated review corpora using the original En-
glish lexicons on both the Mobile and Digicam
corpora. However, reversed results are observed
on the Monitor corpus that classifying on the
translated review corpus using the English sen-
timent lexicons outperforms classifying on the
85
Mobi
le
8085
y?(%)
Mobi
le
70758085
Accurac
Mobi
le
6570758085
()M
PQA
(b)A
il
()S
iWN
()(
)
()(b
)()
Mobi
le
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
seCo
rpora
Englis
hCor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c
)?Sent
iWN
(a)+(c
)
(a)+(b
)+(c)
Mobi
le
Chine
se?Co
rpora
Englis
h?Cor
pora
85
DigiC
am
8085
y?(%)
DigiC
am
70758085
Accurac
DigiC
am
6570758085
(a)M
PQA
(b)Ap
praisa
l(c)
SentiW
N
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
DigiC
am
Chine
se?Co
rpora
Englis
h?Cor
pora
85
MP3
8085
y?(%)
MP3
70758085
Accurac
MP3
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
MP3
Chine
se?Co
rpora
Englis
h?Cor
pora
85
Moni
tor
8085
y?(%)
Moni
tor
70758085
Accurac
Moni
tor
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
6570758085
(a)?M
PQA
(b)?Ap
praisa
l(c)
?Senti
WN
(a)+(c
)
(a)+(b
)+(c)
Moni
tor
Chine
se?Co
rpora
Englis
h?Cor
pora
Figure 1: Comparison of the performance on the Chinese corpora and their translated corpora in
English.
original Chinese review corpus using the trans-
lated sentiment lexicons. In particular, the com-
bination of the MPQA subjectivity lexicon and
SentiWordNet gives the best result of 84% on
the Monitor corpus. As for the MP3 corpus,
classifying on the original Chinese reviews or on
the translated reviews does not differ much ex-
cept that a better result is obtained on the Chi-
nese corpus when using the combination of the
MPQA subjectivity lexicon and SentiWordNet.
The above results can be partially explained by
the ambiguities and changes of meanings intro-
duced in the translation. The Mobile and Digi-
Cam corpora are relatively larger than the MP3
and Monitors corpora and we therefore expect
more ambiguities being introduced which might
result in the change of document polarities.
5.3 Extracted Polarity-Bearing Words
LDA is able to extract polarity-bearing words.
Table 4 lists some of the polarity words identi-
fied by the LDA model which are not found in
the original sentiment lexicons. We can see that
LDA is indeed able to recognize domain-specific
positive or negative words, for example, ?Y
(bluetooth) for mobile phones, ? (compact)
for digital cameras,?^ (metallic) for MP3,?
s (flat screen) and?b (deformation) for mon-
itors.
The iterative approach proposed in (Zagibalov
and Carroll, 2008a) can also automatically ac-
quire polarity words from data. However, it ap-
pears that only positive words were identified
by their approach. Our proposed LDA model
can extract both positive and negative words and
most of them are highly domain-salient as can be
seen from Table 4.
6 Conclusions
This paper has proposed a mechanism to incor-
porate prior information about polarity words
from English sentiment lexicons into LDA
model learning for weakly-supervised Chinese
sentiment classification. Experimental results of
sentiment classification on Chinese product re-
views show that in the absence of a language-
specific sentiment lexicon, the translated En-
glish lexicons can still produce satisfactory re-
sults with the sentiment classification accuracy
of 81% achieved averaging over four different
types of product reviews. With the incorpora-
tion of the Chinese sentiment lexicon NTUSD,
the classification accuracy is further improved to
83%. Compared to the existing approaches to
cross-lingual sentiment classification which ei-
ther rely on labeled corpora for classifier learn-
ing or iterative training for performance gains,
the proposed approach is simple and readily to
Table 4: Extracted example polarity words by LDA.
Corpus Positive Negative
Mobile ? (advantage), ' (large), }( (easy to
use),? (fast), (comfortable),?Y (blue-
tooth),? (new),? (easy)
O (bad), ? (poor), b (slow), ? (no;not), ?
(difficult;hard), (less),?/ (but),? (repair)
DigiCam  ? (advantage),  ? (compact), :
(strong;strength), & (telephoto), ? (dy-
namic), h (comprehensive), Semantic Parsing for Biomedical Event Extraction
Deyu Zhou1 and Yulan He2
1School of Computer Science and Engineering, Southeast University,China
2Knowledge Media Institute, The Open University, UK
Abstract
We propose a biomedical event extraction system, HVS-BioEvent, which employs the hidden
vector state (HVS) model for semantic parsing. Biomedical events extraction needs to deal with
complex events consisting of embedded or hierarchical relations among proteins, events, and their
textual triggers. In HVS-BioEvent, we further propose novel machine learning approaches for event
trigger word identification, and for biomedical events extraction from the HVS parse results. Our
proposed system achieves an F-score of 49.57% on the corpus used in the BioNLP?09 shared task,
which is only two points lower than the best performing system by UTurku. Nevertheless, HVS-
BioEvent outperforms UTurku on the extraction of complex event types. The results suggest that the
HVS model with the hierarchical hidden state structure is indeed more suitable for complex event
extraction since it can naturally model embedded structural context in sentences.
1 Introduction
In the past few years, there has been a surge of interests in utilizing text mining techniques to pro-
vide in-depth bio-related information services. With an increasing number of publications reporting on
protein-protein interactions (PPIs), much effort has been made in extracting information from biomedical
articles using natural language processing (NLP) techniques. Several shared tasks, such as LLL [7] and
BioCreative [4], have been arranged for the BioNLP community to compare different methodologies for
biomedical information extraction.
Comparing to LLL and BioCreative which primarily focus on a simple representation of relations of
bio-molecules, i.e. protein-protein interaction, the BioNLP?09 Shared Task [5] involves the recognition
of bio-molecular events in scientific abstracts, such as gene expression, transcription, protein catabolism,
localization and binding, plus (positive or negative) regulation of proteins. The task concerns the detailed
behavior of bio-molecules, and can be used to support the development of biomedical-related databases.
In the BioNLP?09 shared task evaluation, the system constructed by UTurku [2] achieved an F-score of
51.95% on the core task, the best results among all the participants.
In this paper, we describe a system, called HVS-BioEvent, which employs the hidden vector state
model (HVS) to automatically extract biomedical events from biomedical literature. The HVS model has
been successfully employed to extract PPIs [9]. However, it is not straightforward to extend the usage
of the HVS model for biomedical events extraction. There are two main challenges. First, comparing
to the trigger words used for PPIs which are often expressed as single words or at most two words, the
trigger words for biomedical event are more complex. For example, controlled at transcriptional and
post-transcriptional levels, spanning over 6 words, is considered as the trigger word for the regulation
event. In addition, the same word can be the trigger word for different types of biomedical events in
different context. Second, biomedical events consist of both simple events and complex events. While
simple events are more similar to PPIs which only involve binary or pairwise relations, complex events
involve both n-ary (n > 2) and nested relations. For example, a regulation event may take another
event as its theme or cause which represents a structurally more complex relation. Being able to handle
both simple and complex events thus poses a huge challenge to the development of our HVS-BioEvent
system.
The rest of the paper is organized as follows. Section 2 presents the overall process of the HVS-
BioEvent system, which consists of three steps, trigger words identification, semantic parsing based on
395
the HVS model, and biomedical events extraction from the HVS parse results. Experimental results are
discussed in section 3. Finally, section 4 concludes the paper.
2 Biomedical Event Extraction
We perform biomedical event extraction with the following steps. At the beginning, abstracts are re-
trieved from MEDLINE and split into sentences. Protein names, gene names, trigger words for biomed-
ical events are then identified. After that, each sentence is parsed by the HVS semantic parser. Finally,
biomedical events are extracted from the HVS parse results using a hybrid method based on rules and
machine learning. All these steps process one sentence at a time. Since 95% of all annotated events
are fully annotated within a single sentence, this does not incur a large performance penalty but greatly
reduces the size and complexity of the problem. The remainder of the section will discuss each of the
steps in details.
2.1 Event Trigger Words Identification
Event trigger words are crucial to biomedical events extraction. In our system, we employ two ap-
proaches for event trigger words identification, one is a hybrid approach using both rules and a dictio-
nary, the other treats trigger words identification as a sequence labeling problem and uses a Maximum
Entropy Markov Model (MEMM) to detect trigger words.
For the hybrid approach using both rules and a dictionary, firstly, we constructed a trigger dictionary
from the original GENIA event corpus [6] by extracting the annotated trigger words. These trigger words
were subsequently lemmatized and stemmed. However, the wide variety of potential lexicalized triggers
for an event means that lots of triggers lack discriminative power relative to individual event types. For
example, in certain context, through is the trigger word for the binding event type and are is the trigger
word for localization. Such words are too common and cause potential ambiguities and therefore lead to
many false positive events extracted. We could perform disambiguation by counting the co-occurrence
of a event trigger and a particular event type from the training data and discard those event triggers whose
co-occurrence counts are lower than certain threshold for that event type. After this filtering stage, still,
there might be cases where one trigger might representing multiple event types, we thus define a set of
rules to further process the trigger words matched from the constructed dictionary.
In the second approach, we treat trigger words identification as a sequence labeling problem and train
a first-order MEMM model [8] from the BioNLP?09 shared task training data. As in typical named entity
recognition tasks, the training data are converted into BIO format where ?B? refers to the word which is
the beginning word of an event trigger, ?I? indicates the rest of the words (if the trigger contains more
than one words) and ?O? refers to the other words which are not event triggers. The features used in the
MEMM model was extracted from the surface string and the part-of-speech information of the words
corresponding to (or adjacent to) the target BIO tags.
2.2 Semantic Parsing using the HVS Model
The Hidden Vector State (HVS) model [3] is a discrete Hidden Markov Model (HMM) in which each
HMM state represents the state of a push-down automaton with a finite stack size. State transitions
are factored into separate stack pop and push operations constrained to give a tractable search space.
The sequence of HVS stack states corresponding to the given parse tree is illustrated in Figure 1. The
result is a model which is complex enough to capture hierarchical structure but which can be trained
automatically from only lightly annotated data.
In the HVS-based semantic parser, conventional grammar rules are replaced by three probability
tables. Let each state at time t be denoted by a vector of Dt semantic concept labels (tags) ct =
[ct[1], ct[2], ..ct[Dt]] where ct[1] is the preterminal concept label and ct[Dt] is the root concept label
(SS in Figure 3). Given a word sequence W , concept vector sequence C and a sequence of stack pop
operations N , the joint probability of P (W,C, N) can be decomposed as
P (W,C, N) =
T
?
t=1
P (nt|ct?1)P (ct[1]|ct[2 ? ? ?Dt])P (wt|ct) (1)
396
enhanced tyrosine phosphorylation of STAT1
Positive_regulation
Phosphorylation
Site
ProteinDummy
Positive_regulation
SS
Site
Positive_regulation
SS
Phosphorylation
Site
Positive_regulation
SS
Dummy
Phosphorylation
Site
Positive_regulation
SS
Protein
Phosphorylation
Site
Positive_regulation
SS
IFN-alpha
Dummy
SS
sent_start sent_end
SS
Dummy
SS SE
SS
SE
Figure 1: Example of a parse tree and its vector state equivalent.
where nt is the vector stack shift operation and takes values in the range 0, ? ? ? , Dt?1, and ct[1] = cwt is
the new pre-terminal semantic label assigned to word wt at word position t.
Thus, the HVS model consists of three types of probabilistic move, each move being determined by a
discrete probability table: (1) popping semantic labels off the stack - P (n|c); (2) pushing a pre-terminal
semantic label onto the stack - P (c[1]|c[2 ? ? ?D]); (3) generating the next word - P (w|c). Each of these
tables are estimated in training using an EM algorithm and then used to compute parse trees at run-time
using Viterbi decoding. In training, each word string W is marked with the set of semantic concepts
C that it contains. For example, the sentence IFN-alpha enhanced tyrosine phosphorylation of STAT1
contains the semantic concept/value pairs as shown in Figure 1. Its corresponding abstract semantic
annotation is:
Positive regulation(Site(Phosphorylation(protein)))
where brackets denote the hierarchical relations among semantic concepts1. For each word wk of a
training sentence W , EM training uses the forward-backward algorithm to compute the probability of
the model being in stack state c when wk is processed. Without any constraints, the set of possible stack
states would be intractably large. However, in the HVS model this problem can be avoided by pruning
out all states which are inconsistent with the semantic concepts associated with W . The details of how
this is done are given in [3].
For the sentences in the BioNLP?09 shared task, only event information is provided. However, the
abstract semantic annotation as shown above is required for training the HVS model. We propose Algo-
rithm 1 to automatically convert the annotated event information into the abstract semantic annotations.
An example of how the abstract annotations are generated is given as follows.
Sentence: According to current models the inhibitory capacity of I(kappa)B(alpha) would be mediated
through the retention of Rel/NF-kappaB proteins in the cytosol.
Corresponding Events: E1 Negative regulation: inhibitory capacity Theme: I(kappa)B(alpha)
E2 Positive regulation: mediated Theme: E1
Candidate annotation generation (Steps 1-4 of Algorithm 1):
Negative regulation(Protein) Negative regulation(Protein(Positive regulation))
Abstract annotation pruning (Steps 5-14 of Algorithm 1):
Negative regulation(Protein(Positive regulation))
2.3 Biomedical Events Extraction From HVS Parse Results
Based on HVS parse results, it seems straightforward to extract the event information. However, after
detailed investigation, we found that sentences having the same semantic tags might contain different
events information. For example, the two sentences shown in Table 1 have the same semantic parsing
results but with different event information.
This problem can be solved by classification. For the semantic tags which can represent multiple
event information, we considered each event information as a class and employed hiddenMarkov support
vector machines (HM-SVMs) [1] for disambiguation among possible events. The features used in HM-
SVMs are extracted from surface strings and part-of-speech information of the words corresponding to
(or adjacent to) trigger words.
1We omit SS and SE here which denote sentence start and end.
397
Algorithm 1 Abstract semantic annotation generation.
Input: A sentence W =< w1, w2, ? ? ? , wn >, and its event information Ev =< e1, e2, ? ? ? , em >
Output: Abstract semantic annotation A
1: for each event ei =<Event type:Trigger words Theme:Protein name ...> do
2: Sort the Trigger words, Protein name, and other arguments based on their positions in W and get
a sorted list t1, t2, ..., tk
3: Generate an annotation as t1(t2(..tk)), add it into the annotation list A
4: end for
5: for each annotation ai ? A do
6: if ai contains another event then
7: Replace the event with its corresponding annotation am
8: end if
9: end for
10: for each annotation ai ? A do
11: if ai is a subset of another annotation in A then
12: Remove ai from the annotation list A
13: end if
14: end for
15: Reorder annotations in A based on their positions in W
Sentence We concluded that CTCF expression and activity is con-
trolled at transcriptional and post-transcriptional levels
CONCLUSION: IL-5 synthesis by human helper T cells
is regulated at the transcriptional level
Parse
results
SS+Protein(CTCF) SS+Protein+Gene Expression(expression)
SS+Protein+Gene Expression+Regulation( controlled...levels)
SS+Protein(IL-5) SS+Protein+Gene Expression(synthesis)
SS+Protein+Gene Expression+Regulation( regulated)
Events E1 Gene expression:expression Theme: CTCF E1 Gene expression: synthesis Theme: IL-5
E2 Regulation: controlled...levels Theme: E1 E2 Regulation: regulated Theme: E1
E3 Regulation: controlled...levels Theme: CTCF
Table 1: An example of the same semantic parse results denoting different event information
3 Results and Discussion
Experiments have been conducted on the training data of the BioNLP?09 shared task which consists of
800 abstracts. After cleaning up the sentences which do not contain biomedical events information, 2893
sentences were kept. We split the 2893 sentences randomly into the training set and the test set at the
ratio of 9:1 and conducted the experiments ten times with different training and test data each round.
Method Recall (%) Precision (%) F-score (%)
Trigger Word Identification
Dictionary+Rules 46.31 53.34 49.57
MEMM 45.43 40.91 42.99
Event Extraction from HVS Parse Results
No classification 43.57 52.85 47.77
With Classification 46.31 53.34 49.57
Table 2: Experimental results based on 10 fold cross-validation.
Table 2 shows the performance evaluated using the approximate recursive matching method adopted
from the BioNLP?09 share task evaluation mode. To evaluate the performance impact of trigger word
identification, we also report the overall performance of the system using the two approaches we pro-
posed, dictionary+rules and MEMM. The results show that the hybrid approach combining a trigger
dictionary and rules gives better performance than MEMM which only achieved a F-score around 43%.
For biomedical event extraction from HVS parse results, employing the classification method presented
in Section 2.3 improves the overall performance from 47.77% to 49.57%.
The best performance that HVS-BioEvent achieved is an F-score of 49.57%, which is only two points
lower than UTurku, the best performing system in the BioNLP?09 share task. It should be noted that our
results are based on 10-fold cross validation on the BioNLP?09 shared task training data only since we
don?t have the access to the BioNLP?09 test set while the results generated by UTurku were evaluated
on the BioNLP?09 test set. Although a direct comparison is not possible, we could still speculate that
398
Simple Events Complex Events
Event Class HVS-BioEvent UTurku Event Class HVS-BioEvent UTurku
localization 61.40 61.65 binding 49.90 44.41
gene expression 72.44 73.90 regulation 36.57 30.52
transcription 68.30 50.23 negative regulation 40.61 38.99
protein catabolism 70.27 52.17
phosphorylation 56.52 77.58
Table 3: Per-class performance comparison in F-score (%) between HVS-BioEvent and UTurku.
HVS-BioEvent is comparable to the best performing system in the BioNLP?09 shared task.
The results on the five event types involving only a single theme argument are shown in Table 3
as Simple Events. For the complex events such as ?binding?, ?regulation? and ?negative regulation?
events, the results are shown in Table 3 as Complex Events. We notice that HVS-BioEvent outperforms
UTurku on the extraction of the complex event types, with the performance gain ranging between 2%
and 7%. The results suggest that the HVS model with the hierarchical hidden state structure is indeed
more suitable for complex event extraction since it could naturally model embedded structural context in
sentences.
4 Conclusions
In this paper, we have presented HVS-BioEvent which uses the HVS model to automatically extract
biomedical events from text. The system is able to offer comparable performance compared with the
best performing system in the BioNLP?09 shared task. Moreover, it outperforms the existing systems
on complex events extraction which shows the ability of the HVS model in capturing embedded and
hierarchical relations among named entities. In future work we will explore incorporating arbitrary
lexical features into the HVS model training in order to further improve the extraction accuracy.
References
[1] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden markov support vector machines. In Interna-
tional Conference in Machine Learning, pages 3?10, 2003.
[2] Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola, Tapio Pahikkla, and Tapio Salakoski. Extract-
ing complex biological events with rich graph-based feature sets. In Proceedings of the Workshop
on BioNLP, pages 10?18, 2009.
[3] Yulan He and Steve Young. Semantic processing using the hidden vector state model. Computer
Speech and Language, 19(1):85?106, 2005.
[4] Lynette Hirschman, Alexander Yeh, Christian Blaschke, and Alfonso Valencia. Overview of biocre-
ative: critical assessment of information extraction for biology. BMC Bioinformatics, 2005.
[5] Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun?ichi Tsujii. Overview of
bionlp?09 shared task on event extraction. In Proceedings of the Workshop on BioNLP, pages 1?9,
2009.
[6] Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. Corpus annotation for mining biomedical events
from literature. BMC Bioinformatics, 9(10), 2008.
[7] Claire Ne?dellec. Learning Language in Logic - Genic Interaction Extraction Challenge. In Learning
Language in Logic workshop (LLL05), pages 31?37, 2005.
[8] Nam Nguyen and Yunsong Guo. Comparisons of sequence labeling algorithms and extensions. In
Proceedings of the ICML, pages 681?688, 2007.
[9] Deyu Zhou, Yulan He, and Chee Keong Kwoh. Extracting protein-protein interactions from medline
using the hidden vector state model. International Journal of Bioinformatics Research and Applica-
tions, 4(1):64?80, 2008.
399
