Designing a Common POS-Tagset Framework for Indian Languages 
Sankaran Baskaran, Microsoft Research India. Bangalore. baskaran@microsoft.com 
Kalika Bali, Microsoft Research India. Bangalore. kalikab@microsoft.com 
Tanmoy Bhattacharya, Delhi University, Delhi. tanmoy1@gmail.com 
Pushpak Bhattacharyya, IIT-Bombay, Mumbai. pb@cse.iitb.ac.in 
Girish Nath Jha, Jawaharlal Nehru University, Delhi. girishj@mail.jnu.ac.in 
Rajendran S, Tamil University, Thanjavur. raj_ushush@yahoo.com 
Saravanan K, Microsoft Research India, Bangalore. v-sarak@microsoft.com 
Sobha L, AU-KBC Research Centre, Chennai. sobha@au-kbc.org 
Subbarao K V. Delhi. kvs2811@yahoo.com
 
 
Abstract 
Research in Parts-of-Speech (POS) tagset 
design for European and East Asian lan-
guages started with a mere listing of impor-
tant morphosyntactic features in one lan-
guage and has matured in later years to-
wards hierarchical tagsets, decomposable 
tags, common framework for multiple lan-
guages (EAGLES) etc. Several tagsets 
have been developed in these languages 
along with large amount of annotated data 
for furthering research. Indian Languages 
(ILs) present a contrasting picture with 
very little research in tagset design issues. 
We present our work in designing a com-
mon POS-tagset framework for ILs, which 
is the result of in-depth analysis of eight 
languages from two major families, viz. 
Indo-Aryan and Dravidian. Our framework 
follows hierarchical tagset layout similar to 
the EAGLES guidelines, but with signifi-
cant changes as needed for the ILs. 
1 Introduction 
A POS tagset design should take into consideration 
all possible morphosyntactic categories that can 
occur in a particular language or group of languag-
es (Hardie, 2004). Some effort has been made in 
the past, including the EAGLES guidelines for 
morphosyntactic annotation (Leech and Wilson, 
1996) to define guidelines for a common tagset 
across multiple languages with an aim to capture 
more detailed morphosyntactic features of these 
languages.  
However, most of the tagsets for ILs are lan-
guage specific and cannot be used for tagging data 
in other language. This disparity in tagsets hinders 
interoperability and reusability of annotated corpo-
ra. This further affects NLP research in resource 
poor ILs where non-availability of data, especially 
tagged data, remains a critical issue for researchers. 
Moreover, these tagsets capture the morphosyntac-
tic features only at a shallow level and miss out the 
richer information that is characteristic of these 
languages. 
The work presented in this paper focuses on de-
signing a common tagset framework for Indian 
languages using the EAGLES guidelines as a mod-
el. Though Indian languages belong to (mainly) 
four distinct families, the two largest being Indo-
Aryan and Dravidian, as languages that have been 
in contact for a long period of time, they share sig-
nificant similarities in morphology and syntax. 
This makes it desirable to design a common tagset 
framework that can exploit this similarity to facili-
tate the mapping of different tagsets to each other. 
This would not only allow corpora tagged with 
different tagsets for the same language to be reused 
but also achieve cross-linguistic compatibility be-
tween different language corpora. Most important-
ly, it will ensure that common categories of differ-
ent languages are annotated in the same way. 
In the next section we will discuss the impor-
tance of a common standard vis-?-vis the currently 
available tagsets for Indian languages. Section 3 
will provide the details of the design principles 
The 6th Workshop on Asian Languae Resources, 2008
89
behind the framework presented in this paper. Ex-
amples of tag categories in the common framework 
will be presented in Section 4. Section 5 will dis-
cuss the current status of the paper and future steps 
envisaged.  
2 Common Standard for POS Tagsets 
Some of the earlier POS tagsets were designed 
for English (Greene and Rubin, 1981; Garside, 
1987; Santorini, 1990) in the broader context of 
automatic parsing of English text. These tagsets 
popular even today, though designed for the same 
language differ significantly from each other mak-
ing the corpora tagged by one incompatible with 
the other. Moreover, as these are highly language 
specific tagsets they cannot be reused for any other 
language without substantial changes this requires 
standardization of POS tagsets (Hardie 2004).  
Leech and Wilson (1999) put forth a strong argu-
ment for the need to standardize POS tagset for 
reusability of annotated corpora and interopera-
bility across corpora in different languages. 
EAGLES guidelines (Leech and Wilson 1996) 
were a result of such an initiative to create stan-
dards that are common across languages that share 
morphosyntactic features. 
Several POS tagsets have been designed by a 
number of research groups working on Indian 
Languages though very few are available publicly 
(IIIT-tagset, Tamil tagset). However, as each of 
these tagsets have been motivated by specific re-
search agenda, they differ considerably in terms of 
morphosyntactic categories and features, tag defi-
nitions, level of granularity, annotation guidelines 
etc. Moreover, some of the tagsets (Tamil tagset) 
are language specific and do not scale across other 
Indian languages. This has led to a situation where 
despite strong commonalities between the lan-
guages addressed resources cannot be shared due 
to incompatibility of tasgets. This is detrimental to 
the development of language technology for Indian 
languages which already suffer from a lack of ade-
quate resources in terms of data and tools. 
In this paper, we present a common framework 
for all Indian languages where an attempt is made 
to treat equivalent morphosyntactic phenomena 
consistently across all languages. The hierarchical 
design, discussed in detail in the next section, also 
allows for a systematic method to annotate lan-
guage particular categories without disregarding 
the shared traits of the Indian languages.  
3 Design Principles 
Whilst several large projects have been concerned 
with tagset development very few have touched 
upon the design principles behind them. Leech 
(1997), Cloeren (1999) and Hardie (2004) are 
some important examples presenting universal 
principles for tagset design. 
In this section we restrict the discussion to the 
principles behind our tagset framework. Important-
ly, we diverge from some of the universal prin-
ciples but broadly follow them in a consistent way.  
Tagset structure: Flat tagsets just list down the 
categories applicable for a particular language 
without any provision for modularity or feature 
reusability. Hierarchical tagsets on the other hand 
are structured relative to one another and offer a 
well-defined mechanism for creating a common 
tagset framework for multiple languages while 
providing flexibility for customization according to 
the language and/ or application. 
Decomposability in a tagset alows different fea-
tures to be encoded in a tag by separate sub-stings. 
Decomposable tags help in better corpus analysis 
(Leech 1997) by allowing to search with an un-
derspecified search string. 
In our present framework, we have adopted the 
hierarchical layout as well as decomposable tags 
for designing the tagset. The framework will have 
three levels in the hierarchy with categories, types 
(subcategories) and features occupying the top, 
medium and the bottom layers. 
What to encode? One thumb rule for the POS 
tagging is to consider only the aspects of morpho-
syntax for annotation and not that of syntax, se-
mantics or discourse. We follow this throughout 
and focus only on the morphosyntactic aspects of 
the ILs for encoding in the framework. 
Morphology and Granularity: Indian languag-
es have complex morphology with varying degree 
of richness. Some of the languages such as those of 
the Dravidian family also display agglutination as 
an important characteristic. This entails that mor-
phological analysis is a desirable pre-process for 
the POS tagging to achieve better results in auto-
matic tagging. We encode all possible morphosyn-
tactic features in our framework assuming the exis-
The 6th Workshop on Asian Languae Resources, 2008
90
tence of morphological analysers and leave the 
choice of granularity to users. 
As pointed out by Leech (1997) some of the 
linguistically desirable distinctions may not be 
feasible computationally. Therefore, we ignore 
certain features that may not be computationally 
feasible at POS tagging level. 
Multi-words: We treat the constituents of Mul-
ti-word expressions (MWEs) like Indian Space 
Research Organization as individual words and tag 
them separately rather than giving a single tag to 
the entire word sequence. This is done because: 
Firstly, this is in accordance with the standard 
practice followed in earlier tagsets. Secondly, 
grouping MWEs into a single unit should ideally 
be handled in chunking. 
Form vs. function: We try to adopt a balance 
between form and function in a systematic and 
consistent way through deep analysis. Based on 
our analysis we propose to consider the form in 
normal circumstances and the function for words 
that are derived from other words. More details on 
this will be provided in the framework document 
(Baskaran et al2007) 
Theoretical neutrality: As Leech (1997) points 
out the annotation scheme should be theoretically 
neutral to make it clearly understandable to a larger 
group and for wider applicability. 
Diverse Language families: As mentioned ear-
lier, we consider eight languages coming from two 
major language families of India, viz. Indo-Aryan 
and Dravidian. Despite the distinct characteristics 
of these two families, it is however striking to note 
the typological parallels between them, especially 
in syntax. For example, both families follow SOV 
pattern. Also, several Indo-Aryan languages such 
as Marathi, Bangla etc. exhibit some agglutination, 
though not to the same extent of Dravidian. Given 
the strong commonalities between the two families 
we decided to use a single framework for them 
4 POS Tagset Framework for Indian lan-
guages 
The tagset framework is laid out at the following 
four levels similar to EAGLES. 
I. Obligatory attributes or values are generally 
universal for all languages and hence must be 
included in any morphosyntactic tagset. The 
major POS categories are included here. 
II. Recommended attributes or values are recog-
nised to be important sub-categories and fea-
tures common to a majority of languages.  
III. Special extensions1 
a. Generic attributes or values 
b. Language-specific attributes or values are 
the attributes that are relevant only for few lan-
guages and do not apply to most languages. 
All the tags were discussed and debated in detail 
by a group of linguists and computer scien-
tists/NLP experts for eight Indian languages viz. 
Bengali, Hindi, Kannada, Malayalam, Marathi, 
Sanskrit, Tamil and Telugu.  
Now, because of space constraints we present 
only the partial tagset framework. This is just to 
illustrate the nature of the framework and the com-
plete version as well as the rationale for different 
categories/features in the framework can be found 
in Baskaran et al (2007).2 
In the top level the following 12 categories are 
identified as universal categories for all ILs and 
hence these are obligatory for any tagset. 
 
1. [N] Nouns 7.   [PP] Postpositions  
2. [V] Verbs  8.   [DM] Demonstratives 
3. [PR] Pronouns  9.   [QT] Quantifiers 
4. [JJ] Adjectives  10. [RP] Particles  
5. [RB] Adverbs  11. [PU] Punctuations  
6. [PL] Participles  12. [RD] Residual3 
 
The partial tagset illustrated in Figure 1 high-
lights entries in recommended and optional catego-
ries for verbs and participles marked for three le-
vels.4 The features take the form of attribute-value 
pairs with values in italics and in some cases (such 
as case-markers for participles) not all the values 
are fully listed in the figure. 
5 Current Status and Future Work 
In the preceding sections we presented a common 
framework being designed for POS tagsets for In-
dian Languages. This hierarchical framework has 
                                                 
1
 We do not have many features defined under the special 
extensions and this is mainly retained for any future needs. 
2 Currently this is just the draft version and the final version 
will be made available soon 
3 For words or segments in the text occurring outside the gam-
bit of grammatical categories like foreign words, symbols,etc.   
4  These are not finalised as yet and there might be some 
changes in the final version of the framework. 
The 6th Workshop on Asian Languae Resources, 2008
91
three levels to permit flexibility and interoperabili-
ty between languages. We are currently involved in 
a thorough review of the present framework by 
using it to design the tagset for specific Indian lan-
guages. The issues that come up during this 
process will help refine and consolidate the 
framework further.  In the future, annotation guide-
lines with some recommendations for handling 
ambiguous categories will also be defined.  With 
the common framework in place, it is hoped that 
researchers working with Indian Languages would 
be able to not only reuse data annotated by each 
other but also share tools across projects and lan-
guages. 
References 
Baskaran S. et al 2007. Framework for a Common 
     Parts-of-Speech Tagset for Indic Languages. (Draft) 
    http://research.microsoft.com/~baskaran/POSTagset/ 
  
Cloeren, J. 1999. Tagsets. In Syntactic Wordclass Tagging, 
ed. Hans van Halteren, Dordrecht.: Kluwer Academic. 
Hardie, A . 2004. The Computational Analysis of Morpho-
syntactic Categories in Urdu. PhD thesis submitted to 
Lancaster University. 
Greene, B.B. and Rubin, G.M. 1981. Automatic grammati-
cal tagging of English. Providence, R.I.: Department of 
Linguistics, Brown University 
Garside, R. 1987 The CLAWS word-tagging system. In 
The Computational Analysis of English, ed. Garside, 
Leech and Sampson, London: Longman. 
Leech, G and Wilson, A. 1996. Recommendations for the 
Morphosyntactic Annotation of Corpora. EAGLES Re-
port EAG-TCWG-MAC/R. 
Leech, G. 1997. Grammatical Tagging. In Corpus Annota-
tion: Linguistic Information from Computer Text Cor-
pora, ed: Garside, Leech and McEnery, London: Long-
man  
Leech, G and Wilson, A. 1999. Standards for Tag-sets. In 
Syntactic Wordclass Tagging, ed. Hans van Halteren, 
Dordrecht: Kluwer Academic. 
Santorini, B. 1990. Part-of-speech tagging guidelines for 
the Penn Treebank Project. Technical report MS-CIS-
90-47, Department of Computer and Information 
Science, University of Pennsylvania 
IIIT-tagset. A Parts-of-Speech tagset for Indian languages. 
http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.
pdf 
Tamil tagset. AU-KBC Parts-of-Speech tagset for Tamil. 
http://nrcfosshelpline.in/smedia/images/downloads/Tam
il_Tagset-opensource.odt 
Aspect 
 Perfect 
 Imperfect 
 Progressive 
Mood 
 Declarative 
 Subjunctative/   
        Hortative 
 Conditional 
 Imperative 
 Presumptive 
Level - 3 
Nouns 
Verbs 
Pronouns 
Adjectives 
Adverbs 
Postpositions 
Demonstratives 
Quantifiers 
Particles 
Punctuations 
Residual Participles 
Level - 1 
Type 
 Finite 
 Auxiliary 
 Infinitive 
 Non-finite 
 Nominal 
Gender 
 Masculine 
 Feminine 
 Neuter 
Number 
 Singular 
 Plural/Hon. 
 Dual 
 Honourific 
Person 
 First 
 Second 
 Third 
Tense 
 Past 
 Present 
 Future 
Negative 
Type 
 General 
 Adjectival 
 Verbal 
 Nominal 
Gender 
 As in verbs 
Number 
 Singular 
 Plural 
 Dual 
Case 
 Direct 
 Oblique 
Case-markers 
 Ergative 
 Accusative 
 etc. 
Tense 
 As in verbs 
Negative 
Level - 2 
Fig-1. Tagset framework - partial representation 
The 6th Workshop on Asian Languae Resources, 2008
92
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1089?1099,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Efficient Left-to-Right Hierarchical Phrase-based Translation with
Improved Reordering
Maryam Siahbani, Baskaran Sankaran, Anoop Sarkar
Simon Fraser University
Burnaby BC. CANADA
{msiahban,baskaran,anoop}@cs.sfu.ca
Abstract
Left-to-right (LR) decoding (Watanabe et al,
2006b) is a promising decoding algorithm for
hierarchical phrase-based translation (Hiero).
It generates the target sentence by extending
the hypotheses only on the right edge. LR de-
coding has complexity O(n2b) for input of n
words and beam size b, compared toO(n3) for
the CKY algorithm. It requires a single lan-
guage model (LM) history for each target hy-
pothesis rather than two LM histories per hy-
pothesis as in CKY. In this paper we present an
augmented LR decoding algorithm that builds
on the original algorithm in (Watanabe et al,
2006b). Unlike that algorithm, using experi-
ments over multiple language pairs we show
two new results: our LR decoding algorithm
provides demonstrably more efficient decod-
ing than CKY Hiero, four times faster; and by
introducing new distortion and reordering fea-
tures for LR decoding, it maintains the same
translation quality (as in BLEU scores) ob-
tained phrase-based and CKY Hiero with the
same translation model.
1 Introduction
Hiero (Chiang, 2007) models translation using a lex-
icalized synchronous context-free grammar (SCFG)
extracted from word aligned bitexts. Typically,
CKY-style decoding is used for Hiero with time
complexity O(n3) for source input with n words.
Scoring the target language output using a lan-
guage model within CKY-style decoding requires
two histories per hypothesis, one on the left edge
of each span and one on the right, due to the fact
that the target side is not generated in left to right
order, but rather built bottom-up from sub-spans.
This leads to complex problems in efficient lan-
guage model integration and requires state reduc-
tion techniques (Heafield et al, 2011; Heafield et
al., 2013). The size of a Hiero SCFG grammar is
typically larger than phrase-based models extracted
from the same data creating challenges in rule ex-
traction and decoding time especially for larger
datasets (Sankaran et al, 2012).
In contrast, the LR-decoding algorithm could
avoid these shortcomings such as faster time com-
plexity, reduction in the grammar size and the sim-
plified left-to-right language model scoring. It
means LR decoding has the potential to replace
CKY decoding for Hiero. Despite these attractive
properties, we show that the original LR-Hiero de-
coding proposed by (Watanabe et al, 2006b) does
not perform to the same level of the standard CKY
Hiero with cube pruning (see Table 3). In addition,
the current LR decoding algorithm does not obtain
BLEU scores comparable to phrase-based or CKY-
based Hiero models for different language pairs (see
Table 4). In this paper we propose modifications to
the LR decoding algorithm that addresses these limi-
tations and provides, for the first time, a true alterna-
tive to the standard CKY Hiero algorithm that uses
left-to-right decoding.
We introduce a new extended version of the LR
decoding algorithm presented in (Watanabe et al,
2006b) which is demonstrably more efficient than
the CKY Hiero algorithm. We measure the effi-
ciency of the LR Hiero decoder in a way that is
independent of the choice of system and program-
ming language by measuring the number of lan-
guage model queries. Although more efficient, the
new LR decoding algorithm suffered from lower
BLEU scores compared to CKY Hiero. Our anal-
ysis of left to right decoding showed that it has more
potential for search errors due to early pruning of
good hypotheses. This is unlike bottom-up decoding
(CKY) which keeps best hypotheses for each span.
To address this issue, we introduce two novel fea-
tures into the Hiero SMT model that deal with re-
ordering and distortion. Our experiments show that
LR decoding with these features using prefix lexi-
1089
calized target side rules equals the scores obtained
by CKY decoding with prefix lexicalized target side
rules and phrase-based translation system. It per-
forms four times fewer language model queries on
average, compare to CKY Hiero decoding with un-
restricted Hiero rules: 6466.7 LM queries for CKY
Hiero (with cube pruning) compared to 1500.45 LM
queries in LR Hiero (with cube pruning). While
translation quality suffers by only about 0.67 in
BLEU score on average, across two different lan-
guage pairs.
2 Left-to-Right Decoding for Hiero
Hierarchical phrase-based SMT (Chiang, 2005; Chi-
ang, 2007) uses a synchronous context free gram-
mar (SCFG), where the rules are of the form X ?
??, ??, where X is a non-terminal, ? and ? are
strings of terminals and non-terminals.
Chiang (2007) places certain constraints on the
extracted rules in order to simplify decoding. This
includes limiting the maximum number of non-
terminals (rule arity) to two and disallowing any rule
with consecutive non-terminals on the foreign lan-
guage side. It further limits the length of the initial
phrase-pair as well as the number of terminals and
non-terminals in the rule. For translating sentences
longer than the maximum phrase-pair length, the de-
coder relies on additional glue rules S ? ?X,X?
and S ? ?SX,SX? that allows monotone combi-
nation of phrases. The glue rules are used when no
rules could match or the span length is larger than
the maximum phrase-pair length.
2.1 Rule Extraction for LR Decoding
Left-to-right Hiero (Watanabe et al, 2006b) gener-
ates the target hypotheses left to right, but for syn-
chronous context-free grammar (SCFG) as used in
Hiero. The target-side rules are constrained to be
prefix lexicalized. These constrained SCFG rules
are defined as:
X ? ??,
?
b ?? (1)
where ? is a mixed string of terminals and non-
terminals. ?b is a terminal sequence prefixed to the
possibly empty non-terminal sequence ?. For the
sake of simplicity, We refer to these type of rules as
their work
students
X1
X2
X6
X4 X5
X3have
not yet
done
.
schuler ihre noch nicht gemacht haben .arbeit
students have done their workyet .not
(b)
(a)
gemacht
schuler
X1
X2
X6
X5 X4
X3 haben
noch nicht
ihre arbeit
.
1
2
3 6
45
Figure 1: (a): A word-aligned German-English sentence
pair. The bars above the source words indicate phrase-
pairs having at least two words. (b): its corresponding
left-to-right target derivation tree. Superscripts on the
source non-terminals show the indices of the rules (see
Fig 2) used in derivation.
GNF rules1 in this paper.
Rule extraction is similar to Hiero, except any
rules violating GNF form on the target side are
excluded. Rule extraction considers each smaller
source-target phrase pair within a larger phrase pair
and replaces the spans with non-terminal X , yield-
ing hierarchical rules. Figure 1(a) shows a word-
aligned German-English sentence with a phrase
pair ?ihre arbeit noch nicht gemacht haben,
have not yet done their work? that will lead to a
SCFG rule. Given other smaller phrases (marked by
bars above the source side), we extract a GNF rule2:
X ?
?X
1
noch nicht X
2
haben, have not yet X
2
X
1
?
(2)
In order to avoid data sparsity and for better gen-
eralization, Watanabe et al (2006b) adds four glue
rules for each lexical rule ? ?f, e?? which are analo-
gous to the glue rules defined in (Chiang, 2007) (see
above) except that these glue rules for LR decoding
1Griebach Normal Form (GNF), although the synchronous
grammar is not in this normal form, rather only the target side
is prefix lexicalized as if it were in GNF form.
2 LR-Hiero rule extraction excludes non-GNF rules such as
X ? ?X1 noch nicht gemacht X2, X2 not yet done X1?.
1090
allow reordering as well.
X ? ?
?
fX1, e?X1? X ? ?X1
?
fX2, e?X1X2?
X ? ?X1
?
f, e?X1? X ? ?X1
?
fX2, e?X2X1?
(3)
It might appear that the restriction that target-side
rules be GNF is a severe restriction on the cover-
age of possible hypotheses compared to the full set
of rules permitted by the Hiero extraction heuris-
tic. However there is some evidence in the liter-
ature that discontinuous spans on the source side
in translation rules is a lot more useful than dis-
continuous spans in the target side (which is disal-
lowed in the GNF). For instance, (Galley and Man-
ning, 2010) do an extensive study of discontinuous
spans on source and target side and show that source
side discontinuous spans are very useful but remov-
ing discontinuous spans on the target side only low-
ers the BLEU score by 0.2 points (using the Joshua
SMT system on Chinese-English). Removing dis-
continuous spans means that the target side rules
have the form: uX,Xu,XuX,XXu, or uXX of
which we disallow Xu,XuX,XXu. Zhang and
Zong (2012) also conduct a study on discontinuous
spans on source and target side of Hiero rules and
conclude that source discontinuous spans are always
more useful than discontinuities on the target side
with experiments on four language pairs (zh-en, fr-
en, de-en and es-en). As we shall also see in our
experimental results (see Table 4) we can get close
to the BLEU scores obtained using the full set of Hi-
ero rules by using only target lexicalized rules in our
LR decoder.
2.2 LR-Hiero Decoding
LR-Hiero decoding uses a top-down depth-first
search, which strictly grows the hypotheses in target
surface ordering. Search on the source side follows
an Earley-style search (Earley, 1970), the dot jumps
around on the source side of the rules based on the
order of nonterminals on the target side. This search
is integrated with beam search or cube pruning to
efficiently find the k-best translations.
Several important details about the algorithm of
LR-Hiero decoding are implicit and unexplained
in (Watanabe et al, 2006b). In this section we de-
scribe the LR-Hiero decoding algorithm in more de-
tail than the original description in (Watanabe et al,
Algorithm 1: LR-Hiero Decoding
1: Input sentence: f = f
0
f
1
. . . fn
2: F = FutureCost(f) (Precompute future cost for
spans)
3: for i = 0, . . . , n do
4: Si = {} (Create empty stacks)
5: h
0
= (?s?, [[0, n]], ?,F
[0,n]) (Initial hypothesis
4-tuple)
6: Add h
0
to S
0
(Push initial hyp into first Stack)
7: for i = 0, . . . , n? 1 do
8: for each h in Si do
9: [u, v] = pop(hs) (Pop first uncovered span
from list)
10: R = GetSpanRules([u, v]) (Extract rules
matching the entire span [u, v])
11: for r ? R do
12: h? = GrowHypothesis(h, r, [u, v],F) (New
hypothesis)
13: Add h? to Sl, where l = |h?cov| (Add new
hyp to stack)
14: return arg max(Sn)
15: GrowHypothesis(h, r, [u, v],F)
16: h? = (h?t = ?, h
?
s = hs, h
?
cov = ?, h
?
c = 0)
17: rX = {Xj , Xk, . . . |j C k C . . .} (Get NTs in
surface order)
18: for each X in reverse(rX) do
19: push(h?s, span(X)) (Push uncovered spans to
LIFO list)
20: h?t = Concatenate(ht, rt)
21: h?cov = UpdateCoverage(hcov, rs)
22: h?c = ComputeCost(g(h
?
),F?h?cov )
23: return h?
2006b). We explain our own modified algorithm for
LR decoding with cube pruning in Section 2.3.
Algorithm 1 shows the pseudocode for LR de-
coding. Decoding the example in Figure 1(b)
is explained using a walk-through shown in Fig-
ure 2. Each partial hypothesis h is a 4-tuple
(ht, hs, hcov, hc): consisting of a translation prefix
ht, a (LIFO-ordered) list hs of uncovered spans,
source words coverage set hcov and the hypothesis
cost hc. The initial hypothesis is a null string with
just a sentence-initial marker ?s? and the list hs con-
taining a span of the whole sentence, [0, n]. The hy-
potheses are stored in stacks S0, . . . , Sn, where each
stack corresponds to a coverage vector of same size,
covering same number of source words (Koehn et
al., 2003).
At the beginning of beam search the initial hy-
1091
? X ? schuler ihre arbeit nochnicht gemacht haben .?schuler ? X11?ihrearbeit nochnicht gemacht haben .?schuler ? X12?ihre arbeit nochnicht gemacht ? haben X 22?.?schuler X 13?ihrearbeit ? nochnicht ? X23?gemacht? haben X 22?.?schuler ? X13 ?ihre arbeit? nochnicht gemacht haben X 22?.?schuler ihre arbeit nochnicht gemacht haben ?X 22?.?schuler ihre arbeit nochnicht gemacht haben .
1) X??schuler X1/ students X1?2) X??X1heban X 2/have X 1X 2?3 )X??X 1nochnicht X2/not yet X 2X 1?4 ) X??gemacht /done ?5 )X?? ihre arbeit / their work ?6 )X?? ./ . ?
[0,8]students [1,8 ]students have [1,6 ][7,8]students have not yet [5,6] [1,3 ][7,8]students have not yet done [1,3 ][7,8]students have not yet done their work [7,8]students have not yet done their work .
rules source side coverage hypothesis
GG <s><s><s><s><s><s>
<s>
</s>
Figure 2: Illustration of the LR-Hiero decoding process in Figure 1. (a) Rules pane show the rules used in the derivation
(glue rules are marked byG) (b) Decoder state using Earley dot notation (superscripts show rule#) (c) Hypotheses pane
showing translation prefix and ordered list of yet-to-be-covered spans.
pothesis h0 is added to the decoder stack S0 (line 6
in Algoorithm 1). Hypotheses in each decoder stack
are expanded iteratively, generating new hypotheses,
which are added to the latter stacks corresponding to
the number of source words covered. In each step it
pops from the LIFO list hs, the span [u, v] of the
next hypothesis h to be processed.
All rules that match the entire span [u, v] are then
obtained efficiently via pattern matching (Lopez,
2007). GetSpanRules addresses possible ambigui-
ties in matched rules to the given span [u, v]. For
example, given a rule r, with source side rs :
?X1 the X2? and source phrase p : ?ok, the more
the better?. There is ambiguity in matching r to
p. GetSpanRules returns a distinct matched rule for
each possible matching.
The GrowHypothesis routine creates a new can-
didate by expanding given hypothesis h using rule
r and computes the complete hypothesis score in-
cluding language model score. Since the target-side
rules are in GNF, the translation prefix of the new
hypothesis is obtained by simply concatenating the
terminal prefixes of h and r in same order (line 20).
UpdateCoverage updates source word coverage set
using the source side of r. The hs list is built by
pushing the non-terminal spans of rule r in a reverse
order (lines 17 and 18). The reverse ordering main-
tains the left-to-right generation of the target side.
In the walk-through in Figure 2, the derivation
process starts by expanding the initial hypothesis h0
(first item in the right pane of Fig 2) with the rule
(rule #1 in left pane) to generate a new partial candi-
date having a terminal prefix of ?s? students (second
item in right pane). The second item in the middle
pane shows the current position of the parser em-
ploying Earley?s dot notation, indicating that the first
word has already been translated. Now the decoder
considers the second hypothesis and pops the span
[1, 8]. It then matches the rule (#2) and pushes the
spans [1, 6] and [7, 8] into the list hs in the reverse
order of their appearance in the target-side rule. At
each step the new hypothesis is added to the decoder
stack Sl depending on the number of covered words
in the new hypothesis (line 13 in Algorithm 1).
For pruning we use an estimate of the future cost3
of the spans uncovered by current hypothesis to-
gether with the hypothesis cost. The future cost is
precomputed (line 2 Algorithm 1) in a way simi-
lar to the phrase-based models (Koehn et al, 2007)
using only the terminal rules of the grammar. The
ComputeCost method (line 22 in Algorithm 1) uses
the usual log-linear model and scores a hypothesis
based on its different feature scores g(h?) and the
future cost of the yet to be covered spans (F?h?cov ).
Time complexity of left to right Hiero decoding with
beam search is O(n2b) in practice where n is the
length of source sentence and b is the size of beam
(Huang and Mi, 2010).
2.3 LR-Hiero Decoding with Cube Pruning
The Algorithm 1 presented earlier does an ex-
haustive search as it generates all possible partial
translations for a given stack that are reachable from
the hypotheses in previous stacks. However only a
few of these hypotheses are retained, while majority
of them are pruned away. The cube pruning tech-
nique (Chiang, 2007) avoids the wasteful generation
of poor hypotheses that are likely to be pruned away
by efficiently restricting the generation to only high
scoring partial translations.
We modify the cube pruning for LR-decoding
that takes into account the next uncovered span to
3 Watanabe et al (2006b) also use a similar future cost, even
though it is not discussed in the paper (p.c.).
1092
Algorithm 2: LR-Hiero Decoding with Cube Pruning
1: Input sentence: f = f
0
f
1
. . . fn
2: F = FutureCost(f) (Precompute future cost for
spans)
3: S
0
= {} (Create empty initial stack)
4: h
0
= (?s?, [[0, n]], ?,F
[0,n]) (Initial hypothesis
4-tuple)
5: Add h
0
to S
0
(Push initial hyp into first Stack)
6: for i = 1, . . . , n do
7: cubeList = {} (MRL is max rule length)
8: for p = max(i? MRL, 0), . . . , i? 1 do
9: {G} = Grouped(Sp) (Group based on the first
uncovered span)
10: for g ? {G} do
11: [u, v] = gspan
12: R = GetSpanRules([u, v])
13: for Rs ? R do
14: cube = [ghyps, Rs]
15: Add cube to cubeList
16: Si = Merge(cubeList,F) (Create stack Si and
add new hypotheses to it, see Figure 3)
17: return arg max(Sn)
18: Merge(CubeList,F)
19: heapQ = {}
20: for each (H,R) in cubeList do
21: [u, v] = span of rule R
22: h? = GrowHypothesis(h
1
, r
1
, [u, v],F) (from
Algorithm 1)
23: push(heapQ, (h?c, h
?
, [H,R])
24: hypList = {}
25: while |heapQ| > 0 and |hypList| < K do
26: (h?c, h
?
, [H,R]) = pop(heapQ)
27: push(heapQ,GetNeighbours([H,R])
28: Add h? to hypList
29: return hypList
be translated indicated by the Earley?s dot nota-
tion. The Algorithm 2 shows the pseudocode for
LR-decoding using cube pruning. The structure of
stacks and hypotheses and computing the future cost
is similar to Algorithm 1 (lines 1-5). To fill stack
Si, it iterates over previous stacks (line 8 in Algo-
rithm 2) 4. All hypotheses in each stack Sp (cov-
ering p words on the source-side) are first parti-
tioned into a set of groups, {G}, based on their
first uncovered span (line 9) 5. Each group g is a
4As the length of rules are limited (at most MRL), we can
ignore stacks with index less than i? MRL
5The beam search decoder in Phrase-based system (Huang
and Chiang, 2007; Koehn et al, 2007; Sankaran et al, 2010)
2-tuple (gspan, ghyps), where ghyps is a list of hy-
potheses which share the same first uncovered span
gspan. Rules matching the span gspan are obtained
from routine GetSpanRules, which are then grouped
based on unique source side rules (i.e. each Rs con-
tains rules that share the same source side s but have
different target sides). Each ghyps and possible Rs6
create a cube which is added to cubeList.
In LR-Hiero, each hypothesis is developed with
only one uncovered span, therefore each cube al-
ways has just two dimensions: (1) hypotheses with
the same number of covered words and similar first
uncovered span, (2) rules sharing the same source
side. In Figure 3(a), each group of hypotheses,
ghyps, is shown in a green box (in stacks), and each
rectangle on the top is a cube. Figure 3 is using the
example in Figure 2.
The Merge routine is the core function of cube
pruning which generates the best hypotheses from
all cubes (Chiang, 2007). For each possible cube,
(H,R), the best hypothesis is generated by calling
GrowHypothesis(h1, r1, span,F) where h1 and
r1 are the best hypothesis and rule in H and R re-
spectively (line 22). Figure 3 (b) shows a more de-
tailed view of a cube (shaded cube in Figure 3(a)).
Rows are hypotheses and columns are rules which
are sorted based on their scores.
The first best hypotheses, h?, along with their
score, h?c and corresponding cube, (H,R) are
placed in a priority queue, heapQ (triangle in Fig-
ure 3). Iteratively the best hypothesis is popped
from the queue (line 26) and its neighbours in
the cube are added to the priority queue (using
GetNeighbours([H,Q])). It continues to generate
all K best hypotheses. Using cube pruning tech-
nique, each stack is filled with K best hypotheses
without generating all possible hypotheses in each
cube.
groups the hypotheses in a given stack based on their coverage
vector. But this idea does not work in LRHiero decoding in
which the expansion of each hypothesis is restricted to its first
uncovered span. We have also tried another way of grouping
hypotheses: group by all uncovered spans, hs. Our experiments
did not show any significant difference between the final results
(BLEU score), therefore we decided to stick to the simpler idea:
using first uncovered span for grouping.
6Note that, just rules whose number of terminals in their
source side is equal to i? p can be used.
1093
...
1 2 3 4 5
[1,8][1,8][0,3][0,3][5,8]
[1,6][1,6][1,6][0,3][0,3]
[5,6][5,6][1,4][6,8][6,8]
[5,6][5,6][5,6][1,3][7,8]
[1,3]
thei
trer
the thew
theo tier
tiek
toeitreo
rehtetseudnX126nd453a246vn4y2n4.ocwl4tseh
dnX126n453d46vn43gm231y4.ocwl4thek
bXb(gd453a246vn4y2n4.ocwl4tt o
)312 1v62 1v
trer
the thew
theo tier
tiek
toeitreo
thei
(a) (b)
Figure 3: Example of generating hypotheses in cube pruning using Figure 2: (a) Hypotheses in previous stacks are
grouped based on their first uncovered span, and build cubes (grids on top). Cubes are in different sizes because
of different number of rules and group sizes. Cubes are fed to a priority queue (triangle) and new hypotheses are
iteratively popped from the queue and added to the current stack, S
5
. (b) Generating hypotheses from a cube. The top
side of the grid denotes the target side of rules sharing the same source side (Rs) along with their scores. Left side of
the grid shows the hypotheses in a same group, their first uncovered span and their scores. Hypothesis generated from
row 1 and column 1 is added to the queue at first. Once it is popped from the queue, its neighbours (in the grid) are
subsequently added to the queue.
Figure 3 (b) shows the derivation of the two best
hypotheses from the cube. The best hypothesis of
this cube which is likely created from the best hy-
pothesis and rule (left top most entry) is popped
at first step. Then, GetNeighbours calls GrowHy-
pothesis to generate next potential best hypotheses
of this cube (neighbours of the popped entry which
are shaded in Figure 3(b)). These hypotheses are
added to the priority queue. In the next iteration, the
best hypothesis is popped from all candidates in the
queue and algorithm continues.
3 Features
We use the following standard SMT features for the
log-linear model of LR-Hiero: relative-frequency
translation probabilities p(f |e) and p(e|f), lexical
translation probabilities pl(f |e) and pl(e|f), a lan-
guage model probability, word count and phrase
count. In addition we also use the glue rule count
and the two reordering penalty features employed
by Watanabe et al (2006b; 2006a). These features
compute the height and width (span size of the en-
tire subtree) of all subtrees which are backtraced in
the derivation of a hypothesis. A non-terminal Xi
is pushed into the LIFO list of a partial hypothesis;
it?s backtrace refers to the set of NTs that must be
popped before Xi.
In Figure 1(b), X2 has two subtrees X3 and X6,
where X3 should be processed before X6. The sub-
tree rooted atX3 in Figure 1(b) has a height of 2 and
span [1, 6] having a width of 5. Similarly, X4 should
be backtraced beforeX5 and has height and width of
1. Backtracing applies only for rules having at least
two non-terminals. Thus the total height and width
penalty for this derivation are 3 and 6 respectively.
However, the height and width features do not
distinguish between a rule that reorders the non-
terminals in source and target from one that pre-
serves the ordering. Rules #2 and #3 in Figure 2
are treated equally although they have different or-
derings. The decoder is thus agnostic to this dif-
ference and would not be able to exploit this ef-
fectively to control reordering and instead would
rely on the partial LM score. This issue is exac-
erbated for glue rules, where the decoder has to
choose from different possibilities without any way
to favour one over the others. Instead of the rule
#2, the decoder could use its reordered version
?X1 haben X2, have X2 X1? leading to a poor
translation.
1094
The features we introduce can be used to learn
if the model should favour monotone translations at
the cost of re-orderings or vice versa and hence can
easily adapt to different language pairs. Further, our
experiments (see Section 4) suggest that the features
h andw are not sufficient by themselves to model re-
ordering for language pairs exhibiting very different
syntactic structure.
3.1 Distortion Features
Our distortion features are inspired by their name-
sake in phrase-based system, with some modifica-
tions to adapt the idea for the discontiguous phrases
in LR-Hiero grammar.
r : hf
1
X
1
f
2
X
2
f
3
, tX
2
X
1
i I = [`, f
1
, f
2
, f
3
, X
2
, X
1
,a]
f2 f3 X1 f1 X2 (a)
r : ? X1noch nicht X 2/not yet X2 X1?
I=[(1,1) ,(3,5) ,(5,6) ,(1,3) ,(6,6)]
.1ihre2arbeit3noch4nicht5gemacht 6 (b)
Figure 4: (a) Distortion feature computation using a rule
r. (b) Example of distortion computation for applying r
3
on phrase ?ihre arbeit noch nicht gemacht haben?. sub-
scripts between words show the indices which are used to
build I . Distortion would be: d = 2 + 0 + 5 + 3.
Consider a rule r = ??,?b ??, with the source
term ? being a mixed string of terminals and non-
terminals. Representing the non-terminal spans and
each sequence of terminals in ? as distinct items, our
distortion feature counts the total length of jumps be-
tween the items during Earley parsing.
Figure 4 (a) explains the computation of our dis-
tortion feature for an example rule r. Let I =
[I0, . . . , Ik] be the items denoting the terminal se-
quences and non-terminal spans with I0 and Ik be-
ing dummy items (` and a in Fig) marking the left
and right indices of the rule r in input sentence f .
Other items are arranged by their realization order
on the target-side with the terminal sequences pre-
ceding non-terminal spans. The items for the exam-
ple rule are shown in Figure 4 (a). The distortion
feature is computed as follows:
d(r) =
k?
j=1
|I
L
j ? I
R
j?1| (4)
where superscripts refer to position of left (L) and
right (R) edge of each item in the source sentence
f . These are then aggregated across the rules of a
derivation D as: d =
?
r?D d(r). For each item
Ij , we count the jump from the end of previous item
to the beginning of the current. In Figure 4 (a) the
jumps are indicated by the arrows above the rule.
Figure 4 (b) shows an example of distortion com-
putation for r3 and phrase ?ihre arbeit noch nicht
gemacht haben? from Figure 2.
Since the glue rules are likely to be used in the top
levels (possibly with large distortion) of the deriva-
tion, we would want the decoder to learn the distor-
tion for regular and glue rules separately. We thus
use two distortion features for the two rule types and
we call them dp and dg.
These features do not directly model the source-
target reordering, but only capture the source-side
jumps. Furthermore they apply for both monotone
and reordering rules. We now introduce a new fea-
ture for exclusively modelling the reordering.
3.2 Reordering Feature
This feature simply counts the number of reordering
rules, where the non-terminals in source and target
sides are reordered. Thus r?? = rule(D, ??), where
rule(D, ??) is the number of reordering rules in D.
Similar to width and height, this feature is applied
for rule having at least two non-terminals. This fea-
ture is applied to regular and glue rules.
4 Experiments
We conduct different types of experiments to evalu-
ate LR-Hiero decoding developed by cube pruning
and integrating new features into LR-Hiero system
for two language pairs: German-English (de-en) and
Czech-English (cs-en).Table 1 shows the dataset de-
tails.
4.1 System Setup
In our experiments we use four baselines as well
as our implementation of LR-Hiero (written in
Python):
1095
Corpus Train/Dev/Test
cs-en Europarl(v7), CzEng(v0.9);
News commentary
7.95M/3000/3003
de-en Europarl(v7); News
commentary
1.5M/2000/2000
Table 1: Corpus statistics in number of sentences
Model cs-en de-en
Phrase-based 233.0 77.2
Hiero 1,961.6 858.5
LR-Hiero 230.5 101.3
Table 2: Model sizes (millions of rules). We do not count
glue rules for LR-Hiero which are created at runtime as
needed.
? Hiero: we used Kriya, our open-source im-
plementation of Hiero in Python, which per-
forms comparably to other open-source Hiero
systems (Sankaran et al, 2012). Kriya can
obtain statistically significantly equal BLEU
scores when compared with Moses (Koehn et
al., 2007) for several language pairs (Razmara
et al, 2012; Callison-Burch et al, 2012).
? Hiero-GNF: where we use Hiero decoder with
the restricted LR-Hiero grammar (GNF rules).
? LR-Hiero: our implementation of LR-Hiero
(Watanabe et al, 2006b) in Python.
? phrase-based: Moses (Koehn et al, 2007)
? LR-Hiero+CP: LR-Hiero decoding with cube
pruning.
We use a 5-gram LM trained on the Gigaword cor-
pus and use KenLM (Heafield, 2011) for LM scor-
ing during decoding. We tune weights by minimiz-
ing BLEU loss on the dev set through MERT (Och,
2003) and report BLEU scores on the test set. We
use comparable pop limits in each of the decoders:
1000 for Moses and LR-Hiero and 500 with cube
pruning for CKY Hiero and LR-Hiero+CP. Other
extraction and decoder settings such as maximum
phrase length, etc. were identical across settings so
that the results are comparable.
Table 2 shows how the LR-Hiero grammar is
much smaller than CKY-based Hiero.
Model cs-en de-en
#queries / time(ms) #queries / time(ms)
Hiero 5,679.7 / 16.12 7,231.62 / 20.33
Hiero-GNF 4,952.5 / 14.71 5,858.74 / 18.23
LR-Hiero (1000) 46,333.21 / 163.6 83,518.63 / 328.11
LR-Hiero (500) 24,141.03 / 97.61 42,783.12 / 192.23
LR-Hiero+CP 1,303.2 / 4.2 1,697.7 / 5.67
Table 3: Comparing average number and time of lan-
guage model queries.
4.2 Time Efficiency Comparison
To evaluate the performance of LR-Hiero decod-
ing with cube pruning (LR-Hiero+CP), we compare
it with three baselines: (i) CKY Hiero, (ii) CKY
Hiero-GNF, and (iii) LR-Hiero (without cube prun-
ing) with two different beam size 500 and 1000.
When it comes to instrument timing results, there are
lots of system level details that we wish to abstract
away from, and focus only on the number of ?edges?
processed by the decoder. In comparison of parsing
algorithms, the common practice is to measure the
number of edges processed by different algorithms
for the same reason (Moore and Dowding, 1991).
By analogy to parsing algorithm comparisons, we
compare the different decoding algorithms with re-
spect to the number of calls made to the language
model (LM) since that directly corresponds to the
number of hypotheses considered by the decoder.
A decoder is more time efficient if it can consider
fewer translation hypotheses while maintaining the
same BLEU score. All of the baselines use the same
wrapper to query the language model, and we have
instrumented the wrapper to count the statistics we
need and thus we can say this is a fair comparison.
For this experiment we use a sample set of 50 sen-
tences taken from the test sets.
Table 3 shows the results in terms of average num-
ber of language model queries and times in millisec-
onds.
4.3 Reordering Features
To evaluate the new reordering features proposed
to LR-Hiero (Section 3.2), LR-Hiero+CP with new
features is compared to all baselines. Table 4 shows
the BLEU scores of different models in two lan-
guage pairs. The baseline (Watanabe et al, 2006b)
model uses all the features mentioned therein but is
1096
Model cs-en de-en
Phrase-based 20.32 24.71
CKY Hiero 20.64 25.52
CKY Hiero-GNF 20.04 24.84
LR-Hiero 18.30 23.47
LR-Hiero + reordering feats 20.20 24.90
LR-Hiero + CP + reordering feats 20.15 24.83
CKY Hiero-GNF + reordering feats 20.52 25.09
CKY Hiero + reordering feats 20.77 25.72
Table 4: BLEU scores. The rows are grouped such that
each group use the same model. The last row in part 2 of
table shows LR-Hiero+CP using our new features in ad-
dition to the baseline Watanabe features (line LR-Hiero
baseline). The last part shows CKY Hiero using new re-
ordering features. The reordering features used are dp, dg
and r??. LR-Hiero+CP has a beam size of 500 while LR-
Hiero has a beam size of 1000, c.f. with the LM calls
shown in Table 3.
worse than both phrase-based and CKY-Hiero base-
lines by up to 2.3 BLEU points.
All the reported results are obtained from a single
optimizer run. However we observed insignificant
changes in different tuning runs in our experiments.
We find a gain of about 1 BLEU point when we add
a single distortion feature d and a further gain of
0.3 BLEU (not shown due to lack of space) when
we split the distortion feature for the two rule types
(dp and dg). The last line in part two of Table 4
shows a consistent gain of 1.6 BLEU over the LR-
Hiero baseline for both language pairs. It shows that
LR-Hiero maintains the BLEU scores obtained by
?phrase-based? and ?CKY Hiero-GNF?.
We performed statistical significance tests us-
ing two different tools: Moses bootstrap resam-
pling and MultEval (Clark et al, 2011). The dif-
ference between ?LR-Hiero+CP+reordering feat?
and three baselines: ?phrase-based?, ?CKY Hiero-
GNF?, ?LR-Hiero+reordering feat? are not statis-
tically significant even for p-value of 0.1 for both
tools.
To investigate the impact of proposed reordering
features with other decoder or models. We add these
features to both Hiero and Hiero-GNF7. The last
part of Table 4 shows the performance CKY decoder
7Feature r?? is defined for SCFG rules and cannot be
adopted to phrase-based translation systems; and Moses uses
distortion feature therefore we omit Moses from this experi-
ment.
with different models (full Hiero and GNF) with the
new reordering features in terms of BLEU score.
The results show that these features are helpful in
both models. Although, they do not make a big dif-
ference in Hiero with full model, they can alleviate
the lack of non-GNF rules in Hiero-GNF.
Nguyen and Vogel (2013) integrate traditional
phrase-based features: distortion and lexicalized re-
ordering into Hiero as well. They show that such
features can be useful to boost the translation quality
of CKY Hiero with the full rule set. Nguyen and Vo-
gel (2013) compute the distortion feature in a differ-
ent way, only applicable to CKY. The distortion for
each cell is computed after the translation for non-
terminal sub-spans is complete. In LR-decoding,
we compute distortion for rules even though we are
yet to translate some of the sub-spans. Thus our ap-
proach computes the distortion incrementally for the
untranslated sub-spans which are later added. Un-
like (Nguyen and Vogel, 2013), our distortion fea-
ture can be applied to both LR and CKY-decoding
(Table 4). We have also introduced another reorder-
ing feature (Section 3.2) not proposed previously.
5 Conclusion and Future Work
We provided a detailed description of left-to-right
Hiero decoding, many details of which were only
implicit in (Watanabe et al, 2006b). We presented
an augmented LR decoding algorithm that builds on
the original algorithm in (Watanabe et al, 2006b)
but unlike that algorithm, using experiments over
multiple language pairs we showed two new results:
(i) Our LR decoding algorithm provides demonstra-
bly more efficient decoding than CKY Hiero and the
original LR decoding algorithm in (Watanabe et al,
2006b). And, (ii) by introducing new distortion and
reordering features for LR decoding we show that
it maintains the BLEU scores obtained by phrase-
based and CKY Hiero-GNF.
CKY Hiero uses standard Hiero-style translation
rules capturing better reordering model than prefix
lexicalized target-side translation rules used in LR-
Hiero. Our LR-decoding algorithm is 4 times faster
in terms of LM calls while translation quality suffers
by about 0.67 in BLEU score on average.
Unlike Watanabe et al (2006b), our new features
can easily adapt to the reordering requirements of
different language pairs. We also introduce the use
1097
of future cost in decoding algorithm which is an es-
sential part in decoding. We have shown in this pa-
per that left-to-right (LR) decoding can be consid-
ered as a potential faster alternative to CKY decod-
ing for Hiero-style machine translation systems.
In future work, we plan to apply lexicalized re-
ordering models to LR-Hiero. It has been shown to
be useful for Hiero in some languages therefore it
is promising to improve translation quality in LR-
Hiero which suffers from lack of modeling power
of non-GNF target side rules. We also plan to ex-
tend the glue rules in LR-Hiero to provide a bet-
ter reordering model. We believe such an exten-
sion would be very effective in reducing search er-
rors and capturing better reordering models in lan-
guage pairs involving complex reordering require-
ments like Chinese-English.
Acknowledgments
This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Fac-
ulty Award to the third author. The authors wish
to thank Taro Watanabe and Marzieh Razavi for
their valuable discussions and suggestions, and the
anonymous reviewers for their helpful comments.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical machine
translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In In ACL, pages
263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, HLT ?11, pages 176?181, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Commun. ACM, 13(2):94?102, February.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966?
974, Los Angeles, California, June. Association for
Computational Linguistics.
Kenneth Heafield, Hieu Hoang, Philipp Koehn, Tetsuo
Kiso, and Marcello Federico. 2011. Left language
model state for syntactic machine translation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation, pages 183?190, San Francisco,
California, USA, 12.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013.
Grouping language model boundary words to speed K-
Best extraction from hypergraphs. In Proceedings of
the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, Atlanta, Georgia, USA,
6.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In In Proc. of the Sixth Work-
shop on Statistical Machine Translation.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
In ACL 07.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273?283, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL, pages
976?985.
Robert C. Moore and John Dowding. 1991. Efficient
bottom-up parsing. In HLT. Morgan Kaufmann.
Thuylinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into chart-based
decoder for machine translation. In Proc. of ACL.
1098
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Majid Razmara, Baskaran Sankaran, Ann Clifton, and
Anoop Sarkar. 2012. Kriya - the sfu system for trans-
lation task at wmt-12. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
?12, pages 356?361, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based statis-
tical machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 216?223, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya - an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics (PBML), (97):83?98, apr.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006a. NTT statistical machine translation
for iwslt 2006. In Proceedings of IWSLT 2006, pages
95?102.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006b. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL.
Jiajun Zhang and Chenqqing Zong. 2012. A Compar-
ative Study on Discontinuous Phrase Translation. In
NLPCC 2012, pages 164?175.
1099
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 533?537,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Improved Reordering for Shallow-n Grammar based Hierarchical
Phrase-based Translation
Baskaran Sankaran and Anoop Sarkar
School of Computing Science
Simon Fraser University
Burnaby BC. Canada
{baskaran, anoop}@cs.sfu.ca
Abstract
Shallow-n grammars (de Gispert et al, 2010)
were introduced to reduce over-generation in
the Hiero translation model (Chiang, 2005) re-
sulting in much faster decoding and restricting
reordering to a desired level for specific lan-
guage pairs. However, Shallow-n grammars
require parameters which cannot be directly
optimized using minimum error-rate tuning
by the decoder. This paper introduces some
novel improvements to the translation model
for Shallow-n grammars. We introduce two
rules: a BITG-style reordering glue rule and a
simpler monotonic concatenation rule. We use
separate features for the new rules in our log-
linear model allowing the decoder to directly
optimize the feature weights. We show this
formulation of Shallow-n hierarchical phrase-
based translation is comparable in translation
quality to full Hiero-style decoding (without
shallow rules) while at the same time being
considerably faster.
1 Introduction
Hierarchical phrase-based translation (Chiang,
2005; Chiang, 2007) extends the highly lexicalized
models from phrase-based translation systems in
order to model lexicalized reordering and discon-
tiguous phrases. However, a major drawback in this
approach, when compared to phrase-based systems,
is the total number of rules that are learnt are several
orders of magnitude larger than standard phrase
tables, which leads to over-generation and search
errors and contribute to much longer decoding
times. Several approaches have been proposed to
address these issues: from filtering the extracted
synchronous grammar (Zollmann et al, 2008; He
et al, 2009; Iglesias et al, 2009) to alternative
Bayesian approaches for learning minimal gram-
mars (Blunsom et al, 2008; Blunsom et al, 2009;
Sankaran et al, 2011). The idea of Shallow-n gram-
mars (de Gispert et al, 2010) takes an orthogonal
direction for controlling the over-generation and
search space in Hiero decoder by restricting the
degree of nesting allowed for Hierarchical rules.
We propose an novel statistical model for
Shallow-n grammars which does not require addi-
tional non-terminals for monotonic re-ordering and
also eliminates hand-tuned parameters and instead
introduces an automatically tunable alternative. We
introduce a BITG-style (Saers et al, 2009) reorder-
ing glue rule (? 3) and a monotonic X-glue rule
(? 4). Our experiments show the resulting Shallow-n
decoding is comparable in translation quality to full
Hiero-style decoding while at the same time being
considerably faster.
All the experiments in this paper were done using
Kriya (Sankaran et al, 2012) hierarchical phrase-
based system which also supports decoding with
Shallow-n grammars. We extended Kriya to addi-
tionally support reordering glue rules as well.
2 Shallow-n Grammars
Formally a Shallow-n grammar G is defined as a 5-
tuple: G = (N,T,R,Rg, S), such that T is a set of
finite terminals and N a set of finite non-terminals
{X0, . . . , XN}. Rg refers to the glue rules that
rewrite the start symbol S:
S ? <X, X> (1)
S ? <SX, SX> (2)
R is the set of finite production rules in G and has
two types, viz. hierarchical (3) and terminal (4). The
hierarchical rules at each level n are additionally
conditioned to have at least one Xn?1 non-terminal
533
in them. ? represents the indices for aligning non-
terminals where co-indexed non-terminal pairs are
rewritten synchronously.
Xn ? <?, ?, ? >, ?, ? ? {{Xn?1} ? T+} (3)
X0 ? <?, ?>, ?, ? ? T+ (4)
de Gispert et al (2010) also proposed additional
non-terminals Mk to enable reordering over longer
spans by concatenating the hierarchical rules within
the span. It also uses additional parameters such
as monotonicity level (K1 and K2), maximum and
minimum rule spans allowed for the non-terminals
(?3.1 and 3.2 in de Gispert et al (2010)). The mono-
tonicity level parameters determine the number of
non-terminals that are combined in monotonic or-
der at the N ? 1 level and can be adapted to the
reordering requirements of specific language pairs.
The maximum and minimum rule spans further con-
trol the usage of hierarchical rule in a derivation by
stipulating the underlying span to be within a range
of values. Intuitively, this avoids hierarchical rules
being used for a source phrase that is either too short
or too long. While these parameters offer flexibility
for adapting the translation system to specific lan-
guage pairs, they have to be manually tuned which
is tedious and error-prone.
We propose an elegant and automatically tun-
able alternative for the Shallow-n grammars setting.
Specifically, we introduce a BITG-style reordering
glue rule (? 3) and a monotonic X-glue rule (? 4).
Our experiments show the resulting Shallow-n de-
coding to perform to the same level as full-Hiero de-
coding at the same time being faster.
In addition, our implementation of Shallow-n
grammar differs from (de Gispert et al, 2010) in
at least two other aspects. First, their formula-
tion constrains the X in the glue rules to be at the
top-level and specifically they define them to be:
S ? <SXN , SXN> and S ? <XN , XN>,
where XN is the non-terminal corresponding to the
top-most level. Interestingly, this resulted in poor
BLEU scores and we found the more generic glue
rules (as in (1) and (2)) to perform significantly bet-
ter, as we show later.
Secondly, they also employ pattern-based filter-
ing (Iglesias et al, 2009) in order to reducing redun-
dancies in the Hiero grammar by filtering it based on
certain rule patterns. However in our limited experi-
ments, we observed the filtered grammar to perform
worse than the full grammar, as also noted by (Zoll-
mann et al, 2008). Hence, we do not employ any
grammar filtering in our experiments.
3 Reordering Glue Rule
In this paper, we propose an additional BITG-style
glue rule (called R-glue) as in (5) for reordering the
phrases along the left-branch of the derivation.
S ? <SX, XS> (5)
In order to use this rule sparsely in the derivation,
we use a separate feature for this rule and apply a
penalty of 1. Similar to the case of regular glue
rules, we experimented with a variant of the reorder-
ing glue rule, where X is restricted to the top-level:
S ? <SXN , XNS> and S ? <XN , XN>.
3.1 Language Model Integration
The traditional phrase-based decoders using beam
search generate the target hypotheses in the left-to-
right order. In contrast, Hiero-style systems typ-
ically use CKY chart-parsing decoders which can
freely combine target hypotheses generated in inter-
mediate cells with hierarchical rules in the higher
cells. Thus the generation of the target hypotheses
are fragmented and out of order compared to the left
to right order preferred by n-gram language models.
This leads to challenges in the estimation of lan-
guage model scores for partial target hypothesis,
which is being addressed in different ways in the
existing Hiero-style systems. Some systems add a
sentence initial marker (<s>) to the beginning of
each path and some other systems have this implic-
itly in the derivation through the translation mod-
els. Thus the language model scores for the hypoth-
esis in the intermediate cell are approximated, with
the true language model score (taking into account
sentence boundaries) being computed in the last cell
that spans the entire source sentence.
We introduce a novel improvement in computing
the language model scores: for each of the target
hypothesis fragment, our approach finds the best po-
sition for the fragment in the final sentence and uses
the corresponding score. We compute three different
scores corresponding to the three positions where
the fragment can end up in the final sentence, viz.
534
sentence initial, middle and final: and choose the
best score. As an example for fragment tf consist-
ing of a sequence of target tokens, we compute LM
scores for i) <s> tf , ii) tf and iii) tf </s> and use
the best score for pruning alone1.
This improvement significantly reduces the
search errors while performing cube pruning (Chi-
ang, 2007) at the cost of additional language model
queries. While this approach works well for the
usual glue rules, it is particularly effective in the case
of reordering glue rules. For example, a partial can-
didate covering a non-final source span might trans-
late to the final position in the target sentence. If we
just compute the LM score for the target fragment
as is done normally, this might get pruned early on
before being reordered by the new glue rule. Our ap-
proach instead computes the three LM scores and it
would correctly use the last LM score which is likely
to be the best, for pruning.
4 Monotonic Concatenation Glue rule
The reordering glue rule facilitates reordering at the
top-level. However, this is still not sufficient to allow
long-distance reordering as the shallow-decoding re-
stricts the depth of the derivation. Consider the Chi-
nese example in Table 1, in which translation of the
Chinese word corresponding to the English phrase
the delegates involves a long distance reordering to
the beginning of the sentence. Note that, three of the
four human references prefer this long distance re-
ordering, while the fourth one avoids the movement
by using a complex construction with relative clause
and a sentence initial prepositional phrase.
Such long distance reordering is very difficult in
conventional Hiero decoding and more so with the
Shallow-n grammars. While the R-glue rule per-
mit such long distance movements, it also requires
a long phrase generated by a series of rules to be
moved as a block. We address this issue, by adding
a monotonic concatenation (called X-glue) rule that
concatenates a series of hierarchical rules. In order
to control overgeneration, we apply this rule only at
the N ? 1 level similar to de Gispert et al (2010).
XN?1 ? <XN?1XN?1, XN?1XN?1> (6)
1This ensures the the LM score estimates are never underes-
timated for pruning. We retain the LM score for fragment (case
ii) for estimating the score for the full candidate sentence later.
However unlike their approach, we use this rule as
a feature in the log-linear model so that its weight
can be optimized in the tuning step. Also, our ap-
proach removes the need for additional parameters
K1 and K2 for controlling monotonicity, which was
being tuned manually in their work. For the Chinese
example above, shallow-1 decoding using R and X-
glue rules achieve the complex movement resulting
in a significantly better translation than full-Hiero
decoding as shown in the last two lines in Table 1.
5 Experiments
We present results for Chinese-English translation
as it often requires heavy reordering. We use the
HK parallel text and GALE phase-1 corpus consist-
ing of?2.3M sentence pairs for training. For tuning
and testing, we use the MTC parts 1 and 3 (1928
sentences) and MTC part 4 (919 sentences) respec-
tively. We used the usual pre-processing pipeline
and an additional segmentation step for the Chinese
side of the bitext using the LDC segmenter2.
Our log-linear model uses the standard features
conditional (p(e|f) and p(f |e)) and lexical (pl(e|f)
and pl(f |e)) probabilities, phrase (pp) and word
(wp) penalties, language model and regular glue
penalty (mg) apart from two additional features for
R?glue (rg) and X?glue (xg).
Table 2 shows the BLEU scores and decoding
time for the MTC test-set. We provide the IBM
BLEU (Papineni et al, 2002) scores for the Shallow-
n grammars for order: n = 1, 2, 3 and compare it to
the full-Hiero baseline. Finally, we experiment with
two variants of the S glue rules, i) a restricted ver-
sion where the glue rules combine only X at level
N , (column ?Glue: XN ? in table), ii) more free vari-
ant where they are allowed to use any X freely (col-
umn ?Glue: X? in table).
As it can be seen, the unrestricted glue rules vari-
ant (column ?Glue: X?) consistently outperforms
the glue rules restricted to the top-level non-terminal
XN , achieving a maximum BLEU score of 26.24,
which is about 1.4 BLEU points higher than the lat-
ter and is also marginally higher than full Hiero. The
decoding speeds for free-Glue and restricted-Glue
variants were mostly identical and so we only pro-
vide the decoding time for the latter. Shallow-2 and
2We slightly modified the LDC segmenter, in order to cor-
rectly handle non-Chinese characters in ASCII and UTF8.
535
Source ???????????????????????????????????
Gloss in argentine capital beunos aires participate united nations global climate conference delegates continue
to work.
Ref 0 delegates attending the un conference on world climate continue their work in the argentine capital of
buenos aires.
Ref 1 the delegates to the un global climate conference held in Buenos aires, capital city of argentina, go on with
their work.
Ref 2 the delegates continue their works at the united nations global climate talks in buenos aires, capital of
argentina
Ref 3 in buenos aires, the capital of argentina, the representatives attending un global climate meeting continued
their work.
Full-Hiero:
Baseline
in the argentine capital of buenos aires to attend the un conference on global climate of representatives
continue to work.
Sh-1 Hiero: R-
glue & X-glue
the representatives were in the argentine capital of beunos aires to attend the un conference on global climate
continues to work.
Table 1: An example for the level of reordering in Chinese-English translation
Grammar Glue: XN Glue: X Time
Full Hiero 25.96 0.71
Shallow-1 23.54 24.04 0.24
+ R-Glue 23.41 24.15 0.25
+ X-Glue 23.75 24.74 0.72
Shallow-2 24.54 25.12 0.55
+ R-Glue 24.75 25.60 0.57
+ X-Glue 24.33 25.43 0.69
Shallow-3 24.88 25.89 0.62
+ R-Glue 24.77 26.24 0.63
+ X-Glue 24.75 25.83 0.69
Table 2: Results for Chinese-English. The decoding time
is in secs/word on the Test set for column ?Glue: X?.
Bold font indicate best BLEU for each shallow-order.
shallow-3 free glue variants achieve BLEU scores
comparable to full-Hiero and at the same time being
12? 20% faster.
R-glue (rg) appears to contribute more than
the X-glue (xg) as can be seen in shallow-2 and
shallow-3 cases. Interestingly, xg is more helpful for
the shallow-1 case specifically when the glue rules
are restricted. As the glue rules are restricted, the
X-glue rules concatenates other lower-order rules
before being folded into the glue rules. Both rg and
xg improve the BLEU scores by 0.58 over the plain
shallow case for shallow orders 1 and 2 and performs
comparably for shallow-3 case. We have also con-
ducted experiments for Arabic-English (Table 3) and
we notice that X-glue is more effective and that R-
glue is helpful for higher shallow orders.
Grammar Glue: X Time
Full Hiero 37.54 0.67
Shallow-1 36.90 0.40
+ R-Glue 36.98 0.43
+ X-Glue 37.21 0.57
Shallow-2 36.97 0.57
+ R-Glue 36.80 0.58
+ X-Glue 37.36 0.61
Shallow-3 36.88 0.61
+ R-Glue 37.18 0.63
+ X-Glue 37.31 0.64
Table 3: Results for Arabic-English. The decoding time
is in secs/word on the Test set.
5.1 Effect of our novel LM integration
Here we analyze the effect of our novel LM integra-
tion approach in terms of BLEU score and search er-
rors comparing it to the naive method used in typical
Hiero systems. In shallow setting, our method im-
proved the BLEU scores by 0.4 for both Ar-En and
Cn-En. In order to quantify the change in the search
errors, we compare the model scores of the (corre-
sponding) candidates in the N-best lists obtained by
the two methods and compute the % of high scor-
ing candidates in each. Our approach was clearly
superior with 94.6% and 77.3% of candidates hav-
ing better scores respectively for Cn-En and Ar-En.
In full decoding setting the margin of improvements
were reduced slightly- BLEU improved by 0.3 and
about 57?69% of target candidates had better model
scores for the two language pairs.
536
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of Neural Information Processing Systems.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of Association of
Computational Linguistics, pages 782?790.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
Association of Computational Linguistics, pages 263?
270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Adria` de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hier-
archical phrase-based translation with weighted finite-
state transducers and shallow-n grammars. Computa-
tional Linguistics, 36.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding
monotone composed rule for hierarchical phrase-based
statistical machine translation. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 25?29.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the 12th Conference of the European Chapter of the
ACL, pages 380?388.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting of Association of Computational Lin-
guistics, pages 311?318.
Markus Saers, Joakim Nivre, and Dekai Wu. 2009.
Learning stochastic bracketing inversion transduction
grammars with a cubic time biparsing algorithm. In
Proceedings of the 11th International Conference on
Parsing Technologies, pages 29?32. Association for
Computational Linguistics.
Baskaran Sankaran, Gholamreza Haffari, and Anoop
Sarkar. 2011. Bayesian extraction of minimal scfg
rules for hierarchical phrase-based translation. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, pages 533?541.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya ? an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, (97):83?98, April.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics, pages 1145?1152.
537
Proceedings of NAACL-HLT 2013, pages 947?957,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Multi-Metric Optimization Using Ensemble Tuning
Baskaran Sankaran, Anoop Sarkar
Simon Fraser University
Burnaby BC. CANADA
{baskaran,anoop}@cs.sfu.ca
Kevin Duh
Nara Institute of Science & Technology
Ikoma, Nara. JAPAN
kevinduh@is.naist.jp
Abstract
This paper examines tuning for statistical ma-
chine translation (SMT) with respect to mul-
tiple evaluation metrics. We propose several
novel methods for tuning towards multiple ob-
jectives, including some based on ensemble
decoding methods. Pareto-optimality is a nat-
ural way to think about multi-metric optimiza-
tion (MMO) and our methods can effectively
combine several Pareto-optimal solutions, ob-
viating the need to choose one. Our best
performing ensemble tuning method is a new
algorithm for multi-metric optimization that
searches for Pareto-optimal ensemble models.
We study the effectiveness of our methods
through experiments on multiple as well as
single reference(s) datasets. Our experiments
show simultaneous gains across several met-
rics (BLEU, RIBES), without any significant
reduction in other metrics. This contrasts the
traditional tuning where gains are usually lim-
ited to a single metric. Our human evaluation
results confirm that in order to produce better
MT output, optimizing multiple metrics is bet-
ter than optimizing only one.
1 Introduction
Tuning algorithms are used to find the weights for a
statistical machine translation (MT) model by min-
imizing error with respect to a single MT evalua-
tion metric. The tuning process improves the per-
formance of an SMT system as measured by this
metric; with BLEU (Papineni et al, 2002) being
the most popular choice. Minimum error-rate train-
ing (MERT) (Och, 2003) was the first approach in
MT to directly optimize an evaluation metric. Sev-
eral alternatives now exist: MIRA (Watanabe et al,
2007; Chiang et al, 2008), PRO (Hopkins and May,
2011), linear regression (Bazrafshan et al, 2012)
and ORO (Watanabe, 2012) among others.
However these approaches optimize towards the
best score as reported by a single evaluation met-
ric. MT system developers typically use BLEU and
ignore all the other metrics. This is done despite
the fact that other metrics model wide-ranging as-
pects of translation: from measuring the translation
edit rate (TER) in matching a translation output to a
human reference (Snover et al, 2006), to capturing
lexical choices in translation as in METEOR (Lavie
and Denkowski, 2009) to modelling semantic simi-
larity through textual entailment (Pado? et al, 2009)
to RIBES, an evaluation metric that pays attention
to long-distance reordering (Isozaki et al, 2010).
While some of these metrics such as TER, ME-
TEOR are gaining prominence, BLEU enjoys the
status of being the de facto standard tuning metric
as it is often claimed and sometimes observed that
optimizing with BLEU produces better translations
than other metrics (Callison-Burch et al, 2011).
The gains obtained by the MT system tuned on
a particular metric do not improve performance as
measured under other metrics (Cer et al, 2010), sug-
gesting that over-fitting to a specific metric might
happen without improvements in translation quality.
In this paper we propose a new tuning framework
for jointly optimizing multiple evaluation metrics.
Pareto-optimality is a natural way to think about
multi-metric optimization and multi-metric opti-
mization (MMO) was recently explored using the
notion of Pareto optimality in the Pareto-based
Multi-objective Optimization (PMO) approach (Duh
et al, 2012). PMO provides several equivalent solu-
tions (parameter weights) having different trade-offs
between the different MT metrics. In (Duh et al,
2012) the choice of which option to use rests with
the MT system developer and in that sense their ap-
proach is an a posteriori method to specify the pref-
erence (Marler and Arora, 2004).
In contrast to this, our tuning framework pro-
vides a principled way of using the Pareto optimal
options using ensemble decoding (Razmara et al,
2012). We also introduce a novel method of ensem-
ble tuning for jointly tuning multiple MT evaluation
metrics and further combine this with the PMO ap-
947
proach (Duh et al, 2012). We also introduce three
other approaches for multi-metric tuning and com-
pare their performance to the ensemble tuning. Our
experiments yield the highest metric scores across
many different metrics (that are being optimized),
something that has not been possible until now.
Our ensemble tuning method over multiple met-
rics produced superior translations than single met-
ric tuning as measured by a post-editing task.
HTER (Snover et al, 2006) scores in our human
evaluation confirm that multi-metric optimization
can lead to better MT output.
2 Related Work
In grammar induction and parsing (Spitkovsky et al,
2011; Hall et al, 2011; Auli and Lopez, 2011) have
proposed multi-objective methods based on round-
robin iteration of single objective optimizations.
Research in SMT parameter tuning has seen a
surge of interest recently, including online/batch
learning (Watanabe, 2012; Cherry and Foster, 2012),
large-scale training (Simianer et al, 2012; He
and Deng, 2012), and new discriminative objec-
tives (Gimpel and Smith, 2012; Zheng et al, 2012;
Bazrafshan et al, 2012). However, few works
have investigated the multi-metric tuning problem in
depth. Linear combination of BLEU and TER is re-
ported in (Zaidan, 2009; Dyer et al, 2009; Servan
and Schwenk, 2011); an alternative is to optimize on
BLEU with MERT while enforcing that TER does
not degrade per iteration (He and Way, 2009). Stud-
ies on metric tunability (Liu et al, 2011; Callison-
Burch et al, 2011; Chen et al, 2012) have found
that the metric used for evaluation may not be the
best metric used for tuning. For instance, (Mauser et
al., 2008; Cer et al, 2010) report that tuning on lin-
ear combinations of BLEU-TER is more robust than
a single metric like WER.
The approach in (Devlin and Matsoukas, 2012)
modifies the optimization function to include traits
such as output length so that the hypotheses pro-
duced by the decoder have maximal score according
to one metric (BLEU) but are subject to an output
length constraint, e.g. that the output is 5% shorter.
This is done by rescoring an N-best list (forest) for
the metric combined with each trait condition and
then the different trait hypothesis are combined us-
ing a system combination step. The traits are in-
dependent of the reference (while tuning). In con-
trast, our method is able to combine multiple metrics
(each of which compares to the reference) during the
tuning step and we do not depend on N-best list (or
forest) rescoring or system combination.
Duh et. al. (2012) proposed a Pareto-based ap-
proach to SMT multi-metric tuning, where the lin-
ear combination weights do not need to be known in
advance. This is advantageous because the optimal
weighting may not be known in advance. However,
the notion of Pareto optimality implies that multiple
?best? solutions may exist, so the MT system devel-
oper may be forced to make a choice after tuning.
These approaches require the MT system devel-
oper to make a choice either before tuning (e.g. in
terms of linear combination weights) or afterwards
(e.g. the Pareto approach). Our method here is dif-
ferent in that we do not require any choice. We
use ensemble decoding (Razmara et al, 2012) (see
sec 3) to combine the different solutions resulting
from the multi-metric optimization, providing an el-
egant solution for deployment. We extend this idea
further and introduce ensemble tuning, where the
metrics have separate set of weights. The tuning
process alternates between ensemble decoding and
the update step where the weights for each metric
are optimized separately followed by joint update of
metric (meta) weights.
3 Ensemble Decoding
We now briefly review ensemble decoding (Razmara
et al, 2012) which is used as a component in the al-
gorithms we present. The prevalent model of statis-
tical MT is a log-linear framework using a vector of
feature functions ?:
p(e|f) ? exp
(
w ? ?
)
(1)
The idea of ensemble decoding is to combine sev-
eral models dynamically at decode time. Given mul-
tiple models, the scores are combined for each par-
tial hypothesis across the different models during
decoding using a user-defined mixture operation ?.
p(e|f) ? exp
(
w1 ? ?1 ? w2 ? ?2 ? . . .
)
(2)
(Razmara et al, 2012) propose several mixture
operations, such as log-wsum (simple linear mix-
ture), wsum (log-linear mixture) and max (choose lo-
948
cally best model) among others. The different mix-
ture operations allows the user to encode the be-
liefs about the relative strengths of the models. It
has been applied successfully for domain adaptation
setting and shown to perform better approaches that
pre-compute linear mixtures of different models.
4 Multi-Metric Optimization
In statistical MT, the multi-metric optimization
problem can be expressed as:
w? = argmax
w
g
(
[M1(H), . . . ,Mk(H)]
)
(3)
where H = N (f ;w)
where N (f ;w) is the decoding function generating
a set of candidate hypotheses H based on the model
parameters w, for the source sentences f . For each
source sentence fi ? f there is a set of candidate
hypotheses {hi} ? H . The goal of the optimiza-
tion is to find the weights that maximize the func-
tion g(.) parameterized by different evaluation met-
rics M1, . . . ,Mk.
For the Pareto-optimal based approach such as
PMO (Duh et al, 2012), we can replace g(?) above
with gPMO(?) which returns the points in the Pareto
frontier. Alternately a weighted averaging function
gwavg(?) would result in a linear combination of the
metrics being considered, where the tuning method
would maximize the joint metric. This is similar to
the (TER-BLEU)/2 optimization (Cer et al, 2010;
Servan and Schwenk, 2011).
We introduce four methods based on the above
formulation and each method uses a different type
of g(?) function for combining different metrics and
we compare experimentally with existing methods.
4.1 PMO Ensemble
PMO (Duh et al, 2012) seeks to maximize the num-
ber of points in the Pareto frontier of the metrics con-
sidered. The inner routine of the PMO-PRO tuning
is described in Algorithm 1. This routine is con-
tained within an outer loop that iterates for a fixed
number iterations of decoding the tuning set and op-
timizing the weights.
The tuning process with PMO-PRO is inde-
pendently repeated with different set of weights
for metrics1 yielding a set of equivalent solutions
1For example Duh et al (2012) use five different weight
Algorithm 1 PMO-PRO (Inner routine for tuning)
1: Input: Hypotheses H = N (f ;w); Weights w
2: Initialize T = {}
3: for each f in tuning set f do
4: {h} = H(f)
5: {M({h})} = ComputeMetricScore({h}, e?)
6: {F} = FindParetoFrontier({M({h})})
7: for each h in {h} do
8: if h ? F then add (1, h) to T
9: else add (`, h) to T (see footnote 1)
10: wp ? PRO(T ) (optimize using PRO)
11: Output: Pareto-optimal weights wp
{ps1 , . . . , psn} which are points on the Pareto fron-
tier. The user then chooses one solution by making a
trade-off between the performance gains across dif-
ferent metrics. However, as noted earlier this a pos-
teriori choice ignores other solutions that are indis-
tinguishable from the chosen one.
We alleviate this by complementing PMO with
ensemble decoding, which we call PMO ensemble,
in which each point in the Pareto solution is a dis-
tinct component in the ensemble decoder. This idea
can also be used in other MMO approaches such as
linear combination of metrics (gwavg(.)) mentioned
above. In this view, PMO ensemble is a special case
of ensemble combination, where the decoding is per-
formed by an ensemble of optimal solutions.
The ensemble combination model introduces new
hyperparameters ? that are the weights of the en-
semble components (meta weights). These ensem-
ble weights could set to be uniform in a na??ve
implementation. Or the user can encode her be-
liefs or expectations about the individual solutions
{ps1 , . . . , psn} to set the ensemble weights (based
on the relative importance of the components). Fi-
nally, one could also include a meta-level tuning step
to set the weights ?.
The PMO ensemble approach is graphically il-
lustrated in Figure 1; we will also refer to this fig-
ure while discussing other methods.2 The orig-
settings for metrics (M1,M2), viz. (0.0, 1.0), (0.3, 0.7),
(0.5, 0.5), (0.7, 0.3) and (1.0, 0.0). They combine the met-
ric weights qi with the sentence-level metric scores Mi as
` =
(?
k qkMk
)
/k where ` is the target value for negative
examples (the else line in Alg 1) in the optimization step.
2The illustration is based on two metrics, metric-1 and
metric-2, but could be applied to any number of metrics. With-
out loss of generality we assume accuracy metrics, i.e. higher
949
Me
tric
?2
Metric?1
Figure 1: Illustration of different MMO approaches involving
two metrics. Solid (red) arrows indicate optimizing two met-
rics independently and the dashed (green) arrow optimize them
jointly. The Pareto frontier is indicated by the curve.
inal PMO-PRO seeks to maximize the points on
the Pareto frontier (blue curve in the figure) lead-
ing to Pareto-optimal solutions. On the other hand,
the PMO ensemble combines the different Pareto-
optimal solutions and potentially moving in the di-
rection of dashed (green) arrows to some point that
has higher score in either or both dimensions.
4.2 Lateen MMO
Lateen EM has been proposed as a way of jointly
optimizing multiple objectives in the context of de-
pendency parsing (Spitkovsky et al, 2011). It uses
a secondary hard EM objective to move away, when
the primary soft EM objective gets stuck in a local
optima. The course correction could be performed
under different conditions leading to variations that
are based on when and how often to shift from one
objective function to another during optimization.
The lateen technique can be applied to the multi-
metric optimization in SMT by treating the differ-
ent metrics as different objective functions. While
the several lateen variants are also applicable for our
task, our objective here is to improve performance
across the different metrics (being optimized). Thus,
we restrict ourselves to the style where the search
alternates between the metrics (in round-robin fash-
ion) at each iteration. Since the notion of conver-
gence is unclear in lateen setting, we stop after a
fixed number of iterations optimizing the tuning set.
In terms of Figure 1, lateen MMO corresponds to al-
ternately maximizing the metrics along two dimen-
sions as depicted by the solid arrows.
By the very nature of lateen-alternation, the
metric score is better.
weights obtained at each iteration are likely to be
best for the metric that was optimized in that itera-
tion. Thus, one could use weights from the last k
iterations (for lateen-tuning with as many metrics)
and then decode the test set with an ensemble of
these weights as in PMO ensemble. However in
practice we find the weights to converge and we sim-
ply use the weights from the final iteration to decode
the test set in our lateen experiments.
4.3 Union of Metrics
At each iteration lateen MMO excludes all but one
metric for optimization. An alternative would be to
consider all the metrics at each iteration so that the
optimizer could try to optimize them jointly. This
has been the general motivation for considering the
linear combination of metrics (Cer et al, 2010; Ser-
van and Schwenk, 2011) resulting in a joint metric,
which is then optimized.
However due to the scaling differences between
the scores of different metrics, the linear combi-
nation might completely suppress the metric hav-
ing scores in the lower-range. As an example, the
RIBES scores that are typically in the high 0.7-0.8
range, dominate the BLEU scores that is typically
around 0.3. While the weighted linear combination
tries to address this imbalance, they introduce ad-
ditional parameters that are manually fixed and not
separately tuned.
We avoid this linear combination pitfall by taking
the union of the metrics under which we consider
the union of training examples from all metrics and
optimize them jointly. Mathematically,
w? = argmax
w
g(M1(H)) ? . . . ? g(Mk(H)) (4)
Most of the optimization approaches involve two
phases: i) select positive and negative examples and
ii) optimize parameters to favour positive examples
while penalizing negative ones. In the union ap-
proach, we independently generate positive and neg-
ative sets of examples for all the metrics and take
their union. The optimizer now seeks to move to-
wards positive examples from all metrics, while pe-
nalizing others.
This is similar to the PMO-PRO approach except
that here the optimizer tries to simultaneously max-
imize the number of high scoring points across all
950
metrics. Thus, instead of the entire Pareto frontier
curve in Figure 1, the union approach optimizes the
two dimensions simultaneously in each iteration.
5 Ensemble Tuning
These methods, even though novel, under utilize the
power of ensembles as they combine the solution
only at the end of the tuning process. We would
prefer to tightly integrate the idea of ensembles into
the tuning. We thus extend the ensemble decoding
to ensemble tuning. The feature weights are repli-
cated separately for each evaluation metric, which
are treated as components in the ensemble decoding
and tuned independently in the optimization step.
Initially the ensemble decoder decodes a devset us-
ing a weighted ensemble to produce a single N-best
list. For the optimization, we employ a two-step ap-
proach of optimizing the feature weights (of each
ensemble component) followed by a step for tun-
ing the meta (component) weights. The optimized
weights are then used for decoding the devset in the
next iteration and the process is repeated for a fixed
number of iterations.
Modifying the MMO representation in Equa-
tion 3, we formulate ensemble tuning as:
Hens = Nens
(
f ; {wM};?;?
)
(5)
w? =
{
argmax
wMi
Hens | 1?i?k
}
(6)
? = argmax
?
g ({Mi(Hens)|1?i?k} ;w?) (7)
Here the ensemble decoder function Nens(.)
is parameterized by an ensemble of weights
wM1 , . . . , wMk (denoted as {wM} in Eq 5) for each
metric and a mixture operation (?). ? represents the
weights of the ensemble components.
Pseudo-code for ensemble tuning is shown in Al-
gorithm 2. In the beginning of each iteration (line 2),
the tuning process ensemble decodes (line 4) the
tuning set using the weights obtained from the pre-
vious iteration. Equation 5 gives the detailed expres-
sion for the ensemble decoding, whereHens denotes
the N-best list generated by the ensemble decoder.
The method now uses a dual tuning strategy in-
volving two phases to optimize the weights. In the
first step it optimizes each of the k metrics indepen-
dently (lines 6-7) along its respective dimension in
Algorithm 2 Ensemble Tuning Algorithm
1: Input: Tuning set f ,
Metrics M1, . . . ,Mk (ensemble components)
Initial weights {wM} ? wM1 , . . . wMk and
Component (meta) weights ?
2: for j = 1, . . . do
3: {w(j)M } ? {wM}
4: Ensemble decode the tuning set
Hens = Nens(f ; {w
(j)
M };?;?)
5: {wM} = {}
6: for each metric Mi ? {M} do
7: w?Mi ? PRO(Hens, wMi) (use PRO)
8: Add w?Mi to {wM}
9: ?? PMO-PRO(Hens, {wM}) (Alg 1)
10: Output: Optimal weights {wM} and ?
the multi-metric space (as shown by the solid arrows
along the two axes in Figure 1). This yields a new
set of weights w? for the features in each metric.
The second tuning step (line 9) then optimizes
the meta weights (?) so as to maximize the multi-
metric objective along the joint k-dimensional space
as shown in Equation 7. This is illustrated by the
dashed arrows in the Figure 1. While g(.) could be
any function that combines multiple metrics, we use
the PMO-PRO algorithm (Alg. 1) for this step.
The main difference between ensemble tuning and
PMO ensemble is that the former is an ensemble
model over metrics and the latter is an ensemble
model over Pareto solutions. Additionally, PMO en-
semble uses the notion of ensembles only for the fi-
nal decoding after tuning has completed.
5.1 Implementation Notes
All the proposed methods fit naturally within the
usual SMT tuning framework. However, some
changes are required in the decoder to support en-
semble decoding and in the tuning scripts for op-
timizing with multiple metrics. For ensemble de-
coding, the decoder should be able to use multiple
weight vectors and dynamically combine them ac-
cording to some desired mixture operation. Note
that, unlike Razmara et al (2012), our approach uses
just one model but has different weight vectors for
each metric and the required decoder modifications
are simpler than full ensemble decoding.
While any of the mixture operations proposed
by Razmara et al (2012) could be used, in this pa-
951
per we use log-wsum ? the linear combination of the
ensemble components and log-wmax ? the combina-
tion that prefers the locally best component. These
are simpler to implement and also performed com-
petitively in their domain adaptation experiments.
Unless explicitly noted otherwise, the results pre-
sented in Section 6 are based on linear mixture oper-
ation log-wsum, which empirically performed better
than the log-wmax for ensemble tuning.
6 Experiments
We evaluate the different methods on Arabic-
English translation in single as well as multiple ref-
erences scenario. Corpus statistics are shown in
Table 1. For all the experiments in this paper,
we use Kriya, our in-house Hierarchical phrase-
based (Chiang, 2007) (Hiero) system, and inte-
grated the required changes for ensemble decoding.
Kriya performs comparably to the state of the art in
phrase-based and hierarchical phrase-based transla-
tion over a wide variety of language pairs and data
sets (Sankaran et al, 2012).
We use PRO (Hopkins and May, 2011) for op-
timizing the feature weights and PMO-PRO (Duh
et al, 2012) for optimizing meta weights, wher-
ever applicable. In both cases, we use SVM-
Rank (Joachims, 2006) as the optimizer.
We used the default parameter settings for dif-
ferent MT tuning metrics. For METEOR, we tried
both METEOR-tune and METEOR-hter settings
and found the latter to perform better in BLEU and
TER scores, even though the former was marginally
better in METEOR3 and RIBES scores. We ob-
served the margin of loss in BLEU and TER to out-
weigh the gains in METEOR and RIBES and we
chose METEOR-hter setting for both optimization
and evaluation of all our experiments.
6.1 Evaluation on Tuning Set
Unlike conventional tuning methods, PMO (Duh et
al., 2012) was originally evaluated on the tuning set
to avoid potential mismatch with the test set. In
order to ensure robustness of evaluation, they re-
decode the devset using the optimal weights from
the last tuning iteration and report the scores on 1-
3This behaviour was also noted by Denkowski and Lavie
(2011) in their analysis of Urdu-English system for tunable met-
rics task in WMT11.
best candidates.
Corpus Training size Tuning/ test set
ISI corpus 1.1 M
1664/ 1313 (MTA)
1982/ 987 (ISI)
Table 1: Corpus Statistics (# of sentences) for Arabic-English.
MTA (4-refs) and ISI (1-ref).
We follow the same strategy and compare our
PMO-ensemble approach with PMO-PRO (denoted
P) and a linear combination4 (denoted L) base-
line. Similar to Duh et al (2012), we use
five different BLEU:RIBES weight settings, viz.
(0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and
(1.0, 0.0), marked L1 through L5 or P1 through P5.
The Pareto frontier is then computed from 80 points
(5 runs and 15 iterations per run) on the devset.
Figure 2(a) shows the Pareto frontier of L and P
baselines using BLEU and RIBES as two metrics.
The frontier of the P dominates that of L for most
part showing that the PMO approach benefits from
picking Pareto points during the optimization.
We use the PMO-ensemble approach to combine
the optimized weights from the 5 tuning runs and
re-decode the devset employing ensemble decoding.
This yields the points LEns and PEns in the plot,
which obtain better scores than most of the indi-
vidual runs of L and P. This ensemble approach of
combining the final weights also generalizes to the
unseen test set as we show later.
Figure 2(b) plots the change in BLEU during tun-
ing in the multiple references and the single refer-
ence scenarios. We show for each baseline method L
and P, plots for two different weight settings that ob-
tain high BLEU and RIBES scores. In both datasets,
our ensemble tuning approach dominates the curves
of the (L and P) baselines. In summary, these results
confirm that the ensemble approach achieves results
that are competitive with previous MMO methods
on the devset Pareto curve. We now provide a more
comprehensive evaluation on the test set.
6.2 Evaluation on Test Set
This section contains multi-metric optimization re-
sults on the unseen test sets, one test set has multi-
ple references and the other has a single-reference.
4Linear combination is a generalized version of the com-
bined (TER-BLEU)/2 metric and its variants.
952
 0.765
 0.766
 0.767
 0.768
 0.769
 0.77
 0.771
 0.772
 0.773
 0.33  0.335  0.34  0.345  0.35
R
I
B
E
S
BLEU
 L1
 L2
L3  L4  
 L5
P1   P2 P3
 P4 P5
LEns  
 PEns
PMO-PROLin-Comb
(a) Pareto frontier and BLEU-RIBES scores: MTA 4-refs devset
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
 2  4  6  8  10  12  14
B
L
E
U
Iterations
L1-mtaL5-mtaP1-mtaP5-mtaEns-Tune-mtaL1-isiL3-isiP1-isiP3-isiEns-Tune-isi
(b) Tuning BLEU scores: MTA 4-refs and ISI 1-ref devsets
Figure 2: Devset (redecode): Comparison of Lin-comb (L) and PMO-PRO (P) with Ensemble decoding (Lens and PEns) and
Ensemble tuning (Ens-Tune)
We plot BLEU scores against other metrics (RIBES,
METEOR and TER) and this allows us to compare
the performance of each metric relative to the de-
facto standard BLEU metric.
Baseline points are identified by single letters B
for BLEU, T for TER, etc. and the baseline (single-
metric optimized) score for each metric is indicated
by a dashed line on the corresponding axis. MMO
points use a series of single letters referring to the
metrics used, e.g. BT for BLEU-TER. The union of
metrics method is identified with the suffix ?J? and
lateen method with suffix ?L? (thus BT-L refers to the
lateen tuning with BLEU-TER). MMO points with-
out any suffix use the ensemble tuning approach.
Figures 3 and 4(a) plot the scores for the MTA test
set with 4-references. We see noticeable and some
statistically significant improvements in BLEU and
RIBES (see Table 2 for BLEU improvements).
All our MMO approaches, except for the union
method, show gains on both BLEU and RIBES axes.
Figures 3(b) and 4(a) show that none of the proposed
methods managed to improve the baseline scores for
METEOR and TER. However, several of our en-
semble tuning combinations work well for both ME-
TEOR (BR, BMRTB3, etc.) and TER (BMRT and
BRT) in that they improved or were close to the
baseline scores in either dimension. We again see in
these figures that the MMO approaches can improve
the BLEU-only tuning by 0.3 BLEU points, without
much drop in other metrics. This is in tune with the
finding that BLEU could be tuned easily (Callison-
Burch et al, 2011) and also explains why it remains
Approach and Tuning Metric(s)
BLEU
MTA ISI
Single Objective Baselines
BLEU 36.06 37.20
METEOR 35.05 36.91
RIBES 33.35 36.60
TER 33.92 35.85
Ensemble Tuning: 2 Metrics
B-M 36.02 37.26
B-R 36.15 37.37
B-T 35.72 36.31
Ensemble Tuning: 3 Metrics
B-M-R 36.36 37.37
B-M-T 36.22 36.89
B-R-T 35.97 36.72
Ensemble Tuning: > 3 Metrics
B-M-R-T 35.94 36.84
B-M-R-T-B3 36.16 37.12
B-M-R-T-B3-B2-B1 36.08 37.24
Table 2: BLEU Scores on MTA (4 refs) and ISI (1 ref) test sets
using the standard mteval script. Boldface scores indicate scores
that are comparable to or better than the baseline BLEU-only
tuning. Italicized scores indicate statistically significant differ-
ences at p-value 0.05 computed with bootstrap significance test.
a popular choice for optimizing SMT systems.
Among the different MMO methods the ensem-
ble tuning performs better than lateen or union ap-
proaches. In terms of the number of metrics being
optimized jointly, we see substantial gains when us-
ing a small number (typically 2 or 3) of metrics. Re-
sults seem to suffer beyond this number; probably
because there might not be a space that contain so-
lution(s) optimal for all the metrics that are jointly
optimized.
We hypothesize that each metric correlates well
953
 0.808
 0.81
 0.812
 0.814
 0.816
 0.818
 0.82
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
R
I
B
E
S
BLEU
 B
 M
 R
 T  PEns
 LEns
BM 
BR  
 BT
MR  
 BMR
 BMT
 BRT
BMRT  
 BMRTB3
 BMRTB3B2B1
 BM-J
 BR-J
 BT-J
BMT-J  
 BM-L
BR-L  
BMR-L  
(a) BLEU-RIBES scores
 0.495
 0.5
 0.505
 0.51
 0.515
 0.52
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
M
E
T
E
O
R
BLEU
B
 M
 R
 T
 PEns(BR)
LEns(BR)  BM  BR
 BT
MR  
 BMR
 BMT
 BRT
 BMRTB3BMRTB3B2B1  
 BM-J
 BR-J
 BT-J
BMT-J  
 BM-L
BR-L  
BMR-L  
(b) BLEU-METEOR scores
Figure 3: MTA 4-refs testset: Comparison of different MMO approaches. The dashed lines correspond to baseline scores tuned on
the respective metrics in the axes. The union of metrics method is identified with the suffix J and lateen with suffix L.
 0.4
 0.405
 0.41
 0.415
 0.42
 0.425
 0.43
 0.435
 0.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37
n
T
E
R
BLEU
B  
 M
 R
 T
 PEns(BR)
LEns(BR)  
BM   BR
 BT
MR   BMR
 BMTBRT  
 BMRT
 BMRTB3
 BM-J
BR-J  
 BT-J BMT-J  
BM-L  
BR-L  
BMR-L  
(a) MTA (4-refs)
 0.44
 0.445
 0.45
 0.455
 0.46
 0.465
 0.355  0.36  0.365  0.37  0.375
n
T
E
R
BLEU
B  
M  
 R
 T
PEns(BR)   LEns(BR)
  BM
 BR
 BMR
 B3MT
BRT  
BMRT  
 BMRTB3
BMRTB3B2B1  
 BR-J
BR-L  
(b) ISI (1-ref)
Figure 4: BLEU-TER scores: Comparison of different MMO approaches. We plot nTER (1-TER) scores for easy reading of the
plots. The dashed lines correspond to baseline scores tuned on the respective metrics in the axes.
(in a looser sense) with few others, but not all. For
example, union optimizations BR-J and BMT-J per-
form close to or better than RIBES and TER base-
lines, but get very poor score in METEOR. On the
other hand BM-J is close to the METEOR baseline,
while doing poorly on the RIBES and TER. This be-
haviour is also evident from the single-metric base-
lines, where R and T-only settings are clearly distin-
guished from the M-only system. It is not clear if
such distinct classes of metrics could be bridged by
some optimal solution and the metric dichotomy re-
quires further study as this is key to practical multi-
metric tuning in SMT.
The lateen and union approaches appear to be
very sensitive to the number of metrics and they
generally perform well for two metrics case and
show degradation for more metrics. Unlike other
approaches, the union approach failed to improve
over the baseline BLEU and this could be attributed
to the conflict of interest among the metrics, while
choosing example points for the optimization step.
The positive example preferred by a particular met-
ric could be a negative example for the other metric.
This would only confuse the optimizer resulting in
poor solutions. Our future line of work would be to
study the effect of avoiding such of conflicting ex-
amples in the union approach.
For the single-reference (ISI) dataset, we only
plot the BLEU-TER case in Figure 4(b) due to lack
of space. The results are similar to the multiple
references set indicating that MMO approaches are
equally effective for single references5. Table 2
5One could argue that MMO methods require multiple ref-
erences since each metric might be picking out a different ref-
954
Metric
Single-metric Tuning Ensemble Tuning
B-only M-only B-M-R
BLEU 37.89 37.18 39.01
HBLEU 51.93 53.59 53.14
METEOR 61.31 61.56 61.68
HMETEOR 72.35 72.39 72.74
TER 0.520 0.532 0.516
HTER 0.361 0.370 0.346
Table 3: Post-editing Human Evaluation: Regular (untargeted)
and human-targeted scores. Human targeted scores are com-
puted against the post-edited reference and regular scores are
computed with the original references. Best scores are in bold-
face and statistically significant ones (at p = 0.05) are italicized.
shows the BLEU scores for our ensemble tuning
method (for various combinations) and we again see
improvements over the baseline BLEU-only tuning.
6.3 Human Evaluation
So far we have shown that multi-metric optimiza-
tion can improve over single-metric tuning on a sin-
gle metric like BLEU and we have shown that our
methods find a tuned model that performs well with
respect to multiple metrics. Is the output that scores
higher on multiple metrics actually a better trans-
lation? To verify this, we conducted a post-editing
human evaluation experiment. We compared our en-
semble tuning approach involving BLEU, METEOR
and RIBES (B-M-R) with systems optimized for
BLEU (B-only) and METEOR (M-only).
We selected 100 random sentences (that are at
least 15 words long) from the Arabic-English MTA
(4 references) test set and translated them using the
three systems (two single metric systems and BMR
ensemble tuning). We shuffled the resulting trans-
lations and split them into 3 sets such that each set
has equal number of the translations from three sys-
tems. The translations were edited by three human
annotators in a post-editing setup, where the goal
was to edit the translations to make them as close
to the references as possible, using the Post-Editing
Tool: PET (Aziz et al, 2012). The annotators were
not Arabic-literate and relied only on the reference
translations during post-editing. The identifiers that
link each translation to the system that generated it
are removed to avoid annotator bias.
In the end we collated post-edited translations for
each system and then computed the system-level
erence sentence. Our experiment shows that even with a single
reference MMO methods can work.
human-targeted (HBLEU, HMETEOR, HTER)
scores, by using respective post-edited translations
as the reference. First comparing the HTER (Snover
et al, 2006) scores shown in Table 3, we see
that the single-metric system optimized for ME-
TEOR performs slightly worse than the one op-
timized for BLEU, despite using METEOR-hter
version (Denkowski and Lavie, 2011). Ensemble
tuning-based system optimized for three metrics (B-
M-R) improves HTER by 4% and 6.3% over BLEU
and METEOR optimized systems respectively.
The single-metric system tuned with M-only set-
ting scores high on HBLEU, closely followed by the
ensemble system. We believe this to be caused by
chance rather than any systematic gains by the M-
only tuning; the ensemble system scores high on
HMETEOR compared to the M-only system. While
HTER captures the edit distance to the targeted ref-
erence, HMETEOR and HBLEU metrics capture
missing content words or synonyms by exploiting
n-grams and paraphrase matching.
We also computed the regular variants (BLEU,
METEOR and TER), which are scored against orig-
inal references. The ensemble system outperformed
the single-metric systems in all the three metrics.
The improvements were also statistically significant
at p-value of 0.05 for BLEU and TER.
7 Conclusion
We propose and present a comprehensive study of
several multi-metric optimization (MMO) methods
in SMT. First, by exploiting the idea of ensemble de-
coding (Razmara et al, 2012), we propose an effec-
tive way to combine multiple Pareto-optimal model
weights from previous MMO methods (e.g. Duh et
al. (2012)), obviating the need for manually trading
off among metrics. We also proposed two new vari-
ants: lateen-style MMO and union of metrics.
We also extended ensemble decoding to a new
tuning algorithm called ensemble tuning. This
method demonstrates statistically significant gains
for BLEU and RIBES with modest reduction in ME-
TEOR and TER. Further, in our human evaluation,
ensemble tuning obtains the best HTER among com-
peting baselines, confirming that optimizing on mul-
tiple metrics produces human-preferred translations
compared to the conventional optimization approach
involving a single metric.
955
References
Michael Auli and Adam Lopez. 2011. Training a log-
linear parser with loss functions via softmax-margin.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 333?
343, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Wilker Aziz, Sheila Castilho Monteiro de Sousa, and
Lucia Specia. 2012. PET: a tool for post-editing
and assessing machine translation. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Associa-
tion (ELRA).
Marzieh Bazrafshan, Tagyoung Chung, and Daniel
Gildea. 2012. Tuning as linear regression. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 543?547, Montre?al, Canada. ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. ACL.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the ACL, pages 555?
563. ACL.
Boxing Chen, Roland Kuhn, and Samuel Larkin. 2012.
Port: a precision-order-recall mt evaluation metric for
tuning. In Proceedings of the 50th Annual Meeting
of the ACL (Volume 1: Long Papers), pages 930?939,
Jeju Island, Korea. ACL.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 427?436, Montre?al, Canada. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233. ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, Edinburgh, Scotland, July. ACL.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-based
hypothesis selection for machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 528?532. ACL.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proceedings of
the 50th Annual Meeting of the ACL, Jeju Island, Ko-
rea. ACL.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the North American Chapter of the ACL: Human
Language Technologies, pages 221?231, Montre?al,
Canada. ACL.
Keith Hall, Ryan T. McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
Proceedings of the Empirical Methods in Natural Lan-
guage Processing, pages 1489?1499.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 292?301, Jeju Island, Ko-
rea. ACL.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, Edin-
burgh, Scotland. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944?
952, Cambridge, MA. ACL.
Thorsten Joachims. 2006. Training linear svms in linear
time. In Proceedings of the 12th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 217?226.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105?115.
956
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineer-
ing. Structural and Multidisciplinary Optimization,
26(6):369?395, April.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160?167. ACL.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3):181?193.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of Asso-
ciation of Computational Linguistics, pages 311?318.
ACL.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the ACL, Jeju, Re-
public of Korea. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics (PBML), 97(97):83?98.
Christophe Servan and Holger Schwenk. 2011. Optimis-
ing multiple metrics with mert. Prague Bull. Math.
Linguistics, 96:109?118.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11?21, Jeju Island, Korea. ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas, pages 223?231.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the Empirical Methods in
Natural Language Processing, pages 1269?1280. As-
sociation of Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773. ACL.
Taro Watanabe. 2012. Optimized online rank learn-
ing for machine translation. In Proceedings of the
2012 Conference of the North American Chapter of the
ACL: Human Language Technologies, pages 253?262,
Montre?al, Canada, June. ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Daqi Zheng, Yifan He, Yang Liu, and Qun Liu. 2012.
Maximum rank correlation training for statistical ma-
chine translation. In MT Summit XIII.
957
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940?949,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Mixing Multiple Translation Models in Statistical Machine Translation
Majid Razmara1 George Foster2 Baskaran Sankaran1 Anoop Sarkar1
1 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada
{razmara,baskaran,anoop}@sfu.ca
2 National Research Council Canada, 283 Alexandre-Tache? Blvd, Gatineau, QC, Canada
george.foster@nrc.gc.ca
Abstract
Statistical machine translation is often faced
with the problem of combining training data
from many diverse sources into a single trans-
lation model which then has to translate sen-
tences in a new domain. We propose a novel
approach, ensemble decoding, which com-
bines a number of translation systems dynam-
ically at the decoding step. In this paper,
we evaluate performance on a domain adap-
tation setting where we translate sentences
from the medical domain. Our experimental
results show that ensemble decoding outper-
forms various strong baselines including mix-
ture models, the current state-of-the-art for do-
main adaptation in machine translation.
1 Introduction
Statistical machine translation (SMT) systems re-
quire large parallel corpora in order to be able to
obtain a reasonable translation quality. In statisti-
cal learning theory, it is assumed that the training
and test datasets are drawn from the same distribu-
tion, or in other words, they are from the same do-
main. However, bilingual corpora are only available
in very limited domains and building bilingual re-
sources in a new domain is usually very expensive.
It is an interesting question whether a model that is
trained on an existing large bilingual corpus in a spe-
cific domain can be adapted to another domain for
which little parallel data is present. Domain adap-
tation techniques aim at finding ways to adjust an
out-of-domain (OUT) model to represent a target do-
main (in-domain or IN).
Common techniques for model adaptation adapt
two main components of contemporary state-of-the-
art SMT systems: the language model and the trans-
lation model. However, language model adapta-
tion is a more straight-forward problem compared to
translation model adaptation, because various mea-
sures such as perplexity of adapted language models
can be easily computed on data in the target domain.
As a result, language model adaptation has been well
studied in various work (Clarkson and Robinson,
1997; Seymore and Rosenfeld, 1997; Bacchiani and
Roark, 2003; Eck et al, 2004) both for speech recog-
nition and for machine translation. It is also easier to
obtain monolingual data in the target domain, com-
pared to bilingual data which is required for transla-
tion model adaptation. In this paper, we focused on
adapting only the translation model by fixing a lan-
guage model for all the experiments. We expect do-
main adaptation for machine translation can be im-
proved further by combining orthogonal techniques
for translation model adaptation combined with lan-
guage model adaptation.
In this paper, a new approach for adapting the
translation model is proposed. We use a novel sys-
tem combination approach called ensemble decod-
ing in order to combine two or more translation
models with the goal of constructing a system that
outperforms all the component models. The strength
of this system combination method is that the sys-
tems are combined in the decoder. This enables
the decoder to pick the best hypotheses for each
span of the input. The main applications of en-
semble models are domain adaptation, domain mix-
ing and system combination. We have modified
Kriya (Sankaran et al, 2012), an in-house imple-
mentation of hierarchical phrase-based translation
system (Chiang, 2005), to implement ensemble de-
coding using multiple translation models.
We compare the results of ensemble decoding
with a number of baselines for domain adaptation.
In addition to the basic approach of concatenation of
in-domain and out-of-domain data, we also trained
a log-linear mixture model (Foster and Kuhn, 2007)
940
as well as the linear mixture model of (Foster et al,
2010) for conditional phrase-pair probabilities over
IN and OUT. Furthermore, within the framework of
ensemble decoding, we study and evaluate various
methods for combining translation tables.
2 Baselines
The natural baseline for model adaption is to con-
catenate the IN and OUT data into a single paral-
lel corpus and train a model on it. In addition to
this baseline, we have experimented with two more
sophisticated baselines which are based on mixture
techniques.
2.1 Log-Linear Mixture
Log-linear translation model (TM) mixtures are of
the form:
p(e?|f?) ? exp
( M?
m
?m log pm(e?|f?)
)
where m ranges over IN and OUT, pm(e?|f?) is an
estimate from a component phrase table, and each
?m is a weight in the top-level log-linear model, set
so as to maximize dev-set BLEU using minimum
error rate training (Och, 2003). We learn separate
weights for relative-frequency and lexical estimates
for both pm(e?|f?) and pm(f? |e?). Thus, for 2 compo-
nent models (from IN and OUT training corpora),
there are 4 ? 2 = 8 TM weights to tune. Whenever
a phrase pair does not appear in a component phrase
table, we set the corresponding pm(e?|f?) to a small
epsilon value.
2.2 Linear Mixture
Linear TM mixtures are of the form:
p(e?|f?) =
M?
m
?mpm(e?|f?)
Our technique for setting ?m is similar to that
outlined in Foster et al (2010). We first extract a
joint phrase-pair distribution p?(e?, f?) from the de-
velopment set using standard techniques (HMM
word alignment with grow-diag-and symmeteriza-
tion (Koehn et al, 2003)). We then find the set
of weights ?? that minimize the cross-entropy of the
mixture p(e?|f?) with respect to p?(e?, f?):
?? = argmax
?
?
e?,f?
p?(e?, f?) log
M?
m
?mpm(e?|f?)
For efficiency and stability, we use the EM algo-
rithm to find ??, rather than L-BFGS as in (Foster et
al., 2010). Whenever a phrase pair does not appear
in a component phrase table, we set the correspond-
ing pm(e?|f?) to 0; pairs in p?(e?, f?) that do not appear
in at least one component table are discarded. We
learn separate linear mixtures for relative-frequency
and lexical estimates for both p(e?|f?) and p(f? |e?).
These four features then appear in the top-level
model as usual ? there is no runtime cost for the lin-
ear mixture.
3 Ensemble Decoding
Ensemble decoding is a way to combine the exper-
tise of different models in one single model. The
current implementation is able to combine hierar-
chical phrase-based systems (Chiang, 2005) as well
as phrase-based translation systems (Koehn et al,
2003). However, the method can be easily extended
to support combining a number of heterogeneous
translation systems e.g. phrase-based, hierarchical
phrase-based, and/or syntax-based systems. This
section explains how such models can be combined
during the decoding.
Given a number of translation models which are
already trained and tuned, the ensemble decoder
uses hypotheses constructed from all of the models
in order to translate a sentence. We use the bottom-
up CKY parsing algorithm for decoding. For each
sentence, a CKY chart is constructed. The cells of
the CKY chart are populated with appropriate rules
from all the phrase tables of different components.
As in the Hiero SMT system (Chiang, 2005), the
cells which span up to a certain length (i.e. the max-
imum span length) are populated from the phrase-
tables and the rest of the chart uses glue rules as de-
fined in (Chiang, 2005).
The rules suggested from the component models
are combined in a single set. Some of the rules may
be unique and others may be common with other
component model rule sets, though with different
scores. Therefore, we need to combine the scores
of such common rules and assign a single score to
941
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores. The choice of mixture operation will be
discussed in Section 3.1.
Figure 1 illustrates how the CKY chart is filled
with the rules. Each cell, covering a span, is popu-
lated with rules from all component models as well
as from cells covering a sub-span of it.
In the typical log-linear model SMT, the posterior
probability for each phrase pair (e?, f?) is given by:
p(e? | f?) ? exp
(
?
i
wi?i(e?, f?)
? ?? ?
w??
)
Ensemble decoding uses the same framework for
each individual system. Therefore, the score of a
phrase-pair (e?, f?) in the ensemble model is:
p(e? | f?) ? exp
(
w1 ? ?1? ?? ?
1st model
? w2 ? ?2? ?? ?
2nd model
? ? ? ?
)
where? denotes the mixture operation between two
or more model scores.
3.1 Mixture Operations
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for mixture operation and discuss some of the char-
acteristics of these mixture operations.
? Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities (i.e. linear
mixture).
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component mod-
els, M is the total number of them and ?i is the
weight for component i.
? Weighted Max (wmax): where the ensemble
score is the weighted max of all model scores.
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
? Model Switching (Switch): in model switch-
ing, each cell in the CKY chart gets populated
only by rules from one of the models and the
other models? rules are discarded. This is based
on the hypothesis that each component model
is an expert on certain parts of sentence. In this
method, we need to define a binary indicator
function ?(f? ,m) for each span and component
model to specify rules of which model to retain
for each span.
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each cell,
?(f? , n), could be based on:
? Max: for each cell, the model that has the
highest weighted best-rule score wins:
?(f? , n) = ?n max
e
(wn ? ?n(e?, f?))
? Sum: Instead of comparing only the
scores of the best rules, the model with
the highest weighted sum of the probabil-
ities of the rules wins. This sum has to
take into account the translation table limit
(ttl), on the number of rules suggested by
each model for each cell:
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
computed as:
p(e? | f?) =
M?
m
?(f? ,m) pm(e? | f?)
? Product (prod): in Product models or Prod-
uct of Experts (Hinton, 1999), the probability
of the ensemble model or a rule is computed as
the product of the probabilities of all compo-
nents (or equally the sum of log-probabilities,
i.e. log-linear mixture). Product models can
also make use of weights to control the contri-
bution of each component. These models are
942
Figure 1: The cells in the CKY chart are populated using rules from all component models and sub-span cells.
generally known as Logarithmic Opinion Pools
(LOPs) where:
p(e? | f?) ? exp
(
M?
m
?m (wm ? ?m)
)
Product models have been used in combining
LMs and TMs in SMT as well as some other
NLP tasks such as ensemble parsing (Petrov,
2010).
Each of these mixture operations has a specific
property that makes it work in specific domain adap-
tation or system combination scenarios. For in-
stance, LOPs may not be optimal for domain adapta-
tion in the setting where there are two or more mod-
els trained on heterogeneous corpora. As discussed
in (Smith et al, 2005), LOPs work best when all the
models accuracies are high and close to each other
with some degree of diversity. LOPs give veto power
to any of the component models and this perfectly
works for settings such as the one in (Petrov, 2010)
where a number of parsers are trained by changing
the randomization seeds but having the same base
parser and using the same training set. They no-
ticed that parsers trained using different randomiza-
tion seeds have high accuracies but there are some
diversities among them and they used product mod-
els for their advantage to get an even better parser.
We assume that each of the models is expert in some
parts and so they do not necessarily agree on cor-
rect hypotheses. In other words, product models (or
LOPs) tend to have intersection-style effects while
we are more interested in union-style effects.
In Section 4.2, we compare the BLEU scores of
different mixture operations on a French-English ex-
perimental setup.
3.2 Normalization
Since in log-linear models, the model scores are
not normalized to form probability distributions, the
scores that different models assign to each phrase-
pair may not be in the same scale. Therefore, mixing
their scores might wash out the information in one
(or some) of the models. We experimented with two
different ways to deal with this normalization issue.
A practical but inexact heuristic is to normalize the
scores over a shorter list. So the list of rules coming
from each model for a cell in CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed changing the
scores with the normalized scores hurts the BLEU
score radically. So we use the normalized scores
only for pruning and the actual scores are intact.
We could also globally normalize the scores to ob-
tain posterior probabilities using the inside-outside
algorithm. However, we did not try it as the BLEU
scores we got using the normalization heuristic was
not promissing and it would impose a cost in de-
coding as well. More investigation on this issue has
been left for future work.
A more principled way is to systematically find
the most appropriate model weights that can avoid
this problem by scaling the scores properly. We
used a publicly available toolkit, CONDOR (Van-
den Berghen and Bersini, 2005), a direct optimizer
based on Powell?s algorithm, that does not require
943
explicit gradient information for the objective func-
tion. Component weights for each mixture operation
are optimized on the dev-set using CONDOR.
4 Experiments & Results
4.1 Experimental Setup
We carried out translation experiments using the Eu-
ropean Medicines Agency (EMEA) corpus (Tiede-
mann, 2009) as IN, and the Europarl (EP) corpus1 as
OUT, for French to English translation. The dev and
test sets were randomly chosen from the EMEA cor-
pus.2 The details of datasets used are summarized in
Table 1.
Dataset Sents
Words
French English
EMEA 11770 168K 144K
Europarl 1.3M 40M 37M
Dev 1533 29K 25K
Test 1522 29K 25K
Table 1: Training, dev and test sets for EMEA.
For the mixture baselines, we used a standard
one-pass phrase-based system (Koehn et al, 2003),
Portage (Sadat et al, 2005), with the following 7
features: relative-frequency and lexical translation
model (TM) probabilities in both directions; word-
displacement distortion model; language model
(LM) and word count. The corpus was word-aligned
using both HMM and IBM2 models, and the phrase
table was the union of phrases extracted from these
separate alignments, with a length limit of 7. It
was filtered to retain the top 20 translations for each
source phrase using the TM part of the current log-
linear model.
For ensemble decoding, we modified an in-house
implementation of hierarchical phrase-based sys-
tem, Kriya (Sankaran et al, 2012) which uses the
same features mentioned in (Chiang, 2005): for-
ward and backward relative-frequency and lexical
TM probabilities; LM; word, phrase and glue-rules
penalty. GIZA++(Och and Ney, 2000) has been used
for word alignment with phrase length limit of 7.
In both systems, feature weights were optimized
using MERT (Och, 2003) and with a 5-gram lan-
1www.statmt.org/europarl
2Please contact the authors to access the data-sets.
guage model and Kneser-Ney smoothing was used
in all the experiments. We used SRILM (Stolcke,
2002) as the langugage model toolkit. Fixing the
language model allows us to compare various trans-
lation model combination techniques.
4.2 Results
Table 2 shows the results of the baselines. The first
group are the baseline results on the phrase-based
system discussed in Section 2 and the second group
are those of our hierarchical MT system. Since the
Hiero baselines results were substantially better than
those of the phrase-based model, we also imple-
mented the best-performing baseline, linear mixture,
in our Hiero-style MT system and in fact it achieves
the hights BLEU score among all the baselines as
shown in Table 2. This baseline is run three times
the score is averaged over the BLEU scores with
standard deviation of 0.34.
Baseline PBS Hiero
IN 31.84 33.69
OUT 24.08 25.32
IN + OUT 31.75 33.76
LOGLIN 32.21 ?
LINMIX 33.81 35.57
Table 2: The results of various baselines implemented in
a phrase-based (PBS) and a Hiero SMT on EMEA.
Table 3 shows the results of ensemble decoding
with different mixture operations and model weight
settings. Each mixture operation has been evalu-
ated on the test-set by setting the component weights
uniformly (denoted by uniform) and by tuning the
weights using CONDOR (denoted by tuned) on a
held-out set. The tuned scores (3rd column in Ta-
ble 3) are averages of three runs with different initial
points as in Clark et al (2011). We also reported the
BLEU scores when we applied the span-wise nor-
malization heuristic. All of these mixture operations
were able to significantly improve over the concate-
nation baseline. In particular, Switching:Max could
gain up to 2.2 BLEU points over the concatenation
baseline and 0.39 BLEU points over the best per-
forming baseline (i.e. linear mixture model imple-
mented in Hiero) which is statistically significant
based on Clark et al (2011) (p = 0.02).
Prod when using with uniform weights gets the
944
Mixture Operation Uniform Tuned Norm.
WMAX 35.39 35.47 (s=0.03) 35.47
WSUM 35.35 35.53 (s=0.04) 35.45
SWITCHING:MAX 35.93 35.96 (s=0.01) 32.62
SWITCHING:SUM 34.90 34.72 (s=0.23) 34.90
PROD 33.93 35.24 (s=0.05) 35.02
Table 3: The results of ensemble decoding on EMEA for Fr2En when using uniform weights, tuned weights and
normalization heuristic. The tuned BLEU scores are averaged over three runs with multiple initial points, as in (Clark
et al, 2011), with the standard deviations in brackets .
lowest score among the mixture operations, how-
ever after tuning, it learns to bias the weights to-
wards one of the models and hence improves by
1.31 BLEU points. Although Switching:Sum outper-
forms the concatenation baseline, it is substantially
worse than other mixture operations. One explana-
tion that Switching:Max is the best performing op-
eration and Switching:Sum is the worst one, despite
their similarities, is that Switching:Max prefers more
peaked distributions while Switching:Sum favours a
model that has fewer hypotheses for each span.
An interesting observation based on the results in
Table 3 is that uniform weights are doing reasonably
well given that the component weights are not opti-
mized and therefore model scores may not be in the
same scope (refer to discussion in ?3.2). We suspect
this is because a single LM is shared between both
models. This shared component controls the vari-
ance of the weights in the two models when com-
bined with the standard L-1 normalization of each
model?s weights and hence prohibits models to have
too varied scores for the same input. Though, it may
not be the case when multiple LMs are used which
are not shared.
Two sample sentences from the EMEA test-set
along with their translations by the IN, OUT and En-
semble models are shown in Figure 2. The boxes
show how the Ensemble model is able to use n-
grams from the IN and OUT models to construct
a better translation than both of them. In the first
example, there are two OOVs one for each of the
IN and OUT models. Our approach is able to re-
solve the OOV issues by taking advantage of the
other model?s presence. Similarly, the second exam-
ple shows how ensemble decoding improves lexical
choices as well as word re-orderings.
5 Related Work
5.1 Domain Adaptation
Early approaches to domain adaptation involved in-
formation retrieval techniques where sentence pairs
related to the target domain were retrieved from the
training corpus using IR methods (Eck et al, 2004;
Hildebrand et al, 2005). Foster et al (2010), how-
ever, uses a different approach to select related sen-
tences from OUT. They use language model per-
plexities from IN to select relavant sentences from
OUT. These sentences are used to enrich the IN
training set.
Other domain adaptation methods involve tech-
niques that distinguish between general and domain-
specific examples (Daume? and Marcu, 2006). Jiang
and Zhai (2007) introduce a general instance weight-
ing framework for model adaptation. This approach
tries to penalize misleading training instances from
OUT and assign more weight to IN-like instances
than OUT instances. Foster et al (2010) propose a
similar method for machine translation that uses fea-
tures to capture degrees of generality. Particularly,
they include the output from an SVM classifier that
uses the intersection between IN and OUT as pos-
itive examples. Unlike previous work on instance
weighting in machine translation, they use phrase-
level instances instead of sentences.
A large body of work uses interpolation tech-
niques to create a single TM/LM from interpolating
a number of LMs/TMs. Two famous examples of
such methods are linear mixtures and log-linear mix-
tures (Koehn and Schroeder, 2007; Civera and Juan,
2007; Foster and Kuhn, 2007) which were used as
baselines and discussed in Section 2. Other meth-
ods include using self-training techniques to exploit
monolingual in-domain data (Ueffing et al, 2007;
945
SOURCE ame?norrhe?e , menstruations irre?gulie`res
REF amenorrhoea , irregular menstruation
IN amenorrhoea , menstruations irre?gulie`res
OUT ame?norrhe?e , irregular menstruation
ENSEMBLE amenorrhoea , irregular menstruation
SOURCE le traitement par naglazyme doit e?tre supervise? par un me?decin ayant l? expe?rience de
la prise en charge des patients atteints de mps vi ou d? une autre maladie me?tabolique
he?re?ditaire .
REF naglazyme treatment should be supervised by a physician experienced in the manage-
ment of patients with mps vi or other inherited metabolic diseases .
IN naglazyme treatment should be supervise? by a doctor the with
in the management of patients with mps vi or other hereditary metabolic disease .
OUT naglazyme ?s treatment must be supervised by a doctor with the experience of the care
of patients with mps vi. or another disease hereditary metabolic .
ENSEMBLE naglazyme treatment should be supervised by a physician experienced
in the management of patients with mps vi or other hereditary metabolic disease .
Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems.
Bertoldi and Federico, 2009). In this approach, a
system is trained on the parallel OUT and IN data
and it is used to translate the monolingual IN data
set. Iteratively, most confident sentence pairs are se-
lected and added to the training corpus on which a
new system is trained.
5.2 System Combination
Tackling the model adaptation problem using sys-
tem combination approaches has been experimented
in various work (Koehn and Schroeder, 2007; Hilde-
brand and Vogel, 2009). Among these approaches
are sentence-based, phrase-based and word-based
output combination methods. In a similar approach,
Koehn and Schroeder (2007) use a feature of the fac-
tored translation model framework in Moses SMT
system (Koehn and Schroeder, 2007) to use multiple
alternative decoding paths. Two decoding paths, one
for each translation table (IN and OUT), were used
during decoding. The weights are set with minimum
error rate training (Och, 2003).
Our work is closely related to Koehn and
Schroeder (2007) but uses a different approach to
deal with multiple translation tables. The Moses
SMT system implements (Koehn and Schroeder,
2007) and can treat multiple translation tables in
two different ways: intersection and union. In in-
tersection, for each span only the hypotheses would
be used that are present in all phrase tables. For
each set of hypothesis with the same source and
target phrases, a new hypothesis is created whose
feature-set is the union of feature sets of all corre-
sponding hypotheses. Union, on the other hand, uses
hypotheses from all the phrase tables. The feature
set of these hypotheses are expanded to include one
feature set for each table. However, for the corre-
sponding feature values of those phrase-tables that
did not have a particular phrase-pair, a default log
probability value of 0 is assumed (Bertoldi and Fed-
erico, 2009) which is counter-intuitive as it boosts
the score of hypotheses with phrase-pairs that do not
belong to all of the translation tables.
Our approach is different from Koehn and
Schroeder (2007) in a number of ways. Firstly, un-
like the multi-table support of Moses which only
supports phrase-based translation table combination,
our approach supports ensembles of both hierarchi-
cal and phrase-based systems. With little modifica-
tion, it can also support ensemble of syntax-based
systems with the other two state-of-the-art SMT sys-
946
tems. Secondly, our combining method uses the
union option, but instead of preserving the features
of all phrase-tables, it only combines their scores
using various mixture operations. This enables us
to experiment with a number of different opera-
tions as opposed to sticking to only one combination
method. Finally, by avoiding increasing the number
of features we can add as many translation models
as we need without serious performance drop. In
addition, MERT would not be an appropriate opti-
mizer when the number of features increases a cer-
tain amount (Chiang et al, 2008).
Our approach differs from the model combina-
tion approach of DeNero et al (2010), a generaliza-
tion of consensus or minimum Bayes risk decoding
where the search space consists of those of multi-
ple systems, in that model combination uses forest
of derivations of all component models to do the
combination. In other words, it requires all compo-
nent models to fully decode each sentence, compute
n-gram expectations from each component model
and calculate posterior probabilities over transla-
tion derivations. While, in our approach we only
use partial hypotheses from component models and
the derivation forest is constructed by the ensemble
model. A major difference is that in the model com-
bination approach the component search spaces are
conjoined and they are not intermingled as opposed
to our approach where these search spaces are inter-
mixed on spans. This enables us to generate new
sentences that cannot be generated by component
models. Furthermore, various combination methods
can be explored in our approach. Finally, main tech-
niques used in this work are orthogonal to our ap-
proach such as Minimum Bayes Risk decoding, us-
ing n-gram features and tuning using MERT.
Finally, our work is most similar to that of
Liu et al (2009) where max-derivation and max-
translation decoding have been used. Max-
derivation finds a derivation with highest score and
max-translation finds the highest scoring translation
by summing the score of all derivations with the
same yield. The combination can be done in two
levels: translation-level and derivation-level. Their
derivation-level max-translation decoding is similar
to our ensemble decoding with wsum as the mixture
operation. We did not restrict ourself to this par-
ticular mixture operation and experimented with a
number of different mixing techniques and as Ta-
ble 3 shows we could improve over wsum in our
experimental setup. Liu et al (2009) used a mod-
ified version of MERT to tune max-translation de-
coding weights, while we use a two-step approach
using MERT for tuning each component model sep-
arately and then using CONDOR to tune component
weights on top of them.
6 Conclusion & Future Work
In this paper, we presented a new approach for do-
main adaptation using ensemble decoding. In this
approach a number of MT systems are combined at
decoding time in order to form an ensemble model.
The model combination can be done using various
mixture operations. We showed that this approach
can gain up to 2.2 BLEU points over its concatena-
tion baseline and 0.39 BLEU points over a powerful
mixture model.
Future work includes extending this approach to
use multiple translation models with multiple lan-
guage models in ensemble decoding. Different
mixture operations can be investigated and the be-
haviour of each operation can be studied in more
details. We will also add capability of support-
ing syntax-based ensemble decoding and experi-
ment how a phrase-based system can benefit from
syntax information present in a syntax-aware MT
system. Furthermore, ensemble decoding can be ap-
plied on domain mixing settings in which develop-
ment sets and test sets include sentences from dif-
ferent domains and genres, and this is a very suit-
able setting for an ensemble model which can adapt
to new domains at test time. In addition, we can
extend our approach by applying some of the tech-
niques used in other system combination approaches
such as consensus decoding, using n-gram features,
tuning using forest-based MERT, among other pos-
sible extensions.
Acknowledgments
This research was partially supported by an NSERC,
Canada (RGPIN: 264905) grant and a Google Fac-
ulty Award to the last author. We would like to
thank Philipp Koehn and the anonymous reviewers
for their valuable comments. We also thank the de-
velopers of GIZA++ and Condor which we used for
our experiments.
947
References
M. Bacchiani and B. Roark. 2003. Unsupervised lan-
guage model adaptation. In Acoustics, Speech, and
Signal Processing, 2003. Proceedings. (ICASSP ?03).
2003 IEEE International Conference on, volume 1,
pages I?224 ? I?227 vol.1, april.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. ACL.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ?05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263?270, Mor-
ristown, NJ, USA. ACL.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, StatMT ?07, pages
177?180, Stroudsburg, PA, USA. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume 2,
HLT ?11, pages 176?181. ACL.
P. Clarkson and A. Robinson. 1997. Language model
adaptation using mixtures and an exponentially decay-
ing cache. In Proceedings of the 1997 IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP ?97)-Volume 2 - Volume 2,
ICASSP ?97, pages 799?, Washington, DC, USA.
IEEE Computer Society.
Hal Daume?, III and Daniel Marcu. 2006. Domain
adaptation for statistical classifiers. J. Artif. Int. Res.,
26:101?126, May.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 975?983, Stroudsburg, PA, USA. ACL.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Pro-
ceedings of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for smt. In Proceedings of the Second
Workshop on Statistical Machine Translation, StatMT
?07, pages 128?135, Stroudsburg, PA, USA. ACL.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 451?
459, Stroudsburg, PA, USA. ACL.
Almut Silja Hildebrand and Stephan Vogel. 2009. CMU
system combination for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion, StatMT ?09, pages 47?50, Stroudsburg, PA, USA.
ACL.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the translation
model for statistical machine translation based on in-
formation retrieval. In Proceedings of the 10th EAMT
2005, Budapest, Hungary, May.
Geoffrey E. Hinton. 1999. Products of experts. In Artifi-
cial Neural Networks, 1999. ICANN 99. Ninth Interna-
tional Conference on (Conf. Publ. No. 470), volume 1,
pages 1?6.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264?271, Prague, Czech
Republic, June. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, StatMT ?07, pages 224?
227, Stroudsburg, PA, USA. ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 127?133, Edmonton, May.
NAACL.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ?09, pages
576?584, Stroudsburg, PA, USA. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual Meet-
ing of the ACL, pages 440?447, Hongkong, China, Oc-
tober.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41th Annual Meeting of the ACL, Sapporo, July. ACL.
948
Slav Petrov. 2010. Products of random latent variable
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 19?27, Stroudsburg, PA, USA. ACL.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Joel Martin, and Aaron Tikuisis. 2005.
Portage: A phrase-based machine translation system.
In In Proceedings of the ACL Worskhop on Building
and Using Parallel Texts, Ann Arbor. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97), April.
Kristie Seymore and Ronald Rosenfeld. 1997. Us-
ing story topics for language model adaptation. In
George Kokkinakis, Nikos Fakotakis, and Evangelos
Dermatas, editors, EUROSPEECH. ISCA.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 18?25, Stroudsburg, PA, USA. ACL.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings International Con-
ference on Spoken Language Processing, pages 257?
286.
Jorg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25?32, Prague, Czech Republic, June. ACL.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175,
September.
949
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 216?223,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Incremental Decoding for Phrase-based Statistical Machine Translation
Baskaran Sankaran, Ajeet Grewal and Anoop Sarkar
School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby BC. V5A 2Y1. Canada
{baskaran, asg10, anoop}@cs.sfu.ca
Abstract
In this paper we focus on the incremental
decoding for a statistical phrase-based ma-
chine translation system. In incremental
decoding, translations are generated incre-
mentally for every word typed by a user,
instead of waiting for the entire sentence
as input. We introduce a novel modifi-
cation to the beam-search decoding algo-
rithm for phrase-based MT to address this
issue, aimed at efficient computation of fu-
ture costs and avoiding search errors. Our
objective is to do a faster translation dur-
ing incremental decoding without signifi-
cant reduction in the translation quality.
1 Introduction
Statistical Machine Translation has matured sig-
nificantly in the past decade and half, resulting in
the proliferation of several web-based and com-
mercial translation services. Most of these ser-
vices work on sentence or document level, where
a user enters a sentence or chooses a document
for translation, which are then translated by the
servers. Translation in such typical scenarios is
still offline in the sense that the user input and
translation happen sequentially without any inter-
action between the two phases.
In this paper we study decoding for SMT with
the constraint that translations are to be gener-
ated incrementally for every word typed in by the
user. Such a translation service can be used for
language learning, where the user is fluent in the
target language and experiments with many differ-
ent source language sentences interactively, or in
real-time translation environments such as speech-
speech translation or translation during interactive
chats.
We use a phrase-based decoder similar to
Moses (Koehn et al, 2007) and propose novel
modifications in the decoding algorithm to tackle
incremental decoding. Our system maintains a
partial decoder state at every stage and uses it
while decoding for each newly added word. As
the decoder has access only to the partial sentence
at every stage, the future costs change with ev-
ery additional word and this has to be taken into
account while continuing from an existing partial
decoder state. Another major issue is that as incre-
mental decoding is provided new input one word
at at time, some of the entries that were pruned out
at an earlier decoder state might later turn out to
better candidates resulting in search errors com-
pared to decoding the entire sentence at once. It
is to be noted that, the search error problem is re-
lated to the inability to compute full future cost
in incremental decoding. Our proposed modifica-
tions address these twin challenges and allow for
efficient incremental decoding.
2 Incremental Decoding
2.1 Beam Search for Phrase-based SMT
In this section we review the usual beam search de-
coder for phrase-based MT because we present our
modifications for incremental decoding using the
same notation. Beam search decoding for phrase-
based SMT (Koehn, 2004) begins by collecting
the translation options from the phrase table for all
possible phrases of a given input sentence and pre-
computes the future cost for all possible contigu-
ous sequences in the sentence. The pseudo-code
for the usual beam-search decoding algorithm is
illustrated in Algorithm 1.
The decoder creates n bins for storing hypothe-
ses grouped by the number of source words cov-
ered. Starting from a null hypothesis in bin 0, the
decoder iterates through bins 1 though n filling
them with new hypotheses by extending the en-
tries in the earlier bins.
A hypothesis contains the target words gener-
ated (e), the source positions translated so far (f )
commonly known as coverage set and the score
of the current translation (p) computed by the
weighted log-linear combination of different fea-
ture functions. It also contains a back-pointer to
216
Algorithm 1 Phrase-based Decoder pseudocode
(Koehn, 2004)
1: Given: sentence Sn: s1s2...sn of length n
2: Pre-compute future costs for all contiguous
sequences
3: Initialize bins bi where i = 1 . . . n
4: Create initial hypothesis: {e : (), f : (), p :
1.0}
5: for i = 1 to n do
6: for hyp ? bi do
7: for newHyp that extends hyp do
8: nf := num src words covered by
newHyp
9: Add newHyp to bin bnf
10: Prune bin bnf using future costs
11: Find best hypothesis in bn
12: Output best path that leads to best hypothesis
its parent hypothesis in the previous state and other
information used for pruning and computing cost
in later iterations.
As a new hypothesis is generated by extending
an existing hypothesis with a new phrase pair, de-
coder updates the associated information such as
coverage set, the target words generated, future
cost (for translating rest of the source words) and
its translation score. For example, consider Span-
ish to English translation: for the source sentence
Maria no daba una bofetada, the hypothesis {e :
(Mary), f : (1), p : 0.534} which is the hypoth-
esis that covers Maria can be extended to a new
hypothesis {e : (Mary, slap), f : (1, 3, 4, 5), p :
0.043} by choosing a new phrase pair (daba una
bofetada, slap) covering the source phrases Maria
and daba una bofetada. The probability score is
obtained by weighted log-linear sum of the fea-
tures of the phrases contained in the derivation so
far.
An important aspect of beam search decoding
is the pruning away of low-scoring hypotheses in
each bin to reduce the search space and thus mak-
ing the decoding faster. To do this effectively,
beam search decoding uses the future cost of a hy-
pothesis together with its current cost. The future
cost is an estimate of the translation cost of the
input words that are yet to be translated, and is
typically pre-computed for all possible contiguous
sequences in the input sentence before the decod-
ing step. The future cost prevents the any hypothe-
ses that are low-scoring, but potentially promising,
from being pruned.
2.2 Incremental Decoder - Challenges
Our goal for the incremental decoder (ID) is to
generate output translations incrementally for par-
tial phrases as the source sentence is being input
by the user. We assume white-space to be the word
delimiter and the partial sentence is decoded for
every encounter of the space character. We further
assume the return key to mark end-of-sentence
(EOS) and use it to compute language model score
for the entire sentence.
As we noted above, future costs cannot be pre-
computed as in regular decoding because the com-
plete input sentence is not known while decod-
ing incrementally. Thus the incremental decoder
can only use a partial future cost until the EOS
is reached. The partial future cost could result
in some of the potentially better candidates being
pruned away in earlier stages. This leads to search
errors and result in lower translation quality.
2.3 Approach
We use a modified beam search for incremental
decoding (ID) and the two key modifications are
aimed at addressing the issues of future cost and
search errors. Beam search for ID begins with
a single bin for the first word and more bins are
added as the sentence is completed by the user.
Our approach requires that the decoder states for
the partial source sentence can be stored in a way
that allows efficient retrieval. It also maintains a
current decoder state, which includes all the bins
and the hypotheses contained in them, all pertain-
ing to the present sentence.
At each step ID goes through a pre-process
phase, where it recomputes the partial future costs
for all the spans accounting for the new word and
updates the current decoder state with new partial
future costs. It then generates new hypotheses into
all the earlier bins and in the newly created us-
ing any new phrases (resulting from the new word
added by the user) not used earlier.
Algorithm 2 shows the pseudocode of our incre-
mental decoder. Given a partial sentence Si1 ID
starts with the pre-process phase illustrated sepa-
rately in algorithm 3. We use Ptype(l) to denote
phrases of length l words and Htype to denote the
set of hypotheses; in both cases type correspond to
either old or new, indicating if it was not known in
the previous decoding state or not.
1we use Si and si to denote a i word partial sentence and
ith word in a (partial) sentence respectively
217
Algorithm 2 Incremental Decoder pseudocode
1: Input: (partial) sentence Sp: s1s2...si?1si
with ls words where si is the new word
2: PreProcess(Sp) (Algorithm 3)
3: for every bin bj in (1 . . . i) do
4: Update future cost and cover set ? Hold
5: Add any new phrase of length bj (subject to
d)
6: for bin bk in (bj?MaxPhrLen . . . bj?1) do
7: Generate Hnew for bj by extending:
8: every Hold with every other Pnew(bj ?
bk)
9: every Hnew with every other Pany(bj ?
bk)
10: Prune bin bj
Algorithm 3 PreProcess subroutine
1: Input: partial sentence Sp of length ls
2: Retrieve partial decoder object for Sp?1
3: Identify possible Pnew (subject to Max-
PhrLen)
4: Recompute fc for all spans in 1...ls
5: for every Pnew in local phrase table do
6: Load translation options to table
7: for every Pold in local phrase table do
8: Update fc with the recomputed cost
Given Si, the pre-process phase extracts the new
set of phrases (Pnew) for the ith word and adds
them to the existing phrases (Pold). It then recom-
putes the future-cost (fc) for all the contiguous se-
quences in the partial input and updates existing
entries in the local copy of phrase table with new
fc.
In decoding phase, ID generates new hypothe-
ses in two ways: i) by extending the existing hy-
potheses Hold in the previous decoder state Si?1
with new phrases Pnew and ii) by generating new
hypotheses Hnew that are unknown in the previous
state.
The main difference between incremental de-
coding and regular beam-search decoding is inside
the two ?for? loops corresponding to lines 3? 9 in
algorithm 2. In the outer loop each of the existing
hypotheses are updated to reflect the recomputed
fc and coverage set. Any new phrases belonging
to the current bin are also added to it2.
2Based on our implementation of lazier cube pruning they
are added to a priority queue, the contents of which are
flushed into the bin at the end of inner for-loop and before
the pruning step
Hypothesis surfaces
P. Queue
Hypstack
hyp 2hyp 1
A single surface
     
     
     
     
     





Figure 1: Illustration of Lazier Cube Pruning
The inner for-loop corresponds to the extension
of hypotheses sets (grouped by same coverage set)
to generate new hypotheses. Here a distinction is
made between hypotheses Hold corresponding to
previous decoder state Sp?1 and hypotheses Hnew
resulting from the addition of word si. Hold is ex-
tended only using the newly found phrases Pnew,
whereas the newer hypotheses are processed as in
regular beam-search.
2.4 Lazier Cube Pruning
We have adapted the pervasive lazy algorithm
(or ?lazier cube pruning?) proposed originally for
Hiero-style systems by (Pust and Knight, 2009)
for our phrase-based system. This step corre-
sponds to the lines 5?9 of algorithm 2 and allows
us to only generate as many hypotheses as speci-
fied by the configurable parameters, beam size and
beam threshold. Figure 1 illustrates the process of
lazier cube pruning for a single bin.
At the highest level it uses a priority queue,
which is populated by the different hyper-edges
or surfaces3, each corresponding to a pair of hy-
potheses that are being merged to create a new
hypothesis. New hypotheses are generated iter-
atively, such that the hypothesis with the highest
score is chosen in each iteration from among dif-
ferent hyper-edges bundles.
However, this will lead to search errors as have
been observed earlier. Any hyper-edge that has
been discarded due to poor score in an early stage
might later become a better candidate. The prob-
lem worsens further when using smaller beam
sizes (for interactive decoding in real-time set-
tings, we even consider a beam size of 3). In
3Unlike Hiero-style systems, only two hypotheses are
merged in a phrase-based system and hence the term surface
218
the next section, we introduce the idea of delayed
pruning to reduce search errors.
3 Delayed Pruning
Delayed pruning (DP) in our decoder was inspired
by the well known fable about the race between
a tortoise and a hare. If the decoding is consid-
ered to be a race between competing candidate hy-
potheses with the winner being the best hypothe-
sis for Viterbi decoding or among the top-n candi-
dates for n-best decoding.4
In this analogy, a hypothesis having a poor
score, might just be a tortoise having a slow start
(due to a bad estimate of the true future cost for
what the user intends to type in the future) as op-
posed to a high scoring hare in the same state.
Pruning such hypotheses early on is not risk-free
and might result in search errors. We hypothe-
size that, given enough chance it might improve its
score and move ahead of a hare in terms of trans-
lation score.
We implement DP by relaxing the lazier cube
pruning step to generate a small, fixed number
of hypotheses for coverage sets that are not rep-
resented in the priority queue and place them in
the bin. These hypotheses are distinct from the
usual top-k derivations. Thus, the resulting bin
will have entries from all possible hyper-edge bun-
dles. Though this reduces the search error prob-
lem, it leads to increasing number of possibilities
to be explored at later stages with vast majority
of them being worse hypotheses that should be
pruned away.
We use a two level strategy of delay and then
prune, to avoid such exponentially increasing
search space and at the same time to reduce search
error. At the delay level, the idea is to delay the
pruning for few promising tortoises, instead of re-
taining a fixed number of hypotheses from all un-
represented hyper-edges. We use the normalized
language model scores of the top-hypotheses in
each hyper-edge that is not represented in cube
pruning and based on a threshold (which is ob-
tained using a development test set), we selec-
tively choose few hyper-edge bundles and gen-
erate a small number (typically 1-3) of hypothe-
ses from each of them and flag them as tortoises.
4The analogy is used to compare two or more hypotheses
in terms of their translation scores and not speed. Though our
objective is faster incremental decoding, we use the analogy
here to compare the scores.
These tortoises are extended minimally at each it-
eration subject to their normalized LM score.
While this significantly reduces the total num-
ber of hypotheses at initial bins, many of these
tortoises might not show improvement even after
several bins. Thus at the prune level, we prune out
tortoises that does not improve beyond a threshold
number of bins called race course limit. The race
course limit signifies the number of steps a tortoise
has in order to get into the decoder beam.
When a tortoise improves in score and breaks
into the beam during cube pruning, it is de-
flagged as a tortoise and enters the regular decod-
ing stream. We found DP to be effective in reduc-
ing the search error for incremental decoder in our
experiments.
4 Evaluation and Discussion
The evaluation was performed using our own im-
plementation of the beam-search decoding algo-
rithms. The architecture of our system is similar
to Moses, which we also use for training and for
minimum error rate training (MERT) of the log-
linear model for translation (Och, 2003; Koehn et
al., 2007). Our features include 7 standard phrase-
based features: 4 translation model features, i.e.
p(f |e), p(e|f), plex(f |e) and plex(e|f), where e
and f are target and source phrases respectively;
features for phrase penalty, word penalty and lan-
guage model, and we do not include the reorder-
ing feature. We used Giza++ and Moses respec-
tively for aligning the sentences and training the
system. The decoder was written in Java and in-
cludes cube pruning (Huang and Chiang, 2007)
and lazier cube pruning (Pust and Knight, 2009)
functionalities as part of the decoder. Our de-
coder supports both regular beam search (similar
to Moses) and incremental decoding.
In our experiments we experimented various ap-
proaches for storing partial decoder states includ-
ing memcache and transactional persistence using
JDBM but found that the serialization and deseri-
alization of decoder objects directly into and from
the memory to work better in terms of speed and
memory requirements. The partial object is re-
trieved and deserialized from the memory when
required by the incremental decoder.
We evaluated the incremental decoder for trans-
lations between French and English (in both direc-
tions). We used the Workshop on Machine Trans-
lation shared task (WMT07) dataset for training,
219
optimizing and testing. The system was trained us-
ing Moses and the feature weights were optimized
using MERT. To benchmark our Java decoder, we
compare it with Moses by running it in regular
beam search mode. The Moses systems were also
optimized separately on the WMT07 devsets.
Apart from comparing our decoder with Moses
in regular beam search, we also compared the in-
cremental decoding with regular regular beam us-
ing our decoder. To make it comparable with
incremental decoding, we used the regular beam
search to re-decode the sentence fragments for ev-
ery additional word in the input sentence. We
measured the following parameters in our empir-
ical analysis: translation quality (as measured by
BLEU (Papineni et al, 2002) and TER (Snover et
al., 2006)), search errors and translation speed. Fi-
nally, we also measured the effect of different race
course limits on BLEU and decoding speed for in-
cremental decoding.
4.1 Benchmarking our decoder
In this section we compare our decoder with
Moses for regular beam search decoding. Table 1
gives the BLEU and TER for the two language
pairs. Our decoder implementation compares
favourably with Moses for Fr-En: the slightly bet-
ter BLEU and TER for our decoder in Fr-En is
possibly due to the minor differences in the con-
figuration settings. For En-Fr translation, Moses
performs better in both metrics. There are differ-
ences in the beam size between the two decoders,
in our system the beam size is set to 100 compared
to the default value of 1000 (the cube pruning pop
limit) in Moses; we are planning to explore this
and remove any other differences between them.
However based on our understanding of the Moses
implementation and our experiments, we believe
our decoder to be comparable in accuracy with the
Moses implementation. The numbers in the bold-
face are statistically significant at 95% confidence
interval.
4.2 Re-decoding v.s. Incremental decoding
We test our hypothesis that incremental decod-
ing can benefit by using partial decoder states for
decoding every additional word in the input sen-
tence. In order to do this, we run our incremen-
tal decoder in both regular beam search mode and
in incremental decoding mode. In regular beam
search mode, we forced the beam search decoder
to re-decode the sentence fragments for every ad-
ditional word and in incremental decoding mode,
we used the partial decoding states to incremen-
tally decode lastly added word. We then compare
the BLEU and TER scores between them to vali-
date our hypothesis.
We further test effectiveness of delayed prun-
ing (DP) in incremental decoding by comparing
it to the case where we turn off the DP. For in-
cremental decoding, we set the beam size and the
race course limit (for DP) to be 3. Additionally,
we used a threshold of?2.0 (in log-scale) for nor-
malized LM in the delay phase of DP, which was
obtained by testing on a separate development test
set.
We would like to highlight two observations
from the results in Table 2. First the regular beam
search indicate possible search errors due to the
small beam size (cube pruning pop limit) and the
BLEU scores has decreased by 0.56 for Fr-En
and by over 2.5 for En-Fr, than the scores cor-
responding to a beam size of 100 shown in Ta-
ble 1. Secondly, we find the incremental decoding
to perform better for the same beam size. How-
ever, incremental decoding without delay pruning
still seems to incur search errors when compared
with the regular decoding with a larger beam. De-
layed pruning alleviates this issue and improves
the BLEU and TER significantly. This we believe,
is mainly because the strategy to delay the pruning
retains the potentially better partial hypotheses for
every coverage set. It should be noted that results
in Table 2 pertain only to our decoder implemen-
tation and not with Moses.
We now give a comparative note between our
approach and the pruning strategy in regular beam
search. Delaying the hypothesis pruning is the im-
portant aspect in our approach to incremental de-
coding. In the case of regular beam search, the
hypotheses are pruned when they fall out of the
beam and the idea is to have a larger beam size
to avoid the early pruning of potentially good can-
didates. With the advent of cube pruning (Huang
and Chiang, 2007), the ?cube pruning pop limit?
(in Moses) determines the number of hypotheses
retained in each stack. In both the cases, it is pos-
sible that some of the coverage sets go unrepre-
sented in the stack due to poor candidate scores.
This is not desirable in the incremental decoding
setting as this might lead to search errors while
decoding a partial sentence.
Additionally, Moses offers an option (cube
220
Decoder
Fr-En En-Fr
BLEU TER BLEU TER
Moses 26.98 0.551 27.24 0.610
Our decoder 27.53 0.541 26.96 0.657
Table 1: Regular beam search: Moses v.s. Our decoder
Decoder
Fr-En En-Fr
BLEU TER BLEU TER
Re-decode w/ beam search 26.96 0.548 24.33 0.635
ID w/o delay pruning 27.01 0.547 25.00 0.618
ID w/ delay pruning 27.62 0.545 25.45 0.616
Table 2: BLEU and TER: Re-decoding v.s. Incremental Decoding (ID)
pruning diversity) to control the number of hy-
potheses generated for each coverage set (though
set to ?0? by default). It might be possible to use
this in conjunction with cube pruning pop limit as
an alternative to our delayed pruning in the incre-
mental decoding setting (with the risk of combina-
torial explosion in the search space).
In contrast, the delayed pruning not only avoids
search errors but also provides a dynamically man-
ageable search space (refer section 4.2.2) by re-
taining the best of the potential candidates. In a
practical scenario like real-time translation of in-
ternet chat, translation speed is an important con-
sideration. Furthermore, it is better to avoid large
number of candidates and generate only few best
ones, as only the top few translations will be used
by the system. Thus we believe our delayed prun-
ing approach to be a principled pruning strategy
that combines the different factors in an elegant
framework.
4.2.1 Search Errors
As BLEU only indirectly indicates the number
of search errors made by algorithm, we used a
more direct way of quantifying the search errors
incurred by the ID in comparison to regular beam
search. We define the search error to be the differ-
ence between the translation scores of the best hy-
potheses produced by the ID and the regular beam
search and then compute the mean squared error
(MSE) for the entire test set. We use this method
to compare ID in the two settings of delayed prun-
ing being turned off (using a smaller beam size
of 3 to simulate the requirements of near instanta-
neous translations in real-time environments) and
delayed pruning turned on. We compare the model
score in these cases with the model score for the
best result obtained from the regular beam search
decoder (using a larger beam of size 100).
Direction
Beam search against
Incremental Decoding
w/o DP w/ DP
Fr-En 0.3823 0.3235
En-Fr 1.1559 0.6755
Table 3: Search Errors in Incremental Decoding
The results are shown in Table 3 and as can be
clearly seen, ID shows much lesser mean square
error with the DP turned on than when it is turned
off. Together the BLEU and TER numbers and
the mean square search error show that delayed
pruning is useful in the incremental decoding set-
ting. Comparing the En-Fr and Fr-En results show
that the two language pairs show slightly different
characteristics but the experiments in both direc-
tions support our overall conclusions.
4.2.2 Speed
In this experiment, we set out to evaluate the
ID against the regular beam-search in which sen-
tence fragments are incrementally decoded for ad-
ditional words. In order compare with the in-
cremental decoder, we modified the regular de-
coder to decode the partial phrases, so that it re-
decodes the partial phrase from the scratch instead
of reusing the earlier state.
We ran the timing experiments on a Dell ma-
chine with an Intel Core i7 processor and 12 GB
memory, clocking 2.67 GHz and running Linux
(CentOS 5.3). We measured the time taken for de-
coding the fragment with every word added and
221
averaged it first over the sentence and then the en-
tire test set. The average time (in msecs) includes
the future cost computation for both. We also mea-
sured the average number of hypotheses for every
bin at the end of decoding a complete sentence,
which was also averaged over the test set.
The results in Table 4 show that the incremen-
tal decoder was significantly faster than the beam
search in re-decoding mode almost by a factor of
9 in the best case (for Fr-En). The speedup is pri-
marily due to two factors, i) computing the future
cost for the new phrases as opposed to computing
it for all the phrases and ii) using partial decoder
states without having to re-generate hypotheses
through the cube pruning step and the latencies
associated with computing LM scores for them.
The addition of delayed pruning slowed down the
speed at most by 7 msecs (for En-Fr). In addition,
delayed pruning can be seen generating far more
hypotheses than the other two cases. Clearly, this
is because of the delay in pruning the tortoises un-
til the race course limit. Even with such signifi-
cantly large number of hypotheses being retained
for every bin, DP results in improved speed (over
re-decoding from scratch) and better performance
by avoiding search errors (compared to the incre-
mental decoder that does not use DP).
4.3 Effect of Race course limit
Table 5 shows the effect of different race course
limits on translation quality measured using
BLEU. We generally expect the race course limit
to behave similar to the beam size as they both al-
low more hypotheses in the bin thereby reducing
search error although at the expense of increasing
decoding time.
However, in our experiments for Fr-En, we did
not find significant variations in BLEU for differ-
ent race course limits. This could be due to the
absence of long distance re-orderings between En-
glish and French and that the smallest race course
limit of 3 is sufficient for capturing all cases of lo-
cal re-ordering. As expected, we find the decoding
speed to slightly decrease and the average number
of hypotheses per bin to increase with the increas-
ing race course limit.
5 Related Work
Google5 does seem to perform incremental decod-
ing, but the underlying algorithms are not public
5translate.google.com
knowledge. They may be simply re-translating the
input each time using a fast decoder or re-using
prior decoder states as we do here.
Intereactive translation using text prediction
strategies have been studied well (Foster et al,
1997; Foster et al, 2002; Och et al, 2003). They
all attempt to interactively help the human user in
the postediting process, by suggesting completion
of the word/phrase based on the user accepted pre-
fix and the source sentece. Incremental feedback
is part of Caitra (Koehn, 2009) an interactive tool
for human-aided MT and works on a similar set-
ting to interactive MT. In Caitra, the source text
is pre-translated first and during the interactions it
dynamically generates user suggestions.
Our incremental decoder work differs from
these text prediction based approaches, in the
sense that the input text is not available to the de-
coder beforehand and the decoding is being done
dynamically for every source word as opposed to
generating suggestions dynamically for complet-
ing target sentece.
6 Conclusion and Future Work
We presented a modified beam search algorithm
for an efficient incremental decoder (ID), which
will allow translations to be generated incremen-
tally for every word typed by a user, instead of
waiting for the entire sentence as input by reusing
the partial decoder state. Our proposed modifica-
tions help us to efficiently compute partial future
costs in the incremental setting. We introduced the
notion of delayed pruning (DP) to avoid search
errors in incremental decoding. We showed that
reusing the partial decoder states is faster than re-
decoding the input from the scratch every time a
new word is typed by the user. Our exhaustive ex-
periments further demonstrated DP to be highly
effective in avoiding search errors under the in-
cremental decoding setting. In our experiments in
this paper we used a very tight beam size; in fu-
ture work, we would like to explore the tradeoff
between speed, accuracy and the utility of delayed
pruning by varying the beam size in our experi-
ments.
References
George Foster, Pierre Isabelle, and Pierre Plamondon.
1997. Target-text mediated interactive machine
translation. Machine Translation, 12(1/2):175?194.
222
Decoder
Fr-En En-Fr
Avg time Avg Hyp/ bin Avg time Avg Hyp/ bin
Re-decode 724.46 2.21 130.29 2.32
ID w/o DP 84.85 2.89 27.58 2.89
ID w/ DP 87.01 85.11 34.35 60.46
Table 4: Speed: Re-decoding v.s. Incremental Decoding (ID)
Race Fr-En En-Fr
Course
BLEU Avg time Avg Hyp/ bin BLEU Avg time Avg Hyp/ bin
Limit
3 26.75 87.83 85.11 25.39 36.15 75.03
4 26.77 91.14 86.35 25.37 36.21 77.69
5 26.77 90.81 86.52 25.37 36.25 78.47
6 26.77 95.91 86.56 25.37 37.34 78.71
7 26.77 91.67 86.57 25.37 36.26 78.81
Table 5: Effect of different race course limits
George Foster, Philippe Langlais, and Guy Lapalme.
2002. User-friendly text prediction for translators.
In EMNLP ?02: Proceedings of the ACL-02 con-
ference on Empirical methods in natural language
processing, pages 148?155, Morristown, NJ, USA.
Association for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: A beam search
decoder for phrase-based statistical machine trans-
lation models. In Robert E. Frederking and
Kathryn Taylor, editors, AMTA, volume 3265 of Lec-
ture Notes in Computer Science, pages 115?124.
Springer.
Philipp Koehn. 2009. A web-based interactive com-
puter aided translation tool. In In Proceedings of
ACL-IJCNLP 2009: Software Demonstrations, Sun-
tec, Singapore, August.
Franz Josef Och, Richard Zens, and Hermann Ney.
2003. Efficient search for interactive statistical ma-
chine translation. In EACL ?03: Proceedings of the
tenth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 387?
393, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL.
Michael Pust and Kevin Knight. 2009. Faster mt
decoding through pervasive laziness. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 141?144,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas: AMTA 2006.
223
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 533?541,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Bayesian Extraction of Minimal SCFG Rules for
Hierarchical Phrase-based Translation
Baskaran Sankaran
Simon Fraser University
Burnaby BC, Canada
baskaran@cs.sfu.ca
Gholamreza Haffari
Monash University
Melbourne, Australia
reza@monash.edu
Anoop Sarkar
Simon Fraser University
Burnaby BC, Canada
anoop@cs.sfu.ca
Abstract
We present a novel approach for extracting
a minimal synchronous context-free grammar
(SCFG) for Hiero-style statistical machine
translation using a non-parametric Bayesian
framework. Our approach is designed to ex-
tract rules that are licensed by the word align-
ments and heuristically extracted phrase pairs.
Our Bayesian model limits the number of
SCFG rules extracted, by sampling from the
space of all possible hierarchical rules; addi-
tionally our informed prior based on the lex-
ical alignment probabilities biases the gram-
mar to extract high quality rules leading to im-
proved generalization and the automatic iden-
tification of commonly re-used rules. We
show that our Bayesian model is able to ex-
tract minimal set of hierarchical phrase rules
without impacting the translation quality as
measured by the BLEU score.
1 Introduction
Hierarchical phrase-based (Hiero) machine transla-
tion (Chiang, 2007) has attracted significant interest
within the Machine Translation community. It ex-
tends phrase-based translation by automatically in-
ferring a synchronous grammar from an aligned bi-
text. The synchronous context-free grammar links
non-terminals in source and target languages. De-
coding in such systems employ a modified CKY-
parser that is integrated with a language model.
The primary advantage of Hiero-style systems lie
in their unsupervised model of syntax for transla-
tion: allowing long-distance reordering and cap-
turing certain syntactic constructions, particularly
those that involve discontiguous phrases. It has
been demonstrated to be a successful framework
with comparable performance with other statisti-
cal frameworks and suitable for large-scale cor-
pora (Zollmann et al, 2008). However, one of the
major difficulties in Hiero-style systems has been on
learning a concise and general synchronous gram-
mar from the bitext.
While most of the research in Hiero-style sys-
tems is focused on the improving the decoder, and
in particular the link to the language model, compar-
atively few papers have considered the inference of
the probabilistic SCFG from the word alignments.
A majority of the systems employ the classic rule-
extraction algorithm (Chiang, 2007) which extracts
rules by replacing possible sub-spans (permitted by
the word alignments) with a non-terminal and then
using relative frequencies to estimate the probabilis-
tic synchronous context-free grammar. One of the
issues in building Hiero-style systems is in manag-
ing the size of the synchronous grammar. The origi-
nal approach extracts a larger number of rules when
compared to a phrase-based system on the same data
leading to practical issues in terms of memory re-
quirements and decoding speed.
Extremely large Hiero phrase tables may also lead
to statistical issues, where the probability mass has
to be shared by more rules: the probability p(e|f)
has to be shared by all the rules having the same
source side string f , leading to fragmentation and
resulting in many rules having very poor probability.
Approaches to improve the inference (the induc-
tion of the SCFG rules from the bitext) typically
follows two streams. One focusses on filtering the
extracted hierarchical rules either by removing re-
dundancy (He et al, 2009) or by filtering rules
based on certain patterns (Iglesias et al, 2009),
while the other stream is concerned about alterna-
tive approaches for learning the synchronous gram-
mar (Blunsom et al, 2008; Blunsom et al, 2009; de
Gispert et al, 2010). This paper falls under the lat-
ter category and we use a non-parametric Bayesian
approach for rule extraction for Hiero-style systems.
Our objective in this paper is to provide a principled
533
rule extraction method using a Bayesian framework
that can extract the minimal SCFG rules without re-
ducing the BLEU score.
2 Motivation and Related Work
The large number of rules in Hiero-style systems
leads to slow decoding and increased memory re-
quirements. The heuristic rule extraction algo-
rithm (Chiang, 2007) introduces redundant mono-
tone composed rules (He et al, 2009) in the SCFG
grammar. The research on Hiero rule extraction falls
into two broad categories: i) rule reduction by elim-
inating a subset of rules extracted by the heuristic
approach and ii) alternate approaches for rule extrac-
tion.
There have been approaches to reduce the size of
Hiero phrase table, without significantly affecting
the translation quality. He et. al. (2009) proposed the
idea of discarding monotone composed rules from
the phrase table that can instead be obtained dynami-
cally by combining the minimal rules in the same or-
der. They achieve up to 70% reduction in the phrase
table by discarding these redundant rules, without
appreciable reduction in the performance as mea-
sured by BLEU. Empirically analyzing the effective-
ness of specific rule patterns, (Iglesias et al, 2009)
show that some patterns having over 95% of the to-
tal SCFG rules can be safely eliminated without any
reduction in the BLEU score.
Along a different track, some prior works have
employed alternate rule extraction approaches using
a Bayesian framework (DeNero et al, 2008; Blun-
som et al, 2008; Blunsom et al, 2009). (DeNero
et al, 2008) use a Maximum likelihood model of
learning phrase pairs (Marcu and Wong, 2002), but
use sampling to compute the expected counts of the
phrase pairs for the E-step. Other recent approaches
use Gibbs sampler for learning the SCFG by explor-
ing a fixed grammar having pre-defined rule tem-
plates (Blunsom et al, 2008) or by reasoning over
the space of derivations (Blunsom et al, 2009).
We differ from earlier Bayesian approaches in that
our model is guided by the word alignments to rea-
son over the space of the SCFG rules and this re-
stricts the search space of our model. We believe
the word alignments to encode information, useful
for identifying the good phrase-pairs. For example,
several attempts have been made to learn a phrasal
translation model directly from the bitext without
the word alignments (Marcu and Wong, 2002; DeN-
ero et al, 2008; Blunsom et al, 2008), but without
any clear breakthrough that can scale to larger cor-
pora.
Our model exploits the word alignment informa-
tion in the form of lexical alignment probability in
order to construct an informative prior over SCFG
rules and it moves away from a heuristic framework,
instead using a Bayesian non-parametric model to
infer a minimal, high-quality grammar from the
data.
3 Model
Our model is based on similar assumptions as the
original Hiero system. We assume that the bitext has
been word aligned, and that we can use that word
alignment to extract phrase pairs.
Given the word alignments and the heuristically
extracted phrase pairs Rp, our goal is to extract the
minimal set of hierarchical rules Rg that would best
explain Rp. This is achieved by inferring a distribu-
tion over the derivations for each phrase pair, where
the set of derivations collectively specify the gram-
mar. In the following, we denote the sequence of
derivations for the set of phrase pairs by r, which is
composed of grammar rules r. We will essentially
read off our learned grammar from the sequence of
derivations r.
Our non-parametric model reasons over the space
of the (hierarchical and terminal) rules and sam-
ples a set of rules by employing a prior based on
the alignment probability of the words in the phrase
pairs. We hypothesize that the resulting grammar
will be compact and also will explain the phrase
pairs better (the SCFG rules will maximize the like-
lihood of producing the entire set of observed phrase
pairs).
Using Bayes? rule, the posterior over the deriva-
tions r given the phrase pairs Rp can be written as:
P (r|Rp) ? P (Rp|r)P (r) (1)
where P (Rp|r) is equal to one when the sequence
of rules r and phrase-pairs Rp are consistent, i.e. r
can be partitioned into derivations to compose the
set of phrase-pairs such that the derivations respect
534
the given word alignments; otherwise P (Rp|r) is
zero. The overall structure of the model is analo-
gous to the Bayesian model for inducing Tree Sub-
stitution Grammars proposed by Cohn et al (2009).
Note that, our model extracts hierarchical rules for
the word-aligned phrase pairs and not for the sen-
tences.
Similar to the other Hiero-style systems, we use
two types of rules: terminal and hierarchical rules.
For each phrase-pair, our model either generates a
terminal rule by not segmenting the phrase-pair, or
decides to segment the phrase-pair and extract some
rules.
Though it is possible to segment phrase-pairs by
two (or more) non-overlapping spans, we propose
a simpler model in this paper and restrict the hierar-
chical rules to contain only one non-terminal (unlike
the case of classic Hiero-style grammars containing
two non-terminals). This simpler model, samples
the space of derivations and identifies a sub-span
for introducing the non-terminal, which can be ex-
pressed as terminal rules (it is not decomposed fur-
ther). Figure 1 shows an example phrase-pair with
the Viterbi-best word alignment and Figure 2 shows
two possible derivations for the same phrase-pair
with the non-terminals introduced at different sub-
spans. It can be seen that the sub-phrase correspond-
ing to the non-terminal spanX1 is directly written as
a terminal rule and is not decomposed further.
While the resulting model is slightly weaker than
the original Hiero grammar, it should be noted our
simpler model does allow reordering and discontigu-
ous alignments. For example our model includes
rules such as, X ? (?X1?, ????X1), which can
capture phrases like (not X1, ne X1 pas) in the case
of English-French translation. In terms of the re-
ordering, our model lies in between the hierarchi-
cal phrase-based and phrase-based models. To sum-
marize, the segmentation of each phrase-pair in our
model results in two rules: a hierarchical rule with
one nonterminal as well as a terminal rule.
More specifically, the generative process for gen-
erating a phrase pair x from the grammar rules
may have two steps as follows. In the first step,
the model decides on the type of the rule tx ?
{TERMINAL,HIERARCHICAL} used to generate the
phrase-pair based on a Bernoulli distribution, having
a prior ? coming from a Beta distribution:
tx ? Bernoulli(?)
? ? Beta(lx, 0.5)
The lexical alignment probability lx controls the
tendency for extracting hierarchical rules from the
phrase-pair x. For a given phrase-pair, lx is com-
puted by taking the (geometric or arithmetic) aver-
age of the reverse and forward alignment probabil-
ities, which we explain later in this section. Inte-
grating out ? gives us the conditional probabilities
of choosing the rule type tx as:
p(tterm|x) ? n
x
term + lx (2)
p(thier|x) ? n
x
hier + 0.5 (3)
where nxterm and n
x
hier denote the number of termi-
nal or hierarchical rules, among the rules extracted
so far from the phrase-pair x during the sampling.
In the second step, if the rule type tx =
HIERARCHICAL, the model generates the phrase-
pair by sampling from the hierarchical and terminal
rules. We use a Dirichlet Process (DP) to model the
generation of hierarchical rules r:
G ? DP (?h, P0(r))
r ? G
Integrating out the grammar G, the predictive dis-
tribution of a hierarchical rule rx for generating the
current phrase-pair (conditioned on the rules from
the rest of the phrase-pairs) is:
p(rx|r
?x, ?h, P0) ? n
?x
rx + ?hP0(rx) (4)
where n?xrx is the count of the rule rx in the rest of
the phrase-pairs that is represented by r?x, P0 is the
base measure, and ?h is the concentration parameter
controlling the model?s preference towards using an
existing hierarchical rule from the cache or to create
a new rule sanctioned by the base distribution. We
use the lexical alignment probabilities of the compo-
nent rules as our base measure P0:
P0(r) =
[( ?
(k,l)?a
p(el|fk)
) 1
|a|
( ?
(k,l)?a
p(fk|el)
) 1
|a|
] 1
2
(5)
535
octavo y noveno Fondos Europeos de Desarrollo para el ejercicio
Eighth and Ninth European Development Funds for the financial year
Figure 1: An example phrase-pair with Viterbi alignments
X ? (Eighth and Ninth X1 for the financial year, octavo y noveno X1 para el ejercicio)
X ? (European Development Funds, Fondos Europeos de Desarrollo)
X ? (Eighth and Ninth X1, octavo y noveno X1)
X ? (European Development Funds for the financial year,
Fondos Europeos de Desarrollo para el ejercicio)
Figure 2: Two possible derivations of the phrase-pair in Figure 1
where a is the set of alignments in the given sub-
span; if the sub-span has multiple Viterbi alignments
from different phrase-pairs, we consider the union of
all such alignments. DeNero et al (2008) use a sim-
ilar prior- geometric mean of the forward and reverse
IBM-1 alignments. However, we use the product of
geometric means of the forward and reverse align-
ment scores. We also experimented with the arith-
metic mean of the lexical alignment probabilities.
The lexical prior lx in the first step can be defined
similarly. We found the particular combination of,
?arithmetic mean? for the lexical prior lx (in the first
step) and ?geometric mean? for the base distribution
P0 (in the second step) to work better, as we discuss
later in Section 5.
Assuming the heuristically extracted phrase pairs
to be the input to our inference algorithm, our
approach samples the space of rules to find the
best possible segmentation for the sentences as de-
fined by the cache and base distribution. We ex-
plore a subset of the space of rules being consid-
ered by (Blunsom et al, 2009) ? i.e., only those
rules satisfying the word alignments and heuristi-
cally grown phrase alignments.
4 Inference
We train our model by using a Gibbs sampler ? a
Markov Chain Monte Carlo (MCMC) method for
sampling one variable in the model, conditional to
the other variables. The sampling procedure is re-
peated for what is called a long Gibbs chain span-
ning several iterations, while the counts are collected
at fixed thin intervals in the chain. As is common in
the MCMC procedures, we ignore samples from a
fixed number of initial burn-in iterations, allowing
the model to move away from the initial bias. The
rules in the final sampler state at the end of the Gibbs
chain along with their counts averaged by the num-
ber of thin iterations become our translation model.
In our model, a sample for a given phrase pair
corresponds either to its terminal derivation or two
rules in a hierarchical derivation. The model sam-
ples a derivation from the space of derivations that
are consistent with the word alignments. In order
to achieve this, we need an efficient way to enumer-
ate the derivations for a phrase pair such that they
are consistent with the alignments. We use the lin-
ear time algorithm to maximally decompose a word-
aligned phrase pair, so as to encode it as a compact
alignment tree (Zhang et al, 2008).
f0 f1 f2 f3 f4
e0 e1 e2 e3 e4 e5
Figure 3: Example phrase pair with alignments.
536
For a phrase-pair with a given alignment as shown
in Figure 3, Zhang et al (2008) generalize theO(n+
K) time algorithm for computing all K common in-
tervals of two different permutations of length n.
The contiguous blocks of the alignment are cap-
tured as the nodes in the alignment tree and the tree
structure for the example phrase pair in Figure 3 is
shown in Figure 4. The italicized nodes form a left-
branching chain in the alignment tree and the sub-
spans of this chain also lead to alignment nodes that
are not explicitly captured in the tree (Please refer
to Zhang et al (2008) for details). In our work, each
node in the tree (and also each sub-span in the left-
branching chain) corresponds to an aligned source-
target sub-span within the phrase-pair, and is a po-
tential site for introducing the non-terminal X to
generate hierarchical rules.
Given this alignment tree for a phrase pair, a
derivation can be obtained by introducing a non-
terminal at some node nd in the tree and re-writing
the span rooted at nd as a separate rule. As men-
tioned earlier, we compute the derivation probability
as a product of the probabilities of the component
rules, which are computed using the Equation 4.
We initialize the sampler by using our lexical
alignment prior and sampling from the distribution
of derivations as suggested by the priors. We found
this to perform better in practice, than a naive sam-
pler without an initializer.
At each iteration, the Gibbs sampler processes the
phrase pairs in random order. For each phrase pair
Rp, it visits the nodes in the corresponding align-
ment tree and computes the posterior probability of
the derivations and samples from this posterior dis-
tribution. To speedup the sampling, we store the
pre-computed alignment tree for the phrase pairs and
just recompute the derivation probabilities based on
the sampler state at every iteration. While the sam-
pler state is updated with the counts at each iteration,
we accumulate the counts only at fixed intervals in
the Gibbs chain. In applying the model for decoding,
we use the grammar from the final sampler state.
Since our model includes only one hyperparam-
eter ?h, we tune its value manually by empirically
experimenting on a small set of initial phrase pairs.
We keep for future work the task of automatically
tuning for hyper-parameter values by sampling.
([0,5],[0,4])
([0,2],[0,2])
([0,1],[0,1])
([0,0],[0,0]) ([1,1],[1,1])
([2,2],[2,2])
([4,5],[3,4])
Figure 4: Decomposed alignment tree for the example
alignment in Fig. 3.
5 Experiments
We use the English-Spanish data from WMT-10
shared task for the experiments to evaluate the effec-
tiveness of our Bayesian rule extraction approach.
We used the entire shared task training set except
the UN data for training translation model and the
language model was trained with the same set and
an additional 2 million sentences from the UN data,
using SRILM toolkit with Knesser-Ney discounting.
We tuned the feature weights on the WMT-10 dev-
set using MERT (Och, 2003) and evaluate on the
test set by computing lower-cased BLEU score (Pa-
pineni et al, 2002) using the WMT-10 standard eval-
uation script.
We use Kriya ? an in-house implementation of hi-
erarchical phrase-based translation written predom-
inantly in Python. Kriya supports the entire transla-
tion pipeline of SCFG rule extraction and decoding
with cube pruning (Huang and Chiang, 2007) and
LM integration (Chiang, 2007). We use the 7 fea-
tures (4 translation model features, extracted rules
penalty, word penalty and language model) as is typ-
ical in Hiero-style systems. For tuning the feature
weights, we have adapted the MERT implementa-
tion in Moses1 for use with Kriya as the decoder.
We started by training and evaluating the two
baseline systems using i) two non-terminals and
ii) one non-terminal, which were trained using the
conventional heuristic extraction approach. For the
baseline with one non-terminal, we modified the
heuristic rule extraction algorithm appropriately2.
1www.statmt.org/moses/
2Given an initial phrase pair, the algorithm would introduce
a non-terminal for each sub-span consistent with the alignments
and extract rules corresponding to each sub-span. The con-
537
Experiment
# of rules filtered
for devset
(in millions)
BLEU
Baseline (w/ 2 non-terminals) 52.36 27.45
Baseline (w/ 1 non-terminal) 22.09 26.71
Pattern-based filtering? 18.78 24.61
1 non-terminal; monotone & non-monotone 10.36 24.17
1 non-terminal; non-monotone 3.62 23.99
Table 1: Kriya: Baseline and Filtering experiments. ?: This is the initial rule set used in Iglesias et al (2009) obtained
by greedy filtering. Rows 4 and 5 represents the filtering that uses single non-terminal rules with row 4 allowing
monotone rules in addition to the non-monotone (reordering) rules.
As part of the baseline methods to be applied to min-
imize the number of SCFG rules, We also wanted to
assess the effect of a simpler rule filtering, where
the idea is to filter the heuristically extracted rules
based on certain patterns. Our first baseline filtering
strategy uses the heuristic methods in Iglesias et al
(2009) in order to minimize the number of rules3.
For the other baseline filtering experiments, we re-
tained only one non-terminal rules and then further
limited it by retaining only non-monotone one non-
terminal rules; in both cases the terminal rules were
retained.
Table 1 shows the results for baseline and the rule
filtering experiments. Restricting rule extraction to
just one non-terminal doesn?t affect the BLEU score
significantly and this justifies the simpler model
used in this paper. Secondly, we find significant re-
duction in the BLEU for the pattern-based filtering
strategy and this is because we only use the initial
rule set obtained by greedy filtering without aug-
menting it with other specific patterns. The other
two filtering methods reduced the BLEU further but
not significantly. The second column in the table
gives the number of SCFG rules filtered for the dev-
set, which is typically much less than the full set of
rules. We later use this to put in perspective the
effective reduction in the model size achieved by
our Bayesian model. We can ideally compare our
Bayesian rule extraction using Gibbs sampling with
straints relating to two non-terminals (such as, no adjacent non-
terminals in source side) does not apply for the one non-terminal
case.
3It should be noted that we didn?t use the augmentations to
the initial rule set (Iglesias et al, 2009) and our objective is to
find the impact of the filtering approaches.
the baselines and the filtering approaches. However,
running our Gibbs sampler on the full set of phrase
pairs demand sampling to be distributed, possibly
with approximation (?; ?), which we reserve for our
future work.
In this work, we focus on evaluating our Gibbs
sampler on reasonable sized set of phrase pairs with
corresponding baselines. We filter the initial phrase
pairs based on their frequency using three different
thresholds, viz. 20, 10 and 3- resulting in smaller
sets of initial phrase pairs because we throw out in-
frequent phrase pairs (the threshold-20 case is the
smallest initial set of phrase pairs). This allows us
to run our sampler as a stand-alone instance for the
three sets, obviating the need for distributed sam-
pling.
Table 2 shows the number of unique phrase pairs
in each set. While, the filtering reduces the number
of phrase pairs to a small fraction of the total phrase
pairs, it also increases the unknown words (OOV)
in the test set by a factor between 1.8 and 3. In or-
der to address this issue due to the OOV words, we
additionally added non-decomposable phrase pairs
having just one word at either source or target side,
Phrase-pairs set
# of Unique
phrase-pairs
Testset
OOV
All phrase-pairs 110782174 1136
Threshold-20 292336 3735
Threshold-10 606590 3056
Threshold-3 2689855 2067
Table 2: Phrase-pair statistics for different frequency
threshold
538
Experiment Threshold-20 Threshold-10 Threshold-3
Baseline (w/ 2 non-terminals) 24.30 25.96 26.34
Baseline (w/ 1 non-terminal) 24.00 25.90 26.83
Bayesian rule extraction 23.39 24.30 25.22
Table 3: BLEU scores: Heuristic vs Bayesian rule extraction
Experiment Rules Extracted (in millions) Reduction
Heuristic (1 nt) Bayesian
Threshold-20 1.93 (0.117) 1.86 (0.07) 3.57 (38.34)
Threshold-10 2.91 (1.09) 2.10 (0.28) 27.7 (73.95)
Threshold-3 7.46 (5.64) 2.45 (0.71) 67.17 (87.28)
Table 4: Model compression: Heuristic vs Bayesian rule extraction
Priors ?h BLEU
Arith + Arith means 0.5 22.46
Arith + Geom means 0.5 23.39
Geom + Arith means 0.5 22.96
Arith + Geom means 0.5 22.83
Arith + Geom means 0.1 22.88
Arith + Geom means 0.2 22.97
Arith + Geom means 0.3 22.98
Arith + Geom means 0.4 22.69
Arith + Geom means 0.5 23.39
Arith + Geom means 0.6 22.89
Arith + Geom means 0.7 22.82
Arith + Geom means 0.8 22.82
Arith + Geom means 0.9 22.67
Table 5: Effect of different priors and ?h on Threshold-
20 set. The two priors correspond to the lexical prior lx
in the first step and the base distribution P0 in the second
step.
as coverage rules. The coverage rules (about 1.8
million) were added separately to the SCFG rules
induced by both heuristic algorithm and Gibbs sam-
pler. This is justified because we only add the rules
that can not be decomposed further by both rule ex-
traction approaches. However, note that both ap-
proaches can independently induce rules that over-
lap with the coverage rules set and in such cases we
simply add the original corpus count to the counts
returned by the respective rule extraction method.
The Gibbs sampler considers the phrase pairs in
random order at each iteration and induces SCFG
rules by sampling a derivation for each phrase pair.
Given a phrase pair x with raw corpus frequency fx,
we simply scale the count for its sampled deriva-
tion r by its frequency fx. Alternately, we also ex-
perimented with independently sampling for each
instance of the phrase pair and found their perfor-
mances to be comparable. Sampling phrase pairs
once and then scaling the sampled derivation, help
us to speed up the sampling process. In our experi-
ments, we ran the Gibbs sampler for 2000 iterations
with a burn-in period of 200, collecting counts every
50 iterations. We set the concentration parameter ?h
to be 0.5 based on our experiments detailed later in
this section.
The BLEU scores for the SCFG learned from the
Gibbs sampler are shown in Table 3. We first note
that, the threshold-20 set has lower baseline BLEU
than threshold-10 and threshold-3 sets, as can be ex-
pected because threshold-20 set uses a much smaller
subset of the full set of phrase pairs to extract hier-
archical rules. The Bayesian approach results in a
maximum BLEU score reduction of 1.6 for the sets
using thresholds 10 and 3, compared to the one non-
terminal baseline. The two non-terminal baseline is
also provided to place our results in perspective.
Table 4 shows the model size, including the cov-
erage rules for the two rule extraction approaches.
The number of extracted rules, excluding the cov-
erage rules are shown within the parenthesis. The
last column shows the reduction in the model size
for both with and without the coverage rules; yield-
ing a maximum absolute reduction of 67.17% for the
539
threshold-3 phrase pairs set. It can be seen that the
number of rules are far fewer than the rules extracted
using the baseline heuristic methods for filtering de-
tailed in Table 1. Interestingly, we obtain a smaller
model size, even as we decrease the threshold to in-
clude more initial phrase pairs used as input to the
inference procedure, e.g. a 67.17% reduction over
the rules extracted from the threshold-3 phrase pairs
v.s. a 27.7% reduction for threshold-10.
These results show that our model is capable of
extracting high-value Hiero-style SCFG rules, albeit
with a reduction in the BLEU score. However, our
current approach offers scope for improvement in
several avenues, for example we can use annealing
to perturb the initial sampling iterations to encour-
age the Gibbs sampler to explore several derivations
for each phrase pair. Though this might result in
slightly large models than the current ones, we still
expect substantial reduction than the original Hiero
rule extraction. In future, we also plan to sample the
hyperparameter ?h, instead of using a fixed value.
Table 5 shows the effect of different values of
the concentration parameter ?h and the priors used
in the model. The order of priors in each setting
correspond to the prior used in deciding the rule-
type and identifying the non-terminal span for sam-
pling a derivation. We found the geometric mean to
work better in both cases. We further found that the
concentration parameter ?h value 0.5 gives the best
BLEU score.
6 Conclusion and Future Work
We proposed a novel method for extracting mini-
mal set of hierarchical rules using non-parametric
Bayesian framework. We demonstrated substantial
reduction in the size of extracted grammar with the
best case reduction of 67.17%, as compared to the
heuristic approach, albeit with a slight reduction in
the BLEU scores.
We plan to extend our model to handle two non-
terminals to allow for better reordering. We also
plan to run our sampler on the full set of phrase
pairs using distributed sampling and our prelimi-
nary results in this direction are encouraging. Fi-
nally, we would like to directly sample from the
Viterbi aligned sentence pairs instead of relying on
the heuristically extracted phrase pairs. This can
be accomplished by using a model that is closer
to the Tree Substitution Grammar induction model
in (Cohn et al, 2009) but in our case the model
would infer a Hiero-style SCFG from word-aligned
sentence pairs.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of Neural Information Processing Systems-
08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of Association of
Computational Linguistics-09, pages 782?790. Asso-
ciation for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: North American Chapter of the Association
for Computational Linguistics-09, pages 548?556. As-
sociation for Computational Linguistics.
Adria` de Gispert, Juan Pino, and William Byrne. 2010.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 545?554.
Association for Computational Linguistics.
John DeNero, Alexandre Bouchard-Cote, and Klein Dan.
2008. Sampling alignment structure under a bayesian
translation model. In In Proceedings of Empirical
Methods in Natural Language Processing-08, pages
314?323. Association for Computational Linguistics.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding
monotone composed rule for hierarchical phrase-based
statistical machine translation. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 25?29. ACM.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151.
Association for Computational Linguistics.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern for
efficient hierarchical translation. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 380?388. Association for Com-
putational Linguistics.
540
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In In Proceedings of Empirical Methods in Natu-
ral Language Processing-02, pages 133?139. Associ-
ation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In In Proceedings of
Association of Computational Linguistics, pages 311?
318. Association for Computational Linguistics.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING) - Volume 1, pages 1081?1088. As-
sociation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING) - Vol-
ume 1, pages 1145?1152. Association for Computa-
tional Linguistics.
541
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 356?361,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Kriya - The SFU System for Translation Task at WMT-12
Majid Razmara and Baskaran Sankaran and Ann Clifton and Anoop Sarkar
School of Computing Science
Simon Fraser University
8888 University Drive
Burnaby BC. V5A 1S6. Canada
{razmara, baskaran, aca69, anoop}@cs.sfu.ca
Abstract
This paper describes our submissions for the
WMT-12 translation task using Kriya - our hi-
erarchical phrase-based system. We submitted
systems in French-English and English-Czech
language pairs. In addition to the baseline sys-
tem following the standard MT pipeline, we
tried ensemble decoding for French-English.
The ensemble decoding method improved the
BLEU score by 0.4 points over the baseline
in newstest-2011. For English-Czech, we seg-
mented the Czech side of the corpora and
trained two different segmented models in ad-
dition to our baseline system.
1 Baseline Systems
Our shared task submissions are trained in the hier-
archical phrase-based model (Chiang, 2007) frame-
work. Specifically, we use Kriya (Sankaran et al,
2012) - our in-house Hiero-style system for training
and decoding. We now briefly explain the baseline
systems in French-English and English-Czech lan-
guage pairs.
We use GIZA++ for word alignments and the
Moses (Koehn et al, 2007) phrase-extractor for ex-
tracting the initial phrases. The translation models
are trained using the rule extraction module in Kriya.
In both cases, we pre-processed the training data by
running it through the usual pre-processing pipeline
of tokenization and lowercasing.
For French-English baseline system, we trained
a simplified hierarchical phrase-based model where
the right-hand side can have at most one non-
terminal (denoted as 1NT) instead of the usual two
non-terminal (2NT) model. In our earlier experi-
ments we found the 1NT model to perform com-
parably to the 2NT model for close language pairs
such as French-English (Sankaran et al, 2012) at the
same time resulting in a smaller model. We used the
shared-task training data consisting of Europarl (v7),
News commentary and UN documents for training
the translation models having a total of 15 M sen-
tence pairs (we did not use the Fr-En Giga paral-
lel corpus for the training). We trained a 5-gram
language model for English using the English Gi-
gaword (v4).
For English-Czech, we trained a standard Hiero
model that has up to two non-terminals on the right-
hand side. We used the Europarl (v7), news com-
mentary and CzEng (v0.9) corpora having 7.95M
sentence pairs for training translation models. We
trained a 5-gram language model using the Czech
side of the parallel corpora and did not use the Czech
monolingual corpus.
The baseline systems use the following 8 stan-
dard Hiero features: rule probabilities p(e|f) and
p(f |e); lexical weights pl(e|f) and pl(f |e); word
penalty, phrase penalty, language model and glue
rule penalty.
1.1 LM Integration in Kriya
The kriya decoder is based on a modified CYK al-
gorithm similar to that of Chiang (2007). We use
a novel approach in computing the language model
(LM) scores in Kriya, which deserves a mention
here.
The CKY decoder in Hiero-style systems can
freely combine target hypotheses generated in inter-
356
mediate cells with hierarchical rules in the higher
cells. Thus the generation of the target hypotheses
are fragmented and out of order in Hiero, compared
to the left to right order preferred by n-gram lan-
guage models.
This leads to challenges in estimating LM scores
for partial target hypotheses and this is typically ad-
dressed by adding a sentence initial marker (<s>)
to the beginning of each derivation path.1 Thus the
language model scores for the hypothesis in the in-
termediate cell are approximated, with the true lan-
guage model score (taking into account sentence
boundaries) being computed in the last cell that
spans the entire source sentence.
Kriya uses a novel idea for computing LM scores:
for each of the target hypothesis fragment, it finds
the best position for the fragment in the final sen-
tence and uses the corresponding score. Specifi-
cally, we compute three different scores correspond-
ing to the three states where the fragment can end
up in the final sentence, viz. sentence initial, middle
and final and choose the best score. Thus given a
fragment tf consisting of a sequence of target to-
kens, we compute LM scores for (i) <s> tf , (ii)
tf and (iii) tf </s> and use the best score (only)
for pruning.2 While this increases the number of
LM queries, we exploit the language model state in-
formation in KenLM (Heafield, 2011) to optimize
the queries by saving the scores for the unchanged
states. Our earlier experiments showed significant
reduction in search errors due to this approach, in
addition to a small but consistent increase in BLEU
score (Sankaran et al, 2012).
2 French-English System
In addition to the baseline system, we also trained
separate systems for News and Non-News genres
for applying ensemble decoding (Razmara et al,
2012). The news genre system was trained only us-
ing the news-commentary corpus (about 137K sen-
1Alternately systems add sentence boundary markers (<s>
and </s>) to the training data so that they are explicitly present
in the translation and language models. While this can speed
up the decoding as the cube pruning is more aggressive, it also
limits the applicability of rules having the boundary contexts.
2This ensures the the LM score estimates are never underes-
timated for pruning. We retain the LM score for fragment (case
ii) for estimating the score for the full candidate sentence later.
tence pairs) and the non-news genre system was
trained on the Europarl and UN documents data
(14.8M sentence pairs). The ensemble decoding
framework combines the models of these two sys-
tems dynamically when decoding the testset. The
idea is to effectively use the small amount of news
genre data in order to maximize the performance on
the news-based testsets. In the following sections,
we explain in broader detail how this system combi-
nation technique works as well as the details of this
experiment and the evaluation results.
2.1 Ensemble Decoding
In the ensemble decoding framework we view trans-
lation task as a domain mixing problem involving
news and non-news genres. The official training
data is from two major sources: news-commentary
data and Europarl/UN data and we hope to exploit
the distinctive nature of the two genres. Given that
the news data is smaller comparing to parliamen-
tary proceedings data, we could tune the ensemble
decoding to appropriately boost the weight for the
news genre mode during decoding. The ensemble
decoding approach (Razmara et al, 2012) takes ad-
vantage of multiple translation models with the goal
of constructing a system that outperforms all the
component models. The key strength of this system
combination method is that the systems are com-
bined dynamically at decode time. This enables the
decoder to pick the best hypotheses for each span of
the input.
In ensemble decoding, given a number of transla-
tion systems which are already trained and tuned, all
of the hypotheses from component models are used
in order to translate a sentence. The scores of such
rules are combined in the decoder (i.e. CKY) using
various mixture operations to assign a single score to
them. Depending on the mixture operation used for
combining the scores, we would get different mix-
ture scores.
Ensemble decoding extends the log-linear frame-
work which is found in state-of-the-art machine
translation systems. Specifically, the probability of
a phrase-pair (e?, f?) in the ensemble model is:
p(e? | f?) ? exp
(
w1 ? ?1? ?? ?
1st model
? w2 ? ?2? ?? ?
2nd model
? ? ? ?
)
357
where? denotes the mixture operation between two
or more model scores.
Mixture operations receive two or more scores
(probabilities) and return the mixture score (prob-
ability). In this section, we explore different options
for this mixture operation.
Weighted Sum (wsum): in wsum the ensemble
probability is proportional to the weighted sum
of all individual model probabilities.
p(e? | f?) ?
M?
m
?m exp
(
wm ? ?m
)
where m denotes the index of component mod-
els, M is the total number of them and ?i is the
weight for component i.
Weighted Max (wmax): where the ensemble score
is the weighted max of all model scores.
p(e? | f?) ? max
m
(
?m exp
(
wm ? ?m
))
Product (prod): in prod, the probability of the en-
semble model or a rule is computed as the prod-
uct of the probabilities of all components (or
equally the sum of log-probabilities). When
using this mixture operation, ensemble de-
coding would be a generalization of the log-
linear framework over multiple models. Prod-
uct models can also make use of weights to
control the contribution of each component.
These models are generally known as Logarith-
mic Opinion Pools (LOPs) where:
p(e? | f?) ? exp
(
M?
m
?m wm ? ?m
)
Model Switching: in model switching, each cell in
the CKY chart gets populated only by rules
from one of the models and the other mod-
els? rules are discarded. This is based on the
hypothesis that each component model is an
expert on different parts of sentence. In this
method, we need to define a binary indicator
function ?(f? ,m) for each span and component
model.
?(f? ,m) =
?
?
?
1, m = argmax
n?M
?(f? , n)
0, otherwise
The criteria for choosing a model for each cell,
?(f? , n), could be based on:
Max: for each cell, the model that has the
highest weighted top-rule score wins:
?(f? , n) = ?n max
e
(wn ? ?n(e?, f?))
Sum: Instead of comparing only the score of
the top rules, the model with the high-
est weighted sum of the probability of
the rules wins (taking into account the
ttl(translation table limit) limit on the
number of rules suggested by each model
for each cell):
?(f? , n) = ?n
?
e?
exp
(
wn ? ?n(e?, f?)
)
The probability of each phrase-pair (e?, f?) is
computed as:
p(e? | f?) =
?
m
?(f? ,m) pm(e? | f?)
Since log-linear models usually look for the best
derivation, they do not need to normalize the scores
to form probabilities. Therefore, the scores that dif-
ferent models assign to each phrase-pair may not be
in the same scale. Therefore, mixing their scores
might wash out the information in one (or some)
of the models. We applied a heuristic to deal with
this problem where the scores are normalized over
a shorter list. So the list of rules coming from each
model for a certain cell in the CKY chart is normal-
ized before getting mixed with other phrase-table
rules. However, experiments showed using normal-
ized scores hurts the BLEU score radically. So we
use the normalized scores only for pruning and for
mixing the actual scores are used.
As a more principled way, we used a toolkit,
CONDOR (Vanden Berghen and Bersini, 2005), to
optimize the weights of our component models on
a dev-set. CONDOR, which is publicly available, is
a direct optimizer based on Powell?s algorithm that
does not require explicit gradient information for the
objective function.
2.2 Experiments and Results
As mentioned earlier all the experiments reported
for French-English use a simpler Hiero translation
358
Method Devset Test-11 Test-12
Baseline Hiero 26.03 27.63 28.15
News data 24.02 26.47 26.27
Non-news data 26.09 27.87 28.15
Ensemble PROD 25.66 28.25 28.09
Table 1: French-English BLEU scores. Best performing
setting is shown in Boldface.
model having at most one non-terminal (1NT) on the
right-hand side. We use 7567 sentence pairs from
news-tests 2008 through 2010 for tuning and use
news-test 2011 for testing in addition to the 2012
test data. The feature weights were tuned using
MERT (Och, 2003) and we report the devset (IBM)
BLEU scores and the testset BLEU scores computed
using the official evaluation script (mteval-v11b.pl).
The results for the French-English experiments
are reported in Table 1. We note that both baseline
Hiero model and the model trained from the non-
news genre get comparable BLEU scores. The news
genre model however gets a lesser BLEU score and
this is to be expected due to the very small training
data available for this genre.
Table 2 shows the results of applying various mix-
ture operations on the devset and testset, both in nor-
malized (denoted by Norm.) and un-normalized set-
tings (denoted by Base). We present results for these
mixture operations using uniform weights (i.e. un-
tuned weights) and for PROD we also present the
results using the weights optimized by CONDOR.
Most of the mixture operations outperform the Test-
11 BLEU of the baseline models (shown in Table 1)
even with uniform (untuned) weights. We took the
best performing operation (i.e. PROD) and tuned its
component weights using our optimizer which lead
to 0.26 points improvement over its uniform-weight
version.
The last row in Table 1 reports the BLEU score
for this mixture operation with the tuned weights
on the Test-12 dataset and it is marginally less than
the baseline model. While this is disappointing, this
also runs counter to our empirical results from other
datasets. We are currently investigating this aspect
as we hope to improve the robustness and applicabil-
ity of our ensemble approach for different datasets
and language pairs.
Mix. Operation Weights Base Norm.
WMAX uniform 27.67 27.94
WSUM uniform 27.72 27.95
SWITCHMAX uniform 27.96 26.21
SWITCHSUM uniform 27.98 27.98
PROD uniform 27.99 28.09
PROD optimized 28.25 28.11
Table 2: Applying ensemble decoding with different mix-
ture operations on the Test-11 dataset. Best performing
setting is shown in Boldface.
3 English-Czech System
3.1 Morpheme Segmented Model
For English-Czech, we additionally experimented
using morphologically segmented versions of the
Czech side of the parallel data, since previous
work (Clifton and Sarkar, 2011) has shown that seg-
mentation of morphologically rich languages can
aid translation. To derive the segmentation, we
built an unsupervised morphological segmentation
model using the Morfessor toolkit (Creutz and La-
gus, 2007).
Morfessor uses minimum description length cri-
teria to train a HMM-based segmentation model.
Varying the perplexity threshold in Morfessor does
not segment more word types, but rather over-
segments the same word types. We hand tuned the
model parameters over training data size and per-
plexity; these control the granularity and coverage of
the segmentations. Specifically, we trained different
segmenter models on varying sets of most frequent
words and different perplexities and identified two
sets that performed best based on a separate held-
out set. These two sets correspond to 500k most fre-
quent words and a perplexity of 50 (denoted SM1)
and 10k most frequent words and a perplexity of 20
(denoted SM2). We then used these two models to
segment the entire data set and generate two differ-
ent segmented training sets. These models had the
best combination of segmentation coverage of the
training data and largest segments, since we found
empirically that smaller segments were less mean-
ingful in the translation model. The SM2 segmenta-
tion segmented more words than SM1, but more fre-
quently segmented words into single-character units.
359
For example, the Czech word ?dlaebn??? is broken
into the useful components ?dlaeb + n??? by SM1, but
is oversegmented into ?dl + a + e + b + n??? by SM2.
However, SM1 fails to find a segmentation at all for
the related word ?dlaebn??mi?, while SM2 breaks it
up similiarly with an additional suffix: ?dl + a + e +
b + n?? + mi?.
With these segmentation models, we segmented
the target side of the training and dev data before
training the translation model. Similarly, we also
train segmented language models corresponding to
the two sets SM1 and SM2. The MERT tuning step
uses the segmented dev-set reference to evaluate the
segmented hypotheses generated by the decoder for
optimizing the weights for the BLEU score. How-
ever for evaluating the test-set, we stitched the seg-
ments in the decoder output back into unsegmented
forms in a post-processing step, before performing
evaluation against the original unsegmented refer-
ences. The hypotheses generated by the decoder
can have incomplete dangling segments where one
or more prefixes and/or suffixes are missing. While
these dangling segments could be handled in a dif-
ferent way, we use a simple heuristic of ignoring the
segment marker ?+? by just removing the segment
marker. In next section, we report the results of us-
ing the unsegmented model as well as its segmented
counterparts.
3.2 Experiments and Results
In the English-Czech experiments, we used the same
datasets for the dev and test sets as in French-
English experiments (dev: news-tests 2008, 2009,
2010 with 7567 sentence pairs and test: news-
test2011 with 3003 sentence pairs). Similarly,
MERT (Och, 2003) has been used to tune the feature
weights and we report the BLEU scores of two test-
sets computed using the official evaluation script
(mteval-v11b.pl).
Table 3.2 shows the results of different segmenta-
tion schemes on the WMT-11 and WMT-12 test-sets.
SM1 slightly outperformed the other two models in
Test-11, however the unsegmented model performed
best in Test-12, though marginally. We are currently
investigating this and are also considering the pos-
sibility employing the idea of morpheme prediction
in the post-decoding step in combination with this
morpheme-based translation as suggested by Clifton
Segmentation Test-11 Test-12
Baseline Hiero 14.65 12.40
SM1 : 500k-ppl50 14.75 12.34
SM2 : 10k-ppl20 14.57 12.34
Table 3: The English-Czech results for different segmen-
tation settings. Best performing setting is shown in Bold-
face.
and Sarkar (2011).
4 Conclusion
We submitted systems in two language pairs French-
English and English-Czech for WMT-12 shared
task. In French-English, we experimented the en-
semble decoding framework that effectively utilizes
the small amount of news genre data to improve the
performance in the testset belonging to the same
genre. We obtained a moderate gain of 0.4 BLEU
points with the ensemble decoding over the baseline
system in newstest-2011. For newstest-2012, it per-
forms comparably to that of the baseline and we are
presently investigating the lack of improvement in
newstest-2012. For Cz-En, We found that the BLEU
scores do not substantially differ from each other
and also the minor differences are not consistent for
Test-11 and Test-12.
References
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, pages 32?42.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1):3:1?3:34, February.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
360
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 177?
180. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
Annual Meeting of Association of Computational Lin-
guistics, pages 160?167.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics, Jeju, Republic of Korea,
July. Association for Computational Linguistics. To
appear.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics, 97(97):83?98, April.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175,
September.
361
