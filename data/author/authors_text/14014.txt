Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12?23,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Discourse Analysis for Article-Wide Temporal Classification
Jun-Ping Ng1, Min-Yen Kan1,2, Ziheng Lin3, Wei Feng4, Bin Chen5, Jian Su5, Chew-Lim Tan1
1School of Computing, National University of Singapore, Singapore
2Interactive and Digital Media Institute, National University of Singapore, Singapore
3Research & Innovation, SAP Asia Pte Ltd, Singapore
4Department of Computer Science, University of Toronto, Canada
5Institute for Infocomm Research, Singapore
junping@comp.nus.edu.sg
Abstract
In this paper we classify the temporal relations
between pairs of events on an article-wide ba-
sis. This is in contrast to much of the exist-
ing literature which focuses on just event pairs
which are found within the same or adjacent
sentences. To achieve this, we leverage on dis-
course analysis as we believe that it provides
more useful semantic information than typical
lexico-syntactic features. We propose the use
of several discourse analysis frameworks, in-
cluding 1) Rhetorical Structure Theory (RST),
2) PDTB-styled discourse relations, and 3)
topical text segmentation. We explain how
features derived from these frameworks can be
effectively used with support vector machines
(SVM) paired with convolution kernels. Ex-
periments show that our proposal is effective
in improving on the state-of-the-art signifi-
cantly by as much as 16% in terms of F1, even
if we only adopt less-than-perfect automatic
discourse analyzers and parsers. Making use
of more accurate discourse analysis can fur-
ther boost gains to 35%.
1 Introduction
A good amount of research had been invested in un-
derstanding temporal relationships within text. Par-
ticular areas of interest include determining the re-
lationship between an event mention and a time ex-
pression (timex), as well as determining the relation-
ship between two event mentions. The latter, which
we refer to as event-event (E-E) temporal classifica-
tion is the focus of this work.
For a given event pair which consists of two
events e1 and e2 found anywhere within an article,
we want to be able to determine if e1 happens be-
fore e2 (BEFORE), after e2 (AFTER), or within the
same time span as e2 (OVERLAP).
Consider this sentence1:
At least 19 people were killed and 114 people were
wounded in Tuesday?s southern Philippines airport blast,
officials said, but reports said the death toll could climb
to 30.
(1)
Three event mentions found within the sentence are
bolded. We say that there is an OVERLAP rela-
tionship between the ?killed ? wounded? event pair
as these two events happened together after the air-
port blast. Similarly there is a BEFORE relationship
between both the ?killed ? said?, and ?wounded ?
said? event pairs, as the death and injuries happened
before reports from the officials.
Being able to infer these temporal relationships
allows us to build up a better understanding of the
text in question, and can aid several natural lan-
guage understanding tasks such as information ex-
traction and text summarization. For example, we
can build up a temporal characterization of an article
by constructing a temporal graph denoting the rela-
tionships between all events within an article (Ver-
hagen et al, 2009). This can then be used to help
construct an event timeline which layouts sequen-
tially event mentions in the order they take place (Do
et al, 2012). The temporal graph can also be used
in text summarization, where temporal order can be
used to improve sentence ordering and thereby the
eventual generated summary (Barzilay et al, 2002).
Given the importance and value of temporal re-
lations, the community has organized shared tasks
1From article AFP ENG 20030304.0250 of the ACE 2005
corpus (ACE, 2005).
12
to spur research efforts in this area, including the
TempEval-1, -2 and -3 evaluation workshops (Ver-
hagen et al, 2009; Verhagen et al, 2010; Uzzaman
et al, 2012). Most related work in this area have
focused primarily on the task defintitions of these
evaluation workshops. In the task definitions, E-
E temporal classification involves determining the
relationship between events found within the same
sentence, or in adjacent sentences. For brevity we
will refer to this loosely as intra-sentence E-E tem-
poral classification in the rest of this paper.
This definition however is limiting and insuffi-
cient. It was adopted as a trade-off between com-
pleteness, and the need to simplify the evaluation
process (Verhagen et al, 2009). In particular, one
deficiency is that it does not allow us to construct the
complete temporal graph we seek. As illustrated in
Figure 1, being able to perform only intra-sentence
E-E temporal classification may result in a forest of
disconnected temporal graphs. A sentence s3 sepa-
rates events C and D, as such an intra-sentence E-E
classification system will not be able to determine
the temporal relationship between them. While we
can determine the relationship between A and C in
the figure with the use of temporal transitivity rules
(Setzer et al, 2003; Verhagen, 2005), we cannot re-
liably determine the relationship between say A and
D.
A
B C
D E
s1
s2
s3
s4
Figure 1: A disconnected temporal graph of events within
an article. Horizontal lines depict sentences s1 to s4, and
the circles identify events of interest.
In this work, we seek to overcome this limitation,
and study what can enable effective article-wide E-E
temporal classification. That is, we want to be able
to determine the temporal relationship between two
events located anywhere within an article.
The main contribution of our work is going
beyond the surface lexical and syntactic features
commonly adopted by existing state-of-the-art ap-
proaches. We suggest making use of semantically
motivated features derived from discourse analysis
instead, and show that these discourse features are
superior.
While we are just focusing on E-E temporal
classification, our work can complement other ap-
proaches such as the joint inference approach pro-
posed by Do et al (2012) and Yoshikawa et al
(2009) which builds on top of event-timex (E-T) and
E-E temporal classification systems. We believe that
improvements to the underlying E-T and E-E classi-
fication systems will help with global inference.
2 Related Work
Many researchers have worked on the E-E temporal
classification problem, especially as part of the Tem-
pEval series of evaluation workshops. Bethard and
Martin (2007) presented one of the earliest super-
vised machine learning systems, making use of sup-
port vector machines (SVM) with a variety of lexical
and syntactic features. Kolya et al (2010) described
a conditional random field (CRF) based learner mak-
ing use of similar features. Other researchers includ-
ing Uzzaman and Allen (2010) and Ha et al (2010)
made use of Markov Logic Networks (MLN). By
leveraging on the transitivity properties of temporal
relationships (Setzer et al, 2003), they found that
MLNs are useful in inferring new temporal relation-
ships from known ones.
Recognizing that the temporal relationships be-
tween event pairs and time expressions are related,
Yoshikawa et al (2009) proposed the use of a joint
inference model and showed that improvements in
performance are obtained. However this gain is at-
tributed to the joint inference model they had devel-
oped, making use of similar surface features.
To the best of our knowledge, the only piece
of work to have gone beyond sentence boundaries
and tackle the problem of article-wide E-E temporal
classification is by Do et al (2012). Making use of
integer linear programming (ILP), they built a joint
inference model which is capable of classifying tem-
poral relationships between any event pair within
a given document. They also showed that event
co-reference information can be useful in determin-
ing these temporal relationships. However they did
not make use of features directed specifically at de-
termining the temporal relationships of event pairs
13
across different sentences. Other than event co-
reference information, they adopted the same mix
of lexico-syntactic features.
Underlying these disparate data-driven methods
for similar temporal processing tasks, the reviewed
works all adopted a similar set of surface fea-
tures including vocabulary features, part-of-speech
tags, constituent grammar parses, governing gram-
mar nodes and verb tenses, among others. We ar-
gue that these features are not sufficiently discrimi-
native of temporal relationships because they do not
explain how sentences are combined together, and
thus are unable to properly differentiate between the
different temporal classifications. Supporting our
argument is the work of Smith (2010), where she
argued that syntax cannot fully account for the un-
derlying semantics beneath surface text. D?Souza
and Ng (2013) found out as much, and showed that
adopting richer linguistic features such as lexical re-
lations from curated dictionaries (e.g. Webster and
WordNet) as well as discourse relations help tempo-
ral classification. They had shown that the Penn Dis-
course TreeBank (PDTB) style (Prasad et al, 2008)
discourse relations are useful. We expand on their
study to assess the utility of adopting additional dis-
course frameworks as alternative and complemen-
tary views.
3 Making Use of Discourse
To highlight the deficiencies of surface features, we
quote here an example from Lascarides and Asher
(1993):
[A] Max opened the door. The room was pitch dark.
[B] Max switched off the light. The room was pitch dark.
(2)
The two lines of text A and B in Example 2 have
similar syntactic structure. Given only syntactic fea-
tures, we may be drawn to conclude that they share
similar temporal relationships. However in the first
line of text, the events temporally OVERLAP, while
in the second line they do not. Clearly, syntax alone
is not going to be useful to help us arrive at the cor-
rect temporal relations.
If existing surface features are insufficient, what is
sufficient? Given a E-E pair which crosses sentence
boundaries, how can we determine the temporal re-
lationship between them? We take our cue from the
work of Lascarides and Asher (1993). They sug-
gested instead that discourse relations hold the key
to interpreting such temporal relationships.
Building on their observations, we believe that
discourse analysis is integral to any solution for the
problem of article-wide E-E temporal classification.
We thus seek to exploit a series of different discourse
analysis studies, including 1) the Rhetorical Struc-
ture Theory (RST) discourse framework, 2) Penn
Discourse Treebank (PDTB)-styled discourse rela-
tions based on the lexicalized Tree Adjoining Gram-
mar for Discourse (D-LTAG), and 3) topical text seg-
mentation, and validate their effectiveness for tem-
poral classification.
RST Discourse Framework. RST (Mann and
Thompson, 1988) is a well-studied discourse anal-
ysis framework. In RST, a piece of text is split into a
sequence of non-overlapping text fragments known
as elementary discourse units (EDUs). Neighboring
EDUs are related to each other by a typed relation.
Most RST relations are hypotactic, where one of the
two EDUs participating in the relationship is demar-
cated as a nucleus, and the other a satellite. The nu-
cleus holds more importance, from the point of view
of the writer, while the satellite?s purpose is to pro-
vide more information to help with the understand-
ing of the nucleus. Some RST relations are however
paratactic, where the two participating EDUs are
both marked as nuclei. A discourse tree can be com-
posed by viewing each EDU as a leaf node. Nodes
in the discourse tree are linked to one another via the
discourse relations that hold between the EDUs.
RST discourse relations capture the semantic re-
lation between two EDUs, and these often offer a
clue to the temporal relationship between events in
the two EDUs too. As an example, let us refer once
again to Example 2. Recall that in the second line of
text ?switched off? happens BEFORE ?dark?. The
RST discourse structure for the second line of text
is shown on the left of Figure 2. We see that the
two sentences are related via a ?Result? discourse
relation. This fits our intuition that when there is
causation, there should be a BEFORE/AFTER rela-
tionship. The RST discourse relation in this case is
very useful in helping us determine the relationship
between the two events.
PDTB-styled Discourse Relations. Another widely
adopted discourse relation annotation is the PDTB
framework (Prasad et al, 2008). Unlike the RST
14
Max switched off the light. The room was pitch dark.
RESULT
The room was pitch dark.
CONTINGENCY :: CAUSE
arg1 arg2
Max switched off the light.
Figure 2: RST and PDTB discourse structures for the second line of text in Example 2. The structure on the left is the
RST discourse structure, while the structure on the right is for PDTB.
framework, the discourse relations in PDTB build on
the work on D-LTAG by Webber (2004), a lexicon-
grounded approach to discourse analysis. Practi-
cally, this means that instead of starting from a pre-
identified set of discourse relations, PDTB-styled
annotations are more focused on detecting possible
connectives (can be either explicit or implicit) within
the text, before identifying the text fragments which
they connect and how they are related to one another.
Applied again to the second line of text we have in
Example 2, we get a structure as shown on the right
side of Figure 2. From the figure we can see that
the two sentences are related via a ?Cause? relation-
ship. Similar to what we have explained earlier for
the case of RST, the presence of a causal effect here
strongly hints to us that events in the two sentences
share a BEFORE/AFTER relationship.
At this point we want to note the differences be-
tween the use of the RST framework and PDTB-
styled discourse relations in the context of our work.
The theoretical underpinnings behind these two dis-
course analysis are very different, and we believe
that they can be complementary to each other. First,
the RST framework breaks up text within an article
linearly into non-overlapping EDUs. Relations can
only be defined between neighboring EDUs. How-
ever this constraint is not found in PDTB-styled re-
lations, where a text fragment can participate in one
discourse relation, and a subsequence of it partic-
ipate in another. PDTB relations are also not re-
stricted only to adjacent text fragments. In this as-
pect, the flexibility of the PDTB relations can com-
plement the seemingly more rigid RST framework.
Second, with PDTB-styled relations not every
sentence needs to be in a relation with another as
the PDTB framework does not aim to build a global
discourse tree that covers all sentence pairs. This is
a problem when we need to do an article-wide anal-
ysis. The RST framework does not suffer from this
limitation however as we can build up a discourse
tree connecting all the text within a given article.
Topical Text Segmentation. A third complemen-
tary type of inter-sentential analysis is topical text
segmentation. This form of segmentation separates
a piece of text into non-overlapping segments, each
of which can span several sentences. Each segment
represents passages or topics, and provides a coarse-
grained study of the linear structure of the text (Sko-
rochod?Ko, 1972; Hearst, 1994). The transition be-
tween segments can represent possible topic shifts
which can provide useful information about tempo-
ral relationships.
Referring to Example 32, we have delimited the
different lines of text into segments with parenthe-
ses along with a subscript. Segment (1) talks about
the casualty numbers seen at a medical centre, while
Segment (2) provides background information that
informs us a bomb explosion had taken place. The
segment boundary signals to us a possible temporal
shift and can help us to infer that the bombing event
took place BEFORE the deaths and injuries had oc-
curred.
(The Davao Medical Center, a regional government hos-
pital, recorded 19 deaths with 50 wounded. Medical
evacuation workers however said the injured list was
around 114, spread out at various hospitals.)1
(A powerful bomb tore through a waiting shed at the
Davao City international airport at about 5.15 pm (0915
GMT) while another explosion hit a bus terminal at the
city.)2
(3)
4 Methodology
Having motivated the use of discourse analysis for
our problem, we now proceed to explain how we can
make use of them for temporal classification. The
different facets of discourse analysis that we are ex-
ploring in this work are structural in nature. RST
2From article AFP ENG 20030304.0250 of the ACE 2005
corpus.
15
EDU2 EDU3
r2
r1
EDU1
A
B
Figure 3: A possible RST discourse tree. The two circles
denote two events A and B which we are interested in.
t1 t2
t3
t4
r1 r2
r3
B
A
Figure 4: A possible PDTB-styled discourse annotation
where the circles represent events we are interested in.
and PDTB discourse relations are commonly repre-
sented as graphs, and we can also view the output
of text segmentation as a graph with individual text
segments forming vertices, and the transitions be-
tween them forming edges.
Considering this, we propose the use of support
vector machines (SVM), adopting a convolution ker-
nel (Collins and Duffy, 2001) for its kernel function
(Vapnik, 1999; Moschitti, 2006). The use of convo-
lution kernels allows us to do away with the exten-
sive feature engineering typically required to gener-
ate flat vectorized representations of features. This
process is time consuming and demands specialized
knowledge to achieve representations that are dis-
criminative, yet are sufficiently generalized. Con-
volution kernels had also previously been shown to
work well for the related problem of E-T temporal
classification (Ng and Kan, 2012), where the fea-
tures adopted are similarly structural in nature.
We now describe our use of the discourse analysis
frameworks to generate appropriate representations
for input to the convolution kernel.
RST Discourse Framework. Recall that the RST
framework provides us with a discourse tree for an
entire input article. In recent years several automatic
RST discourse parsers have been made available. In
our work, we first make use of the parser by Feng
and Hirst (2012) to obtain a discourse tree represen-
tation of our input. To represent the meaningful por-
tion of the resultant tree, we encode path information
between the two sentences of interest.
We illustrate this procedure using the example
discourse tree illustrated in Figure 3. EDUs includ-
ing EDU1 to EDU3 form the vertices while dis-
course relations r1 and r2 between the EDUs form
the edges. For a E-E pair, {A,B}, we can obtain a
feature structure by first locating the EDUs within
which A and B are found. A is found inside EDU1
and B is found within EDU3. We trace the short-
est path between EDU1 and EDU3, and use this
path as the feature structure for the E-E pair, i.e.
{r1 ? r2}.
PDTB-styled Discourse Relations. We make use of
the automatic PDTB discourse parser from Lin et al
(2013) to obtain the discourse relations over an input
article. Similar to how we work with the RST dis-
course framework, for a given E-E pair, we retrieve
the relevant text fragments and use the shortest path
linking the two events as a feature structure for our
convolution kernel classifier.
An example of a possible PDTB-styled discourse
annotation is shown in Figure 4. The horizontal
lines represent different sentences in an article. The
parentheses delimit text fragments, t1 to t4, which
have been identified as arguments participating in
discourse relations, r1 to r3. For a given E-E pair
{A,B}, we use the trace of the shortest path be-
tween them i.e. {r1 ? r2} as a feature structure.
We take special care to regularize the input (as,
unlike EDUs in RST, arguments to different PDTB
relations may overlap, as in r2 and r3). We model
each PDTB discourse annotation as a graph and em-
ploy Dijkstra?s shortest path algorithm. The graph
resulting from the annotation in Figure 4 is given in
Figure 5. Each text fragment ti maps to a vertex
ni in the graph. PDTB relations between text frag-
ments form edges between corresponding vertices.
As r2 relates t2 to both t3 and t4, two edges link
up n2 to the corresponding vertices n3 and n4 re-
spectively. By doing this, Dijkstra?s algorithm will
always allow us to find the desired shortest path.
n1 n2 n3 n4
r1
r2 r3
r2
Figure 5: Graph derived from discourse annotation in
Figure 4.
16
Topical Text Segmentation. Taking as input a com-
plete text article, we make use of the state-of-the-art
text segmentation system from Kazantseva and Sz-
pakowicz (2011). The output of the system is a se-
ries of non-overlapping, linear text segments, which
we can number sequentially.
In Figure 6 the horizontal lines represent sen-
tences. Parentheses with subscripts mark out the
segment boundaries. We can see two segments s1
and s2 here. Given a target E-E pair {A,B} (repre-
sented as circles inside the figure), we identify the
segment number of the corresponding segment in
which each of A and B is found. We build a fea-
ture structure with the identified segment numbers,
i.e. {s1 ? s2} to capture the segmentation.
A
B
s1
s2
Figure 6: A possible segmentation of three sentences into
two segments.
5 Results
We conduct a series of experiments to validate the
utility of our proposed features.
Data Set. We make use of the same data set built
by Do et al (2012). The data set consists of 20
newswire articles which originate from the ACE
2005 corpus (ACE, 2005). Initially, the data set
consist of 324 event mentions, and a total of 375
annotated E-E pairs. We perform the same temporal
saturation step as described in Do et al (2012), and
obtained a total of 7,994 E-E pairs3.
A breakdown of the number of instances by each
temporal classes is shown in Table 1. Unlike earlier
data sets such as that for TempEval-2 where more
than half (about 55%) of test instances belong to the
3Though we have obtained the data set from the original au-
thors, there was a discrepancy in the number of E-E pairs. The
original paper reported a total of 376 annotated E-E pairs. Be-
sides this, we also repeated the saturation steps iteratively until
no new relationship pairs are generated. We believe this to be
an enhancement as it ensures that all inferred temporal relation-
ships are generated.
OVERLAP class, OVERLAP instances make up just
10% of the data set.
This difference is due mainly to the fact that our
data set consists not only of intra-sentence E-E pairs,
but also of article-wide E-E pairs. Figure 7 shows
the number of instances for each temporal class bro-
ken down by the number of sentences (i.e. sentence
gap) that separate the events within each E-E pair.
We see that as the sentence gap increases, the pro-
portion of OVERLAP instances decreases. The in-
tuitive explanation for this is that when event men-
tions are very far apart in an article, it becomes more
unlikely that they happen within the same time span.
Class AFTER BEFORE OVERLAP
# E-E pairs 3,588 (45%) 3,589 (45%) 815 (10%)
Table 1: Number of E-E pairs in data set attributable to
each temporal class. Percentages shown in parentheses.
Figure 7: Breakdown of number of E-E pairs for each
temporal class based on sentence gap.
Experiments. The work done in Do et al (2012) is
highly related to our experiments, and so we have
reported the relevant results for local E-E classifi-
cation in Row 1 of Table 2 as a reference. While
largely comparable, note that a direct comparison is
not possible because 1) the number of E-E instances
we have is slightly different from what was reported,
and 2) we do not have access to the exact partitions
they have created for 5-fold cross-validation.
As such, we have implemented a baseline adopt-
ing similar surface lexico-syntactic features used in
previous work (Mani et al, 2006; Bethard and Mar-
tin, 2007; Ng and Kan, 2012; Do et al, 2012), in-
cluding 1) part-of-speech tags, 2) tenses, 3) depen-
dency parses, 4) relative position of events in article,
17
System Precision Recall F1
(1) DO2012 43.86 52.65 47.46
(2) BASE 59.55 38.14 46.50
(3) BASE + RST + PDTB + TOPICSEG 71.89 41.99 53.01
(4) BASE + RST + PDTB + TOPICSEG + COREF 75.23 43.58 55.19
(5) BASE + O-RST + PDTB + O-TOPICSEG + O-COREF 78.35 54.24 64.10
Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive
row is statistically significant, but a comparison is not possible between rows (1) and (2).
5) the number of sentences between the target events
and 6) VerbOcean (Chklovski and Pantel, 2004) re-
lations between events. This baseline system, and
the subsequent systems we will describe, comprises
of three separate one-vs-all classifiers for each of the
temporal classes. The result obtained by our base-
line is shown in Row 2 (i.e. BASE) in Table 2. We
note that our baseline is competitive and performs
similarly to the results obtained by Do et al (2012).
However as we do not have the raw judgements from
Do?s system, we cannot test for statistical signifi-
cance.
We also implemented our proposed features and
show the results obtained in the remaining rows of
Table 2. In Row 3, RST denotes the RST discourse
feature, PDTB denotes the PDTB-styled discourse
features, and TOPICSEG denotes the text segmen-
tation feature. Compared to our own baseline, there
is a relative increase of 14% in F1, which is statis-
tically significant when verified with the one-tailed
Student?s paired t-test (p < 0.01).
In addition, Do et al (2012) have shown the value
of event co-reference. Therefore we have also in-
cluded this feature by making use of an automatic
event co-reference system by Chen et al (2011).
The result obtained after adding this feature (de-
noted by COREF) is shown in Row 4. The relative in-
crease in F1 of about 4% from Row 3 is statistically
significant (p < 0.01) and affirms that event co-
reference is a useful feature to have, together with
our proposed features. We note that our complete
system in Row 4 gives a 16% improvement in F1,
relative to the reference system DO2012 in Row 1.
To get a better idea of the performance we can ob-
tain if oracular versions of our features are available,
we also show the results obtained if hand-annotated
RST discourse structures, text segments, as well as
event co-reference information were used. Annota-
tions for the RST discourse structures and text seg-
ments were performed by the first author (RST an-
notations were made following the annotation guide-
lines given by Carlson and Marcu (2001)). Oracular
event co-reference information was included in the
dataset that we have used.
In Row 5 the prefix O denotes oracular versions
of the features we had proposed. From the results
we see that there is a marked increase of over 15%
in F1 relative to Row 4. Compared to Do?s state-of-
the-art system, there is also a relative gain of at least
35%. These oracular results further confirm the im-
portance of non-local discourse analysis for tempo-
ral processing.
6 Discussion
Ablation tests. We performed ablation tests to as-
sess the efficacy of the discourse features used in
our earlier experiments. Starting from the full sys-
tem, we dropped each discourse feature in turn to see
the effect this has on overall system performance.
Our test is performed over the same data set, again
with 5-fold cross-validation. The results in Table 3
show a statistically significant (based on the one-
tailed Student?s paired t-test) drop in F1 in each case,
which proves that each of our proposed features is
useful and required.
From the ablation tests, we also observe that the
RST discourse feature contributes the most to over-
all system performance while the PDTB discourse
feature contributes the least. However we should not
conclude prematurely that the former is more use-
ful than the latter; as the results are obtained using
parses from automatic systems, and are not reflec-
tive of the full utility of ground truth discourse an-
notations.
Useful Relations. The ablation test results showed
us that discourse relations (in particular RST dis-
18
Figure 8: Proportion of occurence in temporal classes for every RST and PDTB relation.
Ablated Feature Change in F1 Sig
?RST -9.03 **
?TOPICSEG -2.98 **
?COREF -2.18 **
?PDTB -1.42 *
Table 3: Ablation test results. ?**? and ?*? denote statis-
tically significance against the full system with p < 0.01
and p < 0.05, respectively.
course relations) are the most important in our sys-
tem. We have also motivated our work earlier with
the intuition that certain relations such as the RST
?Result? and the PDTB ?Cause? relations provide
very useful temporal cues. We now offer an intro-
spection into the use of these discourse relations.
Figure 8 illustrates the relative proportion of tem-
poral classes in which each RST and PDTB re-
lation appear. If the relations are randomly dis-
tributed, we should expect their distribution to fol-
low that of the temporal classes as shown in Table 1.
However we see that many of the relations do not
follow this distribution. For example, we observe
that several relations such as the RST ?Condition?
and PDTB ?Cause? relations are almost exclusively
found within AFTER and BEFORE event pairs only,
while the RST ?Manner-means? and PDTB ?Syn-
chrony? relations occur in a disproportionately large
number of OVERLAP event pairs. These relations
are likely useful in disambiguating between the dif-
ferent temporal classes.
To verify this, we examine the convolution tree
fragments that lie on the support vector of our SVM
classifier. The work of Pighin and Moschitti (2010)
in linearizing kernel functions allows us to take a
look at these tree fragments. Applying the lineariza-
tion process leads to a different classifier from the
one we have used. The identified tree fragments are
therefore just an approximation to those actually em-
ployed by our classifier. However, this analysis still
offers an introspection as to what relations are most
influential for classification.
BEFORE OVERLAP
B1 (Temporal ... O1 (Manner-means ...
B2 (Temporal (Elaboration ...
B3 (Condition (Explanation ...
B4 (Condition (Attribution ...
B5 (Elaboration (Bckgrnd ...
Table 4: Subset of top RST discourse fragments on sup-
port vectors identified by linearizing kernel function.
Table 4 shows a subset of the top RST discourse
fragments identified for the BEFORE and OVER-
LAP one-vs-all classifiers. The list is in line with
what we expect from Figure 8. The former consists
of fragments containing relations such as ?Tempo-
ral? and ?Condition?, while the latter has a sole frag-
ment containing ?Manner-Means?.
To illustrate what these fragments may mean, we
show several example sentences from our data set
in Example 4. Sentence A consists of the tree frag-
ment B1, i.e. ?(Temporal...?. Its corresponding dis-
course structure is illustrated in the top half of Fig-
ure 9. This fragment indicates to us (correctly) that
the event ?wielded? happened BEFORE Milosevic
was ?swept out? of power. Sentence B is made
up of tree fragment O1, i.e. ?(Manner-means...?,
19
and its discourse structure is shown in the bottom
half of Figure 9. As with the previous example, the
fragment suggests (correctly) that there should be a
OVERLAP relationship for the ?requested ? said?
event pair.
[A] Milosevic and his wife wielded enormous power in
Yugoslavia for more than a decade before he was swept
out of power after a popular revolt in October 2000.
[B] The court order was requested by Jack Welch?s at-
torney, Daniel K. Webb, who said Welch would likely be
asked about his business dealings, his health and entries
in his personal diary.
(4)
Milosevic ? wielded? 
a decade 
before.. swept out.. 
power
after a?  October
2000.
temporal
temporal
The court? requested
by Jack .. Webb,
elaboration
who said Welch would ?
diary.
attribution
manner-means
Figure 9: RST discourse structures for sentences A (top
half) and B (bottom half) in Example 4.
Segment Numbers. From the ablation test results,
text segmentation is the next most important feature
after the RST discourse feature. This is interesting
given that the defined feature structure for topical
text segmentation is not the most intuitive. By us-
ing actual segment numbers, the structure may not
generalize well for articles of different lengths for
example, as each article may have vastly different
number of segments. The transition across segments
may also not carry the same semantic significance
for different articles.
Our experiments have however shown that this
feature design is useful in improving performance.
This is likely because:
1. The default settings of the text segmentation
system we had used are such that precision is
favoured over recall (Kazantseva and Szpakow-
icz, 2011, p. 292). As such there is just an aver-
age of between two to three identified segments
per article. This makes the feature more gener-
alizable despite making use of actual segment
numbers.
2. The style of writing in newswire articles which
we are experimenting on generally follows
common journalistic guidelines. The semantics
behind the transitions across the coarse-grained
segments that were identified are thus likely to
be of a similar nature across many different ar-
ticles.
We leave for future work an investigation into
whether more fine-grained topic segments can lead
to further performance gains. In particular, it will be
interesting to study if work on argumentative zoning
(Teufel and Kan, 2011) can be applied to newswire
articles, and whether the subsequent learnt docu-
ment structures can be used to delineate topic seg-
ments more accurately.
Error Analysis. Besides examining the features we
had used, we also want to get a better idea of the er-
rors made by our classifier. Recall that we are using
separate one-vs-all classifiers for each of the tempo-
ral classes, so each of the three classifiers generates
a column in the aggregate confusion matrix shown
in Table 5. In cases where none of the SVM clas-
sifiers return a positive confidence value, we do not
assign a temporal class (captured as column N). The
high number of event pairs which are not assigned to
any temporal class explains the lower recall scores
obtained by our system, as observed in Table 2.
Predicted
O B A N
O 119 (14.7%) 114 (14.1%) 104 (12.8%) 474 (58.5%)
B 19 (0.5%) 2067 (57.9%) 554 (15.5%) 928 (26.0%)
A 16 (0.5%) 559 (15.7%) 2046 (57.3%) 947 (26.5%)
Table 5: Confusion matrix obtained for the full system,
classifying into (O)VERLAP, (B)EFORE, (A)FTER, and
(N)o result.
Additionally, an interesting observation is the low
percentage of OVERLAP instances that our classi-
fier managed to predict correctly. About 57% of
BEFORE and AFTER instances are classified cor-
20
rectly, however only about 15% of OVERLAP in-
stances are correct.
Figure 10 offers more evidence to suggest that
our classifier works better for the BEFORE and AF-
TER classes than the OVERLAP class. We see that
as sentence gap increases, we achieve a fairly con-
sistent performance for both BEFORE and AFTER
instances. OVERLAP instances are concentrated
where the sentence gap is less than 7, with the best
accuracy figure coming in below 30%.
Although not definitive, this may be because our
data set consists of much fewer OVERLAP in-
stances than the other two classes. This bias may
have led to insufficient training data for accurate
OVERLAP classification. It will be useful to inves-
tigate if using a more balanced data set for training
can help overcome this problem.
Figure 10: Accuracy of the classifer for each temporal
class, plotted against the sentence gap of each E-E pair.
7 Conclusion
We believe that discourse features play an important
role in the temporal ordering of events in text. We
have proposed the use of different discourse anal-
ysis frameworks and shown that they are effective
for classifying the temporal relationships of article-
wide E-E pairs. Our proposed discourse-based fea-
tures are robust and work well even though auto-
matic discourse analysis is noisy. Experiments fur-
ther show that improvements to these underlying
discourse analysis systems will benefit system per-
formance.
In future work, we will like to explore how to
better exploit the various discourse analysis frame-
works for temporal classification. For instance, RST
relations are either hypotactic or paratactic. Marcu
(1997) made use of this to generate automatic sum-
maries by considering EDUs which are nuclei to be
more salient. We believe it is interesting to examine
how such information can help. We are also inter-
ested to apply discourse features in the context of a
global inferencing system (Yoshikawa et al, 2009;
Do et al, 2012), as we think such analyses will also
benefit these systems as well.
Acknowledgments
We like to express our gratitude to Quang Xuan Do,
Wei Lu, and Dan Roth for generously making avail-
able the data set they have used for their work in
EMNLP 2012. We would also like to thank the
anonymous reviewers who reviewed this paper for
their valuable feedback.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
ACE. 2005. The ACE 2005 (ACE05) Evaluation Plan.
October.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring Strategies for Sentence Order-
ing in Multidocument News Summarization. Journal
of Artificial Intelligence Research (JAIR), 17:35?55.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 129?132, June.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, Informa-
tion Sciences Institute, University of Southern Califor-
nia, July.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A Unified Event Coreference Resolution by In-
tegrating Multiple Resolvers. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 102?110, Novem-
ber.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, July.
21
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP), pages
677?689, July.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
Temporal Relations with Rich Linguistic Knowledge.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), pages 918?927, June.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistics Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL), pages 60?68, July.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling Temporal
Relations with Markov Logic and Lexical Ontology.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval), pages 341?344, July.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation
of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 9?16, June.
Anna Kazantseva and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 284?293, July.
Anup Kumar Kolya, Asif Ekbal, and Sivaji Bandyopad-
hyay. 2010. JU CSE TEMP: A First Step Towards
Evaluating Events, Time Expressions and Temporal
Relations. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
345?350, July.
Alex Lascarides and Nicholas Asher. 1993. Temporal
Interpretation, Discourse Relations and Commonsense
Entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013. A
PDTB-styled End-to-End Discourse Parser. Natural
Language Engineering, FirstView:1?34, February.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 753?760, July.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From Discourse Structures to Text
Summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, volume 97,
pages 82?88, July.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), September.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved Tem-
poral Relation Classification using Dependency Parses
and Selective Crowdsourced Annotations. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 2109?2124, December.
Daniele Pighin and Alessandro Moschitti. 2010. On Re-
verse Feature Engineering of Syntactic Tree Kernels.
In Proceedings of the 14th Conference on Natural Lan-
guage Learning (CoNLL), August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC), May.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple.
2003. Using Semantic Inferences for Temporal An-
notation Comparison. In Proceedings of the 4th In-
ternational Workshop on Inference in Computational
Semantics (ICoS), September.
Eduard F. Skorochod?Ko. 1972. Adaptive Method of
Automatic Abstracting and Indexing. In Proceedings
of the IFIP Congress, pages 1179?1182.
Carlota S. Smith. 2010. Temporal Structures in Dis-
course. Text, Time, and Context, 87:285?302.
Simone Teufel and Min-Yen Kan. 2011. Robust Argu-
mentative Zoning for Sensemaking in Scholarly Doc-
uments. In Advanced Language Technologies for Dig-
ital Libraries, pages 154?170. Springer.
Naushad Uzzaman and James F. Allen. 2010. TRIPS and
TRIOS System for TempEval-2: Extracting Temporal
Information. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
276?283, July.
Naushad Uzzaman, Hector Llorens, James F. Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2012. TempEval-3: Evaluating Events, Time Expres-
sions, and Temporal Relations. Computing Research
Repository (CoRR), abs/1206.5333.
Vladimir N. Vapnik, 1999. The Nature of Statistical
Learning Theory, chapter 5. Springer.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval Challenge: Identifying
22
Temporal Relations in Text. Language Resources and
Evaluation, 43(2):161?179.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
57?62, July.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
Bonnie Webber. 2004. D-LTAG: Extending Lexicalized
TAG to Discourse. Cognitive Science, 28(5):751?779.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly Identifying
Temporal Relations with Markov Logic. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL) and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language Pro-
cessing (AFNLP), pages 405?413, August.
23
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 780?790,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Mining Scientific Terms and their Definitions:
A Study of the ACL Anthology
Yiping Jin1 Min-Yen Kan1,2 Jun-Ping Ng1 Xiangnan He1 ?
1Department of Computer Science
2Interactive and Digital Media Institute
National University of Singapore
13 Computing Drive, Singapore 117417
{yiping, kanmy, junping, xiangnan}@comp.nus.edu.sg
Abstract
This paper presents DefMiner, a supervised
sequence labeling system that identifies scien-
tific terms and their accompanying definitions.
DefMiner achieves 85% F1 on a Wikipedia
benchmark corpus, significantly improving
the previous state-of-the-art by 8%.
We exploit DefMiner to process the ACL An-
thology Reference Corpus (ARC) ? a large,
real-world digital library of scientific arti-
cles in computational linguistics. The re-
sulting automatically-acquired glossary rep-
resents the terminology defined over several
thousand individual research articles.
We highlight several interesting observations:
more definitions are introduced for conference
and workshop papers over the years and that
multiword terms account for slightly less than
half of all terms. Obtaining a list of popular
defined terms in a corpus of computational lin-
guistics papers, we find that concepts can of-
ten be categorized into one of three categories:
resources, methodologies and evaluation met-
rics.
1 Introduction
Technical terminology forms a key backbone in
scientific communication. By coining formalized
terminology, scholars convey technical information
precisely and compactly, augmenting the dissemi-
nation of scientific material. Collectively, scholarly
?This research is supported by the Singapore National Re-
search Foundation under its International Research Centre @
Singapore Funding Initiative and administered by the IDM Pro-
gramme Office.
compilation efforts result in reference sources such
as printed dictionaries, ontologies and thesauri.
While online versions are now common in many
fields, these are still largely compiled manually, re-
lying on costly human editorial effort. This leads
to resources that are often outdated or stale with re-
spect to the current state-of-the-art. Another indirect
result of this leads to a second problem: lexical re-
sources tend to be general, and may contain multi-
ple definitions for a single term. For example, the
term ?CRF? connotes ?Conditional Random Fields?
in most modern computational linguistics literature;
however, there are many definitions for this acronym
in Wikipedia. Because only one correct sense ap-
plies, readers may need to expend effort to identify
the appropriate meaning of a term in context.
We address both issues in this work by automat-
ically extracting terms and definitions directly from
primary sources: scientific publications. Since most
new technical terms are introduced in scientific pub-
lications, our extraction process addresses the bottle-
neck of staleness. Second, since science is organized
into disciplines and sub-disciplines, we can exploit
this inherent structure to gather contextual informa-
tion about a term and its definition.
Aside from performance improvements, the key
contributions of our work are in 1) recasting the
problem as a sequence labeling task and exploring
suitable learning architectures, 2) our proposal and
validation of the the use of shallow parsing and de-
pendency features to target definition extraction, and
3) analyzing the ACL Anthology Reference Cor-
pus from statistical, chronological and lexical view-
points.
780
2 Related Work
The task of definition mining has attracted a fair
amount of research interest. The output of such sys-
tems can be used to produce glossaries or answer
definition questions. The primary model for this
task in past work has been one of binary classifi-
cation: does a sentence contain a definition or not?
Existing methods can be cast into three main cat-
egories, namely rule-based (Muresan and Klavans,
2002; Westerhout and Monachesi, 2007), supervised
machine learning (Fahmi and Bouma, 2006; Wester-
hout, 2009), and semi-supervised approaches (Nav-
igli and Velardi, 2010; Reiplinger et al, 2012).
Rule-based approaches are intuitive and efficient,
and were adopted in early research. Here, system
performance is largely governed by the quality of
the rules. Muresan and Klavans (2002) developed a
rule-based system to extract definitions from online
medical articles. The system first selects candidates
using hand-crafted cue-phrases (e.g. is defined as,
is called; analogous to ?IS-A? phrases), further fil-
tering the candidates with grammar analysis. West-
erhout and Monachesi (2007) augmented the set of
rules with part-of-speech (POS) tag patterns, achiev-
ing an F2 of 0.43.
While such manually-crafted expert rules have
high precision, they typically suffer from low re-
call. Definitions can be expressed in a variety of
ways, making it difficult to develop an exhaustive
set of rules to locate all definition sentences. To ad-
dress low recall, later research adopted data-driven
machine learning approaches. Fahmi and Bouma
(2006) made used of supervised machine learning to
extract definitions from a corpus of Dutch Wikipedia
pages in the medical domain. They showed that
a baseline approach which simply classifies every
first sentence as a definition works surprisingly well,
achieving an accuracy of 75.9% (undoubtedly due to
the regular structure and style for Wikipedia). Their
final system, based on the important feature of sen-
tence position, was augmented with surface-level
features (bag-of-words, bigrams, etc.) and syntac-
tic features (type of determiner, position of the sub-
ject in the sentence). Their study with three different
learners ? na??ve Bayes, maximum entropy (MaxEnt)
and the support vector machine (SVM) ? showed
that MaxEnt gave the best results (92.2% accurate).
Westerhout (2009) worked on a hybrid approach,
augmenting a machine learner to a set of hand-
written rules. A random forest classifier is used to
exploit linguistic and structural features. Informed
by Fahmi and Bouma (2006)?s study, she included
article and noun types in her feature set. Lexico-
structural cues like the layout of the text are also
exploited. She evaluated the performance of dif-
ferent cue phrases including the presence of ?IS-
A? phrases, other verbs, punctuations and pronouns.
The highest F2 score of 0.63 is reported for the ?IS-
A? pattern.
Since 2009 the focus of the research shifted to
methods not limited to feature engineering. Borg
et al (2009) implemented a fully automated system
to extract and rank definitions based on genetic al-
gorithms and genetic programming. They defined
two sub-problems including 1) acquiring the rela-
tive importance of linguistic forms, and 2) learn-
ing of new linguistic forms. Starting with a small
set of ten simple hand-coded features, such as hav-
ing sequence ?FW IS? (FW is a tag for foreign
word) or containing keyword identified by the sys-
tem, the system is able to learn simple rules such as
?NN is a NN?. However, their system is optimized
for similar ?IS-A? patterns, as was used in (West-
erhout, 2009). Their system, achieving an average
f-measure of 0.25, also performs poorer than ma-
chine learning systems which exploit more specific
features.
To cope with the generality of patterns, Navigli
and Velardi (2010) proposed the three-step use of
directed acyclic graphs, called Word-Class Lattices
(WCLs), to classify a Wikipedia dataset of defini-
tions. They first replace the uncommon words in the
sentences with a wildcard (*), generating a set of
?star patterns?. Star patterns are then clustered ac-
cording to their similarity. For each cluster, a WCL
is generated by aligning the sentences in the cluster
to form a generalized sentence. Although they re-
ported a higher precision and recall compared with
previous work, the result for WCL (F1 of 75.23%)
is not significantly better than the baseline system
which exploits only star patterns (F1 of 75.05%)
without generating the directed graphs.
Reiplinger et al (2012) took a semi-supervised
approach. They employed bootstrapping to ex-
tract glossary sentences from scientific articles in
781
the ACL Anthology Reference Corpus (ACL ARC)
(Bird et al, 2008). Their results show that bootstrap-
ping is useful for definition extraction. Starting from
a few initial term-definition pairs and hand-written
patterns as seeds, their system iteratively acquires
new term-definition pairs and new patterns.
We note that these previous systems rely heav-
ily on lexico-syntactic patterns. They neither suf-
ficiently exploit the intrinsic characteristics of the
term and definition, nor invest effort to localize them
within the sentence1. Given the significant structure
in definitions, we take a more fine-grained approach,
isolating the term and its definition from sentences.
According to Pearson (1996), a definition can be for-
mally expressed as:
X = Y + distinguishing characteristics,
where ?X? is the definiendum (defined term; here-
after, term), and ?Y + distinguishing characteristics?
can be understood as the definiens (the term?s defini-
tion; hereafter, definition). The connector ?=? can be
replaced by a verb such as ?define?, ?call?, a punc-
tuation mark or other phrase. Our task is thus to find
pairs of terms and their associated definitions in in-
put scholarly articles. The sentence-level task of de-
ciding whether a sentence s is a definition sentence
or not, is thus a simplification of our task.
3 Methodology
Our DefMiner system is based on the sequence la-
beling paradigm ? it directly assigns an annotation
ai from A ? {(T)erm,(D)efinition,(O)ther} for each
input word wi. We post-process our labeler?s results
to achieve parity with the simplified definition sen-
tence task: When we detect both a term?s and defini-
tion?s presence in a sentence, we deem the sentence
a definition sentence. To be clear, this is a require-
ment; when we detect only either a term or a def-
inition, we filter these out as false positives and do
not include them as system output ? by definition in
DefMiner, terms must appear within the same sen-
tence as their definitions.
To train our classifier, we need a corpus of defi-
nition sentences where all terms and definitions are
1While Navigli and Velardi (2010) tagged terms and defini-
tions explicitly in their corpus, their evaluation restricts itself to
the task of definition sentence identification.
annotated. While Navigli and Velardi (2010) com-
piled the WCL definition corpus from the English
Wikipedia pages, we note that Wikipedia has stylis-
tic conventions that make detection of definitions
much easier than in the general case (i.e., ?The first
paragraph defines the topic with a neutral point of
view?2). This makes it unsuitable for training a gen-
eral extraction system from scholarly text.
As such, we choose to construct our own dataset
from articles collected from the ACL ARC, follow-
ing (Reiplinger et al, 2012). We compiled a corpus
? the W00 corpus, named for its prefix in the ACL
Anthology ? derived from a total of 234 workshop
papers published in 2000. Due to limited resources
and time, only one individual (the first author) per-
formed the corpus annotation. We built three dis-
joint prototype classifiers to further filter the sen-
tences. The prototype classifiers are based on sur-
face patterns, keyphrases and linguistic phenomena
(# of NP, # of VP, etc.). We took all the 2,512 sen-
tences marked as definition sentences by at least one
of our individual prototypes and proceeded to an-
notate all of them. In total, 865 of the total 2,512
sentences were real definition sentences.
The annotation instance is a single token (includ-
ing single word or punctuation mark). Each token
wi was marked with ai from A ? {T,D,O} depend-
ing on whether it is part of a term, a definition or
neither (other). Therefore, a sentence that is not a
definition sentence would have all its tokens marked
as O. The corpus and its annotations are available
for comparative study3.
We use Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001) to extract the term and defini-
tion from input. We incorporate evidence (features)
drawn from several levels of granularity: from the
word-, sentence- and document-levels, not limiting
ourselves to the window of previous n words. CRFs
allow us to encode such features which may not be
conditionally independent. We use the open source
implementation, CRF++ (Kudo, 2005) in our work4.
One straightforward approach is to train indepen-
dent classifiers for terms and definitions, which we
2http://en.wikipedia.org/wiki/Wikipedia:
Manual_of_Style/Lead_section
3http://wing.comp.nus.edu.sg/downloads/
term_definition_mining/
4http://code.google.com/p/crfpp/
782
test in Section 4.1. While simple, this is suboptimal
as it ignores the correlation between the presence
of the two components. Term identification (in the
guise of key-word/-phrase extraction) is well studied
and common features such as tf.idf and English or-
thography achieve satisfactory results (Nguyen and
Kan, 2007). In contrast, definitions exhibit more
flexible structure and hence are more difficult to dis-
tinguish from normal English text.
As such, we further investigate a serial archi-
tecture where we perform the classifications in se-
quence. I.e., first utilizing the results from term clas-
sification, and then incorporating them into defini-
tion classification. This two-stage architecture is ex-
plored later in Section 4.2
Our expanded feature set is an amalgamation of
related works and utilizes a mix of simple lexical,
orthography, dictionary lookup and corpus features
(here, idf ). Note that each feature class may derive
more than one feature (e.g., for the POS tag feature,
we extract features from not only the current word
but the surrounding contextual words as well). We
now enumerate the feature classes (FCs) that we
exploit, marking whether they apply to the (W)ord,
(S)entence or (D)ocument levels:
FC1) Lexical (W): The word, POS tag, stemmed word,
and if the word contains a signal suffix5.
FC2) Orthography (W): Whether the word is 1) capital-
ized or 2) mixed case; whether the word contains 3) hy-
phens or 4) digits.
FC3) Keyphrase List (W): Whether the word is in the
keyphrase list of the origin document. We use the open
source KEA keyphrase extraction system (Witten et al,
1999) to extract 20 keyphrases for each document.
FC4) Corpus (W): Discretized Inverse Document Fre-
quency (idf ), calculated as log(Nc ), where N is the total
number of documents and c is the number of occurrences
of the word in all the documents. IDF values are dis-
cretize into eight uniform partitions.
FC5) Position (D,S): The 1) Section ID, 2) name and 3)
the sentence?s relative position in the document.
FC6) Has acronym (S): Whether the sentence contains an
acronym. We use Stanford dependency parser (Cer et al,
2010) to parse the sentences. We deem the sentence to
contain an acronym if the dependency type ?abbrev? is
present in the output of the parser.
FC7) Surface pattern (S): Whether the sentence contain
5The suffixes we extract are ?-ion?,?-ity?,?-tor?,?-ics?,?-
ment?,?-ive? and ?-ic?.
<term > defined (as|by) <definition>
define(s)? <term> as <definition>
definition of <term> <definition>
<term> a measure of <definition>
<term> is DT <definition> (that|which|where)
<term> comprise(s)? <definition>
<term> consist(s)? of <definition>
<term> denote(s)? <definition>
<term> designate(s)? <definition>
<definition> (is|are|also) called <term>
<definition> (is|are|also) known as <term>
Table 1: Hand-crafted surface patterns used in DefMiner.
one of the hand-crafted pattern, as listed in Table 1. The
list is compiled from previous works and augmented
based on our observations on the corpus.
Long Distance Features. During develop-
ment, we noticed that the syntactic variation of
the definition might benefit from features that
identify long-distance dependencies. As such, we
further studied the impact of including additional
features developed from the shallow (chunk) and
dependency parses of the input. Compared to
the above features, these features are much more
computationally-intensive.
FC8) Shallow tag (W): Shallow parsing tag for each word
(e.g., np, vp). We used OpenNLP toolkit to shallow parse
the sentences. (Baldridge, 2005)
FC9) Shallow pattern (S): If the shallow parsing sequence
contains one of the seven parse patterns listed in Table 2.
We also give some example sentences which can be de-
tected by the patterns.
FC10) Governor (W): The word that the current word de-
pends on in a binary dependency relation. (e.g., for the
phrase computational linguistics, the governor of the
word computational is linguistics).
FC11) Dependency path distance (W): Distance from the
current word to the root of the sentence in the dependency
tree.
FC12) Typed dependency path (W): The dependency
path from the current word to the root of the sentence
(recording the dependency types instead of the words in
the path).
783
Pattern Example
NP : NP JavaRAP : An open-source implementa-
tion of the RAP
NP is * NP IR is the activity of obtaining informa-
tion resources
NP is * NP
that/of/which
NLP is a subject which is well studied
NP or NP Conditional Random field or CRF tack-
les ...
known as NP The corpus of English Wikipedia pages,
known as EnWiki
NP ( * NP) Hidden Markov Model (HMM) is used
to solve ...
NP defined by/as
* NP
The accuracy is defined by the produc-
tion of ...
Table 2: Hand-crafted shallow parsing patterns used in
DefMiner.
4 Evaluation
We now assess the overall effectiveness of
DefMiner, at both the word and sentence level. Ad-
ditionally, we want to ascertain the performance
changes as we add features to an informed lexical
baseline. We not only benchmark DefMiner?s per-
formance over our own W00 collection, but also
compare DefMiner against previous published work
on the definition sentence identification task on the
WCL (English Wikipedia) corpus.
4.1 Single-Pass Extraction from W00
We run ten-fold cross validation on our annotated
W00 corpus. We first evaluate our results at word
level, calculating the precision, recall, and F1 scores
for each incrementally enhanced feature set. We
present results on the corpus in the top portion of
Table 3 (Rows 1?9).
We calculate both micro and macro- (category)
averaged F1 scores for term and definition extrac-
tion. Fmicro assigns equal weight to every token,
while Fmacro gives equal weight to every category.
As definition tokens greatly outnumber term tokens
in our corpus (roughly 6:1), we feel that the macro-
average is a better indicator of the balance between
term and definition identification.
Our baseline system makes use of basic word
and POS tag sequences as features (FC1), which
are common to baselines in other sequence labeling
works. We can see that most features result in per-
formance improvements to the baseline, especially
for recall. Interestingly, although the shallow pars-
ing and dependency features we use are rather sim-
ple, they effectively improve the performance of the
system. In System 7, we only use the seven shallow
parsing patterns shown in Table 2, but the Fmacro
measure improves 3%. Our best single-stage sys-
tem (System 9 in Table 3) boosts recall for term and
definition classification by 7% and 5%, respectively,
without sacrificing precision. The Fmacro measure
is improved from 0.44 to 0.48.
Unexpectedly, the inclusion of the position fea-
tures cause performance to drop. One possible rea-
son is that the authors of scientific papers have
more flexibility to choose the positions to present
definitions. This makes the position feature much
less indicative (compared to running on a corpus of
Wikipedia articles). Due to this observation, we ex-
clude the position features when carrying out fol-
lowing experiments.
4.2 Serial Term and Definition Classification
We now investigate the two-stage, serial archi-
tecture where the system first performs term
classification before definition classification (i.e.,
termdefinition). We provision the second-stage
definition classifier with three additional features
from the first-stage term classification output:
whether the current word (1) is a term, and (2) ap-
pears before or (3) after a term.
Row 10 shows this resulting system, which we
coin as DefMiner. Interestingly, there is a 10% in-
crease in the precision of definition classification.
With the two-stage classifier, Fmacro score further
increases from 0.48 to 0.51. The results verify our
intuition that term classification does help in defini-
tion classification. Pipelining in the opposite direc-
tion (definitionterm; Row 11) does not show any
improvement. We posit that since the advantage is
only in a single direction, joint inference may be less
likely to yield benefits.
To determine the upper bound performance that
could result from proper term identification, we pro-
vided correct, oracular term labels from our ground
truth annotations in our corpus to the second-stage
definition classifier. This scenario effectively upper-
bounds the performance that perfect term knowledge
has on definition classification. The results of this
system in Row 12 indicates a strong positive influ-
ence on definition extraction, improving definition
extraction from 49% to 80%, a leap of 31%. This
784
System / Feature Class (cf Section 3) Term Definition Overall
P R F1 P R F1 Fmicro Fmacro
1: Baseline (FC1) 0.49 0.34 0.40 0.41 0.49 0.45 0.45 0.44
2: (1) + Orthography (FC2) 0.46 0.35 0.40 0.42 0.51 0.46 0.46 0.44
3: (2) + Dictionary (FC3) 0.48 0.36 0.41 0.41 0.49 0.44 0.44 0.43
4: (3) + Corpus (FC4) 0.50 0.35 0.41 0.40 0.52 0.45 0.45 0.44
5: (4) + Position (FC5) 0.47 0.37 0.42 0.36 0.48 0.41 0.41 0.41
6: (4) + Shallow parsing tag (FC8) 0.51 0.38 0.43 0.41 0.50 0.45 0.45 0.44
7: (6) + Shallow parse pattern (FC9) 0.50 0.40 0.45 0.42 0.52 0.47 0.47 0.47
8: (7) + Surface pattern (FC7) 0.49 0.39 0.44 0.43 0.53 0.48 0.48 0.47
9: (8) + Dependency + acronym
(FC6,10,11,12)
0.50 0.41 0.45 0.45 0.54 0.49 0.49 0.48
10 [DefMiner]: (9) + 2-stage 0.50 0.41 0.45 0.55 0.58 0.56 0.55 0.51
11: (9) + Reverse 2-stage 0.50 0.40 0.44 0.45 0.54 0.49 0.49 0.48
12: (9) + Term Oracle N/A N/A N/A 0.79 0.82 0.80 N/A N/A
Table 3: 10-fold cross validation word-level performance over different system configurations on our W00 corpus.
motivates future work as how to improve the perfor-
mance of the term classifier so as to reap the benefits
possible with our two-stage classifier.
4.3 Comparative results over the WCL Dataset
For most of the related research reviewed, we could
neither obtain their source code nor the corpora used
in their work, making comparative evaluation diffi-
cult. To the best of our knowledge, Reiplinger et
al. (2012) is the only attempt to extract definitions
from the ACL ARC corpus, which is a superset of
our W00 corpus. It would be desirable to have a di-
rect comparison with their work, but their evaluation
method is mainly based on human judges and their
reported coverage of 90% is only for a sample, short
list of domain terms they defined in advance.
To directly compare with the previous, more com-
plex state-of-the-art system from (Navigli and Ve-
lardi, 2010), we evaluate DefMiner on the defini-
tion sentence detection task. For the sentence-level
evaluation, we calculate the P/R/F1 score based on
whether the sentence is a definition sentence. We
applied DefMiner on their whole WCL annotated
corpus, reporting results in Table 4. We random-
ized the definition and none-definition sentences in
their corpus and applied 10-folds cross validation.
In each each iteration we used 90% of the sentences
for training and 10% for testing.
Compared to their results reported in (Navigli and
System Token Level Sentence
Term Definition Level
P / R / F1 P / R / F1 P / R / F1
DefMiner .82/.78/.80 .82/.79/.81 .92/.79/.85
N&V ?10 ? / ? / ? ? / ? / ? .99/.61/.77
Table 4: Comparative performance over the WCL.
Velardi, 2010), DefMiner improves overall F1 by
8%. While certainly less precise (precision of 92%
versus 99%), recall is improved over their consid-
erably more complex WCL-3 algorithm by almost
20%. Even using just the simple heuristic of only
classifying sentences that have identified terms as
well as definitions as definition sentences, DefMiner
serves to competitively identify definition sentences.
4.4 Manual Inspection of DefMiner Output
To gauge the noise from our system outside of our
cross-validation experiments, we conducted a man-
ual inspection of results over other workshop papers
from other years (2001 and 2002), as a sanity check.
DefMiner identifies 703 and 1,217 sentences in W01
and W02 as definition sentences separately.
Overall, 77.8% of the extracted sentences are real
definition sentences, while the remaining are false
positives.
785
4.4.1 Analysis of Common Errors
The P/R/F1 score by itself only gives a hint
of the system?s overall performance. We are also
interested to study the common errors made by our
system, which could help us engineer better features
for improving DefMiner. As our two-stage classifier
still lags behind the system with oracular term
labels by 24% in F1 for definition detection (Sec-
tion 4.2), we believe there is still much room for
improvement. We show three example misclassified
sentences that represent the major types of errors
we observed, where DefMiner?s output annotations
follow tokens marked as part of terms or definitions.
1) A PSS/TERM thus contains abstract linguistic values
for closed features ( tense/DEF ,/DEF mood/DEF ,/DEF
voice/DEF ,/DEF number/DEF ,/DEF gender/DEF
,/DEF etc/DEF ./DEF ) .
This first instance shows that DefMiner tends to
mark the first several tokens as ?TERM? while the
real term appears somewhere else in the sentence.
The actual term being defined is ?closed features?
instead of ?PSS?. Many terms in the training set
appear at the beginning of the sentence and are
preceded by a determinant. ?PSS? is also likely
to receive a high IDF and orthographic shape
(capitalized) score and therefore are misclassified
as terms. It may be useful to thus model the (usual)
distance between the term and its definition in a
feature in future work.
2) Similarly , ?I?/TERM refers to an/DEF interior/DEF
character/DEF and/DEF ?L?/DEF indicates/DEF
the/DEF last/DEF character/DEF of/DEF a/DEF
word/DEF .
DefMiner is occasionally confused when encoun-
tering recursive definition or multiple definitions in
a single sentence. Sentence 2 contains two parallel
definitions. The classifier fails to classify ?L?
as a separate term, incorporating it as part of the
definition. One possible improvement is to break the
original sentence into clauses that are independent
from each other, perhaps by using even simple
surface cues such as coordinating conjunctions
marked by commas or ?and?.
3) Again one could argue that the ability to convey
such uncertainty and reliability information to a non-
specialist/TERM is a/DEF key/DEF advantage/DEF
of/DEF textual/DEF summaries/DEF over/DEF
graphs/DEF .
Another difficult problem faced by the classifier
is the lack of contextual information. In sentence
3), if we just look at part of the sentence ?a non-
specialist is a key advantage of textual summaries
over graphs?, without trying to understand the mean-
ing of the sentence, we may well conclude that it is
a definition sentence because of the cue phrase ?is
a?. But clearly, the whole sentence is not a defini-
tion sentence. More sophisticated features based on
the sentence parse tree have to be exploited to detect
such false positive examples.
5 Insights from the Definitions Extracted
from the ACL ARC
In this second half of the paper, we apply DefMiner
to gain insights on the distributional and lexical
properties of terms and definitions that appear in
the large corpus of computational linguistics publi-
cations represented by the ACL ARC (Bird et al,
2008). The ARC consists of 10,921 scholarly publi-
cations from ACL venues, of which our earlier W00
corpus is a subset (n.b., as such, there is a small
amount of overlap). We trained a model using the
whole of the W00 corpus and used the obtained clas-
sifier to identify a list of terms and definitions for
each publication in the ACL ARC.
Inspecting such output gives us an understanding
of the properties of definition components, eventu-
ally helping the community to define better features
to capture them, as well as intrinsically deepening
our knowledge of the natural language of definitions
and the structure of scientific discourse.
5.1 Demographics
From a term?s perspective we can introspect prop-
erties of the enclosing paper, the host sentence, the
term itself and its definition.
At the document level, we can analyze document
metadata: its venue (journal, conference or work-
shop published) and year of publication.
786
At the sentence level, we analyze the position of
the sentences that are definition sentences.
Focusing on terms, we want to find out in more
detail the technical terminology that is defined. Are
they different from general keyphrases? What type
of entities are defined? What words do these terms
consist of? What structures are common?
We are interested in analogous questions when
focusing on the accompanying definitions. How
many words or clauses do definition sentences
consist of? Do we lose a lot of recall by restricting
definitions to a single sentence? Are embedded
definitions (definitions embedded with other defini-
tions) common?
We highlight some specific findings from our ba-
sic analyses here:
Where do definitions occur? As terms are usu-
ally defined on first use, we expect the distribution
of definition sentences to skew towards the begin-
ning portions of a scientific document as input. We
count the occurrences of definition sentences in each
of ten equally-sized (by number of sentences) non-
overlapping partitions. The results are shown in Fig-
ure 1, aligning with our intuition: The first three
quantiles contribute almost 40% of all detected def-
inition sentences, while the last three quantiles con-
tain only 17.8%.
5898	 ? 6258	 ? 6574	 ? 6202	 ? 5512	 ? 5020	 ? 4341	 ? 3593	 ? 2931	 ? 2233	 ?
0	 ?
2000	 ?
4000	 ?
6000	 ?
8000	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ? 10	 ?
Fre
qu
en
cy
	 ?
Quan?le	 ?(1	 ?=	 ?first	 ?10%;	 ?10	 ?=	 ?last	 ?10%)	 ?
Figure 1: Occurrences of definitions within different seg-
ments of an article.
How long are the detected terms and defini-
tions? Figure 2 shows the detected aggregate dis-
tributions. Over 54% of the detected terms are sin-
gle tokens, where majority of the remaining 45% of
terms being multi-word terms of six words or less.
Among the single token terms, a further analysis
reveals that 17.4% are detected as single-character
variables, 34.9% are acronyms (consisting of more
than one capitalized letters), while the remaining
47.7% are normal words. Definitions, in contrast,
are longer and more varied, with a peak length of
nine words. Slightly over half of the definitions have
a length of 5?16 words; 75% have lengths between
3 and 23 words.
54.15	 ?
27.84	 ?
8.9	 ? 4.29	 ? 2.4	 ? 1.32	 ? 0.65	 ? 0.33	 ? 0.23	 ? 0.15	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ? 10	 ?
Pe
rce
nt
ag
e	 ?
Number	 ?of	 ?Words	 ?
(t) Term length distribution.
0	 ?
1	 ?
2	 ?
3	 ?
4	 ?
5	 ?
6	 ?
1	 ? 6	 ? 11	 ? 16	 ? 21	 ? 26	 ? 31	 ? 36	 ? 41	 ? 46	 ? 51	 ? 56	 ?
Pe
rce
nt
ag
e	 ?
Number	 ?of	 ?Words	 ?
(b) Definition length distribution.
Figure 2: Length distributions of (top) terms, (bottom)
definitions.
In Table 5, we present the 10 most frequent POS
tag bigrams for terms and definitions. We can see
that among terms, a sequence of consecutive two
nouns is most common, making up four out of the
top five bigrams. We notice that determiners and
prepositions are absent from the term list but are
common in definitions.
5.2 Inspection of Definition over Time
The ACL ARC covers journal articles, conference
and workshop proceedings over a few decades. As
with other fields in recent years, the amount of
computational linguistics literature has steadily in-
creased over the number of years.
We study if definitions appear as frequently in dif-
ferent types of scientific articles (e.g. journals, con-
ference papers or workshop papers). We also want
to investigate if there is a significant shift in the dis-
tribution of definitions across years. In Figure 3,
we present the density of definitions (defined as the
787
Term Definition
NNP NNP DT NN
NN NN NNP NNP
NNP NN NN IN
NN NNP IN DT
JJ NN NN NN
NN JJ JJ NN
NNP : NN :
: NNP DT JJ
NN NNS NNS IN
NN ? NN NNS
Table 5: Most frequent POS bigrams for terms and defi-
nitions.
percentage of sentences that are identified as defini-
tion sentences), for these three different categories
of publications6.
In Figure 3, the three data series overlap each
other, so we cannot conclude definitions appear
more often in one type of papers than another. How-
ever, as a side effect, we see that while the definition
density for journal papers remain relatively constant,
for conference and workshop papers the number of
definitions extracted per sentence has increased no-
ticeably over time. The average number of defini-
tions presented in conference papers, for instance,
increased more than 100% in the 40 years repre-
sented in the ACL ARC.
The increasing number of definitions alone does
not show that new knowledge is introduced at a
faster rate, as definitions may be repeated. To con-
trol for this effect, we also need to know which def-
initions are new or defined in previous year(s). We
studied this effect in more detail for the relatively
smaller set of journal papers (Figure 4). For jour-
nal papers, the number of definitions of previously
introduced terms in each year against the number of
new definitions. We say a definition is new when the
detected term was not identified in any article (not
limited to journals) in previous years.
We see that the number of new terms being de-
fined also increases with the years. But the increase
is much slower than that for the total definitions. The
area between the two lines denotes the definitions
6The ACL ARC is organized by the venue of the publication,
which is associated to a category. For the assigned category for
each venue please refer to Appendix A.
where the same term has been multiply defined from
the same or previous years as the current year under
investigation.
5.3 Trends
We can use terms and definitions to also intro-
spect how the computational linguistics literature
has changed over time. Table 6 shows a subset of
the most frequently defined terms in the ACL ARC,
where we exclude single-character terms (?vari-
ables?).
WordNet (292) Part Of Speech (45)
Precision (172) Probablistic CFG (43)
Recall (167) FrameNet (38)
Noun phrase (97) Conditional Random Field (29)
Word sense disambiguation (60) Inverse document frequency (28)
Support Vector Machine (60) PropBank (27)
Hidden Markov Model (54) Context Free Grammar (25)
Latent Semantic Analysis (57) Accuracy (20)
Table 6: Subset of most frequently defined terms. Raw
counts in parentheses. Variations of the same term (e.g.
plurals, acronyms) are collapsed into one instance.
To be expected, these popular terms are mostly
specific to computational linguistics. From our
observation, we can fit these terms into one of
three categories, including 1) resources (WordNet,
FrameNet, PropBank), 2) methodologies (SVM,
HMM, LSA), and 3) evaluation metrics (Precision,
Recall, Accuracy). We feel that the final category
of evaluation metrics is more general and would be
shared among other scientific disciplines.
An interesting analysis that follows from this cat-
egorization is that we can study major trends and
changes in the research directions of the commu-
nity. This can help to draw the attention of re-
searchers to emerging trends. We illustrate this ap-
proach in Figure 5 that focuses on three sequence
labeling methodologies that have been used to ad-
dress similar problems ? namely, hidden Markov
model (HMM), maximum entropy Markov model
(MEMM), and conditional random fields (CRF) ?
during the period from 1989 to 2006 (where we have
sufficient data points). From the early 90s, we see
that HMM was a clear favorite. However since 2000,
MEMM gained in popularity and use. Lafferty et
al. (2001) introduced CRFs in 2001 and the new
methodology was widely adopted soon after that.
788
0.000	 ?
0.005	 ?
0.010	 ?
0.015	 ?
0.020	 ?
0.025	 ?
0.030	 ?
0.035	 ?
1967	 ? 1973	 ? 1979	 ? 1982	 ? 1987	 ? 1993	 ? 1999	 ? 2005	 ?
De
fin
i?n	 ?
De
ns
ity
	 ?
Year	 ?
Conference	 ?Papers	 ? Journals	 ? Workshops	 ?
Figure 3: Occurrences of definitions across publication cat-
egories.
0	 ?
50	 ?
100	 ?
150	 ?
200	 ?
250	 ?
300	 ?
350	 ?
1980	 ? 1982	 ? 1985	 ? 1988	 ? 1992	 ? 1996	 ? 2001	 ? 2004	 ?
#	 ?C
oll
ec
te
d	 ?D
efi
ni?
on
s	 ?
Year	 ?
New	 ?Terms	 ? Previously	 ?Defined	 ?Terms	 ?
Figure 4: Relative proportions of new and recurring defini-
tions in journal papers.
Figure 5: The occurrence of definitions for various se-
quence labeling methodologies over the years.
6 Conclusions and Future Work
We study the task of identifying definition sentences,
as a two-part entity containing a term and its ac-
companying definition. Unlike previous work, we
propose the harder task of delimiting the component
term and definitions, which admits sequence label-
ing methodologies as a compatible solution.
Leveraging the current best practice of using con-
ditional random fields, we contribute two additional
ideas that lead to DefMiner, a state-of-the-art schol-
arly definition mining system. First, we show that
shallow parsing and dependency parse features that
may provide additional non-local information, are
useful in improving task performance. Second,
viewing the problem as two correlated subproblems
of term and definition extraction, we measure the
tightness and dependency of the correlation. We
find that a two-stage sequential learning architecture
(term first, definition second) leads to best perfor-
mance. DefMiner outperforms the state of the art,
and we feel is fit for macroscopic analysis of scien-
tific corpora, despite significant noise.
We thus deployed DefMiner on the ACL An-
thology Reference Corpus. We demonstrate how
DefMiner can yield insights into both the struc-
ture and semantics of terms and definitions, in both
static and diachronic modes. We think future work
could pursue more in-depth analysis of the distribu-
tional and demographic properties of automatically
extracted lexica. We can use the lexicon obtained
from different years to carry out trend prediction,
which we have illustrated here. Downstream sys-
tems may predict which term will become popular,
or could alert an author if their definition of a term
significantly differs from the original source.
We hope to tackle the annotation bottleneck in fu-
ture work on definition extraction, common in many
data-driven learning fields. We plan to explore iter-
ative, semi-supervised methods to best manage hu-
man effort to maximize the effectiveness of future
annotation.
In addition, with respect to modeling, although
we showed that doing definition classification before
term classification does not improve over our single-
stage classifier, we hope to study whether suitable
joint inference models can benefit from the interac-
tion between the two classification processes.
789
References
Jason Baldridge. 2005. The opennlp project.
Steven Bird, Robert Dale, Bonnie J. Dorr, Bryan Gib-
son, Mark T. Joseph, Min-Yen Kan, Dongwon Lee,
Brett Powley, Dragomir R. Radev, and Yee Fan Tan.
2008. The ACL anthology reference corpus: A ref-
erence dataset for bibliographic research in computa-
tional linguistics. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation Conference (LREC08), pages 1755?1759, Mar-
rakech, Morocco.
Claudia Borg, Mike Rosner, and Gordon Pace. 2009.
Evolutionary algorithms for definition extraction. In
Proceedings of the 1st Workshop on Definition Extrac-
tion, WDE ?09, pages 26?32, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
stanford dependencies: Trade-offs between speed and
accuracy. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC 2010), Val-
letta, Malta.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL workshop on Learning Structured In-
formation in Natural Language Applications, Trento,
Italy.
Taku Kudo. 2005. Crf++: Yet another crf toolkit. Soft-
ware available at http://crfpp. sourceforge. net.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning, ICML ?01, pages 282?
289, San Francisco, CA, USA.
Smaranda Muresan and Judith Klavans. 2002. A method
for automatically building and evaluating dictionary
resources. In Proceedings of the Language Resources
and Evaluation Conference (LREC), Las Palmas, Ca-
nary Islands, Spain.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318?1327, Uppsala, Sweden.
Thuy Dung Nguyen and Min-Yen Kan. 2007. Keyphrase
extraction in scientific publications. In Proceedings of
International Conference on Asian Digital Libraries,
pages 317?326, Hanoi, Vietnam.
Jennifer Pearson. 1996. The expression of definitions in
specialised texts: a corpus-based analysis. In Proceed-
ings of Euralex 96.
Melanie Reiplinger, Ulrich Schafer, and Magdalena Wol-
ska. 2012. Extracting glossary sentences from
scholarly articles: a comparative evaluation of pattern
bootstrapping and deep analysis. In Proceedings of
the ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries, ACL ?12, pages 55?65, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eline Westerhout and Paola Monachesi. 2007. Extrac-
tion of dutch definitory contexts for elearning pur-
poses. In Proceedings of Computational Linguistics
(CLIN 2007).
Eline Westerhout. 2009. Definition extraction using
linguistic and structural features. In Proceedings of
the 1st Workshop on Definition Extraction, WDE ?09,
pages 61?67, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ian H Witten, Gordon W Paynter, Eibe Frank, Carl
Gutwin, and Craig G Nevill-Manning. 1999. Kea :
Practical automatic keyphrase extraction. Computer,
pp:254?255.
790
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 923?933,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exploiting Timelines to Enhance Multi-document Summarization
Jun-Ping Ng
1,2
, Yan Chen
3
, Min-Yen Kan
2,4
, Zhoujun Li
3
1
DSO National Laboratories, Singapore
2
School of Computing, National University of Singapore, Singapore
3
State Key Laboratory of Software Development Environment, Beihang University, China
4
Interactive and Digital Media Institute, National University of Singapore, Singapore
njunping@dso.org.sg
Abstract
We study the use of temporal information
in the form of timelines to enhance multi-
document summarization. We employ a
fully automated temporal processing sys-
tem to generate a timeline for each in-
put document. We derive three features
from these timelines, and show that their
use in supervised summarization lead to a
significant 4.1% improvement in ROUGE
performance over a state-of-the-art base-
line. In addition, we propose TIMEMMR,
a modification to Maximal Marginal Rel-
evance that promotes temporal diversity
by way of computing time span similar-
ity, and show its utility in summarizing
certain document sets. We also propose a
filtering metric to discard noisy timelines
generated by our automatic processes, to
purify the timeline input for summariza-
tion. By selectively using timelines guided
by filtering, overall summarization perfor-
mance is increased by a significant 5.9%.
1 Introduction
There has been a good amount of research in-
vested into improving the temporal interpretation
of text. Besides the increasing availability of an-
notation standards (e.g., TIMEML (Pustejovsky et
al., 2003a)) and corpora (e.g., TIDES (Ferro et
al., 2000), TimeBank (Pustejovsky et al, 2003b)),
the community has also organized three success-
ful evaluation workshops ? TempEval-1 (Verha-
gen et al, 2009), -2 (Verhagen et al, 2010), and
-3 (Uzzaman et al, 2013). As the state-of-the-
art improves, these workshops have moved away
from the piecemeal evaluation of individual tem-
poral processing tasks and towards the evaluation
of complete end-to-end systems in TempEval-3.
We believe our understanding of the temporal in-
formation found in text is sufficiently robust, and
that there is an opportunity to now leverage this in-
formation in downstream applications. In this pa-
per, we present our work in incorporating the use
of such temporal information into multi-document
summarization.
The goal of multi-document summarization is
to generate a summary which includes the main
points from an input collection of documents with
minimal repetition of similar points. We hope to
improve the quality of the summaries that are gen-
erated by considering temporal information found
in the input text. To motivate how temporal in-
formation can be useful in summarization, let us
refer to Figure 1. The three sentences describe a
recent cyclone and a previous one which happened
in 1991. Recognizing that sentence (3) is about a
storm that had happened in the past is important
when writing a summary about the recent storm,
as it is not relevant and can likely be excluded.
It is reasonable to expect that a collection of
documents about the recent storm will contain
more references to it, compared with the earlier
one that happened in 1991. Visualized on a time-
line, this will translate to more events (bolded in
Figure 1) around the time when the recent storm
occurred. There should be fewer events mentioned
in the collection for the earlier 1991 time period.
Figure 2 illustrates a possible timeline laid out
with the events found in Figure 1. The events
from the more recent storm are found together at
the same time. There are fewer events which talk
about the previous storm. Thus, temporal informa-
tion does assist in identifying which sentences are
more relevant to the final summary.
Our work is significant as it addresses an im-
portant gap in the exploitation of temporal infor-
mation. While there has been prior work making
use of temporal information for multi-document
923
(1) A fierce cyclone packing extreme winds and torrential rain smashed into Bangladesh?s southwestern coast Thursday,
wiping out homes and trees in what officials described as the worst storm in years.
(2) More than 100,000 coastal villagers have been evacuated before the cyclone made landfall.
(3) The storm matched one in 1991 that sparked a tidal wave that killed an estimated 138,000 people, Karmakar told AFP.
Figure 1: Modified extract from a news article which describes a cyclone landfall. Several events which
appear in Figure 2 are bolded.
smashed
packing
wiping
describedsparked killed
...
Storm in 1991 Latest cyclone
evacuated
2013-Feb-13 11:32 +0000
Figure 2: Possible timeline for events in Figure 1.
summarization, they 1) have been largely con-
fined to helping to chronologically order content
within summaries (Barzilay et al, 1999), or 2)
focus only on the use of recency as an indicator
of saliency (Goldstein et al, 2000; Wan, 2007).
In this work we construct timelines (as a repre-
sentation of temporal information) automatically
and incorporate them into a state-of-the-art multi-
document summarization system. This is achieved
with 1) three novel features derived from time-
lines to help measure the saliency of sentences,
as well as 2) TIMEMMR, a modification to the
traditional Maximal Marginal Relevance (MMR)
(Carbonell and Goldstein, 1998). TimeMMR pro-
motes diversity by additionally considering tem-
poral information instead of just lexical similari-
ties. Through these, we demonstrate that temporal
information is useful for multi-document summa-
rization. Compared to a competitive baseline, sig-
nificant improvements of up to 4.1% are obtained.
Automatic temporal processing systems are not
perfect yet, and this may have an impact on their
use for downstream applications. This work ad-
ditionally proposes the use of the lengths of time-
lines as a metric to gauge the usefulness of time-
lines. Together with the earlier described contribu-
tions, this metric further improves summarization,
yielding an overall 5.9% performance increase.
2 Related Work
Barzilay et al (1999) were one of the first to use
time for multi-document summarization. They
recognized the importance of generating a sum-
mary which presents the time perspective of the
summarized documents correctly. They estimated
the chronological ordering of events with a small
set of heuristics, and also made use of lexical pat-
terns to perform basic time normalization on terms
like ?today? relative to the document creation
time. The induced ordering is used to present the
selected summary content, following the chrono-
logical order in the original documents.
In another line of work, Goldstein et al (2000)
made use of the temporal ordering of documents
to be summarized. In computing the relevance of a
passage for inclusion into the final summary, they
considered the recency of the passage?s source
document. Passages from more recent documents
are deemed to be more important. Wan (2007)
and Demartini et al (2010) made similar assump-
tions in their work on TIMEDTEXTRANK and en-
tity summarization, respectively.
Instead of just considering the notion of re-
cency, Liu et al (2009) proposed an interesting
approach using a temporal graph. Events within
a document set correspond to vertices in their pro-
posed graph, while edges are determined by the
temporal ordering of events. From the resulting
weakly-connected graph, the largest forests are as-
sumed to contain key topics within the document
set and used to influence a scoring mechanism
which prefers sentences touching on these topics.
Wu (2008) also made use of the relative or-
dering of events. He assigned complete times-
tamps to events extracted from text. After lay-
ing out these events onto a timeline by making
use of these timestamps, the number of events that
happen within the same day is used to influence
sentence scoring. The motivation behind this ap-
proach is that days which have a large number of
events should be more important and more worthy
of reporting than others.
These prior works target either 1) sentence re-
ordering, or 2) the use of recency as an indicator of
saliency. In sentence re-ordering, final summaries
are re-arranged so that the extracted sentences that
form the summary are in a chronological order.
We argue that this may not be appropriate for all
summaries. Depending on the style of writing or
journalistic guidelines, a summary can arguably be
written in a number of ways. The use of recency
924
as an indicator of saliency is useful, yet disregards
other accessible temporal information. If a sum-
mary of a whole sequence of events is desired, re-
cency becomes less useful.
The work of Wu (2008) is closely related to one
of the features proposed in this paper. He had also
made use of temporal information to weight sen-
tences to generate summaries. However his ap-
proach is guided by the number of events hap-
pening within the same time span, and relies on
event co-referencing. In this work, we have sim-
plified this idea by dropping the need for event co-
referencing (removing a source of propagated er-
ror), and augmented it with two additional features
derived from timelines. By doing so, we are able
to make better use of the available temporal infor-
mation, taking into account all known events and
the time in which they occur.
A useful note here is that this work is ar-
guably different from the Temporal Summariza-
tion (TmpSum) track at the Text Retrieval Confer-
ence (Aslam et al, 2013). Given a large stream
of data in real-time, the purpose of the TmpSum
track is to look out for a query event, and retrieve
specific details about the event over a period of
time. Systems are also expected to identify the
source sentences from which these details are re-
trieved. This is not the same as our approach here,
which makes use of temporal information encoded
in timelines to generate prose summaries.
3 Methodology
To incorporate temporal information into multi-
document summarization, we adopt the workflow
in Figure 3, which has two key processes: 1) tem-
poral processing, and 2) summarization.
Input 
Documents
Input 
Documents
Input 
Documents
E-T
Temporal 
Classification
E-E
Temporal 
Classification
Event and Timex
Extraction
Sentence
Scoring
Sentence
Re-ordering
Pre-processing
Summary
Summarization Pipeline
Temporal Processing
Timeline
Generation
Timex
Normalization
Timelines
Timelines
Figure 3: Incorporating temporal information into
the SWING summarization pipeline.
Temporal Processing generates timelines from
text, one for each input document. Timelines are
well-understood constructs which have often been
used to represent temporal information (Denis and
Muller, 2011; Do et al, 2012). They indicate the
temporal relationships between two basic tempo-
ral units: 1) events, and 2) time expressions (or
timexes for short). In this work, we adopt the
definitions proposed in the standardized TIMEML
annotation (Pustejovsky et al, 2003a). An event
refers to an eventuality, a situation that occurs or
an action; while a timex is a reference to a partic-
ular date or time (e.g. ?2013 December 31?).
Following the ?divide-and-conquer? approach
described in Verhagen et al (2010), results from
the three temporal processing steps: 1) timex nor-
malization, 2) event-timex temporal relationship
classification, and 3) event-event temporal rela-
tionship classification, are merged to obtain time-
lines (top half of Figure 3). We tap on existing
systems for each of these steps (Ng and Kan, 2012;
Str?otgen and Gertz, 2013; Ng et al, 2013).
Summarization. We make use of a state-of-
the-art summarization system, SWING (Ng et al,
2012) (bottom half of Figure 3). SWING is a su-
pervised, extractive summarization system which
ranks sentences based on scores computed using
a set of features in the Sentence Scoring phase.
The Maximal Marginal Relevance (MMR) algo-
rithm is then used in the Sentence Re-ordering
phase to re-order and select sentences to form the
final summary. The timelines built in the ear-
lier temporal processing can be incorporated into
this pipeline by deriving a set of features used to
score sentences in Sentence Scoring, and as input
to the MMR algorithm when computing similarity
in Sentence Re-ordering.
3.1 Timelines from Temporal Processing
A typical timeline used in this work has been
shown earlier in Figure 2. The arrowed, horizon-
tal axis is the timeline itself. The timeline can
be viewed as a continuum of time, with points on
the timeline referring to specific moments of time.
Small solid blocks on the timeline itself are ref-
erences to absolute timestamps along the timeline
(e.g., ?2013-Feb-13 11:32 +0000? in the figure).
The black square boxes above the timeline de-
note events. Events can either occur at a specific
instance of time (e.g., an explosion), or over a pe-
riod of time (e.g. a football match). Generalizing,
we refer to the time period an event takes place in
as its time span (vertical dotted lines). As a simpli-
925
left peak of e right peak of e
b
i
g
g
e
s
t
 
c
l
u
s
t
e
r
Time Span A Time Span A+4
e
Figure 4: A simplified timeline illustrating how
the various timeline features can be derived.
fying assumption, events are laid out on the time-
line based on the starting time of their time span.
Note that in our work, time spans may not cor-
respond to specific instances of time, but instead
help in inferring an ordering of events. Events
which appear to the left of others take place ear-
lier, while events within the same time span hap-
pen together over the same time period.
3.2 Sentence Scoring with Timelines
We derive three features from the constructed
timelines, which are then used for downstream
Sentence Scoring. Figure 4 shows a simplified
timeline, along with annotations that are refer-
enced in this section to help explain how these
timeline features are derived.
1. Time Span Importance (TSI). We hypothe-
size that when more events happen within a partic-
ular time span, that time span is potentially more
relevant for summarization. Sentences that men-
tion events found in such a time span should be as-
signed higher scores. Referring to Figure 1, whose
timeline is shown in Figure 2, we see that the time
span with the most number of events is when the
latest cyclone made landfall. Assigning higher
scores for sentences which contain events in this
time span will help us to select more relevant sen-
tences if we want a summary about the cyclone.
Let TS
L
be the time span with the largest num-
ber of events in a timeline. The importance of
a time span TS
i
is computed by normalizing the
number of events in TS
i
against the number of
events in TS
L
. The TSI of a sentence s is then
the sum of the time span importance associated to
all the words in s:
TSI(s) =
?
w?s
|TS
w
|
|TS
L
|
|s|
(1)
where TS
w
denotes the time span which a word
w is associated with, and |TS
w
| is the number of
events within the time span.
2. Contextual Time Span Importance
(CTSI). The importance of a time span may not
depend solely on the number of events that hap-
pen within it. If it is near time spans which are
?important? (i.e., one that has a large number of
events), it should also be of relative importance. A
more concrete illustration of this can also be seen
in Figure 1. Sentence (2) explains that a lot of peo-
ple have been evacuated prior to the cyclone mak-
ing landfall. It is imaginable that this can be use-
ful information to be included in a summary, even
though from looking at the corresponding timeline
in Figure 2, the ?evacuated? event falls in a time
span with a low importance score (i.e., the time
span only has one event). CTSI seeks to promote
sentences such as this.
We derive the CTSI of a sentence by first com-
puting the contextual importance of words in the
sentence. We define the contextual importance of
a word found in time span TS
i
as a weighted sum
of the time span importance of the two nearest
peaks TS
lp
and TS
rp
found to the left and right
of TS
i
, respectively. In Figure 4, taking reference
from event e (shaded in black), the left peak to the
time span which e is in happens to be time span
A, while the right peak is time span A + 4. The
contribution of each peak to the weighted sum is
decayed by its distance from TS
i
. Formally, the
contextual time span importance of a word w can
be expressed as:
?(w) = ?
(
I
lp
|TS
w
? TS
lp
|
)
? (1? ?)
(
I
rp
|TS
rp
? TS
w
|
)
(2)
where TS
w
is the time span associated with w. I
lp
and I
rp
are the time span importance of the peaks
to the left and right of TS
w
respectively, while
|TS
w
? TS
lp
| and |TS
rp
? TS
w
| are the num-
ber of time spans between the left and right peaks
of TS
w
respectively. ? balances the importance of
the left and right peaks, intuitively set to 0.5. The
CTSI of a sentence is computed as:
CTSI(s) =
?
e?E
s
?(e)
|E
s
|
(3)
where E
s
denotes the set of events words in s.
3. Sentence Temporal Coverage Density
(TCD). We first define the temporal coverage of a
sentence. This corresponds to the number of time
spans that the events in a sentence talk about. Sup-
pose a sentence contains events which are associ-
ated with time spans TS
a
, TS
b
, TS
c
. The time
spans are ordered in the sequence they appear on
926
the timeline. Then the temporal coverage of a sen-
tence is defined as the number of time spans be-
tween the earliest time span TS
a
and the latest
time span TS
c
. Referring to Figure 4, suppose
a sentence contains the three events which have
been shaded black. The temporal coverage in this
case includes all the time spans from time span A
to time span A+ 4, inclusive.
The constraint on the number of sentences that
can be included in a summary requires us to select
compact sentences which contain as many rele-
vant facts as possible. Traditional lexical measures
may attempt to achieve this by computing the ra-
tio of keyphrases to the number of words in a sen-
tence (Gong and Liu, 2001). Stated equivalently,
when two sentences are of the same length, if one
contains more keyphrases, it should contain more
useful facts. TCD parallels this idea with the use
of temporal information, i.e. if two sentences are
of the same temporal coverage, then the one with
more events should carry more useful facts.
Formally, if a sentence s contains events E
s
=
{e
1
, . . . , e
n
}, where each event is associated with
a time span TS
i
, then TCD is computed using:
TCD(s) =
|E
s
|
|TS
n
? TS
1
|
(4)
where |E
s
| is the number of events found in s, and
|TS
n
? TS
1
| is the temporal coverage of s.
3.3 Enhancing MMR with TimeMMR
In the sentence re-ordering stage of the SWING
pipeline, the iterative MMR algorithm is used to
adjust the score of a candidate sentence, s. In each
iteration, s is penalized if it is lexically similar to
other sentences that have already been selected to
form the eventual summary S = {s
1
, s
2
, . . .}. The
motivating idea is to reduce repeated information
by preferring sentences which bring in new facts.
Incorporating temporal information can poten-
tially improve this. In Figure 5, the sentences de-
scribe many events which took place within the
same time span. They describe the destruction
caused by a hurricane with trees uprooted and
buildings blown away. A summary about the hur-
ricane need not contain all of these sentences as
they are all describing the same thing. However
it is not trivial for the lexically-motivated MMR
algorithm to detect that events like ?passed?, ?up-
rooted? or ?damaged? are in fact repetitive.
Thus, we propose further penalizing the score
of s if it contains events that happen in similar
time spans as those contained in sentences within
S. We refer to this as TIMEMMR. Modifying the
MMR equation from Ng et al (2012):
TimeMMR(s) = Score(s)? ?R2(s, S)? (1? ?)T (s, S) (5)
where Score(s) is the score of s, S is the set of
sentences already selected to be in the summary
from previous iterations, and R2 is the predicted
ROUGE-2 score of s with respect to the already
selected sentences (S). ? is a weighting parameter
which is empirically set to 0.9 after tuning over a
development dataset. T is the proportion of events
in swhich happen in the same time span as another
event in any other sentence in S. Two events are
said to be in the same time span if one happens
within the time period the other happens in. For
example, an event that takes place in ?2014 June?
is said to take place within the year ?2014?.
While TIMEMMR is proposed here as an im-
provement over MMR, the premise is that incor-
porating temporal information can be helpful to
minimize redundancy in summaries. In future
work, one could apply it to other state-of-the-art
lexical-based approaches including that of Hen-
drickx et al (2009) and Celikyilmaz and Hakkani-
Tur (2010). We also believe the same idea can be
transplanted even to non-lexical motivated tech-
niques such as the corpus-based similarity mea-
sure proposed by Xie and Liu (2008). We chose
to use MMR here as a proof-of-concept to demon-
strate the viability of such a technique, and to eas-
ily integrate our work into SWING.
3.4 Gauging Usefulness of Timelines
Temporal processing is imperfect. Together with
the simplifying assumptions that were made in
timeline construction, our generated timelines
have errors which propagate into the summariza-
tion process. With this in mind, we selectively em-
ploy timelines to generate summaries only when
we are confident of their accuracy. This can be
done by computing a metric which can be used to
decide whether or not timelines should be used for
a particular input document collection. We refer to
this as reliability filtering.
We postulate that the length of a timeline can
serve as a simple reliability filtering metric. The
intuition for this is that for longer timelines (which
contain more events), possible errors are spread
over the entire timeline, and do not overpower any
useful signal that can be obtained from the time-
line features outlined earlier. Errors are however
927
(1) An official in Barisal, 120 kilometres south of Dhaka, spoke of severe destruction as the 500 kilometre-wide mass of cloud
passed overhead.
(2) ?Many trees have been uprooted and houses and schools blown away,? Mostofa Kamal, a district relief and rehabilitation
officer, told AFP by telephone.
(3) ?Mud huts have been damaged and the roofs of several houses blown off,? said the state?s relief minister, Mortaza Hossain.
Figure 5: Extract from a news article which describes several events (bolded) happening at the same
time.
very easily propagated into summary generation
for shorter timelines, leading to less useful results.
We incorporate this into our process as follows:
given an input document collection (which con-
sists of 10 documents), the average size of all the
timelines for each of these 10 documents is com-
puted. Only when this value is larger than a thresh-
old value are the timelines used.
4 Experiments and Results
The proposed timeline features and TIMEMMR
were implemented on top of SWING, and eval-
uated on the test documents from TAC-2011
(Owczarzak and Dang, 2011). SWING makes use
of three generic features and two features targeted
specifically at guided summarization. Since the
focus of this paper is on multi-document summa-
rization, we employ only the three generic fea-
tures, i.e., 1) sentence position, 2) sentence length,
and 3) interpolated n-gram document frequency
in our experiments below. Summarization evalua-
tion is done using ROUGE-2 (R-2) (Lin and Hovy,
2003), as it has previously been shown to correlate
well with human assessment (Lin, 2004) and is of-
ten used to evaluate automatic text summarization.
The results obtained are shown in Table 1. In
the table, each row refers to a specific summariza-
tion system configuration. We also show the re-
sults of two reference systems, CLASSY (Conroy
et al, 2011) and POLYCOM (Zhang et al, 2011),
as benchmarks. CLASSY and POLYCOM are top
performing systems at TAC-2011 (ranked 2nd and
3rd by R-2 in TAC 2011, respectively; the full ver-
sion of SWING was ranked 1st with a R-2 score
of 0.1380). From these results, we can see that
SWING is a very competitive baseline.
Rows 9 to 16 additionally incorporate our time-
line reliability filtering. We assume that the var-
ious input document sets to be summarized are
available at the time of processing. Hence in these
experiments, the threshold for filtering is set to be
the average of all the timeline sizes over the whole
input dataset. In a production environment where
this assumption may not hold, this threshold could
Configuration R-2 Sig
R SWING 0.1339 NA
B1 CLASSY 0.1278 -
B2 POLYCOM 0.1227 **
Without Filtering
1 SWING+TSI+CTSI+TCD 0.1394 *
2 SWING+TSI+CTSI 0.1372 -
3 SWING+TSI+TCD 0.1372 -
4 SWING+CTSI+TCD 0.1387 *
5 SWING+TSI+CTSI+TCD+TMMR 0.1389 -
6 SWING+TSI+CTSI+TMMR 0.1374 -
7 SWING+TSI+TCD+TMMR 0.1343 -
8 SWING+CTSI+TCD+TMMR 0.1363 -
With Filtering
9 SWING+TSI+CTSI+TCD 0.1418 **
10 SWING+TSI+CTSI 0.1378 **
11 SWING+TSI+TCD 0.1389 **
12 SWING+CTSI+TCD 0.1401 **
13 SWING+TSI+CTSI+TCD+TMMR 0.1402 **
14 SWING+TSI+CTSI+TMMR 0.1397 **
15 SWING+TSI+TCD+TMMR 0.1376 *
16 SWING+CTSI+TCD+TMMR 0.1390 **
Table 1: R-2 scores after incorporating temporal
information into SWING. ?**? and ?*? denotes sig-
nificant differences with respect to Row R (paired
one-tailed Student?s t-test; p < 0.05 and p < 0.1
respectively), and TMMR denotes TIMEMMR.
be set by empirical tuning over a development set.
Row 1 shows the usefulness of the proposed
timeline-based features. A statistically significant
improvement of 4.1% is obtained with the use of
all three features over SWING. When we use re-
liability filtering (Row 9), this improvement in-
creases to 5.9%.
The ablation test results in Rows 2 to 4 show
a drop in R-2 each time a feature is left out. With
the exception of Row 4, removing a feature lessens
the improvement in R-2 to be insignificant from
SWING?s. The same drop occurs even when reli-
ability filtering is used (Rows 9 to 12). These in-
dicate that all the proposed features are important
and need to be used together to be effective.
Rows 5 to 8 and Rows 13 to 16 show the ef-
fect of TIMEMMR. While the results do not uni-
formly show that TIMEMMR is effective, it can be
helpful, such as when comparing Rows 2 and 6, or
Rows 10 and 14, where R-2 improves marginally.
Looking at Rows 1 to 8, and Rows 9 to 16, we
see the importance of reliability filtering. It is able
928
to guide the use of timelines such that significant
improvements in R-2 over SWING are obtained.
To help visualize what the differences in these
ROUGE scores mean, Figure 7 shows two sum-
maries
1
generated for document set D1117C of the
TAC-2011 dataset. The left one is produced by the
configuration in Row 9, and the right one is pro-
duced by SWING without the use of any temporal
information.
0.0	

0.2	

0.4	

0.6	

0.8	

1.0	

SP	
 Length	
 INDF	
 TSI	
 CTSI	
 TCD	

Fea
ture
 Sco
re	

Features	

L2	
 R2	

Figure 6: Breakdown of raw feature scores for sen-
tences (L2) and (R2) from Figure 7.
The higher R-2 score obtained by the summary
on the left (0.0873) compared to the one on the
right (0.0723) suggests that temporal information
can help to identify salient sentences more accu-
rately. A closer look at sentences (L2) and (R2)
and their R-2 scores (0.0424 and 0.0249, respec-
tively) is instructive. Figure 6 shows the raw fea-
ture scores of both sentences. Both sentences
score similarly for the SWING features of sen-
tence position (SP), sentence length (Length), and
interpolated n-gram document frequency (INDF);
however, the scores for all three timeline features
higher for (L2) than (R2). This helps our time sen-
sitive system prefer (L2).
5 Discussion
We now examine the proposed 1) timeline fea-
tures, 2) TIMEMMR algorithm, and 3) reliabil-
ity filtering metric in greater detail to gain insight
into their efficacy. For the analysis on timeline
features, we only present an analysis for TSI and
CTSI due to space constraints.
Time Span Importance. Figure 8 shows the
last sentences from a pair of summaries generated
with and without the use of TSI (all other sen-
tences were the same). The original articles de-
scribe an accident where casualties were suffered
when a crane toppled onto a building. It is easy to
see why (L1) scores higher for R-2 ? it describes
the cause of the accident just as it occurred. (R1)
however talks about events which happened before
1
The produced summaries are truncated to fit within a
100-word limit imposed by the TAC-2011 guidelines.
the accident itself (e.g., how much of the tower had
already been erected). In this case time span im-
portance is able to correctly guide summary gen-
eration by favoring time spans containing events
related to the actual toppling.
Contextual Time Span Importance. CTSI
recognizes that events which happen around the
time of a big cluster of other events can be im-
portant too. The benefits of this feature can be
clearly seen in Figure 9. The summary on the left
achieved a R-2 score of 0.1215 while the one on
the right achieved 0.0861. (L2) and (L3) were
both boosted by the use of the contextual impor-
tance feature.
Figure 10 shows an extract of the timeline gen-
erated for the source document from which (L3)
is extracted. The two events inside (L3) fall in
time spans A and B marked in the figure. Their
proximity to the peak P between them gives the
sentence a higher score for CTSI. This boost re-
sults in the sentence being selected for inclusion
in the final summary. It turns out that this sentence
was lifted exactly in one of the model summaries
for this document set, resulting in a very good R-2
score when contextual importance is used.
warn disappear
Peak here affects time span contextual importance of A and B
A BP
Figure 10: Extract of timeline generated for doc-
ument APW ENG 20070615.0356 from the TAC-
2011 dataset.
Is TIMEMMR Useful? The experimental re-
sults do not conclusively affirm the usefulness of
TIMEMMR. However we believe it is because
the ROUGE measures that are used for evalua-
tion are not suited for this purpose. Recall that
TIMEMMR seeks to eliminate redundancy based
on time span similarities and not lexical likeness.
ROUGE, however, measures the latter.
An interesting case in point is given in Fig-
ure 11. The summary on the left is generated
using TIMEMMR and achieved a lower ROUGE
score. The one on the right is generated with-
out TIMEMMR and scores higher, suggesting that
TIMEMMR is not helpful. The key difference in
929
R-2: 0.0873 R-2: 0.0723
(L1,R1) The Army?s surgeon general criticized stories in The Washington Post disclosing problems at Walter
Reed Army Medical Center, saying the series unfairly characterized the living conditions and care for soldiers
recuperating from wounds at the hospital?s facilities.
(L2) Defense Secretary Robert Gates says people
found to have been responsible for allowing sub-
standard living conditions for soldier outpatients at
Walter Reed Army Medical Center in Washington
will be ?held accountable,? although so far no one
in the Army chain of command has offered to re-
sign.
6= 6= (R2) A top Army general vowed to personally
oversee the upgrading of Walter Reed Army Medi-
cal Center?s Building 18, a dilapidated former hotel
that houses wounded soldiers as outpatients.
(L3) Top Army officials visited Building 18, the
decrepit former hotel housing more than 80 recov-
ering soldiers, outside
6= 6= (R3) ?I?m not sure it was an accurate representa-
tion,? Lt. Gen. Kevin Kiley, chief of the Army
Medical Command which oversees Walter Reed
and all Army health care, told reporters during a
news conference.
>>
(R4) The Washington
Figure 7: Generated summaries for document set D1117C from the TAC-2011 dataset. Left summary is
generated by SWING+TSI+CTSI+TCD with filtering; right summary is by SWING.
R-2: 0.1683 R-2: 0.1533
. . . . . . . . . . . .
(L1) A piece of steel fell and sheared off one of the
ties holding it to the building, causing it to detach
and topple, said Stephen Kaplan
6= 6=
(R1) About 19 of the 44 stories of the crane had
been erected and it was to be extended when a
piece of steel fell and sheared
Figure 8: Extract from summaries for document set D1137G from the TAC-2011 dataset. Left extract is
generated by SWING+TSI+CTSI+TCD; right extract is by SWING+CTSI+TCD.
the two summaries is (R3). (L3) is the equivalent
of (R4), while (L4) is the full version of the trun-
cated (R5). TIMEMMR penalizes (R3). (R3) re-
ports that the shoe-throwing incident happened as
the U.S. President Bush appeared together with the
Iraqi Prime Minister Nouri al-Maliki. However
their joint appearance is already reported in (R1)
(and similarly (L1)). (R3) repeats what had been
presented earlier. Since (R1) and (R3) talk about
the same time span, TIMEMMR down-weights
(R3). We argue that this is better even though the
ROUGE scores indicate otherwise. In future work
it will be worthwhile to consider the use of metrics
like Pyramid (Passonneau et al, 2005) which are
less bound to superficial lexicons.
Reliability Filtering. Table 2 shows the ef-
fect of varying the filtering threshold on R-2 for
the best performing configuration from Table 1
(i.e., SWING+TSI+CTSI+TCD). The result ob-
tained in Row 9 using a threshold of 42.68 is also
re-produced for reference. T=0 means that time-
lines are used for all input document sets, whereas
T=100 means that no timelines are used, as the
length of the longest timeline is less than 100.
As the threshold increases from 0 to 40?50,
summarization performance improves while the
T R-2 Sig # T R-2 Sig #
0 0.1394 * 44 50 0.1386 ** 13
10 0.1382 - 43 60 0.1361 * 7
20 0.1377 - 41 70 0.1351 - 3
30 0.1393 ** 35 80 0.1351 - 2
40 0.1426 ** 22 90 0.1353 - 1
42.68 0.1418 ** 21 100 0.1339 - 0
Table 2: Effect of different reliability filtering
thresholds for SWING+TSI+CTSI+TCD. ?T? is
the threshold used; ?#? is the number of input col-
lections (out of 44) where timelines are used; ?**?
and ?*? is statistical significance over SWING of
p < 0.05 and p < 0.1, respectively.
number of document sets where temporal informa-
tion is used is reduced. This suggests that filtering
is successful in identifying timelines that are not
sufficiently accurate to be useful for summariza-
tion. R-2 performance peaks around a threshold
of 40. This affirms our use of the average length
of timelines as the threshold value in our earlier
experiments. Beyond 60, the R-2 scores are still
higher than that obtained by SWING, but no longer
significantly different. At these higher thresholds,
temporal information is still able to help get an im-
provement in R-2. However as this affects only
very few out of the 44 document sets, statistical
variances mean that these R-2 scores are no longer
930
R-2: 0.1215 R-2: 0.0861
((L1,R1) Caribbean coral species essential to the region?s reef ecosystems are at risk of extinction as a result of
climate change.
(L2) But destructive fishing methods and over-
harvesting have reduced worldwide catches by 90
percent in the past two decades.
6= 6= (R2) The Coral Reef Task Force, created in the
Clinton administration, regularly assesses coral
health.
(L3) Scientists warn that up to half of the world?s
coral reefs could disappear by 2045.
6= 6= (R3) With a finished necklace retailing for up
to 20,000 dollars (15,000 euros), red corals are
among the world?s most expensive wildlife com-
modities.
. . . . . . . . . . . .
Figure 9: Extract from summaries for document set D1131F from the TAC-2011 dataset. Left extract is
generated by SWING+TSI+CTSI+TCD; right extract is by SWING+TSI+TCD.
R-2: 0.2643 R-2: 0.2772
(L1,R1) ? An Iraqi reporter threw his shoes at visiting U.S. President George W. Bush and called him a ?dog? in
Arabic during a news conference with Iraqi Prime Minister Nuri al-Maliki in Baghdad
(L2,R2) ?All I can report is it is a size 10,.
(L3) Muntadhar al-Zaidi, reporter of Baghdadiya
television jumped and threw his two shoes one by
one at the president, who ducked and thus narrowly
missed being struck, raising chaos in the hall in
Baghdad?s heavily fortified green Zone.
6= 6= (R3) The incident occurred as Bush was appearing
with Iraqi Prime Minister Nouri al-Maliki.
(L4) The president lowered his head and the first
shoe hit the American and Iraqi flags behind the
two leaders.
6= 6=
(R4) Muntadhar al-Zaidi, reporter of Baghdadiya
television jumped and threw his two shoes one by
one at the president, who ducked and thus narrowly
missed being struck, raising chaos in the hall in
Baghdad?s heavily fortified green Zone.
(L5) The 6= 6=
(R5) The president lowered his head and the
Figure 11: Summaries for document set D1126E from the TAC-2011 dataset. Left summary is generated
by SWING+TSI+CTSI+TCD+TIMEMMR; right summary is by SWING+TSI+CTSI+TCD.
significant from that produced by SWING.
6 Conclusion
We have shown in this work how temporal in-
formation in the form of timelines can be incor-
porated into multi-document summarization. We
achieve this through two means, using: 1) three
novel features derived from timelines to mea-
sure the saliency of sentences, and 2) TIMEMMR
which considers time span similarity to enhance
the traditional MMR?s lexical diversity measure.
To overcome errors propagated from the under-
lying temporal processing systems, we proposed
a reliability filtering metric which can be used to
help decide when temporal information should be
used for summarization. The use of this metric
leads to an overall 5.9% gain in R-2 over the com-
petitive SWING baseline.
In future work, we are keen to study our pro-
posed timeline-related features more intrinsically
in the context of human-generated summaries.
This can help us better understand their value in
improving content selection. As noted earlier,
it will be also be useful to repeat our experi-
ments with less lexicon-influenced measures like
the Pyramid method (Passonneau et al, 2005).
Manual assessment of the generated summaries
can also be done to give a better picture of the
quality of the summaries generated with the use
of timelines. Finally, given the importance of re-
liability filtering, a natural question is if there are
other metrics that can be used to get better results.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
This work is also partially supported by the
National Natural Science Foundation of China
(Grant Nos. 61170189, 61370126, 61202239),
the Fund of the State Key Laboratory of Software
Development Environment (Grant No. SKLSDE-
2013ZX-19), and the Innovation Foundation of
Beihang University for Ph.D. Graduates (YWF-
13-T-YJSY-024).
931
References
Javed Aslam, Matthew Ekstrand-Abueg, Virgil Pavlu,
Fernado Diaz, and Tetsuya Sakai. 2013. TREC
2013 Temporal Summarization. In Proceedings of
the 22nd Text Retrieval Conference (TREC), Novem-
ber.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information Fusion in the Context of
Multi-document Summarization. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics on Computational Linguistics
(ACL), pages 550?557, June.
Jaime Carbonell and Jade Goldstein. 1998. The Use
of MMR, Diversity-based Reranking for Reordering
Documents and Producing Summaries. In Proceed-
ings of the 21st Annual International ACM Confer-
ence on Research and Development in Information
Retrieval (SIGIR), pages 335?336, August.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A Hy-
brid Hierarchical Model for Multi-document Sum-
marization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 815?824, July.
John M. Conroy, Judith D. Schlesinger, Jeff Kubina,
Peter A. Rankel, and Dianne P. O?Leary. 2011.
CLASSY 2011 at TAC: Guided and Multi-lingual
Summaries and Evaluation Metrics. In Proceedings
of the Text Analysis Conference (TAC), November.
Gianluca Demartini, Malik Muhammad Saad Missen,
Roi Blanco, and Hugo Zaragoza. 2010. Entity
Summarization of News Articles. In Proceedings of
the 33rd Annual International ACM Conference on
Research and Development in Information Retrieval
(SIGIR), pages 798?796, July.
Pascal Denis and Philippe Muller. 2011. Predicting
Globally-Coherent Temporal Structures from Texts
via Endpoint Inference and Graph Decomposition.
In Proceedings of the 22nd International Joint Con-
ference on Artificial Intelligence (IJCAI), July.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP),
pages 677?689, July.
Lisa Ferro, Laurie Gerber, Inderjeet Mani, Beth Sund-
heim, and George Wilson. 2000. Instruction Man-
ual for the Annotation of Temporal Expressions.
Technical report, The MITRE Corporation.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document Sum-
marization by Sentence Extraction. In Proceedings
of the 2000 NAACL-ANLP Workshop on Automatic
Summarization, volume 4, pages 40?48, April.
Yihong Gong and Xin Liu. 2001. Generic Text Sum-
marization Using Relevance Measure and Latent Se-
mantic Analysis. In Proceedings of the 24th Annual
International ACM Conference on Research and De-
velopment in Information Retrieval (SIGIR), pages
19?25, September.
Iris Hendrickx, Walter Daelemans, Erwin Marsi, and
Emiel Krahmer. 2009. Reducing Redundancy in
Multi-document Summarization using Lexical Se-
mantic Similarity. In Proceedings of the Workshop
on Language Generation and Summarisation (UC-
NLG+Sum), pages 63?66, August.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology (NAACL), volume 1, pages 71?
78, May.
Chin-Yew Lin. 2004. Looking for a Few Good Met-
rics: ROUGE and its Evaluation. In Working Notes
of the 4th NTCIR Workshop Meeting, June.
Maofu Liu, Wenjie Li, and Huijun Hu. 2009. Extrac-
tive Summarization Based on Event Term Temporal
Relation Graph and Critical Chain. In Information
Retrieval Technology, volume 5839 of Lecture Notes
in Computer Science, pages 87?99. Springer Berlin
Heidelberg.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved
Temporal Relation Classification using Dependency
Parses and Selective Crowdsourced Annotations.
In Proceedings of the International Conference on
Computational Linguistics (COLING), pages 2109?
2124, December.
Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen
Kan, and Chew-Lim Tan. 2012. Exploiting
Category-Specific Information for Multi-Document
Summarization. In Proceedings of the International
Conference on Computational Linguistics (COL-
ING), pages 2093?2108, December.
Jun-Ping Ng, Min-Yen Kan, Ziheng Lin, Wei Feng, Bin
Chen, Jian Su, and Chew-Lim Tan. 2013. Exploit-
ing Discourse Analysis for Article-Wide Temporal
Classification. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 12?23, October.
Karolina Owczarzak and Hoa Dang. 2011. Overview
of the TAC 2011 Summarization Track: Guided
Task and AESOP Task. In Proceedings of the Text
Analysis Conference (TAC), November.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-
eown, and Sergey Sigelman. 2005. Applying the
Pyramid Method in DUC 2005. In Proceedings of
the Document Understanding Conference Workshop
on Text Summarization, October.
932
James Pustejovsky, Jos?e Castano, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust Specification
of Event and Temporal Expressions in Text. In Pro-
ceedings of the 5th International Workshop on Com-
putational Semantics (IWCS), January.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003b. The TIMEBANK
corpus. In Proceedings of Corpus Linguistics, pages
647?656, March.
Jannik Str?otgen and Michael Gertz. 2013. Multilin-
gual and Cross-domain Temporal Tagging. Lan-
guage Resources and Evaluation, 47(2):269?298.
Naushad Uzzaman, Hector Llorens, Leon Derczynski,
Marc Verhagen, James F. Allen, and James Puste-
jovsky. 2013. SemEval-2013 Task 1: TEMPEVAL-
3: Evaluating Time Expressions, Events, and Tem-
poral Relations. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation (SemEval),
June.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The TempEval Challenge: Iden-
tifying Temporal Relations in Text. Language Re-
sources and Evaluation, 43(2):161?179.
Marc Verhagen, Roser Saur??, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation (SemEval),
pages 57?62, July.
Xiaojun Wan. 2007. TimedTextRank: Adding the
Temporal Dimension to Multi-Document Summa-
rization. In Proceedings of the 30th Annual Interna-
tional ACM Conference on Research and Develop-
ment in Information Retrieval (SIGIR), pages 867?
868, July.
Mingli Wu. 2008. Investigations on Temporal-
Oriented Event-Based Extractive Summarization.
Ph.D. thesis, Hong Kong Polytechnic University.
Shasha Xie and Yang Liu. 2008. Using Corpus
and Knowledge-based Similarity Measure in Max-
imum Marginal Relevance for Meeting Summariza-
tion. In Proceedings of the International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 4985?4988, March.
Renxian Zhang, You Ouyang, and Wenjie Li. 2011.
Guided Summarization with Aspect Recognition. In
Proceedings of the Text Analysis Conference (TAC),
November.
933
Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 90?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Formulaic and Free Text Clinical Research Articles Metadata
using Conditional Random Fields
Sein Lin1 Jun-Ping Ng1 Shreyasee Pradhan2
Jatin Shah2 Ricardo Pietrobon2 Min-Yen Kan1
1Department of Computer Science, National University of Singapore
justin@seinlin.com, {junping,kanmy}@comp.nus.edu.sg
2Duke-NUS Graduate Medical School Singapore
{shreyasee.pradhan,jashstar}@gmail.com, rpietro@duke.edu
Abstract
We explore the use of conditional random
fields (CRFs) to automatically extract impor-
tant metadata from clinical research articles.
These metadata fields include formulaic meta-
data about the authors, extracted from the title
page, as well as free text fields concerning the
study?s critical parameters, such as longitudi-
nal variables and medical intervention meth-
ods, extracted from the body text of the arti-
cle. Extracting such information can help both
readers conduct deep semantic search of arti-
cles and policy makers and sociologists track
macro level trends in research. Preliminary re-
sults show an acceptable level of performance
for formulaic metadata and a high precision
for those found in the free text.
1 Introduction
The increasing number of clinical research articles
published each year is a double-edged sword. As
of 2009, PubMed indexed over 19 million citations,
over which 700,000 were added over the previous
year1. While the research results further our knowl-
edge and competency in the field, the volume of in-
formation poses a challenge to researchers who need
to stay up to speed. Even within a single clinical
research area, there can be hundreds of new clini-
cal research results per year. Policy makers, who
need to decide which clinical research proposals to
fund and fast-track, and which proposals could tag
onto existing research and cost share, have equally
daunting information synthesis issues that have both
monetary and public health implications (Johnston
et al, 2006).
1http://www.nlm.nih.gov/bsd/bsd_key.html
Systematic reviews ? secondary publications that
compile evidence and best practices from primary
research results ? partially address these concerns,
but can take years before their final publication, due
to liability and administrative overheads. In many
fast-paced fields of clinical practice, such guidelines
can be outdated by the time of publication. Re-
searchers and policy makers alike still need effective
tools to help them search, digest and organize their
knowledge of the primary literature.
One avenue that researchers have turned to is the
use of automated information extraction (IE). We
distinguish between two distinct uses of Information
Extraction: 1) extracting regular, formulaic fields
(e.g., author names, their institutional affiliation and
email addresses), and 2) extracting free text descrip-
tions of key study parameters (e.g., longitudinal vari-
ables, observation time periods, databases utilized).
Extracting such formulaic fields helps policy
makers determine returns on health-care investments
(Kwan et al, 2007), as well as researchers in large
scale sociological studies understand macroscopic
trends in clinical research authorship and topic shifts
over time (Cappell and Davis, 2008; Lin et al,
2008). But due to the wide variety of publication
venues for clinical research, even performing the
seemingly simple task of author name extraction
turns out to be difficult, and published studies thus
far have relied on manual analysis and extraction.
Proposals to extract values of key study parame-
ters may have more profound effects. Deeper char-
acterization of research artifacts will enable more
semantically-oriented searches of the clinical lit-
erature. Further programmatic access allows and
encourages data sharing of raw clinical trial re-
sults, databases and cohorts (Piwowar and Chap-
90
man, 2008) that may result in cost sharing across
on-going studies, saving funds for other deserving
clinical trials.
On one hand, the medical community has been
proactive in using natural language processing
(NLP) and information extraction technology in an-
alyzing their own literature. Many approaches to
metadata extraction have used regular expressions
or baseline supervised machine learning classifiers.
However, these techniques are not considered state-
of-the-art.
On the other hand, much of the work from the
NLP community applied to biomedical research has
been on in-depth relationship extraction, such as
the identification of gene pathways and protein-
protein interaction (PPI). While certainly difficult
and worthwhile problems to solve, there is room for
contribution even at the basic IE level, to retrieve
both regular and free form metadata fields.
We address this need in this paper. We apply a
linear-chain Conditional Random Field (CRF) (Laf-
ferty et al, 2001) as our methodology for extracting
metadata fields. CRFs are a sequence labeling model
that has shown good performance over a large num-
ber of information extraction tasks. We conduct ex-
periments using basic token features to assess their
efficacy for metadata extraction. While preliminary,
our results indicate that CRFs are suitable for iden-
tifying formulaic metadata, but may need additional
deeper, natural language processing features to iden-
tify free text fields.
2 Related Work
Many researchers have recognized the utility of the
application of IE on biomedical text. These works
have focused mainly on the application of well-
known machine learning algorithms to tag impor-
tant biomedical entities such as genes and proteins
within biomedical articles. (Tanabe and Wilbur,
2002) uses a Na??ve Bayes classifier, while (Zhou et
al., 2004) uses use a Hidden Markov Model (HMM).
The Conditional Random Field (CRF) learning
model combines the strengths of two well known
methods: the Hidden Markov Model (HMM), a se-
quence labeling methodology, and the Maximum
Entropy Model, a classification methodology. It
models the probability of a class label y for a given
token, directly from the observable stream of tokens
x in direct (discriminative) manner, rather than as
a by-product as in generative methods such as in
HMMs. A CRF can model arbitrary dependencies
between observation and class variables, but most
commonly, a simple linear chain sequence is used
(which connects adjacent class variables to each
other and to their corresponding observation vari-
able), making them topologically similar to HMMs.
Since their inception in 2001, (linear chain) CRFs
have been applied extensively to many areas, includ-
ing the biomedical field. CRFs have been used for
the processing and extraction of important medical
entities and their relationships among each other.
(He and Kayaalp, 2008) reports on the suitability of
CRFs to find biological entities, combining basic or-
thographic token features with features derived from
semantic lexicons such as UMLS, ABGene, Sem-
Rep and MetaMap. In a related vein, CRFs have
been applied to gene map and relationship identifi-
cation as well (Bundschus et al, 2008; Talreja et al,
2004).
In a different domain, digital library practition-
ers have also studied how to extract formulaic meta-
data to enable more comprehensive article index-
ing. To extract author and title information, systems
have used both the Support Vector Machine (SVM)
(Han et al, 2003) and CRFs (Peng and McCallum,
2004; Councill et al, 2008). These works have been
applied largely to the computer science community
and have not yet been extensively tested on biomed-
ical and clinical research articles.
Our work differs from the above by making use
of CRFs to extract fields in clinical text. Similar
lexical-based features are employed, however in ad-
dition to regular author metadata, we also attempt to
extract domain-specific fields from the body text of
the article.
3 Method
External to the scope of the research presented here,
our wider project goal focuses on constructing a
knowledge base of clinical researchers, databases,
instruments and expertise in the Asia-Pacific region.
Dataset. In this pilot study, we created a gold
standard dataset consisting of freely-available ar-
ticles available from PubMedCentral. These arti-
91
cles focused on health services research in the Asia-
Pacific region. In particular, we selected open-
access full-text literature documenting oncological
and cardio-vascular studies in the region, over a
three year period from 2005 to 2008.
By constructing an appropriate staged query with
PubMed, we obtained an initial listing of 260 arti-
cles. From an initial analysis, we determined that
a significant portion (?1/3) of the retrieved full-
text were not primary research, but reviews, case
studies, editorials or descriptions. After eliminating
these, the remaining 185 articles were earmarked to
be manually tagged by clinicians affiliated with the
project. Since the resulting corpus compiles arti-
cles across different journals and other publication
venues, their presentation of even the formulaic au-
thor metadata varied.
The clinicians were given rich text (RTF) versions
of the original HTML documents retrieved from
PubMed. They identified and extracted only the sec-
tions of the articles that had pertinent data classes to
tag. This process excluded most introductory, dis-
cussion and result sections, preserving the sections
that described the study and results at a high level
(e.g., Demographics and Methods).
After an initial training session, each clinician
used a word processor to manually insert opening
and closing XML tags for the tagset for a particular
subsection of the 185-article corpus. Due to the high
cost of clinician time, we chose to emphasize cover-
age, rather than have the clinicians multiply annotate
the same articles. As a result, we could not calculate
annotation agreement, but feel that the repeatability
of the annotation was addressed by the initial train-
ing. At the time of writing, 93 articles have been
completely tagged and sectioned, with the remainder
in progress. The average length of the documents is
about 1300 words. Once the dataset has been com-
pleted, we plan to release the annotated data offsets
to the public, to encourage comparative evaluation.
The clinicians annotated the following Formulaic
Author Metadata (3 classes):
? Author (Au): The names of the authors of the
study;
? E-mail (Em): The email addresses of the corre-
sponding authors of the study;
? Institution (In): The names of the institutions
that the authors are from.
Such metadata can be used to build an author ci-
tation network for macro trend analysis. Note that
this data is obtained from the article?s title page it-
self, and not from any references to source articles,
which have been the target of previous studies on
CRF-based information extraction (Peng and Mc-
Callum, 2004; Councill et al, 2008). The clinicians
also annotated the following Key Study Parameters
(10 classes):
? Age Group (Ag): The age range of the subjects
of the study (e.g., 45 and 80 years, 21-79
years);
? Data Analysis Name (Da): The name of the
method or software used in the analysis of data
collected for the study (e.g., proportional haz-
ards survival models, SAS package);
? Data Collection Method (Dc): The data collec-
tion methods for the study (e.g., medical
records, review of medical records and linkage
to computerized discharge abstracts);
? Database Name (Dn): The name of any biomed-
ical databases used or mentioned in the study
(e.g., Queensland Cancer Registry, National
Death Index, population-based registry);
? Data Type (Dt): The type of data involved in the
study (e.g., Cohort study, retrospectively);
? Geographical Area (Ga): The names of the ge-
ographical area in which an experiment takes
place or the subjects are from (e.g., Pune,
Switzerland);
? Intervention (Iv): The name of medical interven-
tion used in the study (e.g., surgery, radiother-
apy, chemotherapy, radio-frequency ablation);
? Longitudinal Variables (Lv): Data collected
over the observation period (e.g., subjects);
? Number of Observations (No): The number of
cases or subjects observed in the study (e.g.,
158 Indigenous, 84 patients);
92
? Time Period (Tp): The duration of an experi-
ment or observation in the study (e.g., 1997?
2002, between January 1988 and June 2006).
As can be seen from the examples, the tagging
guidelines loosely define the criteria for tagging. For
some classes, clinicians tagged entire noun phrases
or clauses, and for others, only numeric values and
modifiers were tagged. This variability arises from
the difficulty in tagging these free text fields.
Features. The CRF model requires a set of bi-
nary features to serve as a representation of the text.
A simple baseline is to use the presence/absence of
particular tokens as features. The CRF software im-
plementation we utilized is CRF++2, which com-
piles the binary features automatically from a con-
text window centered the current labeling problem
instance.
We first preprocess an input article from its RTF
representation and convert it into plain text. This
is a lossy transformation that discards font informa-
tion and corrupts mathematical symbols that could
be helpful in the detection task. We take hyphenated
token forms (e.g., 2006-2007) and convert them into
individual tokens. The plain text is processed to note
the specific locations of the XML tags for the learn-
ing process. The bulk of the words in each article
were not tagged by clinicians, and for these words,
we assigned a Not Applicable (NA) tag. We list
the simple inventory of feature types that we use for
classification.
? Vocabulary: Individual word tokens are stemmed
with Porter?s stemming algorithm (Porter,
1980) and down-cased to collapse orthographic
variants. Each word token is then used as an
individual feature. This feature alone was used
to compile baseline performance as discussed
later in evaluation.
? Lexical: Lists of keywords were compiled to lend
additional weight for specific classes. In par-
ticular, we compiled lists of months, common
names, cue words that signaled observations,
institution names and data analyses methods.
For example, a list of common given and sur-
name names is useful for the Au field; while a
2http://crfpp.sourceforge.net
list of months and their abbreviated forms help
to identify Tp. Each list constitutes a different
feature. As an example, in the case of human
names, the names Alice and Auburn are on the
list. If a word token corresponds to any of the
words in the list, the corresponding feature is
turned on (e.g., isApersonName).
? Position: If a word token is within the first 15
lines of an article, this feature is turned on. This
specifically caters to limit the scope of the for-
mulaic author metadata fields, to match them
only at the beginning of the article.
? Email: We create a specific feature for email ad-
dresses that is turned on when a particular word
token is matched by a handwritten regular ex-
pression.
? Numeric: For some free text classes, such as Ag,
No and Tp, the tagged text often contains nu-
meric data. This can be present in both numeric
and word form (e.g., 23 versus. twenty-three).
We turn this feature on for a token solely con-
taining digits or numeric word forms.
? Orthographic: Orthographic features, such as
the capitalization of a word token are useful to
help identify proper nouns and names. If there
are capital letters within a word token, this fea-
ture is turned on.
4 Evaluation
To ascertain the efficacy of our proposed solution,
three-fold cross validation (CV) was first performed
on a dataset comprising the 93 articles which have
been completely annotated.
Baseline. For the purpose of comparison, we cre-
ated a baseline system that utilizes the same CRF++
toolkit but uses only the vocabulary feature type
with a five-word window (two previous tokens, the
target token to be classified, and two subsequent to-
kens). The performance of this baseline system is
shown in Table 1, where the standard classification
performance measures of precision, recall and F1 are
given. Count measures the number of word tokens
that are predicted as belonging to the stated field.
Discussion. We see that the overwhelming major-
ity of tokens are not tagged (belonging to class NA).
93
The skewness of the dataset is not uncommon for IE
tasks.
The baseline results show weak performance
across the board. Clearly, significant feature engi-
neering could help boost performance. Of particular
surprise was the relatively weak performance on the
formulaic metadata. From our manual analysis, it
was clear that the wide range and variety of tokens
present in names and institutions barred the system
from achieving good performance on these classes.
Comparative studies in citation and reference pars-
ing usually peg classification performance of these
classes at the 95% and above level.
Without suitable customization, detection of the
key study parameters was also not possible. Only
relatively common fields could be captured by the
CRF, and when captured were more precise but
lacked enough data to build a model with any ac-
ceptable level of recall.
Table 2 illustrates the improved results obtained
by running CRF++ with all of the described features
on the same dataset. The same five-word window
size is used for the vocabulary feature. As seen, sig-
nificant improvements over the baseline are obtained
for all except four fields ? Da, Dc, Iv, and Lv.
These four fields were the classes with the most vari-
ability in annotation. For example, the data collec-
tion methodologies (Dc) and interventions (Iv) are
often captured as long sentence fragments and hard
to model with individual word cues.
The largest improvements occurred for the classes
of age groups Ag and time periods Tp, both of which
benefited from the addition of the numeric feature
which boosted recognition performance.
5 Future Work
The work presented here is ongoing, and based on
our current results, we are planning to re-examine
the quality of the annotations and refine our anno-
tation guideline and scheme. We discovered cases
where the CRF tagger correctly annotated key study
parameters which the annotators had missed or mis-
keyed. Drawing on lessons from the initial anno-
tation exercise, a more comprehensive guideline is
planned which will provide concise instructions with
accompanying annotation examples.
We also plan to enrich the feature set. The current
Field Prec. Recall F1 Count
Formulaic Author Metadata
Au 84.6 74.3 79.1 1818
Em 93.4 92.2 92.8 151
In 80.5 69.5 74.6 3906
Macro Avg. 86.2 78.7 82.3
Key Study Parameters
Ag 29.0 40.4 33.8 334
Da 61.0 39.0 47.6 708
Dc 8.3 3.2 4.6 48
Dn 35.9 15.1 21.2 92
Dt 52.8 26.8 35.5 36
Ga 7.3 4.5 5.6 41
Iv 4.6 1.4 2.1 22
Lv 15.4 20.0 17.4 13
No 14.4 5.8 8.3 125
Tp 73.6 55.8 63.5 261
Macro Avg. 30.2 21.2 24.0
NA 97.1 98.5 97.8 119998
Table 1: Baseline aggregated results over 93 tagged arti-
cles under 3 fold cross validation.
Field P. Recall F1 Count
Formulaic Author Metadata
Au 89.0 85.3 87.1 7312
Em 100.0 97.3 98.6 154
In 91.3 78.0 84.1 4515
Macro Avg. 93.4 86.6 89.9
Key Study Parameters
Ag 64.3 35.4 45.7 240
Da 79.3 37.2 50.6 2296
Dc 20.0 1.6 2.9 125
Dn 42.5 10.5 16.8 219
Dt 70.0 19.7 30.7 71
Ga 43.7 10.4 16.8 62
Iv 40.0 2.7 5.1 73
Lv 0.0 0.0 0.0 10
No 43.4 10.7 17.1 308
Tp 82.7 69.4 75.5 344
Macro Avg. 48.5 19.7 26.1
NA 97.5 99.3 98.4 120430
Table 2: Aggregated results using the full feature set un-
der 3 fold cross validation.
94
set employed is still simplistic and serves as a de-
velopmental platform for furthering our feature en-
gineering process. For example, the vocabulary, po-
sition and word lists features can be further modified
to capture more fined-grained information.
Once we exhaust the development of basic fea-
tures, our future work will attempt to harness deeper,
semantic features, making use of part-of-speech
tags, grammar parses, and named entity recognition
for example. The incorporation of these features will
likely be useful in improving the performance of the
CRF learner. We also plan to use both clinical re-
search and general medical ontologies (e.g., UMLS)
to gain additional insight on individual terms that
have special domain-specific meanings.
6 Conclusion
We have developed a CRF-based information ex-
traction system that targets two different types of
metadata present in clinical articles. Our work in
progress demonstrates that formulaic author meta-
data can be effectively extracted using the CRF
methodology. By further performing feature engi-
neering, we were able to extract key study parame-
ters with a moderate level of success. Our post eval-
uation analysis indicates that more careful attention
to annotation and feature engineering will be neces-
sary to garner acceptable performance of such im-
portant clinical study parameters.
Acknowledgments
We like to express our gratitude to the reviewers
whose insightful comments and pointers to addi-
tional relevant studies have helped improve the pa-
per.
References
M. Bundschus, M. Dejori, M. Stetter, V. Tresp, and H.P.
Kriegel. 2008. Extraction of semantic biomedical
relations from text using conditional random fields.
BMC bioinformatics, 9(1):207.
Mitchell S. Cappell and Michael Davis. 2008. A signif-
icant decline in the american domination of research
in gastroenterology with increasing globalization from
1980 to 2005: An analysis of american authorship
among 8,251 articles. The American Journal of Gas-
troenterology, 103:1065?1074.
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
ParsCit: An open-source CRF reference string parsing
package. In Proceedings of the Language Resources
and Evaluation Conference (LREC 08), Marrakesh,
Morrocco.
Hui Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang, and
Ed Fox. 2003. Automatic document meta-data extrac-
tion using support vector machines. In Proceedings of
Joint Conference on Digital Libraries.
Ying He and Mehmet Kayaalp. 2008. Biological entity
recognition with conditional random fields. In Pro-
ceedings of the Annual Symposium of the American
Medical Informatics Association (AMIA), pages 293?
297.
S.C. Johnston, J.D. Rootenberg, S Katrak, Wade S.
Smith, and Jacob S Elkins. 2006. Effect of a us na-
tional institutes of health programme of clinical trials
on public health and costs. Lancet, 367:13191327.
Patrick Kwan, Janice Johnston, Anne Fung, Doris SY
Chong, Richard Collins, and Su Lo. 2007. A system-
atic evaluation of payback of publicly funded health
and health services research in hong kong. BMC
Health Services Research, 7(1):121.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the International Conference on Ma-
chine Learning, pages 282?289.
JM Lin, JW Bohland, P Andrews, Burns GA, CB Allen,
and PP Mitra. 2008. An analysis of the abstracts pre-
sented at the annual meetings of the society for neuro-
science from 2001 to 2006. PLoS ONE, 3(e2052).
F. Peng and A. McCallum. 2004. Accurate information
extraction from research papers using conditional ran-
dom fields. In Proceedings of Human Language Tech-
nology Conference and North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL), pages 329?336.
Heather A. Piwowar and Wendy W. Chapman. 2008.
Identifying data sharing in biomedical literature. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association (AMIA).
M.F. Porter. 1980. An Algorithm For Suffix Stripping.
14(3):130?137.
R. Talreja, A. Schein, S. Winters, and L. Ungar. 2004.
GeneTaggerCRF: An entity tagger for recognizing
gene names in text. Technical report, Univ. of Penn-
sylvania.
L. Tanabe and W.J. Wilbur. 2002. Tagging gene and
protein names in biomedical text. Bioinformatics,
18(8):1124.
G. Zhou, J. Zhang, J. Su, D. Shen, and C. Tan. 2004.
Recognizing names in biomedical texts: a machine
learning approach. Bioinformatics, 20(7):1178?1190.
95
