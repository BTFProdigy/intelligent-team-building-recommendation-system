Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 177?180,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Mining User Reviews: from Specification to Summarization
Xinfan Meng
Key Laboratory of
Computational Linguistics
(Peking University)
Ministry of Education, China
mxf@pku.edu.cn
Houfeng Wang
Key Laboratory of
Computational Linguistics
(Peking University)
Ministry of Education, China
wanghf@pku.edu.cn
Abstract
This paper proposes a method to ex-
tract product features from user reviews
and generate a review summary. This
method only relies on product specifica-
tions, which usually are easy to obtain.
Other resources like segmenter, POS tag-
ger or parser are not required. At fea-
ture extraction stage, multiple specifica-
tions are clustered to extend the vocabu-
lary of product features. Hierarchy struc-
ture information and unit of measurement
information are mined from the specifi-
cation to improve the accuracy of feature
extraction. At summary generation stage,
hierarchy information in specifications is
used to provide a natural conceptual view
of product features.
1 Introduction
Review mining and summarization aims to extract
users? opinions towards specific products from
reviews and provide an easy-to-understand sum-
mary of those opinions for potential buyers or
manufacture companies. The task of mining re-
views usually comprises two subtasks: product
features extraction and summary generation.
Hu and Liu (2004a) use association mining
methods to find frequent product features and use
opinion words to predict infrequent product fea-
tures. A.M. Popescu and O. Etzioni (2005) pro-
poses OPINE, an unsupervised information ex-
traction system, which is built on top of the Kon-
wItAll Web information-extraction system. In or-
der to reduce the features redundancy and pro-
vide a conceptual view of extracted features, G.
Carenini et al (2006a) enhances the earlier work
of Hu and Liu (2004a) by mapping the extracted
features into a hierarchy of features which de-
scribes the entity of interest. M. Gamon et al
(2005) clusters sentences in reviews, then label
each cluster with a keyword and finally provide
a tree map visualization for each product model.
Qi Su et al (2008) describes a system that clus-
ters product features and opinion words simulta-
neously and iteratively.
2 Our Approach
To generate an accurate review summary for a
specific product, product features must be iden-
tified accurately. Since product features are of-
ten domain-dependent, it is desirable that the fea-
tures extraction system is as flexible as possible.
Our approach are unsupervised and relies only on
product specifications.
2.1 Specification Mining
Product specifications can usually be fetched from
web sites like Amazon automatically. Those mate-
rials have several characteristics that are very help-
ful to review mining:
1. Nicely structured, provide a natural concep-
tual view of products;
2. Include only relevant information of the
product and contain few noise words;
3. Except for the product feature itself, usually
also provide a unit to measure this feature.
A typical mobile phone specification is partially
given below:
? Physical features
? Form: Mono block with full keyboard
? Dimensions: 4.49 x 2.24 x 0.39 inch
? Weight: 4.47 oz
? Display and 3D
? Size: 2.36 inch
? Resolution: 320 x 240 pixels (QVGA)
177
2.2 Architecture
The architecture of our approach. is depicted in
Figure 1. We first retrieve multiple specifications
from various sources like websites, user manu-
als etc. Then we run clustering algorithms on
the specifications and generate a specification tree.
And then we use this specification tree to extract
features from product reviews. Finally the ex-
tracted features are presented in a tree form.
Specifications Reviews
            Appearance
            Size
              Thickness
            Price
             ...
Size
Price
Thickness
...
2 Feature
Extraction
           Size: small
                  Thickness: thin
            price: low
1 Clustering
3 Summary
Generation
Figure 1: Architecture Overview
2.3 Specification Clustering
Usually, each product specification describes a
particular product model. Some features are
present in every product specification. But there
are cases that some features are not available in all
specifications. For instance, ?WiFi? features are
only available in a few mobile phones specifica-
tions. Also, different specifications might express
the same features with different words or terms.
So it is necessary to combine multiple specifica-
tions to include all possible features. Clustering
algorithm can be used to combine specifications.
We propose an approach that takes following in-
herent information of specifications into account:
? Hierarchy structure: Positions of features
in hierarchy reflect relationships between fea-
tures. For example, ?length?, ?width? feature
are often placed under ?size? feature.
? Unit of measurement: Similar features are
usually measured in similar units. Though
different specification might refer the same
feature with different terms, the units of mea-
surement used to describe those terms are
usually the same. For example, ?dimension?
and ?size? are different terms, but they share
the same unit ?mm? or ?inch?.
Naturally, a product can be viewed as a tree of
features. The root is the product itself. Each node
in the tree represents a feature in the product. A
complex feature might be conceptually split into
several simple features. In this case, the complex
feature is represented as a parent and the simple
features are represented as its children.
To construct such a product feature tree, we
adopt the following algorithm:
? Parse specifications: We first build a dic-
tionary for common units of measurement.
Then for every specification, we use regular
expression and unit dictionary to parse it to a
tree of (feature, unit) pairs.
? Cluster specification trees: Given multiple
specification trees, we cluster them into a sin-
gle tree. Similarities between features are a
combination of their lexical similarity, unit
similarity and positions in hierarchy:
Sim(f1, f2) =Sim
lex
(f1, f2)
+ Sim
unit
(f1, f2)
+ ? ? Sim
parent
(f1, f2)
+ (1? ?) ? Sim
children
(f1, f2)
The parameter ? is set to 0.7 empirically. If
Sim(f1, f2) is larger than 5, we merge fea-
tures f1 and f2 together.
After clustering, we can get a specification tree
resembles the one in subsection 2.1. However,
this specification tree contains much more features
than any single specification.
2.4 Features Extraction
Features described in reviews can be classified into
two categories: explicit features and implicit fea-
tures (Hu and Liu, 2004a). In the following sec-
tions, we describe methods to extract features in
Chinese product reviews. However, these meth-
ods are designed to be flexible so that they can be
easily adapted to other languages.
178
2.4.1 Explicit Feature Extraction
We generate bi-grams in character level for every
feature in the specification tree, and then match
them to every sentence in the reviews. There might
be cases that some bi-grams would overlap or con-
catenated. In these cases, we join those bi-grams
together to form a longer expression.
2.4.2 Implicit Feature Extraction
Some features are not mentioned directly but can
be inferred from the text. Qi Su et al (2008) in-
vestigates the problem of extracting those kinds
of features. There approach utilizes the associa-
tion between features and opinion words to find
implicit features when opinion words are present
in the text. Our methods consider another kind of
association: the association between features and
units of measurement. For example, in the sen-
tence ?Amobile phone with 8 mega-pixel, not very
common in the market.? feature name is absent in
the sentence, but the unit of measurement ?mega
pixel? indicates that this sentence is describing the
feature ?camera resolution?.
We use regular expression and dictionary of unit
to extract those features.
2.5 Summary Generation
There are many ways to provide a summary. Hu
and Liu (2004b) count the number of positive and
negative review items towards individual feature
and present these statistics to users. G. Carenini
et al (2006b) and M. Gamon et al (2005) both
adopt a tree map visualization to display features
and sentiments associated with features.
We adopt a relatively simple method to generate
a summary. We do not predict the polarities of the
user?s overall attitudes towards product features.
Predicting polarities might entail the construction
of a sentiment dictionary, which is domain depen-
dent. Also, we believe that text descriptions of fea-
tures are more helpful to users. For example, for
feature ?size?, descriptions like ?small? and ?thin?
are more readable than ?positive?.
Usually, the words used to describe a product
feature are short. For each product feature, we re-
port several most frequently occurring uni-grams
and bi-grams as the summary of this feature. In
Figure 2, we present a snippet of a sample sum-
mary output.
? mobile phone: not bad, expensive 
o appearance: cool 
 color: white 
 size: small, thin 
o camera functionality: so-so, acceptable 
 picture quality: good 
 picture resolution: not high 
o entertainment functionality: powerful 
 game: fun, simple 
Figure 2: A Summary Snippet
3 Experiments
In this paper, we mainly focus on Chinese prod-
uct reviews. The experimental data are retrieved
from ZOL websites (www.zol.com.cn). We
collected user reviews on 2 mobile phones, 1 digi-
tal camera and 2 notebook computers. To evaluate
performance of our algorithm on real-world data,
we do not perform noise word filtering on these
data. Then we have a human tagger to tag features
in the user reviews. Both explicit features and im-
plicit features are tagged.
No. of Clustering Mobile Digital Notebook
Specifications Phone Camera Computer
1 153 101 102
5 436 312 211
10 520 508 312
Table 1: No. of Features in Specification Trees.
The specifications for all 3 kinds of products
are retrieved from ZOL, PConline and IT168 web-
sites. We run the clustering algorithm on the spec-
ifications and generate a specification tree for each
kind of product. Table 1 shows that our clustering
method is effective in collecting product features.
The number of features increases rapidly with the
number of specifications input into clustering al-
gorithm. When we use 10 specifications as input,
the clustering methods can collect several hundred
features.
Then we run our algorithm on the data and eval-
uate the precision and recall. We also run the al-
gorithms described in Hu and Liu (2004a) on the
same data as the baseline.
From Table 2, we can see the precision of base-
line system is much lower than its recall. Examin-
ing the features extracted by baseline system, we
find that many mistakenly recognized features are
high-frequency words. Some of those words ap-
pear many times in text. They are related to prod-
179
Product Model
No. of Hu and Liu?s Approach the Proposed Approach
Features Precision Recall F-measure Precision Recall F-measure
Mobile Phone 1 507 0.58 0.74 0.65 0.69 0.78 0.73
Mobile Phone 2 477 0.59 0.65 0.62 0.71 0.77 0.74
Digital camera 86 0.56 0.68 0.61 0.69 0.78 0.73
Notebook Computer 1 139 0.41 0.63 0.50 0.70 0.74 0.72
Notebook Computer 2 95 0.71 0.88 0.79 0.76 0.88 0.82
Table 2: Precision and Recall of Product Extraction.
uct but are not considered to be features. Some
examples of these words are ?advantages?, ?dis-
advantages? and ?good points? etc. And many
other high-frequency words are completely irrel-
evant to product reviews. Those words include
?user?, ?review? and ?comment? etc. In contrast,
our approach recognizes features by matching bi-
grams to the specification tree. Because those
high-frequency words usually are not present in
specifications. They are ignored by our approach.
Thus from Table 2, we can conclude that our ap-
proach could achieve a relatively high precision
while keep a high recall.
Product Model Precision
Mobile Phone 1 0.78
Mobile Phone 2 0.72
Digital camera 0.81
Notebook Computer 1 0.73
Notebook Computer 2 0.74
Table 3: Precision of Summary.
After the summary is given, for each word in
summary, we ask one person to decide whether
this word correctly describe the feature. Table 3
gives the summary precision for each product
model. In general, on-line reviews have several
characteristics in common. The sentences are usu-
ally short. Also, words describing features usu-
ally co-occur with features in the same sentence.
Thus, when the features in a sentence are correctly
recognized, Words describing those features are
likely to be identified by our methods.
4 Conclusion
In this paper, we describe a simple but effective
way to extract product features from user reviews
and provide an easy-to-understand summary. The
proposed approach is based only on product spec-
ifications. The experimental results indicate that
our approach is promising.
In future works, we will try to introduce other
resources and tools into our system. We will also
explore different ways of presenting and visualiz-
ing the summary to improve user experience.
Acknowledgments
This research is supported by National Natural
Science Foundation of Chinese (No.60675035)
and Beijing Natural Science Foundation
(No.4072012).
References
M. Hu and B. Liu. 2004a. Mining and Summariz-
ing Customer Reviews. In Proceedings of the 2004
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168-177.
ACM Press New York, NY, USA.
M. Hu and B. Liu. 2004b. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteenth
National Conference on Artificial Intelligence.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining Customer Opinions from Free
Text. In Proceedings of the 6th International Sym-
posium on Intelligent Data Analysis.
A.M. Popescu and O. Etzioni. 2005. Extracting Prod-
uct Features and Opinions from reviews. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing(EMNLP).
Giuseppe Carenini, Raymond T. Ng, and Adam Pauls.
2006a. Multi-Document Summarization of Evalua-
tive Text. In Proceedings of the conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Giuseppe Carenini, Raymond T. Ng, and Adam Pauls.
2006b. Interactive multimedia summaries of evalu-
ative text. In Proceedings of Intelligent User Inter-
faces (IUI), pages 124-131. ACM Press, 2006.
Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian Wu,
Xiaoxun Zhang, Bin Swen. 2008. Hidden Senti-
ment Association In Chinese Web Opinion Mining.
In Proceedings of the 17th International Conference
on the World Wide Web, pages 959-968.
180
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1209?1217,
Beijing, August 2010
Build Chinese Emotion Lexicons
Using A Graph-based Algorithm and Multiple Resources
Ge Xu, Xinfan Meng, Houfeng Wang
Key Laboratory of Computational Linguistics (Peking University), Ministry of Education
Institute of Computational Linguistics, Peking University
{xuge, mxf, wanghf}@pku.edu.cn
Abstract
For sentiment analysis, lexicons play an
important role in many related tasks. In
this paper, aiming to build Chinese emo-
tion lexicons for public use, we adopted a
graph-based algorithm which ranks words
according to a few seed emotion words.
The ranking algorithm exploits the simi-
larity between words, and uses multiple
similarity metrics which can be derived
from dictionaries, unlabeled corpora or
heuristic rules. To evaluate the adopted
algorithm and resources, two independent
judges were asked to label the top words
of ranking list.
It is observed that noise is almost un-
avoidable due to imprecise similarity met-
rics between words. So, to guarantee
the quality of emotion lexicons, we use
an iterative feedback to combine man-
ual labeling and the automatic ranking al-
gorithm above. We also compared our
newly constructed Chinese emotion lexi-
cons (happiness, anger, sadness, fear and
surprise) with existing counterparts, and
related analysis is offered.
1 Introduction
Emotion lexicons have a great impact on the re-
sults of related tasks. With high-quality emo-
tion lexicons, systems using simple methods
can achieve competitive performance. However,
to manually build an emotion lexicon is time-
consuming. Many research works in building lex-
icons use automatic methods to assist the building
procedure. Such works commonly rank words by
the similarities to a set of seed words, then those
words with high ranking scores are more likely to
be added to the final lexicons or used as additional
seed words.
For Chinese, emotion lexicons are scarce re-
sources. We can get a small set of emotion words
from semantic dictionary (such as CCD, HowNet,
synonym dictionaries) or directly from related pa-
pers (Xu and Tao, 2003) (Chen et al , 2009), but it
is often not sufficient for practical systems. Xu et
al. (2008) constructed a large-scale emotion on-
tology dictionary, but it is not publicly available
yet.
In this paper, we adopted a graph-based algo-
rithm to automatically rank words according to a
few seed words. Similarity between words can be
utilized and multiple resources are used to boost
performance. Combining manual labeling with
automatic ranking through an iterative feedback
framework, we can produce high-quality emotion
lexicons. Our experiments focused on Chinese,
but the method is applicable to any other language
as long as suitable resources exist.
The remainder of this paper is organized as fol-
lows. In Section 2, related works are introduced.
In Section 3, we describe a graph-based algorithm
and how to incorporate multiple resources. Sec-
tion 4 gives the details of applying the algorithm
on five emotions and shows how to evaluate the re-
sults. Section 5 focuses on how to build and evalu-
ate emotion lexicons, linguistic consideration and
instruction for identifying emotions are also in-
cluded. Finally, conclusion is made in Section 6.
1209
2 Related work
Riloff and Shepherd (1997) presented a corpus-
based method that can be used to build seman-
tic lexicons for specific categories. The input to
the system is a small set of seed words for a cat-
egory and a representative text corpus. The out-
put is a ranked list of words that are associated
with the category. An approach proposed by (Tur-
ney, 2002) for the construction of polarity started
with a few positive and negative seeds, then used
a similarity method (pointwise mutual informa-
tion) to grow this seed list from web corpus.
Our experiments are similar with these works, but
we use a different ranking method and incorpo-
rate multiple resources. To perform rating infer-
ence on reviews, Goldberg and Zhu (2006) cre-
ated a graph on both labeled and unlabeled re-
views, and then solved an optimization problem
to obtain a smooth rating function over the whole
graph. Rao and Ravichandran (2009) used three
semi-supervised methods in polarity lexicon in-
duction based on WordNet, and compared them
with corpus-based methods. Encouraging results
show methods using similarity between words can
improve the performance. Wan and Xiao (2009)
presented a method to use two types of similarity
between sentences for document summarization,
namely similarity within a document and simi-
larity between documents. The ranking method
in our paper is similar to the ones used in above
three papers, which fully exploit the relationship
between any pair of sample points (both labeled
and unlabeled). When only limited labeled data
are available, such method achieves significantly
better predictive accuracy over other methods that
ignore the unlabeled examples during training.
Xu et al (2008) at first formed a taxonomy for
emotions, under which an affective lexicon ontol-
ogy exploiting various resources was constructed.
The framework of ontology is filled by the com-
bination of manual classification and automatic
methods?To our best knowledge, this affective
lexicon ontology is the largest Chinese emotion-
oriented dictionary.
3 Our method
3.1 A graph-based algorithm
For our experiments, we chose the graph-based al-
gorithm in (Zhou et al , 2004) which is transduc-
tive learning and formulated as follows:
Given a point set ? = {x1, ..., xl, xl+1, ..., xn},
the first l points xi(i ? l) are labeled and the re-
maining points xu(l+1 ? u ? n) unlabeled. The
goal is to rank the unlabeled points.
Let F denotes an n-dimensional vector whose
elements correspond to ranking scores on the data
set ?. Define another n-dimensional vector Y with
Yi = 1 if xi is labeled and Yi = 0 otherwise. Y
denotes the initial label assignment.
The iterative algorithm is shown in the follow-
ing:
Algorithm 1 A graph-based algorithm
1. Construct the weight matrix W and set Wii to
zero to avoid self-reinforcement. W is domain-
dependent.
2. Construct the similarity matrix S =
D1/2WD1/2 using symmetric normalization. D
is a diagonal matrix with Dii = ?jWij .
3. Iterate F (t + 1) = ?SF (t) + (1 ? ?)Y until
convergence, where ? is a parameter in (0, 1), and
F (0) = Y . We clamp labeled points to 1 after
each iteration.
4. Let F ? denote F (t) when the iteration con-
verges.
In our experiments, labeled points are seed
emotion words, Sij denotes the similarity between
ith word and jth word. In an iteration, each word
absorbs label information from other words. More
similar two words are, more influence they have
on each other. The label information (initially
from seed emotion words) will propagate along S.
The final output F ? contains ranking scores for all
words, and a score indicates how similar the cor-
responding word is to the seed emotion words.
The implementation of the iterative algorithm
is theoretically simple, which only involves ba-
sic matrix operation. Compared with meth-
ods which do not exploit the relationship be-
tween samples, experiments showing advantages
of graph-based learning methods can be found
1210
in (Rao and Ravichandran, 2009),(Goldberg and
Zhu, 2006),(Tong et al , 2005),(Wan and Xiao,
2009),(Zhu and Ghahramani, 2002) etc. When la-
beled data are scarce, such graph-based transduc-
tive learning methods are especially useful.
3.2 Incorporate multiple resources
For building the emotion lexicons, we are faced
with lots of resources, such as semantic dictio-
naries, labeled or unlabeled corpora, and some
linguistic experiences which can be presented as
heuristic rules. Naturally we want to use these
resources together, thus boosting the final perfor-
mance. In graph-base setting, such resources can
be used to construct the emotion-oriented similar-
ity between words, and similarities will be repre-
sented by matrices.
The schemes to fuse similarity matrices are pre-
sented in (Sindhwani et al , 2005), (Zhou and
Burges, 2007), (Wan and Xiao, 2009) and (Tong et
al. , 2005) etc. In our paper, not aiming at compar-
ing different fusion schemes, we used a linear fu-
sion scheme to fuse different similarities matrices
from different resources. The scheme is actually
a convex combination of matrices, with weights
specified empirically.
The fusion of different similarity matrices
falls in the domain of multi-view learning. A
well-known multi-view learning method is Co-
Training, which uses two views (two resources)
to train two interactive classifiers (Blum and
Mitchell, 1998). Since we focus on building emo-
tion lexicons using multiple resources (multiple
views), those who want to see the advantages of
multi-view learning over learning with one view
can refer to (Blum and Mitchell, 1998), (Sind-
hwani et al , 2005), (Zhou and Burges, 2007),
(Wan and Xiao, 2009) and (Tong et al , 2005)
etc.
4 Experiments
We use the method in section 3 to rank for each
emotion with a few seed emotion words. Once we
implement the ranking algorithm 1, the main work
resides in constructing similarity matrices, which
are highly domain-dependent.
4.1 Construct similarity matrices
Here, we introduce how to construct four sim-
ilarity matrices used in building emotion lexi-
cons. Three of them are based on cooccurrence of
words; the fourth matrix is from a heuristic rule.
We use ictclas3.01 to perform word segmenta-
tion and POS tagging.
In our experiments, the number of words in-
volved in ranking is 935062, so theoretically, the
matrices are 93506 ? 93506. If the similarity be-
tween any pair of words is considered, the compu-
tation becomes impractical in both time and space
cost. So we require that each word has at most
500 nearest neighbors.
Four matrices are constructed as follows:
4.1.1 Similarity based on a unlabeled corpus
The unlabeled corpus used is People?s
Daily3(? ? ? ?1997?2004). After word
segmentation and POS tagging, we chose three
POS?s (i,a,l)4. The nouns were not included
to limit the scale of word space. We set the
cooccurrence window to a sentence, and removed
the duplicate occurrences of words. Any pair of
words in a sentence will contribute a unit weight
to the edge which connects the pair of words.
4.1.2 Similarity based on a synonym
dictionary
We used the Chinese synonym dictionary (?
??????????5) for this matrix. In
this dictionary, the words in a synonym set are
presented in one line and separated by spaces, so
there is no need to perform word segmentation
and POS tagging. Any pair of words in one line
will contribute a unit weight to the edge which
connects the pair of words.
4.1.3 Similarity based on a semantic
dictionary
We used The Contemporary Chinese Dictio-
nary (??????) to construct the third simi-
1downloaded from http://www.ictclas.org/
2Words are selected after word segmentation and POS
tagging, see section 4.1.1?4.1.3 for selection of words in de-
tails.
3http://icl.pku.edu.cn/
4i=Chinese idiom, a=adjective, l=Chinese phrase
5http://ir.hit.edu.cn/
1211
larity matrix. Since word segmentation may seg-
ment the entries of the dictionary, we extracted all
the entries in the dictionary and store them in a file
whose words ictclas3.0 was required not to seg-
ment. Furthermore, for an entry in the dictionary,
the example sentences or phrases appearing in its
gloss may contain many irrelevant words in terms
of emotions, so they were removed from the gloss.
After word segmentation and POS tagging6, we
set the cooccurrence window to one line (an en-
try and its gloss without example sentences or
phrases), and removed the duplicate occurrences
of words. An entry and any word in the modi-
fied gloss will contribute a unit weight to the edge
which connects the pair of words. This construct-
ing was a bit different, since we did not consider
the similarity between words in modified gloss.
4.1.4 similarity based on a heuristic rule
In Chinese, a word is composed of one or sev-
eral Chinese characters. A Chinese character is
normally by itself an independent semantic unit,
so the similarity between two words can be in-
ferred from the character(s) that they share. For
example, the Chinese word ? (happy) appears
in the word ?? (readily). Since ?? and ?
share one Chinese character, they are regarded as
similar. Naturally, the larger the proportion that
two words share, the more similar they are. In
this way, the fourth weighted matrix was formed.
To avoid incurring noises, we exclude the cases
where one Chinese character is shared, with the
exception that the Chinese character itself is one
of the two Chinese words.
4.1.5 Fusion of four similarity matrices
After processing all the lines (or sentences), the
weighted matrices are normalized as in algorithm
1, then four similarity matrices are linearly fused
with equal weights (1/4 for each matrix).
4.2 Select seed emotion words
In our experiments, we chose emotions of happi-
ness, sadness, anger, fear and surprise which are
widely accepted as basic emotions7. Empirically,
6since we do not segment entries in this dictionary, all
POS?s are possible
7Guidelines for identifying emotions is in section 5, be-
fore that, we understand emotions through common sense.
we assigned each emotion with seed words given
in Table 1.
Emotion Seed words
?(happiness) ??,??,??,??,?
???,??,??
?(anger) ??,??,??,??,?
?, ??, ??, ??, ?
?,??,??,????,
????,????
?(sadness) ??,??,??,??,?
?,??,??,??,??
??, ??, ????, ?
?,??,??,????,
????
?(fear) ??, ??, ??, ???
?, ??, ??, ??, ?
?,????,????
?(surprise) ??, ???? ,??, ?
?,??,??,?,???
?,??,??
Table 1: Seed emotion words
4.3 Evaluation of our method
We obtained five ranking lists of words using the
method in section 3. Following the work of (Riloff
and Shepherd, 1997), we adopted the following
evaluation setting.
To evaluate the quality of emotion ranking lists,
each list was manually rated by two persons inde-
pendently. For each emotion, we selected the top
200 words of each ranking list and presented them
to judges. We presented the words in random or-
der so that the judges had no idea how our system
had ranked the words. The judges were asked to
rate each word on a scale from 1 to 5 indicating
how strongly it was associated with an emotion, 0
indicating no association. We allowed the judges
to assign -1 to a word if they did not know what
it meant. For the words rated as -1, we manually
assigned ratings that we thought were appropriate.
The results of judges are shown in figures 1-5.
In these figures, horizontal axes are the number of
reviewed words in ranking lists and vertical axes
are number of emotion words found (with 5 dif-
ferent strength). The curve labeled as > x means
that it counts the number of words which are rated
1212
0 50 100 150 2000
20
40
60
80
100
120
140
160
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 1: happiness
0 50 100 150 2000
20
40
60
80
100
120
140
160
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 2: anger
greater than x by either judge.
Curves (> 0, > 1, > 2) display positive slopes
even at the end of the 200 words, which implies
that more emotion words would occur if more
than 200 words are reviewed. By comparison,
curves (> 3, > 4) tend to be flat when they are
close to the right side, which means the cost of
identifying high-quality emotion words will in-
crease greatly as one checks along the ranking list
in descendent order.
It is observed that words which both judges as-
sign 5 are few. In surprise emotion, the number
is even 0. Such results may reflect that emotion
is harder to identify compared with topical cate-
gories in (Riloff and Shepherd, 1997).
0 50 100 150 2000
20
40
60
80
100
120
140
160
180
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 3: sadness
0 50 100 150 2000
20
40
60
80
100
120
140
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 4: fear
0 50 100 150 2000
20
40
60
80
100
120
words reviewed
em
otion
 wor
ds fo
und
>0
>1
>2
>3
>4
Figure 5: surprise
From the semantic dictionary, our method
found many low-frequency emotion words such as
? (pleasant, glad),?? (surprise and happy),?
? (sad), or those used in Chinese dialects such as
?? (fear), ?? (angry). Such emotion words
are necessary for comprehensive emotion lexi-
cons.
Because more POS?s than adjectives and verbs
are included in our experiments, some emotion
words such as the noun ?? (unexpected win-
ner),and the adverb ?? (to one?s surprise) are
also spotted, which to some extent implies the
generality of our method.
5 Construct emotion lexicons
The above section introduced a method to rank
words with a few seed emotion words. How-
ever, to build emotion lexicons requires that we
manually remove the noises incurred by the au-
tomatic ranking method. Accordingly, guide-
lines for identifying emotions are needed, and also
some linguistic consideration in identifying emot-
ing words should be given.
1213
5.1 An iterative feedback to denoise
In our experiments, we observed that noises in-
curred by similarity matrices are almost unavoid-
able. For example, in the unlabeled corpus, ?
??? (state visits) always co-occurred with ?
? (happy) or?? (happy), so in happiness emo-
tion, ???? acquired a high ranking position
(174th); in terms of the heuristic rule, ?? (ex-
pected) shares two Chinese characters with ??
?? (unexpected, surprised), however they have
opposite meaning because?? (exceed, beyond)
is a negative word. ?? unfavorably ranked high
(88th) in surprise emotion; from the semantic dic-
tionary, the gloss of?? (Chinese Spring Festival
pictures) contains?? (happy), thus in happiness
emotion,?? ranked high (158th).
So after each ranking of an emotion, in the de-
scendent order of ranking scores, we manually re-
vised some scores in about top 500. Several crite-
ria (see 5.2 and 5.3) were given to guide if a word
has a specified emotion. For those words surely
bearing the specified emotion, we assigned 1 to
them ,and left others unchanged. Seeing the words
newly revised to be 1 as new seed emotion words,
we run the ranking algorithm again. After such
feedback was repeated 2?3 times, we collected
all the words labeled with 1 to form the final emo-
tion lexicons. In (Zhou et al , 2004), the author
also suggested such iterative feedback to extend
the query (seed) set and improve the ranking out-
put. Commonly, the size of an emotion lexicon is
small, so we do not have to check too many words.
The human revising procedure is sensitive to
annotators? background. To improve the quality
of the emotion lexicons, experts with linguistic or
psychology background will help.
Furthermore, the ranking algorithm used in our
paper is clearly sensitive to the initial seed words,
but since we adopt an iterative feedback frame-
work, the words not appearing in the initial set
of seed words will show up in next iteration with
high ranking scores. We also performed experi-
ments which selected emotion seed words based
on the Chinese synonym dictionary and the emo-
tion words in (Chen et al , 2009), similar results
were found.
5.2 Guidelines for identifying emotions
The same as (Chen et al , 2009), we used the def-
inition that emotion is the felt awareness of bod-
ily reactions to something perceived or thought.
Also, we were highly influenced by the structure
of the affective lexicon presented by (Ortony et
al. , 1987), and used the Affective states and
Affective-Behavioral conditions in the structure to
identify emotion words in our paper8.
With such guidelines,?? (cowardice, relates
more to external evaluation) is not an emotional
word of fear. We also intentionally distinguish be-
tween emotions and expression of emotions. For
example, ?? (laugh), ?? (haw-haw) are seen
as expression of happiness and?? (tremble) as
of fear, but not as emotion words. In addition,
we try to distinguish between an emotion and the
cause of an emotion, see 5.3 for an example.
For each emotion, brief description is given as
below9:
1. Happiness?the emotional reaction to some-
thing that is satisfying.
2. Anger?do not satisfy the current situation
and have a desire to fight or change the situa-
tion. Often there exists a target for this emo-
tion.
3. Sadness?an emotion characterized by feel-
ings of disadvantage, loss, and helplessness.
Sadness often leads to cry.
4. Fear?the emotional response to a perceived
threat. Fear almost always relates to future
events, such as worsening of a situation, or
continuation of a situation that is unaccept-
able.
5. Surprise?the emotional reaction to some-
thing unexpected.
5.3 Linguistic consideration for identifying
emotion words
If a word has multiple senses, we only consider its
emotional one(s). For example,?? (as a verb, it
means be angry, but means vitality or spirits as a
noun) will appear in the emotion lexicon of anger.
8According to (Ortony et al , 1987), surprise should not
be seen as a basic emotion for it relates more to cognition.
However, our paper focuses on the building of emotion lexi-
cons, not the disputable issue of basic emotions
9we mainly referred to http://en.wikipedia.org/wiki
1214
If one sense of a word is the combination of emo-
tions, the word will appear in all related emotions.
We mainly consider four POS?s, namely nouns,
verbs, adjectives and adverb10. If a word has mul-
tiple POS?s, we normally consider its POS with
strongest emotion (Empirically, we think the emo-
tion strength ranks in decedent order as following:
adjectives, verbs, adverbs, nouns.). So we con-
sider the verb of?? (fear) when it can be used
as a noun and a verb in Chinese. The?? exam-
ple above also applies here.
For each of four POS?s, instruction for emotion
identification is given as below:
Nouns: For example,?? (rage, anger),??
(joy or jubilation), ?? (an unexpected winner)
are selected as emotion words. We distinguish be-
tween an emotion and the cause of an emotion.
For example, calamity often leads to sadness, but
does not directly contain the emotion of sadness.
?? appears in the surprise lexicon because we
believe it contains surprise by itself.
Adverbs: The adverbs selected into emotion
lexicons contain the emotions by themselves. For
example,?? (unexpectedly),??? (cheerily),
??? (angrily), ?? (unexpectedly), ???
(sadly) etc.
Verbs: As in (Ortony et al , 1987), Chi-
nese emotion verbs also fall into at least two dis-
tinct classes, causatives and noncausatives. Both
classes are included in our emotion lexicons. For
example, ??? (be angry), ?? (fear) are
noncausative verbs, while ?? (enrage), ??
(to make someone surprised) are causative ones.
Probably due to the abundant usage of ??/?
?/?? (to make someone) etc., causative emo-
tion verbs are few compared to noncausative ones
in Chinese.
Adjective?Quite a lot of emotion words fall in
this POS, since adjectives are the natural expres-
sion of internal states of humans. For example,?
? (happy),?? (surprised),?? (angry) etc.
For any word that it is hard to identify at first
sight, we used a search tool11 to retrieve sentences
10For Chinese idioms, we only considered those used as
these four POS?s, omitted those used as a statement, such
as???? (an army burning with righteous indignation is
bound to win)
11provided by Center for Chinese Linguistics of Peking
University, http://ccl.pku.edu.cn
which contain the word, and then identify if the
word is emotional or not by its usage in the sen-
tences.
5.4 Comparison with existing Chinese
emotion resources
????????????????
????????????????
???????????????
???????????????
???????????????
????????????????
????????????????
????????????????
????????????????
????????????????
???????????????
???????????????
???????????????
????????????????
????????????????
???????????????
????????????????
????????????????
????????????????
????????????????
???????????????
????????????????
????????????????
????????????????
????
Table 2: The emotion lexicon of surprise
Under the guidelines for manually identifying
emotion words, we finally constructed five Chi-
nese emotion lexicons using the iterative feed-
back. The newly constructed emotion lexicons
were also reported as resources together with our
paper. The emotion lexicon of surprise is shown
in Table 2. In this part, we compare our lexicons
with the following counterparts, see Table 3.
Ours1 in the table is the final emotion lexicons,
and Ours2 is the abridged version that excludes
the words of single Chinese character and Chinese
idioms.
Chinese Concept Dictionary (CCD) is a
WordNet-like semantic lexicon(Liu et al , 2003).
1215
? ? ? ? ?
CCD nouns 22 27 38 46 10
(Xu and Tao, 2003) 45 12 28 21 12
(Chen et al , 2009) 28 34 28 17 11
(Xu et al , 2008) 609 187 362 182 47
Ours1 95 118 97 106 99
Ours2 52 77 72 57 65
Table 3: Compare various emotion lexicons
We only considered the noun network which is
richly developed in CCD, as in other semantic dic-
tionaries. For each emotion, we chose its synset
as well as the synsets of its hypernym and hy-
ponym(s). In fact, most of words in the emotion
nouns extracted can be used as verbs or adjectives
in Chinese. However, since CCD is not designed
for emotion analysis, words which are expression
of emotions such as?? (cry) or evaluation such
as?? (cowardice) were included.
Selecting nouns and verbs, Xu and Tao (2003)
offered an emotion taxonomy of 390 emotion
words. The taxonomy contains 24 classes of emo-
tions and excludes Chinese idioms. By our in-
spection to the offered emotion words in this tax-
onomy, the authors tried to exclude expression of
emotions, evaluation and cause of emotions from
emotions, which is similar with our processing12.
Ours2 is intentionally created to compare with this
emotion taxonomy.
Based on (Xu and Tao, 2003), Chen et al
(2009) removed the words of single Chinese char-
acter; let two persons to judge if a word is an
emotional one and only those agreed by the two
persons were seen as emotion words. It is worth
noting that Chen et al (2009) merges? (anger)
and? (fidget) in (Xu and Tao, 2003) to form the
? (anger) lexicon, thus?? (dislike) appears in
anger lexicon. However, we believe?? (dislike)
is different with? (anger), and should be put into
another emotion. Also, we distinguish between?
(hate) and? (anger).
Xu et al (2008) constructed a large-scale affec-
tive lexicon ontology. Given the example words
in their paper, we found that the authors did not
intentionally exclude the expression of emotions
such as???? (literally, red face and ear),?
?? (literally, be smiling). Such criteria of iden-
12Xu and Tao (2003) included words such as ??/??
(be willing to),?? (be careful) in their happiness lexicon,
which we think should not be classified into happiness.
tifying emotion words may partially account for
the large size of their emotion resources.
6 Conclusion and future work
In this paper, aiming to build Chinese emotion lex-
icons, we adopt a graph-based algorithm and in-
corporate multiple resources to improve the qual-
ity of lexicons and save human labor. This is an
initial attempt to build Chinese emotion lexicons,
the quality of constructed emotion lexicons is far
from perfect and is supposed to be improved step
by step.
The method in this paper can be further ex-
tended to subjectivity/polarity classification and
other non-sentimental tasks such as word similar-
ity computing, and can be also adapted to other
languages. The more resources we use, the more
human cost can be saved and the higher the qual-
ity of built emotion lexicons is.
In the future work, we want to construct other
emotion lexicons such as ? (like, love), ? (dis-
like),? (desire) etc. using the same method.
Acknowledgement This research is supported
by National Natural Science Foundation of China
(No.60973053, No.90920011)
References
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training? In Proceed-
ings of the 11th Annual Conference on Computa-
tional Learning Theory, 92-100.
Ying Chen, Sophia Y. M. Lee, and Churen Huang.
2009. A Cognitive-based Annotation System for
Emotion Computing. Proceedings of the Third Lin-
guistic Annotation Workshop (LAW III).
Andrew B. Goldberg, Xiaojin Zhu. 2006. Seeing
stars when there aren?t many stars: graph-based
semi-supervised learning for sentiment categoriza-
tion. Proceedings of TextGraphs: the First Work-
shop on Graph Based Methods for Natural Lan-
guage Processing on the First Workshop on Graph
Based Methods for Natural Language Processing.
Y. Liu and et al 2003. The CCD Construction Model
and Its Auxiliary Tool VACOL. Applied Linguis-
tics, 45(1):83-88.
A. Ortony, G. L. Clore, and M. A. Foss. 1987. The ref-
erential structure of the affective lexicon. Cognitive
Science, 11, 341-364.
1216
Delip Rao and D. Ravichandran. 2009. Semisuper-
vised polarity lexicon induction. Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, 675-682.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons.
In Proceedings of the Second Conference on Em-
pirical Methods in Natural Language Processing,
pages 117-124.
V. Sindhwani, P. Niyogi, and M. Belkin. 2005. A
co-regularization approach to semisupervised learn-
ing with multiple views. Proc. ICML Workshop on
Learning with Multiple views.
H. Tong, J. He, M. Li, C. Zhang, and W. Ma. 2005.
Graph based multi-modality learning. In Proceed-
ings of the 13th Annual ACM international Con-
ference on Multimedia. MULTIMEDIA ?05. ACM,
New York, NY, 862-871.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classi-
fication of reviews. ACL 2002, 417-424.
Xiaojun Wan and Jianguo Xiao. 2009. Graph-Based
Multi-Modality Learning for Topic-Focused Mul-
tiDocument Summarization. IJCAI 2009, 1586-
1591.
Linhong Xu, Hongfei Lin, Yu Pan, Hui Ren and Jian-
mei Chen. 2008. Constructing the Afective Lexicon
Ontology. JOURNAL OF THE CHINA SOCIETY
F0R SCIENTIFIC AND TECHNICAL INFORMA-
TION Vo1.27 No.2, 180-185.
X. Y. Xu, and J. H. Tao. 2003. The study of affective
categorization in Chinese. The 1st Chinese Confer-
ence on Affective Computing and Intelligent Inter-
action. Beijing, China.
Hongbo Xu, Tianfang Yao, and Xuanjing Huang.
2009. The second Chinese Opinion Analysis Eval-
uation(in Chinese). COAE 2009.
D. Zhou, O. Bousquet, T. Lal, J. Weston, and B.
Scholkopf. 2004. Learning with local and global
consistency. Advances in Neural Information Pro-
cessing Systems 16. MIT Press, Cambridge, MA. .
D. Zhou and C. J. C. Burges. 2007. Spectral cluster-
ing and transductive learning with multiple views.
Proceedings of the 24th international conference on
Machine learning.
X. Zhu and Z. Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation.
Technical Report CMUCALD02107. CMU.
1217
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572?581,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-Lingual Mixture Model for Sentiment Classification
Xinfan Meng? ?Furu Wei? Xiaohua Liu? Ming Zhou? Ge Xu? Houfeng Wang?
?MOE Key Lab of Computational Linguistics, Peking University
?Microsoft Research Asia
?{mxf, xuge, wanghf}@pku.edu.cn
?{fuwei,xiaoliu,mingzhou}@microsoft.com
Abstract
The amount of labeled sentiment data in En-
glish is much larger than that in other lan-
guages. Such a disproportion arouse interest
in cross-lingual sentiment classification, which
aims to conduct sentiment classification in the
target language (e.g. Chinese) using labeled
data in the source language (e.g. English).
Most existing work relies on machine trans-
lation engines to directly adapt labeled data
from the source language to the target lan-
guage. This approach suffers from the limited
coverage of vocabulary in the machine transla-
tion results. In this paper, we propose a gen-
erative cross-lingual mixture model (CLMM)
to leverage unlabeled bilingual parallel data.
By fitting parameters to maximize the likeli-
hood of the bilingual parallel data, the pro-
posed model learns previously unseen senti-
ment words from the large bilingual parallel
data and improves vocabulary coverage signifi-
cantly. Experiments on multiple data sets show
that CLMM is consistently effective in two set-
tings: (1) labeled data in the target language are
unavailable; and (2) labeled data in the target
language are also available.
1 Introduction
Sentiment Analysis (also known as opinion min-
ing), which aims to extract the sentiment informa-
tion from text, has attracted extensive attention in
recent years. Sentiment classification, the task of
determining the sentiment orientation (positive, neg-
ative or neutral) of text, has been the most exten-
sively studied task in sentiment analysis. There is
?Contribution during internship atMicrosoft ResearchAsia.
already a large amount of work on sentiment classi-
fication of text in various genres and in many lan-
guages. For example, Pang et al (2002) focus on
sentiment classification of movie reviews in English,
and Zagibalov and Carroll (2008) study the problem
of classifying product reviews in Chinese. During
the past few years, NTCIR1 organized several pi-
lot tasks for sentiment classification of news articles
written in English, Chinese and Japanese (Seki et
al., 2007; Seki et al, 2008).
For English sentiment classification, there are sev-
eral labeled corpora available (Hu and Liu, 2004;
Pang et al, 2002; Wiebe et al, 2005). However, la-
beled resources in other languages are often insuf-
ficient or even unavailable. Therefore, it is desir-
able to use the English labeled data to improve senti-
ment classification of documents in other languages.
One direct approach to leveraging the labeled data
in English is to use machine translation engines as a
black box to translate the labeled data from English
to the target language (e.g. Chinese), and then us-
ing the translated training data directly for the devel-
opment of the sentiment classifier in the target lan-
guage (Wan, 2009; Pan et al, 2011).
Although the machine-translation-based methods
are intuitive, they have certain limitations. First,
the vocabulary covered by the translated labeled
data is limited, hence many sentiment indicative
words can not be learned from the translated labeled
data. Duh et al (2011) report low overlapping
between vocabulary of natural English documents
and the vocabulary of documents translated to En-
glish from Japanese, and the experiments of Duh
1http://research.nii.ac.jp/ntcir/index-en.html
572
et al (2011) show that vocabulary coverage has a
strong correlation with sentiment classification ac-
curacy. Second, machine translation may change the
sentiment polarity of the original text. For exam-
ple, the negative English sentence ?It is too good to
be true? is translated to a positive sentence in Chi-
nese ?????????? by Google Translate
(http://translate.google.com/), which literally means
?It is good and true?.
In this paper we propose a cross-lingual mixture
model (CLMM) for cross-lingual sentiment classifi-
cation. Instead of relying on the unreliable machine
translated labeled data, CLMM leverages bilingual
parallel data to bridge the language gap between the
source language and the target language. CLMM is
a generative model that treats the source language
and target language words in parallel data as gener-
ated simultaneously by a set of mixture components.
By ?synchronizing? the generation of words in the
source language and the target language in a parallel
corpus, the proposed model can (1) improve vocabu-
lary coverage by learning sentiment words from the
unlabeled parallel corpus; (2) transfer polarity label
information between the source language and target
language using a parallel corpus. Besides, CLMM
can improve the accuracy of cross-lingual sentiment
classification consistently regardless of whether la-
beled data in the target language are present or not.
We evaluate the model on sentiment classification
of Chinese using English labeled data. The exper-
iment results show that CLMM yields 71% in accu-
racy when no Chinese labeled data are used, which
significantly improves Chinese sentiment classifica-
tion and is superior to the SVMand co-training based
methods. When Chinese labeled data are employed,
CLMMyields 83% in accuracy, which is remarkably
better than the SVM and achieve state-of-the-art per-
formance.
This paper makes two contributions: (1) we pro-
pose a model to effectively leverage large bilin-
gual parallel data for improving vocabulary cover-
age; and (2) the proposed model is applicable in both
settings of cross-lingual sentiment classification, ir-
respective of the availability of labeled data in the
target language.
The paper is organized as follows. We review re-
lated work in Section 2, and present the cross-lingual
mixture model in Section 3. Then we present the ex-
perimental studies in Section 4, and finally conclude
the paper and outline the future plan in Section 5.
2 Related Work
In this section, we present a brief review of the re-
lated work on monolingual sentiment classification
and cross-lingual sentiment classification.
2.1 Sentiment Classification
Early work of sentiment classification focuses on
English product reviews or movie reviews (Pang et
al., 2002; Turney, 2002; Hu and Liu, 2004). Since
then, sentiment classification has been investigated
in various domains and different languages (Zag-
ibalov and Carroll, 2008; Seki et al, 2007; Seki et
al., 2008; Davidov et al, 2010). There exist two
main approaches to extracting sentiment orientation
automatically. The Dictionary-based approach (Tur-
ney, 2002; Taboada et al, 2011) aims to aggregate
the sentiment orientation of a sentence (or docu-
ment) from the sentiment orientations of words or
phrases found in the sentence (or document), while
the corpus-based approach (Pang et al, 2002) treats
the sentiment orientation detection as a conventional
classification task and focuses on building classifier
from a set of sentences (or documents) labeled with
sentiment orientations.
Dictionary-based methods involve in creating or
using sentiment lexicons. Turney (2002) derives
sentiment scores for phrases by measuring the mu-
tual information between the given phrase and the
words ?excellent? and ?poor?, and then uses the av-
erage scores of the phrases in a document as the
sentiment of the document. Corpus-based meth-
ods are often built upon machine learning mod-
els. Pang et al (2002) compare the performance
of three commonly used machine learning models
(Naive Bayes, Maximum Entropy and SVM). Ga-
mon (2004) shows that introducing deeper linguistic
features into SVM can help to improve the perfor-
mance. The interested readers are referred to (Pang
and Lee, 2008) for a comprehensive review of senti-
ment classification.
2.2 Cross-Lingual Sentiment Classification
Cross-lingual sentiment classification, which aims
to conduct sentiment classification in the target lan-
guage (e.g. Chinese) with labeled data in the source
573
language (e.g. English), has been extensively stud-
ied in the very recent years. The basic idea is to ex-
plore the abundant labeled sentiment data in source
language to alleviate the shortage of labeled data in
the target language.
Most existing work relies on machine translation
engines to directly adapt labeled data from the source
language to target language. Wan (2009) proposes
to use ensemble method to train better Chinese sen-
timent classification model on English labeled data
and their Chinese translation. English Labeled data
are first translated to Chinese, and then two SVM
classifiers are trained on English andChinese labeled
data respectively. After that, co-training (Blum and
Mitchell, 1998) approach is adopted to leverage Chi-
nese unlabeled data and their English translation to
improve the SVM classifier for Chinese sentiment
classification. The same idea is used in (Wan, 2008),
but the ensemble techniques used are various vot-
ing methods and the individual classifiers used are
dictionary-based classifiers.
Instead of ensemblemethods, Pan et al (2011) use
matrix factorization formulation. They extend Non-
negative Matrix Tri-Factorization model (Li et al,
2009) to bilingual view setting. Their bilingual view
is also constructed by using machine translation en-
gines to translate original documents. Prettenhofer
and Stein (2011) use machine translation engines in
a different way. They generalize Structural Corre-
spondence Learning (Blitzer et al, 2006) to multi-
lingual setting. Instead of using machine translation
engines to translate labeled text, the authors use it to
construct the word translation oracle for pivot words
translation.
Lu et al (2011) focus on the task of jointly im-
proving the performance of sentiment classification
on two languages (e.g. English and Chinese) . the
authors use an unlabeled parallel corpus instead of
machine translation engines. They assume paral-
lel sentences in the corpus should have the same
sentiment polarity. Besides, they assume labeled
data in both language are available. They propose
a method of training two classifiers based on maxi-
mum entropy formulation to maximize their predic-
tion agreement on the parallel corpus. However, this
method requires labeled data in both the source lan-
guage and the target language, which are not always
readily available.
3 Cross-Lingual Mixture Model for
Sentiment Classification
In this section we present the cross-lingual mix-
ture model (CLMM) for sentiment classification.
We first formalize the task of cross-lingual sentiment
classification. Then we describe the CLMM model
and present the parameter estimation algorithm for
CLMM.
3.1 Cross-lingual Sentiment Classification
Formally, the task we are concerned about is to de-
velop a sentiment classifier for the target language T
(e.g. Chinese), given labeled sentiment data DS in
the source language S (e.g. English), unlabeled par-
allel corpus U of the source language and the target
language, and optional labeled dataDT in target lan-
guage T . Aligning with previous work (Wan, 2008;
Wan, 2009), we only consider binary sentiment clas-
sification scheme (positive or negative) in this paper,
but the proposed method can be used in other classi-
fication schemes with minor modifications.
3.2 The Cross-Lingual Mixture Model
The basic idea underlying CLMM is to enlarge
the vocabulary by learning sentiment words from the
parallel corpus. CLMM defines an intuitive genera-
tion process as follows. Suppose we are going to
generate a positive or negative Chinese sentence, we
have two ways of generating words. The first way
is to directly generate a Chinese word according to
the polarity of the sentence. The other way is to first
generate an English word with the same polarity and
meaning, and then translate it to a Chinese word.
More formally, CLMM defines a generative mix-
ture model for generating a parallel corpus. The un-
observed polarities of the unlabeled parallel corpus
are modeled as hidden variables, and the observed
words in parallel corpus are modeled as generated by
a set of words generation distributions conditioned
on the hidden variables. Given a parallel corpus, we
fit CLMM model by maximizing the likelihood of
generating this parallel corpus. By maximizing the
likelihood, CLMM can estimate words generation
probabilities for words unseen in the labeled data but
present in the parallel corpus, hence expand the vo-
cabulary. In addition, CLMM can utilize words in
both the source language and target language for de-
574
termining polarity classes of the parallel sentences.
POS 
NEG 
POS 
NEG 
...? 
Source 
Target 
U 
u wt 
ws 
Figure 1: The generation process of the
cross-lingual mixture model
Figure 1 illustrates the detailed process of gener-
ating words in the source language and target lan-
guage respectively for the parallel corpus U , from
the four mixture components in CLMM. Particu-
larly, for each pair of parallel sentences ui ? U , we
generate the words as follows.
1. Document class generation: Generating the
polarity class.
(a) Generating a polarity class cs from a
Bernoulli distribution Ps(C).
(b) Generating a polarity class ct from a
Bernoulli distribution Pt(C)
2. Words generation: Generating the words
(a) Generating source language wordsws from
a Multinomial distribution P (ws|cs)
(b) Generating target language words wt from
a Multinomial distribution P (wt|ct)
3. Words projection: Projecting the words onto
the other language
(a) Projecting the source language wordsws to
target language words wt by word projec-
tion probability P (wt|ws)
(b) Projecting the target language words wt to
source language words ws by word projec-
tion probability P (ws|wt)
CLMM finds parameters by using MLE (Maxi-
mum Likelihood Estimation). The parameters to be
estimated include conditional probabilities of word
to class, P (ws|c) and P (wt|c), and word projection
probabilities, P (ws|wt) and P (wt|ws). We will de-
scribe the log-likelihood function and then show how
to estimate the parameters in subsection 3.3. The
obtained word-class conditional probability P (wt|c)
can then be used to classify text in the target lan-
guages using Bayes Theorem and the Naive Bayes
independence assumption.
Formally, we have the following log-likelihood
function for a parallel corpus U2.
L(?|U) =
|Us|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
[
Nsi log
(
P (ws|cj) + P (ws|wt)P (wt|cj)
)]
+
|Ut|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
[
Nti log
(
P (wt|cj) + P (wt|ws)P (ws|cj)
)]
(1)
where ? is the model parameters;Nsi (Nti) is the oc-
currences of thewordws (wt) in document di; |Ds| is
the number of documents; |C| is the number of class
labels; Vs and Vt are the vocabulary in the source lan-
guage and the vocabulary in the target language.|Us|
and |Ut| are the number of unlabeled sentences in the
source language and target language.
Meanwhile, we have the following log-likelihood
function for labeled data in the source language Ds.
L(?|Ds) =
|Ds|
?
i=1
|C|
?
j=1
|Vs|
?
s=1
Nsi logP (ws|cj)?ij (2)
where ?ij = 1 if the label of di is cj , and 0 otherwise.
In addition, when labeled data in the target lan-
guage is available, we have the following log-
likelihood function.
L(?|Dt) =
|Dt|
?
i=1
|C|
?
j=1
|Vt|
?
t=1
Nti logP (wt|cj)?ij (3)
Combining the above three likelihood functions
together, we have the following likelihood function.
L(?|Dt, Ds, U) = L(?|U) + L(?|Ds) + L(?|Dt)
(4)
Note that the third term on the right hand side
(L(?|Dt)) is optional.
2For simplicity, we assume the prior distribution P (C) is
uniform and drop it from the formulas.
575
3.3 Parameter Estimation
Instead of estimating word projection probability
(P (ws|wt) and P (wt|ws)) and conditional proba-
bility of word to class (P (wt|c) and P (ws|c)) si-
multaneously in the training procedure, we estimate
them separately since the word projection probabil-
ity stays invariant when estimating other parame-
ters. We estimate word projection probability using
word alignment probability generated by the Berke-
ley aligner (Liang et al, 2006). The word align-
ment probabilities serves two purposes. First, they
connect the corresponding words between the source
language and the target language. Second, they ad-
just the strength of influences between the corre-
sponding words. Figure 2 gives an example of word
alignment probability. As is shown, the three words
?tour de force? altogether express a positive mean-
ing, while in Chinese the same meaning is expressed
with only one word ???? (masterpiece). CLMM
use word alignment probability to decrease the in-
fluences from ???? (masterpiece) to ?tour?, ?de?
and ?force? individually, using the word projection
probability (i.e. word alignment probability), which
is 0.3 in this case.
Herman Melville's Moby Dick was a tour de force.  
 
??? ???? ? ?????? ?? ??? 
1  1  .5  .5  1  1  .3  . 3  . 3  
Figure 2: Word Alignment Probability
We use Expectation-Maximization (EM) algo-
rithm (Dempster et al, 1977) to estimate the con-
ditional probability of word ws and wt given class
c, P (ws|c) and P (wt|c) respectively. We derive the
equations for EM algorithm, using notations similar
to (Nigam et al, 2000).
In the E-step, the distribution of hidden variables
(i.e. class label for unlabeled parallel sentences) is
computed according to the following equations.
P (cj |usi) = Z(cusi = cj) =
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
?
cj
?
ws?usi [P (ws|cj) +
?
P (ws|wt)>0 P (ws|wt)P (wt|cj)]
(5)
P (cj |uti) = Z(cuti = cj) =
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
?
cj
?
wt?uti [P (wt|cj) +
?
P (wt|ws)>0 P (wt|ws)P (ws|cj)]
(6)
whereZ(cusi = cj)
(
Z(cuti) = cj
)
is the probability
of the source (target) language sentence usi (uti) in
the i-th pair of sentences ui having class label cj .
In the M-step, the parameters are computed by the
following equations.
P (ws|cj) =
1 +
?|Ds|
i=1 ?s(i)NsiP (cj |di)
|V | +
?|Vs|
s=1 ?(i)NsiP (cj |di)
(7)
P (wt|cj) =
1 +
?|Dt|
i=1 ?t(i)NtiP (cj |di)
|V | +
?|Vt|
t=1 ?(i)NtiP (cj |di)
(8)
where ?s(i) and ?t(i) are weighting factor to con-
trol the influence of the unlabeled data. We set ?s(i)
(
?t(i)
)
to ?s
(
?t
)
when di belongs to unlabeled
data, 1 otherwise. When di belongs to labeled data,
P (cj |di) is 1 when its label is cj and 0 otherwise.
When di belongs to unlabeled data, P (cj |di) is com-
puted according to Equation 5 or 6.
4 Experiment
4.1 Experiment Setup and Data Sets
Experiment setup: We conduct experiments on
two common cross-lingual sentiment classification
settings. In the first setting, no labeled data in the
target language are available. This setting has real-
istic significance, since in some situations we need to
quickly develop a sentiment classifier for languages
that we do not have labeled data in hand. In this
case, we classify text in the target language using
only labeled data in the source language. In the sec-
ond setting, labeled data in the target language are
also available. In this case, a more reasonable strat-
egy is to make full use of both labeled data in the
source language and target language to develop the
sentiment classifier for the target language. In our
experiments, we consider English as the source lan-
guage and Chinese as the target language.
Data sets: For Chinese sentiment classification,
we use the same data set described in (Lu et al,
2011). The labeled data sets consist of two English
data sets and one Chinese data set. The English data
set is from the Multi-Perspective Question Answer-
ing (MPQA) corpus (Wiebe et al, 2005) and the NT-
CIR Opinion Analysis Pilot Task data set (Seki et
al., 2008; Seki et al, 2007). The Chinese data set
also comes from the NTCIR Opinion Analysis Pi-
lot Task data set. The unlabeled parallel sentences
576
are selected from ISI Chinese-English parallel cor-
pus (Munteanu and Marcu, 2005). Following the
description in (Lu et al, 2011), we remove neutral
sentences and keep only high confident positive and
negative sentences as predicted by a maximum en-
tropy classifier trained on the labeled data. Table 1
shows the statistics for the data sets used in the ex-
periments. We conduct experiments on two data set-
tings: (1) MPQA + NTCIR-CH and (2) NTCIR-EN
+ NTCIR-CH.
MPQA NTCIR-EN NTCIR-CH
Positive 1,471(30%) 528 (30%) 2,378 (55%)
Negative 3,487(70%) 1,209(70%) 1,916(44%)
Total 4,958 1,737 4,294
Table 1: Statistics about the Data
CLMM includes two hyper-parameters (?s and
?t) controlling the contribution of unlabeled parallel
data. Larger weights indicate larger influence from
the unlabeled data. We set the hyper-parameters
by conducting cross validations on the labeled data.
WhenChinese labeled data are unavailable, we set?t
to 1 and ?s to 0.1, since no Chinese labeled data are
used and the contribution of target language to the
source language is limited. When Chinese labeled
data are available, we set ?s and ?t to 0.2.
To prevent long sentences from dominating the pa-
rameter estimation, we preprocess the data set by
normalizing the length of all sentences to the same
constant (Nigam et al, 2000), the average length of
the sentences.
4.2 Baseline Methods
For the purpose of comparison, we implement the
following baseline methods.
MT-SVM:We translate the English labeled data to
Chinese using Google Translate and use the transla-
tion results to train the SVM classifier for Chinese.
SVM: We train a SVM classifier on the Chinese
labeled data.
MT-Cotrain: This is the co-training based ap-
proach described in (Wan, 2009). We summarize
the main steps as follows. First, two monolingual
SVM classifiers are trained on English labeled data
and Chinese data translated from English labeled
data. Second, the two classifiers make prediction on
Chinese unlabeled data and their English translation,
respectively. Third, the 100 most confidently pre-
dicted English and Chinese sentences are added to
the training set and the twomonolingual SVMclassi-
fiers are re-trained on the expanded training set. The
second and the third steps are repeated for 100 times
to obtain the final classifiers.
Para-Cotrain: The training process is the same as
MT-Cotrain. However, we use a different set of En-
glish unlabeled sentences. Instead of using the corre-
sponding machine translation of Chinese unlabeled
sentences, we use the parallel English sentences of
the Chinese unlabeled sentences.
Joint-Train: This is the state-of-the-art method de-
scribed in (Lu et al, 2011). This model use En-
glish labeled data and Chinese labeled data to obtain
initial parameters for two maximum entropy clas-
sifiers (for English documents and Chinese docu-
ments), and then conduct EM-iterations to update
the parameters to gradually improve the agreement
of the two monolingual classifiers on the unlabeled
parallel sentences.
4.3 Classification Using Only English Labeled
Data
The first set of experiments are conducted on us-
ing only English labeled data to create the sentiment
classifier for Chinese. This is a challenging task,
since we do not use any Chinese labeled data. And
MPQA and NTCIR data sets are compiled by differ-
ent groups using different annotation guidelines.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM N/A N/A
MT-Cotrain 65.13 59.11
Para-Cotrain 67.21 60.71
Joint-Train N/A N/A
CLMM 70.96 71.52
Table 2: Classification Accuracy Using Only
English Labeled Data
Table 2 shows the accuracy of the baseline sys-
tems as well as the proposed model (CLMM). As
is shown, sentiment classification does not bene-
fit much from the direct machine translation. For
NTCIR-EN+NTCIR-CH, the accuracy of MT-SVM
577
is only 62.34%. For MPQA-EN+NTCIR-CH, the
accuracy is 54.33%, even lower than a trivial
method, which achieves 55.4% by predicting all sen-
tences to be positive. The underlying reason is that
the vocabulary coverage in machine translated data
is low, therefore the classifier learned from the la-
beled data is unable to generalize well on the test
data. Meanwhile, the accuracy of MT-SVM on
NTCIR-EN+NTCIR-CH data set is much better than
that on MPQA+NTCIR-CH data set. That is be-
cause NTCIR-EN and NTCIR-CH cover similar top-
ics. The other two methods using machine translated
data, MT-Cotrain and Para-Cotrain also do not per-
form verywell. This result is reasonable, because the
initial Chinese classifier trained on machine trans-
lated data (MT-SVM) is relatively weak. We also
observe that using a parallel corpus instead of ma-
chine translations can improve classification accu-
racy. It should be noted that we do not have the result
for Joint-Train model in this setting, since it requires
both English labeled data and Chinese labeled data.
4.4 Classification Using English and Chinese
Labeled Data
The second set of experiments are conducted on
using both English labeled data and Chinese labeled
data to develop the Chinese sentiment classifier. We
conduct 5-fold cross validations on Chinese labeled
data. We use the same baseline methods as described
in Section 4.2, but we use natural Chinese sentences
instead of translated Chinese sentences as labeled
data in MT-Cotrain and Para-Cotrain. Table 3 shows
the accuracy of baseline systems as well as CLMM.
Method NTCIR-EN MPQA-EN
NTCIR-CH NTCIR-CH
MT-SVM 62.34 54.33
SVM 80.58 80.58
MT-Cotrain 82.28 80.93
Para-Cotrain 82.35 82.18
Joint-Train 83.11 83.42
CLMM 82.73 83.02
Table 3: Classification Accuracy Using English and
Chinese Labeled Data
As is seen, SVMperforms significantly better than
MT-SVM. One reason is that we use natural Chi-
nese labeled data instead of translated Chinese la-
beled data. Another reason is that we use 5-fold
cross validations in this setting, while the previous
setting is an open test setting. In this setting, SVM
is a strong baseline with 80.6% accuracy. Never-
theless, all three methods which leverage an unla-
beled parallel corpus, namely Para-Cotrain, Joint-
Train and CLMM, still show big improvements over
the SVM baseline. Their results are comparable and
all achieve state-of-the-art accuracy of about 83%,
but in terms of training speed, CLMM is the fastest
method (Table 4). Similar to the previous setting,We
also have the same observation that using a parallel
corpus is better than using translations.
Method Iterations Total Time
Para-Cotrain 100 6 hours
Joint-Train 10 55 seconds
CLMM 10 30 seconds
Table 4: Training Speed Comparison
4.5 The Influence of Unlabeled Parallel Data
We investigate how the size of the unlabeled par-
allel data affects the sentiment classification in this
subsection. We vary the number of sentences in the
unlabeled parallel from 2,000 to 20,000. We use
only English labeled data in this experiment, since
this more directly reflects the effectiveness of each
model in utilizing unlabeled parallel data. From Fig-
ure 3 and Figure 4, we can see that when more unla-
beled parallel data are added, the accuracy of CLMM
consistently improves. The performance of CLMM
is remarkably superior than Para-Cotrain and MT-
Cotrain. When we have 10,000 parallel sentences,
the accuracy of CLMM on the two data sets quickly
increases to 68.77% and 68.91%, respectively. By
contrast, we observe that the performance of Para-
Cotrain and MT-Cotrain is able to obtain accuracy
improvement only after about 10,000 sentences are
added. The reason is that the two methods use ma-
chine translated labeled data to create initial Chinese
classifiers. As is depicted in Table 2, these classifiers
are relatively weak. As a result, in the initial itera-
tions of co-training based methods, the predictions
made by the Chinese classifiers are inaccurate, and
co-training based methods need to see more parallel
578
Number of Sentences
Accur
acy
62
64
66
68
70
l
l l l
l l
l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 3: Accuracy with different size of
unlabeled data for NTICR-EN+NTCIR-CH
Number of Sentences
Accur
acy
55
60
65
70
l
l
l
l l
l l l l l
5000 10000 15000 20000
Modell CLMM MT?Cotrain Para?Cotrain
Figure 4: Accuracy with different size of
unlabeled data for MPQA+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 5: Accuracy with different size of
labeled data for NTCIR-EN+NTCIR-CH
Number of Sentences
Accur
acy
65
70
75
80
l l
l l
l l
l
500 1000 1500 2000 2500 3000 3500
Modell CLMM Joint?Train Para?Cotrain SVM
Figure 6: Accuracy with different size of
labeled data for MPQA+NTCIR-CH
sentences to refine the initial classifiers.
4.6 The Influence of Chinese Labeled Data
In this subsection, we investigate how the size of
the Chinese labeled data affects the sentiment classi-
fication. As is shown in Figure 5 and Figure 6, when
only 500 labeled sentences are used, CLMM is capa-
ble of achieving 72.52% and 74.48% in accuracy on
the two data sets, obtaining 10% and 8% improve-
ments over the SVM baseline, respectively. This
indicates that our method leverages the unlabeled
data effectively. When more sentences are used,
CLMM consistently shows further improvement in
accuracy. Para-Cotrain and Joint-Train show simi-
lar trends. When 3500 labeled sentences are used,
SVM achieves 80.58%, a relatively high accuracy
for sentiment classification. However, CLMM and
the other two models can still gain improvements.
This further demonstrates the advantages of expand-
ing vocabulary using bilingual parallel data.
5 Conclusion and Future Work
In this paper, we propose a cross-lingual mix-
ture model (CLMM) to tackle the problem of cross-
lingual sentiment classification. This method has
two advantages over the existing methods. First, the
proposed model can learn previously unseen senti-
ment words from large unlabeled data, which are not
covered by the limited vocabulary in machine trans-
lation of the labeled data. Second, CLMM can ef-
fectively utilize unlabeled parallel data regardless of
whether labeled data in the target language are used
or not. Extensive experiments suggest that CLMM
consistently improve classification accuracy in both
settings. In the future, we will work on leverag-
ing parallel sentences and word alignments for other
tasks in sentiment analysis, such as building multi-
lingual sentiment lexicons.
Acknowledgment We thank Bin Lu and Lei Wang for
their help. This research was partly supported by National High
Technology Research and Development Program of China (863
Program) (No. 2012AA011101) and National Natural Science
Foundation of China (No.91024009, No.60973053)
579
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, page 120?128.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, page 92?100.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
page 241?249.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), page 1?38.
Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for Cross-Lingual sentiment
classification? In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, page 429?433,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. InProceedings of the
20th international conference on Computational Lin-
guistics, page 841.
Mingqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge
discovery and data mining, page 168?177.
Tao Li, Yi Zhang, and Vikas Sindhwani. 2009. A non-
negative matrix tri-factorization approach to sentiment
classification with lexical prior knowledge. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, page 244?252, Suntec, Singapore, August.
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main con-
ference on Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics, page 104?111.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies-
Volume 1, page 320?330.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classification
from labeled and unlabeled documents using EM. Ma-
chine learning, 39(2):103?134.
Junfeng Pan, Gui-Rong Xue, Yong Yu, and Yang Wang.
2011. Cross-lingual sentiment classification via bi-
view non-negative matrix tri-factorization. Advances
in Knowledge Discovery and Data Mining, page
289?300.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, page 79?86.
Peter Prettenhofer and Benno Stein. 2011. Cross-lingual
adaptation using structural correspondence learning.
ACM Transactions on Intelligent Systems and Technol-
ogy (TIST), 3(1):13.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of opinion analysis pilot task at NTCIR-6.
In Proceedings of NTCIR-6 Workshop Meeting, page
265?278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, Noriko Kando, and Chin-Yew Lin.
2008. Overview of multilingual opinion analysis task
at NTCIR-7. In Proc. of the Seventh NTCIR Workshop.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly
Voll, and Manfred Stede. 2011. Lexicon-Based meth-
ods for sentiment analysis. Comput. Linguist., page to
appear.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics,
page 417?424.
Xiaojun Wan. 2008. Using bilingual knowledge and en-
semble techniques for unsupervised chinese sentiment
analysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
?08, page 553?561, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Xiaojun Wan. 2009. Co-training for cross-lingual senti-
ment classification. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
580
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume 1,
page 235?243.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation,
39(2):165?210.
Taras Zagibalov and John Carroll. 2008. Automatic seed
word selection for unsupervised sentiment classifica-
tion of chinese text. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, page 1073?1080.
581
