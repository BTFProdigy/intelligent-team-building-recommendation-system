The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 289?294,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Memory-based text correction for preposition and determiner errors
Antal van den Bosch
Radboud University Nijmegen
P.O. Box 9103
NL-6500 HD Nijmegen, The Netherlands
a.vandenbosch@let.ru.nl
Peter Berck
Tilburg University
P.O. Box 90153
NL-5000 LE Tilburg, The Netherlands
p.j.berck@tilburguniversity.edu
Abstract
We describe the Valkuil.net team entry for the
HOO 2012 Shared Task. Our systems consists
of four memory-based classifiers that generate
correction suggestions for middle positions in
small text windows of two words to the left
and to the right. Trained on the Google 1TB 5-
gram corpus, the first two classifiers determine
the presence of a determiner or a preposition
between all words in a text in which the actual
determiners and prepositions are masked. The
second pair of classifiers determines which is
the most likely correction given a masked de-
terminer or preposition. The hyperparameters
that govern the classifiers are optimized on
the shared task training data. We point out a
number of obvious improvements to boost the
medium-level scores attained by the system.
1 Introduction
Our Valkuil.net team entry, known under the abbre-
viation ?VA? in the HOO 2012 Shared Task (Dale
et al, 2012), is a simplistic text correction system
based on four memory-based classifiers. The goal of
the system is to be lightweight: simple to set up and
train, fast in execution. It requires a (preferably very
large) corpus to train on, and a closed list of words
which together form the category of interest?in the
HOO 2012 Shared Task context, the two categories
of interest are prepositions and determiners.
As a corpus we used the Google 1TB 5-gram cor-
pus (Brants and Franz, 2006), and we used two lists,
one consisting of 47 prepositions and one consist-
ing of 24 determiners, both extracted from the HOO
2012 Shared Task training data. Using the Google
corpus means that we restricted ourselves to a sim-
ple 5-gram context, which obviously places a limit
on the context sensitivity of our system; yet, we were
able to make use of the entire Google corpus.
Memory-based classifiers have been used for con-
fusible disambiguation (Van den Bosch, 2006) and
agreement error detection (Stehouwer and Van den
Bosch, 2009).1 In both studies it is argued that
fast approximations of memory-based discrimina-
tive classifiers are effective and efficient modules for
spelling correction, particularly because of their in-
sensitivity to the number of classes to be predicted.
They can act as simple binary decision makers (e.g.
for confusible pairs: given this context, is then or
than more likely?), and at the same time they can
handle missing word prediction with up to millions
of possible outcomes, all in the same model. Van
den Bosch (2006) also showed consistent log-linear
performance gains in learning curve experiments,
indicating that more training data continues to be
better for these models even at very large amounts
of training data. The interested reader is referred to
the two studies for more details.
2 System
Our system centers around four classifiers that all
take a windowed input of two words to the left of
the focus, and two words to the right. The focus
may either be a position between two words, or a
determiner or a preposition. In case of a position
1A working context-sensitive spelling checker for Dutch
based on these studies is released under the name Valkuil.net;
see http://valkuil.net ? hence the team name.
289
preposition? determiner?
which preposition? which determiner?
no preposition no determiner
no no
yes
yes
preposition determiner
Figure 1: System architecture. Shaded rectangles are the four classifiers.
between two words, the task is to predict whether
the position should actually be filled by a preposition
or a determiner. When the focus is on a determiner
or preposition, the task may be to decide whether it
should actually be deleted, or whether it should be
replaced.
The main system architecture is displayed in Fig-
ure 1. The classifiers are the shaded rectangular
boxes. They are all based on IGTree, an efficient
decision tree learner (Daelemans et al, 1997), a fast
approximation of memory-based or k-nearest neigh-
bor classification, implemented within the TiMBL2
software package (Daelemans et al, 2010).
The first two classifiers, preposition? and de-
terminer?, are binary classifiers that determine
whether or not there should be a preposition or a de-
terminer, respectively, between two words to the left
and two words to the right:
? The preposition? classifier is trained on all
118,105,582 positive cases of contexts in the
Google 1 TB 5-gram corpus in which one of the
47 known prepositions are found to occur in the
middle position of a 5-gram. To enable the clas-
sifier to answer negatively to other contexts,
roughly the same amount of negative cases of
randomly selected contexts with no preposition
in the middle are added to form a training set
of 235,730,253 cases. In the participating sys-
2http://ilk.uvt.nl/timbl
tem we take each n-gram as a single token, and
ignore the Google corpus token counts. We
performed a validation experiment on a single
90%-10% split of the training data; the classi-
fier is able to make a correct decision on 89.1%
of the 10% heldout cases.
? Analogously, the determiner? classifier takes
all 132,483,802 positive cases of 5-grams with
a determiner in the middle position, and adds
randomly selected negative cases to arrive at a
training set of 252,634,322 cases. On a 90%?
10% split, the classifier makes the correct deci-
sion in 88.4% of the 10% heldout cases.
The second pair of classifiers perform the multi-
label classification task of predicting which preposi-
tion or determiner is most likely given a context of
two words to the left and to the right. Again, these
classifiers are trained on the entire Google 1TB 5-
gram corpus:
? The which preposition? classifier is trained on
the aforementioned 118,105,582 cases of any
of the 47 prepositions occurring in the middle
of 5-grams. The task of the classifier is to gen-
erate a class distribution of likely prepositions
given an input of the four words surrounding
the preposition, with 47 possible outcomes. In
a 90%-10% split experiment on the complete
training set, this classifier labels 59.6% of the
10% heldout cases correctly.
290
? The which determiner? classifier, by analogy,
is trained on the 132,483,802 positive cases of
5-grams with a determiner in the middle po-
sition, and generates class distributions com-
posed of the 24 possible class labels (the pos-
sible determiners). On a 90%-10% split of the
training set, the classifier predicts 63.1% of all
heldout cases correctly.
Using the four classifiers and the system architec-
ture depicted in Figure 1, the system is capable of
detecting missing and unnecessary cases of preposi-
tions and determiners, and of replacing prepositions
and determiners by other more likely alternatives.
Focusing on the preposition half of the system, we
illustrate how these three types of error detection and
correction are carried out.
First, Figure 2 illustrates how a missing preposi-
tion is detected. Given an input text, a four-word
window of two words to the left and two words to the
right is shifted over all words. At any word which is
not in the list of prepositions, the binary preposi-
tion? classifier is asked to determine whether there
should be a preposition in the middle. If the classi-
fier says no, the window is shifted to the next posi-
tion and nothing happens. If the classifier says yes
beyond a certainty threshold (more on this in Sec-
tion 3), the which preposition? classifier is invoked
to make a best guess on which preposition should be
inserted.
preposition?
which preposition?
no preposition
no
yes
preposition
missing 
preposition 
suggestion
Figure 2: Workflow for detecting a missing preposition.
Second, Figure 3 depicts the workflow of how a
preposition deletion is suggested. Given an input
text, all cases of prepositions are sought. Instances
of two words to the left and right of each preposi-
tion are created, and these context windows are pre-
sented to the preposition? classifier. If this classi-
fier says no beyond a certainty threshold, the system
signals that the preposition currently in focus should
be deleted.
preposition?
which preposition?
no preposition
no
yes
preposition
suggested 
deletion of 
preposition
Figure 3: Workflow for suggesting a preposition deletion.
Third, Figure 4 illustrates how a replacement sug-
gestion is generated. Just as with the detection of
deletions, an input text is scanned for all occurrences
of prepositions. Again, contextual windows of two
words to the left and right of each found preposi-
tion are created. These contexts are presented to the
which preposition? classifier, which may produce a
different most likely preposition (beyond a certainty
threshold) than the preposition in the text. If so, the
system signals that the original preposition should
be replaced by the new best guess.
Practically, the system is set up as a master pro-
cess (implemented in Python) that communicates
with the four classifiers over socket connections.
The master process performs all necessary data con-
version and writes its edits to the designated XML
format. First, missing prepositions and determin-
ers are traced according to the procedure sketched
above; second, the classifiers are employed to find
replacement errors; third, unnecessary determiners
and prepositions are sought. The system does not
iterate over its own output.
291
preposition?
which preposition?
no preposition
no
yes
preposition
suggested 
replacement 
of preposition
different?
Figure 4: Workflow for suggesting a preposition replace-
ment.
3 Optimizing the system
When run unfiltered, the four classifiers tend to over-
predict errors massively. They are not very accurate
(the binary classifiers operate at a classification ac-
curacy of 88?89%; the multi-valued classifiers per-
form at 60?63%). On the other hand, they produce
class distributions that have properties that could be
exploited to filter the classifications down to cases
where the system is more certain. This enables us
to tune the precision and recall behavior of the clas-
sifiers, and, for instance, optimize on F-Score. We
introduce five hyperparameter thresholds by which
we can tune our four classifiers.
First we introduce two thresholds for the two bi-
nary classifiers preposition? and determiner?:
M ? When the two binary preposition? and de-
terminer? classifiers are used for detecting
missing prepositions or determiners, the posi-
tive class must be M times more likely than the
negative class.
U ? In the opposite case, when the two binary clas-
sifiers are used for signalling the deletion of an
unnecessary preposition or determiner, the neg-
ative class must be U times more likely than the
positive class.
For the two multi-label classifiers which prepo-
sition? and which determiner? we introduce three
Optimizing on
Task Thresh. Precision Recall F-Score
Prep. M 30 10 20
U 30 4 4
DS 5 50 50
F 50 5 5
R 10 20 20
Det. M 30 10 20
U 30 2 2
DS 5 50 20
F 50 5 20
R 10 20 20
Table 1: Semi-automatically established thresholds that
optimize precision, recall, and F-Score. Optimization
was performed on the HOO 2012 Shared Task training
data.
thresholds (which again can be set separately for de-
terminers and prepositions):
DS ? the distribution size (i.e. the number of la-
bels that have a non-zero likelihood according
to the classifier) must be smaller than DS. A
large DS signals a relatively large uncertainty.
F ? the frequency of occurrence of the most likely
outcome in the training set must be larger than
F . Outcomes with a smaller number of occur-
rences should be distrusted more.
R ? if the most likely outcome is different from the
preposition or determiner currently in the text,
the most likely outcome should be at least R
times more likely than the current preposition
or determiner. Preferably the likelihood of the
latter should be zero.
On the gold training data provided during the
training phase of the HOO 2012 Shared Task we
found, through a semi-automatic optimization pro-
cedure, three settings that optimized precision, re-
call, and F-Score, respectively. Table 3 displays the
optimal settings found. The results given in Sec-
tion 4 always refer to the system optimized on F-
Score, listed in the rightmost column of Table 3.
The table shows that most of the ratio thresholds
found to optimize F-Score are quite high; for ex-
ample, the preposition? classifier needs to assign
292
a likelihood to a positive classification that is at least
20 times more likely than the negative classification
in order to trigger a missing preposition error. The
threshold for marking unnecessary prepositions is
considerably lower at 4, and even at 2 for determin-
ers.
4 Results
The output of our system on the data provided dur-
ing the test phase of the HOO 2012 Shared Task was
processed through the shared task evaluation soft-
ware. The original test data was revised in a correc-
tion round in which a subset of the participants could
suggest corrections to the gold standard. We did not
contribute suggestions for revisions, but our scores
slightly improved after revisions. Table 4 summa-
rizes the best scores of our system optimized on F-
Score, before and after revisions. Our best score is
an overall F-Score of 14.24 on error detection, af-
ter revisions. Our system performs slightly better on
prepositions than on determiners, although the dif-
ferences are small. Optimizing on F-Score implies
that a reasonable balance is found between recall
and precision, but overall our results are not impres-
sive, especially not in terms of correction.
5 Discussion
We presented a preposition and determiner error de-
tection and correction system, the focus task of the
HOO 2012 Shared Task. Our system consists of
four memory-based classifiers and a master process
that communicates with these classifiers in a simple
workflow. It takes several hours to train our system
on the Google 1TB 5-gram corpus, and it takes in the
order of minutes to process the 1,000 training doc-
uments. The system can be trained without need-
ing linguistic knowledge or the explicit computation
of linguistic analysis levels such as POS-tagging or
syntactic analyses, and is to a large extent language-
independent (it does rely on tokenization).
This simple generic approach leads to mediocre
results, however. There is room for improvement.
We have experimented with incorporating the n-
gram counts in the Google corpus in our classi-
fiers, leading to improved recall (post-competition).
It still remains to be seen if the Google corpus is
the best corpus for this task, or for the particu-
lar English-as-a-second-language writer data used
in the HOO 2012 Shared Task. Another likely im-
provement would be to limit which words get cor-
rected by which other words based on confusion
statistics in the training data: for instance, the train-
ing data may tell that ?my? should rarely, if ever, be
corrected into ?your?, but our system is blind to such
likelihoods.
Acknowledgements
The authors thank Ko van der Sloot for his continued
improvements of the TiMBL software. This work is
rooted in earlier joint work funded through a grant
from the Netherlands Organization for Scientific Re-
search (NWO) for the Vici project Implicit Linguis-
tics.
References
T. Brants and A. Franz. 2006. LDC2006T13: Web 1T
5-gram Version 1.
W. Daelemans, A. Van den Bosch, and A. Weijters. 1997.
IGTree: using trees for compression and classification
in lazy learning algorithms. Artificial Intelligence Re-
view, 11:407?423.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2010. TiMBL: Tilburg memory based
learner, version 6.3, reference guide. Technical Report
ILK 10-01, ILK Research Group, Tilburg University.
R. Dale, I. Anisimoff, and G. Narroway. 2012. HOO
2012: A report on the preposition and determiner error
correction shared task. In Proceedings of the Seventh
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, Montreal, Canada.
H. Stehouwer and A. Van den Bosch. 2009. Putting the
t where it belongs: Solving a confusion problem in
Dutch. In S. Verberne, H. van Halteren, and P.-A. Cop-
pen, editors, Computational Linguistics in the Nether-
lands 2007: Selected Papers from the 18th CLIN Meet-
ing, pages 21?36, Nijmegen, The Netherlands.
A. Van den Bosch. 2006. All-word prediction as the
ultimate confusible disambiguation. In Proceedings of
the HLT-NAACL Workshop on Computationally hard
problems and joint inference in speech and language
processing, New York, NY.
293
Before revisions After revisions
Task Evaluation Precision Recall F-Score Precision Recall F-Score
Overall Detection 12.5 15.23 13.73 13.22 15.43 14.24
Recognition 10.87 13.25 11.94 11.59 13.53 12.49
Correction 6.16 7.51 6.77 7.25 8.46 7.8
Prepositions Detection 13.44 14.41 13.91 14.23 14.75 14.49
Recognition 11.46 12.29 11.86 12.65 13.11 12.88
Correction 7.51 8.05 7.77 8.7 9.02 8.85
Determiners Detection 11.04 15.21 12.79 11.71 15.28 13.26
Recognition 10.37 14.29 12.02 10.7 13.97 12.12
Correction 5.02 6.91 5.81 6.02 7.86 6.82
Table 2: Best scores of our system before (left) and after (right) revisions. Scores are reported at the overall level (top),
on prepositions (middle), and determiners (bottom).
294
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 102?108,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Memory-based grammatical error correction
Antal van den Bosch
Radboud University Nijmegen
P.O. Box 9103
NL-6500 HD Nijmegen, The Netherlands
a.vandenbosch@let.ru.nl
Peter Berck
Tilburg University
P.O. Box 90153
NL-5000 LE Tilburg, The Netherlands
p.j.berck@tilburguniversity.edu
Abstract
We describe the ?TILB? team entry for
the CONLL-2013 Shared Task. Our sys-
tem consists of five memory-based classi-
fiers that generate correction suggestions
for center positions in small text windows
of two words to the left and to the right.
Trained on the Google Web 1T corpus, the
first two classifiers determine the presence
of a determiner or a preposition between
all words in a text. The second pair of clas-
sifiers determine which is the most likely
correction of an occurring determiner or
preposition. The fifth classifier is a general
word predictor which is used to suggest
noun and verb form corrections. We re-
port on the scores attained and errors cor-
rected and missed. We point out a num-
ber of obvious improvements to boost the
scores obtained by the system.
1 Introduction
Our team entry, known under the abbreviation
?TILB? in the CONLL-2013 Shared Task, is a sim-
plistic text and grammar correction system based
on five memory-based classifiers implementing
eight different error correctors. The goal of the
system is to be lightweight: simple to set up and
train, fast in execution. It requires a preferably
very large but unannotated corpus to train on, and
closed lists of words that contain categories of in-
terest (in our case, determiners and prepositions).
The error correctors make use of information from
a lemmatizer and a noun and verb inflection mod-
ule. The amount of explicit grammatical infor-
mation input in the system is purposely kept to
a minimum, as accurate deep grammatical infor-
mation cannot be assumed to be present in most
real-world situations and languages. The system
described in this article takes plain text as input
and produces plain text as output.
Memory-based classifiers have been applied to
similar tasks before. (Van den Bosch, 2006) de-
scribes memory based classifiers used for con-
fusible disambiguation, and (Stehouwer and Van
den Bosch, 2009) shows how agreement errors can
be detected. In the 2012 shared task ?Helping Our
Own? (Dale et al, 2012) memory based classifiers
were used to solve the problem of missing and
incorrect determiners and prepositions (Van den
Bosch and Berck, 2012).
The CONLL-2013 Shared Task context limited
the grammatical error correction task to detecting
and correcting five error types:
ArtOrDet Missing, unnecessary or incorrect article or de-
terminer;
Prep Incorrect preposition used;
Nn Wrong form of noun used (e.g. singular instead
of plural);
Vform Incorrect verb form used (e.g. I have went);
SVA Incorrect subject-verb agreement (e.g. He have).
The corrections made by the system are scored
by a program provided by the organizers (Ng,
2012). It takes a plain textfile as input (the output
generated by the system) and outputs a list with
correctly rectified errors followed by precision, re-
call and F-score.
As training material we used two corpora. The
Google Web 1T corpus (Brants and Franz, 2006)
was used to train the classifiers for the ArtOrDet
and Prep error categories. The GigaWord Newspa-
per text corpus1 was used to create the data for the
classifier for the noun and verb-related error cat-
egories. To make the classifiers more compatible
1http://www.ldc.upenn.edu/
102
with each other, future versions of the system will
all be trained on the same corpus. We also used
two lists, one consisting of 64 prepositions and
one consisting of 23 determiners, both extracted
from the CONLL-2013 Shared Task training data.
Using the Google corpus means that we restricted
ourselves to a simple 5-gram context, which ob-
viously places a limit on the context sensitivity of
our system; on the other hand, we were able to
make use of the entire Google Web 1T corpus. The
context for the grammatical error detectors was
kept similar to the other classifiers, also 5-grams.
2 System
Our system is based on five memory-based clas-
sifiers that all run the IGTree classifier algorithm
(Daelemans et al, 1997), a decision-tree approx-
imation of k-nearest neighbour classification im-
plemented in the TiMBL software package.2 The
first two classifiers determine the presence of a de-
terminer or a preposition between all words in a
text in which the actual determiners and prepo-
sitions are masked. The second pair of classi-
fiers determine which is the most likely correction
given a masked determiner or preposition. The
fifth classifier is a general word predictor that is
used for suggesting noun and verb form correc-
tions.
All classifiers take a windowed input of two
words to the left of the focus position, and two
words to the right. The focus may either be a posi-
tion between two words, or be on a word. In case
of a position between two words, the task is to pre-
dict whether the position should actually be filled
by an determiner or a preposition. When the fo-
cus is on the word in question, the task is to decide
whether it should be deleted, or whether it should
be corrected.
It is important to note that not just one classifi-
cation is returned for a given context by the IGTree
classifier, but a distribution of results and their re-
spective occurrence counts. The classifier matches
the words in the context to examples in the tree in
a fixed order, and returns the distribution stored
at that point in the tree when an unknown word
is encountered. This is analogous to the back-
off mechanisms often used in other n-gram based
language modeling systems. When even the first
feature fails to match, the complete class distribu-
tion is returned. The output from the classifiers
2http://ilk.uvt.nl/timbl
is filtered by the error correctors for the correct
answers. Filtering is done based on distribution
size, occurrence counts and ratios in occurrence
counts (in the remainder of the text, where we say
frequency we mean occurrence count), and in the
case of the noun and verb-related error types, on
part-of-speech tags.
The system corrects a text from left to right,
starting with the first word and working its way
to the end. Each error corrector is tried after the
other, in the order specified below, until a correc-
tion is suggested. At this point, the correction is
stored, and the system starts processing the next
word. The other classifiers are not tried anymore
after a correction has been suggested by one of the
classifiers.
The first two classifiers, preposition? and de-
terminer?, are binary classifiers that determine
whether or not there should be a preposition or a
determiner, respectively, between two words to the
left and two words to the right:
? The preposition? classifier is trained on all
120,711,874 positive cases of contexts in the
Google Web 1T corpus in which one of the 64
known prepositions are found to occur in the
middle position of a 5-gram. To enable the
classifier to answer negatively to other con-
texts, roughly the same amount of negative
cases of randomly selected contexts with no
preposition in the middle are added to form
a training set of 238,046,975 cases. We in-
corporate the Google corpus token counts in
our model. We performed a validation exper-
iment on a single 90%-10% split of the train-
ing data; the classifier is able to make a cor-
rect decision on 88.6% of the 10% heldout
cases.
? Analogously, the determiner? classifier
takes all 86,253,841 positive cases of 5-
grams with a determiner in the middle po-
sition, and adds randomly selected negative
cases to arrive at a training set of 169,874,942
cases. On a 90%?10% split, the classifier
makes the correct decision in 90.0% of the
10% heldout cases.
The second pair of classifiers perform the multi-
label classification task of predicting which prepo-
sition or determiner is most likely given a context
of two words to the left and to the right. Again,
103
determiner?
which determiner?
no determiner
no
yes
determiner
suggested 
replacement/
insertion of 
determiner
different and 
confident 
about 
alternative?
preposition?
which preposition?
no preposition
no
yes
preposition
suggested 
replacement/
insertion of 
preposition
different and 
confident 
about 
alternative?
which word?
verb
suggested 
replacement 
of verb
different and 
confident 
about 
alternative?
noun
suggested 
replacement 
of noun
different and 
confident 
about 
alternative?
actual word in text
suggested correction
absence of word
presence classifier
identity classifier
Figure 1: System architecture. Shaded rectangles are the five classifiers.
these classifiers are trained on the entire Google
Web 1T corpus, including its token counts:
? The which preposition? classifier is trained
on the aforementioned 120,711,874 cases of
any of the 64 prepositions occurring in the
middle of 5-grams. The task of the classi-
fier is to generate a class distribution of likely
prepositions given an input of the four words
surrounding the preposition, with 64 possible
outcomes. In a 90%-10% split experiment on
the complete training set, this classifier labels
63.3% of the 10% heldout cases correctly.
? The which determiner? classifier, by anal-
ogy, is trained on the 86,253,841 positive
cases of 5-grams with a determiner in the
middle position, and generates class distribu-
tions composed of the 23 possible class labels
(the possible determiners). On a 90%-10%
split of the training set, the classifier predicts
68.3% of all heldout cases correctly.
The fifth classifier predicts the most likely
word(s) between a context of two words to the left
and two to the right.
? The general word predictor, which word?,
for the grammatical error types, was trained
on 10 million lines of the GigaWord En-
glish Newspaper corpus. This amounts to
66,675,151 5-grams. It predicts the word in
the middle between the two context words on
the left and on the right.
From the predictions of the five classifiers the
following eight error correctors are derived. There
is no one-to-one correspondence between classi-
fier and corrector. The ArtOrDet and Prep error
categories are handled by three separate errors cor-
rectors each that handle replacement, deletion, and
insertion errors. The three error types Nn, Vform
and SVA are handled by just two correctors:
1 missing preposition (Prep)
2 replace preposition (Prep)
3 unnecessary preposition (Prep)
4 missing determiner (ArtOrDet)
5 replace determiner (ArtOrDet)
6 unnecessary determiner (ArtOrDet)
7 noun form (Nn, SVA)
8 verb form (Vform, SVA)
For the latter two error correctors, 7 and 8,
we make additional use of a lemmatizer3 and a
singular-plural determiner and generator4 for noun
form errors, and a verb tense determiner and gen-
erator5 for verb form and SVA errors.
The algorithms for the six preposition and de-
terminer correctors will be explained in the rest of
this section. The algorithms use the same logic,
the difference is in the different lists and parame-
ters used for each error type.
The algorithm for missing preposition (or deter-
miner) is as follows.
1 next word is not a preposition
2 run positive-negative classifier P+?
3 if the classification = + (i.e. we expect a preposition),
and freq(+):freq(?) > MP PNR
4 run the which preposition? classifier
5 if length distribution <= MP DS take answer as missing
preposition
The parameters (MP PNR and MP DS in the
above algorithm) are used to control the certainty
we expect from the classifier. Their values were
determined in our submission to the 2012 ?Helping
3http://www-nlp.stanford.edu/software/
corenlp.shtml
4https://pypi.python.org/pypi/inflect
5http://nodebox.net/code/index.php/
Linguistics
104
Our Own? shared task (Dale et al, 2012), which
focused on determiner and preposition errors (Van
den Bosch and Berck, 2012). Similar classifiers
were used in this year?s system, and the same pa-
rameters were used this time.
In step 3 above, we check the ratio between the
frequency of the positive answer and the negative
answer. If the ratio is larger than the parameter
MP PNR (set to 20) we interpret this as being cer-
tain. In step 5, we prefer a small, sharp distribution
of answers. A large distribution indicates the clas-
sifier not finding any matches in the context and
returning a large distribution with all possible an-
swers. In that case, the majority class tends to be
the majority class of the complete training data,
and not the specific answer(s) in the context we
are looking at. To avoid this we only suggest an
answer when the distribution is equal to or smaller
than a certain preset threshold, MP DS, which was
set to 20 for this task.
The algorithm for replacing propositions (or de-
terminers) proceeds as follows:
1 word in focus is a preposition p
2 run which preposition?, classification is palt
3 if freq(palt) > RP F and
4 if word is in distribution and freq(palt):freq(p) > RP R,
take palt as a correction
This algorithm shows another parameter,
namely a check on frequency (occurrence count).
In order to be generated as a correction, the
alternate answer must have a frequency higher
than RP F, set to 5 in our system, and the ratio
between its frequency and that of the preposition
in the distribution that is the same as in the text
must be larger than RP R. This parameter was set
to 20.
The algorithm for unnecessary preposition (or
determiner) works as follows:
1 word in focus is a preposition
2 run positive-negative classifier P+?
3 if classification = ? and freq(?):freq(+) > UP NPR
4 the preposition is unnecessary
The next two algorithms show the Nn and Vf
correctors. The parameters these correctors use
have not been extensively tweaked, but rather use
the same settings as used in the preposition and
determiner correctors.
The first list shows the algorithm for the noun
type error. This error corrector also makes use of a
noun inflection module to turn singular nouns into
plural and vice versa. The algorithm first looks for
the alternative version of the noun in the distribu-
tion returned by the classifier given the context. If
it is found, and if it is much more frequent in the
distribution than the noun form used in the text, a
noun form error may have been found. The alter-
native form found in the distribution is returned as
the correction.
1 word in focus w is a noun
2 check singular or plural, determine alternate version walt
3 run the which-word? classifier, resulting in distribution
D
4 check if w is in D
5 check if walt is in D
6 if freq(w) in D < 10 and walt is in D use walt as correc-
tion
Finally, the verb form error corrector makes use
of a verb-tense determiner and generator, and a
lemmatizer. The alternative verb forms are gen-
erated from the lemma of the verb and the tense of
the verb. To prevent the system changing, for ex-
ample, give to gave, the generated alternatives are
kept in the same tense as the word in the text. This
does, however, mean that it will not be able to cor-
rect verb tense errors (I see him yesterday versus I
saw him yesterday).
1 word in focus is a verb v
2 determine the lemma of v
3 determine the tense of v
4 generate alternatives in same tense as word, valt
5 run which-word? predictor, resulting in distribution D
6 check if v is in D
7 check which valt are in D, take highest frequency
freq(valt)
8 if freq(valt):freq(v) > 10: take valt as a correction of v
3 Results
Table 1 lists the precision, recall and F-score of our
system on the test data. The test data (Tetreault,
2013) consisted of 300 paragraphs of English text
written by non-native speakers. The system?s out-
put is processed by a scorer supplied by the orga-
nizers (Ng, 2012). For each sentence, it reports the
number of correct, proposed and gold edits, and a
running total of the system?s precision, recall and
F-score.
The system suggested a total of 1,902 edits. Of
these, 118 were correct. The total number of cor-
rect edits was 1,643. To explain the score obtained
by the system, we inspect the kind of errors which
it was subjected to, and what kind of errors it did
correct and which it missed.
105
Precision 6.20%
Recall 7.18%
F1 6.66%
Table 1: Summary Score
We see a number of errors which are difficult to
correct because they depend on understanding the
sentence. Take the following sentence for exam-
ple:
Surveillance technology such as RFID can be
operated twenty-four hours with the absence of
operators to track done every detail about human
activities .
The gold-edit for this sentence is changing with
(word 11) to without. This edit may be question-
able, but questionability aside, it is based on a un-
derstanding of what is being talked about in the
text. Correcting these kinds of errors falls outside
the scope of the system at the moment.
Multi-word edits are also a problem. In All pas-
sengers and pilots were died, the gold-edit is to
change were died to died. In The readers are just
smiling when they flip the page because it never
comes to their mind that one day it might come
true, the gold-edit is to change are just smiling to
just smile. These kind of corrections are missed
by our system at the moment due to the rigid one-
word, left-to-right checking of the sentence.
Inserting more than one word is also problem-
atic for our system at the moment. Take the fol-
lowing sentence.
Firstly , security systems are improved in many
areas such as school campus or at the workplace .
The gold-edit is to insert on the before school. A
potential solution for this problem is to take mul-
tiple passes over the sentence, first inserting on,
followed by the in a later pass.
Nevertheless, the system made a number of cor-
rect edits as well. The next subsections list exam-
ples of each error type and a correction, where ap-
plicable.
Missing determiner
In this sentence, the missing determiner before
smart was corrected by the system.
In spite of that, the smart phone is still a device . . .
In the following sentence however, a determiner
is inserted where it is not needed, before RFID.
. . . the idea of using the RFID to track people . . .
To illustrate the reasoning of our system, the de-
terminer? classifier thinks that it is more than 13
times more likely to find a determiner between of
using and RFID to than not. Of the possible de-
terminers, the determiner the has the highest fre-
quency with 38,809 occurrences.
Replace Determiner
Here is an example of a determiner which is cor-
rected:
. . . signal and also a?the risk that their phone . . .
It also happens that the right determiner is incor-
rectly changed into another determiner, as shown
in the next example.
. . . this kind of tragedy to happen on any?the family.
The determiner the had a frequency of more
than 6 million in the distribution, compared to only
68,612 for any.
Unnecessary Determiner
The system did not detect any unnecessary deter-
miners. It missed, for example, removing the de-
terminer the in this setentence:
. . . technology available for the Man ?s life .
Replace Preposition
In this example, a preposition was corrected.
. . . to be put into?under close surveillance . . .
But in the following sentence
. . . remain functional for?after a long period of . . .
the preposition for is unfortunately changed to af-
ter, which in this context is more common.
Unnecessary Preposition
The following is an example of a correct removal
of a preposition:
106
. . . , many of things that are regarded . . .
Prepositions were also incorrectly removed, as
shown in the following example. Here
. . . that can be out of our imaginations . . .
of is deemed unnecessary.
Missing Preposition
In this example, the missing preposition on was
inserted after live.
. . . find another planet to live on , the earth is . . .
In the sentence
. . . especially in the elderly and the children . . .
the system inserts the preposition in between es-
pecially and and, which in this case was incorrect.
Noun form
The next example shows a noun form correction.
. . . brought harmful side effect?effects to human body
This can, of course, also go wrong:
Since RFID tags?tag attached to the product . . .
Here the singular form of the noun was deemed
correct.
Verb form
Finally, an example of a verb form correction:
People needs?need a safe environment to live . . .
And the final example, an incorrect replacement
of been to was.
. . . that has currently been?was implemented
4 Discussion
We have described a memory-based grammar
checker specialized in correcting the five types of
errors in the CONLL-2013 Shared Task. The sys-
tem is built on five classifiers specialized in the
error categories relevant for the task. They are
trained to find errors in a small local context of
two words to the left and two words to the right.
The system scans each word in each sentence in
the test data and calls the relevant classifier(s) to
determine if a word needs to be replaced, deleted,
or inserted. The classifiers take word tokens as
input; no deep grammatical information was sup-
plied to them. Even though the training data sup-
plied for the task contained syntax trees, they were
not used in creating our system. On the other hand,
the part-of-speech information in the training data
was used to create the lists of prepositions and de-
terminers. Furthermore, a part-of-speech tagger
was used to determine if the noun or verb form
error corrector was to be applied.
There are several obvious shortcomings to this
approach. The most obvious one is that each cor-
rector is applied to single words, using only a
small local context of two words to the left and
right. This may work fine for missing preposi-
tions and determiners, but for spotting grammat-
ical errors like subject-verb agreement this limited
contextual scope is insufficient. It also means that
we are only able to correct ?single words to single
words?. That is, it is not possible to substitute two
words for one, and vice versa. One avenue that
could be explored is larger contexts. In addition,
the classifiers are not limited to words, and con-
texts with other (contextual) information could be
tried as well.
Secondly, the correctors are applied in a strict
order one after the other. This should not be a
big problem as the classifiers are called separately
for their particular part-of-speech category (deter-
miner, preposition, verb, or noun). On the other
hand, this puts a lot of weight on the part of speech
tagger. Ambiguous or wrong tags could cause the
wrong corrector to be tried and even applied, and
could miss a potential correct correction.
Furthermore, the corrected words are not fed
back into the system. This means that the context
after an error still contains that error. This may
cause the classifiers to mismatch and miss the next
error. It should be noted that the small context of
two words to the left and right probably helps to
alleviate this problem. However, making the sys-
tem insert corrections and backtracking a step (or
more) could help towards solving the problem of
multi-word corrections.
Finally, not all correctors found errors. This
may of course depend on the test data, but it seems
unlikely that the data contained no ?missing prepo-
sition? errors. There is a potential gain in tuning
107
the parameters controlling the error correctors.
4.1 Update
The organizers of the shared task updated the m2-
scorer used to calculate the results, resulting in
slightly better scores. Table 2 shows the revised
score of our system, with the old score between
parentheses.
Precision 7.60% (6.20%)
Recall 9.29% (7.18%)
F1 8.36% (6.66%)
Table 2: Revised Summary Score
And to conclude, we continued working on the
system and tweaked some of the parameters con-
trolling the preposition and determiner checkers.
By allowing the correctors to be applied more of-
ten, we see an increase in the number of proposed
and correct edits (2,533 and 178 respectively). The
downside to this is of course that the number of
false positives increases, which decreases the pre-
cision of the system.
The tweaked score is shown in table 3, with the
revised score between parentheses.
Precision 7.03% (7.60%)
Recall 10.83% (9.29%)
F1 8.52% (8.36%)
Table 3: Tweaked Summary Score
These improved scores give us good hope that
the highest scores have not been reached yet.
Acknowledgements
The authors thank Ko van der Sloot for his sus-
tained improvements of the TiMBL software. This
work is rooted in earlier joint work funded through
a grant from the Netherlands Organization for Sci-
entific Research (NWO) for the Vici project Im-
plicit Linguistics.
References
T. Brants and A. Franz. 2006. LDC2006T13: Web 1T
5-gram Version 1.
W. Daelemans, A. Van den Bosch, and A. Weijters.
1997. IGTree: using trees for compression and clas-
sification in lazy learning algorithms. Artificial In-
telligence Review, 11:407?423.
R. Dale, I. Anisimoff, and G. Narroway. 2012. HOO
2012: A report on the preposition and determiner
error correction shared task. In Proceedings of
the Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada.
Daniel Dahlmeier & Hwee Tou Ng. 2012. Better eval-
uation for grammatical error correction. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 568 ? 572.
H. Stehouwer and A. Van den Bosch. 2009. Putting
the t where it belongs: Solving a confusion prob-
lem in Dutch. In S. Verberne, H. van Halteren, and
P.-A. Coppen, editors, Computational Linguistics in
the Netherlands 2007: Selected Papers from the 18th
CLIN Meeting, pages 21?36, Nijmegen, The Nether-
lands.
Hwee Tou Ng & Siew Mei Wu & Yuanbin Wu & Chris-
tian Hadiwinoto & Joel Tetreault. 2013. The conll-
2013 shared task on grammatical error correction. In
Proceedings of the Seventeenth Conference on Com-
putational Natural Language Learning.
A. Van den Bosch and P. Berck. 2012. Memory-based
text correction for preposition and determiner errors.
In Proceedings of the 7th Workshop on the Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 289?294, New Brunswick, NJ. ACL.
A. Van den Bosch. 2006. All-word prediction as the
ultimate confusible disambiguation. In Proceedings
of the HLT-NAACL Workshop on Computationally
hard problems and joint inference in speech and lan-
guage processing, New York, NY.
108
