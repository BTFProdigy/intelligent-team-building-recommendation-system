Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 186?189,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Multi-Representational and Multi-Layered  Treebank for Hindi/Urdu     Rajesh Bhatt  U. of Massachusetts                      Amherst, MA, USA                                    bhatt@linguist.umass.edu  Owen Rambow      Columbia University     New York, NY, USA   rambow@ccls.columbia.edu           
Bhuvana Narasimhan  U. of Colorado                      Boulder, CO, USA                                    narasimb@colorado.edu  Dipti Misra Sharma    Int?l Institute of Info. Technology  Hyderabad, India   dipti@iiit.ac.in                                           
Martha Palmer U. of Colorado Boulder, CO, USA mpalmer@colorado.edu  Fei Xia     University of Washington Seattle, WA, USA fxia@u.washington.edu 
Abstract 
This paper describes the simultaneous develop-ment of dependency structure and phrase structure treebanks for Hindi and Urdu, as well as a Prop-Bank.  The dependency structure and the Prop-Bank are manually annotated, and then the phrase structure treebank is produced automatically.  To ensure successful conversion the development of the guidelines for all three representations are care-fully coordinated.  1 Introduction Annotated corpora have played an increasingly important role in the training of supervised natu-ral language processing components. Today, treebanks have been constructed for many lan-guages, including Arabic, Chinese, Czech, Eng-lish, French, German, Korean, Spanish, and Turkish.  This paper describes the creation of a Hindi/Urdu multi-representational and multi-layered treebank.  Multi-layered means that we design the annotation process from the outset to include both a syntactic annotation and a lexical semantic annotation such as the English Prop-Bank (Palmer et al 2005). Multi-representational means that we distinguish con-ceptually what is being represented from how it is represented; for example, in a case of long-distance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent).  Having made this choice, we can determine how to represent it: For exam-ple, we can use a discontinuous constituent 
(crossing arcs), or we can use a trace and co-indexation.      Flexibility of representation is important be-cause the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al 1999) the automatic mapping from depend-ency to phrase-structure was a major area of re-search. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic represen-tations that applications can make immediate use of (McDonald et al 2005, CoNLL 2006 Shared Task).  We first provide a comparison of de-pendency structure and phrase structure in Sec-tion 2.  Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our con-version approach. 2 Two Kinds of Syntactic Structure  Two different approaches to describing syntactic structure, dependency structure (DS) (Mel??uk 1979) and phrase structure (PS) (Chomsky, 1981), have in a sense divided the field in two, with parallel efforts on both sides.  Formally, in a PS tree, all and only the leaf nodes are labeled 
186
with words from the sentence (or empty catego-ries), while the interior nodes are labeled with nonterminal labels. In a dependency tree, all nodes are labeled with words from the sentence (or empty categories). Linguistically, a PS groups consecutive words hierarchically into phrases (or constituents), and each phrase is as-signed a syntactic label. In a DS, syntactic de-pendency (i.e., the relation between a syntactic head and its arguments and adjuncts) is the pri-mary syntactic relation represented. The notion of constituent is only derived.   In a dependency representation, a node stands for itself, for the lexical category (or ?preterminal?) spanning only the word itself (e.g., N), and for its maximal projection spanning the node and all words in the subtree it anchors (e.g., NP). Thus, intermediate projections which cover only some of the dependents of a word (such as N? or VP) do not directly correspond to anything in a de-pendency representation. Attachments at the dif-ferent levels of projection are therefore not dis-tinguished in a dependency tree. This has certain ramifications for annotation.  Conisder for ex-ample scope in conjunctions.  The two readings of young men and women can be distinguished (are the women young as well or not?). If a de-pendency representation represents conjunction by treating the conjunction as a dependent to the first conjunct, then the two readings do not re-ceive different syntactic representations, unless a scope feature is introduced for the adjective.  Suppose y depends on x in a DS, we need to ad-dress the following questions in order to devise a DS-to-PS conversion algorithm that builds the corresponding phrase structure: 1) What kinds of projections do x and y have? 2) How far should y project before it attaches to x's projection? 3) What position on x's projection chain should y's projec-tion attach to?  These questions are answered by the annotation manual of the target PS represen-tation ? there are many possible answers. If the source dependency representation contains the right kind of information (for example, the scope of adjectives in conjunctions), and if the target phrase structure representation is well docu-mented, then we can devise a conversion algo-rithm.  Another important issue is that of ?non-projectivity? which is used to represent discon-tinuous constituents. Non-projectivity is common in dependency-based syntactic theories, but rare in phrase structure-based theories.  The next sec-
tion highlights our most salient representation choices in Treebank design. 3 Treebank Design Our goal is the delivery of a treebank that is multi-representational: it will have a syntactic dependency version and a phrase structure ver-sion. Another recent trend in treebanking is the addition of deeper, semantic levels of annotation on top of the syntactic annotations of the PTB, for example PropBank (Palmer et al 2005).  A multi-layered approach is also found in the Pra-gue Dependency Treebank (Haji? et al 2001), or in treebanks based on LFG (King et al 2003) or HPSG (Oepen et al 2002). A lesson learned here is that the addition of deeper, more semantic lev-els may be complicated if the syntactic annota-tion was not designed with the possibility of mul-tiple layers of annotation in mind. We therefore also propose a treebank that is from the start multi-layered: we will include a PropBank-style predicate-argument annotation in the release. Crucially, the lexical subcategorization frames that are made explicit during the process of prop-banking should always inform the syntactic structure of the treebanking effort. In addition, some of the distinctions made by PS that are not naturally present in DS, such as unaccusativity and null arguments, are more naturally made dur-ing PropBank annotation. Our current approach anticipates that the addition of the PropBank an-notation to the DS will provide a rich enough structure for accurate PS conversion.   In order to ensure successful conversion from DS to PS, we are simultaneously developing three sets of guidelines for Hindi: dependency struc-ture, phrase structure, and PropBank. While al-lowing DS and PS guidelines to be based on dif-ferent, independently motivated principles (see Section 4), we have been going through a com-prehensive list of constructions in Hindi, care-fully exploring any potentially problematic is-sues.  Specifically, we make sure that both DS and PS represent the same syntactic facts (what is represented): we know that if PS makes a dis-tinction that neither DS nor PropBank make, then we cannot possibly convert automatically. Fur-thermore, we coordinate the guidelines for DS and PS with respect to the examples chosen to support the conversion process.  These examples form a conversion test suite.  
187
4 Syntactic Annotation Choices  4.1 Dependency Structure Guidelines  Our dependency analysis is based on the Pan-inian grammatical model (Bharati et al1999, Sharma et al 2007). The model offers a syntac-tico-semantic level of linguistic knowledge with an especially transparent relationship between the syntax and the semantics.  The sentence is treated as a series of modifier-modified relations which has a primary modified (generally the main verb). The appropriate syntactic cues (rela-tion markers) help in identifying various rela-tions.  The relations are of two types ? karaka and others. 'Karakas' are the roles of various par-ticipants in an action (arguments). For a noun to hold a karaka relation with a verb, it is important that they (noun and verb) have a direct syntactic relation. Relations other than 'karaka' such as purpose, reason, and possession are also captured using the relational concepts of the model (ad-juncts). These argument labels are very similar in spirit to the verb specific semantic role labels used by PropBank, which have already been suc-cessfully mapped to richer semantic role labels from VerbNet and FrameNet. This suggests that much of the task of PropBanking can be done as part of the dependency annotation. 4.2 Phrase Structure Guidelines Our PS guidelines are inspired by the Principles-and-Parameters methodology, as instantiated by the theoretical developments starting with Gov-ernment and Binding Theory (Chomsky 1981). We assume binary branching. There are three theoretical commitments/design considerations that underlie the guidelines. First, any minimal clause distinguishes at most two positions struc-turally (the core arguments). These positions can be identified as the specifier of VP and the com-plement of V. With a transitive predicate, these positions are occupied by distinct NPs while with an unaccusative or passive, the same NP occu-pies both positions. All other NPs are represented as adjuncts. Second, we represent any displace-ment of core arguments from their canonical po-sitions, irrespective of whether a clause boundary is crossed, via traces. The displacement of other arguments is only represented if a clause bound-ary is crossed. Third, syntactic relationships such as agreement and case always require c-command but do not necessarily require a [speci-fier, head] configuration. Within these con-straints, we always choose the simplest structure 
compatible with the word order. We work with a very limited set of category labels (NP, AP, AdvP, VP, CP) assuming that finer distinctions between different kinds of verbal functional heads can be made via features.  4.3 Two Constructions in Hindi We give examples for two constructions in Hindi and show the DS and PS for each. Simple Transitive Clauses:  (1) raam-ne   khiir              khaayii    ram-erg   rice-pudding     ate    ?Ram ate rice-pudding.? The two main arguments of the Hindi verb in Figure 1(b) have dependency types k1 and k2.  They correspond roughly to subject and object, and they are the only arguments that can agree with the verb.  In the PS, Figure 1(a), the two arguments that correspond to k1 and k2 have fixed positions in the phrase structure as ex-plained in Section 4.2. 
 Figure 1: PS and DS for transitive clause in (1).  Unaccusative verbs: (2) darwaazaa  khul   rahaa           hai       door.M        open  Prog.MSg   be.Prs.Sg      ?The door is opening.?  Here, the issue is that the DS guidelines treats unaccusatives like other intransitives, with the surface argument simply annotated as k1.  In contrast, PS shows a derivation in which the sub-ject originates in object position.   
 Figure 2: PS and DS for the unaccusative  in  (2). 5 Conversion Process  The DS-to-PS conversion process has three steps. First, for each (DS, PS) pair appearing in the conversion test suite, we run a consistency 
188
checking algorithm to determine whether the DS and the PS are consistent. The inconsistent cases are studied manually and if the inconsistency cannot be resolved by changing the analyses used in the guidelines, a new DS that is consis-tent with the PS is proposed. We call this new dependency structure ?DScons? (?cons? for ?con-sistency?; DScons is the same as DS for the con-sistent cases). Because the DS and PS guidelines are carefully coordinated, we expect the incon-sistent cases to be rare and well-motivated. Sec-ond, conversion rules are extracted automatically from these (DScons, PS) pairs. Last, given a new DS, a PS is created by applying conversion rules. Note that non-projective DSs will be converted to projective DScons.  (For an alternate account of handling non-projective DSs, see Kuhlman and M?hl (2007).)  A preliminary study on the Eng-lish Penn Treebank showed promising results and error analyses indicated that most conversion errors were caused by ambiguous DS patterns in the conversion rules. This implies that including sufficient information in the input DS could re-duce ambiguity, significantly improving the per-formance of the conversion algorithm. The de-tails of the conversion algorithm and the experi-mental results are described in (Xia et al, 2009). 6 Conclusion We presented our approach to the joint develop-ment of DS and PS treebanks and a PropBank for Hindi/Urdu.  Since from the inception of the pro-ject we have planned manual annotation of DS and automatic conversion to PS, we are develop-ing the annotation guidelines for all structures in parallel.  A series of linguistic constructions with specific examples are being carefully examined for any DS annotation decisions that might result in inconsistency between DS and PS and/or mul-tiple conversion rules with identical DS patterns. Our preliminary studies yield promising results, indicating that coordinating the design of DS/PS and PropBank guidelines and running the con-version algorithm in the early stages is essential to the success of building a multi-representational and multi-layered treebank.   Acknowledgments This work is supported by NSF grants CNS-0751089, CNS-0751171, CNS-0751202, and CNS-0751213.    
References  A. Bharati, V. Chaitanya and R. Sangal. 1999. Natu-ral Language Processesing: A Paninian Per-spective, Prentice Hall of India, New Delhi. N. Chomsky. 1981. Lectures on Government and Binding: The Pisa Lectures. Holland: Foris Pub-lications.  M. Collins, Jan Haji?, L. Ramshaw and C. Tillmann. 1999. A Statistical Parser for Czech. In the Proc of ACL-1999, pages 505-512. J. Haji?, E. Hajicova, M. Holub, P. Pajas, P. Sgall, B. Vidova-Hladka, and V. Reznickova. 2001. The Current Status of the Prague Dependency Tree-bank. Lecture Notes in Artificial Intelligence (LNAI) 2166, pp 11?20, NY. T. H. King, R. Crouch, S. Riezler, M. Dalrymple and R. Kaplan. 2003. The PARC700 Dependency Bank. In Proc. of the 4th Int? Workshop on Linguistically Interpreted Corpora (LINC-2003), Budapest, Hungary. D. Klein and C. D. Manning. 2003. Accurate Unlexi-calized Parsing. In the Proc of ACL-2003,.Japan M. Kuhlmann and M. M?hl. 2007. Mildly context-sensitive dependency language. In the Proc of ACL 2007. Prague, Czech Republic. R. McDonald, F. Pereira, K. Ribarov and J. Haji?. 2005.  Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proc. of HLT-EMNLP 2005.  I. Mel?uk. 1979. Studies in Dependency Syntax. Karoma Publishers, Inc. S. Oepen, K. Toutanova, S. M. Shieber, C. D. Man-ning, D. Flickinger, and T. Brants, 2002. The LinGO Redwoods Treebank: Motivation and Pre-liminary Applications. In Proc. of COLING, 2002. Taipei, Taiwan. M. Palmer, D. Gildea, P. Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Seman-tic Roles.Computational Linguistics, 31(1):71-106. D. M. Sharma, R. Sangal, L. Bai, R. Begam, and K.V. Ramakrishnamacharyulu. 2007. AnnCorra : TreeBanks for Indian Languages, Annotation Guidelines (manuscript), IIIT, Hyderabad, India.  F. Xia, O. Rambow, R. Bhatt, M. Palmer and D. Sharma, 2009. Towards a Multi-Representational Treebank.  In Proc. of the 7th Int?lWorkshop on Treebanks and Linguistic Theories (TLT-7).  
189
Proceedings of the Fifth Law Workshop (LAW V), pages 21?29,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Analysis of the Hindi Proposition Bank using Dependency Structure
Ashwini Vaidya Jinho D. Choi Martha Palmer Bhuvana Narasimhan
Institute of Cognitive Science
University of Colorado at Boulder
{vaidyaa,choijd,mpalmer,narasimb}@colorado.edu
Abstract
This paper makes two contributions. First, we
describe the Hindi Proposition Bank that con-
tains annotations of predicate argument struc-
tures of verb predicates. Unlike PropBanks
in most other languages, the Hind PropBank
is annotated on top of dependency structure,
the Hindi Dependency Treebank. We explore
the similarities between dependency and pred-
icate argument structures, so the PropBank an-
notation can be faster and more accurate. Sec-
ond, we present a probabilistic rule-based sys-
tem that maps syntactic dependents to seman-
tic arguments. With simple rules, we classify
about 47% of the entire PropBank arguments
with over 90% confidence. These preliminary
results are promising; they show how well
these two frameworks are correlated. This can
also be used to speed up our annotations.
1 Introduction
Proposition Bank (from now on, PropBank) is a cor-
pus in which the arguments of each verb predicate
are annotated with their semantic roles (Palmer et
al., 2005). PropBank annotation has been carried
out in several languages; most of them are annotated
on top of Penn Treebank style phrase structure (Xue
and Palmer, 2003; Palmer et al, 2008). However, a
different grammatical analysis has been used for the
Hindi PropBank annotation, dependency structure,
which may be particularly suited for the analysis of
flexible word order languages such as Hindi.
As a syntactic corpus, we use the Hindi Depen-
dency Treebank (Bhatt et al, 2009). Using de-
pendency structure has some advantages. First, se-
mantic arguments1 can be marked explicitly on the
syntactic trees, so annotations of the predicate ar-
gument structure can be more consistent with the
dependency structure. Second, the Hindi Depen-
dency Treebank provides a rich set of dependency
relations that capture the syntactic-semantic infor-
mation. This facilitates mappings between syntac-
tic dependents and semantic arguments. A success-
ful mapping would reduce the annotation effort, im-
prove the inter-annotator agreement, and guide a full
fledged semantic role labeling task.
In this paper, we briefly describe our annotation
work on the Hindi PropBank, and suggest mappings
between syntactic and semantic arguments based on
linguistic intuitions. We also present a probabilistic
rule-based system that uses three types of rules to
arrive at mappings between syntactic and semantic
arguments. Our experiments show some promising
results; these mappings illustrate how well those two
frameworks are correlated, and can also be used to
speed up the PropBank annotation.
2 Description of the Hindi PropBank
2.1 Background
The Hindi PropBank is part of a multi-dimensional
and multi-layered resource creation effort for the
Hindi-Urdu language (Bhatt et al, 2009). This
multi-layered corpus includes both dependency an-
notation as well as lexical semantic information in
the form of PropBank. The corpus also produces
phrase structure representations in addition to de-
1The term ?semantic argument? is used to indicate all num-
bered arguments as well as modifiers in PropBank.
21
pendency structure. The Hindi Dependency Tree-
bank has created an annotation scheme for Hindi
by adapting labels from Panini?s Sanskrit gram-
mar (also known as CPG: Computational Paninian
Grammar; see Begum et al (2008)). Previous work
has demonstrated that the English PropBank tagset
is quite similar to English dependency trees anno-
tated with the Paninian labels (Vaidya et al, 2009).
PropBank has also been mapped to other depen-
dency schemes such as Functional Generative De-
scription (Cinkova, 2006).
2.2 Hindi Dependency Treebank
The Hindi Dependency Treebank (HDT) includes
morphological, part-of-speech and chunking infor-
mation as well as dependency relations. These are
represented in the Shakti Standard Format (SSF; see
Bharati et al (2007)). The dependency labels de-
pict relations between chunks, which are ?minimal
phrases consisting of correlated, inseparable enti-
ties? (Bharati et al, 2006), so they are not neces-
sarily individual words. The annotation of chunks
also assumes that intra-chunk dependencies can be
extracted automatically (Husain et al, 2010).
The dependency tagset consists of about 43 labels,
which can be grouped into three categories: depen-
dency relation labels, modifier labels, and labels for
non-dependencies (Bharati et al, 2009). PropBank
is mainly concerned with those labels depicting de-
pendencies in the domain of locality of verb predi-
cates. The dependency relation labels are based on
the notion of ?karaka?, defined as ?the role played by
a participant in an action?. The karaka labels, k1-5,
are centered around the verb?s meaning. There are
other labels such as rt (purpose) or k7t (location)
that are independent of the verb?s meaning.
2.3 Annotating the Hindi PropBank
The Hindi PropBank (HPB) contains the labeling of
semantic roles, which are defined on a verb-by-verb
basis. The description at the verb-specific level is
fine-grained; e.g., ?hitter? and ?hittee?. These verb-
specific roles are then grouped into broader cate-
gories using numbered arguments (ARG#). Each
verb can also have modifiers not specific to the verb
(ARGM*). The annotation process takes place in two
stages: the creation of frameset files for individual
verb types, and the annotation of predicate argu-
ment structures for each verb instance. As annota-
tion tools, we use Cornerstone and Jubilee (Choi et
al., 2010a; Choi et al, 2010b). The annotation is
done on the HDT; following the dependency anno-
tation, PropBank annotates each verb?s syntactic de-
pendents as their semantic arguments at the chunk
level. Chunked trees are conveniently displayed for
annotators in Jubilee. PropBank annotations gener-
ated in Jubilee can also be easily projected onto the
SSF format of the original dependency trees.
The HPB currently consists of 24 labels including
both numbered arguments and modifiers (Table 1).
In certain respects, the HPB labels make some dis-
tinctions that are not made in some other language
such as English. For instance, ARG2 is subdivided
into labels with function tags, in order to avoid
ARG2 from being semantically overloaded (Yi,
2007). ARGC and ARGA mark the arguments of mor-
phological causatives in Hindi, which is different
from the ARG0 notion of ?causer?. We also intro-
duce two labels to represent the complex predicate
constructions: ARGM-VLV and ARGM-PRX.
Label Description
ARG0 agent, causer, experiencer
ARG1 patient, theme, undergoer
ARG2 beneficiary
ARG3 instrument
ARG2-ATR attribute ARG2-GOL goal
ARG2-LOC location ARG2-SOU source
ARGC causer
ARGA secondary causer
ARGM-VLV verb-verb construction
ARGM-PRX noun-verb construction2
ARGM-ADV adverb ARGM-CAU cause
ARGM-DIR direction ARGM-DIS discourse
ARGM-EXT extent ARGM-LOC location
ARGM-MNR manner ARGM-MNS means
ARGM-MOD modal ARGM-NEG negation
ARGM-PRP purpose ARGM-TMP temporal
Table 1: Hindi PropBank labels.
2.4 Empty arguments in the Hindi PropBank
The HDT and HPB layers have different ways of
handling empty categories (Bhatia et al, 2010).
HPB inserts empty arguments such as PRO (empty
subject of a non-finite clause), RELPRO (empty
22
relative pronoun), pro (pro-drop argument), and
gap-pro (gapped argument). HPB annotates syn-
tactic relations between its semantic roles, notably
co-indexation of the empty argument PRO as well as
gap-pro. The example in Figure 1 shows that Mo-
han and PRO are co-indexed; thus, Mohan becomes
ARG0 of read via the empty argument PRO. There is
no dependency link between PRO and read because
PRO is inserted only in the PropBank layer.
Mohan wanted to read a book
????_?
PRO
?? ???
????
Mohan_ERG
k1
vmod
ARG1
ARG0
ARG0
????
PRO book read want
k2
ARG1
Figure 1: Empty argument example. The upper and lower
edges indicate HDT and HPB labels, respectively.
3 Comparisons between syntactic and
semantic arguments
In this section, we describe the mappings between
HDT and HPB labels based on our linguistic intu-
itions. We show that there are several broad similar-
ities between two tagsets. These mappings form the
basis for our linguistically motivated rules in Sec-
tion 4.2.3. In section 5.5, we analyze whether the
intuitions discussed in this section are borne out by
the results of our probabilistic rule-based system.
3.1 Numbered arguments
The numbered arguments correspond to ARG0-3,
including function tags associated with ARG2. In
PropBank, ARG0 and ARG1 are conceived as
framework-independent labels, closely associated
with Dowty?s Proto-roles (Palmer et al, 2010). For
instance, ARG0 corresponds to the agent, causer, or
experiencer, whether it is realized as the subject of
an active construction or as the object of an adjunct
(by phrase) of the corresponding passive. In this re-
spect, ARG0 and ARG1 are very similar to k1 and
k2 in HDT, which are annotated based on their se-
mantic roles, not their grammatical relation. On the
other hand, HDT treats the following sentences sim-
ilarly, whereas PropBank does not:
? The boy broke the window.
? The window broke.
The boy and the window are both considered k1 for
HDT, whereas PropBank labels the boy as ARG0 and
The window as ARG1. The window is not consid-
ered a primary causer as the verb is unaccusative for
Propbank. For HDT, the notion of unaccusativity is
not taken into consideration. This is an important
distinction that needs to be considered while carry-
ing out the mapping. k1 is thus ambiguous between
ARG0 and ARG1. Also, HDT makes a distinction
between Experiencer subjects of certain verbs, label-
ing them as k4a. As PropBank does not make such
a distinction, k4a maps to ARG0. The Experiencer
subject information is included in the corresponding
frameset files of the verbs. The mappings to ARG0
and ARG1 would be accurate only if they make use
of specific verb information. The mappings for other
numbered arguments as well as ARGC and ARGA are
given in Table 2.
HDT label HPB label
k1 (karta); k4a (experiencer) Arg0
k2 (karma) Arg1
k4 (beneficiary) Arg2
k1s (attribute) Arg2-ATR
k5 (source) Arg2-SOU
k2p (goal) Arg2-GOL
k3 (instrument) Arg3
mk1 (causer) ArgC
pk1 (secondary causer) ArgA
Table 2: Mappings to the HPB numbered arguments.
Note that in HDT annotation practice, k3 and k5
tend to be interpreted in a broad fashion such that
they map not only to ARG3 and ARG2-SOU, but also
to ARGM-MNS and ARGM-LOC (Vaidya and Husain,
2011). Hence, a one-to-one mapping for these la-
bels is not possible. Furthermore, the occurrence of
morphological causatives (ARGC and ARGA) is fairly
low so that we may not be able to test the accuracy
of these mappings with the current data.
3.2 Modifiers
The modifiers in PropBank are quite similar in their
definitions to certain HDT labels. We expect a fairly
high mapping accuracy, especially as these are not
verb-specific. Table 3 shows mappings between
23
HDT labels and HPB modifiers. A problematic map-
ping could be ARGM-MNR, which is quite coarse-
grained in PropBank, applying not only to adverbs
of manner, but also to infinitival adjunct clauses.
HDT label HPB label
sent-adv (epistemic adv) ArgM-ADV
rh (cause/reason) ArgM-CAU
rd (direction) ArgM-DIR
rad (discourse) ArgM-DIS
k7p (location) ArgM-LOC
adv (manner adv) ArgM-MNR
rt (purpose) ArgM-PRP
k7t (time) ArgM-TMP
Table 3: Mappings to the HPB modifiers.
3.3 Simple and complex predicates
HPB distinguishes annotations between simple and
complex predicates. Simple predicates consist of
only a single verb whereas complex predicates con-
sist of a light verb and a pre-verbal element. The
complex predicates are identified with a special label
ARGM-PRX (ARGument-PRedicating eXpresstion),
which is being used for all light verb annotations
in PropBank (Hwang et al, 2010). Figure 2 shows
an example of the predicating noun mention anno-
tated as ARGM-PRX, used with come. The predicat-
ing noun also has its own argument, matter of, in-
dicated with the HDT label r6-k1. The HDT has
two labels, r6-k1 and r6-k2, for the arguments of
the predicating noun. Hence, the argument span for
complex predicates includes not only direct depen-
dents of the verb but also dependents of the noun.
??????_?_????? ??????_?? ????_?? ?? ?_??
hearing_of_during Wed._of matter_of mention_to
k7t
k7t
pof
ARGM-PRX
ARG1
ARGM-TMP
come
???
r6-k1
ARGM-TMP
During the hearing on Wednesday, the matter was mentioned
Figure 2: Complex predicate example.
The ARGM-PRX label usually overlaps with the
HDT label pof, indicating a ?part of units? as pre-
verbal elements in complex predicates. However, in
certain cases, HPB has its own analysis for noun-
verb complex predicates. Hence, not all the nom-
inals labeled pof are labeled as ARGM-PRX. In
the example in Figure 3, the noun chunk important
progress is not considered to be an ARGM-PRX by
HPB (in this example, we have pragati hona; (lit)
progess be; to progress). The nominal for PropBank
is in fact ARG1 of the verb be, rather than a com-
posite on the verb. Additional evidence for this is
that neither the nominal nor the light verb seem to
project arguments of their own.
Important progress has been made in this work
??_???_? ?_?
k7p
pof
ARG1
ARGM-LOC
?????? ??_?? ??
this_work_LOC important_progress be_PRES
Figure 3: HDT vs. HPB on complex predicates.
4 Automatic mapping of HDT to HPB
Mapping between syntactic and semantic structures
has been attempted in other languages. The Penn
English and Chinese Treebanks consist of several se-
mantic roles (e.g., locative, temporal) annotated on
top of Penn Treebank style phrase structure (Marcus
et al, 1994; Xue and Palmer, 2009). The Chinese
PropBank specifies mappings between syntactic and
semantic arguments in frameset files (e.g., SBJ ?
ARG0) that can be used for automatic mapping (Xue
and Palmer, 2003). However, these Chinese map-
pings are limited to certain types of syntactic argu-
ments (mostly subjects and objects). Moreover, se-
mantic annotations on the Treebanks are done inde-
pendently from PropBank annotations, which causes
disagreement between the two structures.
Dependency structure transparently encodes rela-
tions between predicates and their arguments, which
facilitates mappings between syntactic and seman-
tic arguments. Hajic?ova? and Kuc?erova? (2002) tried
to project PropBank semantic roles onto the Prague
Dependency Treebank, and showed that the projec-
tion is not trivial. The same may be true to our case;
however, our goal is not to achieve complete map-
pings between syntactic and semantic arguments,
24
but to find a useful set of mappings that can speed
up our annotation. These mappings will be applied
to our future data as a pre-annotation stage, so that
annotators do not need to annotate arguments that
have already been automatically labeled by our sys-
tem. Thus, it is important to find mappings with high
precision and reasonably good recall.
In this section, we present a probabilistic rule-
based system that identifies and classifies semantic
arguments in the HPB using syntactic dependents in
the HDT. This is still preliminary work; our system
is expected to improve as we annotate more data and
do more error analysis.
4.1 Argument identification
Identifying semantic arguments of each verb pred-
icate is relatively easy given the dependency Tree-
bank. For each verb predicate, we consider all syn-
tactic dependents of the predicate as its semantic
arguments (Figure 4). For complex predicates, we
consider the syntactic dependents of both the verb
and the predicating noun (cf. Section 3.3).
?? ???? ? ?? ??_ ? ?? ??? ??_ ??
Kishori Haridwar_from Delhi come_be
k1
k5
k2p
ARG2-GOL
ARG2-SOU
ARG0
Kishori came from Haridwar to Delhi
Figure 4: Simple predicate example.
With our heuristics, we get a precision of 99.11%,
a recall of 95.50%, and an F1-score of 97.27% for
argument identification. Such a high precision is
expected as the annotation guidelines for HDT and
HPB generally follow the same principles of iden-
tifying syntactic and semantic arguments of a verb.
About 4.5% of semantic arguments are not identi-
fied by our method. Table 4 shows distributions of
the most frequent non-identified arguments.
Label Dist. Label Dist. Label Dist.
ARG0 3.21 ARG1 0.90 ARG2? 0.09
Table 4: Distributions of non-identified arguments caused
by PropBank empty categories (in %).
Most of the non-identified argument are antecedents
of PropBank empty arguments. As shown in Fig-
ure 1, the PropBank empty argument has no depen-
dency link to the verb predicate. Identifying such
arguments requires a task of empty category reso-
lution, which will be explored as future work. Fur-
thermore, we do not try to identify PropBank empty
arguments for now, which will also be explored later.
4.2 Argument classification
Given the identified semantic arguments, we classify
their semantic roles. Argument classification is done
by using three types of rules. Deterministic rules are
heuristics that are straightforward given dependency
structure. Empirically-derived rules are generated
by measuring statistics of dependency features in as-
sociation with semantic roles. Finally, linguistically-
motivated rules are derived from our linguistic intu-
itions. Each type of rule has its own strength; how
to combine them is the art we need to explore.
4.2.1 Deterministic rule
Only one deterministic rule is used in our system.
When an identified argument has a pof dependency
relation with its predicate, we classify the argu-
ment as ARGM-PRX. This emphasizes the advan-
tage of using our dependency structure: classifying
ARGM-PRX cannot be done automatically in most
other languages where there is no information pro-
vided for light verb constructions. This determin-
istic rule is applied before any other type of rule.
Therefore, we do not generate further rules to clas-
sify the ARGM-PRX label.
4.2.2 Empirically-derived rules
Three kinds of features are used for the generation of
empirically-derived rules: predicate ID, predicate?s
voice type, and argument?s dependency label. The
predicate ID is either the lemma or the roleset ID
of the predicate. Predicate lemmas are already pro-
vided in HDT. When we use predicate lemmas, we
assume no manual annotation of PropBank. Thus,
rules generated from predicate lemmas can be ap-
plied to any future data without modification. When
we use roleset ID?s, we assume that sense annota-
tions are already done. PropBank includes anno-
tations of coarse verb senses, called roleset ID?s,
that differentiate each verb predicate with different
25
senses (Palmer et al, 2005). A verb predicate can
form several argument structures with respect to dif-
ferent senses. Using roleset ID?s, we generate more
fine-grained rules that are specific to those senses.
The predicate?s voice type is either ?active? or
?passive?, also provided in HDT. There are not many
instances of passive construction in our current data,
which makes it difficult to generate rules general
enough for future data. However, even with the lack
of training instances, we find some advantage of us-
ing the voice feature in our experiments. Finally, the
argument?s dependency label is the dependency la-
bel of an identified argument with respect to its pred-
icate. This feature is straightforward for the case of
simple predicates. For complex predicates, we use
the dependency labels of arguments with respect to
their syntactic heads, which can be pre-verbal ele-
ments. Note that rules generated with complex pred-
icates contain slightly different features for predicate
lemmas as well; instead of using predicate lemmas,
we use joined tags of the predicate lemmas and the
lemmas of pre-verbal elements.
ID V Drel PBrel #
come a k1 ARG0 1
come a k5 ARG2-SOU 1
come a k2p ARG2-GOL 1
come mention a k7t ARGM-TMP 2
come mention a r6-k1 ARG1 1
Table 5: Rules generated by the examples in Figures 4 and
2. The ID, V, and Drel columns show predicate ID, predicate?s
voice type, and argument?s dependency label. The PBrel col-
umn shows the PropBank label of each argument. The # column
shows the total count of each feature tuple being associated with
the PropBank label. ?a? stands for active voice.
Table 5 shows a set of rules generated by the exam-
ples in Figures 4 (come) and 2 (come mention). No
rule is generated for ARGM-PRX because the label
is already covered by our deterministic rule (Sec-
tion 4.2.1). When roleset ID?s are used in place of
the predicate ID, come and come mention are re-
placed with A.03 and A.01, respectively. These
rules can be formulated as a function rule such that:
rule(id, v, drel) = argmax i P (pbreli)
where P (pbreli) is a probability of the predicted
PropBank label pbreli, given a tuple of features
(id, v, drel). The probability is measured by es-
timating a maximum likelihood of each PropBank
label being associated with the feature tuple. For
example, a feature tuple (come, active, k1) can be
associated with two PropBank labels, ARG0 and
ARG1, with counts of 8 and 2, respectively. In this
case, the maximum likelihoods of ARG0 and ARG1
being associated with the feature tuple is 0.8 and 0.2;
thus rule(come, active, k1) = ARG0.
Since we do not want to apply rules with low con-
fidence, we set a threshold to P (pbrel), so predic-
tions with low probabilities can be filtered out. Find-
ing the right threshold is a task of handling the pre-
cision/recall trade-off. For our experiments, we ran
10-fold cross-validation to find the best threshold.
4.2.3 Linguistically-motivated rules
Linguistically-motivated rules are applied to argu-
ments that the deterministic rule and empirically-
derived rules cannot classify. These rules capture
general correlations between syntactic and seman-
tic arguments for each predicate, so they are not as
fine-grained as empirically-derived rules, but can be
helpful for predicates not seen in the training data.
The rules are manually generated by our annota-
tors and specified in frameset files. Table 6 shows
linguistically-motivated rules for the predicate ?A
(come)?, specified in the frameset file, ?A-v.xml?.3
Roleset Usage Rule
A.01 to come
k1 ? ARG1
k2p ? ARG2-GOL
A.03 to arrive
k1 ? ARG1
k2p ? ARG2-GOL
k5 ? ARG2-SOU
A.02 light verb No rule provided
Table 6: Rules for the predicate ?A (come)?.
The predicate ?A? has three verb senses and each
sense specifies a different set of rules. For instance,
the first rule of A.01 maps a syntactic dependent
with the dependency label k1 to a semantic ar-
gument with the semantic label ARG1. Note that
frameset files include rules only for numbered ar-
guments. Most of these rules should already be in-
cluded in the empirically-derived rules as we gain
3See Choi et al (2010a) for details about frameset files.
26
more training data; however, for an early stage of
annotation, these rules provide useful information.
5 Experiments
5.1 Corpus
All our experiments use a subset of the Hindi Depen-
dency Treebank, distributed by the ICON?10 con-
test (Husain et al, 2010). Our corpus contains about
32,300 word tokens and 2,005 verb predicates, in
which 546 of them are complex predicates. Each
verb predicate is annotated with a verse sense speci-
fied in its corresponding frameset file. There are 160
frameset files created for the verb predicates. The
number may seem small compared to the number
of verb predicates. This is because we do not cre-
ate separate frameset files for light verb construc-
tions, which comprise about 27% of the predicate
instances (see the example in Table 6).
All verb predicates are annotated with argument
structures using PropBank labels. A total of 5,375
arguments are annotated. Since there is a relatively
small set of data, we do not make a separate set for
evaluations. Instead, we run 10-fold cross-validation
to evaluate our rule-based system.
5.2 Evaluation of deterministic rule
First, we evaluate how well our deterministic rule
classifies the ARGM-PRX label. Using the determin-
istic rule, we get a 94.46% precision and a 100%
recall on ARGM-PRX. The 100% recall is expected;
the precision implies that about 5.5% of the time,
light verb annotations in the HPB do not agree with
the complex predicate annotations (pof relation) in
the HDT (cf. Section 3.3). More analysis needs to
be done to improve the precision of this rule.
5.3 Evaluation of empirically-derived rules
Next, we evaluate our empirically-derived rules with
respect to the different thresholds set for P (pbreli).
In general, the higher the threshold is, the higher
and lower the precision and recall become, respec-
tively. Figure 5 shows comparisons between preci-
sion and recall with respect to different thresholds.
Notice that a threshold of 1.0, meaning that using
only rules with 100% confidence, does not give the
highest precision. This is because the model with
this high of a threshold overfits to the training data.
Rules that work well in the training data do not nec-
essarily work as well on the test data.
   0 0.2 0.4 0.6 0.8 1
   
   30
40
50
60
70
80
Threshold
Acc
ura
cy (
in %
)
R
F1
P
0.93
Figure 5: Accuracies achieved by the empirically derived
rules using (lemma, voice, label) features. P, R, and F1
stand for precisions, recalls, and F1-scores, respectively.
We need to find a threshold that gives a high preci-
sion (so annotators do not get confused by the au-
tomatic output) while maintaining a good recall (so
annotations can go faster). With a threshold of 0.93
using features (lemma, voice, dependency label), we
get a precision of 90.37%, a recall of 44.52%, and
an F1-score of 59.65%. Table 7 shows accuracies
for all PropBank labels achieved by a threshold of
0.92 using roleset ID?s instead of predicate?s lem-
mas. Although the overall precision stays about the
same, we get a noticeable improvement in the over-
all recall using roleset ID?s. Note that some labels
are missing in Table 7. This is because either they
do not occur in our current data (ARGC and ARGA)
or we have not started annotating them properly yet
(ARGM-MOD and ARGM-NEG).
5.4 Evaluation of linguistically-motivated rules
Finally, we evaluate the impact of the linguistically-
motivated rules. Table 8 shows accuracies achieved
by the linguistically motivated rules applied after the
empirically derived rules. As expected, the linguis-
tically motivated rules improve the recall of ARGN
significantly, but bring a slight decrease in the pre-
cision. This shows that our linguistic intuitions are
generally on the right track. We may combine some
of the empirically derived rules with linguistically
motivated rules together in the frameset files so an-
notators can take advantage of both kinds of rules in
the future.
27
Dist. P R F1
ALL 100.00 90.59 47.92 62.69
ARG0 17.50 95.83 67.27 79.05
ARG1 27.28 94.47 61.62 74.59
ARG2 3.42 81.48 37.93 51.76
ARG2-ATR 2.54 94.55 40.31 56.52
ARG2-GOL 1.61 64.29 21.95 32.73
ARG2-LOC 0.87 90.91 22.73 36.36
ARG2-SOU 0.83 78.26 42.86 55.38
ARG3 0.08 0.00 0.00 0.00
ARGM-ADV 3.50 31.82 3.93 7.00
ARGM-CAU 1.44 50.00 5.48 9.88
ARGM-DIR 0.43 100.00 18.18 30.77
ARGM-DIS 1.63 26.67 4.82 8.16
ARGM-EXT 1.42 0.00 0.00 0.00
ARGM-LOC 10.77 83.80 27.42 41.32
ARGM-MNR 6.00 57.14 9.18 15.82
ARGM-MNS 0.79 77.78 17.50 28.57
ARGM-PRP 2.15 65.52 17.43 27.54
ARGM-PRX 10.75 94.46 100.00 97.15
ARGM-TMP 7.01 74.63 14.04 23.64
Table 7: Labeling accuracies achieved by the empirically de-
rived rules using (roleset ID, voice, label) features and a thresh-
old of 0.92. The accuracy for ARGM-PRX is achieved by the
deterministic rule. The Dist. column shows a distribution of
each label.
Dist. P R F1
ALL 100.00 89.80 55.28 68.44
ARGN 54.12 91.87 72.36 80.96
ARGM 45.88 85.31 35.14 49.77
ARGN w/o LM 93.63 58.76 72.21
Table 8: Labeling accuracies achieved by the linguistically
motivated rules. The ARGN and ARGM rows show statistics of
all numbered arguments and modifiers combined, respectively.
The ?ARGN w/o LM? row shows accuracies of ARGN achieved
only by the empirically derived rules.
5.5 Error anlaysis
The precision and recall results for ARG0 and ARG1,
are better than expected, despite the complexity of
the mapping (Section 3.1). This is because they oc-
cur most often in the corpus, so enough rules can
be extracted. The other numbered arguments are
closely related to particular types of verbs (e.g., mo-
tion verbs for ARG2-GOL|SOU). Our linguistically
motivated rules are more effective for these types
of HPB labels. We would expect the modifiers to
be mapped independently of the verb, but our ex-
periments show that the presence of the verb lemma
feature enhances the performance of modifiers. Al-
though section 3.2 expects one-to-one mappings for
modifiers, it is not the case in practice.
We observe that the interpretation of labels in an-
notation practice is important. For example, our sys-
tem performs poorly for ARGM-ADV because the la-
bel is used for various sentential modifiers and can
be mapped to as many as four HDT labels. On the
other hand, HPB makes some fine-grained distinc-
tions. For instance, means and causes are distin-
guished using ARGM-CAU and ARGM-MNS labels, a
distinction that HDT does not make. In the example
in Figure 6, we find that aptitude with is assigned to
ARGM-MNS, but gets the cause label rh in HDT.
Rajyapal can call upon any party with his aptitude
???????
Rajyapal
???
his
?? ??_ ?
aptitude_with
?? ??_ ??
any_EMPH
????_ ??
party_DAT
????_ ????_ ?
call_can_be
Figure 6: Means vs. cause example.
6 Conclusion and future work
We provide an analysis of the Hindi PropBank anno-
tated on the Hindi Dependency Treebank. There is
an interesting correlation between dependency and
predicate argument structures. By analyzing the
similarities between the two structures, we find rules
that can be used for automatic mapping of syntactic
and semantic arguments, and achieve over 90% con-
fidence for almost half of the data. These rules will
be applied to our future data, which will make the
annotation faster and possibly more accurate.
We plan to use different sets of rules generated by
different thresholds to see which rule set leads to the
most effective annotation. We also plan to develop
a statistical semantic role labeling system in Hindi,
once we have enough training data. In addition, we
will explore the possibility of using existing lexical
resource such as WordNet (Narayan et al, 2002) to
improve our system.
Acknowledgements
This work is supported by NSF grants CNS- 0751089, CNS-
0751171, CNS-0751202, and CNS-0751213. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
the views of the National Science Foundation.
28
References
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency annotation scheme for indian languages. In
In Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing, IJCNLP?08.
Akshar Bharati, Dipti Misra Sharma, Lakshmi Bai, and
Rajeev Sangal. 2006. AnnCorra: Guidelines for POS
and Chunk Annotation for Indian Languages. Techni-
cal report, IIIT Hyderabad.
Akshar Bharati, Rajeev Sangal, and Dipti Misra Sharma.
2007. Ssf: Shakti standard format guide. Technical
report, IIIT Hyderabad.
Akshara Bharati, Dipti Misra Sharma, Samar Husain,
Lakshmi Bai, Rafiya Begam, and Rajeev Sangal.
2009. Anncorra : Treebanks for indian languages,
guidelines for annotating hindi treebank. Technical re-
port, IIIT Hyderabad.
Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,
Martha Palmer, Owen Rambow, Dipti Misra Sharma,
Michael Tepper, Ashwini Vaidya, and Fei Xia. 2010.
Empty categories in a hindi treebank. In Proceedings
of the 7th International Conference on Language Re-
sources and Evaluation (LREC?10), pages 1863?1870.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,
Owen Rambow, Dipti Sharma, and Fei Xia. 2009. A
Multi-Representational and Multi-Layered Treebank
for Hindi/Urdu. In In the Proceedings of the Third Lin-
guistic Annotation Workshop held in conjunction with
ACL-IJCNLP 2009.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010a.
Propbank frameset annotation guidelines using a ded-
icated editor, cornerstone. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation, LREC?10, pages 3650?3653.
Jinho D. Choi, Claire Bonial, and Martha Palmer. 2010b.
Propbank instance annotation guidelines using a ded-
icated editor, jubilee. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation, LREC?10, pages 1871?1875.
Silvie Cinkova. 2006. From PropBank to EngVALLEX:
Adapting PropBank-Lexicon to the Valency Theory of
Functional Generative Description. In Proceedings
of the fifth International conference on Language Re-
sources and Evaluation (LREC 2006), Genova, Italy.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/valency structure in propbank, lcs database and
prague dependency treebank: A comparative pi-
lot study. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation,
LREC?02, pages 846?851.
Samar Husain, Prashanth Mannem, Bharat Ram Ambati,
and Phani Gadde. 2010. The ICON-2010 tools contest
on Indian language dependency parsing. In Proceed-
ings of ICON-2010 Tools Contest on Indian Language
Dependency Parsing, ICON?10, pages 1?8.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and Martha
Palmer. 2010. PropBank Annotation of Multilingual
Light Verb Constructions. In Proceedings of the Lin-
guistic Annotation Workshop at ACL 2010.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert Macintyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop, pages
114?119.
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience
in building the indo wordnet - a wordnet for hindi.
In Proceedings of the 1st International Conference on
Global WordNet.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohamed Maamouri, Aous Mansouri, and Wa-
jdi Zaghouani. 2008. A pilot arabic propbank. In Pro-
ceedings of the 6th International Language Resources
and Evaluation, LREC?08, pages 28?30.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Semantic role labeling. In Graeme Hirst, editor, Syn-
thesis Lectures on Human Language Technologies.
Morgan and Claypool.
Ashwini Vaidya and Samar Husain. 2011. A classifica-
tion of dependencies in the Hindi/Urdu Treebank. In
Presented at the Workshop on South Asian Syntax and
Semantics, Amherst, MA.
Ashwini Vaidya, Samar Husain, and Prashanth Mannem.
2009. A karaka based dependency scheme for En-
glish. In Proceedings of the CICLing-2009, Mexico
City, Mexico.
Nianwen Xue and Martha Palmer. 2003. Annotating the
propositions in the penn chinese treebank. In Proceed-
ings of the 2nd SIGHAN workshop on Chinese lan-
guage processing, SIGHAN?03, pages 47?54.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the chinese treebank. Natural Language
Engineering, 15(1):143?172.
Szu-Ting Yi. 2007. Automatic Semantic Role Labeling.
Ph.D. thesis, University of Pennsylvania.
29
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 126?131,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Semantic Roles for Nominal Predicates: Building a Lexical Resource
Ashwini Vaidya and Martha Palmer and Bhuvana Narasimhan
Dept of Linguistics
Institute of Cognitive Science
University of Colorado, Boulder
Boulder, CO 80309
{vaidyaa, mpalmer, narasimb}@colorado.edu
Abstract
The linguistic annotation of noun-verb com-
plex predicates (also termed as light verb con-
structions) is challenging as these predicates
are highly productive in Hindi. For semantic
role labelling, each argument of the noun-verb
complex predicate must be given a role la-
bel. For complex predicates, frame files need
to be created specifying the role labels for
each noun-verb complex predicate. The cre-
ation of frame files is usually done manually,
but we propose an automatic method to expe-
dite this process. We use two resources for
this method: Hindi PropBank frame files for
simple verbs and the annotated Hindi Tree-
bank. Our method perfectly predicts 65% of
the roles in 3015 unique noun-verb combi-
nations, with an additional 22% partial pre-
dictions, giving us 87% useful predictions to
build our annotation resource.
1 Introduction
Ahmed et al (2012) describe several types of com-
plex predicates that are found in Hindi e.g. morpho-
logical causatives, verb-verb complex predicates and
noun-verb complex predicates. Of the three types,
we will focus on the noun-verb complex predicates
in this paper. Typically, a noun-verb complex pred-
icate chorii ?theft? karnaa ?to do? has two compo-
nents: a noun chorii and a light verb karnaa giving
us the meaning ?steal?. Complex predicates 1 may
be found in English e.g. take a walk and many other
languages such as Japanese, Persian, Arabic and
Chinese (Butt, 1993; Fazly and Stevenson, 2007).
1They are also otherwise known as light verb, support verb
or conjunct verb constructions.
The verbal component in noun-verb complex
predicates (NVC) has reduced predicating power
(although it is inflected for person, number, and gen-
der agreement as well as tense-aspect and mood) and
its nominal complement is considered the true pred-
icate, hence the term ?light verb?. The creation of
a lexical resource for the set of true predicates that
occur in an NVC is important from the point of view
of linguistic annotation. For semantic role labelling
in particular, similar lexical resources have been cre-
ated for complex predicates in English, Arabic and
Chinese (Hwang et al, 2010).
1.1 Background
The goal of this paper is to produce a lexical re-
source for Hindi NVCs. This resource is in the form
of ?frame files?, which are directly utilized for Prop-
Bank annotation. PropBank is an annotated cor-
pus of semantic roles that has been developed for
English, Arabic and Chinese (Palmer et al, 2005;
Palmer et al, 2008; Xue and Palmer, 2003). In
Hindi, the task of PropBank annotation is part of a
larger effort to create a multi-layered treebank for
Hindi as well as Urdu (Palmer et al, 2009).
PropBank annotation assumes that syntactic
parses are already available for a given corpus.
Therefore, Hindi PropBanking is carried out on top
of the syntactically annotated Hindi Dependency
Treebank. As the name suggests, the syntactic rep-
resentation is dependency based, which has several
advantages for the PropBank annotation process (see
Section 3).
The PropBank annotation process for Hindi fol-
lows the same two-step process used for other Prop-
Banks. First, the semantic roles that will occur with
each predicate are defined by a human expert. Then,
126
these definitions or ?frame files? are used to guide
the annotation of predicate-argument structure in a
given corpus.
Semantic roles are annotated in the form of num-
bered arguments. In Table 1 PropBank-style seman-
tic roles are listed for the simple verb de;?to give?:
de.01 ?to give?
Arg0 the giver
Arg1 thing given
Arg2 recipient
Table 1: A frame file
The labels ARG0, ARG1 and ARG2 are always de-
fined on a verb-by-verb basis. The description at
the verb-specific level gives details about each num-
bered argument. In the example above, the num-
bered arguments correspond to the giver, thing given
and recipient. In the Hindi treebank, which consists
of 400,000 words, there are nearly 37,576 predi-
cates, of which 37% have been identified as complex
predicates at the dependency level. This implies that
a sizeable portion of the predicates are NVCs, which
makes the task of manual frame file creation time
consuming.
In order to reduce the effort required for manual
creation of NVC frame files, we propose a novel au-
tomatic method for generating PropBank semantic
roles. The automatically generated semantic roles
will be used to create frame files for each com-
plex predicate in the corpus. Our method accurately
predicts semantic roles for almost two thirds of
the unique nominal-verb combinations, with around
20% partial predictions, giving us a total of 87% use-
ful predictions.
For our implementation, we use linguistic re-
sources in the form of syntactic dependency labels
from the treebank. In addition we also have manu-
ally created, gold standard frame files for Hindi sim-
ple verbs2. In the following sections we provide lin-
guistic background, followed by a detailed descrip-
tion of our method. We conclude with an error anal-
ysis and evaluation section.
2http://verbs.colorado.edu/propbank/framesets-hindi/
2 The Nominal and the Light Verb
Semantic roles for the arguments of the light verb are
determined jointly by the noun as well as the light
verb. Megerdoomian (2001) showed that the light
verb places some restrictions on the semantic role of
its subject in Persian. A similar phenomenon may
be observed for Hindi. Compare example 1 with ex-
ample 2 below:
(1) Raam-ne
Ram-erg
cycle-kii
cycle-gen
chorii
theft
kii
do.prf
?Ram stole a bicycle?
(2) aaj
Today
cycle-kii
cycle-gen
chorii
theft
huii
be.pres
?Today a bicycle was stolen?
PropBank annotation assumes that sentences in
the corpus have already been parsed. The annotation
task involves identification of arguments for a given
NVC and the labelling of these arguments with se-
mantic roles. In example 1 we get an agentive sub-
ject with the light verb kar ?do?. However, when it
is replaced by the unaccusative ho ?become? in Ex-
ample 2, then the resulting clause has a theme argu-
ment as its subject. Note that the nominal chorii in
both examples remains the same. From the point
of view of PropBank annotation, the NVC chorii
kii will have both ARG0 and ARG1, but chorii huii
will only have ARG1 for its single argument cycle.
Hence, the frame file for a given nominal must make
reference to the type of light verb that occurs with it.
The nominal as the true predicate also contributes
its own arguments. In example 3, which shows a full
(non-light) use of the verb de ?give?, there are three
arguments: giver(agent), thing given(theme) and re-
cipient. In contrast the light verb usage zor de ?em-
phasis give; emphasize?, seen in example 4, has a
locative marked argument baat par ?matter on? con-
tributed by the nominal zor ?emphasis?.
(3) Raam-ne
Ram-erg
Mohan ko
Mohan-dat
kitaab
book
dii
give.prf
?Ram gave Mohan a book?
(4) Ram ne
Ram-erg
is
this
baat
matter
par
loc
zor
emphasis
diyaa
give.prf
?Ram emphasized this matter?
127
As both noun and light verb contribute to the se-
mantic roles of their arguments, we require linguis-
tic knowledge about both parts of the NVC. The
semantic roles for the nominal need to specify the
co-occurring light verb and the nominal?s argument
roles must also be captured. Table 2 describes the
desired representation for a nominal frame file.
Frame file for chorii-n(oun)
chorii.01: theft-n light verb: kar?do; to
steal?
Arg0 person who steals
Arg1 thing stolen
chorii.02 : theft-n light verb: ho
?be/become; to get
stolen?
Arg1 thing stolen
Table 2: Frame file for predicate noun chorii ?theft? with
two frequently occurring light verbs ho and kar. If other
light verbs are found to occur, they are added as addi-
tional rolesets as chorii.03, chorii.04 and so on.
This frame file shows the representation of a nom-
inal chorii ?theft? that can occur in combination with
a light verb kar ?do? or ho ?happen?. For each
combination, we derive a different set of PropBank
roles: agent and patient for chorii.01 and theme for
chorii.02. Note that the nominal?s frame actually
contains the roles for the combination of nominal
and light verb, and not the nominal alone.
Nominal frame files such as these have already
been defined for English PropBank.3 However, for
English, many nominals in NVCs are in fact nom-
inalizations of full verbs, which makes it far easier
to derive their frame files (e.g. walk in take a walk
is a full verb). For Hindi, this is not the case, and
a different strategy needs to be employed to derive
these frames automatically.
3 Generating Semantic Roles
The Hindi Treebank has already identified NVC
cases by using a special label pof or ?part-of?. The
Treebank annotators apply this label on the basis of
native speaker intuition. We use the label given by
the Treebank as a means to extract the NVC cases
(the issues related to complex predicate identifica-
tion are beyond the scope of this paper). Once this
3http://verbs.colorado.edu/propbank/framesets-noun/
extraction step is complete, we have a set of nomi-
nals and a corresponding list of light verbs that occur
with them.
In Section 2, we showed that the noun as well
as the light verb in a sentence influence the type of
semantic roles that will occur. Our method builds
on this idea and uses two resources in order to de-
rive linguistic knowledge about the NVC: PropBank
frame files for simple verbs in Hindi and the Hindi
Treebank, annotated with dependency labels. The
next two sections describe the use of these resources
in some detail.
3.1 Karaka to PropBank Mapping
The annotated Hindi Treebank is based on a depen-
dency framework (Begum et al, 2008) and has a
very rich set of dependency labels. These labels
(also known as karaka labels) represent the relations
between a head (e.g. a verb) and its dependents (e.g.
arguments). Using the Treebank we extract all the
dependency karaka label combinations that occur
with a unique instance of an NVC. We filter them
to include argument labels and discard those labels
that are usually used for adjuncts. We then calculate
the most frequently occurring combination of labels
that will occur with that NVC. Finally, we get a tu-
ple consisting of an NVC, a set of karaka argument
labels that occur with it and a count of the number
of times that NVC has occurred in the corpus. The
karaka labels are then mapped onto PropBank la-
bels. We reproduce in Table 3 the numbered argu-
ments to karaka label mapping found in Vaidya et
al., (2011).
PropBank label Treebank label
Arg0 (agent) k1 (karta); k4a (experiencer)
Arg1 (theme,
patient)
k2 (karma)
Arg2 (beneficiary) k4 (beneficiary)
Arg2-ATR(attribute) k1s (attribute)
Arg2-SOU(source) k5 (source)
Arg2-GOL(goal) k2p (goal)
Arg3 (instrument) k3 (instrument)
Table 3: Mapping from Karaka labels to PropBank
3.2 Verb Frames
Our second resource consists of PropBank frames
for full Hindi verbs. Every light verb that occurs in
128
Hindi is also used as a full verb, e.g. de ?give? in
Table 1 may be used both as a ?full? verb as well as
a ?light? verb. As a full verb, it has a frame file in
Hindi PropBank. The set of roles in the full verb
frame is used to generate a ?canonical? verb frame
for each light verb. The argument structure of the
light verb will change when combined with a nom-
inal, which contributes its own arguments. How-
ever, as a default, the canonical argument structure
list captures the fact that most kar ?do? light verbs
are likely to occur with the roles ARG0 and ARG1
respectively or that ho ?become?, an unaccusative
verb, occurs with only ARG1.
3.3 Procedure
Our procedure integrates the two resources de-
scribed above. First, the tuple consisting of karaka
labels for a particular NVC is mapped to PropBank
labels. But many NVC cases occur just once in the
corpus and the karaka label tuple may not be very
reliable. Hence, the likelihood that the mapped tu-
ple accurately depicts the correct semantic frame is
not very high. Secondly, Hindi can drop manda-
tory subjects or objects in a sentence e.g., (vo) ki-
taab paRegaa; ?(He) will read the book?. These are
not inserted by the dependency annotation (Bhatia
et al, 2010) and are not easy to discover automati-
cally (Vaidya et al, 2012). We cannot afford to ig-
nore any of the low frequency cases as each NVC
in the corpus must be annotated with semantic roles.
In order to get reasonable predictions for each NVC,
we use a simple rule. We carry out a mapping from
karaka to PropBank labels only if the NVC occurs at
least 30 times in the corpus. If the NVC occurs fewer
than 30 times, then we use the ?canonical? verb list.
4 Evaluation
The automatic method described in the previous sec-
tion generated 1942 nominal frame files. In or-
der to evaluate the frame files, we opted for man-
ual checking of the automatically generated frames.
The frame files were checked by three linguists and
the checking focused on the validity of the seman-
tic roles. The linguists also indicated whether an-
notation errors or duplicates were present. There
was some risk that the automatically derived frames
could bias the linguists? choice of roles as it is
quicker to accept a given suggestion than propose
an entirely new set of roles for the NVC. As we
had a very large number of automatically gener-
ated frames, all of which would need to be checked
manually anyway, practical concerns determined the
choice of this evaluation.
After this process of checking, the total number
of frame files stood at 1884. These frame files con-
sisted of 3015 rolesets i.e. individual combinations
of a nominal with a light verb (see Table 2). The
original automatically generated rolesets were com-
pared with their hand corrected counterparts (i.e.
manually checked ?gold? rolesets) and evaluated for
accuracy. We used three parameters to compare the
gold rolesets with the automatically generated ones:
a full match, partial match and no match. Table 4
shows the results derived from each resource (Sec-
tion 3) and the total accuracy.
Type of Match Full Partial None Errors
Karaka Mapping 25 31 4 0
Verbal Frames 1929 642 249 143
Totals 1954 673 245 143
% Overall 65 22 8 5
Table 4: Automatic mapping results, total frames=3015
The results show that almost two thirds of the se-
mantic roles are guessed correctly by the automatic
method, with an additional 22% partial predictions,
giving us a total of 87% useful predictions. Only
8% show no match at all between the automatically
generated labels and the gold labels.
When we compare the contribution of the karaka
labels with the verb frames, we find that the verb
frames contribute to the majority of the full matches.
The karaka mapping contributes relatively less as
only 62 NVC types occur more than 30 times in
the corpus. If we reduce our frequency requirement
from of 30 to 5, the accuracy drops by 5%. The bulk
of the cases are thus derived from the simple verb
frames. We think that the detailed information in
the verb frames, such as unaccusativity contributes
towards generating the correct frame files.
It is interesting to observe that nearly 65% accu-
racy can be achieved from the verbal information
alone. The treebank has two light verbs that occur
with high frequency i.e. kar ?do? and ho ?become?.
These combine with a variety of nominals but per-
129
Light verb Full (%) None (%) Total
Uses*
kar?do? 64 8 1038
ho ?be/become? 81 3 549
de ?give? 55 34 157
A ?come? 31 42 36
Table 5: Light verbs ?do? and ?be/become? vs. ?give? and
?come?. *The unique total light verb usages in the corpus
form more consistently than light verbs such as de
?give? or A ?come?. The light verb kar adds inten-
tionality to the NVC, but appears less often with a
set of semantic roles that are quite different from
its original ?full? verb usage. In comparison, the
light verbs such as de ?give? show far more varia-
tion, and as seen from Table 4, will match with au-
tomatically derived frames to a lesser extent. The
set of nominals that occur in combination with kar,
usually seem to require only a doer and a thing
done. Borrowed English verbs such dijain?design?
or Pona?phone? will appear preferentially with kar
in the corpus and as they are foreign words they do
not add arguments of their own.
One of the advantages of creating this lexical re-
source is the availability of gold standard frame files
for around 3000 NVCs in Hindi. As a next step, it
would be useful to use these frames to make some
higher level generalizations about these NVCs. For
example, much work has already been done on au-
tomatic verb classification for simple predicates e.g.
(Merlo and Stevenson, 2001; Schulte im Walde,
2006), and perhaps such classes can be derived for
NVCs. Also, the frame files do not currently address
the problem of polysemous NVCs which could ap-
pear with a different set of semantic roles, which will
be addressed in future work.
Acknowledgments
I am grateful to Archna Bhatia and Richa Srishti for
their help with evaluating the accuracy of the nom-
inal frames. This work is supported by NSF grants
CNS-0751089, CNS-0751171, CNS-0751202, and
CNS-0751213.
References
Tafseer Ahmed, Miriam Butt, Annette Hautli, and Se-
bastian Sulger. 2012. A reference dependency bank
for analyzing complex predicates. In Proceedings of
the Eight International Conference on Language Re-
sources and Evaluation (LREC?12.
Rafiya Begum, Samar Husain, Arun Dhwaj, Dipti Misra
Sharma, Lakshmi Bai, and Rajeev Sangal. 2008. De-
pendency Annotation Scheme for Indian Languages.
In Proceedings of The Third International Joint Con-
ference on Natural Language Processing (IJCNLP).
Hyderabad, India.
Archna Bhatia, Rajesh Bhatt, Bhuvana Narasimhan,
Martha Palmer, Owen Rambow, Dipti Misra Sharma,
Michael Tepper, Ashwini Vaidya, and Fei Xia. 2010.
Empty Categories in a Hindi Treebank. In Proceed-
ings of the 7th International Conference on Language
Resources and Evaluation (LREC?10), pages 1863?
1870.
Miriam Butt. 1993. The Light Verb Jungle. In G. Aygen,
C. Bowers, and C. Quinn, editors, Harvard Working
Papers in Linguistics: Papers from the GSAS/Dudley
House workshop on light verbs, volume 9.
Afsaneh Fazly and Suzanne Stevenson. 2007. Au-
tomatic Acquisition of Knowledge about Multiword
Predicates. In Proceedings of PACLIC 19, the 19th
Asia-Pacific Conference on Language, Information
and Computation.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue, and Martha
Palmer. 2010. PropBank Annotation of Multilingual
Light Verb Constructions. In Proceedings of the Lin-
guistic Annotation Workshop held in conjunction with
ACL-2010.
Karine Megerdoomian. 2001. Event Structure and Com-
plex Predicates in Persian. Canadian Journal of Lin-
guistics, 46:97?125.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb Classification Based on Statistical Distributions
of Argument Structure. Computational Linguistics,
27(3):373?408.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona
Diab, Mohammed Maamouri, Aous Mansouri, and
Wajdi Zaghouani. 2008. A pilot Arabic PropBank.
In Proceedings of the 6th International Language Re-
sources and Evaluation.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan,
Owen Rambow, Dipti Misra Sharma, and Fei Xia.
2009. Hindi Syntax: Annotating Dependency, Lexical
130
Predicate-Argument Structure, and Phrase Structure.
In Proceedings of ICON-2009: 7th International Con-
ference on Natural Language Processing, Hyderabad.
Sabine Schulte im Walde. 2006. Experiments on the Au-
tomatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159?194.
Ashwini Vaidya, Jinho D. Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2011. Analysis of the Hindi propo-
sition bank using dependency structure. In Proceed-
ings of the 5th Linguistic Annotation Workshop - LAW
V ?11.
Ashwini Vaidya, Jinho D. Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2012. Empty Argument Insertion
in the Hindi PropBank. In Proceedings of the Eighth
International Conference on Language Resources and
Evaluation - LREC-12, Istanbul.
Nianwen Xue and Martha Palmer. 2003. Annotating the
Propositions in the Penn Chinese Treebank. In Pro-
ceedings of the 2nd SIGHAN workshop on Chinese
language processing, SIGHAN?03, pages 47?54.
131
