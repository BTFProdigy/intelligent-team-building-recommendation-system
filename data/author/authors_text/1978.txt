Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1153?1160
Manchester, August 2008
Choosing the Right Translation:
A Syntactically Informed Classification Approach
Simon Zwarts
Centre for Language Technology
Macquarie University
Sydney, Australia
szwarts@ics.mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
madras@ics.mq.edu.au
Abstract
One style of Multi-Engine Machine
Translation architecture involves choos-
ing the best of a set of outputs from
different systems. Choosing the best
translation from an arbitrary set, even
in the presence of human references, is
a difficult problem; it may prove better
to look at mechanisms for making such
choices in more restricted contexts.
In this paper we take a classification-
based approach to choosing between
candidates from syntactically informed
translations. The idea is that using
multiple parsers as part of a classifier
could help detect syntactic problems in
this context that lead to bad transla-
tions; these problems could be detected
on either the source side?perhaps sen-
tences with difficult or incorrect parses
could lead to bad translations?or on
the target side?perhaps the output
quality could be measured in a more
syntactically informed way, looking for
syntactic abnormalities.
We show that there is no evidence that
the source side information is useful.
However, a target-side classifier, when
used to identify particularly bad trans-
lation candidates, can lead to signifi-
cant improvements in Bleu score. Im-
provements are even greater when com-
bined with existing language and align-
ment model approaches.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
1 Introduction
It is fairly safe to say that whenever there
are multiple approaches to solving a problem
in Artificial Intelligence, the idea of trying to
find a better solution by combining those ap-
proaches has been proposed: blackboard archi-
tectures, ensemble methods for machine learn-
ing, and so on.
In Machine Translation (MT), there is a
long tradition of combining multiple machine
translations, as through a Multi-Engine MT
(MEMT) architecture; the origins of this are
generally credited to Frederking and Nirenburg
(1994). One way of dividing up such systems
is into those that take the whole output from
multiple systems and judge between them to
select the best candidate, and those that com-
bine elements of the outputs to construct a
best candidate.
Deciding between whole sentence level out-
puts looks like a classical classification prob-
lem. Of course, deciding between MT out-
puts in the general case is a problem that cur-
rently has no good solution, and is unlikely
to in the near future: Bleu (and similar met-
rics) require one or more reference texts to dis-
tinguish between candidate outputs with the
level of accuracy that they achieve, and even
then they are open to substantial criticism
(Callison-Burch et al, 2006). However, there
are reasons to think that there is some promise
in considering this as a classification problem.
Corston-Oliver et al (2001) build a classifier to
distinguish between human and machine trans-
lations with an 80% accuracy. Several other
later systems have some success in distinguish-
ing between MT outputs using language mod-
els, alignment models, and voting schemes. In
addition, while the problem of deciding be-
tween arbitrary MT outputs is difficult, it may
1153
be feasible in specific cases. The classifier con-
structed by Corston-Oliver et al (2001) takes
advantage of characteristic mistakes found in
the output of the particular MT system used.
In general, we are interested in MT where
syntax is involved. The first part of the main
idea of this paper is that there are two ways
in which problematic translations might be de-
tected. One is on the source side: perhaps sen-
tences with difficult or incorrect parses could
lead to bad use of syntax and hence bad trans-
lations, and this could be detected by a clas-
sifier. The other is that on the target side,
perhaps the output quality could be measured
in a more syntactically informed way, looking
for syntactic abnormalities.
As to the particular system, in this paper
we look at a specific type of MT, the output of
systems that use syntactic reordering as pre-
processing (Collins et al, 2005; Wang et al,
2007; Zwarts and Dras, 2007). In these sys-
tems, the source language is reordered to mir-
ror the syntax of the target language in certain
respects, leading to an improvement in the ag-
gregate quality of the output over the base-
line, although it is not always the case that
each individual sentence in the reordered ver-
sion is better. This could then be framed as an
MEMT, where the reordered candidate is con-
sidered the default one, backing off to the base-
line where the reordered one is worse, based on
the decision of a classifier. Given the ?unnatu-
ral? order of the preprocessed source side, there
is reason to expect that bad or unsuccessful re-
ordered translations might be detectable.
The second part of the main idea of the pa-
per is that a classifier could use a combina-
tion of multiple parsers, as in Mutton et al
(2007), to indicate problems. In that work,
designed to assess fluency of output of gen-
eration systems, metrics were developed from
various parsers?log probability of most likely
parse, number of tree fragments, and so on?
that correlated with human judgements, and
that could be combined in a classifier to pro-
duce a better evaluation metric. We take such
an approach as a starting point for developing
classifiers to indicate problematic source and
target sides within a reordering MT system.
In Section 2 we review some related work. In
Section 3 we investigate the potential gain in
correctly choosing the better translation can-
didate in our context. In Section 4 we build
a classifier using an approximation to fairly
standard language and alignment model fea-
tures, mostly for use as a comparator, while
Sections 5 and 6 present our models based on
source and target language sides respectively.
Section 7 concludes.
2 Related work
In this section we briefly review some relevant
work on deciding between translation candi-
dates in ?sentence-level? MEMT.
Most common is the use of language models,
or voting which may be based on some kind of
alignment, or a combination. Callison-Burch
and Flournoy (2001) use a trigram language
model (LM) on MT outputs to decide the best
candidate, looking at nine systems across four
language directions and domains, and treating
them as black boxes; evaluation is by human
judges and on a fairly small data set. Akiba
et al (2002) score MT outputs by a combina-
tion of a standard LM and an alignment model
(here IBM 4), and then use statistical tests
to determine rankings of MT system outputs.
Eisele (2005) uses a heuristic voting scheme
based on n-gram overlap of the different out-
puts, and adds an LM to make decisions; the
LM reportedly achieves further improvement.
Rosti et al (2007) look at sentence-level com-
binations (as well as word- and phrase-level),
using reranking of n-best lists and confidence
scores derived from generalised linear models
with probabilistic features from n-best lists.
Huang and Papineni (2007) propose a hier-
archical model for word, phrase and sentence
level combination; they use LMs and inter-
estingly find that incorporating rudimentary
linguistic information like Part-of-Speech is
helpful. Riezler and Maxwell (2006) combine
transfer-based and statistical MT; they back
off to the SMT translation when the grammar
is inadequate, analysing the grammar to deter-
mine this.
Other work, like ours, uses a classifier. The
goal of Corston-Oliver et al (2001) is slightly
different, in that it aims to distinguish human
translations from MT output. The classifier
uses syntactic features derived from a manual
error analysis, taking advantage of character-
1154
istics specific to their MT system and parser.
Nomoto (2003) uses a LM and an IBM-based
alignment model, and then constructs sepa-
rate SVMs for regression based on these, each
with a single feature (i.e. the LM value or the
alignment model value); the SVM is thus not
strictly used as a classifier, but as a regression
tool. Nomoto (2004) extends this by decid-
ing on the best LM through a voting scheme.
Other related work not in an MEMT context
that uses parsers to distinguish better from
worse translations are on syntax-based lan-
guage models (Charniak et al, 2003) and on
syntactically informed reranking (Och et al,
2003). Both use only single parsers and work
only with candidate translations generated in-
side an SMT system (either all candidates or
n-best).
3 Potential Gain
The type of system we focus on in this pa-
per operates in two stages. First, syntac-
tically based reordering takes place to make
the source sentence more similar in struc-
ture to the syntax of the target language.
This is then passed to a Phrase-based SMT
(PSMT) component (Pharaoh (Koehn, 2004)
in the cited work). For German to English
(Collins et al, 2005) and Dutch to English
(Zwarts and Dras, 2007) this reordering in-
volves moving some long-distance dependen-
cies closer together, such as clause-final par-
ticiples and verb-second auxiliaries. This im-
proves translation quality by compensating for
the weakness in PSMT of long-distance word
reordering: Collins et al (2005) report a 1.6
Bleu percentage point improvement, Zwarts
and Dras (2007) a 1.0 Bleu percentage point
improvement.
However, individual sentences translated
from the original non-reordered source sen-
tences are sometimes better than their re-
ordered equivalent; examples are given in both
Collins et al (2005) and Zwarts and Dras
(2007). (We refer to these in the rest of the
paper as non-reordered translations and re-
ordered translations respectively.) For there to
be a point to constructing an MEMT-style sys-
tem where the reordered translation is the de-
fault translation and the non-reordered trans-
lation the fallback, it is necessary for the non-
reordered version to be better a reasonable
proportion of the time, allowing scope for a
Bleu improvement across the system.
To determine if this is the case, we construct
an approximate oracle to choose the better
of each pair of reordered and non-reordered
translation sentences. While Bleu is a rea-
sonable choice for evaluating the quality of the
overall composite set of translation sentences,
it is not suitable for sentence-level decisions.
However, in line with Nomoto (2003)?s moti-
vation for developing m-precision as an alter-
native to Bleu, we make the following obser-
vation.
The Bleu score (ignoring brevity) is an har-
monic mean between the different n-gram com-
ponents:
exp(
?
N
n=1
log p
n
)
Here p
n
is the precision for the different n-gram
overlap counts of a candidate sentence with a
gold standard sentence. If we want to glob-
ally optimise this score for an optimal Bleu
document score, we need to pick for each sen-
tence the n-gram counts that contribute most
to the overall score. For example, if we have
to pick between sentence A and sentence B,
where A has 2 unigram counts and 1 bigram
count, and B has 2 unigram counts only, A is
clearly preferred; however, for sentences C and
D, where C has 4 unigram counts and D has 2
unigram counts and 1 bigram count, we do not
know which eventually will lead to the global
maximum Bleu.
However we observe that because it is an
harmonic mean, small values are weighted ex-
ponentially heavier, due to the log operator.
Our heuristic to achieve the highest score is to
have the most extreme possible small values.
Since we know that an n-gram is always less
frequent than an (n? 1)-gram we concentrate
on the higher n-grams first. The decision pro-
cess between sentences is therefore to choose
the candidate with higher n-gram counts for
the maximum value of n, then n ? 1-gram
counts, and so on down to unigrams.
Here we will work with the Dutch?English
data used by Zwarts and Dras (2007). We use
the portions of the Europarl corpus (Koehn,
2003) that were used for training in that work;
and Bleu with 1 reference with n-grams up
to length 4. We then use our heuristic to se-
lect between the reordered and non-reordered
1155
Not-Bleu comparable
Identical 179,327
Undecidable 119,725
Total 299,052
Bleu comparable
Non-Reordered better 128,585
Reordered better 163,172
Total 291,757
Overall Total 590,809
Table 1: Comparing translation quality
Learner Baseline Accuracy
English ? Dutch
SVM - Polynomial 56.0% 56.6%
SVM - Polynomial 50.0% 51.2%
Maximum Entropy 50.0% 51.0%
Dutch ? English
SVM - Polynomial 50.0% 51.4%
Table 2: Results for internal language decider
translation candidates of Zwarts and Dras
(2007) for the language direction Dutch to En-
glish. Selecting the reordered translation as
default and backing off leads to a 1.1 Bleu
percentage point improvement over the 1.0 al-
ready mentioned. Results for English to Dutch
are similar.
In Table 1 we see the breakdown of the en-
tire corpus we work with. Some sentences are
identical, and some are different but with no
indication by our heuristic as to which of the
two is better. In the cases where we do have
an indication we see a sizeable 44% of the non-
reordered translations are better.
4 Internal Indicators
Before looking at our syntax-related ap-
proaches, it would be useful to have a com-
parison based on the approaches of previous
work. As noted in Section 2, these generally
use language models and alignment models, as
usual to estimate fluency and fidelity of candi-
date translations.
Because our two candidate solutions are
both ultimately produced by Pharaoh (Koehn,
2004), our quick-and-dirty solution can use
Pharaoh?s own final translation probabili-
ties, which capture language and alignment
model information. We build a classifier
Learner Baseline Accuracy
English ? Dutch
SVM - Polynomial 50.0% 50.1%
SVM - Radial 50.0% 49.7%
Maximum Entropy 50.0% 50.2%
Table 3: Results for Source language decider
that attempts to distinguish the better of a
pair of reordered and non-reordered transla-
tions. Denoting the non-reordered transla-
tion T
n
, and the reordered T
r
, we take as fea-
tures log(P (T
n
)), log(P (T
r
)), and log(P (T
n
))-
log(P (T
r
)). In addition, because the sentences
do not always have equal length and we do
not want to penalise longer sentences, we also
have three features describing the perplex-
ity: e
log(P (T
n
))/length(T
n
)
, e
log(P (T
r
))/length(T
r
)
,
and the difference between these two. Here
length is the function returning the length of
a sentence in tokens. Our training data we
get by partitioning the sentences according to
whether reordering is beneficial as measured by
our heuristic from Section 3. As machine learn-
ers we used SVM-light
1
(Joachims, 1998) and
the MaxEnt decider from the Stanford Classi-
fier
2
(Manning and Klein, 2003).
Table 2 shows the results the classifier pro-
duces on this data set. While the accuracy
rates for the classifiers are all statistically sig-
nificantly different (at a 95% confidence level)
from the baseline (using a standard test of pro-
portions), the results are not promising.
5 Source Language Indicators
5.1 All Data
The finding that almost half of the reordered
translations degrade the actual translation
quality raises the question of why. Our ini-
tial hypothesis is that because we use more
linguistic tools, this is likely to introduce new
errors. We hypothesise that one of the prob-
lems of reordering is either the parser getting
it wrong, or the rules getting it wrong because
of parse complexity. Our idea for estimating
the wrongness of a parse, or the complexity of
a parse that might lead to incorrect reordering
rule application, is to use ?side-effect? informa-
1
http://svmlight.joachims.org
2
http://nlp.stanford.edu/software/classifier.shtml
1156
Top Correct Accuracy
10 5 50%
50 23 46%
100 48 48%
200 100 50%
500 240 48%
1000 490 49%
Table 4: Accuracy range for Source Side Ex-
treme Predictions
tion from multiple parsers, in a modification of
an idea taken from Mutton et al (2007).
3
For
example, the parser of Collins (1999), in addi-
tion to the actual parse, gives a probability for
the most likely parse; if this most likely parse is
not at all likely, this may be because the parser
is having difficulty. The Link Parser (Grinberg
et al, 1995) produces dependency-style parses,
and gives an unlinked fragment count where a
complete parse cannot be made; this unlinked
fragment count may be indicative of parse dif-
ficulty. For this part, we therefore look only
at translations with English as source side and
Dutch as target, in order to be able to use mul-
tiple parsers on the source side sentences.
Again, we construct a machine learner to
predict which is the better of the reordered and
non-reordered translations. Our training data
is as in Section 4.
As a feature set we use: character and to-
ken length of the sentence, probability values
as supplied by the Collins parser, and the un-
linked fragment count as supplied by the Link
Parser. We used machine learners as in Sec-
tion 4. Both the SVM and the features are
similar to Mutton et al (2007).
The results are calculated on 39k examples,
split 30k training, 9k testing. Table 3 shows
the results for different learning techniques
with different settings. The accuracy scores
show selection no different from random: none
of the differences are statistically significant.
With such poor results, we do not bother to
calculate the Bleu effect of using the classifier
as a decider here.
3
Similar work is that of Albrecht and Hwa (2007);
however this requires human references unavailable
here.
Learner Baseline Accuracy
Dutch ? English
SVM - Polynomial 50.0% 52.3%
Maximum Entropy 50.0% 52.9%
Table 5: Results for target language decider
5.2 Thresholding
Because our MEMT uses the non-reordered
translations as a back-off, even if the classifier
is not accurate over the whole set of sentences,
it could still be useful to identify the poor-
est reordered translations and back off only in
those cases. SVM-light gives prediction scores
as part of its classification; data points that are
firmly within the positive (negative) classifica-
tion spaces are higher positive (negative) val-
ues, while border-line cases have a value very
close to 0. Here we interpret these as an es-
timate of the magnitude of the difference in
quality between reordered and non-reordered
translations. We calculated the accuracy over
the n most extreme predictions for different
values of n. The results in Table 4 show that
the ?extreme range? does not have a higher ac-
curacy either.
6 Target Language Indicators
6.1 All Data
We now consider our second approach, trying
to classify syntactic abnormality of the trans-
lations. Inspecting the sentences by hand,
we found that there are some sentences with
markedly poor grammaticality, even by the
standards of MT output. Examples of of-
ten reoccurring problems include verb posi-
tioning (often still sentence-final), positioning
of modals in the sentence, etc. Most are in the
realm of problems the reordering rules actually
try to target.
Here we use the multiple-parser approach in
a way more like that of Mutton et al (2007),
as an estimate of the fluency of the sentence
with a focus on syntactic characteristics. As in
Section 5, we construct a classifier using mul-
tiple parser outputs to distinguish the better
of a pair of reordered and non-reordered trans-
lations. Similarly, we use as features the most
likely parse probability of the Collins parser
(Collins, 1999) and unlinked fragment count
1157
Learner Baseline Accuracy
SVM - Complete 50.0% 52.3%
SVM - LargeDiff 50.0% 52.9%
SVM - HugeDiff 50.0% 51.2%
Table 6: Varying Bleu training data
from the Link parser (Grinberg et al, 1995).
We combine these with the sentences lengths
in both character count and token count of the
two candidate sentences.
Our translation direction in this section,
Dutch to English, is the opposite of Section 5,
for the same reason that we want to use multi-
ple parsers on the target side. The reordering
on the Dutch language is done on the results of
the Alpino (Bouma et al, 2000) parser. The
rules for reordering are found in Zwarts and
Dras (2006). Our training data is again as in
Section 4.
Table 5 shows the accuracy, calculated on
a 38k examples, split 30k training, 8k testing.
The accuracy again is close to baseline perfor-
mance, although it is clearly better than our
LM and alignment classifier of Section 4. Here
all the improvements are statistically signifi-
cant on a 95% confidence level. This is sur-
prising as Mutton et al (2007) on a somewhat
similar task was much more successful. Their
performance is expressed as a correlation with
human judgement rather than accuracy, but
compared to our performance where the im-
provement in accuracy is only a couple of times
the standard error, their approach performed
much better. A possible explanation could be
that the data we work on has much subtler dif-
ferences than their work. We know both trans-
lations are ultimately generated from the same
input, which makes our both candidates very
close.
6.2 Varying Training Data
In particular in (Mutton et al, 2007) the train-
ing data used human sentences as positive ex-
emplars and very simple bigram-generated sen-
tences as negative ones, so that there was a big
difference in quality between them. So per-
haps there are too many borderline cases in
the training data here.
Therefore we retrained the classifier of Sec-
tion 6.1, selecting only those sentence pairs
Top Correct Accuracy
10 9 90%
20 19 95%
50 40 80%
100 79 79%
200 145 72.5%
500 300 66.6%
1000 538 53.8%
Table 7: Accuracy of Prediction in the extreme
range
where the difference was more distinct. For
the LargeDiff set the difference was at least 4
or more unigrams or 3 or more bigrams; for
the HugeDiff set the difference was at least 6
or more unigrams or 5 or more bigrams.
Table 6 shows the results; all accuracy scores
are better than the baseline with 95% confi-
dence. For LargeDiff, there is an improvement
over using the complete data set. Surprisingly,
for the HugeDiff training data the gain is not
only gone, but this decider performs statis-
tically significantly worse than using all the
data.
We therefore conclude that the nature of
mistakes made when using reordering as a
preprocessing step is of a very subtle kind.
Very big mistakes are made as part of trans-
lation process completely independent of re-
ordering, while the improvement due to re-
ordering is only where subtly a small set of
words, compared to the reference, has been
changed for the better. The training size how-
ever is only reduced to three quarters of the
complete training size. It is therefore very un-
likely this sudden drop in performance is due
to data sparsity.
6.3 Thresholding
As in Section 5.2, we look at the cases where
our SVM gives a higher prediction score that
indicates a greater difference in quality of the
non-reordered translation over the reordered
one. Here we use as training data the LargeDiff
set from Section 6.2.
Results are in Table 7, which unlike the
thresholded results of Section 5.2 are quite
promising. There is a clear pattern here, with
very high accuracy scores in the top range,
slowly dropping to around overall performance
1158
System Bleu
Baseline 0.208
Reordered 0.221
SVM-pick 0.238
Table 8: Bleu results for the different selec-
tions
Features Accuracy
SVM - all 52.3%
SVM - length only 49.8%
SVM - length and Link 50.5%
SVM - length and Collins 50.1%
Table 9: Contribution of Parsers
after 1000 samples. This 1000 mark is out of
3461 negative samples in the test set range,
roughly marking the first third mark before
accuracy scores have reached average perfor-
mance.
Predictions with an extreme score on the
other side of the scale hardly show an improve-
ment. Because this subset of sentences shows
a higher accuracy, it is worthwhile to calculate
Bleu scores over the sentences in the test set
belonging to the top 500 SVM-predictions pos-
itive (reordered translation is better) and the
500 SVM predictions negative (non-reordered
is better). Table 8 shows the improvement of
Bleu scores.
4
The first interesting thing which can be seen
in the table is that this subset of sentences
already has higher improvement than is seen
in the whole data set simply by choosing the
reordered only, because the SVM is already
used to pick the most discriminating sentences.
We note that on this subset of sentences our
technique of picking the right sentence actu-
ally scores an improvement equal to the use of
reordering by itself.
6.4 Parser Contribution
In Table 9 we show the effects of individual
parsers, taking as the starting point the SVM
of Table 5. Clearly, combining parsers leads to
a much better decider.
Learner Baseline Accuracy
Dutch ? English
SVM Polynomial 50.0% 60.5%
Table 10: Combining internal features with
target side features
Top Reordering Non-reordered
10 9 90% 10 100%
20 18 90% 18 90%
50 33 66% 43 86%
100 61 61% 77 77%
200 114 57% 148 74%
500 289 58% 383 76%
1000 564 56% 748 75%
Table 11: Accuracy of the Combined model
6.5 Combining Models
As the results of classifying translation outputs
using features derived from multiple parsers
are promising, we next look at whether it
is useful to combine this information with
the language and alignment model information
from Section 4. Remarkably, as can be seen in
Table 10, the combination of these two fea-
tures has a much greater effect that the two
features sets individually. Comparing these
scores against 80% accuracy achieved in dis-
tinguish MT output from human output in the
work of Corston-Oliver et al (2001), this 60%
on a dataset with much more subtle differences
is quite promising.
Furthermore Table 11 shows the accuracy
ranking of the SVM for the combining model
for the extreme SVM-predictions, similar to
Tables 4 and 7. The last column of Table 11
matches previous tables, but now we also show
an improvement in correct prediction for the
reordered cases.
7 Conclusion
In this paper we have looked at a restricted
MEMT scenario, where we choose between
a syntactic-reordering-as-preprocessing trans-
lation candidate, in the style of (Collins et
al., 2005), and a baseline PSMT candidate.
We have shown that using a classifier built
around outputs of multiple parsers, to decide
4
Baseline here is the same baseline from Zwarts and
Dras (2007), which is the parser read-off of the tree.
1159
whether to back off to the baseline candidate,
can be successful in selecting the right candi-
date. There is no indication that classifying
information on the source side?looking to see
whether sentences with difficult or incorrect
parses could lead to bad reorderings and hence
bad translations?is useful; however, applying
such a classifier to the target side?looking to
see whether the output quality could be mea-
sured in a syntactically informed way, look-
ing for syntactic abnormalities?is successful
in detecting particularly bad translation can-
didates, and leads to an improvement in Bleu
score over the reordered translations equal to
the improvement gained by the reordering ap-
proach over the baseline. Multiple parsers
clearly improve the results over single parsers.
The target-side classifier can also be usefully
combined with language and alignment model
features, improving its accuracy substantially;
continuing with such an approach looks like a
promising direction. As a further step, the re-
sults are sufficiently positive to extend to other
sorts of syntactically informed SMT.
References
Akiba, Yasuhrio, Taro Watanabe, and Eiichiro Sumita.
2002. Using Language and Translation Models to
Select the Best among Outputs from Multiple MT
systems. In Proc. of Coling, pages 8?14.
Albrecht, Joshua S. and Rebecca Hwa. 2007. Regres-
sion for Sentence-Level MT Evaluation. In Proc. of
ACL, pages 296?303.
Bouma, Gosse, Gertjan van Noord, and Robert Mal-
ouf. 2000. Alpino: Wide Coverage Computational
Analysis of Dutch. In Computational Linguistics in
the Netherlands (CLIN).
Callison-Burch, Chris and Raymond S. Flournoy. 2001.
A Program for Automatically Selecting the Best
Output from Multiple Machine Translation Engines.
In Proc. MT Summit, pages 63?66.
Callison-Burch, Chris, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of Bleu in
Machine Translation Research. In Proc. of EACL,
pages 249?256.
Charniak, Eugene, Kevin Knight, and Kenju Yamada.
2003. Syntax-based Language Models for Statistical
Machine Translation. In Proc. of MT Summit, pages
40?46.
Collins, Michael, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540, Ann
Arbor, Michigan, June.
Collins, Michael. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Corston-Oliver, Simon, Michael Gamon, and Chris
Brockett. 2001. A machine learning approach to
the automatic evaluation of machine translation. In
Proc. of ACL, pages 148?155.
Eisele, Andreas. 2005. First steps towards multi-engine
machine translation. In Proc. of the ACL Workshop
on Building and Using Parallel Texts, pages 155?158.
Frederking, Robert and Sergei Nirenburg. 1994. Three
Heads are Better than One. In Proc. of the ACL
Conference on Applied Natural Language Processing,
pages 95 ? 100.
Grinberg, Dennis, John Lafferty, and Daniel Sleator.
1995. A robust parsing algorithm for link grammars.
In Proc. of the International Workshop on Parsing
Technologies.
Huang, Fei and Kishore Papineni. 2007. Hierarchical
System Combination for Machine Translation. In
Proc. of EMNLP, pages 277?286.
Joachims, T. 1998. Making large-scale support vec-
tor machine learning practical. In B. Scho?lkopf,
C. Burges, A. Smola, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
Koehn, Philipp. 2003. Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation Philipp
Koehn, Draft, Unpublished.
Koehn, Philipp. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proc. of AMTA, pages 115?124.
Manning, Christopher and Dan Klein. 2003. Optimiza-
tion, Maxent Models, and Conditional Estimation
without Magic. Tutorial at HLT-NAACL 2003 and
ACL 2003.
Mutton, Andrew, Mark Dras, Stephan Wan, and
Robert Dale. 2007. Gleu: Automatic evaluation of
sentence-level fluency. In Proc of ACL, pages 344?
351.
Nomoto, Tadashi. 2003. Predictive Models of Per-
formance in Multi-Engine Machine Translation. In
Proc. of MT Summit, pages 269?276.
Nomoto, Tadashi. 2004. Multi-Engine Machine Trans-
lation with Voted Language Model. In Proc. of ACL,
pages 494?501.
Och, Franz Josef, Daniel Gildea, Sanjeev Khundanpur,
Anoop Sarkar, Kenju Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jainand Zhen Jin, and Dragomir Radev. 2003.
Final report of Johns Hopkins 2003 summer work-
shop on syntax for statistial machine translation.
Riezler, Stefan and John Maxwell, III. 2006. Gram-
matical machine translation. In Proc of NAACL,
pages 248?255.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and B Bonnie
J. Dorr. 2007. Combining Outputs from Multiple
Machine Translation Systems? in Human Language.
In Proc. of NAACL, pages 228?235.
Wang, Chao, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for Statistical
Machine Translation. In Proc of EMNLP, pages
737?745.
Zwarts, Simon and Mark Dras. 2006. This Phrase-
Based SMT System is Out of Order: Generalised
Word Reordering in Machine Translation. In Proc.
of the Australasian Language Technology Workshop,
pages 149?156.
Zwarts, Simon and Mark Dras. 2007. Syntax-Based
Word Reordering in Phrase-Based Statistical Ma-
chine Translation: Why Does it Work? In Proc.
of MT Summit, pages 559?566.
1160
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 543?552,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Seed and Grow: Augmenting Statistically Generated Summary Sentences
using Schematic Word Patterns
Stephen Wan?? Robert Dale? Mark Dras?
?Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris?
?ICT Centre
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
We examine the problem of content selection
in statistical novel sentence generation. Our
approach models the processes performed by
professional editors when incorporating ma-
terial from additional sentences to support
some initially chosen key summary sentence,
a process we refer to as Sentence Augmen-
tation. We propose and evaluate a method
called ?Seed and Grow? for selecting such
auxiliary information. Additionally, we argue
that this can be performed using schemata, as
represented by word-pair co-occurrences, and
demonstrate its use in statistical summary sen-
tence generation. Evaluation results are sup-
portive, indicating that a schemata model sig-
nificantly improves over the baseline.
1 Introduction
In the context of automatic text summarisation, we
examine the problem of statistical novel sentence
generation, with the aim of moving from the current
state-of-the-art of sentence extraction to abstract-
like summaries. In particular, we focus on the task of
selecting content to include within a generated sen-
tence.
Our approach to novel sentence generation is to
model the processes underlying summarisation as
performed by professional editors and abstractors.
An example of the target output of this kind of gen-
eration is presented in Figure 1. In this example, the
human authored summary sentence was taken verba-
tim from the executive summary of a United Nations
proposal for the provision of aid addressing a partic-
ular humanitarian crisis. Such documents typically
exceed a hundred pages.
Human-Authored Summary Sentence:
Repeated [poor seasonal rains]1 [in 2004]2, culminating
in [food insecurity]3, indicate [another year]4 of crisis,
the scale of which is larger than last year?s and is further
[exacerbated by diminishing coping assets]5 [in both
rural and urban areas]6.
Key Source Sentence:
The consequences of [another year]4 of [poor rains]1 on
[food security]3 are severe.
Auxiliary Source Sentence(s):
However in addition to the needs of economic recovery
activities for IDPs, [food insecurity]3 [over the major-
ity of 2004]2 [has created great stress]5 on the poorest
families in the country, [both within the urban and rural
settings]6.
Figure 1: Alignment of a summary sentence to sentences
in the full document. Phrases of similar meaning are co-
indexed.
To write such summaries, we assume that the hu-
man abstractor begins by choosing key sentences
from the full document. Then, for each key sen-
tence, a set of auxiliary material is identified. The
key sentence is revised incorporating these auxil-
iary sentences to produce the eventual summary sen-
tence.
To study this phenomenon, a corpus of UN docu-
ments was collected and analysed.1 Each document
was divided into two parts comprising its executive
summary, and the remainder, referred to here as the
source. We manually aligned each executive sum-
mary sentence with one or more sentences from the
source, by choosing a key sentence that provided
1This corpus is described in detail in Section 5.1.
543
evidence for the content of the summary sentence
along with additional sentences that provided sup-
porting material.
We refer to the resulting corpus as the UN Con-
solidated Appeals Process (UN CAP) corpus. It is
a collection of sentence alignments, each referred to
as an aligned sentence tuple, which consists of:
1. A human authored summary sentence from the
executive summary;
2. A key sentence from the source;
3. Zero or more auxiliary sentences from the
source.
The key and any auxiliary sentences are referred to
collectively as the aligned source sentences.
We argue that some process that combines infor-
mation from multiple sentences is required if we are
to generate summary sentences similar to that por-
trayed in Figure 1. This is supported by our analysis
of the UN CAP corpus. Of the 580 aligned sentence
tuples, the majority, 61% of cases, appear to be ex-
amples of such a process.
Furthermore, the auxiliary sentences are clearly
necessary. We found that only 30% of the open-class
words in the summary are found in the key sentence.
If one selects all the open-class words from aligned
source sentences, recall increases to an upper limit
of 45% without yet accounting for stemming. This
upper bound is consistent with the upper limit of
50% found by Daume? III and Marcu (2005) which
takes into account stemming differences.
This demonstrates that the auxiliary material is
a valuable source of content which should be inte-
grated into the summary sentence, allowing an im-
provement in recall of up to 15% prior to account-
ing for morphological, synonym and paraphrase dif-
ferences. Of course, the trick is to improve recall
without hurting precision. A naive addition of all
words in the aligned source sentences incurs a drop
in precision from 30% to 23%. The problem thus is
one of selecting the relevant auxiliary content words
without introducing unimportant content. We refer
to this problem of incorporating material from aux-
iliary sentences to supplement a key sentence as Sen-
tence Augmentation.
In this paper, sentence augmentation is modelled
as a noisy channel process and has two facets: con-
tent selection and language modelling. This paper
focuses on the former, in which the system must
rank text segments?in this case, words?for inclu-
sion in the generated sentence. Given a ranked se-
lection of words, a language model would then order
them appropriately, as described in work on sentence
regeneration (for example, see Soricut and Marcu
(2005); Wan et al (2005)).
Provided with an aligned sentence tuple, the prob-
lem lies in effectively selecting words from the aux-
iliary sentences to bolster those taken from the key
sentence. Given that there are on average 2.7 aux-
iliary sentences per aligned sentence tuple, this ad-
ditional influx of words poses a considerable chal-
lenge.
We begin with the premise that, for documents
of a homogeneous type (in this case, the genre is
a funding proposal, and the domain is humanitarian
aid), it may be possible to identify patterns in the or-
ganisation of information in summaries. For exam-
ple, Figure 2 presents three summary sentences from
our corpus that share the same patterned juxtapo-
sition of two concepts DisplacedPersons and Host-
ingCommunities. Documents may exhibit common
patterns since they have a similar goal: namely, to
convince donors to give financial support. In the
above example, the juxtaposition highlights the fact
that those in need are not just those people from the
?epicenter? of the crisis but also those that look after
them.
We propose and evaluate a method called ?Seed
and Grow? for selecting content from auxiliary sen-
tences. That is, we first select the core meaning of
the summary, given here by the key sentence, and
then we find those pieces of additional information
that are conventionally juxtaposed with it.
Such patterns are reminiscent of Schemata, the or-
ganisations of propositional content introduced by
McKeown (1985). Schemata typically involve a
symbolic representation of each proposition?s se-
mantics. However, in our case, a text-to-text gener-
ation scenario, we are without such representations
and so must find other means to encode these pat-
terns.
To alleviate the situation, we turn to word-pair co-
occurrences to approximate schematic patterns. Fig-
544
Sentence 1:
The increased number of [internally displaced persons]1
and the continued presence of refugees have fur-
ther strained the scarce natural resources of [host
communities]2, stretching their capacity to the limit.
Sentence 2:
100,000 people, a significant portion of the population,
remain [displaced]1, burdening the already precarious
living conditions of [host families]2 in Dili and the
Districts.
Sentence 3:
The current humanitarian situation in Timor-Leste is
characterised by: An estimated [100,000 displaced
people]1 (10% of the population) living in camps and
with [host families]2 in the districts; A total or partial de-
struction of over 3,000 homes in Dili affecting at least
14,000 IDPs
Figure 2: Examples of the pattern ?DisplacedPersons[1],
HostingCommunities[2]?.
ure 2 showed that mentions of the plight of interna-
tionally displaced persons are often followed by de-
scriptions of the impact on the host communities that
look after them. In this particular example, this is
realised lexically in the co-occurrences of the words
displaced and host.
Corpus-based methods inspired by the notion of
schemata have been explored in the past by Lap-
ata (2003) and Barzilay and Lee (2004) for order-
ing sentences extracted in a multi-document sum-
marisation application. However, to our knowledge,
using word co-occurrences in this manner to repre-
sent schematic knowledge for the purposes of select-
ing content in a statistically-generated summary sen-
tence has not previously been explored.
This paper seeks to determine whether or not such
patterns exist in homogeneous data; and further-
more, whether such patterns can be used to better
select words from auxiliary sentences. In particular,
we propose the ?Seed and Grow? approach for this
task. The results show that even simple modelling
approaches are able to model this schematic infor-
mation.
In the remainder of this paper, we contrast our ap-
proach to related text-to-text research in Section 2.
The Content Selection model is presented in Section
3. Section 4 describes how a binary classification
model is used in a statistical text generation system.
Section 5 describes our evaluation of the model for a
summary generation task. We conclude, in Section
6, that domain-specific schematic patterns can be ac-
quired and applied to content selection for statistical
sentence generation.
2 Related Work
2.1 Content Selection in Text-to-Text Systems
Statistical text-to-text summarisation applications
have borrowed much from the related field of statis-
tical machine translation. In one of the first works to
present summarisation as a noisy channel approach,
Witbrock and Mittal (1999) presented a conditional
model for learning the suitability of words from a
news article for inclusion in headlines, or ?ultra-
summaries?. Inspired by this approach, and with
the intention of designing a robust statistical gener-
ation system, our work is also based on the noisy
channel model. Into this, we incorporate our con-
tent selection model, which includes Witbrock and
Mittal?s model supplemented with schema-based in-
formation.
Roughly, text-to-text transformations fall into
three categories: those in which information is com-
pressed, conserved, and augmented. We use these
distinctions to organise this overview of the litera-
ture.
In Sentence Compression work, a single sentence
undergoes pruning to shorten its length. Previ-
ous approaches have focused on statistical syntactic
transformations (Knight and Marcu, 2002). For con-
tent selection, discourse-level considerations were
proposed by Daume? III and Marcu (2002), who ex-
plored the use of Rhetorical Structure Theory (Mann
and Thompson, 1988). More recently, Clarke and
Lapata (2007) use Centering Theory (Grosz et al,
1995) and Lexical Chains (Morris and Hirst, 1991)
to identify which information to prune. Our work is
similar in incorporating discourse-level phenomena
for content selection. However, we look at schema-
like information as opposed to chains of references
and focus on the sentence augmentation task.
The work of Barzilay and McKeown (2005) on
Sentence Fusion introduced the problem of convert-
ing multiple sentences into a single summary sen-
545
tence. Each sentence set ideally tightly clusters
around a single news event. Thus, there is one gen-
eral proposition to be realised in the summary sen-
tence, identified by finding the common elements in
the input sentences. We see this as an example of
conservation. In our work, this general proposition
is equivalent to the core information for the sum-
mary sentence before the incorporation of supple-
mentary material.
In contrast to both compression and conservation
work, we focus on augmenting the information in
a key sentence. The closest work is that of Jing
and McKeown (1999) and Daume? III and Marcu
(2005), in which multiple sentences are processed,
with fragments within them being recycled to gener-
ate the novel generated text.
In both works, recyclable fragments are identified
by automatic means. Jing and McKeown (1999) use
models that are based on ?copy-and-paste? opera-
tions learnt from the behaviour of human abstrac-
tors as found in a corpus. Daume? III and Marcu
(2005) propose a model that encodes how likely it
is that different sized spans of text are skipped to
reach words and phrases to recycle.
While similar in task, our models differ substan-
tially in the nature of the phenomenon modelled. In
this work, we focus on content-based considerations
that model which words can be combined to build
up a new sentence.
2.2 Schemata and Text Generation
There exists related work from Natural Language
Generation (NLG) in finding material to build up
sentences. As mentioned above, our content selec-
tion model is inspired by work on schemata from
NLG (McKeown, 1985). Barzilay and Lee (2004)
showed that it is possible to obtain schema-like
knowledge automatically from a corpus for the pur-
poses of extracting sentences and ordering them.
However, their work represents patterns at the sen-
tence level, and is thus not directly comparable to
our work, given our focus on sentence generation.
In our system, what is required is a means to rank
words for use in generation. Thus, we focus on com-
monly occurring word co-occurrences, with the aim
of encoding conventions in the texts we are trying to
generate. In this respect, this is similar to work by
Lapata (2003), who builds a conditional model of
words across adjacent sentences, focusing on words
in particular semantic roles. Like Barzilay and Lee
(2004), this model was used to order extracted sen-
tences in summaries. In contrast, our work focuses
on word patterns found within a summary sentence,
not between sentences. Additionally, our tasks dif-
fer as we examine the statistical sentence generation
instead of sentence ordering.
3 Linguistic Intuitions behind Word
Selection
The ?Seed and Grow? approach proposed in this pa-
per divides the word-level content selection prob-
lem into two underlying subproblems. We address
these with two separate models, called the salience
and schematic models. The salience model chooses
the key content for the summary sentence while the
schematic model attempts to identify what else is
typically mentioned given those salient pieces of in-
formation.
3.1 A Salience Model: Learning ?Buzzwords?
There are a variety of methods for determining the
salient information in a text, and these underpin
most work in automatic text summarisation. As an
example of a salience model trained on corpus data,
Witbrock and Mittal (1999) introduced a method for
scoring summary words for inclusion within news
headlines. In their model, headlines were treated as
?ultra-summaries?. Their model learns which words
are typically used in headlines and encodes, at least
to some degree, which words are attention grabbing.
In the domain of funding proposals, key words
that grab attention may amount to domain-specific
buzzwords. Intuitively, a reader, perhaps someone
in charge of allocating donations, tends to look for
certain types of key information matching donation
criteria, and so human abstract authors will target
their summaries for this purpose.
We thus adapt the Witbrock and Mittal (1999)
model to identify such domain specific buzzwords
(BWM, for ?buzzword model?). For an aligned sen-
tence tuple, the probability that a word is selected
based on the salience of a word with respect to the
domain is defined as:
probbwm(select = 1|w) =
|summaryw|
|sourcew|
(1)
546
where summaryw is the set of aligned sentence tu-
ples that contain the word w in the summary sen-
tence and in the source sentences. The denomina-
tor, sourcew, is the set of aligned sentence tuples that
have the word w in either the key or an auxiliary sen-
tence.
As is implicit in this equation, we could just use
this buzzword model to select content not only from
the key sentence, but from the auxiliary sentences
as well. While it is intended ultimately to find the
key content of the summary, it can also serve as an
alternative baseline for auxiliary content selection to
compare against the ?Seed and Grow? model.
3.2 A Schema Model: Approximation via
Word co-Occurrences
To restate the problem at hand: the task is one
of finding elements of secondary importance that
schematically elaborate on the key information. We
do this by examining sample summary sentences for
conventional juxtapositions of concepts. As men-
tioned in Section 1, schemata are approximated here
with patterns of word-pair co-occurrences. Using a
corpus of human-authored summaries in the domain
of our application, it is thus possible to learn what
those common combinations of words are.
Roughly, the process is as follows. To begin with,
a seed set of words is chosen. The purpose of the
seed set is to represent the core proposition of the
summary sentence.
In this work, this core proposition is given by the
key sentence and so the non-stopwords belonging to
it are used to populate the seed set. In the ?Seed and
Grow? approach, we check to see which words from
auxiliary sentences pair well with words in the seed
set.
3.2.1 Collecting Word-level Patterns
Each training case in the corpus contains a single
human-authored summary sentence that can be used
to learn which pairs of words conventionally occur
in a summary. For each summary sentence, stop-
words are removed. Then, each pairing of words in
the sentence is used to update a pair-wise word co-
occurrence frequency table. When looking up and
storing a frequency, the order of words is ignored.
3.2.2 Scoring Word-Pair Co-occurrence
Strength
For any two words, w1 from the seed set and w2 from
an auxiliary sentence, the word-pair co-occurrence
probability is defined as follows:
probco-oc(w1,w2)
= freq(w1,w2)
freq(w1)+ freq(w2)? freq(w1,w2)
(2)
where f req(w1,w2) is a lookup in the word-pair co-
occurrence frequency table. This table stores co-
occurrence word pairs occurring in the summary
sentence.
3.2.3 Combining a Set of Co-occurrence Scores
Each auxiliary word now has a series of scores,
one for each comparison with a seed word. To rank
each auxiliary word, these need to be combined into
a single score for sorting.
When combining the set of co-occurrence scores,
one might want to account for the fact that each pair-
ing of a seed word with an auxiliary word might
not contribute equally to the overall selection of that
auxiliary word. Intuitively, a word in the seed set,
derived from the key sentence, may only make a
minor contribution to the core meaning of the sum-
mary sentence. For example, words that are part of
an adjunct phrase in the key sentence might not be
good candidates to elaborate upon. Thus, one might
want to weight these seed words lower, to reduce
their influence on triggering schematically associ-
ated words.
To allow for this, a seed weight vector is main-
tained, storing a weight per seed word. Different
weighting schemes are possible. For example, a
scheme might indicate the salience of a word. In
addition to the buzzword model (BWM) described
earlier, one might employ a standard vector space
approach (Salton and McGill, 1983) from Informa-
tion Retrieval, which uses term frequency scores
weighted with an inverse document frequency fac-
tor, or tf-idf. We also implement the case in which all
seed words are treated equally using binary weights,
where 1 indicates the presence of a seed word, and
0 indicates its absence. In the evaluations described
in Section 5, we refer to these three seed weighting
schemes as bwm and tf-idf, and binary respectively.
547
To find the probability of selecting an auxiliary
word using the schematic word-pair co-occurrence
model (WCM), an averaged probability is found
by normalising the sum of the weighted probabili-
ties, where weights are provided by one of the three
schemes above:
probwcm(wi) =
1
Z
?
|seed|
?
k=0
weightsk ?probco-oc(wi,wk) (3)
where seed is the set of seed words and wk is the kth
word in that set. The vector, weights, stores the seed
weights. The normalisation factor for the weighted
average, Z, is the number of auxiliary words.
Finally, since the WCM model only serves to se-
lect words from the auxiliary sentences, words from
the key sentence must be given scores as well. For
these words, the scoring is as follows:
probwcm(w) =
1
Z
(
1
|seed| + probwcm(w)
)
(4)
where Z is a normalisation across the set of seed
words.
4 Combining Buzzwords and Word-Pair
Co-Occurrence Models for Generation
As mentioned above, the noisy channel approach
is used for producing the augmented sentence. Al-
though the focus of this paper is on Content Selec-
tion, an overview of the end-to-end generation pro-
cess is presented for completeness.
Sentence augmentation is essentially a text-to-text
process: A key sentence and auxiliary material are
transformed into a single summary sentence. Fol-
lowing Witbrock and Mittal (1999), the task is to
search for the string of words that maximises the
probability prob(summary|source). Standardly re-
formulating this probability using Bayes? rule re-
sults in the following:
probcm(source|summary)?problm(summary) (5)
In this paper, we are concerned with the first
factor, probcm(source|summary), referred to as the
channel model (CM), which combines both the
buzzword (BWM) and word-pair co-occurrence
(WCM) models. An examination of differences be-
tween the two approaches revealed only a 20% word
overlap on the Jaccard metric.
In order to combine multiple models, we intend
to use machine learning approaches to combine the
information in each model in a similar manner to
Berger et al (1996). We are currently exploring the
use of logistic regression methods to learn a func-
tion that would treat, as features, the probabilities
defined by the salience and schematic content selec-
tion models. Although generation is possible using
each content selection model in isolation, evalua-
tions of the combined model are on-going and are
not presented in this paper.
5 Evaluation
In this evaluation, the task is to select n words from
the aligned source sentences for inclusion in a sum-
mary. As a gold-standard for comparison, we sim-
ply examine what words were actually chosen in the
summary sentence of the aligned sentence tuple. We
are specifically interested in open-class words, and
so a stopword list of closed-class words is used to
filter the sentences in each test case.
We evaluate against the set of open-class words
in the human-authored summary sentence using re-
call and precision metrics. Recall is the size of
the intersection of the selected and gold-standard
sets, normalised by the length of the gold-standard
sentence (in words). This recall metric is similar
to the ROUGE-1 metric, the unigram version of
the ROUGE metric (Lin and Hovy, 2003) used in
the Document Understanding Conferences2 (DUC).
Precision is the size of the intersection normalised
by the number of words selected. We also report the
F-measure, which is the harmonic mean of the recall
and precision scores.
Recall, precision and F-measure are measured at
various values of n ranging from 1 to the number of
open-class words in the gold-standard summary sen-
tence for a particular test case. For the purposes of
evaluation, differences in tokens due to morphology
were explored crudely via the use of Porter?s stem-
ming algorithm. However, the results from stem-
ming are not that different from exact token matches
when examining performance on the entire data set
2http://duc.nist.gov
548
Number of training cases 530
Average words in summary sentence 27.0
Average stopwords in summary sentence 10.3
Average number of auxiliary sentences 2.75
Word count: summary sentences 4630
Word count: source sentences 21356
Word type count in corpus 3800
Table 1: Statistics for the UN CAP training set
and so, for simplicity, these are omitted in this dis-
cussion.
5.1 The Data
The corpus is made up of a number of humanitar-
ian aid proposals called Consolidated Appeals Pro-
cess (UN CAP) documents, which are archived at
the United Nations website.3 135 documents from
the period 2002 to 2007 were downloaded by the au-
thors. A preprocessing stage extracted text from the
PDF files and segmented the documents into execu-
tive summary and source sections. These were then
automatically segmented further into sentences.
Executive summary sentences were manually
aligned by the authors to source key and auxiliary
sentences, producing a corpus of 580 aligned sen-
tence tuples referred to here as the UN CAP cor-
pus. Of these, 230 tuples were paraphrase cases (i.e.
without aligned auxiliary sentences). The remaining
550 cases were instances of sentence augmentation
(with at least one auxiliary sentence).
Of the 580 cases, 50 cases were set aside for test-
ing. The remaining 530 cases were used for train-
ing. Statistics for the training portion of the sentence
augmentation set are provided in Table 1.
In this paper, aligned sentence tuples are obtained
via manual annotation. Automatic construction
of these sentence-level alignments is possible and
has been explored by Jing and McKeown (1999).
We also envisage using tools for scoring sentence
similarity (for example, see Hatzivassiloglou et al
(2001)) for automatically constructing them; this is
the focus of work by Wan and Paris (2008).
3http://ochaonline3.un.org/humanitarianappeal/index.htm
5.2 The Baselines
Three baselines were used in this work: the random,
tf-idf and position baselines. A random word selec-
tor shows what performance might be achieved in
the absence of any linguistic knowledge.
We also sorted all words in the aligned source sen-
tences by their weighted tf-idf scores. This baseline
selects words in order until the desired word limit
is reached. This baseline is referred to as the tf-idf
baseline.
Finally, we selected words based on their sen-
tence order, choosing first those words from the key
sentence. When these are exhausted, auxiliary sen-
tences are sorted by their sentence positions in the
original document. Words from the first auxiliary
sentence are then chosen. This continues until ei-
ther the desired number of words have been chosen,
or no words remain. This baseline is known as the
position baseline.
5.3 Content Selection Results
We compare the three baselines to the two mod-
els presented in Section 3. These are the buzzword
salience model (BWM) and the schematic word-pair
co-occurrence model (WCM).
We begin by presenting recall, precision and F-
measure graphs when selecting from the aligned
source sentences, comprising the key and auxiliary
sentences. Figure 3 shows the results for the two
models against the three baselines. The two mod-
els, the positional, and the tf-idf baselines perform
better than the random baseline, as measured by a
two-tailed Wilcoxon Matched Pairs Signed Ranks
test (? = 0.05).
The WCM consistently out-performs the BWM
on all metrics, and the differences are statistically
significant. In fact, the BWM also generally per-
forms worse than the position and tf-idf baselines.
WCM and the position baseline both significantly
outperform the tf-idf baseline on all metrics for
longer sentence lengths.
That the position baseline and WCM should per-
form similarly is not really surprising since, in ef-
fect, the position baseline first chooses words from
the key sentence and then selects auxiliary words.
The difference essentially lies in how the auxiliary
words are chosen.
549
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  5  10  15  20  25  30
R
ec
al
l
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0  5  10  15  20  25  30
Pr
ec
is
io
n
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
WCM
BWM
Position
tf.idf
Random
Figure 3: Recall, Precision and F-measure performance
for open-class words from the entire input set (key and
auxiliary). Models presented are the Buzzword Model
(BWM), the Word-Pair Co-occurrence Model (WCM)
and position, tf-idf and random baselines.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
WCM
position
Figure 4: F-measure scores for content selection on just
the auxiliary sentences. Models presented are the Word-
Pair Co-occurrence model (WCM) and the position base-
line.
The results of Figure 3 weakly support the
hypothesis that using schematic word-pair co-
occurrences helps improve performance over mod-
els without discourse-related features. The graphs
show that WCM edges above the position base-
line when the number of selected open-class words
ranges from 10 to 15. Note that the average num-
ber of open-class words in a human authored sum-
mary sentence is 16. The only significant difference
found was in the F-measure and precision scores for
19 selected open-class words. Nevertheless, a gen-
eral trend can be observed in which WCM performs
better than the position baseline.
Ultimately, however, what we want to do is select
auxiliary content to supplement the key sentence.
To examine the effect of two best performing ap-
proaches, WCM and the position baseline, on this
task, were both modified so that the key sentence
words were explicitly given a zero probability. Thus,
the recall, precision and F-measure scores obtained
are based solely on the ability of either to select aux-
iliary words. The F-measure scores are presented
Figure 4. WCM consistently outperforms the po-
sition baseline for the selection of auxiliary words.
Differences are significant for 6 or more selected
open-class words.
The results show that even when considering only
exact token matches, we can improve on the re-
call of open-class words, and do so without penalty
in precision. Our working hypothesis is that such
gains are possible because the corpus has a homo-
550
geneous quality and key patterns are sufficiently re-
peated even when the overall data set is of the or-
der of hundreds of cases. The benefit of using a
model encoding some schematic information is fur-
ther shown by the performance of WCM over the
position baseline when selecting words from auxil-
iary sentences.
This is an interesting finding given that do-
main independent methods are increasingly used
on domain-specific corpora such as financial and
biomedical texts, for which we may have access to
only a limited amount of data. We anticipate that as
we introduce methods to account for paraphrase and
synonym differences, performance might rise fur-
ther still.
5.4 Testing Seed Weighting Schemes
We can also weight seed words in the ?Seed and
Grow? approach in a variety of ways. To test
whether weighting schemes have any effect on con-
tent selection performance, we examined the use
of three schemes. We were particularly interested
in those schemes that indicate the contribution of
a seed word to the core meaning of a sentence.
These are the binary, tf-idf and buzzword weight-
ing schemes described in Section 3. We present
the F-measure graph for these three variants of the
schematic word-pair co-occurrence model (WCM)
in Figure 5.
The graphs show that there is no discernible dif-
ference between the seed weighting schemes. No
scheme significantly outperforms another. Thus, we
conclude that the choice of these particular seed
weighting schemes has no effect on performance. In
future work, we intend to examine whether weight-
ing schemes encoding syntactic information might
fare better, since such information might more accu-
rately represent the contribution of a substring to the
main clause of the sentence.
6 Conclusions and Future Work
In this paper, we argued a case for sentence augmen-
tation, a component that facilitates abstract-like text
summarisation. We showed that such a process can
account for summary sentences as authored by pro-
fessional editors. We proposed the use of schemata,
as approximated with a word-pair co-occurrence
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0  5  10  15  20  25  30
Fm
ea
su
re
Number of Open-class Words Selected
binary
tfidf
BWM
Figure 5: F-measure performance for open-class words
from the entire input set (key and auxiliary). Models
presented are variants of the Word-Pair Co-occurrence
Model (WCM) that differ in the seed weighting schemes.
model, and advocated a new schema-based ?Seed
and Grow? content selection model used for statisti-
cal sentence generation.
We also showed that domain-specific patterns,
schematic word-pair co-occurrences in this case, can
be acquired from a limited amount of data as indi-
cated by modest performance gains for content se-
lection using schemata information. We postulate
that this is particularly true when dealing with ho-
mogeneous data.
In future work, we intend to explore other string
matches corresponding to variations due to para-
phrases and synonymy. We would also like to study
the effects of corpus size when learning schematic
patterns. Finally, we are currently investigating the
use of machine learning methods to combine the
best of the Salience and Schemata models in order
to provide a single model for use in decoding.
7 Acknowledgments
We would like to thank the reviewers for their in-
sightful comments. This work was funded by the
CSIRO ICT Centre and Centre for Language Tech-
nology at Macquarie University.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 113?120, Boston,
551
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?328.
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
1?11.
Hal Daume? III and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL ? 2002), pages 449 ? 456,
Philadelphia, PA, July 6 ? 12.
Hal Daume? III and Daniel Marcu. 2005. Induction
of word and phrase alignments for automatic doc-
ument summarization. Computational Linguistics,
31(4):505?530, December.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
V. Hatzivassiloglou, J. Klavans, M. Holcombe, R. Barzi-
lay, M. Kan, and K. McKeown. 2001. Simfinder: A
flexible clustering tool for summarization. pages 41?
49. Association for Computational Linguistics.
Hongyan Jing and Kathleen McKeown. 1999. The de-
composition of human-written summary sentences. In
Research and Development in Information Retrieval,
pages 129?136.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 545?552, Sapporo, Japan.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In NAACL ?03: Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 71?78, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Kathleen R McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21?48.
G. Salton and M. J. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
Radu Soricut and Daniel Marcu. 2005. Towards de-
veloping generation algorithms for text-to-text appli-
cations. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 66?74, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
Stephen Wan and Ce?cile Paris. 2008. In-browser sum-
marisation: Generating elaborative summaries biased
towards the reading context. In Proceedings of ACL-
08: HLT, Short Papers, pages 129?132, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Stephen Wan, Robert Dale Mark Dras, and Ce?cile Paris.
2005. Towards statistical paraphrase generation: pre-
liminary evaluations of grammaticality. In Proceed-
ings of The 3rd International Workshop on Paraphras-
ing (IWP2005), pages 88?95, Jeju Island, South Korea.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization (poster abstract): a statistical approach
to generating highly condensed non-extractive sum-
maries. In SIGIR ?99: Proceedings of the 22nd annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 315?316,
New York, NY, USA. ACM Press.
552
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 852?860,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Improving Grammaticality in Statistical Sentence Generation:
Introducing a Dependency Spanning Tree Algorithm with an Argument
Satisfaction Model
Stephen Wan?? Mark Dras? Robert Dale?
?Centre for Language Technology
Department of Computing
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris?
?ICT Centre
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
Abstract-like text summarisation requires
a means of producing novel summary sen-
tences. In order to improve the grammati-
cality of the generated sentence, we model
a global (sentence) level syntactic struc-
ture. We couch statistical sentence genera-
tion as a spanning tree problem in order to
search for the best dependency tree span-
ning a set of chosen words. We also intro-
duce a new search algorithm for this task
that models argument satisfaction to im-
prove the linguistic validity of the gener-
ated tree. We treat the allocation of modi-
fiers to heads as a weighted bipartite graph
matching (or assignment) problem, a well
studied problem in graph theory. Using
BLEU to measure performance on a string
regeneration task, we found an improve-
ment, illustrating the benefit of the span-
ning tree approach armed with an argu-
ment satisfaction model.
1 Introduction
Research in statistical novel sentence generation
has the potential to extend the current capabili-
ties of automatic text summarisation technology,
moving from sentence extraction to abstract-like
summarisation. In this paper, we describe a new
algorithm that improves upon the grammaticality
of statistically generated sentences, evaluated on a
string regeneration task, which was first proposed
as a surrogate for a grammaticality test by Ban-
galore et al (2000). In this task, a system must
regenerate the original sentence which has had its
word order scrambled.
As an evaluation task, string regeneration re-
flects the issues that challenge the sentence gen-
eration components of machine translation, para-
phrase generation, and summarisation systems
(Soricut and Marcu, 2005). Our research in sum-
marisation utilises the statistical generation algo-
rithms described in this paper to generate novel
summary sentences.
The goal of the string regeneration task is to re-
cover a sentence once its words have been ran-
domly ordered. Similarly, for a text-to-text gen-
eration scenario, the goal is to generate a sen-
tence given an unordered list of words, typically
using an n-gram language model to select the best
word ordering. N-gram language models appear
to do well at a local level when examining word
sequences smaller than n. However, beyond this
window size, the sequence is often ungrammati-
cal. This is not surprising as these methods are un-
able to model grammaticality at the sentence level,
unless the size of n is sufficiently large. In prac-
tice, the lack of sufficient training data means that
n is often smaller than the average sentence length.
Even if data exists, increasing the size of n corre-
sponds to a higher degree polynomial complexity
search for the best word sequence.
In response, we introduce an algorithm for
searching for the best word sequence in a way
that attempts to model grammaticality at the sen-
tence level. Mirroring the use of spanning tree al-
gorithms in parsing (McDonald et al, 2005), we
present an approach to statistical sentence genera-
tion. Given a set of scrambled words, the approach
searches for the most probable dependency tree, as
defined by some corpus, such that it contains each
word of the input set. The tree is then traversed to
obtain the final word ordering.
In particular, we present two spanning tree al-
gorithms. We first adapt the Chu-Liu-Edmonds
(CLE) algorithm (see Chu and Liu (1965) and Ed-
monds (1967)), used in McDonald et al (2005),
to include a basic argument model, added to keep
track of linear precedence between heads and
modifiers. While our adapted version of the CLE
algorithm finds an optimal spanning tree, this does
852
not always correspond with a linguistically valid
dependency tree, primarily because it does not at-
tempt to ensure that words in the tree have plausi-
ble numbers of arguments.
We propose an alternative dependency-
spanning tree algorithm which uses a more
fine-grained argument model representing argu-
ment positions. To find the best modifiers for
argument positions, we treat the attachment of
edges to the spanning tree as a weighted bipartite
graph matching problem (or the assignment
problem), a standard problem in graph theory.
The remainder of this paper is as follows. Sec-
tion 2 outlines the graph representation of the
spanning tree problem. We describe a standard
spanning tree algorithm in Section 3. Section 4 de-
fines a finer-grained argument model and presents
a new dependency spanning tree search algorithm.
We experiment to determine whether a global de-
pendency structure, as found by our algorithm,
improves performance on the string regeneration
problem, presenting results in Section 5. Related
work is presented in Section 6. Section 7 con-
cludes that an argument model improves the lin-
guistic plausibility of the generated trees, thus im-
proving grammaticality in text generation.
2 A Graph Representation of
Dependencies
In couching statistical generation as a spanning
tree problem, this work is the generation analog
of the parsing work by McDonald et al (2005).
Given a bag of words with no additional con-
straints, the aim is to produce a dependency tree
containing the given words. Informally, as all de-
pendency relations between each pair of words are
possible, the set of all possible dependencies can
be represented as a graph, as noted by McDon-
ald et al (2005). Our goal is to find the subset of
these edges corresponding to a tree with maximum
probability such that each vertex in the graph is
visited once, thus including each word once. The
resulting tree is a spanning tree, an acyclic graph
which spans all vertices. The best tree is the one
with an optimal overall score. We use negative log
probabilities so that edge weights will correspond
to costs. The overall score is the sum of the costs
of the edges in the spanning tree, which we want
to minimise. Hence, our problem is the minimum
spanning tree (MST) problem.
We define a directed graph (digraph) in a stan-
dard way, G = (V,E) where V is a set of vertices
and E ? {(u, v)|u, v ? V } is a set of directed
edges. For each sentence w = w1 . . . wn, we de-
fine the digraph Gw = (Vw, Ew) where Vw =
{w0, w1, . . . , wn}, with w0 a dummy root vertex,
and Ew = {(u, v)|u ? Vw, v ? Vw \ {w0}}.
The graph is fully connected (except for the root
vertex w0 which is only fully connected outwards)
and is a representation of possible dependencies.
For an edge (u, v), we refer to u as the head and v
as the modifier.
We extend the original formulation of McDon-
ald et al (2005) by adding a notion of argument
positions for a word, providing points to attach
modifiers. Adopting an approach similar to John-
son (2007), we look at the direction (left or right)
of the head with respect to the modifier; we con-
sequently define a set D = {l, r} to represent
this. Set D represents the linear precedence of the
words in the dependency relation; consequently,
it partially approximates the distinction between
syntactic roles like subject and object.
Each edge has a pair of associated weights, one
for each direction, defined by the function s :
E?D ? R, based on a probabilistic model of de-
pendency relations. To calculate the edge weights,
we adapt the definition of Collins (1996) to use di-
rection rather than relation type (represented in the
original as triples of non-terminals). Given a cor-
pus, for some edge e = (u, v) ? E and direction
d ? D, we calculate the edge weight as:
s((u, v), d) = ?log probdep(u, v, d) (1)
We define the set of part-of-speech (PoS) tags P
and a function pos : V ? P , which maps vertices
(representing words) to their PoS, to calculate the
probability of a dependency relation, defined as:
probdep(u, v, d)
= cnt((u, pos(u)), (v, pos(v)), d)
co-occurs((u, pos(u)), (v, pos(v))) (2)
where cnt((u, pos(u)), (v, pos(v)), d) is the num-
ber of times where (v, pos(v)) and (u, pos(u))
are seen in a sentence in the training data, and
(v, pos(v)) modifies (u, pos(u)) in direction d.
The function co-occurs((u, pos(u)), (v, pos(v)))
returns the number of times that (v, pos(v)) and
(u, pos(u)) are seen in a sentence in the training
data. We adopt the same smoothing strategy as
Collins (1996), which backs off to PoS for unseen
dependency events.
853
3 Generation via Spanning Trees
3.1 The Chu-Liu Edmonds Algorithm
Given the graph Gw = (Vw, Ew), the Chu-Liu
Edmonds (CLE) algorithm finds a rooted directed
spanning tree, specified by Tw, which is an acyclic
set of edges in Ew minimising
?
e?Tw,d?D s(e, d).
The algorithm is presented as Algorithm 1.1
There are two stages to the algorithm. The first
stage finds the best edge for each vertex, connect-
ing it to another vertex. To do so, all outgoing
edges of v, that is edges where v is a modifier, are
considered, and the one with the best edge weight
is chosen, where best is defined as the smallest
cost. This minimisation step is used to ensure that
each modifier has only one head.
If the chosen edges Tw produce a strongly con-
nected subgraph Gmw = (Vw, Tw), then this is the
MST. If not, a cycle amongst some subset of Vw
must be handled in the second stage. Essentially,
one edge in the cycle is removed to produce a sub-
tree. This is done by finding the best edge to join
some vertex in the cycle to the main tree. This has
the effect of finding an alternative head for some
word in the cycle. The edge to the original head
is discarded (to maintain one head per modifier),
turning the cycle into a subtree. When all cycles
have been handled, applying a greedy edge selec-
tion once more will then yield the MST.
3.2 Generating a Word Sequence
Once the tree has been generated, all that remains
is to obtain an ordering of words based upon it.
Because dependency relations in the tree are either
of leftward or rightward direction, it becomes rel-
atively trivial to order child vertices with respect
to a parent vertex. The only difficulty lies in find-
ing a relative ordering for the leftward (to the par-
ent) children, and similarly for the rightward (to
the parent) children.
We traverse Gmw using a greedy algorithm to or-
der the siblings using an n-gram language model.
Algorithm 2 describes the traversal in pseudo-
code. The generated sentence is obtained by call-
ing the algorithm with w0 and Tw as parameters.
The algorithm operates recursively if called on an
1Adapted from (McDonald et al, 2005) and
http://www.ce.rit.edu/? sjyeec/dmst.html . The dif-
ference concerns the direction of the edge and the edge
weight function. We have also folded the function ?contract?
in McDonald et al (2005) into the main algorithm. Again
following that work, we treat the function s as a data
structure permitting storage of updated edge weights.
/* initialisation */
Discard the edges exiting the w0 if any.1
/* Chu-Liu/Edmonds Algorithm */
begin2
Tw ? (u, v) ? E : ?v?V,d?Darg min
(u,v)
s((u, v), d)
3
if Mw = (Vw, Tw) has no cycles then return Mw4
forall C ? Tw : C is a cycle in Mw do5
(e, d)? arg min
e?,d?
s(e?, d?) : e ? C
6
forall c = (vh, vm, ) ? C and dc ? D do7
forall e? = (vi, vm) ? E and d? ? D do8
s(e?, d?)? s(e?, d?)? s(c, dc)? s(e, d)9
end10
end11
s(e, d)? s(e, d) + 112
end13
Tw ? (u, v) ? E : ?v?V,d?Darg min
(u,v)
s((u, v), d)
14
return Mw15
end16
Algorithm 1: The pseudo-code for the Chu-Liu
Edmonds algorithm with our adaptation to in-
clude linear precedence.
inner node. If a vertex v is a leaf in the dependency
tree, its string realisation realise(v) is returned.
We keep track of ordered siblings with two lists,
one for each direction. If the sibling set is left-
wards, the ordered list, Rl, is initialised to be the
singleton set containing a dummy start token with
an empty realisation. If the sibling set is right-
wards then the ordered list, Rr is initialised to be
the realisation of the parent.
For some sibling set C ? Vw to be ordered, the
algorithm chooses the next vertex, v ? C, to insert
into the appropriate ordered list, Rx, x ? D, by
maximising the probability of the string of words
that would result if the realisation, realise(v), were
concatenated with Rx.
The probability of the concatenation is calcu-
lated based on a window of words around the join.
This window length is defined to be 2?floor(n/2),
for some n, in this case, 4.
If the siblings are leftwards, the window con-
sists of the last min(n ? 1, |Rl|) previously cho-
sen words concatenated with the first min(n ?
1, |realise(v)|). If the siblings are rightwards, the
window consists of the last min(n?1, |realise(v)|)
previously chosen words concatenated with the
first min(n ? 1, |Rr|). The probability of a win-
dow of words, w0 . . . wj , of length j+1 is defined
by the following equation:
probLMO(w0 . . . wj)
=
j?k?1
?
i=0
probMLE(wi+k|wi . . . wi+k?1)
(3)
854
/* LMO Algorithm */
input : v, Tw where v ? Vw
output: R ? Vw
begin1
if isLeaf(v) then2
return {realise(v)}3
end4
else5
Cl ? getLeftChildren(v, Tw)6
Cr ? getRightChildren(v, Tw)7
Rl ? {start}8
Rr ? {realise(v)}9
while Cl 6= {} do10
c? arg max
c?Cl
probngram(LMO(c, Tw) ? Rl)11
Rl ? realise(c, Tw) ? Rl12
Cl ? Cl \ {c}13
end14
while Cr 6= {} do15
c? arg max
c?Cr
probngram(Rr ? LMO(c, Tw))16
Rr ? Rr ? realise(c, Tw)17
Cr ? Cr \ {c}18
end19
return Rl ? Rr20
end21
end22
Algorithm 2: The Language Model Ordering al-
gorithm for linearising an Tw.
where k = min(n? 1, j ? 1), and,
probMLE(wi+k|wi . . . wi+k?1)
= cnt(wi . . . wi+k)
cnt(wi . . . wi+k?1)
(4)
where probMLE(wi+k|wi . . . wi+k?1) is the max-
imum likelihood estimate n-gram probability. We
refer to this tree linearisation method as the Lan-
guage Model Ordering (LMO).
4 Using an Argument Satisfaction Model
4.1 Assigning Words to Argument Positions
One limitation of using the CLE algorithm for
generation is that the resulting tree, though max-
imal in probability, may not conform to basic lin-
guistic properties of a dependency tree. In partic-
ular, it may not have the correct number of argu-
ments for each head word. That is, a word may
have too few or too many modifiers.
To address this problem, we can take into ac-
count the argument position when assigning a
weight to an edge. When attaching an edge con-
necting a modifier to a head to the spanning tree,
we count how many modifiers the head already
has. An edge is penalised if it is improbable that
the head takes on yet another modifier, say in the
example of an attachment to a preposition whose
argument position has already been filled.
However, accounting for argument positions
makes an edge weight dynamic and dependent on
surrounding tree context. This makes the search
for an optimal tree an NP-hard problem (McDon-
ald and Satta, 2007) as all possible trees must be
considered to find an optimal solution.
Consequently, we must choose a heuristic
search algorithm for finding the locally optimum
spanning tree. By representing argument positions
that can be filled only once, we allow modifiers
to compete for argument positions and vice versa.
The CLE algorithm only considers this competi-
tion in one direction. In line 3 of Algorithm 1,
only heads compete for modifiers, and thus the so-
lution will be sub-optimal. In Wan et al (2007),
we showed that introducing a model of argument
positions into a greedy spanning tree algorithm
had little effect on performance. Thus, to consider
both directions of competition, we design a new
algorithm for constructing (dependency) spanning
trees that casts edge selection as a weighted bipar-
tite graph matching (or assignment) problem.
This problem is to find a weighted alignments
between objects of two distinct sets, where an ob-
ject from one set is uniquely aligned to some ob-
ject in the other set. The optimal alignment is one
where the sum of alignment costs is minimal. The
graph of all possible assignments is a weighted bi-
partite graph. Here, to discuss bipartite graphs, we
will extend our notation in a fairly standard way,
to write Gp = (U, V,Ep), where U, V are the dis-
joint sets of vertices and Ep the set of edges.
In our paper, we treat the assignment between
attachment positions and words as an assignment
problem. The standard polynomial-time solution
to the assignment problem is the Kuhn-Munkres
(or Hungarian) algorithm (Kuhn, 1955).2
4.2 A Dependency-Spanning Tree Algorithm
Our alternative dependency-spanning tree algo-
rithm, presented as Algorithm 3, incrementally
adds vertices to a growing spanning tree. At
each iteration, the Kuhn-Munkres method assigns
words that are as yet unattached to argument posi-
tions already available in the tree. We focus on the
bipartite graph in Section 4.3.
Let the sentence w have the dependency graph
Gw = (Vw, Ew). At some arbitrary iteration of the
algorithm (see Figure 1), we have the following:
? Tw ? Ew, the set of edges in the spanning
tree constructed so far;
2GPL code: http://sites.google.com/site/garybaker/
hungarian-algorithm/assignment
855
Partially determined spanning tree:
w0
made
john
? l0
? r1 cups
of
? l0
? l1
for
? l0
? l3
johnl0 mader1 ofl0 cupsl1 forl0 madel3
Hw1 Hw2 Hw3 Hw4 Hw5 Hw6
Mw1 Mw2 Mw3 Mw4 Mw5 Mw6
coffee everyone yesterday ?1 ?2 ?3
Figure 1: A snapshot of the generation process.
Each word in the tree has argument positions to
which we can assign remaining words. Padding
Mw with ? is described in Section 4.3.
? Hw = {u, v | (u, v) ? Tw}, the set of ver-
tices in Tw, or ?attached vertices?, and there-
fore potential heads; and
? Mw = Vw\Hw, the set of ?unattached ver-
tices?, and therefore potential modifiers.
For the potential heads, we want to define the set
of possible attachment positions available in the
spanning tree where the potential modifiers can at-
tach. To talk about these attachment positions, we
define the set of labels L = {(d, j)|d ? D, j ?
N}, an element (d, j) representing an attachment
point in direction d, position j. Valid attachment
positions must be in sequential order and not miss-
ing any intermediate positions (e.g. if position 2
on the right is specified, position 1 must be also):
so we define for some i ? N, 0 ? i < N , a set
Ai ? L such that if the label (d, j) ? Ai then the
label (d, k) ? Ai for 0 ? k < j. Collecting these,
we define A = {Ai | 0 ? i < N}.
To map a potential head onto the set of attach-
ment positions, we define a function q : Hw ? A.
So, given some v ? Hw, q(v) = Ai for some
0 ? i < N . In talking about an individual attach-
ment point (d, j) ? q(v) for potential head v, we
/* initialisation */
Hw ? {w0}1
Mw ? V ?2
Uw ? {w0R1}3
U ?w ? {}4
Tw ? {}5
/* The Assignment-based Algorithm */
begin6
while Mw 6= {} and U ?w 6= Uw do7
U ?w ? Uw8
foreach ?u, (d, j)), v? ? Kuhn-Munkres(Gpw =9
(Uw,M?w, E
p
w)) do
Tw ? Tw ? {(u, v)}10
if u ? Hw then11
Uw ? Uw \ {u}12
end13
Uw ? Uw ? next(q(u))14
Uw ? Uw ? next(q(m))15
q(m)? q(m) \ next(q(m))16
q(h)? q(h) \ next(q(h))17
Mw ?Mw \ {m}18
Hw ? Hw ? {m}19
end20
end21
end22
Algorithm 3: The Assignment-based Depen-
dency Tree Building algorithm.
use the notation vdj . For example, when referring
to the second argument position on the right with
respect to v, we use vr2.
For the implementation of the algorithm, we
have defined q, to specify attachment points, as
follows, given some v ? Hw:
q(v) =
?
?
?
?
?
?
?
{vr1} if v = w0, the root
{vl1} if pos(v) is a preposition
L if pos(v) is a verb
{vlj |j ? N} otherwise
Defining q allows one to optionally incorporate
linguistic information if desired.
We define the function next : q(v) ? A, v ?
Hw that returns the position (d, j) with the small-
est value of j for direction d. Finally, we write the
set of available attachment positions in the span-
ning tree as U ? {(v, l) | v ? Hw, l ? q(v)}.
4.3 Finding an Assignment
To construct the bipartite graph used for the as-
signment problem at line 9 of Algorithm 3, given
our original dependency graph Gw = (Vw, Ew),
and the variables defined from it above in Sec-
tion 4.2, we do the following. The first set of
vertices, of possible heads and their attachment
points, is the set Uw. The second set of ver-
tices is the set of possible modifiers augmented
by dummy vertices ?i (indicating no modifica-
tion) such that this set is at least as large as Uw :
M ?w = Mw?{?0, . . . , ?max(0,|Uw|?|Mw|)}. The bi-
856
partite graph is then Gpw = (Uw,M ?w, Epw), where
Epw = {(u, v) |u ? Uw, v ? M ?w}.
The weights on the edges of this graph incor-
porate a model of argument counts. The weight
function is of the form sap : Ep ? R. We
consider some e ? Epw: e = (v?, v) for some
v? ? Uw, v ? M ?w; and v? = (u, (d, j)) for some
u ? Vw, d ? D, j ? N. s(u,M ?w) is defined to re-
turn the maximum cost so that the dummy leaves
are only attached as a last resort. We then define:
sap(e)
= ?log(probdep(u, v, d) ? probarg(u, d, j))
(5)
where probdep(u, v, d) is as in equation 2, using
the original dependency graph defined in Section
2; and probarg(u, d, j), an estimate of the prob-
ability that a word u with i arguments assigned
already can take on more arguments, is defined as:
probarg(u, d, j)
=
??
i=j+1 cntarg(u, d, i)
cnt(u, d) (6)
where cntarg(u, d, i) is the number of times word
u has been seen with i arguments in direction
d; and cnt(u, d) = ?i?N cntarg(u, d, i). As the
probability of argument positions beyond a certain
value for i in a given direction will be extremely
small, we approximate this sum by calculating the
probability density up to a fixed maximum, in this
case 7 argument positions, and assume zero prob-
ability beyond that.
5 Evaluation
5.1 String Generation Task
The best-performing word ordering algorithm is
one that makes fewest grammatical errors. As a
surrogate measurement of grammaticality, we use
the string regeneration task. Beginning with a
human-authored sentence with its word order ran-
domised, the goal is to regenerate the original sen-
tence. Success is indicated by the proportion of the
original sentence regenerated, as measured by any
string comparison method: in our case, using the
BLEU metric (Papineni et al, 2002). One benefit
to this evaluation is that content selection, as a fac-
tor, is held constant. Specifically, the probability
of word selection is uniform for all words.
The string comparison task and its associated
metrics like BLEU are not perfect.3 The evalu-
ation can be seen as being overly strict. It as-
sumes that the only grammatical order is that of the
original human authored sentence, referred to as
the ?gold standard? sentence. Should an approach
chance upon an alternative grammatical ordering,
it would penalised. However, all algorithms and
baselines compared would suffer equally in this
respect, and so this will be less problematic when
averaging across multiple test cases.
5.2 Data Sets and Training Procedures
The Penn Treebank corpus (PTB) was used to pro-
vide a model of dependency relations and argu-
ment counts. It contains about 3 million words
of text from the Wall Street Journal (WSJ) with
human annotations of syntactic structures. Depen-
dency events were sourced from the events file of
the Collins parser package, which contains the de-
pendency events found in training sections 2-22 of
the corpus. Development was done on section 00
and testing was performed on section 23.
A 4-gram language model (LM) was also ob-
tained from the PTB training data, referred to as
PTB-LM. Additionally, a 4-gram language model
was obtained from a subsection of the BLLIP?99
Corpus (LDC number: LDC2000T43) containing
three years of WSJ data from 1987 to 1989 (Char-
niak et al, 1999). As in Collins et al (2004),
the 1987 portion of the BLLIP corpus containing
20 million words was also used to create a lan-
guage model, referred to here as BLLIP-LM. N-
gram models were smoothed using Katz?s method,
backing off to smaller values of n.
For this evaluation, tokenisation was based on
that provided by the PTB data set. This data
set alo delimits base noun phrases (noun phrases
without nested constituents). Base noun phrases
were treated as single tokens, and the rightmost
word assumed to be the head. For the algorithms
tested, the input set for any test case consisted of
the single tokens identified by the PTB tokenisa-
tion. Additionally, the heads of base noun phrases
were included in this input set. That is, we do not
regenerate the base noun phrases.4
3Alternative grammaticality measures have been devel-
oped recently (Mutton et al, 2007). We are currently explor-
ing the use of this and other metrics.
4This would correspond to the use of a chunking algo-
rithm or a named-entity recogniser to find noun phrases that
could be re-used for sentence generation.
857
Algorithms PTB-LM BLLIP-LM
Viterbi baseline 14.9 18.0
LMO baseline 24.3 26.0
CLE 26.4 26.8
AB 33.6 33.7
Figure 2: String regeneration as measured in
BLEU points (maximum 100)
5.3 Algorithms and Baselines
We compare the baselines against the Chu-Liu
Edmonds (CLE) algorithm to see if spanning
tree algorithms do indeed improve upon conven-
tional language modelling. We also compare
the Assignment-based (AB) algorithm against the
baselines and CLE to see if, additionally, mod-
elling argument assignments improves the re-
sulting tree and thus the generated word se-
quence. Two baseline generators based on n-
gram language-models were used, representing
approaches that optimise word sequences based on
the local context of the n-grams.
The first baseline re-uses the LMO greedy se-
quence algorithm on the same set of input words
presented to the CLE and AB algorithms. We ap-
ply LMO in a rightward manner beginning with
a start-of-sentence token. Note that this baseline
generator, like the two spanning tree algorithms,
will score favourably using BLEU since, mini-
mally, the word order of the base noun phrases will
be correct when each is reinserted.
Since the LMO baseline reduces to bigram gen-
eration when concatenating single words, we test
a second language model baseline which always
uses a 4-gram window size. A Viterbi-like gen-
erator with a 4-gram model and a beam of 100 is
used to generate a sequence. For this baseline, re-
ferred to as the Viterbi baseline, base noun phrases
were separated into their constituent words and in-
cluded in the input word set.
5.4 Results
The results are presented in Table 2. Significance
was measured using the sign test and the sampling
method outlined in (Collins et al, 2005). We will
examine the results in the PTB-LM column first.
The gain of 10 BLEU points by the LMO baseline
over the Viterbi baseline shows the performance
improvement that can be gained when reinserting
the base noun phrases.
AB: the dow at this point was down about 35 points
CLE: was down about this point 35 points the dow at
LMO: was this point about at down the down 35 points
Viterbi: the down 35 points at was about this point down
Original: at this point, the dow was down about 35 points
Figure 3: Example generated sentences using the
BLLIP-LM.
The CLE algorithm significantly out-performed
the LMO baseline by 2 BLEU points, from which
we conclude that incorporating a model for global
syntactic structure and treating the search for a
dependency tree as a spanning problem helps for
novel sentence generation. However, the real im-
provement can be seen in the performance of the
AB system which significantly out-performs all
other methods, beating the CLE algorithm by 7
BLEU points, illustrating the benefits of a model
for argument counts and of couching tree building
as an iterative set of argument assignments.
One might reasonably ask if more n-gram data
would narrow the gap between the tree algorithms
and the baselines, which encode global and lo-
cal information respectively. Examining results in
the BLLIP-LM column, all approaches improve
with the better language model. Unsurprisingly,
the improvements are most evident in the base-
lines which rely heavily on the language model.
The margin narrows between the CLE algorithm
and the LMO baseline. However, the AB algo-
rithm still out-performs all other approaches by
7 BLEU points, highlighting the benefit in mod-
elling dependency relations. Even with a language
model that is one order of magnitude larger than
the PTB-LM, the AB still maintains a sizeable lead
in performance. Figure 3 presents sample gener-
ated strings.
6 Related Work
6.1 Statistical Surface Realisers
The work in this paper is similar to research in
statistical surface realisation (for example, Langk-
ilde and Knight (1998); Bangalore and Rambow
(2000); Filippova and Strube (2008)). These start
with a semantic representation for which a specific
rendering, an ordering of words, must be deter-
mined, often using language models to govern tree
traversal. The task in this paper is different as it is
a text-to-text scenario and does not begin with a
representation of semantics.
858
The dependency model and the LMO lineari-
sation algorithm are based heavily on word order
statistics. As such, the utility of this approach is
limited to human languages with minimal use of
inflections, such as English. Approaches for other
language types, for example German, have been
explored (Filippova and Strube, 2007).
6.2 Text-to-Text Generation
As a text-to-text approach, our work is more sim-
ilar to work on Information Fusion (Barzilay et
al., 1999), a sub-problem in multi-document sum-
marisation. In this work, sentences presenting the
same information, for example multiple news arti-
cles describing the same event, are merged to form
a single summary by aligning repeated words and
phrases across sentences.
Other text-to-text approaches for generating
novel sentences also aim to recycle sentence frag-
ments where possible, as we do. Work on phrase-
based statistical machine translation has been
applied to paraphrase generation (Bannard and
Callison-Burch, 2005) and multi-sentence align-
ment in summarisation (Daume? III and Marcu,
2004). These approaches typically use n-gram
models to find the best word sequence.
The WIDL formalism (Soricut and Marcu,
2005) was proposed to efficiently encode con-
straints that restricted possible word sequences,
for example dependency information. Though
similar, our work here does not explicitly repre-
sent the word lattice.
For these text-to-text systems, the order of ele-
ments in the generated sentence is heavily based
on the original order of words and phrases in the
input sentences from which lattices are built. Our
approach has the benefit of considering all possi-
ble orderings of words, corresponding to a wider
range of paraphrases, provided with a suitable de-
pendency model is available.
6.3 Parsing and Semantic Role Labelling
This paper presents work closely related to parsing
work by McDonald et al (2005) which searches
for the best parse tree. Our work can be thought of
as generating projective dependency trees (that is,
without crossing dependencies).
The key difference between parsing and gener-
ation is that, in parsing, the word order is fixed,
whereas for generation, this must be determined.
In this paper, we search across all possible tree
structures whilst searching for the best word or-
dering. As a result, an argument model is needed
to identify linguistically plausible spanning trees.
We treated the alignment of modifiers to head
words as a bipartite graph matching problem. This
is similar to work in semantic role labelling by
Pado? and Lapata (2006). The alignment of an-
swers to question types as a semantic role labelling
task using similar methods was explored by Shen
and Lapata (2007).
Our work is also strongly related to that of
Wong and Mooney (2007) which constructs sym-
bolic semantic structures via an assignment pro-
cess in order to provide surface realisers with in-
put. Our approach differs in that we do not be-
gin with a fixed set of semantic labels. Addition-
ally, our end goal is a dependency tree that encodes
word precedence order, bypassing the surface re-
alisation stage.
7 Conclusions
In this paper, we presented a new use of spanning
tree algorithms for generating sentences from an
input set of words, a task common to many text-
to-text scenarios. The algorithm finds the best de-
pendency trees in order to ensure that the result-
ing string has grammaticality modelled at a global
(sentence) level. Our algorithm incorporates a
model of argument satisfaction which is treated as
an assignment problem, using the Kuhn-Munkres
assignment algorithm. We found a significant im-
provement using BLEU to measure improvements
on the string regeneration task. We conclude that
our new algorithm based on the assignment prob-
lem and an argument model finds trees that are lin-
guistically more plausible, thereby improving the
grammaticality of the generated word sequence.
References
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proceedings of the 18th Conference on
Computational Linguistics, Saarbru?cken, Germany.
Srinivas Bangalore, Owen Rambow, and Steve Whit-
taker. 2000. Evaluation metrics for generation.
In Proceedings of the first international conference
on Natural language generation, Morristown, NJ,
USA.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
859
ciation for Computational Linguistics, Ann Arbor,
Michigan.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings
of the 37th conference on Association for Computa-
tional Linguistics, Morristown, NJ, USA.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 1999. Bllip 1987-89
wsj corpus release 1. Technical report, Linguistic
Data Consortium.
Y. J. Chu and T. H. Liu. 1965. On the shortest
arborescence of a directed graph. Science Sinica,
v.14:1396?1400.
Christopher Collins, Bob Carpenter, and Gerald Penn.
2004. Head-driven parsing for word lattices. In Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, Morristown, NJ,
USA.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, Morristown, NJ, USA.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the Thirty-Fourth Annual Meeting of the As-
sociation for Computational Linguistics, San Fran-
cisco.
Hal Daume? III and Daniel Marcu. 2004. A phrase-
based hmm approach to document/abstract align-
ment. In Proceedings of EMNLP 2004, Barcelona,
Spain..
J. Edmonds. 1967. Optimum branchings. J. Research
of the National Bureau of Standards, 71B:233?240.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in german clauses. In Proceedings
of the 45th Annual Meeting on Association for Com-
putational Linguistics. Prague, Czech Republic.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Con-
ference on Empirical Methods in Natural Language
Processing, Waikiki, Honolulu, Hawaii.
Mark Johnson. 2007. Transforming projective bilex-
ical dependency grammars into efficiently-parsable
cfgs with unfold-fold. In Proceedings of the 45th
Annual Meeting on Association for Computational
Linguistics. Prague, Czech Republic.
H.W. Kuhn. 1955. The hungarian method for the as-
signment problem. Naval Research Logistics Quar-
terly, 219552:83?97 83?97.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of N-grams in derivation. In Proceedings
of the Ninth International Workshop on Natural Lan-
guage Generation, New Brunswick, New Jersey.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of the Tenth International
Conference on Parsing Technologies, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, Morristown, NJ, USA.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of
sentence-level fluency. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, Morristown, NJ, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, Philadelphia, July.
Dan Shen and Mirella Lapata. 2007. Using seman-
tic roles to improve question answering. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, Prague,
Czech Republic.
Radu Soricut and Daniel Marcu. 2005. Towards devel-
oping generation algorithms for text-to-text applica-
tions. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, Ann
Arbor, Michigan.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile
Paris. 2007. Global revision in summarisation:
Generating novel sentences with prim?s algorithm.
In Proceedings of 10th Conference of the Pacific As-
sociation for Computational Linguistic, Melbourne,
Australia.
Yuk Wah Wong and Raymond Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statisti-
cal machine translation. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, Rochester, New York.
860
Towards Statistical Paraphrase Generation: Preliminary Evaluations of
Grammaticality
Stephen Wan
 
Mark Dras
 
Robert Dale
 
 
Center for Language Technology
Div of Information Communication Sciences
Macquarie University
Sydney, NSW 2113
swan,madras,rdale@ics.mq.edu.au
Ce?cile Paris


Information and Communication
Technologies
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
Summary sentences are often para-
phrases of existing sentences. They
may be made up of recycled fragments
of text taken from important sentences
in an input document. We investigate
the use of a statistical sentence gener-
ation technique that recombines words
probabilistically in order to create new
sentences. Given a set of event-related
sentences, we use an extended version
of the Viterbi algorithm which employs
dependency relation and bigram proba-
bilities to find the most probable sum-
mary sentence. Using precision and
recall metrics for verb arguments as a
measure of grammaticality, we find that
our system performs better than a bi-
gram baseline, producing fewer spuri-
ous verb arguments.
1 Introduction
Human authored summaries are more than just
a list of extracted sentences. Often the sum-
mary sentence is a paraphrase of a sentence in the
source text, or else a combination of phrases and
words from important sentences that have been
pieced together to form a new sentence. These
sentences, referred to as Non-Verbatim Sentences,
can replace extracted text to improve readability
and coherence in the summary.
Consider the example in Figure 1 which
presents an alignment between a human authored
summary sentence and a source sentence. The
Summary Sentence:
Every province in the country, except one, endured sporadic fighting, looting
or armed banditry in 2003.
Source Sentence:
However, as the year unfolded, every province has been subjected to fighting,
looting or armed banditry, with the exception of just one province (Kirundo,
in northern Burundi).
Figure 1: An aligned summary and source sen-
tence.
text is taken from a corpus of Humanitarian Aid
Proposals1 produced by the United Nations for
the purpose of convincing donors to support a re-
lief effort.
The example illustrates that sentence extraction
alone cannot account for the breadth of human au-
thored summary sentences. This is supported by
evidence presented in (Jing and McKeown, 1999)
and (Daume? III and Marcu, 2004).
Moving towards the goal of abstract-like auto-
matic summary generation challenges us to con-
sider mechanisms for generating non-verbatim
sentences. Such a mechanism can usefully be
considered as automatically generating a para-
phrase.2 We treat the problem as one in which a
new and previously unseen summary sentence is
to be automatically produced given some closely
related sentences extracted from a source text.
Following on from (Witbrock and Mittal,
1999), we use and extend the Viterbi algorithm
(Forney, 1973) for the purposes of generating
non-verbatim sentences. This approach treats
1These are available publically at
http://www.reliefweb.com.
2Paraphrase here includes sentences generated in an In-
formation Fusion task (Barzilay et al, 1999).
88
sentence generation as a search problem. Given
a set of words (taken from some set of sentences
to paraphrase), we search for the most likely se-
quence given some language model. Intuitively,
we want the generated string to be grammatical
and to accurately reflect the content of the source
text.
Within the Viterbi search process, each time we
append a word to the partially generated sentence,
we consider how well it attaches to a dependency
structure. The focus of this paper is to evaluate
whether or not a series of iterative considerations
of dependency structure results in a grammatical
generated sentence. Previous preliminary evalu-
ations (Wan et al, 2005) indicate that the gen-
erated sequences contain less fragmented text as
measured by an off-the-shelf dependency parser;
more fragments would indicate a grammatically
problematic sentence.
However, while encouraging, such an evalu-
ation says little about what the actual sentence
looks like. For example, such generated text
might only be useful if it contains complete
clauses. Thus, in this paper, we use the precision
and recall metric to measure how many generated
verb arguments, as extracted from dependency re-
lations, are correct.
The remainder of this paper is structured as fol-
lows. Section 2 provides an overview introducing
our approach. In Section 3, we briefly illustrate
our algorithm with examples. A brief survey of
related work is presented in Section 4. We present
our grammaticality experiments in Section 5. We
conclude with further work in Section 6.
2 An Overview of our Approach to
Statistical Sentence Generation
One could characterise the search space as being
a series of nested sets. The outer most set would
contain all possible word sequences. Within this,
a smaller set of strings exhibiting some semblance
of grammaticality might be found, though many
of these might be gibberish. Further nested sets
are those that are grammatical, and within those,
the set of paraphrases that are entailed by the in-
put text.
However, given that we limit ourselves to sta-
tistical techniques and avoid symbolic logic, we
cannot make any claim of strict entailment. We
Original Text
A military transporter was scheduled to take off in the afternoon from Yokota
air base on the outskirts of Tokyo and fly to Osaka with 37,000 blankets .
Mondale said the United States, which has been flying in blankets and is
sending a team of quake relief experts, was prepared to do more if Japan
requested .
United States forces based in Japan will take blankets to help earthquake
survivors Thursday, in the U.S. military?s first disaster relief operation in
Japan since it set up bases here.
Our approach with Dependencies
6: united states forces based in blankets
8: united states which has been flying in blankets
11: a military transporter was prepared to osaka with 37,000 blankets
18: mondale said the afternoon from yokota air base on the united states which
has been flying in blankets
20: mondale said the outskirts of tokyo and is sending a military transporter
was prepared to osaka with 37,000 blankets
23: united states forces based in the afternoon from yokota air base on the
outskirts of tokyo and fly to osaka with 37,000 blankets
27: mondale said the afternoon from yokota air base on the outskirts of tokyo
and is sending a military transporter was prepared to osaka with 37,000 blan-
kets
29: united states which has been flying in the afternoon from yokota air base
on the outskirts of tokyo and is sending a team of quake relief operation in
blankets
31: united states which has been flying in the afternoon from yokota air base
on the outskirts of tokyo and is sending a military transporter was prepared to
osaka with 37,000 blankets
34: mondale said the afternoon from yokota air base on the united states which
has been flying in the outskirts of tokyo and is sending a military transporter
was prepared to osaka with 37,000 blankets
36: united states which has been flying in japan will take off in the after-
noon from yokota air base on the outskirts of tokyo and is sending a military
transporter was prepared to osaka with 37,000 blankets
Figure 2: A selection of example output. Sen-
tences are prefixed by their length.
thus propose an intermediate set of sentences
which conserve the content of the source text
without necessarily being entailed. These are re-
ferred to as the set of verisimilitudes, of which
properly entailed sentences are a subset. The aim
of our choice of features and our algorithm exten-
sion is to reduce the search space from gibberish
strings to that of verisimilitudes. While generat-
ing verisimilitudes is our end goal, in this paper,
we are concerned principally with the generating
of grammatical sentences.
To do so, the extension adds an extra feature
propagation mechanism to the Viterbi algorithm
such that features are passed along a word se-
quence path in the search space whenever a new
word is appended to it. Propagated features are
used to influence the choice of subsequent words
suitable for appending to a partially generated
sentence. In our case, our feature is a depen-
dency structure of the word sequence correspond-
ing to the search path. Our present dependency
representation is based on that of (Kittredge and
89
Mel?cuk, 1983). However, it contains only the
head and modifier of a relation, ignoring relation-
ship labels for the present.
Algorithmically, after appending a word to a
path, a dependency structure of the partially gen-
erated string is obtained probabilistically. Along
with bigram information, the long-distance con-
text of dependency head information of the pre-
ceding word sequence will be useful in generat-
ing better sentences by filtering out all words that
might, at a particular position in the string, lead
to a spurious dependency relation in the final sen-
tence. Example output is presented in Figure 2.
As the dependency ?parsing? mechanism is lin-
ear3 and is embedded within the Viterbi algo-
rithm, the result is an O( 

) algorithm.
By examining surface-syntactic dependency
structure at each step in the search, resulting sen-
tences are likely to be more grammatical. This
marraige of models has been tested in other fields
such as speech recognition (Chelba and Jelinek,
1998) with success. Although it is an impover-
ished representation of semantics, considering de-
pendency features in our application context may
also serendipitously assist verisimilitude genera-
tion.
3 The Extended Viterbi Algorithm:
Propagating Dependency Structure
In this section, we present an overview of the
main features of our algorithm extension. We di-
rect the interested reader to our technical paper
(Wan et al, 2005) for full details.
The Viterbi algorithm (for a comprehensive
overview, see (Manning and Schu?tze, 1999)) is
used to search for the best path across a network
of nodes, where each node represents a word in
the vocabulary. The best sentence is a string of
words, each one emitted by the corresponding vis-
ited node on the path.
Arcs between nodes are weighted using a com-
bination of two pieces of information: a bigram
probability corresponding to that pair of words;
and a probability corresponding to the likelihood
of a dependency relation between that pair of
words. Specifically, the transition probability
3The parse is thus not necessarily optimal, in the sense of
guaranteeing the most likely parse.
defining these weights is the average of the depen-
dency transition probability and the bigram prob-
ability.
To simplify matters in this evaluation, we
assume that the emission probability is always
one. The emission probability is interpreted
as being a Content Selection mechanism that
chooses words that are likely to be in a summary.
Thus, in this paper, each word has an equally
likely chance of being selected for the sentence.
Transition Probability is defined as:
	
		 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344?351,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
GLEU: Automatic Evaluation of Sentence-Level Fluency
Andrew Mutton? Mark Dras? Stephen Wan?,? Robert Dale?
?Centre for Language Technology ?Information and Communication Technologies
Macquarie University CSIRO
NSW 2109 Australia NSW 2109 Australia
madras@ics.mq.edu.au
Abstract
In evaluating the output of language tech-
nology applications?MT, natural language
generation, summarisation?automatic eval-
uation techniques generally conflate mea-
surement of faithfulness to source content
with fluency of the resulting text. In this
paper we develop an automatic evaluation
metric to estimate fluency alone, by examin-
ing the use of parser outputs as metrics, and
show that they correlate with human judge-
ments of generated text fluency. We then de-
velop a machine learner based on these, and
show that this performs better than the indi-
vidual parser metrics, approaching a lower
bound on human performance. We finally
look at different language models for gener-
ating sentences, and show that while individ-
ual parser metrics can be ?fooled? depending
on generation method, the machine learner
provides a consistent estimator of fluency.
1 Introduction
Intrinsic evaluation of the output of many language
technologies can be characterised as having at least
two aspects: how well the generated text reflects
the source data, whether it be text in another lan-
guage for machine translation (MT), a natural lan-
guage generation (NLG) input representation, a doc-
ument to be summarised, and so on; and how well it
conforms to normal human language usage. These
two aspects are often made explicit in approaches
to creating the text. For example, in statistical MT
the translation model and the language model are
treated separately, characterised as faithfulness and
fluency respectively (as in the treatment in Jurafsky
and Martin (2000)). Similarly, the ultrasummarisa-
tion model of Witbrock and Mittal (1999) consists
of a content model, modelling the probability that a
word in the source text will be in the summary, and
a language model.
Evaluation methods can be said to fall into two cate-
gories: a comparison to gold reference, or an appeal
to human judgements. Automatic evaluation meth-
ods carrying out a comparison to gold reference tend
to conflate the two aspects of faithfulness and flu-
ency in giving a goodness score for generated out-
put. BLEU (Papineni et al, 2002) is a canonical ex-
ample: in matching n-grams in a candidate transla-
tion text with those in a reference text, the metric
measures faithfulness by counting the matches, and
fluency by implicitly using the reference n-grams as
a language model. Often we are interested in know-
ing the quality of the two aspects separately; many
human judgement frameworks ask specifically for
separate judgements on elements of the task that cor-
respond to faithfulness and to fluency. In addition,
the need for reference texts for an evaluation metric
can be problematic, and intuitively seems unneces-
sary for characterising an aspect of text quality that
is not related to its content source but to the use of
language itself. It is a goal of this paper to provide
an automatic evaluation method for fluency alone,
without the use of a reference text.
One might consider using a metric based on lan-
guage model probabilities for sentences: in eval-
344
uating a language model on (already existing) test
data, a higher probability for a sentence (and lower
perplexity over a whole test corpus) indicates bet-
ter language modelling; perhaps a higher probability
might indicate a better sentence. However, here we
are looking at generated sentences, which have been
generated using their own language model, rather
than human-authored sentences already existing in
a test corpus; and so it is not obvious what language
model would be an objective assessment of sentence
naturalness. In the case of evaluating a single sys-
tem, using the language model that generated the
sentence will only confirm that the sentence does
fit the language model; in situations such as com-
paring two systems which each generate text using
a different language model, it is not obvious that
there is a principled way of deciding on a fair lan-
guage model. Quite a different idea was suggested
in Wan et al (2005), of using the grammatical judge-
ment of a parser to assess fluency, giving a measure
independent of the language model used to gener-
ate the text. The idea is that, assuming the parser
has been trained on an appropriate corpus, the poor
performance of the parser on one sentence relative
to another might be an indicator of some degree of
ungrammaticality and possibly disfluency. In that
work, however, correlation with human judgements
was left uninvestigated.
The goal of this paper is to take this idea and de-
velop it. In Section 2 we look at some related work
on metrics, in particular for NLG. In Section 3, we
verify whether parser outputs can be used as esti-
mators of generated sentence fluency by correlating
them with human judgements. In Section 4, we pro-
pose an SVM-based metric using parser outputs as
features, and compare its correlation against human
judgements with that of the individual parsers. In
Section 5, we investigate the effects on the various
metrics from different types of language model for
the generated text. Then in Section 6 we conclude.
2 Related Work
In terms of human evaluation, there is no uniform
view on what constitutes the notion of fluency, or its
relationship to grammaticality or similar concepts.
We mention a few examples here to illustrate the
range of usage. In MT, the 2005 NIST MT Evalu-
ation Plan uses guidelines1 for judges to assess ?ad-
equacy? and ?fluency? on 5 point scales, where they
are asked to provide intuitive reactions rather than
pondering their decisions; for fluency, the scale de-
scriptions are fairly vague (5: flawless English; 4:
good English; 3: non-native English; 2: disfluent
English; 1: incomprehensible) and instructions are
short, with some examples provided in appendices.
Zajic et al (2002) use similar scales for summari-
sation. By contrast, Pan and Shaw (2004), for their
NLG system SEGUE tied the notion of fluency more
tightly to grammaticality, giving two human evalu-
ators three grade options: good, minor grammatical
error, major grammatical/pragmatic error. As a fur-
ther contrast, the analysis of Coch (1996) was very
comprehensive and fine-grained, in a comparison of
three text-production techniques: he used 14 human
judges, each judging 60 letters (20 per generation
system), and required them to assess the letters for
correct spelling, good grammar, rhythm and flow,
appropriateness of tone, and several other specific
characteristics of good text.
In terms of automatic evaluation, we are not aware
of any technique that measures only fluency or sim-
ilar characteristics, ignoring content, apart from that
of Wan et al (2005). Even in NLG, where, given the
variability of the input representations (and hence
difficulty in verifying faithfulness), it might be ex-
pected that such measures would be available, the
available metrics still conflate content and form.
For example, the metrics proposed in Bangalore et
al. (2000), such as Simple Accuracy and Generation
Accuracy, measure changes with respect to a refer-
ence string based on the idea of string-edit distance.
Similarly, BLEU has been used in NLG, for example
by Langkilde-Geary (2002).
3 Parsers as Evaluators
There are three parts to verifying the usefulness of
parsers as evaluators: choosing the parsers and the
metrics derived from them; generating some texts
for human and parser evaluation; and, the key part,
getting human judgements on these texts and corre-
lating them with parser metrics.
1http://projects.ldc.upenn.edu/TIDES/
Translation/TranAssessSpec.pdf
345
3.1 The Parsers
In testing the idea of using parsers to judge fluency,
we use three parsers, from which we derive four
parser metrics, to investigate the general applicabil-
ity of the idea. Those chosen were the Connexor
parser,2 the Collins parser (Collins, 1999), and the
Link Grammar parser (Grinberg et al, 1995). Each
produces output that can be taken as representing
degree of ungrammaticality, although this output is
quite different for each.
Connexor is a commercially available dependency
parser that returns head?dependant relations as well
as stemming information, part of speech, and so on.
In the case of an ungrammatical sentence, Connexor
returns tree fragments, where these fragments are
defined by transitive head?dependant relations: for
example, for the sentence Everybody likes big cakes
do it returns fragments for Everybody likes big cakes
and for do. We expect that the number of fragments
should correlate inversely with the quality of a sen-
tence. For a metric, we normalise this number by
the largest number of fragments for a given data set.
(Normalisation matters most for the machine learner
in Section 4.)
The Collins parser is a statistical chart parser that
aims to maximise the probability of a parse using dy-
namic programming. The parse tree produced is an-
notated with log probabilities, including one for the
whole tree. In the case of ungrammatical sentences,
the parser will assign a low probability to any parse,
including the most likely one. We expect that the
log probability (becoming more negative as the sen-
tence is less likely) should correlate positively with
the quality of a sentence. For a metric, we normalise
this by the most negative value for a given data set.
Like Connexor, the Link Grammar parser returns in-
formation about word relationships, forming links,
with the proviso that links cannot cross and that in
a grammatical sentence all links are indirectly con-
nected. For an ungrammatical sentence, the parser
will delete words until it can produce a parse; the
number it deletes is called the ?null count?. We ex-
pect that this should correlate inversely with sen-
tence quality. For a metric, we normalise this by
the sentence length. In addition, the parser produces
2http://www.connexor.com
another variable possibly of interest. In generating
a parse, the parser produces many candidates and
rules some out by a posteriori constraints on valid
parses. In its output the parser returns the number of
invalid parses. For an ungrammatical sentence, this
number may be higher; however, there may also be
more parses. For a metric, we normalise this by the
total number of parses found for the sentence. There
is no strong intuition about the direction of correla-
tion here, but we investigate it in any case.
3.2 Text Generation Method
To test whether these parsers are able to discriminate
sentence-length texts of varying degrees of fluency,
we need first to generate texts that we expect will be
discriminable in fluency quality ranging from good
to very poor. Below we describe our method for gen-
erating text, and then our preliminary check on the
discriminability of the data before giving them to hu-
man judges.
Our approach to generating ?sentences? of a fixed
length is to take word sequences of different lengths
from a corpus and glue them together probabilisti-
cally: the intuition is that a few longer sequences
glued together will be more fluent than many shorter
sequences. More precisely, to generate a sentence of
length n, we take sequences of length l (such that l
divides n), with sequence i of the form wi,1 . . . wi,l,
where wi, is a word or punctuation mark. We start
by selecting sequence 1, first by randomly choos-
ing its first word according to the unigram probabil-
ity P (w1,1), and then the sequence uniformly ran-
domly over all sequences of length l starting with
w1,1; we select subsequent sequences j (2 ? j ?
n/l) randomly according to the bigram probability
P (wj,1 |wj?1,l). Taking as our corpus the Reuters
corpus,3 for length n = 24, we generate sentences
for sequence sizes l = 1, 2, 4, 8, 24 as in Figure 1.
So, for instance, the sequence-size 8 example was
constructed by stringing together the three consecu-
tive sequences of length 8 (There . . . to; be . . . have;
to . . . .) taken from the corpus.
These examples, and others generated, appear to
be of variable quality in accordance with our intu-
ition. However, to confirm this prior to testing them
3http://trec.nist.gov/data/reuters/
reuters.html
346
Extracted (Sequence-size 24)
Ginebra face Formula Shell in a sudden-death playoff on Sun-
day to decide who will face Alaska in a best-of-seven series for
the title.
Sequence-size 8
There is some thinking in the government to be nearly as dra-
matic as some people have to be slaughtered to eradicate the
epidemic.
Sequence-size 4
Most of Banharn?s move comes after it can still be averted the
crash if it should again become a police statement said.
Sequence-size 2
Massey said in line with losses, Nordbanken is well-placed to
benefit abuse was loaded with Czech prime minister Andris
Shkele, said.
Sequence-size 1
The war we?re here in a spokesman Jeff Sluman 86 percent jump
that Spain to what was booked, express also said.
Figure 1: Sample sentences from the first trial
Description Correlation
Small 0.10 to 0.29
Medium 0.30 to 0.49
Large 0.50 to 1.00
Table 1: Correlation coefficient interpretation
out for discriminability in a human trial, we wanted
see whether they are discriminable by some method
other than our own judgement. We used the parsers
described in Section 3.1, in the hope of finding a
non-zero correlation between the parser outputs and
the sequence lengths.
Regarding the interpretation of the absolute value of
(Pearson?s) correlation coefficients, both here and in
the rest of the paper, we adopt Cohen?s scale (Co-
hen, 1988) for use in human judgements, given in
Table 1; we use this as most of this work is to do with
human judgements of fluency. For data, we gener-
ated 1000 sentences of length 24 for each sequence
length l = 1, 2, 3, 4, 6, 8, 24, giving 7000 sentences
in total. The correlations with the four parser out-
puts are as in Table 2, with the medium correlations
for Collins and Link Grammar (nulled tokens) indi-
cating that the sentences are indeed discriminable to
some extent, and hence the approach is likely to be
useful for generating sentences for human trials.
3.3 Human Judgements
The next step is then to obtain a set of human judge-
ments for this data. Human judges can only be ex-
pected to judge a reasonably sized amount of data,
Metric Corr.
Collins Parser 0.3101
Connexor -0.2332
Link Grammar Nulled Tokens -0.3204
Link Grammar Invalid Parses 0.1776
GLEU 0.4144
Table 2: Parser vs sequence size for original data set
so we first reduced the set of sequence sizes to be
judged. To do this we determined for the 7000
generated sentences the scores according to the (ar-
bitrarily chosen) Collins parser, and calculated the
means for each sequence size and the 95% confi-
dence intervals around these means. We then chose
a subset of sequence sizes such that the confidence
intervals did not overlap: 1, 2, 4, 8, 24; the idea was
that this would be likely to give maximally discrim-
inable sentences. For each of these sequences sizes,
we chose randomly 10 sentences from the initial set,
giving a set for human judgement of size 50.
The judges consisted of twenty volunteers, all native
English speakers without explicit linguistic training.
We gave them general guidelines about what consti-
tuted fluency, mentioning that they should consider
grammaticality but deliberately not giving detailed
instructions on the manner for doing this, as we were
interested in the level of agreement of intuitive un-
derstanding of fluency. We instructed them also that
they should evaluate the sentence without consider-
ing its content, using Colourless green ideas sleep
furiously as an example of a nonsensical but per-
fectly fluent sentence. The judges were then pre-
sented with the 50 sentences in random order, and
asked to score the sentences according to their own
scale, as in magnitude estimation (Bard et al, 1996);
these scores were then normalised in the range [0,1].
Some judges noted that the task was difficult be-
cause of its subjectivity. Notwithstanding this sub-
jectivity and variation in their approach to the task,
the pairwise correlations between judges were high,
as indicated by the maximum, minimum and mean
values in Table 3, indicating that our assumption
that humans had an intuitive notion of fluency
and needed only minimal instruction was justified.
Looking at mean scores for each sequence size,
judges generally also ranked sentences by sequence
size; see Figure 2. Comparing human judgement
347
Statistic Corr.
Maximum correlation 0.8749
Minimum correlation 0.4710
Mean correlation 0.7040
Standard deviation 0.0813
Table 3: Data on correlation between humans
Figure 2: Mean scores for human judges
correlations against sequence size with the same cor-
relations for the parser metrics (as for Table 2, but on
the human trial data) gives Table 4, indicating that
humans can also discriminate the different generated
sentence types, in fact (not surprisingly) better than
the automatic metrics.
Now, having both human judgement scores of some
reliability for sentences, and scoring metrics from
three parsers, we give correlations in Table 5. Given
Cohen?s interpretation, the Collins and Link Gram-
mar (nulled tokens) metrics show moderate correla-
tion, the Connexor metric almost so; the Link Gram-
mar (invalid parses) metric correlation is by far the
weakest. The consistency and magnitude of the first
three parser metrics, however, lends support to the
idea of Wan et al (2005) to use something like these
as indicators of generated sentence fluency. The aim
of the next section is to build a better predictor than
the individual parser metrics alone.
Metric Corr.
Humans 0.6529
Collins Parser 0.4057
Connexor -0.3804
Link Grammar Nulled Tokens -0.3310
Link Grammar Invalid Parses 0.1619
GLEU 0.4606
Table 4: Correlation with sequence size for human
trial data set
Metric Corr.
Collins Parser 0.3057
Connexor -0.3445
Link-Grammar Nulled Tokens -0.2939
Link Grammar Invalid Parses 0.1854
GLEU 0.4014
Table 5: Correlation between metrics and human
evaluators
4 An SVM-Based Metric
In MT, one problem with most metrics like BLEU
is that they are intended to apply only to document-
length texts, and any application to individual sen-
tences is inaccurate and correlates poorly with
human judgements. A neat solution to poor
sentence-level evaluation proposed by Kulesza and
Shieber (2004) is to use a Support Vector Machine,
using features such as word error rate, to estimate
sentence-level translation quality. The two main in-
sights in applying SVMs here are, first, noting that
human translations are generally good and machine
translations poor, that binary training data can be
created by taking the human translations as posi-
tive training instances and machine translations as
negative ones; and second, that a non-binary metric
of translation goodness can be derived by the dis-
tance from a test instance to the support vectors. In
an empirical evaluation, Kulesza and Shieber found
that their SVM gave a correlation of 0.37, which
was an improvement of around half the gap between
the BLEU correlations with the human judgements
(0.25) and the lowest pairwise human inter-judge
correlation (0.46) (Turian et al, 2003).
We take a similar approach here, using as features
the four parser metrics described in Section 3. We
trained an SVM,4 taking as positive training data
the 1000 instances of sentences of sequence length
24 (i.e. sentences extracted from the corpus) and
as negative training data the 1000 sentences of se-
quence length 1. We call this learner GLEU.5
As a check on the ability of the GLEU SVM to dis-
tinguish these ?positive? sentences from ?negative?
ones, we evaluated its classification accuracy on a
(new) test set of size 300, split evenly between sen-
tences of sequence length 24 and sequence length 1.
4We used the package SVM-light (Joachims, 1999).
5For GrammaticaLity Evaluation Utility.
348
This gave 81%, against a random baseline of 50%,
indicating that the SVM can classify satisfactorily.
We now move from looking at classification accu-
racy to the main purpose of the SVM, using distance
from support vector as a metric. Results are given
for correlation of GLEU against sequence sizes for
all data (Table 2) and for the human trial data set
(Table 4); and also for correlation of GLEU against
the human judges? scores (Table 5). This last indi-
cates that GLEU correlates better with human judge-
ments than any of the parsers individually, and is
well within the ?moderate? range for correlation in-
terpretation. In particular, for the GLEU?human cor-
relation, the score of 0.4014 is approaching the min-
imum pairwise human correlation of 0.4710.
5 Different Text Generation Methods
The method used to generate text in Section 3.2 is
a variation of the standard n-gram language model.
A question that arises is: Are any of the metrics de-
fined above strongly influenced by the type of lan-
guage model used to generate the text? It may be the
case, for example, that a parser implementation uses
its own language model that predisposes it to favour
a similar model in the text generation process. This
is a phenomenon seen in MT, where BLEU seems to
favour text that has been produced using a similar
statistical n-gram language model over other sym-
bolic models (Callison-Burch et al, 2006).
Our previous approach used only sequences of
words concatenated together. To define some new
methods for generating text, we introduced varying
amounts of structure into the generation process.
5.1 Structural Generation Methods
PoStag In the first of these, we constructed a
rough approximation of typical sentence grammar
structure by taking bigrams over part-of-speech
tags.6 Then, given a string of PoS tags of length
n, t1 . . . tn, we start by assigning the probabilities
for the word in position 1, w1, according to the con-
ditional probability P (w1 | t1). Then, for position j
(2 ? j ? n), we assign to candidate words the value
P (wj | tj)?P (wj |wj?1) to score word sequences.
6We used the supertagger of Bangalore and Joshi (1999).
So, for example, we might generate the PoS tag tem-
plate Det NN Adj Adv, take all the words corre-
sponding to each of these parts of speech, and com-
bine bigram word sequence probability with the con-
ditional probability of words with respect to these
parts of speech. We then use a Viterbi-style algo-
rithm to find the most likely word sequence.
In this model we violate the Markov assumption of
independence in much the same way as Witbrock
and Mittal (1999) in their combination of content
and language model probabilities, by backtracking
at every state in order to discourage repeated words
and avoid loops.
Supertag This is a variant of the approach above,
but using supertags (Bangalore and Joshi, 1999) in-
stead of PoS tags. The idea is that the supertags
might give a more fine-grained definition of struc-
ture, using partial trees rather than parts of speech.
CFG We extracted a CFG from the ?10% of the
Penn Treebank found in the NLTK-lite corpora.7
This CFG was then augmented with productions de-
rived from the PoS-tagged data used above. We then
generated a template of length n pre-terminal cate-
gories using this CFG. To avoid loops we biased the
selection towards terminals over non-terminals.
5.2 Human Judgements
We generated sentences according to a mix of the
initial method of Section 3.2, for calibration, and
the new methods above. We again used a sentence
length of 24, and sequence lengths for the initial
method of l = 1, 8, 24. A sample of sentences gen-
erated for each of these six types is in Figure 3.
For our data, we generated 1000 sentences per gen-
eration method, giving a corpus of 6000 sentences.
For the human judgements we also again took 10
sentences per generation method, giving 60 sen-
tences in total. The same judges were given the same
instructions as previously.
Before correlating the human judges? scores and
the parser outputs, it is interesting to look at how
each parser treats the sentence generation methods,
and how this compares with human ratings (Ta-
ble 6). In particular, note that the Collins parser rates
the PoStag- and Supertag-generated sentences more
7http://nltk.sourceforge.net
349
Extracted (Sequence-size 24)
After a near three-hour meeting and last-minute talks with Pres-
ident Lennart Meri, the Reform Party council voted overwhelm-
ingly to leave the government.
Sequence-size 8
If Denmark is closely linked to the Euro Disney reported a net
profit of 85 million note: the figures were rounded off.
Sequence-size 1
Israelis there would seek approval for all-party peace now com-
plain that this year, which shows demand following year and 56
billion pounds.
POS-tag, Viterbi-mapped
He said earlier the 9 years and holding company?s government,
including 69.62 points as a number of last year but market.
Supertag, Viterbi-mapped
That 97 saying he said in its shares of the market 74.53 percent,
adding to allow foreign exchange: I think people.
Context-Free Grammar
The production moderated Chernomyrdin which leveled gov-
ernment back near own 52 over every a current at from the said
by later the other.
Figure 3: Sample sentences from the second trial
sent. type s-24 s-8 s-1 PoS sup. CFG
Collins 0.52 0.48 0.41 0.60 0.57 0.36
Connexor 0.12 0.16 0.24 0.26 0.25 0.43
LG (null) 0.02 0.06 0.10 0.09 0.11 0.18
LG (invalid) 0.78 0.67 0.56 0.62 0.66 0.53
GLEU 1.07 0.32 -0.96 0.28 -0.06 -2.48
Human 0.93 0.67 0.44 0.39 0.44 0.31
Table 6: Mean normalised scores per sentence type
highly even than real sentences (in bold). These
are the two methods that use the Viterbi-style algo-
rithm, suggesting that this probability maximisation
has fooled the Collins parser. The pairwise correla-
tion between judges was around the same on average
as in Section 3.3, but with wider variation (Table 7).
The main results, determining the correlation of the
various parser metrics plus GLEU against the new
data, are in Table 8. This confirms the very vari-
able performance of the Collins parser, which has
dropped significantly. GLEU performs quite consis-
tently here, this time a little behind the Link Gram-
mar (nulled tokens) result, but still with a better
correlation with human judgement than at least two
Statistic Corr.
Maximum correlation 0.9048
Minimum correlation 0.3318
Mean correlation 0.7250
Standard deviation 0.0980
Table 7: Data on correlation between humans
Metric Corr.
Collins Parser 0.1898
Connexor -0.3632
Link-Grammar Nulled Tokens -0.4803
Link Grammar Invalid Parses 0.1774
GLEU 0.4738
Table 8: Correlation between parsers and human
evaluators on new human trial data
Metric Corr.
Collins Parser 0.2313
Connexor -0.2042
Link-Grammar Nulled Tokens -0.1289
Link Grammar Invalid Parses -0.0084
GLEU 0.4312
Table 9: Correlation between parsers and human
evaluators on all human trial data
judges with each other. (Note also that the GLEU
SVM was not retrained on the new sentence types.)
Looking at all the data together, however, is where
GLEU particularly displays its consistency. Aggre-
gating the old human trial data (Section 3.3) and the
new data, and determining correlations against the
metrics, we get the data in Table 9. Again the SVM?s
performance is consistent, but is now almost twice
as high as its nearest alternative, Collins.
5.3 Discussion
In general, there is at least one parser that correlates
quite well with the human judges for each sentence
type. With well-structured sentences, the probabilis-
tic Collins parser performs best; on sentences that
are generated by a poor probabilistic model lead-
ing to poor structure, Link Grammar (nulled tokens)
performs best. This supports the use of a machine
learner taking as features outputs from several parser
types; empirically this is confirmed by the large ad-
vantage GLEU has on overall data (Table 9).
The generated text itself from the Viterbi-based gen-
erators as implemented here is quite disappoint-
ing, given an expectation that introducing structure
would make sentences more natural and hence lead
to a range of sentence qualities. In hindsight, this
is not so surprising; in generating the structure tem-
plate, only sequences (over tags) of size 1 were used,
which is perhaps why the human judges deemed
them fairly close to sentences generated by the origi-
350
nal method using sequence size 1, the poorest of that
initial data set.
6 Conclusion
In this paper we have investigated a new approach to
evaluating the fluency of individual generated sen-
tences. The notion of what constitutes fluency is
an imprecise one, but trials with human judges have
shown that even if it cannot be exactly defined, or
even articulated by the judges, there is a high level
of agreement about what is fluent and what is not.
Given this data, metrics derived from parser out-
puts have been found useful for measuring fluency,
correlating up to moderately well with these human
judgements. A better approach is to combine these
in a machine learner, as in our SVM GLEU, which
outperforms individual parser metrics. Interestingly,
we have found that the parser metrics can be fooled
by the method of sentence generation; GLEU, how-
ever, gives a consistent estimate of fluency regard-
less of generation type; and, across all types of gen-
erated sentences examined in this paper, is superior
to individual parser metrics by a large margin.
This all suggests that the approach has promise, but
it needs to be developed further for pratical use. The
SVM presented in this paper has only four features;
more features, and in particular a wider range of
parsers, should raise correlations. In terms of the
data, we looked only at sentences generated with
several parameters fixed, such as sentence length,
due to our limited pool of judges. In future we would
like to examine the space of sentence types more
fully. In particular, we will look at predicting the flu-
ency of near-human quality sentences. More gener-
ally, we would like to look also at how the approach
of this paper would relate to a perplexity-based met-
ric; how it compares against BLEU or similar mea-
sures as a predictor of fluency in a context where ref-
erence sentences are available; and whether GLEU
might be useful in applications such as reranking of
candidate sentences in MT.
Acknowledgements
We thank Ben Hutchinson and Mirella Lapata for discussions,
and Srinivas Bangalore for the TAG supertagger. The sec-
ond author acknowledges the support of ARC Discovery Grant
DP0558852.
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguistics,
25(2):237?265.
Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
2000. Evaluation metrics for generation. In Proceedings of the
First International Natural Language Generation Conference
(INLG2000), Mitzpe Ramon, Israel.
E. Bard, D. Robertson, and A. Sorace. 1996. Magnitude esti-
mation and linguistic acceptability. Language, 72(1):32?68.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the Role of Bleu in Machine Translation
Research. In Proceedings of EACL, pages 249?256.
Jose? Coch. 1996. Evaluating and comparing three text-
production strategies. In Proceedings of the 16th International
Conference on Computational Linguistics (COLING?96), pages
249?254.
J. Cohen. 1988. Statistical power analysis for the behavioral
sciences. Erlbaum, Hillsdale, NJ, US.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Dennis Grinberg, John Lafferty, and Daniel Sleator. 1995. A
robus parsing algorithm for link grammars. In Proceedings of
the Fourth International Workshop on Parsing Technologies.
Thorsten Joachims. 1999. Making Large-Scale SVM Learning
Practical. MIT Press.
Daniel Jurafsky and James Martin. 2000. Speech and Lan-
guage Processing: An Introduction to Natural Languge Pro-
cessing, Computational Linguistics, and Speech Recognition.
Prentice-Hall.
Alex Kulesza and Stuart Shieber. 2004. A learning approach to
improving sentence-level MT evaluation. In Proceedings of the
10th International Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD, US.
Irene Langkilde-Geary. 2002. An empirical verification of cov-
erage and correctness for a general-purpose sentence generator.
In Proceedings of the International Natural Language Genera-
tion Conference (INLG) 2002, pages 17?24.
Shimei Pan and James Shaw. 2004. Segue: A hybrid case-
based surface natural language generator. In Proceedings of
the International Conference on Natural Language Generation
(INLG) 2004, pages 130?140.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176, IBM.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003. Evalua-
tion of Machine Translation and its evaluation. In Proceedings
of MT Summit IX, pages 23?28.
Stephen Wan, Robert Dale, Mark Dras, and Ce?cile Paris. 2005.
Searching for grammaticality: Propagating dependencies in the
Viterbi algorithm. In Proceedings of the 10th European Natural
Language Processing Wworkshop, Aberdeen, UK.
Michael Witbrock and Vibhu Mittal. 1999. Ultra-
summarization: A statistical approach to generating highly con-
densed non-executive summaries. In Proceedings of the 22nd
International Conference on Research and Development in In-
formation Retrieval (SIGIR?99).
David Zajic, Bonnie Dorr, and Richard Schwartz. 2002. Au-
tomatic headline generation for newspaper stories. In Pro-
ceedings of the ACL-2002 Workshop on Text Summarization
(DUC2002), pages 78?85.
351
Using Thematic Information in Statistical Headline Generation 
Stephen Wan 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
swan@ics.mq.edu.au 
Mark Dras 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
madras@ics.mq.edu.au 
C?cile Paris 
CSIRO Mathematical 
and Information 
Sciences 
Locked Bag 17 
North Ryde 1670 
Sydney, Australia 
Cecile.Paris@csiro.au 
Robert Dale 
Center for Language 
Technology 
Macquarie University 
Sydney, Australia 
rdale@ics.mq.edu.au 
 
Abstract  
We explore the problem of single 
sentence summarisation.  In the news 
domain, such a summary might 
resemble a headline.  The headline 
generation system we present uses 
Singular Value Decomposition (SVD) to 
guide the generation of a headline 
towards the theme that best represents 
the document to be summarised.   In 
doing so, the intuition is that the 
generated summary will more accurately 
reflect the content of the source 
document.  This paper presents SVD as 
an alternative method to determine if a 
word is a suitable candidate for 
inclusion in the headline.  The results of 
a recall based evaluation comparing 
three different strategies to word 
selection, indicate that thematic 
information does help improve recall. 
1 Introduction 
Ours is an age where many documents are 
archived electronically and are available 
whenever needed. In the midst of this plethora of 
information, the successful completion of a 
research task is affected by the ease with which 
users can quickly identify the relevant electronic 
documents that satisfy their information needs.  
To do so, a researcher often relies on generated 
summaries that reflect the contents of the 
original document.    
We explore the problem of single sentence 
summarisation, the primary focus of this paper.  
Instead of identifying and extracting the most 
important sentence, we generate a new sentence 
from scratch.   The resulting sentence summary 
may not occur verbatim in the source document 
but may instead be a paraphrase combining key 
words and phrases from the text.   
As a precursor to single sentence summarisation, 
we first explore the particular case of headline 
generation in the news domain, specifically 
English news.  Although headlines are often 
constructed to be sensationalist, we regard 
headline generation as an approximation to 
single sentence summarisation, given that a 
corpus of single sentence summaries does not 
exist.   
Our system re-uses words from the news article 
to generate a single sentence summary that 
resembles a headline.  This is done by selecting 
and then appending words from the source 
article. This approach has been explored by a 
number of researchers (eg. see Witbrock and 
Mittal, 1999; Jin and Hauptmann, 2002) and we 
will describe their work further in the next 
section.  In existing approaches, a word is 
selected on the basis of two criteria: how well it 
acts as a summary word, and how grammatical it 
will be given the preceding summary words that 
have already been chosen. 
The purpose of this paper is to present work 
which investigates the use of Singular Value 
Decomposition (SVD) as a means of 
determining if a word is a good candidate for 
inclusion in the headline. 
To introduce the notion of using SVD for single 
sentence summarisation in this paper, we 
examine the simplest summarisation scenario.  
Thus, presently we are only concerned with 
single document summarisation.  In addition, we 
limit the focus of our discussion to the 
generation of generic summaries.  
In the remainder of this paper, we describe our 
motivation for using SVD by describing 
difficulties in generating headlines in Section 2.  
In Section 3, as motivation for our approach, we 
illustrate how words can be used out of context, 
resulting in factually incorrect statements.  
Section 4 provides an overview of related work. 
In Section 5, we give a detailed description of 
how we generate the sentence summary 
statistically and how we use SVD to guide the 
generation process.  In Section 6, we present our 
experimental design in which we evaluated our 
approach, along with the results and 
corresponding discussion.  Finally, in Section 7, 
we present our conclusions and future work. 
2 The Veracity of Generated Summaries 
Berger and Mittal (2000) describe limitations to 
the generation of headlines by recycling words 
from the article.  One such limitation is that the 
proposition expressed by the generated summary 
is not guaranteed to reflect the information in the 
source text.  As an example, they present two 
sentences of differing meaning which uses the 
same words.   We present their example in 
Example 1, which illustrates the case in which 
the subject and object are swapped.   
The dog bit the postman 
The postman bit the dog. 
Example 1. An example of different propositions 
presented in two sentences which use the same 
words. 
However, we believe that the veracity of the 
generated sentence, with respect to the original 
document, is affected by a more basic problem 
than variation in word order.   Because words 
from any part of a source document can be 
combined probabilistically, there is a possibility 
that words can be used together out of context.  
We refer to this as Out-of-Context error.   Figure 
1 presents an example of a generated headline in 
which the adverb wrongly reports stock price 
movement.  It also presents the actual context in 
which that adverb was used. 
Generated headline 
?singapore stocks shares rebound?? 
 
Actual headline: 
?Singapore shares fall, seen higher after 
holidays.? 
 
Original context of use of ?rebound?: 
?Singapore shares closed down below the 
2,200 level on Tuesday but were expected to 
rebound immediately after Chinese Lunar 
New Year and Muslim Eid Al-Fitr holidays, 
dealers said.?
 
Figure 1.  An error in the generated headline due 
to a word being re-used out of context. 
Out-of-Context errors arise due to limitations in 
the two criteria for selecting words mentioned in 
Section 1.  While, for selection purposes, a word 
is scored according to its goodness as candidate 
summary word, word order is determined by a 
notion of grammaticality, modelled 
probabilistically using ngrams of lexemes.  
However, the semantic relationship implied by 
probabilistically placing two words next to each 
other, for example an adjective and a noun, 
might be suspect.  As the name ?Out-of-
Context? suggests, this is especially true if the 
words were originally used in non-contiguous 
and unrelated contexts.  This limitation in the 
word selection criteria can be characterized as 
being due to a lack of long distance relationship 
information. 
3 Our Approach to ?Encouraging Truth? 
In response to this limitation, we explore the use 
of a matrix operation, Singular Value 
Decomposition (SVD) to guide the selection of 
words.  Although our approach still does not 
guarantee factual correctness with respect to the 
source document, it has the potential to alleviate 
the Out-of-Context problem by improving the 
selection criteria of words for inclusion in the 
generated sentence, by considering the original 
contexts in which words were used.  With this 
improved criteria, we hope to "encourage truth" 
by incorporating long distance relationships 
between words.  Conceptually, SVD provides an 
analysis of the data which describes the 
relationship between the distribution of words 
and sentences.  This analysis includes a 
grouping of sentences based on similar word 
distributions, which correspond to what we will  
refer to here as the main themes of the 
document.1  By incorporating this information 
into the word selection criteria, the generated 
sentence will "gravitate" towards a single theme.  
That is, it will tend to use words from that 
theme, reducing the chance that words are 
placed together out of context.   
By reflecting the content of the main theme, the 
summary may be informative (Borko, 1975).  
That is, the primary piece of information within 
the source document might be included within 
the summary. However, it would remiss of us to 
claim that this quality of the summary is 
guaranteed.  In general, the generated summaries 
are at least useful to gauge what the source text 
is about, a characteristic described by Borko as 
being indicative.   
Figure 2 presents the generated summary using 
SVD for the same test article presented in Figure 
1.  In this case, the summary is informative as 
not only are we told that the article is about a 
stock market, but the movement in price in this 
example is correctly determined. 
Generated headline using SVD: 
?singapore shares fall? 
Figure 2. The headline generated using an SVD-
based word selection criterion.  The movement 
in share price is correct. 
4 Related Work 
As the focus of this paper is on statistical single-
sentence summarisation we will not focus on 
preceding work which generates summaries 
greater in length than a sentence.  We direct the 
reader to Paice (1990) for an overview of 
summarisation based on sentence extraction.  
Examples of recent systems include Kupiec et 
al. (1995) and Brandow et al (1995).    For 
examples of work in producing abstract-like 
summaries, see Radev and McKeown (1998), 
which combines work in information extraction 
                                                 
1
 Theme is a term that is used in many ways by many 
researchers, and generally without any kind of formal 
definition.  Our use of the term here is akin to the 
notion that underlies work on text segmentation, 
where sentences naturally cluster in terms of their 
?aboutness?. 
and natural language processing.  Hybrid 
methods for abstract-like summarisation which 
combine statistical and symbolic approaches 
have also been explored; see, for example, 
McKeown et al (1999), Jing and McKeown 
(1999), and Hovy and Lin (1997). 
Statistical single sentence summarisation has 
been explored by a number of researchers (see 
for example, Witbrock and Mittal, 1999; Zajic et 
al., 2002).  We build on the approach employed 
by Witbrock and Mittal (1999) which we will 
describe in more detail in Section 3.   
Interestingly, in the work of Witbrock and Mittal 
(1999), the selection of words for inclusion in 
the headline is decided solely on the basis of 
corpus statistics and does not use statistical 
information about the distribution of words in 
the document itself.  Our work differs in that we 
utilise an SVD analysis to provide information 
about the document to be summarized, 
specifically its main theme.    
Discourse segmentation for sentence extraction 
summarisation has been studied in work such as 
Boguraev and Neff (2000) and Gong and Liu 
(2001).  The motivation behind discovering 
segments in a text is that a sentence extraction 
summary should choose the most representative 
sentence for each segment, resulting in a 
comprehensive summary.  In the view of Gong 
and Liu (2001), segments form the main themes 
of a document.  They present a theme 
interpretation of the SVD analysis, as it is used 
for discourse segmentation, upon which our use 
of the technique is based.  However, Gong and 
Liu use SVD for creating sentence extraction 
summaries, not for generating a single sentence 
summary by re-using words. 
In subsequent work to Witbrock and Mittal 
(1999), Banko et al (2000) describe the use of 
information about the position of words within 
four quarters of the source document.  The 
headline candidacy score of a word is weighted 
by its position in one of quarters.  We interpret 
this use of position information as a means of 
guiding the generation of a headline towards the 
central theme of the document, which for news 
articles typically occurs in the first quarter.  
SVD potentially offers a more general 
mechanism for handling the discovery of the 
central themes and their positions within the 
document.   
Jin et al (2002) have also examined a statistical 
model for headlines in the context of an 
information retrieval application.  Jin and 
Hauptmann (2001) provide a comparison of a 
variety of learning approaches used by 
researchers for modelling the content of 
headlines including the Iterative Expectation-
Maximisation approach, the K-Nearest 
neighbours approach, a term vector approach 
and the approach of Witbrock and Mittal (1999).  
In this comparison, the approach of Witbrock 
and Mittal (1999) fares favourably, ranking 
second after the term vector approach to title 
word retrieval (see Jin and Hauptmann, 2001, 
for details).   However, while it performs well, 
the term vector approach Jin et al (2002) 
advocate doesn't explicitly try to model the way 
a headline will usually discuss the main theme 
and may thus be subject to the Out-of-Context 
problem. 
Finally, for completeness, we mention the work 
of Knight and Marcu (2000), who examine 
single sentence compression.  Like Witbrock 
and Mittal (1999), they couch summarisation as 
a noisy channel problem.  Under this framework, 
the summary is a noise-less source of 
information and the full text is the noisy result.  
However, in contrast to our approach, Knight 
and Marcu (2000) handle parse trees instead of 
the raw text.  Their system learns how to 
simplify parse trees of sentences extracted from 
the document to be summarized, to uncover the 
original noise-less forms. 
5 Generating a Single Sentence Summary 
In this section, we describe our approach to 
single sentence summarisation.  As mentioned 
earlier, our approach is based on that of 
Witbrock and Mittal (1999).  It differs in the 
way we score words for inclusion in the 
headline.  Section 5.1 presents our re-
implementation of Witbrock and Mittal?s (1999) 
framework and introduces the Content Selection 
strategy they employ.  Section 5.2 describes our 
extension using SVD resulting in two alternative 
Content Selection strategies.  
5.1 Searching for a Probable Headline 
We re-implemented the work described in 
Witbrock and Mittal (1999) to provide a single 
sentence summarisation mechanism.  For full 
details of their approach, we direct the reader to 
their paper (Witbrock and Mittal, 1999).  A brief 
overview of our implementation of their 
algorithm is presented here. 
Conceptually, the task is twofold.  First, the 
system must select n words from a news article 
that best reflect its content.  Second, the best 
(grammatical) word ordering of these n words 
must be determined.  Witbrock and Mittal 
(1999) label these two tasks as Content Selection 
and Realisation.  Each of these criteria are 
scored probabilistically, whereby the probability 
is estimated by prior collection of corpus 
statistics.   
To estimate Content Selection probability for 
each word, we use the Maximum Likelihood 
Estimate (MLE).  In an offline training stage, the 
system counts the number of times a word is 
used in a headline, with the condition that it 
occurs in the corresponding news article.  To 
form the probability, this frequency data is 
normalised by the number of times the word is 
used in articles across the whole corpus.  This 
particular strategy of content selection, we refer 
to this as the Conditional probability.   
The Realisation criterion is determined simply 
by the use of bigram statistics, which are again 
collected over a training corpus during the 
training stage.  The MLE of the probability of 
word sequences is calculated using these bigram 
statistics.  Bigrams model the grammaticality of 
a word given the preceding word that has 
already been chosen. 
It should be noted that both the Content 
Selection and Realisation criteria influence 
whether a word is selected for inclusion in the 
headline.  For example, a preposition might 
poorly reflect the content of a news article and 
score a low Content Selection probability.  
However, given the context of the preceding 
word, it may be the only likely choice. 
In both the training stage and the headline 
generation stage, the system employs the same 
preprocessing.  The preprocessing, which 
mirrors that used by Witbrock and Mittal (1999), 
replaces XML markup tags and punctuation 
(except apostrophes) with whitespace.  In 
addition, the remaining text is transformed into 
lower case to make string matching case 
insensitive.  The system performs tokenisation 
by using whitespace as a word delimiter. 
In Witbrock and Mittal?s approach (1999), the 
headline generation problem reduces to finding 
the most probable path through a bag of words 
provided by the source document, essentially a 
search problem.  They use the beam search 
variety of the Viterbi algorithm (Forney, 1973) 
to efficiently search for the headline.  In our 
implementation, we provided the path length as 
a parameter to this search mechanism.  In 
addition, we used a beam size of 20.   
To use the Viterbi algorithm to search for a path, 
the probability of adding a new word to an 
existing path is computed by combining the 
Content selection probability, the Realisation 
probability and the probability of the existing 
path, which is recursively defined. Combining 
each component probability is done by finding 
the logs of the probabilities and adding them 
together.  The Viterbi algorithm sorts the paths 
according to the path probabilities, directing the 
search towards the more probable word 
sequences first.  The use of repeated words in 
the path is not permitted. 
5.2 Using Singular Value Decomposition for 
Content Selection 
As an alternative to the Conditional probability, 
we examine the use of SVD in determining the 
Content Selection probability.  Before we 
outline the procedure for basing this probability 
on SVD, we will first outline our interpretation 
of the SVD analysis, based on that of Gong and 
Liu (2001).  Our description is not intended to 
be a comprehensive explanation of SVD, and we 
direct the reader to Manning and Sch?tze (2000) 
for a description of how SVD is used in 
information retrieval. 
Conceptually, when used to analyse documents, 
SVD can discover relationships between word 
co-occurrences in a collection of text.  For 
example, in the context of information retrieval, 
this provides one way to retrieve additional 
documents that contain synonyms of query 
terms, where synonymy is defined by similarity 
of word co-occurrences.  By discovering 
patterns in word co-occurrences, SVD also 
provides information that can be used to cluster 
documents based on similarity of themes. 
In the context of single document 
summarisation, we require SVD to cluster 
sentences based on similarities of themes.   The 
SVD analysis provides a number of related 
pieces of information relating to how words and 
sentences relate to these themes.  One such piece 
of information is a matrix of scores, indicating 
how representative the sentence is of each 
theme.  Thus, for a sentence extraction 
summary, Gong and Liu (2001) would pick the 
top n themes, and for each of these themes, use 
this matrix to choose the sentence that best 
represents it.   
For single sentence summarisation, we assume 
that the theme of the generated headline will 
match the most important theme of the article.  
The SVD analysis orders its presentation of 
themes starting with the one that accounts for 
the greatest variation between sentences.  The 
SVD analysis provides another matrix which 
scores how well each word relates to each 
theme.  Given a theme, scores for each word, 
contained in a column vector of the matrix, can 
then normalised to form a probability.  The 
remainder of this section provides a more 
technical description of how this is done. 
To begin with, we segment a text into sentences.  
Our sentence segmentation preprocessing is 
quite simple and based on the heuristics found in 
Manning and Sch?tze (2000).  After removing 
stopwords, we then form a terms by sentences 
matrix, A.  Each column of A represents a 
sentence.  Each row represents the usage of a 
word in various sentences. Thus the frequency 
of word t in sentence s is stored in the cell  Ats.  
This gives us an t * s matrix, where t ? s.  That 
is, we expect the lexicon size of a particular 
news article to exceed the number of sentences.   
For such a matrix, the SVD of A is a process 
that provides the right hand side of the following 
equation: 
A = U.S. Vtranspose  
where U is  a t * r matrix, S is an r * r matrix, 
and V is an s * r matrix.  The dimension size r is 
the rank of A, and is less than or equal to the 
number of columns of A, in this case, s.    The 
matrix S is a diagonal matrix with interesting 
properties, the most important of which is that 
the diagonal is sorted by size.  The diagonal 
values indicate the variation across sentences for 
a particular theme, where each theme is 
represented by a separate diagonal element.  The 
matrix V indicates how representative a sentence 
is of a score.  Similarly the matrix U indicates 
how related to the themes each word is.  A 
diagram of this is presented in Figure 3. 
Before describing how we use each of these 
matrices, it is useful to outline what SVD is 
doing geometrically.  Each sentence, a column 
in the matrix A, can be thought of as an object in 
t dimensional space.  SVD uncovers the 
relations between dimensions. For example, in 
the case of text analysis, it would discover 
relationships between words such as synonyms.  
In a trivial extreme of this case where two 
sentences differ only by a synonym, SVD would 
ideally discover that the two synonyms have 
very similar word co-occurrences.  In the 
analysis matrices of U, S and V, the redundant 
dimensions corresponding to these highly 
similar words might be removed, resulting in a 
reduced number of dimensions, r, required to 
represent the sentences.   
 
Figure 3.  A diagram of our interpretation of the 
SVD matrices as it relates to single sentence 
summarisation. 
Of the resulting matrices, V is an indication of 
how each sentence relates to each theme, 
indicated by a score.  Thus, following Gong and 
Liu (2001), a plausible candidate for the most 
important sentence is found by taking the first 
column vector of V (which has s elements), and 
finding the element with the highest value.  This 
sentence will be the one which is most 
representative of the theme.  The index of that 
element is the index of the sentence to extract.   
However, our aim is not to extract a sentence but 
to utilise the theme information.  The U matrix 
of the analysis provides information about how 
well words correspond to a particular theme.  
We examine the first column of the U matrix, 
sum the elements and then normalize each 
element by the sum to form a probability.  This 
probability, which we refer to as the SVD 
probability, is then used as the Content Selection 
probability in the Viterbi search algorithm. 
As an alternative to using the SVD probability 
and the Conditional Probability in isolation, a 
Combined Probability is calculated using the 
harmonic mean of the two.  The harmonic mean 
was used in case the two component 
probabilities differed consistently in their 
respective orders of magnitude.  Intuitively, 
when calculating a combined probability, this 
evens the importance of each component 
probability. 
To summarize, we end up with three alternative 
strategies in estimating the Content Selection 
Probability: the Conditional Probability, the 
SVD Probability and the Combined Probability. 
6 Experiments  
6.1 Data 
In our experiments, we attempted to match the 
experimental conditions of Witbrock and Mittal 
(1999).  We used news articles from the first six 
months of the Reuters 1997 corpus (Jan 1997 to 
June 1997).  Specifically, we only examined 
news articles from the general Reuters category 
(GCAT) which covers primarily politics, sport 
and economics.   This category was chosen not 
because of any particular domain coverage but 
because other categories exhibited frequent use 
of tabular presentation.  The GCAT category 
contains in excess of 65,000 articles.  Following 
Witbrock and Mittal (1999), we randomly 
selected 25,000 articles for training and a further 
1000 articles for testing, ensuring that there was 
no overlap between the two data sets.  During 
the training stage, we collected bigrams from the 
headline data, and the frequency of words 
occurring in headlines. 
6.2 Experiment Design 
We conducted an evaluation experiment to 
compare the performance of the three Content 
Selection strategies that we identified in Section 
5: the Conditional probability, the SVD 
probability, and the Combined probability.  We 
measure performance in terms of recall, i.e. how 
many of the words in the actual headline match 
words in the generated headline.2  The recall 
metric is normalised to form a percentage by 
dividing the word overlap by the number of 
words in the actual headline.   
For each test article, we generated headlines 
using each of the three strategies.  For each 
strategy, we generated headlines of varying 
lengths, ranging from length 1 to 13, where the 
latter is the length of the longest headline found 
in the test set.  We then compared the different 
strategies for generated headlines of equal 
length.   
To determine if differences in recall scores were 
significant, we used the Wilcoxon Matched Pairs 
Signed Ranks (WMPSR) test (Seigel and 
Castellan, 1988).  In our case, for a particular 
pair of Content Selection strategies, the alternate 
hypothesis was that the choice of Content 
Selection strategy affects recall performance.  
The null hypothesis held that there was no 
difference between the two content selection 
strategies.  Our use of the non-parametric test 
was motivated by the observation that recall 
scores were not normally distributed.  In fact, 
our results showed a positive skew for recall 
scores.  To begin with, we compared the recall 
scores of the SVD strategy and the Conditional 
strategy in one evaluation.  The strategy that was 
found to perform better was then compared with 
the Combined strategy. 
                                                 
2
 Word overlap, whilst the easiest way to evaluate the 
summaries quantitatively, is an imprecise measure 
and must be interpreted with the knowledge that non-
recall words in the generated headline might still 
indicate clearly what the source document is about. 
In addition to the recall tests, we conducted an 
analysis to determine the extent to which the 
SVD strategy and the Conditional probability 
strategy were in agreement about which words 
to select for inclusion in the generated headline.  
For this analysis, we ignored the bigram 
probability of the Realisation component and 
just measured the agreement between the top n 
ranking words selected by each content selection 
strategy.  Over the test set, we counted how 
many words were selected by both strategies, 
just one strategy, and no strategies.  By 
normalising scores by the number of test cases, 
we determine the average agreement across the 
test set.  We ran this experiment for a range of 
different values of N, ranging from 1 to 13, the 
length of the longest headline in the test set.   
6.3 Results 
6.3.1 Recall Comparison 
The results for the comparison of recall scores 
are presented in Table 1 and Table 2.  Table 1 
shows results of the WMPSR test when 
comparing the SVD strategy with the 
Conditional strategy.3  Since the Conditional 
strategy was found to perform better, we then 
compared this with the Combined strategy, as 
shown in Table 2.  From Table 1, it is clear that, 
for all sentence lengths, there is a significant 
difference between the SVD strategy and the 
Conditional strategy, and so we reject the null 
hypothesis.  Similarly, Table 2 shows that there 
is a significant difference between the 
Conditional strategy and the Combined strategy, 
and again we reject the null hypothesis. We 
conclude that SVD probability alone is 
outperformed by the Conditional probability; 
however, using both probabilities together leads 
to a better performance. 
 
 
 
 
                                                 
3
 The performance of our Conditional strategy is 
roughly comparable to the results obtained by Banko, 
Mittal and Witbrock (2000), in which they report 
recall scores between 20% to 25%, depending on the 
length of the generated headline.   
Sentence 
Length 
Average 
Recall : 
SVD 
Average 
Recall : 
Cond. Probability 
Reject  
H0 
1 03.68% 03.98% p ? 0.0 yes 
2 07.02% 06.97% p ?  0.5 yes 
3 10.05% 11.44% p ? 0.0 yes 
4 12.39% 13.90% p ? 0.0 yes 
5 14.21% 15.73% p ?0.0 yes 
6 15.57% 17.84% p ?1.1e-05 yes 
7 16.59% 19.14% p ? 1.8e-07 yes 
8 17.74% 20.30% p ? 1.3e-07 yes 
9 18.74% 21.33% p ? 1.3e-06 yes 
10 19.73% 22.44% p ? 1.0e-06 yes 
11 20.19% 23.50% p ? 2.2e-10 yes 
12 20.85% 24.54% p ? 4.4e-13 yes 
13 21.13% 25.13% p ? 1.4e-12 yes 
Table 1. A comparison of recall scores for the 
SVD strategy and the Conditional strategy. 
Sentence 
Length 
Average 
Recall : 
Cond 
Average 
Recall :  
Combined Probability 
Reject  
H0 
1 03.98% 04.05% p ? 0.1305 yes 
2 06.97% 08.60% p ? 2.8e-13 yes 
3 11.44% 12.34% p ? 0.0007 yes 
4 13.90% 15.44% p ? 8.5e-09 yes 
5 15.73% 17.33% p ? 1.9e-09 yes 
6 17.84% 18.72% p ? 0.0003 yes 
7 19.14% 20.34% p ? 1.3e-05 yes 
8 20.30% 21.48% p ? 2.9e-06 yes 
9 21.33% 22.60% p ? 4.0e-06 yes 
10 22.44% 23.82% p ? 1.2e-06 yes 
11 23.50% 24.56% p ? 0.0003 yes 
12 24.54% 25.44% p ? 0.0008 yes 
13 25.13% 26.37% p ? 8.6e-06 yes 
Table 2. A comparison of recall scores for the 
Conditional strategy and the Combined strategy.   
6.3.2 Agreement between Strategies 
The agreement between strategies is presented in 
Table 3.  Interestingly, of the words recalled, the 
majority have only been selected by one content 
selection strategy.  That is, the set of words 
recalled by one content selection strategy do not 
necessarily subsume the set recalled by the 
other.  This supports the results obtained in the 
recall comparison in which a combined strategy 
leads to higher recall.  Interestingly, the last 
column in the table shows that the potential 
combined recall is greater than the recall 
achieved by the combined strategy; we will 
return to this point in Section 6.4. 
 
 
 
 
Sentence 
Length 
Selected 
by neither 
method 
Selected by 
only 1 
method 
Selected 
by both 
methods 
Total 
Recall 
1 91.6% 8.0% 0.3% 8.3% 
2 84.7% 14.1% 1.0% 15.1% 
3 79.9% 17.5% 2.5% 20.0% 
4 76.6% 19.3% 3.9% 23.2% 
5 73.8% 21.0% 5.1% 26.1% 
6 71.4% 22.1% 6.4% 28.5% 
7 69.6% 22.4% 7.8% 30.2% 
8 67.9% 22.9% 9.1% 32.0% 
9 66.4% 23.2% 12.3% 35.5% 
10 65.0% 23.5% 11.3% 34.8% 
11 63.9% 23.6% 12.3% 35.9% 
12 63.0% 23.6% 13.2% 36.8% 
13 62.1% 23.5% 14.3% 37.8% 
Table 3.  Agreement of words chosen between 
the SVD strategy and the Conditional 
probability strategy to content selection 
6.4 Discussion 
The SVD strategy ultimately did not perform as 
well ass we might have hoped.  There are a 
number of possible reasons for this. 
1. Whilst using the Combined probability did 
lead to a significantly improved result, this 
increase in recall was only small.  Indeed, 
the analysis of the agreement between the 
Conditional strategy and the SVD strategy 
indicates that the current method of 
combining the two probabilities is not 
optimal and that there is still considerable 
margin for improvement. 
2. Even though the recall of the SVD strategy 
was poorer by a only a few percent, the lack 
of improvement in recall is perplexing, 
given that we expected the thematic 
information to ensure words were used in 
correct contexts. There are several possible 
explanations, each warranting further 
investigation.  It may be the case that the 
themes identified by the SVD analysis were 
quite narrow, each encompassing only small 
number of sentences.  If this is the case, 
certain words occurring in sentences outside 
the theme would be given a lower 
probability even if they were good headline 
word candidates.  Further investigation is 
necessary to determine if this is a short-
coming of our SVD strategy or an artefact of 
the domain.  For example, it might be the 
case that the sentences of news articles are 
already thematically quite dissimilar.   
3. One might also question our experimental 
design.  Perhaps the kind of improvement 
brought about when using the SVD 
probability cannot be measured by simply 
counting recall.  Instead, it may be the case 
that an evaluation involving a panel of 
judges is required to determine if the 
generated text is qualitatively better in terms 
of how faithful the summary is to the 
information in the source document.  For 
example, a summary that is more accurate 
may not necessarily result in better recall.  
Finally, it is conceivable that the SVD 
strategy might be more sensitive to 
preprocessing stages such as sentence 
delimitation and stopword lists, which are 
not necessary when using the Conditional 
strategy.  
Despite these outstanding questions, there are 
pragmatic benefits when using SVD.  The 
conditional strategy requires a paired training set 
of summaries and source documents.  In our 
case, this was easily obtained by using headlines 
in lieu of single sentence summaries.  However, 
in cases where a paired corpus is not available 
for training, the SVD strategy might be more 
appropriate, given that the performance does not 
differ considerably.  In such a situation, a 
collection of documents is only necessary for 
collecting bigram statistics. 
7 Conclusion 
Combining both the SVD probability and 
Conditional probability marginally improves 
recall, lending support to the intuition that 
thematic information may help generate better 
single sentence summaries.  However, there are 
still many unanswered questions.  In future 
work, we intend to investigate these techniques 
in a domain other than news text so that we can 
draw conclusions as to how well these strategies 
generalise to other genres.  We also intend to 
conduct user evaluations to gauge the quality of 
the generated summaries for both the 
Conditional and the SVD strategies.  Indeed, a 
user-based evaluation would be extremely 
helpful in determining if the thematic 
information provided by the SVD strategy does 
help improve the veracity of the generated 
summaries.    
References  
Banko M., Mittal V., and Witbrock M. (2000) 
Headline generation based on statistical translation. 
In Proceedings of the 38th Annual Meeting of the 
Association for Computational Linguistics. 
Boguraev B., and Neff M. (2000) Discourse 
segmentation in aid of document summarization. In 
Proceedings of the Hawaii International 
Conference on System Sciences (HICSS- 33), 
Minitrack on Digital Documents Understanding. 
Maui, Hawaii: IEEE. 
Borko, H., and Bernier, C. (1975) Abstracting 
Concepts and Methods. New York: Academic 
Press. 
Brandow, R., Mitze, K., and Rau, L. (1995) 
Automatic condensation of electronic publications 
by sentence selection. In Information Processing 
and Management, 31(5), pages 675-685. 
Forney G. D. (1973) The Viterbi Algorithm.  In the 
Proceedings of the IEEE, pages 268-278. 
Gong Y., and Liu, X. (2001) Generic Text 
Summarization Using Relevance Measure and 
Latent Semantic Analysis. In the Proceedings 
SIGIR 2001: pages 19-25. 
Hovy, E. and Lin, C. (1997) Automated text 
summarization in SUMMARIST.  In the 
Proceedings of ACL-EACL?97 Workshop on 
Intelligent Scalable Text Summarization, pages 18-
24. 
Jin, R., and Hauptmann, A. (2001) Learning to Select 
Good Title Words: An New Approach based on 
Reversed Information Retrieval.  In the 
Proceedings of the Eighteen International 
Conference on Machine Learning (ICML 2001), 
Williams College,MA, June 28-July 1. 
Jin, R., Zhai, C., and Hauptmann, A. (2002) Title 
language model for information retrieval. In the 
Proceedings of the 25th Annual International ACM 
SIGIR Conference on Research and Development 
in Information Retrieval (SIGIR 2002), Tampere, 
Finland, August 11-15. 
Jing, H., and McKeown, K. (1999) The 
decomposition of human-written summary 
sentences. In the Proceedings of the 22nd 
Conference on Research and Development in 
Information Retrieval (SIGIR--99).  
Knight, K. and Marcu, D. (2000) Statistics-based 
summarization---Step one: Sentence compression. 
In Proceedings of AAAI-2000. 
Kupiec, J., Pedersen, J., and Chen, F. (1995) A 
Trainable Document Summarizer. In Proceedings 
of the 18th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval. Fox, E., Ingwersen, P., and 
Fidel, R. (Editors), pages 68?73. 
Manning C. and Sch?tze H. (2000) Foundations of 
Statistical Natural Language Processing.  MIT 
Press: Cambridge MA. 
Marcu, D. (2000) The Theory and Practice of 
Discourse Parsing and Summarization. 
Cambridge: The MIT Press. 
McKeown, K., Klavans, J., Hatzivassiloglou, V., 
Barzilay, R., and Eskin, E. (1999) Towards 
multidocument summarization by reformulation: 
Progress and prospects. In the Proceedings of the 
Sixteenth National Conference on Artificial 
Intelligence (AAAI--99). 
Paice, C. (1990) Constructing Literature Abstracts by 
Computers: Techniques and Prospects.  In 
Information Processing and Management, Vol. 26, 
No. 1, pages 171?186. 
Radev, D. and McKeown, K. (1998) Generating 
natural language summaries from multiple on-line 
sources. Computational Linguistics, 24(3):469-500, 
September.  
Siegel, Sidney and Castellan, Jr. N. John. (1988) 
Nonparametric Statistics For The Behavioral 
Sciences. McGraw-Hill, Inc., second edition. 
Witbrock, M., and Mittal, V. (1999) 
Ultrasummarization: A statistical approach to 
generating highly condensed non-extractive 
summaries. In the Proceedings of the 22nd 
International Conference on Research and 
Development in Information Retrieval (SIGIR '99). 
Zajic D., Door B., and Schwartz R. (2002) Automatic 
Headline Generation for Newspaper Stories. In the 
Proceedings of the Document Understanding 
Conference (DUC 2002). 
Searching for Grammaticality: Propagating Dependencies in the Viterbi
Algorithm
Stephen Wan12 Robert Dale1 Mark Dras1
1Centre for Language Technology
Div. of Information Communication Sciences
Macquarie University
Sydney, Australia
swan,rdale,madras@ics.mq.edu.au
Ce?cile Paris2
2Information and Communication
Technologies
CSIRO
Sydney, Australia
Cecile.Paris@csiro.au
Abstract
In many text-to-text generation scenarios (for in-
stance, summarisation), we encounter human-
authored sentences that could be composed by re-
cycling portions of related sentences to form new
sentences. In this paper, we couch the generation
of such sentences as a search problem. We in-
vestigate a statistical sentence generation method
which recombines words to form new sentences.
We propose an extension to the Viterbi algorithm
designed to improve the grammaticality of gener-
ated sentences. Within a statistical framework, the
extension favours those partially generated strings
with a probable dependency tree structure. Our
preliminary evaluations show that our approach
generates less fragmented text than a bigram base-
line.
1 Introduction
In abstract-like automatic summary generation, we often re-
quire the generation of new and previously unseen summary
sentences given some closely related sentences from a source
text. We refer to these as Non-Verbatim Sentences. These
sentences are used instead of extracted sentences for a variety
of reasons including improved conciseness and coherence.
The need for a mechanism to generate such sentences is
supported by recent work showing that sentence extraction is
not sufficient to account for the scope of written human sum-
maries. Jing and McKeown [1999] found that only 42% of
sentences from summaries of news text were extracted sen-
tences. This is also supported by the work of Knight and
Marcu [2002] (cited by [Daume? III and Marcu, 2004]), which
finds that only 2.7% of human summary sentences are ex-
tracts. In our own work on United Nations Humanitarian
Aid Proposals, we noticed that only 30% of sentences are
extracted from the source document, either verbatim or with
trivial string replacements.
While the figures do vary, it shows that additional mecha-
nisms beyond sentence extraction are needed. In response to
this, our general research problem is one in which given a set
of related sentences, a single summary sentence is produced
by recycling words from the input sentence set.
In this paper, we adopt a statistical technique to allow easy
portability across domains. The Viterbi algorithm [Forney,
1973] is used to search for the best traversal of a network of
words, effectively searching through the sea of possible word
sequences. We modify the algorithm so that it narrows the
search space not only to those sequences with a semblance of
grammaticality (via n-grams), but further still to those that are
grammatical sentences preserving the dependency structure
found in the source material.
Consider the following ungrammatical word sequence typ-
ical of that produced by an n-gram generator, ?The quick
brown fox jumped over the lazy dog slept by the log ?. One
diagnosis of the problem is that the word dog is also used
as the subject of the second verb slept. Ideally, we want to
avoid such sequences since they introduce text fragments, in
this case ?slept by the log?. We could, for example, record
the fact that dog is already governed by the verb jumped, and
thus avoid appending a second governing word slept.
To do so, our extension propagates dependency features
during the search and uses these features to influence the
choice of words suitable for appending to a partially gener-
ated sentence. Dependency relations are derived from shal-
low syntactic dependency structure [Kittredge and Mel?cuk,
1983]. Specifically, we use representations of relations be-
tween a head and modifier, ignoring relationship labels for
the present.
Within the search process, we constrain the choice of fu-
ture words by preferring words that are likely to attach to
the dependency structure of the partially generated sentence.
Thus, sequences with plausible structures are ranked higher.
The remainder of the paper is structured as follows. In Sec-
tion 2, we describe the problem in detail and our approach.
We outline our use of the Viterbi algorithm in Section 3. In
Section 4, we describe how this is extended to cater for de-
pendency features. We compare related research in Section 5.
A preliminary evaluation is presented and discussed in Sec-
tion 6. Finally, we conclude with future work in Section 7.
2 Narrowing the Search Space: A Description
of the Statistical Sentence Generation
Problem
In this work, sentence generation is essentially a search for
the most probable sequence of words, given some source text.
However, this constitutes an enormous space which requires
efficient searching. Whilst reducing a vocabulary to a suit-
able subset narrows this space somewhat, we can use statis-
tical models, representing properties of language, to prune
the search space of word sequences further to those strings
that reflect real language usage. For example, n-gram models
limit the word sequences examined to those that seem gram-
matically correct, at least for small windows of text.
However, n-grams alone often result in sentences that,
whilst near-grammatical, are often just gibberish. When com-
bined with a (word) content selection model, we narrow the
search space even further to those sentences that appear to
make sense. Accordingly, approaches such as Witbrock and
Mittal [1999] and Wan et al [2003] have investigated models
that improve the choice of words in the sentence. Witbrock
and Mittal?s content model chooses words that make good
headlines, whilst that of Wan et al attempts to ensure that,
given a short document like a news article, only words from
sentences of the same subtopic are combined to form a new
sentences. In this paper, we narrow the search space to those
sequences that conserve dependency structures from within
the input text.
Our algorithm extension essentially passes along the long-
distance context of dependency head information of the pre-
ceding word sequence, in order to influence the choice of the
next word appended to the sentence. This dependency struc-
ture is constructed statistically by an O(n) algorithm, which
is folded into the Viterbi algorithm. Thus, the extension is
in an O(n4) algorithm. The use of dependency relations fur-
ther constrains the search space. Competing paths through
the search space are ranked taking into account the proposed
dependency structures of the partially generated word se-
quences. Sentences with probable dependency structures are
ranked higher. To model the probability of a dependency re-
lation, we use the statistical dependency models inspired by
those described in Collins [1996].
3 Using The Viterbi Algorithm for Sentence
Generation
We assume that the reader is familiar with the Viterbi al-
gorithm. The interested reader is referred to Manning and
Schutze [1999] for a more complete description. Here, we
summarise our re-implementation (described in [Wan et al,
2003]) of the Viterbi algorithm for summary sentence gener-
ation, as first introduced by Witbrock and Mittal [1999].
In this work, we begin with a Hidden Markov Model
(HMM) where the nodes (ie, states) of the graph are uniquely
labelled with words from a relevant vocabulary. To obtain a
suitable subset of the vocabulary, words are taken from a set
of related sentences, such as those that might occur in a news
article (as is the case for the original work by Witbrock and
Mittal). In this work, we use the clusters of event related sen-
tences from the Information Fusion work by Barzilay et al
[1999]. The edges between nodes in the HMM are typically
weighted using bigram probabilities extracted from a related
corpus.
The three probabilities of the unmodified Viterbi algorithm
are defined as follows:
Transition Probability (using the Maximum Likelihood Esti-
mate to model bigram probabilities)1:
ptrngram (wi+1|wi) =
count(wi, wi+1)
count(wi)
Emission Probability: (For the purposes of testing the new
transition probability function described in Section 4, this is
set to 1 in this paper):
pem(w) = 1
Path Probability is defined recursively as:
ppath(w0, . . . , wi+1) =
ptrngram (wi+1|wi)? pem(w)? ppath(w0 . . . wi)
The unmodified Viterbi algorithm as outlined here would
generate word sequences just using a bigram model. As noted
above, such sequences will often be ungrammatical.
4 A Mechanism for Propagating Dependency
Features in the Extended Viterbi Algorithm
In our extension, we modify the definition of the Transition
Probability such that not only do we consider bigram prob-
abilities but also dependency-based transition probabilities.
Examining the dependency head of the preceding string then
allows us to consider long-distance context when append-
ing a new word. The algorithm ranks highly those words
with a plausible dependency relation to the preceding string,
with respect to the source text being generated from (or sum-
marised).
However, instead of considering just the likelihood of a
dependency relation between adjacent pairs of words, we can
consider the likelihood of a word attaching to the dependency
tree structure of the partially generated sentence. Specifically,
it is the rightmost root-to-leaf branch that can still be modified
or governed by the appending of a new word to the string.
This rightmost branch is stored as a stack. It is updated and
propagated to the end of the path each time we add a word.
Thus, our extension has two components: Dependency
Transition and Head Stack Reduction. Aside from these mod-
ifications, the Viterbi algorithm remains the same.
In the remaining subsections, we describe in detail how the
dependency relations are computed and how the stack is re-
duced. In Figure 3, we present pseudo-code for the extended
Viterbi algorithm.
4.1 Scoring a Dependency Transition
Dependency Parse Preprocessing of Source Text
The Dependency Transition is simply an additional weight on
the HMM edge. The transition probability is the average of
the two transition weights based on bigrams and dependen-
cies:
ptr(wi+1|w1) =
average(ptrngram (wi+1|w1), ptrdep (wi+1|w1))
Before we begin the generation process, we first use a depen-
dency parser to parse all the sentences from the source text to
1Here the subscripts refer to the fact that this is a transition prob-
ability based on n-grams. We will later propose an alternative using
dependency transitions.
obtain dependency trees. A traversal of each dependency tree
yields all parent-child relationships, and we update an adja-
cency matrix of connectivity accordingly. Because the status
of a word as a head or modifier depends on the word order in
English, we consider relative word positions to determine if
a relation has a forward or backward2 direction. Forward and
backward directional relations are stored in separate matri-
ces. The Forward matrix stores relations in which the head is
to the right of modifier in the sentence. Conversely, the Back-
ward matrix stores relations in the head to left of the modifier.
This distinction is required later in the stack reduction step.
As an example, given the two strings (using characters
in lieu of words) ?d b e a c? and ?b e d c a? and the
corresponding dependency trees:
a
b
d e
c
c
d
b
e
a
we obtain the following adjacency matrices:
Forward (or Right-Direction) Adjacency Matrix
?
?
?
?
0 a b c d e
a 0 1 0 0 0
b 0 0 0 1 0
c 0 0 0 1 0
d 0 1 0 0 0
e 0 0 0 0 0
?
?
?
?
Backward (or Left-Direction) Adjacency Matrix
?
?
?
?
0 a b c d e
a 0 0 1 0 0
b 0 0 0 0 2
c 1 0 0 0 0
d 0 0 0 0 0
e 0 0 0 0 0
?
?
?
?
We refer to the matrices as Adjright and Adjleft respectively.The cell value in each matrix indicates the number of times
word i (that is, the row index) governs word j (that is, the
column index).
Computing the Dependency Transition Probability
We define the Dependency Transition weight as:
ptrdep (wi+1|wi) =
p(Depsym(wi+1, headStack(wi))
where Depsym is the symmetric relation stating that some
dependency relation occurs between a word and any of the
words in the stack, irrespective of which is the head. Intu-
itively, the stack is a compressed representation of the depen-
dency tree corresponding to the preceding words. The prob-
ability indicates how likely it is that the new word can attach
itself to this incrementally built dependency tree, either as a
modifier or a governer. Since the stack is cumulatively passed
on at each point, we need only consider the stack stored at the
preceding word.
This is estimated as follows:
p(Depsym(wi+1, headStack(wi))) =
max
h?headStack(wi)
p(Depsym(wi+1, h))
2These are defined analogously to similar concepts in Combina-
tory Categorial Grammar [Steedman, 2000].
the quick brown fox jumps
jumps over the lazy dog .
[
the
]
[
quick
the
]
[
brown
quick
the
]
[
fox
]
[
jumps
]
[
over
jumps
]
[
the
over
jumps
]
[ lazy
the
over
jumps
]
[
dog
over
jumps
]
Figure 1: A path through a lattice. Although separated on
two lines, it represents a single sequence of words. The stack
(oriented upwards) grows and shrinks as we add words. Note
that the modifiers to dog are popped off before it is pushed on.
Note also that modifiers of existing items on the stack, such
as over are merely pushed on. Words with no connection to
previously seen stack items are also pushed on (eg. quick) in
the hope that a head will be found later.
Here, we assume that a word can only attach to the tree
once at a single node; hence, we find the node that max-
imises the probability of node attachment. The relation-
ship Depsym(a, b) is modelled using a simplified version of
Collins? [1996] dependency model.
Because of the status of word as the head relies on thepreservation of word order, we keep track of the direction-
ality of a relation. For two words a and b where a precedes b
in the generated string,
p(Depsym(a, b)) ?
Adjright(a, b) + Adjleft(b, a)
cnt(co-occur(a, b))
where Adjright and Adjleft are the right and left adjacencymatrices. Recall that row indices are heads and column in-
dices are modifiers.
4.2 Head Stack Reduction
Once we decide that a newly considered path is better than
any other previously considered one, we update the head
stack to represent the extended path. At any point in time,
the stack represents the rightmost root-to-leaf branch of the
dependency tree (for the generated sentence) that can still
be modified or governed by concatenating new words to the
string.3 Within the stack, older words may be modified by
newer words. Our rules for modifying the stack are designed
to cater for a projective4 dependency grammar.
There are three possible alternative outcomes of the reduc-
tion. The first is that the proposed top-of-stack (ToS) has
no dependency relation to any of the existing stack items, in
which case the stack remains unchanged. For the second and
third cases, we check each item on the stack and keep a record
3Note that we can scan through the stack as well as push onto
and pop from the top; this is thus the same type of stack as used in,
for example, Nested Stack Automata.
4That is, if wi depends on wj , all words in between wi and wj
are also dependent on wj .
reduceHeadStack(aNode, aStack) returns aStack
Nodenew ?aNode
Stack ?aStack # duplicate
Nodemax ?NULL
Edgeprob ?0
# Find best chunk
While notEmpty(aStack)
Head ?pop(aStack)
if p(depsym (Nodenew, Head)) > Edgeprob
Nodemax ?Head
Edgeprob ?depsym(Nodenew, Head)
# Keep only best chunk
While top(aStack) 6= Nodemax
pop(aStack)
# Determine new head of existing string
if isReduced(Nodenew,Nodemax)
pop(aStack)
else
push(Nodenew, aStack)
Figure 2: Pseudocode for the Head Stack Reduction operation
only of the best probable dependency between the proposed
ToS and the appropriate stack item. The second outcome,
then, is that the proposed ToS is the head of some item on
the stack. All items up to and including that stack item are
popped off and the proposed ToS is pushed on. The third out-
come is that it modifies some item on the stack. All stackitems up to (but not including) the stack item are popped off
and the proposed ToS is pushed on. The pseudocode is pre-
sented in Figure 2. An example of stack manipulation is pre-
sented in Figure 1. We rely on two external functions. The
first function, depsym/2, has already been presented above.
The second function, isReduced/2, relies on an auxiliary
function returning the probability of one word being governed
by the other, given the relative order of the words. This is in
essence our parsing step, determining which word governs
the other. The function is defined as follows:
isReduced(w1, w2) =
p(isHeadRight(w1, w2)) > p(isHeadLeft(w1, w2))
where w1 precedes w2, and:
p(isHeadRight(w1, w2))
? Adjright(w1, w2)
cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))
and similarly,
p(isHeadLeft(w1, w2))
?
Adjleft(w2, w1)
cnt(hasRelation(w1, w2, wherei(w1) < i(w2)))
where hasRelation/2 is the number of times we see
the two words in a dependency relation, and where i(wi)
returns a word position in the corpus sentence. The
function isReduced/2 makes calls to p(isHeadRight/2)
andp(isHeadLeft/2). It returns true if the first parameter
viterbiSearch(maxLength, stateGraph) returns bestPath
numStates ?getNumStates(stateGraph)
viterbi ?a matrix[numStates+2,maxLength+2]
viterbi[0,0].score ?1.0
for each time step t from 0 to maxLength do
# Termination Condition
if ((viterbi[endState, t].score 6= 0)
AND isAcceptable(endState.headStack))
# Backtrace from endState and return path
# Continue appending words
for each state s from 0 to numStates do
for each transition s? from s
newScore ?
viterbi[s,t].score ? ptr(s?|s) ? pem(s?)
if ((viterbi[s?,t+1].score = 0) OR
(newScore > viterbi[s?, t+1]))
viterbi[s?,t+1].score ?newScore
viterbi[s?,t+1].headStack ?
reduceHeadStack(s?,viterbi[s,t].headStack)
backPointer[s?,t+1] ?s
Backtrace from viterbi[endState,t] and return path
Figure 3: Extended Viterbi Algorithm
is the head of the second, and false otherwise. In the com-
parison, the denominator is constant. We thus need only the
numerator in these auxiliary functions.
Collins? distance heuristics [1996] weight the probability
of a dependency relation between two words based on the
distance between them. We could implement a similar strat-
egy by favouring small reductions in the head stack. Thus a
reduction with a more recent stack item which is closer to the
proposed ToS would be less penalised than an older one.
5 Related Work
There is a wealth of relevant research related to sentence gen-
eration. We focus here on a discussion of related work from
statistical sentence generation and from summarisation.
In recent years, there has been a steady stream of research
in statistical text generation. We focus here on work which
generates sentences from some sentential semantic represen-
tation via a statistical method. For examples of related sta-
tistical sentence generators see Langkilde and Knight [1998]
and Bangalore and Rambow [2000]. These approaches be-
gin with a representation of sentence semantics that closely
resembles that of a dependency tree. This semantic represen-
tation is turned into a word lattice. By ranking all traversals
of this lattice using an n-gram model, the best surface realisa-
tion of the semantic representation is chosen. The system then
searches for the best path through this lattice. Our approach
differs in that we do not start with a semantic representation.
Instead, we paraphrase the original text. We search for the
best word sequence and dependency tree structure concur-
rently.
Research in summarisation has also addressed the prob-
lem of generating non-verbatim sentences; see [Jing and
McKeown, 1999], [Barzilay et al, 1999] and more recently
[Daume? III and Marcu, 2004]. Jing presented a HMM for
learning alignments between summary and source sentences
trained using examples of summary sentences generated by
humans. Daume III also provides a mechanism for sub-
sentential alignment but allows for alignments between multi-
ple sentences. Both these approaches provide models for later
recombining sentence fragments. Our work differs primarily
in granularity. Using words as a basic unit potentially offers
greater flexibility in pseudo-paraphrase generation since we
able to modify the word sequence within the phrase.
It should be noted, however, that a successful execution of
our algorithm is likely to conserve constituent structure (ie. a
coarser granularity) via the use of dependencies, whilst still
making available a flexibility at the word level. Addition-
ally, our use of dependencies allows us to generate not only a
string but a dependency tree for that sentence.
6 Evaluation
In this section, we outline our preliminary evaluation of gram-
maticality in which we compare our dependency based gener-
ation method against a baseline. To study any improvements
in grammaticality, we compare our dependency based gener-
ation method against a baseline consisting of sentences gen-
erated using bigram model.
In the evaluation, we do not use any smoothing algorithms
for dependency counts. For both our approach and the base-
line, Katz?s Back-off smoothing algorithm is used for bigram
probabilities.
For our evaluation cases, we use the Information Fusion
data collected by [Barzilay et al, 1999]. This data is made
up of news articles that have been first grouped by topic,
and then component sentences further clustered by similar-
ity of events. There are 100 sentence clusters and on average
there are 4 sentences per cluster. Each sentence in the cluster
is initially passed through the Connexor dependency parser
(www.connexor.com) to obtain dependency relations. Each
sentence cluster forms an evaluation case in which we gener-
ate a single sentence. Example output and the original text of
the cluster is presented in Figure 4.
To give both our approach and the baseline the greatest
chance of generating a sentence, we obtain our bigrams from
our evaluation cases.5 Aside from this preprocessing to col-
lect input sentence bigrams and dependencies, there is no
training as such. For each evaluation case, both our system
and the baseline method generates a set of answer strings,
from 3 to 40 words in length.
For each generated output of a given sentence length, we
count the number of times the Connexor parser resorts to re-
turning partial parses. This count, albeit a noisy one, is used
as our measure of ungrammaticality. We calculate the aver-
age ungrammaticality score across evaluation cases for each
sentence length.
5Note that this is permissible in this case because we are not
making any claims about the coverage of our model.
Original Text
A military transporter was scheduled to take off in the afternoon from Yokota air base
on the outskirts of Tokyo and fly to Osaka with 37,000 blankets .
Mondale said the United States, which has been flying in blankets and is sending a
team of quake relief experts, was prepared to do more if Japan requested .
United States forces based in Japan will take blankets to help earthquake survivors
Thursday, in the U.S. military?s first disaster relief operation in Japan since it set up
bases here.
Our approach with Dependencies and End of Sentence Check
6: united states forces based in blankets
8: united states which has been flying in blankets
11: a military transporter was prepared to osaka with 37,000 blankets
18: mondale said the afternoon from yokota air base on the united states which has been
flying in blankets
20: mondale said the outskirts of tokyo and is sending a military transporter was pre-
pared to osaka with 37,000 blankets
23: united states forces based in the afternoon from yokota air base on the outskirts of
tokyo and fly to osaka with 37,000 blankets
27: mondale said the afternoon from yokota air base on the outskirts of tokyo and is
sending a military transporter was prepared to osaka with 37,000 blankets
29: united states which has been flying in the afternoon from yokota air base on the
outskirts of tokyo and is sending a team of quake relief operation in blankets
31: united states which has been flying in the afternoon from yokota air base on the out-
skirts of tokyo and is sending a military transporter was prepared to osaka with 37,000
blankets
34: mondale said the afternoon from yokota air base on the united states which has
been flying in the outskirts of tokyo and is sending a military transporter was prepared
to osaka with 37,000 blankets
36: united states which has been flying in japan will take off in the afternoon from
yokota air base on the outskirts of tokyo and is sending a military transporter was pre-
pared to osaka with 37,000 blankets
Figure 4: A cluster of related sentences and sample generated
output from our system. Leftmost numbers indicate sentence
length.
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38
Un
gr
am
m
at
ica
lity
 S
co
re
Sentence Length
Ungrammaticality Errors across Sentence Lengths
Baseline
System
Figure 5: Ungrammaticality scores for generated output.
Higher scores indicates worse performance.
The results are presented in Figure 5. Our approach almost
always performs better than the baseline, producing less er-
rors per sentence length. Using the Wilcoxon Signed Rank
Text (alpha = 0.5), we found that for sentences of length
greater than 12, the differences were usually significant.
7 Conclusion and Future Work
In this paper, we presented an extension to the Viterbi al-
gorithm that statistically determines dependency structure of
partially generated sentences and selects of words that are
likely to attach to this structure. The resulting sentence is
more grammatical than that generated using a bigram base-
line. In future work, we intend to conduct experiments to see
whether the smoothing approaches chosen are successful in
parsing without introducing spurious dependency relations.
We would also like to re-integrate the emission probability
(that is, the word content selection model). We are also in
the process of developing a measure of consistency. Finally,
we intend to provide a comparison evaluation with Barzilay?s
Information Fusion work.
8 Acknowledgements
This work was funded by the Centre for Language Technol-
ogy at Macquarie University and the CSIRO Information and
Communication Technology Centre. We would like to thank
the research groups of both organisations for useful com-
ments and feedback.
References
[Bangalore and Rambow, 2000] Srinivas Bangalore and
Owen Rambow. Exploiting a probabilistic hierarchical
model for generation. In Proceedings of the 18th Con-
ference on Computational Linguistics (COLING?2000),
July 31 - August 4 2000, Universita?t des Saarlandes,
Saarbru?cken, Germany, 2000.
[Barzilay et al, 1999] Regina Barzilay, Kathleen R. McKe-
own, and Michael Elhadad. Information fusion in the con-
text of multi-document summarization. In Proceedings of
the 37th conference on Association for Computational Lin-
guistics, pages 550?557, Morristown, NJ, USA, 1999. As-
sociation for Computational Linguistics.
[Collins, 1996] Michael John Collins. A new statistical
parser based on bigram lexical dependencies. In Arivind
Joshi and Martha Palmer, editors, Proceedings of the
Thirty-Fourth Annual Meeting of the Association for Com-
putational Linguistics, pages 184?191, San Francisco,
1996. Morgan Kaufmann Publishers.
[Daume? III and Marcu, 2004] Hal Daume? III and Daniel
Marcu. A phrase-based hmm approach to docu-
ment/abstract alignment. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 119?126,
Barcelona, Spain, July 2004. Association for Computa-
tional Linguistics.
[Forney, 1973] G. David Forney. The viterbi algorithm. Pro-
ceedings of The IEEE, 61(3):268?278, 1973.
[Jing and McKeown, 1999] Hongyan Jing and Kathleen
McKeown. The decomposition of human-written sum-
mary sentences. In Research and Development in Infor-
mation Retrieval, pages 129?136, 1999.
[Kittredge and Mel?cuk, 1983] Richard I. Kittredge and Igor
Mel?cuk. Towards a computable model of meaning-text
relations within a natural sublanguage. In IJCAI, pages
657?659, 1983.
[Knight and Marcu, 2002] Kevin Knight and Daniel Marcu.
Summarization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107, 2002.
[Langkilde and Knight, 1998] Irene Langkilde and Kevin
Knight. The practical value of N-grams in derivation. In
Eduard Hovy, editor, Proceedings of the Ninth Interna-
tional Workshop on Natural Language Generation, pages
248?255, New Brunswick, New Jersey, 1998. Association
for Computational Linguistics.
[Manning and Schu?tze, 1999] Christopher D. Manning and
Hinrich Schu?tze. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge, Mas-
sachusetts, 1999.
[Steedman, 2000] Mark Steedman. The syntactic process.
MIT Press, Cambridge, MA, USA, 2000.
[Wan et al, 2003] Stephen Wan, Mark Dras, Cecile Paris,
and Robert Dale. Using thematic information in statistical
headline generation. In The Proceedings of the Workshop
on Multilingual Summarization and Question Answering
at ACL 2003, Sapporo, Japan, July 2003.
[Witbrock and Mittal, 1999] Michael J. Witbrock and
Vibhu O. Mittal. Ultra-summarization (poster abstract):
a statistical approach to generating highly condensed
non-extractive summaries. In SIGIR ?99: Proceedings of
the 22nd annual international ACM SIGIR conference on
Research and development in information retrieval, pages
315?316, New York, NY, USA, 1999. ACM Press.
Proceedings of the 10th Conference on Parsing Technologies, pages 36?38,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
The Impact of Deep Linguistic Processing on Parsing Technology
Timothy Baldwin
University of Melbourne
tim@csse.unimelb.edu.au
Mark Dras
Macquarie University
madras@ics.mq.edu.au
Julia Hockenmaier
University of Pennsylvania
juliahr@cis.upenn.edu
Tracy Holloway King
PARC
thking@parc.com
Gertjan van Noord
University of Groningen
vannoord@let.rug.nl
Abstract
As the organizers of the ACL 2007 Deep
Linguistic Processing workshop (Baldwin et
al., 2007), we were asked to discuss our per-
spectives on the role of current trends in
deep linguistic processing for parsing tech-
nology. We are particularly interested in
the ways in which efficient, broad coverage
parsing systems for linguistically expressive
grammars can be built and integrated into
applications which require richer syntactic
structures than shallow approaches can pro-
vide. This often requires hybrid technolo-
gies which use shallow or statistical methods
for pre- or post-processing, to extend cover-
age, or to disambiguate the output.
1 Introduction
Our talk will provide a view on the relevance of deep
linguistic processing for parsing technologies from
the perspective of the organizers of the ACL 2007
Workshop on Deep Linguistic Processing (Baldwin
et al, 2007). The workshop was conceived with the
broader aim of bringing together the different com-
putational linguistic sub-communities which model
language predominantly by way of theoretical syn-
tax, either in the form of a particular theory (e.g.
CCG, HPSG, LFG, TAG, the Prague School) or a
more general framework which draws on theoretical
and descriptive linguistics. These ?deep linguistic
processing? approaches differ from shallower meth-
ods in that they yield richer, more expressive, struc-
tural representations which capture long-distance
dependencies or the underlying predicate-argument
structure directly.
Aspects of this research have often had their own
separate fora, such as the ACL 2005 workshop on
deep lexical acquisition (Baldwin et al, 2005), as
well as the TAG+ (Kallmeyer and Becker, 2006),
Alpino (van der Beek et al, 2005), ParGram (Butt
et al, 2002) and DELPH-IN (Oepen et al, 2002)
projects and meetings. However, the fundamental
approaches to building a linguistically-founded sys-
tem and many of the techniques used to engineer
efficient systems are common across these projects
and independent of the specific grammar formal-
ism chosen. As such, we felt the need for a com-
mon meeting in which experiences could be shared
among a wider community, similar to the role played
by recent meetings on grammar engineering (Wint-
ner, 2006; Bender and King, 2007).
2 The promise of deep parsing
Deep linguistic processing has traditionally been
concerned with grammar development (for use in
both parsing and generation). However, the linguis-
tic precision and complexity of the grammars meant
that they had to be manually developed and main-
tained, and were computationally expensive to run.
In recent years, machine learning approaches
have fundamentally altered the field of natural lan-
guage processing. The availability of large, manu-
ally annotated, treebanks (which typically take years
of prior linguistic groundwork to produce) enabled
the rapid creation of robust, wide-coverage parsers.
However, the standard evaluation metrics for which
such parsers have been optimized generally ignore
36
much of the rich linguistic information in the orig-
inal treebanks. It is therefore perhaps only natural
that deep processing methods, which often require
substantial amounts of manual labor, have received
considerably less attention during this period.
But even if further work is required for deep
processing techniques to fully mature, we believe
that applications that require natural language under-
standing or inference, among others, will ultimately
need detailed syntactic representations (capturing,
e.g., bounded and unbounded long-range dependen-
cies) from which semantic interpretations can eas-
ily be built. There is already some evidence that
our current deep techniques can, in some cases, out-
perform shallow approaches. There has been work
demonstrating this in question answering, targeted
information extraction and the recent textual entail-
ment recognition task, and perhaps most notably in
machine translation: in this latter field, after a period
of little use of linguistic knowledge, deeper tech-
niques are beginning to lead to better performance,
e.g. by redefining phrases by syntactic ?treelets?
rather than contiguous word sequences, or by explic-
itly including a syntactic component in the probabil-
ity model, or by syntactic preprocessing of the data.
3 Closing the divide
In the past few years, the divide between ?deep?,
rule-based, methods and ?shallow?, statistical, ap-
proaches, has begun to close from both sides. Re-
cent advances in using the same treebanks that have
advanced shallow techniques to extract more expres-
sive grammars or to train statistical disambiguators
for them, and in developing framework-specific tree-
banks, have made it possible to obtain similar cov-
erage, robustness, and disambiguation accuracy for
parsers that use richer structural representations. As
witnessed by many of the papers in our workshop
(Baldwin et al, 2007), a large proportion of current
deep systems have statistical components to them,
e.g., as pre- or post-processing to control ambigu-
ity, as means of acquiring and extending lexical re-
sources, or even use machine learning techniques
to acquire deep grammars automatically. From the
other side of the divide, many of the purely statistical
approaches are using progressively richer linguistic
features and are taking advantage of these more ex-
pressive features to tackle problems that were tradi-
tionally thought to require deep systems, such as the
recovery of traces or semantic roles.
4 The continued need for research on deep
processing
Although statistical techniques are becoming com-
monplace even for systems built around hand-
written grammars, there is still a need for further
linguistic research and manual grammar develop-
ment. For example, supervised machine-learning
approaches rely on large amounts of manually anno-
tated data. Where such data are available, develop-
ers of deep parsers and grammars can exploit them
to determine frequency of certain constructions, to
bootstrap gold standards for their systems, and to
provide training data for the statistical components
of their systems such as parse disambiguators. But
for the majority of the world?s languages, and even
for many languages with large numbers of speakers,
such corpora are unavailable. Under these circum-
stances, manual grammar development is unavoid-
able, and recent progress has allowed the underlying
systems to become increasingly better engineered,
allowing for more rapid development of any given
grammar, as well as for overlay grammars that adapt
to particular domains and applications and for port-
ing of grammars from one language to another.
Despite recent work on (mostly dependency
grammar-based) multilingual parsing, it is still the
case that most research on statistical parsing is done
on English, a fixed word-order language where sim-
ple context-free approximations are often sufficient.
It is unclear whether our current models and al-
gorithms carry over to morphologically richer lan-
guages with more flexible word order, and it is possi-
ble that the more complex structural representations
allowed by expressive formalisms will cease to re-
main a luxury.
Further research is required on all aspects of
deep linguistic processing, including novel linguis-
tic analyses and implementations for different lan-
guages, formal comparisons of different frame-
works, efficient parse and learning algorithms, better
statistical models, innovative uses of existing data
resources, and new evaluation tools and methodolo-
gies. We were fortunate to receive so many high-
37
quality submissions on all of these topics for our
workshop.
5 Conclusion and outlook
Deep linguistic processing brings together a range of
perspectives. It covers current approaches to gram-
mar development and issues of theoretical linguis-
tic and algorithmic properties, as well as the appli-
cation of deep linguistic techniques to large-scale
applications such as question answering and dialog
systems. Having industrial-scale, efficient parsers
and generators opens up new application domains
for natural language processing, as well as inter-
esting new ways in which to approach existing ap-
plications, e.g., by combining statistical and deep
processing techniques in a triage process to pro-
cess massive data quickly and accurately at a fine
level of detail. Notably, several of the papers ad-
dressed the relationship of deep linguistic process-
ing to topical statistical approaches, in particular in
the area of parsing. There is an increasing inter-
est in deep linguistic processing, an interest which
is buoyed by the realization that new, often hybrid,
techniques combined with highly engineered parsers
and generators and state-of-the-art machines opens
the way towards practical, real-world application of
this research. We look forward to further opportu-
nities for the different computational linguistic sub-
communities who took part in this workshop, and
others, to continue to come together in the future.
References
Timothy Baldwin, Anna Korhonen, and Aline Villavicen-
cio, editors. 2005. Proceedings of the ACL-SIGLEX
Workshop on Deep Lexical Acquisition. Ann Arbor,
USA.
Timothy Baldwin, Mark Dras, Julia Hockenmaier,
Tracy Holloway King, and Gertjan van Noord, editors.
2007. Proceedings of the ACL Workshop on Deep Lin-
guistic Processing, Prague, Czech Republic.
Emily Bender and Tracy Holloway King, editors. 2007.
Grammar Engineering Across Frameworks, Stanford
University. CSLI On-line Publications. to appear.
Miriam Butt, Helge Dyvik, T. H. King, Hiroshi Masuichi,
and Christian Rohrer. 2002. The parallel grammar
project. In COLING Workshop on Grammar Engi-
neering and Evaluation, Taipei, Taiwan.
Laura Kallmeyer and Tilman Becker, editors. 2006. Pro-
ceedings of the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms (TAG+),
Sydney, Australia.
Stephan Oepen, Dan Flickinger, J. Tsujii, and Hand
Uszkoreit, editors. 2002. Collaborative Language En-
gineering: A Case Study in Efficient Grammar-based
Processing. CSLI Publications.
Leonoor van der Beek, Gosse Bouma, Jan Daciuk, Tanja
Gaustad, Robert Malouf, Mark-Jan Nederhof, Gert-
jan van Noord, Robbert Prins, and Bego na Vil-
lada Moiro?n. 2005. Algorithms for linguistic pro-
cessing. NWO Pionier final report. Technical report,
University of Groningen.
Shuly Wintner. 2006. Large-scale grammar development
and grammar engineering. Research workshop of the
Israel Science Foundation.
38
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 78?86,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Coupling hierarchical word reordering and decoding in phrase-based
statistical machine translation
Maxim Khalilov and Jos? A.R. Fonollosa
Universitat Polit?cnica de Catalunya
Campus Nord UPC, 08034,
Barcelona, Spain
{khalilov,adrian}@gps.tsc.upc.edu
Mark Dras
Macquarie University
North Ryde NSW 2109,
Sydney, Australia
madras@ics.mq.edu.au
Abstract
In this paper, we start with the existing idea of
taking reordering rules automatically derived
from syntactic representations, and applying
them in a preprocessing step before translation
to make the source sentence structurally more
like the target; and we propose a new approach
to hierarchically extracting these rules. We
evaluate this, combined with a lattice-based
decoding, and show improvements over state-
of-the-art distortion models.
1 Introduction
One of the big challenges for the MT community is
the problem of placing translated words in a natural
order. This issue originates from the fact that dif-
ferent languages are characterized by different word
order requirements. The problem is especially im-
portant if the distance between words which should
be reordered is high (global reordering); in this case
the reordering decision is very difficult to take based
on statistical information due to dramatic expansion
of the search space with the increase in number of
words involved in the search process.
Classically, statistical machine translation (SMT)
systems do not incorporate any linguistic analysis
and work at the surface level of word forms. How-
ever, more recently MT systems are moving towards
including additional linguistic and syntactic infor-
mative sources (for example, source- and/or target-
side syntax) into word reordering process. In this pa-
per we propose using a syntactic reordering system
operating with fully, partially and non- lexicalized
reordering patterns, which are applied on the step
prior to translation; the novel idea in this paper is in
the derivation of these rules in a hierarchical manner,
inspired by Imamura et al(2005). Furthermore, we
propose generating a word lattice from the bilingual
corpus with the reordered source side, extending the
search space on the decoding step. A thorough study
of the combination of syntactical and word lattice re-
ordering approaches is another novelty of the paper.
2 Related work
Many reordering algorithms have appeared over the
past few years. Word class-based reordering was a
part of Och?s Alignment Template system (Och et
al., 2004); the main criticism of this approach is that
it shows bad performance for the pair of languages
with very distinct word order. The state-of-the-art
SMT system Moses implements a distance-based re-
ordering model (Koehn et al, 2003) and a distor-
tion model, operating with rewrite patterns extracted
from a phrase alignment table (Tillman, 2004).
Many SMT models implement the brute force ap-
proach, introducing several constrains for the re-
ordering search as described in Kanthak et al (2005)
and Crego et al (2005). The main criticism of such
systems is that the constraints are not lexicalized.
Recently there has been interest in SMT exploiting
non-monotonic decoding which allow for extension
of the search space and linguistic information in-
volvement. The variety of such models includes a
constrained distance-based reordering (Costa-juss?
et al, 2006); and a constrained version of distortion
model where the reordering search problem is tack-
led through a set of linguistically motivated rules
used during decoding (Crego and Mari?o, 2007).
78
A quite popular class of reordering algorithms is
a monotonization of the source part of the parallel
corpus prior to translation. The first work on this
approach is described in Nie?en and Ney (2004),
where morpho-syntactic information was used to ac-
count for the reorderings needed. A representative
set of similar systems includes: a set of hand-crafted
reordering patterns for German-to-English (Collins
et al, 2005) and Chinese-English (Wang et al,
2007) translations, emphasizing the distinction be-
tween German/Chinese and English clause struc-
ture; and statistical machine reordering (SMR) tech-
nique where a monotonization of the source words
sequence is performed by translating them into the
reordered one using well established SMT mecha-
nism (Costa-juss? and Fonollosa, 2006). Coupling
of SMR algorithm and the search space extension
via generating a set of weighted reordering hypothe-
ses has demonstrated a significant improvement, as
shown in Costa-juss? and Fonollosa (2008).
The technique proposed in this study is most
similar to the one proposed for French-to-English
translation task in Xia and McCord (2004), where
the authors present a hybrid system for French-
English translation based on the principle of auto-
matic rewrite patterns extraction using a parse tree
and phrase alignments. We propose using a word
distortion model not only to monotonize the source
part of the corpus (using a different approach to
rewrite rule organization from Xia and McCord), but
also to extend the search space during decoding.
3 Baseline phrase-based SMT systems
The reference system which was used as a transla-
tion mechanism is the state-of-the-art Moses-based
SMT (Koehn et al, 2007). The training and weights
tuning procedures can be found on the Moses web
page1.
Classical phrase-based translation is considered
as a three step algorithm: (1) the source sequence
of words is segmented into phrases, (2) each phrase
is translated into the target language using a transla-
tion table, (3) the target phrases are reordered to fit
the target language. The probabilities of the phrases
are estimated by relative frequencies of their appear-
ance in the training corpus.
1http://www.statmt.org/moses/
In baseline experiments we used a phrase depen-
dent lexicalized reordering model, as proposed in
Tillmann (2004). According to this model, mono-
tonic or reordered local orientations enriched with
probabilities are learned from training data. During
decoding, translation is viewed as a monotone block
sequence generation process with the possibility to
swap a pair of neighbor blocks.
4 Syntax-based reordering coupled with
word graph
Our syntax-based reordering system requires access
to source and target language parse trees and word
alignments intersections.
4.1 Notation
Syntax-based reordering (SBR) operates with source
and target parse trees that represent the syntactic
structure of a string in source and target languages
according to a Context-Free Grammar (CFG).
We call this representation "CFG form". We
formally define a CFG in the usual way as G =
?N,T,R, S?, where N is a set of nonterminal sym-
bols (corresponding to source-side phrase and part-
of-speech tags); T is a set of source-side terminals
(the lexicon), R is a set of production rules of the
form ? ? ?, with ? ? N and ?, which is a sequence
of terminal and nonterminal symbols; and S ? N is
the distinguished symbol.
The reordering rules then have the form
?0@0 . . . ?k@k ?
?d0@d0 . . . ?dk@dk|Lexicon|p1 (1)
where ?i ? N for all 0 ? i ? k; (do . . . dk) is
a permutation of (0 . . . k); Lexicon comes from the
source-side set of words for each ?i; and p1 is a prob-
ability associated with the rule. Figure 1 gives two
examples of the rule format.
4.2 Rules extraction
Concept. Inspired by the ideas presented in Imamura
et al (2005), where monolingual correspondences of
syntactic nodes are used during decoding, we extract
a set of bilingual patterns allowing for reordering as
described below:
79
(1) align the monotone bilingual corpus with
GIZA++ (Och and Ney, 2003) and find
the intersection of direct and inverse word
alignments, resulting in the construction
of the projection matrix P (see below));
(2) parse the source and the target parts of the
parallel corpus;
(3) extract reordering patterns from the par-
allel non-isomorphic CFG-trees based on
the word alignment intersection.
Step 2 is straightforward; we explain aspects of
Steps 1 and 3 in more detail below. Figures 1 and 2
show an example of the extraction of two lexicalized
rules for a parallel Arabic-English sentence:
Arabic:
English:
h*A
this
hW
is
fndq
your
+k
hotel
We use this below in our explanations.
Figure 2: Example of subtree transfer and reordering
rules extraction.
Projection matrix. Bilingual content can be rep-
resented in the form of words or sequences of words
depending on the syntactic role of the corresponding
grammatical element (constituent or POS).
Given two parse trees and a word alignment in-
tersection, a projection matrix P is defined as an
M ?N matrix such that M is the number of words
in the target phrase; N is the number of words in
the source phrase; and a cell (i, j) has a value based
on the alignment intersection ? this value is zero
if word i and word j do not align, and is a unique
non-zero link number if they do.
For the trees in Figure 2,
P =
?
???
1 0 0 0
0 2 0 0
0 0 0 3
0 0 4 0
?
???
Unary chains. Given an unary chain of the form
X ? Y , rules are extracted for each level in this
chain. For example given a rule
NP@0ADV P@1 ? ADV P@1NP@0
and a unary chain "ADV P ? AD", a following
equivalent rule will be generated
NP@0AD@1 ? AD@1NP@0.
The role of target-side parse tree. Although re-
ordering is performed on the source side only, the
target-side tree is of great importance: the reorder-
ing rules can be only extracted if the words covered
by the rule are entirely covered by both a node in
the source and in the target trees. It allows the more
accurate determination of the covering and limits of
the extracted rules.
4.3 Rules organization
Once the list of fully lexicalized reordering patterns
is extracted, all the rules are progressively processed
reducing the amount of lexical information. These
initial rules are iteratively expanded such that each
element of the pattern is generalized until all the lex-
ical elements of the rule are represented in the form
of fully unlexicalized categories. Hence, from each
NN@0 NP@1 ? NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >> | p
NN@0 NNP@1 ? NNP@1 NN@0 | NN@0 << fndq >> NNP@1 << +k >> | p?
Figure 1: Directly extracted rules.
80
initial pattern with N lexical elements, 2N ? 2 par-
tially lexicalized rules and 1 general rule are gener-
ated. An example of the process of delexicalization
can be found in Figure 3.
Thus, finally three types of rules are available: (1)
fully lexicalized (initial) rules, (2) partially lexical-
ized rules and (3) unlexicalized (general) rules.
On the next step, the sets are processed separately:
patterns are pruned and ambiguous rules are re-
moved. All the rules from the fully lexicalized, par-
tially lexicalized and general sets that appear fewer
than k times are directly discarded (k is a shorthand
for kful, kpart and kgener). The probability of a
pattern is estimated based on relative frequency of
their appearance in the training corpus. Only one
the most probable rule is stored. Fully lexicalized
rules are not pruned (kful = 0); partially lexicalized
rules that have been seen only once were discarded
(kpart = 1); the thresholds kgener was set to 3: it
limits the number of general patterns capturing rare
grammatical exceptions which can be easily found
in any language.
Only the one-best reordering is used in other
stages of the algorithm, so the rule output function-
ing as an input to the next rule can lead to situa-
tions reverting the change of word order that the
previously applied rule made. Therefore, the rules
that can be ambiguous when applied sequentially
during decoding are pruned according to the higher
probability principle. For example, for the pair of
patterns with the same lexicon (which is empty for
a general rule leading to a recurring contradiction
NP@0 VP@1 ? VP@1 NP@0 p1, VP@0 NP@1
? NP@1 VP@0 p2 ), the less probable rule is re-
moved.
Finally, there are three resulting parameter tables
analogous to the "r-table" as stated in (Yamada and
Knight, 2001), consisting of POS- and constituent-
based patterns allowing for reordering and mono-
tone distortion (examples can be found in Table 5).
4.4 Source-side monotonization
Rule application is performed as a bottom-up parse
tree traversal following two principles:
(1) the longest possible rule is applied, i.e. among
a set of nested rules, the rule with a longest left-side
covering is selected. For example, in the case of the
appearance of an NN JJ RB sequence and presence
of the two reordering rules
NN@0 JJ@1 ? ... and
NN@0 JJ@1 RB@2 ? ...
the latter pattern will be applied.
(2) the rule containing the maximum lexical infor-
mation is applied, i.e. in case there is more than one
alternative pattern from different groups, the lexical-
ized rules have preference over the partially lexical-
ized, and partially lexicalized over general ones.
Figure 4: Reordered source-side parse tree.
Once the reordering of the training corpus is
ready, it is realigned and new more monotonic align-
ment is passed to the SMT system. In theory, the
word links from the original alignment can be used,
however, due to our experience, running GIZA++
again results in a better word alignment since it is
easier to learn on the modified training example.
Example of correct local reordering done with the
SBR model can be found in Figure 4.
Initial rule: NN@0 NP@1 ? NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >> | p1
Part. lexic. rules: NN@0 NP@1 ? NP@1 NN@0 | NN@0 << fndq >> NP@1 << - >> | p2
NN@0 NP@1 ? NP@1 NN@0 | NN@0 << - >> NP@1 << +k >> | p3
General rule: NN@0 NP@1 ? NP@1 NN@0 | p4
Figure 3: Example of a lexical rule expansion.
81
4.5 Coupling with decoding
In order to improve reordering power of the transla-
tion system, we implemented an additional reorder-
ing as described in Crego and Mari?o (2006).
Multiple word segmentations is encoded in a lat-
tice, which is then passed to the input of the de-
coder, containing reordering alternatives consistent
with the previously extracted rules. The decoder
takes the n-best reordering of a source sentence
coded in the form of a word lattice. This approach
is in line with recent research tendencies in SMT, as
described for example in (Hildebrand et al, 2008;
Xu et al, 2005). Originally, word lattice algorithms
do not involve syntax into reordering process, there-
fore their reordering power is limited at representing
long-distance reordering. Our approach is designed
in the spirit of hybrid MT, integrating syntax trans-
fer approach and statistical word lattice methods to
achieve better MT performance on the basis of the
standard state-of-the-art models.
During training a set of word permutation patterns
is automatically learned following given word-to-
word alignment. Since the original and monotonized
(reordered) alignments may vary, different sets of
reordering patterns are generated. Note that no in-
formation about the syntax of the sentence is used:
the reordering permutations are motivated by the
crossed links found in the word alignment and, con-
S 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 L
> n
+ h
+ h
> n
m T E m m T E m
* w
E r y q
> n
* w
E r y q
t A r y x
m T E m
E r y q
* w
E r y q
t A r y x
* w
* w
E r y q
m T E m
t A r y x
E r y q
* w
S 1 2 3 4 5 6 7 8 9
> n
+ h
+ h
> n
m T E m m T E m
> n
* w
t A r y x
* w
m T E m
t A r y x
1 0 L
E r y q
m T E m
E r y q
t A r y x
> n  + h  m T E m  * w  t A r y x  E r y q  W o r d  l a t t i c e ,  p l a i n  t e x t :
W o r d  l a t t i c e ,  r e o r d e r e d  t e x t : > n  + h  m T E m  * w  E r y q  t A r y x  ( c )
( b )
S 1 2 3 4 5> n + h m T E m * w Lt A r y x E r y q
> n  + h  m T E m  * w  t A r y x   E r y qM o n o t o n i c  s e a r c h ,  p l a i n  t e x t :( a )
Figure 5: Comparative example of a monotone search (a), word lattice for a plain (b) and reordered (c) source
sentences.
82
sequently, the generalization power of this frame-
work is limited to local permutations.
On the step prior to decoding, the system gen-
erates word reordering graph for every source sen-
tence, expressed in the form of a word lattice. The
decoder processes word lattice instead of only one
input hypothesis, extending the monotonic search
graph with alternative paths.
Original sentence in Arabic, the English gloss and
reference translation are:
Ar.:
Gl.:
>n +h
this
mTEm
restaurant
*w
has
Eryq
history
tAryx
illustrious
Ref: ?this restaurant has an illustrious history?
The monotonic search graph (a) is extended with
a word lattice for the monotonic train set (b) and re-
ordered train sets (c). Figure 5 shows an example
of the input word graph expressed in the form of a
word lattice. Lattice (c) differ from the graph (b) in
number of edges and provides more input options to
the decoder. The decision about final translation is
taken during decoding considering all the possible
paths, provided by the word lattice.
5 Experiments and results
5.1 Data
The experiments were performed on two Arabic-
English corpora: the BTEC?08 corpus from the
tourist domain and the 50K first-lines extraction
from the corpus that was provided to the NIST?08
evaluation campaign and belongs to the news do-
main (NIST50K). The corpora differ mainly in the
average sentence length (ASL), which is the key cor-
pus characteristic in global reordering studies.
A training set statistics can be found in Table 1.
BTEC NIST50K
Ar En Ar En
Sentences 24.9 K 24.9 K 50 K 50 K
Words 225 K 210 K 1.2 M 1.35 M
ASL 9.05 8.46 24.61 26.92
Voc 11.4 K 7.6 K 55.3 36.3
Table 1: Basic statistics of the BTEC training corpus.
The BTEC development dataset consists of 489
sentences and 3.8 K running words, with 6 human-
made reference translations per sentence; the dataset
used to test the translation quality has 500 sentences,
4.1 K words and is also provided with 6 reference
translations.
The NIST50K development set consists of 1353
sentences and 43 K words; the test data contains
1056 sentences and 33 K running words. Both
datasets have 4 reference translations per sentence.
5.2 Arabic data preprocessing
We took a similar approach to that shown in Habash
and Sadat (2006), using the MADA+TOKAN sys-
tem for disambiguation and tokenization. For dis-
ambiguation only diacritic unigram statistics were
employed. For tokenization we used the D3 scheme
with -TAGBIES option. The scheme splits the fol-
lowing set of clitics: w+, f+, b+, k+, l+, Al+ and
pronominal clitics. The -TAGBIES option produces
Bies POS tags on all taggable tokens.
5.3 Experimental setup
We used the Stanford Parser (Klein and Man-
ning, 2003) for both languages, Penn English Tree-
bank (Marcus et al, 1993) and Penn Arabic Tree-
bank set (Kulick et al, 2006). The English Treebank
is provided with 48 POS and 14 syntactic tags, the
Arabic Treebank has 26 POS and 23 syntactic cate-
gories.
As mentioned above, specific rules are not pruned
away due to a limited amount of training material we
set the thresholds kpart and kgener to relatively low
values, 1 and 3, respectively.
Evaluation conditions were case-insensitive and
with punctuation marks considered. The target-
side 4-gram language model was estimated using
the SRILM toolkit (Stolcke, 2002) and modified
Kneser-Ney discounting with interpolation. The
highest BLEU score (Papineni et al, 2002) was cho-
sen as the optimization criterion. Apart from BLEU,
a standard automatic measure METEOR (Banerjee
and Lavie, 2005) was used for evaluation.
5.4 Results
The scores considered are: BLEU scores obtained
for the development set as the final point of the
MERT procedure (Dev), and BLEU and METEOR
scores obtained on test dataset (Test).
We present BTEC results (Tables 2), character-
ized by relatively short sentence length, and the re-
83
sults obtained on the NIST corpus (Tables 3) with
much longer sentences and much need of global re-
ordering.
Dev Test
BLEU BLEU METEOR
Plain 48.31 45.02 65.98
BL 48.46 47.10 68.10
SBR 48.75 47.52 67.33
SBR+lattice 48.90 48.78 68.85
Table 2: Summary of BTEC experimental results.
Dev Test
BLEU BLEU METEOR
Plain 41.83 43.80 62.03
BL 42.68 43.52 62.17
SBR 42.71 44.01 63.29
SBR+lattice 43.05 44.89 63.30
Table 3: Summary of NIST50K experimental results.
Four SMT systems are contrasted: BL refers to
the Moses baseline system: the training data is not
reordered, lexicalized reordering model (Tillman,
2004) is applied; SBR refers to the monotonic sys-
tem configuration with reordered (SBR) source part;
SBR+lattice is the run with reordered source part, on
the translation step the input is represented as a word
lattice.
We also compare the proposed approach with a
monotonic system configuration (Plain). It shows
the effect of source-reordering and lattice input, also
decoded monotonically.
Automatic scores obtained on the test dataset
evolve similarly when the SBR and word lattice rep-
resentation applied to BTEC and NIST50K tasks.
The combined method coupling two reordering
techniques was more effective than the techniques
applied independently and shows an improvement
in terms of BLEU for both corpora. The METEOR
score is only slightly better for the SBR configura-
tions in case of BTEC task; in the case of NIST50K
the METEOR improvement is more evident. The
general trend is that automatic scores evaluated on
the test set increase with the reordering model com-
plexity.
Application of the SBR algorithm only (without
a word lattice decoding) does not allow achieving
statistical significance threshold for a 95% confi-
dence interval and 1000 resamples (Koehn, 2004)
for either of considered corpora. However, the
SBR+lattice system configuration outperforms the
BL by about 1.7 BLEU points (3.5%) for BTEC task
and about 1.4 BLEU point (3.1%) for NIST task.
These differences is statistically significant.
Figure 6 demonstrates how two reordering tech-
niques interact within a sentence with a need for
both global and local word permutations.
5.5 Syntax-based rewrite rules
As mentioned above, the SBR operates with three
groups of reordering rules, which are the product
of complete or partial delexicalization of the origi-
nally extracted patterns. The groups are processed
and pruned independently. Basic rules statistics for
both translation tasks can be found in Table 4.
The major part of reordering rules consists of
two or three elements (for BTEC task there are
no patterns including more than three nodes). For
NIST50K there are a few rules with higher size in
words of the move (up to 8). In addition, there are
some long lexicalized rules (7-8), generating a high
number of partially lexicalized patterns.
Table 5 shows the most frequent reordering rules
with non-monotonic right part from each group.
Ar. plain.:
En. gloss:
AElnt
announced
Ajhzp
press
AlAElAm
release
l
by
bEvp
mission
AlAmm AlmtHdp
nations united
fy
in
syrAlywn
sierra leone
An
that
...
...
En. ref.: ?a press release by the united nations mission to sierra leone announced that ...?
Ar. reord.: Ajhzp AlAElAm l bEvp AlmtHdp AlAmm fy syrAlywn AElnt An ...
Figure 6: Example of SBR application (highlited bold) and local reordering error corrected with word lattice reorder-
ing (underlined).
84
6 Conclusions
In this study we have shown how the translation
quality can be improved, coupling (1) SBR al-
gorithm and (2) word alignment-based reordering
framework applied during decoding. The system
automatically learns a set of syntactic reordering
patterns that exploit systematic differences between
word order of source and target languages.
Translation accuracy is clearly higher when al-
lowing for SBR coupled with word lattice input rep-
resentation than standard Moses SMT with existing
(lexicalized) reordering models within the decoder
and one input hypothesis condition. We have also
compared the reordering model a monotonic system.
The method was tested translating from Arabic to
English. Two corpora and tasks were considered:
the BTEC task with much need of local reordering
and the NIST50K task requiring long-distance per-
mutations caused by longer sentences.
The reordering approach can be expanded for any
other pair of languages with available parse tools.
We also expect that the method scale to a large train-
ing set, and that the improvement will still be kept,
however, we plan to confirm this assumption exper-
imentally in the near future.
Acknowledgments
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project) and under a FPU grant.
Group # of rules Voc 2-element 3-element 4-element [5-8]-element
BTEC experiments
Specific rules 703 413 406 7 0 0
Partially lexicalized rules 1,306 432 382 50 0 0
General rules 259 5 259 0 0 0
NIST50K experiments
Specific rules 517 399 193 109 72 25
Partially lexicalized rules 17,897 14,263 374 638 1,010 12,241
General rules 489 372 180 90 72 30
Table 4: Basic reordering rules statistics.
Specific rules
NN@0 NP@1 -> NP@1 NN@0 | NN@0 ? Asm ? NP@1 ? +y ? | 0.0270
DTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0 ? AlAmm ?DTJJ@1 ? AlmtHdp ? | 0.0515
Partially lexicalized rules
DTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0 ? NON ?DTJJ@1 ? AlmtHdp ? | 0.0017
NN@0 NNP@1 -> NNP@1 NN@0 | NN@0 ? NON ?NNP@1 ? $rm ? | 0.0017
General rules
PP@0 NP@1 -> PP@0 NP@1 | 0.0432
NN@0 DTNN@1 DTJJ@2 -> NN@0 DTJJ@2 DTNN@1 |0.0259
Table 5: Examples of Arabic-to-English reordering rules.
85
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 65?72.
M. Collins, Ph. Koehn, and I. Kuc?erov?. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on ACL 2005,
pages 531?540.
M.R. Costa-juss? and J.A.R. Fonollosa. 2006. Sta-
tistical machine reordering. In Proceedings of the
HLT/EMNLP 2006.
M.R. Costa-juss? and J.A.R. Fonollosa. 2008. Comput-
ing multiple weighted reordering hypotheses for a sta-
tistical machine translation phrase-based system. In In
Proc. of the AMTA?08, Honolulu, USA, October.
M.R. Costa-juss?, J.M. Crego, A. de Gispert, P. Lambert,
M. Khalilov, J. A. Fonollosa, J.B. Mari no, and R.E.
Banchs. 2006. TALP phrase-based system and TALP
system combination for IWSLT 2006. In Proceedings
of the IWSLT 2006, pages 123?129.
J.M. Crego and J. B Mari?o. 2006. Reordering experi-
ments for N-gram-based SMT. In SLT?06, pages 242?
245.
J.M. Crego and J.B. Mari?o. 2007. Syntax-enhanced N-
gram-based smt. In Proceedings of MT SUMMIT XI.
J.M. Crego, J. B. Mari?o, and A. de Gispert. 2005. Re-
ordered search and tuple unfolding for ngram-based
smt. In In Proc. of MT SUMMIT X, pages 283?289,
September.
S. Nie?en and H. Ney. 2004. Statistical machine transla-
tion with scarce resources using morpho-syntactic in-
formation. volume 30, pages 181?204.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, pages 49?52.
A.S. Hildebrand, K. Rottmann, M. Noamany, Q. Gao,
S. Hewavitharana, N. Bach, and S. Vogel. 2008. Re-
cent improvements in the cmu large scale chinese-
english smt system. In Proceedings of ACL-08: HLT
(Companion Volume), pages 77?80.
K. Imamura, H. Okuma, and E. Sumita. 2005. Practical
approach to syntax-based statistical machine transla-
tion. In Proceedings of MT Summit X, pages 267?274.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In In Proc. of the ACL
Workshop on Building and Using Parallel Texts, pages
167?174, June.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting of
the ACL 2003, pages 423?430.
Ph. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based machine translation. In Proceedings of
the HLT-NAACL 2003, pages 48?54.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open-source toolkit for
statistical machine translation. In Proceedings of ACL
2007, pages 177?180.
Ph. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
S. Kulick, R. Gabbard, and M. Marcus. 2006. Parsing the
Arabic Treebank: Analysis and improvements. Tree-
banks and Linguistic Theories.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F.J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A Smorgasbord of
Features for Statistical Machine Translation. In Pro-
ceedings of HLT/NAACL04, pages 161?168.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311?
318.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proceedings of the Int. Conf. on Spo-
ken Language Processing, pages 901?904.
C. Tillman. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL?04.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proceedings of the Joint Conference on EMNLP.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
Proceedings of the COLING 2004.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. In-
tegrated chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of ACL 2001,
pages 523?530.
86
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 52?60,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Using Hypernymy Acquisition to Tackle (Part of) Textual Entailment
Elena Akhmatova
Centre for Language Technology
Macquarie University
Sydney, Australia
elena@ics.mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
madras@ics.mq.edu.au
Abstract
Within the task of Recognizing Textual
Entailment, various existing work has pro-
posed the idea that tackling specific sub-
types of entailment could be more produc-
tive than taking a generic approach to en-
tailment. In this paper we look at one
such subtype, where the entailment in-
volves hypernymy relations, often found
in Question Answering tasks. We investi-
gate current work on hypernymy acquisi-
tion, and show that adapting one such ap-
proach leads to a marked improvement in
entailment classification accuracy.
1 Introduction
The goal of the Recognizing Textual Entailment
(RTE) task (Dagan et al, 2006) is, given a pair of
sentences, to determine whether a Hypothesis sen-
tence can be inferred from a Text sentence. The
majority of work in RTE is focused on finding a
generic solution to the task. That is, creating a sys-
tem that uses the same algorithm to return a yes
or no answer for all textual entailment pairs. A
generic approach never works well for every sin-
gle entailment pair: there are entailment pairs that
are recognized poorly by all the generic systems.
Some approaches consequently propose a
component-based model. In this framework,
a generic system would have additional special
components that take care of special subclasses of
entailment pairs. Such a component is involved
when a pair of its subclass is recognized. Vander-
wende and Dolan (2005), and subsequently Van-
derwende et al (2006), divide all the entailment
pairs according to whether categorization could
be accurately predicted based solely on syntactic
cues. Related to this, Akhmatova and Dras (2007)
present an entailment type where the relationship
expressed in the Hypothesis is encoded in a syn-
tactic construction in the Text.
Vanderwende et al (2006) note that what they
term is-a relationships are a particular problem in
their approach. Observing that this encompasses
hypernymy relations, and that there has been a
fair amount of recent work on hypernymy acquisi-
tion, where ontologies containing hypernymy rela-
tions are extended with corpus-derived additions,
we propose a HYPERNYMY ENTAILMENT TYPE to
look at in this paper. In this type, the Hypothesis
states a hypernymy relationship between elements
of the Text: for example, This was seen as a be-
trayal by the EZLN and other political groups im-
plies that EZLN is a political group. This subtype
is of particular relevance to Question Answering
(QA): in the RTE-2 dataset,
1
for example, all is-a
Hypotheses were drawn from QA data.
In this paper we take the hypernymy acquisition
work of Snow et al (2005) as a starting point, and
then investigate how to adapt it to an entailment
context. We see this as an investigation of a more
general approach, where work in a separate area of
NLP can be adapted to define a related entailment
subclass.
Section 2 of the paper discusses the relevant
work from the areas of component-based RTE and
hypernymy extraction. Section 3 defines the hy-
pernymy entailment type and expands on the main
idea of the paper. Section 4 describes the experi-
mental set-up and the results; and Section 5 con-
cludes the work.
2 Related Work
2.1 Component-based RTE
Vanderwende et al (2006) use an approach based
on logical forms, which they generate by the NLP-
win parser. Nodes in the resulting syntactic de-
pendency graphs for Text and Hypothesis are then
heuristically aligned; then syntax-based heuristics
1
http://pascallin.ecs.soton.ac.uk/Challenges/RTE2, (Bar-
Haim et al, 2006)
52
are applied to detect false entailments. As noted
above, is-a relations fared particularly badly. In
our approach, we do not use such a heavy duty
representation for the task, using instead the tech-
niques of hypernym acquisition described in Sec-
tion 2.2. Cabrio et al (2008) proposed what they
call a combined specialized entailment engine.
They have created a general framework, based on
distance between T and H (they measure the cost
of the editing operations such as insertion, dele-
tion and substitution, which are required to trans-
form the text T into the hypothesis H) and sev-
eral modular entailment engines, each of which is
able to deal with an aspect of language variabil-
ity such as negation or modal verbs. Akhmatova
and Dras (2007) built a specific component from
a subset of entailment pairs that are poorly recog-
nized by generic systems participating in an RTE
Challenge. These are the entailment pairs where a
specific syntactic construction in the Text encodes
a semantic relationship between its elements that
is explicitly shown in the Hypothesis, as in exam-
ple (1):
(1) Text: Japan?s Kyodo news agency said the
US could be ready to set up a liaison
office?the lowest level of diplomatic
representation?in Pyongyang if it abandons
its nuclear program.
Hypothesis: Kyodo news agency is based in
Japan.
The entailment pairs share a set of similar fea-
tures: they have a very high word overlap regard-
less of being a true or false entailments, for ex-
ample. High word overlap is one of the features
for an RTE system for the majority of the entail-
ment pair types, which presumably hints at true,
but this is not useful in our case. Akhmatova and
Dras (2007) described a two-fold probabilistic ap-
proach to recognizing entailment, that in its turn
was based on the well-known noisy channel model
from Statistical Machine Translation (Brown et
al., 1990). In the work of this paper, by contrast,
we look at only identifying a hypernymy-related
Text, so the problem reduces to one of classifica-
tion over the Text.
2.2 Hypernymy Extraction
The aim of work on hypernymy extraction is usu-
ally the enrichment of a lexical resource such as
WordNet, or creation of specific hierarchical lex-
ical data directly for the purpose of some appli-
cation, such as information extraction or ques-
tion answering. There can be found several ap-
proaches to the task of hypernymy extraction: co-
occurrence approaches, asymmetric association
measures, and pattern-based methods.
Cooccurence Approaches Co-occurrence ap-
proaches first cluster words into similarity classes
and consider the elements of a class to be sib-
lings of one parent. Therefore the search for a
parent for some members from the class gives a
parent for the other members of the class. The
first work that introduced co-occurrence methods
to the field is that of Caraballo (1999). First she
clusters nouns into groups based on conjunctive
and appositive data collected from the Wall Street
Journal. Nouns are grouped according to the sim-
ilarity of being seen with other nouns in conjunc-
tive and appositive relationships. In the second
stage, using some knowledge about which con-
juncts connect hypernyms reliably, a parent for a
group of nouns is searched for in the same text cor-
pora. Other co-occurrence methods can be found
in works by Pantel et al (2004) and Pantel and
Ravichandran (2004).
Asymmetric Association Measures In Asym-
metric Association (see Dias et al (2008)) hy-
pernymy is derived through the measure of how
much one word ?attracts? another one. When hear-
ing ?fruit?, more common fruits will be likely to
come into mind such as ?apple? or ?banana?. In
this case, there exists an oriented association be-
tween ?fruit? and ?mango? (mango? fruit) which
indicates that ?mango? attracts ?fruit? moreso
than ?fruit? attracts ?mango?. As a consequence,
?fruit? is more likely to be a more general term
than ?mango?.
Pattern-based Methods Pattern-based methods
are based on the observation that hypernyms tend
to be connected in the sentences by specific words
or patterns, and that some patterns can predict
hypernymy with very high probability, like the
X and other Y pattern. Generally, some amount
of manual work on finding the seed patterns is
done first. Automated algorithms use these pat-
terns for discovering more patterns and for the
subsequent hypernymy extraction. The fundamen-
tal work for the pattern-based approaches is that of
Hearst (1992). More recently, Snow et al (2005)
and Snow et al (2006) have described a method of
hypernymy extraction using machine learning of
53
patterns. Pattern-based methods are known to be
successfully used for the creation of hierarchical
data for other languages as well, such as Dutch;
for example, see Tjong Kim Sang and Hofmann
(2007). For our purposes, pattern-based methods
are particularly suitable, as we have as context two
words and a single pattern connecting them; we
thus describe these approaches in more detail.
In her early work on pattern-based hypernymy
extraction Hearst (1992) noticed that a particular
semantic relationship between two nouns in the
sentence can be indicated by the presence of cer-
tain lexico-syntactic patterns linking those nouns.
Hypernymy (is-a, is a kind of relation) is one such
relationship.
Linking two noun phrases via the patterns
such NP
y
as NP
x
often implies that NP
x
is a
hyponym of NP
y
, that is NP
x
is a kind of NP
y
.
She gives the following example to illustrate the
patterns
(2) The bow lute, such as the Bambara ndang, is
plucked and has an individual curved neck
for each string.
Hearst comments that most fluent readers of En-
glish who have never before encountered the term
Bambara ndang will nevertheless from this sen-
tence infer that a Bambara ndang is a kind of bow
lute. This is true even if the reader has only a fuzzy
conception of what a bow lute is. The complete
set of patterns semi-automatically found by Hearst
are:
1. NP
y
and other NP
x
2. NP
y
or other NP
x
3. NP
y
such as NP
x
4. such NP
y
as NP
x
5. NP
y
including NP
x
6. NP
y
, especially NP
x
Snow et al (2005) had the aim of building upon
Hearst?s work in order to extend the WordNet
semantic taxonomy by adding to it hypernym-
hyponym pairs of nouns that are connected by a
wider set of lexico-syntactic pairs. They devel-
oped an automatic approach for finding hypernym-
hyponym pairs of nouns in the text corpus without
a set of predefined patterns.
The work was carried out on a corpus of 6 mil-
lion newswire sentences. Every pair of nouns
(n
i
, n
j
) in the sentence was extracted. The pairs
were labelled as Known Hypernym pair if n
j
is
an ancestor of the first sense of n
i
in the WordNet
hypernym taxonomy (Fellbaum, 1998). A noun
pair might have been assigned to the second set
of Known Non-Hypernym pairs if both nouns are
contained within WordNet, but neither noun is an
ancestor of the other in the WordNet hypernym
taxonomy for any senses of either noun. Each
sentence was parsed using MINIPAR. The depen-
dency relations between n
i
and n
j
constituted the
lexico-syntactic patterns connecting Known Hy-
pernyms or Known Non-Hypernyms. The main
idea of their work was then to collect all the lexico-
syntactic patterns that may indicate the hypernymy
relation and use them as the features for a decision
tree to classify NP pairs as hypernym-hyponym or
not-hypernym-hyponym pairs.
Snow et al (2005) state in their work that the de-
pendency paths acquired automatically contained
all the patterns mentioned in Hearst (1992). The
comparison of the results of a classifier whose vec-
tors were created from all the patterns seen with
the Known Hypernyms in their corpus, and a clas-
sifier whose vectors contained only the patterns of
Hearst (1992), showed that the results of the for-
mer classifier are considerably better than that of
the latter one. In an RTE context where the en-
tailment recognition relies on recognising hyper-
nymy, an approach like this, where patterns ac-
quired from a corpus are used, could be useful; but
how it should best be adapted is not clear. That is
then the goal of this paper.
3 Hypernymy Entailment Type
3.1 Definition
We define Hypernymy Entailment to be an en-
tailment relationship where the is-a relationship
between two nouns in the hypothesis is ?hid-
den behind? the lexico-syntactic pattern connect-
ing them in the text. Being more precise, the
Text-Hypothesis pairs of interest have the follow-
ing characteristics:
1. The Hypothesis is a simple sentence. That is
a sentence that consists of a subject, a 3rd per-
son form the verb to be, and a direct object,
and that contains no subordinate clauses.
2. Both subject and object of the Hypothesis (or
in some cases their morphological variants)
are found in the text.
Thus, the hypernymy relationship is not stated in
the Text, but is hidden in the way the subject and
54
object of the Hypothesis are connected to each
other in the Text. Examples of the true hypernymy
entailment pairs are as follows:
2
(3) Text: Soon after the EZLN had returned to
Chiapas, Congress approved a different
version of the COCOPA Law, which did not
include the autonomy clauses, claiming they
were in contradiction with some
constitutional rights (private property and
secret voting); this was seen as a betrayal by
the EZLN and other political groups.
Hypothesis: EZLN is a political group.
Both EZLN and political groups are present in the
text sentence, and are connected by an is-a relation
in the hypothesis. The pattern and other and the
syntactical connection between the noun phrases
give a good indication that the noun phrases are in
the hypernym-hyponym relationship. An example
of a false hypernymy entailment pair is as follows:
(4) Text: Laboring side by side on the outer hull
of the station?s crew quarters, Vladimir
Dezhurov and Mikhail Turin mounted
science packages and two Eastman Kodak
Co. placards while U.S. astronaut Frank
Culbertson looked on from inside the
complex.
Hypothesis: Vladimir Dezhurov is a U.S.
astronaut.
3.2 Idea
In the case of Snow et al (2005) the main accent
is on automatic extraction of all the patterns that
might, even if not reliably on their own, predict
the hypernymy relation between two nouns. Their
task is, given a previously unseen pair of nouns,
to determine whether they are in a hypernymy re-
lationship, using a classifier whose feature values
are derived from many occurrences of acquired
patterns in a corpus.
In our own work we are put in the situation
where there is only one pattern that is available
to judge if two words are in a hypernym/hyponym
relation, not the whole text corpus as in the case
of Snow et al (2005). Thus, we are mostly inter-
ested in the prediction of the hypernymy using this
pattern that is available for us. The fact that the
named entities we are working with, such as per-
son, organization, location, are not that frequently
2
Examples (3) - (4) are taken from the RTE2 test corpus.
seen in any text corpora also shifts the accent onto
the pattern rather than on the word pair itself. As
well as the fact that even in the case when two
words are hypernym-hyponym, that may not fol-
low at all from the sentence that they are seen in;
and non hypernym-hyponym pair can be used as
such in a metaphoric expression or just in a par-
ticular sentence we are dealing with. To illustrate,
consider example (5):
(5) Text: Note that the auxiliary verb function
derives from the copular function; and,
depending on one?s point of view, one can
still interpret the verb as a copula and the
following verbal form as being adjectival.
Hypothesis: A copular is a verb.
Snow et al (2005) aim to determine whether
copular and verb are in a hypernymy relation; to
this end they use the as a pattern as in this exam-
ple, along with all others throughout the corpus.
The reliability of the as a pattern (which as it turns
out is quite high) adds weight to the accumulated
evidence, but is not the sole evidence. In the in-
dividual case, however, it can be incorrect, as in
example (6):
(6) Text: In the 1980s, Minneapolis took its
place as a center of the arts, with the Walker
Arts Center leading the nation in
appreciation of pop and postmodern art , and
a diverse range of musicians, from Prince to
H?usker D?u to the Replacements to the
Suburbs to Soul Asylum keeping up with the
nation in musical innovation.
Hypothesis: A centre is a place.
Example (6) has a similar structure to exam-
ple (5), but center governs a preposition of after
it, that seem to make the hypernymy more doubt-
ful in this context. Taking into account all of the
above, the major focus of the work has shifted for
us from the word pair to the environment it has oc-
curred in. Thus, we use the major ideas from the
work of Snow et al (2005), but as we show be-
low, it is necessary to develop a more complex set
of counts in order to apply this to our entailments
type. In particular, we expect that the division of
patterns into lexical and syntactic parts, in order to
score them separately, is beneficial for entailment.
Again, it is a result of scarcity of information: we
have only one text sentence, not the whole text cor-
pus to make the entailment decision.
55
4 Experimental Setup
4.1 Data
Our goal is to build a classifier that will detect
whether a given potential hypernymy entailment
pair is true or false; we first need to construct sets
of such pairs for training and testing. As our ba-
sic data source, we use 500 000 sentences from
the Wikipedia XML corpus (Denoyer and Galli-
nari, 2006); this is the corpus used by Akhmatova
and Dras (2007), and related to one used in one
set of experiments by Snow et al (2005). These
sentences were parsed with the MINIPAR parser.
We identified Known Hypernym pairs as did
Snow et al (2005) (see Section 2.2); of our ba-
sic corpus, 13310 sentences contained Known Hy-
pernyms. From these sentences we extracted the
dependency relations between the Known Hyper-
nyms, of which there were 166 different types; we
refer to these as syntactic patterns hereafter.
We reserved 259 of these sentences to construct
a test set for our approach, as described below.
These sentences were selected randomly in pro-
portion to the syntactic patterns occurring in the
overall set. The remaining sentences constituted
our SYNTACTIC PATTERN TRAINING SET. For the
test set, these sentences constituted the Texts; to
derive the Hypotheses, we extracted the Known
Hypernyms and connected them by is a. These
sentences were annotated with yes if they entail
hypernymy, and no otherwise; the resulting anno-
tated data has 2:1 ratio of no to yes. The main
annotation was carried out by the first author, with
the second author carrying out a separate annota-
tion to evaluate agreement. The number of items
where there was agreement was 206, giving a ?
of 0.54. This is broadly in line with the ? found in
construction of the RTE datasets (? = 0.6) (Glick-
man, 2006) where it is characterized as ?moder-
ate agreement?, based on Landis and Koch (1977).
Results later are presented for both the overall set
of 259 (based on the first author?s original annota-
tions) and for the subset with agreement of 206.
As our additional, much larger data source for
deriving purely lexical patterns and associated
scores, we use the Web1T n-gram corpus (Brants
and Franz, 2006), which provides n-grams and
their counts for up to 5-grams inclusive. We use
these n-grams to get the lexical patterns of length
1, 2 and 3 that connect Known Hypernyms and
Known Non-Hypernyms correspondingly. The
length is up to 3 as we need 2 slots for the nouns
from the pair itself. The counts are extracted with
the help of the software get1t written by Hawker
et al (2007). We refer to this as our LEXICAL PAT-
TERN TRAINING SET.
4.2 Baselines
We use two baselines. The first is a simple most-
frequent one, choosing always false (noting from
Section 4.1 that this is more common by a ratio
of approximately 2:1). For the second one, we at-
tempt to use the idea of Snow et al (2005) in a
straightforward way. We note again that the fixed
context for a given Known Hypernym pair that we
have, unlike Snow et al (2005), is the single Text;
we therefore cannot apply the classifier from that
work directly. Our second baseline based on their
approach is as follows. For each sentence we look
at all nouns it contains. If a pair of nouns from the
sentence is a Known-Hypernym pair we save the
lexical pattern connecting the nouns and the syn-
tactic pattern between the nouns in a pattern list.
We take into account only those syntactic patterns
that have been seen in the corpus at least three
times. We then consider that a test entailment pair
is a true entailment if both the lexical pattern be-
tween the nouns in question and the syntactic con-
nection between them is found in the list.
4.3 Two-Part Model
We now propose a two-component model to com-
pensate for the fixed context. The first component,
score
lex
, involves the use of the lexical pattern to
predict hypernymy. Unless we know something
else about the structure of the text sentence, the
pattern (a sequence of words) that connects two
entities in question is the only evidence of the pos-
sible hypernym-hyponym relation between them.
It does not guarantee the relation itself, but the
more probable it is that the pattern predicts hyper-
nymy, the more probable it is that the entailment
relation between the Text and Hypothesis holds.
To motivate the second component, we take as an
example the patternNP
y
and other NP
x
, the first
of the Hearst (1992) patterns and a good predictor
of hypernymy, and consider the following exam-
ples:
(7) Text: Mr. Smith and other employees stayed
in the office.
Hypothesis: Mr. Smith is an employee.
(8) Text: I talked to Mr. Smith and other
56
employees stayed in the office.
Hypothesis: Mr. Smith is an employee.
Mr. Smith and an employee are connected in
both cases by and other. We know that the pat-
tern and other is a good indicator of the hyper-
nymy relation. The probability of the pattern and
other to predict the hypernymy relation is the prior
probability of the entailment relation in a text-
hypothesis pair. As can be seen in examples (7)
and (8), there is an entailment relationship only in
example (7); in example (8) entailment does not
hold.
The second component score
synt
is an indica-
tor of the syntactic possibility of the entailment
relationship. Hypernym-hyponyms tend to be in
certain syntactic relations in the sentence, such
as being subjects of the same verb, for example,
in the cases where we can decide on the relation
of the hypernymy between them. Other syntac-
tic relationships, even though they may connect
hypernym and hyponym, do not allow us to con-
clude that there is a hypernymy relation between
the words. As it can be seen from examples (7)
and (8), every syntactical relation has its own level
of certainty about the hypernym relation between
Mr. Smith and an employee, and therefore about
the fact that the Text entails the Hypothesis.
4.3.1 Lexical Patterns
From our lexical pattern training corpus, we de-
rived for both Known Hypernym and Known Non-
Hypernym pairs, the counts of both tokens (to-
tal number of pairs connected) and types (num-
ber of different pairs connected). To illustrate, we
take two example pairs, w
1
= rock and w
2
=
material, and w
1
= rice and w
2
= grain. We
find rock , and other material occurs 47 times, and
rice , and other grain 166 times. Totalling these,
that would give us the following statistics for the
pattern , and other: seen with the Known Hyper-
nyms 213 times (total of tokens), connecting 2 dif-
ferent pairs (total of types). We hypothesize that
knowing the number of different types of patterns
will be important as a way of compensating for the
more limited context relative to Snow et al (2005)
which used only the number of pattern tokens.
The above can be illustrated by the counts ob-
tained for patterns of Hearst (1992); see the first
five rows of Table 1. One can see from the
first three examples that in all cases the number
of times the pattern has been seen with Known
Hypernyms is overwhelmingly higher than with
that of Known Non-Hypernyms. Even more ex-
tremely, in the next two examples in Table 1,
Known Non-Hypernyms were not seen with these
patterns at all. We contrast these with the non-
Hearst patterns (extracted from our lexical pattern
corpus) in the last two rows. As one can see,
the patterns and detailed travel and online game
caribbean have been seen only with the Known
Hypernyms, and the frequency counts are very
close to that of the pattern , especially. Both pat-
terns however have connected the constituents of
only one Known Hypernyms pair. That puts some
doubt on the general reliability of the pattern to
make hypernymy judgements.
We then define our scoring metric, based on
the following quantities: C(h-tok), the number of
times the pattern has been seen with Known Hy-
pernyms; C(nh-tok), the number of times the pat-
tern has been seen with Known Non-Hypernyms;
C(h-type), the number of times the pattern has
been seen with different Known Hypernym pat-
terns; C(nh-type), the number of times the pat-
tern has been seen with different Known Non-
Hypernym patterns. We then define our lexical
scoring function as follows:
score
lex
=
C(h-tok)
C(h-tok) + C(nh-tok)
?
C(h-type)
C(h-type) + C(nh-type)
We use it to score patterns where the number
of times the pattern has been seen with different
Known Hypernyms (C(h-type)) is greater than a
threshold, here 5; for patterns below this thresh-
old, the score is 0. We determined on this scoring
function in comparison to others (notably using
only token proportions, the first term in the scor-
ing function above) by using them to rank patterns
and then assess the relative ranking of the Hearst
patterns among all others. Under the scoring func-
tion above, the Hearst patterns were ranked high-
est, with patterns or other, such as and and other
taking the first, second and third positions respec-
tively.
4.3.2 Syntactic Patterns
To estimate the probability of various syntactic
patterns from our syntactic pattern training cor-
pus, ideally we would annotate every sentence as
57
Table 1: Counts for the patterns of Hearst (1992) obtained from the Web1T corpus
seen with
Pattern
Hypernyms Non- Different Different
Hypernyms Hypernyms Non-Hypernyms
NP
y
and other NP
x
172036 1716 486 3
NP
y
or other NP
x
421083 1016 965 11
NP
y
such as NP
x
86158 384 355 4
NP
y
including NP
x
68098 0 251 0
NP
y
, especially NP
y
10236 0 80 0
NP
y
and detailed travel NP
x
9870 0 1 0
NP
y
online game caribbean NP
x
9874 0 1 0
true or false according to whether the hypernymy
is entailed from the sentence or not. The annota-
tion would allow the calculation of the likelihood
for every syntactical relation to indicate the entail-
ment relationship.
It is quite a time-consuming task to annotate
enough data to get reliable counts for all the syn-
tactical patterns. Therefore, as an approximate
first step we have divided all the sentences into
three groups according to the type of a lexical pat-
terns that connects a pair of Known Hypernyms:
Hearst patterns; the patterns that were found from
our lexical pattern training corpus; and all other
patterns. We have assumed that Hearst patterns,
as being a good indication of hypernymy, may in
most cases predict entailment as well; the auto-
matically derived lexical patterns may still some-
time predict entailment, but less well than the
Hearst patterns; and the unknown patterns are not
considered to be good predictors of the entailment
at all. Thus, for the initial estimate of the syntac-
tical probabilities of the entailment we have em-
ployed a very coarse approximation of the max-
imum likelihood estimate of the probability of a
syntactic pattern implying an entailment, weight-
ing these three groups with the values 1, 0.5 and 0
respectively. This leads to a score as follows:
score
synt-basic
= 0.5?
C(automatic lexical pattern)
C(all patterns)
+ 1.0?
C(Hearst pattern)
C(all patterns)
where C(X) represents the count of occurrences
of the pattern type X .
As a more refined scoring metric, we identi-
fied the set of the most frequent syntactic patterns
Table 2: Syntactic Pattern Probabilities
Pattern Basic P Improved P
obj 0.34 0.0
pcomp-n mod 0.40 0.038
appo 0.73 0.90
conj 0.76 0.10
mod pcomp-n 0.64 0.38
mod pcomp-n mod 0.45 0.023
mod conj 0.97 0.10
Table 3: Model Evaluation (full set of 259 / agreed
subset of 206)
Model Accuracy
Baseline (most frequent) 69% / 70%
Baseline (Snow) 71% / 72%
Lexical component only 60% / 60%
Improved syntactic component only 67% / 69%
Lexical and Basic Syntactic Component 76% / 73%
Lexical and Improved Syntactic Component 82% / 83%
and annotated data for them, in order to improve
their probability estimates. Taking the seven most
frequent, we annotated 100 randomly chosen sen-
tences for each of the syntactical patterns contain-
ing them from the syntactic pattern training cor-
pus. As a result of the annotation the probabilities
of the syntactical patterns to indicate entailment
has changed. The basic probabilities and the re-
vised probabilities for these seven syntactic pat-
terns can be found in Table 2.
4.4 Results and Discussion
We combine the lexical and syntactic scores as
features to the J48 decision tree of WEKA (Wit-
58
ten and Frank, 1999). Our evaluation is a 10-fold
cross-validation on the test set. Results are as in
Table 3, presented for both the full test set of 259
and for the subset with agreement of 206.
We note first of all that the simple approach de-
rived from Snow et al (2005), as described in Sec-
tion 4.2, does perform marginally better than the
baseline of choosing always false. The lexical or
syntactic components alone do not perform better
than the most-frequent baseline approach. This
is expected, as that approach includes both lexi-
cal and syntactic components. The lexical com-
bined with the basic syntactic component does im-
prove over the baselines. However, the lexical
combined with the improved syntactic component
experiences a much higher improvement. Overall,
the results for the full set and for the subset are
broadly the same, showing the same relative be-
haviour.
The lexical only component falsely recognizes
examples such as example (9) as true, as it has no
support of syntax. Just a comma by itself suffi-
ciently frequently indicates entailment in case of
apposition, so the lexical component is misled.
(9) Text: There were occasional outbreaks of
violence, but most observers considered it
remarkable that such an obvious breakdown
of the capitalist system had not led to a rapid
growth of socialism, communism, or fascism
(as happened for example in Germany).
Hypothesis: Communism is a socialism.
Syntax only, even though it prevents the mis-
takes of the lexical-only component for the exam-
ples above, introduces its own mistakes. Knowing
that the subject and object in the Hypothesis are
linked by direct dependency relations to a prepo-
sition in the Text is useful, but without a lexical
pattern can be too permissive, as in example (10):
(10) Text: However, Griffin attracted criticism for
writing in the aftermath of the bombing of
the Admiral Duncan pub bombing (which
killed three people, including a pregnant
woman) that the gay people protesting
against the murders were ?flaunting their
perversion in front of the world?s journalists,
and showed just why so many ordinary
people find these creatures disgusting?.
Hypothesis: Criticism is a writing.
Both baseline and the final hypernymy entailment
engine work well in the cases where the counts for
or against entailment are very high, as in examples
(11) and (12), which are correctly recognized as a
true and a false entailment by both systems.
(11) Text: Carbon compounds form the basis of
all life on Earth and the carbon-nitrogen
cycle provides some of the energy produced
by the sun and other stars.
Hypothesis: Sun is a star.
(12) Text: In 1792 British explorer George
Vancouver set up a small settlement near the
village of Yerba Buena (later downtown San
Francisco) which became a small base for
English, Russian, and other European fur
traders, explorers, and settlers.
Hypothesis: Village is a settlement.
The final hypernymy system works better for more
marginal cases, such as example (13).
(13) Text: The trials were held in the German city
of Nuremberg from 1945 to 1949 at the
Nuremberg Palace of Justice.
Hypothesis: Nuremberg is a city.
The pattern of can not be called a good hint for hy-
pernymy, but in some special cases, like that of the
city and its name, the hypernymy is obvious. Divi-
sion into lexical and syntactic parts helped in dis-
covering the pattern and adjusting better its prob-
ability of entailing hypernymy. All this supports
our idea that to compensate for the lack of infor-
mation in the case of RTE the lexico-syntactic pat-
terns should be divided into their lexical and syn-
tactic components.
5 Conclusion
In this paper we have shown how work in hyper-
nymy acquisition can be adapted to tackle a spe-
cific subtype of related entailment problem. Fol-
lowing work by Snow et al (2005), we have de-
fined an obvious first adaptation which nonethe-
less marginally improves over the baseline. We
have then shown that by separating lexical and
syntactic patterns we can obtain a significant im-
provement on the entailment classification accu-
racy. In our future work we aim to construct a
baseline generic RTE engine and test its perfor-
mance with and without this and other components
in order to analyse the work of a component-based
model as a whole. The approach also suggests that
adapting work from other areas of NLP for entail-
ment subclasses is promising.
59
References
Elena Akhmatova and Mark Dras. 2007. Entailment
due to syntactically encoded semantic relationships.
In Proceedings of ALTA-2007, pages 4?12.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In The second PASCAL
Recognising Textual Entailment Challenge, pages 3?
11, Venice, Italy.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram corpus version 1. Technical report, Google
Research.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
In Computational Linguistics, volume 16, pages 79
? 85.
Elena Cabrio, Milen Kouylekov, and Bernardo
Magnini. 2008. Combining specialized entailment
engines for RTE-4. In Proceedings of TAC-2008.
Sharon Caraballo. 1999. Automatic acquisition of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of ACL-99, pages 120?126.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Quionero-Candela, J.; Dagan, I.;
Magnini, B.; d?Alch-Buc, F. (Eds.) Machine Learn-
ing Challenges. Lecture Notes in ComputerScience,
volume 3944, pages 177 ? 190. Springer.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. In SIGIR Forum, 40(1),
pages 64?69.
Ga?el Dias, Raycho Mukelov, and Guillaume Cleuziou.
2008. Unsupervised learning of general-specific
noun relations from the web. In Proceedings of
FLAIRS Conference, pages 147?152.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Oren Glickman. 2006. Applied Textual Entailment.
Ph.D. thesis, Bar Ilan University.
Tobias Hawker, Mary Gardiner, and Andrew Ben-
netts. 2007. Practical queries of a massive n-gram
database. In Proceedings of ALTA-2007, pages 40?
48.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, pages
539?545.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT/NAACL-04, pages 321?328.
Patrick Pantel, Deepak Ravichandran, and Eduard
Hovy. 2004. Towards terascale semantic acquisi-
tion. In Proceedings of Coling 2004, pages 771?
777, Geneva, Switzerland, Aug 23?Aug 27.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
L?eon Bottou, editors, Advances in Neural Informa-
tion Processing Systems 17, pages 1297?1304, Cam-
bridge, MA. MIT Press.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of ACL-2006, pages 801?
808.
E.F. Tjong Kim Sang and K. Hofmann. 2007. Auto-
matic extraction of dutch hypernym-hyponym pairs.
In Proceedings of CLIN-2006, Leuven, Belgium.
LOT, Netherlands Graduate School of Linguistics.
Lucy Vanderwende andWilliam B. Dolan. 2005. What
syntax can contribute in the entailment task. In Pro-
ceedings of MLCW, pages 205?216.
Lucy Vanderwende, Arul Menezes, and Rion Snow.
2006. Microsoft research at RTE-2: Syntactic con-
tributions in the entailment task: an implementation.
In Proceedings of 2nd PASCAL Challenges Work-
shop on Recognizing Textual Entailment.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
60
Multi-Component TAG and Notions of Formal Power
William Schuler, David Chiang
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
fschuler,dchiangg@linc.cis.upenn.edu
Mark Dras
Inst. for Research in Cognitive Science
University of Pennsylvania
Suite 400A, 3401 Walnut Street
Philadelphia, PA 19104-6228
madras@linc.cis.upenn.edu
Abstract
This paper presents a restricted version
of Set-Local Multi-Component TAGs
(Weir, 1988) which retains the strong
generative capacity of Tree-Local Multi-
Component TAG (i.e. produces the
same derived structures) but has a
greater derivational generative capacity
(i.e. can derive those structures in more
ways). This formalism is then applied as
a framework for integrating dependency
and constituency based linguistic repre-
sentations.
1 Introduction
An aim of one strand of research in gener-
ative grammar is to nd a formalism that
has a restricted descriptive capacity sucient
to describe natural language, but no more
powerful than necessary, so that the reasons
some constructions are not legal in any nat-
ural language is explained by the formalism
rather than stipulations in the linguistic the-
ory. Several mildly context-sensitive grammar
formalisms, all characterizing the same string
languages, are currently possible candidates
for adequately describing natural language;
however, they dier in their capacities to as-
sign appropriate linguistic structural descrip-
tions to these string languages. The work in
this paper is in the vein of other work (Joshi,
2000) in extracting as much structural de-
scriptive power given a xed ability to de-
scribe strings, and uses this to model depen-
dency as well as constituency correctly.
One way to characterize a formalism's de-
scriptive power is by the the set of string lan-
guages it can generate, called its weak gener-
ative capacity. For example, Tree Adjoining
Grammars (TAGs) (Joshi et al, 1975) can
generate the language a
n
b
n
c
n
d
n
and Context-
Free Grammars (CFGs) cannot (Joshi, 1985).
S
a  b
S
a S
a  b
b
S
a S
a S
a  b
b
b
: : :
Figure 1: CFG-generable tree set for a
n
b
n
.
S
a S
b 
S
a S
a S
b S
b 
S
a S
a S
a S
b S
b S
b 
: : :
Figure 2: TAG-generable tree set for a
n
b
n
.
However, weak generative capacity ignores
the capacity of a grammar formalism to gener-
ate derived trees. This is known as its strong
generative capacity. For example, CFGs and
TAGs can both generate the language a
n
b
n
,
but CFGs can only associate the a's and b's
by making them siblings in the derived tree,
as shown in Figure 1, whereas a TAG can gen-
erate the innite set of trees for the language
a
n
b
n
that have a's and b's as siblings, as well
as the innite set of trees where the a's dom-
inate the b's in each tree, shown in Figure 2
(Joshi, 1985); thus TAGs have more strong
generative capacity than CFGs.
In addition to the tree sets and string lan-
guages a formalism can generate, there may
also be linguistic reasons to care about how
these structures are derived. For this reason,
multi-component TAGs (MCTAGs) (Weir,
1988) have been adopted to model some
linguistic phenomena. In multi-component
TAG, elementary trees are grouped into tree
sets, and at each step of the derivation all the
trees of a set adjoin simultaneously. In tree-
local MCTAG (TL-MCTAG) all the trees of
a set are required to adjoin into the same
elementary tree; in set-local MCTAG (SL-
MCTAG) all the trees of a set are required
to adjoin into the same elementary tree set.
TL-MCTAGs can generate the same string
languages and derived tree sets as ordinary
TAGs, so they have the same weak and strong
generative capacities, but TL-MCTAGs can
derive these same strings and trees in more
than TAGs can. One motivation for TL-
MCTAG as a linguistic formalism (Frank,
1992) is that it can generate a functional head
(such as does) in the same derivational step
as the lexical head with which it is associated
(see Figure 3) without violating any assump-
tions about the derived phrase structure tree
{ something TAGs cannot do in every case.

seem
:
S
does
S
.
.
.
VP
seem VP

sleep
:
S
John
VP
to sleep

sleep

seem
S
does S
John VP
seem VP
to sleep
Figure 3: TL-MCTAG generable derivation
This notion of the derivations of a gram-
mar formalism as they relate to the struc-
tures they derive has been called the deriva-
tional generative capacity (1992). Somewhat
more formally (for a precise denition, see
Becker et al (1992)): we annotate each ele-
ment of a derived structure with a code indi-
cating which step of the derivation produced
that element. This code is simply the address
of the corresponding node in the derivation
tree.
1
Then a formalism's derivational gener-
ative capacity is the sets of derived structures,
thus annotated, that it can generate.
1
In Becker et al (1992) the derived structures were
always strings, and the codes were not addresses but
unordered identiers. We trust that our denition is
in the spirit of theirs.
The derivational generative capacity of a
formalism also describes what parts of a de-
rived structure combine with each other. Thus
if we consider each derivation step to corre-
spond to a semantic dependency, then deriva-
tional generative capacity describes what
other elements a semantic element may de-
pend on. That is, if we interpret the derivation
trees of TAG as dependency structures and
the derived trees as phrase structures, then
the derivational generative capacity of TAG
limits the possible dependency structures that
can be assigned to a given phrase structure.
1.1 Dependency and Constituency
We have seen that TL-MCTAGs can gener-
ate some derivations for \Does John seem
to sleep" that TAG cannot, but even TL-
MCTAG cannot generate the string, \Does
John seem likely to sleep" with a derived tree
that matches some linguistic notion of correct
constituency and a derivation that matches
some notion of correct dependency. This is
because the components for `does' and `seem'
would have to adjoin into dierent compo-
nents of the elementary tree set for `likely'
(see Figure 4), which would require a set-local
multi-component TAG instead of tree-local.

seem
:
S
does
S
.
.
.
VP
seem VP

likely
:
S
.
.
.
VP
likely VP

sleep
:
S
John VP
to sleep

sleep

likely

seem
Figure 4: SL-MCTAG generable derivation
Unfortunately, unrestricted set-local multi-
component TAGs not only have more deriva-
tional generative capacity than TAGs, but
they also have more weak generative capac-
ity: SL-MCTAGs can generate the quadru-
ple copy language wwww, for example, which
does not correspond to any known linguis-
tic phenomenon. Other formalisms aiming to
model dependency correctly similarly expand
weak generative capacity, notably D-tree Sub-
stitution Grammar (Rambow et al, 1995),
and consequently end up with much greater
parsing complexity.
The work in this paper follows another
Figure 5: Set-local adjunction.
line of research which has focused on squeez-
ing as much strong generative capacity as
possible out of weakly TAG-equivalent for-
malisms. Tree-local multicomponent TAG
(Weir, 1988), nondirectional composition
(Joshi and Vijay-Shanker, 1999), and seg-
mented adjunction (Kulick, 2000) are exam-
ples of this approach, wherein the constraint
on weak generative capacity naturally limits
the expressivity of these systems. We discuss
the relation of the formalism of this paper,
Restricted MCTAG (R-MCTAG) with some
of these in Section 5.
2 Formalism
2.1 Restricting set-local MCTAG
The way we propose to deal with multi-
component adjunction is rst to limit the
number of components to two, and then,
roughly speaking, to treat two-component
adjunction as one-component adjunction by
temporarily removing the material between
the two adjunction sites. The reasons behind
this scheme will be explained in subsequent
sections, but we mention it now because it
motivates the somewhat complicated restric-
tions on possible adjunction sites:
 One adjunction site must dominate the
other. If the two sites are 
h
and 
l
, call
the set of nodes dominated by one node
but not strictly dominated by the other
the site-segment h
h
; 
l
i.
 Removing a site-segment must not de-
prive a tree of its foot node. That is, no
site-segment h
h
; 
l
i may contain a foot
node unless 
l
is itself the foot node.
 If two tree sets adjoin into the same tree,
the two site-segments must be simulta-
neously removable. That is, the two site-
segments must be disjoint, or one must
contain the other.
Because of the rst restriction, we depict
tree sets with the components connected by
a dominance link (dotted line), in the man-
ner of (Becker et al, 1991). As written, the
above rules only allow tree-local adjunction;
we can generalize them to allow set-local ad-
junction by treating this dominance link like
an ordinary arc. But this would increase the
weak generative capacity of the system. For
present purposes it is sucient just to allow
one type of set-local adjunction: adjoin the
upper tree to the upper foot, and the lower
tree to the lower root (see Figure 5).
This does not increase the weak generative
capacity, as will be shown in Section 2.3. Ob-
serve that the set-local TAG given in Figure 5
obeys the above restrictions.
2.2 2LTAG
For the following section, it is useful to think
of TAG in a manner other than the usual.
Instead of it being a tree-rewriting system
whose derivation history is recorded in a
derivation tree, it can be thought of as a set
of trees (the `derivation' trees) with a yield
function (here, reading o the node labels of
derivation trees, and composing correspond-
ing elementary trees by adjunction or sub-
stitution as appropriate) applied to get the
TAG trees. Weir (1988) observed that several
TAGs could be daisy-chained into a multi-
level TAG whose yield function is the com-
position of the individual yield functions.
More precisely: a 2LTAG is a pair of
TAGs hG;G
0
i = hh;NT ; I; A; Si; hI [ A; I [
A; I
0
; A
0
; S
0
ii.
We call G the object-level grammar, and
G
0
the meta-level grammar. The object-level
grammar is a standard TAG:  and NT are
its terminal and nonterminal alphabets, I and
A are its initial and auxiliary trees, and S 2 I
contains the trees which derivations may start
with.
The meta-level grammar G
0
is dened so
that it derives trees that look like derivation
trees of G:
 Nodes are labeled with (the names of)
elementary trees of G.
 Foot nodes have no labels.
 Arcs are labeled with Gorn addresses.
2
2
The Gorn address of a root node is ; if a node has
Gorn address , then its ith child has Gorn address


Figure 6: Adjoining into  by removing 

.
 An auxiliary tree may adjoin anywhere.
 When a tree  is adjoined at a node ,  is
rewritten as , and the foot of  inherits
the label of .
The tree set of hG;G
0
i, T (hG;G
0
i), is
f
G
[T (G
0
)], where f
G
is the yield function of
G and T (G
0
) is the tree set of G
0
. Thus, the
elementary trees of G
0
are combined to form
a derived tree, which is then interpreted as a
derivation tree for G, which gives instructions
for combining elementary trees of G into the
nal derived tree.
It was shown in Dras (1999) that when the
meta-level grammar is in the regular form of
Rogers (1994) the formalism is weakly equiv-
alent to TAG.
2.3 Reducing restricted R-MCTAG
to RF-2LTAG
Consider the case of a multicomponent tree
set f
1
; 
2
g adjoining into an initial tree 
(Figure 6). Recall that we dened a site-
segment of a pair of adjunction sites to be all
the nodes which are dominated by the upper
site but not the lower site. Imagine that the
site-segment 

is excised from , and that 
1
and 
2
are fused into a single elementary tree.
Now we can simulate the multi-component
adjunction by ordinary adjunction: adjoin the
fused 
1
and 
2
into what is left of ; then
replace 

by adjoining it between 
1
and 
2
.
The replacement of 

can be postponed
indenitely: some other (fused) tree set
f
1
0
; 
2
0
g can adjoin between 
1
and 
2
, and
so on, and then 

adjoins between the last
pair of trees. This will produce the same re-
sult as a series of set-local adjunctions.
More formally:
1. Fuse all the elementary tree sets of the
grammar by identifying the upper foot
  i.
with the lower root. Designate this fused
node the meta-foot.
2. For each tree, and for every possible com-
bination of site-segments, excise all the
site-segments and add all the trees thus
produced (the excised auxiliary trees and
the remainders) to the grammar.
Now that our grammar has been smashed
to pieces, we must make sure that the right
pieces go back in the right places. We could do
this using features, but the resulting grammar
would only be strongly equivalent, not deriva-
tionally equivalent, to the original. Therefore
we use a meta-level grammar instead:
1. For each initial tree, and for every pos-
sible combination of site-segments, con-
struct the derivation tree that will re-
assemble the pieces created in step (2)
above and add it to the meta-level gram-
mar.
2. For each auxiliary tree, and for every pos-
sible combination of site-segments, con-
struct a derivation tree as above, and for
the node which corresponds to the piece
containing the meta-foot, add a child, la-
bel its arc with the meta-foot's address
(within the piece), and mark it a foot
node. Add the resulting (meta-level) aux-
iliary tree to the meta-level grammar.
Observe that set-local adjunction corre-
sponds to meta-level adjunction along the
(meta-level) spine. Recall that we restricted
set-local adjunction so that a tree set can
only adjoin at the foot of the upper tree and
the root of the lower tree. Since this pair of
nodes corresponds to the meta-foot, we can
restate our restriction in terms of the con-
verted grammar: no meta-level adjunction is
allowed along the spine of a (meta-level) aux-
iliary tree except at the (meta-level) foot.
Then all meta-level adjunction is regular
adjunction in the sense of (Rogers, 1994).
Therefore this converted 2LTAG produces
derivation tree sets which are recognizable,
and therefore our formalism is strongly equiv-
alent to TAG.
Note that this restriction is much stronger
than Rogers' regular form restriction. This
was done for two reasons. First, the deni-
tion of our restriction would have been more
complicated otherwise; second, this restric-
tion overcomes some computational dicul-
ties with RF-TAG which we discuss below.
3 Linguistic Applications
In cases where TAG models dependencies cor-
rectly, the use of R-MCTAG is straightfor-
ward: when an auxiliary tree adjoins at a
site pair which is just a single node, it looks
just like conventional adjunction. However, in
problematic cases we can use the extra expres-
sive power of R-MCTAG to model dependen-
cies correctly. Two such cases are discussed
below.
3.1 Bridge and Raising Verbs
S
NP
John
VP
V
thinks
S
.
.
.
S
S
C
that
S
.
.
.
VP
V
seems
VP
S
NP
Mary
VP
V
to sleep
Figure 7: Trees for (1)
Consider the case of sentences which con-
tain both bridge and raising verbs, noted
by Rambow et al (1995). In most TAG-based
analyses, bridge verbs adjoin at S (or C
0
), and
raising verbs adjoin at VP (or I
0
). Thus the
derivation for a sentence like
(1) John thinks that Mary seems to
sleep.
will have the trees for thinks and seems si-
multaneously adjoining into the tree for like,
which, when interpreted, gives an incorrect
dependency structure.
But under the present view we can ana-
lyze sentences like (1) with derivations mir-
roring dependencies. The desired trees for (1)
are shown in Figure 7. Since the tree for that
seems can meta-adjoin around the subject,
the tree for thinks correctly adjoins into the
tree for seems rather than eat.
Also, although the above analysis produces
the correct dependency links, the directions
are inverted in some cases. This is a disad-
vantage compared to, for example, DSG; but
since the directions are consistently inverted,
for applications like translation or statistical
modeling, the particular choice of direction is
usually immaterial.
3.2 More on Raising Verbs
Tree-local MCTAG is able to derive (2a), but
unable to derive (2b) except by adjoining the
auxiliary tree for to be likely at the foot of the
auxiliary tree for seem (Frank et al, 1999).
(2) a. Does John seem to sleep?
b. Does John seem to be likely to
sleep?
The derivation structure of this analysis does
not match the dependencies, however|seem
adjoins into to sleep.
DSG can derive this sentence with a deriva-
tion matching the dependencies, but it loses
some of the advantage of TAG in that, for
example, cases of super-raising (where the
verb is raised out of two clauses) must be ex-
plicitly ruled out by subsertion-insertion con-
straints. Frank et al (1999) and Kulick (2000)
give analyses of raising which assign the de-
sired derivation structures without running
into this problem. It turns out that the anal-
ysis of raising from the previous section, de-
signed for a translation problem, has both
of these properties as well. The grammar is
shown back in Figure 4.
4 A Parser
Figure 8 shows a CKY-style parser for our
restriction of MCTAG as a system of inference
rules. It is limited to grammars whose trees
are at most binary-branching.
The parser consists of rules over items of
one of the following forms, where w
1
  w
n
is
the input; , 
h
, and 
l
specify nodes of the
grammar; i, j, k, and l are integers between 0
and n inclusive; and code is either + or  :
 [; code ; i; ; ; l; ; ] and
[; code ; i; j; k; l; ; ] function as in
a CKY-style parser for standard TAG
(Vijay-Shanker, 1987): the subtree
rooted by  2 T derives a tree whose
fringe is w
i
  w
l
if T is initial, or
w
i
  w
j
Fw
k
  w
l
if T is the lower
auxiliary tree of a set and F is the label
of its foot node. In all four item forms,
code = + i adjunction has taken place
at .
 [; code ; i; j; k; l; ; 
l
] species that the
segment h; 
l
i derives a tree whose
fringe is w
i
  w
j
Lw
k
  w
l
, where L is
the label of 
l
. Intuitively, it means that
a potential site-segment has been recog-
nized.
 [; code ; i; j; k; l; 
h
; 
l
] species, if  be-
longs to the upper tree of a set, that
the subtree rooted by , the segment
h
h
; 
l
i, and the lower tree concatenated
together derive a tree whose fringe is
w
i
  w
j
Fw
k
  w
l
, where F is the la-
bel of the lower foot node. Intuitively, it
means that a tree set has been partially
recognized, with a site-segment inserted
between the two components.
The rules which require dier from a TAG
parser and hence explanation are Pseudopod,
Push, Pop, and Pop-push. Pseudopod applies
to any potential lower adjunction site and is
so called because the parser essentially views
every potential site-segment as an auxiliary
tree (see Section 2.3), and the Pseudopod ax-
iom recognizes the feet of these false auxiliary
trees.
The Push rule performs the adjunction of
one of these false auxiliary trees|that is, it
places a site-segment between the two trees of
an elementary tree set. It is so called because
the site-segment is saved in a \stack" so that
the rest of its elementary tree can be recog-
nized later. Of course, in our case the \stack"
has at most one element.
The Pop rule does the reverse: every com-
pleted elementary tree set must contain a
site-segment, and the Pop rule places it back
where the site-segment came from, emptying
the \stack." The Pop-push rule performs set-
local adjunction: a completed elementary tree
set is placed between the two trees of yet an-
other elementary tree set, and the \stack" is
unchanged.
Pop-push is computationally the most ex-
pensive rule; since it involves six indices and
three dierent elementary trees, its running
time is O(n
6
G
3
).
It was noted in (Chiang et al, 2000) that
for synchronous RF-2LTAG, parse forests
could not be transferred in time O(n
6
). This
fact turns out to be connected to several prop-
erties of RF-TAG (Rogers, 1994).
3
3
Thanks to Anoop Sarkar for pointing out the rst
The CKY-style parser for regular form
TAG described in (Rogers, 1994) essentially
keeps track of adjunctions using stacks, and
the regular form constraint ensures that the
stack depth is bounded. The only kinds of ad-
junction that can occur to arbitrary depth are
root and foot adjunction, which are treated
similarly to substitution and do not aect the
stacks. The reader will note that our parser
works in exactly the same way.
A problem arises if we allow both root
and foot adjunction, however. It is well-known
that allowing both types of adjunction creates
derivational ambiguity (Vijay-Shanker, 1987):
adjoining 
1
at the foot of 
2
produces the
same derived tree that adjoining 
1
at the
root of 
2
would. The problem is not the am-
biguity per se, but that the regular form TAG
parser, unlike a standard TAG parser, does
not always distinguish these multiple deriva-
tions, because root and foot adjunction are
both performed by the same rule (analogous
to our Pop-push). Thus for a given application
of this rule, it is not possible to say which tree
is adjoining into which without examining the
rest of the derivation.
But this knowledge is necessary to per-
form certain tasks online: for example, enforc-
ing adjoining constraints, computing proba-
bilities (and pruning based on them), or per-
forming synchronous mappings. Therefore we
arbitrarily forbid one of the two possibilities.
4
The parser given in Section 4 already takes
this into account.
5 Discussion
Our version of MCTAG follows other
work in incorporating dependency into a
constituency-based approach to modeling
natural language. One such early integra-
tion involved work by Gaifman (1965), which
showed that projective dependency grammars
could be represented by CFGs. However, it
is known that there are common phenom-
ena which require non-projective dependency
grammars, so looking only at projective de-
such connection.
4
Against tradition, we forbid root adjunction, be-
cause adjunction at the foot ensures that a bottom-up
traversal of the derived tree will encounter elementary
trees in the same order as they appear in a bottom-up
traversal of the derivation tree, simplifying the calcu-
lation of derivations.
Goal: [
r
; ; 0; ; ; n; ; ] 
r
an initial root
(Leaf) [;+; i; ; ; j; ; ]  a leaf
(Foot) [;+; i; i; j; j; ; ]  a lower foot
(Pseudopod) [;+; i; i; j; j; ; ]
(Unary)
[
1
;+; i; p; q; j; 
h
; 
l
]
[; ; i; p; q; j; 
h
; 
l
]


1
(Binary 1)
[
1
;+; i; p; q; j; 
h
; 
l
] [
2
;+; j; ; ; k; ; ]
[; ; i; p; q; k; 
h
; 
l
]


1

2
(Binary 2)
[
1
;+; i; ; ; j; ; ] [
2
;+; j; p; q; k; 
h
; 
l
]
[; ; i; p; q; k; 
h
; 
l
]


1

2
(No adjunction)
[; ; i; p; q; j; 
h
; 
l
]
[;+; i; p; q; j; 
h
; 
l
]
(Push)
[
1
;+; j; p; q; k; ; ] [
h
; ; i; j; k; l; ; 
l
]
[;+; i; p; q; l; 
h
; 
l
]

.
.
.

1
(i.e.  is an upper foot
and 
1
is a lower root)
(Pop)
[
l
; ; j; p; q; k; 
h
0
; 
l
0
] [
r
;+; i; j; k; l; 
h
; 
l
]
[
h
;+; i; p; q; l; 
h
0
; 
l
0
]

r
a root of an upper tree
adjoinable at h
h
; 
l
i
(Pop-push)
[
1
;+; j; p; q; k; ; ] [
r
;+; i; j; k; l; 
h
; 
l
]
[;+; i; p; q; l; 
h
; 
l
]

.
.
.

1
, 
r
a root of an upper
tree adjoinable at
h; 
1
i
Figure 8: Parser
pendency grammars is inadequate. Follow-
ing the observation of TAG derivations' sim-
ilarity to dependency relations, other for-
malisms have also looked at relating depen-
dency and constituency approaches to gram-
mar formalisms.
A more recent instance is D-Tree Substi-
tution Grammars (DSG) (Rambow et al,
1995), where the derivations are also inter-
preted as dependency relations. Thought of
in the terms of this paper, there is a clear
parallel with R-MCTAG, with a local set
ultimately representing dependencies having
some yield function applied to it; the idea
of non-immediate dominance also appears in
both formalisms. The dierence between the
two is in the kinds of languages that they are
able to describe: DSG is both less and more
restrictive than R-MCTAG. DSG can gener-
ate the language count-k for some arbitrary
k (that is, fa
1
n
a
2
n
: : : a
k
n
g), which makes
it extremely powerful, whereas R-MCTAG
can only generate count-4. However, DSG
cannot generate the copy language (that is,
fww j w 2 

g with  some terminal al-
phabet), whereas R-MCTAG can; this may
be problematic for a formalism modeling nat-
ural language, given the key role of the copy
language in demonstrating that natural lan-
guage is not context-free (Shieber, 1985). R-
MCTAG is thus a more constrained relaxation
of the notion of immediate dominance in fa-
vor of non-immediate dominance than is the
case for DSG.
Another formalism of particular interest
here is the Segmented Adjoining Grammar of
(Kulick, 2000). This generalization of TAG is
characterized by an extension of the adjoining
operation, motivated by evidence in scram-
bling, clitic climbing and subject-to-subject
raising. Most interestingly, this extension to
TAG, proposed on empirical grounds, is de-
ned by a composition operation with con-
strained non-immediate dominance links that
looks quite similar to the formalism described
in this paper, which began from formal con-
siderations and was then applied to data. This
conuence suggests that the ideas described
here concerning combining dependency and
constituency might be reaching towards some
deeper connection.
6 Conclusion
From a theoretical perspective, extracting
more derivational generative capacity and
thereby integrating dependency and con-
stituency into a common framework is an in-
teresting exercise. It also, however, proves to
be useful in modeling otherwise problematic
constructions, such as subject-auxiliary inver-
sion and bridge and raising verb interleaving.
Moreover, the formalism developed from the-
oretical considerations, presented in this pa-
per, has similar properties to work developed
on empirical grounds, suggesting that this is
worth further exploration.
References
Tilman Becker, Aravind Joshi, and Owen Ram-
bow. 1991. Long distance scrambling and tree
adjoining grammars. In Fifth Conference of the
European Chapter of the Association for Com-
putational Linguistics (EACL'91), pages 21{26.
Tilman Becker, Owen Rambow, and Michael Niv.
1992. The derivational generative power of for-
mal systems, or, Scrambling is beyond LCFRS.
Technical Report IRCS-92-38, Institute for Re-
search in Cognitive Science, University of Penn-
sylvania.
David Chiang, William Schuler, and Mark Dras.
2000. Some Remarks on an Extension of Syn-
chronous TAG. In Proceedings of TAG+5,
Paris, France.
Mark Dras. 1999. A meta-level grammar: re-
dening synchronous TAG for translation and
paraphrase. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL '99).
Robert Frank, Seth Kulick, and K. Vijay-Shanker.
1999. C-command and extraction in tree-
adjoining grammar. Proceedings of the Sixth
Meeting on the Mathematics of Language
(MOL6).
Robert Frank. 1992. Syntactic locality and
tree adjoining grammar: grammatical acquisi-
tion and processing perspectives. Ph.D. the-
sis, Computer Science Department, University
of Pennsylvania.
Haim Gaifman. 1965. Dependency Systems and
Phrase-Structure Systems. Information and
Control, 8:304{337.
Gerald Gazdar. 1988. Applicability of indexed
grammars to natural languages. In Uwe Reyle
and Christian Rohrer, editors, Natural Lan-
guage Parsin and Linguistic Theories. D. Reidel
Publishing Company, Dordrecht, Holland.
Aravind Joshi and K. Vijay-Shanker. 1999. Com-
positional Semantics with Lexicalized Tree-
Adjoining Grammar (LTAG): How Much Un-
derspecication is Necessary? In Proceedings of
the 2nd International Workshop on Computa-
tional Semantics.
Aravind K. Joshi, Leon S. Levy, and M. Taka-
hashi. 1975. Tree adjunct grammars. Journal
of computer and system sciences, 10:136{163.
Aravind K. Joshi. 1985. How much context sen-
sitivity is necessary for characterizing struc-
tural descriptions: Tree adjoining grammars. In
L. Karttunen D. Dowty and A. Zwicky, editors,
Natural language parsing: Psychological, com-
putational and theoretical perspectives, pages
206{250. Cambridge University Press, Cam-
bridge, U.K.
Aravind Joshi. 2000. Relationship between strong
and weak generative power of formal systems.
In Proceedings of TAG+5, pages 107{114, Paris,
France.
Seth Kulick. 2000. A uniform account of locality
constraints for clitic climbing and long scram-
bling. In Proceedings of the Penn Linguistics
Colloquium.
Owen Rambow, David Weir, and K. Vijay-
Shanker. 1995. D-tree grammars. In Proceed-
ings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL '95).
James Rogers. 1994. Capturing CFLs with tree
adjoining grammars. In Proceedings of the 32nd
Annual Meeting of the Association for Compu-
tational Linguistics (ACL '94).
Stuart Shieber. 1985. Evidence against the
context-freeness of natural language. Linguis-
tics and Philosophy, 8:333{343.
K. Vijay-Shanker. 1987. A study of tree adjoining
grammars. Ph.D. thesis, Department of Com-
puter and Information Science, University of
Pennsylvania.
David Weir. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Department of Computer and In-
formation Science, University of Pennsylvania.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1600?1610,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exploiting Parse Structures for Native Language Identification
Sze-Meng Jojo Wong
Centre for Language Technology
Macquarie University
Sydney, Australia
sze.wong@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
mark.dras@mq.edu.au
Abstract
Attempts to profile authors according to their
characteristics extracted from textual data, in-
cluding native language, have drawn attention
in recent years, via various machine learn-
ing approaches utilising mostly lexical fea-
tures. Drawing on the idea of contrastive
analysis, which postulates that syntactic er-
rors in a text are to some extent influenced by
the native language of an author, this paper
explores the usefulness of syntactic features
for native language identification. We take
two types of parse substructure as features?
horizontal slices of trees, and the more gen-
eral feature schemas from discriminative parse
reranking?and show that using this kind of
syntactic feature results in an accuracy score
in classification of seven native languages of
around 80%, an error reduction of more than
30%.
1 Introduction
Inferring characteristics of authors from their tex-
tual data, often termed authorship profiling, has seen
a number of computational approaches proposed in
recent years. The problem is typically treated as a
classification task, where an author is classified with
respect to characteristics such as gender, age, native
language, and so on. This profile information is of-
ten of interest to marketing organisations for prod-
uct promotional reasons as well as governments or
law enforcements for crime investigation purposes.
The particular application that motivates the present
study is detection of phishing (Myers, 2007), the at-
tempt to defraud through texts that are designed to
deceive Internet users into giving away confidential
details. One class of countermeasures to phishing
consists of technical methods such as email authen-
tication; another looks at profiling of the text?s au-
thor(s) (Fette et al, 2007; Zheng et al, 2003), to
find any indications of the source of the text.
In this paper we investigate classification of a text
with respect to an author?s native language, where
this is not the language that that text is written in
(which is often the case in phishing); we refer to
this as native language identification. Initial work
by Koppel et al (2005) was followed by Tsur and
Rappoport (2007), Estival et al (2007), van Halteren
(2008), and Wong and Dras (2009). By and large,
the problem was tackled using various supervised
machine learning approaches, with mostly lexical
features over characters, words, and parts of speech,
as well as some document structure.
Syntactic features, in contrast, in particular those
that capture grammatical errors, which might po-
tentially be useful for this task, have received lit-
tle attention. Koppel et al (2005) did suggest using
syntactic errors in their work but did not investigate
them in any detail. Wong and Dras (2009) noted
the relevance of the concept of contrastive analy-
sis (Lado, 1957), which postulates that native lan-
guage constructions lead to characteristic errors in a
second language. In their experimental work, how-
ever, they used only three manual syntactic construc-
tions drawn from the literature; an ANOVA analysis
showed a detectable effect, but they did not improve
classification accuracy over purely lexical features.
In this paper, we investigate syntactic features for
native language identification that are more general
1600
than, and that do not require the manual construction
of, the above approach. Taking the trees produced
by statistical parsers, we use tree cross-sections as
features in a machine learning approach to deter-
mine which ones characterise non-native speaker er-
rors. Specifically, we look at two types of parse
tree substructure to use as features: horizontal slices
of the trees?that is, characterising parse trees as
sets of context-free grammar production rules?and
the features schemas used in discriminative parse
reranking. The goal of the present study is therefore
to investigate the influence to which syntactic fea-
tures represented by parse structures would have on
the classification task of identifying an author?s na-
tive language relative to, and in combination with,
lexical features.
The remainder of this paper is structured as fol-
lows. In Section 2, we discuss some related work on
the two key topics of this paper: primarily on com-
parable work in native language identification, and
then on how the notion of contrastive analysis can be
applicable here. We then describe the models exam-
ined in Section 3, followed by experimental setup in
Section 4. Section 5 presents results, and Section 6
discussion of those results.
2 Related Work
2.1 Native Language Identification
The earliest work on native language identification
in this classification paradigm is that of Koppel et
al. (2005), in which they deployed a machine learn-
ing approach to the task, using as features func-
tion words, character n-grams, and part-of-speech
(PoS) bi-grams, as well as some spelling mistakes.
With five different groups of English authors (of na-
tive languages Bulgarian, Czech, French, Russian,
and Spanish) selected from the first version of In-
ternational Corpus of Learner English (ICLE), they
gained a relatively high classification accuracy of
80%. Koppel et al (2005) also suggested that syn-
tactic features (syntactic errors) might be useful fea-
tures, but only investigated this idea at a shallow
level by treating rare PoS bigrams as ungrammati-
cal structures.
Tsur and Rappoport (2007) replicated the work
of Koppel et al (2005) to investigate the hypothe-
sis that the choice of words in second language writ-
ing is highly influenced by the frequency of native
language syllables ? the phonology of the native
language. Approximating this by character bi-grams
alone, they managed to achieve a classification accu-
racy of 66%.
Native language is also amongst the characteris-
tics investigated in the task of authorship profiling
by Estival et al (2007), as well as other demographic
and personality characteristics. This study used a va-
riety of lexical and document structure features. For
the native language identification classification task,
their model yielded a reasonably high accuracy of
84%, but this was over a set of only three languages
(Arabic, English and Spanish) and against a most
frequent baseline of 62.9%.
Another related work is that of van Halteren
(2008), who used the Europarl corpus of parliamen-
tary speeches. In Europarl, one original language
is transcribed, and the others translated from it; the
task was to identify the original language. On the
basis of frequency counts of word-based n-grams,
surprisingly high classification accuracies within the
range of 87-97% were achieved across six languages
(English, German, French, Dutch, Spanish, and Ital-
ian). This turns out, however, to be significantly
influenced by the use of particular phrases used by
speakers of different languages in the parliamentary
context (e.g. the way Germans typically address the
chamber).
To our knowledge, Wong and Dras (2009) is the
only work that has investigated the usefulness of
syntactic features for the task of native language
identification. They first replicated the work of
Koppel et al (2005) with the three types of lex-
ical feature, namely function words, character n-
grams, and PoS bi-grams. They then examined the
literature on contrastive analysis (see Section 2.2),
from the field of second language acquisition, and
selected three syntactic errors commonly observed
in non-native English users?subject-verb disagree-
ment, noun-number disagreement and misuse of
determiners?that had been identified as being in-
fluenced by the native language. An ANOVA anal-
ysis showed that the native language identification
constructions were identifiable; however, the over-
all classification was not improved over the lexi-
cal features by using just the three manually de-
tected syntactic errors. The best overall accuracy re-
1601
ported was 73.71%; this was on the second version
of ICLE, across seven languages (those of Koppel
et al (2005), plus the two Asian languages Chinese
and Japanese).
As a possible approach that would improve the
classification accuracy over just the three manually
detected syntactic errors, Wong and Dras (2009)
suggested deploying (but did not carry out) an idea
put forward by Gamon (2004) (citing Baayen et al
(1996)) for the related task of identifying the author
of a text: to use CFG production rules to characterise
syntactic structures used by authors.1 We note that
similar ideas have been used in the task of sentence
grammaticality judgement, which utilise parser out-
puts (both trees and by-products) as classification
features (Mutton et al, 2007; Sun et al, 2007; Fos-
ter et al, 2008; Wagner et al, 2009; Tetreault et al,
2010; Wong and Dras, 2010). We combine this idea
with one we introduce in this paper, of using dis-
criminative reranking features as a broader charac-
terisation of the parse tree.
2.2 Contrastive analysis
Contrastive analysis (Lado, 1957) was an early at-
tempt in the field of second language acquisition
to explain the kinds and source of errors that non-
native speakers make. It arose out of behaviourist
psychology, and saw language learning as an issue
of habit formation that could be inhibited by previ-
ous habits inculcated in learning the native language.
The theory was also tied to structural linguistics:
it compared the syntactic structures of the native
and second languages to find differences that might
cause learning difficulties. The Lado work postu-
lated the Contrastive Analysis Hypothesis (CAH),
claiming that ?those elements which are similar to
[the learner?s] native language will be simple for
him, and those elements that are different will be
difficult?; the consequence is that there will be more
errors made in those difficult elements.
While contrastive analysis was influential at first,
it was increasingly noticed that many errors were
1It is not entirely clear how this might work for author-
ship identification: would the Bronte? sisters, the corpus Gamon
worked with, have used a significant number of different syntac-
tic constructions from each other? In the context of native lan-
guage identification, however, constrastive analysis postulates
that this is exactly the case for the different classes.
common across all language learners regardless of
native language, which could not be explained un-
der contrastive analysis. Corder (1967) then de-
scribed an alternative, error analysis, where con-
trastive analysis-style errors were seen as only one
type of error, ?interlanguage? or ?interference? er-
rors; other types were ?intralingual? and ?develop-
mental? errors, which are not specific to the native
language (Richards, 1971).
In an overview of contrastive analysis after the
emergence of error analysis, Wardhaugh (1970)
noted that there were two interpretations of the
CAH, termed the strong and weak forms. Under the
strong form, all errors were attributed to the native
language, and clearly that was not tenable in light of
error analysis evidence. In the weak form, these dif-
ferences have an influence but are not the sole deter-
minant of language learning difficulty. Wardhaugh
noted claims at the time that the hypothesis was no
longer useful in either the strong or the weak ver-
sion: ?Such a claim is perhaps unwarranted, but a
period of quiescence is probable for CA itself?. This
appears to be the case, with the then-dominant error
analysis giving way to newer, more specialised theo-
ries of second language acquisition, such as the com-
petition model of MacWhinney and Bates (1989)
or the processability theory of Pienemann (1998).
Nevertheless, smaller studies specifically of inter-
language errors have continued to be carried out,
generally restricted in their scope to a specific gram-
matical aspect of English in which the native lan-
guage of the learners might have an influence. To
give some examples, Granger and Tyson (1996) ex-
amined the usage of connectors in English by a num-
ber of different native speakers ? French, German,
Dutch, and Chinese; Vassileva (1998) investigated
the employment of first person singular and plural
by another different set of native speakers ? Ger-
man, French, Russian, and Bulgarian; Slabakova
(2000) explored the acquisition of telicity marking
in English by Spanish and Bulgarian learners; Yang
and Huang (2004) studied the impact of the ab-
sence of grammatical tense in Chinese on the acqui-
sition of English tense-aspect system (i.e. telicity
marking); Franck et al (2002) and Vigliocco et al
(1996) specifically examined the usage of subject-
verb agreement in English by French and Spanish,
respectively. There are also a few teaching resources
1602
for English language teachers that collate such phe-
nomena, such as that of Swan and Smith (2001).
NLP techniques and a probabilistic view of na-
tive language identification now let us revisit and
make use of the weak form of the CAH. Interlan-
guage errors, as represented by differences in parse
trees, may be characteristic of the native language
of a learner; we can use the occurrence of these to
come up with a revised likelihood of the native lan-
guage. In this paper, we use machine learning in a
prediction task as our approach to this.
3 Models
This section describes the three basic models inves-
tigated: the lexical model, based on Koppel et al
(2005), as the baseline; and then the two models that
exploit syntactic information. In Section 5 we look
at the performance of each model independently and
also in combination: to combine, we just concate-
nate feature vectors.
Lexical As Wong and Dras (2009), we replicate
the features of Koppel et al (2005) to produce our
LEXICAL model. These are of three types: function
words,2 character n-grams, and PoS n-grams. We
follow Wong and Dras (2009) in resolving some un-
clear issues from Koppel et al (2005). Specifically,
we use the same list of function words, left unspec-
ified in Koppel et al (2005), that were empirically
determined by Wong and Dras (2009) to be the best
of three candidates; we used character bi-grams, as
the best performing n-grams, although this also had
been left unspecified by Koppel et al (2005); and
we used the most frequently occurring PoS bi-grams
and tri-grams, obtained by using the Brill tagger pro-
vided in NLTK (Bird et al, 2009) being trained on
the Brown corpus. In total, there are 798 features
of this class with 398 function words, 200 most fre-
quently occurring character bi-grams, and 200 most
frequently occurring PoS bi-grams. Both function
words and PoS bi-grams have feature values of bi-
nary type; while for character bi-grams, the feature
value is the relative frequency. (These types of fea-
ture value are the best performing one for each lexi-
2As with most work in authorship profiling, only function
words are used, so that the result is not tied to a particular do-
main, and no clues are obtained from different topics that dif-
ferent authors might write about.
cal feature.)
We omitted the 250 rare bi-grams used by Koppel
et al (2005), as an ablative analysis showed that they
contributed nothing to classification accuracy.
Production Rules Under this model (PROD-
RULE), we take as features horizontal slices of parse
trees, in effect treating them as sets of CFG produc-
tion rules. Feature values are binary. We look at
all possible rules as features, but also present results
for subsets of features chosen using feature selec-
tion. For each language in our dataset, we identify
the n rules most characteristic of the language using
Information Gain (IG). For m classes, we use the
formulation of Yang and Pedersen (1997):
IG(r) = ??mi=1 Pr (ci) log Pr (ci)
+Pr (r)
?m
i=1 Pr (ci|r) log Pr (ci|r)
+Pr (r?)
?m
i=1 Pr (ci|r?) log Pr (ci|r?) (1)
We also investigated simple frequencies, fre-
quency ratios, and pointwise mutual information; as
in much other work, IG performed best, so we do not
present results for the others. Bi-normal separation
(Forman, 2003), often competitive with IG, is only
suitable for binary classification.
It is worth noting that the production rules being
used here are all non-lexicalised ones, except those
lexicalised with function words and punctuation, to
avoid topic-related clues.
Reranking Features As opposed to the horizontal
parse production rules, features used for discrimina-
tive reranking are cross-sections of parse trees that
might capture other aspects of ungrammatical struc-
tures. For these we use the 13 feature schemas de-
scribed in Charniak and Johnson (2005), which were
inspired by earlier work in discriminative estimation
techniques, such as Johnson et al (1999) and Collins
(2000). Examples of these feature schemas include
tuples covering head-to-head dependencies, preter-
minals together with their closest maximal projec-
tion ancestors, and subtrees rooted in the least com-
mon ancestor.
These feature schemas are not the only possible
ones?they were empirically selected for the spe-
cific purpose of augmenting the Charniak parser.
However, much subsequent work has tended to use
1603
these same features, albeit sometimes with exten-
sions for specific purposes (e.g. Johnson and Ural
(2010) for the Berkeley parser (Petrov et al, 2006),
Ng et al (2010) for the C&C parser (Clark and Cur-
ran, 2007)). We also use this standard set, specif-
ically the set of instantiated feature schemas from
the parser from Charniak and Johnson (2005) as
trained on the Wall Street Journal (WSJ), which
gives 1,333,837 potential features.
4 Experimental Setup
4.1 Data
We use the International Corpus of Learner English
(ICLE) compiled by Granger et al (2009) for the
precise purpose of studying the English writings of
non-native English learners from diverse countries.
All the contributors to the corpus are claimed to
possess similar English proficiency levels (ranging
from intermediate to advanced learners) and are in
the same age group (all in their twenties at the time
of corpus collection.) This was also the data used by
Koppel et al (2005) and Tsur and Rappoport (2007),
although where they used the first version of the cor-
pus, we use version 2.
Briefly, the first version contains 11 sub-corpora
of English essays contributed by second-year and
third-year university students of different native lan-
guage backgrounds (mostly European and Slavic
languages) ? Bulgarian, Czech, Dutch, Finnish,
French, German, Italian, Polish, Russian, Spanish,
and Swedish; the second version has been extended
to additional 5 other native languages (including
Asian languages) ? Chinese, Japanese, Norwegian,
Turkish, and Tswana.
As per Wong and Dras (2009), we examine seven
languages, namely Bulgarian, Czech, French, Rus-
sian, Spanish, Chinese, and Japanese. For each na-
tive language, we randomly select from amongst es-
says with length of 500-1000 words. For the purpose
of the present study, we have 95 essays per native
language. For the same reason as highlighted by
Wong and Dras (2009), we intentionally use fewer
essays as compared to Koppel et al (2005)3 with a
view to reserving more data for future work. We
divide these into training sets of 70 essays per lan-
3Koppel et al (2005) took all 258 texts per language from
ICLE Version 1 and evaluated using 10-fold cross valiadation.
guage, with a held-out test set of 25 essays per
language. There are 17,718 training sentences and
6,791 testing sentences.
4.2 Parsers
We use two parsers: the Stanford parser (Klein
and Manning, 2003) and the Charniak and John-
son (henceforth C&J) parser (Charniak and Johnson,
2005). Both are widely used, and produce relatively
accurate parses: the Stanford parser gets a labelled
f-score of 85.61 on the WSJ, and the C&J 91.09.
With the Stanford parser, there are 26,284 unique
parse production rules extractable from our ICLE
training set of 490 texts, while the C&J parser pro-
duces 27,705. For reranking, we use only the C&J
parser?since the parser stores these features during
parsing, we can use them directly as classification
features. On the ICLE training data, there are 6,230
features with frequency >10, and 19,659 with fre-
quency >5.
4.3 Classifiers
For our experiments we used a maximum entropy
(MaxEnt) machine learner, MegaM4 (fifth release)
by Hal Daume? III. (We also used an SVM for com-
parison, but the results were uniformly worse, and
degraded more quickly as number of features in-
creased, so we only report the MaxEnt results here).
The classifier is tuned to obtain an optimal classifi-
cation model.
4.4 Evaluation Methodology
Given our relatively small amount of data, we use k-
fold cross-validation, choosing k = 5. While testing
for statistical significance of classification results is
often not carried out in NLP, we do so here because
the quantity of data could raise questions about the
certainty of any effect. In an encyclopedic survey of
cross-validation in machine learning contexts, Re-
faeilzadeh et al (2009) note that there is as yet no
universal standard for testing of statistical signifi-
cance; and that while more sophisticated techniques
have been proposed, none is more widely accepted
than a paired t-test over folds. We therefore use this
paired t-test over folds, as formulated of Alpaydin
4MegaM is available on http://www.cs.utah.edu/
?hal/megam/.
1604
(2004). Under this cross-validation, 5 separate train-
ing feature sets are constructed, excluding the test
fold; 3 folds are used for training, 1 fold for tuning
and 1 fold for testing.
We also use a held-out test set for comparison,
as it is well-known that cross-validation can over-
estimate prediction error (Hastie et al, 2009). We
do not carry out significance testing here?with this
held-out test set size (n = 125), two models would
have to differ by a great deal to be significant. We
only use it as a check on the effect of applying to
completely new data.
5 Results
Table 1 presents the results for the three models in-
dividually under cross-validation. The first point
to note is that PROD-RULE, under both parsers,
is a substantial improvement over LEXICAL when
(non-lexicalised) parse rules together with rules lex-
icalised with function words are used (rows marked
with * in Table 1), with the largest difference as
much as 77.75% for PROD-RULE[both]* (n = all)
versus 64.29% for LEXICAL; these differences with
respect to LEXICAL are statistically significant. (To
give an idea, the paired t-test standard error for this
largest difference is 2.52%.) In terms of error reduc-
tion, this is over 30%.
There appears to be no difference according to the
parser used, regardless of their differing accuracy on
the WSJ. Using the selection metric for PROD-RULE
without rules lexicalised with function words pro-
duces results all around those for LEXICAL; using
fewer reranking features is worse as the quality of
RERANKING declines as feature cut-offs are raised.
Another, somewhat surprising point is that the
RERANKING results are also generally around those
of LEXICAL even though like PROD-RULE they are
also using cross-sections of the parse tree. We con-
sider there might be two possible reasons for this.
The first is that the feature schemas used were orig-
inally chosen for the specific purpose of augment-
ing the performance of the Charniak parser; perhaps
others might be more appropriate here. The second
is that we selected only those instantiated feature
schemas that occurred in the WSJ, and then applied
them to ICLE. As the WSJ is filled with predomi-
nantly grammatical text, perhaps those that were not
Features MaxEnt
LEXICAL (n = 798) 64.29
PROD-RULE[Stanford] (n = 1000) 65.72
PROD-RULE[Stanford]* (n = 1000) 74.08
PROD-RULE[Stanford]* (n = all) 74.49
PROD-RULE[C&J] (n = 1000) 62.25
PROD-RULE[C&J]* (n = 1000) 71.84
PROD-RULE[C&J]* (n = all) 71.63
PROD-RULE[both] (n = 2000) 67.96
PROD-RULE[both]* (n = 2000) 74.69
PROD-RULE[both]* (n = all) 77.75
RERANKING (all features) 67.96
RERANKING (>5 counts) 66.33
RERANKING (>10 counts) 64.90
Table 1: Classification results based on 5-fold cross vali-
dation with parse rules as syntactic features (accuracy %)
Features MaxEnt
Lexical features (n = 798) 75.43
PROD-RULE[Stanford] (n = 1000) 74.29
PROD-RULE[Stanford]* (n = 1000) 79.43
PROD-RULE[Stanford]* (n = all) 78.86
PROD-RULE[C&J] (n = 1000) 73.71
PROD-RULE[C&J] (n = 1000)* 79.43
PROD-RULE[C&J] (n = all)* 80.00
PROD-RULE[both] (n = 2000) 77.71
PROD-RULE[both] (n = 2000)* 78.85
PROD-RULE[both] (n = all)* 80.00
RERANKING (all features) 77.14
RERANKING (>5 counts) 76.57
RERANKING (>10 counts) 75.43
Table 2: Classification results based on hold-out valida-
tion with parse rules as syntactic features (accuracy %)
seen on the WSJ are precisely those that might indi-
cate ungrammaticality. In contrast, the production
rules of PROD-RULE were selected only from the
ICLE training data.
Table 2 presents the results for the individual
models on the held-out test set. The results are gen-
erally higher than for cross-validation?this is not
surprising, as the texts are of the same type, but all
the training data is used (rather than the 1?1/k pro-
portion for cross-validation). Overall, the pattern is
still the same, with PROD-RULE best, then RERANK-
ING and LEXICAL broadly similar; as expected, no
differences are significant with this smaller dataset.
The gap has narrowed, but without significance test-
1605
Features MaxEnt
LEXICAL (n = 798) 64.29
LEXICAL + PROD-RULE[both] (n = 2000) 63.06
LEXICAL + PROD-RULE[both]* (n = 2000) 72.45
LEXICAL + PROD-RULE[both]* (n = all) 70.82
LEXICAL + RERANKING (n = all) 68.17
Table 3: Classification results based on 5-fold cross vali-
dation for combined models (accuracy %)
Features MaxEnt
LEXICAL (n = 798) 75.43
LEXICAL + PROD-RULE[both] (n = 2000) 80.57
LEXICAL + PROD-RULE[both]* (n = 2000) 81.14
LEXICAL + PROD-RULE[both]* (n = all) 81.71
LEXICAL + RERANKING (n = all) 76.00
Table 4: Classification results based on hold-out valida-
tion for combined models (accuracy %)
ing it is difficult to say whether this is a genuine
phenomenon. The accuracy rate for LEXICAL here
is in line with Wong and Dras (2009); and given
the smaller dataset and larger set of languages, also
broadly in line with Koppel et al (2005).
Tables 3 and 4 present results for model combina-
tions. It can be seen that the model combinations do
not produce results better than PROD-RULE alone.
Combining all features (results not presented here)
seems to degrade the overall performance even of
the MegaM: perhaps we need to derive feature vec-
tors more compactly than by feature concatenation.
6 Discussion
As illustrated in the confusion matrices (Table 5
for the PROD-RULE model, and Table 6 for the
LEXICAL model), misclassifications occur largely in
Spanish and Slavic languages, Bulgarian and Rus-
sian in particular. Unsurprisingly, Chinese is al-
most completely identified since it comes from a
entirely different language family, Sino-Tibetan, as
compared to the rest of the languages which are from
the branches of the Indo-European family (with
Japanese as the exception). Japanese and French
also appear to be easily distinguished, which could
probably be attributed to their word order or sen-
tence structure which are, to some extent, quite dif-
ferent from English. Japanese is a ?subject-object-
verb? language; and French, although having the
same word order as English, heads of phrases in
BL CZ FR RU SP CN JP
BL [14] 6 2 3 - - -
CZ 1 [20] - 3 1 - -
FR - - [25] - - - -
RU 1 4 3 [17] - - -
SP 2 1 3 1 [18] - -
CN - - - - - [24] 1
JP - - - - 1 2 [22]
Table 5: Confusion matrix based on all non-lexicalised
parse rules from both parsers on the held-out set
(BL:Bulgarian, CZ:Czech, FR:French, RU:Russian,
SP:Spanish, CN:Chinese, JP:Japanese)
BL CZ FR RU SP CN JP
BL [14] 3 2 4 2 - -
CZ 6 [16] - 2 1 - -
FR 1 - [24] - - - -
RU 3 2 3 [16] 1 - -
SP 1 2 3 1 [17] - 1
CN - - - - - [24] 1
JP - - - - 1 3 [21]
Table 6: Confusion matrix based on lexical features on
the held-out set (BL:Bulgarian, CZ:Czech, FR:French,
RU:Russian, SP:Spanish, CN:Chinese, JP:Japanese)
French typically come before modifiers as opposed
to English. Overall, the PROD-RULE model results
in fewer misclassifications compared to the LEXI-
CAL model; there are mostly only incremental im-
provements for each language, with perhaps the ex-
ception of the reduction in confusion in the Slavic
languages.
We looked at some of the data, to see what kind
of syntactic substructure is useful in classifying na-
tive language. Although using feature selection with
only 1000 features did not improve performance,
the information gain ranking does identify particu-
lar constructions as characteristic of one of the lan-
guages, and so are useful for inspection.
A phenomenon that the literature has noted as oc-
curring with Chinese speakers is that of the missing
determiner.5 This corresponds to a higher frequency
of NP rules without determiners. These rules may
be valid in other contexts, but are also used to de-
scribe ungrammatical constituents. One example is
5This does happen with native speakers of some other lan-
guages, such as Slavic ones, but not generally (from our knowl-
edge of the literature) with native speakers of others, such as
Romance ones.
1606
Rules Counts
BL CZ FR RU SP CN JP
NNP ? <R> 0 0 3 0 0 67 0
: ? - 55 51 23 39 10 9 4
PRN ? -LRB- X -RRB- 0 1 7 2 0 42 0
SYM ? * 0 1 7 3 1 42 0
: ? : 30 39 58 46 47 11 6
X ? SYM 0 2 7 4 4 42 6
NP ? NNP NNP NNS 0 3 1 0 0 31 0
S ? S : S . 36 34 53 39 41 5 9
PP ? VBG PP 9 15 16 12 13 54 13
: ? ... 16 13 39 11 24 1 3
Table 7: Top 10 rules for the Stanford parser according to Information Gain on the held-out set
(ROOT
(S
(NP
(NP (DT The) (NN development))
(PP (IN of)
(NP (NN country) (NN park))))
(VP (MD can)
(ADVP (RB directly))
(VP (VB elp)
(S
(VP (TO to)
(VP (VB alleviate)
(NP (NNS overcrowdedness)
(CC and)
(NN overpopulation))
(PP (IN in)
(NP (JJ urban)
(NN area))))))))
(. .)))
Figure 1: Parse from Chinese-speaking authors, illustrat-
ing missing determiner
(ROOT
(S
(PP (VBG According)
(PP (TO to)
(NP (NNP <R>))))
(, ,)
(NP
(NP (NN burning))
(PP (IN of)
(NP (JJ plastic)
(NN waste))))
(VP (VBZ generates)
(NP (JJ toxic)
(NNS by-products)))
(. .)))
Figure 2: Parse from Chinese-speaking authors, illustrat-
ing according to
NP ? NN NN. In Figure 1 we give the parse (from
the Stanford parser) of the sentence The develop-
ment of country park can directly elp to alleviate
overcrowdedness and overpopulation in urban area.
The phrase country park should either have a deter-
miner or be plural (in which case the appropriate rule
would be NP ? NN NNS). There is a similar phe-
nomenon with in urban area, although this is an in-
stance of the rule NP ? JJ NN.
Another production rule that occurs typically?
in fact, almost exclusively?in the texts of native
Chinese speakers is PP ? VBG PP (by the Stan-
ford parser), which almost always corresponds to the
phrase according to. In Figure 2 we give the parse
of a short sentence (According to <R>, burning of
1607
(S1
(S
(ADVP (RB Overall))
(, ,)
(NP (NNP cyber))
(VP (VBD cafeis)
(NP (DT a) (JJ good) (NN place))
(PP (IN as)
(NP (JJ recreational)
(NNP centre)))
(PP (IN with)
(NP
(NP
(DT a) (NN bundle))
(PP (IN of)
(NP (JJ up-to-dated)
(NN information))))))
(. .)))
Figure 3: Parse illustrating parser correction
plastic waste generates toxic by-products?<R>is
an in-text citation that was removed in the prepa-
ration of ICLE) that illustrates this particular con-
struction. It appears that speakers of Chinese fre-
quently use this phrase as a translation of ge?n ju`.
So in this case, what is identified is not the sort of
error that is of interest to contrastive analysis, but
just a particular construction that is characteristic of
a certain native speaker?s language, one that is per-
fectly grammatical but which is used relatively infre-
quently by others and has a slightly unusual analysis
by the parser.
We had expected to see more rules that displayed
obvious ungrammaticality, such as VP ? DT IN.
However, both parsers appear to be good at ?ig-
noring? errors, and producing relatively grammati-
cal structures (albeit ones with different frequencies
for different native languages). Figure 3 gives the
C&J parse for Overall, cyber cafeis a good place as
recreational centre with a bundle of up-to-dated in-
formation. The correction of up-to-dated rather than
up-to-date is straightforward, but the simple typo-
graphical error of running together cafe and is leads
to more complex problems for the parser. Neverthe-
less, the parser produces a solid grammatical tree,
specifically assigning the category VBD to the com-
pound cafeis. This appears to be because both the
Stanford and C&J parsers have implicit linguistic
constraints such as assumptions about heads; these
are imposed even when the text does not provide ev-
idence for them.
We also present in Table 7 the top 10 rules chosen
under the IG feature selection for the Stanford parser
on the held-out set. A number of these, and those
ranked lower, are concerned with punctuation: these
seem unlikely to be related to native language, but
perhaps rather to how students of a particular lan-
guage background are taught. Others are more typi-
cal of the sorts of example we illustrated above: PP
? VBG PP, for example, is typically connected to
the according to construction discussed in connec-
tion with Figure 2, and it can be seen that the dom-
inant frequency count there is for native Chinese
speakers (column 6 of the counts).
7 Conclusion
In this paper we have shown that, using cross-
sections of parse trees, we can improve above an al-
ready good baseline in the task of native language
identification. While we do not make any strong
claims for the Contrastive Analysis Hypothesis, the
usefulness of syntax in the context of this problem
does provide some support.
The best features arising from the classification
have been horizontal cross-sections of trees, rather
than the more general discriminative parse reranking
features that might have been expected to perform at
least as well. This relatively poorer performance by
the reranking features may be due to a number of
factors, all of which could be investigated in future
work. One is the use of feature schema instances that
did not appear in the largely grammatical WSJ; an-
other is the extension of feature schemas; and a third
is the use of a parser that does not enforce linguistic
constraints such as the Berkeley parser (Petrov et al,
2006).
Examining some of the substructures showed
some errors that were expected; other constructions
that were grammatical, but were just characteris-
tic translations of constructions that were common
in the native language; and a large number where
grammatical errors were glossed over by the parser?s
linguistic constraints, suggesting another purpose
for further work with the Berkeley parser. Overall,
the use of these led to an error reduction in over 30%
1608
in the cross-validation evaluation with significance
testing.
Acknowledgments
The authors would like to acknowledge the support
of ARC Linkage Grant LP0776267 and ARC Dis-
covery Grant DP1095443, and thank the reviewers
for useful feedback. Much gratitude is due to Mark
Johnson for his guidance on the extraction of rerank-
ing features.
References
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press, Cambridge, MA, USA.
Harald Baayen, Hans van Halteren, and Fiona Tweedie.
1996. Outside the Cave of Shadows: Using Syntactic
Annotation to Enhance Authorship Attribution. Liter-
ary and Linguistic Computing, 11(3):121?131.
Stephen Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python: Analyzing
Text with the Natural Language Toolkit. O?Reilly Me-
dia, Inc.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Stephen P. Corder. 1967. The significance of learners?
errors. International Review of Applied Linguistics in
Language Teaching (IRAL), 5(4):161?170.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Ian Fette, Norman Sadeh, and Anthony Tomasic. 2007.
Learning to detect phishing emails. In Proceedings of
the 16th International World Wide Web Conference.
George Forman. 2003. An extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3:1289?1305.
Jennifer Foster, JoachimWagner, and Josef van Genabith.
2008. Adapting a WSJ-trained parser to grammati-
cally noisy text. In Proceedings of ACL-08: HLT,
Short Papers, pages 221?224, Columbus, Ohio.
Julie Franck, Gabriella Vigliocco, and Janet Nicol. 2002.
Subject-verb agreement errors in French and English:
The role of syntactic hierarchy. Language and Cogni-
tive Processes, 17(4):371?404.
Michael Gamon. 2004. Linguistic correlates of style:
Authorship classification with deep linguistic analy-
sis features. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING),
pages 611?617.
Sylviane Granger and Stephanie Tyson. 1996. Connec-
tor usage in the English essay writing of native and
non-native EFL speakers of English. World Englishes,
15(1):17?27.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvian-la-Neuve.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2009. The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. Springer.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. In Proceed-
ings of Human Language Technologies: the 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL-10), pages 665?668, Los Angeles, CA,
USA, June.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
unification-based grammars. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?99), College Park, MD.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. In Intelligence and Security Informat-
ics, volume 3495 of Lecture Notes in Computer Sci-
ence, pages 209?217. Springer-Verlag.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. University
of Michigan Press, Ann Arbor, MI, US.
Brian MacWhinney and Elizabeth Bates. 1989. The
Crosslinguistic Study of Sentence Processing. Cam-
bridge University Press, New York, NY, USA.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. GLEU: Automatic evaluation of
1609
sentence-level fluency. In Proceedings of the 45th An-
nual Meeting of the Association of Computational Lin-
guistics, pages 344?351, Prague, Czech Republic.
Steven Myers. 2007. Introduction to phishing. In
Markus Jakobsson and Steven Myers, editors, Phish-
ing and Countermeasures: Understanding the In-
creasing Problem of Electronic Identity Theft. John
Wiley & Sons, Inc., Hoboken, NJ, USA.
Dominick Ng, Matthew Honnibal, and James R. Cur-
ran. 2010. Reranking a Wide-Coverage CCG Parser.
In Proceedings of Australasian Language Technology
Association Workshop (ALTA?10), pages 90?98, Mel-
bourne, Australia.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL?06), pages
433?440, Sydney, Australia, July.
Manfred Pienemann. 1998. Language Processing and
Second Language Development: Processability The-
ory. John Benjamins, Amsterdam, The Netherlands.
Payam Refaeilzadeh, Lei Tang, and Huan Liu. 2009.
Cross-validation. In Ling Liu and M. Tamer O?zsu, ed-
itors, Encyclopedia of Database Systems, pages 532?
538. Springer, US.
Jack C. Richards. 1971. A non-contrastive approach to
error analysis. ELT Journal, 25(3):204?219.
Roumyana Slabakova. 2000. L1 transfer revisited:
the L2 acquisition of telicity marking in English by
Spanish and Bulgarian native speakers. Linguistics,
38(4):739?770.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 81?88, Prague, Czech Repub-
lic.
Michael Swan and Bernard Smith, editors. 2001.
Learner English: A teacher?s guide to interference and
other problems. Cambridge University Press, 2nd edi-
tion.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of the ACL 2010
Conference Short Papers, ACLShort ?10, pages 353?
358. Association for Computational Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Hans van Halteren. 2008. Source language markers in
EUROPARL translations. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING), pages 937?944.
Irena Vassileva. 1998. Who am I/how are we in aca-
demic writing? A contrastive analysis of authorial
presence in English, German, French, Russian and
Bulgarian. International Journal of Applied Linguis-
tics, 8(2):163?185.
Garbriella Vigliocco, Brian Butterworth, and Merrill F.
Garrett. 1996. Subject-verb agreement in Spanish
and English: Differences in the role of conceptual con-
straints. Cognition, 61(3):261?298.
JoachimWagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal, 26(3):474?490.
Richard Wardhaugh. 1970. The Contrastive Analysis
Hypothesis. TESOL Quarterly, 4(2):123?130.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification. In
Proceedings of the Australasian Language Technology
Association Workshop 2010, pages 67?75, Melbourne,
Australia, December.
Suying Yang and Yue-Yuan Huang. 2004. The impact of
the absence of grammatical tense in L1 on the acqui-
sition of the tense-aspect system in L2. International
Review of Applied Linguistics in Language Teaching
(IRAL), 42(1):49?70.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the Fourteenth International Con-
ference on Machine Learning (ICML?97), pages 412?
420.
Rong Zheng, Yi Qin, Zan Huang, and Hsinchun Chen.
2003. Authorship analysis in cybercrime investiga-
tion. In Intelligence and Security Informatics, volume
2665 of Lecture Notes in Computer Science, pages 59?
73. Springer-Verlag.
1610
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 699?709, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploring Adaptor Grammars for Native Language Identification
Sze-Meng Jojo Wong Mark Dras Mark Johnson
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
{sze.wong,mark.dras,mark.johnson}@mq.edu.au
Abstract
The task of inferring the native language of
an author based on texts written in a second
language has generally been tackled as a clas-
sification problem, typically using as features
a mix of n-grams over characters and part of
speech tags (for small and fixed n) and un-
igram function words. To capture arbitrar-
ily long n-grams that syntax-based approaches
have suggested are useful, adaptor grammars
have some promise. In this work we investi-
gate their extension to identifying n-gram col-
locations of arbitrary length over a mix of PoS
tags and words, using both maxent and in-
duced syntactic language model approaches to
classification. After presenting a new, simple
baseline, we show that learned collocations
used as features in a maxent model perform
better still, but that the story is more mixed for
the syntactic language model.
1 Introduction
The task of inferring the native language of an author
based on texts written in a second language ? na-
tive language identification (NLI) ? has, since the
seminal work of Koppel et al2005), been primarily
tackled as a text classification task using supervised
machine learning techniques. Lexical features, such
as function words, character n-grams, and part-of-
speech (PoS) n-grams, have been proven to be use-
ful in NLI (Koppel et al2005; Tsur and Rappoport,
2007; Estival et al2007). The recent work of Wong
and Dras (2011), motivated by ideas from Second
Language Acquisition (SLA), has shown that syn-
tactic features ? potentially capturing syntactic er-
rors characteristic of a particular native language ?
improve performance over purely lexical ones.
PoS n-grams can be leveraged to characterise sur-
face syntactic structures: in Koppel et al2005),
for example, ungrammatical structures were approx-
imated by rare PoS bigrams. For the purpose of NLI,
small n-gram sizes like bigram or trigram might not
suffice to capture sequences that are characteristic of
a particular native language. On the other hand, an
attempt to represent these with larger n-grams would
not just lead to feature sparsity problems, but also
computational efficiency issues. Some form of fea-
ture selection should then come into play.
Adaptor grammars (Johnson, 2010), a hierarchi-
cal non-parametric extension of PCFGs (and also in-
terpretable as an extension of LDA-based topic mod-
els), hold out some promise here. In that initial
work, Johnson?s model learnt collocations of arbi-
trary length such as gradient descent and cost func-
tion, under a topic associated with machine learning.
Hardisty et al2010) applied this idea to perspective
classification, learning collocations such as pales-
tinian violence and palestinian freedom, the use of
which as features was demonstrated to help the clas-
sification of texts from the Bitter Lemons corpus as
either Palestinian or Israeli perspective.
Typically in NLI and other authorship attribu-
tion tasks, the feature sets exclude content words,
to avoid unfair cues due to potentially different do-
mains of discourse. In our context, then, what we are
interested in are ?quasi-syntactic collocations? of ei-
ther pure PoS (e.g. NN IN NN) or a mixture of PoS
with function words (e.g. NN of NN). The partic-
ular question of interest for this paper, then, is to
699
investigate whether the power of adaptor grammars
to discover collocations ? specifically, ones of ar-
bitrary length that are useful for classification ? ex-
tends to features beyond the purely lexical.
We examine two different approaches in this pa-
per. We first utilise adaptor grammars for discovery
of high performing ?quasi-syntactic collocations? of
arbitrary length as mentioned above and use them
as classification features in a conventional maximum
entropy (maxent) model for identifying the author?s
native language. In the second approach, we adopt
a grammar induction technique to learn a grammar-
based language model in a Bayesian setting. The
grammar learned can then be used to infer the most
probable native language that a given text written
in a second language is associated with. The latter
approach is actually closer to the work of Hardisty
et al2010) using adaptor grammars for perspec-
tive modeling, which inspired our general approach.
This alternative approach is also similar in nature
to the work of Bo?rschinger et al2011) in which
grounded learning of semantic parsers was reduced
to a grammatical inference task.
The structure of the paper is as follows. In Sec-
tion 2, we review the existing work of NLI as well
as the mechanics of adaptor grammars along with
their applications to classification. Section 3 details
the supervised maxent classification of NLI with
collocation (n-gram) features discovered by adaptor
grammars. The language model-based classifier is
described in Section 4. Finally, we present a dis-
cussion in Section 5 and follow with concluding re-
marks.
2 Related Work
2.1 Native Language Identification
Most of the existing research treats the task of na-
tive language identification as a form of text classi-
fication deploying supervised machine learning ap-
proaches.
The earliest notable work in this classification
paradigm is that of Koppel et al2005) using as
features function words, character n-grams, and PoS
bigrams, together with some spelling errors. Their
experiments were conducted on English essays writ-
ten by authors whose native language one of Bulgar-
ian, Czech, French, Russian, or Spanish. The cor-
pus used is the first version of International Corpus
of Learner English (ICLE). Apart from investigating
lexical features, syntactic features (errors in particu-
lar) were highlighted by Koppel et al2005) as po-
tentially useful features, but they only explored this
by characterising ungrammatical structures with rare
PoS bigrams: they chose 250 rare bigrams from the
Brown corpus.
Features for this task can include content words
or not: Koppel et al2009), in reviewing work in
the general area of authorship attribution (including
NLI), discuss the (perhaps unreasonable) advantage
that content word features can provide, and com-
ment that consequently they ?are careful . . . to dis-
tinguish results that exploit content-based features
from those that do not?. We will not be using con-
tent words as features; we therefore note only ap-
proaches to NLI that similarly do not use them.
Following Koppel et al2005), Tsur and Rap-
poport (2007) replicated their work and hypothe-
sised that word choices in second language writing
is highly influenced by the frequency of native lan-
guage syllables. They investigated this through mea-
suring classification performance with only charac-
ter bigrams as features.
Estival et al2007) tackled the broader task of
developing profiles of authors, including native lan-
guage and various other demographic and psycho-
metric author traits, across a smaller set of languages
(English, Spanish and Arabic). To this end, they de-
ployed various lexical and document structure fea-
tures.
Wong and Dras (2011), starting from the Kop-
pel et al2005) approach, explored the usefulness
of syntactic features in a broader sense in which
they characterised syntactic errors with cross sec-
tions of parse trees obtained from statistical parsers,
both horizontal slices of the parse trees in the form
of CFG production rules, and the feature schemata
used in discriminative parse reranking (Charniak
and Johnson, 2005); they also found that using the
top 200 PoS bigrams helped. Their results on the
second version of the ICLE corpus, across seven
languages (those of Koppel et alplus two Orien-
tal languages, Chinese and Japanese) demonstrated
that syntactic features of these kinds lead to signifi-
cantly better performance than the Koppel et alea-
tures alone, with a top accuracy (on 5-fold cross-
validation) of 77.75%.
700
Subsequently, Wong et al2011) explored
Bayesian topic modeling (Blei et al2003; Griffiths
and Steyvers, 2004) as a form of feature dimension-
ality reduction technique to discover coherent latent
factors (?topics?) that might capture predictive fea-
tures for individual native languages. Their topics,
rather than the typical word n-grams, consisted of
bigrams over (only) PoS. However, while there was
some evidence of topic cluster coherence, this did
not improve classification performance.
The work of the present paper differs in that it
uses Bayesian techniques to discover collocations of
arbitrary length for use in classification, over a mix
of both PoS and function words, rather than for use
as feature dimensionality reduction.
2.2 Adaptor Grammars
Adaptor Grammars are a non-parametric extension
to PCFGs that are associated with a Bayesian in-
ference procedure. Here we provide an informal
introduction to Adaptor Grammars; Johnson et al
(2007) provide a definition of Adaptor Grammars as
a hierarchy of mixtures of Dirichlet (or 2-parameter
Poisson-Dirichlet) Processes to which the reader
should turn for further details.
Adaptor Grammars can be viewed as extending
PCFGs by permitting the grammar to contain an
unbounded number of productions; they are non-
parametric in the sense that the particular produc-
tions used to analyse a corpus depends on the cor-
pus itself. Because the set of possible productions
is unbounded, they cannot be specified by simply
enumerating them, as is standard with PCFGs. In-
stead, the productions used in an adaptor gram-
mar are specified indirectly using a base grammar:
the subtrees of the base grammar?s ?adapted non-
terminals? serve as the possible productions of the
adaptor grammar (Johnson et al2007), much in
the way that subtrees function as productions in Tree
Substitution Grammars .1
Another way to view Adaptor Grammars is that
they relax the independence assumptions associated
with PCFGs. In a PCFG productions are gener-
ated independently conditioned on the parent non-
terminal, while in an Adaptor Grammar the proba-
bility of generating a subtree rooted in an adapted
1For computational efficiency reasons Adaptor Grammars
require the subtrees to completely expand to terminals. The
Fragment Grammars of O?Donnell (2011) lift this restriction.
non-terminal is roughly proportional to the number
of times it has been previously generated (a certain
amount of mass is reserved to generate ?new? sub-
trees). This means that the distribution generated by
an Adaptor Grammar ?adapts? based on the corpus
being generated.
2.2.1 Mechanics of adaptor grammars
Adaptor Grammars are specified by a PCFG G,
plus a subset of G?s non-terminals that are called
the adapted non-terminals, as well as a discount
parameter aA, where 0 ? aA < 1 and a con-
centration parameter bA, where b > ?a, for each
adapted non-terminal A. An adaptor grammar de-
fines a two-parameter Poisson-Dirichlet Process for
each adapted non-terminal A governed by the pa-
rameters aA and bA. For computational purposes it
is convenient to integrate out the Poisson-Dirichlet
Process, resulting in a predictive distribution spec-
ified by a Pitman-Yor Process (PYP). A PYP can
be understood in terms of a ?Chinese Restaurant?
metaphor in which ?customers? (observations) are
seated at ?tables?, each of which is labelled with a
sample from a ?base distribution? (Pitman and Yor,
1997).
In an Adaptor Grammar, unadapted non-terminals
expand just as they do in a PCFG; a production r ex-
panding the non-terminal is selected according to the
multinomial distribution ?r over productions speci-
fied in the grammar. Each adapted non-terminalA is
associated with its own Chinese Restaurant, where
the tables are labelled with subtrees generated by
the grammar rooted in A. In the Chinese Restau-
rant metaphor, the customers are expansions of A,
each table corresponds to a particular subtree ex-
panding A, and the PCFG specifies the base distri-
bution for each of the adapted non-terminals. An
adapted non-terminal A expands as follows. A ex-
pands to a subtree t with probability proportional to
nt, where nt is the number of times t has been pre-
viously generated. In addition, A expands using a
PCFG rule r expanding A with probability propor-
tional to (mA aA + bA) ?r, where mA is the number
of subtrees expanding A (i.e., the number of tables
in A?s restaurant). Because the underlying Pitman-
Yor Processes have a ?rich get richer? property, they
generate power-law distributions over the subtrees
for adapted non-terminals.
701
2.2.2 Adaptor grammars as LDA extension
With the ability to rewrite non-terminals to en-
tire subtrees, adaptor grammars have been used to
extend unigram-based LDA topic models (Johnson,
2010). This allows topic models to capture se-
quences of words with abitrary length rather than
just unigrams of word. It has also been shown that it
is crucial to go beyond the bag-of-words assump-
tion as topical collocations capture more meaning
information and represent more interpretable topics
(Wang et al2007).
Taking the PCFG formulation for the LDA topic
models, it can be modified such that each topic
Topici generates sequences of words by adapting
each of the Topici non-terminals (usually indicated
with an underline in an adaptor grammar). The over-
all schema for capturing topical collocations with an
adaptor grammar is as follows:
Sentence? Docj j ? 1, . . . ,m
Docj ? j j ? 1, . . . ,m
Docj ? Docj Topici i ? 1, . . . , t;
j ? 1, . . . ,m
Topici ?Words i ? 1, . . . , t
Words?Word
Words?Words Word
Word? w w ? V
There is a non-grammar-based approach to find-
ing topical collocations as demonstrated by Wang et
al. (2007). Both of these approaches learned use-
ful collocations: for instance, as mentioned in Sec-
tion 1, Johnson (2010) found collocations such gra-
dient descent and cost function associated with the
topic of machine learning; Wang et al2007) found
the topic of human receptive system comprises of
collocations such as visual cortext and motion de-
tector.
Adaptor grammars have also been deployed as a
form of feature selection in discovering useful collo-
cations for perspective classification. Hardisty et al
(2010) argued that indicators of perspectives are of-
ten beyond the length of bigrams and demonstrated
that the use of the adaptor grammar inferred n-grams
of arbitrary length as features establishes the start-
of-the-art performance for perspective classification
on the Bitter Lemons corpus, depicting two differ-
ent perspectives of Israeli and Pelestinian. We are
adopting a similar approach in this paper for classi-
fying texts with respect to the author?s native lan-
guage; but the key difference with Hardisty et al
(2010)?s approach is that our focus is on collocations
that mix PoS and lexical elements, rather than being
purely lexical.
3 Maxent Classification
In this section, we first explain the procedures taken
to set up the conventional supervised classification
task for NLI through the deployment of adaptor
grammars for discovery of ?quasi-syntactic colloca-
tions? of arbitrary length. We then present the classi-
fication results attained based on these selected sets
of n-gram features. In all of our experiments, we
investigate two sets of collocations: pure PoS and
a mixture of PoS and function words. The idea of
examining the latter set is motivated by the results
of Wong and Dras (2011) where inclusion of parse
production rules lexicalised with function words as
features had shown to improve the classification per-
formance relative to unlexicalised ones.
3.1 Experimental Setup
3.1.1 Data and evaluation
The classification experiments are conducted on
the second version of ICLE (Granger et al2009).2
Following our earlier NLI work in Wong and Dras
(2011), our data set consists of 490 texts written
in English by authors of seven different native lan-
guage groups: Bulgarian, Czech, French, Russian,
Spanish, Chinese, and Japanese. Each native lan-
guage contributes 70 out of the 490 texts. As we are
using a relative small data set, we perform k-fold
cross-validation, choosing k = 5.
3.1.2 Adaptor grammars for supervised
classification
We derive two adaptor grammars for the maxent
classification setting, where each is associated with
a different set of vocabulary (i.e. either pure PoS
or the mixture of PoS and function words). We use
2Joel Tetreault and Daniel Blanchard from ETS have pointed
out (personal communication) that there is a subtle issue with
ICLE that could have an impact on the classification perfor-
mance of NLI tasks; in particular, when character n-grams are
used as features, some special characters used in some ICLE
texts might affect performance. For our case, this should not be
of much issue since they will not appear in our collocations.
702
the grammar of Johnson (2010) as presented in Sec-
tion 2.2.2, except that the vocabulary differs: either
w ? Vpos or w ? Vpos+fw. For Vpos, there are
119 distinct PoS tags based on the Brown tagset.
Vpos+fw is extended with 398 function words as per
Wong and Dras (2011). m = 490 is the number of
documents, and t = 25 the number of topics (chosen
as the best performing one from Wong et al2011)).
Rules of the form Docj ? Docj Topici that
encode the possible topics that are associated with
a document j are given similar ? priors as used
in LDA (? = 5/t where t = 25 in our experi-
ments). Likewise, similar ? priors from LDA are
placed on the adapted rules expanding from Topici
? Words, representing the possible sequences of
words that each topic comprises (? = 0.01).3 The
inference algorithm for the adaptor grammars are
based on the Markov Chain Monte Carlo technique
made available online by Johnson (2010).4
3.1.3 Classification models with n-gram
features
Based on the two adaptor grammars inferred, the
resulting collocations (n-grams) are extracted as fea-
tures for the classification task of identifying au-
thors? native language. These n-grams found by the
adaptor grammars are only a (not necessarily proper)
subset of those n-grams that are strongly characteris-
tic of a particular native language. In principle, one
could find all strongly characteristic n-grams by enu-
merating all the possible instances of n-grams up to
a given length if the vocabulary is of a small enough
closed set, such as for PoS tags, but this is infeasi-
ble when the set is extended to PoS plus function
words. The use of adaptor grammars here can be
viewed as a form of feature selection, as in Hardisty
et al2010).
Baseline models To serve as a baseline, we take
the commonly used PoS bigrams as per the previ-
ous work of NLI (Koppel et al2005). A set of
200 PoS bigrams is selected in two ways: the 200
most frequent in the training data (as in Wong and
Dras (2011)) and the 200 with the highest informa-
tion gain (IG) values in the training data (not evalu-
3The values of ? and ? are also based on the established
values presented in Wong et al2011).
4Adaptor grammar software is available on http://web.
science.mq.edu.au/?mjohnson/Software.htm.
ated in other work).
Enumerated n-gram models Here, we enumer-
ate all the possible n-grams up to a fixed length and
select the best of these according to IG, as a general-
isation of the baseline. The first motivation for this
feature set is that, in a sense, this should give a rough
upper bound for the adaptor grammar?s PoS-alone n-
grams, as these latter should most often be a subset
of the former. The second motivation is that it gives
a robust comparison for the mixed PoS and function
word n-grams, where it is infeasible to enumerate all
of them.
ENUM-POS We enumerate all possible n-grams up
to the length of 5, and select those that actually
occur (i.e. of the
?5
i=1 119
i possible n-grams,
this is 218,042 based on the average of 5 folds).
We look at the top n-grams up to length 5 selected
by IG: the top 2,800 and the top 6,500 (for com-
parability with adaptor grammar feature sets, be-
low), as well as the top 10,000 and the top 20,000
(to study the effect of larger feature space).
Adaptor grammar n-gram models The classifi-
cation features are the two sets of selected colloca-
tions inferred by the adaptor grammars which are the
main interest of this paper.
AG-POS This first set of the adaptor grammar-
inferred features comprise of pure PoS n-grams
(i.e. Vpos). The largest length of n-gram found
is 17, but about 97% of the collocations are of
length between 2 to 5. We investigate three vari-
ants of this feature set: top 200 n-grams of all
lengths (based on IG), all n-grams of all lengths
(n = 2, 795 on average), and all n-grams up to
the length of 5 (n = 2, 710 on average).
AG-POS+FW This second set of the adaptor
grammar-inferred features are mixtures of PoS
and function words (i.e. Vpos+fw). The largest
length of n-gram found for this set is 19 and
the total number of different collocations found
is much higher. For the purpose of comparabil-
ity with the first set of adaptor grammar features,
we investigate the following five variants for this
feature set: top 200 n-grams of all lengths, all n-
grams of all lengths (n = 6, 490 on average), all
n-grams up to the length of 5 (n = 6, 417 on av-
erage), top 2,800 n-grams of all different lengths,
703
Features (n-grams) Accuracy
BASELINE-POS [top200 MOST-FREQ] 53.87
BASELINE-POS [top200 IG] 56.12
AG-POS [top200 IG] 61.02
AG-POS [all ?17-gram] (n ? 2800) 68.37
AG-POS [all ? 5-gram] (n ? 2700) 68.57
AG-POS+FW [top200 IG] 58.16
AG-POS+FW [all ?19-gram] (n ? 6500) 74.49
AG-POS+FW [all ?5-gram] (n ? 6400) 74.49
AG-POS+FW [top2800 IG ? 19-gram] 71.84
AG-POS+FW [top2800 IG ? 5-gram] 71.84
ENUM-POS [top2800 IG ? 5-gram] 69.79
ENUM-POS [top6500 IG ? 5-gram] 72.44
ENUM-POS [top10K IG ? 5-gram] 71.02
ENUM-POS [top20K IG ? 5-gram] 71.43
Table 1: Maxent classification results for individual fea-
ture sets (with 5-fold cross validation).
and top 2,800 n-grams up to the length of 5. (All
the selections are based on IG).
In our models, all feature values are of binary
type. For the classifier, we employ a maximum en-
tropy (MaxEnt) machine learner ? MegaM (fifth re-
lease) by Hal Daume? III.5
3.2 Classification results
Table 1 presents all the classification results for the
individual feature sets, along with the baselines. On
the whole, both sets of the collocations inferred by
the adaptor grammars perform better than the two
baselines. We make the following observations:
? Regarding ENUM-POS as a (rough) upper
bound, the adaptor grammar AG-POS with a
comparable number of features performs al-
most as well. However, because it is possible to
enumerate many more n-grams than are found
during the sampling process, ENUM-POS opens
up a gap over AG-POS of around 4%.
? Collocations with a mix of PoS and function
words do in fact lead to higher accuracy as
compared to those of pure PoS (except for the
top 200 n-grams); for instance, compare the
2,800 n-grams up to length 5 from the two cor-
responding sets (71.84 vs. 68.57).
? Furthermore, the adaptor grammar-inferred
collocations with mixtures of PoS and function
5MegaM software is available on http://www.cs.
utah.edu/?hal/megam/.
Features (n-grams) Accuracy
AG-POS [all ? 5-gram] & FW 72.04
ENUM-POS [top2800 ? 5-gram] & FW 73.67
AG-POS+FW & AG-POS a 75.71
AG-POS+FW & AG-POS b 74.90
AG-POS+FW & ENUM-POS [top2800] a 73.88
AG-POS+FW & ENUM-POS [top2800] b 74.69
AG-POS+FW & ENUM-POS [top10K] b 74.90
AG-POS+FW & ENUM-POS [top20K] b 75.10
Table 2: Maxent classification results for combined fea-
ture sets (with 5-fold cross validation). aFeatures from
the two sets are selected based on the overall top 3700
with highest IG; bfeatures from the two sets are just lin-
early concatenated.
words (AG-POS+FW) in general perform better
than our rough upper bound of PoS colloca-
tions, i.e. the enumerated PoS n-grams (ENUM-
POS): the overall best results of the two feature
sets are 74.49 and 72.44 respectively.
Given that the AG-POS+FW n-grams are captur-
ing different sorts of document characteristics, they
could potentially usefully be combined with the
PoS-alone features. We thus combined them with
both AG-POS and ENUM-POS feature sets, and the
classification results are presented in Table 2. We
tried two ways of integrating the feature sets: one
way is to take the overall top 2,800 of the two sets
based on IG; the other way is to just combine the two
sets of features by concatenation of feature vectors
(as indicated by a and b respectively in the result
table). For comparability purposes, we considered
only n-grams up to the length of 5. A baseline ap-
proach to this is just to add in function words as un-
igram features by feature vector concatenation, giv-
ing two further models, AG-POS [all ? 5-gram] &
FW and ENUM-POS [top2800 ? 5-gram] & FW.
Overall, the classification accuracies attained by
the combined feature sets are higher than the in-
dividual feature sets. The best performing of all
the models is achieved by combining the mixed
PoS and function word collocations with the adap-
tor grammar-inferred PoS, producing the best accu-
racy thus far of 75.71. This demonstrates that fea-
tures inferred by adaptor grammars do capture some
useful information and function words are playing
a role. The way of integrating the two feature sets
has different effects on the types of combination. As
seen in Table 2, method a works better for the com-
704
bination of the two adaptor grammar feature sets;
whereas method b works better for combining adap-
tor grammar features with enumerated n-gram fea-
tures.
Using adaptor grammar collocations also outper-
forms the alternative baseline of adding in function
words as unigrams. For instance, the best perform-
ing combined feature set of both AG-POS and AG-
POS+FW does result in higher accuracy as compared
to the two alternative baseline models, comparing
75.71 with 72.04 (and 75.71 with 73.67). This
demonstrates that our more general PoS plus func-
tion word collocations derived from adaptor gram-
mars are indeed useful, and supports the argument
of Wang et al2007) that they are a useful tech-
nique for looking into features beyond just the bag
of words.
4 Language Model-based Classification
In this section, we take a language modeling ap-
proach to native language identification; the idea
here is to adopt grammatical inference to learn
a grammar-based language model to represent the
texts written by non-English native users. The gram-
mar learned is then used to predict the most probable
native language that a document (a sentence) is as-
sociated with.
In a sense, we are using a parser-based language
model to rank the documents with respect to native
language. We draw on the work of Bo?rschinger et
al. (2011) for this section. In that work, the task
was grounded learning of a semantic parser. Train-
ing examples there consisted of natural language
strings (descriptions of a robot soccer game) and
a set of candidate meanings (actions in the robot
soccer game world) for the string; each was tagged
with a context identifier reflecting the actual action
of the game. A grammar was then induced that
would parse the examples, and was used on test data
(where the context identifier was absent) to predict
the context. We take a similar approach to devel-
oping an grammatical induction technique, although
where they used a standard LDA topic model-based
PCFG, we use an adaptor grammar. We expect that
the results will likely to be lower than for the dis-
criminative approach of Section 3. However, the
approach is of interest for a few reasons: because,
whereas the adaptor grammar plays an ancillary, fea-
ture selection role in Section 3, here the feature se-
lection is an organic part of the approach as per the
actual implementation of Hardisty et al2010); be-
cause adaptor grammars can potentially be extended
in a natural way with unlabelled data; and because,
for the purposes of this paper, it constitutes a second,
quite different way to evaluate the use of n-gram col-
locations.
4.1 Language Models
We derive two adaptor grammar-based language
models. One consists of only unigrams and bi-
grams, and the other finds n-gram collocations, in
both cases over either PoS or the mix of PoS and
function words. The assumption that we make is that
each document (each sentence) is a mixture of two
sets of topics: one is the native language-specific
topic (i.e. characteristic of the native language) and
the other is the generic topic (i.e. characteristic of
the second language ? English in our case). The
generic topic is thus shared across all languages,
and will behave quite differently from a language-
specific topic, which is not shared. In other words,
there are eight topics, representing seven native lan-
guage groups that are of interest (Bulgarian, Czech,
French, Russian, Spanish, Chinese, and Japanese)
and the second language English itself.6
Bigram models The following rule schema is
applicable to both vocabulary types of PoS and the
mixture of PoS and function words.
Root? lang langTopics
langTopics? langTopics langTopic
langTopics? langTopics nullTopic
langTopics? langTopic
langTopics? nullTopic
langTopic?Words
nullTopic?Words
Words?Word Word
Words?Word
Word? w w ? Vpos; w ? Vpos+fw
N-gram models The grammar is the same as
the above with the exception that the non-terminal
Words is now rewritten as follows in order to
6We could just induce a regular PCFG here, rather than an
adaptor grammar, by taking as terminals all pairs of PoS tags.
We use the adaptor grammar formulation for comparability.
705
capture n-gram collocations of arbitrary length.
Words?Words Word
Words?Word
It should be noted that the two grammars above
can in theory be applied to an entire document or on
individual sentences. For this present work, we work
on the sentence level as the run-time of the current
implementation of the adaptor grammars grows pro-
portional to the cube of the sentence length. For each
grammar we try both sparse and uniform Dirichlet
priors (? = {0.01, 0.1, 1.0}). The sparse priors en-
courage only a minority of the rules to be associated
with high probabilities.
4.2 Training and Evaluation
As we are using the same data set as per the pre-
vious approach, we perform 5-fold cross validation
as well. However, the training for each fold is con-
ducted with a different grammar consisting of only
the vocabulary that occur in each training fold. The
reason is that we are now having a form of super-
vised topic models where the learning process is
guided by the native languages. Hence, each of the
training sentences are prefixed with the (native) lan-
guage identifiers lang, as seen in the Root rules of
the grammar presented above.
To evaluate the grammars learned, as in
Bo?rschinger et al2011) we need to slightly modify
the grammars above by removing the language iden-
tifiers ( lang) from theRoot rules and then parse the
unlabeled sentences using a publicly available CKY
parser.7 The predicted native language is inferred
from the parse output by reading off the langTopics
that the Root is rewritten to. We take that as the
most probable native language for a particular test
sentence. At the document level, we select as the
class the language predicted for the largest number
of sentences in that document.
4.3 Parsing Results
Tables 3 and 4 present the parsing results at the sen-
tence level and the document level, respectively. On
the whole, the results at the sentence level are much
poorer as compared to those at the document level.
In light of the results of Section 3.2, it is surprising
7CKY parser by Mark Johnson is available on
http://web.science.mq.edu.au/?mjohnson/
Software.htm.
Features Accuracy
(n-grams) (? = 0.01) (? = 0.1) (? = 1.0)
AG-POS [bigrams] 26.84 27.03 26.77
AG-POS [n-grams] 25.85 25.78 25.62
AG-POS+FW [bigrams] 28.58 28.40 27.43
AG-POS+FW [n-grams] 26.64 27.64 28.75
Table 3: Language modeling-based classification results
based on parsing (at the sentence level).
Features Accuracy
(n-grams) (? = 0.01) (? = 0.1) (? = 1.0)
AG-POS [bigrams] 41.22 38.88 39.69
AG-POS [n-grams] 36.12 34.90 35.20
AG-POS+FW [bigrams] 47.45 46.94 44.64
AG-POS+FW [n-grams] 43.97 49.39 50.15
Table 4: Language modeling-based classification results
based on parsing (at the document level).
that bigram models appear to perform better than n-
gram models for both types of vocabulary, with the
exception of AG-POS+FW at the document level. In
fact, one would expect n-gram models to perform
better in general as it is a generalisation that would
contain all the potential bigrams. Nonetheless, the
language models over the mixture of PoS and func-
tion words appear to be a more suitable representa-
tive of our learner corpus as compared to those over
purely PoS, confirming the usefulness of integrated
function words for the NLI classification task.
It should also be noted that sparse priors gen-
erally appear to be more appriopriate; except that
for AG-POS+FW n-grams, uniform priors are indeed
better and resulted in the highest parsing result of
50.15. (Although all the parsing results are much
weaker as compared to the results presented in Sec-
tion 3.2, they are all higher than the majority base-
line of 14.29% i.e. 70/490).
5 Discussion
Here we take a closer look at how well each ap-
proach does in identifying the individual native lan-
guages. The confusion matrix for the best model
of two approaches are presented in Table 5 and Ta-
ble 6. Both approaches perform reasonably well for
the two Oriental languages (Chinese in particular);
this is not a major surprise, as the two languages
are not part of the language family that the rest of
the languages come from (i.e. Indo-European). Un-
der the supervised maxent classification, misclassi-
fications largely are observed in the Romance ones
(French and Spanish) as well as Russian; for the lan-
guage model-based approach, Bulgarian is identi-
706
BL CZ RU FR SP CN JP
BL [52] 5 7 4 2 - -
CZ 5 [50] 5 3 4 - 3
RU 6 8 [46] 5 1 - 4
FR 7 3 5 [43] 8 - 4
SP 7 2 4 9 [47] - 1
CN - - - - - [70] -
JP - - 2 2 1 2 [63]
Table 5: Confusion matrix based on the best performing
model under maxent setting (BL:Bulgarian, CZ:Czech,
RU:Russian, FR:French, SP:Spanish, CN:Chinese,
JP:Japanese).
BL CZ RU FR SP CN JP
BL [20] 32 9 6 - 1 2
CZ 2 [59] 3 1 - - 5
RU 3 41 [19] 2 1 - 4
FR 8 20 4 [31] 4 - 3
SP 7 27 11 12 [9] - 4
CN - 2 - 2 - [62] 4
JP - 19 1 2 - 1 [47]
Table 6: Confusion matrix based on the best
performing model under language modeling setting
(BL:Bulgarian, CZ:Czech, RU:Russian, FR:French,
SP:Spanish, CN:Chinese, JP:Japanese).
fied poorly, and Spanish moreso. However, the latter
approach appears to be better in identifying Czech.
On the whole, the maxent approach results in much
fewer misclassifications compared to its counterpart.
In fact, there is a subtle difference in the exper-
imental setting of the models derived from the two
approaches with respect to the adaptor grammar: the
number of topics. Under the maxent setting, the
number of topics t was set to 25, while we restricted
the models with the language modeling approach to
only eight topics (seven for the individual native lan-
guages and one for the common second language,
English). Looking more deeply into the topics them-
selves reveals that there appears to be at least two out
of the 25 topics (from the supervised models) asso-
ciated with n-grams that are indicative of the native
languages, taking Chinese and Japanese as examples
(see the associated topics in Table 7).8 Perhaps as-
sociating each native language with only one gener-
alised topic is not sufficient.
Furthermore, the distribution of n-grams among
the topics (i.e. subtrees of collocations derived
from the adaptor grammars) are quite different be-
tween the two approaches although the total num-
8Taking the examples from Wong et al2011) as reference,
we found similar n-grams that are indicative of Japanese and
Chinese.
Top 10 Mixture N-grams
Japanese Chinese
topic2 topic23 topic9 topic17
. . NN .
we VB PPSS VB a NN NN NN
our NNS my NN NN NN NNS
our NN CC VBN by NN
NN VBG NP . RB ,
PPSS VB PPSS think NP of NN
about NN : JJ NN
because PPSS VBD ( NN .
it . RB as VBG NN
we are PPSS ? NN NN NN NN NN NN NN
Table 7: Top mixture n-grams (collocations) for 4 out of
the 25 topics representative of Japanese and Chinese (un-
der maxent setting). N-grams of pronoun with verb are
found at the upper end of Topic2 and Topic23 reflecting
the frequent usage of Japanese; n-grams of noun are top
n-grams under Topic9 and Topic17 indicating Chinese?s
common error of determiner-noun disagreement.
ber of n-grams inferred by each approach is about
the same. For the language modeling ones, a high
number of n-grams were associated with the generic
topic nullTopic9 and each language-specific topic
langTopic has a lower number of n-grams relative
to bi-grams (Table 8) associated with it. For the
maxent models, in contrast, the majority of the top-
ics were associated with a higher number of n-grams
(Table 9). The smaller number of n-grams to be used
as features ? and the fact that their extra length
means that they will occur more sparsely in the doc-
uments ? seems to be the core of the problem.
Nonetheless, the language models inferred dis-
cover relevant n-grams that are representative of
individual native languages. For instance, the bi-
gram NN NN, which Wong and Dras (2011) claim
may reflect the error of determiner-noun disagree-
ment commonly found amongst Chinese learners,
was found under the Chinese topic at the top-2 posi-
tion with a probability of 0.052 as compared to the
other languages at the probability range of 0.0005-
0.003. Similarly, one example for Japanese, the mix-
ture bigram PPSS think, indicating frequent us-
age of pronouns within Japanese was seen under the
Japanese topic at the top-9 position with a probabil-
ity of 0.025 in relation to other languages within the
range of 0.0002-0.006: this phenomenon as char-
9This is quite plausible as there should be quite a number of
structures that are representative of native English speakers that
are shared by non-native speakers.
707
Model N-gram Frequency
Types BGTopic CZTopic FRTopic RUTopic SPTopic CNTopic JPTopic NullTopic
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
Bigrams 374 187 352 219 426 165 350 211 351 156 397 351 394 194 867 6169
N-grams 177 159 226 217 151 152 148 202 128 147 357 255 209 226 3089 7794
Table 8: Distribution of n-grams (collocations) for each topic under language modeling setting. (a) subcolumns are
for n-grams of pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.
N-gram Frequency
Topic1 Topic2 Topic3 Topic4 Topic5 Topic6 Topic7 Topic8 Topic9 Topic10
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
174 443 145 441 136 245 141 341 236 519 169 748 127 340 182 473 109 339 190 236
Topic11 Topic12 Topic13 Topic14 Topic15 Topic16 Topic17 Topic18 Topic19 Topic20
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
57 259 126 455 103 543 211 225 170 459 81 309 238 207 152 475 119 452 333 423
Topic21 Topic22 Topic23 Topic24 Topic25
(a) (b) (a) (b) (a) (b) (a) (b) (a) (b)
245 341 168 492 194 472 201 366 195 190
Table 9: Distribution of n-grams (collocations) for each topic under maxent setting. (a) subcolumns are for n-grams
of pure PoS and (b) subcolumns are for n-grams of mixtures of PoS and function words.
Languages Excerpts from ICLE
Chinese ... the overpopulation problem in urban area ...
... The development of country park can directly ...
... when it comes to urban renewal project ...
... As developing new town in ...
... and reserve some country park as ...
Japanese ... I think many people will ...
... I think governments should not ...
... I think culture is the most significant ...
... I think the state should not ...
... I really think we must live ...
Table 10: Excerpts from ICLE illustrating the common
phenomena observed amongst Chinese and Japanese.
acteristic of Japanese speakers has also been noted
for different corpora by Ishikawa (2011). (Note that
this collocation as well as its pure PoS counterpart
PPSS VB are amongst the top n-grams discovered
under the maxent setting as seen in Table 7.) Table
10 presents some excerpts extracted from the corpus
that illustrate these two common phenomena.
To investigate further the issue associated with the
number of topics under the language modeling set-
ting, we attempted to extend the adaptor grammar
with three additional topics that represent the lan-
guage family of the seven native languages of inter-
est: Slavic, Romance, and Oriental. (The resulting
grammar is presented as below.) However, the pars-
ing result does not improve over the initial setting
with eight topics in total.
Root? lang langTopics
langTopics? langTopics langTopic
langTopics? langTopics familyTopic
langTopics? langTopics nullTopic
langTopics? langTopic
langTopics? familyTopic
langTopics? nullTopic
langTopic?Words
familyTopic?Words
nullTopic?Words
Words?Words Word
Words?Word
Word? w w ? Vpos; w ? Vpos+fw
6 Conclusion and Future Work
This paper has shown that the extension of adap-
tor grammars to discovering collocations beyond the
lexical, in particular a mix of PoS tags and function
words, can produce features useful in the NLI clas-
sification problem. More specifically, when added
to a new baseline presented in this paper, the com-
bined feature set of both types of adaptor grammar
inferred collocations produces the best result in the
context of using n-grams for NLI. The usefulness of
the collocations does vary, however, with the tech-
nique used for classification.
Future work will involve a broader exploration
of the parameter space of the adaptor grammars,
in particular the number of topics and the value
of ?; a look at other non-parametric extensions of
PCFGs, such as infinite PCFGs (Liang et al2007)
for finding a set of non-terminals permitting more
fine-grained topics; and an investigation of how the
approach can be extended to semi-supervised learn-
ing to take advantage of the vast quantity of texts
with errors available on the Web.
708
Acknowledgments
We would like to acknowledge the support of ARC
Linkage Grant LP0776267. We also thank the
anonymous reviewers for useful feedback. Much
gratitude is due to Benjamin Bo?rschinger for his
help with the language modeling implementation.
References
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alation. Journal of Machine
Learning Research, 3:993?1022.
Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-
son. 2011. Reducing grounded learning tasks to gram-
matical inference. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1416?1425, Edinburgh, Scotland, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan, June.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English (Version 2). Presses Universitaires de
Louvain, Louvian-la-Neuve.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Eric A. Hardisty, Jordan Boyd-Graber, and Philip Resnik.
2010. Modeling perspective using adaptor grammars.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 284?292.
Shun?ichiro Ishikawa. 2011. A New Horizon in Learner
Corpus Studies: The Aim of the ICNALE Project. In
G. Weir, S. Ishikawa, and K. Poonpon, editors, Cor-
pora and Language Technologies in Teaching, Learn-
ing and Research, pages 3?11. University of Strath-
clyde Press, Glasgow, UK.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for speci-
fying compositional nonparametric bayesian models.
In Advances in Neural Information Processing Sys-
tems 19: Proceedings of the Twentieth Annual Confer-
ence on Neural Information Processing Systems, pages
641?648.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and the
Structure of Proper Names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. In Intelligence and Security Informat-
ics, volume 3495 of Lecture Notes in Computer Sci-
ence, pages 209?217. Springer-Verlag.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational Methods in Authorship Attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9?26.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite pcfg using hierarchical
dirichlet processes. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 688?697, Prague, Czech Re-
public, June.
Timothy O?Donnell. 2011. Productivity and reuse in
language. Ph.D. thesis, Harvard University.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?900.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Hans van Halteren. 2008. Source language markers in
EUROPARL translations. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (COLING), pages 937?944.
Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Proceedings of
the 2007 Seventh IEEE International Conference on
Data Mining, ICDM ?07, pages 697?702.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, July.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic modeling for native language identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
709
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1385?1390,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Language Transfer Hypotheses with Linear SVM Weights
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
Language transfer, the characteristic sec-
ond language usage patterns caused by na-
tive language interference, is investigated
by Second Language Acquisition (SLA)
researchers seeking to find overused and
underused linguistic features. In this pa-
per we develop and present a methodology
for deriving ranked lists of such features.
Using very large learner data, we show
our method?s ability to find relevant can-
didates using sophisticated linguistic fea-
tures. To illustrate its applicability to SLA
research, we formulate plausible language
transfer hypotheses supported by current
evidence. This is the first work to ex-
tend Native Language Identification to a
broader linguistic interpretation of learner
data and address the automatic extraction
of underused features on a per-native lan-
guage basis.
1 Introduction
It has been noted in the linguistics literature since
the 1950s that speakers of particular languages
have characteristic production patterns when writ-
ing in a second language. This language transfer
phenomenon has been investigated independently
in a number of fields from different perspectives,
including qualitative research in Second Language
Acquisition (SLA) and more recently though pre-
dictive computational models in NLP.
Motivated by the aim of improving foreign lan-
guage teaching and learning, such analyses are of-
ten done manually in SLA, and are difficult to
perform for large corpora. Smaller studies yield
poor results due to the sample size, leading to
extreme variability (Ellis, 2008). Recently, re-
searchers have noted that NLP has the tools to use
large amounts of data to automate this analysis,
using complex feature types. This has motivated
studies in Native Language Identification (NLI), a
subtype of text classification where the goal is to
determine the native language (L1) of an author
using texts they have written in a second language
(L2) (Tetreault et al., 2013).
Despite the good results in predicting L1s, few
attempts have been made to interpret the features
that distinguish L1s. This is partly because no
methods for an SLA-oriented feature analysis have
been proposed; most work focuses on testing fea-
ture types using standard machine learning tools.
The overarching contribution of this work is to
develop a methodology that enables the transfor-
mation of the NLI paradigm into SLA applications
that can be used to link these features to their un-
derlying linguistic causes and explanations. These
candidates can then be applied in other areas such
as remedial SLA strategies or error detection.
2 Related Work
SLA research aims to find distributional differ-
ences in language use between L1s, often referred
to as overuse, the extensive use of some linguis-
tic structures, and underuse, the underutilization
of particular structures, also known as avoidance
(Gass and Selinker, 2008). While there have been
some attempts in SLA to use computational ap-
proaches on small-scale data,
1
these still use fairly
elementary techniques and have several shortcom-
ings, including in the manual approaches to an-
notation and the computational artefacts derived
from these.
Conversely, NLI work has focused on automatic
learner L1 classification using Machine Learning
with large-scale data and sophisticated linguistic
features (Tetreault et al., 2012). Here, feature
ranking could be performed with relevancy meth-
ods such as the F-score:
1
E.g. Chen (2013), Lozan?o and Mendikoetxea (2010) and
Di?ez-Bedmar and Papp (2008).
1385
F (j) ?
(
?x(+)
j
? ?x
j
)
2
+
(
?x(?)
j
? ?x
j
)
2
1
n
+
?1
n
+
?
i=1
(
x
(+)
i,j
? ?x(+)
j
)
2
+
1
n??1
n?
?
i=1
(
x
(?)
i,j
? ?x(?)
j
)
2
(1)
The F-score (Fisher score) measures the ratio
between the intraclass and interclass variance in
the values of feature j, where x represents the fea-
ture values in the negative and positive examples.
2
More discriminative features have higher scores.
Another alternative method is Information Gain
(Yang and Pedersen, 1997). As defined in equation
(2), it measures the entropy gain associated with
feature t in assigning the class label c.
G(t) = ?
?
m
i=1
Pr (c
i
) log Pr (c
i
)
+ Pr (t)
?
m
i=1
Pr (c
i
|t) log Pr (c
i
|t)
+ Pr (
?
t)
?
m
i=1
Pr (c
i
|
?
t) log Pr (c
i
|
?
t)
(2)
However, these methods are limited: they do not
provide ranked lists per-L1 class, and more impor-
tantly, they do not explicitly capture underuse.
Among the efflorescence of NLI work, a new
trend explored by Swanson and Charniak (2014)
aims to extract lists of candidate language transfer
features by comparing L2 data against the writer?s
L1 to find features where the L1 use is mirrored in
L2 use. This allows the detection of obvious ef-
fects, but Jarvis and Crossley (2012) note (p. 183)
that many transfer effects are ?too complex? to ob-
serve in this manner. Moreover, this method is un-
able to detect underuse, is only suitable for syn-
tactic features, and has only been applied to very
small data (4,000 sentences) over three L1s. Ad-
dressing these issues is the focus of the present
work.
3 Experimental Setup
3.1 Corpus
We use TOEFL11, the largest publicly available
corpus of English L2 texts (Blanchard et al.,
2013), containing 11 L1s with 1,100 texts each.
3
3.2 Features
Adaptor grammar collocations Per Wong et al.
(2012), we utilize an adaptor grammar to discover
arbitrary length n-gram collocations. We explore
both the pure part-of-speech (POS) n-grams as
2
See Chang and Lin (2008) for more details.
3
Over 4 million tokens in 12,100 texts.
well as the more promising mixtures of POS and
function words. We derive two adaptor grammars
where each is associated with a different set of vo-
cabulary: either pure POS or the mixture of POS
and function words. We use the grammar pro-
posed by Johnson (2010) for capturing topical col-
locations:
Sentence? Doc
j
j ? 1, . . . ,m
Doc
j
? j j ? 1, . . . ,m
Doc
j
? Doc
j
Topic
i
i ? 1, . . . , t;
j ? 1, . . . ,m
Topic
i
?Words i ? 1, . . . , t
Words?Word
Words?Words Word
Word? w w ? V
pos
;
w ? V
pos+fw
V
pos
contains 119 distinct POS tags based on the
Brown tagset and V
pos+fw
is extended with 398
function words. The number of topics t is set to
50. The inference algorithm for the adaptor gram-
mars are based on the Markov Chain Monte Carlo
technique made available by Johnson (2010).
4
Stanford dependencies We use Stanford de-
pendencies as a syntactic feature: for each
text we extract all the basic dependencies re-
turned by the Stanford Parser (de Marneffe et
al., 2006). We then generate all the variations
for each of the dependencies (grammatical rela-
tions) by substituting each lemma with its cor-
responding POS tag. For instance, a gram-
matical relation of det(knowledge, the)
yields the following variations: det(NN, the),
det(knowledge, DT), and det(NN, DT).
Lexical features Content and function words
are also considered as two feature types related to
learner?s vocabulary and spelling.
3.3 Extracting Linear SVM Feature Weights
Using the extracted features, we train linear Sup-
port Vector Machine (SVM) models for each
L1. We use a one-vs-rest approach to find fea-
tures most relevant to each native language. L2-
regularization is applied to remove noisy features
and reduce the size of the candidate feature list.
More specifically, we employ the LIBLINEAR
SVM package (Fan et al., 2008)
5
as it is well-
suited to text classification tasks with large num-
bers of features and texts as is the case here.
4
http://web.science.mq.edu.au/%7Emjohnson/Software.htm
5
http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/
1386
In training the models for each feature, the SVM
weight vector
6
is calculated according to (3):
w =
?
i
?
i
y
i
x
i
(3)
After training, the positive and negative weights
are split into two lists and ranked by weight.
The positive weights represent overused features,
while features whose absence (i.e. underuse) is
indicative of an L1 class will have large negative
weights. This yields two candidate language trans-
fer feature lists per L1.
4 Results
We now turn to an analysis of the output from our
system to illustrate its applicability for SLA re-
search. Table 1 lists some elements from the un-
deruse and overuse lists for various L1s. The lists
are of different feature types. They have been cho-
sen to demonstrate all feature types and also a va-
riety of different languages. For reasons of space,
only several of the top features are analysed here.
Hindi L1 writers are distinguished by certain
function words including hence, thus, and etc, and
a much higher usage rate of male pronouns. It has
been observed in the literature (Sanyal, 2007, for
example) that the English spoken in India still re-
tains characteristics of the English that was spoken
during the time of the Raj and the East India Com-
pany that have disappeared from other English va-
rieties, so it sounds more formal to other speakers,
or retains traces of an archaic business correspon-
dence style; the features noted fit that pattern.
The second list includes content words overused
by Arabic L1 learners. Analysis of content words
here, and for other L1s in our data, reveals very
frequent misspellings which are believed to be due
to orthographic or phonetic influences (Tsur and
Rappoport, 2007; Odlin, 1989). Since Arabic does
not share orthography with English, we believe
most of these are due to phonetics. Looking at
items 1, 3 and 5 we can see a common pattern:
the English letter u which has various phonetic re-
alizations is being replaced by a vowel that more
often represents that sound. Items 2 and 5 are also
phonetically similar to the intended words.
For Spanish L1 authors we provide both under-
use and overuse lists of syntactic dependencies.
The top 3 overuse rules show the word that is very
often used as the subject of verbs. This is almost
6
See Burges (1998) for a detailed explanation.
certainly a consequence of the prominent syntac-
tic role played by the Spanish word que which, de-
pending on the context, is equivalent to the English
words whom, who, which, and most commonly,
that. The fourth rule shows they often use this as a
determiner for plural nouns. A survey of the cor-
pus reveals many such errors in texts of Spanish
learners, e.g. this actions or this emissions. The
fifth rule shows that the adjectival modifier of a
plural noun is often being incorrectly pluralised to
match the noun in number as would be required in
Spanish, for example, differents subjects.
Turning to the underused features in Spanish L1
texts, we see that four related features rank highly,
showing that these is not commonly used as a de-
terminer for plural nouns and which is rarely used
as a subject. The final feature shows that no is
avoided as a determiner. This may be because
while no mostly has the same role in Spanish as it
does in English, it cannot be used as a determiner;
ning?un must be used instead. We hypothesize that
this construction is being avoided as placing no be-
fore a noun in Spanish is ungrammatical. This ex-
ample demonstrates that our two list methodology
can not only help identify overused structures, but
also uncovers the related constructs that are being
underutilized at their expense.
The final list in Table 1 is of underused Adap-
tor Grammar patterns by Chinese learners. The
first three features show that these writers signif-
icantly underuse determiners, here an, other and
these before nouns. This is not unexpected since
Chinese learners? difficulties with English articles
are well known (Robertson, 2000). More inter-
estingly, we find underuse of features like even if
and might, along with others not listed here such
as could VB
7
plus many other variants related to
the subjunctive mood. One explanation is that lin-
guistic differences between Chinese and English
in expressing counterfactuals could cause them to
avoid such constructions in L2 English. Previous
research in this area has linked the absence of sub-
junctive linguistic structures in Chinese to differ-
ent cognitive representations of the world and con-
sequences for thinking counterfactually (Bloom,
2014), although this has been disputed (Au, 1983;
Garbern Liu, 1985).
Adaptor Grammars also reveal frequent use of
the ?existential there?
8
in German L1 data while
7
e.g. could be, could have, could go and other variants
8
e.g. There is/are ..., as opposed to the locative there.
1387
Overuse Underuse
Hindi Arabic Spanish Spanish Chinese
#2: thus #2: anderstand #1: nsubj(VBP,that) #2: det(NNS,these) #12: an NN
#4: hence #4: mony #2: nsubj(VBZ,that) #3: nsubj(VBZ,which) #16: other NN
#22: his #6: besy #3: nsubj(VB,that) #6: nsubj(VB,which) #18: these NNS
#30: etc #15: diffrent #4: det(NNS,this) #7: nsubj(VBP,which) #19: even if
#33:rather #38: seccessful #25: amod(NNS,differents) #10: det(NN,no) #68: might
Table 1: Example transfer candidates and rankings from the overuse/underuse lists for various L1s and
features types, in order: Hindi function words, Arabic content words, Spanish dependencies (2) and
Chinese Adaptor Grammars.
English Spanish English Spanish
diferent diferente conclution conclusi?on
consecuence consecuencia desagree Neg. affix des-
responsability responsabilidad especific espec??fico
oportunity oportunidad necesary necesario
Table 2: Highly ranked English misspellings of
Spanish learners and their Spanish cognates.
they are highly underused in French L1 data. The
literature supports our data: The German equiv-
alent es gibt is common while French use is far
more constrained (Cappelle and Loock, 2013).
Lexical analysis also revealed Spanish?English
orthographic transfer, listed in Table 2. This list
includes many cognates, in contrast with the Ara-
bic L1 data where most misspellings were pho-
netic in nature.
We also observe other patterns which remain
unexplained. For instance, Chinese, Japanese and
Korean speakers make excessive use of phrases
such as however, first and second. One possibil-
ity is that this relates to argumentation styles that
are possibly influenced by cultural norms. More
broadly, this effect could also be teaching rather
than transfer related. For example, it may be case
that a widely-used text book for learning English
in Korea happens to overuse this construction.
Some recent findings from the 2013 NLI Shared
Task found that L1 Hindi and Telugu learners of
English had similar transfer effects and their writ-
ings were difficult to distinguish. It has been
posited that this is likely due to shared culture and
teaching environments (Malmasi et al., 2013).
Despite some clearcut instances of overuse,
9
more research is required to determine the causal
factors. We hope to expand on this in future work
using more data.
9
More than half of the Korean scripts contained a
sentence-initial however.
5 Discussion and Conclusion
Using the proposed methodology, we generated
lists of linguistic features overused and underused
by English learners of various L1 backgrounds.
Through an analysis of the top items in these
ranked lists, we demonstrated the high applicabil-
ity of the output by formulating plausible language
transfer hypotheses supported by current evidence.
We also showcased the method?s generalizability
to numerous linguistic feature types.
Our method?s output consists of two ranked lists
of linguistic features: one for overuse and the
other for underuse, something which had not been
addressed by research to date. We also found
Adaptor Grammar collocations to be highly infor-
mative for this task.
This work, an intersection of NLP, Machine
Learning and SLA, illustrates how the various dis-
ciplines can complement each other by bringing
together theoretical, experimental and computa-
tional issues. NLP provides accurate and auto-
mated tagging of large corpora with sophisticated
features not available in corpus linguistics, e.g.
with state-of-the-art dependency parsing. Sophis-
ticated machine learning techniques then enable
the processing of large quantities of data (thou-
sands of times the size of manual studies) in a way
that will let SLA researchers explore a variety of
assumptions and theoretical analyses. And con-
versely, NLP can benefit from the long-term study
and language acquisition insights from SLA.
In terms of NLI, this work is the first attempt to
expand NLI to a broad linguistic interpretation of
the data, including feature underuse. NLI systems
achieve classification accuracies of over 80% on
this 11-class task, leading to theoretical questions
about the features that make them so effective.
This work also has a backwards link in this regard
by providing qualitative evidence about the under-
pinning linguistic theories that make NLI work.
1388
The work presented here has a number of ap-
plications; chief among them is the development
of tools for SLA researchers. This would enable
them to not just provide new evidence for previ-
ous findings, but to also perform semi-automated
data-driven generation of new and viable hypothe-
ses. This, in turn, can help reduce expert effort and
involvement in the process, particularly as such
studies expand to more corpora and emerging lan-
guage like Chinese (Malmasi and Dras, 2014b)
and Arabic (Malmasi and Dras, 2014a).
The brief analysis included here represents only
a tiny portion of what can be achieved with this
methodology. We included but a few of the thou-
sands of features revealed by this method; prac-
tical SLA tools based on this would have a great
impact on current research.
In addition to language transfer hypotheses,
such systems could also be applied to aid devel-
opment of pedagogical material within a needs-
based and data-driven approach. Once language
use patterns are uncovered, they can be assessed
for teachability and used to create tailored, L1-
specific exercises and teaching material.
From the examples discussed in Section 4 these
could include highly specific and targeted student
exercises to improve spelling, expand vocabulary
and enrich syntactic knowledge ? all relative to
their mother tongue. Such exercises can not only
help beginners improve their fundamental skills
and redress their errors but also assist advanced
learners in moving closer to near-nativeness.
The extracted features and their weights could
also be used to build statistical models for gram-
matical error detection (Leacock et al., 2014).
Contrary to the norm of developing error checkers
for native writers, such models could be specifi-
cally targeted towards learners or even particular
L1?L2 pairs which could be useful in Computer-
Assisted Language Learning (CALL) systems.
One limitation here is that our features may
be corpus-dependent as they are all exam essays.
This can be addressed by augmenting the data with
new learner corpora, as they become available.
While a strength here is that we compared each L1
against others, a paired comparison only against
native texts can be insightful too.
There are several directions for future work.
The first relates to clustering the data within the
lists. Our intuition is that there might be coher-
ent clusters of related features, with these clusters
characterising typical errors or idiosyncrasies, that
are predictive of a particular L1. As shown in our
results, some features are highly related and may
be caused by the same underlying transfer phe-
nomena. For example, our list of overused syntac-
tic constructs by Spanish learners includes three
high ranking features related to the same transfer
effect. The use of unsupervised learning meth-
ods such as Bayesian mixture models may be ap-
propriate here. For parse features, tree kernels
could help measure similarity between the trees
and fragments (Collins and Duffy, 2001).
Another avenue is to implement weight-based
ranking methods to further refine and re-rank the
lists, potentially by incorporating the measures
mentioned in Section 2 to assign weights to fea-
tures. As the corpus we used includes learner
proficiency metadata, it may also be possible to
create proficiency-segregated models to find the
features that characterise errors at each language
proficiency level. Finally, the use of other lin-
guistic features such as Context-free Grammar
phrase structure rules or Tree Substitution Gram-
mars could provide additional insights.
In addition to these further technical investiga-
tions, we see as a particularly useful direction the
development of an SLA research tool to conduct a
large SLA study with a wide range of experts. We
believe that this study makes a contribution to this
area and hope that it will motivate future work.
References
Terry Kit-Fong Au. 1983. Chinese and English coun-
terfactuals: the Sapir-Whorf hypothesis revisited.
Cognition, 15(1):155?187.
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A Corpus of Non-Native English. Tech-
nical report, Educational Testing Service.
Alfred H Bloom. 2014. The linguistic shaping of
thought: A study in the impact of language on think-
ing in China and the West. Psychology Press.
Christopher JC Burges. 1998. A tutorial on Support
Vector Machines for Pattern Recognition. Data min-
ing and knowledge discovery, 2(2):121?167.
Bert Cappelle and Rudy Loock. 2013. Is there in-
terference of usage constraints?: A frequency study
of existential there is and its French equivalent il
ya in translated vs. non-translated texts. Target,
25(2):252?275.
1389
Yin-Wen Chang and Chih-Jen Lin. 2008. Feature
ranking using linear svm. Causation and Prediction
Challenge Challenges in Machine Learning, Volume
2, page 47.
Meilin Chen. 2013. Overuse or underuse: A cor-
pus study of English phrasal verb use by Chinese,
British and American university students. Interna-
tional Journal of Corpus Linguistics, 18(3).
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Advances in Neu-
ral Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454, Genoa, Italy.
Mar??a Bel?en Di?ez-Bedmar and Szilvia Papp. 2008.
The use of the English article system by Chinese
and Spanish learners. Language and Computers,
66(1):147?176.
Rod Ellis. 2008. The Study of Second Language Ac-
quisition, 2nd edition. Oxford University Press, Ox-
ford, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Lisa Garbern Liu. 1985. Reasoning counterfactually
in Chinese: Are there any obstacles? Cognition,
21(3):239?270.
Susan M. Gass and Larry Selinker. 2008. Second Lan-
guage Acquisition: An Introductory Course. Rout-
ledge, New York.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach. Multilingual Matters, Bristol, UK.
Mark Johnson. 2010. PCFGs, Topic Models, Adaptor
Grammars and Learning Topical Collocations and
the Structure of Proper Names. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1148?1157, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2014. Automated grammati-
cal error detection for language learners. Synthesis
Lectures on Human Language Technologies, 7(1):1?
170.
Cristobal Lozan?o and Amaya Mendikoetxea. 2010. In-
terface conditions on postverbal subjects: A corpus
study of L2 English. Bilingualism: Language and
Cognition, 13(4):475?497.
Shervin Malmasi and Mark Dras. 2014a. Arabic Na-
tive Language Identification. In Proceedings of the
Arabic Natural Language Processing Workshop (co-
located with EMNLP 2014), Doha, Qatar, October.
Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. Chinese
Native Language Identification. Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, April.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. NLI Shared Task 2013: MQ Submis-
sion. In Proceedings of the Eighth Workshop on In-
novative Use of NLP for Building Educational Ap-
plications, pages 124?133, Atlanta, Georgia, June.
Association for Computational Linguistics.
Terence Odlin. 1989. Language Transfer: Cross-
linguistic Influence in Language Learning. Cam-
bridge University Press, Cambridge, UK.
Daniel Robertson. 2000. Variability in the use of the
English article system by Chinese learners of En-
glish. Second Language Research, 16(2):135?172.
Jyoti Sanyal. 2007. Indlish: The Book for Every
English-Speaking Indian. Viva Books Private Lim-
ited.
Ben Swanson and Eugene Charniak. 2014. Data
Driven Language Transfer Hypotheses. EACL 2014,
page 169.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native Tongues, Lost
and Found: Resources and Empirical Evaluations in
Native Language Identification. In Proceedings of
COLING 2012, pages 2585?2602, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48?57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computat.
Language Acquisition, pages 9?16.
Sze-Meng Jojo Wong, Mark Dras, and Mark John-
son. 2012. Exploring Adaptor Grammars for Na-
tive Language Identification. In Proc. Conf. Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 699?709.
Yiming Yang and Jan O Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In ICML, volume 97, pages 412?420.
1390
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 95?99,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Chinese Native Language Identification
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
We present the first application of Na-
tive Language Identification (NLI) to non-
English data. Motivated by theories of lan-
guage transfer, NLI is the task of iden-
tifying a writer?s native language (L1)
based on their writings in a second lan-
guage (the L2). An NLI system was ap-
plied to Chinese learner texts using topic-
independent syntactic models to assess
their accuracy. We find that models using
part-of-speech tags, context-free grammar
production rules and function words are
highly effective, achieving a maximum ac-
curacy of 71% . Interestingly, we also find
that when applied to equivalent English
data, the model performance is almost
identical. This finding suggests a sys-
tematic pattern of cross-linguistic transfer
may exist, where the degree of transfer is
independent of the L1 and L2.
1 Introduction
Native Language Identification (NLI) is the task of
identifying an author?s native language (L1) based
on their writings in a second language (the L2).
NLI works by identifying language use patterns
that are common to groups of speakers that share
the same native language. This process is under-
pinned by the presupposition that an author?s L1
will dispose them towards particular language pro-
duction patterns in their L2, as influenced by their
mother tongue. This relates to Cross-Linguistic
Influence (CLI), a key topic in the field of Second
Language Acquisition (SLA) that analyzes trans-
fer effects from the L1 on later learned languages
(Ortega, 2009).
While NLI has applications in security, most re-
search has a strong linguistic motivation relating to
language teaching and learning. Rising numbers
of language learners have led to an increasing need
for language learning resources, which has in turn
fuelled much of the language acquisition research
of the past decade. In this context, by identify-
ing L1-specific language usage and error patterns,
NLI can be used to better understand SLA and de-
velop teaching methods, instructions and learner
feedback that is specific to their mother tongue.
However, all of the NLI research to date has fo-
cused exclusively on English L2 data. To this end
there is a need to apply NLI to other languages,
not only to gauge their applicability but also to aid
in teaching research for other emerging languages.
Interest in learning Chinese is rapidly growing,
leading to increased research in Teaching Chinese
as a Second Language (TCSL) and the develop-
ment of related resources such as learner corpora
(Chen et al., 2010). The application of these tools
and scientific methods like NLI can greatly assist
researchers in creating effective teaching practices
and is an area of active research.
The aim of this research is to evaluate the cross-
language applicability of NLI techniques by ap-
plying them to Chinese learner texts, evaluating
their efficacy and comparing the results with their
English equivalents.
To the best of our knowledge this is the first
reported application of NLI to non-English data
and we believe this is an important step in gain-
ing deeper insights about the technique.
2 Related Work
NLI is a fairly recent, but rapidly growing area of
research. While some research was conducted in
the early 2000s, the most significant work has only
appeared in the last few years (Wong and Dras,
2009; Wong and Dras, 2011; Swanson and Char-
niak, 2012; Tetreault et al., 2012; Bykh and Meur-
ers, 2012).
Most studies approach NLI as a multi-class su-
pervised classification task. In this experimental
design, the L1 metadata are used as class labels
95
and the individual writings are used as training and
testing data. Using lexical and syntactic features
of increasing sophistication, researchers have ob-
tained good results under this paradigm. While a
detailed exposition of NLI has been omitted here
due to space constraints, a concise review can be
found in Bykh and Meurers (2012).
2.1 NLI 2013 Shared Task
This increased interest brought unprecedented
level of research focus and momentum, resulting
in the first NLI shared task being held in 2013.
1
The shared task aimed to facilitate the comparison
of results by providing a large NLI-specific dataset
and evaluation procedure, to enable direct compar-
ison of results achieved through different methods.
Overall, the event was considered a success, draw-
ing 29 entrants and experts from not only Compu-
tational Linguistics, but also SLA. The best teams
achieved accuracies of greater than 80% on this
11-class classification task. A detailed summary
of the results is presented in Tetreault et al. (2013).
3 Data
Growing interest has led to the recent develop-
ment of the Chinese Learner Corpus (Wang et al.,
2012), the first large-scale corpus of learner texts
comprised of essays written by university stu-
dents. Learners from 59 countries are represented
and proficiency levels have been sampled repre-
sentatively across beginners, intermediate and ad-
vanced learners. However, texts by native speak-
ers of other Asian countries are disproportionately
represented, likely due to geographical proximity.
For this work we extracted 3.75 million tokens
of text from the CLC in the form of individual
sentences.
2
Following the methodology of Brooke
and Hirst (2011), we combine the sentences from
the same L1 to form texts of 600 tokens on aver-
age, creating a set of documents suitable for NLI
3
.
We choose the top 11 languages, shown in Ta-
ble 1, to use in our experiments. This is due to
two considerations. First, while many L1s are rep-
resented in the corpus, most have relatively few
texts. Choosing the top 11 classes allows us to
1
Organised by the Educational Testing Service and co-
located with the eighth instalment of the Building Ed-
ucational Applications Workshop at NAACL/HLT 2013.
sites.google.com/site/nlisharedtask2013/
2
Full texts are not made available, only individual sen-
tences with the relevant metadata (proficiency/nationality).
3
Pending permission from the CLC corpus authors, we
will attempt to release the Chinese NLI dataset publicly.
Language Size Language Size
Filipino FIL 415 Indonesian IND 402
Thai THA 400 Laotian LAO 366
Burmese MYA 349 Korean
?
KOR 330
Khmer KHM 294 Vietnamese VIE 267
Japanese
?
JAP 180 Spanish
?
SPA 112
Mongolian MON 101
Table 1: Our data, broken down by language and
the number of texts in each class. Languages over-
lapping with the TOEFL11 corpus marked with
?
.
balance having a large number of classes, and also
maximizes the amount of data used. Secondly, this
is the same number of classes used in the NLI 2013
shared task, enabling us to draw cross-language
comparisons with the shared task results.
4 Experimental Setup
We also follow the supervised classification ap-
proach described in ?2. We devise and run exper-
iments using several models that capture different
types of linguistic information. For each model,
features are extracted from the texts and a clas-
sifier is trained to predict the L1 labels using the
features. As our data is not topic-balanced, we
avoid using topic-dependent lexical features such
as character or word n-grams.
Each experiment is run with two feature repre-
sentations: binary (presence/absence of a feature)
and normalized frequencies, where feature values
are normalized to text length using the l2-norm.
4.1 Parser
The Stanford CoreNLP
4
suite of NLP tools and
the provided Chinese models are used to tokenize,
PoS tag and parse the unsegmented corpus texts.
4.2 Classifier
We use Support Vector Machines for classifica-
tion. Specifically, we use the LIBLINEAR SVM
package (Fan et al., 2008) as it is well-suited to
text classification tasks with large numbers of fea-
tures and texts. We use the L2-regularized L2-loss
support vector classification (dual) solver.
4.3 Evaluation
The same evaluation metrics and standards used in
the NLI2013 Shared Task are used: we report clas-
sification accuracy under 10-fold cross-validation.
We also use the same number of classes as the
shared task to facilitate comparative analyses.
4
http://nlp.stanford.edu/software/corenlp.shtml
96
Feature Accuracy (%)
Binary Frequency
Random Baseline 9.09 9.09
PoS unigrams 20.12 35.32
Part-of-Speech bigrams 32.83 54.24
Part-of-Speech trigrams 47.24 55.60
Function Words 43.93 51.91
Production Rules 36.14 49.80
All features 61.75 70.61
Table 2: Chinese Native Language Identification
accuracy (%) for all of our models.
5 Experiments and Results
5.1 Part-of-Speech tag n-grams
Our first experiment assesses the utility of the
syntactic information captured by part-of-speech
(PoS) tags for Chinese NLI. The PoS tags for each
text are predicted and n-grams of size 1?3 are ex-
tracted from the tags. These n-grams capture (very
local) syntactic patterns of language use and are
used as classification features.
The results for these three features, and our
other models are shown in Table 2. The trigram
frequencies give the best accuracy of 55.60%, sug-
gesting that there exist group-specific patterns of
Chinese word order and category choice which
provide a highly discriminative cue about the L1.
5.2 Function Words
As opposed to content words, function words are
topic-independent grammatical words that indi-
cate the relations between other words. They
include determiners, conjunctions and auxiliary
verbs. Distributions of English function words
have been found to be useful in studies of author-
ship attribution and NLI. Unlike PoS tags, this
model analyzes the author?s specific word choices.
We compiled a list of 449 Chinese function
words
5
to be used as features in this model. As
shown in Table 2, the function word frequency
features provide the best accuracy of 51.91%,
significantly higher than the random baseline.
This again suggests the presence of L1-specific
grammatical and lexical choice patterns that can
help distinguish the L1, potentially due to cross-
linguistic transfer. Such lexical transfer effects
5
The function word list was compiled from Chinese lan-
guage teaching resources. The complete list can be accessed
at http://comp.mq.edu.au/
?
madras/research/
data/chinese-fw.txt
ROOT
IP
PU
?
VP
VP
IP
VP
VV
??
QP
CLP
M
?
CD
?
VE
?
PP
NP
NP
NN
??
DP
DT
??
P
?
NP
PN
?
IP ? NP VP PU VP ? PP VP
NP ? DP NP PP ? P NP
Figure 1: A constituent parse tree for a sentence
from the corpus along with some of the context-
free grammar production rules extracted from it.
have been previously noted by researchers and
linguists (Odlin, 1989). These effects are medi-
ated not only by cognates and similarities in word
forms, but also word semantics and meanings.
5.3 Context-free Grammar Production Rules
In the next experiment we investigate the differ-
ences in the distribution of the context-free gram-
mar production rules used by the learners. To do
this, constituent parses for all sentences are ob-
tained and the production rules, excluding lexical-
izations, are extracted. Figure 1 shows a sample
tree and rules. These context-free phrase structure
rules capture the overall structure of grammatical
constructions and are used as classification fea-
tures in this experiment.
As seen in Table 2, the model achieves an accu-
racy of 49.80%. This supports the hypothesis that
the syntactic substructures contain characteristic
constructions specific to L1 groups and that these
syntactic cues strongly signal the writer?s L1.
5.4 Combining All Features
Finally, we assess the redundancy of the informa-
tion captured by our models by combining them
all into one vector space to create a single clas-
sifier. From Table 2 we see that for each feature
representation, the combined feature results are
higher than the single best feature, with a max-
97
imum accuracy of 70.61%. This demonstrates
that for at least some of the features, the informa-
tion they capture is orthogonal and complemen-
tary, and combining them can improve results.
6 Discussion
A key finding here is that NLI models can be suc-
cessfully applied to non-English data. This is an
important step for furthering NLI research as the
field is still relatively young and many fundamen-
tal questions have yet to be answered.
All of the tested models are effective, and they
appear to be complementary as combining them
improves overall accuracy. We also note the differ-
ence in the efficacy of the feature representations
and see a clear preference for frequency-based fea-
ture values. Others have found that binary features
are the most effective for English NLI (Brooke and
Hirst, 2012), but our results indicate frequency in-
formation is more informative in this task. The
combination of both feature types has also been
reported to be effective (Malmasi et al., 2013).
To see how these models perform across lan-
guages, we also compare the results against the
TOEFL11 corpus used in the NLI2013 shared
task. We perform the same experiments on that
dataset using the English CoreNLP models, Penn
Treebank PoS tagset and a set of 400 English func-
tion words. Figure 2 shows the results side by side.
Remarkably, we see that the model results
closely mirror each other across corpora. This is a
highly interesting finding from our study that mer-
its further investigation. There is a systematic pat-
tern occurring across data from learners of com-
pletely different L1-L2 pairs. This suggests that
manifestations of CLI via surface phenomena oc-
cur at the same levels and patternings regardless
of the L2. Cross-language studies can help re-
searchers in linguistics and cognitive science to
better understand the SLA process and language
transfer effects. They can enhance our understand-
ing of how language is processed in the brain in
ways that are not possible by just studying mono-
linguals or single L1-L2 pairs, thereby providing
us with important insights that increase our knowl-
edge and understanding of the human language
faculty.
One limitation of this work is the lack of sim-
ilar amounts of training data for each language.
However, many of the early and influential NLI
studies (e.g. Koppel et al. (2005), Tsur and Rap-
poport (2007)) were performed under similar cir-
PoS-1 PoS-2 PoS-3
FW PR
0
20
40
60
A
c
c
u
r
a
c
y
(
%
)
Chinese English
Figure 2: Comparing feature performance on the
Chinese Learner Corpus and English TOEFL11
corpora. PoS-1/2/3: PoS uni/bi/trigrams, FW:
Function Words, PR: Production Rules
cumstances. This issue was noted at the time, but
did not deter researchers as corpora with similar
issues were used for many years. Non-English
NLI is also at a similar state where the extant cor-
pora are not optimal for the task, but no other al-
ternatives exist for conducting this research.
Finally, there are also a number of way to fur-
ther develop this work. Firstly, the experimental
scope could be expanded to use even more lin-
guistically sophisticated features such as depen-
dency parses. Model accuracy could potentially
be improved by using the metadata to develop
proficiency-segregated models. Classifier ensem-
bles could also help in increasing the accuracy.
7 Conclusion
In this work we have presented the first application
of NLI to non-English data. Using the Chinese
Learner Corpus, we compare models based on
PoS tags, function words and context-free gram-
mar production rules and find that they all yield
high classification accuracies.
Comparing the models against an equivalent
English learner corpus we find that the accura-
cies are almost identical across both L2s, suggest-
ing a systematic pattern of cross-linguistic transfer
where the degree of transfer is independent of the
L1 and L2. Further research with other L2 learner
corpora is needed to investigate this phenomena.
Acknowledgments
We wish to thank Associate Professor Maolin
Wang for providing access to the CLC corpus, and
Zhendong Zhao for his assistance. We also thank
the reviewers for their constructive feedback.
98
References
Julian Brooke and Graeme Hirst. 2011. Na-
tive language detection with ?cheap? learner cor-
pora. In Conference of Learner Corpus Research
(LCR2011), Louvain-la-Neuve, Belgium. Presses
universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lex-
icalized Native Language Identification. In Pro-
ceedings of COLING 2012, pages 391?408, Mum-
bai, India, December. The COLING 2012 Organiz-
ing Committee.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence.
In Proceedings of COLING 2012, pages 425?440,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Jianguo Chen, Chuang Wang, and Jinfa Cai. 2010.
Teaching and learning Chinese: Issues and perspec-
tives. IAP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Automatically determining an anonymous au-
thor?s native language. In Intelligence and Security
Informatics, volume 3495 of LNCS, pages 209?217.
Springer-Verlag.
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. Nli shared task 2013: Mq submission.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 124?133, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
Terence Odlin. 1989. Language Transfer: Cross-
linguistic Influence in Language Learning. Cam-
bridge University Press, Cambridge, UK.
Lourdes Ortega. 2009. Understanding Second Lan-
guage Acquisition. Hodder Education, Oxford, UK.
Benjamin Swanson and Eugene Charniak. 2012.
Native Language Detection with Tree Substitution
Grammars. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 193?197, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in na-
tive language identification. In Proceedings of COL-
ING 2012, pages 2585?2602, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48?57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computat.
Language Acquisition, pages 9?16.
Maolin Wang, Qi Gong, Jie Kuang, and Ziyu Xiong.
2012. The development of a chinese learner corpus.
In Speech Database and Assessments (Oriental CO-
COSDA), 2012 International Conference on, pages
1?6. IEEE.
Sze-Meng Jojo Wong and Mark Dras. 2009. Con-
trastive Analysis and Native Language Identifica-
tion. In Proceedings of the Australasian Language
Technology Association Workshop 2009, pages 53?
61, Sydney, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing Parse Structures for Native Language Identifi-
cation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1600?1610, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
99
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 384?388,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Clause Restructuring For SMT Not Absolutely Helpful
Susan Howlett and Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
susan.howlett@students.mq.edu.au, mark.dras@mq.edu.au
Abstract
There are a number of systems that use a
syntax-based reordering step prior to phrase-
based statistical MT. An early work proposing
this idea showed improved translation perfor-
mance, but subsequent work has had mixed re-
sults. Speculations as to cause have suggested
the parser, the data, or other factors. We sys-
tematically investigate possible factors to give
an initial answer to the question: Under what
conditions does this use of syntax help PSMT?
1 Introduction
Phrase-based statistical machine translation (PSMT)
translates documents from one human language to
another by dividing text into contiguous sequences
of words (phrases), translating each, and finally re-
ordering them according to a distortion model.
The PSMT distortion model typically does not
consider linguistic information, and as such encoun-
ters difficulty in language pairs that require specific
long-distance reorderings, such as German?English.
Collins et al (2005) address this problem by re-
ordering German sentences to more closely paral-
lel English word order, prior to translation by a
PSMT system. They find that this reordering-as-
preprocessing approach results in a significant im-
provement in translation performance over the base-
line. However, there have been several other systems
using the reordering-as-preprocessing approach, and
they have met with mixed success.
We systematically explore possible explanations
for these contradictory results, and conclude that,
while reordering is helpful for some sentences, po-
tential improvement can be eroded by many aspects
of the PSMT system, independent of the reordering.
2 Prior Work
Reordering-as-preprocessing systems typically in-
volve three steps. First, the input sentence is parsed.
Second, the parse is used to permute the words ac-
cording to some reordering rules, which may be
automatically or manually determined. Finally, a
phrase-based SMT system is trained and tested us-
ing the reordered sentences as input, in place of the
original sentences. Many such systems exist, with
results being mixed; we review several here.
Xia and McCord (2004) (English-to-French trans-
lation, using automatically-extracted reordering
rules) train on the Canadian Hansard. On a Hansard
test set, an improvement over the baseline was only
seen if the translation system?s phrase table was re-
stricted to phrases of length at most four. On a
news test set, the reordered system performed sig-
nificantly better than the baseline regardless of the
maximum length of phrases. However, this improve-
ment was only apparent with monotonic decoding;
when using a distortion model, the difference dis-
appeared. Xia and McCord attribute the drop-off
in performance on the Hansard set to similarity of
training and test data.
Collins et al (2005) (German-to-English) use six
hand-crafted reordering rules targeting the place-
ment of verbs, subjects, particles and negation. They
train and evaluate their system on Europarl text and
obtain a BLEU score (Papineni et al, 2002) of 26.8,
with the baseline PSMT system achieving 25.2. A
human evaluation confirms that reordered transla-
tions are generally (but not universally) better.
On Web text, Xu et al (2009) report significant
improvements applying one set of hand-crafted rules
to translation from English to each of five SOV lan-
384
guages: Korean, Japanese, Hindi, Urdu and Turkish.
Training on news text, Wang et al (2007)
(Chinese-to-English, hand-crafted rules) report a
significant improvement over the baseline system on
the NIST 2006 test set, using a distance-based dis-
tortion model. Similar results are mentioned in pass-
ing for a lexicalised distortion model.
Also on news text, Habash (2007) (automatically-
extracted rules, Arabic-to-English) reports a very
large improvement when phrases are limited to
length 1 and translation is monotonic. However,
allowing phrases up to 7 words in length or using
a distance-based distortion model causes the differ-
ence in performance to disappear. Habash attributes
this to parser and alignment performance. He also
includes oracle experiments, in which each system
outperforms the other on 40?50% of sentences, sug-
gesting that reordering is useful for many sentences.
Zwarts and Dras (2007) implement six rules for
Dutch-to-English translation, analogous to those of
Collins et al (2005), as part of an exploration of
dependency distance in syntax-augmented PSMT.
Considering only their baseline and reordered sys-
tems, the improvement is from 20.7 to only 20.8;
they attribute their poor result to the parser used.
Howlett and Dras (2010) reimplement the Collins
et al (2005) system for use in lattice-based transla-
tion. In addition to their main system, they give re-
sults for the baseline and reordered systems, training
and testing on Europarl and news text. In strong con-
trast to the results of Collins et al (2005), Howlett
and Dras (2010) report 20.04 for the reordered sys-
tem, below the baseline at 20.77. They explain their
lower absolute scores as a consequence of the differ-
ent test set, but do not explore the reversal in conclu-
sion. Like Habash (2007), Howlett and Dras (2010)
include oracle experiments which demonstrate that
the reordering is useful for some sentences.
In this paper, we focus on the Collins et al (2005)
and Howlett and Dras (2010) systems (hereafter
CKK and HD), as they are the most similar but have
perhaps the most divergent results. Possible expla-
nations for the difference are differences in the re-
ordering process, from either parser performance or
implementation of the rules, and differences in the
translation process, including PSMT system setup
and data used. We examine parser performance in
?3 and the remaining possibilities in ?4?5.
Precision Recall
Dubey and Keller (2003) 65.49 70.45
Petrov and Klein (2008) 69.23 70.41
Howlett and Dras (2010) 72.78 73.15
This paper, lowercased 71.09 73.16
This paper, 50% data 68.65 70.86
This paper, 50% data, lowerc. 67.59 70.23
This paper, 25% data 65.24 67.13
This paper, 10% data 61.56 63.01
Table 1: Precision and recall for the parsers mentioned in
?3. The numbers are collated for reference only and are
not directly comparable; see the text for details.
3 Parser Performance
We first compare the performance of the two parsers
used. CKK uses the Dubey and Keller (2003) parser,
which is trained on the Negra corpus (Skut et al,
1997). HD instead uses the Berkeley parser (Petrov
et al, 2006), trained on Negra?s successor, the larger
Tiger corpus (Brants et al, 2002).
Refer to Table 1 for precision and recall for each
model. Note that the CKK reordering requires not
just category labels (e.g. NP) but also function labels
(e.g. SB for subject); parser performance typically
goes down when these are learnt, due to sparsity. All
models in Table 1 include function labels.
Dubey and Keller (2003) train and test on the
Negra corpus, with 18,602 sentences for training,
1,000 development and 1,000 test, removing sen-
tences longer than 40 words.
Petrov and Klein (2008) train and test the Berke-
ley parser on part of the Tiger corpus, with 20,894
sentences for training and 2,611 sentences for each
of development and test, all at most 40 words long.
The parsing model used by HD is trained on
the full Tiger corpus, unrestricted for length, with
38,020 sentences for training and 2,000 sentences
for development. The figures reported in Table 1
are the model?s performance on this development
set. With twice as much data, the increase in per-
formance is unsurprising.
From these figures, we conclude that sheer parser
grunt is unlikely to be responsible for the discrep-
ancy between CKK and HD. It is possible that parser
output differs qualitatively in some important way;
parser figures alone do not reveal this.
Here, we reuse the HD parsing model, plus five
385
Data Set name Size
CKK Train 751,088
Test 2,000
WMT Train europarl-v4 1,418,115
Tuning test2007 2,000
news-test2008 2,051
Test test2008 2,000
newstest2009 2,525
Table 2: Corpora used, and # of sentence pairs in each.
additional models trained by the same method. The
first is trained on the same data, lowercased; the
next two use only 19,000 training sentences (for one
model, lowercased); the fourth uses 9,500 sentences;
the fifth only 3,800 sentences. The 50% data models
are closer to the amount of data available to CKK,
and the 25% and 10% models are to investigate the
effects of further reduced parser quality.
4 Experiments
We conduct a number of experiments with the HD
system to attempt to replicate the CKK and HD find-
ings. All parts of the system are available online.1
Each experiment is paired: the reordered system
reuses the recasing and language models of its cor-
responding baseline system, to eliminate one source
of possible variation. Training the parser with less
data affects only the reordered systems; for experi-
ments using these models, the corresponding base-
lines (and thus the shared models) are not retrained.
For each system pair, we also run the HD oracle.
4.1 System Variations
CKK uses the PSMT system Pharaoh (Koehn et al,
2003), whereas HD uses its successor Moses (Koehn
et al, 2007). In itself, this should not cause a dra-
matic difference in performance, as the two systems
perform similarly (Hoang and Koehn, 2008).
However, there are a number of other differences
between the two systems. Koehn et al (2003) (and
thus presumably CKK) use an unlexicalised distor-
tion model, whereas HD uses a lexicalised model.
CKK does not include a tuning (minimum error rate
training) phase, unlike HD. Finally, HD uses a 5-
gram language model. The CKK language model is
unspecified; we assume a 3-gram model would be
1http://www.showlett.id.au/
LM DM T Base. Reord. Diff. Oracle
3 dist ? 25.58 26.73 +1.15 28.11
26.63 +1.05 28.03
Table 3: Replicating CKK. Top row: full parsing model;
second row: 50% parsing model. Columns as for Table 4.
more likely for the time. We explore combinations
of all these choices.
4.2 Data
A likely cause of the results difference between HD
and CKK is the data used. CKK used Europarl for
training and test, while HD used Europarl and news
for training, with news for tuning and test.
Our first experiment attempts to replicate CKK as
closely as possible, using the CKK training and test
data. This data came already tokenized and lower-
cased; we thus skip tokenisation in preprocessing,
use the lowercased parsing models, and skip tokeni-
sation and casing steps in the PSMT system. We try
both the full data and 50% data parsing models.
Our next experiments use untokenised and cased
text from the Workshop on Statistical Machine
Translation. To remain close to CKK, we use data
from the 2009 Workshop,2 which provided Europarl
sets for both training and development. We use
europarl-v4 for training, test2007 for tun-
ing, and test2008 for testing.
We also run the 3-gram systems of this set with
each of the reduced parser models.
Our final experiments start to bridge the gap to
HD. We still train on europarl-v4 (diverging
from HD), but substitute one or both of the tuning
and test sets with those of HD: news-test2008
and newstest2009 from the 2010 Workshop.3
For the language model, HD uses both Europarl
and news text. To remain close to CKK, we train our
language models only on the Europarl training data,
and thus use considerably less data than HD here.
4.3 Evaluation
All systems are evaluated using case-insensitive
BLEU (Papineni et al, 2002). HD used the NIST
BLEU scorer, which requires SGML format. The
CKK data is plain text, so instead we report scores
2http://www.statmt.org/wmt09/translation-task.html
3http://www.statmt.org/wmt10/translation-task.html
386
LM DM T Base. Reord. Diff. Oracle
3 dist ? 26.53 27.34 +0.81 28.93
E 27.58 28.65 +1.07 30.31
N 26.99 27.16 +0.17 29.37
lex ? 27.35 27.88 +0.53 29.55
E 28.34 28.76 +0.42 30.79
N 27.77 28.27 +0.50 30.10
5 dist ? 27.23 28.12 +0.89 29.69
E 28.28 28.94 +0.66 30.81
N 27.42 28.38 +0.96 30.08
lex ? 28.24 28.70 +0.46 30.47
E 28.81 29.14 +0.33 31.24
N 28.32 28.59 +0.27 30.69
Table 4: BLEU scores for each experiment on Europarl
test set. Columns give: language model order, distortion
model (distance, lexicalised), tuning data (none (?), Eu-
roparl, News), baseline BLEU score, reordered system
BLEU score, performance increase, oracle BLEU score.
from the Moses multi-reference BLEU script (multi-
bleu), using one reference translation. Comparing
the scripts, we found that the NIST scores are always
lower than multi-bleu?s on test2008, but higher
on newstest2009, with differences at most 0.23.
This partially indicates the noise level in the scores.
5 Results
Results for the first experiments, closely replicat-
ing CKK, are given in Table 3. The results are very
similar to the those CKK reported (baseline 25.2, re-
ordered 26.8). Thus the HD reimplementation is in-
deed close to the original CKK system. Any qualita-
tive differences in parser output not revealed by ?3,
in the implementation of the rules, or in the PSMT
system, are thus producing only a small effect.
Results for the remaining experiments are given in
Tables 4 and 5, which give results on the test2008
and newstest2009 test sets respectively, and Ta-
ble 6, which gives results on the test2008 test set
using the reduced parsing models.
We see that the choice of data can have a profound
effect, nullifying or even reversing the overall result,
even when the reordering system remains the same.
Genre differences are an obvious possibility, but we
have demonstrated only a dependence on data set.
The other factors tested?language model order,
lexicalisation of the distortion model, and use of a
tuning phase?can all affect the overall performance
LM DM T Base. Reord. Diff. Oracle
3 dist ? 16.28 15.96 -0.32 17.12
E 16.43 16.39 -0.04 17.92
N 17.25 16.51 -0.74 18.40
lex ? 16.81 16.34 -0.47 17.82
E 16.75 16.35 -0.40 18.19
N 17.75 17.02 -0.73 18.73
5 dist ? 16.44 15.97 -0.47 17.28
E 16.21 15.89 -0.32 17.55
N 17.27 16.96 -0.31 18.21
lex ? 17.10 16.58 -0.52 18.16
E 17.03 17.04 +0.01 18.76
N 17.73 17.11 -0.62 19.01
Table 5: Results on news test set. Columns as for Table 4.
DM T % Base. Reord. Diff. Oracle
dist ? 50 26.53 27.26 +0.73 28.85
25 27.03 +0.50 28.66
10 27.01 +0.48 28.75
E 50 27.58 28.50 +0.92 30.19
25 28.27 +0.69 30.21
10 28.17 +0.59 30.18
lex ? 50 27.35 27.90 +0.55 29.52
25 27.62 +0.27 29.46
10 27.54 +0.19 29.42
E 50 28.34 28.56 +0.22 30.55
25 28.44 +0.10 30.46
10 28.42 +0.08 30.42
Table 6: Results using the smaller parsing models.
Columns are as for Table 4 except LM removed (all are
3-gram), and parser data percentage (%) added.
gain of the reordered system, but less distinctly. Re-
ducing the quality of the parsing model (by training
on less data) also has a negative effect, but the drop
must be substantial before it outweighs other factors.
In all cases, the oracle outperforms both baseline
and reordered systems by a large margin. Its selec-
tions show that, in changing test sets, the balance
shifts from one system to the other, but both still
contribute strongly. This shows that improvements
are possible across the board if it is possible to cor-
rectly choose which sentences will benefit from re-
ordering.
6 Conclusion
Collins et al (2005) reported that a reordering-
as-preprocessing approach improved overall perfor-
mance in German-to-English translation. The reim-
387
plementation of this system by Howlett and Dras
(2010) came to the opposite conclusion.
We have systematically varied several aspects of
the Howlett and Dras (2010) system and reproduced
results close to both papers, plus a full range in be-
tween. Our results show that choices in the PSMT
system can completely erode potential gains of the
reordering preprocessing step, with the largest effect
due to simple choice of data. We have shown that
a lack of overall improvement using reordering-as-
preprocessing need not be due to the usual suspects,
language pair and reordering process.
Significantly, our oracle experiments show that in
all cases the reordering system does produce better
translations for some sentences. We conclude that
effort is best directed at determining for which sen-
tences the improvement will appear.
Acknowledgements
Our thanks to Michael Collins for providing the data
used in Collins et al (2005), and to members of
the Centre for Language Technology and the anony-
mous reviewers for their helpful comments.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, pages 24?41.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 531?540.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103.
Nizar Habash. 2007. Syntactic preprocessing for statis-
tical machine translation. In Proceedings of the MT
Summit XI, pages 215?222.
Hieu Hoang and Philipp Koehn. 2008. Design of the
Moses decoder for statistical machine translation. In
Software Engineering, Testing, and Quality Assurance
for Natural Language Processing, pages 58?65.
Susan Howlett and Mark Dras. 2010. Dual-path phrase-
based statistical machine translation. In Proceedings
of the Australasian Language Technology Association
Workshop, pages 32?40.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Human Language Technology Confer-
ence and the North American Association for Compu-
tational Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311?318.
Slav Petrov and Dan Klein. 2008. Parsing German with
latent variable grammars. In Proceedings of the ACL-
08: HLT Workshop on Parsing German, pages 33?39.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of the Fifth
Conference on Applied Natural Language Processing,
pages 88?95.
Chao Wang, Michael Collins, and Philipp Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 737?745.
Fei Xia and Michael McCord. 2004. Improving a statisti-
cal MT system with automatically learned rewrite pat-
terns. In Proceedings of the 20th International Confer-
ence on Computational Linguistics, pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 245?253.
Simon Zwarts and Mark Dras. 2007. Syntax-based word
reordering in phrase-based statistical machine transla-
tion: Why does it work? In Proceedings of the MT
Summit XI, pages 559?566.
388
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 124?133,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
NLI Shared Task 2013: MQ Submission
Shervin Malmasi Sze-Meng Jojo Wong Mark Dras
Centre for Language Technology
Macquarie University
Sydney, Australia
{shervin.malmasi,sze.wong,mark.dras}@mq.edu.au
Abstract
Our submission for this NLI shared task used
for the most part standard features found in re-
cent work. Our focus was instead on two other
aspects of our system: at a high level, on pos-
sible ways of constructing ensembles of multi-
ple classifiers; and at a low level, on the gran-
ularity of part-of-speech tags used as features.
We found that the choice of ensemble com-
bination method did not lead to much differ-
ence in results, although exploiting the vary-
ing behaviours of linear versus logistic regres-
sion SVM classifiers could be promising in fu-
ture work; but part-of-speech tagsets showed
noticeable differences.
We also note that the overall architecture, with
its feature set and ensemble approach, had an
accuracy of 83.1% on the test set when trained
on both the training data and development data
supplied, close to the best result of the task.
This suggests that basically throwing together
all the features of previous work will achieve
roughly the state of the art.
1 Introduction
Among the efflorescence of work on Native Lan-
guage Identification (NLI) noted by the shared task
organisers, there are two trends in recent work in
particular that we considered in building our sub-
mission. The first is the proposal and use of new
features that might have relevance to NLI: for exam-
ple, Wong and Dras (2011), motivated by the Con-
trastive Analysis Hypothesis (Lado, 1957) from the
field of Second Language Acquisition, introduced
syntactic structure as a feature; Swanson and Char-
niak (2012) introduced more complex Tree Substi-
tution (TSG) structures, learned by Bayesian infer-
ence; and Bykh and Meurers (2012) used recurring
n-grams, inspired by the variation n-gram approach
to corpus error annotation detection (Dickinson and
Meurers, 2003). Starting from the features intro-
duced in these papers and others, then, other recent
papers have compiled a comprehensive collection of
features based on the earlier work ? Tetreault et
al. (2012) is an example, combining and analysing
most of the features used in previous work. Given
the timeframe of the shared task, there seemed to be
not much mileage in trying new features that were
likely to be more peripheral to the task.
A second trend, most apparent in 2012, was the
examination of other corpora besides the Interna-
tional Corpus of Learner English used in earlier
work, and in particular the use of cross-corpus evalu-
ation (Brooke and Hirst, 2012; Tetreault et al, 2012)
to avoid topic bias in determining native language.
Possible topic bias had been a reason for avoiding
a full range of n-grams, in particular those contain-
ing content words (Koppel et al, 2009); the devel-
opment of new corpora and the analysis of the effect
of topic bias mitigated this. The consequent use of a
full range of n-grams further reinforced the view that
novel features were unlikely to be a major source of
interesting results.
We therefore concentrated on two areas: the use
of classifier ensembles, and the choice of part-of-
speech tags. With classifier ensembles, Tetreault
et al (2012) noted that these were highly useful in
their system; but while that paper had extensive fea-
124
ture descriptions, it did not discuss in detail the ap-
proach to its ensembles. We therefore decided to
examine a range of possible ensemble architectures.
With part-of-speech tags, most work has used the
Penn Treebank tagset, including those based on syn-
tactic structure. Kochmar (2011) on the other hand
used the CLAWS tagset,1 which is much richer and
more oriented to linguistic analysis than the Penn
Treebank one. Given the much larger size of the
TOEFL11 corpus used for this shared task than the
corpora used for much earlier work, data sparsity
could be less of an issue, and the tagset a viable one
for future work.
The description of our submission is therefore in
three parts. In ?2 we present the system description,
with a focus on the ensemble architectures we inves-
tigated; in ?3 we list the features we used, which are
basically those of much of the previous work; in ?4
we present results of some of the variants we tried,
particularly with respect to ensembles and tagsets;
and in ?5 we discuss some of the interesting charac-
teristics of the data we noted during the shared task.
2 System Design
Our overall approach in terms of features and clas-
sifiers used is a fairly standard one. One difference
from most approaches, but inspired by Tetreault et
al. (2012), is that we train multiple classifiers over
subsets of the features, over different feature rep-
resentations, and over different regularisation ap-
proaches; we then combine them in ensembles (Di-
etterich, 2000).
2.1 SVM Ensemble Construction
To construct our ensemble, we train individual clas-
sifiers on a single feature type (e.g. PoS n-grams),
using a specific feature value representation and
classifier. We utilise a parallel ensemble structure
where the classifiers are run on the input texts in-
dependently and their results are then fused into the
final output using a combiner.
Additionally, we also experiment with bagging
(bootstrap aggregating), a commonly used method
for ensemble generation (Breiman, 1996) to gener-
ate multiple ensembles per feature type.
1http://ucrel.lancs.ac.uk/claws/
For our classifier, we use SVMs, specifically the
LIBLINEAR SVM software package (Fan et al,
2008),2 which is well-suited to text classification
tasks with large numbers of features and large num-
bers of documents. LIBLINEAR provides both lo-
gistic regression and linear SVMs; we experiment
with both. In general, the linear classifier performs
better, but it only provides the decision output. The
logistic regression classifier on the other hand gives
probability estimates, which are required by most
of our combination methods (?2.3). We therefore
mostly use the logistic regression classifiers.
2.2 L1- and L2-regularized SVM Classifiers
In our preliminary experiments we noted that
some feature types performed better with L1-
regularization and others with L2. In this work we
generate classifiers using both methods and evaluate
their individual and combined performance.
2.3 Classifier Combination Methods
We experiment with the following decision combi-
nation methods, which have been discussed in the
machine learning literature. Polikar (2006) provides
an exposition of these rules and methods.
Plurality vote: Each classifier votes for a single
class label, the label with the highest number of
votes wins. Ties are broken arbitrarily.
Sum: All probability estimates are added together
and the label with the highest sum is picked.
Average: The mean of all scores for each class
is calculated and the label with the highest average
probability is chosen.
Median: Each label?s estimates are sorted and the
median value is selected as the final score for that
label. The label with the highest value is picked.
Product: For each class label, all of the probabil-
ity estimates are multiplied together to create the la-
bel?s final estimate. The label with the highest esti-
mate is selected. A single low score can have a big
effect on the outcome.
Highest Confidence: In this simple method, the
class label that receives the vote with the largest de-
gree of confidence is selected as the final output.
2Available at http://www.csie.ntu.edu.tw/
?cjlin/liblinear/
125
Borda Count: The confidence estimates are con-
verted to ranks and the final label selected using the
Borda count algorithm (Ho et al, 1994). In this
combination approach, broadly speaking points are
assigned to ranks, and these tallied for the overall
weight.
With the exception of the plurality vote, all of
these can be weighted. In our ensembles we also ex-
periment with weighting the output of each classifier
using its individual accuracy on the training data as
an indication of our degree of confidence in it.
2.4 Feature Representation
Most NLI studies have used two types of feature rep-
resentations: binary (presence or absence of a fea-
ture in a text) and normalized frequencies. Although
binary feature values have been used in some stud-
ies (e.g. Wong and Dras (2011)), most have used
frequency-based values.
In the course of our experiments we have ob-
served that the effect of the feature representation
varies with the feature type, size of the feature space
and the learning algorithm itself. In our current sys-
tem, then, we generate two classifiers for each fea-
ture type, one trained with frequency-based values
(raw counts scaled using the L2-norm) and the other
with binary. Our experiments assess both their indi-
vidual and joint performance.
2.5 Proficiency-level Based Classification
To utilise the proficiency level information provided
in the TOEFL11 corpus (texts are marked as either
low, medium or high proficiency), we also investi-
gate classifiers that are trained using only texts from
specific proficiencies.
Tetreault et al (2012) established that the classi-
fication accuracy of their system varied across pro-
ficiency levels, with high proficiency texts being the
hardest to classify. This is most likely due to the fact
that writers at differing skill levels commit distinct
types of errors at different rates (Ortega, 2009, for
example). If learners of different backgrounds com-
mit these errors with different distributions, these
patterns could be used by a learner to further im-
prove classification accuracy. We will use these fea-
tures in one of our experiments to investigate the
effectiveness of such proficiency-level based classi-
fiers for NLI.
3 Features
We roughly divide out feature types into lexical,
part-of-speech and syntactic. In all of the feature
types below, we perform no feature selection.
3.1 Lexical Features
As all previous work, we use function words as fea-
tures. In addition, given the attempts to control for
topic bias in the TOEFL11 corpus, we also make
use of various lexical features which have been pre-
viously avoided by researchers due to the reported
topic bias (Brooke and Hirst, 2011) in other NLI cor-
pora such as the ICLE corpus.
Function Words In contrast to content words,
function words do not have any meaning themselves,
but rather can be seen as indicating the grammat-
ical relations between other words. Examples in-
clude articles, determiners, conjunctions and auxil-
iary verbs. They have been widely used in studies of
authorship attribution as well as NLI and established
to be informative for these tasks. We use the list
of 398 common English function words from Wong
and Dras (2011). We also tested smaller sets, but ob-
served that the larger sets achieve higher accuracy.
Function Word n-grams We devised and tested a
new feature that attempts to capture patterns of func-
tion word use at the sentence level. We define func-
tion word n-grams as a type of word n-gram where
content words are skipped: they are thus a specific
subtype of skip-gram discussed by Guthrie et al
(2006). For example, the sentence We should all
start taking the bus would be reduced to we should
all the, from which we would extract the n-grams.
Character n-grams Tsur and Rappoport (2007)
demonstrated that character n-grams are a useful
feature for NLI. These n-grams can be considered
as a sub-word feature and their effectiveness is hy-
pothesized to be a result of phoneme transfer from
the writer?s L1. They can also capture orthographic
conventions of a language. Accordingly, we limit
our n-grams to a maximum size of 3 as longer se-
quences would correspond to short words and not
phonemes or syllables.
Word n-grams There has been a shift towards the
use of word-based features in several recent studies
(Brooke and Hirst, 2012; Bykh and Meurers, 2012;
126
Tetreault et al, 2012), with new corpora come into
use for NLI and researchers exploring and address-
ing the issues relating to topic bias that previously
prevented their use. Lexical choice is considered to
be a prime feature for studying language transfer ef-
fects, and researchers have found word n-grams to
be one of the strongest features for NLI. Tetreault
et al (2012) expanded on this by integrating 5-gram
language models into their system. While we did not
replicate this, we made use of word trigrams.
3.2 POS n-grams
Most studies have found that POS tag n-grams are
a very useful feature for NLI (Koppel et al, 2005;
Bykh and Meurers, 2012, for example). The tagset
provided by the Penn TreeBank is the most widely
used in these experiments, with tagging performed
by the Stanford Tagger (Toutanova et al, 2003).
We investigate the effect of tagset granularity
on classification accuracy by comparing the clas-
sification accuracy of texts tagged with the PTB
tagset against those annotated by the RASP Tagger
(Briscoe et al, 2006). The PTB POS tagset contains
36 unique tags, while the RASP system uses a subset
of the CLAWS2 tagset, consisting of 150 tags.
This is a significant size difference and we hy-
pothesize that a larger tagset could provide richer
levels of syntactically meaningful info which is
more fine-grained in distinction between syntactic
categories and contains more morpho-syntactic in-
formation such as gender, number, person, case
and tense. For example, while the PTB tagset
has four tags for pronouns (PRP, PRP$, WP,
WP$), the CLAWS tagset provides over 20 pronoun
tags (PPHO1, PPIS1, PPX2, PPY, etc.) dis-
tinguishing between person, number and grammati-
cal role. Consequently, these tags could help better
capture error patterns to be used for classification.
3.3 Syntactic Features
Adaptor grammar collocations Drawing on
Wong et al (2012), we also utilise an adaptor gram-
mar to discover arbitrary lengths of n-gram collo-
cations for the TOEFL11 corpus. We explore both
the pure part-of-speech (POS) n-grams as well as
the more promising mixtures of POS and function
words. Following a similar experimental setup as
per Wong et al (2012), we derive two adaptor gram-
mars where each is associated with a different set of
vocabulary: either pure POS or the mixture of POS
and function words. We use the grammar proposed
by Johnson (2010) for capturing topical collocations
as presented below:
Sentence ? Docj j ? 1, . . . ,m
Docj ? j j ? 1, . . . ,m
Docj ? Docj Topici i ? 1, . . . , t;
j ? 1, . . . ,m
Topici ? Words i ? 1, . . . , t
Words ? Word
Words ? Words Word
Word ? w w ? Vpos;
w ? Vpos+fw
As per Wong et al (2012), Vpos contains 119
distinct POS tags based on the Brown tagset and
Vpos+fw is extended with 398 function words used
in Wong and Dras (2011). The number of topics t
is set to 50 (instead of 25 as per Wong et al (2012))
given that the TOEFL corpus is larger than the ICLE
corpus. The inference algorithm for the adaptor
grammars are based on the Markov Chain Monte
Carlo technique made available by Johnson (2010).3
Tree Subtitution Grammar fragments In rela-
tion to the context-free grammar (CFG) rules ex-
plored in the previous NLI work of Wong and Dras
(2011), Tree Substitution Grammar (TSG) frag-
ments have been proposed by Swanson and Char-
niak (2012) as another form of syntactic features
for NLI classification tasks. Here, as an approxi-
mation to deploying the Bayesian approach to in-
duce a TSG (Post and Gildea, 2009; Swanson and
Charniak, 2012), we first parse each of the essays in
the TOEFL training corpus with the Stanford Parser
(version 2.0.4) (Klein and Manning, 2003) to obtain
the parse trees. We then extract the TSG fragments
from the parse trees using the TSG system made
available by Post and Gildea (2009).4
Stanford dependencies In Tetreault et al (2012),
Stanford dependencies were investigated as yet an-
other form of syntactic features. We follow a
similar approach: for each essay in the train-
ing corpus, we extract all the basic (rather than
3http://web.science.mq.edu.au/?mjohnson/
Software.htm
4https://github.com/mjpost/dptsg
127
the collapsed) dependencies returned by the Stan-
ford Parser (de Marneffe et al, 2006). Simi-
larly, we generate all the variations for each of
the dependencies (grammatical relations) by sub-
stituting each lemma with its corresponding PoS
tag. For instance, a grammatical relation of
det(knowledge, the) yields the following
variations: det(NN, the), det(knowledge,
DT), and det(NN, DT).
4 Experiments and Results
We report our results using 10-fold cross-validation
on the combined training and development sets, as
well as by training a model using the training and
development data and running it on the test set.
We note that for our submission, we trained only
on the training data; the results here thus differ from
the official ones.
4.1 Individual Feature Results and Analysis
We ran the classifiers generated for each feature type
to assess their performance. The results are summa-
rized in Table 1: the Train + Dev Set results were for
the system when trained on the training and develop-
ment data with 10 fold cross-validation, and the Test
Set results for the system trained on the training and
development data combined.
Character n-grams are an informative feature and
our results are very similar to those reported by pre-
vious researchers (Tsur and Rappoport, 2007). In
particular, it should be noted that the use of punc-
tuation is a very powerful feature for distinguishing
languages. Romance language speakers were most
likely to use more punctuation symbols (colons,
semicolons, ellipsis, parenthesis, etc.) and at higher
rates. Chinese, Japanese and Korean speakers were
far less likely to use punctuation.
The performance for word n-grams, TSG frag-
ments and Stanford Dependencies is very strong and
comparable to previously reported research. For the
adaptor grammar n-grams, the mixed POS/function
word version yielded best results and was included
in the ensemble.
4.2 POS-based Classification and Tagset Size
To compare the tagsets we trained individual classi-
fiers for n-grams of size 1?4 using both tagsets and
tested them. The results are shown in Table 2 and
Feature Train +
Dev Set
Test Set
Chance Baseline 9.1 9.1
Character unigram 33.99 34.70
Character bigram 51.64 49.80
Character trigram 66.43 66.70
RASP POS unigram 43.76 45.10
RASP POS bigram 58.93 61.60
RASP POS trigram 59.39 62.70
Function word unigram 51.38 54.00
Function word bigram 59.73 63.00
Word unigram 74.61 75.50
Word bigram 74.46 76.00
Word trigram 63.60 65.00
TSG Fragments 72.16 72.70
Stanford Dependencies 73.78 75.90
Adaptor Grammar
POS/FW n-grams
69.76 70.00
Table 1: Classification results for our individual features.
N PTB RASP
1 34.03 43.76
2 48.85 58.93
3 51.06 59.39
4 49.85 52.81
Table 2: Classification accuracy results for POS n-grams
of size N using both the PTB and RASP tagset. The larger
RASP tagset performed significantly better for all N.
N Accuracy
1 51.38
2 59.73
3 52.14
Table 3: Classification results for Function Word n-grams
of size N. Our proposed Function Word bigram and tri-
gram features outperform the commonly used unigrams.
128
Ensemble Train +
Dev Set
Test Set
Complete Ensemble 81.50 81.60
Only binary values 82.46 83.10
Only freq values 65.28 67.20
L1-regularized solver only 80.33 81.10
L2-regularized solver only 81.42 81.10
Bin, L1-regularized only 81.57 82.00
Bin, L2-regularized only 82.00 82.50
Table 4: Classification results for our ensembles, best re-
sult in column in bold (binary values with L1- and L2-
regularized solvers).
show that the RASP tagged data provided better per-
formance in all cases. While it is possible that these
differences could be attributed to other factors such
as tagging accuracy, we do not believe this to be the
case as the Stanford Tagger is known for its high ac-
curacy (97%). These differences are quite clear; this
finding also has implications for other syntactic fea-
tures that make use of POS tags, such as Adaptor
Grammars, Stanford Dependencies and Tree Substi-
tution Grammars.
4.3 Function Word n-grams
The classification results using our proposed Func-
tion Word n-gram feature are shown in Table 3.
They show that function word skip-grams are more
informative than the simple function word counts
that have been previously used.
4.4 Ensemble Results
Table 4 shows the results from our ensembles. The
feature types included in the ensemble are those
whose results are listed individually in Table 1. (So,
for example, we only use the RASP-tagged PoS n-
grams, not the Penn Treebank ones.) The complete
ensemble consists of four classifiers per feature type:
L1-/L2-regularized versions with both binary and
freq. values.
Bagging Our experiments with bagging did not
find any improvements in accuracy, even with larger
numbers of bootstrap samples (50 or more). Bag-
ging is said to be more suitable for unstable clas-
sifiers which have greater variability in their perfor-
mance and are more susceptible to noise in the train-
ing data (Breiman, 1996). In our experiments with
individual feature types we have found the classi-
fiers to be quite stable in their performance, across
different folds and training set sizes. This is one po-
tential reason why bagging did not yield significant
improvements.
Combiner Methods Of the methods outlined in
?2.3 we found the sum and weighted sum combiners
to be the best performing, but the weighted results
did not improve accuracy in general over their un-
weighted counterparts. Our results are reported us-
ing the unweighted sum combiner. A detailed com-
parison of the results for the combiners has been
omitted here due to time constraints; the differences
across all combination methods was roughly 1?2%.
Any new approach to ensemble combination meth-
ods would consequently want to be radically differ-
ent to expect a notable improvement in performance.
As noted at the start of this section, results here
are for the system trained on training and develop-
ment data. The best result on the test set (83.1%)
is almost 4% higher than our submission result, and
close to the highest result achieved (83.6%).
Binary & Frequency-Based Feature Values Our
results are consistent with those of Brooke and Hirst
(2012), who conclude that there is a preference
for binary feature values instead of frequency-based
ones. Including both types in the ensemble did not
improve results.
However, in other experiments on the TOEFL11
corpus we have also observed that use of frequency
information often leads to significantly better results
when using a linear SVM classifier: in fact, the lin-
ear classifier is better on all frequency feature types,
and also on some of the binary feature types. We
present results in Table 5 comparing the two. An ap-
proach using the linear SVM that provides an asso-
ciated probability score ? perhaps through bagging
? allowing it to be combined with the methods de-
scribed in ?2.3 could then perhaps boost results. All
these results were from a system using the training
data with 10 fold cross-validation.
Combining Regularisation Approaches Results
show that combining the L1- and L2-regularized
classifiers in the ensemble provided a small in-
129
Feature L2-norm scaled counts Binary
linear log. regr. linear log. regr.
Char unigram 31.60 26.23 25.68 26.36
Char bigram 51.59 41.81 41.20 45.11
Char trigram 65.78 54.97 58.30 61.76
RASP POS bigram 60.38 54.00 50.31 54.56
RASP POS trigram 58.75 53.92 55.93 58.58
Function word unigram 51.38 45.09 46.67 47.13
Function word bigram 58.95 53.22 54.97 58.53
Word unigram 70.33 55.60 69.40 72.00
Word bigram 73.90 54.25 73.65 74.93
Word trigram 63.78 52.46 64.78 64.94
Table 5: Classification results for our individual features.
crease in accuracy. Ensembles with either the L1 or
L2-regularized solver have lower accuracy than the
combined methods (row 2).
4.5 Proficiency-level Based Classification
Table 6 shows our results for training models with
texts of a given proficiency level and the accuracy on
the test set. The numbers show that in general texts
should be classified with a learner trained with texts
of a similar proficiency. They also show that not all
texts in a proficiency level are of uniform quality as
some levels perform better with data from the clos-
est neighbouring levels (e.g. Medium texts perform
best with data from all proficiencies), suggesting
that the three levels form a larger proficiency con-
tinuum where users may fall in the higher or lower
ends of a level. A larger scale with more than three
levels could help address this.
5 Discussion
5.1 Unused Experimental Features
We also experimented with some other feature types
that were not included in the final system.
CCG SuperTag n-grams In order to introduce
additional rich syntactic information into our sys-
tem, we investigated the use CCG SuperTags as fea-
ture for NLI classification. We used the C&C CCG
Train Test Acc. Train Test Acc.
Low Low 52.2 All Med 86.8
Med Low 72.1 M + H Med 85.3
High Low 40.3 L + M Med 83.8
All Low 75.2 Low High 16.1
L + M Low 76.0 Med High 68.1
Low Med 40.7 High High 65.7
Med Med 83.6 M + H High 74.7
High Med 62.1 All High 75.2
Table 6: Results for classifying the test set documents
using classifiers trained with a specific proficiency level.
Each level?s best result in bold.
Parser and SuperTagger (Curran et al, 2007) to ex-
tract SuperTag n-grams from the corpus, which were
then used as features to construct classifiers. The
best results were achieved by using n-grams of size
2?4, which achieved classification rates of around
44%. However, adding these features to our ensem-
ble did not improve the overall system accuracy. We
believe that this is because when coupled with the
other syntactic features in the system, the informa-
tion provided by the SuperTags is redundant, and
thus they were excluded from our final ensemble.
Hapax Legomena and Dis Legomena The spe-
cial word categories Hapax Legomena and Dis
legomena refer to words that appear only once and
130
twice, respectively, in a complete text. In practice,
these features are a subset of our Word Unigram
feature, where Hapax Legomena correspond to un-
igrams with an occurrence count of 1 and Hapax dis
legomena are unigrams with a count of 2.
In our experimental results we found that Ha-
pax Legomena alone provides an accuracy of 61%.
Combining the two features together yields an accu-
racy of 67%. This is an interesting finding as both
of these features alone provide an accuracy close to
the whole set of word unigrams.
5.2 Corpus Representativeness
We conducted a brief analysis of our extracted fea-
tures, looking at the most predictive ones according
to their Information Gain. Although we did not find
any obvious indicators of topic bias, we noted some
other issues of potential concern.
Chinese, Japanese and Korean speakers make ex-
cessive use of phrases such as However, First of all
and Secondly. At first glance, the usage rate of these
phrases seems unnaturally high (more than 50% of
Korean texts had a sentence beginning with How-
ever). This could perhaps be a cohort effect relat-
ing to those individually attempting this particular
TOEFL exam, rather than an L1 effect: it would
be useful to know how much variability there is in
terms of where candidates come from.
It was also noticed that many writers mention the
name of their country in their texts, and this could
potentially create a high correlation between those
words and the language class label, leading perhaps
to an artificial boosting of results. For example, the
words India, Turkey, Japan, Korea and Germany ap-
pear with high frequency in the texts of their corre-
sponding L1 speakers ? hundreds of times, in fact,
in contrast to frequencies in the single figures for
speakers of other L1s. These might also be an arte-
fact of the type of text, rather than related to the L1
as such.
5.3 Hindi vs. Telugu
We single out here this language pair because of
the high level of confusion between the two classes.
Looking at the results obtained by other teams, we
observe that this language pair provided the worst
classification accuracy for almost all teams. No
system was able to achieve an accuracy of 80%
for Hindi (something many achieved for other lan-
guages). In analysing the actual and predicted
classes for all documents classified as Hindi and
Telugu by our system, we find that generally all
of the actual Hindi and Telugu texts (96% and
99%, respectively) are within the set. Our classifier
is clearly having difficulty discriminating between
these two specific classes.
Given this, we posit that the confounding influ-
ence may have more to do with the particular style
of English that is spoken and taught within the
country, rather than the specific L1 itself. Consult-
ing other research about SLA differences in multi-
lingual countries could shed further light on this.
Analysing highly informative features provides
some clues about the influence of a common cul-
ture or national identity: in our classifier, the words
India, Indian and Hindu were highly predictive of
both Hindi and Telugu texts, but no other lan-
guages. In addition, there were terms that were
not geographically- or culturally-specific that were
strongly associated with both Hindi and Telugu:
these included hence, thus, and etc, and a much
higher rate of use of male pronouns. It has been
observed in a number of places (Sanyal, 2007, for
example) that the English spoken across India still
retains characteristics of the English that was spo-
ken during the time of the Raj and the East India
Company that have disappeared from other varities
of English, so that it can sound more formal to other
speakers, or retain traces of an archaic business cor-
respondence style; the features just noted would fit
that pattern. The effect is likely to occur regardless
of the L1.
Looking at individual language pairs in this way
could lead to incremental improvement in the overall
classification accuracy of NLI systems.
References
Leo Breiman. 1996. Bagging predictors. In Machine
Learning, pages 123?140.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In Proceedings
of the COLING/ACL on Interactive presentation ses-
sions, COLING-ACL ?06, pages 77?80, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
131
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Julian Brooke and Graeme Hirst. 2012. Robust, Lexical-
ized Native Language Identification. In Proceedings
of COLING 2012, pages 391?408, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Serhiy Bykh and Detmar Meurers. 2012. Native Lan-
guage Identification using Recurring n-grams ? In-
vestigating Abstraction and Domain Dependence. In
Proceedings of COLING 2012, pages 425?440, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c and
boxer. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 33?36, Prague, Czech Republic, June.
Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC?06), pages
449?454, Genoa, Italy.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-03), pages 107?114, Budapest, Hungary.
Thomas G Dietterich. 2000. Ensemble methods in ma-
chine learning. In Multiple classifier systems, pages
1?15. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,
and Yorick Wilks. 2006. A Close Look at Skip-gram
Modelling. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC 2006), pages 1222?1225, Genoa, Italy.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier
systems. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 16(1):66?75.
Mark Johnson. 2010. Pcfgs, topic models, adaptor gram-
mars and learning topical collocations and the struc-
ture of proper names. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1148?1157, Uppsala, Sweden, July.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Sap-
poro, Japan. Association for Computational Linguis-
tics.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational Methods in Authorship Attribu-
tion. Journal of the American Society for Information
Science and Technology, 60(1):9?26.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. University
of Michigan Press, Ann Arbor, MI, US.
Lourdes Ortega. 2009. Understanding Second Language
Acquisition. Hodder Education, Oxford, UK.
Robi Polikar. 2006. Ensemble based systems in deci-
sion making. Circuits and Systems Magazine, IEEE,
6(3):21?45.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 45?48, Suntec, Singapore. As-
sociation for Computational Linguistics.
Jyoti Sanyal. 2007. Indlish: The Book for Every English-
Speaking Indian. Viva Books Private Limited.
Benjamin Swanson and Eugene Charniak. 2012. Na-
tive Language Detection with Tree Substitution Gram-
mars. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 193?197, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In IN PRO-
CEEDINGS OF HLT-NAACL, pages 252?259.
Oren Tsur and Ari Rappoport. 2007. Using Classifier
Features for Studying the Effect of Native Language
on the Choice of Written Second Language Words.
In Proceedings of the Workshop on Cognitive Aspects
of Computational Language Acquisition, pages 9?16,
132
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
133
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 1?11,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Working with a small dataset - semi-supervised dependency parsing for Irish
Teresa Lynn1,2, Jennifer Foster1, Mark Dras2 and Josef van Genabith1
1NCLT/CNGL, Dublin City University, Ireland
2Department of Computing, Macquarie University, Sydney, Australia
1{tlynn,jfoster,josef}@computing.dcu.ie
2{teresa.lynn,mark.dras}@mq.edu.au,
Abstract
We present a number of semi-supervised pars-
ing experiments on the Irish language carried
out using a small seed set of manually parsed
trees and a larger, yet still relatively small, set
of unlabelled sentences. We take two pop-
ular dependency parsers ? one graph-based
and one transition-based ? and compare re-
sults for both. Results show that using semi-
supervised learning in the form of self-training
and co-training yields only very modest im-
provements in parsing accuracy. We also try
to use morphological information in a targeted
way and fail to see any improvements.
1 Introduction
Developing a data-driven statistical parser relies on
the availability of a parsed corpus for the language
in question. In the case of Irish, the only parsed
corpus available to date is a dependency treebank,
which is currently under development and still rel-
atively small, with only 803 gold-annotated trees
(Lynn et al, 2012a). As treebank development is
a labour- and time-intensive process, in this study
we evaluate various approaches to bootstrapping a
statistical parser with a set of unlabelled sentences
to ascertain how accurate parsing output can be
at this time. We carry out a number of differ-
ent semi-supervised bootstrapping experiments us-
ing self-training, co-training and sample-selection-
based co-training. Our studies differ from previous
similar experiments as our data is taken from a work-
in-progress treebank. Thus, aside from the current
small treebank which is used for training the initial
seed model and for testing, there is no additional
gold-labelled data available to us to directly com-
pare supervised and semi-supervised approaches us-
ing training sets of comparable sizes.
In the last decade, data-driven dependency pars-
ing has come to fore, with two main approaches
dominating ? transition-based and graph-based. In
classic transition-based dependency parsing, the
training phase consists of learning the correct parser
action to take given the input string and the parse
history, and the parsing phase consists of the greedy
application of parser actions as dictated by the
learned model. In contrast, graph-based depen-
dency parsing involves the non-deterministic con-
struction of a parse tree by predicting the maximum-
spanning-tree in the digraph for the input sentence.
In our study, we employ Malt (Nivre et al, 2006),
a transition-based dependency parsing system, and
Mate (Bohnet, 2010), a graph-based parser.
In line with similar experiments carried out on
English (Steedman et al, 2003), we find that co-
training is more effective than self-training. Co-
training Malt on the output of Mate proves to be the
most effective method for improving Malt?s perfor-
mance on the limited data available for Irish. Yet, the
improvement is relatively small (0.6% over the base-
line for LAS, 0.3% for UAS) for the best co-trained
model. The best Mate results are achieved through a
non-iterative agreement-based co-training approach,
in which Mate is trained on trees produced by Malt
which exhibit a minimum agreement of 85% with
Mate (LAS increase of 1.2% and UAS of 1.4%).
The semi-supervised parsing experiments do not
explicitly take into account the morphosyntactic
properties of the Irish language. In order to examine
the effect of this type of information during parsing,
we carry out some orthogonal experiments where we
1
reduce word forms to lemmas and introduce mor-
phological features in certain cases. These changes
do not bring about an increase in parsing accuracy.
The paper is organised as follows. Section 2 is
an overview of Irish morphology. In Section 3 our
previous work carried out on the development of an
Irish dependency treebank is discussed followed in
Section 4 by a description of some of our prior pars-
ing results. Section 5 describes the self-training, co-
training and sample-selection-based co-training ex-
periments, Section 6 presents the preliminary pars-
ing experiments involving morphological features,
and, finally, Section 7 discusses our future work.
2 Irish as a morphologically rich language
Irish is a Celtic language of the Indo-European lan-
guage family. It has a VSO word order and is rich in
morphology. The following provides an overview of
the type of morphology present in the Irish language.
It is not a comprehensive summary as the rules gov-
erning morphological changes are too extensive and
at times too complex to document here.
Inflection in Irish mainly occurs through suffixa-
tion, but initial mutation through lenition and eclip-
sis is also common (Christian-Brothers, 1988). A
prominent feature of Irish (also of Scottish and
Manx), which influences inflection, is the existence
of two sets of consonants, referred to as ?broad? and
?slender? consonants (O? Siadhail, 1989). Conso-
nants can be slenderised by accompanying the con-
sonant with a slender vowel, either e or i. Broaden-
ing occurs through the use of broad vowels; a, o or
u. For example, buail ?to hit? becomes ag bualadh
?hitting? in the verbal noun form. In general, there
needs to be vowel harmony (slender or broad) be-
tween stem endings and the initial vowel in a suffix.
A process known as syncopation also occurs
when words with more than one syllable have a
vowel-initial suffix added. For example imir ?to
play? inflects as imr??m ?I play?.
Nouns While Old Irish employed several gram-
matical cases, Modern Irish uses only three: Nomi-
native, Genitive and Vocative. The nominative form
is sometimes regarded as the ?common form? as it is
now also used to account for accusative and dative
forms. Nouns in Irish are divided into five classes, or
declensions, depending on the manner in which the
genitive case is formed. In addition, there are two
grammatical genders in Irish - masculine and fem-
inine. Case, declension and gender are expressed
through noun inflection. For example, pa?ipe?ar ?pa-
per? is a masculine noun in the first declension. Both
lenition and slenderisation are used to form the geni-
tive singular form: pha?ipe?ir. In addition, possessive
adjectives cause noun inflection through lenition,
eclipsis and prefixation. For example, teach ?house?,
mo theach ?my house?, a?r dteach ?our house?; ainm
?name?, a hainm ?her name?.
Verbs Verbs can incorporate their subject, inflect-
ing for person and number through suffixation. Such
forms are referred to as synthetic verb forms. In
addition, verb tense is often indicated through var-
ious combinations of initial mutation, syncopation
and suffixation. For example, scr??obh ?write? can in-
flect as scr??obhaim ?I write?. The past tense of the
verb tug ?give? is thug ?gave?. Lenition occurs af-
ter the negative particle n??. For example, tugaim ?I
give?; n?? thugaim ?I do not give?; n??or thug me? ?I
did not give?. Eclipsis occurs following clitics such
as interrogative particles (an, nach); complementis-
ers (go, nach); and relativisers (a, nach) (Stenson,
1981). For example, an dtugann se?? ?does he give??;
nach dtugann se?? ?does he not give??.
Adjectives In general, adjectives follow nouns and
agree in number, gender and case. Depending on
the noun they modify, adjectives can also inflect.
Christian-Brothers (1988) note eight declensions of
adjectives. They can decline for genitive singular
masculine, genitive singular feminine and nomina-
tive plural. For example, bacach ?lame? inflects as
bacaigh (Gen.Sg.Masc), baca?? (Gen.Fem.Sg) and
bacacha (Nom.PL). Comparative adjectives are also
formed through inflection. For example, la?idir
?strong?, n??os la?idre ?stronger?; de?anach ?late?, is
de?ana?? ?latest?.
Prepositions Irish has simple and compound
prepositions. Most of the simple prepositions can
inflect for person and number (known as preposi-
tional pronouns or pronominal prepositions), thus
including a nominal element. For example, com-
pare bh?? se? ag labhairt le fear ?he was speaking
with a man? with bh?? se? ag labhairt leis ?he was
speaking with him?. These forms are used quite fre-
2
quently, not only with regular prepositional attach-
ment where pronominal prepositions operate as ar-
guments of verbs or modifiers of nouns and verbs,
but also in idiomatic use where they express emo-
tions and states, e.g. ta? bro?n orm (lit. ?be-worry-
on me?) ?I am worried? or ta? su?il agam (lit. ?be-
expectation-with me?) ?I hope?. Noted by Greene
(1966) as a noun-centered language, nouns are of-
ten used to convey the meaning that verbs often
would. Pronominal prepositions are often used in
these types of structures. For example, bhain me?
geit aisti (lit. extracted-I-shock-from her) ?I fright-
ened her?; bhain me? mo cho?ta d??om (lit. extracted-I-
coat-from me) ?I took off my coat?; bhain me? u?sa?id
as (lit. extracted-I-use-from it) ?I used it?; bhain
me? triail astu (lit. extracted-I-attempt-from them)?I
tried them?.
Derivational morphology There are also some
instances of derivational morphology in Irish. U??
Dhonnchadha (2009) notes that all verb stems and
agentive nouns can inflect to become verbal nouns.
Verbal adjectives are also derived from verb stems
through suffixation. For example, the verb du?n
?close? undergoes suffixation to become du?nadh
?closing? (verbal noun) and du?nta ?closed? (verbal
adjective). An emphatic suffix -sa/-se (both broad
and slender form) can attach to nouns or pronouns.
It can also be attached to any verb that has been in-
flected for person and number and also to pronom-
inal prepositions. For example mo thuairim ?my
opinion??mo thuairimse ?my opinion; tu? ?you?(sg)
? tusa ?you?; cloisim ?I hear?? cloisimse ?I hear?;
liom ?with me?? liomsa ?with me?. In addition, the
diminutive suffix -??n can attach to all nouns to form
a derived diminutive form. The rules of slenderisa-
tion apply here also. For example, buachaill ?boy?
becomes buachaill??n ?little boy?, and tamall ?while?
becomes tamaill??n ?short while?.
3 The Irish Dependency Treebank
Irish is the official language of Ireland, yet English
is the primary language for everyday use. Irish is
therefore considered an EU minority language and
is lacking in linguistic resources that can be used to
develop NLP applications (Judge et al, 2012).
Recently, in efforts to address this issue, we have
begun work on the development of a dependency
treebank for Irish (Lynn et al, 2012a). The treebank
has been built upon a gold standard 3,000 sentence
POS-tagged corpus1 developed by U?? Dhonnchadha
(2009). Our labelling scheme is based on an ?LFG-
inspired? dependency scheme developed for English
by C?etinog?lu et al (2010). This scheme was adopted
with the aim of identifying functional roles while
at the same time circumventing outstanding, unre-
solved issues in Irish theoretical syntax.2 The Irish
labelling scheme has 47 dependency labels in the la-
bel set. The treebank is in the CoNLL format with
the following fields: ID, FORM, LEMMA, CPOSTAG,
POSTAG, HEAD and DEPREL. The coarse-grained
part of speech of a word is marked by the la-
bel CPOSTAG, and POSTAG marks the fine-grained
part of speech for that word. For example, prepo-
sitions are tagged with the CPOSTAG Prep and
one of the following POSTAGs: Simple: ar ?on?,
Compound: i ndiaidh ?after?, Possessive: ina
?in its?, Article: sa ?in the?.
At an earlier stage of the treebank?s develop-
ment, we carried out on an inter-annotator agree-
ment (IAA) study. The study involved four stages.
(i) The first experiment (IAA-1) involved the as-
sessment of annotator agreement following the in-
troduction of a second annotator. The results re-
ported a Kappa score of 0.79, LAS of 74.4% and
UAS of 85.2% (Lynn et al, 2012a). (ii) We then
held three workshops that involved thorough anal-
ysis of the output of IAA-1, highlighting disagree-
ments between annotators, gaps in the annotation
guide, shortcomings of the labelling scheme and lin-
guistic issues not yet addressed. (iii) The annotation
guide, labelling scheme and treebank were updated
accordingly, addressing the highlighted issues. (iv)
Finally, a second inter-annotator agreement exper-
iment (IAA-2) was carried out presenting a Kappa
score of 0.85, LAS of 79.2% and UAS of 87.8%
(Lynn et al, 2012b).
We found that the IAA study was valuable in the
development of the treebank, as it resulted in im-
1A tagged, randomised subset of the NCII, (New Corpus for
Ireland - Irish http://corpas.focloir.ie/), comprised of text from
books, news data, websites, periodicals, official and government
documents.
2For example there are disagreements over the existence of
a VP in Irish and whether the language has a VSO or an under-
lying SVO structure.
3
provement of the quality of the labelling scheme,
the annotation guide and the linguistic analysis of
the Irish language. Our updated labelling scheme
is now hierarchical, allowing for a choice between
working with fine-grained or coarse-grained labels.
The scheme has now been finalised. A full list of
the labels can be found in Lynn et al (2012b). The
treebank currently contains 803 gold-standard trees.
4 Preliminary Parsing Experiments
In our previous work (Lynn et al, 2012a), we car-
ried out some preliminary parsing experiments with
MaltParser and 10-fold cross-validation using 300
gold-standard trees. We started out with the fea-
ture template used by C?etinog?lu et al (2010) and ex-
amined the effect of omitting LEMMA, WORDFORM,
POSTAG and CPOSTAG features and combinations
of these, concluding that it was best to include all
four types of information. Our final LAS and UAS
scores were 63.3% and 73.1% respectively. Follow-
ing the changes we made to the labelling scheme
as a result of the second IAA study (described
above), we re-ran the same parsing experiments on
the newly updated seed set of 300 sentences - the
LAS increased to 66.5% and the UAS to 76.3%
(Lynn et al, 2012b).
In order to speed up the treebank creation, we also
applied an active learning approach to bootstrapping
the annotation process. This work is also reported in
Lynn et al (2012b). The process involved training a
MaltParser model on a small subset of the treebank
data, and iteratively, parsing a new set of sentences,
selecting a 50-sentence subset to hand-correct, and
adding these new gold sentences to the training set.
We compared a passive setup, in which the parses
that were selected for correction were chosen at ran-
dom, to an active setup, in which the parses that
were selected for correction were chosen based on
the level of disagreement between two parsers (Malt
and Mate). The active approach to annotation re-
sulted in superior parsing results to the passive ap-
proach (67.2% versus 68.1% LAS) but the differ-
ence was not statistically significant.
5 Semi-Supervised Parsing Experiments
In order to alleviate data sparsity issues brought
about by our lack of training material, we experi-
ment with automatically expanding our training set
using well known semi-supervised techniques.
5.1 Self-Training
5.1.1 Related Work
Self-training, the process of training a system on
its own output, has a long and chequered history in
parsing. Early experiments by Charniak (1997) con-
cluded that self-training is ineffective because mis-
takes made by the parser are magnified rather than
smoothed during the self-training process. The self-
training experiments of Steedman et al (2003) also
yielded disappointing results. Reichart and Rap-
paport (2007) found, on the other hand, that self-
training could be effective if the seed training set
was very small. McClosky et al (2006) also re-
port positive results from self-training, but the self-
training protocol that they use cannot be considered
to be pure self-training as the first-stage Charniak
parser (Charniak, 2000) is retrained on the output of
the two-stage parser (Charniak and Johnson, 2005)
They later show that the extra information brought
by the discriminative reranking phase is a factor
in the success of their procedure (McClosky et al,
2008). Sagae (2010) reports positive self-training re-
sults even without the reranking phase in a domain
adaptation scenario, as do Huang and Harper (2009)
who employ self-training with a PCFG-LA parser.
5.1.2 Experimental Setup
The labelled data available to us for this experi-
ment comprises the 803 gold standard trees referred
to in Section 3. This small treebank includes the
150-tree development set and 150-tree test set used
in experiments by Lynn et al (2012b). We use the
same development and test sets for this study. As
for the remaining 503 trees, we remove any trees
that have more than 200 tokens. The motivation for
this is two-fold: (i) we had difficulties training Mate
parser with long sentences due to memory resource
issues, and (ii) in keeping with the findings of Lynn
et al (2012b), the large trees were sentences from
legislative text that were difficult to analyse for au-
tomatic parsers and human annotators. This leaves
us with 500 gold-standard trees as our seed training
data set.
For our unlabelled data, we take the next 1945
sentences from the gold standard 3,000-sentence
4
A is a parser.
M iA is a model of A at step i.
P iA is a set of trees produced using M
i
A.
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA is labelled training data for A at step i.
Initialise:
L0A ? L.
M0A ? Train(A,L
0
A)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
Li+1A ? L
i
A + P
i
A
M i+1A ? Train(A,L
i+1
A )
end for
Figure 1: Self-training algorithm
POS-tagged corpus referred to in Section 3. When
we remove sentences with more than 200 tokens, we
are left with 1938 sentences in our unlabelled set.
The main algorithm for self-training is given in
Figure 1. We carry out two separate experiments
using this algorithm. In the first experiment we use
Malt. In the second experiment, we substitute Mate
for Malt.3
The steps are as follows: Initialisation involves
training the parser on a labelled seed set of 500 gold
standard trees (L0A), resulting in a baseline parsing
model: M iA. We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1?6], we parse U i. Each time, the set
of newly parsed sentences (PA) is added to the train-
ing set LiA to make a larger training set of L
i+1
A . A
new parsing model (M i+1A ) is then induced by train-
ing with the new training set.
5.1.3 Results
The results of our self-training experiments are
presented in Figure 2. The best Malt model was
trained on 2115 trees, at the 5th iteration (70.2%
LAS). UAS scores did not increase over the baseline
(79.1%). The improvement in LAS over the baseline
is not statistically significant. The best Mate model
was trained on 1792 trees, at the 4th iteration (71.2%
3Versions used: Maltparser v1.7 (stacklazy parsing algo-
rithm); Mate tools v3.3 (graph-based parser)
Figure 2: Self-Training Results on the Development Set
LAS, 79.2% UAS). The improvement over the base-
line is not statistically significant.
5.2 Co-Training
5.2.1 Related Work
Co-training involves training a system on the out-
put of a different system. Co-training has found
more success in parsing than self-training, and it
is not difficult to see why this might be the case
as it can be viewed as a method for combining the
benefits of individual parsing systems. Steedman
et al (2003) directly compare co-training and self-
training and find that co-training outperforms self-
training. Sagae and Tsujii (2007) successfully em-
ploy co-training in the domain adaption track of the
CoNLL 2007 shared task on dependency parsing.
5.2.2 Experimental Setup
In this and all subsequent experiments, we use
both the same training data and unlabelled data that
we refer to in Section 5.1.2.
Our co-training algorithm is given in Figure 3 and
it is the same as the algorithm provided by Steedman
et al (2003). Again, our experiments are carried out
using Malt and Mate. This time, the experiments are
run concurrently as each parser is bootstrapped from
the other parser?s output.
5
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
Li+1A ? L
i
A + P
i
B
Li+1B ? L
i
B + P
i
A
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 3: Co-training algorithm
The steps are as follows: Initialisation involves
training both parsers on a labelled seed set of 500
gold standard trees (L0A and L
0
B), resulting in two
separate baseline parsing models: M iA (Malt) and
M iB (Mate). We divide the set of gold POS-tagged
sentences (U ) into 6 sets, each containing 323 sen-
tences U i. For each of the six iterations in this ex-
periment i = [1? 6], we use Malt and Mate to parse
U i. This time, the set of newly parsed sentences P iB
(Mate output) is added to the training set LiA to make
a larger training set of Li+1A (Malt training set). Con-
versely, the set of newly parsed sentences P iA (Malt
output) is added to the training set LiB to make a
larger training set of Li+1B (Mate training set). Two
new parsing models (M i+1A and M
i+1
B ) are then in-
duced by training Malt and Mate respectively with
their new training sets.
5.2.3 Results
The results of our co-training experiment are pre-
sented in Figure 4. The best Malt model was trained
on 2438 trees, at the final iteration (71.0% LAS
and 79.8% UAS). The improvement in UAS over
the baseline is statistically significant. Mate?s best
model was trained on 823 trees on the second iter-
ation (71.4% LAS and 79.9% UAS). The improve-
ment over the baseline is not statistically significant.
Figure 4: Co-Training Results on the Development Set
5.3 Sample-Selection-Based Co-Training
5.3.1 Related Work
Sample selection involves choosing training items
for use in a particular task based on some criteria
which approximates their accuracy in the absence of
a label or reference. In the context of parsing, Re-
hbein (2011) chooses additional sentences to add to
the parser?s training set based on their similarity to
the existing training set ? the idea here is that sen-
tences that are similar to training data are likely to
have been parsed properly and so are ?safe? to add
to the training set. In their parser co-training experi-
ments, Steedman et al (2003) sample training items
based on the confidence of the individual parsers (as
approximated by parse probability).
In Active Learning research, the Query By Com-
mittee selection method (Seung et al, 1992) is used
to choose items for annotation ? if a committee of
two or more systems disagrees on an item, this is ev-
idence that the item needs to be prioritised for man-
ual correction (see for example Lynn et al (2012b)).
Steedman et al (2003) discuss a sample selection
approach based on differences between parsers ? if
parser A and parser B disagree on an analysis, parser
A can be improved by being retrained on parser B?s
analysis, and vice versa. In contrast, Ravi et al
(2008) show that parser agreement is a strong in-
6
dicator of parse quality, and in parser domain adap-
tation, Sagae and Tsujii (2007) and Le Roux et al
(2012) use agreement between parsers to choose
which automatically parsed target domain items to
add to the training set.
Sample selection can be used with both self-
training and co-training. We restrict our attention
to co-training since our previous experiments have
demonstrated that it has more potential than self-
training. In the following set of experiments, we ex-
plore the role of both parser agreement and parser
disagreement in sample selection in co-training.
5.3.2 Agreement-Based Co-Training
Experimental Setup The main algorithm for
agreement-based co-training is given in Figure 5.
Again, Malt and Mate are used. However, this algo-
rithm differs from the co-training algorithm in Fig-
ure 3 in that rather than adding the full set of 323
newly parsed trees (P iA and P
i
B) to the training set
at each iteration, selected subsets of these trees (P iA?
and P iB?) are added instead. To define these subsets,
we identify the trees that have 85% or higher agree-
ment between the two parser output sets. As a re-
sult, the number of trees in the subsets differ at each
iteration. For iteration 1, 89 trees reach the agree-
ment threshold; iteration 2, 93 trees; iteration 3, 117
trees; iteration 4, 122 trees; iteration 5, 131 trees;
iteration 6, 114 trees. The number of trees in the
training sets is much smaller compared with those
in the experiments of Section 5.2.
Results The results for agreement-based co-
training are presented in Figure 6. Malt?s best
model was trained on 1166 trees at the final iteration
(71.0% LAS and 79.8% UAS). Mate?s best model
was trained on 1052 trees at the 5th iteration (71.5%
LAS and 79.7% UAS). Neither result represents a
statistically significant improvement over the base-
line.
5.3.3 Disagreement-based Co-Training
Experimental Setup This experiment uses the
same sample selection algorithm we used for
agreement-based co-training (Figure 5). For this ex-
periment, however, the way in which the subsets
of trees (P iA? and P
i
B?) are selected differs. This
time we choose the trees that have 70% or higher
disagreement between the two parser output sets.
A and B are two different parsers.
M iA and M
i
B are models of A and B at step i.
P iA and P
i
B are a sets of trees produced using M
i
A and M
i
B .
U is a set of sentences.
U i is a subset of U at step i.
L is the manually labelled seed training set.
LiA and L
i
B are labelled training data for A and B at step i.
Initialise:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
for i = 1? N do
U i ? Add set of unlabelled sentences from U .
P iA ? Parse(U
i , M iA)
P iB ? Parse(U
i , M iB)
P iA? ? a subset of X trees from P
i
A
P iB ? ? a subset of X trees from P
i
B
Li+1A ? L
i
A + P
i
B ?
Li+1B ? L
i
B + P
i
A?
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
end for
Figure 5: Sample selection Co-training algorithm
Again, the number of trees in the subsets differ at
each iteration. For iteration 1, 91 trees reach the dis-
agreement threshold; iteration 2, 93 trees; iteration
3, 73 trees; iteration 4, 74 trees; iteration 5, 68 trees;
iteration 6, 71 trees.
Results The results for our disagreement-based
co-training experiment are shown in Figure 7. The
best Malt model was trained with 831 trees at the
4th iteration (70.8% LAS and 79.8% UAS). Mate?s
best models were trained on (i) 684 trees on the 2nd
iteration (71.0% LAS) and (ii) 899 trees on the 5th
iteration (79.4% UAS). Neither improvement over
the baseline is statistically significant.
5.3.4 Non-Iterative Agreement-based
Co-Training
In this section, we explore what happens when
we add the additional training data at once rather
than over several iterations. Rather than testing this
idea with all our previous setups, we choose sample-
selection-based co-training where agreement be-
tween parsers is the criterion for selecting additional
training data.
Experimental Setup Again, we also follow the
algorithm for agreement-based co-training as pre-
sented in Figure 5. However, two different ap-
7
Figure 6: Agreement-based Co-Training Results on the
Development Set
proaches are taken this time, involving only one it-
eration in each. For the first experiment (ACT1a),
the subsets of trees (P iA? and P
i
B?) that are added to
the training data are chosen based on an agreement
threshold of 85% between parsers, and are taken
from the full set of unlabelled data (where U i = U ),
comprising 1938 trees. In this instance, the subset
consists of 603 trees, making a final training set of
1103 trees.
For the second experiment (ACT1b), only trees
meeting a parser agreement threshold of 100% are
added to the training data. 253 trees (P iA? and P
i
B?)
out of 1938 trees (U i = U ) meet this threshold. The
final training set consists of 753 trees.
Results ACT1a proved to be the most accurate
parsing model for Mate overall. The addition of
603 trees that met the agreement threshold of 85%
increased the LAS and UAS scores over the base-
line by 1.0% and 1.3% to 71.8 and 80.4 respec-
tively. This improvement is statistically significant.
Malt showed a LAS improvement of 0.93% and
a UAS improvement of 0.42% (71.0% LAS and
79.6% UAS). The LAS improvement over the base-
line is statistically significant.
The increases for ACT1b, where 100% agreement
trees are added, are less pronounced and are not sta-
Figure 7: Disagreement-based Co-Training Results on
the Development Set
tistically significant. Results showed a 0.5% LAS
and 0.2% UAS increase over the baseline with Malt,
based on the 100% agreement threshold (adding 235
trees). Mate performs at 0.5% above the LAS base-
line and 0.1% above the UAS baseline.
5.4 Analysis
We perform an error analysis for the Malt and Mate
baseline, self-trained and co-trained models on the
development set. We observe the following trends:
? All Malt and Mate parsing models confuse the
subj and obj labels. A few possible rea-
sons for this stand out: (i) It is difficult for
the parser to discriminate between analytic verb
forms and synthetic verb forms. For example,
in the phrase pho?sfainn thusa ?I would marry
you?, pho?sfainn is a synthetic form of the verb
po?s ?marry? that has been inflected with the in-
corporated pronoun ?I?. Not recognising this,
the parser decided that it is an intransitive verb,
taking ?thusa?, the emphatic form of the pro-
noun tu? ?you?, as its subject instead of object.
(ii) Possibly due to a VSO word order, when
the parser is dealing with relative phrases, it
can be difficult to ascertain whether the follow-
ing noun is the subject or object. For example,
an chail??n a chonaic me? inne? ?the girl whom
8
I saw yesterday/ the girl who saw me yester-
day?.4 (iii) There is no passive verb form in
Irish. The autonomous form is most closely
linked with passive use and is used when the
agent is not known or mentioned. A ?hidden?
or understood subject is incorporated into the
verbform. Casadh eochair i nglas ?a key was
turned in a lock? (lit. somebody turned a key
in a lock). In this sentence, eochair ?key? is the
object.
? For both parsers, there is some confusion be-
tween the labelling of obl and padjunct,
both of which mark the attachment between
verbs and prepositions. Overall, Malt?s con-
fusion decreases over the 6 iterations of self-
training, but Mate begins to incorrectly choose
padjunct over obl instead. Mixed results
are obtained using the various variants of co-
training.
? Mate handles coordination better than Malt.5 It
is not surprising then that co-training Malt us-
ing Mate parses improves Malt?s coordination
handling whereas the opposite is the case when
co-training Mate on Malt parses, demonstrat-
ing that co-training can both eliminate and in-
troduce errors.
? Other examples of how Mate helps Malt during
co-training is in the distinction between top
and comp relations, between vparticle
and relparticle, and in the analysis of
xcomps.
? Distinguishing between relative and cleft par-
ticles is a frequent error for Mate, and there-
fore Malt also begins to make this kind of error
when co-trained using Mate. Mate improves
using sample-selection-based co-training with
Malt.
? The sample-selection-based co-training vari-
ants show broadly similar trends to the basic
co-training.
4Naturally ambiguous Irish sentences like this require con-
text for disambiguation.
5Nivre and McDonald (2007) make a similar observation
when they compare the errors made by graph and transition
based dependency parsers.
Parsing Models LAS UAS
Development Set
Malt Baseline: 70.0 79.1
Malt Best (co-train) : 71.0 80.2
Mate Baseline: 70.8 79.1
Mate Best (85% threshold ACT1a): 71.8 80.4
Test Set
Malt Baseline: 70.2 79.5
Malt Best (co-train) : 70.8 79.8
Mate Baseline: 71.9 80.1
Mate Best (85% threshold ACT1a): 73.1 81.5
Table 1: Results for best performing models
5.5 Test Set Results
The best performing parsing model for Malt on
the development set is in the final iteration of the
basic co-training approach in Section 5.2. The
best performing parsing model for Mate on the de-
velopment set is the non-iterative 85% threshold
agreement-based co-training approach described in
Section 5.3.4. The test set results for these opti-
mal development set configurations are also shown
in Table 1. The baseline model for Malt obtains
a LAS of 70.2%, the final co-training iteration a
LAS of 70.8%. The baseline model for Mate ob-
tains a LAS of 71.9%, and the non-iterative 85%
agreement-based co-trained model obtains a LAS of
73.1%.
6 Parsing Experiments Using
Morphological Features
As well as the size of the dataset, data sparsity is
also confounded by the number of possible inflected
forms for a given root form. With this in mind,
and following on from the discussion in Section 5.4,
we carry out further parsing experiments in an at-
tempt to make better use of morphological informa-
tion during parsing. We attack this in two ways: by
reducing certain words to their lemmas and by in-
cluding morphological information in the optional
FEATS (features) field. The reasoning behind re-
ducing certain word forms to lemmas is to further
reduce the differences between inflected forms of
the same word, and the reasoning behind including
morphological information is to make more explicit
the similarity between two different word forms in-
flected in the same way. All experiments are car-
9
Parsing Models (Malt) LAS UAS
Baseline: 70.0 79.1
Lemma (Pron Prep): 69.7 78.9
Lemma + Pron Prep Morph Features: 69.6 78.9
Form + Pron Prep Morph Features: 69.8 79.1
Verb Morph Features: 70.0 79.1
Table 2: Results with morphological features on the de-
velopment set
ried out with MaltParser and our seed training set
of 500 gold trees. We focus on two phenomena:
prepositional pronouns or pronominal prepositions
(see Section 2) and verbs with incorporated subjects
(see Section 2 and Section 5.4).
In the first experiment, we include extra mor-
phological information for pronominal prepositions.
We ran three parsing experiments: (i) replacing the
value of the surface form (FORM) of pronominal
prepositions with their lemma form (LEMMA), for
example agam?ag, (ii) including morphological in-
formation for pronominal prepositions in the FEATS
column. For example, in the case of agam ?at me?,
we include Per=1P|Num=Sg, (iii) we combine
both approaches of reverting to lemma form and also
including the morphological features. The results
are given in Table 2.
In the second experiment, we include morpholog-
ical features for verbs with incorporated subjects:
imperative verb forms, synthetic verb forms and au-
tonomous verb forms such as those outlined in Sec-
tion 5.4. For each instance of these verb types, we
included incorpSubj=true in the FEATS col-
umn. The results are also given in Table 2.
The experiments on the pronominal prepositions
show a drop in parsing accuracy while the experi-
ments carried out using verb morphological infor-
mation showed no change in parsing accuracy.6 In
the case of inflected prepositions, perhaps we have
not seen any improvement because we have not fo-
cused on a phenomenon which is critical for parsing.
More experimentation is necessary.
7 Concluding Remarks
We have presented two sets of experiments which
aim to improve dependency parsing performance for
6Although the total number of correct attachments are the
same, the parser output is different.
a minority language with a very small treebank. In
the first set of experiments, the main focus of the pa-
per, we tried to overcome the limited treebank size
by increasing the parsers? training sets using auto-
matically parsed sentences. While we do manage
to achieve statistically significant improvements in
some settings, it is clear from the results that the
gains in parser accuracy through semi-supervised
bootstrapping methods are fairly modest. Yet, in the
absence of more gold labelled data, it is difficult to
know now whether we would achieve similar or im-
proved results by adding the same amount of gold
training data. This type of analysis will be interest-
ing at a later date when the unlabelled trees used in
these experiments are eventually annotated and cor-
rected manually.
The second set of experiments tries to mitigate
some of the data sparseness issues by exploiting
morphological characteristics of the language. Un-
fortunately, we do not see any improvements but we
may get different results if we repeat these experi-
ments using the larger semi-supervised training sets
from the first set of experiments.
There are many directions this parsing research
could take us in the future. Our unlabelled data con-
sisted of sentences annotated with gold POS tags.
In the future we would like to take advantage of
the fully unlabelled, untagged data in the New Cor-
pus for Ireland ? Irish, which consists of 30 million
words. We would also like to experiment with a fully
unsupervised parser using this dataset. Our Malt fea-
ture models are manually optimised ? it would be in-
teresting to experiment with optimising them using
MaltOptimizer (Ballesteros, 2012). An additional
avenue of research would be to exploit the hierar-
chical nature of the dependency scheme to arrive at
more flexible way of measuring agreement or dis-
agreement in sample selection.
Acknowledgements
We thank the three anonymous reviewers for their
helpful feedback. This work is supported by Sci-
ence Foundation Ireland (Grant No. 07/CE/I1142)
as part of the Centre for Next Generation Localisa-
tion (www.cngl.ie) at Dublin City University.
10
References
Miguel Ballesteros. 2012. Maltoptimizer: A sys-
tem for maltparser optimization. In Proceedings of
the Eighth International Conference on Linguistic Re-
sources and Evaluation (LREC), pages 2757?2763, Is-
tanbul, Turkey.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING.
O?zlem C?etinog?lu, Jennifer Foster, Joakim Nivre, Deirdre
Hogan, Aoife Cahill, and Josef van Genabith. 2010.
LFG without c-structures. In Proceedings of the 9th
International Workshop on Treebanks and Linguistic
Theories.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd ACL.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL-00).
Christian-Brothers. 1988. New Irish Grammar. Dublin:
C J Fallon.
David Greene. 1966. The Irish Language. Dublin: The
Three Candles.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of EMNLP.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda,
Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012.
The Irish Language in the Digital Age. Springer Pub-
lishing Company, Incorporated.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
soul Samed Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the sancl 2012 shared task.
In Working Notes of SANCL.
Teresa Lynn, O?zlem C?etinog?lu, Jennifer Foster, Elaine U??
Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing. In Proceedings
of the Eight International Conference on Language
Resources and Evaluation, pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U??
Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceeedings of the Australasian Lan-
guage Technology Workshop (ALTA), pages 23?32.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City, USA, June. Association for Computational
Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2008. When is self-training effective for parsing? In
Proceedings of COLING.
Joakim Nivre and Ryan McDonald. 2007. Characteriz-
ing the errors of data-driven dependency parsing mod-
els. In Proceedings of EMNLP-CoNLL, Prague, Czech
Republic.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC2006).
M??chea?l O? Siadhail. 1989. Modern Irish: Grammatical
structure and dialectal variation. Cambridge: Cam-
bridge University Press.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of EMNLP, Hawaii.
Ines Rehbein. 2011. Data point selection for self-
training. In Proceedings of the Second Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2011), Dublin, Ireland.
Roi Reichart and Ari Rappaport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL shared task
session of EMNLP-CoNLL.
Kenji Sagae. 2010. Self-training without reranking for
parser domain adapation and its impact on semantic
role labelling. In Proceedings of the ACL Workshop
on Domain Adaptation for NLP.
Sebastian Seung, Manfred Opper, and Haim Sompolin-
sky. 1992. Query by committee. In Proceedings
of the Fifth Annual ACM Workshop on Computational
Learning Theory.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the tenth conference on European chapter
of the Association for Computational Linguistics - Vol-
ume 1, EACL ?03, pages 331?338, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nancy Stenson. 1981. Studies in Irish Syntax. Tu?bingen:
Gunter Narr Verlag.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging
and Partial Parsing for Irish using Finite-State Trans-
ducers and Constraint Grammar. Ph.D. thesis, Dublin
City University.
11
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 180?186,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Arabic Native Language Identification
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
In this paper we present the first appli-
cation of Native Language Identification
(NLI) to Arabic learner data. NLI, the task
of predicting a writer?s first language from
their writing in other languages has been
mostly investigated with English data, but
is now expanding to other languages. We
use L2 texts from the newly released Ara-
bic Learner Corpus and with a combina-
tion of three syntactic features (CFG pro-
duction rules, Arabic function words and
Part-of-Speech n-grams), we demonstrate
that they are useful for this task. Our sys-
tem achieves an accuracy of 41% against
a baseline of 23%, providing the first evi-
dence for classifier-based detection of lan-
guage transfer effects in L2 Arabic. Such
methods can be useful for studying lan-
guage transfer, developing teaching mate-
rials tailored to students? native language
and forensic linguistics. Future directions
are discussed.
1 Introduction
Researchers in Second Language Acquisition
(SLA) investigate the multiplex of factors that
influence our ability to acquire new languages
and chief among these factors is the role of the
learner?s mother tongue. Recently this fundamen-
tal factor has been studied in Native Language
Identification (NLI), which aims to infer the native
language (L1) of an author based on texts writ-
ten in a second language (L2). Machine Learning
methods are usually used to identify language use
patterns common to speakers of the same L1.
The motivations for NLI are manifold. The use
of such techniques can help SLA researchers iden-
tify important L1-specific learning and teaching
issues. In turn, the identification of such issues can
enable researchers to develop pedagogical mate-
rial that takes into consideration a learner?s L1 and
addresses them. It can also be applied in a forensic
context, for example, to glean information about
the discriminant L1 cues in an anonymous text.
While almost all NLI research to date has fo-
cused on English L2 data, there is a growing need
to apply the techniques to other language in or-
der to assess the cross-language applicability. This
need is partially driven by the increasing number
of learners of various other languages.
One such case is the teaching of Arabic as a
Foreign Language, which has experienced unpar-
alleled growth in the past two decades. For a long
time the teaching of Arabic was not considered a
priority, but this view has now changed. Arabic is
now perceived as a critical and strategically use-
ful language (Ryding, 2013), with enrolments ris-
ing rapidly and already at an all time high (Wahba
et al., 2013). This trend is also reflected in the
NLP community, evidenced by the continuously
increasing research focus on Arabic tools and re-
sources (Habash, 2010).
A key objective of this study is to investigate
the efficacy of syntactic features for Arabic, a lan-
guage which is significantly different to English.
Arabic orthography is very different to English
with right-to-left text that uses connective letters.
Moreover, this is further complicated due to the
presence of word elongation, common ligatures,
zero-width diacritics and allographic variants. The
morphology of Arabic is also quite rich with many
morphemes that can appear as prefixes, suffixes or
even circumfixes. These mark grammatical infor-
mation including case, number, gender, and defi-
niteness amongst others. This leads to a sophisti-
cated morphotactic system.
Given the aforementioned differences with En-
glish, the main objective of this study is to deter-
mine if NLI techniques can be effective for detect-
ing L1 transfer effects in L2 Arabic.
180
2 Background
NLI has drawn the attention of many researchers
in recent years. With the influx of new researchers,
the most substantive work in this field has come
in the last few years, leading to the organization
of the inaugural NLI Shared Task in 2013 which
was attended by 29 teams from the NLP and SLA
areas. A detailed exposition of the shared task re-
sults and a review of prior NLI work can be found
in Tetreault et al. (2013).
While there exists a large body of literature pro-
duced in the last decade, almost all of this work
has focused exclusively on L2 English. The most
recent work in this field successfully presented
the first application of NLI to a large non-English
dataset (Malmasi and Dras, 2014a), evidencing the
usefulness of syntactic features in distinguishing
L2 Chinese texts.
3 Data
Although the majority of currently available
learner corpora are based on English L2 (Granger,
2012), data from learners of other languages such
as Chinese have also attracted attention in the past
several years.
No Arabic learner corpora were available for a
long time. This paucity of data has been noted by
researchers (Abuhakema et al., 2008; Zaghouani
et al., 2014) and is thought to be due to issues such
as difficulties with non-Latin script and a lack of
linguistic and NLP software to work with the data.
More recently, the first version of the Arabic
Learner Corpus
1
(ALC) was released by Alfaifi
and Atwell (2013). The corpus includes texts by
Arabic learners studying in Saudi Arabia, mostly
timed essays written in class. In total, 66 different
L1 backgrounds are represented. While texts by
native Arabic speakers studying to improve their
writing are also included, we do not utilize these.
We use the more recent second version of the
ALC (Alfaifi et al., 2014) as the data for our exper-
iments. While there are 66 different L1s in the cor-
pus, the majority of these have less than 10 texts
and cannot reliably be used for NLI. Instead we
use a subset of the corpus consisting of the top
seven native languages by number of texts. The
languages and document counts in each class are
shown in Table 1.
Both plain text and XML versions of the learner
1
http://www.arabiclearnercorpus.com/
Native Language Texts
Chinese 76
Urdu 64
Malay 46
French 44
Fulani 36
English 35
Yoruba 28
Total 329
Table 1: The L1 classes included in this experi-
ment and the number of texts within each class.
texts are provided with the corpus. Here we use
text versions and strip the metadata information
from the files, leaving only the author?s writings.
4 Experimental Methodology
In this study we employ a supervised multi-class
classification approach. The learner texts are or-
ganized into classes according on the author?s L1
and these documents are used for training and test-
ing in our experiments. A diagram conceptualiz-
ing our NLI system is shown in Figure 1.
4.1 Word Segmentation
The tokenization and word segmentation of Arabic
is an important preprocessing step for addressing
the orthographic issues discussed in ?1. For this
task we utilize the Stanford Word Segmenter
2
.
4.2 Parsing and Part-of-Speech Tagging
To extract the syntactic information required for
our models, the Arabic texts are POS tagged and
parsed using the Stanford Arabic Parser
3
.
4.3 Classifier
We use a linear Support Vector Machine to per-
form multi-class classification in our experiments.
In particular, we use the LIBLINEAR
4
package
(Fan et al., 2008) which has been shown to be effi-
cient for text classification problems such as this.
4.4 Evaluation Methodology
In the same manner as many previous NLI stud-
ies and also the NLI 2013 shared task, we report
our results as classification accuracy under k-fold
cross-validation, with k = 10. In recent years this
2
http://nlp.stanford.edu/software/segmenter.shtml
3
http://nlp.stanford.edu/projects/arabic.shtml
4
http://www.csie.ntu.edu.tw/%7Ecjlin/liblinear/
181
Arabic Text Chinese L1
NLI
Arabic Text
Arabic Text
Arabic Text
French L1  
English L1 
Malay L1  
Figure 1: Illustration of our NLI system that identifies the L1 of Arabic learners from their writing.
has become a de facto standard for reporting NLI
results.
5 Experiments
We experiment using three syntactic feature types
described in this section. As the ALC is not bal-
anced for topic, we do not consider the use of lex-
ical features such as word n-grams in this study.
Topic bias can occur as a result of the subject mat-
ters or topics of the texts to be classified not not
evenly distributed across the classes. For exam-
ple, if in our training data all the texts written by
English L1 speakers are on topic A, while all the
French L1 authors write about topic B, then we
have implicitly trained our classifier on the topics
as well. In this case the classifier learns to dis-
tinguish our target variable through another con-
founding variable.
5.1 Context-free Grammar Production Rules
Context-free phrase structure rules (without lexi-
calizations) are extracted from parse trees of the
sentences in each learner text. One such con-
stituent parse tree and extracted rules are shown
in Figure 2. These production rules are used as
classification features
5
. Linguistically, they cap-
ture the global syntactic structures used by writers.
5.2 Arabic Function Words
The distributions of grammatical function words
such as determiners and auxiliary verbs have
proven to be useful in NLI. This is considered to
be a useful syntactic feature as these words indi-
cate the relations between content words and are
5
All models use relative frequency feature representations
 ????? ?? ?????? ???? ?? ???? ??? ?? ??????? ?????? ?? ???? ?????
???????? ?? ????? ????. 
 DTNN IN NN DTNN PRP VBD VBP IN VBN DTNN IN NN DTNN CC NN PRP$ IN NN JJ PUNC 
Figure 3: An example of a sentence written by a
learner and its Part-of-Speech tag sequence. Un-
igrams, bigrams and trigrams are then extracted
from this tag sequence.
topic independent. The frequency distributions of
a set of 150 function words were extracted from
the learner texts and used as features in this model.
5.3 Part-of-Speech n-grams
In this model POS n-grams of size 1?3 were ex-
tracted. These n-grams capture small and very lo-
cal syntactic patterns of language production and
were used as classification features.
6 Results
The results from all experiments are shown in Ta-
ble 2. The majority baseline is calculated by us-
ing the largest class, in this case Chinese
6
, as
the default classification. The frequency distri-
butions of the production rules yield 31.7% accu-
racy, demonstrating their ability to identify struc-
tures that are characteristic of L1 groups. Simi-
larly, the distribution of function words is helpful,
with 29.2% accuracy.
While all the models provide results well above
the baseline, POS tag n-grams are the most useful
features, with bigrams providing the highest accu-
racy for a single feature type with 37.6%. This
6
76/329 = 23.1%
182
The options for nodes are all handled by TikZ and are described in detail
in the TikZ documentation. For example, if you have a font named \ar and
want to set all the leaf labels in this font:
.ROOT
.S .
.PUNC
..
.
.S .
.VP .
.NP .
.NP
.NN
.??????
.
.CD
.200
.
.VBD
.????
.
.NP .
.NP
.PRP$
.??
.
.NN
.???
.
.CC
.?
.
.S .
.VP .
.NP .
.NP .
.PP .
.NP
.DTNN
.??????
.
.IN
.?? .
.NP
.NN
.?????
.
.NN
.??
.
.VBD
.???? .
.NP
.NNP
.?????
.
.CC
.?
1
S ? S CC S PUNC VP ? VBD NP
NP ? DTNN PP ? IN NP
Figure 2: A constituent parse tree for a sentence from the corpus along with some of the context-free
grammar production rules extracted from it.
Feature Accuracy (%)
Majority Baseline 23.1
CFG Production Rules 31.7
Function Words 29.2
Part-of-Speech unigrams 36.0
Part-of-Speech bigrams 37.6
Part-of-Speech trigrams 36.5
All features combined 41.0
Table 2: Arabic Native Language Identification
accuracy for the three experiments in this study.
seems to suggest that the greatest difference be-
tween groups lies in their word category ordering.
Combining all of the models into a single fea-
ture space provides the highest accuracy of 41%.
This demonstrates that the information captured
by the various models is complementary and that
the feature types are not redundant.
7 Discussion
The most prominent finding here is that NLI tech-
niques can be successfully applied to Arabic, a
morphologically complex language differing sig-
nificantly from English, which has been the focus
of almost all previous research.
This is one of the very first applications of NLI
to a language other than English and an important
step in the growing field of NLI, particularly with
the current drive to investigate other languages.
This research, though preliminary, presents an ap-
proach to Arabic NLI and can serve as a step to-
wards further research in this area.
NLI technology has practical applications in
various fields. One potential application of NLI
is in the field of forensic linguistics (Gibbons,
2003; Coulthard and Johnson, 2007), a juncture
where the legal system and linguistic stylistics
intersect (Gibbons and Prakasam, 2004; McMe-
namin, 2002). In this context NLI can be used as a
tool for Authorship Profiling (Grant, 2007) in or-
der to provide evidence about the linguistic back-
ground of an author.
There are a number of situations where a text,
such as an anonymous letter, is the central piece of
evidence in an investigation. The ability to extract
additional information from an anonymous text
can enable the authorities and intelligence agen-
cies to learn more about threats and those respon-
sible for them. Clues about the native language
of a writer can help investigators in determining
the source of anonymous text and the importance
of this analysis is often bolstered by the fact that in
such scenarios, the only data available to users and
investigators is the text itself. One recently studied
example is the analysis of extremist related activ-
ity on the web (Abbasi and Chen, 2005).
Accordingly, we can see that from a forensic
point of view, NLI can be a useful tool for intel-
ligence and law enforcement agencies. In fact, re-
cent NLI research such as that related to the work
presented by (Perkins, 2014) has already attracted
183
interest and funding from intelligence agencies
(Perkins, 2014, p. 17).
In addition to applications in forensic linguis-
tics, Arabic NLI can aid the development of re-
search tools for SLA researchers investigating lan-
guage transfer and cross-linguistic effects. Simi-
lar data-driven methods have been recently applied
to generate potential language transfer hypothe-
ses from the writings of English learners (Malmasi
and Dras, 2014c). With the use of an error anno-
tated corpus, which was not the case in this study,
the annotations could be used in conjunction with
similar linguistic features to study the syntactic
contexts in which different error types occur (Mal-
masi and Dras, 2014b).
Results from such approaches could be used
to create teaching material that is customized for
the learner?s L1. This approach has been pre-
viously shown to yield learning improvements
(Laufer and Girsai, 2008). The need for such
SLA tools is particularly salient for a complex lan-
guage such as Arabic which has several learning
stages (Mansouri, 2005), such as phrasal and inter-
phrasal agreement morphology, which are hierar-
chical and generally acquired in a specific order
(Nielsen, 1997).
The key shortcoming of this study, albeit be-
yond our control, is the limited amount of data
available for the experiments. To the best of our
knowledge, this is the smallest dataset used for this
task in terms of document count and length. In this
regard, we are surprised by relatively high classifi-
cation accuracy of our system, given the restricted
amount of training data available.
While it is hard to make comparisons with
most other experiments due to differing number
of classes, one comparable study is that of Wong
and Dras (2009) which used some similar features
on 7-class English dataset. Despite their use of
a much larger dataset
7
, our individual models are
only around 10% lower in accuracy.
We believe that this is a good result, given
our limited data. In their study of NLI corpora,
Brooke and Hirst (2011) showed that increasing
the amount of training data makes a very signifi-
cant difference in NLI accuracy for both syntactic
and lexical features. This was verified by Tetreault
et al. (2012) who showed that there is a very steep
rise in accuracy as the corpus size is increased to-
7
Wong and Dras (2009) had 110 texts per class, with av-
erage text lengths of more than 600 words.
wards 11,000 texts
8
. Based on this, we are con-
fident that given similarly sized training data, an
Arabic NLI system can achieve similar accuracies.
On a broader level, this highlights the need for
more large-scale L2 Arabic corpora.
Future work includes the application of our
methods to large-scale Arabic learner data as it be-
comes available. With the ongoing development
of the Arabic Learner Corpus and other projects
like the Qatar Arabic Language Bank (Mohit,
2013), this may happen in the very near future.
The application of more linguistically sophisti-
cated features also merits further investigation, but
this is limited by the availability of Arabic NLP
tools and resources. From a machine learning per-
spective, classifier ensembles have been recently
used for this task and shown to improve classifi-
cation accuracy (Malmasi et al., 2013; Tetreault et
al., 2012). Their application here could also in-
crease system accuracy.
We also leave the task of interpreting the lin-
guistic features that differentiate and characterize
L1s to future work. This seems to be the next log-
ical phase in NLI research and some methods to
automate the detection of language transfer fea-
tures have been recently proposed (Swanson and
Charniak, 2014; Malmasi and Dras, 2014c). This
research, however, is still at an early stage and
could benefit from the addition of more sophisti-
cated machine learning techniques.
More broadly, additional NLI experiments with
different languages are needed. Comparative stud-
ies using equivalent syntactic features but with dis-
tinct L1-L2 pairs can help us better understand
Cross-Linguistic Influence and its manifestations.
Such a framework could also help us better un-
derstand the differences between different L1-L2
language pairs.
8 Conclusion
In this work we identified the appropriate data and
tools to perform Arabic NLI and demonstrated that
syntactic features can be successfully applied, de-
spite a scarcity of available L2 Arabic data. Such
techniques can be used to generate cross-linguistic
hypotheses and build research tools for Arabic
SLA. As the first machine learning based inves-
tigation of language transfer effects in L2 Ara-
bic, this work contributes important additional ev-
idence to the growing body of NLI work.
8
Equivalent to 1000 texts per L1 class.
184
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying
authorship analysis to extremist-group Web forum
messages. IEEE Intelligent Systems, 20(5):67?75.
Ghazi Abuhakema, Reem Faraj, Anna Feldman, and
Eileen Fitzpatrick. 2008. Annotating an Arabic
Learner Corpus for Error. In LREC.
Abdullah Alfaifi and Eric Atwell. 2013. Arabic
Learner Corpus v1: A New Resource for Arabic
Language Research.
Abdullah Alfaifi, Eric Atwell, and I Hedaya. 2014.
Arabic learner corpus (ALC) v2: a new written and
spoken corpus of Arabic learners. In Proceedings of
the Learner Corpus Studies in Asia and the World
(LCSAW), Kobe, Japan.
Julian Brooke and Graeme Hirst. 2011. Na-
tive language detection with ?cheap? learner cor-
pora. In Conference of Learner Corpus Research
(LCR2011), Louvain-la-Neuve, Belgium. Presses
universitaires de Louvain.
Malcolm Coulthard and Alison Johnson. 2007. An in-
troduction to Forensic Linguistics: Language in evi-
dence. Routledge.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
John Gibbons and Venn Prakasam. 2004. Language in
the Law. Orient Blackswan.
John Gibbons. 2003. Forensic Linguistics: An Intro-
duction To Language In The Justice System.
Sylviane Granger. 2012. Learner corpora. The Ency-
clopedia of Applied Linguistics.
Tim Grant. 2007. Quantifying evidence in forensic
authorship analysis. International Journal of Speech
Language and the Law, 14(1):1?25.
Nizar Y Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies, 3(1):1?187.
Batia Laufer and Nany Girsai. 2008. Form-focused
instruction in second language vocabulary learning:
A case for contrastive analysis and translation. Ap-
plied Linguistics, 29(4):694?716.
Shervin Malmasi and Mark Dras. 2014a. Chinese
Native Language Identification. Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. From Vi-
sualisation to Hypothesis Construction for Second
Language Acquisition. In Graph-Based Methods for
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014c. Language
Transfer Hypotheses with Linear SVM Weights.
Proceedings of the 2014 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP).
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark
Dras. 2013. Nli shared task 2013: Mq submission.
In Proceedings of the Eighth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions, pages 124?133, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
Fethi Mansouri. 2005. Agreement morphology in Ara-
bic as a second language. Cross-linguistic aspects of
Processability Theory, pages 117?253.
Gerald R McMenamin. 2002. Forensic linguistics:
Advances in Forensic Stylistics. CRC press.
Behrang Mohit. 2013. QALB: Qatar Arabic language
bank. In Qatar Foundation Annual Research Con-
ference, number 2013.
Helle Lykke Nielsen. 1997. On acquisition order of
agreement procedures in Arabic learner language.
Al-Arabiyya, 30:49?93.
Ria Perkins. 2014. Linguistic identifiers of L1 Persian
speakers writing in English: NLID for authorship
analysis. Ph.D. thesis, Aston University.
Karin C. Ryding. 2013. Teaching Arabic in the United
States. In Kassem M Wahba, Zeinab A Taha, and
Liz England, editors, Handbook for Arabic language
teaching professionals in the 21st century. Rout-
ledge.
Ben Swanson and Eugene Charniak. 2014. Data
Driven Language Transfer Hypotheses. EACL 2014,
page 169.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, Beata
Beigman-Klebanov, and Martin Chodorow. 2012.
Native Tongues, Lost and Found: Resources and
Empirical Evaluations in Native Language Identifi-
cation. In Proc. Internat. Conf. on Computat. Lin-
guistics (COLING).
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Build-
ing Educational Applications, pages 48?57, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Kassem M Wahba, Zeinab A Taha, and Liz England.
2013. Handbook for Arabic language teaching pro-
fessionals in the 21st century. Routledge.
Sze-Meng Jojo Wong and Mark Dras. 2009. Con-
trastive analysis and native language identification.
In Proc. Australasian Language Technology Work-
shop (ALTA), pages 53?61.
185
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines
and Framework. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Hrafn
Loftsson, Bente Maegaard, Joseph Mariani, Asun-
cion Moreno, Jan Odijk, and Stelios Piperidis, ed-
itors, Proceedings of the Ninth International Con-
ference on Language Resources and Evaluation
(LREC?14), Reykjavik, Iceland, may. European
Language Resources Association (ELRA).
186
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 56?64,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
From Visualisation to Hypothesis Construction
for Second Language Acquisition
Shervin Malmasi
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
shervin.malmasi@mq.edu.au
Mark Dras
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
mark.dras@mq.edu.au
Abstract
One research goal in Second Language Acqui-
sition (SLA) is to formulate and test hypothe-
ses about errors and the environments in which
they are made, a process which often involves
substantial effort; large amounts of data and
computational visualisation techniques promise
help here. In this paper we have defined a new
task for finding contexts for errors that vary
with the native language of the speaker that are
potentially useful for SLA research. We pro-
pose four models for approaching this task, and
find that one based only on error-feature co-
occurrence and another based on determining
maximum weight cliques in a feature associ-
ation graph discover strongly distinguishing
contexts, with an apparent trade-off between
false positives and very specific contexts.
1 Introduction
SLA researchers are interested in a wide variety of as-
pects of humans learning a new language (L2) different
from their native one (L1): cognitive issues and devel-
opmental sequences for learners Pienemann (2005), so-
ciocultural factors (Lantolf, 2001), and so on. One long-
standing question, dating back to at least Lado (1957),
is expressed by Ortega (2009) in the following way:
?What is the role played by first language in L2 develop-
ment, vis-`a-vis the role of other universal development
forces??
An example of SLA research that looks at this ques-
tion is the study of Di?ez-Bedmar and Papp (2008), com-
paring Chinese and Spanish learners of English with
respect to the English article system (a, an, the) using
corpora of essays by native and non-native speakers
of English (Granger, 2011). Drawing on the 175 non-
native texts, they take a particular theoretical analysis
(the so-called Bickerton semantic wheel), use the simple
Wordsmith tools designed to extract data for lexicogra-
phers to identify errors in a semi-automatic way, and
evaluate whether Chinese and Spanish L1 speakers do
behave differently via hypothesis testing (ANOVA, chi-
square and z-tests, in their case). They conclude that
Chinese and Spanish do have characteristic differences,
with patterns of zero article and definite article use dif-
fering according to semantic context. Such studies are
typically carried out on relatively small datasets, and
use fairly elementary tools. Sources such as Ellis (2008)
and Ortega (2009) give good overviews of such studies
and of SLA research in general.
A goal of this paper is to investigate a particular way
in which Natural Language Processing (NLP) can use-
fully contribute to SLA. In terms of existing work, the
subfield of Native Language Identification (NLI) has
been quite active recently, which looks at predicting
the L1 of writers writing in a common L2 within a
classification task framework; see for example the re-
cent NLI shared task with 29 entrants (Tetreault et al.,
2013).
1
From within linguistics, there has been much
interest in how data-driven approaches can contribute to
SLA. Granger (2011) discusses a body of work based
on the the methodology of carrying out corpus-based
approaches to SLA with a focus on NLP tools; Jarvis
and Crossley (2012) in an edited collection present re-
cent work by linguists who extend the corpus-based
setup by using a text classification approach, looking at
what feature selection might say for SLA. From within
NLP, Swanson and Charniak (2013) and Swanson and
Charniak (2014) take a data-driven approach to SLA
investigations much in the spirit of this work.
One particular approach to finding aspects of texts
characteristic of their L1s that has motivated the present
work is described in Yannakoudakis et al. (2012), the
goal of which is to develop visualisation tools for SLA
researchers. They present graphs of the relationships
between errors and their contexts, such that SLA re-
searchers can navigate through the graphs to find con-
texts for particular errors that can lead to hypotheses
like that of Di?ez-Bedmar and Papp (2008) above. In this
paper, we look at approaches to finding such hypothesis
candidates automatically in the context of L1?L2 inter-
action by analysing the graphs used in the visualisations
1
http://sites.google.com/site/
nlisharedtask2013/
56
of Yannakoudakis et al. (2012). Specifically, we do the
following:
? We propose a new task that is more directly ori-
ented to SLA research than NLI has been for the
most part, with the goal of identifying error-related
contexts that are characteristic of L1s.
? We evaluate a number of models for finding such
contexts, ranging from a simple baseline to treat-
ing the problem as a graph-theoretic maximum
weighted clique one.
? We examine the results of some of the models to
see how the task and the models might contribute
to SLA research.
Because we draw heavily on the work of Yan-
nakoudakis et al. (2012), we first review relevant aspects
of that work in ?2; we then present our task definition
and experimental setup in ?3; we give results along with
a discussion in ?4; we follow with some more detail on
related work in ?5; and we conclude in ?6.
2 Developing Hypotheses: A
Visualisation Tool
The context of the Yannakoudakis et al. (2012) work
is automated grading of English as a Second or Other
Language (ESOL) exam scripts, as described in Briscoe
et al. (2010). The automated grading takes a classifi-
cation approach, using a binary discriminative learner,
with useful features including lexical and part-of-speech
(PoS) n-grams.
The publicly available dataset on which the work was
carried out consists of texts from the First Certificate in
English (FCE) exam, aimed at upper-intermediate stu-
dents of English across various L1s, and was presented
in Yannakoudakis et al. (2011). This FCE corpus
2
con-
sists of a subset of 1244 texts of the Cambridge Learner
Corpus,
3
and is manually annotated with errors and their
corrections, as well as a classification according to an
error typology, as in Figure 1.
Yannakoudakis et al. (2012) present their English
Profile (EP) visualiser as a way to ?visually analyse as
well as perform a linguistic interpretation of discrimi-
native features that characterise learner English?, using
the features of this essay classification task. They de-
fine a measure of co-occurrence of features, among
themselves and with errors, as a core part of their
analysis. Given the set of all sentences in the corpus
S = {s
1
, s
2
, . . . , s
|S|
} and the set of all features F =
{f
1
, f
2
, . . . , f
|F |
}, a feature f
i
? F is associated with
a feature f
j
? F (i 6= j, 1 ? i, j ? M ) according to
the score given in Equation (1), for s
k
? S, 1 ? k ? N
2
http://ilexir.co.uk/applications/
ep-visualiser/
3
http://www.cup.cam.ac.uk/gb/elt/
catalogue/subject/custom/item364603/
and exists() a binary function returning true if the input
feature occurs in s
k
.
score
ff
(f
j
, f
i
) =
?
|S|
k=1
exists(f
j
, f
i
, s
k
)
?
|S|
k=1
exists(f
i
, s
k
)
(1)
They mention an analogous measure for feature-error
co-occurrence; we assume given the set of all errors
E = {e
1
, e
2
, . . . , e
|E|
} that this is defined as follows:
score
ef
(f
j
, e
i
) =
?
|S|
k=1
exists(f
j
, e
i
, s
k
)
?
|S|
k=1
exists(e
i
, s
k
)
(2)
A graph is defined with features and errors as vertices;
an edge between features (resp. features and errors) is
established if score
ff
() (resp. score
ef
) is within some
user-defined range. This graph of feature?feature (resp.
feature?error) relationships is then presented visually.
The paper then presents a case study of how the EP vi-
sualiser can be used to assist SLA researchers. The case
study starts by noting that RG_JJ_NN1 is the 18th most
discriminative negative feature from the essay classi-
fier; then, further inspecting the graph of discriminative
features, that it?s linked to JJ_NN1_II and VBZ_RG.
Then, looking at feature-error relations, it investigates
an association with error MD (missing determiner), and
presents some examples that match the features (e.g.
Unix is very powerful system but there is one thing
against it), along with a discussion of relationships to
various L1s. It is this process of finding interesting fea-
tures and linking them to particular errors and L1s that
we present an approach to automating in this paper.
3 Task Definition & Experimental Setup
At a general level, our goal is to find which kinds of
constructions (in a loose sense) centred around errors
are particularly characteristic of various L1s.
The specific task we define for this paper, then, is
to select a set of features (in the terminology of Yan-
nakoudakis et al. (2012))?which we refer to as the
ERROR CONTEXT?that, when combined with the er-
ror, show a strong association with L1, in a manner
we describe below. So, for example, this may involve
finding that an MD error in the context of RG_JJ_NN1,
JJ_NN1_II and VBZ_RG shows a strong association
with L1. We investigate a number of models for this
selection process: the task then is the identification of
which models produce poor error contexts (which will
not rank highly in hypothesis testing) and which pro-
duce good ones (potentially worth considering by an
SLA researcher). Below we discuss the data we use,
the measure of association for an error and its context,
the set of errors chosen, and the models for selecting
context.
3.1 Data
The corpus we use for evaluating the models for our task
is derived from the FCE corpus of Yannakoudakis et al.
57
Verb Agreement <p>Some people <ns type="AGV"><i>says</i><c>say</c></ns> ...</p>
Incorrect Verb <p>The day I <ns type="IV"><i>shaked</i><c>shook</c></ns> their
Inflection
hands,...</p>
Missing Determiner <p>I am <ns type="MD"><c>a</c></ns> really good singer.</p>
Figure 1: FCE corpus examples. Error types indicated by <ns type>...</ns>; errors indicated by <i>...</i>;
corrections indicated by <c>...</c>.
language size
Chinese CHI 66
French FRE 146
German GER 69
Italian ITA 76
Japanese JAP 81
Korean KOR 86
Spanish SPA 200
Turkish TUR 75
Table 1: FCESUB, broken down by language
(2012). The full FCE corpus consists of 1244 scripts
over 16 languages; script counts range from 2 (Dutch)
to 200 (Spanish).
The features used by Yannakoudakis et al. (2012)
were derived from their essay classification task. As we
are interested in associations with L1, we instead use
features from a system submitted to the NLI shared task
(Anonymous, 2013), which was applied to a dataset of
Test of English as a Foreign Language (TOEFL) scripts:
the task and its designated corpus are described in the
task overview paper (Tetreault et al., 2013). In this work
we use a system trained on the TOEFL11 corpus con-
sisting of texts written in English from speakers of 11
different L1s, with 1100 essays per L1 and balanced
across topic. We only use PoS n-grams (n = 1, 2, 3) as
features in this work. Note that we use the terminology
of Yannakoudakis et al. (2012) here: what had their
origin as features in the essay classification task are still
referred to as features in the visualisation tool, although
the task carried out there is not a classification one. Sim-
ilarly, we refer to our PoS n-grams as features, although
we are not classifying errors using these features and
so are not carrying out feature selection for the typical
purpose of optimising classification performance.
For this, as did Yannakoudakis et al. (2012), we use
the RASP parser (Briscoe et al., 2006) for tagging; the
tags are consequently from the CLAWS2 tagset,
4
which
are more fine-grained in terms of linguistic analysis than
the more frequently used Penn Treebank tags.
For our task, we then used the subset of the FCE cor-
pus where the languages overlapped with the TOEFL11
corpus: we refer to this as FCESUB. This gives 799
scripts over 8 languages, distributed as in Table 1; a
positive byproduct is that the L1s are more similar in
size than the full FCE corpus.
4
http://ucrel.lancs.ac.uk/claws2tags.
html
language mean
CHI 0.885790
FRE 0.460894
GER 0.366587
ITA 0.581401
JAP 1.058159
KOR 1.067211
SPA 0.472253
TUR 1.014129
F-stat 18.031
sig. <0.001
Table 2: ANOVA results giving mean score (number
of sentences with MD error per 10 sentences) for each
language, the ANOVA F-statistic, and significance value
3.2 Association Measure
We noted in ?1 that SLA studies such as Di?ez-Bedmar
and Papp (2008) use standard hypothesis testing tech-
niques. We take this as a starting point. We could, for
example, evaluate whether a particular raw error (that
is, without a feature context) is strongly associated with
L1s by using a single factor ANOVA test.
5
The indepen-
dent variable would be the L1. The dependent variable
could be one of a number of alternatives; we choose the
number of sentences with a particular error per 10 sen-
tences.
6
To illustrate, we give the ANOVA results from
FCESUB for the MD error in Table 2. The ANOVA
calculation is based on an F-statistic which compares
variance between treatments against variance within
treatments; this is compared against critical values for
the F-statistic to determine statistical significance. The
expected value of the F-statistic under the null hypoth-
esis is 1, with values above 1 increasingly inconsistent
with the null hypothesis. The data in Table 2 shows
that the MD error does vary significantly with L1; a
post-hoc Tukey HSD test lets us identify which specific
languages exhibit this difference and shows that, for
example (and as can be observed in the means), German
L1 speakers are significantly different from Korean L1
speakers in the occurrence of MD errors.
For our task we are not interested in significance per
se. Rather, we are interested in whether we can find oc-
currences of errors plus contexts that are more strongly
associated with, or that vary across, L1s, e.g. that an
5
See, e.g., Jackson (2009).
6
We note that the texts differ significantly in length by L1,
so it would not be suitable to normalise as occurrences per
document.
58
type name F-stat p-val N
DJ Wrong Derived 3.27 .002 332
Adjective
DN Wrong Derived 0.70 .671 294
Noun
MD Missing Determiner 18.03 .000 1702
MT Missing Preposition 2.81 .007 985
UD Unnecessary Determiner 1.20 .301 807
UT Unnecessary Preposition 0.26 .968 689
UV Unnecessary Verb 0.78 .606 317
Table 3: Error types chosen for evaluation, including F-
statistic, ANOVA p-value and corpus count of sentences
containing error.
MD error in the context of RG_JJ_NN1, JJ_NN1_II
and VBZ_RG is more strongly associated with L1s; and
we are also interested in which of our proposed methods
for identifying an error?s feature context does this best.
For this purpose, then, we use just the F-statistic from
the ANOVA test, this time with the dependent variable
as the ratio of occurrences of error plus error context
per 10 sentences: a higher F-statistic shows a stronger
association with L1s.
7
We also consider the ?
2
-statistic from Pearson?s chi-
squared test, noting that it is also used in SLA hypothe-
sis testing and that it was additionally found by Swanson
and Charniak (2013) to be good at distinguishing inter-
esting features in their related task (see ?5 for more
detail). The F-statistic and ?
2
-statistic are closely re-
lated: a random variate of the F-distribution is the ratio
of two chi-squared variates scaled by their degrees of
freedom. A difference is that ?
2
compares observed
versus expected counts rather than proportions: to take
account of the differing text lengths, our observed fre-
quency is the number of sentences with error and error
context per L1; our expected frequency is the total num-
ber of sentences with that error and error context scaled
according to the proportion of sentences labelled with
that L1 relative to the corpus as a whole.
3.3 Errors Chosen
From the 74 error types in the FCE corpus, we select a
subset to evaluate our models. In addition to the MD er-
ror used in the case study of Yannakoudakis et al. (2012),
we choose a subset which has a range of F-statistic val-
ues as described above: some show very similar patterns
across L1s (i.e. with low F-statistic), such as DN Wrong
Derived Noun (e.g. hot vs heat); others do vary signif-
icantly with L1, such as DJ Wrong Derived Adjective
(e.g. reasonally vs reasonable). Having errors with
a range of F-statistic values lets us evaluate whether
finding good error contexts works only for strongly L1-
associated errors, weakly L1-associated errors, or across
7
As we are only using the F-statistic to evaluate ranks, we
do not need a multiple comparison adjustment such as the
Bonferroni correction: this would only apply for comparisons
to a significance threshold, and in any case the Bonferroni is
monotonic and does not affect rankings.
the spectrum. Our subset is in Table 3, along with their
F-statistic, ANOVA p-value and counts in FCESUB.
3.4 Models
We propose four models for choosing error contexts.
These models rank error contexts; we evaluate the
ranked error contexts by F-statistic and ?
2
-statistic val-
ues (?3.2).
ERRORCOOCC In this model we rank features by
error-feature co-occurrence scores given by Equation
(2). The L1 is not taken into account, so this will just
return common features which may be equally strongly
associated with errors across all L1s. We look at results
for when k = 1..3 features are chosen. For k = 2, 3,
we add the individual error-feature scores together for
the ranking.
8
It may be the case that interesting results
could be obtained for k > 3, but we only look at the
k = 1..3 in this preliminary work to see if there are
any discernible trends suggesting that larger values of k
could help.
L1ASSOC Here we use features that are strongly as-
sociated with the L1s from the TOEFL11 corpus and
NLI shared task. Specifically, we rank features by their
Information Gain with respect to L1s as in the process of
feature selection from the shared task.
9
The relationship
between errors and features (in the form of error-feature
co-occurrence scores) is not taken into account here.
Again, we look at results for when k = 1..3 features
are chosen, and for k = 2, 3, we add the individual
error-feature scores together for the ranking.
MAXWEIGHTCLIQUE Both of the preceding mod-
els look only at one factor that might be relevant: error-
feature scores (finding features that are related to the
errors) and a measure of the association of features with
L1s; but there is no link between them, and interaction
of features is not taken into account. In Yannakoudakis
et al. (2012), the visualiser provides to the SLA re-
searcher a graph showing the relatedness of features,
based on Equation (1), and the SLA researcher com-
bines this with error-feature scores to find interesting
candidate error contexts; we create a similar graph and
aim to imitate the process by incorporating error-feature
scores as follows.
We define a weighted undirected graph G = (V,A)
such that V is the set of features used in the above
models (i.e. PoS n-grams from ERRORCOOCC); A is
defined such that (v
i
, v
j
) ? A for vertices v
i
, v
j
? V
if 0.8 ? score
ff
(v
i
, v
j
) ? 1.0 where score
ff
() is as
defined as in Equation (1).
10
Given our set of errors
E defined at Equation (2) above, the weight of a ver-
tex v
i
is defined as score
ef
(v
i
, e
j
) for some e
j
? E.
8
For k = 2 the combinations were made from the top 100
features from k = 1, and for k = 3 from the top 50.
9
We recalculated this over the subset of eight languages
used in this paper.
10
We choose this threshold value as it is the one used in the
graph definition of Yannakoudakis et al. (2012).
59
model r
ERRORCOOCC 0.95
L1ASSOC 0.97
MAXWEIGHTCLIQUE 0.95
MAXWEIGHTCLIQUE-L1 0.92
Table 4: Average correlation coefficient r between F-
statistic and ?
2
-statistic for each model
Given this graph, it is possible to characterise the find-
ing of related features with strong aggregate associations
with errors as an instance of the MAXIMUM WEIGHT
CLIQUE PROBLEM (Bomze et al., 1999). As the name
suggests, this finds a clique of maximum weight, here
the strongest aggregate feature?error association. While
this is an NP-hard problem, there are quite efficient algo-
rithms for solving it; we use one proposed by
?
Osterg?ard
(1999).
11
MAXWEIGHTCLIQUE-L1 We also look at a vari-
ant of MAXWEIGHTCLIQUE where we construct the
graphs based only on relationships among features for
a particular L1. That is, there will be eight weighted
graphs per error of interest.
4 Results and Discussion
4.1 Overall Results
We only present the F-statistic results here; the ?
2
-
statistic showed very similar patterns. The average
correlation between the two for each model shows the
strong similarity (Table 4).
For the F-statistic results, presented in Table 5, we
report the highest F-statistic in the N -best list (N =
1, 5, 20, 50) for each model. For models ERRORCOOCC
and L1ASSOC we report the highest F-statistic for each
value of k (k = 1, 2, 3). The number of occurrences
of the error context with the highest F-statistic is given
in parentheses after the F-statistic; the highest value
for each N is in bold. For MAXWEIGHTCLIQUE-L1,
we also note the language of the graph from which the
highest score was derived.
We note by comparing Table 5 with Table 3 that for
each error type except for MD, it is possible to find
an error context that is more strongly associated with
L1s than is the raw error type alone. For MD this is
not surprising, as its frequency of occurrence is very
strongly linked to the L1, as noted in Table 2 and ?3.2.
12
(For the error type MT also, no model produces an error
context more strongly associated with the L1 for the
single best choice where N = 1, but does for larger
values of N .)
11
Code for the used wclique is available at http://tcs.
legacy.ics.tkk.fi/
?
pat/wclique.html.
12
The fact that determiner errors are very widely studied in
terms of analysing cross-linguistic influence suggests a broad
consensus that they vary strongly with L1. In addition to Di?ez-
Bedmar and Papp (2008), a sample of other studies includes
Parrish (1987), Young (1996) and Ionin and Montrul (2010).
With respect to the individual models, the simple ER-
RORCOOCC scores highly, giving the best result about
half the time, and the best results can occur for any
of k = 1, 2, 3. The number of instances returned for
each error plus error context is larger than for the other
models as well, which is not surprising as the model
aims to find contexts strongly associated with the errors
rather than with L1s. However, these are then likely to
be features that are fairly common across L1s; we look
at some examples in ?4.2.
L1ASSOC performs fairly poorly on our evaluation
measure, although in many cases it does find an error
context more strongly associated with the L1 than just
the raw error type. Counts are also lower. Also, for this
model, k = 2, 3 are always worse than k = 1: bringing
in a second context feature reduces the number of oc-
currences to such an extent that the F-statistic can drop
dramatically. This is probably in part an artefact of the
size of the FCE corpus (and particularly our FCESUB
subcorpus): these features derived from the TOEFL11
corpus just do not occur sufficiently often in our evalua-
tion corpus (and in fact there are often large numbers of
zero occurrences for k = 2, 3).
MAXWEIGHTCLIQUE also performs fairly poorly.
However, in many cases it also finds an error context
more strongly associated with L1 than the raw error type
alone (DN, MT, UD, UT, UV), even if not always for
N = 1, and it has intermediate counts of occurrences.
MAXWEIGHTCLIQUE-L1 gives the best results in
the other half of the cases where ERRORCOOCC does
not. The error contexts that it finds, however, are very
specific, often to a single language (as might be expected
by its definition) with very small numbers of counts.
4.2 Some Examples
We look at some examples in Figure 2, to illustrate both
interesting error contexts found and areas where the
models do a poor job. In these sample sentences, only
errors of interest are retained and highlighted.
The DJ error with context { JJ, NN1 } illustrates the
top result found under the ERRORCOOCC model for
N = 20. In the first sentence the model seems to find a
useful pattern: the adjective that is at the centre of the
error occurs in the context of a singular noun. On the
other hand, the second sentence illustrates a problem:
because the range of the context is the whole sentence,
frequent features such as NN1 will occur a lot in other
parts of the sentence that have no apparent relation to
the actual error. The ERRORCOOCC model is thus likely
to be picking up false positives by virtue of the relatively
high frequencies of its error contexts.
The UV error with context { TO_VV0_II, NNL1,
II, NN2, VV0_II } illustrates the top result found
under the MAXWEIGHTCLIQUE-L1 model for N =
5. This is very specific, and its three instances only
appear in Turkish. But all three are similar errors from
different documents, so it appears likely to be a genuine
pattern, although the NN2 seems only to have a tenuous
60
e
r
r
o
r
N
E
R
R
O
R
C
O
O
C
C
L
1
A
S
S
O
C
M
A
X
W
E
I
G
H
T
C
L
I
Q
U
E
M
A
X
W
E
I
G
H
T
C
L
I
Q
U
E
-
L
1
D
J
1
2
.
7
8
(
2
7
4
)
/
3
.
1
9
(
2
2
7
)
/
2
.
9
5
(
1
5
8
)
1
.
5
9
(
3
1
)
/
1
.
5
9
(
3
1
)
/
0
.
8
1
(
6
)
0
.
9
9
(
1
5
)
3
.
0
8
(
2
)
[
G
E
R
]
5
3
.
6
0
(
2
6
8
)
/
3
.
1
9
(
2
2
7
)
/
3
.
0
2
(
1
4
8
)
2
.
1
9
(
1
2
)
/
1
.
5
9
(
3
1
)
/
0
.
8
1
(
6
)
1
.
7
4
(
4
1
)
3
.
2
4
(
2
)
[
C
H
I
]
2
0
3
.
7
2
(
1
9
4
)
/
3
.
3
3
(
1
6
3
)
/
4
.
0
2
(
9
3
)
2
.
5
3
(
7
0
)
/
1
.
5
9
(
3
1
)
/
1
.
3
6
(
1
)
2
.
3
4
(
2
4
)
3
.
5
0
(
5
)
[
I
T
A
]
5
0
3
.
7
2
(
1
9
4
)
/
3
.
3
9
(
1
1
4
)
/
4
.
0
2
(
9
3
)
2
.
5
8
(
1
0
7
)
/
1
.
5
9
(
3
1
)
/
1
.
5
9
(
3
1
)
2
.
4
8
(
1
8
)
3
.
8
4
(
3
)
[
I
T
A
]
D
N
1
0
.
7
7
(
2
6
8
)
/
1
.
6
3
(
1
8
5
)
/
1
.
7
3
(
1
1
9
)
1
.
0
9
(
4
0
)
/
1
.
0
9
(
4
0
)
/
0
.
7
0
(
7
)
1
.
2
6
(
6
3
)
3
.
2
4
(
2
)
[
C
H
I
]
5
1
.
8
0
(
1
9
1
)
/
2
.
2
9
(
1
5
3
)
/
2
.
5
4
(
1
4
2
)
1
.
2
5
(
5
)
/
1
.
3
6
(
1
)
/
1
.
3
6
(
1
)
1
.
2
6
(
6
3
)
3
.
2
4
(
2
)
[
C
H
I
]
2
0
2
.
3
4
(
8
6
)
/
2
.
6
9
(
1
4
4
)
/
2
.
9
5
(
1
1
3
)
2
.
0
4
(
2
6
)
/
1
.
3
6
(
1
)
/
1
.
3
6
(
1
)
1
.
7
6
(
3
0
)
3
.
2
4
(
2
)
[
C
H
I
]
5
0
2
.
8
6
(
6
1
)
/
3
.
1
6
(
1
2
0
)
/
2
.
9
5
(
1
1
3
)
3
.
8
9
(
4
)
/
2
.
7
5
(
2
)
/
2
.
7
5
(
2
)
3
.
4
1
(
1
8
)
4
.
2
7
(
1
0
)
[
S
P
A
]
M
D
1
1
4
.
2
8
(
1
3
1
9
)
/
9
.
0
9
(
9
8
5
)
/
6
.
3
8
(
7
5
3
)
5
.
8
3
(
1
9
8
)
/
5
.
8
3
(
1
9
8
)
/
0
.
5
4
(
2
)
3
.
0
7
(
2
9
7
)
4
.
0
5
(
9
1
)
[
K
O
R
]
5
1
4
.
2
8
(
1
3
1
0
)
/
1
2
.
1
8
(
7
6
9
)
/
6
.
7
5
(
5
8
2
)
8
.
2
0
(
2
6
8
)
/
5
.
8
3
(
1
9
8
)
/
1
.
9
3
(
3
)
5
.
8
3
(
1
9
8
)
5
.
8
3
(
1
9
8
)
[
K
O
R
]
2
0
1
4
.
4
1
(
8
5
0
)
/
1
2
.
1
8
(
7
6
9
)
/
6
.
8
2
(
5
9
3
)
8
.
2
0
(
2
6
8
)
/
5
.
8
3
(
1
9
8
)
/
2
.
6
0
(
3
6
)
5
.
8
3
(
1
9
8
)
5
.
8
3
(
1
9
8
)
[
K
O
R
]
5
0
1
4
.
4
1
(
8
5
0
)
/
1
2
.
1
8
(
7
6
9
)
/
7
.
9
9
(
4
8
3
)
8
.
3
6
(
8
3
1
)
/
5
.
8
3
(
1
9
8
)
/
5
.
8
3
(
1
9
8
)
5
.
8
3
(
1
9
8
)
6
.
4
7
(
1
1
0
)
[
K
O
R
]
M
T
1
3
.
3
4
(
7
9
4
)
/
3
.
0
0
(
6
6
6
)
/
3
.
0
2
(
4
8
5
)
1
.
8
5
(
7
9
)
/
1
.
8
5
(
7
9
)
/
1
.
5
5
(
1
3
)
1
.
7
0
(
6
1
)
2
.
4
8
(
2
0
)
[
C
H
I
]
5
3
.
3
4
(
7
9
4
)
/
3
.
4
6
(
4
7
8
)
/
3
.
3
7
(
3
7
8
)
2
.
5
4
(
1
0
1
)
/
1
.
8
5
(
7
9
)
/
1
.
5
5
(
1
3
)
2
.
1
4
(
6
4
)
4
.
4
7
(
3
)
[
C
H
I
]
2
0
4
.
4
4
(
2
9
5
)
/
3
.
6
4
(
3
7
5
)
/
4
.
6
0
(
2
9
4
)
4
.
4
4
(
2
9
5
)
/
3
.
1
1
(
2
5
)
/
3
.
1
1
(
2
5
)
2
.
7
9
(
4
4
)
4
.
4
7
(
3
)
[
C
H
I
]
5
0
4
.
5
0
(
2
7
7
)
/
5
.
2
1
(
2
4
7
)
/
4
.
7
2
(
2
1
5
)
4
.
4
4
(
2
9
5
)
/
3
.
8
6
(
3
3
)
/
3
.
1
1
(
2
5
)
4
.
5
4
(
7
4
)
4
.
6
1
(
3
)
[
G
E
R
]
U
D
1
0
.
6
9
(
6
7
9
)
/
1
.
0
5
(
4
7
5
)
/
2
.
0
8
(
3
3
4
)
1
.
4
5
(
6
2
)
/
1
.
4
5
(
6
2
)
/
0
.
7
3
(
1
0
)
0
.
6
4
(
4
7
)
1
.
5
4
(
2
0
)
[
G
E
R
]
5
1
.
7
0
(
4
0
5
)
/
1
.
1
7
(
4
5
2
)
/
2
.
0
8
(
3
3
4
)
1
.
5
9
(
2
6
)
/
1
.
4
5
(
6
2
)
/
1
.
3
6
(
1
)
1
.
4
5
(
6
2
)
3
.
5
4
(
9
)
[
C
H
I
]
2
0
2
.
0
8
(
2
2
3
)
/
2
.
1
1
(
3
6
0
)
/
2
.
3
2
(
2
7
6
)
3
.
4
1
(
5
1
)
/
1
.
4
5
(
6
2
)
/
1
.
3
6
(
1
)
1
.
9
0
(
2
9
)
3
.
9
3
(
3
)
[
I
T
A
]
5
0
3
.
2
7
(
1
1
2
)
/
3
.
0
1
(
1
8
8
)
/
2
.
3
3
(
1
9
8
)
3
.
4
1
(
5
1
)
/
1
.
5
4
(
4
)
/
1
.
5
4
(
4
)
2
.
8
5
(
6
6
)
4
.
0
6
(
3
)
[
I
T
A
]
U
T
1
0
.
1
4
(
5
4
8
)
/
0
.
4
5
(
4
1
4
)
/
1
.
1
2
(
2
5
9
)
1
.
0
1
(
5
1
)
/
1
.
0
1
(
5
1
)
/
0
.
4
3
(
1
)
0
.
8
1
(
3
5
)
3
.
0
6
(
2
)
[
G
E
R
]
5
0
.
8
2
(
3
6
8
)
/
1
.
1
6
(
3
2
1
)
/
1
.
5
8
(
2
4
9
)
2
.
2
8
(
2
3
)
/
1
.
3
6
(
1
)
/
1
.
3
6
(
1
)
1
.
0
1
(
5
1
)
4
.
1
0
(
3
)
[
T
U
R
]
2
0
1
.
5
1
(
3
5
1
)
/
1
.
7
7
(
2
7
5
)
/
1
.
8
9
(
2
2
5
)
2
.
9
1
(
5
1
)
/
1
.
5
3
(
6
)
/
1
.
3
6
(
1
)
2
.
5
8
(
4
5
)
4
.
1
0
(
3
)
[
T
U
R
]
5
0
2
.
2
5
(
1
1
2
)
/
2
.
6
6
(
2
0
1
)
/
3
.
1
8
(
1
7
8
)
2
.
9
1
(
5
1
)
/
1
.
5
3
(
6
)
/
1
.
3
6
(
1
)
2
.
5
8
(
4
5
)
4
.
1
0
(
3
)
[
T
U
R
]
U
V
1
0
.
8
8
(
2
6
0
)
/
0
.
9
7
(
1
8
6
)
/
1
.
1
8
(
1
1
9
)
1
.
0
6
(
1
5
)
/
1
.
0
6
(
1
5
)
/
1
.
2
9
(
2
)
1
.
4
9
(
2
8
)
2
.
5
3
(
2
)
[
J
A
P
]
5
2
.
2
2
(
1
7
5
)
/
2
.
2
1
(
1
6
2
)
/
1
.
6
8
(
1
0
9
)
2
.
2
9
(
8
)
/
1
.
2
9
(
2
)
/
1
.
2
9
(
2
)
1
.
4
9
(
2
8
)
4
.
0
9
(
3
)
[
T
U
R
]
2
0
2
.
2
5
(
1
2
5
)
/
2
.
8
2
(
1
2
7
)
/
3
.
1
3
(
9
6
)
3
.
2
2
(
8
)
/
1
.
5
2
(
1
)
/
1
.
5
2
(
1
)
2
.
3
8
(
1
5
)
4
.
0
9
(
3
)
[
T
U
R
]
5
0
2
.
5
6
(
6
1
)
/
3
.
0
1
(
1
0
1
)
/
3
.
1
3
(
9
6
)
3
.
2
2
(
8
)
/
1
.
5
2
(
1
)
/
1
.
5
2
(
1
)
2
.
3
8
(
1
5
)
4
.
6
3
(
3
)
[
C
H
I
]
T
a
b
l
e
5
:
R
e
s
u
l
t
s
f
o
r
t
h
e
c
h
o
s
e
n
e
r
r
o
r
t
y
p
e
s
u
n
d
e
r
t
h
e
f
o
u
r
p
r
o
p
o
s
e
d
m
o
d
e
l
s
.
A
l
l
e
r
r
o
r
t
y
p
e
s
a
n
d
m
o
d
e
l
s
r
e
p
o
r
t
t
h
e
b
e
s
t
F
-
s
t
a
t
i
s
t
i
c
f
o
r
t
h
e
s
e
l
e
c
t
e
d
e
r
r
o
r
c
o
n
t
e
x
t
a
n
d
f
r
e
q
u
e
n
c
y
w
i
t
h
i
n
t
h
e
t
o
p
N
(
N
=
1
,
5
,
2
0
,
5
0
)
.
E
R
R
O
R
C
O
O
C
C
a
n
d
L
1
A
S
S
O
C
g
i
v
e
t
h
e
b
e
s
t
s
c
o
r
e
f
o
r
t
h
e
s
e
t
o
f
k
f
e
a
t
u
r
e
s
(
k
=
1
,
2
,
3
)
.
M
A
X
W
E
I
G
H
T
C
L
I
Q
U
E
-
L
1
a
l
s
o
n
o
t
e
s
t
h
e
l
a
n
g
u
a
g
e
g
r
a
p
h
w
i
t
h
t
h
e
b
e
s
t
r
e
s
u
l
t
.
61
error context example sentences
DJ JJ, NN1 Basically/RR ,/, I/PPIS1 helped/VVD them/PPHO2 liaise/VV0
with/IW the/AT local/JJ police/NN and/CC get/VV0 some/DD
<ns type="DJ"><i>electronical</i><c>electronic/JJ</c></ns> equipmen-
t/NN1 that/CST they/PPHS2 needed/VVD.
The/AT show/NN1 will/VM be/VB0 at/II the/AT Central/JJ
Exhibition/NN1 Hall/NP1 and/CC it/PPH1 will/VM be/VB0
<ns type="DJ"><i>opened</i><c>open/JJ</c></ns> until/ICS 7/MC.
UV TO_VV0_II,
NNL1, II, NN2,
VV0_II
I/PPIS1 used/VMK to/TO <ns type="UV"><i>be</i></ns> play/VV0 in/II the/AT
school/NNL1 team/NN1 . . . and/CC our/APP$ team/NN1 was/VBDZ one/MC1 of/IO the/AT
best/JJT basketball/NN1 teams/NN2 . . .
DN XX, XX_VV0,
VM_XX_VV0, NN1
Never/RR the/AT less/DAR ,/, in/II summer/NNT1 we/PPIS2 can/VM n?t/XX resist/VV0
such/DA <ns type="DN"><i>hot</i><c>heat/NN1</c></ns>!
. . . I/PPIS1 think/VV0 you/PPY should/VM have/VH0 a/AT1 <ns type="DN"><i>baby-
parking</i><c>kindergarten/NP1</c></ns> ,/, in/II fact/NN1 a/AT1 certain/JJ num-
ber/NN1 of/IO women/NN2 could/VM n?t/XX see/VV0 the/AT Festival/NN1 because/CS
of/IO their/APP$ sons/NN2.
MD VBZ_RG,
RG_JJ_NN1
The/AT first/MD and/CC most/RR important/JJ thing/NN1 is/VBZ that/RG modern/JJ
technology/NN1 has/VHZ made/VVN our/APP$ life/NN1 easier/JJR ,/, for/IF instance/NN1
<ns type="MD"><c>the/AT</c></ns> rice/NN1 cooker/NN1 is/VBZ a/AT1 great/JJ
invention/NN1 . . .
Figure 2: Examples for sample error types and specific error contexts. Error contexts are bolded.
connection.
The DN error with context { XX, XX_VV0,
VM_XX_VV0, NN1 } illustrates the top result found un-
der the MAXWEIGHTCLIQUE-L1 model for N = 50.
A number of this reasonably sized set are similar to the
first sentence, where the context appears interesting. In
this example, hot is used for heat; the other examples
of this type are from Spanish and Italian (similarly, e.g.,
live for life), where the error seems to be connected
to words where the English derivational morphology
is not simply affixation. However, there are some like
the second sentence, where (as for the DJ error) the
error context appears in a different clause, and likely
irrelevant.
The MD error in the last row we examine because (a
more complex version of) it was the focus of the case
study in Yannakoudakis et al. (2012), which from the
examples of that paper looked quite convincing as an
error context of relevance to SLA research. However, it
and the related examples of Yannakoudakis et al. (2012)
were not in the publicly available corpus,
13
and in fact
there is only one example of this error and context in the
whole FCE corpus, illustrating the issue of data sparsity.
Further, this example also illustrates the issue of tagging
error: that is tagged as RG (degree adverb) where it
should be CST.
So as might be anticipated from the frequency num-
bers in Table 5, the MAXWEIGHTCLIQUE-L1 model
produces context that looks interesting from an SLA per-
spective, but is relatively limited in scope; the ERROR-
COOCC model produces a much larger set of candidates,
and can successfully find error context such that they
behave differently with respect to the L1s according
to the ANOVA F-statistic, but produces false positives.
Overall, a recurring issue illustrated for all models by
13
We assume that the multiple examples come from the
larger CLC corpus.
the examples is the proposal of error context far away
from any likely relevance to SLA.
5 Related Work
While Native Language Identification (NLI) as a sub-
field of NLP has seen much new work in the last few
years ? the papers from the shared task (Tetreault et
al., 2013) provide a recent sample ? the emphasis on
optimising classification task results, for example by
using classifier ensembles (Malmasi et al., 2013), ver-
sus analysing features for relevance to other tasks has
varied. Below we discuss works which directly look
at how features might be related to language-learning
tasks or SLA research.
The seminal work of Koppel et al. (2005) that pre-
sented NLI as a classification task included, in addition
to standard lexical and PoS n-gram features, errors made
by the writers; these errors were automatically identi-
fied using Microsoft Word grammar checker. Kochmar
(2011) used the FCE corpus for NLI, including the man-
ually annotated errors as features, and presented an anal-
ysis of usefulness of features (including errors) with
respect to L1.
Wong and Dras (2011) used syntactic features on the
basis of SLA theory that posits that L1 constructions
may be reflected in some form of characteristic errors or
patterns in L2 constructions to some extent, or through
overuse or avoidance of particular constructions in L2
(Lado, 1957; Ellis, 2008); they did note distributional
differences of features related to L1. Wong et al. (2012)
induced topic models over function words and PoS n-
grams, where some of the topics appeared to reflect L1-
specific characteristics. These works, while interested
in the nature of the features, do not evaluate them except
via classification accuracy.
Swanson and Charniak (2012) similarly explore us-
ing syntax, where they propose a richer representation
62
for L1-specific constructions through Tree Substitution
Grammar (TSG). Swanson and Charniak (2013) sub-
sequently examine both relevancy and redundancy of
features through a number of metrics (including the
?
2
-statistic used in this paper). They then extend a
Bayesian induction model for TSG inference based on
a supervised mixture of hierarchical grammars, in order
to extract a filtered set of more linguistically informed
features that could benefit both NLI and SLA research;
an aim was to find relatively rare features that are nev-
ertheless useful for L1 prediction. Swanson and Char-
niak (2014) continue on from this with a data-driven
approach to inferring possible relationships between L1
and L2 structures, again using TSGs. Malmasi and Dras
(2014c) also propose a method for identifying potential
language transfer effects by using additional linguistic
features such as adaptor grammars and grammatical de-
pendencies to analyse differences in learner language.
This body of work thus shares some similarities with the
present paper, but our focus is on errors rather than on
the distributional differences, and we look at error con-
texts that may not constitute a TSG tree or grammatical
dependency.
Coming from a linguistic perspective, the works in
Jarvis and Crossley (2012) use Linear Discriminant
Analysis for classification of texts by L1, and identify
interesting features by a stepwise feature selection pro-
cess in the course of classification, rather than via the
measurement of their variability across L1s as here.
More recently, several of these NLI techniques have
been adapted and applied to languages other than En-
glish, such as Arabic and Chinese (Malmasi and Dras,
2014a; Malmasi and Dras, 2014b).
6 Conclusion
In this paper, prompted by work on using computa-
tional visualisation techniques to help SLA researchers
form hypotheses about errors and the environments in
which they are made, we have defined a new task for
finding interesting contexts for errors that vary with
the native language of the speaker. We proposed four
models, ranging from one based on simple error-feature
co-occurrence statistics to one based on the maximum
weighted clique on an L1-specific feature association
graph; these all managed to find contexts that were more
strongly associated with L1s than the raw errors alone,
and produced (albeit with many false positives in the
case of the simple model) some error contexts that look
potentially useful for SLA.
This paper is largely intended to prompt more work
on applying NLP techniques to SLA more broadly. As
such, there are many ways in which the work could be
further developed. First, to get rid of obviously incor-
rect cases, the size of the area over which the feature-
feature and feature-error scores are calculated could be
restricted, perhaps to the relevant clause or a certain
window size. Second, it may not be the case that the
ANOVA F-statistic or ?
2
are the best evaluation mea-
sure: in medical work, for example, there is the notion
of clinical significance, which takes effect size into ac-
count and is often more relevant to the practitioner than
statistical significance. Similarly, the current features
may not be the most meaningful. As part of this, an im-
portant step would be to bring in SLA researchers, to as-
sess proposed error contexts and look at what evaluation
measures best relate to this. The role of the present work
would then be to rule out models for producing error
contexts (like L1ASSOC) that produce weaker results in
hypothesis testing: it would thus be complementary to
the visualisation work from which it stems, guiding SLA
researchers away from unproductive areas of the space
of possible hypotheses. And third, the size of the corpus
is (as always) an issue: as these error-annotated corpora
are few and far between, a semi-supervised approach
or one that in some way incorporated unannotated data
would be useful, perhaps using some of the extensive
recent work on error annotation.
References
Immanuel M. Bomze, Marco Budinich, Panos Parda-
los, and Marcello Pelillo. 1999. The Maximum
Clique Problem. In D.-Z. Du and P. M. Pardalos, edi-
tors, Handbook of Combinatorial Optimization (supp.
Vol. A), pages 1?74. Kluwer Academic, Dordrecht,
Netherlands.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proc. of
the COLING/ACL Interactive Presentation Sessions,
pages 77?80, Stroudsburg, PA, USA.
Ted Briscoe, Ben Medlock, and ?istein Andersen. 2010.
Automated Assessment of ESOL Free Text Exami-
nations. Technical Report TR-790, University of
Cambridge, Computer Laboratory.
Mar??a Bel?en Di?ez-Bedmar and Szilvia Papp. 2008. The
use of the English article system by Chinese and Span-
ish learners. Language and Computers, 66(1):147?
176.
Rod Ellis. 2008. The Study of Second Language Acqui-
sition, 2nd edition. Oxford University Press, Oxford,
UK.
Sylviane Granger. 2011. How to Use Foreign and Sec-
ond Language Learner Corpora. In Alison Mackey
and Susan M. Gass, editors, Research Methods in
Second Language Acquisition: A Practical Guide.
Wiley-Blackwell.
Tania Ionin and Silvina Montrul. 2010. The role of L1
transfer in the interpretation of articles with definite
plurals. Language Learning, 60(4):877?925.
Sherri L. Jackson. 2009. Statistics: Plain and Simple.
Wadsworth, Cengage Learning, Belmont, CA, US.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach. Multilingual Matters, Bristol, UK.
63
Ekaterina Kochmar. 2011. Identification of a writer?s
native language by error analysis. MPhil thesis, Uni-
versity of Cambridge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author?s
native language. In Intelligence and Security In-
formatics, volume 3495 of LNCS, pages 209?217.
Springer-Verlag.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. Univ. of
Michigan Press, Ann Arbor, MI, US.
James P. Lantolf. 2001. Sociocultural Theory and
Second Language Learning. Oxford University Press,
Oxford, UK.
Shervin Malmasi and Mark Dras. 2014a. Arabic Na-
tive Language Identification. In Proceedings of the
Arabic Natural Language Processing Workshop (co-
located with EMNLP 2014), Doha, Qatar, October.
Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. Chinese Na-
tive Language Identification. Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014c. Language
Transfer Hypotheses with Linear SVM Weights. Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras.
2013. NLI Shared Task 2013: MQ Submission. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 124?133, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lourdes Ortega. 2009. Understanding Second Lan-
guage Acquisition. Hodder Education, Oxford, UK.
Patric
?
Osterg?ard. 1999. A New Algorithm for the
Maximum-Weight Clique Problem. Electronic Notes
in Discrete Mathematics, 3:153?156, May.
Betsy Parrish. 1987. A New Look at Methodologies in
the Study of Article Acquisition for Learners of ESL.
Language Learning, 37(3):361?384.
Manfred Pienemann. 2005. Cross-linguistic Aspects of
Processability Theory. John Benjamins, Amsterdam,
Netherlands.
Benjamin Swanson and Eugene Charniak. 2012. Native
Language Detection with Tree Substitution Gram-
mars. In Proc. Meeting Assoc. Computat. Linguistics
(ACL), pages 193?197.
Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language ac-
quisition. In Proc. Conf. North American Assoc. for
Computat. Linguistics: Human Language Technolo-
gies (NAACL-HLT), pages 85?94, Atlanta, Georgia,
June.
Ben Swanson and Eugene Charniak. 2014. Data Driven
Language Transfer Hypotheses. In Proc. Conf. Euro-
pean Assoc. for Computat. Linguistics (EACL), pages
169?173, Gothenburg, Sweden, April.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications (BEA), pages 48?57, Atlanta,
Georgia, June.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. Conf. Empirical Methods in Natural Language
Processing (EMNLP), pages 1600?1610.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proc. Conf. Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
699?709.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In Proc. Meeting Assoc. Com-
putat. Linguistics (ACL), pages 180?189.
Helen Yannakoudakis, Ted Briscoe, and Theodora Alex-
opoulou. 2012. Automating Second Language Ac-
quisition Research: Integrating Information Visualisa-
tion and Machine Learning. In Proc. EACL Workshop
of LINGVIS & UNCLH, pages 35?43.
Richard Young. 1996. Form-Function Relations in Arti-
cles in English Interlanguage. In R. Bayley and D. R.
Preston, editors, Second Language Acquisition and
Linguistic Variation, pages 135?175. John Benjamins,
Amsterdam, The Netherlands.
64
Proceedings of the First Celtic Language Technology Workshop, pages 41?49,
Dublin, Ireland, August 23 2014.
Cross-lingual Transfer Parsing for Low-Resourced Languages: An Irish
Case Study
Teresa Lynn
1,2
, Jennifer Foster
1
, Mark Dras
2
and Lamia Tounsi
1
1
CNGL, School of Computing, Dublin City University, Ireland
2
Department of Computing, Macquarie University, Sydney, Australia
1
{tlynn,jfoster,ltounsi}@computing.dcu.ie
2
{teresa.lynn,mark.dras}@mq.edu.au
Abstract
We present a study of cross-lingual direct transfer parsing for the Irish language. Firstly we
discuss mapping of the annotation scheme of the Irish Dependency Treebank to a universal de-
pendency scheme. We explain our dependency label mapping choices and the structural changes
required in the Irish Dependency Treebank. We then experiment with the universally annotated
treebanks of ten languages from four language family groups to assess which languages are the
most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised pars-
ing models which are then applied to sentences from the Irish Dependency Treebank. The best
results are achieved when using Indonesian, a language from the Austronesian language family.
1 Introduction
Considerable efforts have been made over the past decade to develop natural language processing re-
sources for the Irish language (U?? Dhonnchadha et al., 2003; U?? Dhonnchadha and van Genabith, 2006;
U?? Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource
is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard de-
pendency parse trees. These trees are labelled with deep syntactic information, marking grammatical
roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not
compare in size to similar resources of other languages.
1
The small size of the treebank affects the accu-
racy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate
whether training data from other languages can be successfully utilised to improve Irish parsing.
Cross-lingual transfer parsing involves training a parser on one language, and parsing data of another
language. McDonald et al. (2011) describe two types of cross-lingual parsing, direct transfer parsing in
which a delexicalised version of the source language treebank is used to train a parsing model which
is then used to parse the target language, and a more complicated projected transfer approach in which
the direct transfer approach is used to seed a parsing model which is then trained to obey source-target
constraints learned from a parallel corpus. These experiments revealed that languages that were typo-
logically similar were not necessarily the best source-target pairs, sometimes due to variations between
their language-specific annotation schemes. In more recent work, however, McDonald et al. (2013) re-
ported improved results on cross-lingual direct transfer parsing using a universal annotation scheme, to
which six chosen treebanks are mapped for uniformity purposes. Underlying the experiments with this
new annotation scheme is the universal part-of-speech (POS) tagset designed by Petrov et al. (2012).
While their results confirm that parsers trained on data from languages in the same language group (e.g.
Romance and Germanic) show the most accurate results, they also show that training data taken across
language-groups also produces promising results. We attempt to apply the direct transfer approach with
Irish as the target language.
The Irish language belongs to the Celtic branch of the Indo-European language family. The natural
first step in cross-lingual parsing for Irish would be to look to those languages of the Celtic language
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
For example, the Danish dependency treebank has 5,540 trees (Kromann, 2003); the Finnish dependency treebank has
15,126 trees (Haverinen et al., 2013)
41
group, i.e. Welsh, Scots Gaelic, Manx, Breton and Cornish, as a source of training data. However,
these languages are just as, if not further, under-resourced. Thus, we attempt to use the languages of the
universal dependency treebanks (McDonald et al., 2013).
The paper is organised as follows. In Section 2, we give an overview of the status of the Irish lan-
guage and the Irish Dependency Treebank. Section 3 describes the mapping of the Irish Dependency
Treebank?s POS tagset (U?? Dhonnchadha and van Genabith, 2006) to that of Petrov et al. (2012), and
the Irish Dependency Treebank annotation scheme (Lynn et al. (2012b)) to the Universal Dependency
Scheme. Following that, in Section 4 we carry out cross-lingual direct transfer parsing experiments with
ten harmonised treebanks to assess whether any of these languages are suitable for such parsing transfer
for Irish. Section 5 summarises our work.
2 Irish Language and Treebank
Irish, a minority EU language, is the national and official language of Ireland. Despite this status, Irish
is only spoken on a daily basis by a minority. As a Celtic language, Irish shares specific linguistic
features with other Celtic languages, such as a VSO (verb-subject-object) word order and interesting
morphological features such as inflected prepositions and initial mutations, for example.
Compared to other EU-official languages, Irish language technology is under-resourced, as highlighted
by a recent study (Judge et al., 2012). In the area of morpho-syntactic processing, recent years have seen
the development of a part-of-speech tagger (U?? Dhonnchadha and van Genabith, 2006), a morphological
analyser (U?? Dhonnchadha et al., 2003), a shallow chunker (U?? Dhonnchadha, 2009), a dependency tree-
bank (Lynn et al., 2012a; Lynn et al., 2012b) and statistical dependency parsing models for MaltParser
(Nivre et al., 2006) and Mate parser (Bohnet, 2010) trained on this treebank (Lynn et al., 2013).
The annotation scheme for the Irish Dependency Treebank (Lynn et al., 2012b) was inspired by Lexical
Functional Grammar (Bresnan, 2001) and has its roots in the dependency annotation scheme described
by C?etino?glu et al. (2010). It was extended and adapted to suit the linguistic characterisics of the Irish
language. The final label set consists of 47 dependency labels, defining grammatical and functional
relations between the words in a sentence. The label set is hierarchical in nature with labels such as
vparticle (verb particle) and vocparticle (vocative particle), for example, representing more
fine-grained versions of the particle label.
3 A universal dependency scheme for the Irish Dependency Treebank
In this section, we describe how a ?universal? version of the Irish Dependency Treebank was created by
mapping the original POS tags to universal POS tags and mapping the original dependency scheme to the
universal dependency scheme. The result of this effort is an alternative version of the Irish Dependency
Treebank which will be made available to the research community along with the original.
3.1 Mapping the Irish POS tagset to the Universal POS tagset
The Universal POS tagset (Petrov et al., 2012) has been designed to facilitate unsupervised and cross-
lingual part-of-speech tagging and parsing research, by simplifying POS tagsets and unifying them across
languages. The Irish Dependency Treebank was built upon a POS-tagged corpus developed by U?? Dhon-
nchadha and van Genabith (2006). The treebank?s tagset contains both coarse- and fine-grained POS tags
which we map to the Universal POS tags (e.g. Prop Noun? NOUN). Table 1 shows the mappings.
Most of the POS mappings made from the Irish POS tagset to the universal tagset are intuitive. How-
ever, some decisions require explanation.
Cop ? VERB There are two verbs ?to be? in Irish: the substantive verb b?? and the copula is. For
that reason, the Irish POS tagset differentiates the copula by using the POS tag Cop. In Irish syntax
literature, there is some discussion over its syntactic role, whether it is a verb or a linking particle. The
role normally played is that of a linking element between a subject and a predicate. However, Lynn et al.
(2012a)?s syntactic analysis of the copula is in line with that of Stenson (1981), regarding it as a verb. In
addition, because the copula is often labelled in the Irish annotation scheme as the syntactic head of the
matrix clause, we have chosen VERB as the most suitable mapping for this part of speech.
42
Part-of-speech (POS) mappings
Universal Irish Universal Irish
NOUN
Noun Noun, Pron Ref,
Subst Subst, Verbal Noun,
Prop Noun
ADP
Prep Deg, Prep Det, Prep Pron,
Prep Simp, Prep Poss,
Prep CmpdNoGen, Prep Cmpd,
Prep Art, Pron Prep
PRON
Pron Pers, Pron Idf, Pron Q,
Pron Dem
ADV
Adv Temp, Adv Loc, Adv Dir,
Adv Q, Adv Its, Adv Gn
VERB
Cop Cop, Verb PastInd, Verb PresInd,
Verb PresImp, Verb VI, Verb VT,
Verb VTI, Verb PastImp, Verb Cond,
Verb FutInd, Verb VD, Verb Imper
PRT
Part Vb, Part Sup, Part Inf, Part Pat,
Part Voc, Part Ad, Part Deg, Part Comp,
Part Rel, Part Num, Part Cp,
DET Art Art, Det Det NUM Num Num
ADJ Prop Adj, Verbal Adj, Adj Adj X
Item Item, Abr Abr, CM CM, CU CU,
CC CC, Unknown Unknown,
Guess Abr, Itj Itj, Foreign Foreign,
CONJ Conj Coord, Conj Subord . . . ... ... ? ? ! ! : : ? . Punct Punct
Table 1: Mapping of Irish Coarse and Fine-grained POS pairs (coarse fine) to Universal POS tagset.
Pron Prep?ADP Pron Prep is the Irish POS tag for pronominal prepositions, which are also referred
to as prepositional pronouns. Characteristic of Celtic languages, they are prepositions inflected with their
pronominal objects ? compare, for example, le mo chara ?with my friend? with leis ?with him?. While
the Irish POS labelling scheme labels them as pronouns in the first instance, our dependency labelling
scheme treats the relationship between them and their syntactic heads as obl (obliques) or padjunct
(prepositional adjuncts). Therefore, we map them to ADP (adpositions).
3.2 Mapping the Irish Dependency Scheme to the Universal Dependency Scheme
The departure point for the design of the Universal Dependency Annotation Scheme (McDonald et
al., 2013) was the Stanford typed dependency scheme (de Marneffe and Manning, 2008), which was
adapted based on a cross-lingual analysis of six languages: English, French, German, Korean, Spanish
and Swedish. Existing English and Swedish treebanks were automatically mapped to the new universal
scheme. The rest of the treebanks were developed manually to ensure consistency in annotation. The
study also reports some structural changes (e.g. Swedish treebank coordination structures).
2
There are 41 dependency relation labels to choose from in the universal annotation scheme
3
. McDon-
ald et al. (2013) use all labels in the annotation of the German and English treebanks. The remaining
languages use varying subsets of the label set. In our study we map the Irish dependency annotation
scheme to 30 of the universal labels. The mappings are given in Table 2.
As with the POS mapping discussed in Section 3.1, mapping the Irish dependency scheme to the
universal scheme was relatively straightforward, due in part, perhaps, to a similar level of granularity
suggested by the similar label set sizes (Irish 47; standard universal 41). That said, there were significant
considerations made in the mapping process, which involved some structural change in the treebank and
the introduction of more specific analyses in the labelling scheme. These are discussed below.
3.2.1 Structural Differences
The following structural changes were made manually before the dependency labels were mapped to the
universal scheme.
coordination The most significant structural change made to the treebank was an adjustment to the
analysis of coordination. The original Irish Dependency Treebank subscribes to the LFG coordination
analysis, where the coordinating conjunction (e.g. agus ?and?) is the head, with the coordinates as its
dependents, labelled coord (see Figure 1). The Universal Dependency Annotation scheme, on the
2
There are two versions of the annotation scheme: the standard version (where copulas and adpositions are syntactic heads),
and the content-head version which treats content words as syntactic heads. We are using the standard version for our study.
3
The vmod label is used only in the content-head version.
43
Dependency Label Mappings
Universal Irish Universal Irish
root top csubj csubj
acomp adjpred, advpred, ppred dep for
adpcomp N/A det det, det2, dem
adpmod padjunct, obl, obl2, obl ag dobj obj, vnobj, obj q
adpobj pobj mark subadjunct
advcl N/A nmod addr, nadjunct
advmod
adjunct, advadjunct, quant,
advadjunct q
nsubj subj, subj q
amod adjadjunct num N/A
appos app p punctuation
attr npred parataxis N/A
aux toinfinitive poss poss
cc N/A prt
particle, vparticle, nparticle, advparticle,
vocparticle, particlehead, cleftparticle,
qparticle, aug
ccomp comp rcmod relmod
compmod nadjunct rel relparticle
conj coord xcomp xcomp
Table 2: Mapping of Irish Dependency Annotation Scheme to Universal Dependency Annotation Scheme
other hand, uses right-adjunction, where the first coordinate is the head of the coordination, and the
rest of the phrase is adjoined to the right, labelling coordinating conjunctions as cc and the following
coordinates as conj (Figure 2).
coord det subj advpred top coord det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 1: LFG-style coordination of original Irish Dependency Treebank
top det subj advpred cc conj det subj advpred obl det pobj
Bh?? an l?a an-te agus bh?? gach duine sti?ugtha leis an tart
Be-PAST the day very-hot and be-PAST every person parched with the thirst
?The day was very hot and everyone was parched with the thirst?
Figure 2: Stanford-style coordination changes to original Irish Dependency Treebank
subordinate clauses In the original Irish Dependency Treebank, the link between a matrix clause and
its subordinate clause is similar to that of LFG: the subordinating conjunction (e.g. mar ?because?, nuair
?when?) is a subadjunct dependent of the matrix verb, and the head of the subordinate clause is a
comp dependent of the subordinating conjunction (Figure 3). In contrast, the universal scheme is in
line with the Stanford analysis of subordinate clauses, where the head of the clause is dependent on the
matrix verb, and the subordinating conjunction is a dependent of the clause head (Figure 4).
3.2.2 Differences between dependency types
We found that the original Irish scheme makes distinctions that the universal scheme does not ? this
finer-grained information takes the form of the following Irish-specific dependency types: advpred,
44
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 3: LFG-style subordinate clause analysis (with original Irish Dependency labels)
top subj xcomp obl det pobj adjadjunct subadjunct comp subj ppred pobj num
Caithfidh t?u brath ar na himreoir?? ?aiti?ula nuair at?a t?u i Roinn 1
Have-to-FUT you rely on the players local when REL-be-PRES you in Division 1
?You have to rely on the local players when you are in Division 1?
Figure 4: Stanford-style subordinate clause analysis (with original Irish Dependency labels)
ppred, subj q, obj q, advadjunct q, obl, obl2. In producing the universal version of the tree-
bank, these Irish-specific dependency types are mapped to less informative universal ones (see Table 2).
Conversely, we found that the universal scheme makes distinctions that the Irish scheme does not. Some
of these dependency types are not needed for Irish. For example, there is no indirect object iobj in Irish,
nor is there a passive construction that would require nsubjpass, csubjpass or auxpass. Also, in
the Irish Dependency Treebank, the copula is usually the root (top) or the head of a subordinate clause
(e.g. comp) which renders the universal type cop redundant. Others that are not used are adp, expl,
infmod, mwe, neg, partmod. However, we did identify some dependency relationships in the univer-
sal scheme that we introduce to the universal Irish Dependency Treebank (adpcomp, adposition,
advcl, num, parataxis). These are explained below.
comp? adpcomp, advcl, parataxis, ccomp The following new mappings were previously subsumed
by the Irish dependency label comp (complement clause). The mapping for comp has thus been split
between adpcomp, advcl, parataxis and ccomp.
? adpcomp is a clausal complement of an adposition. An example from the English data is ?some
understanding of what the company?s long-term horizon should begin to look like?, where ?begin?,
as the head of the clause, is a dependent of the preposition ?of?. An example of how we use this
label in Irish is: an l??ne l?antosach is m?o cl?u a th?ainig as Ciarra?? ?o bh?? aimsir Sheehy ann ?the most
renowned forward line to come out of Kerry since Sheehy?s time? (lit. ?from it was Sheehy?s time?).
The verb bh?? ?was?, head of the dependent clause, is an adcomp dependent of the preposition ?o.
? advcl is used to identify adverbial clause modifiers. In the English data, they are often introduced
by subordinating conjunctions such as ?when?, ?because?, ?although?, ?after?, ?however?, etc. An
example is ?However, because the guaranteed circulation base is being lowered, ad rates will be
higher?. Here, ?lowered? is a advcl dependent of ?will?. An example of usage is: T?a truailli?u m?or
san ?ait mar nach bhfuil c?oras s?earachais ann ?There is a lot of pollution in the area because there
is no sewerage system?, where bhfuil ?is? is an advcl dependent of T?a ?is?.
45
? parataxis labels clausal structures that are separated from the previous clause with punctuation
such as ? ... : () ; and so on. Examples in Irish Is l?eir go bhfuil ag ?eir?? le feachtas an IDA ?
meastar gur in
?
Eirinn a lonna??tear timpeall 30% de na hionaid ?It is clear that the IDA campaign is
succeeding ? it is believed that 30% of the centres are based in Ireland?. Here, meastar ?is believed?
is a parataxis dependent of Is ?is?.
? ccomp covers all other types of clausal complements. For example, in English, ?Mr. Amos says the
Show-Crier team will probably do two live interviews a day?. The head of the complement clause
here is ?do?, which is a comp dependent of the matrix verb ?says?. A similar Irish example is: D?uirt
siad nach bhfeiceann siad an cine?al seo chomh minic ?They said that they don?t see this type as
often?. Here, bhfeiceann ?see? is the head of the complement clause, which is a comp dependent of
the verb D?uirt ?Said?.
quant? num, advmod The Irish Dependency Scheme uses one dependency label (quant) to cover
all types of numerals and quantifiers. We now use the universal scheme to differentiate between quanti-
fiers such as m?or?an ?many? and numerals such as fiche ?twenty?.
nadjunct? nmod, compmod The Irish dependency label nadjunct accounts for all nominal mod-
ifiers. However, in order to map to the universal scheme, we discriminate two kinds: (i) nouns that mod-
ify nouns (usually genitive case in Irish) are mapped to compmod (e.g. plean marga??ochta ?marketing
plan?) and (ii) nouns that modify clauses are mapped to nmod (e.g. bliain ?o shin ?a year ago?).
4 Parsing Experiments
We now describe how we extend the direct transfer experiments described in McDonald et al. (2013)
to Irish. In Section 4.1, we describe the datasets used in our experiments and explain the experimental
design. In Section 4.2, we present the results, which we then discuss in Section 4.3.
4.1 Data and Experimental Setup
We present the datasets used in our experiments and explain how they are used. Irish is the target
language for all our parsing experiments.
Universal Irish Dependency Treebank This is the universal version of the Irish Dependency Treebank
which contains 1020 gold-standard trees, which have been mapped to the Universal POS tagset and
Universal Dependency Annotation Scheme, as described in Section 3. In order to establish a monolingual
baseline against which to compare our cross-lingual results, we perform a five-fold cross-validation by
dividing the full data set into five non-overlapping training/test sets. We also test our cross-lingual models
on an delexicalised version of this treebank.
Transfer source training data For our direct transfer cross-lingual parsing experiments, we use 10 of
the standard version harmonised training data sets
4
made available by McDonald et al. (2013): Brazilian
Portuguese (PT-BR), English (EN), French (FR), German (DE), Indonesian (ID), Italian (IT), Japanese
(JA), Korean (KO), Spanish (ES) and Swedish (SV). For the purposes of uniformity, we select the first
4447 trees from each treebank ? to match the number of trees in the smallest data set (Swedish). We
delexicalise all treebanks and use the universal POS tags as both the coarse- and fine-grained values.
5
We train a parser on all 10 source data sets outlined and use each induced parsing model to parse and test
on a delexicalised version of the Universal Irish Dependency Treebank.
Largest transfer source training data - Universal English Dependency Treebank English has the
largest source training data set (sections 2-21 of the Wall Street Journal data in the Penn Treebank (Mar-
cus et al., 1993) contains 39, 832 trees). As with the smaller transfer datasets, we delexicalise this dataset
and use the universal POS tag values only. We experiment with this larger training set in order to establish
whether more training data helps in a cross-lingual setting.
4
Version 2 data sets downloaded from https://code.google.com/p/uni-dep-tb/
5
Note that the downloaded treebanks had some fine-grained POS tags that were not used across all languages: e.g. VERB-
VPRT (Spanish), CD (English).
46
Parser and Evaluation Metrics We use a transition-based dependency parsing system, MaltParser
(Nivre et al., 2006) for all of our experiments. All our models are trained using the stacklazy algorithm,
which can handle the non-projective trees present in the Irish data. In each case we report Labelled
Attachment Score (LAS) and Unlabelled Attachment Score (UAS).
6
4.2 Results
All cross-lingual results are presented in Table 3. Note that when we train and test on Irish (our mono-
lingual baseline), we achieve an average accuracy of 78.54% (UAS) and 71.59% (LAS) over the five
cross-validation runs. The cross-lingual results are substantially lower than this baseline. The LAS
results range from 0.84 (JA) to 43.88 (ID) and the UAS from 16.74 (JA) to 61.69 (ID).
SingleT MultiT LargestT
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
UAS 51.72 56.84 49.21 61.69 50.98 16.74 18.02 57.31 57.00 49.95 57.69 51.59
LAS 35.03 37.91 33.04 43.88 37.98 0.84 9.35 42.13 41.94 34.02 41.38 33.97
Experiment SingleT-30 MultiT-30 LargestT-30
Training EN FR DE ID IT JA KO PT-BR ES SV All EN
Avg sent len 23 24 16 21 21 9 11 24 26 14 19 23
UAS 55.97 60.98 53.42 64.86 54.47 16.88 19.27 60.47 60.53 54.40 61.40 55.54
LAS 38.42 41.44 36.24 46.45 40.56 1.19 10.08 45.04 45.23 37.76 44.63 37.08
Table 3: Multi-lingual transfer parsing results
A closer look at the single-source transfer parsing evaluation results (SingleT) shows that some lan-
guage sources are particularly strong for parsing accuracy of certain labels. For example, ROOT (for
Indonesian), adpobj (for French) and amod (for Spanish). In response to these varied results, we ex-
plore the possibility of combining the strengths of all the source languages (multi-source direct transfer
(MultiT) ? also implemented by McDonald et al. (2011)). A parser is trained on a concatenation of
all the delexicalised source data described in Section 4.1 and tested on the full delexicalised Universal
Irish Dependency Treebank. Combining all source data produces parsing results of 57.69% (UAS) and
41.38% (LAS), which is outperformed by the best individual source language model.
Parsing with the large English training set (LargestT) yielded results of 51.59 (UAS) and 33.97 (LAS)
compared to a UAS/LAS of 51.72/35.05 for the smaller English training set. We investigated more
closely why the larger training set did not improve performance by incrementally adding training sen-
tences to the smaller set ? none of these increments reveal any higher scores, suggesting that English is
not a suitable source training language for Irish.
It is well known that sentence length has a negative effect on parsing accuracy. As noted in earlier
experiments (Lynn et al., 2012b), the Irish Dependency Treebank contains some very long difficult-to-
parse sentences (some legal text exceeds 300 tokens in length). The average sentence length is 27 tokens.
By placing a 30-token limit on the Universal Irish Dependency Treebank we are left with 778 sentences,
with an average sentence length of 14. We use this new 30-token-limit version of the Irish Dependency
Treebank data to test our parsing models. The results are shown in the lower half of Table 3. Not
surprisingly, the results rise substantially for all models.
4.3 Discussion
McDonald et al. (2013)?s single-source transfer parsing results show that languages within the same
language groups make good source-target pairs. They also show reasonable accuracy of source-target
pairing across language groups. For instance, the baseline when parsing French is 81.44 (UAS) and 73.37
(LAS), while the transfer results obtained using an English treebank are 70.14 (UAS) and 58.20(LAS).
Our baseline parser for Irish yields results of 78.54 (UAS) and 71.59 (LAS), while Indonesian-Irish
transfer results are 61.69 (UAS) and 43.88 (LAS).
The lowest scoring source language is Japanese. This parsing model?s output shows less than 3%
accuracy when identifying the ROOT label. This suggests the effect that the divergent word orders have
6
All scores are micro-averaged.
47
on this type of cross-lingual parsing ? VSO (Irish) vs SOV (Japanese). Another factor that is likely to be
playing a role is the size of the Japanese sentences. The average sentence length in the Japanese training
data is only 9 words, which means that this dataset is comparatively smaller than the others. It is also
worth noting that the universal Japanese treebank uses only 15 of the 41 universal labels (the universal
Irish treebank uses 30 of these labels).
As our best performing model (Indonesian) is an Austronesian language, we investigate why this
language does better when compared to Indo-European languages. We compare the results obtained by
the Indonesian parser with those of the English parser (SingleT). Firstly, we note that the Indonesian
parser captures nominal modification much better than English, resulting in an increased precision-recall
score of 60/67 on compmod. This highlights that the similarities in noun-noun modification between
Irish and Indonesian helps cross-lingual parsing. In both languages the modifying noun directly follows
the head noun, e.g. ?the statue of the hero? translates in Irish as dealbh an laoich (lit. statue the hero);
in Indonesian as patung palawan (lit. statue hero). Secondly, our analysis shows that the English parser
does not capture long-distance dependencies as well as the Indonesian parser. For example, we have
observed an increased difference in precision-recall of 44%-44% on mark, 12%-17.88% on cc and
4%-23.17% on rcmod when training on Indonesian. Similar differences have also been observed when
we compare with the French and English (LargestT) parsers. The Irish language allows for the use
of multiple conjoined structures within a sentence and it appears that long-distance dependencies can
affect cross-lingual parsing. Indeed, excluding very long sentences from the test set reveals substantial
increases in precision-recall scores for labels such as advcl, cc, conj and ccomp ? all of which are
labels associated with long-distance dependencies.
With this study, we had hoped that we would be able to identify a way to bootstrap the development
of the Irish Dependency Treebank and parser through the use of delexicalised treebanks annotated with
the Universal Annotation Scheme. While the current treebank data might capture certain linguistic phe-
nomena well, we expected that some cross-linguistic regularities could be taken advantage of. Although
the best cross-lingual model failed to outperform the monolingual model, perhaps it might be possible to
combine the strengths of the Indonesian and Irish treebanks? We performed 5-fold cross-validation on
the combined Indonesian and Irish data sets. The results did not improve over the Irish model. We then
analysed the extent of their complementarity by counting the number of sentences where the Indonesian
model outperformed the Irish model. This happened in only 20 cases, suggesting that there is no benefit
in using the Indonesian data over the Irish data nor in combining them at the sentence-level.
5 Conclusion and Future Work
In this paper, we have reported an implementation of cross-lingual direct transfer parsing of the Irish
language. We have also presented and explained our mapping of the Irish Dependency Treebank to the
Universal POS tagset and Universal Annotation Scheme. Our parsing results show that an Austronesian
language surpasses Indo-European languages as source data for cross-lingual Irish parsing.
In extending this research, there are many interesting avenues which could be explored including
the use of Irish as a source language for another Celtic language and experimenting with the projected
transfer approach of McDonald et al. (2011).
Acknowledgements
This research is supported by the Science Foundation Ireland (Grant 12/CE/I2267) as part of the CNGL
(www.cngl.ie) at Dublin City University. We thank the three anonymous reviewers for their helpful
feedback. We also thank Elaine U?? Dhonnchadha (Trinity College Dublin) and Brian
?
O Raghallaigh
(Fiontar, Dublin City University) for their linguistic advice.
References
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COL-
ING?10.
48
Joan Bresnan. 2001. Lexical Functional Syntax. Oxford: Blackwell.
?
Ozlem C?etino?glu, Jennifer Foster, Joakim Nivre, Deirdre Hogan, Aoife Cahill, and Josef van Genabith. 2010. LFG
without C-structures. In Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Workshop on Crossframework and Cross-domain Parser Evaluation (COLING2008).
Katri Haverinen, Jenna Nyblom, Timo Viljanen, Veronika Laippala, Samuel Kohonen, Anna Missil?a, Stina Ojala,
Tapio Salakoski, and Filip Ginter. 2013. Building the essential resources for Finnish: the Turku dependency
treebank. Language Resources and Evaluation, pages 1?39.
John Judge, Ailbhe N?? Chasaide, Rose N?? Dhubhda, Kevin P. Scannell, and Elaine U?? Dhonnchadha. 2012. The
Irish Language in the Digital Age. Springer Publishing Company, Incorporated.
Matthias Kromann. 2003. The Danish Dependency Treebank and the DTAG Treebank Tool. In Proceedings of
the Second Workshop on Treebanks and Linguistic Theories (TLT2003).
Teresa Lynn,
?
Ozlem C?etino?glu, Jennifer Foster, Elaine U?? Dhonnchadha, Mark Dras, and Josef van Genabith.
2012a. Irish treebanking and parsing: A preliminary evaluation. In Proceedings of the Eight International
Conference on Language Resources and Evaluation (LREC?12), pages 1939?1946.
Teresa Lynn, Jennifer Foster, Mark Dras, and Elaine U?? Dhonnchadha. 2012b. Active learning and the Irish
treebank. In Proceedings of the Australasian Language Technology Workshop (ALTA), pages 23?32.
Teresa Lynn, Jennifer Foster, and Mark Dras. 2013. Working with a small dataset ? semi-supervised depen-
dency parsing for Irish. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich
Languages, pages 1?11, Seattle, Washington, USA, October. Association for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The Penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
62?72, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu, and Castell?o Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of ACL ?13.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for depen-
dency parsing. In Proceedings of the Fifth International Conference on Language Resources and Evaluation
(LREC2006).
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the
Eight International Conference on Language Resources and Evaluation (LREC?12).
Nancy Stenson. 1981. Studies in Irish Syntax. T?ubingen: Gunter Narr Verlag.
Elaine U?? Dhonnchadha and Josef van Genabith. 2006. A part-of-speech tagger for Irish using finite-state morphol-
ogy and constraint grammar disambiguation. In Proceedings of the 5th International Conference on Language
Resources and Evaluation (LREC 2006).
Elaine U?? Dhonnchadha, Caoilfhionn Nic Ph?aid??n, and Josef van Genabith. 2003. Design, implementation and
evaluation of an inflectional morphology finite state transducer for Irish. Machine Translation, 18:173?193.
Elaine U?? Dhonnchadha. 2009. Part-of-Speech Tagging and Partial Parsing for Irish using Finite-State Transduc-
ers and Constraint Grammar. Ph.D. thesis, Dublin City University.
49
