A linguistic and navigational knowledge approach to text navigation 
Javier Couto 
Facultad de Ingenier?a - UdelaR 
Julio Herrera y Reissig 565 
11300 ? Montevideo ? Uruguay 
jcouto@fing.edu.uy 
Jean-Luc Minel 
MoDyCO, UMR 7114 CNRS ? Univ. Paris X 
200, avenue de la R?publique 
92 001 ? Nanterre ? France 
jminel@u-paris10.fr 
Abstract 
We present an approach to text navigation 
conceived as a cognitive process exploiting 
linguistic information present in texts. We 
claim that the navigational knowledge in-
volved in this process can be modeled in a 
declarative way with the Sextant language. 
Since Sextant refers exhaustively to specific 
linguistic phenomena, we have defined a 
customized text representation. These dif-
ferent components are implemented in the 
text navigation system NaviTexte. Two ap-
plications of NaviTexte are described. 
1 Introduction 
Text navigation has several interpretations. Usu-
ally, this term refers to hypertext systems, which 
offer the possibility to activate hyperlinks, moving 
the reading point from a text unit (source) to an-
other one (target), this change being intra or inter-
textual. From our point of view, this conception 
presents some limitations. First, the hyperlink acti-
vation is not assisted. In other words, imprecise, 
poor or no information is provided to the reader 
before s/he activates the link. Second, the reader 
does not know where the movement will be carried 
out in the text (before or after the reading point or 
outside the text), which generates the ?lost in 
hyperspace? problem (Edwards and Hardman 
1989). Finally, hyperlinks are embedded in the hy-
pertext. Therefore, there is no clearly distinction 
between text constituents and navigation knowl-
edge. In addition, by not explicitly modeling this 
knowledge, it is not reusable. 
Different solutions have been proposed to ad-
dress the problems mentioned. Some researchers 
(Danielson, 2002) have tried to mitigate the lost in 
hyperspace problem offering global maps where 
the reading point is clearly identified. Adaptive 
hypertext (Mathe and Chen, 1994; Brusilovsky, 
1996) relying on user model, proposes to modify 
the way the text is shown on the screen. Dynamic 
hypertext (Bodner and Chignell, 1999) computes 
the value of hyperlinks using several criteria such 
as text similarity or predefined relations. In this 
approach, a hyperlink is defined as a query return-
ing a text node. 
In some way, our conception of text navigation 
is related to the notion of computed query, but 
rather than taking into account criteria depending 
on the reader, the target is computed by exploiting 
linguistic information in texts. Moreover, the que-
ries are not placed in texts but they are encapsu-
lated as knowledge by a specific language (Sex-
tant), which allows isolating the navigational 
knowledge to create knowledge bases. Both texts 
and queries (navigational knowledge) are inter-
preted by NaviTexte, which manages the interac-
tions with a reader. 
The remainder of this paper is organized as fol-
lows. In the next section, we discuss our approach 
to text navigation. The third section describes a 
navigational knowledge modeling language called 
Sextant. The fourth section details the text naviga-
tion system NaviTexte. The fifth section describes 
two applications of NaviTexte. Then we address 
the evaluation aspects of our approach. At last, 
conclusions are presented. 
2 Defining text navigation 
Our conception of text navigation lies in the 
hypothesis that navigating through texts is the 
expression of a cognitive process related to specific 
knowledge (Minel, 2003; Couto and Minel, 2006). 
More precisely: we claim that a reader moves 
through texts applying some knowledge to exploit 
linguistic information present in texts (e.g. 
discursive markers). Moreover, we claim that this 
667
knowledge may be articulated in a declarative way 
(cf. Sec. 3) relying on information in texts coded, 
on the one hand, by its structure, and, on the other 
hand, by specific annotations. 
The main difference between classic hypertext 
and our conception of navigation lies on the status 
of texts and on the definition of navigational 
knowledge. In the case of hypertext, the visualiza-
tion of a text is unique and navigational knowledge 
is encoded (embedded) in the text. In our approach, 
there are several ways to visualize a text (Couto, 
2006), each way called text view, and for each 
view, different navigational knowledge may be 
defined. As a consequence, the navigation is not 
guided by the author, compared to hypertext navi-
gation where s/he determines the links, but it is the 
result of an interpretation process made by the 
reader, relying on text structure and annotations. 
 
Figure 1. Elements of text navigation. 
Our conception of navigation (cf. Fig.1) relies on 
four elements: i) a text representation allowing lin-
guistic specific phenomena and annotations (see 
Couto, 2006); ii) a language to model navigational 
knowledge (cf. Sec. 3); iii) an agent (an individual 
or a software) able to encode such knowledge; iv) a 
system, called NaviTexte, to interpret and apply 
knowledge to a specific text (cf. Sec. 4). 
3 Modeling navigational knowledge: the 
Sextant language 
To allow the unambiguous isolation of navigational 
knowledge we need a formal modeling language. 
We want to model the knowledge applied by a 
reader to move through texts, claiming that this 
knowledge exploits linguistic information present 
in texts. We do not say that this is the only way a 
reader may move through texts (e.g. strolling 
courses proposed by G?ry (2002) is a counterex-
ample), but we say that this is the kind of way than 
we are going to model. Different views of a text 
and the fact that each view contains specific indica-
tions of the possible reading courses constitute the 
heart of the language. 
3.1 Knowledge modules ant text views 
A text view may be a full view or a partial view 
focused in some specific phenomena present in the 
text (for example a view of all discourse referents). 
The constituent elements of a view are formalized 
in a view description, which contains the type of 
view, its parameters, the creation constraints (i.e. 
conditions to verify by the TU of the view) and the 
navigation operations (see next section). At pre-
sent, four types of view have been defined: plain-
text, tree, graph and temporality. The three firsts 
types are described in (Couto, 2006). The last one 
graphically represents temporality in texts and a 
complete description is in (Battistelli et al, 2006). 
Several view descriptions may be gathered by 
the encoder in an entity called navigational knowl-
edge module. The creation of a view may be con-
ceptualized as the application of a view description 
to a specific text. Thus, the application of a module 
implies the creation of a set of text views. 
3.2 The navigation operation 
The notion of computed query mentioned in sec-
tion 1 is formalized in Sextant as a navigation op-
eration, which links a source TU to a target TU. In 
classic hypertext systems one must explicitly con-
nect the specific source to the specific target. For 
example, if a reader wants, for all definitions in a 
scientific paper, to move from one to the following 
one, several hyperlinks must be defined. In our ap-
proach we specify the source and the target using 
conditions. As a result, we can abstract, for exam-
ple, the navigational knowledge that states ?go 
from one definition to the following one?, being 
?definition? one of the TU annotations. 
We can specify several conditions for the source 
and the target. We say that a navigation operation 
is available for a TU if this TU verifies the source 
conditions. A text navigation system should find 
the TU that verifies the target conditions. As sev-
eral TU in the text may verify them, we need a way 
of disambiguation. This is done by the orientation 
parameter, which specifies the direction of the tar-
get search by using one of these options: first, last, 
forward(i), backward(i). First and last indicate that 
the search of the target is absolute: the TU to select 
668
will be the first (respectively the last) TU that ver-
ify the conditions. Forward(i) and backward(i) in-
dicate that the search is carried out relatively to the 
source (before or after) and indexed by the integer i. 
For example, ?forward(3)? is interpreted as the 
third TU, after the source, of all the TU verifying 
the target conditions. 
3.3 The conditions language 
The conditions language is an important compo-
nent of Sextant ant it is composed by basic condi-
tions, TU elements existence conditions, hierarchi-
cal conditions and non-hierarchical conditions. 
Basic conditions concern TU?s attributes and 
annotations. For this kind of condition we use a 
notation close to the pattern notion. We define an 
operator called TU, having five operands that cor-
respond to the following properties: type, number, 
level, annotations and string. With the three first 
operands and the fifth one, we denote constraints of 
equality, inequality, order, prefix, suffix and sub-
string occurrence. The fourth operand is used to 
indicate the existence or non-existence of annota-
tions, whether it is an annotation name, a value or a 
name-value pair. 
For TU elements existence conditions, we define 
operators without operands to verify if a TU has 
annotations, string, title, parent and children. 
For conditions dealing with hierarchical rela-
tionship between different TU, a set of unary op-
erators have been defined, taking a basic condition 
as an argument. For example, the isAscendant op-
erator verifies if a TU is the ascendant of another 
TU specified by a basic condition. The other opera-
tors are: isParent, isChild, isSibling, isDescendant, 
hasInTitle, isInTitle. We would like to draw atten-
tion to the fact that these operators allow to move 
through the hierarchy of TU from a starting TU 
(Couto, 2006). 
Non-hierarchical conditions concern constructed 
units? attributes and annotations as well as TU con-
stitution. 
All conditions may be combined using the clas-
sic logic operators OR, AND and NOT. Figure 2 
presents an example of a language expression that 
formulates the following condition: TU of type 
?NP?, having an annotation of name ?discourse 
referent?, for which it exists, among its descen-
dants, a TU of type ?paragraph? not having an an-
notation of name ?semantic label? whose value is 
?conclusion?.  
TU(type = NP, *,*,{(discourse referent, *)},*) AND 
isDescendant(TU(type = paragraphe,*,*, {???(semantic 
label, conclusion)},*)) 
Figure 2. Conditions language example. 
This condition means: noun phrases being a dis-
course referent that does not occur in a concluding 
paragraph. 
4 NaviTexte: a text navigation system 
Several adaptive navigation systems have been 
proposed (Benyon and Murray, 1993; Kaplan et al, 
1993; Boyle and Encarnacion, 1994; Brusilovsky 
and Pesin, 1994; Brusilovsky et al, 1996). While 
they are goal specific (learning, tutoring, reading, 
etc.), NaviTexte (Couto, 2006) is a generic text 
navigation system implementing our approach. 
This means that, depending on texts and knowledge 
modules, NaviTexte may be used, for example, as a 
learning, tutoring or reading system. Another im-
portant difference is that NaviTexte gives the user 
the liberty to navigate through the text following its 
own interests (the system propose - the reader 
chooses), while the mentioned systems try to main-
tain a user stuck to a given route (the user chooses - 
the system propose) (H??k and Svensson, 1999). 
NaviTexte consists of sub-systems dealing with: 
text representation, navigational knowledge, visual 
representation and user interaction. The first one 
builds a text representation in memory from a text 
annotated manually or by dedicated software (Cun-
ningham et al, 2002; Bilhaut et al, 2003). The 
second sub-system loads and compiles the knowl-
edge modules. The result of this compilation is a 
graph of potential navigation courses that in prac-
tice is calculated as needed and stored in optimiza-
tion data structures. The third sub-system calcu-
lates and displays different text views and the 
fourth one manages the user interaction. 
The reader has the possibility to load and unload 
several texts and knowledge modules in the same 
work session. A complete description of NaviTexte 
may be found in (Couto, 2006). 
5 Applications of NaviTexte 
Building an application with NaviTexte requires a 
set of texts and navigational knowledge modules. 
Both text representation and Sextant language have 
XML implementations with dedicated editors to 
669
use in case of a manual text annotation and a hu-
man knowledge encoder, respectively (cf. Fig.1). 
So far four applications have been developed: al-
ternative automatic summarization (Couto and 
Minel, 2006), the NaviLire project (Couto et al, 
2005; Lunquist et al, 2006), re-reading Madame 
Bovary (Couto and Minel, 2006) and temporality in 
texts (Battistelli et al, 2006). We present two of 
them to illustrate NaviTexte?s potential. 
5.1 NaviLire: a text linguistics application 
For the past thirty years, text linguistic researchers 
have worked on describing linguistic markers of 
textual coherence in order to bring out principles of 
text structuring (Lundquist, 1980). A set of con-
cepts and models of textual interpretation has been 
worked out, including for example, anaphora, con-
nectors, mental spaces, etc. In particular, these 
studies have shown that even for languages appar-
ently close like French and Danish, texts are not 
organized in the same way (Lundquist, 2005). Con-
sequently, text linguistics has important implica-
tions in foreign language teaching, especially from 
a contrastive point of view, when language pairs 
are analyzed through texts used in authentic com-
munication situations. It seems that the teaching of 
text linguistics contributes to sharpen the attention 
of students towards the building of well-formed 
texts and to stimulate their own text production. 
Therefore, a tool that allows the student to perceive 
text units that contribute to and maintain text co-
herence and to navigate between them, can be sup-
posed to be an important didactic tool for teaching 
reading of foreign language texts, as well as pro-
ducing written texts in the foreign language.  
In the reading process, the student has to deal 
with two basic types of cognitive problems. First, 
s/he has to identify discourse referents in a text and 
choose the correct relations between the noun 
phrases that refer to them. Second, s/he has to iden-
tify the function and orientation intended by the 
sender. In the NaviLire project, navigation opera-
tions assisting the student are defined used Sextant 
and the texts are manually annotated by a text lin-
guistics expert. 
5.2 Navigation as an alternative to automatic 
summarization 
Many automatic summarization systems have been 
proposed (Mani, 2001; Minel, 2003). All these sys-
tems, based on the principle of phrase, proposition 
or group extraction, have been confronted to two 
problems intrinsic to the extraction procedure: i) 
the rupture of text cohesion, like in cases of anaph-
ora where the corresponding discourse referent is 
missing; ii) the adaptation of the summary to reader 
specific needs. Actually, there are no completely 
satisfying solutions to these problems. An alterna-
tive approach is to consider the summarizing proc-
ess as a reading course belonging to the reader 
(Crispino and Couto, 2004). Thereby, instead of 
extracting text fragments, we propose specific 
reading courses, whose specifications are based on 
propositions of (Endres-Niggermeyer et al, 1995) 
and on practice observations made in the frame of 
the automatic summarization system SERAPHIN 
evaluation (Minel et al, 1997) and the Filtext 
framework (Minel et al, 2001). 
These works showed that professional summar-
izers are interested by discourse categories that 
they retrieve by exploiting text organization and 
lexical markers. They also showed that these pro-
fessionals navigate through texts using heuristics 
acquired by experience. For example, they begin 
by reading the conclusion, and then they continue 
by looking, in the introduction, for nominal groups 
that occurred in the conclusion. This is the knowl-
edge we have modeled with Sextant. 
A specific reading course specifies, on the one 
hand, the kind of discursive category searched by a 
reader (e.g. a conclusion, a definition, an argument, 
a hypothesis, etc. 1 ) and on the other hand, the 
course in which the segments that linguistically 
enunciate these categories (typically phrases) must 
be presented to the reader. 
To carry out these reading courses, it is neces-
sary to locate the discursive categories involved 
and mark them in the text. For this purpose, we 
used ContextO (Minel et al, 2001). A reading 
course example is presented in Fig. 3. The reading 
point is positioned over the first TU, a phrase in 
this case, annotated ?Thematic Announcement?. 
When the user clicks over the TU, NaviTexte rec-
ommends her/him four navigation operations. The 
first one suggests bringing her/him to the following 
?Thematic Announcement?. The others ones sug-
gest going to the first ?Conclusion?, the first ?Re-
capitulation? and the first ?Argument?. For a given 
TU, each navigation operation available has three 
                                                 
1For more details on different categories or on what empirical 
basis were these types derived, see (Minel et al, 2001). 
670
possible states (and there is a visual semantic asso-
ciated to states), depending if it has been executed 
(bold font and ?*? prefix) or not (normal font and 
no prefix), and if it exists a TU target (clickable 
menu option) or not (non-clickable menu option). 
 
Figure 3. Automatic summarization courses. 
 
These kinds of suggestions (i.e. showing avail-
able navigation operations for a TU) are made all 
over the reading process. Consequently, along 
her/his reading continuum, the reader is assisted by 
the display of a specific set of signs, and there is no 
rupture of cohesion because s/he is able to continu-
ally see all the text (Battistelli and Minel, 2006). 
6 Evaluations 
There are few studies of adaptive navigation in hy-
permedia systems and most of them are focused in 
measures such as the number of visited nodes or 
the task completion time (H??k and Svensson, 
1999). Are we interested in this kind of measures? 
Being NaviTexte a generic text navigation system, 
we think that what it has to be evaluated are the 
different applications. Each application requires 
pertinent measures. For example, in NaviLire, the 
number of nodes or the time factor seems less use-
ful that the comprehension of the text analyzed.  
So far, NaviLire has been put into practice on a 
small scale only, viz. in the teaching of French 
texts and text linguistics to Danish language stu-
dents in the 4th year of Language and Communica-
tion studies at the Copenhagen Business School. A 
pilot experiment was carried out in order to evalu-
ate the effects of using the program.  
The first results are based on forty answers, of 
which 35 concern questions about the content of 
the text. These results show that the navilistes 
(people using NaviLire) have a better comprehen-
sion performance than the papiristes (people using 
paper and pencil) for 14 questions, an identical per-
formance for 16 other questions, and a poorer per-
formance for 5 questions (cf. Table 1). 
 #questions % 
Navilistes better than Papiristes 14 40 
Navilistes the same as Papiristes 16 45,7 
Navilistes worse than Papiristes 5 14,3 
Total 35 100 
Table 1. Comparison of navilistes and papiristes 
(Lundquist et al, 2006) 
Evaluations of the alternative automatic summa-
rization approach are ongoing. Our main problem 
is that automatic summarization evaluations, well 
known as difficult to carry out, typically compare 
to summaries made by professional summarizers 
(Mani, 2001; Minel, 2003). On the one hand, since 
we do not create a summary, we do not have an 
object to compare. On the other hand, since we 
have modeled the professional heuristics, we can-
not compare the behavior of our system to theirs 
because it is exactly what it has been modeled. 
7 Conclusions and future work 
We have presented our approach to text navigation 
conceived as a cognitive process that exploits lin-
guistic information present in texts. We have de-
fined it and explained the main differences with the 
hypertext navigation approach. The four elements 
needed to implement our approach are described: a 
text representation, the navigation knowledge mod-
eling language Sextant, the knowledge encoding 
agents (via applications) and the NaviTexte system. 
Two distinct applications of NaviTexte have 
been presented, showing the versatility of our ap-
proach. The quantitative results of our experimen-
tation with Danish students learning French con-
firm the improvement obtained by using text navi-
gation.  
A long term consequence of modeling naviga-
tional knowledge is the creation of knowledge 
bases exchangeable and reusable. Actual collabora-
tions are reusing the knowledge coming from the 
NaviLire project into others e-learning projects. 
We think that our approach may have a signifi-
cant impact on the way text is being read when its 
amount or nature does not allow sequential reading 
(e.g. the Web). Related to last works in Web Wise, 
we plan to couple our approach to Semantic Web 
approaches to exploit existing annotations. 
671
Acknowledgments 
NaviTexte is supported by a grant (U05H01) from 
Ecos-Sud Program. 
References 
Battistelli D. and Minel J.-L. 2006. Les syst?mes de 
r?sum? automatique: comment assurer une continuit? 
r?f?rentielle dans la lecture des textes, in Sabah (Ed.), 
Compr?hension des langues et interaction, 295-330. 
Battistelli D., Minel J-L and Schwer S. 2006, Repr?sen-
tation des expressions calendaires dans les textes: 
vers une application ? la lecture assist?e de biogra-
phies. Revue TAL, 47(3): 1-26. 
Benyon D. and Murray D. 1993. Developing Adaptive 
Systems to Fit Individual Aptitudes, In W.D.Gray, 
W.E., Helfley and D.Murray (eds.), Proceedings of 
the 1993 International Workshop on IUI, 115-122. 
Orlando, FL., New York, ACM Press. 
Bilhaut F., Ho-Dac M., Borillo A., Charnois T., Enjal-
bert P., Le Draoulec A., Mathet Y., Miguet H., Pery-
Woodley M.P. and Sarda L. 2003. Indexation discur-
sive pour la navigation. intradocumentaire: cadres 
temporels et spatiaux dans l?information g?ographi-
que. Actes de TALN 2003, 315-320. 
Bodner R. and Chignell M. 1999. Dynamic hypertext: 
querying and linking, ACM Computing Surveys, 
31(4): 120-132. 
Boyle C. and Encarnacion A. 1994. MetaDoc: An Adap-
tive Hypertext Reading System, UMUAI, 4: 1-19. 
Brusilovsky P. 1996. Methods and techniques of adap-
tive hypermedia . UMUAI, 6(2-3): 87-129. 
Brusilovsky P. and Pesin L. 1994. ISIS-Tutor: An adap-
tive hypertext learning environment. In H.Ueono & 
V.Stefanuk (eds.), Proceedings of JCKBSE?94. 
Brusilovsky P., Schwartz E. and Weber G. 1996. ELM-
ART: An Intelligent Tutoring System on World Wide 
Web, ITS?96, Berlin, Springer, 261-269. 
Couto J. 2006. Mod?lisation des connaissances pour une 
navigation textuelle assist?e. La plate-forme logicielle 
NaviTexte. PhD, Universit? Paris-Sorbonne. 
Couto J., Lundquist L., Minel J.-L. 2005. Naviguer pour 
apprendre. EIAH 2005, Montpellier, 45-56. 
Couto J. and Minel J-L. 2006. Navigation textuelle: Re-
pr?sentation des textes et des connaissances, Revue 
TAL, 47(2): 1-24, Herm?s, Paris. 
Crispino G. and Couto J. 2004. Construction automati-
que de r?sum?s. Une approche dynamique. Revue 
TAL, 45(1): 95-120, Herm?s, Paris. 
Cunningham, H., Maynard, D., Bontcheva, K. and Tab-
lan, V. 2002. GATE: A Framework and Graphical 
Development Environment for Robust NLP Tools 
and Applications, ACL?02, ACM Press, 168-175. 
Danielson D.R. 2002. Web navigation and the be-
havorial effects of constantly visible maps, Interact-
ing with Computers, 14: 601-618. 
Edwards D.M. and Hardman L. 1989. Lost in hyper-
space: cognitive mapping and navigation in a hyper-
text environment, in Hypertext: Theory and Practice. 
Oxford, Intellect Books, 105-125. 
Endres-Niggemeyer B., Maier E. and Sigel A. 1995. 
How to implement a naturalistic model of abstracting: 
four core working steps of an expert abstractor, In-
formation Processing et Management, 31(5): 631-674 
G?ry M. 2002. Un mod?le d?hyperdocument en contexte 
pour la recherche d?information structur?e sur le 
Web. Revue des Sciences et Technologies de 
l?Information, 7/2002, Herm?s, Paris, 11-44. 
H??k K. and Svensson M. 1999. Evaluating Adaptive 
Navigation Support. In: Social Navigation of Infor-
mation Space. Springer Verlag, 240-251. 
Kaplan C., Fenwick J. and Chen J. 1993. Adaptive Hy-
pertext Navigation Based On User Goals and Con-
text, UMUAI, 3, 193-220. 
Lundquist L. 1980. La coh?rence textuelle, syntaxe, s?-
mantique, pragmatique, Copenhagen, Nordisk Forlag. 
Lundquist L. 2005. Noms, verbes et anaphores 
(in)fid?les. Pourquoi les Danois sont plus fid?les que 
les Fran?ais. Langue fran?aise. Vol 145: 73-92. 
Lundquist L., Minel J.L. and Couto J. 2006. NaviLire, 
Teaching French by Navigating in Texts, 
IPMU?2006, Paris. 
Mani I. 2001. Automatic Summarization, Amsterdam, 
John Benjamins Publishing Company. 
Mathe N. and Chen J. 1994. A User-Centered Approach 
to Adaptive Hypertext based on an Information Rele-
vance Model, UM'94, Hyannis, MA, 107-114. 
Minel J.-L, Cartier E., Crispino G., Descl?s J.P., Ben 
Hazez S. and Jackiewicz A. 2001. R?sum? automati-
que par filtrage s?mantique d?informations dans des 
textes, Pr?sentation de la plate-forme FilText. Tech-
nique et Science Informatiques, n?3: 369-396. 
Minel J-L. 2003. Filtrage s?mantique. Du r?sum? ? la 
fouille de textes. Paris, Herm?s. 
Minel, J-L., Nugier, S. and Piat, G. 1997. How to appre-
ciate the Quality of Automatic Text Summarization. 
EACL 97, Madrid, 25-30. 
672
Representing and Visualizing
Calendar Expressions in Texts
Delphine Battistelli
Univ. Paris-Sorbonne (France)
email: Delphine.Battistelli@paris-sorbonne.fr
Javier Couto
INCO, FING, UdelaR (Uruguay)
email: jcouto@fing.edu.uy
Jean-Luc Minel
MoDyCo, CNRS-Univ. ParisX (France)
email: Jean-Luc.Minel@u-paris10.fr
Sylviane R. Schwer
LIPN, CNRS-Univ. ParisXIII (France)
email: Sylviane.Schwer@lipn.univ-paris13.fr
Abstract
Temporal expressions that refer to a part of a calendar area in terms of
common calendar divisions are studied. Our claim is that such a ?cal-
endar expression" (CE) can be described by a succession of operators
operating on a calendar base (CB). These operators are categorized: a
pointing operator that transform a CB into a CE; a focalizing/shifting op-
erator that reduces or shifts the CE into another CE, and finally a zoning
operator that provides the wanted CE from this last CE. Relying on these
operators, a set of annotations is presented which are used to automat-
ically annotate biographic texts. A software application, plugged in the
platformNavitext, is described that builds a calendar view of a biographic
text.
365
366 Battistelli, Couto, Minel, and Schwer
1 Introduction
Taking into account temporality expressed in texts appears as fundamental, not only
in a perspective of global processing of documents, but also in the analysis of the
structure of a document.1 The analysis of temporality within texts has been studied
principally by considering verbal times (e.g. Song and Cohen (1991); Hitzeman et al
(1995) and temporal adverbials (see below).
Our approach is focused on temporal adverbials ? in French ? that refer directly
to text units concerning common calendar divisions, that we name ?calendar expres-
sions? (CEs for short). Several analyses of this kind of expressions has generated
a lot of interest, ranging from their automatic recognition and annotation in texts to
their analysis in terms of discursive frames (Charolles, 1997; Tannen, 1997), following
work of Halliday (1994) which put the emphasis on the importance of the temporal
adverbial expressions as modes of discursive organization.
Nowadays, in the field of temporality processing, automatic identification and an-
notation tasks of CEs are the most developed, mainly because identifying and anno-
tating expressions which contain calendar units are considered? a priori ? as trivial
tasks. Those tasks have been particularly explored in three contexts:
1. Systems which aim to set events on a time scale depending on their duration
and according to a hierarchy of unities called granularities (Schilder and Habel,
2001);
2. Systems for summarizing multi-documents (Barzilay et al, 2001); and
3. QA systems (Pustejovsky et al, 1993; Harabagiu and Bejan, 2005).
Please note that the proposition of the well-known standard temporalmeta-language
named TimeML (Pustejovsky et al, 2003) initially took place in the context of a QA
systems worshop (Pustejovsky, 2002), and mainly integrates two schemes of annota-
tions ? namely TIDES TIMEX2 (Ferro et al, 2004) and Sheffield STAG (Setzer and
Gaizauskas, 2000) ? which were essentially put forward from the analysis of CEs.
In this paper, we propose a formal description of CEs in written French texts, by ex-
plicitly distinguishing several classes of linguistic markers which must be interpreted
as successive operators. This work is driven in order to propose a set of fine and
well-defined annotations which will be used to navigate temporally in an annotated
document. Our approach differs from the preceding ones in two crucial ways:
? Our goal is not to link a CE to an event, neither to fix it on a ?temporal line",
using a set of values relying on ISO 8601 standard format (Mani and Wilson,
2000; Setzer and Gaizauskas, 2000; Filatova and Hovy, 2001); instead our goal
is to link CEs between themselves, that is to say to establish their qualitative
relative positions (the set of those relations is named ?proper text calendar?);
? We design CE semantics as algebraic expressions.
1This research is funded with an ANR grant (Projet Blanc Conique).
Representing and Visualizing Calendar Expressions in Texts 367
The remainder of this paper is organized as follows. In the next section, we in-
troduce an algebra of CEs. In Section 3 we describe a software application, which
exploits functional representation, built with previous way exhibited operators and
plugged in the NaviTexte platform, aiming to support text reading. Finally, conclu-
sions and future research directions are presented in Section 4.
2 An Algebra for Calendar Expressions
We postulate that a CE, say E , used to refer to a calendar area can be described by a
succession of operators applied on an argument, named calendar base (CB), say B,
that bears a granulariry and a value for anchoring allowing fixing it in the calendar
system used and that gives access at the calendar area described by the CE.
Each operator gives a piece of the processing following a specific order: on B is
applied a pointing operation, usually expressed by a determinant, whose result is an
CE, E1 part of E . On E1 is applied a second kind of operator expressing the useful
part of this base (all, the beginning, the middle, the end, a fuzzy area around) given as
result a new CE E2 which is part of E and is associated with a piece of the calendar
that cuts the time line in three areas (illustrated by Figure 1):
? the former half-line (A),
? the Useful portion (U),
? posterior half-line (P)2.
The useful part can also be obtained either by shifting, like in ?trois semaines plus
tard" (three weeks later), or by zooming, as in ?l?automne de cette ann?e l?" (the au-
tumn of this present year).3 A third kind of operator gives access at the area described
by the complete CE E: selecting one of the three portioned areas, like in ?jusqu?en
avril 2006" (until April 2006).
Figure 1: Partition of the time line for a unary CE
The order of operators is the following: a pointing operator OpPointing, followed
by one or more focalising or shifting operatorsOpFocalising/Shi f ting+ and finally at
least one zoning operator OpZoning?.4 Some operators can be omitted, usually when
2This Time line is pragmatically limited bounded. For instance, (P) can be naturally limited by the
present moment, as we do in Figure 1.
3For such deictic CEs, the CB has the granularity year, and the value current.
4Usually one, but we also can find two zoning operators, for instance in ?jusqu?? avant No?l (until before
Christmas"). In this case, the order of the operators is more constraint than the order of Focalising/Shifting
operators. Therefore we use the ? symbol instead of +
368 Battistelli, Couto, Minel, and Schwer
they do not provide any new information. In sum, the representation of CEs has the
following generic form: OpZoning?(OpFocalising/Shi f ting+(OpPoin-ting(CB)).
For instance, let us analyse the CE E=?Avant le d?but de la fin de l?ann?e 2008"
(before the beginning of the end of the year 2008). B=?ann?e 2008". Firstly, the
operator of pointing, triggered by the identification of ?l?" (the contraction of ?le")
is applied, given E1=L?ann?e 2008".5 Secondly, two operators of focalising/shifting
are applied successively: the first one triggered by ?la fin de" , provides E ?2 and the
second one, triggered by ?le d?but de", provides E2. Finally an operator of zoning is
associated with ?avant", provided E . Consequently, the CE ?avant le d?but de la fin
de l?ann?e 2008" is produced as avant (le d?but de (la fin de(l?(ann?e 2008)))). The
sequence of this CE is depicted and visualized in Figure 2.
Figure 2: Computation of ?avant le d?but de la fin de l?ann?e 2008"
Each operator is characterized by its arity (the number of its arguments) and type.
With regard to arity, in this paper we focus on unary operators.
2.1 Unary operators
Three types of operators have been defined: pointing, focalising/shifting and zoning.
The pointing operator is trivial (it transforms B into a CE of type E1) but the two
others need some refinements.
Focalising/Shifting operators
Focalising/Shifting operators transform a CE of type E1 into a CE of type E2. Several
kinds of focalising/shifting time may be expressed. For instance, in the expression ?au
d?but de mai 2005" (at the beginning of may 2005) the focalising/shifting is localised
inside the BC (mai 2005), whereas in the expression ?trois semaines avant mai 2005"
(three weeks before may 2005) it is outside the BC. Consequently, six sub-operators
have been identified and are shown Table 1. It should be noted that ShiftingBeginning
and ShiftingAfter operators refers to a family of operators, because for these ones it is
necessary to precise two parameters, the granularity and the value of the shifting.
For some reasons of implementation, except for the operator IdShifting, which
refers at the identity, all others operators are treated as idempotent. In other words, we
consider as equivalent these two expressions ?au d?but du d?but des ann?es 1980" (at
the beginning of the early eighties) and ?au d?but des ann?es 1980? (in the early of
eighties). The next version will improve at this point.
Zoning operators
A Zoning operator transforms a CE of type E2, associated to the useful portion U of
Figure 1, into the CE E analysed. A Zoning operator refers to one of the six possible
5This pointing operator, as mentioned previously, is not an operator of the CE algebra, but all the other
operators are part of the CE algebra.
Representing and Visualizing Calendar Expressions in Texts 369
Table 1: Focalising/Shifting operators
Operators Examples
IdShifting ? en 1945
? au mois d?ao?t
ZoomBeginning ? ? l?aube des ann?es 1980
? au d?but de mai 1945
ZoomMiddle ? au milieu des ann?es 1980
ZoomEnding ? ? la fin des ann?es 1980
ShiftingBefore (granularity, -n) ? 10 jours avant le 14 juillet 2005
ShiftingAfter (granularity, +n) ? 10 jours apr?s le 14 juillet 2005
zones6 built from A, P and U: that is A, A+U, U, U+P, P, A+P. These six kinds of
zoning are associated with a set of prepositions, whose prototypes are shown Table 2.
Fuzzy expressions like ?peu avant? (short before) can double this number. Table 2
also illustrates the the ZoningAbout operator <U>. Further, note that ZoningId is not
expressed, but has to be taken into account.
Table 2: Zoning operators
Operators Expression
ZoningBefore [A] avant fin avril 2008
ZoningUntil [A+U] jusqu?? fin avril 2008
ZoningId [U] [ /0] fin avril 2008
ZoningAbout <U> vers la fin avril 2008
ZoningSince [U+P] depuis la fin avril 2008
ZoningAfter [P] apr?s fin avril 2008
ZoningApart [A+P] except? fin avril 2008
2.2 N-ary or sequence operators
As mentioned before, it is necessary to use several N-ary operators to represent some
CE. For instance, a binary operator is used for representing an expression like ?entre
fin mai 2005 et avril 2006" (between the end of may 2005 and april 2006). This oper-
ator, Between, applies to two CEs, so for the preceding expression the representation
is Between ((ZoomEnding(Pointing(may 2005), Pointing(april 2006)). Moreover, a
sequence operator is needed to represent a CE like ?le mardi 21, le mercredi 22 et le
vendredi 24 mai 1980" (on Tuesday 21, Wednesday 22 and Friday 24 of May). The
study of these operators, associated with even more complex CEs with quantifications,
is currently under investigation.
6The empty zone, expressed by ?jamais? (never) and the full zone, that is A+U+P, expressed by ?tou-
jours? (always) are CE, but not associated with unary operators associated to a BC, as defined here, hence
excluded of our precedent study.
370 Battistelli, Couto, Minel, and Schwer
3 Application
Many applications which exploit temporal expressions in texts, in particular in the area
of information extraction, have been implemented (Pazienza, 1999). Our application
is plugged into the textual navigation workstation NaviTexte (Couto, 2006; Couto and
Minel, 2007), in order to combine a traditional linear reading with a chronological
one. With this intention, we have undertaken the construction of a computerized aided
reading of biographies. Consequently, we have addressed two issues. First, identifying
temporal expressions and ordering chronologically text segments in which they are
included. Second, building calendar views of the text and navigating through these
views.
3.1 Identifying and ordering calendar expressions
From the linguistic study presented above, we have defined a set of annotations which
are used to automatically annotate biographic texts. This process is carried out by
transducers which put XML7 annotations through the processed text. These annota-
tions describe on the one hand, the granularity of CEs, and on the other hand, the
kind of identified operator. For instance, the following XML code illustrates how the
temporal expression ?avant le d?but de la fin de l?ann?e 2008" (Before the beginning
of the end of the year 2008) will be annotated:
<UT Type="Expression Calendaire" Nro="7">
<Annotation Nom="Grain">Annee</Annotation>
<Annotation Nom="Annee">2008</Annotation>
<Annotation Nom="RelationCalendrier">Absolue</Annotation>
<Annotation Nom="OpTempR?O?gion1">Avant</Annotation>
<Annotation Nom="OpTempD?O?placement1">FocalFin</Annotation>
<Annotation Nom="OpTempD?O?placement2">FocalDebut</Annotation>
<Chaine>
avant le debut de la fin de l?annee 2008
</Chaine>
</UT>
From these annotations, an automatic ordering relying on values of CEs can be
carried out. A first implementation took only into account disjoined CEs, because they
are linearly ordered. Intersecting CEs, like ?En juin 2007 (. . . ) en ?t? 2007" (in June
2007 (. . . ) in summer 2007) requires a more powerful formalism. A formalism relying
both on S-Languages (Schwer, 2002b) and granules (Schwer, 2002a) is required to
provide a full automatic ordering.
3.2 Building a text calendar view
A new kind of view, a calendar one, has been built in the NaviTexte platform. This
view is built from texts which contain CEs annotated as described above. An example
is shown in Figure 3. Conceptually, a calendar view is a graph coordinated with a two-
dimensional grid. In the left part of the view, lexical chains of various occurrences
of CEs in the text are displayed. By default, those are ordered according to their
order of appearance in the text, but it is possible to display a chronological order,
7A DTD is defined in Couto (2006)
Representing and Visualizing Calendar Expressions in Texts 371
by using options offered in the panel located in bottom of the view. Nodes in the
graph represent these lexical chains. The visual representation of a CE depends of the
functional representation computed as described before Figure 2.
A simple CE, with only a pointing operator like in ?l?ann?e 2008" (the year 2008)
is always visualised like a white ellipse. An operator of focalising/Shifting like ?la fin
de" (the end of) selects an area of the ellipse and blackens it. Finally, a zoning operator
like ?avant" (before) is visualised by a bold line displaying the area that is referred to.
The plug-in is implemented with the JGaph package and we largely use some of
its functionalities, like zooming or the partial layout cache. We also use html tooltip
text in Swing to contextualise a CE in the original text. For example, in Figure 3,
the whole paragraph which contains the CE ?en 1953? (in 1953) is displayed and the
occurrence of a CE is highlighted.
3.3 Evaluation
Two kinds of evaluation could be performed on this work: (i) evaluation of automatic
recognition and semantic annotation of CEs in text, (ii) evaluation of the calendar
view. The former calls for a classical protocol in NLP, whereas the latter is more
complex to carry out.
So far, only recognition has been carried out by Teissedre (2007) who computed
recall and precision on three kinds of corpora. Due to the fact that an annotation is
made up of several fields the recall has been computed like this: a score zero when
a CE is not identified, a score 1 when the identification is total, and 0.5 when the
identification is partial. Applying these rules, recall is 0.8 and precision is 0.9.
We would like to make two remarks on this result. First, quantified CEs like ?tous
les mardis? (every Tuesday) or ?un mardi sur deux? (one Tuesday out of two) and
n-aries (n? 3) CEs like "entre 2008 et 2009 et en juin 2010" (between 2008 and 2009
and in june 2010) are identified but are not yet taken into account in the semantic
annotation process. Second, syntactic ambiguities like in ?il a dormi deux jours avant
No?l? (he slept two days before Christmas) are not taken into account either. However
in this example, there are two possible syntactic structures. In the first case, "avant
No?l" is the CE and the operator is the Regionalisation one; in the second case, "deux
jours avant No?l" is the CE and the operator is the Shifting one. Presently, our analysis
provides only the second one like in Aunargue et al (2001) but we intend to upgrade
it in order to provide both analyses.
Evaluation of the calendar view should be studied from a cognitive point of view
and is highly dependent on the application. We plan to work with cognitive scien-
tists to build a relevant protocol to study this aspect of evaluation which calls for the
specification of a set of navigation operations based on the algebra of operators.
4 Conclusion
We proposed an algebra of CEs with three kinds of operators to analyse calendar
expressions and build a functional representation of these expressions. We described
an implementation of this approach in the platformNaviTexte and we have shown how
the functional representation is used to visualise a calendar view of a text. In future
work, we will rely on a methodology presented in Battistelli and Chagnoux (2007) in
372 Battistelli, Couto, Minel, and Schwer
Figure 3: Example of calendar view in NaviTexte
order to take into account several temporal axis, and thus several calendar structures,
which are expressed in texts by different levels of enunciations, like citations.
References
Aunargue, M., M. Bras, L. Vieu, and N. Asher (2001). The syntax and semantics of
locating adverbials. Cahiers de Grammaire 26, 11?35.
Barzilay, R., N. Elhadad, and K. McKeown (2001). Sentence ordering in multidocu-
ment summarization. In First International Conference on Human Language Tech-
nology Research (HLT-01), pp. 149?156.
Battistelli, D. and M. Chagnoux (2007). Repr?senter la dynamique ?nonciative et
modale de textes. In actes TALN?07 (Traitement automatique du langage naturel,
pp. 13?23.
Charolles, M. (1997). L?encadrement du discours ? univers, champs, domaines et
espaces. In Cahiers de recherche linguistique, Volume 6 of LANDISCO, pp. 1?73.
Universit? Nancy 2.
Couto, J. (2006). Mod?lisation des connaissances pour une navigation textuelle
assist?e. La plate-forme logicielle NaviTexte. Ph. D. thesis, Universit? Paris-
Sorbonne.
Couto, J. and J.-L. Minel (2007). Navitexte, a text navigation tool. In , Lecture Notes
in Artificial Intelligence 4733, pp. 251?259. Springer-Verlag.
Representing and Visualizing Calendar Expressions in Texts 373
Ferro, L., L. Gerber, I. Mani, B. Sundheim, and G. Wilson (2004). Standard for the
annotation of temporal expressions. Technical report, timex2.mitre.org, MITRE
Corporation.
Filatova, E. and E. Hovy (2001). Assigning time-stamps to event-clauses. In Work-
shop on Temporal and Spatial Information Processing, ACL?2001, pp. 88?95.
Halliday, M. A. K. (1994). An introduction to functional grammar. London: Edward
Arnold.
Harabagiu, S. and C. A. Bejan (2005). Question answering based on temporal infer-
ence. In AAAI-2005 Workshop on Inference for Textual Question Answering.
Hitzeman, J., M.Moens, and C. Grover (1995). Algorithms for analyzing the temporal
structure of discourse. In EACL?95, pp. 253?260.
Mani, I. and G. Wilson (2000). Robust temporal processing of news. In Proceedings
38th ACL, pp. 69?76.
Pazienza, M. T. (1999). Information Extraction, toward scalable, adaptable systems.
New York: Springer-Verlag.
Pustejovsky, J. (Ed.) (2002). TERQAS 2002: An ARDA Workshop on Advanced Ques-
tion Answering Technology.
Pustejovsky, J., J. Castano, R. Ingria, R. Sauri, R. Gaizauskas, A. Setzer, and G. Katz
(2003). Timeml: Robust specification of event and temporal expressions in text. In
IWCS-5 Fifth International Workshop on Computational Semantics.
Pustejovsky, J., R. Knippen, J. Lintman, and R. Sauri (1993). Temporal and event
information in natural language text. Lexique 11, 123?164.
Schilder, F. and C. Habel (2001). From temporal expressions to temporal informa-
tion: Semantic tagging of news messages. In Proceedings of ACL?01 workshop on
temporal and spatial information processing, pp. 65?72.
Schwer, S. R. (2002a). Reasoning with intervals on granules. Journal of Universal
Computer Science 8 (8), 793?808.
Schwer, S. R. (2002b). S-arrangements avec r?p?titions. Comptes Rendus de
l?Acad?mie des Sciences de Paris S?rie I 334, 261?266.
Setzer, A. and R. Gaizauskas (2000). Annotating events and temporal information in
newswire texts. In Proceeedings 2rd LRC, pp. 64?66.
Song, F. and R. Cohen (1991). Tense interpretation in the context of narrative. In 9th
AAAI, pp. 131?136.
Tannen, D. (1997). Framing in Discourse. Oxford: Oxford University Press.
Teissedre, C. (2007). La temporalit? dans les textes : de l?annotation s?mantique ? la
navigation textuelle. Master?s thesis, Universit? Paris-Sorbonne.
Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas,
pages 54?61, Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Opinion Identification in Spanish Texts
Aiala Ros? Dina Wonsever
Grupo de Procesamiento de Lenguaje Natural,
Facultad de Ingenier?a, UDELAR
Grupo de Procesamiento de Lenguaje Natural,
Facultad de Ingenier?a, UDELAR
J. Herrera y Reissig 565 J. Herrera y Reissig 565
Montevideo, 11300, Uruguay Montevideo, 11300, Uruguay
Modyco, UMR 7114, 
Universit? Paris Ouest Nanterre La D?fense, 
CNRS France
wonsever@fing.edu.uy
200, avenue de la R?publique, Batiment A, 
Bureau 420, 92 001 Nanterre Cedex 
aialar@fing.edu.uy
Jean-Luc Minel
Modyco, UMR 7114, 
Universit? Paris Ouest Nanterre La D?fense, CNRS France
200, avenue de la R?publique, Batiment A, 
Bureau 420, 92 001 Nanterre Cedex 
jean-luc.minel@u-paris10.fr
Abstract
We present our work on the identification of opin-
ions and its components: the source, the topic and 
the message. We describe a rule-based system for 
which we achieved a recall of 74% and a precision 
of  94%.  Experimentation  with  machine-learning 
techniques for the same task is currently underway.
1 Introduction
For some tasks in language processing such as In-
formation  Extraction  or  Q&A Systems,  it  is  im-
portant to know the opinions expressed by differ-
ent sources and their polarity, positive or negative, 
with  respect  to  different  topics.  There  are  even 
commercial applications that provide this kind of 
service (http://www.jodange.com). 
We here present a system for identifying opin-
ions in Spanish texts. We define opinion as the re-
port of someone's statement about any subject ( El  
investigador de la Polit?cnica afirma que el prin-
cipal  problema de este sistema es conseguir que  
sea f?cil de usar / The researcher at the Polit?c-
nica asserts that the main problem with this system 
is making it easy to use), or as any mention of dis-
course participants? beliefs  (El  PRI acepta parti-
cipar en el debate / The PRI agrees to participate  
in the debate).
As a first step, we study the impact of elements 
that typically introduce such expressions in written 
text. These elements are mainly verbs of commu-
nication (decir, declarar / say, state) but other verb 
classes  (belief,  agreement,  appreciation)  are  also 
considered. In other cases, the opinions will be ex-
pressed through nouns  (opini?n/opinion,  declara-
ci?n/statement) or  segments  introduced by  seg?n 
(according to) or similar expressions. To complete 
the  opinion,  we  identify  its  characteristic  argu-
ments: the source, the topic and the message. 
In addition to recognizing an opinion, we try to 
determine its semantic orientation. To this end, we 
consider certain subjective elements and operators 
(reverse,  intensifier,  enhancing,  neutralizing,  etc.) 
which affect them. In this article, we present only 
results  on   the  semantic  orientation  of  opinion 
verbs,  opinion  nouns  and  topic  introducers 
(sobre/about, con respecto a/with respect to, etc.). 
There  are  many  studies  that  address  these  is-
sues: Pang and Lee (2008), for instance, discuss in 
54
detail  various  concepts  in  the  area  of  "Opinion 
Mining" or "Sentiment  Analysis" and present the 
main  proposals,  resources  and  applications.  For 
our work,  which focuses on the identification of 
source, topic and message, we have mainly drawn 
on the following: the scheme for annotating opin-
ions  and  emotions  proposed  by  Wiebe,  Wilson, 
and  Cardie  (2005);  the  work  on  opinion-holder 
(source)  propositional  opinion  identification 
presented in (Bethard et  al.,  2004);  a  system for 
source  identification  using  statistical  methods 
(Choi  et  al,  2005);  a  method  for  opinion-holder 
and topic extraction from Kim and Hovy (2006); 
the study on the identification of source and target 
presented  in  (Ruppenhofer  et  al.,  2008);  and  a 
work on topic  annotation  (Stoyanov  and Cardie, 
2008). 
For  our  semantic  orientation  study,  we  have 
taken  some  concepts  from  Turney  and  Littman 
(2003) and  analyzed  some work on subjectivity 
operators  (Polanyi  and  Zaenen,  2004;  Moilanen 
and Pulman, 2007; Choi and Cardie, 2008). 
In what follows, we briefly present the model 
that  has  been  defined  to  represent  opinions  and 
two methods for their automatic recognition. First, 
we describe a rule-based system that incorporates 
lexical resources. This system, whose evaluation is 
detailed below, achieves a recall of 74% and a pre-
cision of 97%. During the evaluation process we 
produced an annotated corpus of 13,000 words, by 
manually correcting the system output. The second 
system, currently under development, involves the 
application of machine-learning techniques to the 
annotated corpus.  
2 Opinion components
An opinion is composed of a predicative element 
and its characteristic arguments. The set of opinion 
predicates includes verbs, nouns and prepositions 
(or prepositional locutions). Verbs belong to vari-
ous semantic classes: communication (decir / say,  
declarar  /  state), assessment  (criticar  /  criticize,  
felicitar  /  compliment), belief  (creer /  believe,  
opinar / think) and acceptance  (aceptar / accept,  
rechazar / reject). 
These classes  are  similar  to  those proposed in 
(Asher et al, 2008), the main difference being that 
they include  the  class  Sentiment  but  we  do  not. 
Nouns  are  generally  derived  from the  aforemen-
tioned  verbs  (opini?n  /  opinion,  declaraci?n  /  
statement, apoyo / support). Some prepositions and 
prepositional  locutions  are  seg?n,  de  acuerdo a,  
para / according to. 
The relevant arguments that we identified for the 
opinion  predicates  are,  as  already  mentioned, 
source,  topic  and  message.  To  establish  this 
scheme we analysed  syntactico-semantic schemes 
proposed  in  ADESSE2 for  selected  verb  classes 
(Garc?a-Miguel et al, 2005) and some of the Span-
ish  FrameNet  frames3 (Subirats-R?ggeberg  and 
Petruck.,  2003),  mainly the  opinion frame whose 
frame  elements  include  cognizer  (source),  topic 
and  opinion  (message)  and  the  communication 
frame for which some elements are communicator 
(source), topic and message. 
Our definition deviates from much of the literat-
ure on this subject because we limit our work to 
opinions introduced by an opinion predicate, as ex-
plained  above,  while  many  of  the  cited  works 
identify all kinds of subjective expressions, mainly 
adjectives  with  positive  or  negative  polarity,  as 
with the  expressive subjective elements described 
in (Wiebe et al, 2005). 
As in our work we focus on finding the source, 
the message and the topic for each opinion, we ig-
nore  all  the  text  fragments  in  which  there  is  no 
evidence that the author is quoting or referring to 
other participants? opinions. These text fragments 
constitute the message, as defined above, stated by 
the text author. So, once our system has identified 
other  participants?  opinions,  the  remaining  text 
should be attributed to the text author. 
Identifying  subjective  elements  is  necessary in 
order to determine the semantic orientation of the 
opinion. We think the treatment of these elements 
within the author?s message is similar to the treat-
ment that must be applied within the message at-
tributed to any other source.  Such a treatment  is 
not addressed in this work, since the semantic ori-
entation study presented here is restricted to opin-
ion predicates and topic introducers. 
In some respects our work is related to that of 
(Bethard et al, 2004). For opinions introduced by 
opinion  verbs,  they  identify  the  source  (opinion 
holder)  and  the  message  (propositional  opinion), 
restricting  the  study  to  messages  that  constitute 
subordinate  clauses.  However,  we  seek  also  to 
identify the explicit references to the topic and we 
consider not only verbs but also some nouns and 
prepositions such as  seg?n / according to. A fur-
55
ther difference is that they distinguish propositions 
containing an opinion from those transmitting facts 
or predictions, whereas we do not make this dis-
tinction. 
In our recognition of  the topic we consider only 
explicit references to the opinion subject. We look 
for  topic-introducing  elements,  such  as  sobre  /  
about, con respecto a / regarding, en contra de /  
against, without  trying  to  deduce the  topic  from 
the study of the message itself.
For this general scheme, there are different in-
stances in which the arguments can take different 
forms.  Thus,  for  some opinion verbs such as  re-
chazar /  reject, the message is usually empty. For 
other verbs the topic will be a noun phrase, such as 
aceptar  la  propuesta  / to  accept  the  proposal, 
while for others it will be a prepositional phrase, 
for example, hablar de literatura / to speak about  
literature.
2.1 Some opinion examples 
In  a  standard  reported  speech  utterance  (1),  the 
opinion  predicate  is  a  communication  verb.  The 
source is the subject of the verb and the message is 
contained  in  the  subordinate  clause.  Normally, 
there is not a segment expressing the topic. 
(1) [El investigador de la Polit?cnica]f [afirma]p 
[que el principal problema de este sistema es con-
seguir que sea f?cil de usar]m.
(1) [The researcher at the Polit?cnica]f [said] p 
[that the main problem with this system is making 
it easy to use] m.  
In (2),  there is  a verb that  introduces referred 
speech in which a verbal act is mentioned, but the 
words uttered (message) are not reproduced (Mal-
donado, 1999). 
(2) [El abogado de Fernando Botero]s [habl?]p 
[sobre el tema]t con Semana. 
(2) [The lawyer of Fernando Botero]s [spoke]p 
[about the subject]t with Semana. 
However, we also found cases in which repor-
ted speech includes an explicit mention of the topic 
(3) and cases in which referred speech includes the 
uttered words (4). In both examples all the defined 
arguments are present in the text. 
(3) [Sobre la partitura]t [Ros Marb?]s 
[afirma]p [que es "enormemente teatral. Se define 
a los personajes desde la propia m?sica, ...."]m.
(3) [Concerning the score]t [Ros Marb?]s [said]p 
[it is "very theatrical. The characters are defined 
from the music itself,  ....]m.
 
(4) En una carta escrita por Dal? en Neuilly en 
abril de 1951, [el artista]s [habla]p [sobre su divina 
inspiraci?n]t: ["Yo quer?a que ..."]m. 
(4) In a letter written by Dal? at Neuilly in April 
1951, [the artist]s [talks]p [about his divine 
 inspiration]t : ["I wanted to ..."]m.
As noted earlier, the opinion predicate can be a 
noun or a preposition such as seg?n / according to. 
In (5),  the source is the noun complement,  intro-
duced by  de /  of. In  (6),  the  source is  the  noun 
phrase within the prepositional phrase headed by 
seg?n / according to. 
(5) No tenemos por qu? criticar las 
[declaraciones]p de [Elizardo S?nchez]s. 
(5) We need not criticize [Elizardo 
 Sanchez?]f [statements] p . 
(6)  [Este  sistema  se  utiliza  en  Estados  Unidos 
desde 1982]m, [seg?n]p [Roque Pifarr?]f.
(6) [This system has been used in the 
United  States  since  1982]m,  [according  to]p 
[Roque Pifarr?]s.
 
Note that in (5) there is another opinion predic-
ate,  the verb  criticize, occurring in a non-factive 
context. The factivity of events is not addressed in 
this work, but it can be expected to affect opinion 
recognition.
3    The rule-based system
We developed a rule-based system for the identi-
fication of the opinion elements. The system takes 
as input a pre-processed text using the POS-tagger 
Freeling (Atserias et al, 2006) and Clatex (Won-
sever et al 2006), a system that segments texts into 
propositions.  Several  rule  modules  are  then  ap-
plied,  introducing  XML annotations  showing the 
identified opinions and their elements. 
The  following  example  illustrates  the  system 
output: 
56
<opinion><menssage>Hasta el momento el virus 
H1N1 tiene una predominancia mayor que la de 
los dem?s virus en esos estudios</message>,
<predicate>precis?</predicate><source>la 
ministra</source></opinion>. 
<opinion><message>So far, the H1N1 virus has a 
higher prevalence than other viruses in these stud-
ies</message>, <predicate>said</predicate> 
<source>Minister</source></opinion>.
 
The  rules  are  based  on  the  contextual  rules 
formalism defined by Wonsever and Minel (2004), 
including  some  further  extensions.  This  type  of 
rule allows the specification of contexts, exclusion 
zones, optionality, negation, and elimination of ex-
isting labels,  among others.  In addition, for each 
rule it is possible to check various conditions on its 
components, for example, membership in a list of 
words.  For applying  the rules we used a system 
implemented in Prolog.
The hand-written rules were derived from cor-
pus analysis.  They are grouped into modules ac-
cording to the element they recognize: opinion pre-
dicate (verbs, nouns and prepositions), source, top-
ic and message. There is also a final module that 
builds the entire opinion and some auxiliary mod-
ules: the complex noun phrase identifying module 
(El director del Hospital Maciel, Daniel Parada / 
The  director  of  the  Hospital  Maciel,  Daniel  
Parada) and the subjective elements and operators 
identifying module. Table 1 shows the number of 
rules contained in each module. In the next section 
we describe the source rules module.
module # rules
opinion predicate 27
source 42
topic 22
message 8
opinion 37
auxiliary 7
TOTAL 143
Table 1: Number of rules in each module
3.1 Source rules
In  order  to  show the  rules  features,  we  will  de-
scribe  the  source  module.  Table  2  shows  some 
(simplified) rules for source identification.  
 
fue1a no(prep), <np>, (zone,3), verOp
fue1b punt, verOp, (zone,3), <np>
fue1c punt, verOp, (zone,3), prep, np, <np>
fue2 verOpPart,  "por", <np> 
fue3a nOp,  "de", <np>
fue3b <np>, verSup, op(det), nOp
fue3c nOp, verSupPart, "por", <np>
fue4a "seg?n", op(verOp), <np>
fue4b endS,  "para", <np>
fue4c "de acuerdo a", <np>
fue4d "de acuerdo con", <np>
fue4e "a juicio de", <np>
Table  2  Simplified  rules  for  source  recognition. 
Notation used: np - nominal phrase; < > - element 
labeled by the rule; zone,x - exclusion zone up to x 
words; verOpFin - finite opinion verb; verOpPart - 
opinion verb, participle;  nOp - opinion noun; ver-
Sup - support verb; endS - end of sentence; det - 
determiner; op - optionality operator 
These rules assign the source tag to text segments 
that match the rule body (indicated by <> in the 
table).  The  elements  that  precede  the  body  and 
those that follow it are the left and right contexts, 
respectively.  In addition to assigning the tag, the 
rules assign values to some attributes: 
- code of the rule that assigned the label 
- syntactic structure (subject before verb / sub-
ject after verb / noun complement introduced by 
de)
- semantic orientation value (-, +, 0)
The three rules fue1 identify sources that are the 
subject of an opinion verb. We allow up to 3 words 
between the subject and the verb; these words can-
not be verb, np, punctuation or conjunction (<El 
senador> este martes dijo ... / <the senator> said  
Tuesday ...). For rule fue1c we also allow a prepos-
itional phrase (prep + np) between the source and 
the  verb  (..., dijo  ayer  a  la  prensa  <el  
senador> / ..., said yesterday to the reporters the  
senator). As mentioned, we show simplified rules; 
the actual rules include other restrictions such as 
checking for subject-verb agreement. 
Rule fue2 is applied when the opinion verb is in 
participle form and the source is an agent comple-
ment (las palabras expresadas por el senador / the  
words uttered by the senator). 
57
The three rules fue3 concern noun phrases. The 
source is usually introduced by  de  (las opiniones 
del senador / the senator?s opinions) but it is also 
common to find nouns in a support verb construc-
tion (el senador emiti?  una declaraci?n / the sen-
ator issued a statement). 
Finally, the five rules fue4 identify sources in-
troduced by seg?n, para, de acuerdo a, de acuerdo 
con, a juicio de / according to. When the source 
introducer is  seg?n,  we can find an opinion verb 
between seg?n and the source (seg?n el senador /  
seg?n dijo el senador / according to the senator), 
For the preposition  para / for,  preceding punctu-
ation is required because of its high ambiguity. 
3.2 Lexical Resources 
Some of the rules, especially those for opinion pre-
dicate  identification,  rely  heavily  on  lexical  re-
sources: lists of opinion verbs and nouns, person 
indicators  (se?or, doctor, senador / Mr., Dr., sen-
ator), institution  indicators  (instituci?n,  hospital,  
diario  /  institution,  hospital,  journal), support 
verbs (plantear, emitir / make, deliver), topic intro-
ducers (sobre, con respecto a / about, with respect  
to), positive subjective elements (bueno, excelente,  
diversi?n / good, excellent, fun), negative subject-
ive elements (malo, negativo, pesimista / bad, neg-
ative,  pessimist), and operators  (muy,  extremada-
mente, a penas / very, extremely, just). 
In particular, the list of opinion verbs and nouns 
was  manually  created  from  corpora  containing 
Spanish texts: Corin (Grassi et al, 2001), Corpus 
del  Espa?ol  (Davies,  2002)  and  a  digital  media 
corpus created for this study. Only those verbs and 
nouns that are frequently used in opinion contexts 
were included in the list, so as to minimize ambi-
guity. At the time of evaluation, the list comprised 
86 verbs and 42 nouns. 
3.2.1 The opinion verbs and nouns list 
For each verb or noun, we register its lemma and 
other  information  related  to  its  syntactic  and se-
mantic properties. 
For verbs, we record the following information: 
? semantic orientation [-, 0, +] 
? semantic role of the subject [source, topic] 
? prepositions that introduce the subject. 
? subordinate clause admitted (message) 
For example, for the verb  decir  /  say, the cor-
responding values are (0, source, [ ], yes) for the 
verb apoyar / support: (+, source, [a, np], no), for 
the verb molestar / annoy: (-, topic, [ ], no). 
For nouns, the information of interest is: 
? semantic orientation [-, 0, +] 
? semantic  role  of  the  complement  intro-
duced by de [source, topic, ambiguous] 
For example, for the noun anuncio / announce-
ment, the corresponding values are (0, ambiguous). 
Note that this noun is ambiguous because the com-
plement introduced by de can be either the source 
(el anuncio del senador / the senator?s announce-
ment)  or the topic  (el anuncio de la extensi?n del  
plazo /  the announcement about the deadline ex-
tension).  For the noun  comentario / comment the 
values are (0,source) and for  apoyo / support  the 
values are (+, source).
The information associated to  opinion predic-
ates is taken into account when applying the rules. 
For example,  the second attribute of  the opinion 
noun is checked when rule fue3a is applied: if the 
attribute value is "source", the rule matches all np 
satisfying the remaining rule conditions, whereas if 
the  attribute  value  is  "ambiguous",  the  rule  re-
quires that the np contain a person or institution in-
dicator.  The  rule  does  not  apply  if  the  attribute 
value is "topic". 
Some  of  the  message  rules  (not  shown here) 
check that the final opinion verb attribute has the 
value "yes", indicating that it accepts a subordinate 
clause (dijo que ... / he said that ...). These rules la-
bel  the proposition following the verb as a mes-
sage. The proposition has already been segmented 
by Clatex.
The  attribute  that  indicates  which  is  the  verb 
subject role is important in differentiating the rules 
shown in the table (fue1 to fue4), which only re-
cognize verbs for which the subject role is source, 
from a set  of  additional  rules  (not  shown in the 
table) that look for the source in the dative case, 
when the subject role is topic (la propuesta gust? 
a los senadores / senators liked the proposal). 
3.3 Semantic orientation
For each element recognized, the rules assign a se-
mantic orientation value. For the opinion predicate, 
source and topic this value comes from the lexical 
resources. For the message, this value is calculated 
from  its  subjective  elements  and  operators.  We 
58
consider that the final opinion semantic orientation 
can be calculated from the orientation values of its 
elements.  We hypothesize  that  when the  opinion 
predicate  or  the  topic  introducer  are  not  neutral 
(they have a positive or negative semantic orienta-
tion) the complete opinion takes on the same value 
and  there  is  no  need  to  analyze  the  message.  If 
these two elements are neutral the opinion semant-
ic orientation must be obtained from the message. 
To determine the message semantic orientation 
we carried out some experiments that are still on-
going. Semantic orientation values for opinion pre-
dicates  are  stated  in  the  verb  and  noun  lists,  as 
mentioned. The semantic orientation for topic in-
troducers  is  also  stated  in  the  corresponding  list 
(sobre / about is neutral,  en contra de / against is 
negative,  etc.).  The  number  of  elements  of  this 
type is very limited.  We did not study the source 
semantic orientation, in future work we will ana-
lyze  expressions  like  Los  optimistas  sostienen 
que ... / Optimists say that ....
4 System evaluation 
To evaluate the system we worked with a digit-
al  media  corpus;  the  texts  were  taken  from  the 
same publications as those used to create the deriv-
ation corpus. The corpus contains 38 texts with an 
average of 300 words each, making a total size of 
approximately 13,000 words. 
We applied the system to the entire corpus and 
performed a manual review of the output in order 
to  evaluate  the  identification  of  the  defined  ele-
ments and also the complete opinion identification. 
We also made a partial semantic orientation evalu-
ation, taking into account only opinion predicates 
and topic introducers' values and their effect on the 
complete opinion value. 
In addition to assessing the rules performance, 
during the review stage the annotated corpus was 
manually corrected in order to obtain an opinion 
annotated  corpus  suitable  for  machine-learning. 
Table 3 shows the evaluation results. Rows repres-
ent: 
- total: total number of elements in the text,
- corr-c:  number  of  completely  recognized 
items, 
-  corr-p:  number  of  partially  recognized  ele-
ments, 
- non-rec: number of unrecognized elements, 
- incorr: number of marked segments which do 
not correspond to the item, 
- PR: precision, 
- REC-c: recall calculated using corr-c, 
- REC-p: recall calculated using corr-p, 
- F: F-measure.   
pred sour top mess opinion
total 281 212 74 243 302
corr-c 256 133 33 140 128
corr-p 0 20 13 64 104
no rec 25 57 28 39 70
incorr 23 11 2 10 14
PR 92 % 93 % 96 % 95 % 94 %
REC-c 91 % 63 % 45 % 58 % 42 %
REC-p 91 % 72 % 62 % 84 % 77 %
F 91.5 % 81 % 75 % 89 % 85 %
Table 3: System evaluation results. 
Most opinion predicates present  in the corpus 
are included in  our opinion verbs  and nouns list 
(91%). 
Several sources and topics were partially recog-
nized because the  rules  do not  incorporate  some 
complements (prepositional complements or subor-
dinate clause) to the noun phrase.
Message is partially recognized when a pseudo-
direct  discourse  is  used (Parada agreg? que "la 
empresa reconoci? que hubo un c?lculo entre hor-
as estimadas y horas reales y eso fue lo que pas?.  
Nosotros,  primero  empezamos  a  controlar  a  
nuestro personal ..."). This style is usually present 
in journalistic texts (Maldonado, 1999).
4.1 Semantic orientation evaluation
We recognized 25 non neutral opinion predicates 
in the corpus:  12 positive verbs and 14 negative 
verbs. One verb (especular / speculate) was incor-
rectly assigned a negative value, its means in this 
particular context is neutral. 
We just found 3 non-neutral topic introducers, 
the 3 are negative.    
The opinion predicates or topic introducers' se-
mantic  orientation  values  were  assigned  to  the 
opinions containing them. This method for calcu-
lating opinion semantic orientation was correct in 
59
all cases (except for the verb especular that was in-
correctly analyzed). 
5 Machine-learning system 
The evaluation system resulted in the generation of 
an annotated corpus, processed by the rule-based 
system and then manually reviewed and corrected. 
This corpus of about 13,000 words allows us to un-
dertake  some  experiments  applying   machine-
learning techniques. 
We are currently experimenting with Condition-
al  Random  Fields,  using  the  CRF++  tool 
(http://crfpp.sourceforge.net/). We are now determ-
ining the attributes to be considered for the training 
phase and defining the most appropriate templates 
for the kind of learning we need. While carrying 
out these prior tasks, we will extend the corpus us-
ing  the  same  semi-automatic  procedure  as  that 
already implemented.
6 Linguistic resources 
Many of the linguistic resources needed to achieve 
our objectives have already been mentioned. Some 
of them were created especially in the context of 
this work and are available as a contribution to the 
development of Spanish text processing: 
? opinion verbs and nouns lists with syntact-
ic and semantic attributes, 
? person and institution indicators lists, 
? topic introducers list, 
? subjective  elements  lists,  created  from 
available resources for  Spanish (Redondo 
et al 2007) and English (General Inquirer: 
www.wjh.harvard.edu),  the  latter  translated 
into Spanish, 
? subjective operators list. 
We also used some resources that are available 
for Spanish, including: 
? Freeling (POS-tagger), 
? Clatex (propositions analyzer). 
Freeling also provides a dependency parser that 
was not used here because the tests we carried out 
scored poorly in sentences containing opinions. 
Resources such as a semantic role tagger or an 
anaphora resolution tool could no doubt improve 
our  system,  but  as  far  as  we  know they are  not 
available for Spanish.
As we did for the General Inquirer dictionary, 
we can apply machine translation to other English 
resources:  subjective  dictionaries  and  annotated 
corpora (Brooke et al, 2009, Banea et al, 2009). 
Tools for  subjectivity analysis  in English can be 
applied to a translated Spanish raw corpus (Banea 
et al, 2009).
7 Conclusions 
We  have  implemented  a  rule-based  system  for 
opinion  identification  in  Spanish  texts.  We  have 
also created some resources for  Spanish:  opinion 
verbs and nouns lists, subjective elements lists and 
an opinion annotated corpus.  We think these  re-
sources are an important contribution to the devel-
opment of Spanish text processing. 
In our present work, we are experimenting with 
machine-learning techniques for recognizing opin-
ion elements.  The results  will  be  compared  with 
those obtained by the rule-based system. We hope 
to  improve  our  results  by  combining  rule-based 
and machine-learning modules.
References 
N. Asher, F. Benamara and Y. Mathieu. 2008. Distilling 
Opinion  in  Discourse:  A  Preliminary  Study.  COL-
ING ? Posters. 
J.  Atserias,  B.  Casas,  E.  Comelles,  M.  Gonz?lez,  L. 
Padr? and M. Padr?. 2006.  FreeLing 1.3: Syntactic  
and  semantic  services  in  an  open-source  NLP lib-
rary. In Proceedings of the fifth international confer-
ence on Language Resources and Evaluation (LREC) 
ELRA.
Carmen Banea,  Rada Mihalcea,  Janyce  Wiebe,  Samer 
Hassan. 2008. Multilingual Subjectivity Analysis Us-
ing Machine Translation.  Conference  on Empirical 
Methods in Natural Language Processing (EMNLP). 
J. Brooke, M. Tofiloski and M. Taboada. 2009. Cross-
Linguistic  Sentiment  Analysis:  From  English  to  
Spanish. RANLP 2009, Recent Advances in Natural 
Language Processing. Borovets, Bulgaria. 
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios 
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic  
extraction of opinion propositions and their holders. 
In AAAI Spring Symposium on Exploring Attitude 
and Affect in Text: Theories and Applications. 
Yejin  Choi,  Claire  Cardie,  Ellen Riloff  and Siddharth 
Patwardhan.  2005.  Identifying  sources  of  opinions 
with  conditional  random fields  and extraction  pat-
terns.  In  Proceedings of the Conference on Human 
Language  Technology  and  Empirical  Methods  in 
Natural  Language  Processing  (Vancouver,  British 
Columbia,  Canada).  Human  Language  Technology 
60
Conference. Association for Computational Linguist-
ics.
Yejin  Choi  and  Claire  Cardie.  2008.  Learning  with 
Compositional Semantics as Structural Inference for  
Subsentencial Sentiment Analysis. EMNLP. 
Mark Davies. 2002. Corpus del espa?ol (100 millones  
de palabras, siglo XIII - siglo XX). Disponible actual-
mente en http://www.corpusdelespanol.org.
J.  Garc?a-Miguel,  L.  Costas  and  S.  Mart?nez.  2005. 
Di?tesis  verbales  y  esquemas  construccionales.  
Verbos,  clases  sem?nticas  y  esquemas  sint?ctico-
sem?nticos en el proyecto ADESSE. Entre sem?ntica 
l?xica, teor?a del l?xico y sintaxis, 373-384.
Mariela Grassi,  Marisa  Malcuori,  Javier  Couto,  Juan 
Jos?  Prada  and  Dina  Wonsever. 2001. Corpus  in-
formatizado: textos del espa?ol del Uruguay (COR-
IN), SLPLT-2 - Second International  Workshop on 
Spanish Language Processing and Language Techno-
logies - Ja?n, Espa?a.
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online 
news media text. In Proceedings of the Workshop on 
Sentiment  and  Subjectivity  in  Text  (Sydney,  Aus-
tralia, July 22 - 22, 2006). ACL Workshops. Associ-
ation for Computational Linguistics, Morristown, NJ, 
1-8.  
Concepci?n Maldonado. 1999. Discurso directo y dis-
curso indirecto.  In Ignacio Bosque and  Violeta De-
monte, Gram?tica descriptiva de la lengua espa?ola 
(Entre la oraci?n y el discurso. Morfolog?a), 3549-
3596. 
K. Moilanen and S. Pulman. 2007. Sentiment Composi-
tion. In RANLP.
Bo  Pang and Lillian  Lee.  2008.  Opinion Mining and 
Sentiment  Analysis.  Foundations  and  Trends  in  In-
formation Retrieval 2(1-2), pp. 1?135.
L.  Polanyi  and A.  Zaenen.  2004.  Contextual  Valence  
Shifters. In AAAI spring Symposium on Attitude.
J. Redondo, I. Fraga, I. Padr?n and M. Comesa?a. 2007. 
The Spanish Adaptation of ANEW (Affective Norms  
for  English  Words).  Behavior  Research  Methods, 
39(3):600-605, Agosto.
Josef Ruppenhofer, Swapna Somasundaran and Janyce 
Wiebe.  2008.  Finding  the  Sources  and  Targets  of  
Subjective Expressions. The Sixth International Con-
ference  on  Language  Resources  and  Evaluation 
(LREC 2008). 
Veselin Stoyanov and Claire Cardie. 2008.  Annotating 
Topics of Opinions. Proceedings of the Sixth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2008), Marrakech, Morocco.
Carlos Subirats-R?ggeberg and Miriam R. L.  Petruck. 
2003.  Surprise: Spanish FrameNet!  In  E. Hajicova, 
A. Kotesovcova & Jiri Mirovsky (eds.), Proceedings 
of CIL 17. CD-ROM. Prague: Matfyzpress.
P. Turney and M. Littman. 2003. Measuring Praise and 
Criticism:  Inference  of  Semantic  Orientation  from 
Association. In  ACM  Transactions  on  Information 
Systems, 21:315--346.
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. 
Annotating expressions of opinions and emotions in  
language.  In  Language  Resources  and  Evaluation 
(formerly  Computers  and  the  Humanities), 39(2-
3):165210.
Dina Wonsever and Jean-Luc Minel. 2004. Contextual 
Rules for Text Analysis. En Lecture Notes in Com-
puter Science.
Dina Wonsever, Serrana Caviglia, Javier Couto and Ai-
ala Ros? and. 2006.  Un sistema para la segmenta-
ci?n en proposiciones de textos en espa?ol. In Letras 
de hoje 144 (41).
61
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 37?46, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Improving Speculative Language Detection using Linguistic Knowledge
Guillermo Moncecchi
Facultad de Ingenier??a
Universidad de la Repu?blica
Montevideo, Uruguay
Jean-Luc Minel
Laboratoire MoDyCo
Universite? Paris Ouest
Nanterre La De?fense, France
Dina Wonsever
Facultad de Ingenier??a
Universidad de la Repu?blica
Montevideo, Uruguay
Abstract
In this paper we present an iterative method-
ology to improve classifier performance by in-
corporating linguistic knowledge, and propose
a way to incorporate domain rules into the
learning process. We applied the methodol-
ogy to the tasks of hedge cue recognition and
scope detection and obtained competitive re-
sults on a publicly available corpus.
1 Introduction
A common task in Natural Language Processing
(NLP) is to extract or infer factual information from
textual data. In the field of natural sciences this task
turns out to be of particular importance, because
science aims to discover or describe facts from the
world around us. Extracting these facts from the
huge and constantly growing body of research ar-
ticles in areas such as, for example, molecular biol-
ogy, becomes increasingly necessary, and has been
the subject of intense research in the last decade
(Ananiadou et al, 2006). The fields of information
extraction and text mining have paid particular atten-
tion to this issue, seeking to automatically populate
structured databases with data extracted or inferred
from text. In both cases, the problem of speculative
language detection is a challenging one, because it
may correspond to a subjective attitude of the writer
towards the truth value of certain facts, and that in-
formation should not be lost when the fact is ex-
tracted or inferred.
When researchers express facts and relations in
their research articles, they often use speculative lan-
guage to convey their attitude to the truth of what
is said. Hedging, a term first introduced by Lakoff
(1973) to describe ?words whose job is to make
things fuzzier or less fuzzy? is ?the expression of ten-
tativeness and possibility in language use? (Hyland,
1995), and is extensively used in scientific writing.
Hyland (1996a) reports one hedge in every 50 words
of a corpus of research articles; Light et al (2004)
mention that 11% of the sentences in MEDLINE
contain speculative language. Vincze et al (2008)
report that 18% of the sentences in the scientific ab-
stracts section of the Bioscope corpus correspond to
speculations.
Early work on speculative language detection
tried to classify a sentence either as speculative
or non-speculative (see, for example, Medlock and
Briscoe (2007)). This approach does not take into
account the fact that hedging usually affects propo-
sitions or claims (Hyland, 1995) and that sentences
often include more than one of them. When the Bio-
scope corpus (Vincze et al, 2008) was developed
the notions of hedge cue (corresponding to what was
previously called just ?hedges? in the literature) and
scope (the propositions affected by the hedge cues)
were introduced. In this context, speculative lan-
guage recognition can be seen as a two-phase pro-
cess: first, the existence of a hedge cue in a sentence
is detected, and second, the scope of the induced
hedge is determined. This approach was first used
by Morante et al (2008) and subsequently in many
of the studies presented in the CoNLL-2010 Confer-
ence Shared Task (Farkas et al, 2010a), and is the
one used in this paper.
For example, the sentence
(1) This finding {suggests suggests that the BZLF1
37
promoter {may may be regulated by the degree
of squamous differentiation}may}suggests.
contains the word ?may? that acts as a hedge cue
(i.e. attenuating the affirmation); this hedge only
affects the propositions included in the subordinate
clause that contains it.
Each of these phases can be modelled (albeit with
some differences, described in the following sec-
tions) as a sequential classification task, using a sim-
ilar approach to that commonly used for named en-
tity recognition or semantic labelling: every word
in the sentence is assigned a class, identifying spans
of text (as, for example, scopes) with, for example, a
special class for the first and last element of the span.
Correctly learning these classes is the computational
task to be solved.
In this paper we present a methodology and ma-
chine learning system implementing it that, based
on previous work on speculation detection, studies
how to improve recognition by analysing learning
errors and incorporating advice from domain experts
in order to solve the errors without hurting overall
performance. The methodology proposes the use of
domain knowledge rules that suggest a class for an
instance, and shows how to incorporate them into
the learning process. In our particular task domain
knowledge is linguistic knowledge, as hedging and
scopes issues are general linguistic devices. In this
paper we are going both terms interchangeably.
The paper is organized as follows. In Section 2
we review previous theoretical work on speculative
language and the main computational approaches to
the task of detecting speculative sentences. Section
3 briefly describes the corpus used for training and
evaluation. In Section 4 we present the specific com-
putational task to which our methodology was ap-
plied. In Section 5 we present the learning method-
ology we propose to use, and describe the system
we implemented, including lexical, syntactic and se-
mantic attributes we experimented with. We present
and discuss the results obtained in Section 6. Finally,
in Section 7 we analyse the approach presented here
and discuss its advantages and problems, suggesting
future lines of research.
2 Related work
The grammatical phenomenon of modality, defined
as ?a category of linguistic meaning having to do
with the expression of possibility and necessity?
(von Fintel, 2006) has been extensively studied in
the linguistic literature. Modality can be expressed
using different linguistic devices: in English, for ex-
ample, modal auxiliaries (such as ?could? or ?must?),
adverbs (?perhaps?), adjectives (?possible?), or other
lexical verbs (?suggest?,?indicate?), are used to ex-
press the different ways of modality. Other lan-
guages express modality in different forms, for ex-
ample using the subjunctive mood. Palmer (2001)
considers modality as the grammaticalization of
speakers? attitudes and opinions, and epistemic
modality, in particular, applies to ?any modal sys-
tem that indicates the degree of commitment by the
speaker to what he says?.
Although hedging is a concept that is closely
related to epistemic modality, they are different:
modality is a grammatical category, whereas hedg-
ing is a pragmatic position (Morante and Sporleder,
2012). This phenomenon has been theoretically
studied in different domains and particularly in sci-
entific writing (Hyland, 1995; Hyland, 1996b; Hy-
land, 1996a).
From a computational point of view, speculative
language detection is an emerging area of research,
and it is only in the last five years that a relatively
large body of work has been produced. In the re-
mainder of this section, we survey the main ap-
proaches to hedge recognition, particularly in En-
glish and in research discourse.
Medlock and Briscoe (2007) applied a weakly su-
pervised learning algorithm to classify sentences as
speculative or non-speculative, using a corpus they
built and made publicly available. Morante and
Daelemans (2009) not only tried to detect hedge
cues but also to identify their scope, using a met-
alearning approach based on three supervised learn-
ing methods. They achieved an F1 of 84.77 for
hedge identification, and 78.54 for scope detection
(using gold-standard hedge signals) in the Abstracts
sections of the Bioscope corpus.
Task 2 of the CoNLL-2010 Conference Shared
Task (Farkas et al, 2010b) proposed solving the
problem of in-sentence hedge cue phrase identi-
38
fication and scope detection in two different do-
mains (biological publications and Wikipedia arti-
cles), based on manually annotated corpora. The
evaluation criterion was in terms of precision, recall
and F-measure, accepting a scope as correctly clas-
sified if the hedge cue and scope boundaries were
both correctly identified.
The best result on hedge cue identification (Tang
et al, 2010) obtained an F-score of 81.3 using a su-
pervised sequential learning algorithm to learn BIO
classes from lexical and shallow parsing informa-
tion, also including certain linguistic rules. For
scope detection, Morante et al (2010) obtained an
F-score of 57.3, using also a sequence classification
approach for detecting boundaries (tagged in FOL
format, where the first token of the span is marked
with an F, while the last one is marked with an
L). The attributes used included lexical information,
dependency parsing information, and some features
based on the information in the parse tree.
The approximation of Velldal et al (2010) for
scope detection was somewhat different: they de-
veloped a set of handcrafted rules, based on depen-
dency parsing and lexical features. With this ap-
proach, they achieved an F-score of 55.3, the third
best for the task. Similarly, Kilicoglu and Bergler
(2010) used a pure rule-based approach based on
constituent parse trees in addition to syntactic de-
pendency relations, and achieved the fourth best F-
score for scope detection, and the highest precision
of the whole task (62.5). In a recent paper, Vell-
dal et al (2012) reported a better F-score of 59.4 on
the same corpus for scope detection using a hybrid
approach that combined a set of rules on syntactic
features and n-gram features of surface forms and
lexical information and a machine learning system
that selected subtrees in constituent structures.
3 Corpus
The system presented in this paper uses the Bio-
scope corpus (Vincze et al, 2008) as a learning
source and for evaluation purposes. The Bioscope
corpus is a freely available corpus of medical free
texts, biological full papers and biological abstracts,
annotated at a token level with negative and specu-
lative keywords, and at sentence level with their lin-
guistic scope.
Clinical Full Abstract
#Documents 954 9 1273
#Sentences 6383 2670 11871
%Hedge Sentences 13.4 19.4 17.7
#Hedge cues 1189 714 2769
Table 1: Bioscope corpus statistics about hedging
Table 1, extracted from Vincze et al (2008), gives
some statistics related to hedge cues and sentences
for the three sub corpora included in Bioscope.
For the present study, we usee only the Abstract
sub corpus for training and evaluation. We randomly
separated 20% of the corpus, leaving it for evalu-
ation purposes. We further sub-divided the remain-
ing training corpus, separating another 20% that was
used as a held out corpus. All the models presented
here were trained on the resulting training corpus
and their performance evaluated on the held out cor-
pus. The final results were computed on the previ-
ously unseen evaluation corpus.
4 Task description
From a computational point of view, both hedge
cue identification and scope detection can be seen
as a sequence classification problem: given a sen-
tence, classify each token as part of a hedge cue (or
scope) or not. In almost every classification prob-
lem, two main approaches can be taken (although
many variations and combinations exist in the lit-
erature): build the classifier as a set of handcrafted
rules, which, from certain attributes of the instances,
decide which category it belongs to, or learn the
classifier from previously annotated examples, in a
supervised learning approach.
The rules approach is particularly suitable when
domain experts are available to write the rules, and
when features directly represent linguistic informa-
tion (for example, POS-tags) or other types of do-
main information. It is usually a time-consuming
task, but it probably grasps the subtleties of the lin-
guistic phenomena studied better, making it possible
to take them into account when building the classi-
fier. The supervised learning approach needs tagged
data; in recent years the availability of tagged text
39
has grown, and this type of method has become the
state-of-the-art solution for many NLP problems.
In our particular problem, we have both tagged
data and expert knowledge (represented by the body
of work on modality and hedging), so it seems rea-
sonable to see how we can combine the two methods
to achieve better classification performance.
4.1 Identifying hedge cues
The best results so far for this task used a token
classification approach or sequential labelling tech-
niques, as Farkas et al (2010b) note. In both cases,
every token in the sentence is assigned a class la-
bel indicating whether or not that word is acting as a
hedge cue. To allow for multiple-token hedge cues,
we identify the first token of the span with the class
B and every other token in the span with I, keeping
the O class for every token not included in the span,
as the following example shows:
(2) The/O findings/O indicate/B that/I MNDA/O
expression/O is/O . . . [ 401.8]
After token labelling, hedge cue identification can
be seen as the problem of assigning the correct class
to each token of an unlabelled sentence. Hedge cue
identification is a sequential classification task: we
want to assign classes to an entire ordered sequence
of tokens and try to maximize the probability of as-
signing the correct classes to every token in the se-
quence, considering the sequence as a whole, not
just as a set of isolated tokens.
4.2 Determining the scope of hedge cues
The second sub-task involves marking the part of the
sentence affected by the previously identified hedge
cue. Scopes are also spans of text (typically longer
than multi-word hedge cues), so we could use the
same reduction to a token classification task. Be-
ing longer, FOL classes are usually used for clas-
sification, identifying the first token of the scope
as F, the last token as L and any other token in
the sentence as O. Scope detection poses an addi-
tional problem: hedge cues cannot be nested, but
scopes (as we have already seen) usually are. In
example 1, the scope of ?may? is nested within the
scope of ?suggests?. To overcome this, Morante
and Daelemans (2009) propose to generate a dif-
ferent learning example for each cue in the sen-
tence. In this setting, each example becomes a pair
?labelled sentence, hedge cue position?. So, for ex-
ample 1, the scope learning instances would be:
(3) ?This/O finding/O suggests/F that/O the/O
BZLF1/O promoter/O may/O be/O
regulated/O by/O the/O degree/O of/O
squamous/O differentiation/L./O, 3?
(4) ?This/O finding/O suggests/O that/O the/F
BZLF1/O promoter/O may/O be/O
regulated/O by/O the/O degree/O of/O
squamous/O differentiation/L./O, 8?
Learning on these instances, and using a similar
approach to the one used in the previous task, we
should be able to identify scopes for previously un-
seen examples. Of course, the two tasks are not in-
dependent: the success of the second one depends
on the success of the first. Accordingly, evaluation
of the second task can be done using gold standard
hedge cues or with the hedge cues learned in the first
task.
5 Methodology and System Description
To approach both sequential learning tasks, we fol-
low a learning methodology (depicted in Figure 1),
that starts with an initial guess of attributes for su-
pervised learning and a learning method, and tries
to improve its performance by incorporating domain
knowledge. We consider that expressing this knowl-
edge through rules (instead of learning features) is a
better way for a domain expert to suggest new use-
ful information or to generalize certain relations be-
tween attributes and classification results when the
learning method cannot achieve this because of in-
sufficient training data. These rules, of course, have
to be converted to attributes to incorporate them into
the learning process. These attributes are what we
call knowledge rules and their generation will be de-
scribed in the Analysis section.
5.1 Preprocessing
Before learning, we propose to add every possible
item of external information to the corpus so as to
integrate different sources of knowledge (either the
result of external analysis or in the form of seman-
tic resources). After this step, all the information
is consolidated into a single structure, facilitating
40
subsequent analysis. In our case, we incorporate
POS-tagging information, resulting from the appli-
cation of the GENIA tagger (Tsuruoka et al, 2005),
and deep syntax information obtained with the ap-
plication of the Stanford Parser (Klein and Manning,
2003), leading to a syntax-oriented representation of
the training data. For a detailed description of the
enriching process, the reader is referred to Moncec-
chi et al (2010).
5.2 Initial Classifier
The first step for improving performance is, of
course, to select an initial set of learning features,
and learn from training data to obtain the first clas-
sifier, in a traditional supervised learning scenario.
The sequential classification method will depend on
the addressed task. After learning, the classifier is
applied on the held out corpus to evaluate its per-
formance (usually in terms of Precision, Recall and
F1-measure), yielding performance results and a list
of errors for analysis. This information is the source
for subsequent linguistic analysis. As such, it seems
important to provide ways to easily analyse instance
attributes and learning errors. For our tasks, we
have developed visualization tools to inspect the tree
representation of the corpus data, the learning at-
tributes, and the original and predicted classes.
5.3 Analysis
From the classifier results on the held-out corpus,
an analysis phase starts, which tries to incorporate
linguistic knowledge to improve performance.
One typical form of introducing new information
is through learning features: for example, we can
add a new attribute indicating if the current instance
(in our case, a sentence token) belongs to a list of
common hedge cues.
However, linguistic or domain knowledge can
also naturally be stated as rules that suggest the class
or list of classes that should be assigned to instances,
based on certain conditions on features, linguistic
knowledge or data observation. For example, based
on corpus annotation guidelines, a rule could state
that the scope of a verb hedge cue should be the verb
phrase that includes the cue, as in the expression
(5) This finding {suggests suggests that the BZLF1
promoter may be regulated by the degree of
squamous differentiation}suggests.
We assume that these rules take the form ?if a con-
dition C holds then classify instance X with class Y?.
In the previous example, assuming a FOL format
for scope identification, the token ?suggest? should
be assigned class F and the token ?differentiation?
should be assigned class L, assigning class O to ev-
ery other token in the sentence.
The general problem with these rules is that as
we do not know in fact if they always apply, we do
not want to directly modify the classification results,
but to incorporate them as attributes for the learning
task. To do this, we propose to use a similar ap-
proach to the one used by Rosa? (2011), i.e. to incor-
porate these rules as a new attribute, valued with the
class predictions of the rule, trying to ?help? the clas-
sifier to detect those cases where the rule should fire,
without ignoring the remaining attributes. In the pre-
vious example, this attribute would be (when the rule
condition holds) valued F or L if the token corre-
sponds to the first or last word of the enclosing verb
phrase, respectively. We have called these attributes
knowledge rules to reflect the fact that they suggest
a classification result based on domain knowledge.
This configuration allows us to incorporate
heuristic rules without caring too much about their
potential precision or recall ability: we expect the
classification method to do this for us, detecting cor-
relations between the rule result (and the rest of the
attributes) and the predicted class.
There are some cases where we do actually want
to overwrite classifier results: this is the case when
we know the classifier has made an error, because
the results are not well-formed. For example, we
have included a rule that modifies the assigned
classes when the classifier has not exactly found one
F token and one L token, as we know for sure that
something has gone wrong. In this case, we decided
to assign the scope based on a series of postprocess-
ing rules: for example, assign the scope of the en-
closing clause in the syntax tree as hedge scope, in
the case of verb hedge cues.
For sequential classification tasks, there is an ad-
ditional issue: sometimes the knowledge rule indi-
cates the beginning of the sequence, and its end can
be determined using the remaining attributes. For
example, suppose the classifier suggests the class
41
Hedge PPOS GPPOS Lemma PScope GPScope Scope
O VP S This O O O
O VP S finding O O O
O VP S suggest O O O
O VP S that O O O
O VP S the O F F
O VP S BZLF1 O O O
O VP S promoter O O O
B VP S may F O O
O VP S be O O O
O VP S regulate O O O
O VP S by O O O
O VP S the O O O
O VP S degree O O O
O VP S of O O O
O VP S squamous O O O
O VP S differentiation L L O
O VP S . O O O
Table 2: Evaluation instance where the scope ending
could not be identified
scope in the learning instance shown in table 2 (us-
ing as attributes the scopes of the parent and grand-
parent constituents for the hedge cue in the syntax
tree). If we could associate the F class suggested
by the classifier with the grand parent scope rule,
we would not be concerned about the prediction for
the last token, because we would knew it would al-
ways correspond to the last token of the grand par-
ent clause. To achieve this, we modified the class we
want to learn, introducing a new class, say X, instead
of F, to indicate that, in those cases, the L token must
not be learned, but calculated in the postprocessing
step, in terms of other attributes? values (in this ex-
ample, using the hedge cue grandparent constituent
limits). This change also affects the classes of train-
ing data instances (in the example, every training
instance where the scope coincides with the grand
parent scope attribute will have its F-classified to-
ken class changed to X).
In the previous example, if the classifier assigns
class X to the ?the? token, the postprocessing step
will change the class assigned to the ?differentiation?
token to L, no matter which class the classifier had
predicted, changing also the X class to the original
F, yielding a correctly identified scope.
After adding the new attributes and changing the
relevant class values in the training set, the process
starts over again. If performance on the held out cor-
pus improves, these attributes are added to the best
configuration so far, and used as the starting point
for a new analysis. When no further improvement
can be achieved, the process ends, yielding the best
Figure 1: Methodology overview
classifier as a result.
We applied the proposed methodology to the tasks
of hedge cue detection and scope resolution. We
were mainly interested in evaluating whether sys-
tematically applying the methodology would indeed
improve classifier performance. The following sec-
tions show how we tackled each task, and how we
managed to incorporate expert knowledge and im-
prove classification.
5.4 Hedge Cue Identification
To identify hedge cues we started with a sequen-
tial classifier based on Conditional Random Fields
(Lafferty et al, 2001), the state-of-the-art classifi-
cation method used for sequence supervised learn-
ing in many NLP tasks. The baseline configuration
we started with included a size-2 window of surface
forms to the left and right of the current token, pairs
and triples of previous/current surface forms. This
led to a highly precise classifier (an F-measure of
95.5 on the held out corpus). After a grid search
on different configurations of surface forms, lemmas
and POS tags, we found (somewhat surprisingly)
that the best precision/recall tradeoff was obtained
just using a window of size 2 of unigrams of sur-
face forms, lemmas and tokens with a slightly worse
precision than the baseline classifier, but compen-
42
Configuration P R F1
Baseline 95.5 74.0 83.4
Conf1 94.7 80.3 86.9
Conf2 91.3 84.0 87.5
Table 3: Classification performance on the held out cor-
pus for hedge cue detection. Conf1 corresponds to win-
dows of Word, Lemma and POS attributes and Conf2 in-
corporates hedge cue candidates and cooccuring words
sated by an improvement of about six points in re-
call, achieving an F-score of 86.9.
In the analysis step of the methodology we found
that most errors came from False Negatives, i.e.
words incorrectly not marked as hedges. We also
found that those words actually occurred in the train-
ing corpus as hedge cues, so we decided to add new
rule attributes indicating membership to certain se-
mantic classes. After checking the literature, we
added three attributes:
? Hyland words membership: this feature was set
to Y if the word was part of the list of words
identified by Hyland (2005)
? Hedge cue candidates: this feature was set to
Y if the word appeared as a hedge cue in the
training corpus
? Words co-occurring with hedge cue candidates:
this feature was set to Y if the word cooccured
with a hedge cue candidate in the training cor-
pus. This feature is based on the observation
that 43% of the hedges in a corpus of scientific
articles occur in the same sentence as at least
another device (Hyland, 1995).
After adding these attributes and tuning the win-
dow sizes, performance improved to an F-score of
87.5 in the held-out corpus
5.5 Scope identification
To learn scope boundaries, we started with a similar
configuration of a CRF classifier, using a window of
size 2 of surface forms, lemmas and POS-tags, and
the hedge cue identification attribute (either obtained
from the training corpus when using gold standard
hedge cues or learned in the previous step), achiev-
ing a performance of 63.7 in terms of F-measure.
When we incorporated information in the form of a
knowledge rule that suggested the scope of the con-
stituent of the parsing tree headed by the parent node
of the first word of the hedge cue, and an attribute
containing the parent POS-tag, performance rapidly
improved about two points measured in terms of F-
score.
After several iterations, and analyzing classifica-
tion errors, we included several knowledge rules, at-
tributes and postprocessing rules that dramatically
improved performance on the held-out corpus:
? We included attributes for the scope of the next
three ancestors of the first word of the hedge
cue in the parsing tree, and their respective
POS-tags, in a similar way as with the parent.
We also included a trigram with the ancestors
POS from the word upward in the tree.
? For parent and grandparent scopes, we incor-
porated X and Y classes instead of F, and mod-
ified postprocessing to use the last token of the
corresponding scope when one of these classes
was learned.
? We modified the ancestors scopes to reflect
some corpus annotation guidelines or other cri-
teria induced after data examination. For ex-
ample, we decided not to include adverbial
phrases or prepositional phrases at the begin-
ning of scopes, when they corresponded to a
clause, as in
(6) In addition,{unwanted and potentially
hazardous specificities may be
elicited. . .}
? We added postprocessing rules to cope with
cases where (probably due to insufficient train-
ing data), the classifier missclasified certain in-
stances. For example, we forced classification
to use the next enclosing clause (instead of verb
phrase), when the hedge cue was a verb conju-
gated in passive voice, as in
(7) {GATA3 , a member of the GATA family
that is abundantly expressed in the
T-lymphocyte lineage , is thought to
participate in ...}.
43
Configuration Gold-P P R F1
Baseline 66.4 68.6 59.6 63.8
Conf1 68.7 71.3 61.8 66.2
Conf2 73.3 75.6 65.4 70.1
Conf3 80.9 82.1 71.3 76.3
Conf4 88.2 82.0 76.3 79.1
Table 4: Classification performance on the held out cor-
pus. The baseline used a window of Word, Lemma,
POS attributes and hedge cue tag; Conf1 included parent
scopes, Conf2 added grandparents information; Conf3
added postprocessing rules. Finally, Conf4 used adjusted
scopes and incorporated new postprocessing rules
? We excluded references at the end of sentences
from all the calculated scopes.
? We forced classification to the next S,VP or NP
ancestor constituent in the syntax tree (depend-
ing on the hedge cue POS), when full scopes
could not be determined by the statistical clas-
sifier (missing either L or F, or learning more
than one of them in the same sentence).
Table 4 summarizes the results of scope identifi-
cation in the held out corpus. The first results were
obtained using gold-standard hedge cues, while the
second ones used the hedge cues learned in the pre-
vious step (for hedge cue identification, we used the
best configuration we found). In the gold-standard
results, Precision, Recall and the F-measure are
the same because every False Positive (incorrectly
marked scope) implied a False Negative (the missed
right scope).
6 Evaluation
To determine classifier performance, we evaluated
the classifiers found after improvement on the eval-
uation corpus. We also evaluated the less efficient
classifiers to see whether applying the iterative im-
provement had overfitted the classifier to the corpus.
To evaluate scope detection, we used the best con-
figuration found in the evaluation corpus for hedge
cue identification. Tables 5 and 6 show the results
for the hedge cue recognition and scope resolution,
respectively. In both tasks, classifier performance
Configuration P R F1
Baseline 97.9 78.0 86.8
Conf1 95.9 84.9 90.1
Conf2 94.1 88.6 91.3
Table 5: Classification performance on the evaluation
corpus for hedge cue detection
Configuration Gold-P P R F1
Baseline 74.0 71.9 68.1 70.0
Conf1 76.5 74.4 70.2 72.3
Conf2 80.0 77.2 72.9 75.0
Conf3 83.1 80.0 75.2 77.3
Conf4 84.7 80.1 75.8 77.9
Table 6: Classification performance on the evaluation
corpus for scope detection
improved in a similar way to the results obtained on
the held out corpus.
Finally, to compare our results with state-of-the-
art methods (even though that was not the main
objective of the study), we used the corpus of de
CoNLL 2010 Shared Task to train and evaluate our
classifiers, using the best configurations found in the
evaluation corpus, and obtained competitive results
in both subtasks of Task 2. Our classifier for hedge
cue detection achieved an F-measure of 79.9, bet-
ter than the third position in the Shared Task for
hedge identification. Scope detection results (us-
ing learned hedge cues) achieved an F-measure of
54.7, performing better than the fifth result in the
corresponding task, and five points below the best
results obtained so far in the corpus (Velldal et al,
Hedge cue iden-
tification
Scope detection
Best results 81.7/81.0/81.3 59.6/55.2/57.3
Our results 83.2/76.8/79.9 56.7/52.8/54.7
Table 7: Classification performance compared with
best results in CoNLL Shared Task. Figures represent
Precision/Recall/F1-measure
44
2012). Table 7 summarizes these results in terms of
Precision/Recall/F1-measure.
7 Conclusions and Future Research
In this paper we have presented an iterative method-
ology to improve classifier performance by incor-
porating linguistic knowledge, and proposed a way
to incorporate domain rules to the learning process.
We applied the methodology to the task of hedge
cue recognition and scope finding, improving per-
formance by incorporating information of training
corpus occurrences and co-occurrences for the first
task, and syntax constituents information for the sec-
ond. In both tasks, results were competitive with
the best results obtained so far on a publicly avail-
able corpus. This methodology could be easily used
for other sequential (or even traditional) classifica-
tion tasks.
Two directions are planned for future research:
first, to improve the classifier results by incorporat-
ing more knowledge rules such as those described by
Velldal et al (2012) or semantic resources, specially
for the scope detection task. Second, to improve the
methodology, for example by adding some way to
select the most common errors in the held out cor-
pus and write rules based on their examination.
References
S. Ananiadou, D. Kell, and J. Tsuj. 2006. Text min-
ing and its potential applications in systems biology.
Trends in Biotechnology, 24(12):571?579, December.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010a. The CoNLL-
2010 shared task: Learning to detect hedges and their
scope in natural language text. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 1?12, Uppsala, Sweden,
July. Association for Computational Linguistics.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Szarvas,
Gyo?rgy Mo?ra, and Ja?nos Csirik, editors. 2010b. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Uppsala, Sweden, July.
Ken Hyland. 1995. The author in the text: Hedging sci-
entific writing. Hongkong Papers in Linguistics and
Language Teaching, 18:33?42.
Ken Hyland. 1996a. Talking to the academy: Forms of
hedging in science research articles. Written Commu-
nication, 13(2):251?281.
Ken Hyland. 1996b. Writing without conviction? Hedg-
ing in science research articles. Applied Linguistics,
17(4):433?454, December.
Ken Hyland. 2005. Metadiscourse: Exploring Interac-
tion in Writing. Continuum Discourse. Continuum.
Halil Kilicoglu and Sabine Bergler. 2010. A high-
precision approach to detecting hedges and their
scopes. In Proceedings of the Fourteenth Conference
on Computational Natural Language Learning, pages
70?77, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423?430, Morristown, NJ,
USA. Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML-01, pages 282?289.
George Lakoff. 1973. Hedges: A study in meaning crite-
ria and the logic of fuzzy concepts. Journal of Philo-
sophical Logic, 2(4):458?508, October.
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and
statements in between. In Lynette Hirschman and
James Pustejovsky, editors, HLT-NAACL 2004 Work-
shop: BioLINK 2004, Linking Biological Literature,
Ontologies and Databases, pages 17?24, Boston,
Massachusetts, USA, May. Association for Computa-
tional Linguistics.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Guillermo Moncecchi, Jean-Luc Minel, and Dina Won-
sever. 2010. Enriching the bioscope corpus with lex-
ical and syntactic information. In Workshop in Natu-
ral Language Processing and Web-based Tecnhologies
2010, pages 137?146, November.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special issue.
Computational Linguistics, pages 1?72, February.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008. Learning the scope of negation in
biomedical texts. In EMNLP ?08: Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715?724, Morristown, NJ,
USA. Association for Computational Linguistics.
45
Roser Morante, Vincent Van Asch, and Walter Daele-
mans. 2010. Memory-based resolution of in-sentence
scopes of hedge cues. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 40?47, Uppsala, Sweden, July.
Association for Computational Linguistics.
R. F. Palmer. 2001. Mood and Modality. Cambridge
Textbooks in Linguistics. Cambridge University Press,
New York.
Aiala Rosa?. 2011. Identificacio?n de opiniones de difer-
entes fuentes en textos en espan?ol. Ph.D. thesis, Uni-
versidad de la Repu?blica (Uruguay), Universite? Paris
Ouest (France), September.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 13?17,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Panayiotis
Bozanis and Elias N. Houstis, editors, Advances in In-
formatics, volume 3746, chapter 36, pages 382?392.
Springer Berlin Heidelberg, Berlin, Heidelberg.
Erik Velldal, Lilja ?vrelid, and Stephan Oepen. 2010.
Resolving speculation: Maxent cue classification and
dependency-based scope rules. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 48?55, Uppsala, Sweden,
July. Association for Computational Linguistics.
Erik Velldal, Lilja ?vrelid, Jonathon Read, and Stephan
Oepen. 2012. Speculation and negation: Rules,
rankers, and the role of syntax. Computational Lin-
guistics, pages 1?64, February.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The bioscope cor-
pus: biomedical texts annotated for uncertainty, nega-
tion and their scopes. BMC Bioinformatics, 9(Suppl
11):S9+.
Kail von Fintel, 2006. Modality and Language. MacMil-
lan Reference USA.
46
