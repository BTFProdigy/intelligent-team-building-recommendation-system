Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 725?735,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Multiword Expression Identification with Tree Substitution Grammars:
A Parsing tour de force with French
Spence Green*, Marie-Catherine de Marneffe?, John Bauer*, and Christopher D. Manning*?
*Computer Science Department, Stanford University
?Linguistics Department, Stanford University
{spenceg,mcdm,horatio,manning}@stanford.edu
Abstract
Multiword expressions (MWE), a known nui-
sance for both linguistics and NLP, blur the
lines between syntax and semantics. Previous
work onMWE identification has relied primar-
ily on surface statistics, which perform poorly
for longer MWEs and cannot model discontin-
uous expressions. To address these problems,
we show that even the simplest parsing mod-
els can effectively identify MWEs of arbitrary
length, and that Tree Substitution Grammars
achieve the best results. Our experiments show
a 36.4% F1 absolute improvement for French
over an n-gram surface statistics baseline, cur-
rently the predominant method for MWE iden-
tification. Our models are useful for several
NLP tasks in which MWE pre-grouping has
improved accuracy.
1 Introduction
Multiword expressions (MWE) have long been a
challenge for linguistic theory and NLP. There is
no universally accepted definition of the term, but
MWEs can be characterized as ?idiosyncratic inter-
pretations that cross word boundaries (or spaces)?
(Sag et al, 2002) such as traffic light, or as ?fre-
quently occurring phrasal units which are subject
to a certain level of semantic opaqueness, or non-
compositionality? (Rayson et al, 2010).
MWEs are often opaque fixed expressions, al-
though the degree to which they are fixed can vary.
Some MWEs do not allow morphosyntactic varia-
tion or internal modification (e.g., in short, but *in
shorter or *in very short). Other MWEs are ?semi-
fixed,? meaning that they can be inflected or undergo
internal modification. The type of modification is of-
ten limited, but not predictable, so it is not possible
to enumerate all variants (Table 1).
French English
? terme in the near term
? court terme in the short term
? tr?s court terme in the very short term
? moyen terme in the mediumterm
? long terme in the long term
? tr?s long terme in the very long term
Table 1: Semi-fixed MWEs in French and English. The
French adverb ? terme ?in the end? can be modified by
a small set of adjectives, and in turn some of these ad-
jectives can be modified by an adverb such as tr?s ?very?.
Similar restrictions appear in English.
Merging known MWEs into single tokens has
been shown to improve accuracy for a variety of
NLP tasks: dependency parsing (Nivre and Nilsson,
2004), constituency parsing (Arun andKeller, 2005),
sentence generation (Hogan et al, 2007), and ma-
chine translation (Carpuat andDiab, 2010). Most ex-
periments use gold MWE pre-grouping or language-
specific resources like WordNet. For unlabeled text,
the best MWE identification methods, which are
based on surface statistics (Pecina, 2010), suffer
from sparsity induced by longer n-grams (Ramisch
et al, 2010). A dilemma thus exists: MWE knowl-
edge is useful, but MWEs are hard to identify.
In this paper, we show the effectiveness of statis-
tical parsers for MWE identification. Specifically,
Tree Substitution Grammars (TSG) can achieve a
36.4% F1 absolute improvement over a state-of-the-
art surface statistics method. We choose French,
which has pervasive MWEs, for our experiments.
Parsing models naturally accommodate discontinu-
ous MWEs like phrasal verbs, and provide syntac-
tic subcategorization. By contrast, surface statistics
methods are usually limited to binary judgements for
contiguous n-grams or dependency bigrams.
725
FTB (train) WSJ (train)
Sentences 13,449 39,832
Tokens 398,248 950,028
#Word Types 28,842 44,389
#Tag Types 30 45
#Phrasal Types 24 27
Per Sentence
Depth (?/?2) 4.03 / 0.360 4.18 / 0.730
Breadth (?/?2) 13.5 / 6.79 10.7 / 4.59
Length (?/?2) 29.6 / 17.3 23.9 / 11.2
Constituents (?) 20.3 19.6
? Constituents / ? Length 0.686 0.820
Table 2: Gross corpus statistics for the pre-processed FTB
(training set) andWSJ (sec. 2-21). The FTB sentences are
longer with broader syntactic trees. The FTB POS tag set
has 33% fewer types than theWSJ. The FTB dev set OOV
rate is 17.77% vs. 12.78% for the WSJ.
Type #Total #Single %Single %Total
MWN noun 9,680 2,737 28.3 49.7
MWADV adverb 3,852 449 11.7 19.8
MWP prep. 3,526 342 9.70 18.1
MWC conj. 814 73 8.97 4.18
MWV verb. 585 243 41.5 3.01
MWD det. 328 69 21.0 1.69
MWA adj. 324 126 38.9 1.66
MWPRO pron. 266 33 12.4 1.37
MWCL clitic 59 1 1.69 0.30
MWET foreign 24 18 0.75 0.12
MWI interj. 4 2 0.50 0.02
19,462 4,093 21.0% 100.0%
Table 3: Frequency distribution of the 11 MWE subcate-
gories in the FTB (training set). MWEs account for 7.08%
of the bracketings and 13.0% of the tokens in the treebank.
Only 21% of the MWEs occur once (?single?).
We first introduce a new instantiation of the
French Treebank that, unlike previous work, does not
use gold MWE pre-grouping. Consequently, our ex-
perimental results also provide a better baseline for
parsing raw French text.
2 French Treebank Setup
The corpus used in our experiments is the French
Treebank (Abeill? et al (2003), version from June
2010, hereafter FTB). In French, there is a linguis-
tic tradition of lexicography which compiles lists
of MWEs occurring in the language. For exam-
ple, Gross (1986) shows that dictionaries contain
about 1,500 single-word adverbs but that French con-
tains over 5,000 multiword adverbs. MWEs occur
in every part-of-speech (POS) category (e.g., noun
trousse de secours ?first-aid kit?; verb faire main-
basse [do hand-low] ?seize?; adverb comme dans du
beurre [as in butter] ?easily?; adjective ?? part en-
ti?re? ?wholly?).
The FTB explicitly annotates MWEs (also called
compounds in prior work). We used the subset of
the corpus with functional annotations, not for those
annotations but because this subset is known to be
more consistently annotated. POS tags for MWEs
are given not only at the MWE level, but also inter-
nally: most tokens that constitute an MWE also have
a POS tag. Table 2 compares this part of the FTB to
the WSJ portion of the Penn Treebank.
2.1 Preprocessing
The FTB requires significant pre-processing prior to
parsing.
Tokenization We changed the default tokenization
for numbers by fusing adjacent digit tokens. For ex-
ample, 500 000 is tagged as an MWE composed of
two words 500 and 000. We made this 500000 and
retained the MWE POS, although we did not mark
the new token as an MWE. For consistency, we used
one token for punctuated numbers like ?17,9?.
MWE Tagging We marked MWEs with a flat
bracketing in which the phrasal label is the MWE-
level POS tag with an ?MW? prefix, and the preter-
minals are the internal POS tags for each terminal.
The resulting POS sequences are not always unique
to MWEs: they appear in abundance elsewhere in
the corpus. However, some MWEs contain normally
ungrammatical POS sequences (e.g., adverb ? la va
vite ?in a hurry?: PDVADV [at the goes quick]), and
some words appear only as part of an MWE, such as
insu in ? l?insu de ?to the ignorance of?.
Labels We augmented the basic FTB label set?
which contains 14 POS tags and 19 phrasal tags?in
two ways. First, we added 16 finer-grained POS tags
for punctuation.1 Second, we added the 11 MWE
1Punctuation tag clusters?as used in the WSJ?did not im-
prove accuracy. Enriched tag sets like that of Crabb? and Can-
dito (2008) could also be investigated and compared to our re-
sults since Evalb is insensitive to POS tags.
726
labels shown in Table 3, resulting in 24 total phrasal
categories.
Corrections Historically, the FTB suffered from
annotation errors such as missing POS and phrasal
tags (Arun and Keller, 2005). We found that this
problem has been largely resolved in the current re-
lease. However, 1,949 tokens and 36 MWE spans
still lacked tags. We restored the labels by first as-
signing each token its most frequent POS tag else-
where in the treebank, and then assigning the most
frequent MWE phrasal category for the resulting
POS sequence.2
Split We used the 80/10/10 split described by
Crabb? and Candito (2008). However, they used a
previous release of the treebank with 12,531 trees.
3,391 trees have been added to the present version.
We appended these extra trees to the training set, thus
retaining the same development and test sets.
2.2 Comparison to Prior FTB Representations
Our pre-processing approach is simple and auto-
matic3 unlike the three major instantiations of the
FTB that have been used in previous work:
Arun-Cont and Arun-Exp (Arun and Keller,
2005): Two instantiations of the full 20,000 sentence
treebank that differed principally in their treatment of
MWEs: (1) Cont, in which the tokens of eachMWE
were concatenated into a single token (en moyenne
? en_moyenne); (2)Exp, in which theyweremarked
with a flat structure. For both representations, they
also gave results in which coordinated phrase struc-
tures were flattened. In the published experiments,
they mistakenly removed half of the corpus, believ-
ing that the multi-terminal (per POS tag) annotations
of MWEs were XML errors (Schluter and Genabith,
2007).
MFT (Schluter andGenabith, 2007): Manual revi-
sion to 3,800 sentences. Major changes included co-
ordination raising, an expanded POS tag set, and the
273 of the unlabeled word types did not appear elsewhere
in the treebank. All but 11 of these were nouns. We manually
assigned the correct tags, but we would not expect a negative
effect by deterministically labeling all of them as nouns.
3We automate tree manipulation with Tregex/Tsurgeon
(Levy and Andrew, 2006). Our pre-processing package is avail-
able at http://nlp.stanford.edu/software/lex-parser.shtml.
correction of annotation errors. Like Arun-Cont,
MFT contains concatenated MWEs.
FTB-UC (Candito and Crabb?, 2009): An in-
stantiation of the functionally annotated section that
makes a distinction between MWEs that are ?syn-
tactically regular? and those that are not. Syntacti-
cally regular MWEs were given internal structure,
while all other MWEs were concatenated into sin-
gle tokens. For example, nouns followed by ad-
jectives, such as loi agraire ?land law? or Union
mon?taire et ?conomique ?monetary and economic
Union? were considered syntactically regular. They
are MWEs because the choice of adjective is arbi-
trary (loi agraire and not *loi agricole, similarly to
?coal black? but not *?crow black? for example), but
their syntactic structure is not intrinsic to MWEs.
In such cases, FTB-UC gives the MWE a conven-
tional analysis of an NP with internal structure. Such
analysis is indeed sufficient to recover the mean-
ing of these semantically compositional MWEs that
are extremely productive. On the other hand, the
FTB-UC loses information about MWEs with non-
compositional semantics.
Almost all work on the FTB has followed Arun-
Cont and used goldMWEpre-grouping. As a result,
most results for French parsing are analogous to early
results for Chinese, which used gold word segmen-
tation, and Arabic, which used gold clitic segmenta-
tion. Candito et al (2010) were the first to acknowl-
edge and address this issue, but they still used FTB-
UC (with some pre-grouped MWEs). Since the syn-
tax and definition of MWEs is a contentious issue,
we take a more agnostic view?which is consistent
with that of the FTB annotators?and leave them to-
kenized. This permits a data-oriented approach to
MWE identification that is more robust to changes
to the status of specific MWE instances.
To set a baseline prior to grammar development,
we trained the Stanford parser (Klein and Manning,
2003) with no grammar features, achieving 74.2%
labeled F1 on the development set (sentences ? 40
words). This is lower than the most recent results ob-
tained by Seddah (2010). However, the results are
not comparable: the data split was different, they
made use of morphological information, and more
importantly they concatenated MWEs. The focus of
727
our work is on models and data representations that
enable MWE identification.
3 MWEs in Lexicon-Grammar
The MWE representation in the FTB is close to
the one proposed in the Lexicon-Grammar (Gross,
1986). In the Lexicon-Grammar, MWEs are classi-
fied according to their global POS tags (noun, verb,
adverb, adjective), and described in terms of the se-
quence of the POS tags of the words that constitute
the MWE (e.g., ?N de N? garde d?enfant [guard of
child] ?daycare?, pied de guerre [foot of war] ?at the
ready?). In other words, MWEs are represented by a
flat structure. The Lexicon-Grammar distinguishes
between units that are fixed and have to appear as is
(en tout et pour tout [in all and for all] ?in total?) and
units that accept some syntactic variation such as ad-
mitting the insertion of an adverb or adjective, or the
variation of one of the words in the expression (e.g.,
a possessive as in ?from the top of one?s hat?). It also
notes whether the MWE displays some selectional
preferences (e.g., it has to be preceded by a verb or
by an adjective).
Our FTB instantiation is largely consistent with
the Lexicon-Grammar. Recall that we defined differ-
ent MWE categories based on the global POS. We
now detail three of the categories.
MWN The MWN category consists of proper
nouns (1a), foreign common nouns (1b), as well as
common nouns. The common nouns appear in sev-
eral syntactically regular sequences of POS tags (2).
Multiword nouns allow inflection (singular vs. plu-
ral) but no insertion.
(1) a. London Sunday Times, Los Angeles
b. week - end, mea culpa, joint - venture
(2) a. NA: corpsm?dical ?medical staff?, dette
publique ?public debt?
b. N PN:mode d?emploi ?instruction man-
ual?
c. N N: num?ro deux ?number two?, mai-
sonm?re [housemother] ?headquarters?,
gr?ve surprise ?sudden strike?
d. N P D N: imp?t sur le revenu ?income
tax?, ministre de l??conomie ?finance
minister?
MWA Multiword adjectives appear with different
POS sequences (3). They include numbers such as
vingt et uni?me ?21st?. Some items in (3b) allow in-
ternal variation: some adverbs or adjectives can be
added to both examples given (? tr?s haut risque, de
toute derni?re minute).
(3) a. P N: d?antan [from before] ?old?, en
question ?under discussion?
b. P A N: ? haut risque ?high-risk?, de
derni?re minute [from the last minute]
?at the eleventh hour?
c. A C A: pur et simple [pure and simple]
?straightforward?, noir et blanc ?black
and white?
MWV Multiword verbs also appear in several POS
sequences (4). All verbs allow number and tense in-
flections. Some MWVs containing a noun or an ad-
jective allow the insertion of a modifier (e.g., don-
ner grande satisfication ?give great satisfaction?),
whereas others do not. When an adverb intervenes
between the main verb and its complement, the FTB
marks the two parts of the MWV discontinuously
(e.g., [MWV [V prennent]] [ADV d?j?] [MWV [P en] [N
cause]] ?already take into account?).
(4) a. V N: avoir lieu ?take place?, donner sat-
isfaction ?give satisfaction?
b. V P N: mettre en place ?put in place?,
entrer en vigueur ?to come into effect?
c. V P ADV: mettre ? mal [put at bad]
?harm?, ?tre ? m?me [be at same] ?be
able?
d. V D N P N: tirer la sonnette d?alarme
?ring the alarm bell?, avoir le vent en
poupe ?to have the wind astern?
4 Parsing Models
We develop two parsers for French with the goal
of improving MWE identification. The first is a
manually-annotated grammar that we incorporate
into the Stanford parser. Manual annotation results in
human interpretable grammars that can inform future
treebank annotation decisions. Moreover, the gram-
mar can be used as the base distribution in our sec-
ond model, a Probabilistic Tree Substitution Gram-
mar (PTSG) parser. PTSGs learn parameters for tree
728
Feature States Tags F1 ?F1
? 4325 31 74.21
tagPA 4509 215 76.94 +2.73
markInf 4510 216 77.42 +0.48
markPart 4511 217 77.73 +0.31
markVN 5986 217 78.32 +0.59
markCoord 7361 217 78.45 +0.13
markDe 7521 233 79.11 +0.66
markP 7523 235 79.34 +0.23
markMWE 7867 235 79.23 ?0.11
Table 4: Effects on grammar size and labeled F1 for each
of the manual state splits (development set, sentences ?
40 words). markMWE decreases overall accuracy, but
increases both the number of correctly parsed trees (by
0.30%) and per category MWE accuracy.
fragments larger than basic CFG rules. PTSG rules
may also be lexicalized. This means that commonly
observed collocations?some of which areMWEs?
can be stored in the grammar.
4.1 Stanford Parser
We configure the Stanford parser with settings that
are effective for other languages: selective parent an-
notation, lexicon smoothing, and factored parsing.
We use the head-finding rules of Dybro-Johansen
(2004), which we find to yield an approximately
1.0% F1 development set improvement over those of
Arun (2004). Finally, we include a simple unknown
word model consisting entirely of surface features:
- Nominal, adjectival, verbal, adverbial, and plu-
ral suffixes
- Contains a digit or punctuation
- Is capitalized (except the first word in a sen-
tence)
- Consists entirely of capital letters
- If none of the above, add a one- or two-character
suffix
Combined with the grammar features, this unknown
word model yields 97.3% tagging accuracy on the
development set.
4.1.1 Grammar Development
Table 4 lists the symbol refinements used in our
grammar. Most of the features are POS splits as
many phrasal tag splits did not lead to any improve-
ment. Parent annotation of POS tags (tagPA) cap-
tures information about the external context. mark-
Inf and markPart accomplish a finite/nonfinite dis-
tinction: they respectively specify whether the verb
is an infinitive or a participle based on the type of
the grandparent node. markVN captures the notion
of verbal distance as in Klein and Manning (2003).
We opted to keep the COORD phrasal tag, and
to capture parallelism in coordination, we mark CO-
ORD with the type of its child (NP, AP, VPinf, etc.).
markDe identifies the preposition de and its variants
(du, des, d?) which is very frequent and appears in
several different contexts. markP identifies preposi-
tions which introduce PPs modifying a noun. Mark-
ing other kinds of prepositional modifiers (e.g., verb)
did not help. markMWE adds an annotation to sev-
eral MWE categories for frequently occuring POS
sequences. For example, we mark MWNs that occur
more than 600 times (e.g., ?N P N? and ?N N?).
4.2 DP-TSG Parser
A shortcoming of CFG-based grammars is that they
do not explicitly capture idiomatic usage. For exam-
ple, consider the two utterances:
(5) a. He [MWV kicked the bucket] .
b. He [VP kicked [NP the pail]] .
The examples in (5) may be equally probable and re-
ceive the same analysis under a PCFG; words are
generated independently. However, recall that in
our representation, (5a) should receive a flat analysis
as MWV, whereas (5b) should have a conventional
analysis of the verb kicked and its two arguments.
An alternate view of parsing is one in which new
utterances are built from previously observed frag-
ments. This is the original motivation for data ori-
ented parsing (DOP) (Bod, 1992), in which ?id-
iomaticity is the rule rather than the exception?
(Scha, 1990). If we have seen the collocation kicked
the bucket several times before, we should store that
whole fragment for later use.
We consider a variant of the non-parametric PTSG
model of Cohn et al (2009) in which tree fragments
are drawn from a Dirichlet process (DP) prior.4
The DP-TSG can be viewed as a DOP model with
Bayesian parameter estimation. A PTSG is a 5-tuple
?V,?, R,?,?? where c ? V are non-terminals;
4Similar models were developed independently by
O?Donnell et al (2009) and Post and Gildea (2009).
729
?c DP concentration parameter for each c ? V
P0(e|c) CFG base distribution
x Set of non-terminal nodes in the treebank
S Set of sampling sites (one for each x ? x)
S A block of sampling sites, where S ? S
b = {bs}s?S Binary variables to be sampled (bs = 1 ?
frontier node)
z Latent state of the segmented treebank
m Number of sites s ? S s.t. bS = 1
n = {nc,e} Sufficient statistics of z
?nS:m Change in counts by setting m sites in S
Table 5: DP-TSG model notation. For consistency, we
largely follow the notation of Liang et al (2010). Note
that z = (b,x), and as such z = ?c, e?.
t ? ? are terminals; e ? R are elementary trees;5
? ? V is a unique start symbol; and ?c,e ? ? are
parameters for each tree fragment. A PTSG deriva-
tion is created by successively applying the substitu-
tion operator to the leftmost frontier node (denoted
by c+). All other nodes are internal (denoted by c?).
In the supervised setting, DP-TSG grammar ex-
traction reduces to a segmentation problem. We have
a treebank T that we segment into the set R, a pro-
cess that we model with Bayes? rule:
p(R | T ) ? p(T | R) p(R) (1)
Since the tree fragments completely specify each
tree, p(T | R) is either 0 or 1, so all work is per-
formed by the prior over the set of elementary trees.
The DP-TSG contains a DP prior for each c ? V
(Table 5 defines further notation). We generate ?c, e?
tuples as follows:
?c|c, ?c, P0(?|c) ? DP (?c, P0)
e|?c ? ?c
The data likelihood is given by the latent state z and
the parameters ?: p(z|?) =?z?z ?
nc,e(z)
c,e . Integrat-
ing out the parameters, we have:
p(z) =
?
c?V
?
e(?cP0(e|c))nc,e(z)
?nc,?(z)c
(2)
where xn = x(x + 1) . . . (x + n ? 1) is the rising
factorial. (?A.1 contains ancillary details.)
Base Distribution The base distribution P0 is the
same maximum likelihood PCFG used in the Stan-
5We use the terms tree fragment and elementary tree inter-
changeably.
NP+
PUNC-(1)
?
N+
Jacques
N-
Chirac
PUNC+(2)
?
Figure 1: Example of two conflicting sites of the same
type. Define the type of a site t(z, s) def= (?ns:0,?ns:1).
Sites (1) and (2) above have the same type since t(z, s1) =
t(z, s2). However, the two sites conflict since the prob-
abilities of setting bs1 and bs2 both depend on counts for
the tree fragment rooted at NP. Consequently, sites (1) and
(2) are not exchangeable: the probabilities of their assign-
ments depend on the order in which they are sampled.
ford parser.6,7 After applying the manual state splits,
we perform simple right binarization, collapse unary
rules, and replace rare words with their signatures
(Petrov et al, 2006).
For each non-terminal type c, we learn a stop prob-
ability sc ? Beta(1, 1). Under P0, the probability of
generating a rule A+ ? B? C+ composed of non-
terminals is
P0(A+ ? B? C+) = pMLE(A ? B C)sB(1?sC)
(3)
For lexical insertion rules, we add a penalty propor-
tional to the frequency of the lexical item:
P0(c ? t) = pMLE(c ? t)p(t) (4)
where p(t) is equal to the MLE unigram probabil-
ity of t in the treebank. Lexicalizing a rule makes it
very specific, so we generally want to avoid lexical-
ization with rare words. Empirically, we found that
this penalty reduces overfitting.
Type-based Inference Algorithm To learn the pa-
rameters ? we use the collapsed, block Gibbs sam-
pler of Liang et al (2010). We sample binary vari-
ables bs associated with each non-terminal node/site
in the treebank. The key idea is to select a block
of exchangeable sites S of the same type that do not
conflict (Figure 1). Since the sites in S are exchange-
able, we can set bS randomly so long as we know m,
the number of sites with bs = 1. Because this algo-
rithm is a not a contribution of this paper, we refer
the reader to Liang et al (2010).
6The Stanford parser is a product model, so the results in ?5.1
include the contribution of a dependency parser.
7Bansal and Klein (2010) also experimented with symbol re-
finement in an all-fragments (parametric) TSG for English.
730
After each Gibbs iteration, we sample each sc di-
rectly using binomial-Beta conjugacy. We re-sample
the DP concentration parameters ?c with the auxil-
iary variable procedure of West (1995).
Decoding We compute the rule score of each tree
fragment from a single grammar sample as follows:
?c,e =
nc,e(z) + ?cP0(e|c)
nc,?(z) + ?c
(5)
To make the grammar more robust, we also include
all CFG rules in P0 with zero counts inn. Scores for
these rules follow from (5) with nc,e(z) = 0.
For decoding, we note that the derivations of a
TSG are a CFGparse forest (Vijay-Shanker andWeir,
1993). As such, we can use a Synchronous Context
Free Grammar (SCFG) to translate the 1-best parse
to its derivation. Consider a unique tree fragment ei
rooted at X with frontier ?, which is a sequence of
terminals and non-terminals. We encode this frag-
ment as an SCFG rule of the form
[X ? ? , X ? i, Y1, . . . , Yn] (6)
where Y1, . . . , Yn is the sequence of non-terminal
nodes in ?.8 During decoding, the input is re-
written as a sequence of tree fragment (rule) indices
{i, j, k, . . . }. Because the TSG substitution operator
always applies to the leftmost frontier node, we can
deterministically recover the monolingual parse with
top-down re-writes of ?.
The SCFG formulation has a practical benefit: we
can take advantage of the heavily-optimized SCFG
decoders for machine translation. We use cdec
(Dyer et al, 2010) to recover the Viterbi derivation
under a DP-TSG grammar sample.
5 Experiments
5.1 Standard Parsing Experiments
We evaluate parsing accuracy of the Stanford and
DP-TSG models (Table 6). For comparison, we also
include the Berkeley parser (Petrov et al, 2006).9
For the DP-TSG, we initialized all bs with fair coin
tosses and ran for 400 iterations, after which likeli-
hood stopped improving.
8This formulation is due to Chris Dyer.
9Training settings: right binarization, no parent annotation,
six split-merge cycles, and random initialization.
Leaf Ancestor Evalb
Corpus Sent LP LR F1 EX%
PA-PCFG 0.793 0.812 68.1 67.0 67.6 10.5
DP-TSG 0.823 0.842 75.6 76.0 75.8 15.1
Stanford 0.843 0.861 77.8 79.0 78.4 17.5
Berkeley 0.880 0.891 82.4 82.0 82.2 21.4
Table 6: Standard parsing experiments (test set, sentences
? 40 words). All parsers exceed 96% tagging accuracy.
Berkeley and DP-TSG results are the average of three in-
dependent runs.
We report two different parsing metrics. Evalb
is the standard labeled precision/recall metric.10
Leaf Ancestor measures the cost of transforming
guess trees to the reference (Sampson and Babar-
czy, 2003). It was developed in response to the non-
terminal/terminal ratio bias of Evalb, which penal-
izes flat treebanks like the FTB. The range of the
score is between 0 and 1 (higher is better). We report
micro-averaged (whole corpus) and macro-averaged
(per sentence) scores.
In terms of parsing accuracy, the Berkeley parser
exceeds both Stanford and DP-TSG. This is consis-
tent with previous experiments for French by Sed-
dah et al (2009), who show that the Berkeley parser
outperforms other models. It also matches the or-
dering for English (Cohn et al, 2010; Liang et al,
2010). However, the standard baseline for TSGmod-
els is a simple parent-annotated PCFG (PA-PCFG).
For English, Liang et al (2010) showed that a similar
DP-TSG improved over PA-PCFG by 4.2% F1. For
French, our gain is a more substantial 8.2% F1.
5.2 MWE Identification Experiments
Table 7 lists overall and per-category MWE identifi-
cation results for the parsing models. Although DP-
TSG is less accurate as a general parsing model, it is
more effective at identifying MWEs.
The predominant approach to MWE identification
is the combination of lexical association measures
(surface statistics) with a binary classifier (Pecina,
2010). A state-of-the-art, language independent
package that implements this approach for higher
order n-grams is mwetoolkit (Ramisch et al,
2010).11 In Table 8 we compare DP-TSG to both
10Available at http://nlp.cs.nyu.edu/evalb/ (v.20080701).
11Available at http://multiword.sourceforge.net/. See ?A.2 for
731
#gold Stanford DP-TSG Berkeley
MWET 3 0.0 0.0 0.0
MWV 26 64.0 57.7 50.7
MWA 8 26.1 32.2 29.8
MWN 456 64.1 67.6 67.1
MWD 15 70.3 65.5 70.1
MWPRO 17 73.7 78.0 76.2
MWADV 220 74.6 72.7 70.4
MWP 162 81.3 80.5 77.7
MWC 47 83.5 83.5 80.8
954 70.1 71.1 69.6
Table 7: MWE identification per category and overall re-
sults (test set, sentences ? 40 words). MWI and MWCL
do not occur in the test set.
Model F1
mwetoolkit All 15.4
PA-PCFG 32.6
mwetoolkit Filter 34.7
PA-PCFG+Features 63.1
DP-TSG 71.1
Table 8: MWE identification F1 of the best parsing model
vs. the mwetoolkit baseline (test set, sentences ? 40
words). PA-PCFG+Features includes the grammar fea-
tures in Table 4, which is the CFG from which the TSG is
extracted. For mwetoolkit, All indicates the inclusion
of all n-grams in the training corpus. Filter indicates pre-
filtering of the training corpus by removing rare n-grams
(see ?A.2 for details).
mwetoolkit and the CFG from the which the TSG
is extracted. The TSG-based parsing model outper-
forms mwetoolkit by 36.4% F1 while providing
syntactic subcategory information.
6 Discussion
Automatic learning methods run the risk of produc-
ing uninterpretable models. However, the DP-TSG
model learns useful generalizations over MWEs. A
sample of the rules is given in Table 9. Some spe-
cific sequences like ?[MWN [coup de N]]? are part of
the grammar: such rules can indeed generate quite
a few MWEs, e.g., coup de pied ?kick?, coup de
coeur, coup de foudre ?love at first sight?, coup de
main ?help?, coup d??tat, coup de gr?ce (note that
only some of these MWEs are seen in the training
configuration details.
MWN MWV MWP
soci?t?s de N sous - V de l?ordre de
prix de N faire N y compris
coup de N V les moyens au N de
N d??tat V de N en N de
N de N V en N ADV de
N ? N
Table 9: Sample of the TSG rules learned.
MWN
N
tour
P
de
N
passe
-
-
N
passe
(a) Reference
NP
N
tour
PP
P
de
NP
MWN
N
passe
-
-
N
passe
(b) DP-TSG
Figure 2: Example of an MWE error for tour de passe-
passe ?magic trick?. (dev set)
data). For MWV, ?V de N? as in avoir de cesse ?give
no peace?, perdre de vue [lose from sight] ?forget?,
prendre de vitesse [take from speed] ?outpace?), is
learned. For prepositions, the grammar stores full
subtrees of MWPs, but can also generalize the struc-
ture of very frequent sequences: ?en N de? occurs in
manymultiword prepositions (e.g., en compagnie de,
en face de, en mati?re de, en terme de, en cours de,
en faveur de, en raison de, en fonction de). The TSG
grammar thus provides a categorization of MWEs
consistent with the Lexicon-Grammar. It also learns
verbal phrases which contain discontinuous MWVs
due to the insertion of an adverb or negation such as
?[VN [MWV va] [MWADV d?ailleurs] [MWV bon train]]?
[go indeed well], ?[VN [MWV a] [ADV jamais] [MWV
?t? question d?]]? [has never been in question].
A significant fraction of errors for MWNs occur
with adjectives that are not recognized as part of the
MWE. For example, since ?tablissements priv?s ?pri-
vate corporation? is unseen in the training data, it is
not found. Sometimes the parser did not recognize
the whole structure of an MWE. Figure 2 shows an
example where the parser only found a subpart of the
MWN tour de passe-passe ?magic trick?.
Other DP-TSG errors are due to inconsistencies in
the FTB annotation. For example, sous pr?texte que
732
MWC
P
sous
N
pr?texte
C
que
(a) Reference
PP
P
sous
NP
N
pr?texte
Ssub
C
que
(b) Reference
Figure 3: Example of an inconsistent FTB annotation for
sous pr?texte que ?on the pretext of?.
?on the pretext of? is tagged as both MWC and as a
regular PP structure (Figure 3). However, the parser
always assigns a MWC structure, which is a better
analysis than the gold annotation. We expect that
more consistent annotation would help the DP-TSG
more than the CFG-based parsers.
The DP-TSG is not immune to false positives: in
Le march? national, fait-on remarquer, est enfin en
r?gression . . . ?The national economy, people at last
note, is going down? the parser tags march? national
asMWN. As noted, the boundary of what should and
should not count as an MWE can be fuzzy, and it is
therefore hard to assess whether or not this should be
an MWE. The FTB does not mark it as one.
There are multiple examples were the DP-TSG
found the MWE whereas Stanford (its base distribu-
tion) did not, such as in Figure 4. Note that the ?N
P N? structure is quite frequent for MWNs, but the
TSG correctly identifies the MWADV in emplois ?
domicile [jobs at home] ?homeworking?.
7 Related Work
There is a voluminous literature on MWE identi-
fication. Here we review closely related syntax-
based methods.12 The linguistic and computa-
tional attractiveness of lexicalized grammars for
modeling idiosyncratic constructions in French was
identified by Abeill? (1988) and Abeill? and Sch-
abes (1989). They manually developed a small
Tree Adjoining Grammar (TAG) of 1,200 elemen-
tary trees and 4,000 lexical items that included
MWEs. The classic statistical approach to MWE
identification, Xtract (Smadja, 1993), used an in-
12See Seretan (2011) for a comprehensive survey of syntax-
based methods for MWE identification. For an overview of n-
gram methods like mwetoolkit, see Pecina (2010).
MWN
N
campagne
P
de
N
promotion
(a) DP-TSG
NP
N
campagne
PP
P
de
NP
N
promotion
(b) Stanford
NP
N
emplois
MWADV
P
?
N
domicile
(c) DP-TSG
NP
N
emplois
PP
P
?
NP
N
domicile
(d) Stanford
Figure 4: Correct analyses by DP-TSG. (dev set)
cremental parser in the third stage of its pipeline
to identify predicate-argument relationships. Lin
(1999) applied information-theoretic measures to
automatically-extracted dependency relations to find
MWEs. To our knowledge, Wehrli (2000) was the
first to use syntactically annotated corpora to im-
prove a parser for MWE identification. He pro-
posed to rank analyses of a symbolic parser based
on the presence of collocations, although details of
the ranking function were not provided.
The most similar work to ours is that of Nivre
and Nilsson (2004), who converted a Swedish cor-
pus into two versions: one in which MWEs were
left as tokens, and one in which they were merged.
On the first version, they showed that a deterministic
dependency parser could identify MWEs at 71.1%
F1, albeit without subcategory information. On
the second version?which simulated perfect MWE
identification?they showed that labeled attachment
improved by about 1%.
Recent statistical parsing work on French has in-
cluded Stochastic Tree Insertion Grammars (STIGs),
which are related to TAGs, but with a restricted ad-
junction operation.13 Seddah et al (2009) and Sed-
dah (2010) showed that STIGs underperform CFG-
based parsers on the FTB. In their experiments,
MWEs were concatenated.
13TSGs differ from TAGs and STIGs in that they do not in-
clude an adjunction operator.
733
8 Conclusion
The main result of this paper is that an existing sta-
tistical parser can achieve a 36.4% F1 absolute im-
provement for MWE identification over a state-of-
the-art n-gram surface statistics package. Parsers
also provide syntactic subcategorization, and do not
require pre-filtering of the training data. We have
also demonstrated that TSGs can capture idiomatic
usage better than a PCFG.While the DP-TSG, which
is a relatively new parsing model, still lags state-of-
the-art parsers in terms of overall labeling accuracy,
we have shown that it is already very effective for
other tasks like MWE identification. We plan to im-
prove the DP-TSG by experimenting with alternate
parsing objectives (Cohn et al, 2010), lexical rep-
resentations, and parameterizations of the base dis-
tribution. A particularly promising base distribution
is the latent variable PCFG learned by the Berkeley
parser. However, initial experiments with this distri-
bution were negative, so we leave further develop-
ment to future work.
We chose French for these experiments due to the
pervasiveness ofMWEs and the availability of an an-
notated corpus. However, MWE lists and syntactic
treebanks exist for many of the world?s major lan-
guages. We will investigate automatic conversion of
these treebanks (by flattening MWE bracketings) for
MWE identification.
A Appendix
A.1 Notes on the Rising Factorial
The rising factorial?also known as the ascending
factorial or Pochhammer symbol?arises in the con-
text of samples from a Dirichlet process (see Prop.
3 of Antoniak (1974) for details). For a positive in-
teger n and a complex number x, the rising factorial
xn is defined14 by
xn = x(x + 1) . . . (x + n? 1)
=
n?
j=1
(x + j ? 1) (7)
The rising factorial can be generalized to a com-
plex number ? with the gamma function:
x? = ?(x + ?)?(x) (8)
14We adopt the notation of Knuth (1992).
where x0 ? 1.
In our type-based sampler, we computed (7) di-
rectly in a dynamic program. We found that (8) was
prohibitively slow for sampling.
A.2 mwetoolkit Configuration
We configured mwetoolkit15 with the four stan-
dard lexical features: the maximum likelihood esti-
mator, Dice?s coefficient, pointwise mutual informa-
tion (PMI), and Student?s t-score. We added the POS
sequence for each n-gram as a single feature. We re-
moved the web counts features to make the experi-
ments comparable. To compensate for the absence
of web counts, we computed the lexical features us-
ing the gold lemmas from the FTB instead of using
an automatic lemmatizer.
Since MWE n-grams only account for a small
fraction of the n-grams in the corpus, we filtered the
training and test sets by removing all n-grams that
occurred once. To further balance the proportion of
MWEs, we trained on all valid MWEs plus 10x ran-
domly selected non-MWE n-grams. This proportion
matches the fraction of MWE/non-MWE tokens in
the FTB. Since we generated a random training set,
we reported the average of three independent runs.
We created feature vectors for the training n-
grams and trained a binary Support Vector Machine
(SVM) classifier with Weka (Hall et al, 2009). Al-
though mwetoolkit defaults to a linear kernel,
we achieved higher accuracy on the development set
with an RBF kernel.
The FTB is sufficiently large for the corpus-based
methods implemented in mwetoolkit. Ramisch
et al (2010)?s experiments were on Genia, which
contains 18k sentences and 490k tokens, similar to
the FTB. Their test set had 895 sentences, smaller
than ours. They reported 30.6% F1 for their task
against an Xtract baseline, which only obtained 7.3%
F1. These results are comparable inmagnitude to our
FTB results.
Acknowledgments We thank Marie Candito, Chris Dyer,
Dan Flickinger, Percy Liang, Carlos Ramisch, Djam?
Seddah, and Val Spitkovsky for their helpful comments.
The first author is supported by a National Defense Sci-
ence and Engineering Graduate (NDSEG) fellowship.
15We re-implemented mwetoolkit in Java for compatibil-
ity with Weka and our pre-processing routines.
734
References
A. Abeill? and Y. Schabes. 1989. Parsing idioms in lexicalized
TAGs. In EACL.
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building a tree-
bank for French, chapter 10. Kluwer.
A. Abeill?. 1988. Parsing Frenchwith TreeAdjoiningGrammar:
some linguistic accounts. In COLING.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152?1174.
A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic
probabilistic parsing: The case of French. In ACL.
A. Arun. 2004. Statistical parsing of the French treebank. Tech-
nical report, University of Edinburgh.
M. Bansal and D. Klein. 2010. Simple, accurate parsing with
an all-fragments grammar. In ACL.
R. Bod. 1992. A computation model of language performance:
Data-Oriented Parsing. In COLING.
M. Candito and B. Crabb?. 2009. Improving generative statisti-
cal parsing with semi-supervised word clustering. In IWPT.
M. Candito, B. Crabb?, and P. Denis. 2010. Statistical French
dependency parsing: treebank conversion and first results. In
LREC.
M. Carpuat and M. Diab. 2010. Task-based evaluation of mul-
tiword expressions: a pilot study in statistical machine trans-
lation. In HLT-NAACL.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing compact
but accurate tree-substitution grammars. In HLT-NAACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2010. Inducing tree-
substitution grammars. JMLR, 11:3053?3096, Nov.
B. Crabb? and M. Candito. 2008. Exp?riences d?analyse syn-
taxique statistique du fran?ais. In TALN.
A. Dybro-Johansen. 2004. Extraction automatique de gram-
maires ? partir d?un corpus fran?ais. Master?s thesis, Univer-
sit? Paris 7.
C. Dyer, A. Lopez, J. Ganitkevitch, J.Weese, F. Ture, et al 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL Sys-
tem Demonstrations.
M. Gross. 1986. Lexicon-Grammar: the representation of com-
pound words. In COLING.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten. 2009. The WEKA data mining software: an
update. SIGKDD Explorations Newsletter, 11:10?18.
D. Hogan, C. Cafferkey, A. Cahill, and J. van Genabith. 2007.
Exploiting multi-word units in history-based probabilistic
generation. In EMNLP-CoNLL.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. E. Knuth. 1992. Two notes on notation. American Mathe-
matical Monthly, 99:403?422, May.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon: tools for
querying and manipulating tree data structures. In LREC.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based MCMC.
In HLT-NAACL.
D. Lin. 1999. Automatic identification of non-compositional
phrases. In ACL.
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic pars-
ing. In Methodologies and Evaluation of Multiword Units in
Real-World Applications (MEMURA).
T. J. O?Donnell, J. B. Tenenbaum, and N. D. Goodman. 2009.
Fragment grammars: Exploring computation and reuse in
language. Technical report, MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series, MIT-
CSAIL-TR-2009-013.
P. Pecina. 2010. Lexical association measures and collocation
extraction. Language Resources and Evaluation, 44:137?
158.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
accurate, compact, and interpretable tree annotation. In ACL.
M. Post and D. Gildea. 2009. Bayesian learning of a tree sub-
stitution grammar. In ACL-IJCNLP, Short Papers.
C. Ramisch, A. Villavicencio, and C. Boitet. 2010. mwe-
toolkit: a framework for multiword expression identifi-
cation. In LREC.
P. Rayson, S. Piao, S. Sharoff, S. Evert, and B. Moir?n. 2010.
Multiword expressions: hard going or plain sailing? Lan-
guage Resources and Evaluation, 44:1?5.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In CICLing.
G. Sampson and A. Babarczy. 2003. A test of the leaf-ancestor
metric for parse accuracy. Natural Language Engineering,
9:365?380.
R. Scha, 1990. Taaltheorie en taaltechnologie: competence en
performance, pages 7?22. Landelijke Vereniging van Neer-
landici (LVVNjaarboek).
N. Schluter and J. Genabith. 2007. Preparing, restructuring,
and augmenting a French treebank: Lexicalised parsers or
coherent treebanks? In Pacling.
D. Seddah, M. Candito, and B. Crabb?. 2009. Cross parser
evaluation and tagset variation: a French treebank study. In
IWPT.
D. Seddah. 2010. Exploring the Spinal-STIG model for parsing
French. In LREC.
V. Seretan. 2011. Syntax-Based Collocation Extraction, vol-
ume 44 of Text, Speech, and Language Technology. Springer.
F. Smadja. 1993. Retrieving collocations from text: Xtract.
Computational Linguistics, 19:143?177.
K. Vijay-Shanker and D. J. Weir. 1993. The use of shared forests
in tree adjoining grammar parsing. In EACL.
E. Wehrli. 2000. Parsing and collocations. In Natural Lan-
guage Processing?NLP 2000, volume 1835 of Lecture Notes
in Computer Science, pages 272?282. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
735
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455?465,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Parsing with Compositional Vector Grammars
Richard Socher John Bauer Christopher D. Manning Andrew Y. Ng
Computer Science Department, Stanford University, Stanford, CA 94305, USA
richard@socher.org, horatio@gmail.com, manning@stanford.edu, ang@cs.stanford.edu
Abstract
Natural language parsing has typically
been done with small sets of discrete cat-
egories such as NP and VP, but this rep-
resentation does not capture the full syn-
tactic nor semantic richness of linguistic
phrases, and attempts to improve on this
by lexicalizing phrases or splitting cate-
gories only partly address the problem at
the cost of huge feature spaces and sparse-
ness. Instead, we introduce a Compo-
sitional Vector Grammar (CVG), which
combines PCFGs with a syntactically un-
tied recursive neural network that learns
syntactico-semantic, compositional vector
representations. The CVG improves the
PCFG of the Stanford Parser by 3.8% to
obtain an F1 score of 90.4%. It is fast
to train and implemented approximately as
an efficient reranker it is about 20% faster
than the current Stanford factored parser.
The CVG learns a soft notion of head
words and improves performance on the
types of ambiguities that require semantic
information such as PP attachments.
1 Introduction
Syntactic parsing is a central task in natural lan-
guage processing because of its importance in me-
diating between linguistic expression and mean-
ing. For example, much work has shown the use-
fulness of syntactic representations for subsequent
tasks such as relation extraction, semantic role la-
beling (Gildea and Palmer, 2002) and paraphrase
detection (Callison-Burch, 2008).
Syntactic descriptions standardly use coarse
discrete categories such as NP for noun phrases
or PP for prepositional phrases. However, recent
work has shown that parsing results can be greatly
improved by defining more fine-grained syntactic
(riding,V,       )    (a,Det,       )        (bike,NN,       )
(a bike,NP,       )
(riding a bike,VP,       )
Discrete Syntactic ? Continuous Semantic 
Representations in the Compositional Vector Grammar
Figure 1: Example of a CVG tree with (cate-
gory,vector) representations at each node. The
vectors for nonterminals are computed via a new
type of recursive neural network which is condi-
tioned on syntactic categories from a PCFG.
categories, which better capture phrases with simi-
lar behavior, whether through manual feature engi-
neering (Klein and Manning, 2003a) or automatic
learning (Petrov et al, 2006). However, subdi-
viding a category like NP into 30 or 60 subcate-
gories can only provide a very limited represen-
tation of phrase meaning and semantic similarity.
Two strands of work therefore attempt to go fur-
ther. First, recent work in discriminative parsing
has shown gains from careful engineering of fea-
tures (Taskar et al, 2004; Finkel et al, 2008). Fea-
tures in such parsers can be seen as defining effec-
tive dimensions of similarity between categories.
Second, lexicalized parsers (Collins, 2003; Char-
niak, 2000) associate each category with a lexical
item. This gives a fine-grained notion of semantic
similarity, which is useful for tackling problems
like ambiguous attachment decisions. However,
this approach necessitates complex shrinkage esti-
mation schemes to deal with the sparsity of obser-
vations of the lexicalized categories.
In many natural language systems, single words
and n-grams are usefully described by their distri-
butional similarities (Brown et al, 1992), among
many others. But, even with large corpora, many
455
n-grams will never be seen during training, espe-
cially when n is large. In these cases, one cannot
simply use distributional similarities to represent
unseen phrases. In this work, we present a new so-
lution to learn features and phrase representations
even for very long, unseen n-grams.
We introduce a Compositional Vector Grammar
Parser (CVG) for structure prediction. Like the
above work on parsing, the model addresses the
problem of representing phrases and categories.
Unlike them, it jointly learns how to parse and how
to represent phrases as both discrete categories and
continuous vectors as illustrated in Fig. 1. CVGs
combine the advantages of standard probabilistic
context free grammars (PCFG) with those of re-
cursive neural networks (RNNs). The former can
capture the discrete categorization of phrases into
NP or PP while the latter can capture fine-grained
syntactic and compositional-semantic information
on phrases and words. This information can help
in cases where syntactic ambiguity can only be re-
solved with semantic information, such as in the
PP attachment of the two sentences: They ate udon
with forks. vs. They ate udon with chicken.
Previous RNN-based parsers used the same
(tied) weights at all nodes to compute the vector
representing a constituent (Socher et al, 2011b).
This requires the composition function to be ex-
tremely powerful, since it has to combine phrases
with different syntactic head words, and it is hard
to optimize since the parameters form a very deep
neural network. We generalize the fully tied RNN
to one with syntactically untied weights. The
weights at each node are conditionally dependent
on the categories of the child constituents. This
allows different composition functions when com-
bining different types of phrases and is shown to
result in a large improvement in parsing accuracy.
Our compositional distributed representation al-
lows a CVG parser to make accurate parsing de-
cisions and capture similarities between phrases
and sentences. Any PCFG-based parser can be im-
proved with an RNN. We use a simplified version
of the Stanford Parser (Klein and Manning, 2003a)
as the base PCFG and improve its accuracy from
86.56 to 90.44% labeled F1 on all sentences of the
WSJ section 23. The code of our parser is avail-
able at nlp.stanford.edu.
2 Related Work
The CVG is inspired by two lines of research:
Enriching PCFG parsers through more diverse
sets of discrete states and recursive deep learning
models that jointly learn classifiers and continuous
feature representations for variable-sized inputs.
Improving Discrete Syntactic Representations
As mentioned in the introduction, there are several
approaches to improving discrete representations
for parsing. Klein and Manning (2003a) use
manual feature engineering, while Petrov et
al. (2006) use a learning algorithm that splits
and merges the syntactic categories in order
to maximize likelihood on the treebank. Their
approach splits categories into several dozen
subcategories. Another approach is lexicalized
parsers (Collins, 2003; Charniak, 2000) that
describe each category with a lexical item, usually
the head word. More recently, Hall and Klein
(2012) combine several such annotation schemes
in a factored parser. We extend the above ideas
from discrete representations to richer continuous
ones. The CVG can be seen as factoring discrete
and continuous parsing in one model. Another
different approach to the above generative models
is to learn discriminative parsers using many well
designed features (Taskar et al, 2004; Finkel et
al., 2008). We also borrow ideas from this line of
research in that our parser combines the generative
PCFG model with discriminatively learned RNNs.
Deep Learning and Recursive Deep Learning
Early attempts at using neural networks to de-
scribe phrases include Elman (1991), who used re-
current neural networks to create representations
of sentences from a simple toy grammar and to
analyze the linguistic expressiveness of the re-
sulting representations. Words were represented
as one-on vectors, which was feasible since the
grammar only included a handful of words. Col-
lobert and Weston (2008) showed that neural net-
works can perform well on sequence labeling lan-
guage processing tasks while also learning appro-
priate features. However, their model is lacking
in that it cannot represent the recursive structure
inherent in natural language. They partially cir-
cumvent this problem by using either independent
window-based classifiers or a convolutional layer.
RNN-specific training was introduced by Goller
and Ku?chler (1996) to learn distributed represen-
tations of given, structured objects such as logi-
cal terms. In contrast, our model both predicts the
structure and its representation.
456
Henderson (2003) was the first to show that neu-
ral networks can be successfully used for large
scale parsing. He introduced a left-corner parser to
estimate the probabilities of parsing decisions con-
ditioned on the parsing history. The input to Hen-
derson?s model consists of pairs of frequent words
and their part-of-speech (POS) tags. Both the orig-
inal parsing system and its probabilistic interpre-
tation (Titov and Henderson, 2007) learn features
that represent the parsing history and do not pro-
vide a principled linguistic representation like our
phrase representations. Other related work in-
cludes (Henderson, 2004), who discriminatively
trains a parser based on synchrony networks and
(Titov and Henderson, 2006), who use an SVM to
adapt a generative parser to different domains.
Costa et al (2003) apply recursive neural net-
works to re-rank possible phrase attachments in
an incremental parser. Their work is the first to
show that RNNs can capture enough information
to make correct parsing decisions, but they only
test on a subset of 2000 sentences. Menchetti et
al. (2005) use RNNs to re-rank different parses.
For their results on full sentence parsing, they re-
rank candidate trees created by the Collins parser
(Collins, 2003). Similar to their work, we use the
idea of letting discrete categories reduce the search
space during inference. We compare to fully tied
RNNs in which the same weights are used at every
node. Our syntactically untied RNNs outperform
them by a significant margin. The idea of untying
has also been successfully used in deep learning
applied to vision (Le et al, 2010).
This paper uses several ideas of (Socher et al,
2011b). The main differences are (i) the dual
representation of nodes as discrete categories and
vectors, (ii) the combination with a PCFG, and
(iii) the syntactic untying of weights based on
child categories. We directly compare models with
fully tied and untied weights. Another work that
represents phrases with a dual discrete-continuous
representation is (Kartsaklis et al, 2012).
3 Compositional Vector Grammars
This section introduces Compositional Vector
Grammars (CVGs), a model to jointly find syntac-
tic structure and capture compositional semantic
information.
CVGs build on two observations. Firstly, that a
lot of the structure and regularity in languages can
be captured by well-designed syntactic patterns.
Hence, the CVG builds on top of a standard PCFG
parser. However, many parsing decisions show
fine-grained semantic factors at work. Therefore
we combine syntactic and semantic information
by giving the parser access to rich syntactico-
semantic information in the form of distributional
word vectors and compute compositional semantic
vector representations for longer phrases (Costa
et al, 2003; Menchetti et al, 2005; Socher et
al., 2011b). The CVG model merges ideas from
both generative models that assume discrete syn-
tactic categories and discriminative models that
are trained using continuous vectors.
We will first briefly introduce single word vec-
tor representations and then describe the CVG ob-
jective function, tree scoring and inference.
3.1 Word Vector Representations
In most systems that use a vector representa-
tion for words, such vectors are based on co-
occurrence statistics of each word and its context
(Turney and Pantel, 2010). Another line of re-
search to learn distributional word vectors is based
on neural language models (Bengio et al, 2003)
which jointly learn an embedding of words into an
n-dimensional feature space and use these embed-
dings to predict how suitable a word is in its con-
text. These vector representations capture inter-
esting linear relationships (up to some accuracy),
such as king?man+woman ? queen (Mikolov
et al, 2013).
Collobert and Weston (2008) introduced a new
model to compute such an embedding. The idea
is to construct a neural network that outputs high
scores for windows that occur in a large unla-
beled corpus and low scores for windows where
one word is replaced by a random word. When
such a network is optimized via gradient ascent the
derivatives backpropagate into the word embed-
ding matrix X . In order to predict correct scores
the vectors in the matrix capture co-occurrence
statistics.
For further details and evaluations of these em-
beddings, see (Turian et al, 2010; Huang et al,
2012). The resulting X matrix is used as follows.
Assume we are given a sentence as an ordered list
of m words. Each word w has an index [w] = i
into the columns of the embedding matrix. This
index is used to retrieve the word?s vector repre-
sentation aw using a simple multiplication with a
binary vector e, which is zero everywhere, except
457
at the ith index. So aw = Lei ? Rn. Henceforth,
after mapping each word to its vector, we represent
a sentence S as an ordered list of (word,vector)
pairs: x = ((w1, aw1), . . . , (wm, awm)).
Now that we have discrete and continuous rep-
resentations for all words, we can continue with
the approach for computing tree structures and
vectors for nonterminal nodes.
3.2 Max-Margin Training Objective for
CVGs
The goal of supervised parsing is to learn a func-
tion g : X ? Y , where X is the set of sentences
and Y is the set of all possible labeled binary parse
trees. The set of all possible trees for a given sen-
tence xi is defined as Y (xi) and the correct tree
for a sentence is yi.
We first define a structured margin loss ?(yi, y?)
for predicting a tree y? for a given correct tree.
The loss increases the more incorrect the proposed
parse tree is (Goodman, 1998). The discrepancy
between trees is measured by counting the number
of nodes N(y) with an incorrect span (or label) in
the proposed tree:
?(yi, y?) =
?
d?N(y?)
?1{d /? N(yi)}. (1)
We set ? = 0.1 in all experiments. For a given
set of training instances (xi, yi), we search for the
function g?, parameterized by ?, with the smallest
expected loss on a new sentence. It has the follow-
ing form:
g?(x) = arg max
y??Y (x)
s(CVG(?, x, y?)), (2)
where the tree is found by the Compositional Vec-
tor Grammar (CVG) introduced below and then
scored via the function s. The higher the score of
a tree the more confident the algorithm is that its
structure is correct. This max-margin, structure-
prediction objective (Taskar et al, 2004; Ratliff
et al, 2007; Socher et al, 2011b) trains the CVG
so that the highest scoring tree will be the correct
tree: g?(xi) = yi and its score will be larger up to
a margin to other possible trees y? ? Y(xi):
s(CVG(?, xi, yi)) ? s(CVG(?, xi, y?)) + ?(yi, y?).
This leads to the regularized risk function for m
training examples:
J(?) = 1m
m?
i=1
ri(?) +
?
2 ???
2
2, where
ri(?) = max
y??Y (xi)
(
s(CVG(xi, y?)) + ?(yi, y?)
)
? s(CVG(xi, yi)) (3)
Intuitively, to minimize this objective, the score of
the correct tree yi is increased and the score of the
highest scoring incorrect tree y? is decreased.
3.3 Scoring Trees with CVGs
For ease of exposition, we first describe how to
score an existing fully labeled tree with a standard
RNN and then with a CVG. The subsequent sec-
tion will then describe a bottom-up beam search
and its approximation for finding the optimal tree.
Assume, for now, we are given a labeled
parse tree as shown in Fig. 2. We define
the word representations as (vector, POS) pairs:
((a,A), (b, B), (c, C)), where the vectors are de-
fined as in Sec. 3.1 and the POS tags come from
a PCFG. The standard RNN essentially ignores all
POS tags and syntactic categories and each non-
terminal node is associated with the same neural
network (i.e., the weights across nodes are fully
tied). We can represent the binary tree in Fig. 2
in the form of branching triplets (p ? c1c2).
Each such triplet denotes that a parent node p has
two children and each ck can be either a word
vector or a non-terminal node in the tree. For
the example in Fig. 2, we would get the triples
((p1 ? bc), (p2 ? ap1)). Note that in order
to replicate the neural network and compute node
representations in a bottom up fashion, the parent
must have the same dimensionality as the children:
p ? Rn.
Given this tree structure, we can now compute
activations for each node from the bottom up. We
begin by computing the activation for p1 using
the children?s word vectors. We first concatenate
the children?s representations b, c ? Rn?1 into a
vector
[
b
c
]
? R2n?1. Then the composition
function multiplies this vector by the parameter
weights of the RNN W ? Rn?2n and applies an
element-wise nonlinearity function f = tanh to
the output vector. The resulting output p(1) is then
given as input to compute p(2).
p(1) = f
(
W
[
b
c
])
, p(2) = f
(
W
[
a
p1
])
458
(A , a=       )        ( B , b=       )       ( C, c=       )
P(1 ), p(1 )=       
 P(2 ), p(2 )=        
Standard Recursive Neural Network
= f   W bc
= f   W ap(1 )
Figure 2: An example tree with a simple Recursive
Neural Network: The same weight matrix is repli-
cated and used to compute all non-terminal node
representations. Leaf nodes are n-dimensional
vector representations of words.
In order to compute a score of how plausible of
a syntactic constituent a parent is the RNN uses a
single-unit linear layer for all i:
s(p(i)) = vT p(i),
where v ? Rn is a vector of parameters that need
to be trained. This score will be used to find the
highest scoring tree. For more details on how stan-
dard RNNs can be used for parsing, see Socher et
al. (2011b).
The standard RNN requires a single composi-
tion function to capture all types of compositions:
adjectives and nouns, verbs and nouns, adverbs
and adjectives, etc. Even though this function is
a powerful one, we find a single neural network
weight matrix cannot fully capture the richness of
compositionality. Several extensions are possible:
A two-layered RNN would provide more expres-
sive power, however, it is much harder to train be-
cause the resulting neural network becomes very
deep and suffers from vanishing gradient prob-
lems. Socher et al (2012) proposed to give ev-
ery single word a matrix and a vector. The ma-
trix is then applied to the sibling node?s vector
during the composition. While this results in a
powerful composition function that essentially de-
pends on the words being combined, the number
of model parameters explodes and the composi-
tion functions do not capture the syntactic com-
monalities between similar POS tags or syntactic
categories.
Based on the above considerations, we propose
the Compositional Vector Grammar (CVG) that
conditions the composition function at each node
on discrete syntactic categories extracted from a
(A , a=       )        ( B , b=       )       ( C, c=       )
P(1 ), p(1 )=       
 P(2 ), p(2 )=        
Syntactically Untied Recursive Neural Network
= f   W (B ,C) bc
= f   W (A ,P  ) ap(1 )
(1 )
Figure 3: Example of a syntactically untied RNN
in which the function to compute a parent vector
depends on the syntactic categories of its children
which we assume are given for now.
PCFG. Hence, CVGs combine discrete, syntactic
rule probabilities and continuous vector composi-
tions. The idea is that the syntactic categories of
the children determine what composition function
to use for computing the vector of their parents.
While not perfect, a dedicated composition func-
tion for each rule RHS can well capture common
composition processes such as adjective or adverb
modification versus noun or clausal complementa-
tion. For instance, it could learn that an NP should
be similar to its head noun and little influenced by
a determiner, whereas in an adjective modification
both words considerably determine the meaning of
a phrase. The original RNN is parameterized by a
single weight matrixW . In contrast, the CVG uses
a syntactically untied RNN (SU-RNN) which has
a set of such weights. The size of this set depends
on the number of sibling category combinations in
the PCFG.
Fig. 3 shows an example SU-RNN that com-
putes parent vectors with syntactically untied
weights. The CVG computes the first parent vec-
tor via the SU-RNN:
p(1) = f
(
W (B,C)
[
b
c
])
,
where W (B,C) ? Rn?2n is now a matrix that de-
pends on the categories of the two children. In
this bottom up procedure, the score for each node
consists of summing two elements: First, a single
linear unit that scores the parent vector and sec-
ond, the log probability of the PCFG for the rule
that combines these two children:
s
(
p(1)
)
=
(
v(B,C)
)T p(1) + logP (P1 ? B C),
(4)
459
where P (P1 ? B C) comes from the PCFG.
This can be interpreted as the log probability of a
discrete-continuous rule application with the fol-
lowing factorization:
P ((P1, p1)? (B, b)(C, c)) (5)
= P (p1 ? b c|P1 ? B C)P (P1 ? B C),
Note, however, that due to the continuous nature
of the word vectors, the probability of such a CVG
rule application is not comparable to probabilities
provided by a PCFG since the latter sum to 1 for
all children.
Assuming that node p1 has syntactic category
P1, we compute the second parent vector via:
p(2) = f
(
W (A,P1)
[
a
p(1)
])
.
The score of the last parent in this trigram is com-
puted via:
s
(
p(2)
)
=
(
v(A,P1)
)T p(2) + logP (P2 ? A P1).
3.4 Parsing with CVGs
The above scores (Eq. 4) are used in the search for
the correct tree for a sentence. The goodness of a
tree is measured in terms of its score and the CVG
score of a complete tree is the sum of the scores at
each node:
s(CVG(?, x, y?)) =
?
d?N(y?)
s
(
pd
)
. (6)
The main objective function in Eq. 3 includes a
maximization over all possible trees maxy??Y (x).
Finding the global maximum, however, cannot be
done efficiently for longer sentences nor can we
use dynamic programming. This is due to the fact
that the vectors break the independence assump-
tions of the base PCFG. A (category, vector) node
representation is dependent on all the words in its
span and hence to find the true global optimum,
we would have to compute the scores for all bi-
nary trees. For a sentence of length n, there are
Catalan(n) many possible binary trees which is
very large even for moderately long sentences.
One could use a bottom-up beam search, keep-
ing a k-best list at every cell of the chart, possibly
for each syntactic category. This beam search in-
ference procedure is still considerably slower than
using only the simplified base PCFG, especially
since it has a small state space (see next section for
details). Since each probability look-up is cheap
but computing SU-RNN scores requires a matrix
product, we would like to reduce the number of
SU-RNN score computations to only those trees
that require semantic information. We note that
labeled F1 of the Stanford PCFG parser on the test
set is 86.17%. However, if one used an oracle to
select the best tree from the top 200 trees that it
produces, one could get an F1 of 95.46%.
We use this knowledge to speed up inference via
two bottom-up passes through the parsing chart.
During the first one, we use only the base PCFG to
run CKY dynamic programming through the tree.
The k = 200-best parses at the top cell of the
chart are calculated using the efficient algorithm
of (Huang and Chiang, 2005). Then, the second
pass is a beam search with the full CVG model (in-
cluding the more expensive matrix multiplications
of the SU-RNN). This beam search only consid-
ers phrases that appear in the top 200 parses. This
is similar to a re-ranking setup but with one main
difference: the SU-RNN rule score computation at
each node still only has access to its child vectors,
not the whole tree or other global features. This
allows the second pass to be very fast. We use this
setup in our experiments below.
3.5 Training SU-RNNs
The full CVG model is trained in two stages. First
the base PCFG is trained and its top trees are
cached and then used for training the SU-RNN
conditioned on the PCFG. The SU-RNN is trained
using the objective in Eq. 3 and the scores as ex-
emplified by Eq. 6. For each sentence, we use the
method described above to efficiently find an ap-
proximation for the optimal tree.
To minimize the objective we want to increase
the scores of the correct tree?s constituents and
decrease the score of those in the highest scor-
ing incorrect tree. Derivatives are computed via
backpropagation through structure (BTS) (Goller
and Ku?chler, 1996). The derivative of tree i has
to be taken with respect to all parameter matrices
W (AB) that appear in it. The main difference be-
tween backpropagation in standard RNNs and SU-
RNNs is that the derivatives at each node only add
to the overall derivative of the specific matrix at
that node. For more details on backpropagation
through RNNs, see Socher et al (2010)
460
3.6 Subgradient Methods and AdaGrad
The objective function is not differentiable due to
the hinge loss. Therefore, we generalize gradient
ascent via the subgradient method (Ratliff et al,
2007) which computes a gradient-like direction.
Let ? = (X,W (??), v(??)) ? RM be a vector of all
M model parameters, where we denote W (??) as
the set of matrices that appear in the training set.
The subgradient of Eq. 3 becomes:
?J
?? =
?
i
?s(xi, y?max)
?? ?
?s(xi, yi)
?? + ?,
where y?max is the tree with the highest score. To
minimize the objective, we use the diagonal vari-
ant of AdaGrad (Duchi et al, 2011) with mini-
batches. For our parameter updates, we first de-
fine g? ? RM?1 to be the subgradient at time step
? and Gt = ?t?=1 g?gT? . The parameter update at
time step t then becomes:
?t = ?t?1 ? ? (diag(Gt))?1/2 gt, (7)
where ? is the learning rate. Since we use the di-
agonal of Gt, we only have to store M values and
the update becomes fast to compute: At time step
t, the update for the i?th parameter ?t,i is:
?t,i = ?t?1,i ?
???t
?=1 g2?,i
gt,i. (8)
Hence, the learning rate is adapting differ-
ently for each parameter and rare parameters get
larger updates than frequently occurring parame-
ters. This is helpful in our setting since some W
matrices appear in only a few training trees. This
procedure found much better optima (by ?3% la-
beled F1 on the dev set), and converged more
quickly than L-BFGS which we used previously
in RNN training (Socher et al, 2011a). Training
time is roughly 4 hours on a single machine.
3.7 Initialization of Weight Matrices
In the absence of any knowledge on how to com-
bine two categories, our prior for combining two
vectors is to average them instead of performing a
completely random projection. Hence, we initial-
ize the binary W matrices with:
W (??) = 0.5[In?nIn?n0n?1] + ,
where we include the bias in the last column and
the random variable is uniformly distributed:  ?
U [?0.001, 0.001]. The first block is multiplied by
the left child and the second by the right child:
W (AB)
?
?
a
b
1
?
? =
[
W (A)W (B)bias
]
?
?
a
b
1
?
?
= W (A)a+W (B)b+ bias.
4 Experiments
We evaluate the CVG in two ways: First, by a stan-
dard parsing evaluation on Penn Treebank WSJ
and then by analyzing the model errors in detail.
4.1 Cross-validating Hyperparameters
We used the first 20 files of WSJ section 22
to cross-validate several model and optimization
choices. The base PCFG uses simplified cate-
gories of the Stanford PCFG Parser (Klein and
Manning, 2003a). We decreased the state split-
ting of the PCFG grammar (which helps both by
making it less sparse and by reducing the num-
ber of parameters in the SU-RNN) by adding
the following options to training: ?-noRightRec -
dominatesV 0 -baseNP 0?. This reduces the num-
ber of states from 15,276 to 12,061 states and 602
POS tags. These include split categories, such as
parent annotation categories like VP?S. Further-
more, we ignore all category splits for the SU-
RNN weights, resulting in 66 unary and 882 bi-
nary child pairs. Hence, the SU-RNN has 66+882
transformation matrices and scoring vectors. Note
that any PCFG, including latent annotation PCFGs
(Matsuzaki et al, 2005) could be used. However,
since the vectors will capture lexical and semantic
information, even simple base PCFGs can be sub-
stantially improved. Since the computational com-
plexity of PCFGs depends on the number of states,
a base PCFG with fewer states is much faster.
Testing on the full WSJ section 22 dev set (1700
sentences) takes roughly 470 seconds with the
simple base PCFG, 1320 seconds with our new
CVG and 1600 seconds with the currently pub-
lished Stanford factored parser. Hence, increased
performance comes also with a speed improve-
ment of approximately 20%.
We fix the same regularization of ? = 10?4
for all parameters. The minibatch size was set to
20. We also cross-validated on AdaGrad?s learn-
ing rate which was eventually set to ? = 0.1 and
word vector size. The 25-dimensional vectors pro-
vided by Turian et al (2010) provided the best
461
Parser dev (all) test? 40 test (all)
Stanford PCFG 85.8 86.2 85.5
Stanford Factored 87.4 87.2 86.6
Factored PCFGs 89.7 90.1 89.4
Collins 87.7
SSN (Henderson) 89.4
Berkeley Parser 90.1
CVG (RNN) 85.7 85.1 85.0
CVG (SU-RNN) 91.2 91.1 90.4
Charniak-SelfTrain 91.0
Charniak-RS 92.1
Table 1: Comparison of parsers with richer state
representations on the WSJ. The last line is the
self-trained re-ranked Charniak parser.
performance and were faster than 50-,100- or 200-
dimensional ones. We hypothesize that the larger
word vector sizes, while capturing more seman-
tic knowledge, result in too many SU-RNN matrix
parameters to train and hence perform worse.
4.2 Results on WSJ
The dev set accuracy of the best model is 90.93%
labeled F1 on all sentences. This model re-
sulted in 90.44% on the final test set (WSJ sec-
tion 23). Table 1 compares our results to the
two Stanford parser variants (the unlexicalized
PCFG (Klein and Manning, 2003a) and the fac-
tored parser (Klein and Manning, 2003b)) and
other parsers that use richer state representations:
the Berkeley parser (Petrov and Klein, 2007),
Collins parser (Collins, 1997), SSN: a statistical
neural network parser (Henderson, 2004), Fac-
tored PCFGs (Hall and Klein, 2012), Charniak-
SelfTrain: the self-training approach of McClosky
et al (2006), which bootstraps and parses addi-
tional large corpora multiple times, Charniak-RS:
the state of the art self-trained and discrimina-
tively re-ranked Charniak-Johnson parser combin-
ing (Charniak, 2000; McClosky et al, 2006; Char-
niak and Johnson, 2005). See Kummerfeld et al
(2012) for more comparisons. We compare also
to a standard RNN ?CVG (RNN)? and to the pro-
posed CVG with SU-RNNs.
4.3 Model Analysis
Analysis of Error Types. Table 2 shows a de-
tailed comparison of different errors. We use
the code provided by Kummerfeld et al (2012)
and compare to the previous version of the Stan-
ford factored parser as well as to the Berkeley
and Charniak-reranked-self-trained parsers (de-
fined above). See Kummerfeld et al (2012) for
details and comparisons to other parsers. One of
Error Type Stanford CVG Berkeley Char-RS
PP Attach 1.02 0.79 0.82 0.60
Clause Attach 0.64 0.43 0.50 0.38
Diff Label 0.40 0.29 0.29 0.31
Mod Attach 0.37 0.27 0.27 0.25
NP Attach 0.44 0.31 0.27 0.25
Co-ord 0.39 0.32 0.38 0.23
1-Word Span 0.48 0.31 0.28 0.20
Unary 0.35 0.22 0.24 0.14
NP Int 0.28 0.19 0.18 0.14
Other 0.62 0.41 0.41 0.50
Table 2: Detailed comparison of different parsers.
the largest sources of improved performance over
the original Stanford factored parser is in the cor-
rect placement of PP phrases. When measuring
only the F1 of parse nodes that include at least one
PP child, the CVG improves the Stanford parser
by 6.2% to an F1 of 77.54%. This is a 0.23 re-
duction in the average number of bracket errors
per sentence. The ?Other? category includes VP,
PRN and other attachments, appositives and inter-
nal structures of modifiers and QPs.
Analysis of Composition Matrices. An analy-
sis of the norms of the binary matrices reveals
that the model learns a soft vectorized notion of
head words: Head words are given larger weights
and importance when computing the parent vec-
tor: For the matrices combining siblings with cat-
egories VP:PP, VP:NP and VP:PRT, the weights in
the part of the matrix which is multiplied with the
VP child vector dominates. Similarly NPs dom-
inate DTs. Fig. 5 shows example matrices. The
two strong diagonals are due to the initialization
described in Sec. 3.7.
Semantic Transfer for PP Attachments. In this
small model analysis, we use two pairs of sen-
tences that the original Stanford parser and the
CVG did not parse correctly after training on
the WSJ. We then continue to train both parsers
on two similar sentences and then analyze if the
parsers correctly transferred the knowledge. The
training sentences are He eats spaghetti with a
fork. and She eats spaghetti with pork. The very
similar test sentences are He eats spaghetti with a
spoon. and He eats spaghetti with meat. Initially,
both parsers incorrectly attach the PP to the verb
in both test sentences. After training, the CVG
parses both correctly, while the factored Stanford
parser incorrectly attaches both PPs to spaghetti.
The CVG?s ability to transfer the correct PP at-
tachments is due to the semantic word vector sim-
ilarity between the words in the sentences. Fig. 4
shows the outputs of the two parsers.
462
(a) Stanford factored parserS
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
DT
a
NN
spoon
S
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
PRP
meat
(b) Compositional Vector GrammarS
NP
PRP
He
VP
VBZ
eats
NP
NNS
spaghetti
PP
IN
with
NP
DT
a
NN
spoon
S
NP
PRP
He
VP
VBZ
eats
NP
NP
NNS
spaghetti
PP
IN
with
NP
NN
meat
Figure 4: Test sentences of semantic transfer for PP attachments. The CVG was able to transfer se-
mantic word knowledge from two related training sentences. In contrast, the Stanford parser could not
distinguish the PP attachments based on the word semantics.
 
 
10 20 30 40 50
5
10
15
20
25 ?0.2
0
0.2
0.4
0.6
0.8
DT-NP
 
 
10 20 30 40 50
5
10
15
20
25
?0.4
?0.2
0
0.2
0.4
0.6
VP-NP
 
 
10 20 30 40 50
5
10
15
20
25 ?0.2
0
0.2
0.4
0.6
ADJP-NP
Figure 5: Three binary composition matrices
showing that head words dominate the composi-
tion. The model learns to not give determiners
much importance. The two diagonals show clearly
the two blocks that are multiplied with the left and
right children, respectively.
5 Conclusion
We introduced Compositional Vector Grammars
(CVGs), a parsing model that combines the speed
of small-state PCFGs with the semantic richness
of neural word representations and compositional
phrase vectors. The compositional vectors are
learned with a new syntactically untied recursive
neural network. This model is linguistically more
plausible since it chooses different composition
functions for a parent node based on the syntac-
tic categories of its children. The CVG obtains
90.44% labeled F1 on the full WSJ test set and is
20% faster than the previous Stanford parser.
Acknowledgments
We thank Percy Liang for chats about the paper.
Richard is supported by a Microsoft Research PhD
fellowship. The authors gratefully acknowledge
the support of the Defense Advanced Research
Projects Agency (DARPA) Deep Exploration and
Filtering of Text (DEFT) Program under Air Force
Research Laboratory (AFRL) prime contract no.
FA8750-13-2-0040, and the DARPA Deep Learn-
ing program under contract number FA8650-10-
C-7020. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of DARPA, AFRL, or the US govern-
ment.
463
References
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Lin-
guistics, 18.
C. Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP, pages 196?205.
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking.
In ACL.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of ACL, pages 132?139.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In ACL.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?637.
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proceedings of
ICML, pages 160?167.
F. Costa, P. Frasconi, V. Lombardo, and G. Soda. 2003.
Towards incremental parsing of natural language us-
ing recursive neural networks. Applied Intelligence.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12, July.
J. L. Elman. 1991. Distributed representations, sim-
ple recurrent networks, and grammatical structure.
Machine Learning, 7(2-3):195?225.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Efficient, feature-based, conditional random field
parsing. In Proceedings of ACL, pages 959?967.
D. Gildea and M. Palmer. 2002. The necessity of pars-
ing for predicate argument recognition. In Proceed-
ings of ACL, pages 239?246.
C. Goller and A. Ku?chler. 1996. Learning task-
dependent distributed representations by backprop-
agation through structure. In Proceedings of the In-
ternational Conference on Neural Networks.
J. Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
MIT.
D. Hall and D. Klein. 2012. Training factored pcfgs
with expectation propagation. In EMNLP.
J. Henderson. 2003. Neural network probability esti-
mation for broad coverage parsing. In Proceedings
of EACL.
J. Henderson. 2004. Discriminative training of a neu-
ral network statistical parser. In ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies (IWPT 2005).
E. H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.
2012. Improving Word Representations via Global
Context and Multiple Word Prototypes. In ACL.
D. Kartsaklis, M. Sadrzadeh, and S. Pulman. 2012. A
unified sentence space for categorical distributional-
compositional semantics: Theory and experiments.
Proceedings of 24th International Conference on
Computational Linguistics (COLING): Posters.
D. Klein and C. D. Manning. 2003a. Accurate un-
lexicalized parsing. In Proceedings of ACL, pages
423?430.
D. Klein and C.D. Manning. 2003b. Fast exact in-
ference with a factored model for natural language
parsing. In NIPS.
J. K. Kummerfeld, D. Hall, J. R. Curran, and D. Klein.
2012. Parser showdown at the wall street corral: An
empirical investigation of error types in parser out-
put. In EMNLP.
Q. V. Le, J. Ngiam, Z. Chen, D. Chia, P. W. Koh, and
A. Y. Ng. 2010. Tiled convolutional neural net-
works. In NIPS.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In ACL.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In NAACL.
S. Menchetti, F. Costa, P. Frasconi, and M. Pon-
til. 2005. Wide coverage natural language pro-
cessing using kernel methods and neural networks
for structured data. Pattern Recognition Letters,
26(12):1896?1906.
T. Mikolov, W. Yih, and G. Zweig. 2013. Linguis-
tic regularities in continuous spaceword representa-
tions. In HLT-NAACL.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of ACL, pages 433?440.
N. Ratliff, J. A. Bagnell, and M. Zinkevich. 2007. (On-
line) subgradient methods for structured prediction.
In Eleventh International Conference on Artificial
Intelligence and Statistics (AIStats).
R. Socher, C. D. Manning, and A. Y. Ng. 2010. Learn-
ing continuous phrase representations and syntactic
parsing with recursive neural networks. In Proceed-
ings of the NIPS-2010 Deep Learning and Unsuper-
vised Feature Learning Workshop.
464
R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and
C. D. Manning. 2011a. Dynamic Pooling and Un-
folding Recursive Autoencoders for Paraphrase De-
tection. In NIPS. MIT Press.
R. Socher, C. Lin, A. Y. Ng, and C.D. Manning. 2011b.
Parsing Natural Scenes and Natural Language with
Recursive Neural Networks. In ICML.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng.
2012. Semantic Compositionality Through Recur-
sive Matrix-Vector Spaces. In EMNLP.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of
EMNLP, pages 1?8.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In CoNLL-X.
I. Titov and J. Henderson. 2007. Constituent parsing
with incremental sigmoid belief networks. In ACL.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of ACL, pages
384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
465
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55?60,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Stanford CoreNLP Natural Language Processing Toolkit
Christopher D. Manning
Linguistics & Computer Science
Stanford University
manning@stanford.edu
Mihai Surdeanu
SISTA
University of Arizona
msurdeanu@email.arizona.edu
John Bauer
Dept of Computer Science
Stanford University
horatio@stanford.edu
Jenny Finkel
Prismatic Inc.
jrfinkel@gmail.com
Steven J. Bethard
Computer and Information Sciences
U. of Alabama at Birmingham
bethard@cis.uab.edu
David McClosky
IBM Research
dmcclosky@us.ibm.com
Abstract
We describe the design and use of the
Stanford CoreNLP toolkit, an extensible
pipeline that provides core natural lan-
guage analysis. This toolkit is quite widely
used, both in the research NLP community
and also among commercial and govern-
ment users of open source NLP technol-
ogy. We suggest that this follows from
a simple, approachable design, straight-
forward interfaces, the inclusion of ro-
bust and good quality analysis compo-
nents, and not requiring use of a large
amount of associated baggage.
1 Introduction
This paper describe the design and development of
Stanford CoreNLP, a Java (or at least JVM-based)
annotation pipeline framework, which provides
most of the common core natural language pro-
cessing (NLP) steps, from tokenization through to
coreference resolution. We describe the original
design of the system and its strengths (section 2),
simple usage patterns (section 3), the set of pro-
vided annotators and how properties control them
(section 4), and how to add additional annotators
(section 5), before concluding with some higher-
level remarks and additional appendices. While
there are several good natural language analysis
toolkits, Stanford CoreNLP is one of the most
used, and a central theme is trying to identify the
attributes that contributed to its success.
2 Original Design and Development
Our pipeline system was initially designed for in-
ternal use. Previously, when combining multiple
natural language analysis components, each with
their own ad hoc APIs, we had tied them together
with custom glue code. The initial version of the
Tokeniza)on*
Sentence*Spli0ng*
Part4of4speech*Tagging*
Morphological*Analysis*
Named*En)ty*Recogni)on*
Syntac)c*Parsing*
Other*Annotators*
Coreference*Resolu)on**
Raw*text*
Execu)
on*Flow
* Annota)on*Object*
Annotated*text*
(tokenize)*
(ssplit)*
(pos)*
(lemma)*
(ner)*
(parse)*
(dcoref)*
(gender, sentiment)!
Figure 1: Overall system architecture: Raw text
is put into an Annotation object and then a se-
quence of Annotators add information in an analy-
sis pipeline. The resulting Annotation, containing
all the analysis information added by the Annota-
tors, can be output in XML or plain text forms.
annotation pipeline was developed in 2006 in or-
der to replace this jumble with something better.
A uniform interface was provided for an Annota-
tor that adds some kind of analysis information to
some text. An Annotator does this by taking in an
Annotation object to which it can add extra infor-
mation. An Annotation is stored as a typesafe het-
erogeneous map, following the ideas for this data
type presented by Bloch (2008). This basic archi-
tecture has proven quite successful, and is still the
basis of the system described here. It is illustrated
in figure 1. The motivations were:
? To be able to quickly and painlessly get linguis-
tic annotations for a text.
? To hide variations across components behind a
common API.
? To have a minimal conceptual footprint, so the
system is easy to learn.
? To provide a lightweight framework, using plain
Java objects (rather than something of heav-
ier weight, such as XML or UIMA?s Common
Analysis System (CAS) objects).
55
In 2009, initially as part of a multi-site grant
project, the system was extended to be more easily
usable by a broader range of users. We provided
a command-line interface and the ability to write
out an Annotation in various formats, including
XML. Further work led to the system being re-
leased as free open source software in 2010.
On the one hand, from an architectural perspec-
tive, Stanford CoreNLP does not attempt to do ev-
erything. It is nothing more than a straightforward
pipeline architecture. It provides only a Java API.
1
It does not attempt to provide multiple machine
scale-out (though it does provide multi-threaded
processing on a single machine). It provides a sim-
ple concrete API. But these requirements satisfy
a large percentage of potential users, and the re-
sulting simplicity makes it easier for users to get
started with the framework. That is, the primary
advantage of Stanford CoreNLP over larger frame-
works like UIMA (Ferrucci and Lally, 2004) or
GATE (Cunningham et al., 2002) is that users do
not have to learn UIMA or GATE before they can
get started; they only need to know a little Java.
In practice, this is a large and important differ-
entiator. If more complex scenarios are required,
such as multiple machine scale-out, they can nor-
mally be achieved by running the analysis pipeline
within a system that focuses on distributed work-
flows (such as Hadoop or Spark). Other systems
attempt to provide more, such as the UIUC Cu-
rator (Clarke et al., 2012), which includes inter-
machine client-server communication for process-
ing and the caching of natural language analyses.
But this functionality comes at a cost. The system
is complex to install and complex to understand.
Moreover, in practice, an organization may well
be committed to a scale-out solution which is dif-
ferent from that provided by the natural language
analysis toolkit. For example, they may be using
Kryo or Google?s protobuf for binary serialization
rather than Apache Thrift which underlies Cura-
tor. In this case, the user is better served by a fairly
small and self-contained natural language analysis
system, rather than something which comes with
a lot of baggage for all sorts of purposes, most of
which they are not using.
On the other hand, most users benefit greatly
from the provision of a set of stable, robust, high
1
Nevertheless, it can call an analysis component written in
other languages via an appropriate wrapper Annotator, and
in turn, it has been wrapped by many people to provide Stan-
ford CoreNLP bindings for other languages.
quality linguistic analysis components, which can
be easily invoked for common scenarios. While
the builder of a larger system may have made over-
all design choices, such as how to handle scale-
out, they are unlikely to be an NLP expert, and
are hence looking for NLP components that just
work. This is a huge advantage that Stanford
CoreNLP and GATE have over the empty tool-
box of an Apache UIMA download, something
addressed in part by the development of well-
integrated component packages for UIMA, such
as ClearTK (Bethard et al., 2014), DKPro Core
(Gurevych et al., 2007), and JCoRe (Hahn et al.,
2008). However, the solution provided by these
packages remains harder to learn, more complex
and heavier weight for users than the pipeline de-
scribed here.
These attributes echo what Patricio (2009) ar-
gued made Hibernate successful, including: (i) do
one thing well, (ii) avoid over-design, and (iii)
up and running in ten minutes or less! Indeed,
the design and success of Stanford CoreNLP also
reflects several other of the factors that Patricio
highlights, including (iv) avoid standardism, (v)
documentation, and (vi) developer responsiveness.
While there are many factors that contribute to the
uptake of a project, and it is hard to show causal-
ity, we believe that some of these attributes ac-
count for the fact that Stanford CoreNLP is one of
the more used NLP toolkits. While we certainly
have not done a perfect job, compared to much
academic software, Stanford CoreNLP has gained
from attributes such as clear open source licens-
ing, a modicum of attention to documentation, and
attempting to answer user questions.
3 Elementary Usage
A key design goal was to make it very simple to
set up and run processing pipelines, from either
the API or the command-line. Using the API, run-
ning a pipeline can be as easy as figure 2. Or,
at the command-line, doing linguistic processing
for a file can be as easy as figure 3. Real life is
rarely this simple, but the ability to get started us-
ing the product with minimal configuration code
gives new users a very good initial experience.
Figure 4 gives a more realistic (and complete)
example of use, showing several key properties of
the system. An annotation pipeline can be applied
to any text, such as a paragraph or whole story
rather than just a single sentence. The behavior of
56
Annotator pipeline = new StanfordCoreNLP();
Annotation annotation = new Annotation(
"Can you parse my sentence?");
pipeline.annotate(annotation);
Figure 2: Minimal code for an analysis pipeline.
export StanfordCoreNLP_HOME /where/installed
java -Xmx2g -cp $StanfordCoreNLP_HOME/*
edu.stanford.nlp.StanfordCoreNLP
-file input.txt
Figure 3: Minimal command-line invocation.
import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;
public class StanfordCoreNlpExample {
public static void main(String[] args) throws IOException {
PrintWriter xmlOut = new PrintWriter("xmlOutput.xml");
Properties props = new Properties();
props.setProperty("annotators",
"tokenize, ssplit, pos, lemma, ner, parse");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = new Annotation(
"This is a short sentence. And this is another.");
pipeline.annotate(annotation);
pipeline.xmlPrint(annotation, xmlOut);
// An Annotation is a Map and you can get and use the
// various analyses individually. For instance, this
// gets the parse tree of the 1st sentence in the text.
List<CoreMap> sentences = annotation.get(
CoreAnnotations.SentencesAnnotation.class);
if (sentences != null && sentences.size() > 0) {
CoreMap sentence = sentences.get(0);
Tree tree = sentence.get(TreeAnnotation.class);
PrintWriter out = new PrintWriter(System.out);
out.println("The first sentence parsed is:");
tree.pennPrint(out);
}
}
}
Figure 4: A simple, complete example program.
annotators in a pipeline is controlled by standard
Java properties in a Properties object. The most
basic property to specify is what annotators to run,
in what order, as shown here. But as discussed be-
low, most annotators have their own properties to
allow further customization of their usage. If none
are specified, reasonable defaults are used. Run-
ning the pipeline is as simple as in the first exam-
ple, but then we show two possibilities for access-
ing the results. First, we convert the Annotation
object to XML and write it to a file. Second, we
show code that gets a particular type of informa-
tion out of an Annotation and then prints it.
Our presentation shows only usage in Java, but
the Stanford CoreNLP pipeline has been wrapped
by others so that it can be accessed easily from
many languages, including Python, Ruby, Perl,
Scala, Clojure, Javascript (node.js), and .NET lan-
guages, including C# and F#.
4 Provided annotators
The annotators provided with StanfordCoreNLP
can work with any character encoding, making use
of Java?s good Unicode support, but the system
defaults to UTF-8 encoding. The annotators also
support processing in various human languages,
providing that suitable underlying models or re-
sources are available for the different languages.
The system comes packaged with models for En-
glish. Separate model packages provide support
for Chinese and for case-insensitive processing of
English. Support for other languages is less com-
plete, but many of the Annotators also support
models for French, German, and Arabic (see ap-
pendix B), and building models for further lan-
guages is possible using the underlying tools. In
this section, we outline the provided annotators,
focusing on the English versions. It should be
noted that some of the models underlying annota-
tors are trained from annotated corpora using su-
pervised machine learning, while others are rule-
based components, which nevertheless often re-
quire some language resources of their own.
tokenize Tokenizes the text into a sequence of to-
kens. The English component provides a PTB-
style tokenizer, extended to reasonably handle
noisy and web text. The corresponding com-
ponents for Chinese and Arabic provide word
and clitic segmentation. The tokenizer saves the
character offsets of each token in the input text.
cleanxml Removes most or all XML tags from
the document.
ssplit Splits a sequence of tokens into sentences.
truecase Determines the likely true case of tokens
in text (that is, their likely case in well-edited
text), where this information was lost, e.g., for
all upper case text. This is implemented with
a discriminative model using a CRF sequence
tagger (Finkel et al., 2005).
pos Labels tokens with their part-of-speech (POS)
tag, using a maximum entropy POS tagger
(Toutanova et al., 2003).
lemma Generates the lemmas (base forms) for all
tokens in the annotation.
gender Adds likely gender information to names.
ner Recognizes named (PERSON, LOCATION,
ORGANIZATION, MISC) and numerical
(MONEY, NUMBER, DATE, TIME, DU-
RATION, SET) entities. With the default
57
annotators, named entities are recognized
using a combination of CRF sequence taggers
trained on various corpora (Finkel et al., 2005),
while numerical entities are recognized using
two rule-based systems, one for money and
numbers, and a separate state-of-the-art system
for processing temporal expressions (Chang
and Manning, 2012).
regexner Implements a simple, rule-based NER
over token sequences building on Java regular
expressions. The goal of this Annotator is to
provide a simple framework to allow a user to
incorporate NE labels that are not annotated in
traditional NL corpora. For example, a default
list of regular expressions that we distribute
in the models file recognizes ideologies (IDE-
OLOGY), nationalities (NATIONALITY), reli-
gions (RELIGION), and titles (TITLE).
parse Provides full syntactic analysis, including
both constituent and dependency representa-
tion, based on a probabilistic parser (Klein and
Manning, 2003; de Marneffe et al., 2006).
sentiment Sentiment analysis with a composi-
tional model over trees using deep learning
(Socher et al., 2013). Nodes of a binarized tree
of each sentence, including, in particular, the
root node of each sentence, are given a senti-
ment score.
dcoref Implements mention detection and both
pronominal and nominal coreference resolution
(Lee et al., 2013). The entire coreference graph
of a text (with head words of mentions as nodes)
is provided in the Annotation.
Most of these annotators have various options
which can be controlled by properties. These can
either be added to the Properties object when cre-
ating an annotation pipeline via the API, or spec-
ified either by command-line flags or through a
properties file when running the system from the
command-line. As a simple example, input to the
system may already be tokenized and presented
one-sentence-per-line. In this case, we wish the
tokenization and sentence splitting to just work by
using the whitespace, rather than trying to do any-
thing more creative (be it right or wrong). This can
be accomplished by adding two properties, either
to a properties file:
tokenize.whitespace: true
ssplit.eolonly: true
in code:
/** Simple annotator for locations stored in a gazetteer. */
package org.foo;
public class GazetteerLocationAnnotator implements Annotator {
// this is the only method an Annotator must implement
public void annotate(Annotation annotation) {
// traverse all sentences in this document
for (CoreMap sentence:annotation.get(SentencesAnnotation.class)) {
// loop over all tokens in sentence (the text already tokenized)
List<CoreLabel> toks = sentence.get(TokensAnnotation.class);
for (int start = 0; start < toks.size(); start++) {
// assumes that the gazetteer returns the token index
// after the match or -1 otherwise
int end = Gazetteer.isLocation(toks, start);
if (end > start) {
for (int i = start; i < end; i ++) {
toks.get(i).set(NamedEntityTagAnnotation.class,"LOCATION");
}
}
}
}
}
}
Figure 5: An example of a simple custom anno-
tator. The annotator marks the words of possibly
multi-word locations that are in a gazetteer.
props.setProperty("tokenize.whitespace", "true");
props.setProperty("ssplit.eolonly", "true");
or via command-line flags:
-tokenize.whitespace -ssplit.eolonly
We do not attempt to describe all the properties
understood by each annotator here; they are avail-
able in the documentation for Stanford CoreNLP.
However, we note that they follow the pattern of
being x.y, where x is the name of the annotator
that they apply to.
5 Adding annotators
While most users work with the provided annota-
tors, it is quite easy to add additional custom an-
notators to the system. We illustrate here both how
to write an Annotator in code and how to load it
into the Stanford CoreNLP system. An Annotator
is a class that implements three methods: a sin-
gle method for analysis, and two that describe the
dependencies between analysis steps:
public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
The information in an Annotation is updated in
place (usually in a non-destructive manner, by
adding new keys and values to the Annotation).
The code for a simple Annotator that marks loca-
tions contained in a gazetteer is shown in figure 5.
2
Similar code can be used to write a wrapper Anno-
tator, which calls some pre-existing analysis com-
ponent, and adds its results to the Annotation.
2
The functionality of this annotator is already provided by
the regexner annotator, but it serves as a simple example.
58
While building an analysis pipeline, Stanford
CoreNLP can add additional annotators to the
pipeline which are loaded using reflection. To pro-
vide a new Annotator, the user extends the class
edu.stanford.nlp.pipeline.Annotator
and provides a constructor with the signature
(String, Properties). Then, the user adds
the property
customAnnotatorClass.FOO: BAR
to the properties used to create the pipeline. If
FOO is then added to the list of annotators, the
class BAR will be loaded to instantiate it. The
Properties object is also passed to the constructor,
so that annotator-specific behavior can be initial-
ized from the Properties object. For instance, for
the example above, the properties file lines might
be:
customAnnotatorClass.locgaz: org.foo.GazetteerLocationAnnotator
annotators: tokenize,ssplit,locgaz
locgaz.maxLength: 5
6 Conclusion
In this paper, we have presented the design
and usage of the Stanford CoreNLP system, an
annotation-based NLP processing pipeline. We
have in particular tried to emphasize the proper-
ties that we feel have made it successful. Rather
than trying to provide the largest and most engi-
neered kitchen sink, the goal has been to make it
as easy as possible for users to get started using
the framework, and to keep the framework small,
so it is easily comprehensible, and can easily be
used as a component within the much larger sys-
tem that a user may be developing. The broad us-
age of this system, and of other systems such as
NLTK (Bird et al., 2009), which emphasize acces-
sibility to beginning users, suggests the merits of
this approach.
A Pointers
Website: http://nlp.stanford.edu/software/
corenlp.shtml
Github: https://github.com/stanfordnlp/CoreNLP
Maven: http://mvnrepository.com/artifact/edu.
stanford.nlp/stanford-corenlp
License: GPL v2+
Stanford CoreNLP keeps the models for ma-
chine learning components and miscellaneous
other data files in a separate models jar file. If you
are using Maven, you need to make sure that you
list the dependency on this models file as well as
the code jar file. You can do that with code like the
following in your pom.xml. Note the extra depen-
dency with a classifier element at the bottom.
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
</dependency>
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
<classifier>models</classifier>
</dependency>
B Human language support
We summarize the analysis components supported
for different human languages in early 2014.
Annotator Ara- Chi- Eng- Fre- Ger-
bic nese lish nch man
Tokenize X X X X X
Sent. split X X X X X
Truecase X
POS X X X X X
Lemma X
Gender X
NER X X X
RegexNER X X X X X
Parse X X X X X
Dep. Parse X X
Sentiment X
Coref. X
C Getting the sentiment of sentences
We show a command-line for sentiment analysis.
$ cat sentiment.txt
I liked it.
It was a fantastic experience.
The plot move rather slowly.
$ java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators
tokenize,ssplit,pos,lemma,parse,sentiment -file sentiment.txt
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/
english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/
englishPCFG.ser.gz ... done [1.4 sec].
Adding annotator sentiment
Ready to process: 1 files, skipped 0, total 1
Processing file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt ... writing to /Users/manning/Software/
stanford-corenlp-full-2014-01-04/sentiment.txt.xml {
Annotating file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt [0.583 seconds]
} [1.219 seconds]
Processed 1 documents
Skipped 0 documents, error annotating 0 documents
Annotation pipeline timing information:
PTBTokenizerAnnotator: 0.0 sec.
WordsToSentencesAnnotator: 0.0 sec.
POSTaggerAnnotator: 0.0 sec.
59
MorphaAnnotator: 0.0 sec.
ParserAnnotator: 0.4 sec.
SentimentAnnotator: 0.1 sec.
TOTAL: 0.6 sec. for 16 tokens at 27.4 tokens/sec.
Pipeline setup: 3.0 sec.
Total time for StanfordCoreNLP pipeline: 4.2 sec.
$ grep sentiment sentiment.txt.xml
<sentence id="1" sentimentValue="3" sentiment="Positive">
<sentence id="2" sentimentValue="4" sentiment="Verypositive">
<sentence id="3" sentimentValue="1" sentiment="Negative">
D Use within UIMA
The main part of using Stanford CoreNLP within
the UIMA framework (Ferrucci and Lally, 2004)
is mapping between CoreNLP annotations, which
are regular Java classes, and UIMA annotations,
which are declared via XML type descriptors
(from which UIMA-specific Java classes are gen-
erated). A wrapper for CoreNLP will typically de-
fine a subclass of JCasAnnotator ImplBase whose
process method: (i) extracts UIMA annotations
from the CAS, (ii) converts UIMA annotations to
CoreNLP annotations, (iii) runs CoreNLP on the
input annotations, (iv) converts the CoreNLP out-
put annotations into UIMA annotations, and (v)
saves the UIMA annotations to the CAS.
To illustrate part of this process, the ClearTK
(Bethard et al., 2014) wrapper converts CoreNLP
token annotations to UIMA annotations and saves
them to the CAS with the following code:
int begin = tokenAnn.get(CharacterOffsetBeginAnnotation.class);
int end = tokenAnn.get(CharacterOffsetEndAnnotation.class);
String pos = tokenAnn.get(PartOfSpeechAnnotation.class);
String lemma = tokenAnn.get(LemmaAnnotation.class);
Token token = new Token(jCas, begin, end);
token.setPos(pos);
token.setLemma(lemma);
token.addToIndexes();
where Token is a UIMA type, declared as:
<typeSystemDescription>
<name>Token</name>
<types>
<typeDescription>
<name>org.cleartk.token.type.Token</name>
<supertypeName>uima.tcas.Annotation</supertypeName>
<features>
<featureDescription>
<name>pos</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
<featureDescription>
<name>lemma</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
</features>
</typeDescription>
</types>
</typeSystemDescription>
References
Steven Bethard, Philip Ogren, and Lee Becker. 2014.
ClearTK 2.0: Design patterns for machine learning
in UIMA. In LREC 2014.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Joshua Bloch. 2008. Effective Java. Addison Wesley,
Upper Saddle River, NJ, 2nd edition.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In LREC 2012.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
learned to stop worrying and love NLP pipelines).
In LREC 2012.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In ACL 2002.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, pages 449?454.
David Ferrucci and Adam Lally. 2004. UIMA: an
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327?348.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL 43, pages 363?370.
I. Gurevych, M. M?uhlh?auser, C. M?uller, J. Steimle,
M. Weimer, and T. Zesch. 2007. Darmstadt knowl-
edge processing repository based on UIMA. In
First Workshop on Unstructured Information Man-
agement Architecture at GLDV 2007, T?ubingen.
U. Hahn, E. Buyko, R. Landefeld, M. M?uhlhausen,
Poprat M, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the Julie lab UIMA component
registry. In LREC 2008.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Suzanna Becker, Sebastian
Thrun, and Klaus Obermayer, editors, Advances in
Neural Information Processing Systems, volume 15,
pages 3?10. MIT Press.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Anthony Patricio. 2009. Why this project is success-
ful? https://community.jboss.org/wiki/
WhyThisProjectIsSuccessful.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP 2013, pages 1631?1642.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL 3, pages 252?259.
60
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 148?153,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Feature-Rich Phrase-based Translation: Stanford University?s Submissionto the WMT 2013 Translation Task
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt*, John Bauer
Sida Wang, Natalia Silveira?, Julia Neidert and Christopher D. Manning
Computer Science Department, Stanford University
*Center for East Asian Studies, Stanford University
?Department of Linguistics, Stanford University
{spenceg,cerd,kreschke,robvoigt,horatio,sidaw,natalias,jneid,manning}@stanford.edu
Abstract
We describe the Stanford University NLP
Group submission to the 2013 Workshop
on Statistical Machine Translation Shared
Task. We demonstrate the effectiveness of a
new adaptive, online tuning algorithm that
scales to large feature and tuning sets. For
both English-French and English-German,
the algorithm produces feature-rich mod-
els that improve over a dense baseline and
compare favorably to models tuned with
established methods.
1 Introduction
Green et al (2013b) describe an online, adaptive
tuning algorithm for feature-rich translation mod-
els. They showed considerable translation quality
improvements over MERT (Och, 2003) and PRO
(Hopkins and May, 2011) for two languages in a
research setting. The purpose of our submission to
the 2013 Workshop on Statistical Machine Trans-
lation (WMT) Shared Task is to compare the algo-
rithm to more established methods in an evaluation.
We submitted English-French (En-Fr) and English-
German (En-De) systems, each with over 100k fea-
tures tuned on 10k sentences. This paper describes
the systems and also includes new feature sets and
practical extensions to the original algorithm.
2 Translation Model
Our machine translation (MT) system is Phrasal
(Cer et al, 2010), a phrase-based system based on
alignment templates (Och and Ney, 2004). Like
many MT systems, Phrasal models the predictive
translation distribution p(e|f ;w) directly as
p(e|f ;w) = 1Z(f) exp
[
w>?(e, f)
]
(1)
where e is the target sequence, f is the source se-
quence, w is the vector of model parameters, ?(?)
is a feature map, and Z(f) is an appropriate nor-
malizing constant. For many years the dimension
of the feature map ?(?) has been limited by MERT,
which does not scale past tens of features.
Our submission explores real-world translation
quality for high-dimensional feature maps and as-
sociated weight vectors. That case requires a more
scalable tuning algorithm.
2.1 Online, Adaptive Tuning Algorithm
FollowingHopkins andMay (2011) we castMT tun-
ing as pairwise ranking. Consider a single source
sentence f with associated references e1:k. Let d
be a derivation in an n-best list of f that has the
target e = e(d) and the feature map ?(d). Define
the linear model scoreM(d) = w ? ?(d). For any
derivation d+ that is better than d? under a gold
metric G, we desire pairwise agreement such that
G
(
e(d+), e1:k
)
> G
(
e(d?), e1:k
)
?? M(d+) > M(d?)
Ensuring pairwise agreement is the same as ensur-
ing w ? [?(d+)? ?(d?)] > 0.
For learning, we need to select derivation pairs
(d+, d?) to compute difference vectors x+ =
?(d+) ? ?(d?). Then we have a 1-class separa-
tion problem trying to ensure w ? x+ > 0. The
derivation pairs are sampled with the algorithm of
Hopkins and May (2011). Suppose that we sample
s pairs for source sentence ft to compute a set of
difference vectors Dt = {x1:s+ }. Then we optimize
`t(w) = `(Dt, w) = ?
?
x+?Dt
log 11 + e?w?x+
(2)
which is the familiar logistic loss. Hopkins and
May (2011) optimize (2) in a batch algorithm
that alternates between candidate generation (i.e.,
n-best list or lattice decoding) and optimization
(e.g., L-BFGS). We instead use AdaGrad (Duchi
148
et al, 2011), a variant of stochastic gradient de-
scent (SGD) in which the learning rate is adapted
to the data. Informally, AdaGrad scales the weight
updates according to the geometry of the data ob-
served in earlier iterations. Consider a particu-
lar dimension j of w, and let scalars vt = wt,j ,
gt = ?j`t(wt?1), and Gt = ?ti=1 g2i . The Ada-Grad update rule is
vt = vt?1 ? ? G?1/2t gt (3)
Gt = Gt?1 + g2t (4)
In practice,Gt is a diagonal approximation. IfGt =
I , observe that (3) is vanilla SGD.
In MT systems, the feature map may generate
exponentially many irrelevant features, so we need
to regularize (3). The L1 norm of the weight vec-
tor is known to be an effective regularizer in such
a setting (Ng, 2004). An efficient way to apply
L1 regularization is the Forward-Backward split-
ting (FOBOS) framework (Duchi and Singer, 2009),
which has the following two-step update:
wt? 12 = wt?1 ? ?t?1?`t?1(wt?1) (5)
wt = argmin
w
1
2?w ? wt? 12 ?
2
2 + ?t?1r(w)
(6)
where (5) is just an unregularized gradient descent
step and (6) balances the regularization term r(w)
with staying close to the gradient step.
For L1 regularization we have r(w) = ?||w||1
and the closed-form solution to (6) is
wt = sign(wt? 12 )
[
|wt? 12 | ? ?t?1?
]
+
(7)
where [x]+ = max(x, 0) is the clipping function
that in this case sets a weight to 0 when it falls below
the threshold ?t?1?.
Online algorithms are inherently sequential; this
algorithm is no exception. If we want to scale the
algorithm to large tuning sets, then we need to par-
allelize the weight updates. Green et al (2013b)
describe the parallelization technique that is imple-
mented in Phrasal.
2.2 Extensions to (Green et al, 2013b)
Sentence-Level Metric We previously used the
gold metric BLEU+1 (Lin and Och, 2004), which
smoothes bigram precisions and above. This metric
worked well with multiple references, but we found
that it is less effective in a single-reference setting
like WMT. To make the metric more robust, Nakov
et al (2012) extended BLEU+1 by smoothing both
the unigram precision and the reference length. We
found that this extension yielded a consistent +0.2
BLEU improvement at test time for both languages.
Subsequent experiments on the data sets of Green
et al (2013b) showed that standard BLEU+1 works
best for multiple references.
Custom regularization parameters Green et al
(2013b) showed that large feature-rich models over-
fit the tuning sets. We discovered that certain fea-
tures caused greater overfitting than others. Custom
regularization strengths for each feature set are one
solution to this problem. We found that technique
largely fixed the overfitting problem as shown by
the learning curves presented in section 5.1.
Convergence criteria Standard MERT imple-
mentations approximate tuning BLEU by re-
ranking the previous n-best lists with the updated
weight vector. This approximation becomes infeasi-
ble for large tuning sets, and is less accurate for algo-
rithms like ours that do not accumulate n-best lists.
We approximate tuning BLEU by maintaining the
1-best hypothesis for each tuning segment. At the
end of each epoch, we compute corpus-level BLEU
from this hypothesis set. We flush the set of stored
hypotheses before the next epoch begins. Although
memory-efficient, we find that this approximation
is less dependable as a convergence criterion than
the conventional method. Whereas we previously
stopped the algorithm after four iterations, we now
select the model according to held-out accuracy.
3 Feature Sets
3.1 Dense Features
The baseline ?dense? model has 19 features: the
nine Moses (Koehn et al, 2007) baseline features, a
hierarchical lexicalized re-ordering model (Galley
and Manning, 2008), the (log) bitext count of each
translation rule, and an indicator for unique rules.
The final dense feature sets for each language
differ slightly. The En-Fr system incorporates a
second language model. The En-De system adds a
future cost component to the linear distortion model
(Green et al, 2010).The future cost estimate allows
the distortion limit to be raised without a decrease
in translation quality.
149
3.2 Sparse Features
Sparse features do not necessarily fire on each hy-
pothesis extension. Unlike prior work on sparseMT
features, our feature extractors do not filter features
based on tuning set counts. We instead rely on the
regularizer to select informative features.
Several of the feature extractors depend on
source-side part of speech (POS) sequences and
dependency parses. We created those annotations
with the Stanford CoreNLP pipeline.
Discriminative Phrase Table A lexicalized in-
dicator feature for each rule in a derivation. The
feature weights can be interpreted as adjustments
to the associated dense phrase table features.
Discriminative Alignments A lexicalized indi-
cator feature for the phrase-internal alignments in
each rule in a derivation. For one-to-many, many-to-
one, and many-to-many alignments we extract the
clique of aligned tokens, perform a lexical sort, and
concatenate the tokens to form the feature string.
Discriminative Re-ordering A lexicalized indi-
cator feature for each rule in a derivation that ap-
pears in the following orientations: monotone-with-
next, monotone-with-previous, non-monotone-
with-next, non-monotone-with-previous. Green
et al (2013b) included the richer non-monotone
classes swap and discontinuous. However, we found
that these classes yielded no significant improve-
ment over the simpler non-monotone classes. The
feature weights can be interpreted as adjustments
to the generative lexicalized re-ordering model.
Source Content-Word Deletion Count-based
features for source content words that are ?deleted?
in the target. Content words are nouns, adjectives,
verbs, and adverbs. A deleted source word is ei-
ther unaligned or aligned to one of the 100 most
frequent target words in the target bitext. For each
deleted word we increment both the feature for the
particular source POS and an aggregate feature for
all parts of speech. We add similar but separate
features for head content words that are either un-
aligned or aligned to frequent target words.
Inverse Document Frequency Numeric fea-
tures that compare source and target word frequen-
cies. Let idf(?) return the inverse document fre-
quency of a token in the training bitext. Suppose
a derivation d = {r1, r2, . . . , rn} is composed of
n translation rules, where e(r) is the target side of
the rule and f(r) is the source side. For each rule
Bilingual Monolingual
Sentences Tokens Tokens
En-Fr 5.0M 289M 1.51B
En-De 4.4M 223M 1.03B
Table 1: Gross corpus statistics after data selection
and pre-processing. The En-Fr monolingual counts
include French Gigaword 3 (LDC2011T10).
r that translates j source tokens to i target tokens
we compute
q =
?
i
idf(e(r)i)?
?
j
idf(f(r)j) (8)
We add two numeric features, one for the source and
another for the target. When q > 0 we increment
the target feature by q; when q < 0 we increment
the target feature by |q|. Together these features
penalize asymmetric rules that map rare words to
frequent words and vice versa.
POS-based Re-ordering The lexicalized dis-
criminative re-ordering model is very sparse, so we
added re-ordering features based on source parts of
speech. When a rule is applied in a derivation, we
extract the associated source POS sequence along
with the POS sequences from the previous and next
rules. We add a ?with-previous? indicator feature
that is the conjunction of the current and previous
POS sequences; the ?with-next? indicator feature is
created analogously. This feature worked well for
En-Fr, but not for En-De.
4 Data Preparation
Table 1 describes the pre-processed corpora from
which our systems are built.
4.1 Data Selection
We used all of the monolingual and parallel En-
De data allowed in the constrained condition. We
incorporated all of the French monolingual data,
but sampled a 5M-sentence bitext from the approx-
imately 40M available En-Fr parallel sentences.
To select the sentences we first created a ?target?
corpus by concatenating the tuning and test sets
(newstest2008?2013). Then we ran the feature
decay algorithm (FDA) (Bi?ici and Yuret, 2011),
which samples sentences that most closely resem-
ble the target corpus. FDA is a principled method
for reducing the phrase table size by excluding less
relevant training examples.
150
4.2 Tokenization
We tokenized the English (source) data according
to the Penn Treebank standard (Marcus et al, 1993)
with Stanford CoreNLP. The French data was to-
kenized with packages from the Stanford French
Parser (Green et al, 2013a), which implements a
scheme similar to that used in the French Treebank
(Abeill? et al, 2003).
German is more complicated due to pervasive
compounding. We first tokenized the data with the
same English tokenizer. Then we split compounds
with the lattice-based model (Dyer, 2009) in cdec
(Dyer et al, 2010). To simplify post-processing we
added segmentation markers to split tokens, e.g.,
?berschritt? ?ber #schritt.
4.3 Alignment
We aligned both bitexts with the Berkeley Aligner
(Liang et al, 2006) configured with standard set-
tings. We symmetrized the alignments according
to the grow-diag heuristic.
4.4 Language Modeling
We estimated unfiltered 5-gram language models
using lmplz (Heafield et al, 2013) and loaded them
with KenLM (Heafield, 2011). For memory effi-
ciency and faster loading we also used KenLM to
convert the LMs to a trie-based, binary format. The
German LM included all of the monolingual data
plus the target side of the En-De bitext. We built
an analogous model for French. In addition, we
estimated a separate French LM from the Gigaword
data.1
4.5 French Agreement Correction
In French verbs must agree in number and person
with their subjects, and adjectives (and some past
participles) must agree in number and gender with
the nouns they modify. On their own, phrasal align-
ment and target side language modeling yield cor-
rect agreement inflection most of the time. For
verbs, we find that the inflections are often accurate:
number is encoded in the English verb and subject,
and 3rd person is generally correct in the absence
of a 1st or 2nd person pronoun. However, since En-
glish does not generally encode gender, adjective
inflection must rely on language modeling, which
is often insufficient.
1The MT system learns significantly different weights for
the two LMs: 0.086 for the primary LM and 0.044 for the
Gigaword LM.
To address this problem we apply an automatic
inflection correction post-processing step. First, we
generate dependency parses of our system?s out-
put using BONSAI (Candito and Crabb?, 2009),
a French-specific extension to the Berkeley Parser
(Petrov et al, 2006). Based on these dependencies,
we match adjectives with the nouns they modify
and past participles with their subjects. Then we
use Lefff (Sagot, 2010), a machine-readable French
lexicon, to determine the gender and number of the
noun and to choose the correct inflection for the
adjective or participle.
Applied to our 3,000 sentence development set,
this correction scheme produced 200 corrections
with perfect accuracy. It produces a slight (?0.014)
drop in BLEU score. This arises from cases where
the reference translation uses a synonymous but
differently gendered noun, and consequently has
different adjective inflection.
4.6 German De-compounding
Split German compounds must be merged after
translation. This process often requires inserting
affixes (e.g., s, en) between adjacent tokens in the
compound. Since the German compounding rules
are complex and exception-laden, we rely on a dic-
tionary lookup procedure with backoffs. The dic-
tionary was constructed during pre-processing. To
compound the final translations, we first lookup
the compound sequence?which is indicated by
segmentation markers?in the dictionary. If it is
present, then we use the dictionary entry. If the com-
pound is novel, then for each pair of words to be
compounded, we insert the suffix most commonly
appended in compounds to the first word of the pair.
If the first word itself is unknown in our dictionary,
we insert the suffix most commonly appended after
the last three characters. For example, words end-
ing with ung most commonly have an s appended
when they are used in compounds.
4.7 Recasing
Phrasal includes an LM-based recaser (Lita et al,
2003), which we trained on the target side of the
bitext for each language. On the newstest2012 de-
velopment data, the German recaser was 96.8% ac-
curate and the French recaser was 97.9% accurate.
5 Translation Quality Experiments
During system development we tuned on
newstest2008?2011 (10,570 sentences) and tested
151
#iterations #features tune newstest2012 newstest2013?
Dense 10 20 30.26 31.12 ?
Feature-rich 11 207k 32.29 31.51 29.00
Table 2: En-Fr BLEU-4 [% uncased] results. The tuning set is newstest2008?2011. (?) newstest2013 is
the cased score computed by the WMT organizers.
#iterations #features tune newstest2012 newstest2013?
Dense 10 19 16.83 18.45 ?
Feature-rich 13 167k 17.66 18.70 18.50
Table 3: En-De BLEU-4 [% uncased] results.
on newstest2012 (3,003 sentences). We compare
the feature-rich model to the ?dense? baseline.
The En-De system parameters were: 200-best
lists, a maximum phrase length of 8, and a distortion
limit of 6 with future cost estimation. The En-Fr
system parameters were: 200-best lists, a maximum
phrase length of 8, and a distortion limit of 5.
The online tuning algorithm used a default learn-
ing rate ? = 0.03 and a mini-batch size of 20. We
set the regularization strength ? to 10.0 for the dis-
criminative re-ordering model, 0.0 for the dense
features, and 0.1 otherwise.
5.1 Results
Tables 2 and 3 show En-Fr and En-De results, re-
spectively. The ?Feature-rich? model, which con-
tains the full complement of dense and sparse fea-
tures, offers ameager improvement over the ?Dense?
baseline. This result contrasts with the results
of Green et al (2013b), who showed significant
translation quality improvements over the same
dense baseline for Arabic-English and Chinese-
English. However, they had multiple target refer-
ences, whereas the WMT data sets have just one.
We speculate that this difference is significant. For
example, consider a translation rule that rewrites
to a 4-gram in the reference. This event can in-
crease the sentence-level score, thus encouraging
the model to upweight the rule indicator feature.
More evidence of overfitting can be seen in Fig-
ure 1, which shows learning curves on the devel-
opment set for both language pairs. Whereas the
dense model converges after just a few iterations,
the feature-rich model continues to creep higher.
Separate experiments on a held-out set showed that
generalization did not improve after about eight
iterations.
6 Conclusion
We submitted a feature-rich MT system to WMT
2013. While sparse features did offer a measur-
able improvement over a baseline dense feature set,
the gains were not as significant as those shown
by Green et al (2013b). One important difference
between the two sets of results is the number of ref-
erences. Their NIST tuning and test sets had four
references; the WMT data sets have just one. We
speculate that sparse features tend to overfit more
in this setting. Individual features can greatly in-
fluence the sentence-level metric and thus become
large components of the gradient. To combat this
phenomenon we experimented with custom reg-
ularization strengths and a more robust sentence-
level metric. While these two improvements greatly
reduced the model size relative to (Green et al,
2013b), a generalization problem remained. Nev-
ertheless, we showed that feature-rich models are
now competitive with the state-of-the-art.
Acknowledgments This work was supported by the Defense
Advanced Research Projects Agency (DARPA) Broad Opera-
tional Language Translation (BOLT) program through IBM.
Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not
necessarily reflect the view of DARPA or the US government.
References
A. Abeill?, L. Cl?ment, and A. Kinyon, 2003. Building
a treebank for French, chapter 10. Kluwer.
E. Bi?ici and D. Yuret. 2011. Instance selection for
machine translation using feature decay algorithms.
In WMT.
M. Candito and B. Crabb?. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT.
152
ll l l l l
l l l l
l
l
l l
l l
l l
l l
29
30
31
32
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(a) En-Fr tuning
l
l
l l l l l l
l l
l
l
l l l
l l
l l l
7.5
10.0
12.5
15.0
17.5
1 2 3 4 5 6 7 8 9 10Epoch
BLEU
 new
test2
008?
2011
Model
l
l
densefeature?rich
(b) En-De tuning
Figure 1: BLEU-4 [% uncased] Learning curves on newstest2008?2011 with loess trend lines.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899?2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121?2159.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In NAACL.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In HLT-NAACL.
S. Green, M-C. de Marneffe, and C. D. Manning.
2013a. Parsing models for identifying multiword
expressions. Computational Linguistics, 39(1):195?
227.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013b.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
M. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19:313?330.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-ization, and rotational invariance. In ICML.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL.
B. Sagot. 2010. The Lefff, a freely available and
large-coverage morphological and syntactic lexicon
for French. In LREC.
153
