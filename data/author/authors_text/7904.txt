	
						
			Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 774?782, Prague, June 2007. c?2007 Association for Computational Linguistics
Scalable Term Selection for Text Categorization
Jingyang Li
National Lab of Intelligent Tech. & Sys.
Department of Computer Sci. & Tech.
Tsinghua University, Beijing, China
lijingyang@gmail.com
Maosong Sun
National Lab of Intelligent Tech. & Sys.
Department of Computer Sci. & Tech.
Tsinghua University, Beijing, China
sms@tsinghua.edu.cn
Abstract
In text categorization, term selection is an
important step for the sake of both cate-
gorization accuracy and computational ef-
ficiency. Different dimensionalities are ex-
pected under different practical resource re-
strictions of time or space. Traditionally
in text categorization, the same scoring or
ranking criterion is adopted for all target
dimensionalities, which considers both the
discriminability and the coverage of a term,
such as ?2 or IG. In this paper, the poor ac-
curacy at a low dimensionality is imputed to
the small average vector length of the docu-
ments. Scalable term selection is proposed
to optimize the term set at a given dimen-
sionality according to an expected average
vector length. Discriminability and cover-
age are separately measured; by adjusting
the ratio of their weights in a combined cri-
terion, the expected average vector length
can be reached, which means a good com-
promise between the specificity and the ex-
haustivity of the term subset. Experiments
show that the accuracy is considerably im-
proved at lower dimensionalities, and larger
term subsets have the possibility to lower
the average vector length for a lower com-
putational cost. The interesting observations
might inspire further investigations.
1 Introduction
Text categorization is a classical text information
processing task which has been studied adequately
(Sebastiani, 2002). A typical text categorization pro-
cess usually involves these phases: document in-
dexing, dimensionality reduction, classifier learn-
ing, classification and evaluation. The vector space
model is frequently used for text representation
(document indexing); dimensions of the learning
space are called terms, or features in a general ma-
chine learning context. Term selection is often nec-
essary because:
? Many irrelevant terms have detrimental effect
on categorization accuracy due to overfitting
(Sebastiani, 2002).
? Some text categorization tasks have many rel-
evant but redundant features, which also hurt
the categorization accuracy (Gabrilovich and
Markovitch, 2004).
? Considerations on computational cost:
(i) Many sophisticated learning machines are
very slow at high dimensionalities, such as
LLSF (Yang and Chute, 1994) and SVMs.
(ii) In Asian languages, the term set is often
very large and redundant, which causes the
learning and the predicting to be really slow.
(iii) In some practical cases the computational
resources (time or space) are restricted, such as
hand-held devices, real-time applications and
frequently retrained systems. (iv) Some deeper
analysis or feature reconstruction techniques
rely on matrix factorization (e.g. LSA based
on SVD), which might be computationally in-
tractable while the dimensionality is large.
Sometimes an aggressive term selection might be
needed particularly for (iii) and (iv). But it is no-
table that the dimensionality is not always directly
774
connected to the computational cost; this issue will
be touched on in Section 6. Although we have
many general feature selection techniques, the do-
main specified ones are preferred (Guyon and Elis-
seeff, 2003). Another reason for ad hoc term se-
lection techniques is that many other pattern clas-
sification tasks has no sparseness problem (in this
study the sparseness means a sample vector has
few nonzero elements, but not the high-dimensional
learning space has few training samples). As a ba-
sic motivation of this study, we hypothesize that the
low accuracy at low dimensionalities is mainly due
to the sparseness problem.
Many term selection techniques were presented
and some of them have been experimentally tested
to be high-performing, such as Information Gain, ?2
(Yang and Pedersen, 1997; Rogati and Yang, 2002)
and Bi-Normal Separation (Forman, 2003). Every-
one of them adopt a criterion scoring and ranking
the terms; for a target dimensionality d, the term se-
lection is simply done by picking out the top-d terms
from the ranked term set. These high performing cri-
teria have a common characteristic ? both discrim-
inability and coverage are implicitly considered.
? discriminability: how unbalanced is the distri-
bution of the term among the categories.
? coverage: how many documents does the term
occur in.
(Borrowing the terminologies from document index-
ing, we can say the specificity of a term set corre-
sponds to the discriminability of each term, and the
exhaustivity of a term set corresponds to the cov-
erage of each term.) The main difference among
these criteria is to what extent the discriminability is
emphasized or the coverage is emphasized. For in-
stance, empirically IG prefers high frequency terms
more than ?2 does, which means IG emphasizes the
coverage more than ?2 does.
The problem is, these criteria are nonparametric
and do the same ranking for any target dimensional-
ity. Small term sets meet the specificity?exhaustivity
dilemma. If really the sparseness is the main rea-
son of the low performance of a small term set, the
specificity should be moderately sacrificed to im-
prove the exhaustivity for a small term set; that is
to say, the term selection criterion should consider
coverage more than discriminability. Contrariwise,
coverage could be less considered for a large term
set, because we need worry little about the sparse-
ness problem and the computational cost might de-
crease.
The remainder of this paper is organized as fol-
lows: Section 2 describes the document collections
used in this study, as well as other experiment set-
tings; Section 3 investigates the relation between
sparseness (measured by average vector length) and
categorization accuracy; Section 4 explains the basic
idea of scalable term selection and proposed a poten-
tial approach; Section 5 carries out experiments to
evaluate the approach, during which some empirical
rules are observed to complete the approach; Sec-
tion 6 makes some further observations and discus-
sions based on Section 5; Section 7 gives a conclud-
ing remark.
2 Experiment Settings
2.1 Document Collections
Two document collections are used in this study.
CE (Chinese Encyclopedia): This is from the
electronic version of the Chinese Encyclopedia. We
choose a Chinese corpus as the primary document
collection because Chinese text (as well as other
Asian languages) has a very large term set and a
satisfying subset is usually not smaller than 50000
(Li et al, 2006); on the contrary, a dimensional-
ity lower than 10000 suffices a general English text
categorization (Yang and Pedersen, 1997; Rogati
and Yang, 2002). For computational cost reasons
mentioned in Section 1, Chinese text categorization
would benefit more from an high-performing ag-
gressive term selection. This collection contains 55
categories and 71674 documents (9:1 split to train-
ing set and test set). Each documents belongs to
only one category. Each category contains 399?
3374 documents. This collection was also used by
Li et al (2006).
20NG (20 Newsgroups1): This classical English
document collection is chosen as a secondary in this
study to testify the generality of the proposed ap-
proach. Some figures about this collection are not
shown in this paper as the figures about CE, viz. Fig-
ure 1?4 because they are similar to CE?s.
1http://people.csail.mit.edu/jrennie/
20Newsgroups
775
2.2 Other Settings
For CE collection, character bigrams are chosen to
be the indexing unit for its high performance (Li et
al., 2006); but the bigram term set suffers from its
high dimensionality. This is exactly the case we tend
to tackle. For 20NG collection, the indexing units
are stemmed2 words. Both term set are df -cut by
the most conservative threshold (df ? 2). The sizes
of the two candidate term sets are |TCE| = 1067717
and |T20NG| = 30220.
Term weighting is done by tfidf (ti, dj) =
log(tf (ti, dj) + 1) ? log
( df (ti)+1
Nd
)
3, in which ti de-
notes a term, dj denotes a document, Nd denotes the
total document number.
The classifiers used in this study are support
vector machines (Joachims, 1998; Gabrilovich and
Markovitch, 2004; Chang and Lin, 2001). The ker-
nel type is set to linear, which is fast and enough
for text categorization. Also, Brank et al (2002)
pointed out that the complexity and sophistication of
the criterion itself is more important to the success
of the term selection method than its compatibility
in design with the classifier.
Performance is evaluated by microaveraged F1-
measure. For single-label tasks, microaveraged pre-
cision, recall and F1 have the same value.
?2 is used as the term selection baseline for its
popularity and high performance. (IG was also re-
ported to be good. In our previous experiments, ?2
is generally superior to IG.) In this study, features
are always selected globally, which means the maxi-
mum are computed for category-specific values (Se-
bastiani, 2002).
3 Average Vector Length (AVL)
In this study, vector length (how many different
terms does the document hold after term selection)
is used as a straightforward sparseness measure for a
document (Brank et al, 2002). Generally, document
sizes have a lognormal distribution (Mitzenmacher,
2003). In our experiment, vector lengths are also
found to be nearly lognormal distributed, as shown
in Figure 1. If the correctly classified documents
2Stemming by Porter?s Stemmer (http://www.
tartarus.org/ martin/PorterStemmer/).
3In our experiments this form of tfidf always outperforms
the basic tfidf (ti, dj) = tf (ti, dj) ? log
?
df (ti)+1
Nd
?
form.
1 10 100 1000
0.00
0.01
0.02
p
r
o
b
 
d
e
n
s
i
t
y
vector length
 correct
 wrong
 all
Figure 1: Vector Length Distributions (smoothed),
on CE Document Collection
1 10 100 1000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
e
r
r
o
r
 
r
a
t
e
vector length
Figure 2: Error Rate vs. Vector Length (smoothed),
on CE Collection, 5000 Dimensions by ?2
and the wrongly classified documents are separately
investigated, they both yield a nearly lognormal dis-
tribution.
Also in Figure 1, wrongly classified documents
shows a relatively large proportion at low dimen-
sionalities. Figure 2 demonstrates this with more
clarity. Thus the hypothesis formed in Section 1 is
confirmed: there is a strong correlation between the
sparseness degree and the categorization error rate.
Therefore, it is quite straightforward a thought to
measure the ?sparseness of a term subset? (or more
precisely, the exhaustivity) by the corresponding av-
erage vector length (AVL) of all documents.4 In the
4Due to the lognormal distribution of vector length, it seems
more plausible to average the logarithmic vector length. How-
ever, for a fixed number of documents , log
P |dj |
|D| should hold
a nearly fixed ratio to
P log |dj |
|D| , in which |D| denotes the doc-
ument number and |dj | denotes the document vector length.
776
remainder of this paper, (log) AVL is an important
metric used to assess and control the sparseness of a
term subset.
4 Scalable Term Selection (STS)
Since the performance droping down at low dimen-
sionalities is attributable to low AVLs in the previous
section, a scalable term selection criterion should
automatically accommodate its favor of high cov-
erage to different target dimensionalities.
4.1 Measuring Discriminability and Coverage
The first step is to separately measure the discrim-
inability and the coverage of a term. A basic guide-
line is that these two metrics should not be highly
(positive) correlated; intuitively, they should have a
slight negative correlation. The correlation of the
two metrics can be visually estimated by the joint
distribution figure. A bunch of term selection met-
rics were explored by Forman (2003). df (document
frequency) is a straightforward choice to measure
coverage. Since df follows the Zipf?s law (inverse
power law), log(df ) is adopted. High-performing
term selection criterion themselves might not be
good candidates for the discriminability metric be-
cause they take coverage into account. For exam-
ple, Figure 3 shows that ?2 is not satisfying. (For
readability, the grayness is proportional to the log
probability density in Figure 3, Figure 4 and Fig-
ure 12.) Relatively, probability ratio (Forman, 2003)
is a more straight metric of discriminability.
PR(ti, c) = P (ti|c+)P (ti|c?) =
df (ti, c+)/df (c+)
df (ti, c?)/df (c?)
It is a symmetric ratio, so log(PR) is likely to be
more appropriate. For multi-class categorization,
a global value can be assessed by PRmax(ti) =
maxc PR(ti, c), like ?2max for ?2 (Yang and Ped-
ersen, 1997; Rogati and Yang, 2002; Sebastiani,
2002); for brief, PR denotes PRmax hereafter. The
joint distribution of log(PR) and log(df ) is shown in
Figure 12. We can see that the distribution is quite
even and they have a slight negative correlation.
4.2 Combined Criterion
Now we have the two metrics: log(PR) for discrim-
inability and log(df ) for coverage, and a parametric
log(df )
?2
1.10 10.591.7
42033.0
Figure 3: (log(df ), ?2) Distribution, on CE
log(df )
log
(PR
)
1.10 10.590.40
9.46
Figure 4: (log(df ), log(PR)) Distribution, on CE
term selection criterion comes forth:
?(ti;?) =
( ?
log(PR(ti)) +
1? ?
log(df (ti))
)?1
A weighted harmonic averaging is adopted here be-
cause either metric?s being too small is a severe
detriment. ? ? [0, 1] is the weight for log(PR),
which denotes how much the discriminability is
emphasized. When the dimensionality is fixed, a
smaller ? leads to a larger AVL and a larger ? leads
to a smaller AVL. The optimal ? should be a function
777
of the expected dimensionality (k):
??(k) = argmax
?
F1(Sk(?))
in which the term subset Sk(?) ? T is selected by
?(?;?) , |Sk| = k, and F1 is the default evaluation
criterion. Naturally, this optimal ? leads to a corre-
sponding optimal AVL:
AVL?(k) ?? ??(k)
For a concrete implementation, we should have an
(empirical) function to estimate ?? or AVL?:
AVL?(k) .= AVL?(k)
In the next section, the values of AVL? (as well as ??)
for some k-s are figured out by experimental search;
then an empirical formula, AVL?(k), comes forth. It
is interesting and inspiring that by adding the ?cor-
pus AVL? as a parameter this formula is universal
for different document collections, which makes the
whole idea valuable.
5 Experiments and Implementation
5.1 Experiments
The expected dimensionalities (k) chosen for exper-
imentation are
CE: 500, 1000, 2000, 4000, . . . , 32000, 64000;
20NG: 500, 1000, 2000, . . . , 16000, 30220.5
For a given document collection and a given target
dimensionality, there is a corresponding AVL for a ?,
and vice versa (for the possible value range of AVL).
According to the observations in Section 5.2, AVL
other than ? is the direct concern because it is more
intrinsic, but ? is the one that can be tuned directly.
So, in the experiments, we vary AVL by tuning ? to
produce it, which means to calculate ?(AVL).
AVL(?) is a monotone function and fast to cal-
culate. For a given AVL, the corresponding ? can
be quickly found by a Newton iteration in [0,1]. In
fact, AVL(?) is not a continuous function, so ? is
only tuned to get an acceptable match, e.g. within
?0.1.
5STS is tested to the whole T on 20NG but not on CE, be-
cause (i) TCE is too large and time consuming for training and
testing, and (ii) ?2 was previously tested on larger k and the
performance (F1) is not stable while k > 64000.
For each k, by the above way of fitting ?,
we manually adjust AVL (only in integers) until
F1(Sk(?(AVL))) peaks. By this way, Figure 5?11
are manually tuned best-performing results as obser-
vations for figuring out the empirical formulas.
Figure 5 shows the F1 peaks at different dimen-
sionalities. Comparing to ?2, STS has a consid-
erable potential superiority at low dimensionalities.
The corresponding values of AVL? are shown in Fig-
ure 6, along with the AVLs of ?2-selected term sub-
sets. The dotted lines show the trend of AVL?; at the
overall dimensionality, |TCE| = 1067717, they have
the same AVL = 898.5. We can see that log(AVL?)
is almost proportional to log(k) when k is not too
large. The corresponding values of ?? are shown in
Figure 7; the relation is nearly linear between ?? and
log(k).
Now it is necessary to explain why an empirical
AVL?(k) derived from the straight line in Figure 6
can be used instead of AVL?(k) in practice. One
important but not plotted property is that the per-
formance of STS is not very sensitive to a small
value change of AVL. For instance, at k = 4000,
AVL? = 120 and the F1 peak is 85.8824%, and
for AVL = 110 and 130 the corresponding F1 are
85.8683% and 85.6583%; at the same k, the F1
of ?2 selection is 82.3950%. This characteristic of
STS guarantee that the empirical AVL?(k) has a very
close performance to AVL?(k); due to the limited
space, the performance curve of AVL?(k) will not
be plotted in Section 5.2.
Same experiments are done on 20NG and the re-
sults are shown in Figure 8, Figure 9 and Figure 10.
The performance improvements is not as signifi-
cant as on the CE collection; this will be discussed
in Section 6.2. The conspicuous relations between
AVL?, ?? and k remain the same.
5.2 Algorithm Completion
In Figure 6 and Figure 9, the ratios of log(AVL?(k))
to log(k) are not the same on CE and 20NG. Tak-
ing into account the corpus AVL (the AVL produced
by the whole term set): AVLTCE = 898.5286 and
AVLT20NG = 82.1605, we guess log(AVL
?(k))
log(AVLT ) is ca-
pable of keeping the same ratio to log(k) for both
CE and 20NG. This hypothesis is confirmed (not for
too high dimensionalities) by Figure 11; Section 6.2
778
100 1000 10000 100000
60
65
70
75
80
85
90
F
1
 
(
%
)
dimensionality (k)
 
2
 STS
Figure 5: Performance Comparison, on CE
1 10 100 1000 10000 100000 1000000
1
10
100
1000
 
2
 STS
A
V
L
*
dimensionality (k)
Figure 6: AVL Comparison, on CE
1 10 100 1000 10000 100000 1000000
0.00
0.02
0.04
0.06
0.08
0.10
0.12
dimensionality (k)
Figure 7: Optimal Weights of log(PR), on CE
100 1000 10000 100000
72
74
76
78
80
82
84
86
 
2
 STS
F
1
 
(
%
)
dimensionality (k)
Figure 8: Performance Comparison, on 20NG
1 10 100 1000 10000 100000
1
10
100
 
2
 STS
A
V
L
*
dimensionality (k)
Figure 9: AVL Comparison, on 20NG
1 10 100 1000 10000 100000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
dimensionality (k)
Figure 10: Optimal Weights of log(PR), on 20NG
779
1 10 100 1000 10000 100000 1000000
0.0
0.2
0.4
0.6
0.8
1.0
l
o
g
(
A
V
L
*
(
k
)
)
 
/
 
l
o
g
(
A
V
L
T
)
dimensionality (k)
 CE
 20NG
Figure 11: log(AVL?(k))log(AVLT ) , on Both CE and 20NG
contains some discussion on this.
From the figure, we get the value of this ratio (the
base of log is set to e):
? = log(AVL
?(k))/log(AVLT )
log(k)
?= 0.085
which should be a universal constant for all text cat-
egorization tasks.
So the empirical estimation of AVL?(k) is given
by
AVL?(k) = exp(? log(AVLT ) ? log(k))
= AVL ? log(k)T
and the final STS criterion is
?(ti, k) = ?(ti;?(AVL?(k)))
= ?(ti;?(AVL ? log(k)T ))
in which ?(?) can be calculated as in Section 5.1.
The target dimensionality, k, is involved as a param-
eter, so the approach is named scalable term selec-
tion. As stated in Section 5.1, AVL?(k) has a very
close performance to AVL?(k) and its performance
is not plotted here.
6 Further Observation and Discussion
6.1 Comparing the Selected Subsets
An investigation shows that for a quite large range
of ?, term rankings by ?(ti;?) and ?2(ti) have a
strong correlation (the Spearman?s rank correlation
coefficient is bigger than 0.999). In order to com-
log(df )
log
(PR
)
1.10 10.590.40
9.46
500100
0
200
0
400
0
800
0
160
00
320
00
640
00STS
?2
Figure 12: Selection Area Comparison of STS and
?2 on Various Dimensionalities, on CE
log(df )
log
(PR
)
1.10 9.140.11
8.04
500100
0
200
0
400
0
800
0
160
00STS
?2
Figure 13: Selection Area Comparison of STS and
?2 on Various Dimensionalities, on 20NG
pare the two criteria?s preferences for discriminabil-
ity and coverage, the selected subsets of different
dimensionalities are shown in Figure 12 (the cor-
responding term density distribution was shown in
Figure 4) and Figure 13. For different dimension-
780
alities, the selection areas of STS are represented by
boundary lines, and the selection areas of ?2 are rep-
resented by different grayness.
In Figure 12, STS shows its superiority at low di-
mensionalities by more emphasis on the coverage
of terms. In Figure 13, STS shows its superior-
ity at high dimensionalities by more emphasis on
the discriminability of terms; lower coverage yields
smaller index size and lower computational cost.
At any dimensionality, STS yields a relatively fixed
bound for either discriminability or coverage, other
than a compromise between them like ?2; this is at-
tributable to the harmonic averaging.
6.2 Adaptability of STS
There are actually two kinds of sparseness in a (vec-
torized) document collection:
collection sparseness: the high-dimensional learn-
ing space contains few training samples;
document sparseness: a document vector has few
nonzero dimensions.
In this study, only the document sparseness is inves-
tigated. The collection sparseness might be a back-
room factor influencing the actual performance on
different document collections. This might explain
why the explicit characteristics of STS are not the
same on CE to 20NG: (comparing with ?2, see Fig-
ure 5, Figure 6, Figure 8 and Figure 9)
CE. The significant F1 improvements at low di-
mensionalities sacrifice the short of AVL. In some
learning process implementations, it is AVL other
than k that determines the computational cost; in
many other cases, k is the determinant. Further
more, possible post-processing, like matrix factor-
ization, might benefit from a low k.
20NG. The F1 improvements at low dimension-
alities is not quite significant, but AVL remains a
lower level. For higher k, there is less difference in
F1, but the smaller AVL yield lower computational
cost than ?2.
Nevertheless, STS shows a stable behavior for
various dimensionalities and quite different docu-
ment collections. The existence of the universal
constant ? empowers it to be adaptive and practi-
cal. As shown in Figure 11, STS draws the rela-
tive log AVL?(k) to the same straight line, ? log(k),
for different document collections. This might
means that the relative AVL is an intrinsic demand
for the term subset size k.
7 Conclusion
In this paper, Scalable Term Selection (STS) is pro-
posed and supposed to be more adaptive than tra-
ditional high-performing criteria, viz. ?2, IG, BNS,
etc. The basic idea of STS is to separately measure
discriminability and coverage, and adjust the relative
importance between them to produce a optimal term
subset of a given size. Empirically, the constant re-
lation between target dimensionality and the optimal
relative average vector length is found, which turned
the idea into implementation.
STS showed considerable adaptivity and stability
for various dimensionalities and quite different doc-
ument collections. The categorization accuracy in-
creasing at low dimensionalities and the computa-
tional cost decreasing at high dimensionalities were
observed.
Some observations are notable: the loglinear rela-
tion between optimal average vector length (AVL?)
and dimensionality (k), the semi-loglinear relation
between weight ? and dimensionality, and the uni-
versal constant ?. For a future work, STS needs to be
conducted on more document collections to check if
? is really universal.
In addition, there could be other implementations
of the general STS idea, via other metrics of discrim-
inability and coverage, other weighted combination
forms, or other term subset evaluations.
Acknowledgement
The research is supported by the National Natural
Science Foundation of China under grant number
60573187, 60621062 and 60520130299.
References
Janez Brank, Marko Grobelnik, Natas?a Milic-
Fraylingand, and Dunjia Mladenic. 2002. Interaction
of feature selection methods and linear classifica-
tion models. Workshop on Text Learning held at
ICML-2002.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
781
George Forman. 2003. An extensive empirical study of
feature selection metrics for text classification. Jour-
nal of Machine Learning Research, 3:1289?1305.
Evgeniy Gabrilovich and Shaul Markovitch. 2004. Text
categorization with many redundant features: using
aggressive feature selection to make svms competitive
with c4.5. In ICML ?04: Proceedings of the twenty-
first international conference on Machine learning,
page 41, New York, NY, USA. ACM Press.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157?1182.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures. In Proceedings of ECML ?98, number 1398,
pages 137?142. Springer Verlag, Heidelberg, DE.
Jingyang Li, Maosong Sun, and Xian Zhang. 2006. A
comparison and semi-quantitative analysis of words
and character-bigrams as features in chinese text cat-
egorization. In Proceedings of COLING-ACL ?06,
pages 545?552. Association for Computational Lin-
guistics, July.
Michael Mitzenmacher. 2003. A brief history of genera-
tive models for power law and lognormal distributions.
Internet Mathematics, 1:226?251.
Monica Rogati and Yiming Yang. 2002. High-
performing feature selection for text classification.
In Proceedings of CIKM ?02, pages 659?661. ACM
Press.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Surveys
(CSUR), 34(1):1?47.
Yiming Yang and Christopher G. Chute. 1994. An
example-based mapping method for text categoriza-
tion and retrieval. ACM Transactions on Information
Systems (TOIS), 12(3):252?277.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Douglas H. Fisher, editor, Proceedings of ICML-
97, 14th International Conference on Machine Learn-
ing, pages 412?420, Nashville, US. Morgan Kauf-
mann Publishers, San Francisco, US.
782
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257?266,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Clustering to Find Exemplar Terms for Keyphrase Extraction
Zhiyuan Liu, Peng Li, Yabin Zheng, Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, pengli09, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn
Abstract
Keyphrases are widely used as a brief
summary of documents. Since man-
ual assignment is time-consuming, vari-
ous unsupervised ranking methods based
on importance scores are proposed for
keyphrase extraction. In practice, the
keyphrases of a document should not only
be statistically important in the docu-
ment, but also have a good coverage of
the document. Based on this observa-
tion, we propose an unsupervised method
for keyphrase extraction. Firstly, the
method finds exemplar terms by leverag-
ing clustering techniques, which guaran-
tees the document to be semantically cov-
ered by these exemplar terms. Then the
keyphrases are extracted from the doc-
ument using the exemplar terms. Our
method outperforms sate-of-the-art graph-
based ranking methods (TextRank) by
9.5% in F1-measure.
1 Introduction
With the development of Internet, information on
the web is emerging exponentially. How to effec-
tively seek and manage information becomes an
important research issue. Keyphrases, as a brief
summary of a document, provide a solution to help
organize, manage and retrieve documents, and are
widely used in digital libraries and information re-
trieval.
Keyphrases in articles of journals and books
are usually assigned by authors. However,
most articles on the web usually do not have
human-assigned keyphrases. Therefore, automatic
keyphrase extraction is an important research task.
Existing methods can be divided into supervised
and unsupervised approaches.
The supervised approach (Turney, 1999) re-
gards keyphrase extraction as a classification task.
In this approach, a model is trained to determine
whether a candidate term of the document is a
keyphrase, based on statistical and linguistic fea-
tures. For the supervised keyphrase extraction
approach, a document set with human-assigned
keyphrases is required as training set. However,
human labelling is time-consuming. Therefore, in
this study we focus on unsupervised approach.
As an example of an unsupervised keyphrase
extraction approach, the graph-based ranking (Mi-
halcea and Tarau, 2004) regards keyphrase extrac-
tion as a ranking task, where a document is repre-
sented by a term graph based on term relatedness,
and then a graph-based ranking algorithm is used
to assign importance scores to each term. Existing
methods usually use term cooccurrences within a
specified window size in the given document as an
approximation of term relatedness (Mihalcea and
Tarau, 2004).
As we know, none of these existing works
gives an explicit definition on what are appropri-
ate keyphrases for a document. In fact, the existing
methods only judge the importance of each term,
and extract the most important ones as keyphrases.
From the observation of human-assigned
keyphrases, we conclude that good keyphrases
of a document should satisfy the following
properties:
1. Understandable. The keyphrases are un-
derstandable to people. This indicates the
extracted keyphrases should be grammatical.
For example, ?machine learning? is a gram-
matical phrase, but ?machine learned? is not.
2. Relevant. The keyphrases are semantically
relevant with the document theme. For ex-
ample, for a document about ?machine learn-
ing?, we want the keyphrases all about this
theme.
3. Good coverage. The keyphrases should
257
cover the whole document well. Sup-
pose we have a document describing ?Bei-
jing? from various aspects of ?location?,
?atmosphere? and ?culture?, the extracted
keyphrases should cover all the three aspects,
instead of just a partial subset of them.
The classification-based approach determines
whether a term is a keyphrase in isolation, which
could not guarantee Property 3. Neither does the
graph-based approach guarantee the top-ranked
keyphrases could cover the whole document. This
may cause the resulting keyphrases to be inappro-
priate or badly-grouped.
To extract the appropriate keyphrases for a doc-
ument, we suggest an unsupervised clustering-
based method. Firstly the terms in a document are
grouped into clusters based on semantic related-
ness. Each cluster is represented by an exemplar
term, which is also the centroid of each cluster.
Then the keyphrases are extracted from the docu-
ment using these exemplar terms.
In this method, we group terms based on se-
mantic relatedness, which guarantees a good cov-
erage of the document and meets Property 2 and
3. Moreover, we only extract the keyphrases in ac-
cordance with noun group (chunk) patterns, which
guarantees the keyphrases satisfy Property 1.
Experiments show that the clustering-based
method outperforms the state-of-the-art graph-
based approach on precision, recall and F1-
measure. Moreover, this method is unsupervised
and language-independent, which is applicable in
the web era with enormous information.
The rest of the paper is organized as follows.
In Section 2, we introduce and discuss the re-
lated work in this area. In Section 3, we give an
overview of our method for keyphrase extraction.
From Section 4 to Section 7, the algorithm is de-
scribed in detail. Empirical experiment results are
demonstrated in Section 8, followed by our con-
clusions and plans for future work in Section 9.
2 Related Work
A straightforward method for keyphrase extrac-
tion is to select keyphrases according to frequency
criteria. However, the poor performance of this
method drives people to explore other methods. A
pioneering achievement is carried out in (Turney,
1999), as mentioned in Section 1, a supervised ma-
chine learning method was suggested in this paper
which regards keyphrase extraction as a classifi-
cation task. In this work, parameterized heuristic
rules are combined with a genetic algorithm into a
system for keyphrase extraction. A different learn-
ing algorithm, Naive Bayes method, is applied in
(Frank et al, 1999) with improved results on the
same data used in (Turney, 1999). Hulth (Hulth,
2003; Hulth, 2004) adds more linguistic knowl-
edge, such as syntactic features, to enrich term
representation, which significantly improves the
performance. Generally, the supervised methods
need manually annotated training set, which may
sometimes not be practical, especially in the web
scenario.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becom-
ing the most widely used unsupervised approach
for keyphrase extraction. The work in (Litvak
and Last, 2008) applies HITS algorithm on the
word graph of a document under the assumption
that the top-ranked nodes should be the document
keywords. Experiments show that classification-
based supervised method provides the highest key-
word identification accuracy, while the HITS al-
gorithm gets the highest F-measure. Work in
(Huang et al, 2006) also considers each document
as a term graph where the structural dynamics of
these graphs can be used to identify keyphrases.
Wan and Xiao (Wan and Xiao, 2008b) use a
small number of nearest neighbor documents to
provide more knowledge to improve graph-based
keyphrase extraction algorithm for single docu-
ment. Motivated by similar idea, Wan and Xiao
(Wan and Xiao, 2008a) propose to adopt cluster-
ing methods to find a small number of similar doc-
uments to provide more knowledge for building
word graphs for keyword extraction. Moreover,
after our submission of this paper, we find that
a method using community detection on seman-
tic term graphs is proposed for keyphrase extrac-
tion from multi-theme documents (Grineva et al,
2009). In addition, some practical systems, such
as KP-Miner (Elbeltagy and Rafea, 2009), also
do not need to be trained on a particular human-
annotated document set.
In recent years, a number of systems are de-
veloped for extracting keyphrases from web docu-
ments (Kelleher and Luz, 2005; Chen et al, 2005),
email (Dredze et al, 2008) and some other spe-
cific sources, which indicates the importance of
keyphrase extraction in the web era. However,
258
none of these previous works has overall consid-
eration on the essential properties of appropriate
keyphrases mentioned in Section 1.
We should also note that, although the preci-
sion and recall of most current keyphrase extrac-
tors are still much lower compared to other NLP-
tasks, it does not indicate the performance is poor
because even different annotators may assign dif-
ferent keyphrases to the same document. As de-
scribed in (Wan and Xiao, 2008b), when two anno-
tators were asked to label keyphrases on 308 doc-
uments, the Kappa statistic for measuring inter-
agreement among them was only 0.70.
3 Algorithm Overview
The method proposed in this paper is mainly in-
spired by the nature of appropriate keyphrases
mentioned in Section 1, namely understandable,
semantically relevant with the document and high
coverage of the whole document.
Let?s analyze the document describing ?Bei-
jing? from the aspects of ?location?, ?atmosphere?
and ?culture?. Under the bag-of-words assump-
tion, each term in the document, except for func-
tion words, is used to describe an aspect of the
theme. Based on these aspects, terms are grouped
into different clusters. The terms in the same clus-
ter are more relevant with each other than with
the ones in other clusters. Taking the terms ?tem-
perature?, ?cold? and ?winter? for example, they
may serve the aspect ?atmosphere? instead of ?lo-
cation? or some other aspects when talking about
?Beijing?.
Based on above description, it is thus reason-
able to propose a clustering-based method for
keyphrase extraction. The overview of the method
is:
1. Candidate term selection. We first filter out
the stop words and select candidate terms for
keyphrase extraction.
2. Calculating term relatedness. We use some
measures to calculate the semantic related-
ness of candidate terms.
3. Term clustering. Based on term relatedness,
we group candidate terms into clusters and
find the exemplar terms of each cluster.
4. From exemplar terms to keyphrases. Fi-
nally, we use these exemplar terms to extract
keyphrases from the document.
In the next four sections we describe the algo-
rithm in detail.
4 Candidate Term Selection
Not all words in a document are possible to be se-
lected as keyphrases. In order to filter out the noisy
words in advance, we select candidate terms using
some heuristic rules. This step proceeds as fol-
lows. Firstly the text is tokenized for English or
segmented into words for Chinese and other lan-
guages without word-separators. Then we remove
the stop words and consider the remaining single
terms as candidates for calculating semantic relat-
edness and clustering.
In methods like (Turney, 1999; Elbeltagy and
Rafea, 2009), candidate keyphrases were first
found using n-gram. Instead, in this method, we
just find the single-word terms as the candidate
terms at the beginning. After identifying the ex-
emplar terms within the candidate terms, we ex-
tract multi-word keyphrases using the exemplars.
5 Calculating Term Relatedness
After selecting candidate terms, it is important to
measure term relatedness for clustering. In this pa-
per, we propose two approaches to calculate term
relatedness: one is based on term cooccurrence
within the document, and the other by leveraging
human knowledge bases.
5.1 Cooccurrence-based Term Relatedness
An intuitive method for measuring term relat-
edness is based on term cooccurrence relations
within the given document. The cooccurrence
relation expresses the cohesion relationships be-
tween terms.
In this paper, cooccurrence-based relatedness is
simply set to the count of cooccurrences within a
window of maximum w words in the whole doc-
ument. In the following experiments, the window
size w is set from 2 to 10 words.
Each document can be regarded as a word se-
quence for computing cooccurrence-based relat-
edness. There are two types of word sequence
for counting term cooccurrences. One is the origi-
nal word sequence without filtering out any words,
and the other is after filtering out the stop words
or the words with specified part-of-speech (POS)
tags. In this paper we select the first type because
each word in the sequence takes important role for
measuring term cooccurrences, no matter whether
259
it is a stop word or something else. If we filter
out some words, the term relatedness will not be
as precise as before.
In experiments, we will investigate how the
window size influences the performance of
keyphrase extraction.
5.2 Wikipedia-based Term Relatedness
Many methods have been proposed for measuring
the relatedness between terms using external re-
sources. One principled method is leveraging hu-
man knowledge bases. Inspired by (Gabrilovich
and Markovitch, 2007), we adopt Wikipedia, the
largest encyclopedia collected and organized by
human on the web, as the knowledge base to mea-
sure term relatedness.
The basic idea of computing term related-
ness by leveragting Wikipedia is to consider each
Wikipedia article as a concept. Then the se-
mantic meaning of a term could be represented
as a weighted vector of Wikipedia concepts, of
which the values are the term?s TFIDF within cor-
responding Wikipedia articles. We could com-
pute the term relatedness by comparing the con-
cept vectors of the terms. Empirical evaluations
confirm that the idea is effective and practical
for computing term relatedness (Gabrilovich and
Markovitch, 2007).
In this paper, we select cosine similarity, Eu-
clidean distance, Point-wise Mutual Information
and Normalized Google Similarity Distance (Cili-
brasi and Vitanyi, 2007) for measuring term relat-
edness based on the vector of Wikipedia concepts.
Denote the Wikipedia-concept vector of the
term t
i
as C
i
= {c
i1
, c
i2
, ..., c
iN
}, where N in-
dicates the number of Wikipedia articles, and c
ik
is the TFIDF value of w
i
in the kth Wikipedia ar-
ticle. The cosine similarity is defined as
cos(i, j) =
C
i
? C
j
?C
i
??C
j
?
(1)
The definition of Euclidean distance is
euc(i, j) =
?
?
?
?
N
?
k=1
(c
ik
? c
jk
)
2 (2)
Point-wise Mutual Information (PMI) is a com-
mon approach to quantify relatedness. Here we
take three ways to measure term relatedness using
PMI. One is based on Wikipedia page count,
pmi
p
(i, j) = log
2
N ? p(i, j)
p(i) ? p(j)
(3)
where p(i, j) is the number of Wikipedia articles
containing both t
i
and t
j
, while p(i) is the number
of articles which contain t
i
. The second is based
on the term count in Wikipedia articles,
pmi
t
(i, j) = log
2
T ? t(i, j)
t(i) ? t(j)
(4)
where T is the number of terms in Wikipedia,
t(i, j) is the number of t
i
and t
j
occurred adja-
cently in Wikipedia, and t(i) is the number of t
i
in
Wikipedia. The third one is a combination of the
above two PMI ways,
pmi
c
(i, j) = log
2
N ? pt(i, j)
p(i) ? p(j)
(5)
where pt(i, j) indicates the number of Wikipedia
articles containing t
i
and t
j
as adjacency. It is ob-
vious that pmi
c
(i, j) ? pmi
p
(i, j), and pmi
c
(i, j)
is more strict and accurate for measuring related-
ness.
Normalized Google Similarity Distance (NGD)
is a new measure for measuring similarity between
terms proposed by (Cilibrasi and Vitanyi, 2007)
based on information distance and Kolmogorov
complexity. It could be applied to compute term
similarity from the World Wide Web or any large
enough corpus using the page counts of terms.
NGD used in this paper is based on Wikipedia ar-
ticle count, defined as
ngd(i, j) =
max(log p(i), log p(j)) ? logp(i, j)
logN ? min(logp(i), logp(j))
(6)
where N is the number of Wikipedia articles used
as normalized factor.
Once we get the term relatedness, we could then
group the terms using clustering techniques and
find exemplar terms for each cluster.
6 Term Clustering
Clustering is an important unsupervised learning
problem, which is the assignment of objects into
groups so that objects from the same cluster are
more similar to each other than objects from dif-
ferent clusters (Han and Kamber, 2005). In this
paper, we use three widely used clustering algo-
rithms, hierarchical clustering, spectral clustering
and Affinity Propagation, to cluster the candidate
terms of a given document based on the semantic
relatedness between them.
260
6.1 Hierarchical Clustering
Hierarchical clustering groups data over a variety
of scales by creating a cluster tree. The tree is a
multilevel hierarchy, where clusters at one level
are joined as clusters at the next level. The hier-
archical clustering follows this procedure:
1. Find the distance or similarity between every
pair of data points in the dataset;
2. Group the data points into a binary and hier-
archical cluster tree;
3. Determine where to cut the hierarchical tree
into clusters. In hierarchical clustering, we
have to specify the cluster number m in ad-
vance.
In this paper, we use the hierarchical cluster-
ing implemented in Matlab Statistics Toolbox.
Note that although we use hierarchical clustering
here, the cluster hierarchy is not necessary for the
clustering-based method.
6.2 Spectral Clustering
In recent years, spectral clustering has become one
of the most popular modern clustering algorithms.
Spectral clustering makes use of the spectrum of
the similarity matrix of the data to perform dimen-
sionality reduction for clustering into fewer di-
mensions, which is simple to implement and often
outperforms traditional clustering methods such as
k-means. Detailed introduction to spectral cluster-
ing could be found in (von Luxburg, 2006).
In this paper, we use the spectral clustering tool-
box developed by Wen-Yen Chen, et al (Chen et
al., 2008) 1. Since the cooccurrence-based term
relatedness is usually sparse, the traditional eigen-
value decomposition in spectral clustering will
sometimes get run-time error. In this paper, we
use the singular value decomposition (SVD) tech-
nique for spectral clustering instead.
For spectral clustering, two parameters are re-
quired to be set by the user: the cluster number
m, and ? which is used in computing similarities
from object distances
s(i, j) = exp(
?d(i, j)
2
2?
2
) (7)
where s(i, j) and d(i, j) are the similarity and dis-
tance between i and j respectively.
1The package could be accessed via http://www.cs.
ucsb.edu/
?
wychen/sc.html.
6.3 Affinity Propagation
Another powerful clustering method, Affinity
Propagation, is based on message passing tech-
niques. AP was proposed in (Frey and Dueck,
2007), where AP was reported to find clusters with
much lower error than those found by other meth-
ods. In this paper, we use the toolbox developed
by Frey, et al 2.
Detailed description of the algorithm could be
found in (Frey and Dueck, 2007). Here we intro-
duced three parameters for AP:
? Preference. Rather than requiring prede-
fined number of clusters, Affinity Propaga-
tion takes as input a real number p for each
term, so that the terms with larger p are more
likely to be chosen as exemplars, i.e., cen-
troids of clusters. These values are referred
to as ?preferences?. The preferences are usu-
ally be set as the maximum, minimum, mean
or median of s(i, j), i 6= j.
? Convergence criterion. AP terminates if (1)
the local decisions stay constant for I
1
itera-
tions; or (2) the number of iterations reaches
I
2
. In this work, we set I
1
to 100 and I
2
to
1, 000.
? Damping factor. When updating the mes-
sages, it is important to avoid numerical os-
cillations by using damping factor. Each
message is set to ? times its value from the
previous iteration plus 1 ? ? times its pre-
scribed updated value, where the damping
factor ? is between 0 and 1. In this paper we
set ? = 0.9.
7 From Exemplar Terms to Keyphrases
After term clustering, we select the exemplar
terms of each clusters as seed terms. In Affinity
Propagation, the exemplar terms are directly ob-
tained from the clustering results. In hierarchical
clustering, exemplar terms could also be obtained
by the Matlab toolbox. While in spectral cluster-
ing, we select the terms that are most close to the
centroid of a cluster as exemplar terms.
As reported in (Hulth, 2003), most manually
assigned keyphrases turn out to be noun groups.
Therefore, we annotate the document with POS
2The package could be accessed via http://www.
psi.toronto.edu/affinitypropagation/.
261
tags using Stanford Log-Linear Tagger 3, and then
extract the noun groups whose pattern is zero or
more adjectives followed by one or more nouns.
The pattern can be represented using regular ex-
pressions as follows
(JJ) ? (NN |NNS|NNP )+
where JJ indicates adjectives and various forms
of nouns are represented using NN , NNS and
NNP . From these noun groups, we select the
ones that contain one or more exemplar terms to
be the keyphrases of the document.
In this process, we may find single-word
keyphrases. In practice, only a small fraction of
keyphrases are single-word. Thus, as a part of
postprocessing process, we have to use a frequent
word list to filter out the terms that are too com-
mon to be keyphrases.
8 Experiment Results
8.1 Datasets and Evaluation Metric
The dataset used in the experiments is a collec-
tion of scientific publication abstracts from the In-
spec database and the corresponding manually as-
signed keyphrases 4. The dataset is used in both
(Hulth, 2003) and (Mihalcea and Tarau, 2004).
Each abstract has two kinds of keyphrases: con-
trolled keyphrases, restricted to a given dictionary,
and uncontrolled keyphrases, freely assigned by
the experts. We use the uncontrolled keyphrases
for evaluation as proposed in (Hulth, 2003) and
followed by (Mihalcea and Tarau, 2004).
As indicated in (Hulth, 2003; Mihalcea and
Tarau, 2004), in uncontrolled manually assigned
keyphrases, only the ones that occur in the cor-
responding abstracts are considered in evaluation.
The extracted keyphrases of various methods and
manually assigned keyphrases are compared after
stemming.
In the experiments of (Hulth, 2003), for her su-
pervised method, Hulth splits a total of 2, 000 ab-
stracts into 1, 000 for training, 500 for validation
and 500 for test. In (Mihalcea and Tarau, 2004),
due to the unsupervised method, only the test set
was used for comparing the performance of Tex-
tRank and Hulth?s method.
3The package could be accessed via http://http://
nlp.stanford.edu/software/tagger.shtml.
4Many thanks to Anette Hulth for providing us the dataset.
For computing Wikipedia-based relatedness,
we use a snapshot on November 11, 2005 5. The
frequent word list used in the postprocessing step
for filtering single-word phrases is also computed
from Wikipedia. In the experiments of this pa-
per, we add the words that occur more than 1, 000
times in Wikipedia into the list.
The clustering-based method is completely un-
supervised. Here, we mainly run our method on
test set and investigate the influence of relatedness
measurements and clustering methods with differ-
ent parameters. Then we compare our method
with two baseline methods: Hulth?s method and
TextRank. Finally, we analyze and discuss the per-
formance of the method by taking the abstract of
this paper as a demonstration.
8.2 Influence of Relatedness Measurements
We first investigate the influence of semantic re-
latedness measurements. By systematic experi-
ments, we find that Wikipedia-based relatedness
outperforms cooccurrence-based relatedness for
keyphrase extraction, though the improvement is
not significant. In Table 1, we list the perfor-
mance of spectral clustering with various related-
ness measurements for demonstration. In this ta-
ble, the w indicates the window size for counting
cooccurrences in cooccurrence-based relatedness.
cos, euc, etc. are different measures for com-
puting Wikipedia-based relatedness which we pre-
sented in Section 5.2.
Table 1: Influence of relatedness measurements
for keyphrase extraction.
Parameters Precision Recall F1-measure
Cooccurrence-based Relatedness
w = 2 0.331 0.626 0.433
w = 4 0.333 0.621 0.434
w = 6 0.331 0.630 0.434
w = 8 0.330 0.623 0.432
w = 10 0.333 0.632 0.436
Wikipedia-based Relatedness
cos 0.348 0.655 0.455
euc 0.344 0.634 0.446
pmi
p
0.344 0.621 0.443
pmi
t
0.344 0.619 0.442
pmi
c
0.350 0.660 0.457
ngd 0.343 0.620 0.442
5The dataset could be get from http://www.cs.
technion.ac.il/
?
gabr/resources/code/
wikiprep/.
262
We use spectral clustering here because it out-
performs other clustering techniques, which will
be shown in the next subsection. The results in Ta-
ble 1 are obtained when the cluster number m =
2
3
n, where n is the number of candidate terms ob-
tained in Section 5. Besides, for Euclidean dis-
tance and Google distance, we set ? = 36 of For-
mula 7 to convert them to corresponding similari-
ties, where we get the best result when we conduct
different trails with ? = 9, 18, 36, 54, though there
are only a small margin among them.
As shown in Table 1, although the method using
Wikipedia-based relatedness outperforms that us-
ing cooccurrence-based relatedness, the improve-
ment is not prominent. Wikipedia-based related-
ness is computed according to global statistical in-
formation on Wikipedia. Therefore it is more pre-
cise than cooccurrence-based relatedness, which is
reflected in the performance of the keyphrase ex-
traction. However, on the other hand, Wikipedia-
based relatedness does not catch the document-
specific relatedness, which is represented by the
cooccurrence-based relatedness. It will be an in-
teresting future work to combine these two types
of relatedness measurements.
From this subsection, we conclude that, al-
though the method using Wikipedia-based related-
ness performs better than cooccurrence-based one,
due to the expensive computation of Wikipedia-
based relatedness, the cooccurrence-based one is
good enough for practical applications.
8.3 Influence of Clustering Methods and
Their Parameters
To demonstrate the influence of clustering meth-
ods for keyphrase extraction, we fix the relat-
edness measurement as Wikipedia-based pmi
c
,
which has been shown in Section 8.2 to be the best
relatedness measurement.
In Table 2, we show the performance of three
clustering techniques for keyphrase extraction.
For hierarchical clustering and spectral clustering,
the cluster number m are set explicitly as the pro-
portion of candidate terms n, while for Affinity
Propagation, we set preferences as the minimum,
mean, median and maximum of s(i, j) to get dif-
ferent number of clusters, denoted as min, mean,
median and max in the table respectively.
As shown in the table, when cluster number m
is large, spectral clustering outperforms hierarchi-
cal clustering and Affinity Propagation. Among
Table 2: Influence of clustering methods for
keyphrase extraction.
Parameters Precision Recall F1-measure
Hierarchical Clustering
m =
1
4
n 0.365 0.369 0.367
m =
1
3
n 0.365 0.369 0.367
m =
1
2
n 0.351 0.562 0.432
m =
2
3
n 0.346 0.629 0.446
m =
4
5
n 0.340 0.657 0.448
Spectral Clustering
m =
1
4
n 0.385 0.409 0.397
m =
1
3
n 0.374 0.497 0.427
m =
1
2
n 0.374 0.497 0.427
m =
2
3
n 0.350 0.660 0.457
m =
4
5
n 0.340 0.679 0.453
Affinity Propagation
p = max 0.331 0.688 0.447
p = mean 0.433 0.070 0.121
p = median 0.422 0.078 0.132
p = min 0.419 0.059 0.103
these methods, only Affinity Propagation under
some parameters performs poorly.
8.4 Comparing with Other Algorithms
Table 3 lists the results of the clustering-based
method compared with the best results reported
in (Hulth, 2003; Mihalcea and Tarau, 2004) on
the same dataset. For each method, the table lists
the total number of assigned keyphrases, the mean
number of keyphrases per abstract, the total num-
ber of correct keyphrases, and the mean number of
correct keyphrases. The table also lists precision,
recall and F1-measure. In this table, hierarchical
clustering, spectral clustering and Affinity Propa-
gation are abbreviated by ?HC?, ?SC? and ?AP?
respectively.
The result of Hulth?s method listed in this ta-
ble is the best one reported in (Hulth, 2003) on the
same dataset. This is a supervised classification-
based method, which takes more linguistic fea-
tures in consideration for keyphrase extraction.
The best result is obtained using n-gram as candi-
date keyphrases and adding POS tags as candidate
features for classification.
The result of TextRank listed here is the best
one reported in (Mihalcea and Tarau, 2004) on the
same dataset. To obtain the best result, the authors
built an undirected graph using window w = 2
on word sequence of the given document, and ran
263
Table 3: Comparison results of Hulth?s method, TextRank and our clustering-based method.
Assigned Correct
Method Total Mean Total Mean Precision Recall F1-measure
Hulth?s 7,815 15.6 1,973 3.9 0.252 0.517 0.339
TextRank 6,784 13.7 2,116 4.2 0.312 0.431 0.362
HC 7,303 14.6 2,494 5.0 0.342 0.657 0.449
SC 7,158 14.3 2,505 5.0 0.350 0.660 0.457
AP 8,013 16.0 2,648 5.3 0.330 0.697 0.448
PageRank on it.
In this table, the best result of hierarchical clus-
tering is obtained by setting the cluster number
m =
2
3
n and using Euclidean distance for comput-
ing Wikipedia-based relatedness. The parameters
of spectral clustering are the same as in last sub-
section. For Affinity Propagation, the best result
is obtained under p = max and using Wikipedia-
based Euclidean distance as relatedness measure.
From this table, we can see clustering-
based method outperforms TextRank and Hulth?s
method. For spectral clustering, F1-measure
achieves an approximately 9.5% improvement as
compared to TextRank.
Furthermore, since the clustering-based method
is unsupervised, we do not need any set for train-
ing and validation. In this paper, we also carry out
an experiment on the whole Hulth?s dataset with
2, 000 abstracts. The performance is similar to
that on 500 abstracts as shown above. The best
result is obtained when we use spectral clustering
by setting m = 2
3
n with Wikipedia-based pmi
c
relatedness, which is the same in 500 abstracts. In
this result, we extract 29, 517 keyphrases, among
which 9, 655 are correctly extracted. The preci-
sion, recall and F1-measure are 0.327, 0.653 and
0.436 respectively. The experiment results show
that the clustering-based method is stable.
8.5 Analysis and Discussions
From the above experiment results, we can see the
clustering-based method is both robust and effec-
tive for keyphrase extraction as an unsupervised
method.
Here, as an demonstration, we use spectral clus-
tering and Wikipedia-based pmi
c
relatedness to
extract keyphrases from the abstract of this pa-
per. The extracted stemmed keyphrases under var-
ious cluster numbers are shown in Figure 1. In
this figure, we find that when m = 1
4
n,
1
3
n,
1
2
n,
the extracted keyphrases are identical, where the
exemplar terms under m = 1
3
n are marked in
boldface. We find several aspects like ?unsuper-
vised?, ?exemplar term? and ?keyphrase extrac-
tion? are extracted correctly. In fact, ?clustering
technique? in the abstract should also be extracted
as a keyphrase. However, since ?clustering? is
tagged as a verb that ends in -ing, which disagrees
the noun group patterns, thus the phrase is not
among the extracted keyphrases.
When m = 2
3
n, the extracted keyphrases
are noisy with many single-word phrases. As
the cluster number increases, more exemplar
terms are identified from these clusters, and more
keyphrases will be extracted from the document
based on exemplar terms. If we set the cluster
number to m = n, all terms will be selected as
exemplar terms. In this extreme case, all noun
groups will be extracted as keyphrases, which
is obviously not proper for keyphrase extraction.
Thus, it is important for this method to appropri-
ately specify the cluster number.
In the experiments, we also notice that frequent
word list is important for keyphrase extraction.
Without the list for filtering, the best F1-measure
will decrease by about 5 percent to 40%. How-
ever, the solution of using frequent word list is
somewhat too simple, and in future work, we plan
to investigate a better combination of clustering-
based method with traditional methods using term
frequency as the criteria.
9 Conclusion and Future Work
In this paper, we propose an unsupervised
clustering-based keyphrase extraction algorithm.
This method groups candidate terms into clus-
ters and identify the exemplar terms. Then
keyphrases are extracted from the document based
on the exemplar terms. The clustering based on
term semantic relatedness guarantees the extracted
keyphrases have a good coverage of the document.
Experiment results show the method has a good ef-
264
Figure 1: Keyphrases in stemmed form extracted
from this paper?s abstract.
Keyphrases when m = 1
4
n,
1
3
n,
1
2
n
unsupervis method; various unsupervis rank
method; exemplar term; state-of-the-art
graph-bas rank method; keyphras; keyphras
extract
Keyphrases when m = 2
3
n
unsupervis method; manual assign; brief sum-
mari; various unsupervis rank method; exem-
plar term; document; state-of-the-art graph-bas
rank method; experi; keyphras; import score;
keyphras extract
fectiveness and robustness, and outperforms base-
lines significantly.
Future work may include:
1. Investigate the feasibility of clustering di-
rectly on noun groups;
2. Investigate the feasibility of combining
cooccurrence-based and Wikipedia-based re-
latedness for clustering;
3. Investigate the performance of the method on
other types of documents, such as long arti-
cles, product reviews and news;
4. The solution of using frequent word list
for filtering out too common single-word
keyphrases is undoubtedly simple, and we
plan to make a better combination of
the clustering-based method with traditional
frequency-based methods for keyphrase ex-
traction.
Acknowledgments
This work is supported by the National 863 Project
under Grant No. 2007AA01Z148 and the Na-
tional Science Foundation of China under Grant
No. 60621062. The authors would like to thank
Anette Hulth for kindly sharing her datasets.
References
Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan
Lam. 2005. A practical system of keyphrase extrac-
tion for web pages. In Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 277?278.
Wen Y. Chen, Yangqiu Song, Hongjie Bai, Chih J. Lin,
and Edward Chang. 2008. Psc: Paralel spectral
clustering. Submitted.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Transactions on
Knowledge and Data Engineering, 19(3):370?383.
Mark Dredze, Hanna M. Wallach, Danny Puller, and
Fernando Pereira. 2008. Generating summary key-
words for emails using topics. In Proceedings of the
13th international conference on Intelligent user in-
terfaces, pages 199?206.
S. Elbeltagy and A. Rafea. 2009. Kp-miner: A
keyphrase extraction system for english and arabic
documents. Information Systems, 34(1):132?144.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence, pages 668?673.
Brendan J J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6?12.
M. Grineva, M. Grinev, and D. Lizorkin. 2009. Ex-
tracting key terms from noisy and multi-theme docu-
ments. In Proceedings of the 18th international con-
ference on World wide web, pages 661?670. ACM
New York, NY, USA.
Jiawei Han and Micheline Kamber. 2005. Data Min-
ing: Concepts and Techniques, second edition. Mor-
gan Kaufmann.
Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of the 6th International Conference on
Data Mining, pages 275?284.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 216?223.
A. Hulth. 2004. Reducing false positives by expert
combination in automatic keyword indexing. Re-
cent Advances in Natural Language Processing III:
Selected Papers from RANLP 2003, page 367.
Daniel Kelleher and Saturnino Luz. 2005. Automatic
hypertext keyphrase detection. In Proceedings of the
19th International Joint Conference on Artificial In-
telligence.
265
Marina Litvak and Mark Last. 2008. Graph-based
keyword extraction for single-document summariza-
tion. In Proceedings of the workshop Multi-source
Multilingual Information Extraction and Summa-
rization, pages 17?24.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
Peter D. Turney. 1999. Learning to Extract Keyphrases
from Text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
U. von Luxburg. 2006. A tutorial on spectral clus-
tering. Technical report, Max Planck Institute for
Biological Cybernetics.
Xiaojun Wan and Jianguo Xiao. 2008a. Col-
labrank: Towards a collaborative approach to single-
document keyphrase extraction. In Proceedings of
COLING, pages 969?976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, pages
855?860.
266
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 34 ? 45, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Automatic Image Annotation Using  
Maximum Entropy Model 
Wei Li and Maosong Sun 
State Key Lab of Intelligent Technology and Systems, 
Department of Computer Science and Technology, Tsinghua University, 
Beijing 100084, China 
wei.lee04@gmail.com, sms@mail.tsinghua.edu.cn 
Abstract. Automatic image annotation is a newly developed and promising 
technique to provide semantic image retrieval via text descriptions. It concerns 
a process of automatically labeling the image contents with a pre-defined set of 
keywords which are exploited to represent the image semantics. A Maximum 
Entropy Model-based approach to the task of automatic image annotation is 
proposed in this paper. In the phase of training, a basic visual vocabulary con-
sisting of blob-tokens to describe the image content is generated at first; then 
the statistical relationship is modeled between the blob-tokens and keywords by 
a Maximum Entropy Model constructed from the training set of labeled images. 
In the phase of annotation, for an unlabeled image, the most likely associated 
keywords are predicted in terms of the blob-token set extracted from the given 
image. We carried out experiments on a medium-sized image collection with 
about 5000 images from Corel Photo CDs. The experimental results demon-
strated that the annotation performance of this method outperforms some tradi-
tional annotation methods by about 8% in mean precision, showing a potential 
of the Maximum Entropy Model in the task of automatic image annotation. 
1   Introduction 
Last decade has witnessed an explosive growth of multimedia information such as 
images and videos. However, we can?t access to or make use of the relevant informa-
tion more leisurely unless it is organized so as to provide efficient browsing and que-
rying. As a result, an important functionality of next generation multimedia informa-
tion management system will undoubtedly be the search and retrieval of images and 
videos on the basis of visual content.  
In order to fulfill this ?intelligent? multimedia search engines on the world-wide-
web, content-based image retrieval techniques have been studied intensively during 
the past few years. Through the sustained efforts, a variety of state-of-the-art methods 
employing the query-by-example (QBE) paradigm have been well established. By this 
we mean that queries are images and the targets are also images. In this manner, vis-
ual similarity is computed between user-provided image and database images based 
on the low-level visual features such as color, texture, shape and spatial relationships. 
However, two important problems still remain. First, due to the limitation of object 
recognition and image understanding, semantics-based image segmentation algorithm 
 Automatic Image Annotation Using Maximum Entropy Model 35 
is unavailable, so segmented region may not correspond to users? query object. Sec-
ond, visual similarity is not semantic similarity which means that low-level features 
are easily extracted and measured, but from the users? point of view, they are non-
intuitive. It is not easy to use them to formulate the user?s needs. We encounter a so-
called semantic gap here. Typically the starting point of the retrieval process is the 
high-level query from users. So extracting image semantics based on the low-level 
visual features is an essential step. As we know, semantic information can be repre-
sented more accurately by using keywords than by using low-level visual features. 
Therefore, building relationship between associated text and low-level image features 
is considered to an effective solution to capture the image semantics. By means of this 
hidden relationship, images can be retrieved by using textual descriptions, which is 
also called query-by-keyword (QBK) paradigm. Furthermore, textual queries are a 
desirable choice for semantic image retrieval which can resort to the powerful text-
based retrieval techniques. The key to image retrieval using textual queries is image 
annotation. But most images are not annotated and manually annotating images is a 
time-consuming, error-prone and subjective process. So, automatic image annotation 
is the subject of much ongoing research. Its main goal is to assign descriptive words 
to whole images based on the low-level perceptual features, which has been recog-
nized as a promising technique for bridging the semantic gap between low-level im-
age features and high-level semantic concepts. 
Given a training set of images labeled with text (e.g. keywords, captions) that de-
scribe the image content, many statistical models have been proposed by research-
ers to construct the relation between keywords and image features. For example, co-
occurrence model, translation model and relevance-language model. By exploiting 
text and image feature co-occurrence statistics, these methods can extract hidden 
semantics from images, and have been proven successful in constructing a nice 
framework for the domain of automatic image annotation and retrieval.  
In this paper, we propose a novel approach for the task of automatic image anno-
tation using Maximum Entropy Model. Though Maximum Entropy method has 
been successfully applied to a wide range of application such as machine transla-
tion, it is not much used in computer vision domain, especially in image auto  
annotation.  
This paper is organized as follows: Section 2 presents related work. Section 3 de-
scribes the representation of labeled and unlabeled images, gives a brief introduc-
tion to Maximum Entropy Model and then details how to use it for automatically 
annotating unlabeled images. Section 4 demonstrates our experimental results. Sec-
tion 5 presents conclusions and a comment for future work. 
2   Related Work 
Recently, many statistical models have been proposed for automatic image annotation 
and retrieval. The work of associating keywords with low-level visual features can be 
addressed from two different perspectives.  
36 W. Li and M. Sun 
2.1   Annotation by Keyword Propagation  
This kind of approach usually formulates the process of automatic image annotation 
as one of supervised classification problems. With respect to this method, accurate 
annotation information is demanded. That is to say, given a set of training images 
labeled with semantic keywords, detailed labeling information should be provided. 
For example, from training samples, we can know which keyword corresponds to 
which image region or what kind of concept class describes a whole-image. So each 
or a set of annotated keyword can be considered as an independent concept class, 
followed by training each class model with manually labeled images, then the model 
is applied to classify each unlabeled image into a relevant concept class, and finally 
producing annotation by propagating the corresponding class words to unlabeled 
images.  
Wang and Li [8] introduced a 2-D multi- resolution HMM model to automate lin-
guistic indexing of images. Clusters of fixed-size blocks at multiple resolution and the 
relationships between these clusters is summarized both across and within the resolu-
tions. To annotate the unlabeled image, words of the highest likelihood is selected 
based on the comparison between feature vectors of new image and the trained con-
cept models. Chang et al[5] proposed content-based soft annotation (CBSA) for pro-
viding images with semantic labels using (BPM) Bayesian Point Machine. Starting 
with labeling a small set of training images, an ensemble of binary classifier for each 
keyword is then trained for predicting label membership for images. Each image is 
assigned one keyword vector, with each keyword in the vector assigned a confidence 
factor. In the process of annotation, words with high confidence are considered to be 
the most likely descriptive words for the new images. The main practical problem 
with this kind of approaches is that a large labeled training corpus is needed. More-
over, during the training and application stages, the training set is fixed and not in-
cremented. Thus if a new domain is introduced, new labeled examples must be pro-
vided to ensure the effectiveness of such classifiers. 
2.2   Annotation by Statistical Inference 
More recently, there have been some efforts to solve this problem in a more general 
way. The second approach takes a different strategy which focuses on discovering the 
statistical links between visual features and words using unsupervised learning meth-
ods. During training, a roughly labeled image datasets is provided where a set of se-
mantic labels is assigned to a whole image, but the word-to-region information is 
hidden in the space of image features and keywords. So an unsupervised learning 
algorithm is usually adopted to estimate the joint probability distribution of words and 
image features.  
Mori et al[4] were the earliest to model the statistics using a co-occurrence prob-
abilistic model, which predicate the correct probability of associating keywords by 
counting the co-occurrence of words with image regions generated using a fixed-size 
blocks. Blocks are vector quantized to form clusters which inherit the whole set of  
 
 Automatic Image Annotation Using Maximum Entropy Model 37 
keywords assigned to each image. Then clusters are in turn used to predict the key-
words for unlabeled images. The disadvantage is that the model is a little simple and 
the rough fixed-size blocks are unable to model objects effectively, leading to poor 
annotation accuracy. Instead of using fixed-size blocks, Barnard et al[1] performed 
Blobworld segmentation and Normalized cuts to produce semantic meaningful re-
gions. They constructed a hierarchical model via EM algorithm. This model combines 
both asymmetric clustering model which maps words and image regions into clusters 
and symmetric clustering model which models the joint distribution of words and 
regions. Duygulu et al[2] proposed a translation model to map keywords to individ-
ual image regions. First, image regions are created by using a segmentation algorithm. 
For each region, visual features are extracted and then blob-tokens are generated by 
clustering the features for each region across whole image datasets. Each image can 
be represented by a certain number of these blob-tokens. Their Translation Model 
uses machine translation model ?of IBM to annotate a test set of images based on a 
large number of annotated training images. Another approach using cross-media rele-
vance models (CMRM) was introduced by Jeon et al[3]. They assumed that this 
could be viewed as analogous to the cross-lingual retrieval problem and a set of key-
words{ }nwww ...,,, 21  is related to the set of blob-tokens{ }nbbb ...,,, 21 , rather 
than one-to-one correspondence between the blob-tokens and keywords. Here the 
joint distribution of blob-tokens and words was learned from a training set of anno-
tated images to perform both automatic image annotation and ranked retrieval. Jeon et 
al [9] introduced using Maximum Entropy to model the fixed-size block and key-
words, which gives us a good hint to implement it differently. Lavrenko et al[11] 
extended the cross-media relevance model using actual continuous-valued features 
extracted from image regions. This method avoids the clustering and constructing the 
discrete visual vocabulary stage. 
3   The Implementation of Automatic Annotation Model 
3.1   The Hierarchical Framework of Automatic Annotation and Retrieval 
The following Fig. 1 shows the framework for automatic image annotation and key-
word-based image retrieval. Given a training dataset of images labeled with key-
words. First, we segment a whole image into a collection of sub-images, followed by 
extracting a set of low-level visual features to form a feature vector to describe the 
visual content of each region. Second, a visual vocabulary of blob-tokens is generated 
by clustering all the regions across the whole dataset so that each image can be repre-
sented by a number of blob-tokens from a finite set of visual symbols. Third, both 
textual information and visual information is provided to train the Maximum Entropy 
model, and the learned model is then applied to automatically generate keywords to 
describe the semantic content of an unlabeled image based on the low-level features. 
Consequently, both the users? information needs and the semantic content of images 
can be represented by textual information, which can resort to the powerful text IR 
techniques to implement this cross-media retrieval, suggesting the importance of 
textual information in semantics-based image retrieval. 
38 W. Li and M. Sun 
 
Fig. 1. Hierarchical Framework of Automatic Annotation and Retrieval 
                 learning correlations between blob-tokens and textual annotations 
                 applying correlations to generate annotations for unlabeled images 
3.2   Image Representation and Pre-processing 
A central issue in content-based image annotation and retrieval is how to describe the 
visual information in a way compatible with human visual perception. But until now, 
no general framework is proposed. For different tasks and goals, different low-level 
features are used to describe and analyze the visual content of images. On the whole, 
there are two kinds of interesting open questions remain unresolved. First, what fea-
ture sets should be selected to be the most expressive for any image region. Second, 
how blob-tokens can be generated, that is to say, how can one create such a visual 
vocabulary of blob-tokens to represent each image in the collection using a number of 
symbols from this finite set? In our method, we carried out these following two steps: 
First, segment images into sub-images, Second, extract appropriate features for any 
sub-images, cluster similar regions by k-means and then use the centroid in each clus-
 Automatic Image Annotation Using Maximum Entropy Model 39 
ter as a blob-token. The first step can be employed by either using a segmentation 
algorithm to produce semantically meaningful units or partitioning the image into 
fixed-size rectangular grids. Both methods have pros and cons, a general purpose 
segmentation algorithm may produce semantic regions, but due to the limitation in 
computer vision and image processing, there are also the problems of erroneous and 
unreliable region segmentation. The advantage of regular grids is that is does not need 
to perform complex image segmentation and is easy to be conducted. However, due to 
rough fixed-size rectangular grids, the extracted blocks are unable to model objects 
effectively, leading to poor annotation accuracy in our experiment.  
                   
Fig. 2. Segmentation Results using Normalized cuts and JSEG 
In this paper, we segment images into a number of meaningful regions using Nor-
malized cuts [6] against using JSEG. Because the JSEG is only focusing on local 
features and their consistencies, but Ncuts aims at extracting the global impression of 
an image data. So Ncuts may get a better segmentation result than JSEG. Fig. 2 shows 
segmentation result using Normalized cuts and JSEG respectively, the left is the origi-
nal image, the mid and the right are the segmentation result using Ncuts and JSEG 
respectively. After segmentation, each image region is described by a feature vector 
formed by HSV histograms and Gabor filters. Similar regions will be grouped to-
gether based on k-means clustering to form the visual vocabulary of blob-tokens. Too 
much clusters may cause data sparseness and too few can not converge. Then each of 
the labeled and unlabeled images can be described by a number of blob-tokens, in-
stead of the continuous-valued feature vectors. So we can avoid the image data mod-
eling in a high-dimensional and complex feature space. 
3.3   The Annotation Strategy Based on Maximum Entropy 
Maximum Entropy Model is a general purpose machine learning and classification 
framework whose main goal is to account for the behavior of a discrete-valued ran-
dom process. Given a random process whose output value y may be influenced by 
some specific contextual information x, such a model is a method of estimating the 
conditional probability. 
?
=
=
k
j
yxfj
j
xZ
xyp
1
),(
)(
1)|( ?                                              (1) 
In the process of annotation, images are segmented using normalized cuts, every 
image region is represented by a feature vector consisting of HSV color histogram 
and the Gabor filters, and then a basic visual vocabulary containing 500 blob-tokens 
is generated by k-means clustering. Finally, each segmented region is assigned to the 
label of its closest blob-token. Thus the complex visual contents of images can be 
40 W. Li and M. Sun 
represented by a number of blob-tokens. Due to the imbalanced distribution of key-
words frequency and the data sparseness problem, the size of the pre-defined keyword 
vocabulary is reduced from 1728 to 121 keywords, by keeping only the keywords 
appearing more than 30 times in the training dataset. 
We use a series of feature function ( )ji wbf ,Label,FC  to model the co-occurrence 
statistics of blob-tokens ib  and keywords jw , where FC denote the context of feature 
constraints for each blob-token. The following example represents the co-occurrence 
of the blob-token 
?
b  and the keyword ?water? in an image I. 
( ) ( )
??
? ====
=
otherwise
truebFCandwaterwif
wbf iwjji 0
''1
,water ,FC w       (2) 
If blob-token ib  satisfies the context of feature constraints and keyword ?water? 
also occurs in image I. In other words, if the color and texture feature components are 
coordinated with the semantic label ?water?, and then the value of the feature function 
is 1, otherwise 0. 
The following Fig. 3 shows the annotation procedure that using MaxEnt captures 
the hidden relationship between blob-tokens and keywords from a roughly labeled 
training image sets. 
 
Fig. 3. Learning the statistics of blob-tokens and words 
In the recent past, many models for automatic image annotation are limited by the 
scope of the representation. In particular, they fail to exploit the context in the images 
and words. It is the context in which an image region is placed that gives it meaning-
ful interpretation. 
 
 Automatic Image Annotation Using Maximum Entropy Model 41 
In our annotation procedure, each annotated word is predicted independently by the 
Maximum Entropy Model, word correlations are not taken into consideration. How-
ever, correlations between annotated words are essentially important in predicting 
relevant text descriptions. For example, the words ?trees? and ?grass? are more likely 
to co-occur than the words ?trees? and ?computers?. In order to generate appropriate 
annotations, a simple language model is developed that takes the word-correlation 
information into account, and then the textual description is determined not only by 
the model linking keywords and blob-tokens but also by the word-to-word correla-
tion. We simply count the co-occurrence information between words in the pre-
defined textual set to produce a simple word correlation model to improve the annota-
tion accuracy. 
4   Experiments and Analysis 
We carried out experiments using a mid-sized image collection, comprising about 
5,000 images from Corel Stock Photo CDs, 4500 images for training and 500 for 
testing. The following table 1 shows the results of automatic image annotation using 
Maximum Entropy. 
Table 1. Automatic image annotation results 
Images Original Annotation Automatic Annotation 
 
sun city sky mountain Sun sky mountain 
clouds 
 
flowers tulips mountain sky Flowers sky trees grass 
 
tufa snow sky grass snow sky grass stone 
 
polar bear snow post bear snow sky rocks 
42 W. Li and M. Sun 
For our training datasets, the visual vocabulary and the pre-defined textual set con-
tain 500 blob-tokens and 121 keywords respectively, so the number of the training 
pairs ( )ji wb ,  is 60500. After the procedure of feature selection, only 9550 pairs left. 
For model parameters estimation, there are a few algorithms including Generalized 
Iterative Scaling and Improved Iterative Scaling which are widely used. Here we use 
Limited Memory Variable Metric method which has been proved effective for Maxi-
mum Entropy Model [10]. Finally, we can get the model linking blob-tokens and 
keywords, and then the trained model ( )xyp  is applied to predict textual annota-
tions { }nwww ,,, 21 K  given an unseen image formed by{ }mbbb ,,, 21 K . 
To further verify the feasibility and effectiveness of Maximum Entropy model, we 
have implemented the co-occurrence model as one of the baselines whose conditional 
probability ( )ij bwp  can be estimated as follows: 
( ) ( ) ( )
( ) ( )
( )( )
( )( ) i
ij
N
k
ik
ij
N
k
kkik
jjij
N
k
iki
iji
ij M
m
m
m
Nnnm
Nnnm
wpwbp
wpwbp
bwp ==?=
???
=== 111
    (3) 
Where ijm denote the co-occurrence of ib  and jw , jn denote the occurring num-
ber of  jw in the total N words. 
The following Fig. 4 shows the some of the retrieval results using the keyword  
?water? as a textual query.  
 
Fig. 4. Some of retrieved images using ?water? as a query 
The following Fig. 5 and Fig. 6 show the precision and recall of using a se of high-
frequency keywords as user queries. We implemented two statistical models to link 
blob-tokens and keywords.  
 Automatic Image Annotation Using Maximum Entropy Model 43 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
w
a
t
e
r
s
k
y
t
r
e
e
s
p
e
o
p
l
e
g
r
a
s
s
s
n
o
w
c
l
o
u
d
s
f
l
o
w
e
r
s
m
o
u
n
t
a
i
n
s
b
u
i
l
d
i
n
g
s
t
o
n
e
b
u
i
l
d
i
n
g
s
s
t
r
e
e
t
s
a
n
d
f
i
e
l
d
b
e
a
r
b
e
a
c
h
t
r
e
e
j
e
t
P
r
e
c
i
s
i
o
n
Maximum Entropy
Co-occurrence
 
Fig. 5. Precision of retrieval using some high-frequency keywords 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
w
a
t
e
r
s
k
y
t
r
e
e
s
p
e
o
p
l
e
g
r
a
s
s
s
n
o
w
c
l
o
u
d
s
f
l
o
w
e
r
s
m
o
u
n
t
a
i
n
s
b
u
i
l
d
i
n
g
s
t
o
n
e
b
u
i
l
d
i
n
g
s
s
t
r
e
e
t
s
a
n
d
f
i
e
l
d
b
e
a
r
b
e
a
c
h
t
r
e
e
j
e
t
R
e
c
a
l
l
Maximum Entropy
Co-Occurrence
 
Fig. 6. Recall of retrieval using some high-frequency keywords 
The annotation accuracy is evaluated by using precision and recall indirectly. After 
posing a keyword query for images, the measure of precision and recall can be de-
fined as follows: 
BA
Aprecision
+
=              
CA
A
recall
+
=                              (4) 
Where A denote the number of relevant images retrieved, B denote the number of 
irrelevant images retrieved, C denote the number of relevant images not retrieved in 
the image datasets, and images whose labels containing the query keyword is consid-
ered relevant, otherwise irrelevant.  
Table 2. Experimental results with average precision and mean 
Method Mean precision Mean recall 
Co-occurrence 0.11 0.18 
Maximum Entropy 0.17 0.25 
44 W. Li and M. Sun 
The above experimental results in table 2 show that our method outperforms the 
Co-occurrence model [4] in the average precision and recall. Since our model uses the 
blob-tokens to represent the contents of the image regions and converts the task of 
automatic image annotation to a process of translating information from visual lan-
guage (blob-tokens) to textual language (keywords). So Maximum Entropy Model is 
a natural and effective choice for our task, which has been successfully applied to the 
dyadic data in which observations are made from two finite sets of objects. But disad-
vantages also exist. There are two fold problems to be considered. First, since Maxi-
mum Entropy is constrained by the equation ( ) ( )fpfp ~= , which assumes that the 
expected value of output of the stochastic model should be the same as the expected 
value of the training sample. However, due to the unbalanced distribution of key-
words frequency in the training subset of Corel data, this assumption will lead to an 
undesirable problem that common words with high frequency are usually associated 
with too many irrelevant blob-tokens, whereas uncommon words with low frequency 
have little change to be selected as annotations for any image regions, consider word 
?sun? and ?apple? , since both words may be related to regions with ?red? color and 
?round? shape, but it is difficult to make a decision between the word ?sun? and ?ap-
ple?. However, since ?sun? is a common word as compared to ?apple? in the lexical 
set, the word ?sun? will definitely used as the annotation for these kind of regions. To 
address this kind of problems, our future work will mainly focus on the more sophis-
ticated language model to improve the statistics between image features and key-
words. Second, the effects of segmentation may also affect the annotation perform-
ance. As we know, semantic image segmentation algorithm is a challenging and com-
plex problem, current segmentation algorithm based on the low-level visual features 
may break up the objects in the images, that is to say, segmented regions do not defi-
nitely correspond to semantic objects or semantic concepts, which may cause the 
Maximum Entropy Model to derive a wrong decision given an unseen image. 
5   Conclusion and Future Work 
In this paper, we propose a novel approach for automatic image annotation and re-
trieval using Maximum Entropy Model. Compared to other traditional classical meth-
ods, the proposed model gets better annotation and retrieval results. But three main 
challenges are still remain: 
1) Semantically meaningful segmentation algorithm is still not available, so the 
segmented region may not correspond to a semantic object and region features 
are insufficient to describe the image semantics. 
2) The basic visual vocabulary construction using k-means is only based on the 
visual features, which may lead to the fact that two different semantic objects 
with similar visual features fall into the same blob-token. This may degrade the 
annotation quality. 
3) Our annotation task mainly depend on the trained model linking image features 
and keywords, the spatial context information of image regions and the word cor-
relations are not fully taken into consideration. 
In the future, more work should be done on image segmentation techniques, clus-
tering algorithms, appropriate feature extraction and contextual information between 
regions and words to improve the annotation accuracy and retrieval performance. 
 Automatic Image Annotation Using Maximum Entropy Model 45 
Acknowledgements 
We would like to express our deepest gratitude to Kobus Barnard and J.Wang for 
making their image datasets available. This research is supported by the National 
Natural Science Foundation of China under grant number 60321002 and the National 
863 Project of China under grant number 2001AA114210-03, and the ALVIS Project 
co-sponsored by EU PF6 and NSFC. 
References 
1. K. Barnard, P. Dyugulu, N. de Freitas, D. Forsyth, D. Blei, and M. I. Jordan. Matching 
words and pictures. Journal of Machine Learning Research, 3: 1107-1135, 2003. 
2. P. Duygulu, K. Barnard, N. de Freitas, and D. Forsyth. Ojbect recognition as machine 
translation: Learning a lexicon fro a fixed image vocabulary. In Seventh European Conf. 
on Computer Vision, 97-112, 2002. 
3. J. Jeon, V. Lavrenko and R. Manmatha. Automatic image annotation and retrieval using 
cross-media relevance models. In Proceedings of the 26th intl. SIGIR Conf, 119-126, 2003. 
4. Y. Mori, H. Takahashi, and R. Oka, Image-to-word transformation based on dividing and 
vector quantizing images with words. First International Workshop on Multimedia Intelli-
gent Storage and Retrieval Management, 1999. 
5. Edward Chang, Kingshy Goh, Gerard Sychay and Gang Wu. CBSA: Content-based soft 
annotation for multimodal image retrieval using bayes point machines. IEEE Transactions 
on Circuts and Systems for Video Technology Special Issue on Conceptual and Dynamical 
Aspects of Multimedia Content Descriptions, 13(1): 26-38, 2003. 
6. J. shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions On Pat-
tern Analysis and Machine Intelligence, 22(8): 888-905, 2000. 
7. A. Berger, S. Pietra and V. Pietra. A maximum entropy approach to natural language proc-
essing. In Computational Linguistics, 39-71, 1996. 
8. J. Li and J. A. Wang. Automatic linguistic indexing of pictures by a statistical modeling 
approach. IEEE Transactions on PAMI, 25(10): 175-1088, 2003. 
9. Jiwoon Jeon, R. Manmatha. Using maximum entropy for automatic image annotation. In 
proceedings of third international conference on image and video retrieval, 24-31, 2004. 
10. Robert Malouf. A comparison of algorithms for maximum entropy parameter estimation. 
In Proceedings of the 6th Workshop on Computational Language Learning, 2003. 
11. V. Lavrenko, R. Manmatha and J. Jeon. A model for learning  the semantics of pictures. In 
Proceedings of the 16th Annual Conference on Neural Information Processing Systems, 
2004. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 245 ? 256, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Method of Recognizing Entity and Relation 
Xinghua Fan1, 2 and Maosong Sun1 
1
 State Key Laboratory of Intelligent Technology and Systems,  
Tsinghua University, Beijing 100084, China 
fanxh@tsinghua.org.cn, sms@mail.tsinghua.edu.cn 
2
 State Intellectual Property Office of P.R. China, Beijing, 100088, China 
Abstract. The entity and relation recognition, i.e. (1) assigning semantic classes 
to entities in a sentence, and (2) determining the relations held between entities, 
is an important task in areas such as information extraction. Subtasks (1) and 
(2) are typically carried out sequentially, but this approach is problematic: the 
errors made in subtask (1) are propagated to subtask (2) with an accumulative 
effect; and, the information available only in subtask (2) cannot be used in sub-
task (1). To address this problem, we propose a method that allows subtasks (1) 
and (2) to be associated more closely with each other. The process is performed 
in three stages: firstly, employing two classifiers to do subtasks (1) and (2) in-
dependently; secondly, recognizing an entity by taking all the entities and rela-
tions into account, using a model called the Entity Relation Propagation Dia-
gram; thirdly, recognizing a relation based on the results of the preceding stage. 
The experiments show that the proposed method can improve the entity and re-
lation recognition in some degree. 
1   Introduction 
The entity and relation recognition, i.e. assigning semantic classes (e.g., person, or-
ganization and location) to entities in a sentence and determining the relations (e.g., 
born-in and employee-of) that hold between entities, is an important task in areas such 
as information extraction (IE) [1] [2] [3] [4], question answering (QA) [5] and story 
comprehension [6]. In a QA system, many questions concern the specific entities in 
some relations. For example, the question that ?Where was Poe born?? in TREC-9 
asks for the location entity in which Poe was born. In a typical IE task in constructing 
a job database from unstructured texts, the system are required to extract many mean-
ingful entities like titles and salary from the texts and to determine how these entities 
are associated with job positions. 
The task of recognizing entity and relation is usually treated as two separate sub-
tasks carried out sequentially: (1) to recognize entities using an entity recognizer, and 
(2) to determine the relations held between them. This approach has two shortcom-
ings. Firstly, the errors made in subtask (1) will be propagated to subtask (2) with an 
accumulative effect, leading to a loss in performance of relation recognition. For 
example, if ?Boston? is mislabeled as a person, it will never have chance to be classi-
fied as the location of Poe?s birthplace. Secondly, the information available only in 
246 X. Fan and M. Sun 
subtask (2) cannot be used for subtask (1). For example, if we feel difficult to  
determine whether the entity X is a person or not, but we can determine that there 
exists a relation born-in between X and China easily, it is obvious that we can claim 
that X must be a person. 
To address the problems described above, this paper presents a novel approach 
which allows subtasks (1) and (2) to be linked more closely together. The process is 
separated into three stages. Firstly, employing two classifiers to perform subtasks (1) 
and (2) independently. Secondly, recognizing an entity by taking all the entities and 
relations into account using a particularly designed model called the Entity Relation 
Propagation Diagram. And, thirdly, recognizing a relation based on the results of the 
preceding step. 
The rest of the paper is organized as follows. Section 2 defines the problem of en-
tity and relation recognition in a formal way. Section 3 describes the proposed method 
of recognizing entity and relation. Section 4 gives the experimental results. Section 5 
is the related work and comparison. Section 6 is conclusions. 
2   The Problem of Entity and Relation Recognition 
Conceptually, the entities and relations in a sentence can be viewed, while taking 
account of the mutual dependencies among them, as a labeled graph in Fig. 1. 
 
Fig. 1. Concept view of the entities and relations among them 
In Fig.1, a node represents an entity and a link denotes the relation held between 
two entities. The arrowhead of a link represents the direction of the relation. Each 
entity or relation has several attributes, which are structured as a list of the node or the 
edge. These attributes can be classified into two classes. Some of them that are easy to 
acquire, such as words in an entity and parts of speech of words in a context, are 
called local attributes; the others that are difficult to acquire, such as semantic classes 
of phrases and relations among them, are called decision attributes. The issue of entity 
and relation recognition is to determine a unique value for each decision attribute of 
all entities and relations, by considering the local attributes of them. To describe the 
problem in a formal way, we first give some basic definitions as follows. 
 A Method of Recognizing Entity and Relation 247 
Definition 1 (Entity). An entity can be a single word or a set of consecutive words 
with a predefined boundary. A sentence is a linked list, which consists of words and 
entities. Entities in a sentence are denoted as E1, E2 ? according to their order, with 
values ranging over a set of entity class CE. For example, the sentence in Fig. 2 has 
three entities: E1= ?Dole?, E2= ?Elizabeth? and E3= ?Salisbury, N.C.?. Note that it is 
not easy to determine the entity boundaries [7]. Here we assume that it has been 
solved and its output serves as the input to our model.  
 
Fig. 2. A sentence that have three entities 
Definition 2 (Relation). In this paper, we only consider the relation between two 
entities. An entity pair (Ei, Ej) represents a relation Rij from entity Ei and Ej, where Ei 
is the first argument and Ej is the second argument. Relation Rij takes its value that 
ranges over a set of relation class CR. Note that (Ei, Ej) is an ordered pair, and there 
exist two relations Rij =(Ei, Ej) and Rji =(Ej, Ei) between entities Ei and Ej. 
Definition 3 (Class). The class of an entity or relation is its decision attribute, which 
is one of the predefined class set and is unknown before being recognized. We denote 
the sets of predefined entity class and relation class as CE and CR respectively. CE has 
one special element other-ent, which represents any unlisted entity class. For algo-
rithmic reasons, we suppose all elements in CE are mutually exclusive. Similarly, CR 
also has one special element other-rel, which represents that the two involved entities 
are irrelevant or their relation class is undefined. For algorithmic reasons, we suppose 
all elements in CR are mutually exclusive. In fact, because the class of an entity or a 
relation is only a label that we want to predict, if an entity or a relation have more 
than one labels simultaneously, to satisfy the constraint that all elements in CE or CR 
are mutually exclusive, we can separate it into several cases and construct several 
predefined entity class sets and relation class sets.  
The classes of entities and relations in a sentence must satisfy some constraints. For 
example, if the class of entity E1, which is the first argument of relation R12, is a loca-
tion, then the class of relation R12 cannot be born-in because the class of the first ar-
gument in relation R12 has to be a person. 
Definition 4 (Constraint). A constraint is a 5-tuple ),,, R,( R21 ????? . The symbols 
are defined as follows. RCR ?  represents the class of relation R. E21 C, ??? repre-
sents the classes of the first argument Ei and the second argument Ej in the relation R 
respectively. ]1,0[?R?  is a real number that represents a joint conditional probability 
distribution }R|,Pr{ 21R ??? = . ]1,0[???  is a real number that represents a condi-
tional probability distribution },|RPr{ 21 ???? = . Note that R?  and ??  need not to 
be specified manually and can be learned from an annotated training dataset easily. 
248 X. Fan and M. Sun 
Definition 5 (Observation). We denote the observations of an entity and a relation in 
a sentence as OE and OR respectively. OE or OR represent all the ?known? local attrib-
utes of an entity or a relation, e.g., the spelling of a word, parts of speech, and  
semantic related attributes acquired from external resources such as WordNet. The  
observations OE and OR can be viewed as a random event, and 1}Pr{O}Pr{O RE ?=  
because OE and OR in a sentence are known.        
Based on the above definitions, the issue of entity and relation recognition can be 
described in a formal way as follows. Suppose in a sentence, the set of entity is {E1, 
E2 ? En}, the set of relation is {R12, R21, R13, R31, ?, R1n, Rn1, ?, Rn-1,n, Rn,n-1}, the 
predefined sets of entity class and relation class are CE ={e1, e2, ? em} and CR ={ r1, 
r2, ? rk} respectively, the observation of entity Ei is EiO , and the observation of rela-
tion Rij is RijO . n, m and k represent the number of entity, the number of the prede-
fined entity class and the number of the predefined relation class respectively. The 
problem is to search the most probable class assignment for each entity and each 
relation of interest, given the observations of all entities and relations. In other words, 
the problem is to solve the following two equations, using two kinds of constraint 
knowledge ???  ,R  and the interaction among entities and relations.  
} O ,O ,,O ,O , ,O ,O ,O, ,O ,O|ePr{E max arge R 1-nn,R n1,-nRn1R1nR21R12EnE2E1did LLL==  (1) 
} O ,O ,,O ,O , ,O ,O ,O, ,O ,O|rPr{R max argr R 1-nn,R n1,-nRn1R1nR21R12EnE2E1dijd LLL==  (2) 
In (1), d =1, 2, ?, m, and in (2), d=1, 2, ?, k. 
3   The Proposed Method 
Because the class assignment of a single entity or relation depends not only on local 
attributes itself, but also on those of all other entities and relations, the equations (1) 
and equation (2) cannot be solved directly. To simplify the problem, we present the 
following method consisting of three stages. Firstly, employ two classifiers to perform 
entity recognition and relation recognition independently. Their outputs are the condi-
tional probability distributions Pr{E| OE} and Pr{R|OR}, given the corresponding 
observations. Secondly, recognize an entity by taking account of all entities and rela-
tions, as computed in the previous step. This is achieved by using the model Entity 
Relation Propagation Diagram (ERPD). And, recognize a relation based on the results 
of the second step at last.       
In this paper, we concentrate on the processes at the second and the third stages, as-
suming that the process at the first stage is solved and its output are given to us as 
input. At the second stage, the aim of introducing ERPD is to estimate the conditional 
probability distribution 
 ERPD}|Pr{E  given the constraint R?  in Definition 5 and 
the sets {  }O|Pr{E Eii } and { }O|Pr{R R ijij } (i, j=1,?,n), as computed at the first 
stage. For the readability, suppose 
 ERPD}|Pr{E is given, the entity recognition 
equation (1) becomes the equation (3). 
 A Method of Recognizing Entity and Relation 249 
??
???
?=
>=
=
?
?
RV     ERPD}|ePr{E max arg
RV     }O|ePr{E max arg
e
dd
E
dd
i
i
i
 (3) 
where ?  is a threshold determined by the experiment. RV? [0, 1] is a real number, 
called the reliable value, representing the belief degree of the output of the entity 
recognizer at the first stage. Suppose the maximum value of the conditional probabil-
ity distribution  }O|Pr{E E is Vm and the second value is Vs, RV is defined as: 
sm
sm
VV
VVRV
+
?
=  (4) 
The reason of introducing RV is due to a fact that only for ambiguous entities, it is 
effective by taking the classes of all entities in a sentence into account. ?Reliable 
Value? measures whether an entity is ambiguous. 
At the third stage, the basic idea of recognizing a relation is to search the probable 
relation given its observation, under a condition of satisfying the constraints imposed 
by the results of entity recognition at the second stage. The relation recognition equa-
tion (2) becomes the equation (5). 
RR
k
k W}O|rPr{R max argr ?==  
??
???
=
>
=
 0}?,?|Pr{r if  0
0}?,?|Pr{r if   1
W
21
21
R  
(5) 
where 21,??  is the results of entity recognition at the second stage, },|Pr{r 21 ??  is 
constraint knowledge ??  in Definition 4, and WR is the weight of the constraint 
knowledge. 
In the following sections, we present ERPD and two algorithms to estimate the 
conditional probability distribution
 ERPD}|Pr{E . 
3.1   The Entity Relation Propagation Diagram 
To represent the mutual dependencies among entities and relations, a model named 
the Entity Relation Propagation Diagram that can deal with cycles, similar to the 
Causality Diagram [8][9] for the complex system fault diagnosis, is developed for 
entity and relation recognition. 
The classes of any two entities are dependent on each other through the relations 
between them, while taking account of the relations in between. For example, the 
class of entity Ei in Fig. 3 (a) depends on the classes of relations Rji between entities 
Ei and Ej, and the classes of relations Rij and Rji depend on the classes of entities  
Ei and Ej. This means that we can predict the class of a target entity according to the 
class of its neighboring entity, making use of the relations between them. We  
further introduce the relation reaction intensity to describe the prediction ability of 
this kind. 
250 X. Fan and M. Sun 
 
Fig. 3. Illustration of relation reaction 
Definition 6 (Relation Reaction Intensity). We denote the relation reaction intensity 
from entity Ei to entity Ej as Pij, which represents the ability that we guess the class of 
Ej if we know the class of its neighboring entity Ei and the relation Rij between them. 
The relation reaction intensity could be modeled using a condition probability distri-
bution Pij=Pr {Ej |Ei}. 
The element klijp of Pij represents the conditional probability Pr {Ej=el |Ei=ek}:  
?
=
=
====
====
N
1t ki
tijljkitij
kilj
kl
ij }ePr{E
}rR| eE ,e}Pr{ErPr{R}eE|eEPr{p  
according to Definition 5:  
}O|rPr{R}rPr{R Rijtijtij === , }O|ePr{E}ePr{E Eikiki ===  
Then, we have: 
 }O|ePr{E
}rR| eE ,e}Pr{EO|rPr{R
 
N
1t
E
iki
tijljki
R
ijtijkl
ij ?
=
=
====
=p  (6) 
where Rt Cr ? , N is the number of relations in relation class set. In equation (6), 
}rR| eE ,ePr{E tijljki === represents the constraint knowledge R?  among entities 
and relations. }O|rPr{R Rijtij =  and }O|ePr{E Eiki =  represent the outputs at the  
first stage.  
Definition 7 (Observation Reaction Intensity). We denote the observation reaction 
intensity as the conditional probability distribution }O|Pr{E E  of an entity class, 
given the observation, which is the output at the first stage. 
The Entity Relation Propagation Diagram (ERPD). is a directed diagram that 
allows cycles. As illustrated in Fig. 4, the symbols used in the ERPD are defined as 
follows. A circle node represents an event variable that can be any one from a set of 
mutually exclusive events, which all together cover the whole sample space. Here, an  
event variable represents an entity, an event represents a predefined entity class, and 
the whole sample space represents the set of predefined entity classes. Box node 
represents a basic event which is one of the independent sources of the associated 
event variable. Here, a basic event represents the observation of an entity. Directed 
arc represents a linkage event variable that may or may not enable an input event to 
cause the corresponding output event. The linkage event variable from an event  
 A Method of Recognizing Entity and Relation 251 
variable to another event variable represents the relation reaction intensity in Defini-
tion 6. And, the linkage event variable from a basic event to the corresponding event 
variable represents the observation reaction intensity in Definition 7. All arcs pointing 
to a node are in a logical OR relationship. 
 
Fig. 4. Illustration of the Entity Relation Propagation Diagram 
Now, we present two algorithms to compute the conditional probability distribu-
tion
 ERPD}|Pr{E , one is based on the entity relation propagation tree, and the other 
is the directed iteration algorithm on ERPD. 
3.2   The Entity Relation Propagation Tree 
The Entity Relation Propagation Tree (ERPT). is a tree decomposed from an 
ERPD, which represents the relation reaction propagation from all basic events to 
each event variable logically. Each event variable in the ERPD corresponds to an 
ERPT. For example, the ERPT of X1 in Fig. 4 is illustrated in Fig. 5. The symbols 
used in the ERPT are defined as follows. The root of the tree, denoted as Circle, is an 
event variable corresponding to the event variable in the ERPD. A leaf of the tree, 
denoted as Box, is a basic event corresponding to the basic event in the ERPD. The 
middle node of the tree, denoted as Diamond, is a logical OR gate variable, which is 
made from an event variable that has been expanded in the ERPD, and, the label in 
Diamond corresponds to the label of the expanded event variable. The directed arc of 
the tree corresponds to the linkage event variable in the ERPD. All arcs pointing to a 
node are in a logical OR relationship. The relation between the directed arc and the 
node linked to it is in logical AND relationship. 
To decompose an ERPD into entity relation propagation trees, firstly we decom-
pose the ERPD into mini node trees. Each event variables in the ERPD corresponds to 
a mini node tree, in which the root of the mini tree is the event variable in concern at 
present, and the leaves are composed of all neighboring basic events and event vari-
ables that are connected to the linkage event variables pointing to the top event vari-
ables. Secondly, expand a mini node tree into an entity relation propagation tree, i.e., 
the neighboring event variables in the mini node tree are replaced with their corre-
sponding mini trees. During expanding a node event variable, when there are loops, 
Rule BreakLoop is applied to break down the loops. 
252 X. Fan and M. Sun 
 
Fig. 5. Illustration of the entity relation propagation tree 
Rule BreakLoop. An event variable cannot propagate the relation reaction to itself. 
Rule 1 is derived from a law commonsense - one can attest that he is sinless. When such 
a loop is encountered, the descendant event variable, which is same as the head event 
variable of the loop, is treated as a null event variable, together with its connected link-
age event variable to be deleted.    
Compute the Conditional Probability Distribution in an ERPT. After an ERPD is 
decomposed into entity relation propagation trees, the conditional probability distribu-
tion  ERPD}|Pr{E becomes  ERPT}|Pr{E . When an event variable Xi has more than 
one input, these inputs will be in logic OR relationship, as defined in the ERPD. Since 
these inputs are independent, there exists such a case that one input causes Xi to be an 
instance kiX  while another input causes Xi to be an instance
l
iX , this would be impos-
sible because kiX  and 
l
iX  are exclusive. In the real world, the mechanism, in which 
iX  can response to more than one independent input properly, is very complicated 
and may vary from one case to another. To avoid this difficulty, a basic assumption is 
introduced.           
Assumption. When there is more than one input to Xi, each input will contribute a 
possibility to Xi. For each input, its contribution to this possibility equals to the prob-
ability that it causes Xi directly, as if the other inputs do not exist. The final possibility 
that Xi occurs is the sum of the possibilities from all inputs. 
Suppose an event variable X has m inputs, and the probability distributions of all 
linkage event variables, linked basic events or event variables are Pi and Pr {Xi} re-
spectively, i=1,2?m. Based on the above assumption, the formula for computing the 
probability distribution of X can be derived as: 
)
}Pr{X
}Pr{X
PNorm(
}Pr{X
}Pr{X
m
1i n
i
1
i
i
n
1
?
= ??
?
?
?
??
?
?
?
?=
??
?
?
?
??
?
?
?
MM  (7) 
 A Method of Recognizing Entity and Relation 253 
where, Norm () is a function that normalizes the vector in {}, and n is the state  
number of X. 
So, the probability distribution  ERPT}|Pr{E of the variable X in the correspond-
ing ERPT can be computed in the following steps. Firstly, to find the middle node 
sequence in the corresponding ERPT in the depth-first search; secondly, according to 
the sequence, for each middle node, equation (7) is applied to compute its probability 
distribution. In this procedure, the previous results can be used for the latter  
computation. 
3.3   The Directed Iteration Algorithm on ERPD 
The idea is to compute the probability distribution of the event variable on the ERPD 
directly, without decomposing the ERPD to some ERPTs. The aim is to avoid the 
computational complexity of using ERPT. This is achieved by adopting an iteration 
strategy, which is the same as that used in the loopy belief network [10]. 
The Directed Iteration Algorithm. is as follows: Firstly, only take the basic event as 
input, and initialize each event variable according to formula (7), i.e., assigning an 
initialized probability distribution to each event variable. Secondly, take the basic 
event and the probability distributions of all neighboring nodes computed in the pre-
vious step as input, and iterate to update the probability distributions of all nodes in 
ERPD in parallel according to formula (7). Thirdly, if none of the probability distribu-
tion of all nodes in ERPD in successive iterations changes larger than a small thresh-
old, the iteration is said to converge and then stops. 
4   Experiments 
Dataset. The dataset in our experiments is the same as the Roth?s dataset ?all? [11], 
which consists of 245 sentences that have the relation kill, 179 sentences that have the 
relation born-in and 502 sentences that have no relations. The predefined entity 
classes are other-ent, person and location, and the predefined relation classes are 
other-rel, kill and born-in. In fact, we use the results at the first stage in our method as 
the input, which are provided by W. Yih. 
Experiment Design. We compare five approaches in the experiments: Basic, Omnis-
cient, ERPD, ERPD* and BN. The Basic approach, which is a baseline, tests the per-
formance of the two classifiers at the first stage, which are learned from their local 
attributes independently. The Omniscient approach is similar to Basic, the only defer-
ence is that the classes of entities are exposed to relation classifier and vice versa. 
Note that it is certainly impossible to know the true classes of an entity and a relation 
in advance. The BN is the method based on the belief network, -- we follow the BN 
method according to the description in [11]. The ERPD is the proposed method based 
on ERPT, and the ERPD* is the proposed method based on the directed iteration 
algorithm. The threshold of RV is 0.4. 
Results. The experimental results are shown in Table 1. It can be seen from the table 
that 1) it is very difficult to improve the entity recognition because BN and Omnis-
cient almost do not improve the performance of Basic; 2) the proposed method can 
254 X. Fan and M. Sun 
improve the precision, which is thought of being more important than the recall for 
the task of recognizing entity; 3) the relation recognition can be improved if we can 
improve the entity recognition, as indicated by the comparisons of Basic, ERPD and 
Omniscient; 4) the proposed method can improve the relation recognition, and it per-
formance is almost equal to that of BN; 5) the performance of ERPD and ERPD* is 
almost equal, so the directly iteration algorithm is effective. 
Table 1. Experimental results 
 
5   Related Work and Comparison  
Targeting at the problems mentioned above, a method based on the belief network has 
been presented in [11], in which two subtasks are carried out simultaneously. Its pro-
cedure is as follows: firstly, two classifiers are trained for recognizing entities and 
relations independently and their outputs are treated as the conditional probability 
distributions for each entity and relation, given the observed data; secondly, this in-
formation together with the constraint knowledge among relations and entities are 
represented in a belief network [12] and are used to make global inferences for all 
entities and relations of interest. This method is denoted BN in our experiments.  
Although BN can block the error propagation from the entity recognizer to the rela-
tion classifier as well as improve the relation recognition, it cannot make use of the 
information, which is only available in relation recognition, to help entity recognition. 
Experiments show that BN cannot improve entity recognition. 
Comparing to BN, the proposed method in this paper can overcome the two short-
comings of it. Experiments show that it can not only improve the relation recognition, 
but also improve the precision of entity recognition. Moreover, the model ERPD 
could be more expressive enough than the belief network for the task of recognizing 
 A Method of Recognizing Entity and Relation 255 
entity and relation. It can represent the mutually dependences between entities and 
relations by introducing relation reaction intensity, and can deal with a loop without 
the limitation of directed acyclic diagram (DAG) in the belief network. At the same 
time, the proposed method can merge two kinds of constraint knowledge (i.e. 
???  and R  in Definition 4), but the method based on belief network can only use ?? . 
Finally, the proposed method has a high computation efficiency while using the di-
rected iteration algorithm. 
6   Conclusions 
The subtasks of entity recognition and relation recognition are typically carried out 
sequentially. This paper proposed an integrated approach that allows the two subtasks 
to be performed in a much closer way. Experimental results show that this method can 
improve the entity and relation recognition in some degree. 
In addition,  the Entity Relation Propagation Diagram (ERPD) is used to figure out 
the dependencies among entities and relations. It can also merge some constraint 
knowledge. Regarding to ERPD, two algorithms are further designed, one is based on 
the entity relation propagation tree, the other is the directed iteration algorithm on 
ERPD. The latter can be regarded as an approximation of the former with a higher 
computational efficiency. 
Acknowledgements 
We would like to express our deepest gratitude to Roth D. and Yih W. for making 
their dataset available for us. The research is supported in part by the National 863 
Project of China under grant number 2001AA114210-03, the National Natural Sci-
ence Foundation of China under grant number 60321002, and the Tsinghua-ALVIS 
Project co-sponsored by the National Natural Science Foundation of China under 
grant number 60520130299 and EU FP6. 
References 
1. Chinchor, N. MUC-7 Information Extraction Task Definition. In Proceeding of the Sev-
enth Message Understanding Conference (MUC-7), Appendices, 1998. 
2. Califf, M. and Mooney, R. Relational Learning of Pattern-match Rules for Information 
Extraction. In Proceedings of the Sixteenth National Conference on Artificial Intelligence 
and Eleventh Conference on Innovative Applications of Artificial Intelligence, 328-334, 
Orlando, Florida, USA, AAAI Press, 1999. 
3. Freitag, D. Machine Learning for Information Extraction in Informal Domains. Machine 
learning, 39(2/3): 169-202, 2000. 
4. Roth, D. and Yih, W. Relational Learning via Prepositional Algorithms: An Information 
Extraction Case Study. In Proceedings of the Seventeenth International Joint Conference on 
Artificial Intelligence, 1257-1263, Seattle, Washington, USA, Morgan Kaufmann, 2001. 
5. Voorhees, E. Overview of the Trec-9 Question Answering Track. In The Ninth Text Re-
trieval Conference (TREC-9), 71-80, 2000. 
256 X. Fan and M. Sun 
6. Hirschman, L., Light, M., Breck, E. and Burger, J. Deep Read: A Reading Comprehension 
System. In Proceedings of the 37th Annual Meeting of Association for Computational Lin-
guistics, 1999. 
7. Abney, S.P. Parsing by Chunks. In S. P. Abney, R. C. Berwick, and C. Tenny, editors, 
Principle-based parsing: Computation and Psycholinguistics, 257-278. Kluwer, Dordrecht, 
1991. 
8. Xinghua Fan. Causality Diagram Theory Research and Applying it to Fault Diagnosis of 
Complexity System, Ph.D. Dissertation of Chongqing University, P.R. China,  2002. 
9. Xinghua Fan, Zhang Qin, Sun Maosong, Huang Xiyue. Reasoning Algorithm in Multi-
Valued Causality Diagram, Chinese Journal of Computers, 26(3), 310-322, 2003. 
10. Murphy, K., Weiss, Y., and Jordan, M. Loopy Belief Propagation for Approximate Infer-
ence: An empirical study. In Proceeding of Uncertainty in AI, 467-475, 1999. 
11. Roth, D. and Yih, W. Probability Reasoning for Entity & Relation Recognition. In Pro-
ceedings of 20th International Conference on Computational Linguistics (COLING-02), 
835-841, 2002. 
12. Pearl, J. Probability Reasoning in Intelligence Systems. Morgan Kaufmann, 1988. 
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 302 ? 313, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Classifying Chinese Texts in Two Steps 
Xinghua Fan1, 2, 3, Maosong Sun1, Key-sun Choi3, and Qin Zhang2 
1
 State Key Laboratory of Intelligent Technology and Systems, Tsinghua University,  
Beijing 100084, China 
fanxh@tsinghua.org.cn, sms@tsinghua.edu.cn 
2
 State Intellectual Property Office of P.R. China, Beijing, 100088, China 
zhangqin@sipo.gov.cn 
3
 Computer Science Division, Korterm, KAIST, 373-1 Guseong-dong Yuseong-gu, 
Daejeon 305-701, Korea 
kschoi@cs.kaist.ac.kr 
Abstract. This paper  proposes a two-step method for Chinese text categoriza-
tion (TC). In the first step, a Na?ve Bayesian classifier is used to fix the fuzzy 
area between two categories, and, in the second step, the classifier with more 
subtle and powerful features is used to deal with documents in the fuzzy area, 
which are thought of being unreliable in the first step. The preliminary experi-
ment validated the soundness of this method. Then, the method is extended 
from two-class TC to multi-class TC. In this two-step framework, we try to fur-
ther improve the classifier by taking the dependences among features into con-
sideration in the second step, resulting in a Causality Na?ve Bayesian Classifier. 
1   Introduction 
Text categorization (TC) is a task of assigning one or multiple predefined category 
labels to natural language texts. To deal with this sophisticated task, a variety of sta-
tistical classification methods and machine learning techniques have been exploited 
intensively[1], including the Na?ve Bayesian (NB) classifier [2], the Vector Space 
Model (VSM)-based classifier [3], the example-based classifier [4], and the Support 
Vector Machine [5]. 
Text  filtering is a basic type of text categorization (two-class TC). It can find 
many real-life applications [6], a typical one is the ill information filtering, such as 
erotic information and garbage information filtering on the web, in e-mails and in 
short messages of mobile phone. It is obvious that this sort of information should be 
carefully controlled. On the other hand, the filtering performance using the existing 
methodologies is still not satisfactory in general. The reason lies in that there exist a 
number of documents with high degree of ambiguity, from the TC point of view, in a 
document collection, that is, there is a fuzzy area across the border of two classes (for 
the sake of expression, we call the class consisting of the ill information-related texts, 
or, the negative samples, the category of TARGET, and, the class consisting of the ill 
information-not-related texts, or, the positive samples, the category of Non-
TARGET). Some documents in one category may have great similarities with some 
other documents in the other category, for example, a lot of words concerning love 
 Classifying Chinese Texts in Two Steps 303 
story and sex are likely appear in both negative samples and positive samples if the 
filtering target is erotic information. We observe that most of the classification errors 
come from the documents falling into the fuzzy area between two categories. 
The idea of this paper is inspired by the fuzzy area between categories.  A two-step 
TC method is thus proposed: in the first step, a classifier is used to fix the fuzzy area 
between categories; in the second step, a classifier (probably the same as that in the 
first step) with more subtle and powerful features is used to deal with documents in 
the fuzzy area which are thought of being unreliable in the first step. Experimental 
results validate the soundness of this method. Then we extend it from two-class TC to 
multi-class TC. Furthermore, in this two-step framework, we try to improve the clas-
sifier by taking the dependences among features into consideration in the second step, 
resulting in a Causality Na?ve Bayesian Classifier. 
This paper is organized as follows: Section 2 describes the two-step method in the 
context of two-class Chinese TC; Section 3 extends it to multi-class TC; Section 4 
introduces the Causality Na?ve Bayesian Classifier; and Section 5 is conclusions.  
2   Basic Idea: A Two-Step Approach to Text Categorization 
2.1   Fix the Fuzzy Area Between Categories by the Na?ve Bayesian Classifier 
We use the Na?ve Bayesian Classifier to fix the fuzzy area in the first step. For a 
document represented by a binary-valued vector d = (W1, W2, ?, W|D|), the two-class 
Na?ve Bayesian Classifier is given as follows: 
???
===
?++=
=
||
1 2
2
||
1 1
1
||
1 2
1
2
1
2
1
1
log
1
log
1
1
log}Pr{
}Pr{
log
}Pr{
}Pr{
log)(
D
k k
k
k
D
k k
k
k
D
k k
k
-p
p
W
-p
p
W
-p
-p
c
c
|dc
|dcdf
 
 
(1) 
where Pr{
?
} is the probability that event {
?
} occurs, ci  is category i, and 
pki=Pr{Wk=1|ci} (i=1,2). If f(d) ?0, the document d will be assigned the category label 
c1, otherwise, c2.   
Let:  
?
=
+=
||
1 2
1
2
1
1
1log}Pr{
}Pr{log
D
k k
k
-p
-p
c
cCon  (2) 
?
=
=
||
1 1
1
1
log
D
k k
k
k
-p
p
WX  (3) 
?
=
=
||
1 2
2
1
log
D
k k
k
k
-p
pWY  (4) 
304 X. Fan et al 
where Con is a constant relevant only to the training set, X and Y are the measures 
that the document d belongs to categories c1 and c2 respectively.  
We rewrite (1) as: 
ConYXdf +?=)(  (5) 
Apparently,  f(d)=0 is the separate line in a two-dimensional space with X and Y 
being X-coordinate and Y-coordinate. In this space, a given document d can be 
viewed as a point (x, y), in which the values of x and y are calculated according to (3) 
and (4).  
As shown in Fig.1, the distance from the point (x, y) to the separate line will be: 
)(
2
1 ConyxDist +?=  (6) 
 
 
Fig. 1. Distance from point (x, y) to the separate line 
Fig. 2 illustrates the distribution of a training set (refer to Section 2.2) regarding 
Dist in the two-dimensional space, with the curve on the left for the negative samples, 
and the curve on the right for the positive samples. As can be seen in the figure, most 
of the misclassified documents, which unexpectedly across the separate line, are near 
the line. The error rate of the classifier is heavily influenced by this area, though the 
documents falling into this area only constitute a small portion of the training set.  
 
Fig. 2. Distribution of the training set in the two-dimensional space 
 Classifying Chinese Texts in Two Steps 305 
Thus, the space can be partitioned into reliable area and unreliable area: 
??
??
?
<
>
??
  reliable is   to  label  theAssigning                 
 reliable is    to  label  theAssigning        
 unreliable is for Decision          
22
11
12
dc,     DistDist
 dc     ,         DistDist
d, DistDistDist
 (7) 
where Dist1 and Dist2 are constants determined by experiments, Dist1 is positive real 
number and Dist2 is negative real number. 
In the second step, more subtle and powerful features will be designed in particular 
to tackle the unreliable area identified in the first step. 
2.2   Experiments on the Two-Class TC 
The dataset used here is composed of 12,600 documents with 1,800 negative samples 
of TARGET and 10,800 positive samples of Non-TARGET. It is split into 4 parts 
randomly, with three parts as training set and one part as test set. All experiments in 
this section are performed in 4-fold cross validation.  
CSeg&Tag3.0, a Chinese word segmentation and POS tagging system developed 
by Tsinghua University, is used to perform the morphological analysis for Chinese 
texts. In the first step, Chinese words with parts-of-speech verb, noun, adjective and 
adverb are considered as features. The original feature set is further reduced to a much 
smaller one according to formula (8) or (9). A Na?ve Bayesian Classifier is then ap-
plied to the test set. In the second step, only the documents that are identified unreli-
able in terms of (7) in the first step are concerned. This time, bigrams of Chinese 
words with parts-of-speech verb and noun are used as features, and the Na?ve Bayes-
ian Classifier is re-trained and applied again. 
?
=
=
n
i ik
ik
ikk
ct
,ct
,ct,ctMI
1
1 }Pr{}Pr{
}Pr{
log}Pr{)(  (8) 
?
=
=
n
i ik
ik
k
ct
,ct
,ctMI
1
2 }Pr{}Pr{
}Pr{
log)(  (9) 
where tk stands for the kth feature, which may be a Chinese word or a word bigram, 
and ci is the ith predefined category.  
We try five methods as follows.  
Method-1: Use Chinese words as features, reduce features with (9), and classify 
documents directly without exploring the two-step strategy.  
Method-2: same as Method-1 except feature reduction with (8).  
Method-3: same as Method-1 except Chinese word bigrams as features.  
Method-4: Use the mixture of Chinese words and Chinese word bigrams as fea-
tures, reduce features with (8), and classify documents directly.  
Method-5: (i.e., the proposed method): Use Chinese words as features in the first 
step and then use word bigrams as features in the second step, reduce features with 
(8), and classify the documents in two steps. 
306 X. Fan et al 
Note that the proportion of negative samples and positive samples is 1:6. Thus if 
all the documents in the test set is arbitrarily set to positive, the precision will reach 
85.7%. For this reason, only the experimental results for negative samples are consid-
ered in evaluation, as given in Table 1. For each method, the number of features is set 
by the highest point in the curve of the classifier performance with respect to the 
number of features (For the limitation of space, we omit all the curves here). The 
numbers of features set in five methods are 4000, 500, 15000, 800 and 500+3000 (the 
first step + the second step) respectively. 
Table 1.  Performance comparisons of the five methods in two-class TC 
 
Comparing Method-1 and Method-2, we can see that feature reduction formula (8) 
is superior to (9). Moreover, the number of features determined in the former is less 
than that in the latter (500 vs. 4000). Comparing Method-2, Method-3 and Method-4, 
we can see that Chinese word bigrams as features have better discriminating capabil-
ity meanwhile with more serious data sparseness: the performances of Method-3 and 
Method-4 are higher than that of Method-2, but the number of features used in 
Method-3 is more than those used in Method-2 and Method-4 (15000 vs. 500 and 
800). Table 1 shows that the proposed method (Methond-5) has the best performance 
(95.54% F1) and good efficiency. It integrates the merit of words and word bigrams. 
Using words as features in the first step aims at its better statistical coverage, -- the 
500 selected features in the first step can treat a majority of documents, constituting 
63.13% of the test set. On the other hand, using word bigrams as features in the sec-
ond step aims at its better discriminating capability, although the number of features 
becomes comparatively large (3000). Comparing Method-5 with Method-2, Method-3 
and Method-4, we find that the two-step approach is superior to either using only one 
kind of features (word or word bigram) in the classifier, or using the mixture of two 
kinds of features in one step. 
3   Extending the Two-Step Approach to the Multi-class TC 
We extend the two-step method presented in Section 2 to handle the multi-class TC 
now. The idea is to transfer the multi-class TC to the two-class TC. Similar to two-
class TC, the emphasis is still on the misclassified documents given by a classifier, 
though we use a modified multi-class Na?ve Bayesian Classifier here. 
 Classifying Chinese Texts in Two Steps 307 
3.1   Fix the Fuzzy Area Between Categories by the Multi-class Bayesian      
Classifier 
For a document represented by a binary-valued vector d = (W1, W2, ?, W|D|), the 
multi-class Na?ve Bayesian Classifier can be re-written as: 
??
==?
? ++=
||
1
||
1
)
1
log)1(log}{Prlog(maxarg
D
k ki
ki
k
D
k
kii
Cc -p
p
 W-p cc
i
 (10) 
where Pr{
?
} is the probability that event {
?
} occurs, pki=Pr{Wk=1|ci}, (i=1,2, ?, |C|), C is the number of predefined categories. Let: 
 ??
==
++=
||
1
||
1 1
log)1(log}{Prlog
D
k ki
ki
k
D
k
kiii
-p
p
 W-p cMV  (11) 
         )(maximummax_ iCcF MVMV i?=  (12) 
         
Cc
iS
i
MVMV
?
= )imum(second_maxmax_  (13) 
where MVi stands for the likelihood of assigning a label ci?C to the document d,  
MVmax_F and MVmax_S are the maximum and the second maximum over all MVi 
(i?|C|) respectively. We approximately rewrite (10) as: 
SF MVMVdf max_max_)( ?=           (14) 
We try to transfer the multi-class TC described by (10) into a two-class TC de-
scribed by (14). Formula (14) means that the binary-valued multi-class Na?ve Bayes-
ian Classifier can be approximately regarded as searching a separate line in a two-
dimensional space with MVmax_F being the X-coordinate and MVmax_S being the Y-
coordinate. The distance from a given document, represented as a point (x, y) with the 
values of x and y calculated according to (12) and (13) respectively, to the separate 
line in this two-dimensional space will be:  
      y)(xDist ?=
2
1
    (15) 
The value of Dist directly reflects the degree of confidence of assigning the label c* 
to the document d. 
The distribution of a training set (refer to Section 3.2) regarding Dist in this two-
dimensional space, and, consequently, the fuzzy area for the Na?ve Bayesian Classi-
fier, are observed and identified, similar to its counterpart in Section 2.2.  
3.2   Experiments on the Multi-class TC 
We construct a dataset, including 5 categories and the total of 17756 Chinese docu-
ments. The document numbers of five categories are 4192, 6968, 2080, 3175 and 
308 X. Fan et al 
1800 respectively, among which the last three categories have the high degree of 
ambiguity each other. The dataset is split into four parts randomly, one as the test set 
and the other three as the training set. We again run the five methods described in 
Section 2.2 on this dataset. The strategy of determining the number of features also 
follows that used in Section 2.2. The experimentally determined numbers of features 
regarding the five methods are 8000, 400, 5000, 800 and 400 + 9000 (the first step +  
the second step) respectively. 
The average precision, average recall and average F1 over the five categories are 
used to evaluate the experimental results, as shown in Table 2. 
Table 2.  Performance comparisons of the five methods in multi-class TC 
 
We can see from Table 2 that the very similar conclusions as that in the two-class 
TC in Section 2.2 can be obtained here: 
1) Formula (8) is superior to (9) in feature reduction. This comes from the per-
formance comparison between Method-2 and Method-1: the former has higher per-
formance and higher efficiency that the latter (the average F1, 97.20% vs. 91.48%, and 
the number of features used, 400 vs. 8000). 
2) Word bigrams as features have better discriminating capability than words as 
features, along with more serious data sparseness. The performances of Method-3 and 
Method-4, which use Chinese word bigrams and the mixture of words and word bi-
grams as features respectively, are higher than that of Method-2, which only uses 
Chinese words as features. But the number of features used in Method-3 is much 
more than those used in Method-2 and Method-4 (5000 vs. 400 and 800). 
3) The proposed method (Methond-5) has the best performances and acceptable ef-
ficiency. In term of the average F1, the performance is improved from the baseline 
91.48% (Method-1) to 98.56% (Method-5). In the first step in Method-5, the number 
of feature set is small (only 400), but a majority of documents can be treated by it. 
The number of features exploited in Method-5 is the highest among the five methods 
(9000), but it is still acceptable. 
4   Using Dependences Among Features in Two-Step 
Categorization  
In this section, a two-step text categorization method taking the dependences among 
features into account is presented. We do the same task with the Na?ve Bayesian Clas-
sifier in the first step, exactly same as what we did in Section 2 and Section 3. In the 
 Classifying Chinese Texts in Two Steps 309 
second step, each document identified unreliable in the first step are further processed 
by exploring the dependences among features. This is realized by a model named the 
Causality Na?ve Bayesian Classifier. 
4.1   The Causality Na?ve Bayesian Classifier (CNB) 
The Causality Na?ve Bayesian Classifier (CNB) is an improved Na?ve Bayesian Clas-
sifier. It contains two additional parts, i.e., the k-dependence feature list and the fea-
ture causality diagram. The former is used to represent the dependence relation among 
features, and the latter is used to estimate the probability distribution of a feature 
dynamically while taking its dependences into account.  
K-Dependence Feature List (K-DFL): CNB allows each feature node Y to have a 
maximum of k features nodes as parents that constitute the k-dependence feature list 
representing the dependences among features. In other words, ?(Y) = {Yd, C}, where 
Yd is the set of at most k features nodes, C is the category node, and ?(C) =?. 
Note that we can build a K-DFL for each feature under each class ct, which repre-
sents different dependence relations under different class.  
Obviously, there exists a 0-dependence feature list for every feature in the Na?ve 
Bayesian Classifier, from the definition of K-DFL. 
The algorithm of constructing K-DFL is as follows: Given the maximum depend-
ence number k, mutual information threshold ? and the class ct. For each feature Y, 
repeat the follow steps. 1) Compute class conditional mutual information MI(Yi, Yj| 
ct), for every pair of features Yi and Yj, where i?j. 2) Construct the set Si={ Yj | 
MI(Yi, Yj| ct) > ?}. 3) Let m= min (k, | Si|), select the top m features as K-DFL  
from Si. 
Feature Causality Diagram (FCD): CNB allows each feature Y, which occurs in a 
given document, to have a Feature Causality Diagram (FCD). FCD is a double-layer 
directed diagram, in which the first layer has only the feature node Y, and the second 
layer allows to have multiple nodes that include the class node C and the correspond-
ing dependence node set S of Y. Here, S=Sd?SF, Sd is the K-DFL node set of Y and 
SF={Xi| Xi is a feature node that occurs in the given document. There exists a directed 
arc from every node Xi at the second layer to the node Y at the first layer. The arc is 
called causality link event Li which represents the causality intensity between node Y 
and Xi, and the probability of Li is pi=Pr{Li}=Pr{Y=1|Xi=1}. The relation among all 
arcs is logical OR. The Feature Causality Diagram can be considered as a sort of 
simplified causality diagram [9][10]. 
Suppose feature Y?s FCD is G, and it parent node set S={X1, X2,?,Xm } (m?1) in 
G, we can estimate the conditional probability as follows while considering the de-
pendences among features: 
? ?
=
?
==
?+===?===
m
i
i
j
ji
i
i ppp
2
1
1
1
m
1
m1 )1(}LPr{ G}|1Pr{Y1}X,1,X|1Pr{Y UL  (16) 
Note that when m=1, C}|1Pr{YG}|1Pr{Y1}X|1Pr{Y 1 ====== . 
310 X. Fan et al 
Causality Na?ve Bayesian Classifier (CNB): For a document represented by a bi-
nary-valued vector d=(X1 ,X2 , ?,X|d|), divide the features into two sets X1 and X2, 
X1= {Xi| Xi=1} and X2= {Xj| Xj=0}. The Causality Na?ve Bayesian Classifier can be 
written as: 
}))c|{XPrlog(1}G|logPr{X}(logPr{cmax argc*
||
1
||
1
tj
ji
iit
Cct
??
==?
?++=
21 XX
 (17) 
4.2   Experiments on CNB 
As mentioned earlier, the first step remains unchanged as that in Section 2 and Sec-
tion 3. The difference is in the second step: for the documents identified unreliable in 
the first step, we apply the Causality Na?ve Bayesian Classifier to handle them. 
We use two datasets in the experiments. one is the two-class dataset described in 
Section 2.2, called Dataset-I, and the other one is the multi-class dataset described in 
Section 3.2, called Dataset-I. 
To evaluate CNB and compare all methods presented in this paper, we experiment 
the following methods:  
1) Na?ve Bayesian Classifier (NB), i.e., the method-2 in Section 2.2;  
2) CNB without exploring the two-step strategy;  
3) The two-step strategy: NB and CNB in the first and second step (TS-CNB);  
4) Limited Dependence Bayesian Classifier (DNB) [11];  
5) Method-5 in Section 2.2 and Section 3.2 (denoted TS-DF here).  
Experimental results for two-class Dataset-I and multi-class Dataset-II are listed in 
Table3 and Table 4. The data for NB and TS-DF are derived from the corresponding 
columns of Table 1 and Table 2. The parameters in CNB and TS-CNB are that the 
dependence number k=1 and 5, the threshold?= 0.0545 and 0.0045 for Dataset-I and 
Dataset-II respectively. The parameters in DNB are that dependence number k=1and 
3, the threshold?= 0.0545 and 0.0045 for Dataset-I and Dataset-II respectively. 
Table 3.  Performance comparisons in two-class Dataset-I 
 
Table 3 and Table 4 demonstrate that 1) The  performance of the Na?ve Bayesian 
Classifier can be improved by taking the dependences among features into account, as 
evidenced by the fact that CNB, TS-CNB and DNB outperform NB. By tracing the 
experiment, we find an interesting phenomenon, as expected: for the documents  
 Classifying Chinese Texts in Two Steps 311 
identified reliable by NB, CNB cannot improve it, but for those identified unreliable 
by NB, CNB can improve it. The reason should be even though NB and CNB use the 
same features, but CNB uses the dependences among features additionally. 2) CNB 
and TS-CNB have the same capability in effectiveness, but TS-CNB has a higher 
computational efficiency. As stated earlier, TS-CNB uses NB to classify documents in 
the reliable area and then uses CNB to classify documents in the unreliable area. At 
the first glance, the efficiency of TS-CNB seems lower than that of using CNB only 
because the former additionally uses NB in the first step, but in fact, a majority of 
documents (e.g., 63.13% of the total documents in dataset-I) fall into the reliable area 
and are then treated by NB successfully (obviously, NB is higher than CNB in effi-
ciency) in the first step, so they will never go to the second step, resulting in a higher 
computational efficiency of TS-CNB than CNB. 3) The performances of CNB, TS-
CNB and DNB are almost identical, among which, the efficiency of TS-CNB is the 
highest. And, the efficiency of CNB is higher than that of DNB, because CNB uses a 
simpler network structure than DNB, with the same learning and inference formalism. 
4) TS-DF has the highest performance among the all. Meanwhile, the ranking of 
computational efficiency (in descending order) is NB, TS-DF, TS-CNB, CNB,  
and DNB.  
Table 4.  Performance comparisons in multi-class Dataset-II 
 
5   Related Works 
Combining multiple methodologies or representations has been studied in several 
areas of information retrieval so far, for example, retrieval effectiveness can be im-
proved by using multiple representations [12]. In the area of text categorization in 
particular, many methods of combining different classifiers have been developed. For 
example, Yang et al [13] used simple equal weights for normalized score of each 
classifier output so as to integrate multiple classifiers linearly in the domain of Topic 
Detection and Tracking; Hull at al. [14] used linear combination for probabilities or 
log odds scores of multiple classifier output in the context of document filtering. Lar-
key et al [15] used weighted linear combination for system ranks and scores of multi-
ple classifier output in the medical document domain; Li and Jain [16] used voting 
and classifier selection technique including dynamic classifier selection and adaptive 
classifier. Lam and Lai [17] automatically selected a classifier for each category based 
on the category-specific statistical characteristics. Bennett et al [18] used voting, 
classifier-selection techniques and a hierarchical combination method with  
reliability indicators. 
312 X. Fan et al 
6   Conclusions 
The issue of how to classify Chinese documents characterized by high degree ambi-
guity from text categorization?s point of view is a challenge. For this issue, this paper 
presents two solutions in a uniform two-step framework, which makes use of the 
distributional characteristics of misclassified documents, that is, most of the misclas-
sified documents are near to the separate line between categories. The first solution is 
a two-step TC approach based on the Na?ve Bayesian Classifier. The second solution 
is to further introduce the dependences among features into the model, resulting in a 
two-step approach based on the so-called Causality Na?ve Bayesian Classifier. Ex-
periments show that the second solution is superior to the Na?ve Bayesian Classifier, 
and is equal to CNB without exploring two-step strategy in performance, but has a 
higher computational efficiency than the latter. The first solution has the best per-
formance in all the experiments, outperforming all other methods (including the sec-
ond solution): in the two-class experiments, its F1 increases from the baseline 82.67% 
to the final 95.54%, and in the multi-class experiments, its average F1 increases from 
the baseline 91.48% to the final 98.56%. 
In addition, the other two conclusions can be drawn from the experiments: 1) Us-
ing Chinese word bigrams as features has a better discriminating capability than using 
words as features, but more serious data sparseness will be faced; 2) formula (8) is 
superior to (9) in feature reduction in both the two-class and multi-class Chinese text 
categorization. 
It is worth point out that we believe the proposed method is in principle language 
independent, though all the experiments are performed on Chinese datasets. 
Acknowledgements 
The research is supported in part by the National 863 Project of China under grant 
number 2001AA114210-03, 2003 Korea-China Young Scientists Exchange Program, 
the Tsinghua-ALVIS Project co-sponsored by the National Natural Science Founda-
tion of China under grant number 60520130299 and EU FP6, and the National Natu-
ral Science Foundation of China under grant number 60321002.  
References 
1. Sebastiani, F. Machine Learning in Automated Text Categorization. ACM Computing Sur-
veys, 34(1):1-47, 2002. 
2. Lewis, D. Naive Bayes at Forty: The Independence Assumption in Information Retrieval. 
In Proceedings of ECML-98, 4-15, 1998. 
3. Salton, G. Automatic Text Processing: The Transformation, Analysis, and Retrieval of In-
formation by Computer. Addison-Wesley, Reading, MA, 1989. 
4. Mitchell, T.M. Machine Learning. McCraw Hill, New York, NY, 1996. 
5. Yang, Y., and Liu, X. A Re-examination of Text Categorization Methods. In Proceedings 
of SIGIR-99, 42-49,1999. 
6. Xinghua Fan. Causality Reasoning and Text Categorization, Postdoctoral Research Report 
of Tsinghua University, P.R. China, April 2004. (In Chinese) 
 Classifying Chinese Texts in Two Steps 313 
7. Dumais, S.T., Platt, J., Hecherman, D., and Sahami, M. Inductive Learning Algorithms 
and Representation for Text Categorization. In Proceedings of CIKM-98, Bethesda, MD, 
148-155, 1998. 
8. Sahami, M., Dumais, S., Hecherman, D., and Horvitz, E. A. Bayesian Approach to Filter-
ing Junk E-Mail. In Learning for Text Categorization: Papers from the AAAI Workshop, 
55-62, Madison Wisconsin. AAAI Technical Report WS-98-05, 1998. 
9. Xinghua Fan. Causality Diagram Theory Research and Applying It to Fault Diagnosis of 
Complexity System, Ph.D. Dissertation of Chongqing University, P.R. China, April 2002. 
(In Chinese) 
10. Xinghua Fan, Zhang Qin, Sun Maosong, and Huang Xiyue. Reasoning Algorithm in 
Multi-Valued Causality Diagram, Chinese Journal of Computers, 26(3), 310-322, 2003. 
(In Chinese) 
11. Sahami, M. Learning Limited Dependence Bayesian Classifiers. In Proceedings of the 
Second International Conference on Knowledge Discovery and Data Mining, Portland, 
335-338, 1996. 
12. Rajashekar, T. B. and Croft, W. B. Combining Automatic and Manual Index Representa-
tions in Probabilistic Retrieval. Journal of the American society for information science, 
6(4): 272-283,1995. 
13. Yang, Y., Ault, T. and Pierce, T. Combining Multiple Learning Strategies for Effective 
Cross Validation. In Proceedings of  ICML 2000, 1167?1174, 2000. 
14. Hull, D. A., Pedersen, J. O. and H. Schutze. Method Combination for Document Filtering. 
In Proceedings of SIGIR-96, 279?287, 1996. 
15. Larkey, L. S. and Croft, W. B. Combining Classifiers in Text Categorization. In Proceed-
ings of SIGIR-96, 289-297, 1996. 
16. Li, Y. H., and Jain, A. K. Classification of Text Documents. The Computer Journal, 41(8): 
537-546, 1998. 
17. Lam, W., and Lai, K.Y. A Meta-learning Approach for Text Categorization. In Proceed-
ings of SIGIR-2001, 303-309, 2001. 
18. Bennett, P. N., Dumais, S. T., and Horvitz, E. Probabilistic Combination of Text Classifi-
ers Using Reliability Indicators: Models and Results. In Proceedings of SIGIR-2002, 11-
15, 2002. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 545?552,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Comparison and Semi-Quantitative Analysis of Words and  
Character-Bigrams as Features in Chinese Text Categorization 
 
 
Jingyang Li                       Maosong Sun                     Xian Zhang 
National Lab. of Intelligent Technology & Systems, Department of Computer Sci. & Tech. 
Tsinghua University, Beijing 100084, China 
lijingyang@gmail.com  sms@tsinghua.edu.cn  kevinn9@gmail.com 
 
  
 
Abstract 
Words and character-bigrams are both 
used as features in Chinese text process-
ing tasks, but no systematic comparison 
or analysis of their values as features for 
Chinese text categorization has been re-
ported heretofore. We carry out here a 
full performance comparison between 
them by experiments on various docu-
ment collections (including a manually 
word-segmented corpus as a golden stan-
dard), and a semi-quantitative analysis to 
elucidate the characteristics of their be-
havior; and try to provide some prelimi-
nary clue for feature term choice (in most 
cases, character-bigrams are better than 
words) and dimensionality setting in text 
categorization systems. 
1 Introduction1 
Because of the popularity of the Vector Space 
Model (VSM) in text information processing, 
document indexing (term extraction) acts as a 
pre-requisite step in most text information proc-
essing tasks such as Information Retrieval 
(Baeza-Yates and Ribeiro-Neto, 1999) and Text 
Categorization (Sebastiani, 2002). It is empiri-
cally known that the indexing scheme is a non-
trivial complication to system performance, es-
pecially for some Asian languages in which there 
are no explicit word margins and even no natural 
semantic unit. Concretely, in Chinese Text Cate-
gorization tasks, the two most important index-
                                                 
1 This research is supported by the National Natural Science 
Foundation of China under grant number 60573187 and  
60321002, and the Tsinghua-ALVIS Project co-sponsored 
by the National Natural Science Foundation of China under 
grant number 60520130299 and EU FP6. 
ing units (feature terms) are word and character-
bigram, so the problem is: which kind of terms2 
should be chosen as the feature terms, words or 
character-bigrams? 
To obtain an all-sided idea about feature 
choice beforehand,  we review here the possible 
feature variants (or, options). First, at the word 
level, we can do stemming, do stop-word prun-
ing, include POS (Part of Speech) information, 
etc. Second, term combinations (such as ?word-
bigram?, ?word + word-bigram?, ?character-
bigram + character-trigram?3, etc.) can also be 
used as features (Nie et al, 2000). But, for Chi-
nese Text Categorization, the ?word or bigram? 
question is fundamental. They have quite differ-
ent characteristics (e.g. bigrams overlap each 
other in text, but words do not) and influence the 
classification performance in different ways. 
In Information Retrieval, it is reported that bi-
gram indexing schemes outperforms word 
schemes to some or little extent (Luk and Kwok, 
1997; Leong and Zhou 1998; Nie et al, 2000). 
Few similar comparative studies have been re-
ported for Text Categorization (Li et al, 2003) so 
far in literature. 
Text categorization and Information Retrieval 
are tasks that sometimes share identical aspects 
(Sebastiani, 2002) apart from term extraction 
(document indexing), such as tfidf term weight-
ing and performance evaluation. Nevertheless, 
they are different tasks. One of the generally ac-
cepted connections between Information Re-
trieval and Text Categorization is that an infor-
mation retrieval task could be partially taken as a 
binary classification problem with the query as 
the only positive training document. From this 
                                                 
2 The terminology ?term? stands for both word and charac-
ter-bigram. Term or  combination of terms (in word-bigram 
or other forms) might be chosen as ?feature?. 
3 The terminology ?character? stands for Chinese character, 
and ?bigram? stands for character-bigram in this paper. 
545
viewpoint, an IR task and a general TC task have 
a large difference in granularity. To better illus-
trate this difference, an example is present here. 
The words ????(film producer)? and ???
?(dubbed film)? should be taken as different 
terms in an IR task because a document with one 
would not necessarily be a good match for a 
query with the other, so the bigram ???(film 
production)? is semantically not a shared part of 
these two words, i.e. not an appropriate feature 
term. But in a Text Categorization task, both 
words might have a similar meaning at the cate-
gory level (?film? category, generally), which 
enables us to regard the bigram ???? as a se-
mantically acceptable representative word snip-
pet for them, or for the category. 
There are also differences in some other as-
pects of IR and TC. So it is significant to make a 
detailed comparison and analysis here on the 
relative value of words and bigrams as features 
in Text Categorization. The organization of this 
paper is as follows: Section 2 shows some ex-
periments on different document collections to 
observe the common trends in the performance 
curves of the word-scheme and bigram-scheme; 
Section 3 qualitatively analyses these trends; 
Section 4 makes some statistical analysis to cor-
roborate the issues addressed in Section 3; Sec-
tion 5 summarizes the results and concludes. 
2 Performance Comparison 
Three document collections in Chinese language 
are used in this study. 
The electronic version of Chinese Encyclo-
pedia (?CE?): It has 55 subject categories and 
71674 single-labeled documents (entries). It is 
randomly split by a proportion of 9:1 into a train-
ing set with 64533 documents and a test set with 
7141 documents. Every document has the full-
text. This data collection does not have much of 
a sparseness problem. 
The training data from a national Chinese 
text categorization evaluation4 (?CTC?): It has 
36 subject categories and 3600 single-labeled5 
documents. It is randomly split by a proportion 
of 4:1 into a training set with 2800 documents 
and a test set with 720 documents. Documents in 
this data collection are from various sources in-
cluding news websites, and some documents 
                                                 
4 The Annual Evaluation of  Chinese Text Categorization 
2004, by 863 National Natural Science Foundation. 
5 In the original document collection, a document might 
have a secondary category label. In this study, only the pri-
mary category label is reserved. 
may be very short. This data collection has a 
moderate sparseness problem. 
A manually word-segmented corpus from 
the State Language Affairs Commission 
(?LC?): It has more than 100 categories and 
more than 20000 single-labeled documents6. In 
this study, we choose a subset of 12 categories 
with the most documents (totally 2022 docu-
ments). It is randomly split by a proportion of 2:1 
into a training set and a test set. Every document 
has the full-text and has been entirely word-
segmented7 by hand (which could be regarded as 
a golden standard of segmentation). 
All experiments in this study are carried out at 
various feature space dimensionalities to show 
the scalability. Classifiers used in this study are 
Rocchio and SVM. All experiments here are 
multi-class tasks and each document is assigned 
a single category label. 
The outline of this section is as follows: Sub-
section 2.1 shows experiments based on the Roc-
chio classifier, feature selection schemes besides 
Chi and term weighting schemes besides tfidf to 
compare the automatic segmented word features 
with bigram features on CE and CTC, and both 
document collections lead to similar behaviors; 
Subsection 2.2 shows experiments on CE by a 
SVM classifier,  in which, unlike with the Roc-
chio method, Chi feature selection scheme and 
tfidf term weighting scheme outperform other 
schemes; Subsection 2.3 shows experiments by a 
SVM classifier with Chi feature selection and 
tfidf term weighting on LC (manual word seg-
mentation) to compare the best word features 
with bigram features. 
2.1 The Rocchio Method and Various Set-
tings 
The Rocchio method is rooted in the IR tradition, 
and is very different from machine learning ones 
(such as SVM) (Joachims, 1997; Sebastiani, 
2002). Therefore, we choose it here as one of the 
representative classifiers to be examined. In the 
experiment, the control parameter of negative 
examples is set to 0, so this Rocchio based classi-
fier is in fact a centroid-based classifier. 
Chimax is a state-of-the-art feature selection 
criterion for dimensionality reduction (Yang and 
Peterson, 1997; Rogati and Yang, 2002). Chi-
max*CIG (Xue and Sun, 2003a) is reported to be 
better in Chinese text categorization by a cen-
                                                 
6 Not completed. 
7 And POS (part-of-speech) tagged as well. But POS tags 
are not used in this study. 
546
troid based classifier, so we choose it as another 
representative feature selection criterion besides 
Chimax. 
Likewise, as for term weighting schemes, in 
addition to tfidf, the state of the art (Baeza-Yates 
and Ribeiro-Neto, 1999), we also choose 
tfidf*CIG (Xue and Sun, 2003b). 
Two word segmentation schemes are used for 
the word-indexing of documents. One is the 
maximum match algorithm (?mmword? in the 
figures), which is a representative of simple and 
fast word segmentation algorithms.  The other is 
ICTCLAS8 (?lqword? in the figures). ICTCLAS 
is one of the best word segmentation systems 
(SIGHAN 2003) and reaches a segmentation 
precision of more than 97%, so we choose it as a 
representative of state-of-the-art schemes for 
automatic word-indexing of document). 
For evaluation of single-label classifications,  
F1-measure, precision, recall and accuracy 
(Baeza-Yates and Ribeiro-Neto, 1999; Sebastiani, 
2002) have the same value by microaveraging9, 
and are labeled with ?performance? in the fol-
lowing figures. 
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
p
e
rf
o
rm
a
n
ce
mmword
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
lqword
p
e
rf
o
rm
a
n
ce
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
bigram
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf      
chicig-tfidfcig
 
Figure 1. chi-tfidf and chicig-tfidfcig on CE 
Figure 1 shows the performance-
dimensionality curves of the chi-tfidf approach 
and the approach with CIG, by mmword, lqword 
and bigram document indexing, on the CE 
document collection. We can see that the original 
chi-tfidf approach is better at low dimensional-
ities (less than 10000 dimensions), while the CIG 
version is better at high dimensionalities and 
reaches a higher limit.10 
                                                 
8 http://www.nlp.org.cn/project/project.php?proj_id=6 
9 Microaveraging is more prefered in most cases than 
macroaveraging (Sebastiani 2002). 
10 In all figures in this paper, curves might be truncated due 
to the large scale of dimensionality, especially the curves of 
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
p
e
rf
o
rm
a
n
ce
mmword
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
lqword
p
e
rf
o
rm
a
n
ce
chi-tfidf      
chicig-tfidfcig
1 2 3 4 5 6 7 8
x 10
4
0.5
0.6
0.7
0.8
bigram
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf      
chicig-tfidfcig
 
Figure 2. chi-tfidf and chicig-tfidfcig on CTC 
Figure 2 shows the same group of curves for 
the CTC document collection. The curves fluctu-
ate more than the curves for the CE collection 
because of sparseness; The CE collection is more 
sensitive to the additions of terms that come with 
the increase of dimensionality. The CE curves in 
the following figures show similar fluctuations 
for the same reason. 
For a parallel comparison among mmword, 
lqword and bigram schemes, the curves in  Fig-
ure 1 and Figure 2 are regrouped and shown in 
Figure 3 and Figure 4. 
2 4 6 8
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf
mmword
lqword
bigram
2 4 6 8
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
dimensionality
chicig-tfidfcig
mmword
lqword
bigram
 
Figure 3. mmword, lqword and bigram on CE 
 
1 2 3 4 5
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
p
e
rf
o
rm
a
n
ce
dimensionality
chi-tfidf
mmword
lqword
bigram
1 2 3 4 5
x 10
4
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
dimensionality
chicig-tfidfcig
mmword
lqword
bigram
 
Figure 4. mmword, lqword and bigram on CTC 
                                                                          
bigram scheme. For these kinds of figures, at least one of 
the following is satisfied: (a) every curve has shown its 
zenith; (b) only one curve is not complete and has shown a 
higher zenith than other curves; (c) a margin line is shown 
to indicate the limit of the incomplete curve. 
547
We can see that the lqword scheme outper-
forms the mmword scheme at almost any dimen-
sionality, which means the more precise the word 
segmentation the better the classification per-
formance. At the same time, the bigram scheme 
outperforms both of the word schemes on a high 
dimensionality, wherea the word schemes might 
outperform the bigram scheme on a low dimen-
sionality. 
Till now, the experiments on CE and CTC 
show the same characteristics despite the per-
formance fluctuation on CTC caused by sparse-
ness. Hence in the next subsections CE is used 
instead of both of them because its curves are 
smoother. 
2.2 SVM on Words and Bigrams 
As stated in the previous subsection, the lqword 
scheme always outperforms the mmword scheme; 
we compare here only the lqword scheme with 
the bigram scheme.  
Support Vector Machine (SVM) is one of the 
best classifiers at present (Vapnik, 1995; 
Joachims, 1998), so we choose it as the main 
classifier in this study. The SVM implementation 
used here is LIBSVM (Chang, 2001); the type of 
SVM is set to ?C-SVC? and the kernel type is set 
to linear, which means a one-with-one scheme is 
used in the multi-class classification. 
Because the CIG?s effectiveness on a SVM 
classifier is not examined in Xue and Sun (2003a, 
2003b)?s report, we make here the four combina-
tions of schemes with and without CIG in feature 
selection and term weighting. The experiment 
results are shown in Figure 5. The collection 
used is CE. 
 
1 2 3 4 5 6 7
x 10
4
0.6
0.65
0.7
0.75
0.8
0.85
0.9
p
e
rf
o
rm
a
n
ce
dimensionality
lqword
chi-tfidf      
chi-tfidfcig   
chicig-tfidf   
chicig-tfidfcig
1 2 3 4 5 6 7
x 10
4
0.6
0.65
0.7
0.75
0.8
0.85
0.9
dimensionality
bigram
chi-tfidf      
chi-tfidfcig   
chicig-tfidf   
chicig-tfidfcig
 
Figure 5. chi-tfidf and cig-involved approaches 
on lqword and bigram 
Here we find that the chi-tfidf combination 
outperforms any approach with CIG, which is the 
opposite of the results with the Rocchio method. 
And the results with SVM are all better than the 
results with the Rocchio method. So we find that 
the feature selection scheme and the term 
weighting scheme are related to the classifier, 
which is worth noting. In other words, no feature 
selection scheme or term weighting scheme is 
absolutely the best for all classifiers. Therefore, a 
reasonable choice is to select the best performing 
combination of feature selection scheme, term 
weighting scheme and classifier, i.e. chi-tfidf and 
SVM. The curves for the lqword scheme and the 
bigram scheme are redrawn in Figure 6 to make 
them clearer. 
1 2 3 4 5 6 7
x 10
4
0.75
0.8
0.85
0.9
p
e
rf
o
rm
a
n
c
e
dimensionality
lqword
bigram
 
Figure 6. lqword and bigram on CE 
The curves shown in Figure 6 are similar to 
those in Figure 3. The differences are: (a) a lar-
ger dimensionality is needed for the bigram 
scheme to start outperforming the lqword scheme; 
(b) the two schemes have a smaller performance 
gap. 
The lqword scheme reaches its top perform-
ance at a dimensionality of around 40000, and 
the bigram scheme reaches its top performance 
at a dimensionality of around 60000 to 70000, 
after which both schemes? performances slowly 
decrease. The reason is that the low ranked terms 
in feature selection are in fact noise and do not 
help to classification, which is why the feature 
selection phase is necessary. 
2.3 Comparing Manually Segmented 
Words and Bigrams 
0 1 2 3 4 5 6 7 8 9 10
x 10
4
72
74
76
78
80
82
84
86
88
dimansionality
p
e
rf
o
rm
a
n
c
e
word        
bigram      
bigram limit
 
Figure 7. word and bigram on LC 
548
Up to now, bigram features seem to be better 
than word ones for fairly large dimensionalities. 
But it appears that word segmentation precision 
impacts classification performance. So we 
choose here a fully manually segmented docu-
ment collection to detect the best performance a 
word scheme could  reach and compare it with 
the bigram scheme. 
Figure 7 shows such an experiment result on 
the LC document collection (the circles indicate 
the maximums and the dash-dot lines indicate the 
superior limit and the asymptotic interior limit of 
the bigram scheme). The word scheme reaches a 
top performance around the dimensionality of 
20000, which is a little higher than the bigram 
scheme?s zenith around 70000. 
Besides this experiment on 12 categories of 
the LC document collection, some experiments 
on fewer (2 to 6) categories of this subset were 
also done, and showed similar behaviors. The 
word scheme shows a better performance than 
the bigram scheme and needs a much lower di-
mensionality. The simpler the classification task 
is, the more distinct this behavior is. 
3 Qualitative Analysis 
To analyze the performance of words and bi-
grams as feature terms in Chinese text categori-
zation, we need to investigate two aspects as fol-
lows. 
3.1 An Individual Feature Perspective 
The word is a natural semantic unit in Chinese 
language and expresses a complete meaning in 
text. The bigram is not a natural semantic unit 
and might not express a complete meaning in 
text, but there are also reasons for the bigram to 
be a good feature term. 
First, two-character words and three-character 
words account for most of all multi-character 
Chinese words (Liu and Liang, 1986). A two-
character word can be substituted by the same 
bigram. At the granularity of most categorization 
tasks, a three-character words can often be sub-
stituted by one of its sub-bigrams (namely the 
?intraword bigram? in the next section)  without 
a change of meaning. For instance, ???? is a 
sub-bigram of the word ????(tournament)? 
and could represent it without ambiguity. 
Second, a bigram may overlap on two succes-
sive words (namely the ?interword bigram? in 
the next section), and thus to some extent fills the 
role of a word-bigram. The word-bigram as a 
more definite (although more sparse) feature  
surely helps the classification. For instance, ??
?? is a bigram overlapping on the two succes-
sive words ? ? ? (weather)? and ? ? ?
(forecast)?, and could almost replace the word-
bigram (also a phrase) ?????(weather fore-
cast)?, which is more likely to be a representative 
feature of the category ????(meteorology)? 
than either word. 
Third, due to the first issue, bigram features 
have some capability of identifying OOV (out-
of-vocabulary) words 11 , and help improve the 
recall of classification. 
The above issues state the advantages of bi-
grams compared with words. But in the first and 
second issue, the equivalence between bigram 
and word or word-bigram is not perfect. For in-
stance, the word ???(literature)? is a also sub-
bigram of the word ????(astronomy)?, but 
their meanings are completely different. So the 
loss and distortion of semantic information is a 
disadvantage of bigram features over word fea-
tures.  
Furthermore, one-character words cover about 
7% of words and more than 30% of word occur-
rences in the Chinese language; they are effev-
tive in the word scheme and are not involved in 
the above issues. Note that the impact of effec-
tive one-character words on the classification is 
not as large as their total frequency, because the 
high frequency ones are often too common to 
have a good classification power, for instance, 
the word ?? (of, ?s)?. 
3.2 A Mass Feature Perspective 
Features are not independently acting in text 
classification. They are assembled together to 
constitute a feature space. Except for a few mod-
els such as Latent Semantic Indexing (LSI) 
(Deerwester et al, 1990), most models assume 
the feature space to be orthogonal. This assump-
tion might not affect the effectiveness of the 
models, but the semantic redundancy and com-
plementation among the feature terms do impact 
on the classification efficiency at a given dimen-
sionality. 
According to the first issue addressed in the 
previous subsection, a bigram might cover for 
more than one word. For instance, the bigram 
???? is a sub-bigram of the words ???
(fabric)?,???? (cotton fabric)?, ????
(knitted fabric)?, and also a good substitute of 
                                                 
11 The ?OOV words? in this paper stand for the words that 
occur in the test documents but not in the training document. 
549
them. So, to a certain extent, word features are 
redundant with regard to the bigram features as-
sociated to them. Similarly, according to the sec-
ond issue addressed, a bigram might cover for 
more than one word-bigram. For instance, the 
bigram ???? is a sub-bigram of the word-
bigrams (phrases) ?????(short story)?, ??
???(novelette)?, ?????(novel)? and also 
a good substitute for them. So, as an addition to 
the second issue stated in the previous subsection, 
a bigram feature might even cover for more than 
one word-bigram. 
On the other hand, bigrams features are also 
redundant with regard to word features associ-
ated with them. For instance, the ???? and ??
?? are both sub-bigrams of the previously men-
tioned word ?????. In some cases, more than 
one sub-bigram can be a good representative of a 
word. 
We make a word list and a bigram list sorted 
by the feature selection criterion in a descending 
order. We now try to find how the relative re-
dundancy degrees of the word list and the bigram 
list vary with the dimensionality. Following is-
sues are elicited by an observation on the two 
lists (not shown here due to space limitations). 
The relative redundancy rate in the word list 
keeps even while the dimensionality varies to a 
certain extent, because words that share a com-
mon sub-bigram might not have similar statistics 
and thus be scattered in the word feature list. 
Note that these words are possibly ranked lower 
in the list than the sub-bigram because feature 
selection criteria (such as Chi) often prefer 
higher frequency terms to lower frequency ones, 
and every word containing the bigram certainly 
has a lower frequency than the bigram itself. 
The relative redundancy in the bigram list 
might be not as even as in the word list. Good 
(representative) sub-bigrams of a word are quite 
likely to be ranked close to the word itself. For 
instance, ???? and ???? are sub-bigrams of 
the word ????(music composer)?, both the 
bigrams and the word are on the top of the lists. 
Theretofore, the bigram list has a relatively large 
redundancy rate at low dimensionalities. The 
redundancy rate should decrease along with the 
increas of dimensionality for: (a) the relative re-
dundancy in the word list counteracts the redun-
dancy in the bigram list, because the words that 
contain a same bigram are gradually included as 
the dimensionality increases; (b) the proportion 
of interword bigrams increases in the bigram list 
and there is generally no redundancy between 
interword bigrams and intraword bigrams. 
Last, there are more bigram features than word 
features because bigrams can overlap each other 
in the text but words can not. Thus the bigrams 
as a whole should theoretically contain more in-
formation than the words as a whole. 
From the above analysis and observations, bi-
gram features are expected to outperform word 
features at high dimensionalities. And word fea-
tures are expected to outperform bigram features 
at low dimensionalities.  
4 Semi-Quantitative Analysis 
In this section, a preliminary statistical analysis 
is presented to corroborate the statements in the 
above qualitative analysis and expected to be 
identical with the experiment results shown in 
Section 1. All statistics in this section are based 
on the CE document collection and the lqword 
segmentation scheme (because the CE document 
collection is large enough to provide good statis-
tical characteristics). 
4.1 Intraword Bigrams and Interword Bi-
grams 
In the previous section, only the intraword bi-
grams were discussed together with the words. 
But every bigram may have both intraword oc-
currences and interword occurrences. Therefore 
we need to distinguish these two kinds of bi-
grams at a statistical level. For every bigram, the 
number of intraword occurrences and the number 
of interword occurrences are counted and we can 
use 
 
1
log
1
interword#
intraword#
+? ?? ?+? ?  
as a metric to indicate its natual propensity to be 
a intraword bigram. The probability density of 
bigrams about on this metric is shown in Figure 
8. 
-12 -10 -8 -6 -4 -2 0 2 4 6 8 10
0
0.05
0.1
0.15
0.2
0.25
log(intraword#/interword#)
p
ro
b
a
b
ili
ty
 d
e
n
s
it
y
 
Figure 8. Bigram Probability Density on 
log(intraword#/interword#) 
550
The figure shows a mixture of two Gaussian 
distributions, the left one for ?natural interword 
bigrams? and the right one for ?natural intraword 
bigrams?. We can moderately distinguish these 
two kinds of bigrams by a division at -1.4. 
4.2 Overall Information Quantity of a Fea-
ture Space 
The performance limit of a classification is re-
lated to the quantity of information used. So a 
quantitative metric of the information a feature 
space can provide is need. Feature Quantity (Ai-
zawa, 2000) is suitable for this purpose because 
it comes from information theory and is additive; 
tfidf was also reported as an appropriate metric of 
feature quantity (defined as ?probability ? infor-
mation?). Because of the probability involved as 
a factor, the overall information provided by a 
feature space can be calculated on training data 
by summation. 
The redundancy and complementation men-
tioned in Subsection 3.2 must be taken into ac-
count in the calculation of overall information 
quantity. For bigrams, the redundancy with re-
gard to words associated with them between two 
intraword bigrams is given by 
 { }
1,2
1 2( ) min ( ), ( )
b w
tf w idf b idf b
?
??  
in which b1 and b2 stand for the two bigrams and 
w stands for any word containing both of them. 
The overall information quantity is obtained by 
subtracting the redundancy between each pair of 
bigrams from the sum of all features? feature 
quantity (tfidf). Redundancy among more than 
two bigrams is ignored. For words, there is only 
complementation among words but not redun-
dancy, the complementation with regard to bi-
grams associated with them is given by 
 
{ } if  exists;
if  does not exists.
( ) min ( ) ,
( ) ( ),
b w
b
b
tf w idf b
tf w idf w
?
????
???
 
in which b is an intraword bigram contained by 
w. The overall information is calculated by 
summing the complementations of all words. 
4.3 Statistics and Discussion 
Figure 9 shows the variation of these overall in-
formation metrics on the CE document collection. 
It corroborates the characteristics analyzed in 
Section 3 and corresponds with the performance 
curves in Section 2.  
Figure 10 shows the proportion of interword 
bigrams at different dimensionalities, which also 
corresponds with the analysis in Section 3. 
0 2 4 6 8 10 12 14 16
x 10
4
0
2
4
6
8
10
12
14
16
x 10
7
dimensionality
o
v
e
ra
ll 
in
fo
rm
a
ti
o
n
 q
u
a
n
ti
ty
word  
bigram
 
Figure 9. Overall Information Quantity on CE 
The curves do not cross at exactly the same 
dimensionality as in the figures in Section 1, be-
cause other complications impact on the classifi-
cation performance: (a) OOV word identifying 
capability, as stated in Subsection 3.1; (b) word 
segmentation precision; (c) granularity of the 
categories (words have more definite semantic 
meaning than bigrams and lead to a better per-
formance for small category granularities); (d) 
noise terms, introduced in the feature space dur-
ing the increase of dimensionality. With these 
factors, the actual curves would not keep increas-
ing as they do in Figure 9. 
0 2 4 6 8 10 12 14 16
x 10
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
dimensionality
in
te
rw
o
rd
 b
ig
ra
m
 p
ro
p
o
rt
io
n
 
Figure 10. Interword Bigram Proportion on CE 
5 Conclusion 
In this paper, we aimed to thoroughly compare 
the value of words and bigrams as feature terms 
in text categorization, and make the implicit 
mechanism explicit. 
Experimental comparison showed that the Chi 
feature selection scheme and the tfidf term 
weighting scheme are still the best choices for 
(Chinese) text categorization on a SVM classifier. 
In most cases, the bigram scheme outperforms 
the word scheme at high dimensionalities and 
usually reaches its top performance at a dimen-
551
sionality of around 70000. The word scheme of-
ten outperforms the bigram scheme at low di-
mensionalities and reaches its top performance at 
a dimensionality of less than 40000. 
Whether the best performance of the word 
scheme is higher than the best performance 
scheme depends considerably on the word seg-
mentation precision and the number of categories. 
The word scheme performs better with a higher 
word segmentation precision and fewer (<10) 
categories. 
A word scheme costs more document indexing 
time than a bigram scheme does; however a bi-
gram scheme costs more training time and classi-
fication time than a word scheme does at the 
same performance level due to its higher dimen-
sionality. Considering that the document index-
ing is needed in both the training phase and the 
classification phase, a high precision word 
scheme is more time consuming as a whole than 
a bigram scheme. 
As a concluding suggestion: a word scheme is 
more fit for small-scale tasks (with no more than 
10 categories and no strict classification speed 
requirements) and needs a high precision word 
segmentation system; a bigram scheme is more 
fit for large-scale tasks (with dozens of catego-
ries or even more) without too strict training 
speed requirements (because a high dimensional-
ity and a large number of categories lead to a 
long training time). 
Reference 
Akiko Aizawa. 2000. The Feature Quantity: An In-
formation Theoretic Perspective of Tfidf-like 
Measures, Proceedings of ACM SIGIR 2000, 104-
111. 
Ricardo Baeza-Yates, Berthier Ribeiro-Neto. 1999. 
Modern Information Retrieval, Addison-Wesley 
Chih-Chung Chang, Chih-Jen Lin. 2001. LIBSVM: A 
Library for Support Vector Machines, Software 
available at http://www.csie.ntu.edu.tw/~cjlin/ 
libsvm 
Steve Deerwester, Sue T. Dumais, George W. Furnas, 
Richard Harshman. 1990. Indexing by Latent Se-
mantic Analysis, Journal of the American Society 
for Information Science, 41:391-407. 
Thorsten Joachims. 1997. A Probabilistic Analysis of 
the Rocchio Algorithm with TFIDF for Text Cate-
gorization, Proceedings of 14th International Con-
ference on Machine Learning (Nashville, TN, 
1997), 143-151. 
Thorsten Joachims. 1998. Text Categorization with 
Support Vector Machine: Learning with Many 
Relevant Features, Proceedings of the 10th Euro-
pean Conference on Machine Learning, 137-142. 
Mun-Kew Leong, Hong Zhou. 1998. Preliminary 
Qualitative Analysis of Segmented vs. Bigram In-
dexing in Chinese, The 6th Text Retrieval Confer-
ence (TREC-6), NIST Special Publication 500-240, 
551-557. 
Baoli Li, Yuzhong Chen, Xiaojing Bai, Shiwen Yu. 
2003. Experimental Study on Representing Units in 
Chinese Text Categorization, Proceedings of the 
4th International Conference on  Computational 
Linguistics and Intelligent Text Processing (CI-
CLing 2003), 602-614. 
Yuan Liu, Nanyuan Liang. 1986. Basic Engineering 
for Chinese Processing ? Contemporary Chinese 
Words Frequency Count, Journal of Chinese In-
formation Processing, 1(1):17-25. 
Robert W.P. Luk, K.L. Kwok. 1997. Comparing rep-
resentations in Chinese information retrieval. Pro-
ceedings of ACM SIGIR 1997, 34-41. 
Jianyun Nie, Fuji Ren. 1999. Chinese Information 
Retrieval: Using Characters or Words? Informa-
tion Processing and Management, 35:443-462. 
Jianyun Nie, Jianfeng Gao, Jian Zhang, Ming Zhou. 
2000. On the Use of Words and N-grams for Chi-
nese Information Retrieval, Proceedings of 5th In-
ternational Workshop on Information Retrieval 
with Asian Languages 
Monica Rogati, Yiming Yang. 2002. High-performing 
Feature Selection for Text Classification, Proceed-
ings of ACM Conference on Information and 
Knowledge Management 2002, 659-661. 
Gerard Salton, Christopher Buckley. 1988. Term 
Weighting Approaches in Automatic Text Retrieval, 
Information Processing and Management, 
24(5):513-523. 
Fabrizio Sebastiani. 2002. Machine Learning in 
Automated Text Categorization, ACM Computing 
Surveys, 34(1):1-47 
Dejun Xue, Maosong Sun. 2003a. Select Strong In-
formation Features to Improve Text Categorization 
Effectiveness, Journal of Intelligent Systems, Spe-
cial Issue. 
Dejun Xue, Maosong Sun. 2003b. A Study on Feature 
Weighting in Chinese Text Categorization, Pro-
ceedings of the 4th International Conference on  
Computational Linguistics and Intelligent Text 
Processing (CICLing 2003), 594-604. 
Vladimir Vapnik. 1995. The Nature of Statistical 
Learning Theory, Springer. 
Yiming Yang, Jan O. Pederson. 1997.  A Comparative 
Study on Feature Selection in Text Categorization, 
Proceedings of ICML 1997, 412-420. 
552
Two-Character Chinese Word Extraction Based on  
Hybrid of Internal and Contextual Measures 
Shengfen Luo,    Maosong Sun 
National Lab. of Intelligent Tech. & Systems
Tsinghua University, Beijing 100084, China 
lkc-dcs@mail.tsinghua.edu.cn 
 
 
Abstract 
Word extraction is one of the important 
tasks in text information processing. 
There are mainly two kinds of statistic-
based measures for word extraction: the 
internal measure and the contextual 
measure. This paper discusses these two 
kinds of measures for Chinese word 
extraction. First, nine widely adopted 
internal measures are tested and 
compared on individual basis. Then 
various schemes of combining these 
measures are tried so as to improve the 
performance. Finally, the left/right 
entropy is integrated to see the effect of 
contextual measures. Genetic algorithm is 
explored to automatically adjust the 
weights of combination and thresholds. 
Experiments focusing on two-character 
Chinese word extraction show a 
promising result: the F-measure of 
mutual information, the most powerful 
internal measure, is 57.82%, whereas the 
best combination scheme of internal 
measures achieves the F-measure of 
59.87%. With the integration of the 
contextual measure, the word extraction 
achieves the F-measure of 68.48% at last. 
1 Introduction 
New words are generated quite often with the 
rapid development of Chinese society, resulting 
that the lexicon of Chinese cannot well meet the 
requirement of natural language processing. How 
to extract word automatically from immense text 
collection has thus become an important problem. 
The task of extracting Chinese words with 
multi-characters from texts is quite similar to that 
of extracting phrases (e.g., compound nouns) in 
English, if we regard Chinese characters as English 
words. Research in word/phrase extraction has 
been carried out extensively. Currently the 
mainstream approach is statistic-based. In general, 
there are two kinds of statistic-based measures for 
estimating the soundness of an extracted item 
being a word/phrase: One is the internal measure, 
which estimates the soundness by the internal 
associative strength between constituents of the 
item. Nine widely adopted internal measures are 
listed in (Schone et al 2001), including Frequency, 
Mutual Information, Selectional Association, 
Symmetric Conditional Probability, Dice Formula, 
Log-likelihood, Chi-squared, Z-score, Student?s t-
score. The other kind is the contextual measure, 
which estimates the soundness by the dependency 
of the item on its context, such as the left/right 
entropy (Sornlertlamvanich et al 2000), and the 
left/right context dependency (Chien 1999). 
This paper firstly analyzes nine internal 
measures mentioned above, tests and compares 
their word extraction performance on individual 
basis, then tries to improve the performance by 
properly combining these measures. Furthermore, 
the contextual measure is integrated with internal 
measures to acquire more improvement. 
Throughout the experiments, genetic algorithm is 
explored to adjust weights of combination and 
thresholds automatically. We only concern two-
character word extraction in this paper, because 
two-character words reflect the most popular 
word-formation of Chinese and possess the largest 
proportion in Chinese lexicon.  
2 
2.1 
Internal Measures 
Nine internal measures are discussed and 
compared in this section. These measures tend to 
estimate the internal associative strength from 
different perspectives, so it is possible to improve 
the word extraction performance by properly 
combining them. This paper will try two 
combination schemes, i.e., direct combination and 
interval-based combination. 
As stated earlier, the evaluation is based on two-
character Chinese word extraction. PDR9596, a 
raw corpus of People Daily of 1995 and 1996 with 
about 50.0M characters, is used to train the matrix 
of Chinese character bigrams throughout the 
experiments. PDA98J, a manually word-
segmented corpus composed of People Daily of 
January 1998 with about 1.3M characters 
(developed by Institute of Computational 
Linguistics of Peking University), is further used 
to exhaustively generate a list of Chinese character 
bigrams. The list contains 218,863 distinct bigrams. 
We randomly divide the list into two parts, 9/10 of 
it as TS1, the rest 1/10 as TS2.  
Nine Widely Adopted Internal Measures 
Table 1 lists nine widely adopted internal 
measures, as mentioned in (Schone 2000). In the 
table: xy  represents any two-character item, x  
stands for all characters except x, is the size of 
training corpus,  and are frequency and 
probability of x respectively, and  are 
frequency and probability of 
N
xy
xf xp
f xyp
xy  respectively, and 
xy?  is the frequency expectation of xy  suppose  x 
and y are independent.  
Obviously: 
NffNppNp yxyxxyxy /===?  
 
Table 1. Nine Widely Adopted Internal Measures 
Measure Marked As Formula 
Frequency Freq xyf  
Mutual 
Information MI 
yx
xy
pp
p
2log  
Selectional Association SA ?
z
zyMIyzp
xyMIyxp
)()|(
)()|(
  
Symmetric Conditional Probability SCP 
yx
xy
pp
p 2
 
Dice Formula Dice 
yx
xy
ff
f
+
2
 
Log-likelihood LogL 
yxxy
y
f
yxyx
f
xyxy
f
yxyx
pppp
pppp
)()(
)(
log2?  
Chi-squared Chi 
))()()((
)( 2
yxxyyxxyyxxyyxxy
yxyxxyxy
ffffffff
ffffN
++++
?
 
Z-Score ZS )1( N
f
xyxy
xyxy
??
?
?
?
 
Student?s t-Score TS )1( Nff
f
xyxy
xyxy
?
??
 
Table 2. Comparison of Word Extraction Performance of Internal Measures (Open Test  on TS1) 
Top 17,333 Freq MI SA SCP Dice LogL Chi ZS TS 
F-measure (%) 26.28 54.77 42.98 51.77 49.37 43.13 52.97 53.20 39.12 
Comparison of F-measure:  MI > ZS > Chi > SCP >Dice > LogL > SA > TS > Freq  
Table 3. Weights for Direct Combination Scheme (Fitness Function is Based on TS1) 
Freq MI SA SCP Dice LogL Chi ZS TS 
0.000598 0.351393 0.000263 0.146348 0.214541 0.002804 0.035930 0.072293 0.17583
Comparison of weights:  MI > Dice > TS > SCP > ZS > Chi > LogL > Freq > SA 
When using these nine measures for word 
extraction, the hypothesis is same: the larger value 
of measure means the stronger associative 
strength between x and y, and thus the more 
possibility of xy  being a word. The criterion of 
judgment is very simple: xy  would be accepted 
as a word if its internal associative strength is 
larger than a given threshold.  
2.2 
2.3 
Word Extraction Performance of Each 
Internal Measure 
The performance of each internal measure is 
tested on TS1. TS1 contains 196,977 distinct 
bigrams, among which 17,333 are two-character 
words according to PDA98J. The procedure of 
word extraction is to sort the 196,977 candidate 
bigrams in descending order in terms of the value 
of  the measure to be tested, and then to select the 
top 17,333 bigrams as words. In this case, the 
precision rate, recall rate and F-measure are 
exactly the same. Table 2 shows the comparison 
of performances of these nine internal measures. 
Mutual information achieves the best performance 
with the F-measure of 54.77%.  
Direct Combination of Internal Measures 
The first combination scheme is to directly 
combine the nine measures with appropriate 
weights. The internal associative strength of an 
item xy  is estimated by: 
?
=
?=
9
1
))(()(
i
ii xyscorewtxyscore  
where  is the value of )(xyscorei xy
9
1
?
=
wt
 given by the 
i-th measure, and is the weight for the i-th 
measure accordingly (satisfying ). 
iwt
1=
i
i
The determination of weights is not 
straightforward due to the presence of 
combinatorial explosion (notice that  is real 
number). Genetic algorithm (Pan 1998) is 
explored to adjust the weights automatically, 
trying to find the optimal one. Let (wt
iwt
1, wt2,?,wt9) 
be a possible solution, we set the F-measure of 
word extraction on TS1 to be the fitness function, 
and set the size of population to be 25. We simply 
use the GenocopIII software (Michalewicz) to do 
the job.  
In a PIII650 PC, GenocopIII runs 12 hours, 
iterates 1,161 generation, and converges to a 
group of weights (as shown in Table 3). With this 
group of weights, the F-measure of word 
extraction on TS1 is 55.44%, improving only 
0.67% over the most powerful single measure MI 
(54.77%). Note this is not a pure open test, 
because the fitness function of genetic algorithm 
is based on TS1. 
2.4 Interval-based Combination of Internal 
Measures 
The experimental result in section 2.3 shows 
that it is not so effective to combine the nine 
measures directly. We try another combination 
scheme now, i.e., interval-based combination.  
2.4.1 The Idea 
The idea is as follows: for every measure 
mentioned above, we first discretize its value 
range into a number of intervals. Every interval of 
every measure is then assigned a corresponding 
probability that indicates the tendency of any item 
being regarded as a word if its value with respect 
to this measure falls into this interval. We name 
this kind of probability ?the interval probability?.  
The soundness of an item being a word would be 
the weighted sum of all of its interval probabilities 
over nine measures. 
We describe the idea in a more formal way. 
Suppose is the internal associative 
strength of any item 
)(xyscorei
xy  with respect to the i-th 
measure,  is its corresponding interval 
determined by the value of score ,  
is the interval probability of v , then the 
soundness of 
)(xyvi
)(xyi
)(xyi
)(xypvi
xy  being word, , will be 
given by:  
)(xypv
?
=
?=
9
1
))(()(
i
ii xypvwtxypv  
where is the weight for the i-th measure.  iwt
If  is larger than a threshold, )(xypv xy  
would be extracted out as a word.  
2.4.2 The Related Issues 
Three related issues need to be clarified. 
(1) How to discretize the range of a measure 
with continuous values?  
We use D-2, an entropy-based top-down 
algorithm (Catlett 1991) to discretize the value 
range by supervised learning. It adopts the 
information gain as the criterion to decide whether 
a given training set should be further partitioned 
or not. Given a set of examples S, the information 
gain caused by a cut point t  will be: 
)(
||
||
)(
||
||
)(),( 2
2
1
1 SEnt
S
S
SEnt
S
S
SEntStIG ??=
1 S
 
where Ent(S) is the entropy of S, and S  and  
are two subsets of S partitioned by the cut point t . 
2
It has been proved that the information gain 
obtains optimal discretization only on boundary 
points (Fayyad et al 1992, Elomaa et al 2000). So 
only boundary points need to be examined as 
potential cut points. Suppose T is the set of 
boundary points, the D-2 algorithm for 
discretizing set S  is: 
ALGORITHM DISCRETE (S,T)    
BEGIN 
Step1. For each  in T, calculate  t ),( StIG
Step2. Select ,  is  )),((maxarg0 StIGt
t
= S
partitioned into two subsets: ,  1S 2S
Step3. If stopping criteria are satisfied, 
Step4.    then DO NOT partition , Return S ? .   
//?  is an empty set 
Step5.    else     P1 = DISCRETE( , T1S 1) 
                         P2 = DISCRETE( ,T2S 2) 
      P = P1 + { } + P2, Return P.   0t
//P is the set of cut points for 
discretizing S 
END 
This algorithm only considers two stopping 
criteria. One is the minimal number of samples in 
an interval, the other is the minimum information 
gain. With this algorithm, we finally get a set of 
cut points each measure, which discretize the 
value range to variable-length intervals. 
(2) How to assign the interval probability to 
each interval? 
After discretization, training examples (i.e., a 
list of Chinese character bigrams) will be 
distributed to a certain interval according to its 
value of a given measure. Let  represent the j-th 
interval of the i-th measure, then pv , the 
interval probability of v , is defined as: 
ijv
)( ijv
ij
inwordsbeingbigrams
ij
ij
ij vinbigramsof
vof
vpv
#
#
)( =
)(xyi
   (3) How to set the weights for combining all 
in the process of word extraction?  pv
Genetic algorithm is again invoked to adjust the 
weight . The configuration of GenecopIII is 
the same as that in session 2.3. 
iwt
2.4.3 Effect of the Stopping Criteria and 
Discretization Strategy on Combination 
The stopping criteria and discretization strategy 
take effect on the word extraction performance of 
interval-based combination. 
First, have a look at the effect of stopping 
criteria. In DISCRETE, two stopping criteria are 
needed to set. We fix the minimal number of 
samples in one interval on 50 arbitrarily. And, we 
change the setting of the minimum information 
gain. In Table 4, performances under five different 
minimum information gains are compared, 
marked as D1, D2 ,?, D5 respectively. It can be 
seen that, the smaller the minimal information 
gain, the finer the granularity of discretization, 
and the better the performance of word extraction. 
But if the discretization is too grainy, it may cause 
over-fitting problem. Compared with D4 and D5, 
D3 achieves nearly the same performance but has 
a much rough discretization. So, we set the 
minimum information gain to be 0.0001 (D3) in 
the following experiments. 
Second, observe the effect of the discretization 
strategy. The equal-length discretization is 
compared to the variable-length discretization. We 
divide the value range of each measure into equal-
length intervals, and let the number of intervals be 
identical to that in D3 accordingly. As shown in 
Table 4, the equal-length discretization only 
achieves the F-measure of 55.56%, which is much 
less than D3 (57.45%). This means the entropy-
based discretization is more reasonable than 
equal-length discretization, and the discretization 
strategy has significant impact on the performance 
of interval-based combination. 
2.4.4 Reduction of Measures for Combination 
To improve the performance of word extraction 
through combination, the premise is that there 
must be enough mutual supplements among those 
measures. However, if the combination involves 
too many measures, interference may become 
obvious. We try to reduce the number of measures 
for combination. 
The reduction procedure is recursive: It first 
compares the performance after removing any of 
the n measures, then reduces the one that can 
bring the most improvement of performance if it 
is removed. Repeat this reduction procedure in the 
left n-1 measures, until the performance cannot 
improve anymore. 
Table 5 shows the reduction procedure of the 
nine internal measures. It indicates that, excluding 
SA and SCP, the interval-based combination of 
other seven measures could achieve the best F-
measure of 57.77%, with the weights in Table 6. 
That result is 3.00% higher than that of the most 
powerful internal measure MI (54.77%).  
Note again that all tests in section 2.4 are not 
pure open, because all the related parameters such 
as granularity of discretization, reduction of 
measures and adjustment of combination weights, 
are based on TS1. 
Table 4. Effect of the Stopping Criteria and Discretization Strategy (Based on TS1)  
Entropy-based  
Discretization Number of Partitions 
 Min Gain 
F-measure 
(%) 
Freq MI SA SCP Dice LogL Chi ZS TS 
D1 0.001 55.92 80 117 88 63 122 126 237 189 66 
D2 0.0005 56.75 104 341 230 96 255 314 380 343 625 
D3 0.0001 57.45 340 1449 641 109 471 1390 949 1411 1234
D4 0.00005 57.67 385 1660 693 113 543 1777 1316 1555 1808
D5 0.00001 57.69 423 2204 754 120 581 2522 2375 2233 2304
Equal-length 
Discretization 55.56 340 1449 641 109 471 1390 949 1411 1234
Table 5. The Reduction Procedure of the Nine Measures (Based on TS1) 
F-measure (%) after Removing N 
Freq MI SA SCP Dice LogL Chi Zs Ts 
Action 
9 57.62 55.09 57.63 57.69 57.48 57.65 57.64 57.50 57.37 Reduce SCP 
8 57.71 55.19 57.77  57.63 57.60 57.40 57.36 57.49 Reduce SA 
7 57.67 55.25   57.66 57.71 57.74 57.64 57.48 No Reduction 
Table 6. Weights for Interval-based Combination (Based on TS1) 
Freq MI SA SCP Dice LogL Chi ZS TS 
0.00034 0.47238 0 0 0.00238 0.00125 0.09339 0.25636 0.17390 
Comparison of weights:  MI > ZS > TS > Chi > Dice > Freq 
Table 7.  Open Test for Effect of Internal Measures, the Contextual Measures and the Hybrid (on TS2) 
 Precision(%) Recall(%) F-measure(%) Setting t1 and t2 for Left/Right Entropy 
MI 56.72 58.97 57.82 N.A. 
Comb 60.41 59.35 59.87 N.A. 
MI+Le/Re 83.53 54.88 66.24 MI-tuned threshold 
Comb+Le/Re* 85.69 55.76 67.56 MI-tuned threshold 
Comb+Le/Re 85.71 57.02 68.48 Comb-tuned threshold 
 
3 The Contextual Measure 
This section turns to discuss how to make use 
of contextual measures. The most commonly used 
contextual measure is the left/right entropy: 
?
??
??=
Aa
xyaxypxyaxypxyLe )|(log)|()( 2  
?
??
??=
Ab
xyxybpxyxybpxy )|(log)|()Re( 2  
where: xy  is the candidate item, a, b are Chinese 
characters belonging to A, the set of Chinese 
characters. 
In the sight of entropy, the larger the value of 
Le(xy) and Re(xy), the more various the characters 
coming after/before xy, and thus the more possible 
xy to be a word. 
4 The Hybrid  of Internal and Contextual 
Measures 
Combining the contextual measure with internal 
measures, the word extraction process would 
become like this: First, any candidate item xy  not 
satisfying the contextual condition is rejected. The 
contextual condition is, Le(xy)>t1 and Re(xy)>t2. 
Second, those residual candidates will be 
extracted out as words if their internal measure or 
combination of internal measures is high than a 
given threshold t3. In this paper, we try two 
alternatives of hybrid for comparison: one is the 
contextual measure with mutual information, the 
best single internal measure; another one is the 
contextual measure with Comb, the best result of 
interval-based combination of seven internal 
measures (Freq, MI, Dice, LogL, Chi, ZS, TS).  
We need to determine three thresholds in above 
process, Threshold t3 are set as the value to select 
the top 17,333 candidates from TS1: according to  
experiments in section 2, MI will choose t3=4.0, 
while Comb will choose t3=0.26. To set 
appropriate thresholds t1 and t2, we still employ 
genetic algorithm. We let a group of threshold (t1, 
t2) be a possible solution, and let the F-measure of 
word extraction on TS1 be fitness. Two groups of 
thresholds can be thus obtained: 
(1) MI-tuned thresholds: t1=2.2, t2=1.4. 
(2) Comb-tuned thresholds: t1=1.8, t2=1.2. 
To further investigate the effect of internal 
measures, the contextual measure and the hybrid, 
we conduct a series of open tests on TS2, as 
demonstrated in Table 7. Since the left/right 
entropy would become less reliable in cases that 
the occurrences of contexts are not sufficient, we 
drop out those candidates whose frequencies are 
no more than 5 in TS2. After dropping, TS2 
contains 14,867 candidates, out of which 1,589 
are words according to PDA98J. 
      In the first two rows of Table 7, the best single 
internal measure, MI, and our best combination of 
internal measures Comb are open tested. The 
successive three rows show the effect of 
contextual measures. The row of ?MI+Le/Re? 
selects MI as the internal measure, and use the 
MI-tuned thresholds as t1 and t2. The rows of 
?Comb+Le/Re*? and ?Comb+Le/Re? both select 
Comb as the internal measure, but use different t1 
and t2: The former uses MI-tuned thresholds, 
while the latter uses Comb-tuned thresholds. 
From Table 7, we can draw several conclusions: 
(1) With open test, the F-measure of MI, the best 
single internal measure, is 57.82%, whereas the F-
measure of our interval-based combination is 
59.87%; (2) The integration of the commonly 
used contextual measure, the left/right entropy 
with internal measures, can bring a large 
improvement of about 8%~9%; (3) There is only a 
modest difference between the performances of 
?Comb+Le/Re*? and ?Comb+Le/Re?, and two 
group of thresholds adjusted by different internal 
measures  have small difference as well. 
5 Conclusion 
    This paper focuses on the research of pure 
statistic-based measures for automatic extraction 
of two-character Chinese words. Two kinds of 
statistic-based measures are discussed: internal 
measures and contextual measures. Nine internal 
measures are tested and compared. Two schemes 
are tried to improve the performance by properly 
combining these nine measures. Experimental 
results in open tests show that, the best 
combination scheme, interval-based combination, 
achieves the F-measure of 59.87%, improving 
2.05% over the best single internal measure 
mutual information. On the other hand, the 
left/right entropy, a kind of contextual measure, is 
integrated to acquire further improvement in word 
extraction. With the left/right entropy and 
interval-based combination of internal measures, 
the F-measure ultimately achieves 68.48%. 
Another point of this paper is that, weights for 
combination and thresholds for left/right entropy 
are adjusted automatically by genetic algorithm, 
rather than manually. 
Future work will extend the proposed method 
to automatic extraction of multi-character Chinese 
words. Other useful information, such as lexicon 
and semantic resource, are expected to be 
included for consideration so as to further improve 
the performance. 
Acknowledgements  
This research is supported by the National 
?973? Plan of China under grant no G1998030507 
and NSFC under grant no 60083005. 
References 
Catlett. (1991) On changing continuous attributes into 
odered discrete attributes. In Proceedings of the 
European Working Session on Learning, Berlin, 
Germany. pp. 164-178 
Chien, L.F. (1999) Pat-tree-based adaptive keyphrase 
extraction for intelligent Chinese information 
retrieval. Information Processing and Management 
vol.35 pp.501-521 
Elomaa T., Rousu J., (2000) Generalizing boundary 
points. In Proceedings of the 17th National 
Conference on Artificial Intelligence, Menlo Park, 
CA. 
Fayyad, U., Irani, K., (1992) On the handling of 
continuous-valued attributes in decision tree 
generation. Machine Learning. Vol.(8) pp.87-102 
Pan, Z.J., (1998) Evolution Computing. Tsinghua 
University Press, Beijing. 
Sornlertlamvanich V., Potipiti T., Charoenporn T. 
(2000) Automatic corpus-based Thai word 
extraction with the C4.5 learning algorithm. In 
Proceedings of COLING 2000. 
Schone, P., Jurafsky D. (2001) Is knowledge-free 
induction of multiword unit dictionary headwords a 
solved problem? In proceedings of EMNLP 2001. 
Michalewicz, Z., Genocop III, available at:  
http://www.coe.uncc.edu/~gnazhiya/gchome.html 
 
 
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 179?186,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Word Segmentation Standard in Chinese, Japanese and Korean 
 
Key-Sun Choi 
KAIST 
Daejeon Korea 
kschoi@kaist.ac.kr 
Hitoshi Isahara 
NICT 
Kyoto Japan 
isahara@nict.go.jp 
Kyoko Kanzaki
NICT 
Kyoto Japan 
kanzaki@nict.go.jp
Hansaem Kim
National Inst. 
Korean Lang.
Seoul  Korea
thesis00@korea.kr
Seok Mun Pak 
Baekseok Univ. 
Cheonan Korea 
smpark@bu.ac.kr 
Maosong Sun 
Tsinghua Univ.
Beijing China 
sms@tsinghua.edu.cn
 
Abstract 
Word segmentation is a process to divide a 
sentence into meaningful units called ?word 
unit? [ISO/DIS 24614-1]. What is a word 
unit is judged by principles for its internal in-
tegrity and external use constraints. A word 
unit?s internal structure is bound by prin-
ciples of lexical integrity, unpredictability 
and so on in order to represent one syntacti-
cally meaningful unit. Principles for external 
use include language economy and frequency 
such that word units could be registered in a 
lexicon or any other storage for practical re-
duction of processing complexity for the fur-
ther syntactic processing after word segmen-
tation. Such principles for word segmentation 
are applied for Chinese, Japanese and Korean, 
and impacts of the standard are discussed. 
1 Introduction 
Word segmentation is the process of dividing of 
sentence into meaningful units. For example, 
?the White House? consists of three words but 
designates one concept for the President?s resi-
dence in USA. ?Pork? in English is translated 
into two words ?pig meat? in Chinese, Korean 
and Japanese. In Japanese and Korean, because 
an auxiliary verb must be followed by main verb, 
they will compose one word unit like ?tabetai? 
and ?meoggo sipda? (want to eat). So the word 
unit is defined by a meaningful unit that could be 
a candidate of lexicon or of any other type of 
storage (or expanded derived lexicon) that is use-
ful for the further syntactic processing. A word 
unit is more or less fixed and there is no syntactic 
interference in the inside of the word unit. In the 
practical sense, it is useful for the further syntac-
tic parsing because it is not decomposable by 
syntactic processing and also frequently occurred 
in corpora.  
There are a series of linguistic annotation 
standards in ISO: MAF (morpho-syntactic anno-
tation framework), SynAF (syntactic annotation 
framework), and others in ISO/TC37/SC4 1 . 
These standards describe annotation methods but 
not for the meaningful units of word segmenta-
tion. In this aspect, MAF and SynAF are to anno-
tate each linguistic layer horizontally in a stan-
dardized way for the further interoperability. 
Word segmentation standard would like to rec-
ommend what word units should be candidates to 
be registered in some storage or lexicon, and 
what type of word sequences called ?word unit? 
should be recognized before syntactic processing. 
In section 2, principles of word segmentation 
will be introduced based on ISO/CD 24614-1. 
Section 3 will describe the problems in word 
segmentation and what should be word units in 
each language of Chinese, Japanese and Korean. 
The conclusion will include what could be 
shared among three languages for word segmen-
tation. 
2 Word Segmentation: Framework and 
Principles 
Word unit is a layered pre-syntactical unit. That 
means that a word unit consists of the smaller 
word units. But the maximal word unit is fre-
quently occurred in corpora under the constraints 
that the syntactic processing will not refer the 
internal structure of the word unit 
Basic atoms of word unit are word form, mor-
pheme including bound morpheme, and non-
lexical items like punctuation mark, numeric 
string, foreign character string and others as 
shown in Figure 1. Usually we say that ?word? is 
lemma or word form. Word form is a form that a 
lexeme takes when used in a sentence. For ex-
ample, strings ?have?, ?has?, and ?having? are 
word forms of the lexeme HAVE, generally dis-
tinguished by the use of capital letters. [ISO/CD 
24614-1] Lemma is a conventional form used to 
represent a lexeme, and lexeme is an abstract 
unit generally associated with a set of word 
forms sharing a common meaning. 
                                                 
1 Refer to http://www.tc37sc4.org/ for documents 
MAF, SynAF and so on. 
179
 
Figure 1. Configuration of Word Unit 
 
BNF of word unit is as follows: 
<word unit> ::= <word form> | <morpheme> | 
<non lexical items> | <word unit>, 
where <word unit> is recursively defined be-
cause a longer word unit contains smaller word 
units. 
Bunsetsu in Japanese is the longest word unit, 
which is an example of layered maximized pre-
syntactical unit. Eojeol in Korean is a spacing 
unit that consists of one content word (noun, 
verb, adjective or adverb) plus a sequence of 
functional elements. Such language-specific 
word units will be described in section 3.  
Principles for word segmentation will set the 
criteria to validate each word unit, to recognize 
its internal structure and non-lexical word unit, to 
be a candidate of lexicon, and other consistency 
to be necessitated by practical applications for 
any text in any language. The meta model of 
word segmentation will be explained in the 
processing point of view, and then their prin-
ciples of word units in the following subsections.  
2.1 Metamodel of Word Segmentation 
A word unit has a practical unit that will be later 
used for syntactic processing. While the word 
segmentation is a process to identify the longest 
word unit and its internal structure such that the 
word unit is not the object to interfere any syn-
tactic operation, ?chunking? is to identify the 
constituents but does not specify their internal 
structure. Syntactic constituent has a syntactic 
role, but the word unit is a subset of syntactic 
constituent. For example, ?blue sky? could be a 
syntactic constituent but not a word unit. Figure 
2 shows the meta model of word segmentation. 
[ISO CD 24614-1]  
2.2 Principles of Word Unit Validation 
Principles for validating a word unit can be ex-
plained by two perspectives: one is linguistic one 
and the other is processing-oriented practical 
perspective.   
In ISO 24614-1, principles from a linguistic 
perspective, there are five principles: principles 
of (1) bound morpheme, (2) lexical integrity, (3) 
unpredictability, (4) idiomatization, and (5) un-
productivity.  
First, bound morpheme is something like ?in? 
of ? inefficient? . The principle of bound mor-
pheme says that each bound morpheme plus 
word will make another word. Second, principle 
of lexical integrity means that any syntactic 
processing cannot refer the internal structure of 
word (or word unit). From the principle, we can 
say that the longest word unit is the maximal 
meaningful unit before syntactic processing. 
Third, another criterion to recognize a word is 
the principle of unpredictability. If we cannot 
infer the real fact from the word, we consider it 
as a word unit. For example, we cannot image 
what is the colour of blackboard because some is 
green, not black. [ISO 24614-1] The fourth prin-
ciple is that the idiom should be one word, which 
could be a subsidiary principle that follows the 
principle of unpredictability. In the last principle, 
unproductivity is a stronger definition of word; 
there is no pattern to be copied to generate an-
other word from this word formation. For exam-
ple, in ????  (white vegetable) in Chinese, 
there is no other coloured vegetable like ?blue 
vegetable.? 
Another set of principles is derived from the 
practical perspective. There are four principles: 
frequency, Gestalt, prototype and language 
economy. Two principles of frequency and lan-
guage economy are to recognize the frequent 
occurrence in corpora. Gestalt and prototype 
principles are the terms in cognitive science 
about what are in our mental lexicon, and what 
are perceivable words.  
Principle of language economy is to say about 
very practical processing efficiency: ?if the in-
clusion of a word (unit) candidate into the lex-
icon can decrease the difficulty of linguistic 
analysis for it, then it is likely to be a word 
(unit).? 
Gestalt principle is to perceive as a whole. ?It 
supports some perceivable phrasal compounds 
into the lexicon even though they seem to be free 
combinations of their perceivable constituent 
parts,? [ISO/CD 24614-1] where the phrasal 
compound is frequently used linguistic unit and 
its meaning is predictable from its constituent 
elements. Similarly, principle of prototype pro-
180
vides a rationale for including some phrasal 
compounds into lexicon, and phrasal compounds 
serve as prototypes in a productive word forma-
tion pattern, like ?apple pie? with the pattern 
?fruit + pie? into the lexicon.  
 
 
Figure 2. Meta model of word segmentation proess 
[ISO/CD 24614-1] 
2.3 Principles for Word Unit Formation 
As a result of word segmentation of sentence, we 
will get word units. These principles will de-
scribe the internal structure of word unit. They 
have four principles: granularity, scope maximi-
zation of affixations, scope maximization of 
compounding, and segmentation for other strings. 
In the principle of granularity, a word unit has its 
internal structure, if necessary for various appli-
cation of word segmentation.  
Principles of scope maximization of affixa-
tions and compounding are to recognize the 
longest word unit as one word unit even if it is 
composed of stem + affixes or compound of 
word units. For example, ?unhappy? or ?happy? 
is one word unit respectively. ?Next generation 
Internet? is one word unit. The principle of seg-
mentation for other strings is to recognize non-
lexical strings as one word unit if it carries some 
syntactic function, for example, 2009 in ?Now 
this year is 2009.?  
3 Word Segmentation for Chinese, Jap-
anese and Korean 
If the word is derived from Chinese characters, 
three languages have common characteristics. If 
their word in noun consists of two or three Chi-
nese characters, they will be one word unit if 
they are ?tightly combined and steadily used.? 
Even if it is longer length, it will be a word unit 
if it is fixed expression or idiom. But if the first 
character is productive with the following num-
eral, unit word or measure word, it will be seg-
mented. If the last character is productive in a 
limited manner, it forms a word unit with the 
preceding word, for example, ????? (Tokyo 
Metropolis), ?8?? (August) or ????? (acce-
lerator). But if it is a productive suffix like plural 
suffix and noun for locality, it is segmented in-
dependently in Chinese word segmentation rule, 
for example, ???|?? (friends), ???|??? 
(north of the Yangtzi River ) or ???|?? (on 
the table) in Chinese. They may have different 
phenomena in each language. 
Negation character of verb and adjective is 
segmented independently in Chinese, but they 
form one word unit in Japanese. For example, 
?yasashikunai? (?????, not kind) is one 
word unit in Japanese, but ??|?? (not to write),  
??| ?? (cannot),  ??|??? (did not research) 
and ?? | ??? (not completed) will be seg-
mented independently in Chinese. In Korean, 
?chinjeolhaji anhta? (???? ??, not kind) 
has one space inserted between two eojeols but it 
could be one word unit. ?ji anhta? makes nega-
tion of adjectival stem ?chinjeolha?.  
We will carefully investigate what principles 
of word units will be applied and what kind of 
conflicts exists. Because the motivation of word 
segmentation standard is to recommend what 
word units should be registered in a type of lex-
icon (where it is not the lexicon in linguistics but 
any kind of practical indexed container for word 
units), it has two possibly conflicting principles. 
For example, principles of unproductivity, fre-
quency, and granularity could cause conflicts 
because they have different perspectives to de-
fine a word unit.  
3.1 Word Segmentation for Chinese 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into 13 types: noun, verb, adjective, 
pronoun, numeral, measure word, adverb, prepo-
sition, conjunction, auxiliary word, modal word, 
exclamation word, imitative word. 
3.1.1 Noun 
There is word unit specification for common 
nouns as follows: 
- Two-character word or compact two-character 
noun phrase, e.g., ??(beef) ??(steel) 
181
- Noun phrase with compact structure, if violate 
original meaning after segmentation, e.g., ??
?? (Active power) 
- Phrase consisting of adjective with noun, e.g., 
?? (green leave) 
- The meaning changed phrase consisting of ad-
jective, e.g., ???(young wife)  
- Prefix with noun, e.g., ??(elder brother) ?
? (old eagle) ??? (nonmetal)  ???
(ultrasonic) 
- Noun segmentation unit with following postfix 
(e.g. ? ? ? ? ? ? ? ? ?), e.g., ???
(scientist) 
- Noun segmentation unit with prefix and postfix, 
e.g., ???(superconductance) 
- Time noun or phrase, e.g., ??(May), 11 ? 
42 ? 8 ?(forty two minutes and eight seconds 
past eleven), ??(the day before yesterday), 
??(First day of a month in the Chinese lunar 
calendar )   
But the followings are independent word 
units for noun of locality (e.g., ??|? (on the 
table), ??|?? (north of the Yangtzi River)), 
and the ??? suffix referring to from a plural of 
front noun (e.g., ?? ?(friends) ) except ??
??, ?????(pals),  ?????(guys), etc. Prop-
er nouns have similar specification. 
3.1.2 Verb 
The following verb forms will be one word unit 
as: 
- Various forms of reiterative verb, e.g., ??
(look at), ????(come and go) 
- Verb?object structural word, or compact and 
stably used verb phrase, e.g., ??(meeting)  ?
?(dancing) 
- Verb?complement structural word (two-
character), or stably used Verb-complement 
phrase (two-character), e.g., ??(improve) 
- Adjective with noun word, and compact, and 
stably used adjective with noun phrase, e.g., ?
?(make trouble) ,  ??(talk nonsense) 
- Compound directional verb, e.g., ??(go out)  
??(come in). 
But the followings are independent word 
units: 
- ?AAB, ABAB? or ?A? A, A? A, A?? A?, 
e.g., ??|??(have a discuss), ?|?|? (have 
a good chat) 
- Verb delimited by a negative meaning charac-
ters, e.g., ?|?(not to write)   ?|?(cannot)    
?|??(did not research)    ?|??(not com-
pleted) 
- ?Verb + a negative meaning Chinese character 
+  the same verb" structure, e.g., ??|?|?(say 
or not say)?? 
- Incompact or verb?object structural phrase with 
many similar structures, e.g., ? |?(Eat fish)    
?|??(learn skiing) 
- ?2with1? or ?1with2? structural verb- comple-
ment phrase, e.g., ??|?(clean up), ?|??
(speak clearly),  ??|??(explain clearly) 
- Verb-complement word for phrase, if inserted 
with ?? or ??, e.g., ?|?|? (able to knock 
down), and compound directional verb of direc-
tion, e.g., ?|?|?(able to go out) 
- Phrase formed by verb with directional verb, 
e.g., ?|?(send), ?|?|?(run out) 
- Combination of independent single verbs with-
out conjunction, e.g., ?|?(cover with), ?|?,  
?|? (listen, speaking, read and write) 
- Multi-word verb without conjunction, e.g., ?
?|??(investigate and research) 
3.1.3 Adjective 
One word unit shall be recognized in the follow-
ing cases: 
- Adjective in reiterative form of ?AA, AABB, 
ABB, AAB, A+"?"+AB?, e.g., ??(big), ?
???(careless), except the adjectives in rei-
terative form of ?ABAB?, e.g., ?? |??
(snowy white)     
- Adjective phrase in from of ?? A? B??
A? B?? A? B?? A? B?? A? B?, 
e.g., ????(wholeheartedly) 
- Two single-character adjectives with word fea-
tures varied, ?? (long-short)  ?? (deep-
shallow)  ??(big-small) 
- Color adjective word or phrase, e.g., ??(light 
yellow)   ???(olive green) 
But the followings are segmented as indepen-
dent word units: 
- Adjectives in parataxis form and maintaining 
original adjective meaning, e.g., ? |???
(size), ?? |??(glory) 
- Adjective phrase in positive with negative form 
to indicate question, e.g., ??| ?| ??(easy 
or not easy), except the brachylogical one like 
????(easy or not). 
3.1.4 Pronoun 
The followings shall be one word unit: 
182
- Single-character pronoun with ???, e.g.,?? 
(we) 
- ??????? with unit word ??? or ???
????????, e.g., ??(this) 
- Interrogative adjective or phrase, e.g., ??
(how many) 
But, the following will be independent word 
units: 
- ???????  with numeral , unit word or 
noun word segmentation unit, e.g., ? |? ?
(these 10 days), ?| ?(that person) 
- Pronoun of ???????????????, 
etc. shall be segmented from followed measure 
word or noun, e.g., ?| ? (each country), ?| 
?(each type). 
3.1.5 Numeral 
The followings will be one word unit: 
- Chinese digit word, e.g., ?????????
???(180,040,723) 
- ???? percent in fractional number, e.g., ? 
???(third fifth) 
- Paratactic numerals indicating approximate 
number, e.g., ?? ??(eight or nine kg) 
On the other hand, Independent word unit cas-
es are as follows: 
- Numeral shall be segmented from measure 
word, e.g., ?| ?(three) 
- Ordinal number of ???  shall be segmented 
from followed numeral, e.g., ? ? (first) 
- ?????????????, used after adjec-
tive or verb for indicating approximate number. 
3.1.6 Measure word 
Reiterative measure word and compound meas-
ure word or phrase is a word unit, e.g., ??
(every year), ?? man/year. 
3.1.7 Adverb 
Adverb is one word unit. But????????
???, etc. acting as conjunction shall be seg-
mented, e.g., ? ? ? ?(sweet yet savory). 
3.1.8 Preposition 
It is one word unit. For example, ??(be born 
in), and  ????(according to the regulations). 
3.1.9 Conjunction 
Conjunction shall be deemed as segmentation 
unit. 
3.1.10 Auxiliary word 
Structural auxiliary word ????????? 
and tense auxiliary word ??????? are one 
word unit, e.g., ? |? |?  (his book), ? |?
(watched). But the auxiliary word ??? shall be 
segmented from its followed verb, e.g., ?  ?
(what one thinks).  
3.1.11 Modal word 
It is one word unit, e.g., ???? (How are 
you?). 
3.1.12 Exclamation word 
Exclamation word shall be deemed as segmenta-
tion unit. For example, ??????? (How 
beautiful it is!) 
3.1.13 Imitative word 
It is one word unit like ???? (tinkle).  
3.2 Word Segmentation for Japanese 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into mainly 10 types: meishi (noun), 
doushi (verb), keiyoushi (adjective), rentaishi 
(adnominal noun: only used in adnominal usage), 
fukushi (adverb), kandoushi (exclamation), set-
suzoushi (conjunction), setsuji (affix), joshi (par-
ticle), and jodoushi (auxiliary verb). These parts 
of speech are divided into more detailed classes 
in terms of grammatical function. 
The longest "word segmentation" corresponds 
to ?Bunsetsu? in Japanese. 
3.2.1 Noun 
When a noun is a member constituting a sentence, 
it is usually followed by a particle or auxiliary 
verb (e.g. ???? (neko_ga) which is composed 
from ?Noun + a particle for subject marker?). 
Also, if a word like an adjective or adnominal 
noun modifies a noun, then a modifier (adjec-
tives, adnominal noun, adnominal phrases) and a 
modificand (a noun) are not segmented. 
3.2.2 Verb 
A Japanese verb has an inflectional ending. The 
ending of a verb changes depending on whether 
it is a negation form, an adverbial form, a base 
form, an adnominal form, an assumption form, or 
an imperative form. Japanese verbs are often 
used with auxiliary verbs and/or particles, and a 
verb with auxiliary verbs and/or particles is con-
sidered as a word segmentation unit (e.g. ???
??? ? (aruki_mashi_ta) is composed from 
?Verb + auxiliary verb for politeness + auxiliary 
verb for tense?). 
3.2.3 Adjective 
A Japanese adjective has an inflectional ending. 
Based on the type of inflectional ending, there 
183
are two kinds of adjectives, "i_keiyoushi" and 
"na_keiyoushi". However, both are treated as 
adjectives. 
In terms of traditional Japanese linguistics, 
?keiyoushi? refers to ?i_keiyoushi?(e.g. ???, 
utsukushi_i) and ?keiyoudoushi?(e.g. ??? , 
shizuka_na) refers to ?na_keiyoushi.? In terms of 
inflectional ending of ?na_keiyoushi,? it is some-
times said to be considered as ?Noun + auxiliary 
verb (da)?. 
The ending of an adjective changes depending 
on whether it is a negation form, an adverbial 
form, a base form, an adnominal form, or an as-
sumption form. Japanese adjectives in predica-
tive position are sometimes used with auxiliary 
verbs and/or particles, and they are considered as 
a word segmentation unit. 
3.2.4 Adnominal noun 
An adnominal noun does not have an inflectional 
ending; it is always used as a modifier. An ad-
nominal noun is considered as one segmentation 
unit. 
3.2.5 Adverb 
An adverb does not have an inflectional ending; 
it is always used as a modifier of a sentence or a 
verb. It is considered as one segmentation unit. 
3.2.6 Conjunction 
A conjunction is considered as one segmentation 
unit. 
3.2.7 Exclamation 
An exclamation is considered as one segmenta-
tion unit. 
3.2.8 Affix 
A prefix and a suffix used as a constituent of a 
word should not be segmented as a word unit. 
3.2.9 Particle 
Particles can be divided into two main types. 
One is a case particle which serves as a case 
marker. The other is an auxiliary particle which 
appears at the end of a phrase or a sentence. 
Auxiliary particles represent a mood and a 
tense. 
Particles should not be segmented from a word. 
A particle is always used with a word like a noun, 
a verb, or an adjective, and they are considered 
as one segmentation unit. 
3.2.10 Auxiliary verb 
Auxiliary verbs represent various semantic func-
tions such as a capability, a voice, a tense, an 
aspect and so on. An auxiliary verb appears at 
the end of a phrase or a sentence. Some linguist 
consider ??? (da), which is Japanese copura, as 
a specific category such as ???(hanteishi).   
An auxiliary verb should not be segmented 
from a word. An auxiliary verb is always used 
with a word like a noun, a verb, or an adjective, 
and is considered as one segmentation unit. 
3.2.11 Idiom and proverb 
Proverbs, mottos, etc. should be segmented if 
their original meanings are not violated after 
segmentation. For example: 
Kouin yano  gotoshi 
noun  noun+particle auxiliary verb 
time  arrow  like (flying) 
Time flies fast. 
3.2.12 Abbreviation 
An abbreviation should not be segmented. 
3.3 Word Segmentation for Korean 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into 12 types: noun, verb, adjective, 
pronoun, numeral, adnominal, adverb, exclama-
tion, particle, auxiliary verb, auxiliary adjective, 
and copula. The basic parts of speech can be di-
vided into more detailed classes in terms of 
grammatical function. Classification in this paper 
is in accord with the top level of MAF. 
In addition, we treat some multi-Eojeol units 
as the word unit for practical purpose. Korean 
Eojeol is a spacing unit that consists of one con-
tent word (like noun, verb) and series of func-
tional elements (particles, word endings). Func-
tional elements are not indispensable. Eojeol is 
similar with Bunsetsu from some points, but an 
Eojeol is recognized by white space in order to 
enhance the readability that enables to use only 
Hangul alphabet in the usual writing.  
3.3.1 Noun 
When a noun is a member constituting a sentence, 
it is usually followed by a particle. (e.g. 
???_?? (saja_ga, ?a lion is?) which is com-
posed from ?Noun + a particle for subject mark-
er?). Noun subsumes common noun, proper noun, 
and bound noun.  
If there are two frequently concatenated Eoje-
ols that consist of modifier (an adjective or an 
adnominal) and modificand (a noun), they can be 
one word unit according to the principle of lan-
guage economy. Other cases of noun word unit 
are as follows: 
1) Compound noun that consists of two more 
nouns can be a word unit. For example, 
184
???? (son_mok, ?wrist?) where son+mok 
= ?hand?+?neck?. 
2) Noun phrase that denotes just one concept 
can be a word unit. For example, ???? 
??? (yesul_ui jeondang, ?sanctuary of the 
arts? that is used for the name of concert 
hall). 
3.3.2 Verb  
A Korean verb has over one inflectional endings. 
The endings of a verb can be changed and at-
tached depending on grammatical function of 
verb (e.g. ??/??/?/?/?/?/?? (break 
[+emphasis] [+polite] [+past] [+conjectural] final 
ending [+polite]). Compound verb (verb+verb, 
noun+verb, adverb+verb) can be a segmentation 
unit by right. For example, ?????? (dola-
ga-da, ?pass away?) is literally translated into 
?go+back? (verb+verb).  ????? (bin-na-da, 
?be shiny?) is derived from ?light + come out? 
(noun+verb). ?????? (baro-jap-da, ?cor-
rect?) is one word unit but it consists of 
?rightly+hold? (adverb+verb).  
3.3.3 Adjective 
A Korean adjective has inflectional endings like 
verb. For example, in ???/?/?/?/?/?? 
(pretty [+polite] [+past] [+conjectural] final end-
ing [+polite]), one adjective has five endings. 
Compound adjective can be a segmentation unit 
by right. (e.g. ????? (geom-buk-da, ?be 
blackish red?)) 
3.3.4 Adnominal  
An adnominal is always used as a modifier for 
noun. Korean adnominal is not treated as noun 
unlike Japanese one. (e.g. ?? ?? (sae jip, ?new 
house?)? which consist of ?adnominal + noun?). 
3.3.5 Adverb 
An adverb does not have an inflectional ending; 
it is always used as a modifier of a sentence or a 
verb. In Korean, adverb includes conjunction. It 
is considered as one segmentation unit. Com-
pound adverb can be a segmentation unit by right. 
Examples are ???? (bam-nat, ?day and night?), 
and ???? (gotgot, ?everywhere? whose literal 
translation is ?where where?). 
3.3.6 Pronoun 
A pronoun is considered as one segmentation 
unit. Typical examples are ??? (na, ?I?), ??? 
(neo, ?you?), and ???? (uri, ?we). Suffix of 
plural ??? (deul, ?-s?) can be attached to some 
of pronouns in Korean. (e.g. ????? (neohui-
deul, ?you+PLURAL?), ???? (geu-deul, ?they? 
= ?he/she+PLURAL?)). 
3.3.7 Numeral 
A numeral is considered as one segmentation 
unit: e.g. ???? (hana, ?one?), ???? (cheojjae, 
?first?). Also figures are treated as one unit like 
?2009?? (the year 2009). 
3.3.8 Exclamation 
An exclamation is considered as one segmenta-
tion unit. 
3.3.9 Particle 
Korean particles can not be segmented from a 
word just like Japanese particles. A particle is 
always used with a word like a noun, a verb, or 
an adjective, but it is considered as one segmen-
tation unit. 
Particles can be divided into two main types. 
One is a case particle that serves as a case marker. 
The other is an auxiliary particle that appears at 
the end of a phrase or a sentence. Auxiliary par-
ticle represents a mood and a tense. 
3.3.10 Auxiliary verb 
A Korean auxiliary verb represents various se-
mantic functions such as a capability, a voice, a 
tense, an aspect and so on.  
Auxiliary verb is only used with a verb plus 
endings with special word ending depending on 
the auxiliary verb. For example, ???? (boda, 
?try to?), an auxiliary verb has the same inflec-
tional endings but it should follow a main verb 
with a connective ending ??? (eo) or ??? (?go?).  
Consider ?try to eat? in English where ?eat? is a 
main verb, and ?try? is an auxiliary verb with 
specialized connective ?to?. In this case, we need 
two Korean Eojeols that corresponds to ?eat + 
to? and ?try?. Because ?to? is a functional ele-
ment that is attached after main verb ?eat?, it 
constitutes one Eojeol. It causes a conflict be-
tween Eojeol and word unit. That means every 
Eojeol cannot be a word unit. What are the word 
units and Eojeols in this case? There are two 
choices: (1) ?eat+to? and ?try?, (2) ?eat?+ ?to 
try?. According to the definition of Eojeol, (1) is 
correct for two concatenated Eojeols. But if the 
syntactic processing is preferable, (2) is more 
likely to be a candidate of word units.  
3.3.11 Auxiliary adjective 
Unlike Japanese, there is auxiliary adjective in 
Korean. Function and usage of it are very similar 
to auxiliary verb. Auxiliary adjective is consi-
dered as one segmentation unit. 
185
Auxiliary verb can be used with a verb or ad-
jective plus endings with special word ending 
depending on the auxiliary adjective. For exam-
ple, in ??? ??? (meokgo sipda, ?want to 
eat?), sipda is an auxiliary adjective whose mean-
ing is ?want? while meok is a main verb ?want? 
and go corresponds to ?to?; so meokgo is ?to eat?.  
3.3.12 Copula 
A copula is always used for changing the func-
tion of noun. After attaching the copula, noun 
can be used like verb. It can be segmented for 
processing. 
3.3.13 Idiom and proverb 
Proverbs, mottos, etc. should be segmented if 
their original meanings are not violated after 
segmentation like Chinese and Japanese. 
3.3.14 Ending 
Ending is attached to the root of verb and adjec-
tive. It means honorific, tense, aspect, modal, etc. 
There are two endings: prefinal ending and fi-
nal ending. They are functional elements to 
represent honorific, past, or future functions in 
prefinal position, and declarative (-da) or con-
cessive (-na)-functions in final ending. Ending is 
not a segmentation unit in Korean. It is just a unit 
for inflection. 
3.3.15 Affix 
A prefix and a suffix used as a constituent of a 
word should not be segmented as a word unit. 
4 Conclusion 
Word segmentation standard is to recommend 
what type of word sequences should be identified 
as one word unit in order to process the syntactic 
parsing. Principles of word segmentation want to 
provide the measure of such word units. But 
principles of frequency and language economy 
are based on a statistical measure, which will be 
decided by some practical purpose.  
Word segmentation in each language is 
somewhat different according to already made 
word segmentation regulation, even violating one 
or more principles of word segmentation. In the 
future, we have to discuss the more synchronized 
word unit concept because we now live in a mul-
ti-lingual environment. For example, consider 
figure 3. Its English literal translation is ?white 
vegetable and pig meat?, where ?white vegeta-
ble? (??) is an unproductive word pattern and 
forms one word unit without component word 
units, and ?pig meat? in Chinese means one Eng-
lish word ?pork?. So ?pig meat? (??) is the 
longest word unit in this case. But in Japanese 
and Korean, ?pig meat? in Chinese characters 
cannot be two word units, because each compo-
nent character is not used independently. 
 
Figure 3. Basic segmentation and word segmenta-
tion [ISO/CD 24614-1] 
What could be shared among three languages 
for word segmentation? The common things are 
not so much among CJK. The Chinese character 
derived nouns are sharable for its word unit 
structure, but not the whole. On the other hand, 
there are many common things between Korean 
and Japanese. Some Korean word endings and 
Japanese auxiliary verbs have the same functions. 
It will be an interesting study to compare for the 
processing purpose.  
The future work will include the role of word 
unit in machine translation. If the corresponding 
word sequences have one word unit in one lan-
guage, it is one symptom to recognize one word 
unit in other languages. It could be ?principle of 
multi-lingual alignment.?   
The concept of ?word unit? is to broaden the 
view about what could be registered in lexicon of 
natural language processing purpose, without 
much linguistic representation. In the result, we 
would like to promote such language resource 
sharing in public, not just dictionaries of words 
in usual manner but of word units. 
Acknowledgement 
This work has been supported by ISO/TC37, 
KATS and Ministry of Knowledge Economy 
(ROKorea), CNIS and SAC (China), JISC (Ja-
pan) and CSK (DPRK) with special contribution 
of Jeniffer DeCamp (ANSI) and Kiyong Lee. 
References 
ISO CD24614-1, Language Resource Management ? 
Word segmentation of written texts for monolin-
gual and multilingual information processing ? Part 
1: Basic concepts and general principles, 2009.  
ISO WD24614-2, ? Part 2: Word segmentation for 
Chinese, Japanese and Korean, 2009. 
186
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1011?1019,
Beijing, August 2010
Explore the Structure of Social Tags by Subsumption Relations
Xiance Si, Zhiyuan Liu, Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University
{sixiance,lzy.thu}@gmail.com, sms@tsinghua.edu.cn
Abstract
Thanks to its simplicity, social tagging
system has accumulated huge amount of
user contributed tags. However, user
contributed tags lack explicit hierarchi-
cal structure, while many tag-based ap-
plications would benefit if such a struc-
ture presents. In this work, we explore
the structure of tags with a directed and
easy-to-evaluate relation, named as the
subsumption relation. We propose three
methods to discover the subsumption rela-
tion between tags. Specifically, the tagged
document?s content is used to find the re-
lations, which leads to better result. Be-
sides relation discovery, we also propose
a greedy algorithm to eliminate the re-
dundant relations by constructing a Lay-
ered Directed Acyclic Graph (Layered-
DAG) of tags. We perform quantita-
tive evaluations on two real world data
sets. The results show that our methods
outperform hierarchical clustering-based
approach. Empirical study of the con-
structed Layered-DAG and error analysis
are also provided.
1 Introduction
In this work, we aim at exploring the structure of
social tags. Social tagging is widely used in Web-
based services, in which a user could use any word
to annotate an object. Thanks to its simplicity, ser-
vices with social tagging features have attracted a
lot of users and have accumulated huge amount of
annotations. However, comparing to taxonomies,
social tagging has an inherent shortcoming, that
Figure 1: Examples of (a) flat tag cloud, (b) hier-
archical clusters, and (c) subsumption relations.
there is no explicit hierarchical relations between
tags. Figure 1 (a) shows an example of the com-
monly used flat tag cloud, in which only the pop-
ularity of a tag is concerned. Kome et al (2005)
argued that implicit hierarchical relations exist in
social tags. Previous literature shows that orga-
nizing tags in hierarchical structures will help tag-
based Information Retrieval applications (Begel-
man et al, 2006; Brooks and Montanez, 2006).
Hierarchical clustering could reveal the simi-
larity relations of tags. Figure 1 (b) shows an
example of a typical hierarchical clustering of
tags. While clusters can capture similarity be-
tween tags, problems still remain: First, clusters
mix different relations, such as synonyms and hy-
pernyms. Second, clusters also ignore the direc-
tion of relations, for example, the direction in
browser ? firefox. Third, it is hard to evalu-
ate the correctness of clustering. Specifically, it
is hard to tell if two tags are similar or not. In
practice, directed and easy-to-evaluate relations
between tags are preferred, such as Figure 1 (c).
In this work, we explore the structure of so-
cial tags by discovering a directed and easy-to-
evaluate relation between tags, named subsump-
tion relation. A tag ta subsumes tb, if and only
if wherever tb is used, we can also replace it
1011
with ta. Unlike similar-to, subsumption relation
is asymmetric, and its correctness is easier to as-
sess. Then, we propose three ways to discover the
subsumption relations, through tag-tag, tag-word
and tag-reason co-occurrences respectively. In the
third way, A tag?s reason is defined as the word
in the content that explains the using of the tag.
We employ the Tag Allocation Model (TAM) pro-
posed by Si et al (2010) to find the reason for
each tag. Besides subsumption relation discov-
ery, we also propose a greedy algorithm to remove
the redundant relations. The removal is done by
constructing a Layered Directed Acyclic Graph
(Layered-DAG) of tags with the subsumption re-
lations.
We carried out the experiments on two real
world data sets. The results of quantitative evalu-
ation showed that tag-reason based approach out-
performed other two methods and a commonly
used hierarchical clustering-based method. We
also do empirical study on the output of Layered-
DAG construction.
The contribution of this paper can be summa-
rized as follows:
1. We explore the structure of social tags by
a clearly defined subsumption relation. We
propose methods to discover the subsump-
tion relation automatically, leveraging both
the co-occurred tags and the content of an-
notated document.
2. We propose an algorithm to eliminate the re-
dundant relations by constructing a Layered-
DAG of tags.
3. We perform both empirical and quantitative
evaluation of proposed methods on two real
world data sets.
The rest of the paper is organized as follows:
Section 2 surveys the related work; Section 3 de-
fines the subsumption relation we used, and pro-
poses methods for relation discovery; Section 4
proposes a greedy algorithm for Layered-DAG
construction; Section 5 explains the experimen-
tal settings and shows the evaluation results. Sec-
tion 6 concludes the paper.
2 Related Work
To explore the hierarchical relations between tags,
an intuitive way is to cluster the tags into hier-
archical clusters. Wu et al (2006b) used a fac-
torized model, namely Latent Semantic Analy-
sis, to group tags into non-hierarchical topics for
better recommendation. Brooks et al (2006) ar-
gued that performing Hierarchical Agglomerative
Clustering (HAC) on tags can improve the col-
laborative tagging system. Later, HAC on tags
was also used for improving personalized recom-
mendation (Shepitsen et al, 2008). Heymann et
al. (2006) clustered tags into a tree by a similarity-
based greedy tree-growing method. They evalu-
ated the obtained trees empirically, and reported
that the method is simple yet powerful for orga-
nizing tags with hierarchies. Based on Heymann
et al?s work, Schwarzkopf et al (2007) proposed
an approach for modeling users with the hierarchy
of tags. Begelman et al (2006) used top-down hi-
erarchical clustering, instead of bottom-up HAC,
to organize tags, and argued that tag hierarchies
improve user experiences in their system. Most
of the hierarchical clustering algorithms rely on
the symmetric similarity between tags, while the
discovered relations are hard to evaluate quantita-
tively, since one cannot distinguish similar from
not-similar with a clear boundary.
People have also worked on bridging social tag-
ging systems and ontologies. An ontology defines
relations between entities. Peter Mika (2005) pro-
posed an extended scheme of social tagging that
includes actors, concepts and objects, and used
tag co-occurrences to construct an ontology from
social tags. Wu et al (2006a) used hierarchical
clustering to build ontology from tags that also
use similar-to relations. Later, ontology schemes
that fits social tagging system were proposed, such
as (Van Damme et al, 2007) and (Echarte et
al., 2007), which mainly focused on the relation
between tags, objects and users, rather than be-
tween tags themselves. Alexandre Passant (2007)
mapped tags to domain ontologies manually to
improve information retrieval in social media. To
construct tag ontology automatically, Angeletou
et al (2007) used ontologies built by domain ex-
perts to find relations between tags, but observed
a very low coverage. Specia et al (2007) pro-
posed an integrated framework for organizing tags
by existing ontologies, but no experiment was per-
formed. Kim et al (2008) summarized the state-
1012
of-the-art methods to model tags with semantic
annotations.
Before social tagging was invented, Sanderson
et al (1999) proposed to use subsumption relation
to organize words in text hierarchically. Schmitz
et al (2006) followed the idea to use subsumption
relation for organizing Flickr 1 tag, where tag-tag
co-occurrences are used for discover the relations.
We follow the idea of subsumption relation in this
paper, and explore alternative ways for relation
discovery.
3 Subsumption Relations in Tags
In this section, we define the subsumption relation
used in our study, and propose three methods to
discover the subsumption relations.
3.1 Definitions
First, we introduce the symbols used through out
the paper: A tag is denoted as t ? T , where T is
the set of all tags. To distinguish from words, we
use fixed-width to represent the example tags.
An annotated document is denoted as d ? D,
where D is the set of all documents. The words
in d are denoted as a set {wdi}, where i ? [1, |d|],
and |d| is the number of words in d.
Inspired by (Sanderson and Croft, 1999), we
define the subsumption relation between ta and tb
as follows: ta subsumes tb, means that wherever
the tag tb is used, ta can also be used without
ambiguity. The subsumption relation between ta
and tb is denoted as ta ?s tb.
Subsumption relation is directional, that is,
ta ?s tb does not imply tb ?s ta. For ex-
ample, literature ?s chineseliterature,
since for any document annotated with
chineseliterature, we can also annotate
it with literature. However, if we swapped the
two tags, the statement would not hold.
Subsumption relation is more strict than simi-
larity. For example, during the time of Haiti earth-
quake, the tag earthquake is close to haiti in
similarity, but none of them implies the use of the
other one: document annotated with earthquake
may refer to the earthquake in China, while docu-
1http://www.flickr.com. An image sharing site that allows
users to annotate images with tags
ment annotated with haiti may mean the travel-
ing experience in Haiti.
Note that the subsumption has transitivity prop-
erty, that ta ?s tb and tb ?s tc means ta ?s
tc, which corresponds to our intuition. For in-
stance, naturaldisaster ?s earthquake and
disaster?snaturaldisaster means disaster
?searthquake.
3.2 Discover Subsumption Relation
We discover the subsumption relations by estimat-
ing the probability p(ta|tb). The motivation is, if
ta ?s tb and tb is used, it would be more likely to
see ta. So, by sorting all (ta, tb) pairs by p(ta|tb)
in descending order, top-ranked pairs are more
likely to have subsumption relations.
In this work, we present three methods to esti-
mate the probability p(ta|tb), using tag-tag, tag-
word and tag-reason co-occurrences respectively.
By using tag-word and tag-reason co-occurrences,
we leverage the content of the annotated docu-
ment for subsumption relation discovery.
3.2.1 Tag-Tag Co-occurrences Approach
The most intuitive way to estimate p(ta|tb) is
via tag-tag co-occurrences. Specifically, we use
the following formula:
p(ta|tb) =
Nd(ta, tb)
Nd(tb)
, (1)
where Nd(ta, tb) is the number of documents that
are annotated by both ta and tb, and Nd(tb) is the
number of documents annotated by tb. We de-
note the tag-tag co-occurrences approach as TAG-
TAG.
The use of TAG-TAG can be found in previous
literature for organizing tags for photos(Schmitz,
2006). One of TAG-TAG?s benefits is that it does
not rely on the content of the annotated document,
thus it can be applied to tags for non-text objects,
such as images and music. However, when com-
ing to text documents, this benefit is also a short-
coming, that TAG-TAG makes no use of the con-
tent when it is available.
Using TAG-TAG for subsumption relation dis-
covery relies on an implication, that if a user has
annotated d with tb, he would also annotate all
tags that subsumes tb. The implication may not
always hold in real world situations. For example,
1013
a novel reader would use tags such as scifi and
mystery to organize his collections, but he is not
likely to annotate each of his collection as novel
or book, since they are too obvious for him. We
name the problem as the omitted-tag problem.
3.2.2 Tag-Word Co-occurrences Approach
When the content of the annotated document
is available, using it for estimating p(ta|tb) is a
natural thought. The content is expected to be
complete and information-rich whether or not the
user has omitted any tags. We use the follow-
ing formula to estimate p(ta|tb) by tag-word co-
occurrences:
p(ta|tb) =
?
w?W
p(ta|w)p(w|tb)
=
?
w?W
Nd(ta, w)
Nd(w)
Nd(tb, w)
Nd(tb)
, (2)
where Nd(ta, w) is the number of documents that
contains both tag ta and word w, and Nd(w) is
the number of documents that contains the word
w. We denote this approach as TAG-WORD.
Instead of computing tag-tag co-occurrences
directly, TAG-WORD uses words in the document
as a bridge to estimate p(ta|tb). By introduc-
ing words, the estimation is less affected by the
omitted-tag problem, Take the novel reader exam-
ple again: Although he does not use the tag novel
too often, the words in book descriptions would
suggest the using of novel, according to all other
documents annotated by novel.
While using the content may weaken the
omitted-tag problem, it also brings the noise in
text to the estimation. Not every word in the con-
tent is related to one of the tags. To the oppo-
site, most words are functional words or that about
other aspects of the document. p(ta|tb) estimated
by using all words may largely depends on these
irrelevant words.
3.2.3 Tag-Reason Co-occurrences Approach
To focus on the words that are highly relevant
to the interested tags, we propose the third method
that uses tag-reason co-occurrences. The reason is
defined as the word(s) that can explain the using
of a tag in the document. For example, the tag
scifi for a book could be explained by the words
?robot?, ?Asimov? in the book description. If the
reason of each tag could be identified, the noise in
content-based p(ta|tb) could be reduced.
Si et al (2010) proposed a probabilistic model
for content-based social tags, named Tag Allo-
cation Model (TAM). TAM introduces a latent
variable r for each tag in the data set, known
as the reason variable. The value of r can be a
word in the corresponding document, or a global
noise variable ?. Allowing the reason of tags to
be a global noise makes TAM deal with content-
irrelevant tags and mistakenly annotated tags ef-
fectively. The likelihood that a document d is an-
notated by tag t is given as:
p(t|d) =
?
w?d
p(t|r = w)p(r = w|d)p(s = 0)
+ p(t|?)p(r = ?)p(s = 1), (3)
where r is the reason of the tag t, r ? {wdi|i ?
[0, |d|]} ? {?}, ? is the global noise variable. s is
the source of reason t, s = 0 means the source is
the content of the document, while s = 1 means
the source is the global noise variable ?. TAM
can be trained use Gibbs sampling method. For
the details of TAM, please refer to (Si and Sun,
2010).
With a trained TAM, we can infer p(t|r), the
probability of seeing a tag t when using r as the
reason, and p(r|t), the probability of choosing r
as the reason for tag t. With these probabilities,
we can estimate p(ta|tb) by
p(ta|tb) =
?
r?W
p(ta|r)p(r|tb). (4)
Note that we use only word reasons (r ? W ),
ignoring the noise reason ? completely. We de-
note this approach as TAG-REASON.
With the help of TAM, TAG-REASON cov-
ers the problems of the TAG-WORD method in
two aspects: First, instead of using all words,
TAG-REASON emphasizes on the really relevant
words, which are the reasons identified by TAM.
Second, by ignoring the noise variable ?, TAG-
REASON is less affected by the content-irrelevant
noise tags, such as thingstodo or myown.
After p(ta|tb) is estimated for each (ta, tb) ?
T ?T , we use the top-n pairs with largest p(ta|tb)
1014
Figure 2: DAG and Layered-DAG
as the final set of discovered subsumption rela-
tions.
4 Remove Redundancy with
Layered-DAG Construction
The discovered subsumption relations connect all
tags into a directed graph G = {V,E}, where V
is the set of nodes, with each node is a tag; E is
the set of edges, an edge eta,tb from ta to tb means
ta ?s tb. Furthermore, we define the weight of
each edge we as the probability p(ta|tb).
Recalling that subsumption relation has transi-
tivity property, to avoid the cyclic references in G,
we would like to turn G into a Directed Acyclic
Graph (DAG). Further, DAG may also contains
redundant information. Figure 2 (a) shows a part
of a DAG. Note the edge marked as ?*?, which
is perfectly correct, but does not provide extra
information, since literature ?s novel and
novel?s scifi-novel have already implied that
literature?s novel. We would like to remove
these redundant relations, turning a DAG into the
form of Figure 2 (b).
We define Layered-DAG formally as follows:
For a DAG G, when given any pair of nodes, if ev-
ery path that can connect them has equal length, G
is a Layered-DAG. Layered-DAG prohibits edges
that link cross layers, such like edge ?*? in Fig-
ure 2 (a). Constructing a Layered-DAG from the
discovered relations can eliminate the redundant
information.
Given a set of subsumption relations, multiple
Layered-DAGs may be constructed. In particular,
we want to find the Layered-DAG that maximizes
the sum of all edges? weights. Weight maximiza-
tion implies two concerns: First, when we need
to remove a relation to resolve the conflicts or re-
dundancy, the one with lower weight is prefered.
Layered-DAG Construction Algorithm
Input: A set of weighted relations, R = {ta ?s tb|ta ? T, tb ? T},
wta?stb > 0
Output: A Layered-DAG of tags G? = {V ?, E?}
1: V ? = {}
2: while R 6= ?
3: if V ? = ?
4: choose ta ?s tb ? R with highest weight.
5: E? ? ta ?s tb
6: V ? ? ta, V ? ? tb.
7: remove ta ?s tb from R.
8: else
9: C ? {ta ?s tb|ta ?s tb ? R, {ta, tb} ? V ? 6= ?}
10: for ta ?s tb ? C in descending weight order
11: if adding ta ?s tb to G? keeps G? a Layered-DAG.
12: E? ? ta ?s tb
13: V ? ? ta, V ? ? tb.
14: break
15: endif
16: remove ta ?s tb from R.
17: endfor
18: endif
19: endwhile
20: output G?
Figure 3: A greedy algorithm for constructing
Layered-DAG of tags
Second, when more than one valid Layered-DAGs
are available, we want to use the one that contains
as many edges as possible.
Finding and proving an optimal algorithm for
maximum Layered-DAG construction are beyond
the scope of this paper. Here we present a greedy
algorithm that works well in practice, as described
in Figure 3.
The proposed algorithm starts with a minimal
Layered-DAG G? that contains only the high-
est weighted relation in R (Steps 1-8). Then, it
moves an edge in G to G? once a time, ensuring
that adding the new edge still keeps G? a valid
Layered-DAG (Step 11), and the new edge has the
highest weights among all valid candidates (Steps
9-10).
5 Experiments
In this section, we show the experimental results
of proposed methods. Specifically, we focus on
the following points:
? The quality of discovered subsumption rela-
tions by different methods.
? The characteristics of wrong subsumption re-
lations discovered.
? The effect of Layered-DAG construction on
the quality of relations.
? Empirical study of the resulted Layered-
DAG.
1015
Name N N?tag N?content
BLOG 100,192 2.78 332.87
BOOK 110,371 8.51 204.76
Table 1: Statistics of the data sets. N is the num-
ber of documents. N?tag is the mean number of
tags per document. N?content is the mean number
of words per document.
5.1 Data Sets
We use two real world social tagging data sets.
The first data set, named BLOG, is a collection
of blog posts annotated by blog authors, which
is crawled from the web. The second data set,
named BOOK, is from a book collecting and shar-
ing site2, which contains description of Chinese
books and user contributed tags. Table 1 lists the
basic statistics of the data sets.
The two data sets have different characteristics.
Documents in BLOG are longer, not well written,
and the number of tags per document is small. To
the opposite, documents in BOOK are shorter but
well written, and there are more tags for each doc-
ument.
5.2 Discovered Subsumption Relations
5.2.1 Experimental Settings
For BLOG, we use the tags that have been used
more than 10 times; For BOOK, we use the tags
that have been used more than 50 times. We per-
form 100 iterations of Gibbs sampling when train-
ing the TAM model, with first 50 iterations as
the burn-in iterations. All the estimation meth-
ods require proper smoothing. Here we use ad-
ditive smoothing for all methods, which adds a
very small number (0.001 in our case) to all raw
counts. Sophisticated smoothing method could be
employed, but is out of the scope of this paper.
5.2.2 Evaluation
We use precision and coverage to evaluate the
discovered relations at any given cut-off threshold
n. First, we sort the discovered relations by their
weights in descending order. Then, we take the
top-n relations, discarding the others. For the re-
maining relations, precision is computed as Nc/n,
Nc is the number of correct relations in the top-n
2http://www.douban.com
list; coverage is computed as Nt/|T |, where Nt is
the number of unique tags appeared in the top-n
list, and |T | is the total number of tags.
To get Nc, the number of correct relations, we
need a standard judgement of the correctness of
relations, which involves human labeling. To min-
imize the bias in human assessment, we use pool-
ing, which is a widely accepted method in Infor-
mation Retrieval research (Voorhees and Harman,
2005). Pooling works as follows: First, relations
obtained by different methods are mixed together,
creating a pool of relations. Second, the pool is
shuffled, so that the labeler cannot identify the
source of a single relation. Third, annotators are
requested to label the relations in the pool as cor-
rect or incorrect, based on the definition of sub-
sumption relation. After all relations in the pool
are labeled, we use them as the standard judge-
ment to evaluate each method?s output.
Precision measures the proportion of correct re-
lations, while coverage measures the proportion of
tags that are connected by the relations. The cut-
off threshold n affects both precision and cover-
age: the larger the n, the lower the precision, and
the higher the coverage.
5.2.3 Baseline methods
Besides TAG-TAG, TAG-WORD and TAG-
REASON, we also include the method described
in (Heymann and Garcia-Molina, 2006) as a
baseline, denoted as HEYMANN. HEYMANN
method was designed to find similar-to relation
rather than subsumption relation. The similar-to
relation is symmetric, while subsumption relation
is more strict and asymmetric. In our experiments,
we use the same evaluation process to evalu-
ate TAG-TAG, TAG-WORD, TAG-REASON and
HEYMANN, in which only subsumption relations
will be marked as correct.
5.2.4 Results
For each method, we set the cut-off threshold
n from 1 to 500, so as to plot the psrecision-
coverage curves. The result is shown in Figure 4.
The larger the area under the curve, the better the
method?s performance.
We have three observations from Figure 4.
First, TAG-REASON has the best performance
1016
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Pr
ec
is
io
n
Coverage
TAG-REASON
TAG-TAG
TAG-WORD
HEYMANN
(a) BLOG
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
Pr
ec
is
io
n
Coverage
TAG-REASON
TAG-TAG
TAG-WORD
HEYMANN
(b) BOOK
Figure 4: The precision and coverage of TAG-TAG, TAG-WORD, TAG-REASON and HEYMANN
methods. The larger the area under the curve, the better the result. The cut-off threshold n ? [1, 500].
BLOG BOOK
Insufficient Reversed Irrelevant Insufficient Reversed Irrelevant
childedu?s father stock?s security travel?sbuilding textbook?s exam English?s foreignlang japan?slightnovel
childedu?s grandma stock?s financial emotion?stime history?s military biography?speople building?stextbook
emotion?swarm delicious?staste emotion?soriginal piano?sscores jpbuilding?s jpculture sales?sO
childedu?schild delicious?sfood culture?sspring history?sculture novel?spureliterature japan?s shower
education?schild earthquake?sdisaster poem?snight novel?slove ancientgreek?sgreek photo?sumbrella
Total 52% Total 14% Total 34% Total 37% Total 48% Total 15%
Table 2: Examples of mistakes and the percentage of each mistake type.
on both data sets: On the BOOK data set, TAG-
REASON outperforms others by a marked mar-
gin; On the BLOG data set, TAG-REASON has
higher precision when coverage is smaller (which
means within top-ranked relations), and has com-
parable precision to TAG-TAG when coverage
increases. Second, similarity-based clustering
method (namely HEYMANN) performed worse
than others, suggesting it may not be adequate for
discovering subsumption relation. Third, while
also using content information, TAG-WORD per-
forms poorer than both TAG-REASON and TAG-
TAG, which suggests that noise in the content
would prevent TAG-WORD from getting the cor-
rect estimation of p(ta|tb).
To summarize, by leveraging relevant con-
tent, TAG-REASON could discover better sub-
sumption relations than just using tag-tag co-
occurrences and similarity-based hierarchical
clustering.
5.2.5 Mistakes in Discovered Relations
We also studied the type of mistakes in sub-
sumption relation discovery. To our observation, a
mistakenly discovered relation ta ?s tb falls into
one of the following categories:
1. insufficient ta relates with tb, but using tb
does not implies the using of ta in all cases.
2. reversed tb ?s ta is correct, while ta ?s tb
is not.
3. irrelevant There is no obvious connection
between ta and tb.
We collected all incorrect relations discovered
by the TAG-REASON method. Then, the type of
mistake for each relation is labeled manually. The
result is shown in Table 2, along with selected ex-
amples of each type.
Table 2 shows different error patterns for
BLOG and BOOK. In BLOG, most of the
mistakes are of the type insufficient. Taking
?education?s child? for example, annotating a
document as child does not imply that it is about
child education, it may about food or clothes for
a child. In BOOK, most of the mistakes are re-
versed mistakes, which is a result of the omitted-
tag problem discussed in Section 3.2.1.
1017
Figure 5: Part of the constructed Layered-DAG from the BOOK data set.
BLOG BOOK
Method Precision Coverage Precision Coverage
TAG-TAG ?4.7% +7.9% ?7.4% +12.5%
TAG-WORD 0% 0% ?9.0% +2.2%
TAG-REASON ?3.6% +5.4% ?0.9% +5.4%
Table 3: The effects on precision and coverage by
Layered-DAG construction
5.3 Layered-DAG Construction
Using the algorithm introduced in Section 4, we
constructed Layered-DAGs from the discovered
relations. Constructing Layered-DAG will re-
move certain relations, which will decrease the
precision and increase the coverage. Table 3
shows the changes of precision and coverage
brought by Layered-DAG construction. In most
of the cases, the increasing of coverage is more
than the decreasing of precision.
As a representative example, we show part of
a constructed Layered-DAG from the BOOK data
set in Figure 5, since the whole graph is too big to
fit in the paper. All tags in Chinese are translated
to English.
6 Conclusion and Future Work
In this paper, we explored the structure of social
tags by discovering subsumption relations. First,
we defined the subsumption relation ta ?s tb
as ta can be used to replace tb without ambigu-
ity. Then, we cast the subsumption relation iden-
tification problem to the estimation of p(ta|tb).
We proposed three methods, namely TAG-TAG,
TAG-WORD and TAG-REASON, while the last
two leverage the content of document to help esti-
mation. We also proposed an greedy algorithm for
constructing a Layered-DAG from the discovered
relations, which helps minimizing redundancy.
We performed experiments on two real world
data sets, and evaluated the discovered subsump-
tion relations quantitatively by pooling. The
results showed that the proposed methods out-
perform similarity-based hierarchical clusteing
in finding subsumption relations. The TAG-
REASON method, which uses only the relevant
content to the tags, has the best performance. Em-
pirical study showed that Layered-DAG construc-
tion works effectively as expected.
The results suggest two directions for future
work: First, more ways for p(ta|tb) estima-
tion could be explored, for example, combining
TAG-TAG and TAG-REASON; Second, external
knowledge, such as the Wikipedia and the Word-
Net, could be exploited as background knowledge
to improve the accuracy.
ACKNOWLEDGEMENTS
This work is supported by the National Science
Foundation of China under Grant No. 60873174
and the National 863 High-Tech Program of China
under Grant No. 2007AA01Z148. We also thank
Douban Inc.(www.douban.com) for providing the
DOUBAN data set, and Shoukun Wang, Guozhu
Wen et al of Douban Inc. for insightful discus-
sion.
1018
References
Angeletou, S., M. Sabou, L. Specia, and E. Motta.
2007. Bridging the gap between folksonomies and
the semantic web: An experience report. In Work-
shop: Bridging the Gap between Semantic Web and
Web, volume 2. Citeseer.
Begelman, Grigory, Keller, and F. Smadja. 2006. Au-
tomated tag clustering: Improving search and explo-
ration in the tag space. In Collaborative Web Tag-
ging Workshop, 15 th International World Wide Web
Conference.
Brooks, Christopher H. and Nancy Montanez. 2006.
Improved annotation of the blogosphere via auto-
tagging and hierarchical clustering. In WWW ?06:
Proceedings of the 15th international conference on
World Wide Web, pages 625?632, New York, NY,
USA. ACM.
Echarte, F., J. J. Astrain, A. Co?rdoba, and J. Villadan-
gos. 2007. Ontology of folksonomy: A New mod-
eling method. Proceedings of Semantic Authoring,
Annotation and Knowledge Markup (SAAKM).
Heymann, Paul and Hector Garcia-Molina. 2006. Col-
laborative creation of communal hierarchical tax-
onomies in social tagging systems. Technical Re-
port 2006-10, Stanford University, April.
Kim, Hak L., Simon Scerri, John G. Breslin, Stefan
Decker, and Hong G. Kim. 2008. The state of the
art in tag ontologies: a semantic model for tagging
and folksonomies. In DCMI ?08: Proceedings of
the 2008 International Conference on Dublin Core
and Metadata Applications, pages 128?137. Dublin
Core Metadata Initiative.
Kome, Sam H. 2005. Hierarchical subject relation-
ships in folksonomies. Master?s thesis, University
of North Carolina at Chapel Hill, November.
Mika, P. 2005. Ontologies are us: A unified model of
social networks and semantics. The Semantic Web?
ISWC 2005, pages 522?536.
Passant, Alexandre. 2007. Using ontologies to
strengthen folksonomies and enrich information re-
trieval in weblogs. In Proceedings of International
Conference on Weblogs and Social Media.
Sanderson, M. and B. Croft. 1999. Deriving concept
hierarchies from text. In Proceedings of the 22nd
annual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 206?213. ACM.
Schmitz, P. 2006. Inducing ontology from flickr
tags. In Collaborative Web Tagging Workshop at
WWW2006, Edinburgh, Scotland, pages 210?214.
Citeseer.
Schwarzkopf, E., D. Heckmann, and D. Dengler.
2007. In Workshop on Data Mining for User Mod-
eling, ICUM?07, page 63. Citeseer.
Shepitsen, Andriy, Jonathan Gemmell, Bamshad
Mobasher, and Robin Burke. 2008. Personalized
recommendation in collaborative tagging systems
using hierarchical clustering. In Proceedings of
ACM RecSys?08.
Si, Xiance and Maosong Sun. 2010. Tag allocation
model: Modeling noisy social annotations by reason
finding. In Proceedings of 2010 IEEE/WIC/ACM
International Conferences on Web Intelligence and
Intelligent Agent Technology.
Specia, Lucia and Enrico Motta. 2007. Integrating
folksonomies with the semantic web. pages 624?
639.
Van Damme, C., M. Hepp, and K. Siorpaes. 2007.
Folksontology: An integrated approach for turning
folksonomies into ontologies. Bridging the Gap be-
tween Semantic Web and Web, 2:57?70.
Voorhees, E.M. and D.K. Harman. 2005. TREC: Ex-
periment and evaluation in information retrieval.
MIT Press.
Wu, Harris, Mohammad Zubair, and Kurt Maly.
2006a. Harvesting social knowledge from folk-
sonomies. In HYPERTEXT ?06: Proceedings of the
seventeenth conference on Hypertext and hyperme-
dia, pages 111?114, New York, NY, USA. ACM.
Wu, Xian, Lei Zhang, and Yong Yu. 2006b. Exploring
social annotations for the semantic web. In WWW
?06: Proceedings of the 15th international con-
ference on World Wide Web, pages 417?426, New
York, NY, USA. ACM.
1019
Coling 2010: Poster Volume, pages 710?718,
Beijing, August 2010
Fast-Champollion: A Fast and Robust
Sentence Alignment Algorithm
Peng Li and Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
pengli09@gmail.com, sms@tsinghua.edu.cn
Ping Xue
The Boeing Company
ping.xue@boeing.com
Abstract
Sentence-level aligned parallel texts are
important resources for a number of nat-
ural language processing (NLP) tasks and
applications such as statistical machine
translation and cross-language informa-
tion retrieval. With the rapid growth
of online parallel texts, efficient and ro-
bust sentence alignment algorithms be-
come increasingly important. In this
paper, we propose a fast and robust
sentence alignment algorithm, i.e., Fast-
Champollion, which employs a combi-
nation of both length-based and lexicon-
based algorithm. By optimizing the pro-
cess of splitting the input bilingual texts
into small fragments for alignment, Fast-
Champollion, as our extensive experi-
ments show, is 4.0 to 5.1 times as fast
as the current baseline methods such as
Champollion (Ma, 2006) on short texts
and achieves about 39.4 times as fast on
long texts, and Fast-Champollion is as ro-
bust as Champollion.
1 Introduction
Sentence level aligned parallel corpora are very
important resources for NLP tasks including ma-
chine translation, cross-language information re-
trieval and so on. These tasks typically require
support by large aligned corpora. In general, the
more aligned text we have, the better result we
achieve. Although there is a huge amount of bilin-
gual text on the Internet, most of them are either
only aligned at article level or even not aligned
at all. Sentence alignment is a process mapping
sentences in the source text to their correspond-
ing units in the translated text. Manual sentence
alignment operation is both expensive and time-
consuming, and thus automated sentence align-
ment techniques are necessary. A sentence align-
ment algorithm for practical use should be (1)
fast enough to process large corpora, (2) robust
enough to tackle noise commonly present in the
real data, and (3) effective enough to make as few
mistakes as possible.
Various sentence alignment algorithms have
been proposed, which generally fall into three
types: length-based, lexicon-based, and the hy-
brid of the above two types. Length-based algo-
rithms align sentences according to their length
(measured by character or word). The first length-
based algorithm was proposed in (Brown et al,
1991). This algorithm is fast and has a good per-
formance if there is minimal noise (e.g., sentence
or paragraph omission) in the input bilingual texts.
As this algorithm does not use any lexical infor-
mation, it is not robust. Lexicon-based algorithms
are usually more robust than the length-based al-
gorithm, because they use the lexical information
from source and translation lexicons instead of
solely sentence length to determine the transla-
tion relationship between sentences in the source
text and the target text. However, lexicon-based
algorithms are slower than length-based sentence
alignment algorithms, because they require much
more expensive computation. Typical lexicon-
based algorithms include (Ma, 2006; Chen, 1993;
Utsuro et al, 1994; Melamed, 1996). Sentence
length and lexical information are also combined
to achieve more efficient algorithms in two ways.
One way is to use both sentence length and lex-
710
ical information together to determine whether
two sentences should be directly aligned or not
(Simard et al, 1993; Wu, 1994). The other way is
to produce a rough alignment based on sentence
length (and possibly some lexical information at
the same time), and then build more precise align-
ment by using more effective lexicon-based algo-
rithms (Moore, 2002; Varga et al, 2005). But both
of the two ways suffer from high computational
cost and are not fast enough for processing large
corpora.
Lexical information is necessary for improving
robustness of a sentence alignment algorithm, but
use of lexical information will introduce higher
computational cost and cause a lower speed. A
common fact is that the shorter the text is, the
less combination possibilities it would introduce
and the less computational cost it would need. So
if we can first split the input bilingual texts into
small aligned fragments reliably with a reasonable
amount of computational cost, and then further
align these fragments one by one, we can speed
up these algorithms remarkably. This is the main
idea of our algorithm Fast-Champollion.
The rest of this paper is organized as fol-
lows: Section 2 presents formal definitions of sen-
tence alignment problem, and briefly reviews the
length-based sentence alignment algorithm and
Champollion algorithm; Section 3 proposes the
Fast-Champollion algorithm. Section 4 shows the
experiment results; and Section 5 is the conclu-
sion.
2 Definitions and Related Work
2.1 Definitions and Key Points
A segment is one or more consecutive sen-
tence(s). A fragment consists of one segment of
the source text (denoted by S) and one segment of
the target text (denoted by T ), and a fragment can
be further divided into one or more beads. A bead
represents a group of one or more sentences in
the source text and the corresponding sentence(s)
in the target text, denoted by Ai = (SAi;TAi) =
(Sai?1+1, Sai?1+2, ? ? ? , Sai ;Tbi?1+1, Tbi?1+2, ? ? ? ,
Tbi), where Si and Tj are the ith and jth sentence
of S and T respectively.
In practice, we rarely encounter crossing align-
ment, e.g., sentences Si and Sj of the source lan-
guage are aligned to the sentences Tj and Ti of
the target language respectively. But much more
effort has to be taken for an algorithm to process
crossing alignment well. So we do not consider
crossing alignment here.
In addition, only a few type of beads are fre-
quently observed in the real world. As it can save
significantly in terms of computational cost and it
would not do significant harm to algorithm with-
out considering rare bead types, a common prac-
tice for designing sentence alignment algorithms
is to only consider the frequently observed types
of beads. Following this practice, we only con-
sider beads of 1-to-0, 0-to-1, 1-to-1, 1-to-2, 2-to-
1, 1-to-3, 3-to-1, 1-to-4, 4-to-1 and 2-to-2 types in
our algorithm, where n-to-m means the bead con-
sists of n sentence(s) of the source language and
m sentence(s) of the target language.
2.2 Length-based Sentence Alignment
Algorithm
Length-based sentence alignment algorithm was
first proposed in (Brown et al, 1991). This algo-
rithm captures the idea that long or short sentences
tend to be translated into long or short sentences.
A probability is produced for each bead based on
the sentence length, and a dynamic programming
algorithm is used to search for the alignment with
the highest probability, which is treated as the best
alignment.
This algorithm is fast and can produce good
alignment when the input bilingual texts do not
contain too much noise, but it is not robust, be-
cause it only uses the sentence length information.
When there is too much noise in the input bilin-
gual texts, sentence length information will be no
longer reliable.
2.3 Champollion Aligner
Champollion aligner was proposed in (Ma, 2006).
It borrows the idea of tf-idf value, which is widely
used in information retrieval, to weight term1 pair
similarity. Greater weight is assigned to the less
frequent translation term pairs, because these term
1Here terms are not limited to linguistic words, but also
can be tokens like ?QX6800?
711
pairs have much stronger evidence for two seg-
ments to be aligned. For any two segments, a sim-
ilarity is assigned based on the term pair weight,
sentence number and sentence length. And the dy-
namic programming algorithm is used to search
for the alignment with the greatest total similarity.
This alignment is treated as the best alignment.
Champollion aligner can produce good align-
ment even on noisy input as reported in (Ma,
2006). Its simplicity and robustness make it a
good candidate for practical use. But this aligner
is slow. Because its time complexity is O(n2) and
it has to look up the dictionary multiple times in
each step of the dynamic programming algorithm,
which needs higher computational cost.
3 Fast-Champollion Algorithm
In this section we propose a new sentence align-
ment algorithm: Fast-Champollion. Its basis
is splitting the input bilingual texts into small
aligned fragments and then further aligning them
one by one to reduce running time while maintain-
ing Champollion-equivalent (or better) alignment
quality; it takes the advantages of both length-
based and lexicon-based algorithms to the maxi-
mum extent. The outline of the algorithm is that
first the length-based splitting module is used to
split the input bilingual texts into aligned frag-
ments, and then the components of each of these
fragments will be identified and aligned by a
Champollion-based algorithm. The details are de-
scribed in the following sections.
3.1 Length-based Splitting Module
Although length-based sentence alignment algo-
rithm is not robust enough, it can produce rough
alignment very fast with a certain number of re-
liably translated beads. Length-based splitting
module is designed to select these reliably trans-
lated beads to be used for delimiting and splitting
the input bilingual texts into fragments. These
beads will be referred to as anchor beads in the
remaining sections.
There are four steps in this module as described
below in detail.
Step 1: decide whether to skip step 2-4 or not
When there is too much noise in the input bilin-
gual texts, the percentage of reliably translated
beads in the alignment produced by the length-
based algorithm will be very low. In this case, we
will skip step 2 through 4.
An evidence for such a situation is that the
difference between the sentence numbers of the
source and target language is too big. Suppose
NS and NT are the number of sentences of the
source and target language respectively. We spec-
ify r = |NS ? NT |/min{NS , NT } as a measure
of the difference, where min means minimum. If
r is bigger than a threshold, we say the difference
is too big. In our experiments, the threshold is set
as 0.4 empirically.
Step 2: align the input texts using
length-based algorithm
In this step, length-based sentence alignment
algorithm is used to align the input bilingual texts.
Brown, et al (1991) models the process of sen-
tence alignment as two steps. First, a bead is gen-
erated according to a fixed probability distribution
over bead types, and then sentence length in the
bead is generated according to this model: for the
0-to-1 and 1-to-0 type of beads, it is assumed that
the sentence lengths are distributed according to
a probability distribution estimated from the data.
For other type of beads, the lengths of sentences of
the source language are generated independently
from the probability distribution for the 0-to-1 and
1-to-0 type of beads, and the total length of sen-
tences of the target language is generated accord-
ing to a probability distribution conditioned on the
total length of sentences of the source language.
For a bead Ai = (SAi;TAi), lSAi and lTAi are
the total lengths of sentences in SAi and TAi re-
spectively, which are measured by word2. Brown,
et al (1991) assumed this conditioned probability
distribution is
Prob(lTAi |lSAi) = ? exp
(
?(?i ? ?)
2
2?2
)
,
where ?i = log(lTAi/lSAi) and ? is a normal-
ization factor. Moore (2002) assumed the condi-
2For Chinese, word segmentation should be done first to
identify words.
712
tioned probability distribution is
Prob(lTAi |lSAi) =
exp (?lSAir) (lSAir)lTAi
lTAi !
,
where r is the ratio of the mean length of sen-
tences of the target language to the mean length
of sentences of the source language. We tested the
two models on our development corpus and the re-
sult shows that the first model performs better, so
we choose the first one.
Step 3: determine the anchor beads
In this step, the reliably translated beads in
the alignment produced by the length-based algo-
rithm in Step 2 will be selected as anchor beads.
The length-based algorithm can generate a
probability for each bead it produces. So a triv-
ial way is to choose the beads with a probability
above certain threshold as anchor beads. But as
pointed out before, when there is too much noise,
the alignment produced by the length-based algo-
rithm is no longer reliable, and so is it with the
probability. A fact is that if we select a non-
translated bead as an anchor bead, we will split
the input bilingual texts into wrong fragments and
may cause many errors. So we have to make de-
cision conservatively in this step and we decide to
use lexical information instead of the probability
to determine the anchor beads.
For a bead Ai = (SAi;TAi), the proportion of
translation term-pairs is a good measure for de-
termine whether this bead is reliably translated
or not. In addition, use of local information will
also be greatly helpful. To explain the use of ?lo-
cal information?, let?s define the fingerprint of a
sentence first. Suppose we have a sequence of
sentences S1, S2, ? ? ? , Sm, and W (Si)is the set of
distinct words in Si, then the fingerprint of Si is
f(Si) = W (Si)?W (Si?1)?W (Si+1),
and specially
f(S1) = W (S1)?W (S2),
f(Sm) = W (Sm)?W (Sm?1).
The fingerprints of SAi and TAi, denoted by
f(SAi) and f(TAi), are the unions of all the fin-
gerprints of sentences in SAi and TAi respectively.
As you can see, the fingerprint of a sentence is the
set of words in the sentence that do not appear in
the adjacent sentence(s), and thus can distinguish
this sentence from its neighbors. So fingerprint
is also a good measure. By combining these two
measures together, we can select out more reliably
translated beads.
For a word w, we use dD(w) to denote the set
of all its translations in a bilingual dictionary D,
and use tD(w) to denote the union of {w} and
dD(w), i.e., tD(w) = {w} ? dD(w). Given two
sets of words A and B. We say a word w of A is
translated by B if either one of its translations in
the dictionary D or the word itself appears in B,
i.e., tD(w)?B 6= ?. The set of all the words of A
that are translated by B is:
hD(A,B) = {w ? A and tD(w) ?B 6= ?}.
Then the proportion of terms in A that are trans-
lated by B is
rD(A,B) =
|hD(A,B)|
|A| .
We specify the proportion of translation term
pairs in a bead, denoted as arD(Ai), to be
min{rD(W (SAi),W (TAi)), rD(W (TAi),W (SAi))},
where W (SAi) and W (TAi) are the sets of dis-
tinct words in SAi and TAi respectively. Also we
specify the proportion of translation term-pairs
in the fingerprint, denoted as frD(Ai), to be
min{rD(f(SAi), f(TAi)), rD(f(TAi), f(SAi))}.
Given thresholds THar and THfr, a bead is
selected as an anchor bead when arD(Ai) and
frD(Ai) are not smaller than THar and THfr
respectively. We will show that Fast-Champollion
algorithm is not sensitive to THar and THfr to
some extent in Section 4.2.
Step 4: split the input bilingual texts
The anchor beads determined in Step 3 are used
to split the input texts into fragments. The ending
location of each anchor bead is regarded as a split-
ting point, resulting in two fragments.
3.2 Aligning Fragments with Champollion
Aligner
The similarity function used by Champollion
aligner is defined as follows. Given two (source
713
and target language) groups of sentences in a
fragment, denoted by GS=S1, S2,? ? ? ,Sm and
GT=T1, T2,? ? ? ,Tn, suppose there are k pairs
of translated terms in GS and GT denoted
by (ws1, wt1),(ws2, wt2),? ? ? ,(wsk, wtk), where
wsi is in GS and wti is in GT . For each pair of
the translated terms (wsi, wti), define idtf(wsi)
to be
Total # of terms in the whole document
# occurrences of wsi in GS
,
and define
stf(wsi, wti) = min{stf(wsi), stf(wti)},
where stf(wsi) and stf(wti) are the frequency
of wsi and wti in GS and GT respectively. The
similarity between GS and GT is defined as
k?
i=1
log (idtf(wsi)? stf(wsi, wti))
?alignment penalty
?length penalty,
where alignment penalty is 1 for 1-to-1 align-
ment type of beads and a number between 0 and 1
for other type of beads, length penalty is a func-
tion of the total sentence lengths of GS and GT .
The reason for choosing Champollion aligner
instead of other algorithms will be given in Sec-
tion 4.2. And another question is how idtf values
should be calculated. idtf is used to estimate how
widely a term is used. An intuition is that idtf
will work better if the texts are longer, because
if the texts are short, most words will have a low
frequency and will seem to only appear locally. In
Fast-Champollion, we calculate idtf according to
the whole document instead of each fragment. In
this way, a better performance is achieved.
3.3 Parameter Estimation
A development corpus is used to estimate the pa-
rameters needed by Fast-Champollion.
For the length-based algorithm, there are five
parameters that need to be estimated. The first one
is the probability distribution over bead types. The
ratio of different types of beads in the develop-
ment corpus is used as the basis for the estimation.
The second and third parameters are the proba-
bility distributions over the sentence length of the
source language and the target language. These
distributions are estimated as the distributions ob-
served from the input bilingual texts. That is to
say, these two distributions will not be the same
for different bilingual input texts. The forth and
fifth are ? and ?. They are estimated as the mean
and variance of ?i over the development corpus.
For Champollion aligner, alignment penalty
and length penalty need to be determined. Be-
cause the Perl version of Champollion aligner3
is well developed, we borrow the two definitions
from it directly.
4 Experiments
4.1 Datasets and Evaluation Metrics
We have two English-Chinese parallel corpora,
one for the development purpose and one for the
testing purpose. Both of the two corpora are col-
lected from the Internet and are manually aligned.
The development corpus has 2,004 beads.
Given the space constraint, detailed information
about the development corpus is omitted here.
The testing corpus contains 26 English-Chinese
bilingual articles collected from the Internet, in-
cluding news reports, novels, science articles,
television documentary subtitles and the record of
government meetings. There are 9,130 English
sentences and 9,052 Chinese sentences in these ar-
ticles4. The number of different type of beads in
the golden standard answer is shown in Table 1.
Type Number Percentage(%)
1:1 7275 83.19
1:2 2:1 846 9.67
1:3 3:1 77 0.88
1:4 4:1 16 0.18
2:2 32 0.37
1:0 0:1 482 5.51
others 17 0.19
total 8745 100.00
Table 1: Types of beads in the golden standard
Both the Fast-Champollion algorithm and the
Champollion aligner need a bilingual dictionary
and we supply the same bidirectional dictionary to
3http://champollion.sourceforge.net
4The definition of ?sentence? is slightly different from the
common sense here. We also treat semicolon and colon as the
end of a sentence.
714
them in the following evaluations. This dictionary
contains 45,439 pair of English-Chinese transla-
tion terms.
We use four commonly used measures for eval-
uating the performance of a sentence alignment
algorithm, which are the running time,
Precision = |GB ? PB||PB| ,
Recall = |GB ? PB||GB| ,
and
F1-measure = 2? Precision?RecallPrecision+Recall ,
where GB is the set of beads in the golden stan-
dard, and PB is the set of beads produced by the
algorithm.
All the following experiments are taken on a PC
with an Intel QX6800 CPU and 8GB memory.
4.2 Algorithm Design Issues
Why Choose Champollion?
We compared Champollion aligner with two
other sentence alignment algorithms which also
make use of lexical information. And the result
is shown in Table 2. ?Moore-1-to-1? and ?Moore-
all? are corresponding to the algorithm proposed
in (Moore, 2002). The difference between them is
how Recall is calculated. Moore?s algorithm can
only output 1-to-1 type of beads. For ?Moore-1-
to-1?, we only consider beads of 1-to-1 type in
the golden standard when calculating Recall, but
all types of beads are considered for ?Moore-all?.
The result suggests that ignoring the beads that are
not of 1-to-1 type does have much negative effect
on the overall performance of Moore?s algorithm.
Our goal is to design a general purpose sentence
alignment algorithm that can process frequently
observed types of beads. So Moore?s algorithm is
not a good choice. Hunalign refers to the hunalign
algorithm proposed in (Varga et al, 2005). The re-
sources provided to Champollion aligner and hu-
nalign algorithm are the same in the test, but hu-
nalign algorithm?s performance is much lower. So
hunalign algorithm is not a good choice either.
Champollion algorithm is simple and has a high
overall performance. So it is a better choice for
us.
Aligner Precision Recall F1-measure
Champollion 0.9456 0.9546 0.9501
Moore-1-to-1 0.9529 0.9436 0.9482
Moore-all 0.9529 0.7680 0.8505
Hunalign 0.8813 0.9037 0.8923
Table 2: The performance of different aligners on
the development corpus
The Effect of THar and THfr
THar and THfr are two thresholds for select-
ing anchor beads in Step 3 of length-based split-
ting module. In order to investigate the effect of
these two thresholds on the performance of Fast-
Champollion, we run Fast-Champollion on the de-
velopment corpus with different THar and THfr.
Both THar and THfr vary from 0 to 1 with step
0.05. And the running time and F1-measure are
shown in Figure 1 and Figure 2 respectively.
0 0.5
1
00.5
10
50100
150200
TH arTHfr
Runnin
g time(s)
Figure 1: The running time corresponding to dif-
ferent THar and THfr
0 0.5
1
00.5
10.8
0.850.9
0.951
TH arTHfr
F1?me
asure
Figure 2: The F1-measure corresponding to dif-
ferent THar and THfr
715
From Figure 1 and Figure 2, we see that for a
large range of the possible values of THar and
THfr, the running time of Fast-Champollion in-
creases slowly while F1-measure are nearly the
same. In other words, Fast-Champollion are not
sensitive to THar and THfr to some extent. So
making choice for the exact values of THar and
THfr becomes simple. And we use 0.5 for both
of them in the following experiments.
4.3 Performance of Fast-Champollion
We use three baselines in the following evalua-
tions. One is an implementation of the length-
based algorithm in Java, one is a re-implemented
Champollion aligner in Java according to the Perl
version, and the last one is Fast-Champollion-
Recal. Fast-Champollion-Recal is the same as
Fast-Champollion except that it calculates idtf
values according to the fragments themselves in-
dependently instead of the whole document, and
the Java versions of the length-based algorithm
and Champollion aligner are used for evaluation.
Performance on Texts from the Internet
Table 3 shows the performance of Fast-
Champollion and the baselines on the testing cor-
pus. The result shows that Fast-Champollion
achieves slightly better performance than Fast-
Champollion-Recal. The running time of Cham-
pollion is about 2.6 times longer than Fast-
Champollion with lower Precision, Recall and
F1-measure. It should be pointed out that Fast-
Champollion achieves better Precision, Recall and
F1-measure than Champollion does because the
splitting process may split the regions hard to
align into different fragments and reduces the
chance for making mistakes. Because of the noise
in the corpus, the F1-measure of the length-based
algorithm is low. This result suggests that Fast-
Champollion is fast, robust and effective enough
for aligning texts from the Internet.
Robustness of Fast-Champollion
In order to make a more precise investigation
on the robustness of Fast-Champollion against
noise, we made the following evaluation. First
we manually removed all the 1-to-0 and 0-to-
1 type of beads from the testing corpus to pro-
duce a clean corpus. This corpus contains 8,263
beads. Then we added 8263?n% 1-to-0 or 0-to-
1 type of beads to this corpus at arbitrary posi-
tions to produce a series of noisy corpora, with
n having the values of 5,10,...,100. Finally we
ran Fast-Champollion algorithm and the baselines
on these corpora respectively and the results are
shown in Figure 3 and Figure 4, which indi-
cate that for Fast-Champollion, when n increases
1, Precision drops 0.0021, Recall drops 0.0038
and F1-measure drops 0.0030 on average, which
are very similar to those of Champollion, but
Fast-Champollion is 4.0 to 5.1 times as fast as
Champollion. This evaluation proves that Fast-
Champollion is robust against noise and is a more
reasonable choice for practical use.
0 20 40 60 80 100050
100150200
250300
Noisy Level
Running tim
e(s)
 
 
Time of Fast?ChampollionTime of Fast?Champllion?RecalTime of ChampollionTime of length?based aligner
Figure 3: Running Time of Fast-Champollion,
Fast-Champollion-Recal, Champollion and the
length-based algorithm
0 20 40 60 80 10000.2
0.40.6
0.81
 
 
X: 100Y: 0.5427
Noisy Level
F1 of FCF1 of FCRF1 of CF1 of LP of FCP of FCRP of CP of LR of FCR of FCRR of CR of L
Figure 4: Precision (P), Recall (R) and F1-
measure (F1) of Fast-Champollion (FC), Fast-
Champollion-Recall (FCR), Champollion (C) and
the length-base algorithm (L)
Performance on Long Texts
In order to test the scalability of Fast-
Champollion algorithm, we evaluated it on long
texts. We merged all the articles in the testing cor-
716
Aligner Precision Recall F1-measure Running time(s)
Fast-Champollion 0.9458 0.9408 0.9433 48.0
Fast-Champollion-Recal 0.9470 0.9373 0.9421 45.4
Champollion 0.9450 0.9385 0.9417 173.5
Length-based 0.8154 0.7878 0.8013 11.3
Table 3: Performance on texts from the Internet
Aligner Precision Recall F1-measure Running time(s)
Fast-Champollion 0.9457 0.9418 0.9437 51.5
Fast-Champollion-Recall 0.9456 0.9362 0.9409 50.7
Champollion 0.9464 0.9412 0.9438 2029.0
Length-based 0.8031 0.7729 0.7877 23.8
Table 4: Performance on long text
pus into a single long ?article?. Its length is com-
parable to that of the novel of Wuthering Heights.
Table 4 shows the evaluation results on this long
article. Fast-Champollion is about 39.4 times as
fast as Champollion with slightly lower Precision,
Recall and F1-measure, and is just about 1.2 times
slower than the length-based algorithm, which has
much lower Precision, Recall and F1-measure. So
Fast-Champollion is also applicable for long text,
and has a significantly higher speed.
4.4 Evaluation of the Length-based Splitting
Module
The reason for Fast-Champollion can achieve rel-
atively high speed is that the length-based split-
ting module can split the bilingual input texts into
many small fragments reliably. We investigate the
fragments produced by the length-based splitting
module when aligning the long article used in Sec-
tion 4.3. The length-based splitting module splits
the long article at 1,993 places, and 1,972 seg-
ments are correct. The numbers of Chinese and
English segments with no more than 30 Chinese
and English sentences are shown in Figure 5. As
there are only 27 and 29 segments with more than
30 sentences for Chinese and English respectively,
we omit them in the figure. We can conclude that
although the length-based splitting module is sim-
ple, it is efficient and reliable.
5 Conclusion and Future Work
In this paper we propose a new sentence align-
ment algorithm Fast-Champollion. It reduces the
running time by first splitting the bilingual input
texts into small aligned fragments and then further
aligning them one by one. The evaluations show
0 5 10 15 20 25 300
200
400
600
800
Number of Sentences Contained in a Segment
Numbe
r
 
 
English SegmentChinese Segment
Figure 5: Numbers of Chinese/English segments
with no more than 30 Chinese/English sentences
that Fast-Champollion is fast, robust and effective
enough for practical use, especially for aligning
large amount of bilingual texts or long bilingual
texts.
Fast-Champollion needs a dictionary for align-
ing sentences, and shares the same problem of
Champollion aligner as indicated in (Ma, 2006),
that is the precision and recall will drop as the
size of the dictionary decreases. So how to build
bilingual dictionaries automatically is an impor-
tant task for improving the performance of Fast-
Champollion in practice, and is a critical problem
for applying Fast-Champollion on language pairs
without a ready to use dictionary.
Acknowledgement
This research is supported by the Boeing-
Tsinghua Joint Research Project ?Robust Chi-
nese Word Segmentation and High Performance
English-Chinese Bilingual Text Alignment?.
717
References
Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In
Proceedings of the 29th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 169?
176, Berkeley, California, USA, June. Association
for Computational Linguistics.
Chen, Stanley F. 1993. Aligning sentences in bilin-
gual corpora using lexical information. In Proceed-
ings of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 9?16, Colum-
bus, Ohio, USA, June. Association for Computa-
tional Linguistics.
Ma, Xiaoyi. 2006. Champollion: A robust paral-
lel text sentence aligner. In Proceedings of LREC-
2006: Fifth International Conference on Language
Resources and Evaluation, pages 489?492.
Melamed, I. Dan. 1996. A geometric approach to
mapping bitext correspondence. In Proceedings of
the First Conference on Empirical Methods in Nat-
ural Language Processing, pages 1?12.
Moore, Robert C. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, pages 135?144,
London, UK. Springer-Verlag.
Simard, Michel, George F. Foster, and Pierre Isabelle.
1993. Using cognates to align sentences in bilingual
corpora. In Proceedings of the 1993 Conference of
the Centre for Advanced Studies on Collaborative
Research, pages 1071?1082. IBM Press.
Utsuro, Takehito, Hiroshi Ikeda, Masaya Yamane, Yuji
Matsumoto, and Makoto Nagao. 1994. Bilingual
text matching using bilingual dictionary and statis-
tics. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1076?1082, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, and
Nagy V. 2005. Parallel corpora for medium den-
sity languages. In Proceedings of the RANLP 2005,
pages 590?596.
Wu, Dekai. 1994. Aligning a parallel english-chinese
corpus statistically with lexical criteria. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 80?87, Las
Cruces, New Mexico, USA, June. Association for
Computational Linguistics.
718
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1897?1907, Dublin, Ireland, August 23-29 2014.
A Neural Reordering Model for Phrase-based Translation
Peng Li
?
Yang Liu
?
Maosong Sun
?
Tatsuya Izuha
?
Dakun Zhang
?
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
?
Toshiba (China) R&D Center
zhangdakun@toshiba.com.cn
Abstract
While lexicalized reordering models have been widely used in phrase-based translation systems,
they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity. We propose a
neural reordering model that conditions reordering probabilities on the words of both the current
and previous phrase pairs. Including the words of previous phrase pairs significantly improves
context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we
build one classifier for all phrase pairs, which are represented as continuous space vectors. Ex-
periments on the NIST Chinese-English datasets show that our neural reordering model achieves
significant improvements over state-of-the-art lexicalized reordering models.
1 Introduction
Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004).
While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still
remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete
(Knight, 1999; Zaslavskiy et al., 2009).
The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al.,
2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et
al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012;
Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn
et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Un-
like the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements
in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabil-
ities conditioned on the words of each phrase pair. They often distinguish between three orientations
with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering
models capture the phenomenon that some words are far more likely to be displaced than others, they
outperform unlexicalized reordering models substantially.
Despite their apparent success in statistical machine translation, lexicalized reordering models suffer
from the following three drawbacks:
1. Context insensitivity. Lexicalized reordering models determine the orientations only depending on
the words of current phrase pairs. In fact, a phrase pair usually has different orientations in different
contexts. It is important to include more contexts to improve the expressive power of reordering
models.
2. Ambiguity. Short phrase pairs, which are observed in the training data more frequently, usually have
multiple orientations. We observe that about 92.4% of one-word Chinese-English phrase pairs are
ambiguous. This makes it hard to decide which orientation should be properly used in decoding.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1897
Figure 1: Ambiguity in phrase reordering. The phrase pair ??yingyun?, ?business?? is labeled with
different orientations in different contexts: (a) monotone, (b) swap, and (c) discontinuous. Lexicalized
reordering models use fixed probability distributions (e.g., 17.50% for M, 1.59% for S, and 80.92% for
D) in decoding even though the surrounding contexts keep changing.
3. Sparsity. Lexicalized reordering models maintain a reordering probability distribution for each
phrase pair. As most long phrase pairs that are capable of memorizing local word selection and
reordering only occur once in the training data, maximum likelihood estimation can hardly train the
models accurately.
In this work, we propose a neural reordering model for phrase-based translation. The contribution is
twofold. Firstly, unlike conventional lexicalized reordering models, the neural reordering model condi-
tions reordering probabilities on the words of both the current and previous phrase pairs. Including the
words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambi-
guity. Secondly, to alleviate the data sparsity problem, we build a neural classifier for all phrase pairs,
which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets
show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized
models.
2 Lexicalized Reordering Models
The lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have
become the de facto standard in modern phrase-based systems. These models are called lexicalized
because they condition reordering probabilities on the words of each phrase pair. Depending on the
relationship between the current and previous phrase pairs, lexicalized reordering models often define
orientations to classify different reordering patterns.
More formally, we use f = {
?
f
1
, . . . ,
?
f
n
} to denote a sequence of source phrases, e = {e?
1
, . . . , e?
n
}
to denote the phrase sequence on the target side, and a = {a
1
, . . . , a
n
} to denote the alignment be-
tween source and target phrases. A source phrase
?
f
a
i
and a target phrase e?
i
form a phrase pair. Lex-
icalized reordering models aim to estimate the conditional probability of a sequence of orientations
o = {o
1
, . . . , o
n
}:
P (o|f , e,a) =
n
?
i=1
P (o
i
|f , e?
1
, . . . , e?
i
, a
1
, . . . , a
i
) (1)
where each o
i
takes values over a set of predefined orientations. For simplicity, current lexicalized
1898
model
source phrase length
1 2 3 4 5 6 7
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) 92.74 54.01 24.09 14.40 10.78 8.47 6.95
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
a
i?1
, a
i?1
, a
i
) 21.72 5.22 2.63 1.48 0.98 0.67 0.54
Table 1: Percentages of phrase pairs that have multiple orientations. Including previous phrase pairs in
modeling significantly reduces the reordering ambiguity for the M/S/D orientations. For example, while
92.74% of 1-word Chinese-English phrase pairs have multiple orientations observed in the training data,
the ratio dramatically drops to 21.72% if the orientations are conditioned on both the current and previous
phrase pairs.
reordering models use orientations conditioned only on a
i?1
and a
i
:
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) (2)
The most widely used orientations are monotone (M), swap (S), and discontinuous (D):
1
o
i
=
?
?
?
M if a
i
? a
i?1
= 1
S if a
i
? a
i?1
= ?1
D if |a
i
? a
i?1
| =? 1
(3)
As lexicalized reordering models maintain a reordering probability distribution for each phrase pair,
it is hard to accurately learn reordering probabilities for long phrase pairs that are usually observed only
once in the training data. On the contrary, short phrase pairs that occur in the training data for many times
tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ??yingyun?,
?business?? is observed to have different orientations in different contexts.
It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding
contexts keep changing. Previous study shows that considering more contexts into reordering modeling
improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful
mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity.
3 A Neural Reordering Model
3.1 The Model
Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase
pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering
model is given by
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) (4)
where ?
?
f
a
i?1
, e?
i?1
? is the previous phrase pair.
Including the previous phrase pairs improves the context sensitivity. For example, given a phrase pair
??yingyun?, ?business??, its orientation is more likely to be monotone if it is preceded by a noun phrase
pair such as ??xinyongka?, ?credit card??. On the contrary, the probability of the discontinuous orienta-
tion is higher if the previous phrase pairs contain verbs such as ??gaishan?, ?improve??. Therefore, the
new model is capable of capturing the phenomenon that the orientation of a phrase pair depends on its
surrounding contexts.
Another advantage of including previous phrase pairs is the reduction of reordering ambiguity. As
shown in Table 1, 92.74% of 1-word Chinese-English phrase pairs have multiple orientations (i.e., M, S,
1
There are many variants of lexicalized reordering models depending on the model type, orientation, directionality, lan-
guage, and collapsing. See http://www.statmt.org/moses/?n=FactoredTraining.BuildReorderingModel for more details.
1899
and D) observed in the training data. The ratio decreases with the increase of phrase length. In contrast,
the new model is much less ambiguous (e.g., the ratio of ambiguous one-word phrase pairs dramatically
drops to 21.72%) as it is conditioned on both the current and previous phrase pairs.
Unfortunately, including more contexts in modeling also increases the data sparsity. We observe that
about 90% of reordering examples (i.e., the current and previous phrase pairs) are observed only once in
the training data. As a result, it is more difficult to train lexicalized reordering models accurately using
maximum likelihood estimation.
To alleviate the data sparsity problem, we use the following two strategies:
1. Reordering as classification. Instead of maintaining a reordering probability distribution for each
phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013).
This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as
training examples. We find that 500, 000 reordering examples suffice to train a robust classifier
(Section 4.5).
2. Continuous space representation. Instead of using a symbolic representation of phrases, we use
a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al.,
2011b; Li et al., 2013). Consider two phrases ?in London? and ?in Centara Grand?. It is usually
easy to predict the orientations of ?in London? because it might be observed in the training data for
many times. This is not the case for ?in Centara Grand? as it might occur only once. However, if
the two phrases happen to have very similar continuous space representations, ?in Centara Grand?
is likely to have a similar reordering probability distribution with ?in London?.
To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive
autoencoders. Given two words w
1
and w
2
, suppose their vector space representations are c
1
and c
2
.
The vector space representation p of the two-word phrase {w
1
, w
2
} can be computed using a two-layer
neural network:
p = g
(1)
(W
(1)
[c
1
; c
2
] + b
(1)
) (5)
where [c
1
; c
2
] ? R
2n
is the concatenation of c
1
and c
2
, W
(1)
? R
n?2n
is a weight matrix, b
(1)
is a bias
vector, and g
(1)
is an element-wise activation function.
In order to measure how well p represents c
1
and c
2
, they can be reconstructed using another two-layer
neural network:
[c
?
1
; c
?
2
] = g
(2)
(W
(2)
p + b
(2)
) (6)
where c
?
1
? R
n
and c
?
2
? R
n
are reconstructed vectors of c
1
and c
2
, W
(2)
? R
2n?n
is a weight matrix,
b
(2)
? R
n
is a bias vector, and g
(2)
is an element-wise activation function. The reconstruction error can
be measured by comparing c
1
and c
2
with c
?
1
and c
?
2
. This process runs recursively in a bottom-up style
to obtain the vector space representation of a multi-word phrase (Socher et al., 2011a). Socher et al.
(2011a) find that minimizing the norms of hidden layers leads to the reduction of reconstruction error in
an undesirable way. Therefore, we normalize p such that ||p||
2
= 1.
Treating phrase reordering as a classification problem, we propose a neural reordering classifier that
takes the current and previous phrase pairs as input. The neural network consists of four recursive
autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current
phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e.,
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
)
into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that
the recursive autoencoders for the same language share with the same parameters. Our neural network is
similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector
space representation for variable-sized blocks ranging from words to sentences on the fly both in training
and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training
phase, which makes our approach simpler and more scalable to large data.
1900
Formally, given the previous phrase pair ?
?
f
a
i?1
, e?
i?1
?, the current phrase pair ?
?
f
i
, e?
i
? and the orienta-
tion o
i
, the reordering probability is computed as
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) = g(W
o
c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) + b
o
), (7)
where W
o
is a weight matrix, b
o
is a bias vector, c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) is the concatenation of the vectors
of the four phrases.
2
Following Och (2003), we use a linear model in our decoder with conventional features (e.g., trans-
lation probabilities and n-gram language model). The neural reordering model is incorporated into the
discriminative framework as an additional feature.
3.2 Training
Training the neural reordering model involves minimizing the following two kinds of errors:
? Reconstruction error: It measures how well the computed vector space representations represent
the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees
formed during computing the vector space representation for all the phrases in the training data.
? Classification error: It measures how well the resulting classifier predicts the reordering orienta-
tions. It is defined as the average cross-entropy errors of all the training examples.
In our experiments, the objective function is a linear interpolation of the reconstruction error and the
classification error.
Following Socher et al. (2011b), we use L-BFGS (Liu and Nocedal, 1989) to optimize the parameters.
At the beginning of each iteration, a binary tree for each phrase is constructed using a greedy algorithm
(Socher et al., 2011b).
3
With these trees fixed, the partial derivatives with respect to parameters are
computed via the backpropagation through structures algorithm (Goller and Kuchler, 1996).
When optimizing the parameters of the softmax layer, the training procedure keeps the parameters of
the recursive autoencoders and word embedding matrices fixed. The corresponding error function is the
classification error as described above. We also use L-BFGS to optimize the parameters and the standard
error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives.
3.3 Decoding
As the vector space representation of a phrase is calculated based on all the words in the phrase, using
the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et
al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model
is directly integrated in decoding, making the decoder to only explore in a much smaller search space.
4
Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and
Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model.
4 Experiments
4.1 Data Preparation
We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M
sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was
trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which
contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set,
and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used
2
In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive
autoencoders to the softmax layer. Taking ?resident population? as an example, there are three vectors in the binary tree used
by the corresponding recursive autoencoder, denoted as x?
1
, x?
2
and x?
3
. The average vector is computed as x? =
1
3
?
3
i=1
x?
i
.
3
As phrases in phrase-based translation are not necessarily syntactic constituents, we do not use parse trees in this work.
4
Experimental results show that we can only achieve comparable performance with Moses by integrating neural reordering
model directly in decoding.
1901
Model Orientation MT06 MT02 MT03 MT04 MT05 MT08
distance N/A 29.56 31.40 31.27 31.34 29.98 23.87
word
M/S/D 30.19 32.03 31.86 32.09 30.55 24.20
left/right 30.17 31.98 31.52 31.98 30.19 24.30
phrase
M/S/D 30.24 32.35 31.85 32.00 30.78 24.33
left/right 29.57 32.64 31.53 31.90 30.70 24.28
hierarchical
M/S/D 30.46 32.52 31.89 32.09 30.39 24.11
left/right 30.03 32.13 31.59 31.91 30.21 24.41
neural
M/S/D 30.68 32.19 31.94 32.20 30.81 24.71
left/right 31.03** 33.03** 32.48** 32.52** 31.11* 25.20**
Table 2: Comparison of distance-based, lexicalized, and neural reordering models in terms of case-
insensitive BLEU-4 scores. ?distance? denotes the distance-based reordering model (Koehn et al., 2003),
?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes the phrase-based
lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-based reordering
model (Galley and Manning, 2008), and ?neural? denotes our model. The ?left? and ?right? orientations
only considers whether the current source phrase is on the left of the previous source phrase or not. We
use ?*? to highlight the result that is significantly better than the best baseline (highlighted in italic)
at p < 0.05 level and ?**? at p < 0.01 level. The neural model does not work well for the M/S/D
orientations due to the non-separability problem (Section 4.3).
as the evaluation metric. As a trade-off between expressive power and computational cost, we set the
dimension of the word embedding vectors to 25.
5
Both g
(1)
and g
(2)
are set to tanh(?). The other
hyperparameters are optimized via random search (Bergstra and Bengio, 2012).
4.2 Comparison of Distance-based, Lexicalized, and Neural Reordering Models
We compare three kinds of reordering models with increasing expressive power:
1. distance-based model: penalizing phrase displacements proportionally to the amount of nonmono-
tonicity (Koehn et al., 2003);
2. lexicalized models: conditioning the reordering probabilities on the current phrase pairs. The ori-
entations can be determined with respect to words (Tillman, 2004), phrases (Koehn et al., 2007), or
hierarchical phrases (Galley and Manning, 2008);
3. neural model: conditioning the reordering probabilities on both the current and previous phrase
pairs.
For lexicalized and neural models, we further distinguish between two kinds of orientation sets:
{monotone, swap, discontinuous} and {left, right}. The left/right orientations only consider whether
the current source phrase is on the left of the previous source phrase or not. Therefore, swap and
discontinuous-left are merged into left while monotone and discontinuous-right into right.
All these reordering models are tested using Moses (Koehn et al., 2007), except that the neural model
needs an additional hypergraph reranking procedure (Section 3.3). Implemented using Java, it takes the
reranker 0.748 second to rerank a hypergraph on average.
Table 2 shows the case-insensitive BLEU-scores of distance-based, lexicalized, and neural reordering
models on the NIST Chinese-English datasets. ?distance? denotes the distance-based reordering model
(Koehn et al., 2003), ?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes
the phrase-based lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-
based reordering model (Galley and Manning, 2008), and ?neural? denotes our model.
5
We find that the dimensions of vectors do not have a significant impact on translation performance. For efficiency, we set
the dimension to 25.
1902
Figure 2: The non-separability problem for the neural reordering model. Given an aligned Chinese-
English sentence pair, the unaligned Chinese word ?de? makes a big difference in determining M/S/D
orientations. In (a), ?de? is included in the previous source phrase and thus the orientation is monotone.
In (b), however, it is not included in the previous source phrase and the orientation is discontinuous. In
our neural reordering model, ?liu wan de? and ?liu wan? have very similar vector space representations
yet different orientations (i.e., M and D). In other words, training examples labeled with M, S, D are
prone to be mixed with each other in the vector space. Therefore, it is difficult to find a hyperplane to
separate M, S and D examples in the high-dimensional space.
We find that lexicalized reordering models obtain significant improvements over the distance-based
model, which indicates that conditioning reordering probabilities on the words of the current phrase
pairs does improve the expressive power. Our neural model using left/right orientations significantly
outperforms all variants of lexicalized models. We use ?*? to highlight the result that is significantly
better than the best baseline (highlighted in italic) at p < 0.05 level and ?**? at p < 0.01 level. This
suggests that conditioning reordering probabilities on the words of current and previous phrase pairs is
helpful for resolving reordering ambiguities and reducing context insensitivity.
4.3 The Non-Separability Problem
In Table 2, the neural model using the M/S/D orientations fails to outperform lexicalized models signifi-
cantly. One possible reason is that the neural model suffers from the non-separability problem due to the
M/S/D orientations.
As shown in Figure 2, given an aligned Chinese-English sentence pair, the unaligned Chinese function
word ?de? makes a big difference in determining M/S/D orientations. In (a), ?de? is included in the
previous source phrase and thus the orientation is monotone. In (b), however, ?de? is not included in the
previous source phrase and the orientation is discontinuous. In our neural reordering model, ?liu wan
de? and ?liu wan? have very similar vector space representations yet different orientations (i.e., M and
D). In other words, training examples labeled with M, S, D are prone to be mixed with each other in
the vector space. Therefore, it is difficult to find a hyperplane to separate M, S and D examples in the
high-dimensional space.
Fortunately, we find that using the left/right orientations can alleviate this problem. As the left/right
orientations only consider whether the current source phrase is on the left of the previous source phrase
or not, unaligned source words will not change orientations. For example, both Figure 2(a) and 2(b) are
identified as the right orientation.
As a result, using left/right orientations in the neural reordering model not only has a higher classifi-
cation accuracy (85%) over using the M/S/D orientations (69%), but also achieves higher BLEU scores
on all NIST datasets systematically.
4.4 The Effect of Distortion Limit
Figure 3 shows the performance of the lexicalized model and our neural model with various distortion
limits. The lexicalized model is the word-based model with M/S/D orientations. The neural model uses
left/right orientations. The neural model consistently outperforms the lexicalized model, especially for
large distortion limits. This finding suggests that the neural model is superior to lexicalized models in
predicting long-distance reordering.
1903
2 4 6 822
23
24
25
Distortion Limit
BLEU
 
 
neurallexicalized
Figure 3: BLEU with various distortion limits.
# examples Accuracy BLEU
100,000 83.55 30.92
200,000 84.40 31.03
300,000 84.55 31.01
400,000 84.95 30.93
500,000 85.25 31.27
3,000,000 85.55 31.03
Table 3: Effect of training corpus size.
Vectors MT06 MT02 MT03 MT04 MT05 MT08
ours 31.03 33.03 32.48 32.52 31.11 25.20
word2vec 30.44 32.28 32.00 32.07 30.24 24.54
Table 4: Comparison of neural reordering models trained based on word vectors produced by our model
(ours) and word2vec (Mikolov et al., 2013).
4.5 The Effect of Training Corpus Size
Table 3 shows the classification accuracy and translation performance with various number of randomly
sampled reordering examples for training the neural classifier. The classification accuracy and transla-
tion performance generally rise as the number of reordering example increases.
6
Surprisingly, both the
classification accuracy and translation performance of using 500,000 reordering examples are close to
using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples
are enough for training a robust classifier.
4.6 Learned Vector Space Representations
We randomly sampled 200,000 English phrases and found 999 clusters according to the vector space
representations computed by recursive autoencoders using the k-means algorithm (MacQueen, 1967).
The distance between two phrases is calculated by the Euclidean distance between their vector space
representations.
Figure 4 shows 10 of the 999 clusters. An interesting finding is that phrase pairs that are close in the
vector space share with similar reordering patterns rather than semantic similarity. For example, ?by
june 1? and ?within the agencies? have similar distributions on the left/right orientations but are totally
unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled
data hardly helps in training the neural reordering model. Table 4 shows the results when we replace
the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive
autoencoders and the classifier are retrained. The performance of the neural reordering model trained in
this way drops significantly, which confirms our analysis.
5 Related Work
Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006)
use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order
in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering
models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finer-
grained distance bins instead. Another direction is to learn sparse reordering features and create more
flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major
challenge. In contrast, our neural reordering model is capable of learning features automatically.
6
The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated
with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the
performance. We leave this for future work.
1904
but is willing toeconomy is required to
range of services to said his visit is to
is making use of
june 18, 2001late 2011
as detention centergroup all togethertake care of oldby june 1
and complete by end 1998
or other economicand for other
within the agencies
Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases
that have similar reordering probability distributions rather than similar semantic similarity fall into one
cluster.
Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al.,
2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et
al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and
target side contexts, these approaches still face the data sparsity problem.
Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to
compute vector space representation for variable-sized blocks ranging from words to sentences on the
fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7
words in the training phase, which makes our approach simpler and more scalable to large data.
6 Conclusion
We have shown that surrounding context is effective for resolving reordering ambiguities in phrase-based
models. As the data sparseness problem is the major challenge for using context in reordering models,
we propose to use a single classifier based on recursive autoencoders to predict reordering orientations.
Experimental results show that our neural reordering model outperforms the state-of-the-art lexicalized
reordering models significantly and consistently across all the NIST datasets under various settings.
There are a few future directions we plan to explore. First, as the machine translation system and neu-
ral classifier are trained separately, the neural network training only has an indirect effect on translation
quality. Jointly training the machine translation system and neural classifier is an interesting topic. Sec-
ond, it is interesting to develop more efficient models to leverage larger contexts to resolve reordering
ambiguities. Third, we plan to extend our work to other translation models such as syntax-based and
n-gram based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013). Finally, as we cast
phrase reordering as two-category classification problem (i.e, left vs. right), it is interesting to intersect
structured SVM (Tsochantaridis et al., 2005) with neural networks to develop a large margin training
algorithm for our neural reordering model.
Acknowledgements
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013), the 863 Program (No. 2012AA011102), Toshiba Corporation
Corporate Research & Development Center, and the Singapore National Research Foundation under its
International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme.
1905
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529?536.
James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine
Learning Research, 13(1):281?305.
Arianna Bisazza and Marcello Federico. 2012. Modified distortion matrices for phrase-based statistical machine
translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 478?487.
Colin Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrat-
ed reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013. Can Markov models
over minimal translation units help phrase-based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 399?405.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, pages 285?293.
Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856.
Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backprop-
agation through structure. In Proceedings of 1996 IEEE International Conference on Neural Networks (Vol-
ume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statis-
tical machine translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics, pages 867?875.
Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011
Sixth Workshop on Statistical Machine Translation, pages 187?197.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151.
Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
586?594.
Maxim Khalilov and Khalil Simaan. 2010. Source reordering using maxent classifiers and supertags. In Proceed-
ings of The 14th Annual Conference of the European Association for Machine Translation, pages 292?299.
Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Computational Linguistics,
25(4):607?615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577.
1906
DongC. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math-
ematical Programming, 45(1-3):503?528.
James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281?297.
Jos?e B. Mari`no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527?549.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen. 2009. Improving a lexicalized
hierarchical reordering model using maximum entropy. In Proceedings of The twelfth Machine Translation
Summit (MT Summit XII).
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proceedings of the Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: Main
Proceedings, pages 161?168.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics, pages 160?167.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning internal representations by
error propagation. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1:
Foundations, pages 318?362.
Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of Advances in Neural
Information Processing Systems 24, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Processing, pages 151?161.
Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the
Human Language Technology Conference of the North American Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004: Short Papers, pages 101?104.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453?1484.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational Linguistics, pages 521?528.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic distortion in a discriminative reordering model for statistical
machine translation. In Proceedings of the 7th International Workshop on Spoken Language Translation, pages
353?360.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation
as a traveling salesman problem. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 333?341.
Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-
based statistical machine translation. In Proceedings of the 20th International Conference on Computational
Linguistics, pages 205?211.
1907
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2031?2041, Dublin, Ireland, August 23-29 2014.
Query Lattice for Translation Retrieval
Meiping Dong
?
, Yong Cheng
?
, Yang Liu
?
, Jia Xu
?
, Maosong Sun
?
,
Tatsuya Izuha

, Jie Hao
#
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
hellodmp@163.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Institute for Interdisciplinary Information Sciences
Tsinghua University, Beijing, China
chengyong3001@gmail.com, xu@tsinghua.edu.cn

Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
#
Toshiba (China) R&D Center
haojie@toshiba.com.cn
Abstract
Translation retrieval aims to find the most likely translation among a set of target-language strings
for a given source-language string. Previous studies consider the single-best translation as a query
for information retrieval, which may result in translation error propagation. To alleviate this
problem, we propose to use the query lattice, which is a compact representation of exponentially
many queries containing translation alternatives. We verified the effectiveness of query lattice
through experiments, where our method explores a much larger search space (from 1 query to
1.24 ? 10
62
queries), runs much faster (from 0.75 to 0.13 second per sentence), and retrieves
more accurately (from 83.76% to 93.16% in precision) than the standard method based on the
query single-best. In addition, we show that query lattice significantly outperforms the method
of (Munteanu and Marcu, 2005) on the task of parallel sentence mining from comparable corpora.
1 Introduction
Translation retrieval aims to search for the most probable translation candidate from a set of target-
language strings for a given source-language string. Early translation retrieval methods were widely
used in example-based and memory-based translation systems (Sato and Nagao, 1990; Nirenburg et al.,
1993; Baldwin and Tanaka, 2000; Baldwin, 2001). Often, the document set is a list of translation records
that are pairs of source-language and target-language strings. Given an input source string, the retrieval
system returns a translation record of maximum similarity to the input on the source side. Although these
methods prove to be effective in example-based and memory-based translation systems, they heavily rely
on parallel corpora that are limited both in size and domain.
More recently, Liu et al. (2012) have proposed a new translation retrieval architecture that depends
only on monolingual corpora. Given an input source string, their system retrieves translation candidates
from a set of target-language sentences. This can be done by combining machine translation (MT) and
information retrieval (IR): machine translation is used to transform the input source string to a coarse
translation, which serves as a query to retrieve the most probable translation in the monolingual corpus.
Therefore, it is possible for translation retrieval to have access to a huge volume of monolingual corpora
that are readily available on the Web.
However, the MT + IR pipeline suffers from the translation error propagation problem. Liu et al.
(2012) use 1-best translations, which are inevitably erroneous due to the ambiguity and structural di-
vergence of natural languages, as queries to the IR module. As a result, translation mistakes will be
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
Corresponding author: Jia Xu. Tel: +86-10-62781693 Ext 1683. Homepage: iiis.tsinghua.edu.cn/?xu
2031
propagated to the retrieval process. This situation aggravates when high-accuracy MT systems are not
available for resource-scarce languages.
In this work, we propose to use query lattice in translation retrieval to alleviate the translation error
propagation problem. A query lattice is a compact representation of exponentially many queries. We
design a retrieval algorithm that takes the query lattice as input to search for the most probable translation
candidate from a set of target-language sentences. As compared with Liu et al. (2012), our approach
explores a much larger search space (from 1 query to 1.24? 10
62
queries), runs much faster (from 0.75
second per sentence to 0.13), and retrieves more accurately (from 83.76% to 93.16%). We also evaluate
our approach on extracting parallel sentences from comparable corpora. Experiments show that our
translation retrieval system significantly outperforms a state-of-the-art parallel corpus mining system.
2 Related Work
Our work is inspired by three research topics: retrieving translation candidates from parallel corpus,
using lattice to compactly represent exponentially many alternatives, and using lattice as query in infor-
mation retrieval.
1. Translation Retrieval using Parallel Corpus. The idea of retrieving translation candidates from
existing texts originated in example-based and memory-based translation (Sato and Nagao, 1990;
Nirenburg et al., 1993; Baldwin and Tanaka, 2000; Baldwin, 2001). As these early efforts use
a parallel corpus (e.g., translation records that are pairs of source-language and target-language
strings), they focus on calculating the similarity between two source-language strings. In contrast,
we evaluate the translational equivalence of a given source string and a target string in a large
monolingual corpus.
2. Lattice in Machine Translation. Lattices have been widely used in machine translation: consider-
ing Chinese word segmentation alternatives (Xu et al., 2005), speech recognition candidates (Mat-
soukas et al., 2007), SCFG (Dyer et al., 2008) and so on in the decoding process, minimum bayes
risk decoding (Tromble et al., 2008), minimum error rate training (Macherey et al., 2008), system
combination (Feng et al., 2009), just to name a few. In this work, we are interested in how to use a
lattice that encodes exponentially many translation candidates as a single query to retrieve similar
target sentences via an information retrieval system.
3. Query Lattice in Information Retrieval. The use of lattices in information retrieval dates back to
Moore (1958). Most current lattice-based IR systems often treat lattices as conceptional hierarchies
or thesauri in formal concept analysis (Priss, 2000; Cheung and Vogel, 2005). In spoken document
retrieval, however, lattices are used as a compact representation of multiple speech recognition
transcripts to estimate the expected counts of words in each document (Saraclar and Sproat, 2004;
Zhou et al., 2006; Chia et al., 2010). Our work is significantly different from previous work that
uses the bag-of-words model because translation retrieval must take structure and dependencies in
text into account to ensure translational equivalence.
3 Query Lattice for Translation Retrieval
3.1 Translation Retrieval
Let f be a source-language string, E be a set of target-language strings, the problem is how to find the
most probable translation
?
e from E. Note that E is a monolingual corpus rather than a parallel corpus.
Therefore, string matching on the source side (Sato and Nagao, 1990; Nirenburg et al., 1993; Baldwin
and Tanaka, 2000; Baldwin, 2001) does not apply here.
We use P (e|f) to denote the probability that a target-language sentence e is the translation of a source-
language sentence f . As suggested by Liu et al. (2012), it can be decomposed into two sub-models by
2032
introducing a coarse translation q as a hidden variable:
P (e|f) =
?
q?Q(f)
P (q, e|f) (1)
=
?
q?Q(f)
P (q|f)? P (e|q, f) (2)
where P (q|f) is a translation sub-model, P (e|q, f) is a retrieval sub-model, and Q(f) is the set of all
possible translations of the sentence f . Note that q actually serves as a query to the retrieval sub-model.
To take advantage of various translation and retrieval information sources, we use a log-linear model
(Och and Ney, 2002) to define the conditional probability of a query q and a target sentence e conditioned
on a source sentence f parameterized by a real-valued vector ?:
P (q, e|f ;?) =
exp(? ? h(q, e, f))
?
q
?
?Q(f)
?
e
?
?E
exp(? ? h(q
?
, e
?
, f))
(3)
where h(?) is a vector of feature functions and ? is the corresponding feature weight vector.
Accordingly, the decision rule for the latent variable model is given by
?
e = argmax
e?E
{
?
q?Q(f)
exp(? ? h(q, e, f))
}
(4)
As there are exponentially many queries, it is efficient to approximate the summation over all possible
queries by using maximization instead:
?
e ? argmax
e?E
{
max
q?Q(f)
{
? ? h(q, e, f)
}
}
(5)
Unfortunately, the search space is still prohibitively large since we need to enumerate all possible
queries. Liu et al. (2012) split Eq. (5) into two steps. In the first step, a translation module runs to
produce the 1-best translation
?
q of the input string f as a query:
?
q ? argmax
q?Q(f)
{
?
t
? h
t
(q, e, f)
}
(6)
where h
t
(?) is a vector of translation features and ?
t
is the corresponding feature weight vector. In the
second step, a monolingual retrieval module takes the 1-best translation
?
q as a query to search for the
target string
?
e with the highest score:
?
e ? argmax
e?E
{
?
r
? h
r
(
?
q, e, f)
}
(7)
where h
r
(?) is a vector of retrieval features and ?
r
is the corresponding feature weight vector.
Due to the ambiguity of translation, however, state-of-the-art MT systems are still far from producing
high-quality translations, especially for distantly-related languages. As a result, the 1-best translations
are usually erroneous and potentially introduce retrieval mistakes.
A natural solution is to use n-best lists as queries:
?
e ? argmax
e?E
{
max
q?N(f)
{
? ? h(q, e, f)
}
}
(8)
where N(f) ? T(f) is the n-best translations of the input source sentence f .
2033
Figure 1: Two kinds of query lattices: (a) search graph that is generated after phrase-based decoding and
(b) translation option graph that is generated before decoding. Translation option graph is more compact
and encodes more translation candidates.
Although using n-best lists apparently improves the retrieval accuracy over using 1-best lists, there
are two disadvantages. First, the decision rule in Eq. (8) requires to enumerate all the n translations and
retrieve for n times. In other words, the time complexity increases linearly. Second, an n-best list only
accounts for a tiny fraction of the exponential search space of translation. To make things worse, there
are usually very few variations in n-best translations because of spurious ambiguity - a situation where
multiple derivations give similar or even identical translations.
Therefore, we need to find a more elegant way to enable the retrieval module to explore exponentially
many queries without sacrificing efficiency.
3.2 Query Lattice
We propose to use query lattice to compactly represent exponentially many queries. For example, given
a source sentence ?bushi yu shalong juxing huitan?, we can use the search graph produced by a phrase-
based translation system (Koehn et al., 2007) as a lattice to encode exponentially many derivations.
Figure 1(a) shows a search graph for the example source sentence. Each edge is labeled with an
English phrase as well as the corresponding translation feature value vector. Node 0 denotes the starting
node. Node 7 and node 8 are two ending nodes. Each path from the starting node to an ending node
denotes a query. Paths that reach the same node in the lattice correspond to recombined hypotheses
that have equivalent feature histories (e.g., coverage, last generated target words, the end of last covered
source phrase, etc) in phrase-based decoding.
However, there are two problems with using search graph as query lattice. First, it is computationally
expensive to run a phrase-based system to generate search graphs. The time complexity for phrase-based
decoding with beam search is O(n
2
b) (Koehn et al., 2007), where n is the length of source string and b is
the beam width. Moreover, the memory requirement is usually very high due to language models. As a
result, translation is often two orders of magnitude slower than retrieval. Second, a search graph has too
many ?duplicate? edges due to different reordering, which increase the time complexity of retrieval (see
Section 3.3). For example, in Figure 1(a), the English phrase ?Sharon? occurs two times due to different
reordering.
Alternatively, we propose to use translation option graph as query lattice. In a phrase-based trans-
lation system, translation options that are phrase pairs matching a substring in the input source string
are collected before decoding. These translation options form a query lattice with monotonic reorder-
ing. Figure 1(b) shows an example translation option graph, in which nodes are sorted according to the
positions of source words. Each edge is labeled with an English phrase as well as the corresponding
translation feature value vector.
We believe that translation option graph has three advantages over search graph:
1. Improved efficiency in translation. Translation option graph requires no decoding.
2. Improved efficiency in retrieval. Translation option graph has no duplicate edges.
2034
Algorithm 1 Retrieval with lattice as query.
1: procedure LATTICERETRIEVE(L(f),E, k)
2: Q? GETWORDS(L(f)) . Get distinct words in the lattice to form a coarse query
3: E
k
? RETRIEVE(E, Q, k) . Retrieve top-k target sentences using the coarse query
4: for all e ? E
k
do
5: FINDPATH(L(f), e) . Find a path with the highest score
6: end for
7: SORT(E
k
) . Sort retrieved sentences according the scores
8: return E
k
9: end procedure
Algorithm 2 Find a path with the highest score.
1: procedure FINDPATH(L(f), e)
2: for v ? L(f) in topological order do
3: path(v)? ? . Initialize the Viterbi path at node v
4: score(v)? 0 . Initialize the Viterbi score at node v
5: for u ? IN(v) do . Enumerate all antecedents
6: p? path(u) ? {e
u?v
} . Generate a new path
7: s? score(u) + COMPUTESCORE(e
u?v
) . Compute the path score
8: if s > score(v) then
9: path(v)? p . Update the Viterbi path
10: score(v)? s . Update the Viterbi score
11: end if
12: end for
13: end for
14: end procedure
3. Enlarged search space. Translation option graph represents the entire search space of monotonic
decoding while search graph prunes many translation candidates.
In Figure 1, the search graph has 9 nodes, 10 edges, 4 paths, and 3 distinct translations. In contrast,
the translation option graph has 6 nodes, 9 edges, 10 paths, and 10 distinct translations. Therefore,
translation option graph is more compact and encodes more translation candidates.
Although translation option graph ignores language model and lexcialized reordering models, which
prove to be critical information sources in machine translation, we find that it achieves comparable or
even better retrieval accuracy than search graph (Section 4). This confirms the finding of Liu et al. (2012)
that language model and lexicalized reordering models only have modest effects on translation retrieval.
3.3 Retrieval with Query Lattice
Given a target corpus E and a query lattice L(f) ? Q(f), our goal is to find the target sentence
?
e with
the highest score ? ? h(q, e, f):
?
e ? argmax
e?E
{
max
q?L(f)
{
? ? h(q, e, f)
}
}
(9)
Due to the exponentially large search space, we use a coarse-to-fine algorithm to search for the target
sentence with the highest score, as shown in Algorithm 1. We use an example to illustrate the basic idea.
Given an input source sentence ?bushi yu shalong juxing le huitan?, our system first generates a query
lattice like Figure 1(a). It is non-trivial to directly feed the query lattice to a retrieval system. Instead, we
would like to first collect all distinct words in the lattice: {?Bush?, ?and? , ?Sharon?, ?held?, ?a?, ?talk?,
?talks?, ?with?}. This set serves as a coarse single query and the retrieval system returns a list of target
sentences that contain these words:
2035
Chinese English
Training 1.21M 1.21M
Dev in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Test in-domain query 5K
document 2.23M
out-of-domain query 5K
document 2.23M
Table 1: The datasets for the retrieval evaluation. The training set is used to train the phrase-based
translation model and language model for Moses (Koehn et al., 2007). The development set is used
to optimize feature weights using the minimum-error-rate algorithm (Och, 2003). A development set
consists of a query set and a document set. The test set is used to evaluate the retrieval accuracy. To
examine the effect of domains on retrieval performance, we used two development and test sets: in-
domain and out-domain.
President Bush gave a talk at a meeting
Bush held a meeting with Sharon
Sharon and Bush attended a meeting held at London
Note that as a retrieval system usually ignores the structural dependencies in text, the retrieved sentences
(scored by retrieval features) are relevant but not necessarily translations of the input. Therefore, we
can match each retrieved sentence against the query lattice to find a path with the highest score using
additional translation features. For example, the Viterbi path for ?Bush held a meeting with Sharon? in
Figure 1(a) is ?Bush held talks with Sharon?. The translation features of matched arcs in the path are
collected to compute the overall score according to Eq. (9). Finally, the algorithm returns a sorted list:
Bush held a meeting with Sharon
President Bush gave a talk at a meeting
Sharon and Bush attended a meeting held at London
More formally, the input of Algorithm 1 are a query lattice L(f), a target corpus E, and a parameter
k (line 1). The function GETWORDS simply collects all the distinct words appearing in the lattice (line
2), which are used for constructing a coarse boolean query Q. Then, the function RETRIEVE runs to
retrieve the top-k target sentences E
k
in the target corpus E only using standard IR features according
to the query Q (line 3). These first two steps eliminate most unlikely candidates and return a coarse set
of target sentence candidates efficiently.
1
Then, a procedure FINDPATH(L(f), e) runs to search for the
translation with the highest score for each candidate (lines 4-6). Finally, the algorithm returns the sorted
list of target sentences (lines 7-9).
Algorithm 2 shows the procedure FINDPATH(L(f), e), which searches for the path with higher score
using a Viterbi-style algorithm. The function COMPUTESCORE scores an edge according to the Eq. (9)
which linearly combines the translation and retrieval features.
Generally, the lattice-based retrieval algorithm has a time complexity of O(k|E|), where |E| is the
number of edges in the lattice.
4 Experiments
In this section, we try to answer two questions:
1. Does using query lattices improve translation retrieval accuracy over using n-best lists?
2. How does translation retrieval benefit other end-to-end NLP tasks such as machine translation?
1
In our experiments, we set the parameter k to 500 as a larger value of k does not give significant improvements but introduce
more noises.
2036
Accordingly, we evaluated our system in two tasks: translation retrieval (Section 4.1) and parallel
corpus mining (Section 4.2).
4.1 Evaluation on Translation Retrieval
4.1.1 Experimental Setup
In this section, we evaluate the accuracy of translation retrieval: given a query set (i.e., source sentences),
our system returns a sorted list of target sentences. The evaluation metrics include precision@n and
recall.
The datasets for the retrieval evaluation are summarized in Table 1. The training set, which is used to
train the phrase-based translation model and language model for the-state-of-the-art phrase-based system
Moses (Koehn et al., 2007), contains 1.21M Chinese-English sentences with 32.0M Chinese words and
35.2M English words. We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on
the English side of the training corpus. The development set, which is used to optimize feature weights
using the minimum-error-rate algorithm (Och, 2003), consists of query set and a document set. We
sampled 5K parallel sentences randomly, in which 5K Chinese sentences are used as queries and half
of their parallelled English sentences(2.5K) mixed with other English sentences(2.3M) as the retrieval
document set. As a result, we can compute precision and recall in a noisy setting. The test set is used
to compute retrieval evaluation metrics. To examine the effect of domains on retrieval performance, we
used two data sets: in-domain and out-domain. The in-domain development and test sets are close to
the training set while the out-domain data sets are not.
We compare three variants of translation retrieval: 1-best list, n-best list, and lattice. For query lattice,
we further distinguish between search graph and translation option graph. They are generated by Moses
with the default setting.
We use both translation and retrieval features in the experiments. The translation features include
phrase translation probabilities, phrase penalty, distance-based and lexicalized reordering models, lan-
guage models, and word penalty. Besides the conventional IR features such as term frequency and
inverse document frequency, we use five additional featured derived from BLEU (Papineni et al., 2002):
the n-gram matching precisions between query and retrieved target sentence (n = 1, 2, 3, 4) and brevity
penalty. These features impose structural constraints on retrieval and ensure translation closeness of re-
trieved target sentences. The minimum-error-rate algorithm supports a variety of loss functions. The loss
function we used in our experiment is 1?P@n. Note that using translation option graph as query lattice
does not include language models and distance-based lexicalized reordering models as features.
4.1.2 Evaluation Results
Table 2 shows the results on the in-domain test set. The ?# candidates? column gives the number of
translation candidates explored by the retrieval module for each source sentence on average. The lattices,
either generated by search graph or by translation options, contain exponentially many candidates. We
find that using lattices dramatically improves the precisions over using 1-best and n-best lists. All the
improvements over 1-best and n-best lists are significant statistically. The 1-best, n-best, and the search
graph lattice share with the same translation time: 5,640 seconds for translating 5,000 queries. Note
that the translation time is zero for the translation option graph because it does not need phrase-based
decoding. For retrieval, the time cost for the n-best list method generally increases linearly. As the search
graph lattice contains many edges, the retrieval time increases by an order of magnitude as compared
with 100-best list. An interesting finding is that using translation options as a lattice contains more
candidates and consumes much less time for retrieval than using search graph as a lattice. One possible
reason is that a search graph generated by Moses usually contains many redundant edges. For example,
Figure 1 is actually a search graph and many phrases occurs multiple times in the lattice (e.g., ?and?
and ?Sharon?). In contrast, a lattice built by translation options hardly has any redundant edges but
still represents exponentially many possible translations. We can also see that the lattice constructed by
search graph considering language model can benefit the precision much, especially when n is little. But
this advantage decreases with n increasing and the time consumed by translation options as lattice is
much less than the search graph as lattice. Besides, the margin between them is not too large so we can
2037
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 87.40 91.40 92.24 92.88 93.64 5,640 82
10-best 10 89.84 93.20 93.96 94.36 95.56 5,640 757
100-best 100 90.76 94.32 95.00 95.76 96.76 5,640 7,421
lattice (graph) 1.20? 10
54
93.60 96.08 96.28 96.52 96.80 5,640 89,795
lattice (options) 4.14? 10
62
93.28 95.84 95.96 96.16 96.84 0 307
Table 2: Results on the in-domain test set. We use the minimum-error-rate training algorithm (Och,
2003) to optimize the feature with the respect to 1?P@n.
method # candidates
P@n time
n=1 n=5 n=10 n=20 n=100 translation retrieval
1-best 1 67.32 76.60 79.40 81.80 83.76 3,660 92
10-best 10 72.68 80.96 83.36 85.84 88.76 3,660 863
100-best 100 78.60 85.76 87.76 89.64 92.16 3,660 8,418
lattice (graph) 1.51? 10
61
84.32 89.40 90.68 91.56 92.44 3,660 67,205
lattice (options) 1.24? 10
65
81.92 88.00 89.80 91.24 93.16 0 645
Table 3: Results on the out-of-domain test set.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 2: In-domain Precision-Recall curves.
0.3 0.4 0.5 0.6 0.7 0.8 0.90.3
0.4
0.5
0.6
0.7
0.8
0.9
1
recall
preci
sion
 
 
lattice(search graph)lattice(translation options)100?best?list10?best1?best
Figure 3: Out-domain Precision-Recall curves.
abandon some little precision for obtain the large time reducing. Therefore, using translation options as
lattices seems to be both effective and efficient.
Table 3 shows the results on the out-of-domain test set. While the precisions for all methods drop, the
margins between lattice-based retrieval and n-best list retrieval increase, suggesting that lattice-based
methods are more robust when dealing with noisy datasets.
Figures 2 and 3 show the Precision-Recall curves on the in-domain and out-of-domain test sets. As
the query set is derived from parallel sentences, recall can be computed in our experiments. The curves
show that using lattices clearly outperforms using 1-best and n-best lists. The margins are larger on the
out-of-domain test set.
4.2 Evaluation on Parallel Corpus Mining
In this section, we evaluate translation retrieval on the parallel corpus mining task: extracting a parallel
corpus from a comparable corpus.
4.2.1 Experimental Setup
The comparable corpus for extracting parallel sentences contains news articles published by Xinhua
News Agency from 1995 to 2010. Table 4 shows the detailed statistics. There are 1.2M Chinese and
1.7M English articles.
We re-implemented the method as described in (Munteanu and Marcu, 2005) as the baseline system.
2038
language articles sentences words vocabulary
Chinese 1.2M 18.5M 441.2M 2.1M
English 1.7M 17.8M 440.2M 3.4M
Table 4: The Xinhua News Comparable Corpus from 1995 to 2010
Munteanu and Marcu (2005) this work
English words Chinese words BLEU English words Chinese Words BLEU
5.00M 4.12M 22.84 5.00M 3.98M 25.44
10.00M 8.20M 25.10 10.00M 8.17M 26.62
15.00M 12.26M 25.41 15.00M 12.49M 26.49
20.00M 16.30M 25.56 20.00M 16.90M 26.87
Table 5: Comparison of BLEU scores using parallel corpora extracted by the baseline and our system.
Given a comparable corpus (see Table 4), both systems extract parallel corpora that are used for training
phrase-base models (Koehn et al., 2007). The baseline system is a re-implementation of the method
described in (Munteanu and Marcu, 2005). Our system uses translation option graph as query lattice.
Our system significantly outperforms the baseline for various sizes.
It assigned a score to each sentence pair using a classifier. Our system used translation option graph as
query lattices due to its simplicity and effectiveness. For each source sentence in the comparable corpus,
our system retrieved the top target sentence together with a score.
To evaluate the quality of extracted parallel corpus, we trained phrase-based models on it and ran
Moses on NIST datasets. The development set is the NIST 2005 test set and the test set is the NIST 2006
test set. The final evaluation metric is case-insensitive BLEU-4.
4.2.2 Evaluation Results
Table 5 shows the comparison of BLEU scores using parallel corpora extracted by the baseline and our
system. We find that our system significantly outperforms the baseline for various parallel corpus sizes.
This finding suggests that using lattice to compactly represent exponentially many alternatives does help
to alleviate the translation error propagation problem and identify parallel sentences of high translational
equivalence.
5 Conclusion
In this work, we propose to use query lattice to address the translation error propagation problem in
translation retrieval. Two kinds of query lattices are used in our experiments: search graph and translation
option graph. We show that translation option graph is more compact and represents a much larger
search space. Our experiments on Chinese-English datasets show that using query lattices significantly
outperforms using n-best lists in the retrieval task. Moreover, we show that translation retrieval is capable
of extracting high-quality parallel corpora from a comparable corpus. In the future, we plan to apply
our approach to retrieving translation candidates directly from the Web, which can be seen as a huge
monolingual corpus.
Acknowledgments
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013 and No. 61033001), the 863 Program (No. 2012AA011102),
Toshiba Corporation Corporate Research & Development Center, and the Singapore National Research
Foundation under its International Research Centre @ Singapore Funding Initiative and administered by
the IDM Programme.
2039
References
T. Baldwin and H. Tanaka. 2000. The effects of word order and segmentation on translation retrieval performance.
In Proceedings of COLING.
Timothy Baldwin. 2001. Low-cost, high-performance translation retrieval: Dumber is better. In Proceedings of
ACL, pages 18?25, Toulouse, France, July. Association for Computational Linguistics.
Karen Cheung and Douglas Vogel. 2005. Complexity reduction in lattice-based information retrieval. Information
Retrieval, pages 285?299.
Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee Tou Ng. 2010. Statistical lattice-based spoken document
retrieval. ACM Transactions on Information Systems, 28(1).
Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-HLT, pages 1012?1020, Columbus, Ohio, June. Association for Computational Linguistics.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Yajuan L?u. 2009. Lattice-based system combination for statis-
tical machine translation. In Proceedings of EMNLP, pages 1105?1113, Singapore, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL - Demo
and Poster Sessions, pages 177?180, Prague, Czech Republic, June. Association for Computational Linguistics.
Chunyang Liu, Qi Liu, Yang Liu, and Maosong Sun. 2012. THUTR: A translation retrieval system. In Proceed-
ings of COLING - Demo and Poster Sessions, pages 321?328, Mumbai, India, December. The COLING 2012
Organizing Committee.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based minimum error rate
training for statistical machine translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Spyros Matsoukas, Ivan Bulyko, Bing Xiang, Kham Nguyen, Richard Schwartz, and John Makhoul. 2007. In-
tegrating speech recognition and machine translation. In Proceedings of ICASSP, volume 4, pages IV?1281.
IEEE.
C.N. Moore. 1958. A mathematical theory of the use of language symbols in retrieval. In ICSI 1958.
Dragos Munteanu and Daniel Marcu. 2005. Improving machine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 31(4):477?1504.
S. Nirenburg, C. Domashnev, and D.J. Grannes. 1993. Two approaches to matching in example-based machine
translation. In TMI 1993.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL, pages 295?302, Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan, July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL, pages 311?318, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Uta Priss. 2000. Lattice-based information retrieval. Knwoledge Organization, 27(3):132?142.
Murat Saraclar and Richard Sproat. 2004. Lattice-based search for spoken utterance retrieval. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-NAACL, pages 129?136, Boston, Massachusetts, USA, May.
Association for Computational Linguistics.
S. Sato and M. Nagao. 1990. Toward memory-based translation. In Proceedings of COLING.
Andreas Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proceedings of ICSLP.
2040
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding
for statistical machine translation. In Proceedings of EMNLP, pages 620?629, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated chinese word segmentation in
statistical machine translation. In Proceedings of IWSLT 2005, pages 141?147, Pittsburgh, PA, October.
Zheng-Yu Zhou, Peng Yu, Ciprian Chelba, and Frank Seide. 2006. Towards spoken-document retrieval for the
internet: Lattice indexing for large-scale web-search architectures. In Proceedings of HLT-NAACL, pages 415?
422, New York City, USA, June. Association for Computational Linguistics.
2041
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 366?376,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Keyphrase Extraction via Topic Decomposition
Zhiyuan Liu, Wenyi Huang, Yabin Zheng and Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, harrywy, yabin.zheng}@gmail.com,
sms@tsinghua.edu.cn
Abstract
Existing graph-based ranking methods for
keyphrase extraction compute a single impor-
tance score for each word via a single ran-
dom walk. Motivated by the fact that both
documents and words can be represented by
a mixture of semantic topics, we propose to
decompose traditional random walk into mul-
tiple random walks specific to various topics.
We thus build a Topical PageRank (TPR) on
word graph to measure word importance with
respect to different topics. After that, given
the topic distribution of the document, we fur-
ther calculate the ranking scores of words and
extract the top ranked ones as keyphrases. Ex-
perimental results show that TPR outperforms
state-of-the-art keyphrase extraction methods
on two datasets under various evaluation met-
rics.
1 Introduction
Keyphrases are defined as a set of terms in a doc-
ument that give a brief summary of its content for
readers. Automatic keyphrase extraction is widely
used in information retrieval and digital library (Tur-
ney, 2000; Nguyen and Kan, 2007). Keyphrase ex-
traction is also an essential step in various tasks of
natural language processing such as document cate-
gorization, clustering and summarization (Manning
and Schutze, 2000).
There are two principled approaches to extracting
keyphrases: supervised and unsupervised. The su-
pervised approach (Turney, 1999) regards keyphrase
extraction as a classification task, in which a model
is trained to determine whether a candidate phrase
is a keyphrase. Supervised methods require a doc-
ument set with human-assigned keyphrases as train-
ing set. In Web era, articles increase exponentially
and change dynamically, which demands keyphrase
extraction to be efficient and adaptable. However,
since human labeling is time consuming, it is im-
practical to label training set from time to time.
We thus focus on the unsupervised approach in this
study.
In the unsupervised approach, graph-based rank-
ing methods are state-of-the-art (Mihalcea and Ta-
rau, 2004). These methods first build a word graph
according to word co-occurrences within the docu-
ment, and then use random walk techniques (e.g.,
PageRank) to measure word importance. After that,
top ranked words are selected as keyphrases.
Existing graph-based methods maintain a single
importance score for each word. However, a docu-
ment (e.g., news article or research article) is usu-
ally composed of multiple semantic topics. Taking
this paper for example, it refers to two major top-
ics, ?keyphrase extraction? and ?random walk?. As
words are used to express various meanings corre-
sponding to different semantic topics, a word will
play different importance roles in different topics
of the document. For example, the words ?phrase?
and ?extraction? will be ranked to be more impor-
tant in topic ?keyphrase extraction?, while the words
?graph? and ?PageRank? will be more important in
topic ?random walk?. Since they do not take topics
into account, graph-based methods may suffer from
the following two problems:
1. Good keyphrases should be relevant to the ma-
jor topics of the given document. In graph-
based methods, the words that are strongly con-
nected with other words tend to be ranked high,
366
which do not necessarily guarantee they are rel-
evant to major topics of the document.
2. An appropriate set of keyphrases should also
have a good coverage of the document?s ma-
jor topics. In graph-based methods, the ex-
tracted keyphrases may fall into a single topic
of the document and fail to cover other substan-
tial topics of the document.
To address the problem, it is intuitive to consider
the topics of words and document in random walk
for keyphrase extraction. In this paper, we pro-
pose to decompose traditional PageRank into multi-
ple PageRanks specific to various topics and obtain
the importance scores of words under different top-
ics. After that, with the help of the document topics,
we can further extract keyphrases that are relevant
to the document and at the same time have a good
coverage of the document?s major topics. We call
the topic-decomposed PageRank as Topical PageR-
ank (TPR).
In experiments we find that TPR can extract
keyphrases with high relevance and good cover-
age, which outperforms other baseline methods un-
der various evaluation metrics on two datasets. We
also investigate the performance of TPR with dif-
ferent parameter values and demonstrate its robust-
ness. Moreover, TPR is unsupervised and language-
independent, which is applicable in Web era with
enormous information.
TPR for keyphrase extraction is a two-stage pro-
cess:
1. Build a topic interpreter to acquire the topics of
words and documents.
2. Perform TPR to extract keyphrases for docu-
ments.
We will introduce the two stages in Section 2 and
Section 3.
2 Building Topic Interpreters
To run TPR on a word graph, we have to acquire
topic distributions of words. There are roughly two
approaches that can provide topics of words: (1) Use
manually annotated knowledge bases, e.g., Word-
Net (Miller et al, 1990); (2) Use unsupervised ma-
chine learning techniques to obtain word topics from
a large-scale document collection. Since the vocab-
ulary in WordNet cannot cover many words in mod-
ern news and research articles, we employ the sec-
ond approach to build topic interpreters for TPR.
In machine learning, various methods have been
proposed to infer latent topics of words and docu-
ments. These methods, known as latent topic mod-
els, derive latent topics from a large-scale document
collection according to word occurrence informa-
tion. Latent Dirichlet Allocation (LDA) (Blei et al,
2003) is a representative of topic models. Com-
pared to Latent Semantic Analysis (LSA) (Landauer
et al, 1998) and probabilistic LSA (pLSA) (Hof-
mann, 1999), LDA has more feasibility for inference
and can reduce the risk of over-fitting.
In LDA, each word w of a document d is regarded
to be generated by first sampling a topic z from d?s
topic distribution ?(d), and then sampling a word
from the distribution over words ?(z) that charac-
terizes topic z. In LDA, ?(d) and ?(z) are drawn
from conjugate Dirichlet priors ? and ?, separately.
Therefore, ? and ? are integrated out and the prob-
ability of word w given document d and priors is
represented as follows:
pr(w|d, ?, ?) =
K
?
z=1
pr(w|z, ?)pr(z|d, ?), (1)
where K is the number of topics.
Using LDA, we can obtain the topic distribution
of each word w, namely pr(z|w) for topic z ? K.
The word topic distributions will be used in TPR.
Moreover, using the obtained word topic distribu-
tions, we can infer the topic distribution of a new
document (Blei et al, 2003), namely pr(z|d) for
each topic z ? K, which will be used for ranking
keyphrases.
3 Topical PageRank for Keyphrase
Extraction
After building a topic interpreter to acquire the
topics of words and documents, we can perform
keyphrase extraction for documents via TPR. Given
a document d, the process of keyphrase extraction
using TPR consists of the following four steps which
is also illustrated in Fig. 1:
1. Construct a word graph for d according to word
co-occurrences within d.
367
Figure 1: Topical PageRank for Keyphrase Extraction.
2. Perform TPR to calculate the importance
scores for each word with respect to different
topics.
3. Using the topic-specific importance scores of
words, rank candidate keyphrases respect to
each topic separately.
4. Given the topics of document d, integrate the
topic-specific rankings of candidate keyphrases
into a final ranking, and the top ranked ones are
selected as keyphrases.
3.1 Constructing Word Graph
We construct a word graph according to word co-
occurrences within the given document, which ex-
presses the cohesion relationship between words
in the context of document. The document is re-
garded as a word sequence, and the link weights be-
tween words is simply set to the co-occurrence count
within a sliding window with maximum W words in
the word sequence.
It was reported in (Mihalcea and Tarau, 2004)
the graph direction does not influence the perfor-
mance of keyphrase extraction very much. In this
paper we simply construct word graphs with direc-
tions. The link directions are determined as follows.
When sliding a W -width window, at each position,
we add links from the first word pointing to other
words within the window. Since keyphrases are usu-
ally noun phrases, we only add adjectives and nouns
in word graph.
3.2 Topical PageRank
Before introducing TPR, we first give some formal
notations. We denote G= (V,E) as the graph of a
document, with vertex set V = {w1, w2, ? ? ? , wN}
and link set (wi, wj) ? E if there is a link from
wi to wj . In a word graph, each vertex represents
a word, and each link indicates the relatedness be-
tween words. We denote the weight of link (wi, wj)
as e(wi, wj), and the out-degree of vertex wi as
O(wi)=
?
j:wi?wj e(wi, wj).
Topical PageRank is based on PageRank (Page et
al., 1998). PageRank is a well known ranking al-
gorithm that uses link information to assign global
importance scores to web pages. The basic idea of
PageRank is that a vertex is important if there are
other important vertices pointing to it. This can be
regarded as voting or recommendation among ver-
tices. In PageRank, the score R(wi) of word wi is
defined as
R(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
R(wj) + (1? ?)
1
|V | ,
(2)
where ? is a damping factor range from 0 to 1, and
|V | is the number of vertices. The damping fac-
tor indicates that each vertex has a probability of
(1 ? ?) to perform random jump to another vertex
within this graph. PageRank scores are obtained by
running Eq. (2) iteratively until convergence. The
second term in Eq. (2) can be regarded as a smooth-
ing factor to make the graph fulfill the property of
being aperiodic and irreducible, so as to guarantee
that PageRank converges to a unique stationary dis-
368
tribution. In PageRank, the second term is set to be
the same value 1|V | for all vertices within the graph,
which indicates there are equal probabilities of ran-
dom jump to all vertices.
In fact, the second term of PageRank in Eq. (2)
can be set to be non-uniformed. Suppose we as-
sign larger probabilities to some vertices, the final
PageRank scores will prefer these vertices. We call
this Biased PageRank.
The idea of Topical PageRank (TPR) is to run
Biased PageRank for each topic separately. Each
topic-specific PageRank prefers those words with
high relevance to the corresponding topic. And
the preferences are represented using random jump
probabilities of words.
Formally, in the PageRank of a specific topic
z, we will assign a topic-specific preference value
pz(w) to each word w as its random jump proba-
bility with
?
w?V pz(w) = 1. The words that are
more relevant to topic z will be assigned larger prob-
abilities when performing the PageRank. For topic
z, the topic-specific PageRank scores are defined as
follows:
Rz(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
Rz(wj)+(1??)pz(wi).
(3)
In Fig. 1, we show an example with two topics. In
this figure, we use the size of circles to indicate how
relevant the word is to the topic. In the PageRanks
of the two topics, high preference values will be as-
signed to different words with respect to the topic.
Finally, the words will get different PageRank val-
ues in the two PageRanks.
The setting of preference values pz(w) will have
a great influence to TPR. In this paper we use three
measures to set preference values for TPR:
? pz(w) = pr(w|z), is the probability that word
w occurs given topic z. This indicates how
much that topic z focuses on word w.
? pz(w) = pr(z|w), is the probability of topic z
given word w. This indicates how much that
word w focuses on topic z.
? pz(w) = pr(w|z) ? pr(z|w), is the product of
hub and authority values. This measure is in-
spired by the work in (Cohn and Chang, 2000).
Both PageRank and TPR are all iterative algo-
rithms. We terminate the algorithms when the num-
ber of iterations reaches 100 or the difference of each
vertex between two neighbor iterations is less than
0.001.
3.3 Extract Keyphrases Using Ranking Scores
After obtaining word ranking scores using TPR, we
begin to rank candidate keyphrases. As reported in
(Hulth, 2003), most manually assigned keyphrases
turn out to be noun phrases. We thus select noun
phrases from a document as candidate keyphrases
for ranking.
The candidate keyphrases of a document is ob-
tained as follows. The document is first tokenized.
After that, we annotate the document with part-
of-speech (POS) tags 1. Third, we extract noun
phrases with pattern (adjective)*(noun)+,
which represents zero or more adjectives followed
by one or more nouns. We regard these noun phrases
as candidate keyphrases.
After identifying candidate keyphrases, we rank
them using the ranking scores obtained by TPR.
In PageRank for keyphrase extraction, the ranking
score of a candidate keyphrase p is computed by
summing up the ranking scores of all words within
the phrase: R(p)=
?
wi?p R(wi) (Mihalcea and Ta-
rau, 2004; Wan and Xiao, 2008a; Wan and Xiao,
2008b). Then candidate keyphrases are ranked in
descending order of ranking scores. The top M can-
didates are selected as keyphrases.
In TPR for keyphrase extraction, we first com-
pute the ranking scores of candidate keyphrases sep-
arately for each topic. That is for each topic z we
compute
Rz(p) =
?
wi?p
Rz(wi). (4)
By considering the topic distribution of document,
We further integrate topic-specific rankings of can-
didate keyphrases into a final ranking and extract
top-ranked ones as the keyphrases of the document.
Denote the topic distribution of the document d
as pr(z|d) for each topic z. For each candidate
keyphrase p, we compute its final ranking score as
1In experiments we use Stanford POS Tagger from http:
//nlp.stanford.edu/software/tagger.shtml
with English tagging model left3words-distsim-wsj.
369
follows:
R(p) =
K
?
z=1
Rz(p)? pr(z|d). (5)
After ranking candidate phrases in descending order
of their integrated ranking scores, we select the top
M as the keyphrases of document d.
4 Experiments
4.1 Datasets
To evaluate the performance of TPR for keyphrase
extraction, we carry out experiments on two
datasets.
One dataset was built by Wan and Xiao 2 which
was used in (Wan and Xiao, 2008b). This dataset
contains 308 news articles in DUC2001 (Over et al,
2001) with 2, 488 manually annotated keyphrases.
There are at most 10 keyphrases for each document.
In experiments we refer to this dataset as NEWS.
The other dataset was built by Hulth 3 which was
used in (Hulth, 2003). This dataset contains 2, 000
abstracts of research articles and 19, 254 manually
annotated keyphrases. In experiments we refer to
this dataset as RESEARCH.
Since neither NEWS nor RESEARCH itself is
large enough to learn efficient topics, we use the
Wikipedia snapshot at March 2008 4 to build topic
interpreters with LDA. After removing non-article
pages and the articles shorter than 100 words, we
collected 2, 122, 618 articles. After tokenization,
stop word removal and word stemming, we build the
vocabulary by selecting 20, 000 words according to
their document frequency. We learn LDA models by
taking each Wikipedia article as a document. In ex-
periments we learned several models with different
numbers of topics, from 50 to 1, 500 respectively.
For the words absent in topic models, we simply set
the topic distribution of the word as uniform distri-
bution.
4.2 Evaluation Metrics
For evaluation, the words in both standard and ex-
tracted keyphrases are reduced to base forms using
2http://wanxiaojun1979.googlepages.com.
3It was obtained from the author.
4http://en.wikipedia.org/wiki/Wikipedia_
database.
Porter Stemmer 5 for comparison. In experiments
we select three evaluation metrics.
The first metric is precision/recall/F-measure rep-
resented as follows,
p = ccorrectcextract
, r = ccorrectcstandard
, f = 2prp+ r , (6)
where ccorrect is the total number of correct
keyphrases extracted by a method, cextract the to-
tal number of automatic extracted keyphrases, and
cstandard the total number of human-labeled stan-
dard keyphrases.
We note that the ranking order of extracted
keyphrases also indicates the method performance.
An extraction method will be better than another one
if it can rank correct keyphrases higher. However,
precision/recall/F-measure does not take the order
of extracted keyphrases into account. To address the
problem, we select the following two additional met-
rics.
One metric is binary preference measure
(Bpref) (Buckley and Voorhees, 2004). Bpref is
desirable to evaluate the performance considering
the order in which the extracted keyphrases are
ranked. For a document, if there are R correct
keyphrases within M extracted keyphrases by a
method, in which r is a correct keyphrase and n is
an incorrect keyphrase, Bpref is defined as follows,
Bpref = 1R
?
r?R
1? |n ranked higher than r|M . (7)
The other metric is mean reciprocal rank
(MRR) (Voorhees, 2000) which is used to evaluate
how the first correct keyphrase for each document is
ranked. For a document d, rankd is denoted as the
rank of the first correct keyphrase with all extracted
keyphrases, MRR is defined as follows,
MRR =
1
|D|
?
d?D
1
rankd
, (8)
where D is the document set for keyphrase extrac-
tion.
Note that although the evaluation scores of most
keyphrase extractors are still lower compared to
5http://tartarus.org/
?
martin/
PorterStemmer.
370
other NLP-tasks, it does not indicate the perfor-
mance is poor because even different annotators may
assign different keyphrases to the same document.
4.3 Influences of Parameters to TPR
There are four parameters in TPR that may influence
the performance of keyphrase extraction including:
(1) window size W for constructing word graph, (2)
the number of topics K learned by LDA, (3) dif-
ferent settings of preference values pz(w), and (4)
damping factor ? of TPR.
In this section, we look into the influences of these
parameters to TPR for keyphrase extraction. Except
the parameter under investigation, we set parameters
to the following values: W =10, K=1, 000, ?=0.3
and pz(w) = pr(z|w), which are the settings when
TPR achieves the best (or near best) performance on
both NEWS and RESEARCH. In the following tables,
we use ?Pre.?, ?Rec.? and ?F.? as the abbreviations
of precision, recall and F-measure.
4.3.1 Window Size W
In experiments on NEWS, we find that the perfor-
mance of TPR is stable when W ranges from 5 to 20
as shown in Table 1. This observation is consistent
with the findings reported in (Wan and Xiao, 2008b).
Size Pre. Rec. F. Bpref MRR
5 0.280 0.345 0.309 0.213 0.636
10 0.282 0.348 0.312 0.214 0.638
15 0.282 0.347 0.311 0.214 0.646
20 0.284 0.350 0.313 0.215 0.644
Table 1: Influence of window size W when the num-
ber of keyphrases M=10 on NEWS.
Similarly, when W ranges from 2 to 10, the per-
formance on RESEARCH does not change much.
However, the performance on NEWS will become
poor when W = 20. This is because the abstracts
in RESEARCH (there are 121 words per abstract on
average) are much shorter than the news articles
in NEWS (there are 704 words per article on av-
erage). If the window size W is set too large on
RESEARCH, the graph will become full-connected
and the weights of links will tend to be equal, which
cannot capture the local structure information of ab-
stracts for keyphrase extraction.
4.3.2 The Number of Topics K
We demonstrate the influence of the number of
topics K of LDA models in Table 2. Table 2 shows
the results when K ranges from 50 to 1, 500 and
M =10 on NEWS. We observe that the performance
does not change much as the number of topics
varies until the number is much smaller (K = 50).
The influence is similar on RESEARCH which indi-
cates that LDA is appropriate for obtaining topics of
words and documents for TPR to extract keyphrases.
K Pre. Rec. F. Bpref MRR
50 0.268 0.330 0.296 0.204 0.632
100 0.276 0.340 0.304 0.208 0.632
500 0.284 0.350 0.313 0.215 0.648
1000 0.282 0.348 0.312 0.214 0.638
1500 0.282 0.348 0.311 0.214 0.631
Table 2: Influence of the number of topics K when
the number of keyphrases M=10 on NEWS.
4.3.3 Damping Factor ?
Damping factor ? of TPR reconciles the influ-
ences of graph walks (the first term in Eq.(3)) and
preference values (the second term in Eq.(3)) to the
topic-specific PageRank scores. We demonstrate
the influence of ? on NEWS in Fig. 2. This fig-
ure shows the precision/recall/F-measure when ? =
0.1, 0.3, 0.5, 0.7, 0.9 and M ranges from 1 to 20.
From this figure we find that, when ? is set from 0.2
to 0.7, the performance is consistently good. The
values of Bpref and MRR also keep stable with the
variations of ?.
4.3.4 Preference Values
Finally, we explore the influences of different set-
tings of preference values for TPR in Eq.(3). In Ta-
ble 3 we show the influence when the number of
keyphrases M = 10 on NEWS. From the table, we
observe that pr(z|w) performs the best. The similar
observation is also got on RESEARCH.
In keyphrase extraction task, it is required to find
the keyphrases that can appropriately represent the
topics of the document. It thus does not want to ex-
tract those phrases that may appear in multiple top-
ics like common words. The measure pr(w|z) as-
signs preference values according to how frequently
that words appear in the given topic. Therefore, the
371
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Keyphrase Number
Pr
ec
is
io
n
 
 
?=0.1
?=0.3
?=0.5
?=0.7
?=0.9
(a) Precision
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Keyphrase Number
R
ec
al
l
 
 
?=0.1
?=0.3
?=0.5
?=0.7
?=0.9
(b) Recall
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
Keyphrase Number
F?
m
ea
su
re
 
 
?=0.1
?=0.3
?=0.5
?=0.7
?=0.9
(c) F-measure
Figure 2: Precision, recall and F-measure of TPR with ?=0.1, 0.3, 0.5, 0.7 and 0.9 when M ranges from 1
to 20 on NEWS.
common words will always be assigned to a rela-
tively large value in each topic-specific PageRank
and finally obtain a high rank. pr(w|z) is thus not a
good setting of preference values in TPR. In the con-
trast, pr(z|w) prefers those words that are focused
on the given topic. Using pr(z|w) to set preference
values for TPR, we will tend to extract topic-focused
phrases as keyphrases.
Pref Pre. Rec. F. Bpref MRR
pr(w|z) 0.256 0.316 0.283 0.192 0.584
pr(z|w) 0.282 0.348 0.312 0.214 0.638
prod 0.259 0.320 0.286 0.193 0.587
Table 3: Influence of three preference value settings
when the number of keyphrases M=10 on NEWS.
4.4 Comparing with Baseline Methods
After we explore the influences of parameters to
TPR, we obtain the best results on both NEWS and
RESEARCH. We further select three baseline meth-
ods, i.e., TFIDF, PageRank and LDA, to compare
with TPR.
The TFIDF computes the ranking scores of words
based on words? tfidf values in the document,
namely R(w) = tfw ? log(idfw). While in PageR-
ank (i.e., TextRank), the ranking scores of words are
obtained using Eq.(2). The two baselines do not use
topic information of either words or documents. The
LDA computes the ranking score for each word us-
ing the topical similarity between the word and the
document. Given the topics of the document d and
a word w, We have used various methods to com-
pute similarity including cosine similarity, predic-
tive likelihood and KL-divergence (Heinrich, 2005),
among which cosine similarity performs the best on
both datasets. Therefore, we only show the results of
the LDA baseline calculated using cosine similarity.
In Tables 4 and 5 we show the compar-
ing results of the four methods on both NEWS
and RESEARCH. Since the average number of
manual-labeled keyphrases on NEWS is larger than
RESEARCH, we set M = 10 for NEWS and M =
5 for RESEARCH. The parameter settings on both
NEWS and RESEARCH have been stated in Section
4.3.
Method Pre. Rec. F. Bpref MRR
TFIDF 0.239 0.295 0.264 0.179 0.576
PageRank 0.242 0.299 0.267 0.184 0.564
LDA 0.259 0.320 0.286 0.194 0.518
TPR 0.282 0.348 0.312 0.214 0.638
Table 4: Comparing results on NEWS when the num-
ber of keyphrases M=10.
Method Pre. Rec. F. Bpref MRR
TFIDF 0.333 0.173 0.227 0.255 0.565
PageRank 0.330 0.171 0.225 0.263 0.575
LDA 0.332 0.172 0.227 0.254 0.548
TPR 0.354 0.183 0.242 0.274 0.583
Table 5: Comparing results on RESEARCHwhen the
number of keyphrases M=5.
From the two tables, we have the following obser-
vations.
372
0.2 0.25 0.3 0.35 0.4 0.45 0.50
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Precision
R
ec
al
l
 
 
TFIDF
PageRank
LDA
TPR
Figure 3: Precision-recall results on NEWS when M
ranges from 1 to 20.
First, TPR outperform all baselines on both
datasets. The improvements are all statistically sig-
nificant tested with bootstrap re-sampling with 95%
confidence. This indicates the robustness and effec-
tiveness of TPR.
Second, LDA performs equal or better than
TFIDF and PageRank under precision/recall/F-
measure. However, the performance of LDA un-
der MRR is much worse than TFIDF and PageR-
ank, which indicates LDA fails to correctly extract
the first keyphrase earlier than other methods. The
reason is: (1) LDA does not consider the local struc-
ture information of document as PageRank, and (2)
LDA also does not consider the frequency infor-
mation of words within the document. In the con-
trast, TPR enjoys the advantages of both LDA and
TFIDF/PageRank, by using the external topic infor-
mation like LDA and internal document structure
like TFIDF/PageRank.
Moreover, in Figures 3 and 4 we show the
precision-recall relations of four methods on NEWS
and RESEARCH. Each point on the precision-recall
curve is evaluated on different numbers of extracted
keyphrases M . The closer the curve to the upper
right, the better the overall performance. The results
again illustrate the superiority of TPR.
4.5 Extracting Example
At the end, in Table 6 we show an example of
extracted keyphrases using TPR from a news arti-
cle with title ?Arafat Says U.S. Threatening to Kill
PLO Officials? (The article number in DUC2001
is AP880510-0178). Here we only show the top
10 keyphrases, and the correctly extracted ones
0.3 0.32 0.34 0.36 0.38 0.4 0.420
0.05
0.1
0.15
0.2
0.25
0.3
Precision
R
ec
al
l
 
 
TFIDF
PageRank
LDA
TPR
Figure 4: Precision-recall results on RESEARCH
when M ranges from 1 to 10.
are marked with ?(+)?. We also mark the num-
ber of correctly extracted keyphrases after method
name like ?(+7)? after TPR. We also illustrate the
top 3 topics of the document with their topic-
specific keyphrases. It is obvious that the top topics,
on ?Palestine?, ?Israel? and ?terrorism? separately,
have a good coverage on the discussion objects of
this article, which also demonstrate a good diversity
with each other. By integrating these topic-specific
keyphrases considering the proportions of these top-
ics, we obtain the best performance of keyphrase ex-
traction using TPR.
In Table 7 we also show the extracted keyphrases
of baselines from the same news article. For TFIDF,
it only considered the frequency properties of words,
and thus highly ranked the phrases with ?PLO?
which appeared about 16 times in this article, and
failed to extract the keyphrases on topic ?Israel?.
LDA only measured the importance of words using
document topics without considering the frequency
information of words and thus missed keyphrases
with high-frequency words. For example, LDA
failed to extract keyphrase ?political assassination?,
in which the word ?assassination? occurred 8 times
in this article.
5 Related Work
In this paper we proposed TPR for keyphrase ex-
traction. A pioneering achievement in keyphrase ex-
traction was carried out in (Turney, 1999) which re-
garded keyphrase extraction as a classification task.
Generally, the supervised methods need manually
annotated training set which is time-consuming and
in this paper we focus on unsupervised method.
373
TPR (+7)
PLO leader Yasser Arafat(+), Abu Jihad, Khalil
Wazir(+), slaying Wazir, political assassina-
tion(+), Palestinian guerrillas(+), particulary
Palestinian circles, Israeli officials(+), Israeli
squad(+), terrorist attacks(+)
TPR, Rank 1 Topic on ?Palestine?
PLO leader Yasser Arafat(+), United States(+),
State Department spokesman Charles Redman,
Abu Jihad, U.S. government document, Palestine
Liberation Organization leader, political assassi-
nation(+), Israeli officials(+), alleged document
TPR, Rank 2 Topic on ?Israel?
PLO leader Yasser Arafat(+), United States(+),
Palestine Liberation Organization leader, Israeli
officials(+), U.S. government document, alleged
document, Arab government, slaying Wazir, State
Department spokesman Charles Redman, Khalil
Wazir(+)
TPR, Rank 3 Topic on ?terrorism?
terrorist attacks(+), PLO leader Yasser Arafat(+),
Abu Jihad, United States(+), alleged docu-
ment, U.S. government document, Palestine Lib-
eration Organization leader, State Department
spokesman Charles Redman, political assassina-
tion(+), full cooperation
Table 6: Extracted keyphrases by TPR.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becoming
the most widely used unsupervised approach for
keyphrase extraction. Litvak and Last (2008) ap-
plied HITS algorithm on the word graph of a docu-
ment for keyphrase extraction. Although HITS itself
worked the similar performance to PageRank, we
plan to explore the integration of topics and HITS in
future work. Wan (2008b; 2008a) used a small num-
ber of nearest neighbor documents to provide more
knowledge for keyphrase extraction. Some meth-
ods used clustering techniques on word graphs for
keyphrase extraction (Grineva et al, 2009; Liu et
al., 2009). The clustering-based method performed
well on short abstracts (with F-measure 0.382 on
RESEARCH) but poorly on long articles (NEWS with
F-measure score 0.216) due to two non-trivial is-
sues: (1) how to determine the number of clus-
TFIDF (+5)
PLO leader Yasser Arafat(+), PLO attacks, PLO
offices, PLO officials(+), PLO leaders, Abu Ji-
had, terrorist attacks(+), Khalil Wazir(+), slaying
wazir, political assassination(+)
PageRank (+3)
PLO leader Yasser Arafat(+), PLO officials(+),
PLO attacks, United States(+), PLO offices, PLO
leaders, State Department spokesman Charles
Redman, U.S. government document, alleged
document, Abu Jihad
LDA (+5)
PLO leader Yasser Arafat(+), Palestine Liberation
Organization leader, Khalil Wazir(+), Palestinian
guerrillas(+), Abu Jihad, Israeli officials(+), par-
ticulary Palestinian circles, Arab government,
State Department spokesman Charles Redman,
Israeli squad(+)
Table 7: Extracted keyphrases by baselines.
ters, and (2) how to weight each cluster and select
keyphrases from the clusters. In this paper we fo-
cus on improving graph-based methods via topic de-
composition, we thus only compare with PageRank
as well as TFIDF and LDA and do not compare with
clustering-based methods in details.
In recent years, two algorithms were proposed to
rank web pages by incorporating topic information
of web pages within PageRank (Haveliwala, 2002;
Nie et al, 2006). The method in (Haveliwala, 2002),
is similar to TPR which also decompose PageRank
into various topics. However, the method in (Haveli-
wala, 2002) only considered to set the preference
values using pr(w|z) (In the context of (Haveliwala,
2002), w indicates Web pages). In Section 4.3.4 we
have shown that the setting of using pr(z|w) is much
better than pr(w|z).
Nie et al (2006) proposed a more complicated
ranking method. In this method, topical PageRanks
are performed together. The basic idea of (Nie et al,
2006) is, when surfing following a graph link from
vertex wi to wj , the ranking score on topic z of wi
will have a higher probability to pass to the same
topic of wj and have a lower probability to pass to
a different topic of wj . When the inter-topic jump
probability is 0, this method is identical to (Haveli-
374
wala, 2002). We implemented the method and found
that the random jumps between topics did not help
improve the performance for keyphrase extraction,
and did not demonstrate the results of this method.
6 Conclusion and Future Work
In this paper we propose a new graph-based frame-
work, Topical PageRank, which incorporates topic
information within random walk for keyphrase ex-
traction. Experiments on two datasets show that
TPR achieves better performance than other base-
line methods. We also investigate the influence of
various parameters on TPR, which indicates the ef-
fectiveness and robustness of the new method.
We consider the following research directions as
future work.
1. In this paper we obtained latent topics us-
ing LDA learned from Wikipedia. We de-
sign to obtain topics using other machine learn-
ing methods and from other knowledge bases,
and investigate the influence to performance of
keyphrase extraction.
2. In this paper we integrated topic information
in PageRank. We plan to consider topic infor-
mation in other graph-based ranking algorithms
such as HITS (Kleinberg, 1999).
3. In this paper we used Wikipedia to train
LDA by assuming Wikipedia is an exten-
sive snapshot of human knowledge which can
cover most topics talked about in NEWS and
RESEARCH. In fact, the learned topics are
highly dependent on the learning corpus. We
will investigate the influence of corpus selec-
tion in training LDA for keyphrase extraction
using TPR.
Acknowledgments
This work is supported by the National Natu-
ral Science Foundation of China under Grant No.
60873174. The authors would like to thank Anette
Hulth and Xiaojun Wan for kindly sharing their
datasets. The authors would also thank Xiance Si,
Tom Chao Zhou, Peng Li for their insightful sug-
gestions and comments.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
C. Buckley and E.M. Voorhees. 2004. Retrieval evalu-
ation with incomplete information. In Proceedings of
SIGIR, pages 25?32.
David Cohn and Huan Chang. 2000. Learning to prob-
abilistically identify authoritative documents. In Pro-
ceedings of ICML, pages 167?174.
M. Grineva, M. Grinev, and D. Lizorkin. 2009. Extract-
ing key terms from noisy and multi-theme documents.
In Proceedings of WWW, pages 661?670.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
Proceedings of WWW, pages 517?526.
G. Heinrich. 2005. Parameter estimation for text anal-
ysis. Web: http://www. arbylon. net/publications/text-
est.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of SIGIR, pages 50?57.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of EMNLP, pages 216?223.
J.M. Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604?
632.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the workshop Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17?24.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.
2009. Clustering to find exemplar terms for keyphrase
extraction. In Proceedings of EMNLP, pages 257?
266.
C.D. Manning and H. Schutze. 2000. Foundations of
statistical natural language processing. MIT Press.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, pages
404?411.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
WordNet: An on-line lexical database. International
Journal of Lexicography, 3:235?244.
Thuy Nguyen and Min-Yen Kan. 2007. Keyphrase ex-
traction in scientific publications. In Proceedings of
the 10th International Conference on Asian Digital Li-
braries, pages 317?326.
375
Lan Nie, Brian D. Davison, and Xiaoguang Qi. 2006.
Topical link analysis for web search. In Proceedings
of SIGIR, pages 91?98.
P. Over, W. Liggett, H. Gilbert, A. Sakharov, and
M. Thatcher. 2001. Introduction to duc-2001: An in-
trinsic evaluation of generic news text summarization
systems. In Proceedings of DUC2001.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Library Tech-
nologies Project, 1998.
Peter D. Turney. 1999. Learning to extract keyphrases
from text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303?336.
E.M. Voorhees. 2000. The trec-8 question answering
track report. In Proceedings of TREC, pages 77?82.
Xiaojun Wan and Jianguo Xiao. 2008a. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING,
pages 969?976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single document
keyphrase extraction using neighborhood knowledge.
In Proceedings of AAAI, pages 855?860.
376
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1577?1588,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Simple Word Trigger Method for Social Tag Suggestion
Zhiyuan Liu, Xinxiong Chen and Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, cxx.thu}@gmail.com, sms@tsinghua.edu.cn
Abstract
It is popular for users in Web 2.0 era to
freely annotate online resources with tags.
To ease the annotation process, it has been
great interest in automatic tag suggestion. We
propose a method to suggest tags according to
the text description of a resource. By consid-
ering both the description and tags of a given
resource as summaries to the resource written
in two languages, we adopt word alignment
models in statistical machine translation to
bridge their vocabulary gap. Based on the
translation probabilities between the words in
descriptions and the tags estimated on a large
set of description-tags pairs, we build a word
trigger method (WTM) to suggest tags accord-
ing to the words in a resource description.
Experiments on real world datasets show that
WTM is effective and robust compared with
other methods. Moreover, WTM is relatively
simple and efficient, which is practical for
Web applications.
1 Introduction
In Web 2.0, Web users often use tags to collect and
share online resources such as Web pages, photos,
videos, movies and books. Table 1 shows a book
entry annotated with multiple tags by users1. On
the top of Table 1 we list the title and a short
introduction of the novel ?The Count of Monte
Cristo?. The bottom half of Table 1 shows the
annotated tags, each of which is followed by a
number in bracket, the total number of users who
1The original record is obtained from the book review
website Douban (www.douban.com) in Chinese. Here we
translate it to English for comprehension.
use the tag to annotate this book. Since the tags of
a resource are annotated collaboratively by multiple
users, we also name these tags as social tags. For
a resource, we refer to the additional information,
such as the title and introduction of a book, as
description, and the user-annotated social tags as
annotation.
Description
Title: The Count of Monte Cristo
Intro: The Count of Monte Cristo is one of the most
popular fictions by Alexandre Dumas. The writing of
the work was completed in 1844. ...
Annotation
Dumas (2748), Count of Monte Cristo (2716), foreign
literature (1813), novel (1345), France (1096), classic
(1062), revenge (913), famous book (759), ...
Table 1: An example of social tagging. The number
in the bracket after each tag is the total count of users
that annotate the tag on this book.
Social tags concisely indicate the main content
of the given resource, and potentially reflect user
interests. Social tagging has thus been widely
studied and successfully applied in recommender
systems (Eck et al, 2007; Yanbe et al, 2007; Zhou
et al, 2010), trend detection and tracking (Hotho
et al, 2006), personalization (Wetzker et al, 2010),
advertising (Mirizzi et al, 2010), etc.
The task of automatic social tag suggestion is
to automatically recommend tags for a user when
he/she wants to annotate a resource. Social tag
suggestion, as a crucial component for social tag-
ging systems, can help users annotate resources.
Moreover, social tag suggestion is usually consid-
ered as an equivalent problem to modeling social
1577
tagging behaviors, which is playing a more and more
important role in social computing and information
retrieval (Wang et al, 2007).
Most online resources contain descriptions, which
usually contain much resource information. For
example, on a book review website, each book entry
contains a title, the author(s) and an introduction
of the book. Some researchers thus propose
to automatically suggest tags based on resource
descriptions, which are collectively known as the
content-based approach.
One may think to suggest tags by selecting
important words from descriptions. This is far from
enough because descriptions and annotations are
using diverse vocabularies, usually referred to as a
vocabulary gap problem. Take the book entry in
Table 1 for instance, the word ?popular? used in the
description contrasts the tags ?classic? and ?famous
book? in the annotation; the word ?novel? is used in
the description, while most users annotate with the
tag ?fiction?. The vocabulary gap usually reflects in
two main issues:
? Some tags in the annotation do appear in the
corresponding description, but they may not be
statistically significant.
? Some tags may even not appear in the descrip-
tion.
It is not trivial to reduce the vocabulary gap and
find the semantic correspondence between descrip-
tions and annotations. By regarding both the de-
scription and the annotation as parallel summaries
of a resource, we use word alignment models in
statistical machine translation (SMT) (Brown et
al., 1993) to estimate the translation probabilities
between the words in descriptions and annotations.
SMT has been successfully applied in many ap-
plications to bridge vocabulary gap. For detailed
descriptions of related work, readers can refer to
Section 2.2. In this paper, besides employing word
alignment models to social tagging, we also propose
a method to efficiently build description-annotation
pairs for sufficient learning translation probabilities
by word alignment models.
Based on the learned translation probabilities
between words in descriptions and annotations,
we regard the tagging behavior as a word trigger
process:
1. A user reads the resource description to realize
its substance by seeing some important words
in the description.
2. Triggered by these important words, the user
translates them into the corresponding tags, and
annotates the resource with these tags.
Based on this perspective, we build a simple word
trigger method (WTM) for social tag suggestion. In
Fig. 1, we use a simple example to show the basic
idea of using word trigger for social tag suggestion.
In this figure, some words in the first sentence of the
book description in Table 1 are triggered to the tags
in annotation.
Figure 1: An example of the word trigger method
for suggesting tags given a description.
2 Related Work
2.1 Social Tag Suggestion
Previous work has been proposed to automatic
social tag suggestion.
Many researchers built tag suggestion systems
based on collaborative filtering (CF) (Herlocker et
al., 1999; Herlocker et al, 2004), a widely used
technique in recommender systems (Resnick and
Varian, 1997). These collaboration-based methods
typically base their suggestions on the tagging
history of the given resource and user, without con-
sidering resource descriptions. FolkRank (Jaschke
et al, 2008) and Matrix Factorization (Rendle et al,
2009) are representative CF methods for social tag
suggestion. Most of these methods suffer from the
cold-start problem, i.e., they are not able to perform
effective suggestions for resources that no one has
annotated yet.
The content-based approach for social tag sug-
gestion remedies the cold-start problem of the
1578
collaboration-based approach by suggesting tags
according to resource descriptions. Therefore, the
content-based approach plays an important role in
social tag suggestion.
Some researchers regarded social tag suggestion
as a classification problem by considering each tag
as a category label (Ohkura et al, 2006; Mishne,
2006; Lee and Chun, 2007; Katakis et al, 2008;
Fujimura et al, 2008; Heymann et al, 2008).
Various classifiers such as Naive Bayes, kNN, SVM
and neural networks have been explored to solve the
social tag suggestion problem.
There are two issues emerging from the
classification-based methods:
? The annotations provided by users are noisy,
and the classification-based methods can not
handle the issue well.
? The training cost and classification cost of
many classification-based methods are usually
in proportion to the number of classification
labels. These methods may thus be inefficient
for a real-world social tagging system, where
hundreds of thousands of unique tags should be
considered as classification labels.
Inspired by the popularity of latent topic models
such as Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), various methods have been proposed to
model tags using generative latent topic models.
One intuitive approach is assuming that both tags
and words are generated from the same set of latent
topics. By representing both tags and descriptions
as the distributions of latent topics, this approach
suggests tags according to their likelihood given
the description (Krestel et al, 2009; Si and Sun,
2009). Bundschus et al (2009) proposed a joint
latent topic model of users, words and tags. Iwata
et al (2009) proposed an LDA-based topic model,
Content Relevance Model (CRM), which aimed at
finding the content-related tags for suggestion. Em-
pirical experiments showed that CRM outperformed
both classification methods and Corr-LDA (Blei and
Jordan, 2003), a generative topic model for contents
and annotations.
Most latent topic models have to pre-specify the
number of topics before training. We can either use
cross validation to determine the optimal number
of topics or employ the infinite topic models, such
as Hierarchical Dirichlet Process (HDP) (Teh et al,
2006) and nested Chinese Restaurant Process (Blei
et al, 2010), to automatically adjust the number
of topics during training. Both solutions are
usually computationally complicated. What is more
important, topic-based methods suggest tags by
measuring the topical relevance of tags and resource
descriptions. The latent topics are of concept-level
which are usually too general to precisely suggest
those specific tags such as named entities, e.g.,
the tags ?Dumas? and ?Count of Monte Cristo? in
Table 1. To remedy the problem, Si et al (2010)
proposed a generative model, Tag Allocation Model
(TAM), which considers the words in descriptions
as the possible topics to generate tags. However,
TAM assumes each tag can only have at most one
word as its reason. This is against the fact that a tag
may be annotated triggered by multiple words in the
description.
It should also be noted that social tag suggestion is
different from automatic keyphrase extraction (Tur-
ney, 2000; Frank et al, 1999; Liu et al, 2009a; Liu
et al, 2010b; Liu et al, 2011). Keyphrase extraction
aims at selecting terms from the given document
to represent the main topics of the document. On
the contrary, in social tag suggestion, the suggested
tags do not necessarily appear in the given resource
description. We can thus regard social tag sugges-
tion as a task of selecting appropriate tags from
a controlled tag vocabulary for the given resource
description.
2.2 Applications of SMT
SMT techniques have been successfully used in
many tasks of information retrieval and natural
language processing to bridge the vocabulary gap
between two types of objects. Some typical tasks are
document information retrieval (Berger and Laffer-
ty, 1999; Murdock and Croft, 2004; Karimzadehgan
and Zhai, 2010), question answering (Berger et al,
2000; Echihabi and Marcu, 2003; Soricut and Brill,
2006; Riezler et al, 2007; Surdeanu et al, 2008;
Xue et al, 2008), query expansions (Riezler et al,
2007; Riezler et al, 2008; Riezler and Liu, 2010),
paraphrasing (Quirk et al, 2004; Zhao et al, 2010a;
Zhao et al, 2010b), summarization (Banko et al,
2000), collocation extraction (Liu et al, 2009b;
1579
Liu et al, 2010c), keyphrase extraction (Liu et
al., 2011), sentiment analysis (Dalvi et al, 2009),
computational advertising (Ravi et al, 2010), and
image/video annotation and retrieval (Duygulu et
al., 2002; Jeon et al, 2003).
3 Word Trigger Method for Social Tag
Suggestion
3.1 Method Framework
We describe the word trigger method (WTM) for
social tag suggestion as a 3-stage process:
1. Preparing description-annotation pairs.
Given a collection of annotated resources, we first
prepare description-annotation pairs for learning
translation probabilities using word alignment mod-
els.
2. Learning a translation model. Given a
collection of description-annotation pairs, we adopt
IBM Model-1, a widely used word alignment model,
to learn the translation probabilities between words
in descriptions and tags in annotations.
3. Suggesting tags given a resource description.
After building translation probabilities between
words and tags, given a resource description, we
first compute the trigger power of each word in the
description and then suggest tags according to their
translation probabilities from the triggered words.
Before introducing the method in details, we
introduce the notations. In a social tagging system,
a resource is denoted as r ? R, where R is the set of
all resources. Each resource contains a description
and an annotation containing a set of tags. The
description dr of resource r can be regarded as a bag
of words wr = {(wi, ei)}Nri=1, where ei is the count
of word wi and Nr is the number of unique words
in r. The annotation ar of resource r is represented
as tr = {(ti, ei)}Mri=1, where ei is the count of tag ti
and Mr is the number of unique tags for r.
3.2 Preparing Description-Annotation Pairs
Learning translation probabilities requires a parallel
training dataset consisting of a number of aligned
sentence pairs. We assume the description and the
annotation of a resource as being written in two
distinct languages. We thus prepare our parallel
training dataset by pairing descriptions with anno-
tations.
The annotation of a resource is a bag of tags with
no position information. We thus select IBM Model-
1 (Brown et al, 1993) for training, which does not
take word position information into account on both
sides for each aligned pair.
In a social tagging system, the length of a
resource description is usually limited to hundreds
of words. Meanwhile, it is common that some
popular resources are annotated by multiple users
with thousands of tags. For example, the tag
Dumas is annotated by 2, 748 users for the book
in Table 1. We have to deal with the length-
unbalance between a resource description and its
corresponding annotation for two reasons.
? It is impossible to list all annotated tags on
the annotation side of a description-annotation
pair. The performance of word alignment
models will also suffer from the unbalanced
length of sentence pairs in the parallel training
data set (Och and Ney, 2003).
? Moreover, the annotated tags may have differ-
ent importance for the resource. It would be
unfair to treat these tags without distinction.
Here we propose a sampling method to pre-
pare length-balanced description-annotation pairs
for word alignment. The basic idea is to sample
a bag of tags from the annotation according to tag
weights and make the generated bag of tags with
comparable length with the description.
We consider two parameters when sampling tags.
First, we have to select a tag weighting type for
sampling. In this paper, we investigate two straight-
forward sampling types, including tag frequen-
cy (TFt) within the annotation and tag-frequency
inverse-resource-frequency (TF-IRFt). Given re-
source r, TFt and TF-IRFt of tag t are defined
as TFt = et/
?
t et and TF-IRFt = et/
?
t et ?
log
(
|R|/|?r?R Iet>0|
)
, where |?r?R Iet>0| in-
dicates the number of resources that have been
annotated with tag t.
Another parameter is the length ratio between the
description and the sampled annotation. We denote
the ratio as ? = |wr|/|tr|, where |wr| is the number
of words in the description and |tr| is the number of
tags in the annotation.
1580
3.3 Learning Translation Probabilities Using
Word Alignment Models
Suppose the source language is resource description
and the target language is resource annotation.
In IBM Model-1, the relationship of the source
language w = wJ1 and the target language t = tI1
is connected via a hidden variable describing an
alignment mapping from source position j to target
position aj :
Pr(wJ1 |tI1) =
?
aJ1
Pr(wJ1 , aJ1 |tI1). (1)
The alignment aJ1 also contains empty-word align-
ments aj = 0 which align source words to the
an empty word. IBM Model-1 can be trained
using Expectation-Maximization (EM) algorithm in
an unsupervised fashion, and obtains the translation
probabilities of two vocabularies, i.e., Pr(w|t),
where t is a tag and w is a word.
IBM Model-1 only produces one-to-many align-
ments from source language to target language.
The learned model is thus asymmetric. We will
learn translation models on two directions: one is
regarding descriptions as the source language and
annotations as the target language, and the other is
in reverse direction of the pairs. We denote the first
model as Prd2a and the latter as Pra2d. We further
define Pr(t|w) as the harmonic mean of the two
models:
Pr(t|w) ?
(
?/Pr d2a(t|w)+(1??)/Pr a2d(t|w)
)?1
,
(2)
where ? is the harmonic factor to combine the two
models. When ? = 1 or ? = 0, it simply uses model
Prd2a or Pra2d correspondingly.
3.4 Tag Suggestion Using Triggered Words and
Translation Probabilities
When given the description of a resource, we can
rank tags by computing the scores:
Pr(t|d = wd) =
?
w?wd
Pr(t|w) Pr(w|d), (3)
in which Pr(w|d) is the trigger power of the word w
in the description, which indicates the importance of
the word. According to the ranking scores, we can
suggest the top-ranked tags to users.
Here we explore three methods to compute the
trigger power of a word in a resource description:
TF-IRFw, TextRank and their product. TF-IRFw and
TextRank are two most widely adopted methods for
keyword extraction.
Similar to TF-IRFt mentioned in Section 3.2, TF-
IRFw considers both the local importance (TFw) and
global specification (IRFw).
TextRank (Mihalcea and Tarau, 2004) is a graph-
based method to compute term importance. Given
a resource description, TextRank first builds a term
graph by connecting the terms in the description
according to their semantic relations, and then run
PageRank algorithm (Page et al, 1998) to measure
the importance of each term in the graph. Readers
can refer to (Mihalcea and Tarau, 2004) for detailed
information.
We also use the product of TF-IRFw and Tex-
tRank to weight terms, which potentially takes both
global information and term relations into account.
Emphasize Tags Appearing In Description for
WTM (EWTM) In some social tagging systems,
the tags that appear in the resource description are
more likely to be selected by users for annotation.
Therefore, we propose to emphasize the tags in the
description by ranking tags as follows
Pr(t|d) =
?
w?wd
(
?It(w)+(1??) Pr(t|w)
)
Pr(w|d),
(4)
where It(w) is an indicator function which gets
value 1 when t = w and 0 when t 6= w; and ? is
the smooth factor with range ? ? [0.0, 1.0]. When
? = 1.0, it suggests tags simply according to their
trigger powers within the description, while when
? = 0.0, it does not emphasize the tags appearing in
the description and just suggests according to their
translation probabilities. In Section 4.4, we will
show the performance of EWTM.
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets In our experiments, we select two real
world datasets which are of diverse properties to
evaluate our methods. In Table 2 we show the
detailed statistical information of the two datasets.
1581
Data R W T N?w N?t
BOOK 70, 000 174, 748 46, 150 211.6 3.5
BIBTEX 158, 924 91, 277 50, 847 5.8 2.7
Table 2: Statistical information of two datasets. R,
W , T , N?w and N?t are the number of resources, the
vocabulary of descriptions, the vocabulary of tags,
the average number of words in each description
and the average number of tags in each resource,
respectively.
The first dataset, denoted as BOOK, is obtained
from a popular Chinese book review website www.
douban.com, which contains the descriptions of
books and the tags collaboratively annotated by
users. The second dataset, denoted as BIBTEX, is
obtained from an English online bibliography web-
site www.bibsonomy.org2. The dataset contains
the descriptions for academic papers (including the
title and note for each paper) and the tags annotated
by users. As shown in Table 2, the average length of
descriptions in the BIBTEX dataset is much shorter
than the BOOK dataset. Moreover, the BIBTEX
dataset does not provide how many times each tag
is annotated to a resource.
Evaluation Metrics We use precision, recall and
F-measure to evaluate the performance of tag sug-
gestion methods. For a resource, we denote the
original tags (gold standard) as Ta, the suggested
tags as Ts, and the correctly suggested tags as Ts ?
Ta. Precision, recall and F-measure are defined as
p = |Ts ? Ta||Ts|
, r = |Ts ? Ta||Ta|
, F = 2pr(p + r) .
(5)
The final evaluation scores are computed by micro-
averaging (i.e., averaging on resources of test set).
We perform 5-fold cross validation for each method
on all two datasets. In experiments, the number of
suggested tags M ranges from 1 to 10.
4.2 Comparing Results
Baseline Methods We select four content-based
algorithms as the baselines for comparison: Naive
Bayes (NB) (Manning et al, 2008), k nearest
neighbor algorithm (kNN) (Manning et al, 2008),
2The dataset can be obtained from http://www.kde.
cs.uni-kassel.de/bibsonomy/dumps
Content Relevance (CRM) model (Iwata et al,
2009) and Tag Allocation Model (TAM) (Si et al,
2010).
NB and kNN are two representative classification
methods. NB is a simple generative model, which
models the probability of each tag t given descrip-
tion d as
Pr(t|d) ? Pr(t)
?
w?d
Pr(w|t). (6)
Pr(t) is estimated by the frequency of the resources
annotated with the tag t. Pr(w|t) is estimated by the
frequency of the word w in the resource descriptions
annotated with the tag t. kNN is a widely used
classification method for tag suggestion, which
recommends tags to a resource according to the
annotated tags of similar resources measured using
vector space models (Manning et al, 2008).
CRM and TAM are selected to represent topic-
based methods for tag suggestion. CRM is an LDA-
based generative model. The number of latent topics
K is the key parameter for CRM. In experiments, we
evaluated the performance of CRM with different K
values, and here we only show the best one obtained
by setting K = 1, 024. TAM is also a generative
model which considers the words in descriptions as
the topics to further generate tags for the resource.
We set parameters for TAM as in (Si et al, 2010).
For comparison, we denote our method as WTM.
Complexity Analysis We compare the complexity
of these methods. We denote the number of training
iterations in CRM, TAM and WTM as I 3, and
the number of topics in CRM as K. For the
training phase, the complexity of NB is O(RN?wN?t),
kNN is O(1), TAM is O(IRN?wN?t), CRM is
O(IKRN?wN?t), and WTM is O(IRN?wN?t)4. When
suggesting for a given resource description with
length Nw, the complexity of NB is O(NwT ),
kNN is O(RN?wN?t), CRM is O(IKNwT ), TAM
3In fact, the numbers of iterations of the three methods are
different from each other. For simplicity, here we denote them
using the same notation.
4In more detail, the training phase of WTM contains
preparing parallel training dataset with O(RN?t) and learning
translation probabilities using word alignment models with
O(IRN?wN?t), where I is the number of iterations for learning
translation probabilities, and N?t is the average number of tags
for each resource after sampling.
1582
is O(INwT ) and WTM is O(NwT ). From the
analysis, we can see that WTM is a relatively simple
method for both training and suggestion. This is
especially valuable because WTM also shows good
effectiveness for tag suggestion compared with other
methods as we will shown later.
Parameter Settings We use GIZA++ (Och and
Ney, 2003)5 as IBM Model-1 to learn transla-
tion probabilities using description-annotation pairs
for WTM. The experimental results of WTM are
obtained by setting parameters as follows: tag
weighting type as TF-IRFt, length ratio ? = 1,
harmonic factor ? = 0.5 and the type of word trigger
strength as TF-IRFw. The influence of parameters to
WTM can be found in Section 4.3.
Experiment Results and Analysis In Fig. 2 we
show the precision-recall curves of NB, kNN, CRM
and WTM on two datasets. Each point of a
precision-recall curve represents different numbers
of suggested tags from M = 1 (bottom right, with
higher precision and lower recall) to M = 10
(upper left, with higher recall but lower precision)
respectively. The closer the curve to the upper right,
the better the overall performance of the method.
From Fig. 2, we observe that:
? WTM consistently performs the best on both
datasets. This indicates that WTM is robust and
effective for tag suggestion.
? The advantage of WTM is more significant on
the BOOK dataset. The reason is that WTM
can take a good advantage of annotation count
information of tags compared to other methods.
? The average length of resource descriptions is
short in the BIBTEX dataset, which makes
it difficult to determine the trigger powers of
words. But even on the BIBTEX dataset
with no count information of tags, WTM still
outperforms other methods especially when
recommending first several tags.
To further demonstrate the performance of WTM
and other baseline methods, in Table 3 we show the
5GIZA++ is freely available on code.google.com/p/
giza-pp. The toolkit is widely used for word alignment in
SMT. In this paper, we use the default setting of parameters for
training.
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65
R
ec
al
l
Precision
 WAM
 NB
 kNN
 CRM
 TAM
(a) BOOK
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
R
ec
al
l
Precision
 WAM
 NB
 kNN
 CRM
 TAM
(b) BIBTEX
Figure 2: Performance comparison between NB,
kNN, CRM, TAM and WTM on two datasets.
precision, recall and F-measure of NB, kNN, CRM,
TAM and WTM on BOOK dataset when suggesting
M = 3 tags6. Due to the limit of space, we only
show the variance of F-measure. In fact, WTM
achieves its best performance when M = 2, where
the F-measure of WTM is 0.370, outperforming
both CRM (F = 0.263) and TAM (F = 0.277) by
about 10%.
An Example In Table 4 we show top 10 tags
suggested by NB, CRM, TAM and WTM for the
book in Table 1. The number in bracket after
the name of each method is the count of correctly
suggested tags. The correctly suggested tags are
marked in bold face. We select not to show
6We select to show this number because it is near the average
number of tags for BOOK dataset
1583
Method Precision Recall F-measure
NB 0.271 0.302 0.247? 0.004
kNN 0.280 0.314 0.258? 0.002
CRM 0.292 0.323 0.266? 0.004
TAM 0.310 0.344 0.283? 0.001
WTM 0.368 0.452 0.355? 0.002
Table 3: Comparing results of NB, kNN, CRM,
TAM and WTM on BOOK dataset when suggesting
M = 3 tags.
the results of kNN because the tags suggested by
kNN are totally unrelated to the book due to the
insufficient finding of nearest neighbors.
From Table 4, we observe that NB, CRM and
TAM, as generative models, tend to suggest general
tags such as ?novel?, ?literature?, ?classic? and
?France?, and fail in suggesting specific tags such as
?Alexandre Dumas? and ?Count of Monte Cristo?.
On the contrary, WTM succeeds in suggesting both
general and specific tags related to the book.
NB (+6): novel, foreign literature, literature, his-
tory, Japan, classic, France, philosophy, America,
biography
CRM (+5): novel, foreign literature, literature, bi-
ography, philosophy, culture, France, British, comic,
history
TAM (+5): novel, sociology, finance, foreign liter-
ature, France, literature, biography, France litera-
ture, comic, China
WTM (+7): novel, Alexandre Dumas, history,
Count of Monte Cristo, foreign literature, biogra-
phy, suspense, comic, America, France
Table 4: Top 10 tags suggested by NB, CRM, TAM
and WTM for the book in Table 1.
In Table 5, we list four important words (using
TF-IRFw as weighting metric) of the description and
their corresponding tags with the highest translation
probabilities. The values in brackets are the proba-
bility of tag t given word w, Pr(t|w). For each word,
we eliminated the tags with the probability less than
0.1. We can see that the translation probabilities can
map the words in descriptions to their semantically
corresponding tags in annotations.
Count of Monte Cristo: Count of Monte Cristo
(0.728), Alexandre Dumas (0.270), . . .
Alexandre Dumas: Alexandre Dumas (0.966), . . .
revenge: foreign literature (0.168), classic (0.130),
martial arts (0.123), Alexandre Dumas (0.122), . . .
France: France (0.99), . . .
Table 5: Four important words (in bold face) in the
book description in Table 1 and their corresponding
tags with the highest translation probabilities.
4.3 Parameter Influences
We explore the parameter influences to WTM for
social tag suggestion. The parameters include
harmonic factor, length ratio, tag weighting types,
and types of word trigger strength. When inves-
tigating one parameter, we set other parameters
to be the values inducing the best performance
as mentioned in Section 4.2. Finally, we also
investigate the influence of training data size for
suggestion performance. In experiments we find
that WTM reveals similar trends on both the BOOK
dataset and the BIBTEX dataset. We thus only show
the experimental results on the BOOK dataset for
analysis.
Harmonic Factor In Fig. 3 we investigate the
influence of harmonic factor via the curves of F-
measure of WTM versus the number of suggested
tags on the BOOK dataset when harmonic factor ?
ranges from 0.0 to 1.0. As shown in Section 3.3,
harmonic factor ? controls the proportion between
model Prd2a and Pra2d.
From Fig. 3, we observe that neither single model
Prd2a (? = 1.0) nor Pra2d (? = 0.0) achieves
the best performance. When the two models are
combined by harmonic mean, the performance is
consistently better, especially when ? ranges from
0.2 to 0.6. This is reasonable because IBM Model-
1 constrains that only the term in source language
can be aligned to multiple terms in target language,
which makes the translation probability learned by a
single model be asymmetric.
Length Ratio Fig. 4 shows the influence of length
ratios on the BOOK dataset. From the figure, we
observe that the performance for tag suggestion is
robust as the length ratio varies, except when the
ratio breaks the default restriction of GIZA++ (i.e.,
1584
? = 10)7.
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
1 2 3 4 5 6 7 8 9 10
F-
m
ea
su
re
Number of Suggested Tags
? = 0.0
? = 0.2
? = 0.4
? = 0.5
? = 0.6
? = 0.8
? = 1.0
Figure 3: F-measure of WTM versus the number of
suggested tags on the BOOK dataset when harmonic
factor ? ranges from 0.0 to 1.0.
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
1 2 3 4 5 6 7 8 9 10
F-
m
ea
su
re
Number of Suggested Tags
? = 10/1
? = 10/3
? = 10/5
? = 1/1
? = 1/2
? = 1/5
Figure 4: F-measure of WTM versus the number of
suggested tags on the BOOK dataset when length
ratio ? ranges from 10/1 to 1/5.
Tag Weighting Types The influence of two
weighting types, TFt and TF-IRFt, on social tag
suggestion when M = 3 on the BOOK dataset
is shown in Table 6. TF-IRFt tends to select the
tags more specific to the resource while TFt tends
to select the most popular tags, because the latter
does not consider global information (the IRFt part).
7GIZA++ restricts the values of length ratio within [ 19 , 9] by
setting parameter maxfertility=10. From Fig. 4, we can
see when ? = 10, the performance becomes much worse since
GIZA++ will cut off the sentences out of range.
Table 6 verifies the analysis, where TF-IRFt is
slightly better than TFt.
Weighting Precision Recall F-measure
TFt 0.356 0.437 0.342? 0.002
TF-IRFt 0.368 0.452 0.355? 0.002
Table 6: Evaluation results for different tag weight-
ing types when M = 3 on the BOOK dataset.
Methods for Computing Word Trigger Power
In Table 7, we show the performance of social tag
suggestions on the BOOK dataset with different
methods for computing word trigger power. From
the table, we can see that there is not significant
difference between TF-IRFw and the product of TF-
IRFw and TextRank, while TextRank itself performs
the worst. This indicates that TextRank is less
competitive to measure word trigger power since it
does not take global information into consideration.
Weighting Precision Recall F-measure
TF-IRFw 0.368 0.452 0.355? 0.002
TextRank 0.345 0.424 0.332? 0.002
Product 0.368 0.451 0.354? 0.002
Table 7: Evaluation results for different methods for
computing word trigger powers when M = 3 on the
BOOK dataset.
Training Data Size We investigate the influence
of training data size for social tag suggestion. As
shown in Fig. 5, we increased the training data size
from 8, 000 to 56, 000 step by 8, 000, and carried
out evaluation on 4, 000 resources. The figure shows
that:
? When the training data size is small (e.g.,
8, 000), WTM can still achieve good sugges-
tion performance.
? As the training data size increases, the perfor-
mance of WTM improves, while the improve-
ment speed declines.
The observation indicates that WTM does not
require huge-size dataset to achieve good perfor-
mance.
1585
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.1  0.2  0.3  0.4  0.5  0.6  0.7
R
ec
al
l
Precision
  8,000
 16,000
 24,000
 32,000
 40,000
 48,000
 56,000
Figure 5: Precision-recall curves when the training
data size increases from 8, 000 thousand to 56, 000
thousand on the BOOK dataset.
Conclusion By analyzing the influences of pa-
rameters on WTM, we find that WTM is robust to
parameter variations.
4.4 Performance of EWTM
At the end of this section, we investigate the
performance of EWTM for social tag suggestion.
Here we simply set the smooth factor ? = 0.5.
As shown in Table 8, EWTM improves the
performance of WTM (in Table 7) on the BOOK
dataset when using TF-IRFw and the product as the
methods for computing the word trigger powers,
but decays when using TextRank. This verifies
that TF-IRFw is the best method to measure word
trigger powers for WTM. Table 8 indicates that
emphasizing the tags appearing in the descriptions
may enhance the suggestion power of the word
trigger method.
Weighting Precision Recall F-measure
TF-IRFw 0.385 0.472 0.371? 0.001
TextRank 0.344 0.423 0.332? 0.002
Product 0.374 0.457 0.360? 0.001
Table 8: The evaluation results of EWTM with dif-
ferent methods for computing word trigger powers
when M = 3 on the BOOK dataset.
However, the performance of EWTM on the
BIBTEX dataset decays much compared to WTM.
The F-measure of EWTM is only F = 0.229
compared with WTM F = 0.267. The main reason
of the decay is that: the resource descriptions in
the BIBTEX dataset are usually too short to provide
sufficient information to precisely emphasize tags.
In this case, EWTM may emphasize wrong tags and
drop correct tags.
The experimental results on EWTM suggest that,
the performance of EWTM is heavily influenced by
the length of resource descriptions. Therefore, we
have to analyze the characteristics of social tagging
systems to decide whether to emphasize the tags that
appear in the corresponding resource descriptions.
As future work, we will investigate the influence
of the smooth factor ? to EWTM. It is also worth
to investigate the problem when combining with
collaboration-based methods for social tag sugges-
tion.
5 Conclusion and Future Work
In this paper, we present a new perspective to social
tagging and propose the word trigger method for
social tag suggestion based on word alignment in
statistical machine translation. Experiments show
that our method is effective and efficient for social
tag suggestion compared to other baselines.
There are still several open problems that should
be further investigated:
1. We can exploit other word alignment methods
like log-linear models (Liu et al, 2010a) for
social tag suggestion.
2. We will ensemble WTM with other content-
based and collaboration-based methods to build
a practical social tag suggestion system.
3. WTM and EWTM can only suggest the tags
that have appeared in translation models. In
future, we plan to incorporate keyphrase ex-
traction in social tag suggestion to make it
suggest more appropriate tags not only from
translation models but also from the resource
descriptions.
Acknowledgments
This work is supported by the National Natural
Science Foundation of China (NSFC) under Grant
No. 60873174. The authors would like to thank
Peng Li for his insightful suggestions and thank the
anonymous reviewers for their helpful comments.
1586
References
M. Banko, V.O. Mittal, and M.J. Witbrock. 2000.
Headline generation based on statistical translation. In
Proceedings of ACL, pages 318?325.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222?229.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approaches to answer-finding. In Proceedings of
SIGIR, pages 192?199.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of SIGIR, pages 127?134.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. JMLR, 3:993?1022.
D.M. Blei, T.L. Griffiths, and M.I. Jordan. 2010.
The nested chinese restaurant process and bayesian
nonparametric inference of topic hierarchies. Journal
of the ACM, 57(2):7.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
M. Bundschus, S. Yu, V. Tresp, A. Rettinger, M. Dejori,
and H.P. Kriegel. 2009. Hierarchical bayesian models
for collaborative tagging systems. In Proceedings of
ICDM, pages 728?733.
N. Dalvi, R. Kumar, B. Pang, and A. Tomkins. 2009. A
translation model for matching reviews to objects. In
Proceeding of CIKM, pages 167?176.
P. Duygulu, K. Barnard, J. De Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary.
Proceedings of ECCV, pages 97?112.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Proceedings of
ACL, pages 16?23.
D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green.
2007. Automatic generation of social tags for music
recommendation. In Proceedings of NIPS, pages 385?
392.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 668?673.
S. Fujimura, KO Fujimura, and H. Okuda. 2008.
Blogosonomy: Autotagging any text using bloggers?
knowledge. In Proceedings of WI, pages 205?212.
J.L. Herlocker, J.A. Konstan, A. Borchers, and J. Riedl.
1999. An algorithmic framework for performing
collaborative filtering. In Proceedings of SIGIR, pages
230?237.
J.L. Herlocker, J.A. Konstan, L.G. Terveen, and J.T.
Riedl. 2004. Evaluating collaborative filtering recom-
mender systems. ACM Transactions on Information
Systems, 22(1):5?53.
P. Heymann, D. Ramage, and H. Garcia-Molina. 2008.
Social tag prediction. In Proceedings of SIGIR, pages
531?538.
A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme.
2006. Trend detection in folksonomies. Semantic
Multimedia, pages 56?70.
T. Iwata, T. Yamada, and N. Ueda. 2009. Modeling
social annotation data with content relevance using a
topic model. In Proceedings of NIPS, pages 835?843.
R. Jaschke, L. Marinho, A. Hotho, L. Schmidt-Thieme,
and G. Stumme. 2008. Tag recommendations in
social bookmarking systems. AI Communications,
21(4):231?247.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Automat-
ic image annotation and retrieval using cross-media
relevance models. In Proceedings of SIGIR, pages
119?126.
M. Karimzadehgan and C.X. Zhai. 2010. Estimation of
statistical translation models based on mutual informa-
tion for ad hoc information retrieval. In Proceedings
of SIGIR, pages 323?330.
I. Katakis, G. Tsoumakas, and I. Vlahavas. 2008. Mul-
tilabel text classification for automated tag suggestion.
ECML PKDD Discovery Challenge 2008, page 75.
R. Krestel, P. Fankhauser, and W. Nejdl. 2009. Latent
dirichlet alocation for tag recommendation. In
Proceedings of ACM RecSys, pages 61?68.
S.O.K. Lee and A.H.W. Chun. 2007. Automatic
tag recommendation for the web 2.0 blogosphere
using collaborative tagging and hybrid ann semantic
structures. In Proceedings of WSEAS, pages 88?93.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009a. Clustering
to find exemplar terms for keyphrase extraction. In
Proceedings of EMNLP, pages 257?266.
Z. Liu, H. Wang, H. Wu, and S. Li. 2009b. Collocation
extraction using monolingual word alignment method.
In Proceedings of EMNLP, pages 487?495.
Y. Liu, Q. Liu, and S. Lin. 2010a. Discriminative
word alignment by linear modeling. Computational
Linguistics, 36(3):303?339.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010b. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP, pages 366?376.
Z. Liu, H. Wang, H. Wu, and S. Li. 2010c. Improving
statistical machine translation with monolingual collo-
cation. In Proceedings of ACL, pages 825?833.
Z. Liu, X. Chen, Y. Zheng, and M. Sun. 2011. Automatic
keyphrase extraction by bridging vocabulary gap. In
Proceedings of CoNLL, pages 135?144.
1587
C.D. Manning, P. Raghavan, and H. Schtze. 2008.
Introduction to information retrieval. Cambridge
University Press New York, NY, USA.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of EMNLP, pages
404?411.
R. Mirizzi, A. Ragone, T. Di Noia, and E. Di Sciascio.
2010. Semantic tags generation and retrieval for
online advertising. In Proceedings of CIKM, pages
1089?1098.
G. Mishne. 2006. Autotag: a collaborative approach
to automated tag assignment for weblog posts. In
Proceedings of WWW, pages 953?954.
V. Murdock and W.B. Croft. 2004. Simple translation
models for sentence retrieval in factoid question an-
swering. In Proceedings of SIGIR.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
T. Ohkura, Y. Kiyota, and H. Nakagawa. 2006. Browsing
system for weblog articles based on automated folk-
sonomy. In Proceedings of WWW.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In
Proceedings of EMNLP, volume 149.
S. Ravi, A. Broder, E. Gabrilovich, V. Josifovski,
S. Pandey, and B. Pang. 2010. Automatic generation
of bid phrases for online advertising. In Proceedings
of WSDM, pages 341?350.
S. Rendle, L. Balby Marinho, A. Nanopoulos, and
L. Schmidt-Thieme. 2009. Learning optimal ranking
with tensor factorization for tag recommendation. In
Proceedings of KDD, pages 727?736.
P. Resnick and H.R. Varian. 1997. Recommender
systems. Communications of the ACM, 40(3):56?58.
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proccedings of ACL,
pages 464?471.
S. Riezler, Y. Liu, and A. Vasserman. 2008. Translating
queries into snippets for improved query expansion. In
Proceedings of COLING, pages 737?744.
X. Si and M. Sun. 2009. Tag-LDA for scalable real-
time tag recommendation. Journal of Computational
Information Systems, 6(1):23?31.
X. Si, Z. Liu, and M. Sun. 2010. Modeling social
annotations via latent reason identification. IEEE
Intelligent Systems, 25(6):42 ? 49.
R. Soricut and E. Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Infor-
mation Retrieval, 9(2):191?206.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online qa collec-
tions. In Proceedings of ACL, pages 719?727.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei.
2006. Hierarchical dirichlet processes. Journal of
the American Statistical Association, 101(476):1566?
1581.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
F.Y. Wang, K.M. Carley, D. Zeng, and W. Mao. 2007.
Social computing: From social informatics to social
intelligence. IEEE Intelligent Systems, 22(2):79?83.
R. Wetzker, C. Zimmermann, C. Bauckhage, and S. Al-
bayrak. 2010. I tag, you tag: translating tags for
advanced user models. In Proceedings of WSDM,
pages 71?80.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval models
for question and answer archives. In Proceedings of
SIGIR, pages 475?482.
Y. Yanbe, A. Jatowt, S. Nakamura, and K. Tanaka. 2007.
Can social bookmarking enhance search in the web?
In Proceedings of JCDL, pages 107?116.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010a. Leverag-
ing multiple mt engines for paraphrase generation. In
Proceedings of COLING, pages 1326?1334.
S. Zhao, H. Wang, and T. Liu. 2010b. Paraphrasing with
search engine query logs. In Proceedings of COLING,
pages 1317?1325.
T.C. Zhou, H. Ma, M.R. Lyu, and I. King. 2010.
UserRec: A user recommendation framework in social
tagging systems. In Proceedings of AAAI, pages
1486?1491.
1588
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Autoencoders for ITG-based Translation
Peng Li, Yang Liu and Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
Abstract
While inversion transduction grammar (ITG)
is well suited for modeling ordering shifts
between languages, how to make applying
the two reordering rules (i.e., straight and
inverted) dependent on actual blocks being
merged remains a challenge. Unlike previous
work that only uses boundary words, we pro-
pose to use recursive autoencoders to make
full use of the entire merging blocks alter-
natively. The recursive autoencoders are ca-
pable of generating vector space representa-
tions for variable-sized phrases, which enable
predicting orders to exploit syntactic and se-
mantic information from a neural language
modeling?s perspective. Experiments on the
NIST 2008 dataset show that our system sig-
nificantly improves over the MaxEnt classifier
by 1.07 BLEU points.
1 Introduction
Phrase-based models (Koehn et al, 2003; Och and
Ney, 2004) have been widely used in practical ma-
chine translation (MT) systems due to their effec-
tiveness, simplicity, and applicability. First, as se-
quences of consecutive words, phrases are capable
of memorizing local word selection and reorder-
ing, making them an effective mechanism for trans-
lating idioms or translations with word insertions
or omissions. Moreover, n-gram language models
can be seamlessly integrated into phrase-based de-
coders since partial translations grow left to right
in decoding. Finally, phrase-based systems can be
applicable to most domains and languages, espe-
cially for resource-scarce languages without high-
accuracy parsers.
However, as phrase-based decoding casts transla-
tion as a string concatenation problem and permits
arbitrary permutations, it proves to be NP-complete
(Knight, 1999). Therefore, phrase reordering mod-
eling has attracted intensive attention in the past
decade (e.g., Och et al, 2004; Tillman, 2004; Zens
et al, 2004; Al-Onaizan and Papineni, 2006; Xiong
et al, 2006; Koehn et al, 2007; Galley and Man-
ning, 2008; Feng et al, 2010; Green et al, 2010;
Bisazza and Federico, 2012; Cherry, 2013).
Among them, reordering models based on inver-
sion transduction grammar (ITG) (Wu, 1997) are
one of the important ongoing research directions.
As a formalism for bilingual modeling of sentence
pairs, ITG is particularly well suited to predicting
ordering shifts between languages. As a result, a
number of authors have incorporated ITG into left-
to-right decoding to constrain the reordering space
and reported significant improvements (e.g., Zens et
al., 2004; Feng et al, 2010). Along another line,
Xiong et al (2006) propose a maximum entropy
(MaxEnt) reordering model based on ITG. They use
the CKY algorithm to recursively merge two blocks
(i.e., a pair of source and target strings) into larger
blocks, either in a straight or an inverted order. Un-
like lexicalized reordering models (Tillman, 2004;
Koehn et al, 2007; Galley and Manning, 2008) that
are defined on individual bilingual phrases, the Max-
Ent ITG reordering model is a two-category classi-
fier (i.e., straight or inverted) for two arbitrary bilin-
gual phrases of which the source phrases are adja-
cent. This potentially alleviates the data sparseness
567
problem since there are usually a large number of
reordering training examples available (Xiong et al,
2006). As a result, the MaxEnt ITG model and its
extensions (Xiong et al, 2008; Xiong et al, 2010)
have achieved competing performance as compared
with state-of-the-art phrase-based systems.
Despite these successful efforts, the ITG reorder-
ing classifiers still face a major challenge: how to
extract features from training examples (i.e., a pair
of bilingual strings). It is hard to decide which words
are representative for predicting reordering, either
manually or automatically, especially for long sen-
tences. As a result, Xiong et al (2006) only use
boundary words (i.e., the first and the last words in
a string) to predict the ordering. What if we look
inside? Is it possible to avoid manual feature engi-
neering and learn semantic representations from the
data?
Fortunately, the rapid development of intersect-
ing deep learning with natural language processing
(Bengio et al, 2003; Collobert and Weston, 2008;
Collobert et al, 2011; Glorot et al, 2011; Bordes et
al., 2011; Socher et al, 2011a; Socher et al, 2011b;
Socher et al, 2011c; Socher et al, 2012; Bordes et
al., 2012; Huang et al, 2012; Socher et al, 2013;
Hermann and Blunsom, 2013) brings hope for alle-
viating this problem. In these efforts, natural lan-
guage words are represented as real-valued vectors,
which can be naturally fed to neural networks as in-
put. More importantly, it is possible to learn vec-
tor space representations for multi-word phrases us-
ing recursive autoencoders (Socher et al, 2011c),
which opens the door to leveraging semantic repre-
sentations of phrases in reordering models from a
neural language modeling point of view.
In this work, we propose an ITG reordering clas-
sifier based on recursive autoencoders. The neu-
ral network consists of four autoencoders (i.e., the
first source phrase, the first target phrase, the sec-
ond source phrase, and the second target phrase)
and a softmax layer. The recursive autoencoders,
which are trained on reordering examples extracted
from word-aligned bilingual corpus, are capable
of producing vector space representations for arbi-
trary multi-word strings in decoding. Therefore,
our model takes the whole phrases rather than only
boundary words into consideration when predict-
ing phrase permutations. Experiments on the NIST
2008 dataset show that our system significantly im-
proves over the MaxEnt classifier by 1.07 in terms
of case-insensitive BLEU score.
2 Recursive Autoencoders for ITG-based
Translation
2.1 Inversion Transduction Grammar
Inversion transduction grammar (ITG) (Wu, 1997)
is a formalism for synchronous parsing of bilingual
sentence pairs. Xiong et al (2006) apply bracketing
transduction grammar (BTG), which is a simplified
version of ITG, to phrase-based translation using the
following production rules:
X ? [X1, X2] (1)
X ? ?X1, X2? (2)
X ? f/e (3)
where X is a block that consists of a pair of source
and target strings, f is a source phrase, and e is a tar-
get phrase. X1 and X2 are two neighboring blocks
of which the two source phrases are adjacent. While
rule (1) merges two target phrases in a straight or-
der, rule (2) merges in an inverted order. Besides
these two reordering rules, rule (3) is a lexical rule
that translates a source phrase f into a target phrase
e. This is exactly a bilingual phrase used in conven-
tional phrase-based systems.
An ITG derivation, which consists of a sequence
of production rules, explains how a sentence pair is
generated simultaneously. Figure 1 shows an ITG
derivation for a Chinese sentence and its English
translation. We distinguish between two types of
blocks:
1. atomic blocks: blocks generated by applying
lexical rules,
2. composed blocks: blocks generated by apply-
ing reordering rules.
In Figure 1, the sentence pair is segmented into
five atomic blocks:
X0,3,0,3 : wo you yi ge? I have a
X3,5,5,6 : cong mei you? never
X5,8,6,8 : jian guo de? seen before
X8,10,3,5 : nv xing peng you? female friend
X10,11,8,9 : .? .
568
(1) X0,11,0,9 ? [X0,10,0,8, X10,11,8,9]
(2) X0,10,0,8 ? [X0,3,0,3, X3,10,3,8]
(3) X0,3,0,3 ? wo you yi ge / I have a
(4) X3,10,3,8 ? ?X3,8,5,8, X8,10,3,5?
(5) X3,8,5,8 ? [X3,5,5,6, X5,8,6,8]
(6) X3,5,5,6 ? cong mei you / never
(7) X5,8,6,8 ? juan guo de / seen before
(8) X8,10,3,5 ? nv xing peng you/ female friend
(9) X10,11,8,9 ? . / .
Figure 1: An ITG derivation for a Chinese sentence and its translation. We useXi,j,k,l = ?f
j
i , e
l
k? to represent a block.
Our neural ITG reordering model first assigns vector space representations to single words and then produces vectors
for phrases using recursive autoencoders, which form atomic blocks. The atomic blocks are recursively merged into
composed blocks, the vector space representations of which are produced by recursive autoencoders simultaneously.
The neural classifier makes decisions at each node using the vectors of all its descendants.
569
where X3,5,5,6 indicates that the block consists of a
source phrase spanning from position 3 to position 5
(i.e., ?cong mei you?) and a target phrase spanning
from position 5 to position 6 (i.e., ?never?). More
formally, a block Xi,j,k,l = ?f
j
i , e
l
k? is a pair of a
source phrase f ji = fi+1 . . . fj and a target phrase
elk = ek+1 . . . el. Obviously, these atomic blocks
are generated by lexical rules.
Two blocks of which the source phrases are adja-
cent can be merged into a larger one in two ways:
concatenating the target phrases in a straight order
using rule (1) or in an inverted order using rule (2).
For example, atomic blocks X3,5,5,6 and X5,8,6,8 are
merged into a composed block X3,8,5,8 in a straight
order, which is further merged with an atomic block
X8,10,3,5 into another composed block X3,10,3,8 in
an inverted order. This process recursively proceeds
until the entire sentence pair is generated.
The major challenge of applying ITG to machine
translation is to decide when to merge two blocks
in a straight order and when in an inverted order.
Therefore, the ITG reordering model can be seen as
a two-category classifier P (o|X1, X2), where o ?
{straight, inverted}.
A naive way is to assign fixed probabilities to two
reordering rules, which is referred to as flat model
by Xiong et al (2006):
P (o|X1, X2) =
{
p o = straight
1? p o = inverted
(4)
The drawback of the flat model is ignoring the
actual blocks being merged. Intuitively, different
blocks should have different preferences between
the two orders.
To alleviate this problem, Xiong et al (2006) pro-
pose a maximum entropy (MaxEnt) classifier:
P (o|X1, X2) =
exp(? ? h(o,X1, X2))
?
o? exp(? ? h(o
?, X1, X2))
(5)
where h(?) is a vector of features defined on the
blocks and the order, ? is a vector of feature weights.
While MaxEnt is a flexible and powerful frame-
work for including arbitrary features, feature engi-
neering becomes a major challenge for the MaxEnt
classifier. Xiong et al (2006) find that boundary
words (i.e., the first and the last words in a string)
are informative for predicting reordering. Actually,
Figure 2: A recursive autoencoder for multi-word strings.
The example is adapted from (Socher et al, 2011c). Blue
and grey nodes are original and reconstructed ones, re-
spectively.
it is hard to decide which internal words in a long
composed blocks are representative and informa-
tive. Therefore, they only use boundary words as
the main features.
However, it seems not enough to just consider
boundary words and ignore all internal words when
making order predictions, especially for long sen-
tences.1 Indeed, Xiong et al (2008) find that the
MaxEnt classifier with boundary words as features
is prone to make wrong predictions for long com-
posed blocks. As a result, they have to impose a hard
constraint to always prefer merging long composed
blocks in a monotonic way.
Therefore, it is important to consider more than
boundary words to make more accurate reordering
predictions. We need a new mechanism to achieve
this goal.
2.2 Recursive Autoencoders
2.2.1 Vector Space Representations for Words
In neural networks, a natural language word is
represented as a real-valued vector (Bengio et al,
2003; Collobert and Weston, 2008). For example,
we can use [0.1 0.8 0.4]T to represent ?female? and
1Strictly speaking, the ITG reordering model is not a phrase
reordering model since phrase pairs are only the atomic blocks.
Instead, it is defined to work on arbitrarily long strings because
composed blocks become larger and larger until the entire sen-
tence pair is generated.
570
Figure 3: A neural ITG reordering model. The binary classifier makes decisions based on the vector space representa-
tions of the source and target sides of merging blocks.
[0.7 0.1 0.5]T to represent ?friend?. Such vector
space representations enable natural language words
to be fed to neural networks as input.
Formally, we denote each word as a vector x ?
Rn. These word vectors are then stacked into a word
embedding matrix L ? Rn?|V |, where |V | is the vo-
cabulary size. Given a sentence that is an ordered list
ofmwords, each word has an associated vocabulary
index k into the word embedding matrix L that we
use to retrieve the word?s vector space representa-
tion. This look-up operation can be seen as a simple
projection layer:
xi = Lbk ? Rn (6)
where bk is a binary vector which is zero in all posi-
tions except for the kth index.
In Figure 1, we assume n = 3 for simplicity and
can retrieve vectors for Chinese and English words
from two embedding matrices, respectively.
2.2.2 Vector Space Representations for
Multi-Word Strings
To apply neural networks to ITG-based transla-
tion, it is important to generate vector space repre-
sentations for atomic and composed blocks.
For example, since the vector of ?female? is
[0.1 0.8 0.4]T and the vector of ?friend? is
[0.7 0.1 0.5]T , what is the vector of the phrase ?fe-
male friend?? If we denote ?female friend? as p
(i.e., parent), ?female? as c1 (i.e., the first child),
and ?friend? as c2 (i.e., the second child), this can
be done by applying a function f (1):
p = f (1)(W (1)[c1; c2] + b
(1)) (7)
where [c1; c2] ? R2n?1 is the concatenation of c1
and c2, W (1) ? Rn?2n is a parameter matrix, b(1) ?
Rn?1 is a bias term, and f (1) is an element-wise ac-
tivation function such as tanh(?), which is used in
our experiments.
Note that the resulting vector for the parent is also
an n-dimensional vector, e.g, [0.6 0.9 0.2]T . The
same neural network can be recursively applied to
two strings until the vector of the entire sentence is
generated. As ITG derivation builds a binary parse
tree, the neural network can be naturally integrated
into CKY parsing.
To assess how well the learned vector p represents
its children, we can reconstruct the children in a
reconstruction layer:
[c?1; c
?
2] = f
(2)(W (2)p+ b(2)) (8)
where c?1 and c
?
2 are the reconstructed children,W
(2)
is a parameter matrix for reconstruction, b(2) is a bias
term for reconstruction, and f (2) is an element-wise
activation function, which is also set as tanh(?) in
our experiments. Similarly, the same reconstruction
neural network can be applied to each node in an
ITG parse.
These neural networks are called recursive au-
toencoders (Socher et al, 2011c). Figure 2 illus-
trates an application of a recursive autoencoder to a
571
binary tree. The blue and grey nodes are the original
and reconstructed nodes, respectively. The autoen-
coder is re-used at each node of the tree. The bi-
nary tree is composed of a set of triplets in the form
of (p ? c1 c2), where p is a parent vector and c1
and c2 are children vectors of p. Each child can be
either an input word vector or a multi-word vector.
Therefore, the tree in Figure 2 can be represented as
three triplets: (y1 ? x1 x2), (y2 ? y1 x3), and
(y3 ? y2 x4).
In Figure 1, we use recursive autoencoders to gen-
erate vector space representations for Chinese and
English phrases, which form the atomic blocks for
further block merging.
2.2.3 A Neural ITG Reordering Model
Once the vectors for blocks are generated, it is
straightforward to introduce a neural ITG reorder-
ing model. As shown in Figure 3, the neural net-
work consists of an input layer and a softmax layer.
The input layer is composed of the vectors of the
first source phrase, the first target phrase, the second
source phrase, and the second target phrase. Note
that all phrases in the same language use the the
same recursive autoencoder. The softmax layer out-
puts the probabilities of the two merging orders:
P (o|X1, X2) =
exp(g(o,X1, X2))
?
o? exp(g(o
?, X1, X2))
(9)
g(o,X1, X2) = f(W oc(X1, X2) + bo) (10)
where o ? {straight, inverted}, W o ? R1?4n
is a parameter matrix, bo ? R is a bias term, and
c(X1, X2) ? R4n?1 is the concatenation of the vec-
tors of the four phrases.
3 Training
There are three sets of parameters in our recursive
autoencoders:
1. ?L: word embedding matrix L for both source
and target languages (Section 2.2.1);
2. ?rec: recursive autoencoder parameter matrices
W (1), W (2) and bias terms b(1), b(2) for both
source and target languages (Section 2.2.2);
3. ?reo: neural ITG reordering model parameter
matrix W o and bias term bo (Section 2.2.3).
All these parameters are learned automatically from
the training data. For clarity, we will use ? to denote
all these parameters in the rest of the paper.
For training word embedding matrix, there are
two settings commonly used. In the first setting,
the word embedding matrix is initialized randomly.
This works well in a supervised scenario, in which
a neural network updates the matrix in order to op-
timize some task-specific objectives (Collobert et
al., 2011; Socher et al, 2011c). In the second set-
ting, the word embedding matrix is pre-trained us-
ing an unsupervised neural language model (Bengio
et al, 2003; Collobert and Weston, 2008) with huge
amount of unlabeled data. In this work, we prefer to
the first setting because the word embedding matri-
ces can be trained to minimize errors with respect to
reordering modeling.
There are two kinds of errors involved
1. reconstruction error: how well the learned
vector space representations represent the cor-
responding strings?
2. reordering error: how well the classifier pre-
dicts the merging order?
As described in Section 2.2.2, the input vector
c1 and c2 of a recursive autoencoder can be recon-
structed using Eq. 8 as c?1 and c
?
2. We use Euclidean
distance between the input and the reconstructed
vectors to measure the reconstruction error:
Erec([c1; c2]; ?) =
1
2
?
?[c1; c2]? [c
?
1; c
?
2]
?
?2 . (11)
Given a sentence, there are exponentially many
ways to obtain its vector space representation. Note
that each way corresponds to a binary tree like Fig-
ure 2. To find a binary tree with minimal reconstruc-
tion error, we follow Socher et al (2011c) to use a
greedy algorithm. Taking Figure 2 as an example,
the greedy algorithm begins with computing the re-
construction error Erec(?) for each pair of consecu-
tive vectors, i.e., Erec([x1;x2]; ?), Erec([x2;x3]; ?)
and Erec([x3;x4]; ?). Suppose Erec([x1;x2]; ?) is
the smallest, the algorithm will replace x1 and x2
with their vector representation y1 produced by the
recursive autoencoder. Then, the algorithm evalu-
ates Erec([y1;x3]; ?) and Erec([x3;x4]; ?) and re-
peats the above replacing steps until only one vector
572
remains. Socher et al (2011c) find that the greedy
algorithm runs fast without significant loss in perfor-
mance as compared with CKY-style algorithms.
Given a training example set S = {ti =
(oi, X1i , X
2
i )}, the average reconstruction error on
the source side on the training set is defined as
Erec,s(S; ?) =
1
Ns
?
i
?
p?T ?R(ti,s)
Erec([p.c1, p.c2]; ?)
(12)
where T ?R(ti, s) denotes all the intermediate nodes
on the source side in binary trees, Ns is the num-
ber of these intermediate nodes, and p.ck is the kth
child vector of p. The average reconstruction error
on the target side, denoted by Erec,t(S; ?), can be
computed in a similar way.
Therefore, the reconstruction error is defined as
Erec(S; ?) = Erec,s(S; ?) + Erec,t(S; ?). (13)
Given a training example ti = (oi, X1i , X
2
i ), we
assume the probability distribution dti for its label
is [1, 0] when oi = straight, and [0, 1] when oi =
inverted. Then the cross-entropy error is
Ec(ti; ?) = ?
?
o
dti(o) log
(
P?(o|X
1, X2)
)
(14)
where o ? {straight, inverted}. As a result, the
reordering error is defined as
Ereo(S; ?) =
1
|S|
?
i
Ec(ti; ?). (15)
Therefore, the joint training objective function is
J = ?Erec(S; ?)+(1??)Ereo(S; ?)+R(?) (16)
where ? is a parameter used to balance the prefer-
ence between reconstruction error and reordering er-
ror, R(?) is the regularizer and defined as 2
R(?) =
?L
2
??L?
2 +
?rec
2
??rec?
2 +
?reo
2
??reo?
2 .
(17)
As Socher et al (2011c) stated, a naive way for
lowering the reconstruction error is to make the
magnitude of the hidden layer very small, which is
2The bias terms b(1), b(2) and bo are not regularized. We do
not exclude them from the equation explicitly just for clarity.
not desirable. In order to prevent such behavior, we
normalize all the output vectors of the hidden layers
to have length 1 in the same way as (Socher et al,
2011c). Namely we set p = p||p|| after computing p
as in Eq. 7, and c?1 =
c?1
||c?1||
, c?2 =
c?2
||c?2||
in Eq. 8.
Following Socher et al (2011c), we use L-BFGS
to estimate the parameters with respect to the joint
training objective. Given a set of parameters, we
construct binary trees for all the phrases using the
greedy algorithm. The derivatives for these fixed
binary trees can be computed via backpropagation
through structures (Goller and Kuchler, 1996).
4 Experiments
4.1 Data Preparation
We evaluated our system on Chinese-English trans-
lation. The training corpus contains 1.23M sen-
tence pairs with 32.1M Chinese words and 35.4M
English words. We used SRILM (Stolcke, 2002)
to train a 4-gram language model on the Xinhua
portion of the GIGAWORD corpus, which con-
tains 398.6M words. We used the NIST 2006 MT
Chinese-English dataset as the development set and
NIST 2008 dataset as the test set. The evaluation
metric is case-insensitive BLEU. Because of the ex-
pensive computational cost for training our neural
ITG reordering model, only the reordering exam-
ples extracted from about 1/5 of the entire parallel
training corpus were used to train our neural ITG re-
ordering model.
For the neural ITG reordering model, we set the
dimension of the word embedding vectors to 25 em-
pirically, which is a trade-off between computational
cost and expressive power. We use the early stop-
ping principle to determine when to stop L-BFGS.
The hyper-parameters ?, ?L, ?rec and ?reo are op-
timized by random search (Bergstra and Bengio,
2012). As preliminary experiments show that classi-
fication accuracy has a high correlation with BLEU
score, we optimize these hyper-parameters with re-
spect to classification accuracy instead of BLEU
to reduce computational cost. We randomly select
400,000 reordering examples as training set, 500 as
development set, and another 500 as test set. The
numbers of straight and inverted reordering exam-
ples in the development/test set are set to be equal
to avoid biases. We draw ? uniformly from 0.05
573
System NIST 2006 (tune) NIST 2008
maxent 30.40 23.75
neural 31.61* 24.82*
Table 1: BLEU scores on the NIST 2006 and 2008
datasets. *: significantly better (p < 0.01). ?maxent?
denotes the baseline maximum entropy system and ?neu-
ral? denotes our recursive autoencoder system.
length > = <
[1, 10] 43 121 57
[11, 20] 181 67 164
[21, 30] 170 11 152
[31, 40] 105 3 90
[41, 50] 69 1 53
[51, 119] 40 0 30
Table 2: Number of sentences that our system has a
higher (>), equal (=) or lower (<) sentence-level BLEU-
4 score on the NIST 2008 dataset.
to 0.3, and ?L, ?rec, ?reo exponentially from 10?8
to 10?2. We use the following hyper-parameters in
our experiments: ? = 0.11764, ?L = 7.59 ? 10?5,
?rec = 1.30? 10?5 and ?reo = 3.80? 10?4. 3
The baseline system is a re-implementation of
(Xiong et al, 2006). Our system is different from the
baseline by replacing the MaxEnt reordering model
with a neural model. Both the systems have the same
pruning settings: the threshold pruning parameter is
set to 0.5 and the histogram pruning parameter to
40. For minimum-error-rate training, both systems
generate 200-best lists.
4.2 MT Evaluation
Table 1 shows the case-insensitive BLEU-4 scores
of the baseline system and our system on the devel-
opment and test sets. Our system outperforms the
baseline system by 1.21 BLEU points on the de-
velopment set and 1.07 on the test set. Both the
differences are statistically significant at p = 0.01
level (Riezler and Maxwell, 2005).
Table 2 shows the number of sentences that our
system has a higher (>), equal (=) or lower (<)
BLEU score on the NIST 2008 dataset. We find that
our system is superior to the baseline system for long
3The choice of ? is very important for achieving high BLEU
scores. We tried a number of intervals and found that the clas-
sification accuracy is most stable in the interval [0.100,0.125].
5 10 15 20 25 30 351
90
95
100
88 Length
Accur
acy (%
)
 
 
neuralmaxent
Figure 4: Comparison of reordering classification accu-
racies between the MaxEnt and neural classifiers over
varying phrase lengths. ?Length? denotes the sum of the
lengths of two source phrases in a reordering example.
Our classifier (neural) outperforms the MaxEnt classi-
fier (maxent) consistently, especially for predicting long-
distance reordering.
# of examples NIST 2006 (tune) NIST 2008
100,000 30.88 23.78
200,000 30.75 23.89
400,000 30.80 24.35
800,000 31.01 24.45
6,004,441 31.61 24.82
Table 3: The effect of reordering training data size on
BLEU scores. The BLEU scores rise with the increase of
training data size. Due to the computational cost, we only
used 1/5 of the entire bilingual corpus to train our neural
reordering model.
sentences.
Figure 4 compares classification accuracies of the
neural and MaxEnt classifiers. ?Length? denotes the
sum of the lengths of two source phrases in a re-
ordering example. For each length, we randomly se-
lect 200 unseen reordering examples to calculate the
classification accuracy. Our classifier outperforms
the baseline consistently, especially for long com-
posed blocks.
Xiong et al (2008) find that the performance of
the baseline system can be improved by forbidding
inverted reordering if the phrase length exceeds a
pre-defined distortion limit. This heuristic increases
the BLEU score of the baseline system significantly
to 24.46 but is still significantly worse (p < 0.05)
than our system without the heuristic. We find that
imposing this heuristic fails to improve our system
574
cluster 1 cluster 2 cluster 3 cluster 4 cluster 5
1.18 works for alternative duties these people who of the three
accessibility verify on one-day conference the reasons why on the fundamental
wheelchair tunnels from armed groups the story of how over the entire
candies transparency in chinese language works the system which through its own
cough opinion at eating habits the trend towards with the best
Table 4: Words and phrases that are close in the Euclidean space. The words and phrases in the same cluster have
similar behaviors from a reordering point of view rather than relatedness, suggesting that the vector representations
produced by the recursive autoencoders are helpful for capturing reordering regularities.
significantly. One possible reason is that there is
limited room for improvement as our system makes
fewer wrong predictions for long composed blocks.
The above results suggest that our system does go
beyond using boundary words and make a better use
of the merging blocks by using vector space repre-
sentations.
Table 3 shows the effect of training dataset size
on BLEU scores. We find that BLEU scores on both
the development and test sets rise with the increase
of the training dataset size. As the training process is
very time-consuming, only the reordering examples
extracted from 1/5 of the entire parallel training cor-
pus are used in our experiments to train our model.
Obviously, with more efficient training algorithms,
making full use of all the reordering examples ex-
tracted from the entire corpus will result in better
results. We leave this for future work.
4.3 Qualitative Analysis on Vector
Representations
Table 4 shows a number of words and phrases that
are close (measured by Euclidean distance) in the
n-dimensional space. We randomly select about
370K target side phrases used in our experiments
and cluster them into 983 clusters using k-means al-
gorithm (MacQueen, 1967). The distance between
two phrases are measured by the Euclidean distance
between their vector representations. As shown in
Table 4, cluster 1 mainly consists of nouns, clus-
ter 2 mainly contains verb/noun+preposition struc-
tures, cluster 3 contains compound phrases, cluster
4 consists of phrases which should be followed by
a clause, and cluster 5 mainly contains the begin-
ning parts of prepositional phrases that tend to be
followed by a noun phrase or word. We find that
the words and phrases in the same cluster have sim-
ilar behaviors from a reordering point of view rather
than relatedness. This indicates that the vector rep-
resentations produced by the recursive autoencoders
are helpful for capturing reordering regularities.
5 Conclusion
We have presented an ITG reordering classifier
based on recursive autoencoders. As recursive au-
toencoders are capable of producing vector space
representations for arbitrary multi-word strings in
decoding, our neural ITG system achieves an ab-
solute improvement of 1.07 BLEU points over the
baseline on the NIST 2008 Chinese-English dataset.
There are a number of interesting directions we
would like to pursue in the near future. First, re-
placing the MaxEnt classifier with a neural one re-
defines the conditions for risk-free hypothesis re-
combination. We find that the number of hypothe-
ses that can be recombined reduces in our system.
Therefore, we plan to use forest reranking (Huang,
2008) to alleviate this problem. Second, it is in-
teresting to follow Socher et al (2013) to combine
linguistically-motivated labels with recursive neural
networks. Another problem with our system is that
the decoding speed is much slower than the baseline
system because of the computational overhead intro-
duced by RAEs. It is necessary to investigate more
efficient decoding algorithms. Finally, it is possible
to apply our method to other phrase-based and even
syntax-based systems.
Acknowledgments
This research is supported by the 863 Program un-
der the grant No. 2012AA011102, by the Boeing
Tsinghua Joint Research Project on Language Pro-
cessing (Agreement TBRC-008-SDB-2011 Phase 3
575
(2013)), by the Singapore National Research Foun-
dation under its International Research Centre @
Singapore Funding Initiative and administered by
the IDM Programme Office, and by a Research Fund
No. 20123000007 from Tsinghua MOE-Microsoft
Joint Laboratory. Many thanks go to Chunyang Liu
and Chong Kuang for their great help for setting up
the computing platform. We also thank Min-Yen
Kan, Meng Zhang and Yu Zhao for their insightful
discussions.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
529?536, Sydney, Australia, July.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155, March.
James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. The Jour-
nal of Machine Learning Research, 13(1):281?305,
February.
Arianna Bisazza and Marcello Federico. 2012. Modi-
fied distortion matrices for phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 478?487, Jeju Is-
land, Korea, July.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelli-
gence, pages 301?306.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words and
meaning representations for open-text semantic pars-
ing. In International Conference on Artificial Intelli-
gence and Statistics, pages 127?135.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 22?31,
Atlanta, Georgia, June.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceed-
ings of the 25th International Conference on Machine
Learning, pages 160?167.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics: Posters, pages 285?293, Beijing, China, August.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceed-
ings of the 28th International Conference on Machine
Learning (ICML-11), pages 513?520.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Proceedings of 1996
IEEE International Conference on Neural Networks
(Volume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Proceedings of Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 867?875,
Los Angeles, California, June.
Karl Moritz Hermann and Phil Blunsom. 2013. The role
of syntax in vector space models of compositional se-
mantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 894?904, Sofia, Bulgaria,
August.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 873?882, Jeju Island, Korea, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 586?594, Columbus, Ohio, June.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
576
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June.
James MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of the Fifth Berkeley Symposium on Mathe-
matical Statistics and Probability, volume 1.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004: Main Proceedings, pages 161?168,
Boston, Massachusetts, USA, May.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Proceedings of Advances
in Neural Information Processing Systems 24, pages
801?809.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning (ICML), pages 129?136.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161, Edinburgh, Scot-
land, UK., July.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201?
1211, Jeju Island, Korea, July.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 455?465, Sofia,
Bulgaria, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
vol. 2, pages 901?904, September.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004: Short Pa-
pers, pages 101?104, Boston, Massachusetts, USA,
May.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July.
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu,
and Shouxun Lin. 2008. Refinements in BTG-based
statistical machine translation. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing: Volume-I, pages 505?512.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2010.
Linguistically annotated reordering: Evaluation and
analysis. Computational Linguistics, 36(3):535?568,
September.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 205?211, Geneva,
Switzerland, Aug 23?Aug 27.
577
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025?1035,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Unified Model for Word Sense Representation and Disambiguation
Xinxiong Chen, Zhiyuan Liu, Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
cxx.thu@gmail.com, {lzy, sms}@tsinghua.edu.cn
Abstract
Most word representation methods assume
that each word owns a single semantic vec-
tor. This is usually problematic because
lexical ambiguity is ubiquitous, which is
also the problem to be resolved by word
sense disambiguation. In this paper, we
present a unified model for joint word
sense representation and disambiguation,
which will assign distinct representation-
s for each word sense.
1
The basic idea is
that both word sense representation (WS-
R) and word sense disambiguation (WS-
D) will benefit from each other: (1) high-
quality WSR will capture rich informa-
tion about words and senses, which should
be helpful for WSD, and (2) high-quality
WSD will provide reliable disambiguat-
ed corpora for learning better sense rep-
resentations. Experimental results show
that, our model improves the performance
of contextual word similarity compared to
existing WSR methods, outperforms state-
of-the-art supervised methods on domain-
specific WSD, and achieves competitive
performance on coarse-grained all-words
WSD.
1 Introduction
Word representation aims to build vectors for each
word based on its context in a large corpus, usually
capturing both semantic and syntactic information
of words. These representations can be used as
features or inputs, which are widely employed in
information retrieval (Manning et al., 2008), doc-
ument classification (Sebastiani, 2002) and other
NLP tasks.
1
Our sense representations can be downloaded at http:
//pan.baidu.com/s/1eQcPK8i.
Most word representation methods assume each
word owns a single vector. However, this is usual-
ly problematic due to the homonymy and polyse-
my of many words. To remedy the issue, Reisinger
and Mooney (2010) proposed a multi-prototype
vector space model, where the contexts of each
word are first clustered into groups, and then each
cluster generates a distinct prototype vector for a
word by averaging over all context vectors with-
in the cluster. Huang et al. (2012) followed this
idea, but introduced continuous distributed vectors
based on probabilistic neural language models for
word representations.
These cluster-based models conduct unsuper-
vised word sense induction by clustering word
contexts and, thus, suffer from the following is-
sues:
? It is usually difficult for these cluster-based
models to determine the number of cluster-
s. Huang et al. (2012) simply cluster word
contexts into static K clusters for each word,
which is arbitrary and may introduce mis-
takes.
? These cluster-based models are typically off-
line , so they cannot be efficiently adapted to
new senses, new words or new data.
? It is also troublesome to find the sense that
a word prototype corresponds to; thus, these
cluster-based models cannot be directly used
to perform word sense disambiguation.
In reality, many large knowledge bases have
been constructed with word senses available
online, such as WordNet (Miller, 1995) and
Wikipedia. Utilizing these knowledge bases to
learn word representation and sense representation
is a natural choice. In this paper, we present a uni-
fied model for both word sense representation and
disambiguation based on these knowledge bases
and large-scale text corpora. The unified model
1025
can (1) perform word sense disambiguation based
on vector representations, and (2) learn continu-
ous distributed vector representation for word and
sense jointly.
The basic idea is that, the tasks of word sense
representation (WSR) and word sense disam-
biguation (WSD) can benefit from each other: (1)
high-quality WSR will capture rich semantic and
syntactic information of words and senses, which
should be helpful for WSD; (2) high-quality WS-
D will provide reliable disambiguated corpora for
learning better sense representations.
By utilizing these knowledge bases, the prob-
lem mentioned above can be overcome:
? The number of senses of a word can be de-
cided by the expert annotators or web users.
? When a new sense appears, our model can be
easily applied to obtain a new sense represen-
tation.
? Every sense vector has a corresponding sense
in these knowledge bases.
We conduct experiments to investigate the per-
formance of our model for both WSR and WS-
D. We evaluate the performance of WSR using a
contextual word similarity task, and results show
that out model can significantly improve the cor-
relation with human judgments compared to base-
lines. We further evaluate the performance on
both domain-specific WSD and coarse-grained all-
words WSD, and results show that our model
yields performance competitive with state-of-the-
art supervised approaches.
2 Methodology
We describe our method as a 3-stage process:
1. Initializing word vectors and sense vectors.
Given large amounts of text data, we first use
the Skip-gram model (Mikolov et al., 2013),
a neural network based language model, to
learn word vectors. Then, we assign vector
representations for senses based on their def-
initions (e.g, glosses in WordNet).
2. Performing word sense disambiguation.
Given word vectors and sense vectors, we
propose two simple and efficient WSD algo-
rithms to obtain more relevant occurrences
for each sense.
3. Learning sense vectors from relevant oc-
currences. Based on the relevant occur-
rences of ambiguous words, we modify the
training objective of Skip-gram to learn word
vectors and sense vectors jointly. Then, we
obtain the sense vectors directly from the
model.
Before illustrating the three stages of our
method in Sections 2.2, 2.3 and 2.4, we briefly
introduce our sense inventory, WordNet, in Sec-
tion 2.1. Note that, although our experiments will
use the WordNet sense inventory, our model is not
limited to this particular lexicon. Other knowledge
bases containing word sense distinctions and defi-
nitions can also serve as input to our model.
2.1 WordNet
WordNet (Miller, 1995) is the most widely used
computational lexicon of English where a concep-
t is represented as a synonym set, or synset. The
words in the same synset share a common mean-
ing. Each synset has a textual definition, or gloss.
Table 1 shows the synsets and the corresponding
glosses of the two common senses of bank.
Before introducing the method in detail, we in-
troduce the notations. The unlabeled texts are de-
noted as R, and the vocabulary of the texts is de-
noted as W . For a word w in W , w
s
i
is the ith
sense in WordNet WN. Each sense w
s
i
has a gloss
gloss(w
s
i
) in WN. The word embedding of w is
denoted as vec(w), and the sense embedding of its
ith sense w
s
i
is denoted as vec(w
s
i
).
2.2 Initializing Word Vectors and Sense
Vectors
Initializing word vectors. First, we use Skip-
gram to train the word vectors from large amounts
of text data. We choose Skip-gram for its sim-
plicity and effectiveness. The training objective of
Skip-gram is to train word vector representations
that are good at predicting its context in the same
sentence (Mikolov et al., 2013).
More formally, given a sequence of training
words w
1
, w
2
, w
3
,...,w
T
, the objective of Skip-
gram is to maximize the average log probability
1
T
T
?
t=1
(
?
?k? j?k, j 6=0
log p(w
t+ j
|w
t
)
)
(1)
where k is the size of the training window. The
inner summation spans from ?k to k to compute
1026
Sense Synset Gloss
bank
s
1
(sloping land (especially the slope beside a body of water))
bank ?they pulled the canoe up on the bank?;
?he sat on the bank of the river and watched the currents?
bank
s
2
depository institution, (a financial institution that accepts deposits and channels the
bank, money into lending activities)
banking concern, ?he cashed a check at the bank?;
banking company ?that bank holds the mortgage on my home?
Table 1: Example of a synset in WordNet.
the log probability of correctly predicting the word
w
t+ j
given the word in the middle w
t
. The outer
summation covers all words in the training data.
The prediction task is performed via softmax, a
multiclass classifier. There, we have
p(w
t+ j
|w
t
) =
exp(vec
?
(w
t+ j
)
>
vec(w
t
))
?
W
w=1
exp(vec
?
(w)
>
vec(w
t
))
(2)
where vec(w) and vec
?
(w) are the ?input? and
?output? vector representations of w. This formu-
lation is impractical because the cost of comput-
ing p(w
t+ j
|w
t
) is proportional to W , which is often
large( 10
5
?10
7
terms).
Initializing sense vectors. After learning the
word vectors using the Skip-gram model, we ini-
tialize the sense vectors based on the glosses of
senses. The basic idea of the sense vector initial-
ization is to represent the sense by using the sim-
ilar words in the gloss. From the content words
in the gloss, we select those words whose cosine
similarities with the original word are larger than
a similarity threshold ? . Formally, for each sense
w
s
i
in WN, we first define a candidate set from
gloss(w
s
i
)
cand(w
s
i
) = {u|u ? gloss(w
s
i
),u 6= w,
POS(u) ?CW,cos(vec(w),vec(u)) > ?} (3)
where POS(u) is the part-of-speech tagging of the
word u and CW is the set of all possible part-of-
speech tags that content words could have. In this
paper, CW contains the following tags: noun, verb,
adjective and adverb.
Then the average of the word vectors in
cand(w
s
i
) is used as the initialization value of the
sense vector vec(w
s
i
).
vec(w
s
i
) =
1
|cand(w
s
i
)|
?
u?cand(w
s
i
)
vec(u) (4)
For example, in WordNet, the gloss of the sense
bank
s
1
is ?sloping land (especially the slope beside
a body of water)) they pulled the canoe up on the
bank; he sat on the bank of the river and watched
the currents?. The gloss contains a definition of
the sense and two examples of the sense. The
content words and the cosine similarities with the
word ?bank? are listed as follows: (sloping, 0.12),
(land, 0.21), (slope, 0.17), (body, 0.01), (water,
0.10), (pulled, 0.01), (canoe, 0.09), (sat, 0.06),
(river, 0.43), (watch, -0.11), (currents, 0.01). If
the threshold, ? , is set to 0.05, then cand(bank
s
1
)
is {sloping, land, slope, water, canoe, sat, riv-
er}. Then the average of the word vectors in
cand(bank
s
i
) is used as the initialization value of
vec(bank
s
i
).
2.3 Performing Word Sense Disambiguation.
One of the state-of-the-art WSD results can be
obtained using exemplar models, i.e., the word
meaning is modeled by using relevant occurrences
only, rather than merging all of the occurrences in-
to a single word vector (Erk and Pado, 2010). In-
spired by this idea, we perform word sense disam-
biguation to obtain more relevant occurrences.
Here, we perform knowledge-based word sense
disambiguation for training data on an all-words
setting, i.e., we will disambiguate all of the con-
tent words in a sentence. Formally, the sentence S
is a sequence of words (w
1
,w
2
,...,w
n
), and we will
identify a mapping M from words to senses such
that M(i) ? Senses
WN
(w
i
), where Senses
WN
(w
i
) is
the set of senses encoded in the WN for word w
i
.
For sentence S, there are
?
n
i=1
|Sense
WN
(w
i
)| pos-
sible mapping answers, which are impractical to
compute. Thus, we design two simple algorithms,
L2R (left to right) algorithm and S2C (simple to
complex) algorithm, for word sense disambigua-
tion based on the sense vectors.
The main difference between L2R and S2C is
1027
the order of words when performing word sense
disambiguation. When given a sentence, the L2R
algorithm disambiguates the words from left to
right (the natural order of a sentence), whereas the
S2C algorithm disambiguates the words with few-
er senses first. The main idea of S2C algorithm
is that the words with fewer senses are easier to
disambiguate, and the disambiguation result can
be helpful to disambiguate the words with more
senses. Both of the algorithms have three steps:
Context vector initialization. Similar to the ini-
tialization of sense vectors, we use the average of
all of the content words? vectors in a sentence as
the initialization vector of context.
vec(context) =
1
|cand(S)|
?
u?cand(S)
vec(u) (5)
where cand(S) is the set of content words
cand(S) = {u|u ? S,POS(u) ?CW}.
Ranking words. For L2R, we do nothing in this
step. For S2C, we rank the words based on the
ascending order of |Senses
WN
(w
i
)|.
Word sense disambiguation. For both L2R and
S2C, we denote the order of words as L and per-
form word sense disambiguation according to L.
First, we skip a word if the word is not
a content word or the word is monosemous
(|Senses
WN
(w
i
)| = 1). Then, for each word in
L, we can compute the cosine similarities be-
tween the context vector and its sense vectors. We
choose the sense that yields the maximum cosine
similarity as its disambiguation result. If the s-
core margin between the maximum and the sec-
ond maximum is larger than the threshold ? , we
are confident with the disambiguation result of w
i
and then use the sense vector to replace the word
vector in the context vector. Thus, we obtain a
more accurate context vector for other words that
are still yet to be disambiguated.
For example, given a sentence ?He sat on the
bank of the lake?, we first explain how S2C work-
s. In the sentence, there are three content word-
s, ?sat?, ?bank? and ?lake?, to be disambiguated.
First, the sum of the three word vectors is used as
the initialization of the context vector. Then we
rank the words by |Senses
W
N(w
i
)|, in ascending
order, that is, lake (3 senses), bank (10 senses), sat
(10 senses). We first disambiguate the word ?lake?
based on the similarities between its sense vectors
and context vector. If the score margin is larger
bankinput
projection
output sat on the of the lakesit lake1 1
Figure 1: The architecture of our model. The
training objective of Skip-gram is to train word
vector representations that are not only good at
predicting its context words but are also good at
predicting its context words? senses. The center
word ?bank? is used to predict not only its context
words but also the sense ?sit
1
? and ?lake
1
?.
than the threshold ? , then we are confident with
this disambiguation result and replace the word
vector with the sense vector to update the contex-
t vector. It would be helpful to disambiguate the
next word, ?bank?. We repeat this process until all
three words are disambiguated.
For L2R, the order of words to be disambiguat-
ed will be ?sat?, ?bank? and ?lake?. In this time,
when disambiguating ?bank? (10 senses), we still
don?t know the sense of ?lake? (3 senses).
2.4 Learning Sense Vectors from Relevant
Occurrences.
Based on the disambiguation result, we modify the
training objective of Skip-gram and train the sense
vectors directly from the large-scale corpus. Our
training objective is to train the vector representa-
tions that are not only good at predicting its con-
text words but are also good at predicting its con-
text words? senses. The architecture of our model
is shown in Figure 1.
More formally, given the disambiguation result
M(w
1
), M(w
2
), M(w
3
),...,M(w
T
), the training ob-
jective is modified to
1
T
T
?
t=1
(
k
?
j=?k
log{p(w
t+ j
|w
t
)p(M(w
t+ j
)|w
t
)}
)
(6)
where k is the size of the training window. The
inner summation spans from ?k to k to compute
the log probability of correctly predicting the word
w
t+ j
and the log probability of correctly predicting
1028
the sense M(w
t+ j
) given the word in the middle
w
t
. The outer summation covers all words in the
training data.
Because not all of the disambiguation results are
correct, we only disambiguate the words that we
are confident in. Similar to step 3 of our WSD
algorithm, we only disambiguate words under the
condition that the score margin between the max-
imum and the second maximum is larger than the
score margin threshold, ? .
We also use the softmax function to define
p(w
t+ j
|w
t
) and p(M(w
t+ j
)|w
t
). Then, we use hi-
erarchical softmax (Morin and Bengio, 2005) to
greatly reduce the computational complexity and
learn the sense vectors directly from the relevant
occurrences.
3 Experiments
In this section, we first present the nearest neigh-
bors of some words and their senses, showing that
our sense vectors can capture the semantics of
words. Then, we use three tasks to evaluate our u-
nified model: a contextual word similarity task to
evaluate our sense representations, and two stan-
dard WSD tasks to evaluate our knowledge-based
WSD algorithm based on the sense vectors. Ex-
perimental results show that our model not only
improves the correlation with human judgments
on the contextual word similarity task but also out-
performs state-of-the-art supervised WSD system-
s on domain-specific datasets and competes with
them in a coarse-grained all-words setting.
We choose Wikipedia as the corpus to train
the word vectors because of its wide coverage
of topics and words usages. We use an English
Wikipedia database dump from October 2013
2
,
which includes roughly 3 million articles and 1
billion tokens. We use Wikipedia Extractor
3
to
preprocess the Wikipedia pages and only save the
content of the articles.
We use word2vec
4
to train Skip-gram. We use
the default parameters of word2vec and the dimen-
sion of the vector representations is 200.
We use WordNet
5
as our sense inventory. The
datasets for different tasks are tagged with differ-
ent versions of WordNet. The version of WordNet
2
http://download.wikipedia.org.
3
The tool is available from http://medialab.di.
unipi.it/wiki/Wikipedia_Extractor.
4
The code is available from https://code.
google.com/p/word2vec/.
5
http://wordnet.princeton.edu/.
Word or sense Nearest neighbors
bank banks, IDBI, CitiBank
bank
s
1
river, slope, Sooes
bank
s
2
mortgage, lending, loans
star stars, stellar, trek
star
s
1
photosphere, radiation,
gamma-rays
star
s
2
someone, skilled, genuinely
plant plants, glavaticevo, herbaceous
plant
s
1
factories, machinery,
manufacturing
plant
s
2
locomotion, organism,
organisms
Table 2: Nearest neighbors of word vectors and
sense vectors learned by our model based on co-
sine similarity. The subscript of each sense label
corresponds to the index of the sense in Word-
Net. For example, bank
s
2
is the second sense of
the word bank in WordNet.
is 1.7 for the domain-specific WSD task and 2.1
for the coarse-grained WSD task.
We use the S2C algorithm described in Section
2.3 to perform word sense disambiguation to ob-
tain more relevant occurrences for each sense. We
compare S2C and L2R on the coarse-grained WS-
D task in a all-words setting.
The experimental results of our model are ob-
tained by setting the similarity threshold as ? = 0
and the score margin threshold as ? = 0.1. The in-
fluence of parameters on our model can be found
in Section 3.5.
3.1 Examples for Sense Vectors
Table 2 shows the nearest neighbors of word vec-
tors and sense vectors based on cosine similari-
ty. We see that our sense representations can i-
dentify different meanings of a word, allowing our
model to capture more semantic and syntactic re-
lationships between words and senses. Note that
each sense vector in our model corresponds to a
sense in WordNet; thus, our sense vectors can be
used to perform knowledge-based word sense dis-
ambiguation, whereas the vectors of cluster-based
models cannot.
3.2 Contextual Word Similarity
Experimental setting. A standard dataset for e-
valuating a vector-space model is the WordSim-
353 dataset (Finkelstein et al., 2001), which con-
1029
Model ??100
C&W-S 57.0
Huang-S 58.6
Huang-M AvgSim 62.8
Huang-M AvgSimC 65.7
Our Model-S 64.2
Our Model-M AvgSim 66.2
Our Model-M AvgSimC 68.9
Table 3: Spearman?s ? on the SCWS dataset. Our
Model-S uses one representation per word to com-
pute similarities, while Our Model-M uses one
representation per sense to compute similarities.
AvgSim calculates the similarity with each sense
contributing equally, while AvgSimC weighs the
sense according to the probability of the word
choosing that sense in context c.
sists of 353 pairs of nouns. However, each pair of
nouns in WordSim-353 is presented without con-
text. This is problematic because the meanings
of homonymous and polysemous words depend
highly on the words? contexts. Thus we choose the
Stanford?s Contextual Word Similarities (SCWS)
dataset from (Huang et al., 2012)
6
. The SCWS
dataset contains 2003 pairs of words and each pair
is associated with 10 human judgments on similar-
ity on a scale from 0 to 10. In the SCWS dataset,
each word in a pair has a sentential context.
In our experiments, the similarity between a
pair of words (w, w
?
) is computed as follows:
AvgSimC(w,w
?
) =
1
MN
M
?
i=1
N
?
j=1
p(i|w,c)p( j|w
?
,c
?
)d(vec(w
s
i
),vec(w
?
s
j
)) (7)
where p(i|w,c) is the likelihood that word w
chooses its ith sense given context c. d(vec,vec
?
)
is a function computing the similarity between two
vectors, and here we use cosine similarity.
Results and discussion. For evaluation, we
compute the Spearman correlation between a
model?s computed similarity scores and human
judgements. Table 3 shows our results com-
pared to previous methods, including (Collobert
and Weston, 2008)?s language model (C&W), and
Huang?s model which utilize the global context
and multi-prototype to improve the word represen-
tations.
6
The dataset can be downloaded at http://ai.
stanford.edu/
?
ehhuang/.
From Table 3, we observe that:
? Our single-vector version outperforms
Huang?s single-vector version. This indi-
cates that, by training the word vectors and
sense vectors jointly, our model can better
capture the semantic relationships between
words and senses.
? With one representation per sense, our mod-
el can outperform the single-vector version
without using context (66.2 vs. 64.2).
? Our model obtains the best performance
(68.9) by using AvgSimC, which takes con-
text into account.
3.3 Domain-Specific WSD
Experimental setting. We use Wikipedia as
training data because of its wide coverage for spe-
cific domains. To test our performance on do-
main word sense disambiguation, we evaluated
our system on the dataset published in (Koeling
et al., 2005). This dataset consists of examples
retrieved from the Sports and Finance sections of
the Reuters corpus. 41 words related to the Sports
and Finance domains were selected, with an aver-
age polysemy of 6.7 senses, ranging from 2 to 13
senses.
Approximately 100 examples for each word
were annotated with senses from WordNet v.1.7
by three reviewers, yielding an inter-tagger agree-
ment of 65%. (Koeling et al., 2005) did not clarify
how to select the ?correct? sense for each word, so
we followed the work of (Agirre et al., 2009) and,
used the sense chosen by the majority of taggers
as the correct answer.
Baseline methods. As a baseline, we use the
most frequent WordNet sense (MFS), as well as
a random sense assignment. We also compare our
results with four systems
7
: Static PageRank (A-
girre et al., 2009), the k nearest neighbor algorith-
m (k-NN), Degree (Navigli and Lapata, 2010) and
Personalized PageRank (Agirre et al., 2009).
Static PageRank applies traditional PageRank
over the semantic graph based on WordNet and
obtains a context-independent ranking of word
senses.
k-NN is a widely used classification method,
where neighbors are the k labeled examples most
7
We compare only with those systems performing token-
based WSD, i.e., disambiguating each instance of a target
word separately.
1030
Algorithm
Sports Finance
Recall Recall
Random BL 19.5 19.6
MFS BL 19.6 37.1
k-NN 30.3 43.4
Static PR 20.1 39.6
Personalized PR 35.6 46.9
Degree 42.0 47.8
Our Model 57.3 60.6
Table 4: Performance on the Sports and Finance
sections of the dataset from (Koeling et al., 2005).
similar to the test example. The k-NN system is
trained on SemCor (Miller et al., 1993), the largest
publicly available annotated corpus.
Degree and Personalized PageRank are state-
of-the-art systems that exploit WordNet to build
a semantic graph and exploit the structural proper-
ties of the graph in order to choose the appropriate
senses of words in context.
Results and discussion. Similar to other work
on this dataset, we use recall (the ratio of correct
sense labels to the total labels in the gold standard)
as our evaluation measure. Table 4 shows the re-
sults of different WSD systems on the dataset, and
the best results are shown in bold. The differences
between other results and the best result in each
column of the table are statistically significant at
p < 0.05.
The results show that:
? Our model outperforms k-NN on the t-
wo domains by a large margin, support-
ing the findings from (Agirre et al., 2009)
that knowledge-based systems perform bet-
ter than supervised systems when evaluated
across different domains.
? Our model also achieves better results than
the state-of-the-art system (+15.3% recall on
Sports and +12.8% recall on Finance against
Degree). The reason for this is that when
dealing with short sentences or context words
that are not in WordNet, our model can still
compute similarity based on the context vec-
tor and sense vectors, whereas Degree will
have difficulty building the semantic graph.
? Moreover, our model achieves the best per-
formance by only using the unlabeled text da-
ta and the definitions of senses, whereas other
Algorithm Type
Nouns only All words
F
1
F
1
Random BL U 63.5 62.7
MFS BL Semi 77.4 78.9
SUSSX-FR Semi 81.1 77.0
NUS-PT S 82.3 82.5
SSI Semi 84.1 83.2
Degree Semi 85.5 81.7
Our Model
L2R
U 79.2 73.9
Our Model
S2C
U 81.6 75.8
Our Model
L2R
Semi 82.5 79.6
Our Model
S2C
Semi 85.3 82.6
Table 5: Performance on Semeval-2007 coarse-
grained all-words WSD. In the type column,
U, Semi and S stand for unsupervised, semi-
supervised and supervised, respectively. The dif-
ferences between the results in bold in each col-
umn of the table are not statistically significant at
p < 0.05.
methods rely greatly on high-quality seman-
tic relations or annotated data, which are hard
to acquire.
3.4 Coarse-grained WSD
Experimental setting. We also evaluate our
WSD model on the Semeval-2007 coarse-grained
all-words WSD task (Navigli et al., 2007). There
are multiple reasons that we perform experiments
in a coarse-grained setting: first, it has been ar-
gued that the fine granularity of WordNet is one
of the main obstacles to accurate WSD (cf. the
discussion in (Navigli, 2009)); second, the train-
ing corpus of word representations is Wikipedia,
which is quite different from WordNet.
Baseline methods. We compare our model with
the best unsupervised system SUSSX-FR (Koel-
ing and McCarthy, 2007), and the best supervised
system, NUS-PT (Chan et al., 2007), participat-
ing in the Semeval-2007 coarse-grained all-words
task. We also compare with SSI (Navigli and Ve-
lardi, 2005) and the state-of-the-art system De-
gree (Navigli and Lapata, 2010). We use different
baseline methods for the two WSD tasks because
we want to compare our model with the state-
of-the-art systems that are applicable to different
datasets and show that our WSD method can per-
form robustly in these different WSD tasks.
1031
Results and discussion. We report our results in
terms of F
1
-measure on the Semeval-2007 coarse-
grained all-words dataset (Navigli et al., 2007).
Table 5 reports the results for nouns (1,108 words)
and all words (2,269 words). The difference be-
tween unsupervised and semi-supervised methods
is whether the method uses MFS as a back-off s-
trategy.
We can see that the S2C algorithm outperforms
the L2R algorithm no matter on the nouns subset
or on the entire set. This indicates that words with
fewer senses are easier to disambiguate, and it can
be helpful to disambiguate the words with more
senses.
On the nouns subset, our model yields compa-
rable performance to SSI and Degree, and it out-
performs NUS-PT and SUSSX-FR. Moreover, our
unsupervised WSD method (S2C) beats the MF-
S baseline, which is notably a difficult competitor
for knowledge-based systems.
On the entire set, our semi-supervised model is
significantly better than SUSSX-FR, and it is com-
parable with SSI and Degree. In contrast to SSI,
our model is simple and does not rely on a cost-
ly annotation effort to engineer the set of semantic
relations.
Overall, our model achieves state-of-the-art per-
formance on the Semeval-2007 coarse-grained all-
words dataset compared to other systems, with a
simple WSD algorithm that only relies on a large-
scale unlabeled text corpora and a sense inventory.
3.5 Parameter Influence
We investigate the influence of parameters on our
model with coarse-grained all-words WSD task.
The parameters include the similarity threshold, ? ,
and the score margin threshold, ? .
Similarity threshold. In Table 6, we show the
performance of domain WSD when the similari-
ty threshold ? ranges from ?0.1 to 0.3. The co-
sine similarity interval is [-1, 1], and we focus on
the performance in the interval [-0.1, 0.3] for two
reasons: first, no words are removed from glosses
when ? < ?0.1; second, nearly half of the word-
s are removed when ? > 0.3 and the performance
drops significantly for the WSD task. From table
6, we can see that our model achieves the best per-
formance when ? = 0.0.
Score margin threshold. In Table 7, we show
the performance on the coarse-grained all-words
Parameter Nouns only All words
? =?0.10 79.8 74.3
? =?0.05 81.0 74.6
? = 0.00 81.6 75.8
? = 0.05 81.3 75.4
? = 0.10 80.8 75.2
? = 0.15 80.0 75.0
? = 0.20 77.1 73.3
? = 0.30 75.0 72.1
Table 6: Evaluation results on the coarse-grained
all-words WSD when the similarity threshold ?
ranges from ?0.1 to 0.3.
Parameter Nouns only All words
? = 0.00 78.2 72.9
? = 0.05 79.5 74.5
? = 0.10 81.6 75.8
? = 0.15 81.2 74.7
? = 0.20 80.9 75.1
? = 0.25 80.2 74.8
? = 0.30 80.4 74.9
Table 7: Evaluation results on the coarse-grained
all-words WSD when the score margin threshold
? ranges from 0.0 to 0.3.
WSD when the score margin threshold ? ranges
from 0.0 to 0.3. When ? = 0.0, we use every
disambiguation result to update the context vec-
tor. When ? 6= 0, we only use the confident disam-
biguation results to update the context vector if the
score margin is larger than ? . Our model achieves
the best performance when ? = 0.1.
4 Related Work
4.1 Word Representations
Distributed representations for words were pro-
posed in (Rumelhart et al., 1986) and have been
successfully used in language models (Bengio et
al., 2006; Mnih and Hinton, 2008) and many nat-
ural language processing tasks, such as word rep-
resentation learning (Mikolov, 2012), named enti-
ty recognition (Turian et al., 2010), disambigua-
tion (Collobert et al., 2011), parsing and tag-
ging (Socher et al., 2011; Socher et al., 2013).
They are very useful in NLP tasks because they
can be used as inputs to learning algorithms or as
extra word features in NLP systems. Hence, many
NLP applications, such as keyword extraction (Li-
1032
u et al., 2010; Liu et al., 2011b; Liu et al., 2012),
social tag suggestion (Liu et al., 2011a) and text
classification (Baker and McCallum, 1998), may
also potentially benefit from distributed word rep-
resentation. The main advantage is that the rep-
resentations of similar words are close in vector
space, which makes generalization to novel pat-
terns easier and model estimation more robust.
Word representations are hard to train due to the
computational complexity. Recently, (Mikolov et
al., 2013) proposed two particular models, Skip-
gram and CBOW, to learn word representations in
large amounts of text data. The training objective
of the CBOW model is to combine the representa-
tions of the surrounding words to predict the word
in the middle, while the Skip-gram model?s is to
learn word representations that are good at predict-
ing its context in the same sentence (Mikolov et
al., 2013). Our paper uses the model architecture
of Skip-gram.
Most of the previous vector-space models use
one representation per word. This is problematic
because many words have multiple senses. The
multi-prototype approach has been widely stud-
ied. (Reisinger and Mooney, 2010) proposed the
multi-prototype vector-space model. (Huang et
al., 2012) used the multi-prototype models to learn
the vector for different senses of a word. All of
these models use the clustering of contexts as a
word sense and can not be directly used in word
sense disambiguation.
After our paper was submitted, we perceive the
following recent advances: (Tian et al., 2014) pro-
posed a probabilistic model for multi-prototype
word representation. (Guo et al., 2014) explored
bilingual resources to learn sense-specific word
representation. (Neelakantan et al., 2014) pro-
posed an efficient non-parametric model for multi-
prototype word representation.
4.2 Knowledge-based WSD
The objective of word sense disambiguation (WS-
D) is to computationally identify the meaning of
words in context (Navigli, 2009). There are t-
wo approaches of WSD that assign meaning of
words from a fixed sense inventory, supervised and
knowledge-based methods. Supervised approach-
es require large labeled training sets, which are
time consuming to create. In this paper, we on-
ly focus on knowledge-based word sense disam-
biguation.
Knowledge-based approaches exploit knowl-
edge resources (such as dictionaries, thesauri, on-
tologies, collocations, etc.) to determine the
senses of words in context. However, it has
been shown in (Cuadros and Rigau, 2006) that
the amount of lexical and semantic information
contained in such resources is typically insuf-
ficient for high-performance WSD. Much work
has been presented to automatically extend ex-
isting resources, including automatically linking
Wikipedia to WordNet to include full use of the
first WordNet sense heuristic (Suchanek et al.,
2008), a graph-based mapping of Wikipedia cat-
egories to WordNet synsets (Ponzetto and Nav-
igli, 2009), and automatically mapping Wikipedia
pages to WordNet synsets (Ponzetto and Navigli,
2010).
It was recently shown that word representation-
s can capture semantic and syntactic information
between words (Mikolov et al., 2013). Some re-
searchers tried to incorporate WordNet senses in a
neural model to learn better word representation-
s (Bordes et al., 2011). In this paper, we have pro-
posed a unified method for word sense representa-
tion and disambiguation to extend the information
contained in the vector representations to the ex-
isting resources. Our method only requires a large
amount of unlabeled text to train sense representa-
tions and a dictionary to provide the definitions of
word meanings, which makes it easily applicable
to other resources.
5 Conclusion
In this paper, we present a unified model for word
sense representation and disambiguation that us-
es one representation per sense. Experimental re-
sults show that our model improves the perfor-
mance of contextual word similarity compared to
existing WSR methods, outperforms state-of-the-
art supervised methods on domain-specific WSD,
and achieves competitive performance on coarse-
grained all-words WSD. Our model only requires
large-scale unlabeled text corpora and a sense in-
ventory for WSD, thus it can be easily applied to
other corpora and tasks.
There are still several open problems that
should be investigated further:
1. Because the senses of words change over
time (new senses appear), we will incorpo-
rate cluster-based methods in our model to
find senses that are not in the sense inventory.
1033
2. We can explore other WSD methods based
on sense vectors to improve our performance.
For example, (Li et al., 2010) used LDA to
perform data-driven WSD in a manner simi-
lar to our model. We may integrate the advan-
tages of these models and our model together
to build a more powerful WSD system.
3. To learn better sense vectors, we can exploit
the semantic relations (such as the hypernym
and hyponym relations defined in WordNet)
between senses in our model.
Acknowledgments
This work is supported by National Key Ba-
sic Research Program of China (973 Program
2014CB340500) and National Natural Science
Foundation of China (NSFC 61133012).
References
Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, and
Informatika Fakultatea. 2009. Knowledge-based
wsd and specific domains: Performing better than
generic supervised wsd. In Proceedings of IJCAI,
pages 1501?1506.
L Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of SIGIR, pages 96?
103.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137?186.
Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured
embeddings of knowledge bases. In Proceedings of
AAAI, pages 301?306.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253?256.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493?2537.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proceedings of EMNLP, pages 534?541.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92?97.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-
hud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of WWW, pages 406?
414.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Li-
u. 2014. Learning sense-specific word embeddings
by exploiting bilingual resources. In Proceedings of
COLING, pages 497?507.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873?882.
Rob Koeling and Diana McCarthy. 2007. Sussx: Ws-
d using automatically acquired predominant senses.
In Proceedings of SemEval, pages 314?317.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419?426.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proceedings of
ACL, pages 1138?1147.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of
EMNLP, pages 366?376.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun.
2011a. A simple word trigger method for social tag
suggestion. In Proceedings of EMNLP, pages 1577?
1588.
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011b. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
CoNLL, pages 135?144.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun. 2012.
Mining the interests of chinese microbloggers via
keyword extraction. Frontiers of Computer Science,
6(1):76?87.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval. Cambridge University Press Cambridge.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of ICLR.
Tomas Mikolov. 2012. Statistical Language Model-
s Based on Neural Networks. Ph.D. thesis, Ph. D.
thesis, Brno University of Technology.
1034
George A Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the workshop on HLT, pages 303?
308.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Proceedings of NIPS, pages 1081?1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsu-
pervised word sense disambiguation. IEEE PAMI,
32(4):678?692.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE PAMI,
27(7):1075?1086.
Roberto Navigli, Kenneth C Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30?35.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. CSUR, 41(2):10.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating wikipedia. In Proceedings of IJCAI,
volume 9, pages 2083?2088.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522?1531.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of HLT-NAACL, pages 109?
117.
David E Rumelhart, Geoffrey E Hintont, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533?536.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. CSUR, 34(1):1?47.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011. Parsing natural scenes and natural
language with recursive neural networks. In Pro-
ceedings of ICML, pages 129?136.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING, pages 151?160.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384?394.
1035
Punctuation as Implicit Annotations
for Chinese Word Segmentation
Zhongguo Li?
Tsinghua University
Maosong Sun??
Tsinghua University
We present a Chinese word segmentation model learned from punctuation marks which are
perfect word delimiters. The learning is aided by a manually segmented corpus. Our method
is considerably more effective than previous methods in unknown word recognition. This is a
step toward addressing one of the toughest problems in Chinese word segmentation.
1. Introduction
Paragraphs are composed of sentences. Hence when a paragraph begins, a sentence
must begin, and as a paragraph closes, some sentence must finish. This observation
is the basis of the sentence boundary detection method proposed by Riley (1989).
Similarly, sentences consist of words. As a sentence begins or ends there must be word
boundaries.
Inspired by this notion, we invent a method to learn a Chinese word segmenta-
tion model with punctuation marks in a large raw corpus. The learning is guided by
a segmented corpus (Section 3.2). Section 4 demonstrates that our method improves
notably the recognition of out-of-vocabulary (OOV) words with respect to approaches
which use only annotated data (Xue 2003; Low, Ng, and Guo 2005). This work has
practical implications in that the OOV problem has long been a big challenge for the
research community.
2. Segmentation as Tagging
We call the first character of a Chinese word its left boundary L, and the last character
its right boundary R. If we regard L and R as random events, then we can derive four
events (or tags) from them:
b = L ?R, m = L ? R, s = L ?R, e = L ? R
? Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China.
E-mail: eemath@gmail.com.
?? Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China.
E-mail: sms@mail.thu.edu.cn.
Submission received: 16 July 2008; revised submission received: 26 March 2009; accepted for publication:
4 May 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
Here R means not R, and thus tag b represents the left but not the right boundary of
a word. The other tags can be interpreted similarly. This coding scheme was used by
Borthwick (1999) and Xue (2003), where b, m, s, and e stand for begin, middle, only
member, and end of a word, respectively. We reformulate them in terms of L and R to
facilitate the presentation of our method.
For a sentence S = c1c2 ? ? ? cn and a sequence T = t1t2 ? ? ? tn of b, m, s, e tags, we
define
P (T|S) =
n?
i=1
Pr(ti|contexti) (1)
where contexti is ci with up to four surrounding characters. The legal tag sequence
(e.g., tag b followed by s is illegal) with highest P gives the segmentation result of
S. Then from Equation (1) it is obvious that knowing the probability distribution of
b, m, s, and e given context is adequate for carrying out Chinese word segmentation. The
purpose of this article is to show that punctuation can play a major role in estimating
this distribution.
We use the maximum entropy approach to model the conditional probability
Pr(y|x), which has the following parametric form according to Berger, Della Pietra, and
Della Pietra (1996):
Pr(y|x) = 1Z(x) exp
(?
i
?i fi(x, y)
)
(2)
Z(x) =
?
y
exp
(?
i
?i fi(x, y)
)
(3)
For Chinese word segmentation, the binary valued functions fi are defined through the
10 features shown in Table 2. Xue (2003) explains how these features map to the feature
functions in Equations (2) and (3).
3. Method
Our key idea is to approximate probabilities of b, m, s, and e with those of L and R.
To do this, we assume L and R are conditionally independent given context. Then we
have
Pr(b |context) = Pr(L ?R|context) (definition of b)
= Pr(L |context) ? Pr(R|context) (independence)
= Pr(L |context) ? (1 ? Pr(R |context)) (4)
Probabilities for m, s, and e can be derived in the same way and so their derivations are
not provided here. As mentioned earlier, these probabilities are sufficient for Chinese
word segmentation. Now to model Pr(L |context) and Pr(R |context) with the maximum
506
Li and Sun Punctuation as Implicit Annotations
entropy technique, we must have positive and negative examples of L and R. It is here
that punctuation comes into play.
3.1 Positive Examples
Punctuation offers directly positive examples of L and R. For instance, we can extract
four training examples from the sentence in Table 1, as listed in Table 2.
3.2 Negative Examples
Suppose for the moment we know the real probability distribution of tags b, m, s, and e
given context. Then a character in context is itself a word and should be tagged s if
Pr(s |context) > max
y?{b,m,e}
Pr(y|context) (5)
Each positive example given by punctuation is subjected to the test in (5). If an example
labeled L passes this test, then it is also a positive example of R because s = L ?R, and
failing this test gives a negative R. In a similar way we obtain negative examples of L.
This process is summarized in Figure 1.
A segmented corpus is needed to estimate the probabilities in test (5) with maxi-
mum entropy modeling. Here we use the data provided by Microsoft Research in the
SIGHAN 2005 Bakeoff. The trained model (the MSR model) was used in earlier work
(Low, Ng, and Guo 2005) and is one of the state-of-the-art models for Chinese word
segmentation.
With the MSR model, only the last example in Table 2 passes test (5). Hence we get
the three negative examples shown in Table 3. Examples like 1, 3, 6, and 8 are used to
estimate Pr(L |context) and those like 2, 4, 5, and 7 are used to estimate Pr(R |context).
Appendix A provides more details on this issue.
Table 1
Illustration of word boundaries near punctuation in a simple sentence.
? means the label is unknown with only the help of punctuation.
sentence 3 I  ?  0  G ? 
word boundary L ? ? R L ? ? R
Table 2
Positive training examples extracted from the sentence in Table 1.
features of context
No. label c?2 c?1 c0 c1 c2 c?1c1 c?2c?1 c?1c0 c0c1 c1c2
1 L 3 I  3I I
2 R I  ? 0  0 I ? ?0 0
3 L  ? 0  G ? ? ?0 0 G
4 R  G ? G G?
507
Computational Linguistics Volume 35, Number 4
Figure 1
How to get negative examples of L and R. Test (5) is applied to all positive examples given by
punctuation. Those failing this test are negative training examples. It is test (5) that invokes the
need of a manually segmented corpus.
Table 3
Training examples derived from those in Table 2. We have 1?5, 2?6, 3?7, and 4?8.
features of context
No. label c?2 c?1 c0 c1 c2 c?1c1 c?2c?1 c?1c0 c0c1 c1c2
5 R 3 I  3I I
6 L I  ? 0  0 I ? ?0 0
7 R  ? 0  G ? ? ?0 0 G
8 L  G ? G G?
3.3 Training
In all, we collected 10 billion L-L and R-R examples, each from a comprehensive Web
corpus.1 To cope with so much training data, we use the partitioning method of Yamada
and Matsumoto (2003). An alternative is the Vowpal Wabbit (fast on-line learning)
algorithm.2 Such an algorithm allows incremental training as more raw texts become
available.
4. Evaluation
We evaluate our method with the data and scoring script provided by the SIGHAN 2005
Bakeoff. The data sets of Academia Sinica and City University of Hong Kong, which are
in Traditional Chinese, are not used here because the raw corpus is mainly in Simplified
Chinese. Table 4 gives the evaluation results on the data from Microsoft Research (MSR)
and Peking University (PKU).
It seems our method is over 10% below state of the art in precision on the MSR data.
However, we find that multiword expressions are consistently segmented into smaller
words. Take the one multiword ?-?z/vb-??v@? [Institute of Chinese
1 Freely available for research purposes. See www.sogou.com/labs.
2 http://hunch.net/?vw/. We thank one of the anonymous reviewers for telling us about this
implementation.
508
Li and Sun Punctuation as Implicit Annotations
Culture, Chinese Academy of Arts] in the standard answer of the test data as an example.
Our method segments it into six correct words ?-? z/ vb -? ? v@?
[China, art, academy, China, culture, institute], all of which are considered wrong by the
scoring script. This is arguable because the only difference is the granularity of the
segmentation.
4.1 Influence of Granularity
We check every error detected by the scoring script on the MSR data, and find that
for our method, 15,071 errors are actually correct segmentations of 5,463 multiwords,
whereas for the MSR model, the corresponding counts are 858 and 355, respectively. The
gold standard contains 106,873 words. These statistics combined with Table 4 allow us
to calculate the metrics as in Table 5, if errors caused by correctly segmented multiwords
are not counted.
We see that, when the influence of granularity is considered, our method is slightly
better than the MSR model. However, as Table 4 shows, both models degrade on the
PKU data due to the difference in segmentation standards. This kind of degradation
was also documented by Peng, Feng, and McCallum (2004).
4.2 Named Entity List Recovery
The SIGHAN data sets contain relatively few OOV words (2.6% for the MSR data). What
if the rate is much higher than that? We expect our model to be less vulnerable to OOV
problems because it is trained with billions of examples from a large corpus. To verify
this, we generate four data sets from each of these lists of names:
(a) 702 cities and counties of China seen in the MSR data
(b) 1,634 cities and counties of China not seen in the MSR data
(c) 7,470 Chinese personal names seen in the MSR data
(d) 20,000 Chinese personal names not seen in the MSR data
Table 4
Evaluation results on SIGHAN Bakeoff 2005 data sets.
our method the MSR model
data set P R F P R F
MSR 84.8 91.3 87.9 96.0 95.6 95.8
PKU 84.2 86.1 85.1 85.2 82.3 83.7
Table 5
Amended evaluation results for MSR data.
P R F
our method 98.0 96.7 97.3
the MSR model 96.7 96.0 96.3
509
Computational Linguistics Volume 35, Number 4
Table 6
Results on tasks of named entity list recovery.
our method the MSR model
data set P R F P R F
(a) 91.0 93.8 92.4 43.3 29.1 34.8
(b) 79.4 85.3 82.2 25.1 16.9 20.2
(c) 74.9 85.0 79.6 69.4 66.5 67.9
(d) 86.3 91.5 88.8 65.4 61.0 63.1
The generation method is: Randomly permute each list and then put the result into lines,
with each line having about 30 names, and repeat this process until we get 1 million
tokens for each data set. We use the MSR model and our method to segment these data
sets. The results are in Table 6.
It is clear that our method performs better on these data sets. This provides evidence
that it could handle situations where many OOV words turn up. Table 6 also indicates
that, especially for the MSR model, recognition of Chinese personal names is easier
than location names. This is reasonable because the former has more regularity than the
latter. Besides, although there are no OOV words in data sets (a) and (c), many words
occur very sparsely in the MSR data. Hence the MSR model doesn?t do well even on
these two data sets.
4.3 Unknown Words Recognition
To further test our model?s ability to recognize unknown words, we make 27,470 sen-
tences with the pattern ?X / Y ?  X ?" Y ? (X is a resident of Y, and X loves
Y), where X and Y are the personal and location names in Section 4.2. The results on
this data set are in Table 7. Again our method outperforms the MSR model by a large
margin, proving once more that it is stronger in unknown word recognition. For both
methods, the metrics in Table 7 are better than those in Table 6, reflecting the fact that
unknown word recognition here is easier than the named entity list recovery task.
4.4 Summary
Evaluation shows that when there are many new words, the improvement of our
method is obvious. In addition, a model is of limited use if it fits the SIGHAN data well,
but can?t maintain that accuracy elsewhere. Our model has a wider coverage through
Table 7
Results of unknown word recognition in 24,470 sentences.
P R F
our method 96.2 97.9 97.1
the MSR model 88.3 84.5 86.3
510
Li and Sun Punctuation as Implicit Annotations
mining the Web. It tends to segment long multiword expressions into their component
words. This is not a disadvantage as long as the result is consistent.
5. Related Work
Punctuation gives naturally occurring unambiguous word boundaries. Gao et al (2005)
described how to remove overlapping ambiguities in an annotated corpus to train a
model for resolving these ambiguities. A raw corpus doesn?t play a role in that method,
and the model involves no punctuation marks.
Chinese word segmentation based on position tagging was initiated by Xue (2003).
This method and its subsequent developments have achieved state-of-the-art perfor-
mance in word segmentation (Peng, Feng, and McCallum 2004; Low, Ng, and Guo 2005;
Zhao, Huang, and Li 2006). Yet the system degrades when there are lots of previously
unknown words, whereas our method performs particular well in this case thanks to
the use of a huge Web corpus.
In the past decade, much work has been done in unsupervised word segmentation
(Sun, Shen, and Tsou 1998; Peng and Schuurmans 2001; Feng et al 2004; Goldwater,
Griffiths, and Johnson 2006; Jin and Tanaka-Ishii 2006). These methods could also take
advantage of the ever-growing amount of online text to model Chinese word segmen-
tation, but usually are less accurate and more complicated than ours.
6. Conclusion
With a virtually unlimited supply of raw corpus data, punctuation marks give us ample
training examples and thus can be quite useful as implicit annotations for Chinese
word segmentation. We also note that shallow parsing (Sha and Pereira 2003) is a close
analogy to word segmentation. Hence our method can potentially be applied to this
task as well.
Appendix A: Input to the Training Algorithm
We give readers a feel for the input data used to train our probability models. First, to
estimate Pr(L |context), the input to the learning algorithm for the maximum entropy
models looks like this:
+L C0=3 C1=I C2= C0C1=3I C1C2=I
+L C0=0 C1= C2=G C0C1=0 C1C2=G
-L C-2=I C-1= C0=? C-2C-1=I C-1C0=?
+L C-2= C-1=G C0=? C-2C-1=G C-1C0=G?
Whereas to estimate Pr(R | context), the input data are something like the following
+R C-2=I C-1= C0=? C-2C-1=I C-1C0=?
+R C-2= C-1=G C0=? C-2C-1=G C-1C0=G?
-R C0=3 C1=I C2= C0C1=3I C1C2=I
-R C0=0 C1= C2=G C0C1=0 C1C2=G
To save space, not all features in Table 2 are included here. From this illustration,
interested readers can get a general idea of our input to the learning algorithm in
Section 3.3.
511
Computational Linguistics Volume 35, Number 4
Acknowledgments
This work is supported by the National
Science Foundation of China under Grant
No. 60621062 and 60873174, and the National
863 Project under Grant No. 2007AA01Z148.
We thank our reviewers sincerely for many
helpful comments and suggestions which
greatly improved this article. Thanks also go
to sogou.com for sharing their Web corpora
and entity names. The maximum entropy
modeling toolkit used here is contributed by
Zhang Le of the University of Edinburgh.
References
Berger, Adam L., Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Borthwick, Andrew. 1999. A Maximum
Entropy Approach to Named Entity
Recognition. Ph.D. thesis, New York
University.
Feng, Haodi, Kang Chen, Xiaotie Deng, and
Weimin Zheng. 2004. Accessor variety
criteria for Chinese word extraction.
Computational Linguistics, 30(1):75?93.
Gao, Jianfeng, Mu Li, Andi Wu, and
Chang-Ning Huang. 2005. Chinese word
segmentation and named entity
recognition: A pragmatic approach.
Computational Linguistics, 31(4):531?574.
Goldwater, Sharon, Thomas L. Griffiths,
and Mark Johnson. 2006. Contextual
dependencies in unsupervised word
segmentation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 673?680, Sydney.
Jin, Zhihui and Kumiko Tanaka-Ishii. 2006.
Unsupervised segmentation of Chinese
text by use of branching entropy. In
Proceedings of the COLING/ACL on Main
Conference Poster Sessions, pages 428?435,
Morristown, NJ.
Low, Jim Kiat, Hwee Tou Ng, and Wenyuan
Guo. 2005. A maximum entropy
approach to Chinese word segmentation.
In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing,
pages 161?164, Jeju Island.
Peng, Fuchun, Fangfang Feng, and Andrew
McCallum. 2004. Chinese segmentation
and new word detection using conditional
random fields. In COLING ?04: Proceedings
of the 20th International Conference on
Computational Linguistics, pages 562?569,
Morristown, NJ.
Peng, Fuchun and Dale Schuurmans.
2001. Self-supervised Chinese word
segmentation. Lecture Notes in Computer
Science, 2189:238?249.
Riley, Michael D. 1989. Some applications
of tree-based modelling to speech and
language. In HLT ?89: Proceedings of the
Workshop on Speech and Natural Language,
pages 339?352, Morristown, NJ.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional
random fields. In NAACL ?03: Proceedings
of the 2003 Conference of the North
American Chapter of the Association for
Computational Linguistics on Human
Language Technology, pages 134?141,
Morristown, NJ.
Sun, Maosong, Dayang Shen, and
Benjamin K. Tsou. 1998. Chinese word
segmentation without using lexicon and
hand-crafted training data. In Proceedings
of the 17th International Conference on
Computational Linguistics, pages 1265?1271,
Morristown, NJ.
Xue, Nianwen. 2003. Chinese word
segmentation as character tagging.
Computational Linguistics and Chinese
Language Processing, 8(1):29?48.
Yamada, Hiroyasu and Yuji Matsumoto.
2003. Statistical dependency analysis
with support vector machines. In
Proceedings of the 8th International Workshop
on Parsing Technologies (IWPT2003),
pages 195?206, Nancy.
Zhao, Hai, Chang-Ning Huang, and Mu Li.
2006. An improved Chinese word
segmentation system with conditional
random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language
Processing, pages 162?165, Sydney.
512
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 93?101,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Semi-Supervised SimHash for Efficient Document Similarity Search
Qixia Jiang and Maosong Sun
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China
qixia.jiang@gmail.com, sms@tsinghua.edu.cn
Abstract
Searching documents that are similar to a
query document is an important component
in modern information retrieval. Some ex-
isting hashing methods can be used for effi-
cient document similarity search. However,
unsupervised hashing methods cannot incor-
porate prior knowledge for better hashing.
Although some supervised hashing methods
can derive effective hash functions from prior
knowledge, they are either computationally
expensive or poorly discriminative. This pa-
per proposes a novel (semi-)supervised hash-
ing method named Semi-Supervised SimHash
(S3H) for high-dimensional data similarity
search. The basic idea of S3H is to learn the
optimal feature weights from prior knowledge
to relocate the data such that similar data have
similar hash codes. We evaluate our method
with several state-of-the-art methods on two
large datasets. All the results show that our
method gets the best performance.
1 Introduction
Document Similarity Search (DSS) is to find sim-
ilar documents to a query doc in a text corpus or
on the web. It is an important component in mod-
ern information retrieval since DSS can improve the
traditional search engines and user experience (Wan
et al, 2008; Dean et al, 1999). Traditional search
engines accept several terms submitted by a user
as a query and return a set of docs that are rele-
vant to the query. However, for those users who
are not search experts, it is always difficult to ac-
curately specify some query terms to express their
search purposes. Unlike short-query based search,
DSS queries by a full (long) document, which allows
users to directly submit a page or a document to the
search engines as the description of their informa-
tion needs. Meanwhile, the explosion of information
has brought great challenges to traditional methods.
For example, Inverted List (IL) which is a primary
key-term access method would return a very large
set of docs for a query document, which leads to the
time-consuming post-processing. Therefore, a new
effective algorithm is required.
Hashing methods can perform highly efficient but
approximate similarity search, and have gained great
success in many applications such as Content-Based
Image Retrieval (CBIR) (Ke et al, 2004; Kulis et
al., 2009b), near-duplicate data detection (Ke et
al., 2004; Manku et al, 2007; Costa et al, 2010),
etc. Hashing methods project high-dimensional ob-
jects to compact binary codes called fingerprints and
make similar fingerprints for similar objects. The
similarity search in the Hamming space1 is much
more efficient than in the original attribute space
(Manku et al, 2007).
Recently, several hashing methods have been pro-
posed. Specifically, SimHash (SH) (Charikar M.S.,
2002) uses random projections to hash data. Al-
though it works well with long fingerprints, SH has
poor discrimination power for short fingerprints. A
kernelized variant of SH, called Kernelized Local-
ity Sensitive Hashing (KLSH) (Kulis et al, 2009a),
is proposed to handle non-linearly separable data.
These methods are unsupervised thus cannot incor-
porate prior knowledge for better hashing. Moti-
1Hamming space is a set of binary strings of length L.
93
vated by this, some supervised methods are pro-
posed to derive effective hash functions from prior
knowledge, i.e., Spectral Hashing (Weiss et al,
2009) and Semi-Supervised Hashing (SSH) (Wang
et al, 2010a). Regardless of different objectives,
both methods derive hash functions via Principle
Component Analysis (PCA) (Jolliffe, 1986). How-
ever, PCA is computationally expensive, which lim-
its their usage for high-dimensional data.
This paper proposes a novel (semi-)supervised
hashing method, Semi-Supervised SimHash (S3H),
for high-dimensional data similarity search. Un-
like SSH that tries to find a sequence of hash func-
tions, S3H fixes the random projection directions
and seeks the optimal feature weights from prior
knowledge to relocate the objects such that simi-
lar objects have similar fingerprints. This is im-
plemented by maximizing the empirical accuracy
on the prior knowledge (labeled data) and the en-
tropy of hash functions (estimated over labeled and
unlabeled data). The proposed method avoids us-
ing PCA which is computationally expensive espe-
cially for high-dimensional data, and leads to an
efficient Quasi-Newton based solution. To evalu-
ate our method, we compare with several state-of-
the-art hashing methods on two large datasets, i.e.,
20 Newsgroups (20K points) and Open Directory
Project (ODP) (2.4 million points). All experiments
show that S3H gets the best search performance.
This paper is organized as follows: Section 2
briefly introduces the background and some related
works. In Section 3, we describe our proposed Semi-
Supervised SimHash (S3H). Section 4 provides ex-
perimental validation on two datasets. The conclu-
sions are given in Section 5.
2 Background and Related Works
Suppose we are given a set of N documents, X =
{xi | xi ? RM}Ni=1. For a given query doc q, DSS
tries to find its nearest neighbors in X or a subset
X ? ? X in which distance from the documents to
the query doc q is less than a give threshold. How-
ever, such two tasks are computationally infeasible
for large-scale data. Thus, it turns to the approxi-
mate similarity search problem (Indyk et al, 1998).
In this section, we briefly review some related ap-
proximate similarity search methods.
2.1 SimHash
SimHash (SH) is first proposed by Charikar
(Charikar M.S., 2002). SH uses random projections
as hash functions, i.e.,
h(x) = sign(wTx) =
{
+1, if wTx ? 0
?1, otherwise (1)
where w ? RM is a vector randomly generated. SH
specifies the distribution on a family of hash func-
tions H = {h} such that for two objects xi and xj ,
Pr
h?H
{h(xi) = h(xj)} = 1?
?(xi,xj)
?
(2)
where ?(xi,xj) is the angle between xi and xj . Ob-
viously, SH is an unsupervised hashing method.
2.2 Kernelized Locality Sensitive Hashing
A kernelized variant of SH, named Kernelized
Locality Sensitive Hashing (KLSH) (Kulis et al,
2009a), is proposed for non-linearly separable data.
KLSH approximates the underling Gaussian distri-
bution in the implicit embedding space of data based
on central limit theory. To calculate the value of
hashing fuction h(?), KLSH projects points onto the
eigenvectors of the kernel matrix. In short, the com-
plete procedure of KLSH can be summarized as fol-
lows: 1) randomly select P (a small value) points
from X and form the kernel matrix, 2) for each hash
function h(?(x)), calculate its weight ? ? RP just
as Kernel PCA (Scho?lkopf et al, 1997), and 3) the
hash function is defined as:
h(?(x)) = sign(
P
?
i=1
?i ? ?(x,xi)) (3)
where ?(?, ?) can be any kernel function.
KLSH can improve hashing results via the kernel
trick. However, KLSH is unsupervised, thus design-
ing a data-specific kernel remains a big challenge.
2.3 Semi-Supervised Hashing
Semi-Supervised Hashing (SSH) (Wang et al,
2010a) is recently proposed to incorporate prior
knowledge for better hashing. Besides X , prior
knowledge in the form of similar and dissimilar
object-pairs is also required in SSH. SSH tries to
find L optimal hash functions which have maximum
94
empirical accuracy on prior knowledge and maxi-
mum entropy by finding the top L eigenvectors of
an extended covariance matrix2 via PCA or SVD.
However, despite of the potential problems of nu-
merical stability, SVD requires massive computa-
tional space and O(M3) computational time where
M is feature dimension, which limits its usage for
high-dimensional data (Trefethen et al, 1997). Fur-
thermore, the variance of directions obtained by
PCA decreases with the decrease of the rank (Jol-
liffe, 1986). Thus, lower hash functions tend to have
smaller entropy and larger empirical errors.
2.4 Others
Some other related works should be mentioned. A
notable method is Locality Sensitive Hashing (LSH)
(Indyk et al, 1998). LSH performs a random
linear projection to map similar objects to similar
hash codes. However, LSH suffers from the effi-
ciency problem that it tends to generate long codes
(Salakhutdinov et al, 2007). LAMP (Mu et al,
2009) considers each hash function as a binary par-
tition problem as in SVMs (Burges, 1998). Spec-
tral Hashing (Weiss et al, 2009) maintains similar-
ity between objects in the reduced Hamming space
by minimizing the averaged Hamming distance3 be-
tween similar neighbors in the original Euclidean
space. However, spectral hashing takes the assump-
tion that data should be distributed uniformly, which
is always violated in real-world applications.
3 Semi-Supervised SimHash
In this section, we present our hashing method,
named Semi-Supervised SimHash (S3H). Let XL =
{(x1, c1) . . . (xu, cu)} be the labeled data, c ?
{1 . . . C}, x ? RM , and XU = {xu+1 . . .xN} the
unlabeled data. Let X = XL ? XU . Given the
labeled data XL, we construct two sets, attraction
set ?a and repulsion set ?r. Specifically, any pair
(xi,xj) ? ?a, i, j ? u, denotes that xi and xj
are in the same class, i.e., ci = cj , while any pair
(xi,xj) ? ?r, i, j ? u, denotes that ci ?= cj . Unlike
2The extended covariance matrix is composed of two com-
ponents, one is an unsupervised covariance term and another is
a constraint matrix involving labeled information.
3Hamming distance is defined as the number of bits that are
different between two binary strings.
previews works that attempt to find L optimal hyper-
planes, the basic idea of S3H is to fix L random hy-
perplanes and to find an optimal feature-weight vec-
tor to relocate the objects such that similar objects
have similar codes.
3.1 Data Representation
Since L random hyperplanes are fixed, we can rep-
resent a object x ? X as its relative position to these
random hyperplanes, i.e.,
D = ? ?V (4)
where the element Vml ? {+1,?1, 0} of V indi-
cates that the object x is above, below or just in the
l-th hyperplane with respect to them-th feature, and
? = diag(|x1|, |x2|, . . . , |xM |) is a diagonal matrix
which, to some extent, reflects the distance from x
to these hyperplanes.
3.2 Formulation
Hashing maps the data set X to an L-dimensional
Hamming space for compact representations. If we
represent each object as Equation (4), the l-th hash
function is then defined as:
hl(x) = ~l(D) = sign(wTdl) (5)
where w ? RM is the feature weight to be deter-
mined and dl is the l-th column of the matrixD.
Intuitively, the ?contribution? of a specific feature
to different classes is different. Therefore, we hope
to incorporate this side information in S3H for better
hashing. Inspired by (Madani et al, 2009), we can
measure this contribution overXL as in Algorithm 1.
Clearly, if objects are represented as the occurrence
numbers of features, the output of Algorithm 1 is
just the conditional probability Pr(class|feature).
Finally, each object (x, c) ? XL can be represented
as anM ? L matrixG:
G = diag(?1,c, ?2,c, . . . , ?M,c) ?D (6)
Note that, one pair (xi,xj) in ?a or ?r corresponds
to (Gi,Gj) while (Di,Dj) if we ignore features?
contribution to different classes.
Furthermore, we also hope to maximize the em-
pirical accuracy on the labeled data ?a and ?r and
95
Algorithm 1: Feature Contribution Calculation
for each (x, c) ? XL do
for each f ? x do
?f ? ?f + xf ;
?f,c ? ?f,c + xf ;
end
end
for each feature f and class c do
?f,c ?
?f,c
?f
;
end
maximize the entropy of hash functions. So, we de-
fine the following objective for ~(?)s:
J(w) = 1
Np
L
?
l=1
{
?
(xi,xj)??a
~l(xi)~l(xj)
?
?
(xi,xj)??r
~l(xi)~l(xj)
}
+ ?1
L
?
l=1
H(~l)
(7)
where Np = |?a| + |?r| is the number of attrac-
tion and repulsion pairs and ?1 is a tradeoff between
two terms. Wang et al have proven that hash func-
tions with maximum entropy must maximize the
variance of the hash values, and vice-versa (Wang
et al, 2010b). Thus, H(~(?)) can be estimated over
the labeled and unlabeled data, XL and XU .
Unfortunately, direct solution for above problem
is non-trivial since Equation (7) is not differentiable.
Thus, we relax the objective and add an additional
regularization term which could effectively avoid
overfitting. Finally, we obtain the total objective:
L(w) = 1
Np
L
?
l=1
{
?
(Gi,Gj)??a
?(wTgi,l)?(wTgj,l)
?
?
(Gi,Gj)??r
?(wTgi,l)?(wTgj,l)}
+ ?1
2N
L
?
l=1
{
u
?
i=1
?2(wTgi,l) +
N
?
i=u+1
?2(wTdi,l)}
? ?2
2
?w?22
(8)
where gi,l and di,l denote the l-th column ofGi and
Di respectively, and ?(t) is a piece-wise linear func-
tion defined as:
?(t) =
?
?
?
Tg t > Tg
t ?Tg ? t ? Tg
?Tg t < ?Tg
(9)
This relaxation has a good intuitive explanation.
That is, similar objects are desired to not only have
the similar fingerprints but also have sufficient large
projection magnitudes, while dissimilar objects are
desired to not only differ in their fingerprints but also
have large projection margin. However, we do not
hope that a small fraction of object-pairs with very
large projection magnitude or margin dominate the
complete model. Thus, a piece-wise linear function
?(?) is applied in S3H.
As a result, Equation (8) is a simply uncon-
strained optimization problem, which can be ef-
ficiently solved by a notable Quasi-Newton algo-
rithm, i.e., L-BFGS (Liu et al, 1989). For descrip-
tion simplicity, only attraction set ?a is considered
and the extension to repulsion set ?r is straightfor-
ward. Thus, the gradient of L(w) is as follows:
?L(w)
?w =
1
Np
L
?
l=1
{
?
(Gi,Gj) ? ?a,
|wT gi,l| ? Tg
?(wTgj,l) ? gi,l
+
?
(Gi,Gj) ? ?a,
|wT gj,l| ? Tg
?(wTgi,l) ? gj,l
}
(10)
+ ?1
N
L
?
l=1
{ u
?
i = 1,
|wT gi,l| ? Tg
?(wTgi,l) ? gi,l
+
N
?
i = u + 1,
|wTdi,l| ? Tg
?(wTdi,l) ? di,l
}
? ?2w
Note that ??(t)/?t = 0 when |t| > Tg.
3.3 Fingerprint Generation
When we get the optimal weight w?, we generate
fingerprints for given objects through Equation (5).
Then, it tunes to the problem how to efficiently ob-
tain the representation as in Figure 4 for a object.
After analysis, we find: 1) hyperplanes are randomly
generated and we only need to determine which
sides of these hyperplanes the given object lies on,
and 2) in real-world applications, objects such as
docs are always very sparse. Thus, we can avoid
heavy computational demands and efficiently gener-
ate fingerprints for objects.
In practice, given an object x, the procedure of
generating anL-bit fingerprint is as follows: it main-
tains an L-dimensional vector initialized to zero.
Each feature f ? x is firstly mapped to an L-bit
hash value by Jenkins Hashing Function4. Then,
4http://www.burtleburtle.net/bob/hash/doobs.html
96
Algorithm 2: Fast Fingerprint Generation
INPUT: x and w?;
initialize ?? 0,? ? 0, ?,? ? RL;
for each f ? x do
randomly project f to hf ? {?1,+1}L;
?? ?+ xf ? w?f ? hf ;
end
for l = 1 to L do
if ?l > 0 then
?l ? 1;
end
end
RETURN ?;
these L bits increment or decrement the L compo-
nents of the vector by the value xf ? w?f . After all
features processed, the signs of components deter-
mine the corresponding bits of the final fingerprint.
The complete algorithm is presented in Algorithm 2.
3.4 Algorithmic Analysis
This section briefly analyzes the relation between
S3H and some existing methods. For analysis sim-
plicity, we assume ?(t) = t and ignore the regular-
ization terms. So, Equation (8) can be rewritten as
follows:
J(w)S3H =
1
2
wT [
L
?
l=1
?l(?+ ???)?Tl ]w (11)
where ?+ij equals to 1 when (xi,xj) ? ?a otherwise
0, ??ij equals to 1 when (xi,xj) ? ?r otherwise
0, and ?l = [g1,l . . .gu,l,du+1,l . . .dN,l]. We de-
note
?
l ?l?+?Tl and
?
l ?l???Tl as S+ and S?
respectively. Therefore, maximizing above function
is equivalent to maximizing the following:
?J(w)S3H =
|wTS+w|
|wTS?w|
(12)
Clearly, Equation (12) is analogous to Linear Dis-
criminant Analysis (LDA) (Duda et al, 2000) ex-
cept for the difference: 1) measurement. S3H uses
similarity while LDA uses distance. As a result, the
objective function of S3H is just the reciprocal of
LDA?s. 2) embedding space. LDA seeks the best
separative direction in the original attribute space. In
contrast, S3H firstly maps data from RM to RM?L
through the following projection function
?(x) = x ? [diag(sign(r1)), . . . ,diag(sign(rL))] (13)
where rl ? RM , l = 1, . . . , L, are L random hyper-
planes. Then, in that space (RM?L), S3H seeks a
direction5 that can best separate the data.
From this point of view, it is obvious that the basic
SH is a special case of S3H when w is set to e =
[1, 1, . . . , 1]. That is, SH firstly maps the data via
?(?) just as S3H. But then, SH directly separates the
data in that feature space at the direction e.
Analogously, we ignore the regularization terms
in SSH and rewrite the objective of SSH as:
J(W)SSH =
1
2
tr[WTX(?+ ???)XTW] (14)
where W = [w1, . . . ,wL] ? RM?L are L hyper-
planes and X = [x1, . . . ,xN ]. Maximizing this ob-
jective is equivalent to maximizing the following:
?J(W)SSH =
| tr[WTS?+W]|
| tr[WTS??W]|
(15)
where S?+ = X?+XT and S?? = X??XT . Equa-
tion (15) shows that SSH is analogous to Multiple
Discriminant Analysis (MDA) (Duda et al, 2000).
In fact, SSH uses top L best-separative hyperplanes
in the original attribute space found via PCA to hash
the data. Furthermore, we rewrite the projection
function ?(?) in S3H as:
?(x) = x ? [R1, . . . ,RL] (16)
where Rl = diag(sign(rl)). Each Rl is a mapping
from RM to RM and corresponds to one embedding
space. From this perspective, unlike SSH, S3H glob-
ally seeks a direction that can best separate the data
in L different embedding spaces simultaneously.
4 Experiments
We use two datasets 20 Newsgroups and Open Di-
rectory Project (ODP) in our experiments. Each doc-
ument is represented as a vector of occurrence num-
bers of the terms within it. The class information
of docs is considered as prior knowledge that two
docs within a same class should have more similar
fingerprints while two docs within different classes
should have dissimilar fingerprints. We will demon-
strate that our S3H can effectively incorporate this
prior knowledge to improve the DSS performance.
5The direction is determined by concatenating w L times.
97
24 32 40 48 56 64
0.1
0.2
0.3
0.4
0.5
M
e
a
n
 
A
v
e
r
a
g
e
d
 
P
r
e
c
i
s
i
o
n
 
(
M
A
P
)
Number of bits
 S
3
H
 S
3
H
f
 SSH
 SH
 KLSH
(a)
24 32 40 48 56 64
0.1
0.2
0.3
0.4
0.5
M
e
a
n
 
A
v
e
r
a
g
e
d
 
P
r
e
c
i
s
i
o
n
 
(
M
A
P
)
Number of bits
 S
3
H
 S
3
H
f
 SSH
 SH
 KLSH
(b)
Figure 1: Mean Averaged Precision (MAP) for different
number of bits for hash ranking on 20 Newsgroups. (a)
10K features. (b) 30K features.
We use Inverted List (IL) (Manning et al, 2002)
as the baseline. In fact, given a query doc, IL re-
turns all the docs that contain any term within it.
We also compare our method with three state-of-
the-art hashing methods, i.e., KLSH, SSH and SH.
In KLSH, we adopt the RBF kernel ?(xi,xj) =
exp(??xi?xj?
2
2
?2 ), where the scaling factor ?
2 takes
0.5 and the other two parameters p and t are set to
be 500 and 50 respectively. The parameter ? in SSH
is set to 1. For S3H, we simply set the parameters ?1
and ?2 in Equation (8) to 4 and 0.5 respectively. To
objectively reflect the performance of S3H, we eval-
uate our S3H with and without Feature Contribution
Calculation algorithm (FCC) (Algorithm 1). Specif-
ically, FCC-free S3H (denoted as S3Hf ) is just a
simplification whenGs in S3H are simply set toDs.
For quantitative evaluation, as in literature (Wang
et al, 2010b; Mu et al, 2009), we calculate the pre-
cision under two scenarios: hash lookup and hash
ranking. For hash lookup, the proportion of good
neighbors (have the same class label as the query)
among the searched objects within a given Hamming
radius is calculated as precision. Similarly to (Wang
et al, 2010b; Weiss et al, 2009), for a query doc-
ument, if no neighbors within the given Hamming
radius can be found, it is considered as zero preci-
sion. Note that, the precision of IL is the propor-
tion of good neighbors among the whole searched
objects. For hash ranking, all the objects in X are
ranked in terms of their Hamming distance from the
query document, and the top K nearest neighbors
are returned as the result. Then, Mean Averaged Pre-
cision (MAP) (Manning et al, 2002) is calculated.
We also calculate the averaged intra- and inter- class
Hamming distance for various hashing methods. In-
24 32 40 48 56 64
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
P
r
e
c
i
s
i
o
n
 
w
i
t
h
i
n
 
H
a
m
m
i
n
g
 
r
a
d
i
u
s
 
3
Number of bits
 S
3
H
 S
3
H
f
 SSH
 SH
 KLSH
 IL
(a)
24 32 40 48 56 64
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
P
r
e
c
i
s
i
o
n
 
w
i
t
h
i
n
 
H
a
m
m
i
n
g
 
r
a
d
i
u
s
 
3
Number of bits
 S
3
H
 S
3
H
f
 SSH
 SH
 KLSH
 IL
(b)
Figure 2: Precision within Hamming radius 3 for hash
lookup on 20 Newsgroups. (a) 10K features. (b) 30K
features.
tuitively, a good hashing method should have small
intra-class distance while large inter-class distance.
We test all the methods on a PC with a 2.66 GHz
processor and 12GB RAM. All experiments repeate
10 times and the averaged results are reported.
4.1 20 Newsgroups
20 Newsgroups6 contains 20K messages, about 1K
messages from each of 20 different newsgroups.
The entire vocabulary includes 62,061 words. To
evaluate the performance for different feature di-
mensions, we use Chi-squared feature selection al-
gorithm (Forman, 2003) to select 10K and 30K fea-
tures. The averaged message length is 54.1 for 10K
features and 116.2 for 30K features. We randomly
select 4K massages as the test set and the remain
16K as the training set. To train SSH and S3H,
from the training set, we randomly generate 40K
message-pairs as ?a and 80K message-pairs as ?r.
For hash ranking, Figure 1 shows MAP for vari-
ous methods using different number of bits. It shows
that performance of SSH decreases with the grow-
ing of hash bits. This is mainly because the variance
of the directions obtained by PCA decreases with
the decrease of their ranks. Thus, lower bits have
larger empirical errors. For S3H, FCC (Algorithm 1)
can significantly improve the MAP just as discussed
in Section 3.2. Moreover, the MAP of FCC-free
S3H (S3Hf ) is affected by feature dimensions while
FCC-based (S3H) is relatively stable. This implies
FCC can also improve the satiability of S3H. As we
see, S3Hf ignores the contribution of features to dif-
ferent classes. However, besides the local descrip-
tion of data locality in the form of object-pairs, such
6http://www.cs.cmu.edu/afs/cs/project/theo-3/www/
98
 S
3
H
 S
3
H
f
 SSH
 SH
 KLSH
 IL
24 32 40 48 56 64
10
-1
10
0
10
1
10
2
10
3
10
4
N
u
m
b
e
r
 
o
f
 
s
e
a
r
c
h
e
d
 
d
a
t
a
Number of bits
(a)
 S
3
H
 S
3
H
f
 SSH
 SH
 KLSH
 IL
24 32 40 48 56 64
10
-1
10
0
10
1
10
2
10
3
10
4
N
u
m
b
e
r
 
o
f
 
s
e
a
r
c
h
e
d
 
d
a
t
a
Number of bits
(b)
Figure 3: Averaged searched sample numbers using 4K
query messages for hash lookup. (a) 10K features. (b)
30K features.
(global) information also provides a proper guidance
for hashing. So, for S3Hf , the reason why its re-
sults with 30K features are worse than the results
with 10K features is probably because S3Hf learns
to hash only according to the local description of
data locality and many not too relevant features lead
to relatively poor description. In contrast, S3H can
utilize global information to better understand the
similarity among objects. In short, S3H obtains the
best MAP for all bits and feature dimensions.
For hash lookup, Figure 2 presents the precision
within Hamming radius 3 for different number of
bits. It shows that IL even outperforms SH. This
is because few objects can be hashed by SH into one
hash bucket. Thus, for many queries, SH fails to
return any neighbor even in a large Hamming radius
of 3. Clearly, S3H outperforms all the other methods
for different number of hash bits and features.
The number of messages searched by different
methods are reported in Figure 3. We find that the
number of searched data of S3H (with/without FCC)
decreases much more slowly than KLSH, SH and
SSHwith the growing of the number of hash bits. As
discussed in Section 3.4, this mainly benefits from
the design of S3H that S3H (globally) seeks a di-
rection that can best separate the data in L embed-
ding spaces simultaneously. We also find IL returns
a large number of neighbors of each query message
which leads to its poor efficiency.
The averaged intra- and inter- class Hamming dis-
tance of different methods are reported in Table 1.
As it shows, S3H has relatively larger margin (?)
between intra- and inter-class Hamming distance.
This indicates that S3H is more effective to make
similar points have similar fingerprints while keep
intra-class inter-class ?
S3H 13.1264 15.6342 2.5078
S3Hf 12.5754 13.3479 0.7725
SSH 6.4134 6.5262 0.1128
SH 15.3908 15.6339 0.2431
KLSH 10.2876 10.8713 0.5841
Table 1: Averaged intra- and inter- class Hamming dis-
tance of 20 Newsgroups for 32-bit fingerprint. ? is the
difference between the averaged inter- and intra- class
Hamming distance. Large ? implies good hashing.
10 20 30 40
10
0
10
1
10
2
10
3
T
i
m
e
 
(
s
e
c
.
)
Feature dimension (K)
 S
3
H
 SSH
 SH
 KLSH
 IL
(a)
10 20 30 40
10
1
10
2
10
3
10
4
S
p
a
c
e
 
(
M
B
)
 
Feature dimension (K)
 S
3
H
 SSH
 SH
 KLSH
 IL
(b)
Figure 4: Computational complexity of training for dif-
ferent feature dimensions for 32-bit fingerprint. (a) Train-
ing time (sec). (b) Training space cost (MB).
the dissimilar points away enough from each other.
Figure 4 shows the (training) computational com-
plexity of different methods. We find that the time
and space cost of SSH grows much faster than SH,
KLSH and S3H with the growing of feature dimen-
sion. This is mainly because SSH requires SVD to
find the optimal hashing functions which is compu-
tational expensive. Instead, S3H seeks the optimal
feature weights via L-BFGS, which is still efficient
even for very high-dimensional data.
4.2 Open Directory Project (ODP)
Open Directory Project (ODP)7 is a multilingual
open content directory of web links (docs) organized
by a hierarchical ontology scheme. In our exper-
iment, only English docs8 at level 3 of the cate-
gory tree are utilized to evaluate the performance.
In short, the dataset contains 2,483,388 docs within
6,008 classes. There are totally 862,050 distinct
words and each doc contains 14.13 terms on aver-
age. Since docs are too short, we do not conduct
7http://rdf.dmoz.org/
8The title together with the corresponding short description
of a page are considered as a document in our experiments.
99
1 10 100 1k 10k 100k
0.00
0.01
0.02
0.03
0.04
P
e
r
c
e
n
t
a
g
e
Class size
(a)
0 20 40 60 80 100 120
0.00
0.02
0.04
0.06
0.08
0.10
P
e
r
c
e
n
t
a
g
e
Document length
(b)
Figure 5: Overview of ODP data set. (a) Class distribu-
tion at level 3. (b) Distribution of document length.
intra-class inter-class ?
S3H 14.0029 15.9508 1.9479
S3Hf 14.3801 15.5260 1.1459
SH 14.7725 15.6432 0.8707
KLSH 9.3382 10.5700 1.2328
Table 2: Averaged intra- and inter- class Hamming dis-
tance of ODP for 32-bit fingerprint (860K features). ?
is the difference between averaged intra- and inter- class
Hamming distance.
feature selection9. An overview of ODP is shown in
Figure 5. We randomly sample 10% docs as the test
set and the remain as the training set. Furthermore,
from training set, we randomly generate 800K doc-
pairs as ?a, and 1 million doc-pairs as ?r. Note
that, since there are totally over 800K features, it
is extremely inefficient to train SSH. Therefore, we
only compare our S3H with IL, KLSH and SH.
The search performance is given in Figure 6. Fig-
ure 6(a) shows the MAP for various methods using
different number of bits. It shows KLSH outper-
forms SH, which mainly contributes to the kernel
trick. S3H and S3Hf have higher MAP than KLSH
and SH. Clearly, FCC algorithm can improve the
MAP of S3H for all bits. Figure 6(b) presents the
precision within Hamming radius 2 for hash lookup.
We find that IL outperforms SH since SH fails for
many queries. It also shows that S3H (with FCC)
can obtain the best precision for all bits.
Table 2 reports the averaged intra- and inter-class
Hamming distance for various methods. It shows
that S3H has the largest margin (?). This demon-
9We have tested feature selection. However, if we select
40K features via Chi-squared feature selection method, docu-
ments are represented by 3.15 terms on average. About 44.9%
documents are represented by no more than 2 terms.
24 32 40 48 56 64
0.15
0.20
0.25
0.30
0.35
M
e
a
n
 
A
v
e
r
a
g
e
d
 
P
r
e
c
i
s
i
o
n
 
(
M
A
P
)
Number of bits
 S
3
H
 S
3
H
f
 SH
 KLSH
(a)
24 32 40 48 56 64
0.03
0.06
0.09
0.12
0.15
0.18
P
r
e
c
i
s
i
o
n
 
w
i
t
h
i
n
 
H
a
m
m
i
n
g
 
r
a
d
i
u
s
 
2
Number of bits
 S
3
H
 S
3
H
f
 SH
 KLSH
 IL
(b)
Figure 6: Retrieval performance of different methods on
ODP. (a) Mean Averaged Precision (MAP) for different
number of bits for hash ranking. (b) Precision within
Hamming radius 2 for hash lookup.
strates S3H can measure the similarity among the
data better than KLSH and SH.
We should emphasize that KLSH needs 0.3ms
to return the results for a query document for hash
lookup, and S3H needs <0.1ms. In contrast, IL re-
quires about 75ms to finish searching. This is mainly
because IL always returns a large number of ob-
jects (dozens or hundreds times more than S3H and
KLSH) and requires much time for post-processing.
All the experiments show S3H is more effective,
efficient and stable than the baseline method and the
state-of-the-art hashing methods.
5 Conclusions
We have proposed a novel supervised hashing
method named Semi-Supervised Simhash (S3H) for
high-dimensional data similarity search. S3H learns
the optimal feature weights from prior knowledge
to relocate the data such that similar objects have
similar fingerprints. This is implemented by max-
imizing the empirical accuracy on labeled data to-
gether with the entropy of hash functions. The
proposed method leads to a simple Quasi-Newton
based solution which is efficient even for very high-
dimensional data. Experiments performed on two
large datasets have shown that S3H has better search
performance than several state-of-the-art methods.
6 Acknowledgements
We thank Fangtao Li for his insightful suggestions.
We would also like to thank the anonymous review-
ers for their helpful comments. This work is sup-
ported by the National Natural Science Foundation
of China under Grant No. 60873174.
100
References
Christopher J.C. Burges. 1998. A tutorial on support
vector machines for pattern recognition. Data Mining
and Knowledge Discovery, 2(2):121-167.
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings
of the 34th annual ACM symposium on Theory of
computing, pages 380-388.
Gianni Costa, Giuseppe Manco and Riccardo Ortale.
2010. An incremental clustering scheme for data de-
duplication. Data Mining and Knowledge Discovery,
20(1):152-187.
Jeffrey Dean and Monika R. Henzinge. 1999. Finding
Related Pages in the World Wide Web. Computer
Networks, 31:1467-1479.
Richard O. Duda, Peter E. Hart and David G. Stork.
2000. Pattern classification, 2nd edition. Wiley-
Interscience.
George Forman 2003. An extensive empirical study of
feature selection metrics for text classification. The
Journal of Machine Learning Research, 3:1289-1305.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the 30th annual
ACM symposium on Theory of computing, pages
604-613.
Ian Jolliffe. 1986. Principal Component Analysis.
Springer-Verlag, New York.
Yan Ke, Rahul Sukthankar and Larry Huston. 2004.
Efficient near-duplicate detection and sub-image
retrieval. In Proceedings of the ACM International
Conference on Multimedia.
Brian Kulis and Kristen Grauman. 2009. Kernelized
locality-sensitive hashing for scalable image search.
In Proceedings of the 12th International Conference
on Computer Vision, pages 2130-2137.
Brian Kulis, Prateek Jain and Kristen Grauman. 2009.
Fast similarity search for learned metrics. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, pages 2143-2157.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1): 503-528.
Omid Madani, Michael Connor and Wiley Greiner.
2009. Learning when concepts abound. The Journal
of Machine Learning Research, 10:2571-2613.
Gurmeet Singh Manku, Arvind Jain and Anish Das
Sarma. 2007. Detecting near-duplicates for web
crawling. In Proceedings of the 16th international
conference on World Wide Web, pages 141-150.
Christopher D. Manning, Prabhakar Raghavan and Hin-
rich Schu?tze. 2002. An introduction to information
retrieval. Spring.
Yadong Mu, Jialie Shen and Shuicheng Yan. 2010.
Weakly-Supervised Hashing in Kernel Space. In Pro-
ceedings of International Conference on Computer
Vision and Pattern Recognition, pages 3344-3351.
Ruslan Salakhutdinov and Geoffrey Hintona. 2007.
Semantic hashing. In SIGIR workshop on Information
Retrieval and applications of Graphical Models.
Bernhard Scho?lkopf, Alexander Smola and Klaus-Robert
Mu?ller. 1997. Kernel principal component analysis.
Advances in Kernel Methods - Support Vector Learn-
ing, pages 583-588. MIT.
Lloyd N. Trefethen and David Bau. 1997. Numerical
linear algebra. Society for Industrial Mathematics.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2008.
Towards a unified approach to document similarity
search using manifold-ranking of blocks. Information
Processing & Management, 44(3):1032-1048.
Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010a.
Semi-Supervised Hashing for Scalable Image Re-
trieval. In Proceedings of International Conference
on Computer Vision and Pattern Recognition, pages
3424-3431.
Jun Wang, Sanjiv Kumar and Shih-Fu Chang. 2010b.
Sequential Projection Learning for Hashing with
Compact Codes. In Proceedings of International
Conference on Machine Learning.
Yair Weiss, Antonio Torralba and Rob Fergus. 2009.
Spectral hashing. In Proceedings of Advances in Neu-
ral Information Processing Systems.
101
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 485?490,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Why Press Backspace? Understanding User Input Behaviors in Chinese
Pinyin Input Method
Yabin Zheng1, Lixing Xie1, Zhiyuan Liu1, Maosong Sun1, Yang Zhang2, Liyun Ru1,2
1State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
2Sogou Inc., Beijing 100084, China
{yabin.zheng,lavender087,lzy.thu,sunmaosong}@gmail.com
{zhangyang,ruliyun}@sogou-inc.com
Abstract
Chinese Pinyin input method is very impor-
tant for Chinese language information pro-
cessing. Users may make errors when they
are typing in Chinese words. In this paper, we
are concerned with the reasons that cause the
errors. Inspired by the observation that press-
ing backspace is one of the most common us-
er behaviors to modify the errors, we collect
54, 309, 334 error-correction pairs from a real-
world data set that contains 2, 277, 786 user-
s via backspace operations. In addition, we
present a comparative analysis of the data to
achieve a better understanding of users? input
behaviors. Comparisons with English typos
suggest that some language-specific properties
result in a part of Chinese input errors.
1 Introduction
Unlike western languages, Chinese is unique due
to its logographic writing system. Chinese users
cannot directly type in Chinese words using a QW-
ERTY keyboard. Pinyin is the official system to
transcribe Chinese characters into the Latin alpha-
bet. Based on this transcription system, Pinyin input
methods have been proposed to assist users to type
in Chinese words (Chen, 1997).
The typical way to type in Chinese words is
in a sequential manner (Wang et al, 2001). As-
sume users want to type in the Chinese word ??
?(what)?. First, they mentally generate and type
in corresponding Pinyin ?shenme?. Then, a Chinese
Pinyin input method displays a list of Chinese words
which share that Pinyin, as shown in Fig. 1. Users
Figure 1: Typical Chinese Pinyin input method for a
correct Pinyin (Sogou-Pinyin).
Figure 2: Typical Chinese Pinyin input method for a
mistyped Pinyin (Sogou-Pinyin).
visually search the target word from candidates and
select numeric key ?1? to get the result. The last t-
wo steps do not exist in typing process of English
words, which indicates that it is more complicated
for Chinese users to type in Chinese words.
Chinese users may make errors when they are typ-
ing in Chinese words. As shown in Fig. 2, a user
may mistype ?shenme? as ?shenem?. Typical Chi-
nese Pinyin input method can not return the right
word. Users may not realize that an error occurs and
select the first candidate word ????? (a mean-
ingless word) as the result. This greatly limits us-
er experience since users have to identify errors and
modify them, or cannot get the right word.
In this paper, we analyze the reasons that cause
errors in Chinese Pinyin input method. This analy-
sis is helpful in enhancing the user experience and
the performance of Chinese Pinyin input method. In
practice, users press backspace on the keyboard to
modify the errors, they delete the mistyped word and
re-type in the correct word. Motivated by this ob-
485
servation, we can extract error-correction pairs from
backspace operations. These error-correction pairs
are of great importance in Chinese spelling correc-
tion task which generally relies on sets of confusing
words.
We extract 54, 309, 334 error-correction pairs
from user input behaviors and further study them.
Our comparative analysis of Chinese and English ty-
pos suggests that some language-specific properties
of Chinese lead to a part of input errors. To the best
of our knowledge, this paper is the first one which
analyzes user input behaviors in Chinese Pinyin in-
put method.
The rest of this paper is organized as follows.
Section 2 discusses related works. Section 3 intro-
duces how we collect errors in Chinese Pinyin input
method. In Section 4, we investigate the reasons that
result in these errors. Section 5 concludes the whole
paper and discusses future work.
2 Previous Work
For English spelling correction (Kukich, 1992;
Ahmad and Kondrak, 2005; Chen et al, 2007;
Whitelaw et al, 2009; Gao et al, 2010), most ap-
proaches make use of a lexicon which contains a list
of well-spelled words (Hirst and Budanitsky, 2005;
Islam and Inkpen, 2009). Context features (Ro-
zovskaya and Roth, 2010) of words provide useful
evidences for spelling correction. These features
are usually represented by an n-gram language mod-
el (Cucerzan and Brill, 2004; Wilcox-O?Hearn et
al., 2010). Phonetic features (Toutanova and Moore,
2002; Atkinson, 2008) are proved to be useful in En-
glish spelling correction. A spelling correction sys-
tem is trained using these features by a noisy channel
model (Kernighan et al, 1990; Ristad et al, 1998;
Brill and Moore, 2000).
Chang (1994) first proposes a representative ap-
proach for Chinese spelling correction, which re-
lies on sets of confusing characters. Zhang et al
(2000) propose an approximate word-matching al-
gorithm for Chinese to solve Chinese spell detec-
tion and correction task. Zhang et al (1999) present
a winnow-based approach for Chinese spelling cor-
rection which takes both local language features and
wide-scope semantic features into account. Lin and
Yu (2004) use Chinese frequent strings and report
an accuracy of 87.32%. Liu et al (2009) show that
about 80% of the errors are related to pronunciation-
s. Visual and phonological features are used in Chi-
nese spelling correction (Liu et al, 2010).
Instead of proposing a method for spelling cor-
rection, we mainly investigate the reasons that cause
typing errors in both English and Chinese. Some
errors are caused by specific properties in Chinese
such as the phonetic difference between Mandarin
and dialects spoken in southern China. Meanwhile,
confusion sets of Chinese words play an importan-
t role in Chinese spelling correction. We extract a
large scale of error-correction pairs from real user
input behaviors. These pairs contain important ev-
idence about confusing Pinyins and Chinese words
which are helpful in Chinese spelling correction.
3 User Input Behaviors Analysis
We analyze user input behaviors from anonymous
user typing records in a Chinese input method. Data
set used in this paper is extracted from Sogou Chi-
nese Pinyin input method1. It contains 2, 277, 786
users? typing records in 15 days. The numbers of
Chinese words and characters are 3, 042, 637, 537
and 5, 083, 231, 392, respectively. We show some
user typing records in Fig. 3.
[20100718 11:10:38.790ms] select:2 zhe? WINWORD.exe 
[20100718 11:10:39.770ms] select:1 shi? WINWORD.exe 
[20100718 11:10:40.950ms] select:1 shenem??? WINWORD.exe 
[20100718 11:10:42.300ms] Backspace WINWORD.exe 
[20100718 11:10:42.520ms] Backspace WINWORD.exe 
[20100718 11:10:42.800ms] Backspace WINWORD.exe 
[20100718 11:10:45.090ms] select:1 shenme ?? WINWORD.exe 
 
Figure 3: Backspace in user typing records.
From Fig. 3, we can see the typing process of a
Chinese sentence ?????? (What is this). Each
line represents an input segment or a backspace op-
eration. For example, word ???? (what) is type-
d in using Pinyin ?shenme? with numeric selection
?1? at 11:10am in Microsoft Word application.
The user made a mistake to type in the third
Pinyin (?shenme? is mistyped as ?shenem?). Then,
he/she pressed the backspace to modify the errors
he has made. the word ????? is deleted and re-
placed with the correct word ???? using Pinyin
1Sogou Chinese Pinyin input method, can be accessed from
http://pinyin.sogou.com/
486
?shenme?. As a result, we compare the typed-
in Pinyins before and after backspace operations.
We can find the Pinyin-correction pairs ?shenem-
shenme?, since their edit distance is less than a
threshold. Threshold is set to 2 in this paper, as
Damerau (1964) shows that about 80% of typos are
caused by a single edit operation. Therefore, using a
threshold of 2, we should be able to find most of the
typos. Furthermore, we can extract corresponding
Chinese word-correction pairs ????-??? from
this typing record.
Using heuristic rules discussed above, we extrac-
t 54, 309, 334 Pinyin-correction and Chinese word-
correction pairs. We list some examples of extracted
Pinyin-correction and Chinese word-correction pairs
in Table 1. Most of the mistyped Chinese words are
meaningless.
Pinyin-correction Chinese word-correction
shenem-shenme ???-??(what)
dianao-diannao ??-??(computer)
xieixe-xiexie ????-??(thanks)
laing-liang ???-?(two)
ganam-ganma ???-??(what?s up)
zhdiao-zhidao ??-??(know)
lainxi-lianxi ???-??(contact)
zneme-zenme ???-??(how)
dainhua-dianhua ???-??(phone)
huiali-huilai ???-??(return)
Table 1: Typical Pinyin-correction and Chinese
word-correction pairs.
We want to evaluate the precision and recall of
our extraction method. For precision aspect, we ran-
domly select 1, 000 pairs and ask five native speak-
ers to annotate them as correct or wrong. Annota-
tion results show that the precision of our method is
about 75.8%. Some correct Pinyins are labeled as
errors because we only take edit distance into con-
sideration. We should consider context features as
well, which will be left as our future work.
We choose 15 typical mistyped Pinyins to evalu-
ate the recall of our method. The total occurrences
of these mistyped Pinyins are 259, 051. We success-
fully retrieve 144, 020 of them, which indicates the
recall of our method is about 55.6%. Some errors
are not found because sometimes users do not modi-
fy the errors, especially when they are using Chinese
input method under instant messenger softwares.
4 Comparisons of Pinyin typos and
English Typos
In this section, we compare the Pinyin typos and En-
glish typos. As shown in (Cooper, 1983), typing er-
rors can be classified into four categories: deletions,
insertions, substitutions, and transpositions. We aim
at studying the reasons that result in these four kinds
of typing errors in Chinese Pinyin and English, re-
spectively.
For English typos, we generate mistyped word-
correction pairs from Wikipedia2 and SpellGood.3,
which contain 4, 206 and 10, 084 common mis-
spellings in English, respectively. As shown in Ta-
ble 2, we reach the first conclusion: about half
of the typing errors in Pinyin and English are
caused by deletions, which indicates that users are
more possible to omit some letters than other three
edit operations.
Deletions Insertions Substitutions Transpositions
Pinyin 47.06% 28.17% 19.04% 7.46%
English 43.38% 18.89% 17.32% 18.70%
Table 2: Different errors in Pinyin and English.
Table 3 and Table 4 list Top 5 letters that produce
deletion errors (users forget to type in some letters)
and insertion errors (users type in extra letters) in
Pinyin and English.
Pinyin Examples English Examples
i xianza-xianzai e achive-achieve
g yingai-yinggai i abilties-abilities
e shenm-shenme c acomplish-accomplish
u pengyo-pengyou a agin-again
h senme-shenme t admited-admitted
Table 3: Deletion errors in Pinyin and English.
Pinyin Examples English Examples
g yingwei-yinwei e analogeous-analogous
i tiebie-tebie r arround-around
a xiahuan-xihuan s asside-aside
o huijiao-huijia i aisian-asian
h shuibian-suibian n abandonned-abandoned
Table 4: Insertion errors in Pinyin and English.
2http://en.wikipedia.org/wiki/Wikipedia:
Lists_of_common_misspellings/For_machines
3http://www.spellgood.net/
487
We can see from Table 3 and Table 4 that: (1)
vowels (a, o, e, i, u) are deleted or inserted more fre-
quently than consonants in Pinyin. (2) some specific
properties in Chinese lead to insertion and deletion
errors. Many users in southern China cannot dis-
tinguish the front and the back nasal sound (?ang? -
?an?, ?ing? - ?in?, ?eng? - ?en?) as well as the retroflex
and the blade-alveolar (?zh? - ?z?, ?sh? - ?s?, ?ch? -
?c?). They are confused about whether they should
add letter ?g? or ?h? under these situations. (3) the
same letters can occur continuously in English, such
as ?acomplish-accomplish? and ?admited-admitted?
in our examples. English users sometimes make in-
sertion or deletion errors in these cases. We also
observe this kind of errors in Chinese Pinyin, such
as ?yingai-yinggai?, ?liange-liangge? and ?dianao-
diannao?.
For transposition errors, Table 5 lists Top 10 pat-
terns that produce transposition errors in Pinyin and
English. Our running example ?shenem-shenme?
belongs to this kind of errors. We classify the let-
ters of the keyboard into two categories, i.e. ?left?
and ?right?, according to their positions on the key-
board. Letter ?e? is controlled by left hand while ?m?
is controlled by right hand. Users mistype ?shenme?
as ?shenem? because they mistake the typing order
of ?m? and ?e?.
Fig. 4 is a graphic representation, in which we add
a link between ?m? and ?e?. The rest patterns in Ta-
ble 5 can be done in the same manner. Interestingly,
from Fig. 4, we reach the second conclusion: most
of the transposition errors are caused by mistak-
ing the typing orders across left and right hands.
For instance, users intend to type in a letter (?m?)
controlled by right hand. But they type in a letter
(?e?) controlled by left hand instead.
Pinyin Examples English Examples
ai xaing-xiang ei acheive-achieve
na xinag-xiang ra clera-clear
em shenem-shenme re vrey-very
ia xianzia-xianzai na wnat-want
ne zneme-zenme ie hieght-height
oa zhidoa-zhidao er befoer-before
ei jiejei-jiejie it esitmated-estimated
hs haihsi-haishi ne scinece-science
ah sahng-shang el littel-little
ou rugou-ruguo si epsiode-episode
Table 5: Transpositions errors in Pinyin and English.
Letters Controlled 
by Left Hand
Letters Controlled 
by Right Hand
r a
e
s
t
i
n
m
o
h
l
u
Figure 4: Transpositions errors on the keyboard.
For substitution errors, we study the reason why
users mistype one letter for another. In the Pinyin-
correction pairs, users always mistype ?a? as ?e? and
vice versa. The reason is that they have similar pro-
nunciations in Chinese. As a result, we add two di-
rected edges ?a? and ?e? in Fig. 5. Some letters are
mistyped for each other because they are adjacent
on the keyboard although they do not share similar
pronunciations, such as ?g? and ?f?.
We summarize the substitution errors in English
in Fig. 6. Letters ?q?, ?k? and ?c? are often mixed up
with each other because they sound alike in English
although they are apart on the keyboard. However,
the three letters are not connected in Fig. 5, which
indicates that users can easily distinguish them in
Pinyin.
Figure 5: Substitutions errors in Pinyin.
488
Figure 6: Substitutions errors in English.
Mistyped
letter
pairs
Similar
pronunciations
in Chinese
Similar
pronunciations
in English
Adjacent
on
keyboard
(m,n) X X X
(b,p);(d,t) X X ?
(z,c,s);(g,k,h) X ? X
(j,q,x);(u,v) X ? ?
(i,y) ? X X
(q,k,c) ? X ?
(j,h);(z,x) ? ? X
Table 6: Pronunciation properties and keyboard dis-
tance in Chinese Pinyin and English
We list some examples in Table 6. For example,
letters ?m? and ?n? have similar pronunciations in
both Chinese and English. Moreover, they are adja-
cent on the keyboard, which leads to interferences or
confusion in both Chinese and English. Letters ?j?,
?q? and ?x? are far from each other on the keyboard.
But they sound alike in Chinese, which makes them
connected in Fig. 5. In Fig. 6, letters ?b? and ?p?
are connected to each other because they have simi-
lar pronunciations in English, although they are not
adjacent on the keyboard.
Finally, we summarize the third conclusion: sub-
stitution errors are caused by language specific
similarities (similar pronunciations) or keyboard
neighborhood (adjacent on the keyboard).
All in all, we generally classify typing errors in
English and Chinese into four categories and investi-
gate the reasons that result in these errors respective-
ly. Some language specific properties, such as pro-
nunciations in English and Chinese, lead to substitu-
tion, insertion and deletion errors. Keyboard layouts
play an important role in transposition errors, which
are language-independent.
5 Conclusions and Future Works
In this paper, we study user input behaviors in Chi-
nese Pinyin input method from backspace opera-
tions. We aim at analyzing the reasons that cause
these errors. Users signal that they are very likely
to make errors if they press backspace on the key-
board. Then they modify the errors and type in the
correct words they want. Different from the previous
research, we extract abundant Pinyin-correction and
Chinese word-correction pairs from backspace op-
erations. Compared with English typos, we observe
some language-specific properties in Chinese have
impact on errors. All in all, user behaviors (Zheng
et al, 2009; Zheng et al, 2010; Zheng et al, 2011b)
in Chinese Pinyin input method provide novel per-
spectives for natural language processing tasks.
Below we sketch three possible directions for the
future work: (1) we should consider position fea-
tures in analyzing Pinyin errors. For example, it is
less likely that users make errors in the first letter
of an input Pinyin. (2) we aim at designing a self-
adaptive input method that provide error-tolerant
features (Chen and Lee, 2000; Zheng et al, 2011a).
(3) we want to build a Chinese spelling correction
system based on extracted error-correction pairs.
Acknowledgments
This work is supported by a Tsinghua-Sogou join-
t research project and the National Natural Science
Foundation of China under Grant No. 60873174.
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 955?962.
K. Atkinson. 2008. Gnu aspell 0.60.6.
http://aspell.sourceforge.net.
E. Brill and R.C. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics, pages 286?293.
C.H. Chang. 1994. A pilot study on automatic Chinese
spelling error correction. Communication of COLIPS,
4(2):143?149.
Z. Chen and K.F. Lee. 2000. A new statistical ap-
proach to Chinese Pinyin input. In Proceedings of the
489
38th Annual Meeting on Association for Computation-
al Linguistics, pages 241?247.
Q. Chen, M. Li, and M. Zhou. 2007. Improving query
spelling correction using web search results. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 181?189.
Y. Chen. 1997. Chinese Language Processing. Shanghai
Education publishing company.
W.E. Cooper. 1983. Cognitive aspects of skilled type-
writing. Springer-Verlag.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing, pages 293?300.
F.J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. Communications of
the ACM, 7(3):171?176.
J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. 2010.
A large scale ranker-based system for search query
spelling correction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 358?366.
G. Hirst and A. Budanitsky. 2005. Correcting real-word
spelling errors by restoring lexical cohesion. Natural
Language Engineering, 11(01):87?111.
A. Islam and D. Inkpen. 2009. Real-word spelling cor-
rection using GoogleWeb 1T 3-grams. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1241?1249.
M.D. Kernighan, K.W. Church, and W.A. Gale. 1990.
A spelling correction program based on a noisy chan-
nel model. In Proceedings of the 13th conference on
Computational linguistics, pages 205?210.
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys,
24(4):377?439.
Y.J. Lin and M.S. Yu. 2004. The properties and further
applications of Chinese frequent strings. Computa-
tional Linguistics and Chinese Language Processing,
9(1):113?128.
C.L. Liu, K.W. Tien, M.H. Lai, Y.H. Chuang, and S.H.
Wu. 2009. Capturing errors in written Chinese word-
s. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 25?28.
C.L. Liu, M.H. Lai, Y.H. Chuang, and C.Y. Lee. 2010.
Visually and phonologically similar characters in in-
correct simplified chinese words. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 739?747.
E.S. Ristad, P.N. Yianilos, M.T. Inc, and NJ Princeton.
1998. Learning string-edit distance. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(5):522?532.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 961?970.
K. Toutanova and R.C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 144?151.
J. Wang, S. Zhai, and H. Su. 2001. Chinese input with
keyboard and eye-tracking: an anatomical study. In
Proceedings of the SIGCHI conference on Human fac-
tors in computing systems, pages 349?356.
C. Whitelaw, B. Hutchinson, G.Y. Chung, and G. El-
lis. 2009. Using the web for language independent
spellchecking and autocorrection. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 890?899.
A. Wilcox-O?Hearn, G. Hirst, and A. Budanitsky. 2010.
Real-word spelling correction with trigrams: A recon-
sideration of the Mays, Damerau, and Mercer model.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 605?616.
L. Zhang, M. Zhou, C. Huang, and HH Pan. 1999.
Multifeature-based approach to automatic error detec-
tion and correction of Chinese text. In Proceedings of
the First Workshop on Natural Language Processing
and Neural Networks.
L. Zhang, C. Huang, M. Zhou, and H. Pan. 2000. Auto-
matic detecting/correcting errors in Chinese text by an
approximate word-matching algorithm. In Proceed-
ings of the 38th Annual Meeting on Association for
Computational Linguistics, pages 248?254.
Y. Zheng, Z. Liu, M. Sun, L. Ru, and Y. Zhang. 2009. In-
corporating user behaviors in new word detection. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence, pages 2101?2106.
Y. Zheng, Z. Liu, and L. Xie. 2010. Growing relat-
ed words from seed via user behaviors: a re-ranking
based approach. In Proceedings of the ACL 2010 Stu-
dent Research Workshop, pages 49?54.
Y. Zheng, C. Li, and M. Sun. 2011a. CHIME: An ef-
ficient error-tolerant chinese pinyin input method. In
Proceedings of the 22nd International Joint Confer-
ence on Artificial Intelligence (accepted).
Y. Zheng, Z. Liu, L. Xie, M. Sun, L. Ru, and Y. Zhang.
2011b. User Behaviors in Related Word Retrieval
and New Word Detection: A Collaborative Perspec-
tive. ACM Transactions on Asian Language Informa-
tion Processing, Special Issue on Chinese Language
Processing (accepted).
490
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135?144,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Automatic Keyphrase Extraction by Bridging Vocabulary Gap ?
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, cxx.thu, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn
Abstract
Keyphrase extraction aims to select a set of
terms from a document as a short summary
of the document. Most methods extract
keyphrases according to their statistical prop-
erties in the given document. Appropriate
keyphrases, however, are not always statis-
tically significant or even do not appear in
the given document. This makes a large
vocabulary gap between a document and its
keyphrases. In this paper, we consider that
a document and its keyphrases both describe
the same object but are written in two different
languages. By regarding keyphrase extraction
as a problem of translating from the language
of documents to the language of keyphrases,
we use word alignment models in statistical
machine translation to learn translation proba-
bilities between the words in documents and
the words in keyphrases. According to the
translation model, we suggest keyphrases giv-
en a new document. The suggested keyphrases
are not necessarily statistically frequent in the
document, which indicates that our method
is more flexible and reliable. Experiments
on news articles demonstrate that our method
outperforms existing unsupervised methods
on precision, recall and F-measure.
1 Introduction
Information on the Web is emerging with the
development of Internet. It is becoming more and
more important to effectively search and manage
information. Keyphrases, as a brief summary of a
document, provide a solution to help organize and
?Zhiyuan Liu and Xinxiong Chen have equal contribution
to this work.
retrieve documents, which have been widely used
in digital libraries and information retrieval (Turney,
2000; Nguyen and Kan, 2007). Due to the explosion
of information, it is ineffective for professional
human indexers to manually annotate documents
with keyphrases. How to automatically extract
keyphrases from documents becomes an important
research problem, which is usually referred to as
keyphrase extraction.
Most methods for keyphrase extraction try to
extract keyphrases according to their statistical prop-
erties. These methods are susceptible to low perfor-
mance because many appropriate keyphrases may
not be statistically frequent or even not appear in the
document, especially for short documents. We name
the phenomenon as the vocabulary gap between
documents and keyphrases. For example, a research
paper talking about ?machine transliteration? may
less or even not mention the phrase ?machine
translation?. However, since ?machine transliter-
ation? is a sub-field of ?machine translation?, the
phrase ?machine translation? is also reasonable to
be suggested as a keyphrase to indicate the topics
of this paper. Let us take another example: in a
news article talking about ?iPad? and ?iPhone?, the
word ?Apple? may rarely ever come up. However,
it is known that both ?iPad? and ?iPhone? are the
products of ?Apple?, and the word ?Apple? may thus
be a proper keyphrase of this article.
We can see that, the essential challenge of
keyphrase extraction is the vocabulary gap between
documents and keyphrases. Therefore, the task of
keyphrase extraction is how to capture the semantic
relations between the words in documents and in
keyphrases so as to bridge the vocabulary gap.
In this paper, we provide a new perspective to
135
documents and their keyphrases: each document
and its keyphrases are descriptions to the same
object, but the document is written using one lan-
guage, while keyphrases are written using another
language. Therefore, keyphrase extraction can be
regarded as a translation problem from the language
of documents into the language of keyphrases.
Based on the idea of translation, we use word
alignment models (WAM) (Brown et al, 1993) in
statistical machine translation (SMT) (Koehn, 2010)
and propose a unified framework for keyphrase
extraction: (1) From a collection of translation pairs
of two languages, WAM learns translation probabil-
ities between the words in the two languages. (2)
According to the translation model, we are able to
bridge the vocabulary gap and succeed in suggesting
appropriate keyphrases, which may not necessarily
frequent in their corresponding documents.
As a promising approach to solve the problem
of vocabulary gap, SMT has been widely ex-
ploited in many applications such as information
retrieval (Berger and Lafferty, 1999; Karimzade-
hgan and Zhai, 2010), image and video anno-
tation (Duygulu et al, 2002), question answer-
ing (Berger et al, 2000; Echihabi and Marcu, 2003;
Murdock and Croft, 2004; Soricut and Brill, 2006;
Xue et al, 2008), query expansion and rewrit-
ing (Riezler et al, 2007; Riezler et al, 2008; Riezler
and Liu, 2010), summarization (Banko et al, 2000),
collocation extraction (Liu et al, 2009b; Liu et al,
2010b) and paraphrasing (Quirk et al, 2004; Zhao
et al, 2010). Although SMT is a widely adopted
solution to vocabulary gap, for various applications
using SMT, the crucial and non-trivial problem is
to find appropriate and enough translation pairs for
SMT.
The most straightforward translation pairs for
keyphrase extraction is document-keyphrase pairs.
In practice, however, it is time-consuming to anno-
tate a large collection of documents with keyphrases
for sufficient WAM training. In order to solve
the problem, we use titles and summaries to build
translation pairs with documents. Titles and sum-
maries are usually accompanying with the corre-
sponding documents. In some special cases, titles
or summaries may be unavailable. We are also able
to extract one or more important sentences from
the corresponding documents to construct sufficient
translation pairs.
2 State of the Art
Some researchers (Frank et al, 1999; Witten et al,
1999; Turney, 2000) regarded keyphrase extraction
as a binary classification problem (is-keyphrase or
non-keyphrase) and learned models for classifica-
tion using training data. These supervised methods
need manually annotated training set, which is time-
consuming. In this paper, we focus on unsupervised
methods for keyphrase extraction.
The most simple unsupervised method for
keyphrase extraction is using TFIDF (Salton and
Buckley, 1988) to rank the candidate keyphrases and
select the top-ranked ones as keyphrases. TFIDF
ranks candidate keyphrases only according to their
statistical frequencies, which thus fails to suggest
keyphrases with low frequencies.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becoming
the state-of-the-art methods for keyphrase extrac-
tion (Liu et al, 2009a; Liu et al, 2010a). Given
a document, TextRank first builds a word graph,
in which the links between words indicate their
semantic relatedness, which are estimated by the
word co-occurrences in the document. By executing
PageRank (Page et al, 1998) on the graph, we obtain
the PageRank score for each word to rank candidate
keyphrases.
In TextRank, a low-frequency word will benefit
from its high-frequency neighbor words and thus be
ranked higher as compared to using TFIDF. This
alleviates the problem of vocabulary gap to some
extent. TextRank, however, still tends to extract
high-frequency words as keyphrases because these
words have more opportunities to get linked with
other words and obtain higher PageRank scores.
Moreover, TextRank usually constructs a word
graph simply according to word co-occurrences as
an approximation of the semantic relations between
words. This will introduce much noise because of
connecting semantically unrelated words and highly
influence extraction performance.
Some methods have been proposed to improve
TextRank, of which ExpandRank (Wan and Xi-
ao, 2008b; Wan and Xiao, 2008a) uses a smal-
l number, namely k, of neighbor documents to
136
provide more information of word relatedness for
the construction of word graphs. Compared to
TextRank, ExpandRank performs better when facing
the vocabulary gap by borrowing the information on
document level. However, the finding of neighbor
documents are usually arbitrary. This process may
introduce much noise and result in topic drift when
the document and its so-called neighbor documents
are not exactly talking about the same topics.
Another potential approach to alleviate vocabu-
lary gap is latent topic models (Landauer et al,
1998; Hofmann, 1999; Blei et al, 2003), of which
latent Dirichlet alocation (LDA) (Blei et al, 2003)
is most popular. Latent topic models learn topics
from a collection of documents. Using a topic
model, we can represent both documents and words
as the distributions over latent topics. The semantic
relatedness between a word and a document can be
computed using the cosine similarities of their topic
distributions. The similarity scores can be used as
the ranking criterion for keyphrase extraction (Hein-
rich, 2005; Blei and Lafferty, 2009). On one hand,
latent topic models use topics instead of statistical
properties of words for ranking, which abates the
vocabulary gap problem on topic level. On the other
hand, the learned topics are usually very coarse, and
topic models tend to suggest general words for a
given document. Therefore, the method usually fails
to capture the specific topics of the document.
In contract to the above-mentioned methods, our
method addresses vocabulary gap on word level,
which prevents from topic drift and works out better
performance. In experiments, we will show our
method can better solve the problem of vocab-
ulary gap by comparing with TFIDF, TextRank,
ExpandRank and LDA.
3 Keyphrase Extraction by Bridging
Vocabulary Gap Using WAM
First, we give a formal definition of keyphrase
extraction: given a collection of documents D, for
each document d ? D, keyphrase extraction aims
to rank candidate keyphrases according to their
likelihood given the document d, i.e., Pr(p|d) for all
p ? P, where P is the candidate keyphrase set. Then
we select top-Md ones as keyphrases, where Md can
be fixed or automatically determined by the system.
The document d can be regarded as a sequence of
words wd = {wi}Nd1 , where Nd is the length of d.
In Fig. 1, we demonstrate the framework of
keyphrase extraction using WAM. We divide the
algorithm into three steps: preparing translation
pairs, training translation models and extracting
keyphrases for a given document. We will introduce
the three steps in details from Section 3.1 to
Section 3.3.
Input: A large collection of documents D for keyphrase
extraction.
Step 1: Prepare Translation Pairs. For each d ? D, we
may prepare two types of translation pairs:
? Title-based Pairs. Use the title td of each document
d and prepare translation pairs, denote as ?D,T ?.
? Summary-based Pairs. Use the summary sd of
each document d and prepare translation pairs,
denote as ?D,S?.
Step 2: Train Translation Model. Given translation
pairs, e.g., ?D,T ?, train word-word translation model
Pr?D,T ?(t|w) using WAM, where w is the word in docu-
ment language and t is the word in title language.
Step 3: Keyphrase Extraction. For a document d,
extract keyphrases according to a trained translation
model, e.g., Pr?D,T ?(t|w).
1. Measure the importance score Pr(w|d) of each word
w in document d.
2. Compute the ranking score of candidate keyphrase
p by
Pr(p|d) =
?t?p?w?d Pr?D,T ?(t|w)Pr(w|d) (1)
3. Select top-Md ranked candidate keyphrases accord-
ing to Pr(p|d) as the keyphrases of document d.
Figure 1: WAM for keyphrase extraction.
3.1 Preparing Translation Pairs
Training dataset for WAM consists of a number
of translation pairs written in two languages. In
keyphrase extraction task, we have to construct
sufficient translation pairs to capture the semantic
relations between documents and keyphrases. Here
we propose to construct two types of translation
pairs: title-based pairs and summary-based pairs.
137
3.1.1 Title-based Pairs
Title is usually a short summary of the given doc-
ument. In most cases, documents such as research
papers and news articles have corresponding titles.
Therefore, we can use title to construct translation
pairs for a document.
WAM assumes each translation pair should be of
comparable length. However, a document is usually
much longer than title. It will hurt the performance
if we fill the length-unbalanced pairs for WAM
training. We propose two methods to address the
problem: sampling method and split method.
In sampling method, we perform word sampling
for each document to make it comparable to the
length of its title. Suppose the lengths of a document
and its title are Nd and Nt , respectively. For
document d, we first build a bag of words bd =
{(wi,ei)}Wdi=1, where Wd is the number of unique
words in d, and ei is the weights of word wi in d.
In this paper, we use TFIDF scores as the weights
of words. Using bd , we sample words for Nt
times with replacement according to the weights of
words, and finally form a new bag with Nt words
to represent document d. In the sampling result,
we keep the most important words in document d.
We can thus construct a document-title pair with
balanced length.
In split method, we split each document into
sentences which are of comparable length to its
title. For each sentence, we compute its semantic
similarity with the title. There are various methods
to measure semantic similarities. In this paper, we
use vector space model to represent sentences and
titles, and use cosine scores to compute similarities.
If the similarity is smaller than a threshold ? , we
will discard the sentence; otherwise, we will regard
the sentence and title as a translation pair.
Sampling method and split method have their
own characteristics. Compared to split method,
sampling method loses the order information of
words in documents. While split method generates
much more translation pairs, which leads to longer
training time of WAM. In experiment section, we
will investigate the performance of the two methods.
3.1.2 Summary-based Pairs
For most research articles, authors usually pro-
vide abstracts to summarize the articles. Many news
articles also have short summaries. Suppose each
document itself has a short summary, we can use
the summary and document to construct translation
pairs using either sampling method or split method.
Because each summary usually consists of multiple
sentences, split method for constructing summary-
based pairs has to split both the document and
summary into sentences, and the sentence pairs with
similarity scores above the threshold are filled in
training dataset for WAM.
3.2 Training Translation Models
Without loss of generality, we take title-based pairs
as the example to introduce the training process
of translation models, and suppose documents are
written in one language and titles are written in
another language. In this paper, we use IBM Model-
1 (Brown et al, 1993) for WAM training. IBM
Model-1 is a widely used word alignment algorithm
which does not require linguistic knowledge for two
languages 1.
In IBM Model-1, for each translation pair
?wd ,wt?, the relationship of the document language
wd = {wi}Ldi=0 and the title language wt = {ti}
Lt
i=0
is connected via a hidden variable a = {ai}Ldi=1
describing an alignment mapping from words of
documents to words of titles,
Pr(wd |wt) = ?aPr(wd ,a|wt) (2)
For example, a j = i indicates word w j in wd at
position j is aligned to word ti in wt at position i.
The alignment a also contains empty-word align-
ments a j = 0 which align words of documents to
an empty word. IBM Model-1 can be trained using
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977) in an unsupervised fashion. Using
IBM Model-1, we can obtain the translation prob-
abilities of two language-vocabularies, i.e., Pr(t|w)
and Pr(w|t), where w is a word in document
vocabulary and t is a word in title vocabulary.
IBM Model-1 will produce one-to-many align-
ments from one language to another language, and
the trained model is thus asymmetric. Hence, we can
1We have also employed more sophisticated WAM al-
gorithms such as IBM Model-3 for keyphrase extraction.
However, these methods did not achieve better performance
than the simple IBM Model-1. Therefore, in this paper we only
demonstrate the experimental results using IBM Model-1.
138
train two different translation models by assigning
translation pairs in two directions, i.e., (document?
title) and (title ? document). We denote the former
model as Prd2t and the latter as Prt2d. We define
Pr?D,T ?(t|w) in Eq.(1) as the harmonic mean of the
two models:
Pr?D,T ?(t|w) ?
(
?
Prd2t(t|w) +
(1?? )
Prt2d(t|w)
)?1
(3)
where ? is the harmonic factor to combine the two
models. When ? = 1.0 or ? = 0.0, it simply uses
model Prd2t or Prt2d, correspondingly. Using the
translation probabilities Pr(t|w) we can bridge the
vocabulary gap between documents and keyphrases.
3.3 Keyphrase Extraction
Given a document d, we rank candidate keyphrases
by computing their likelihood Pr(p|d). Each can-
didate keyphrase p may be composed of multiple
words. As shown in (Hulth, 2003), most keyphrases
are noun phrases. Following (Mihalcea and Tarau,
2004; Wan and Xiao, 2008b), we simply select
noun phrases from the given document as candidate
keyphrases with the help of POS tags. For each
word t, we compute its likelihood given d, Pr(t|d) =
?w?d Pr(t|w)Pr(w|d), where Pr(w|d) is the weight
of the word w in d, which is measured using
normalized TFIDF scores. Pr(t|w) is the translation
probabilities obtained from WAM training.
Using the scores of all words in candidate
keyphrases, we compute the ranking score of each
candidate keyphrase by summing up the scores
of each word in the candidate keyphrase, i.e.,
Pr(p|d) =
?t?pPr(t|d). In all, the ranking scores
of candidate keyphrases is formalized in Eq. (1)
of Fig. 1. According to the ranking scores, we can
suggest top-Md ranked candidates as the keyphrases,
where Md is the number of suggested keyphrases to
the document d pre-specified by users or systems.
We can also consider the number of words in the
candidate keyphrase as a normalization factor to Eq.
(1), which will be our future work.
4 Experiments
To perform experiments, we crawled a collection of
13,702 Chinese news articles 2 from www.163.
2The dataset can be obtained from http://nlp.csai.
tsinghua.edu.cn/?lzy/datasets/ke_wam.html.
com, one of the most popular news websites in Chi-
na. The news articles are composed of various topics
including science, technology, politics, sports, arts,
society and military. All news articles are manually
annotated with keyphrases by website editors, and
all these keyphrases come from the corresponding
documents. Each news article is also provided with
a title and a short summary.
In this dataset, there are 72,900 unique words in
documents, and 12,405 unique words in keyphrases.
The average lengths of documents, titles and sum-
maries are 971.7 words, 11.6 words, and 45.8 words,
respectively. The average number of keyphrases
for each document is 2.4. In experiments, we
use the annotated titles and summaries to construct
translation pairs.
In experiments, we select GIZA++ 3 (Och and
Ney, 2003) to train IBM Model-1 using translation
pairs. GIZA++, widely used in various applications
of statistical machine translation, implements IBM
Models 1-5 and an HMM word alignment model.
To evaluate methods, we use the annotated
keyphrases by www.163.com as the standard
keyphrases. If one suggested keyphrase exact-
ly matches one of the standard keyphrases, it
is a correct keyphrase. We use precision p =
ccorrect/cmethod , recall r = ccorrect/cstandard and F-
measure f = 2pr/(p + r) for evaluation, where
ccorrect is the number of keyphrases correctly sug-
gested by the given method, cmethod is the number
of suggested keyphrases, and cstandard is the number
of standard keyphrases. The following experiment
results are obtained by 5-fold cross validation.
4.1 Evaluation on Keyphrase Extraction
4.1.1 Performance Comparison and Analysis
We use four representative unsupervised methods
as baselines for comparison: TFIDF, TextRank (Mi-
halcea and Tarau, 2004), ExpandRank (Wan and
Xiao, 2008b) and LDA (Blei et al, 2003). We
denote our method as WAM for short.
In Fig. 2, we demonstrate the precision-recall
curves of various methods for keyphrase extraction
including TFIDF, TextRank, ExpandRank, LDA
and WAM with title-based pairs prepared using
3The website for GIZA++ package is http://code.
google.com/p/giza-pp/.
139
sampling method (Title-Sa) and split method (Title-
Sp), and WAM with summary-based pairs prepared
using sampling method (Summ-Sa) and split method
(Summ-Sp). For WAM, we set the harmonic factor
? = 1.0 and threshold ? = 0.1, which is the optimal
setting as shown in the later analysis on parameter
influence. For TextRank, LDA and ExpandRank, we
report their best results after parameter tuning, e.g.,
the number of topics for LDA is set to 400, and the
number of neighbor documents for ExpandRank is
set to 5 .
The points on a precision-recall curve represent
different numbers of suggested keyphrases from
Md = 1 (bottom right) to Md = 10 (upper left),
respectively. The closer the curve is to the upper
right, the better the overall performance of the
method is. In Table 1, we further demonstrate the
precision, recall and F-measure scores of various
methods when Md = 2 4. In Table 1, we also show
the statistical variances after ?. From Fig. 2 and
Table 1, we have the following observations:
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Rec
all
Precision
TFIDFTextRankLDAExpandRankTitle-SaTitle-SpSumm-SaSumm-Sp
Figure 2: The precision-recall curves of various
methods for keyphrase extraction.
First, our method outperforms all baselines. It
indicates that the translation perspective is valid
for keyphrase extraction. When facing vocabu-
lary gap, TFIDF and TextRank have no solutions,
ExpandRank adopts the external information on
document level which may introduce noise, and
LDA adopts the external information on topic level
which may be too coarse. In contrast to these
baselines, WAM aims to bridge the vocabulary gap
on word level, which avoids topic drift effectively.
4We select Md = 2 because WAM gains the best F-measure
score when Md = 2, which is close to the average number of
annotated keyphrases for each document 2.4.
Method Precision Recall F-measure
TFIDF 0.187 0.256 0.208?0.005
TextRank 0.217 0.301 0.243?0.008
LDA 0.181 0.253 0.203?0.002
ExpandRank 0.228 0.316 0.255?0.007
Title-Sa 0.299 0.424 0.337?0.008
Title-Sp 0.300 0.425 0.339?0.010
Summ-Sa 0.258 0.361 0.289?0.009
Summ-Sp 0.273 0.384 0.307?0.008
Table 1: Precision, recall and F-measure of various
methods for keyphrase extraction when Md = 2.
Therefore, our method can better solve the problem
of vocabulary gap in keyphrase extraction.
Second, WAM with title-based pairs performs
better than summary-based pairs consistently, no
matter prepared using sampling method or split
method. This indicates the titles are closer to
the keyphrase language as compared to summaries.
This is also consistent with the intuition that titles
are more important than summaries. Meanwhile, we
can save training efforts using title-based pairs.
Last but not least, split method achieves better or
comparable performance as compared to sampling
method on both title-based pairs and summary-
based pairs. The reasons are: (1) the split method
generates more translation pairs for adequate train-
ing than sampling method; and (2) split method
also keeps the context of words, which helps to
obtain better word alignment, unlike bag-of-words
in sampling method.
4.1.2 Influence of Parameters
We also investigate the influence of parameters
to WAM with title-based pairs prepared using split
method, which achieves the best performance as
shown in Fig. 2. The parameters include: harmonic
factor ? (described in Eq. 3) and threshold factor
? . Harmonic factor ? controls the weights of the
translation models trained in two directions, i.e.,
Prd2t(t|w) and Prt2d(t|w) as shown in Eq. (3). As
described in Section 3.1.1, using threshold factor ?
we filter out the pairs with similarities lower than ? .
In Fig. 3, we show the precision-recall curves
of WAM for keyphrase extraction when harmonic
factor ? ranges from 0.0 to 1.0 stepped by 0.2. From
the figure, we observe that the translation model
Prd2t(t|w) (i.e., when ? = 1.0) performs better than
140
Prt2d(t|w) (i.e., when ? = 0.0). This indicates that
it is sufficient to simply train a translation model
in one direction (i.e., Prd2t(t|w)) for keyphrase
extraction.
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Rec
all
Precision
? = 0.0? = 0.2? = 0.4? = 0.6? = 0.8? = 1.0
Figure 3: Precision-recall curves of WAM when
harmonic factor ? ranges from 0.0 to 1.0.
In Fig. 4, we show the precision-recall curves
of WAM for keyphrase extraction when threshold
factor ? ranges from 0.01 to 0.90. In title-
based pairs using split method, the total number
of pairs without filtering any pairs (i.e., ? = 0)
is 347,188. When ? = 0.01, 0.10 and 0.90, the
numbers of retained translation pairs are 165,023,
148,605 and 41,203, respectively. From Fig. 4,
we find that more translation pairs result in better
performance. However, more translation pairs also
indicate more training time of WAM. Fortunately,
we can see that the performance does not drop much
when discarding more translation pairs with low
similarities. Even when ? = 0.9, our method can
still achieve performance with precision p = 0.277,
recall r = 0.391 and F-measure f = 0.312 when
Md = 2. Meanwhile, we reduce the training efforts
by about 50% as compared to ? = 0.01.
In all, based on the above analysis on two
parameters, we demonstrate the effectiveness and
robustness of our method for keyphrase extraction.
4.1.3 When Titles/Summaries Are Unavailable
Suppose in some special cases, the titles or sum-
maries are unavailable, how can we construct trans-
lation pairs? Inspired by extraction-based document
summarization (Goldstein et al, 2000; Mihalcea and
Tarau, 2004), we can extract one or more important
sentences from the given document to construct
translation pairs. Unsupervised sentence extraction
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45
Rec
all
Precision
? = 0.01? = 0.05? = 0.10? = 0.30? = 0.50? = 0.70? = 0.90
Figure 4: Precision-recall curves of WAM when
threshold ? ranges from 0.01 to 0.90.
for document summarization is a well-studied task
in natural language processing. As shown in Table 2,
we only perform two simple sentence extraction
methods to demonstrate the effectiveness: (1) Select
the first sentence of a document (denoted as ?First?);
and (2) Compute the cosine similarities between
each sentence and the whole document represented
as two bags-of-words (denoted as ?Importance?).
It is interesting to find that the method of using
the first sentence performs similar to using titles.
This profits from the characteristic of news articles
which tend to give a good summary for the whole
article using the first sentence. Although the second
method drops much on performance as compared to
using titles, it still outperforms than other existing
methods. Moreover, the second method will im-
prove much if we use more effective measures to
identify the most important sentence.
Method Precision Recall F-measure
First 0.290 0.410 0.327?0.013
Importance 0.260 0.367 0.293?0.010
Table 2: Precision, recall and F-measure of
keyphrase extraction when Md = 2 by extracting one
sentence to construct translation pairs.
4.2 Beyond Extraction: Keyphrase Generation
In Section 4.1, we evaluate our method on keyphrase
extraction by suggesting keyphrases from docu-
ments. In fact, our method is also able to suggest
keyphrases that have not appeared in the content of
given document. The ability is important especially
when the length of each document is short, which
141
itself may not contain appropriate keyphrases. We
name the new task keyphrase generation. To
evaluate these methods on keyphrase generation,
we perform keyphrase generation for the titles of
documents, which are usually much shorter than
their corresponding documents. The experiment
setting is as follows: the training phase is the
same to the previous experiment, but in the test
phase we suggest keyphrases only using the titles.
LDA and ExpandRank, similar to our method, are
also able to select candidate keyphrases beyond the
titles. We still use the annotated keyphrases of the
corresponding documents as standard answers. In
this case, about 59% standard keyphrases do not
appear in titles.
In Table 3 we show the evaluation results of vari-
ous methods for keyphrase generation when Md = 2.
ForWAM, we only show the results using title-based
pairs prepared with split method. From the table,
we have three observations: (1) WAM outperforms
other methods on keyphrase generation. Moreover,
there are about 10% correctly suggested keyphrases
by WAM do not appear in titles, which indicates the
effectiveness of WAM for keyphrase generation. (2)
The performance of TFIDF and TextRank is much
lower as compared to Table 1, because the titles are
so short that they do not provide enough candidate
keyphrases and even the statistical information to
rank candidate keyphrases. (3) LDA, ExpandRank
and WAM roughly keep comparable performance as
in Table 1 (The performance of ExpandRank drops
a bit). This indicates the three methods are able to
perform keyphrase generation, and verifies again the
effectiveness of our method.
Method Precision Recall F-measure
TFIDF 0.105 0.141 0.115?0.004
TextRank 0.107 0.144 0.118?0.005
LDA 0.180 0.256 0.204?0.008
ExpandRank 0.194 0.268 0.216?0.012
WAM 0.296 0.420 0.334?0.009
Table 3: Precision, recall and F-measure of various
methods for keyphrase generation when Md = 2.
To demonstrate the features of our method for
keyphrase generation, in Table 4 we list top-5
keyphrases suggested by LDA, ExpandRank and
WAM for a news article entitled Israeli Military
Claims Iran Can Produce Nuclear Bombs and
Considering Military Action against Iran (We trans-
late the original Chinese title and keyphrases into
English for comprehension.). We have the following
observations: (1) LDA suggests general words like
?negotiation? and ?sanction? as keyphrases because
the coarse-granularity of topics. (2) ExpandRank
suggests some irrelevant words like ?Lebanon? as
keyphrases, which are introduced by neighbor doc-
uments talking about other affairs related to Israel.
(3) Our method can generate appropriate keyphrases
with less topic-drift. Moreover, our method can find
good keyphrases like ?nuclear weapon? which even
do not appear in the title.
LDA: Iran, U.S.A., negotiation, Israel, sanction
ExpandRank: Iran, Israel, Lebanon, U.S.A., Israeli
Military
WAM: Iran, military action, Israeli Military, Israel,
nuclear weapon
Table 4: Top-5 keyphrases suggested by LDA,
ExpandRank and WAM.
5 Conclusion and Future Work
In this paper, we provide a new perspective to
keyphrase extraction: regarding a document and its
keyphrases as descriptions to the same object written
in two languages. We use IBM Model-1 to bridge
the vocabulary gap between the two languages for
keyphrase generation. We explore various methods
to construct translation pairs. Experiments show
that our method can capture the semantic relations
between words in documents and keyphrases. Our
method is also language-independent, which can be
performed on documents in any languages.
We will explore the following two future work:
(1) Explore our method on other types of articles
and on other languages. (2) Explore more com-
plicated methods to extract important sentences for
constructing translation pairs.
Acknowledgments
This work is supported by the National Natural
Science Foundation of China (NSFC) under Grant
No. 60873174. The authors would like to thank
Peng Li and Xiance Si for their suggestions.
142
References
M. Banko, V.O. Mittal, and M.J. Witbrock. 2000.
Headline generation based on statistical translation. In
Proceedings of ACL, pages 318?325.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222?229.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approaches to answer-finding. In Proceedings of
SIGIR, pages 192?199.
D.M. Blei and J.D. Lafferty, 2009. Text mining:
Classification, Clustering, and Applications, chapter
Topic models. Chapman & Hall.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
A.P. Dempster, N.M. Laird, D.B. Rubin, et al 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society.
Series B (Methodological), 39(1):1?38.
P. Duygulu, Kobus Barnard, J. F. G. de Freitas, and
David A. Forsyth. 2002. Object recognition as
machine translation: Learning a lexicon for a fixed
image vocabulary. In Proceedings of ECCV, pages
97?112.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Proceedings of
ACL, pages 16?23.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 668?673.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence
extraction. In Proceedings of NAACL-ANLP 2000
Workshop on Automatic summarization, pages 40?48.
G. Heinrich. 2005. Parameter estimation for text anal-
ysis. Web: http://www. arbylon. net/publications/text-
est.
T. Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of SIGIR, pages 50?57.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216?223.
M. Karimzadehgan and C.X. Zhai. 2010. Estimation of
statistical translation models based on mutual informa-
tion for ad hoc information retrieval. In Proceedings
of SIGIR, pages 323?330.
P. Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25:259?284.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009a. Clustering
to find exemplar terms for keyphrase extraction. In
Proceedings of EMNLP, pages 257?266.
Z. Liu, H. Wang, H. Wu, and S. Li. 2009b. Collocation
extraction using monolingual word alignment method.
In Proceedings of EMNLP, pages 487?495.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010a. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP, pages 366?376.
Z. Liu, H. Wang, H. Wu, and S. Li. 2010b. Improving
statistical machine translation with monolingual collo-
cation. In Proceedings of ACL, pages 825?833.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of EMNLP, pages
404?411.
V. Murdock and W.B. Croft. 2004. Simple translation
models for sentence retrieval in factoid question an-
swering. In Proceedings of SIGIR.
T. Nguyen and M.Y. Kan. 2007. Keyphrase extraction
in scientific publications. In Proceedings of the 10th
International Conference on Asian Digital Libraries,
pages 317?326.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project, 1998.
C. Quirk, C. Brockett, andW. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In
Proceedings of EMNLP, volume 149.
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proccedings of ACL,
pages 464?471.
S. Riezler, Y. Liu, and A. Vasserman. 2008. Translating
queries into snippets for improved query expansion. In
Proceedings of COLING, pages 737?744.
G. Salton and C. Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information
processing and management, 24(5):513?523.
R. Soricut and E. Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Infor-
mation Retrieval, 9(2):191?206.
143
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
X. Wan and J. Xiao. 2008a. Collabrank: towards a
collaborative approach to single-document keyphrase
extraction. In Proceedings of COLING, pages 969?
976.
X. Wan and J. Xiao. 2008b. Single document
keyphrase extraction using neighborhood knowledge.
In Proceedings of AAAI, pages 855?860.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. Kea: Practical automatic
keyphrase extraction. In Proceedings of DL, pages
254?255.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval models
for question and answer archives. In Proceedings of
SIGIR, pages 475?482.
S. Zhao, H. Wang, and T. Liu. 2010. Paraphrasing with
search engine query logs. In Proceedings of COLING,
pages 1317?1325.
144
