Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 229?232,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Speakers? Intention Prediction Using Statistics of Multi-level Features in 
a Schedule Management Domain 
Donghyun Kim Hyunjung Lee Choong-Nyoung Seon 
Diquest Research Center Computer Science & Engineering Computer Science & Engineering 
Diquest Inc. Sogang University Sogang University 
Seoul, Korea Seoul, Korea Seoul, Korea 
kdh2007@sogang.ac.kr juvenile@sogang.ac.kr wilowisp@gmail.com 
   
Harksoo Kim Jungyun Seo 
Computer & Communications Engineering Computer Science & Engineering 
Kangwon National University Sogang University 
Chuncheon, Korea Seoul, Korea 
nlpdrkim@kangwon.ac.kr seojy@sogang.ac.kr 
 
Abstract 
Speaker?s intention prediction modules can be 
widely used as a pre-processor for reducing 
the search space of an automatic speech re-
cognizer. They also can be used as a pre-
processor for generating a proper sentence in a 
dialogue system. We propose a statistical 
model to predict speakers? intentions by using 
multi-level features. Using the multi-level fea-
tures (morpheme-level features, discourse-
level features, and domain knowledge-level 
features), the proposed model predicts speak-
ers? intentions that may be implicated in next 
utterances. In the experiments, the proposed 
model showed better performances (about 
29% higher accuracies) than the previous 
model. Based on the experiments, we found 
that the proposed multi-level features are very 
effective in speaker?s intention prediction. 
1 Introduction 
A dialogue system is a program in which a user 
and system communicate in natural language. To 
understand user?s utterance, the dialogue system 
should identify his/her intention. To respond 
his/her question, the dialogue system should gen-
erate the counterpart of his/her intention by refer-
ring to dialogue history and domain knowledge. 
Most previous researches on speakers? intentions 
have been focused on intention identification tech-
niques. On the contrary, intention prediction tech-
niques have been not studied enough although 
there are many practical needs, as shown in Figure 
1. 
 
When is the changed date?
Response, Timetable-update-dateAsk-ref, Timetable-update-date
It is changed into 4 May.
It is changed into 14 May.
?
Prediction of
user?s intention
Identification of
system?s intention
Reducing the search space 
of an ASR
It is changed into 12:40.
The date is changed.
Is it changed into 4 May?
?
It is changed into 4 May.
The result of
speech recognition
Example 1: Prediction of user?s intention
Example 2: Prediction of system?s intention
It is 706-8954.
Ask-confirm, Timetable-insert-phonenumResponse, Timetable-insert-phonenum
Response generation
Is it 706-8954?
Identification of
user?s intention
Prediction of
system?s intention
 
Figure 1. Motivational example 
 
In Figure 1, the first example shows that an inten-
tion prediction module can be used as a pre-
processor for reducing the search space of an ASR 
(automatic speech recognizer). The second exam-
ple shows that an intention prediction module can 
be used as a pre-processor for generating a proper 
sentence based on dialogue history. 
There are some researches on user?s intention 
prediction (Ronnie, 1995; Reithinger, 1995). Rei-
thinger?s model used n-grams of speech acts as 
input features. Reithinger showed that his model 
can reduce the searching complexity of an ASR to 
19~60%. However, his model did not achieve good 
performances because the input features were not 
rich enough to predict next speech acts. The re-
searches on system?s intention prediction have 
been treated as a part of researches on dialogue 
models such as a finite-state model, a frame-based 
229
model (Goddeau, 1996), and a plan-based model 
(Litman, 1987). However, a finite-state model has 
a weak point that dialogue flows should be prede-
fined. Although a plan-based model can manage 
complex dialogue phenomena using plan inference, 
a plan-based model is not easy to be applied to the 
real world applications because it is difficult to 
maintain plan recipes. In this paper, we propose a 
statistical model to reliably predict both user?s in-
tention and system?s intention in a schedule man-
agement domain. The proposed model determines 
speakers? intentions by using various levels of lin-
guistic features such as clue words, previous inten-
tions, and a current state of a domain frame.  
2 Statistical prediction of speakers? inten-
tions 
2.1 Generalization of speakers? intentions 
In a goal-oriented dialogue, speaker?s intention can 
be represented by a semantic form that consists of 
a speech act and a concept sequence (Levin, 2003). 
In the semantic form, the speech act represents the 
general intention expressed in an utterance, and the 
concept sequence captures the semantic focus of 
the utterance.  
 
Table 1. Speech acts and their meanings 
Speech act Description 
Greeting The opening greeting of a dialogue 
Expressive The closing greeting of a dialogue 
Opening Sentences for opening a goal-oriented dialogue 
Ask-ref WH-questions 
Ask-if YN-questions 
Response Responses of questions or requesting actions 
Request Declarative sentences for requesting actions 
Ask-
confirm Questions for confirming the previous actions 
Confirm Reponses of ask-confirm 
Inform Declarative sentences for giving some information 
Accept Agreement 
 
Table 2. Basic concepts in a schedule management 
domain. 
Table name Operation name Field name 
Timetable Insert, Delete, Select, Update 
Agent, Date, Day-of-week, 
Time, Person, Place 
Alarm Insert, Delete, Select, Update Date, Time 
 
Based on these assumptions, we define 11 domain-
independent speech acts, as shown in Table 1, and 
53 domain-dependent concept sequences according 
to a three-layer annotation scheme (i.e. Fully con-
necting basic concepts with bar symbols) (Kim, 
2007) based on Table 2. Then, we generalize 
speaker?s intention into a pair of a speech act and a 
concept sequence. In the remains of this paper, we 
call a pair of a speech act and a concept sequence) 
an intention. 
2.2 Intention prediction model 
Given n utterances 
nU ,1  in a dialogue, let 1+nSI  de-
note speaker?s intention of the n+1th utterance. 
Then, the intention prediction model can be for-
mally defined as the following equation: 
 
)|,(maxarg)|(
,111
,
,11
11
nnn
CSSA
nn UCSSAPUSIP
nn
+++
++
?       (1) 
 
In Equation (1), 1+nSA  and 1+nCS  are the speech act 
and the concept sequence of the n+1th utterance, 
respectively. Based on the assumption that the 
concept sequences are independent of the speech 
acts, we can rewrite Equation (1) as Equation (2). 
 
)|()|(maxarg)|(
,11,11
,
,11
11
nnnn
CSSA
nn UCSPUSAPUSIP
nn
+++
++
?
   (2) 
 
In Equation (2), it is impossible to directly com-
pute )|(
,11 nn USAP +  and )|( ,11 nn UCSP +  because a speaker 
expresses identical contents with various surface 
forms of n sentences according to a personal lin-
guistic sense in a real dialogue. To overcome this 
problem, we assume that n utterances in a dialogue 
can be generalized by a set of linguistic features 
containing various observations from the first ut-
terance to the nth utterance. Therefore, we simplify 
Equation (2) by using a linguistic feature set 1+nFS  
(a set of features that are accumulated from the 
first utterance to nth utterance) for predicting the 
n+1th intention, as shown in Equation (3). 
 
)|()|(maxarg)|( 1111
,
,11
11
+++++
++
? nnnn
CSSA
nn FSCSPFSSAPUSIP
nn
     (3) 
 
All terms of the right hand side in Equation (3) are 
represented by conditional probabilities given a 
various feature values. These conditional probabili-
ties can be effectively evaluated by CRFs (condi-
tional random fields) (Lafferty, 2001) that globally 
consider transition probabilities from the first ut-
230
terance to the n+1th utterance, as shown in Equa-
tion (4). 
 
)),(exp()(
1)|(
)),(exp()(
1)|(
1
11,1
1,11,1
1
11,1
1,11,1


+
=+
++
+
=+
++
=
=
n
i j
iijj
n
nnCRF
n
i j
iijj
n
nnCRF
FSCSF
FSZ
FSCSP
FSSAF
FSZ
FSSAP
?
?
 (4) 
 
In Equation (4), ),( iij FSSAF and ),( iij FSCSF  are fea-
ture functions for predicting the speech act and the 
concept sequence of the ith utterance, respectively. 
)(FSZ  is a normalization factor. The feature func-
tions receive binary values (i.e. zero or one) ac-
cording to absence or existence of each feature.  
2.3 Multi-level features 
The proposed model uses multi-level features as 
input values of the feature functions in Equation 
(4). The followings give the details of the proposed 
multi-level features. 
? Morpheme-level feature: Sometimes a few 
words in a current utterance give important 
clues to predict an intention of a next utterance. 
We propose two types of morpheme-level fea-
tures that are extracted from a current utterance: 
One is lexical features (content words annotated 
with parts-of-speech) and the other is POS fea-
tures (part-of-speech bi-grams of all words in 
an utterance). To obtain the morpheme-level 
features, we use a conventional morphological 
analyzer. Then, we remove non-informative 
feature values by using a well-known 2?  statis-
tic because the previous works in document 
classification have shown that effective feature 
selection can increase precisions (Yang, 1997). 
? Discourse-level feature: An intention of a cur-
rent utterance affects that dialogue participants 
determine intentions of next utterances because 
a dialogue consists of utterances that are se-
quentially associated with each other. We pro-
pose discourse-level features (bigrams of 
speakers? intentions; a pair of a current inten-
tion and a next intention) that are extracted 
from a sequence of utterances in a current di-
alogue. 
? Domain knowledge-level feature: In a goal-
oriented dialogue, dialogue participants accom-
plish a given task by using shared domain 
knowledge. Since a frame-based model is more 
flexible than a finite-state model and is more 
easy-implementable than a plan-based model, 
we adopt the frame-based model in order to de-
scribe domain knowledge. We propose two 
types of domain knowledge-level features; slot-
modification features and slot-retrieval features. 
The slot-modification features represent which 
slots are filled with suitable items, and the slot-
retrieval features represent which slots are 
looked up. The slot-modification features and 
the slot-retrieval features are represented by bi-
nary notation. In the slot-modification features, 
?1? means that the slot is filled with a proper 
item, and ?0? means that the slot is empty. In 
the slot-retrieval features, ?1? means that the 
slot is looked up one or more times. To obtain 
domain knowledge-level features, we prede-
fined speakers? intentions associated with slot 
modification (e.g. ?response & timetable-
update-date?) and slot retrieval (e.g. ?request & 
timetable-select-date?), respectively. Then, we 
automatically generated domain knowledge-
level features by looking up the predefined in-
tentions at each dialogue step. 
3 Evaluation 
3.1 Data sets and experimental settings 
We collected a Korean dialogue corpus simulated 
in a schedule management domain such as ap-
pointment scheduling and alarm setting. The dialo-
gue corpus consists of 956 dialogues, 21,336 
utterances (22.3 utterances per dialogue). Each 
utterance in dialogues was manually annotated 
with speech acts and concept sequences. The ma-
nual tagging of speech acts and concept sequences 
was done by five graduate students with the know-
ledge of a dialogue analysis and post-processed by 
a student in a doctoral course for consistency. To 
experiment the proposed model, we divided the 
annotated messages into the training corpus and 
the testing corpus by a ratio of four (764 dialogues) 
to one (192 dialogues). Then, we performed 5-fold 
cross validation. We used training factors of CRFs 
as L-BGFS and Gaussian Prior. 
3.2 Experimental results 
Table 3 and Table 4 show the accuracies of the 
proposed model in speech act prediction and con-
cept sequence prediction, respectively. 
231
 Table 3. The accuracies of speech act prediction 
Features Accuracy-S (%) Accuracy-U (%) 
Morpheme-level 
features 76.51 72.01 
Discourse-level 
features 87.31 72.80 
Domain know-
ledge-level feature 63.44 49.03 
All features 88.11 76.25 
 
Table 4. The accuracies of concept sequence pre-
diction 
Features Accuracy-S (%) Accuracy-U (%) 
Morpheme-level 
features 66.35 59.40 
Discourse-level 
features 86.56 62.62 
Domain know-
ledge-level feature 37.68 49.03 
All features 87.19 64.21 
 
In Table 3 and Table 4, Accuracy-S means the ac-
curacy of system?s intention prediction, and Accu-
racy-U means the accuracy of user?s intention 
prediction. Based on these experimental results, we 
found that multi-level features include different 
types of information and cooperation of the multi-
level features brings synergy effect. We also found 
the degree of feature importance in intention pre-
diction (i.e. discourse level features > morpheme-
level features > domain knowledge-level features). 
To evaluate the proposed model, we compare 
the accuracies of the proposed model with those of 
Reithinger?s model (Reithinger, 1995) by using the 
same training and test corpus, as shown in Table 5. 
 
Table 5. The comparison of accuracies 
Speaker Type Reithinger?s 
model 
The proposed 
model 
System 
Speech act 43.37 88.11 
Concept sequence 68.06 87.19 
User Speech act 37.59 76.25 Concept sequence 49.48 64.21 
 
As shown in Table 5, the proposed model outper-
formed Reithinger?s model in all kinds of predic-
tions. We think that the differences between 
accuracies were mainly caused by input features: 
The proposed model showed similar accuracies to 
Reithinger?s model when it used only domain 
knowledge-level features. 
4 Conclusion 
We proposed a statistical prediction model of 
speakers? intentions using multi-level features. The 
model uses three levels (a morpheme level, a dis-
course level, and a domain knowledge level) of 
features as input features of the statistical model 
based on CRFs. In the experiments, the proposed 
model showed better performances than the pre-
vious model. Based on the experiments, we found 
that the proposed multi-level features are very ef-
fective in speaker?s intention prediction. 
Acknowledgments 
This research (paper) was performed for the Intel-
ligent Robotics Development Program, one of the 
21st Century Frontier R&D Programs funded by 
the Ministry of Commerce, Industry and Energy of 
Korea. 
References  
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and S. 
Busayapongchai. 1996. ?A Form-Based Dialogue 
Manager for Spoken Language Applications?, Pro-
ceedings of International Conference on Spoken 
Language Processing, 701-704. 
D. Litman and J. Allen. 1987. A Plan Recognition Mod-
el for Subdialogues in Conversations, Cognitive 
Science, 11:163-200. 
H. Kim. 2007. A Dialogue-based NLIDB System in a 
Schedule Management Domain: About the method to 
Find User?s Intentions, Lecture Notes in Computer 
Science, 4362:869-877. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. ?Condi-
tional Random Fields: Probabilistic Models for Seg-
menting And Labeling Sequence Data?, Proceedings 
of ICML, 282-289. 
L. Levin, C. Langley, A. Lavie, D. Gates, D. Wallace, 
and K. Peterson. 2003. ?Domain Specific Speech 
Acts for Spoken Language Translation?, Proceedings 
of the 4th SIGdial Workshop on Discourse and Di-
alogue. 
N. Reithinger and E. Maier. 1995. ?Utilizing Statistical 
Dialog Act Processing in VerbMobil?, Proceedings 
of ACL, 116-121. 
R. W. Smith and D. R. Hipp, 1995. Spoken Natural 
Language Dialogue Systems: A Practical Approach, 
Oxford University Press. 
Y. Yang and J. Pedersen. 1997. ?A Comparative Study 
on Feature Selection in Text Categorization?, Pro-
ceedings of the 14th International Conference on 
Machine Learning. 
232
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 13?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Information extraction using finite state automata and syllable n-grams 
in a mobile environment 
 
 
Choong-Nyoung Seon Harksoo Kim Jungyun Seo 
Computer Science and Engi-
neering 
Computer and Communications 
Engineering 
Computer Science and Engi-
neering 
Sogang University Kangwon National University Sogang University 
Seoul, Korea Chuncheon, Korea Seoul, Korea 
wilowisp@gmail.com nlpdrkim@kangwon.ac.kr seojy@sogang.ac.kr 
 
Abstract 
We propose an information extraction system 
that is designed for mobile devices with low 
hardware resources. The proposed system ex-
tracts temporal instances (dates and times) and 
named instances (locations and topics) from 
Korean short messages in an appointment 
management domain. To efficiently extract 
temporal instances with limited numbers of 
surface forms, the proposed system uses well-
refined finite state automata. To effectively 
extract various surface forms of named in-
stances with low hardware resources, the pro-
posed system uses a modified HMM based on 
syllable n-grams. In the experiment on in-
stance boundary labeling, the proposed system 
showed better performances than traditional 
classifiers. 
1 Introduction 
Recently, many people access various multi-media 
contents using mobile devices such as a cellular 
phone and a PDA (personal digital assistant). Ac-
cordingly, users? requests on NLP (natural lan-
guage processing) are increasing because they 
want to easily and simply look up the multi-media 
contents. Information extraction is one of useful 
applications in NLP that helps users to easily 
access core information in a large amount of free 
texts. Unfortunately, it is not easy to implement an 
information extraction system in mobile devices 
because target texts include many morphological 
variations (e.g. blank omission, typos, word ab-
breviation) and mobile devices have many hard-
ware limitations (e.g. a small volume of a main 
memory and the absence of an arithmetic logic unit 
for floating-point calculation) 
There are some researches on information ex-
traction from short messages in a mobile device, 
and Cooper?s research (Cooper, 2005) is represent-
ative. Cooper predefined various syntactic patterns 
with placeholders and matched an input message 
against the syntactic patterns. Then, he extracted 
texts in the placeholders and assigned them the 
attribute name of the placeholders. This method 
has some advantages like easy implementation and 
fast response time. However, it is inadequate to 
apply Cooper?s method to languages with partially-
free word-order like Korean and Japanese because 
a huge amount of syntactic patterns should be pre-
defined according as the degree of freedom on 
word order increases. Kang (2004) proposed a 
NLIDB (natural language interface to database) 
system using lightweight shallow NLP techniques. 
Kang raised problems of deep NLP techniques 
such as low portability and error-proneness. Kang 
proposed a lightweight approach to natural lan-
guage interfaces, where translation knowledge is 
semi-automatically acquired and user questions are 
only syntactically analyzed. Although Kang?s me-
thod showed good performances in spite of using 
shallow NLP techniques, it is difficult to apply his 
method to mobile devices because his method still 
needs a morphological analyzer with a large size of 
dictionary. In this paper, we propose an informa-
tion extraction system that is designed for mobile 
devices with low hardware resources.  The pro-
posed system extracts appointment-related infor-
mation (i.e. dates, times, locations, and topics) 
from Korean short messages. 
This paper is organized as follows. In section 2, 
we proposed an information extraction system for 
a mobile device in an appointment domain. In sec-
13
tion 3, we explain experimental setup and report 
some experimental results. Finally, we draw some 
conclusions in section 4. 
2 Lightweight information extraction sys-
tem 
Figure 1 shows an overall architecture of the pro-
posed system. 
 
 
Output
Normalization 
Part
Extraction PartInput
Short Message
Temporal instance 
extraction
Converting machine-
manageable forms
Date/Time extraction 
results
Named instance 
extraction
Ranking instances
Location/Topic 
extraction results
Date/Time
FSAs
n-gram statistics
 
Figure 1. The system architecture  
 
As shown in Figure 1, the proposed system con-
sists of an extraction part and a normalization part.  
In the extraction part, the proposed system first 
extracts temporal instance candidates (i.e. dates 
and times) using FSA (finite-state automata). Then, 
the proposed system extracts named instance can-
didates (i.e. locations and topics) using syllable n-
grams. Finally, the proposed system ranks the ex-
tracted instances and selects the highest one per 
target category. In the normalization part, the pro-
posed system converts the temporal instances into 
suitable forms. 
2.1 Information extraction using finite state 
automata 
Although short messages in an appointment do-
main often include many incorrect words, temporal 
instances like dates and times are expressed as cor-
rect as possible because they are very important to 
appointment management. In addition, temporal 
instances are expressed in tractable numbers of 
surface forms in order to make message receivers 
easily be understood. In MUC-7, these kinds of 
temporal instances are called TIMEX (Chinchor, 
1998), and it has known that TIMEX can be easily 
extracted by using FSA (Srihari, 2001). Based on 
these previous works, the proposed system extracts 
temporal instances from short messages by using 
FSA, as shown in Figure 2. 
 
 
Figure 2. An example of FSA for date extraction 
2.2 Information extraction using statistical 
syllable n-grams 
Unlike dates and times, locations and topics not 
only have various surface forms, but also their 
constituent words are not included in a closed set. 
In MUC-7, these kinds of named entities are called 
NAMEX (Chinchor, 1998), and many researches 
on NAMEX have been performed by using rules 
and statistics. Generally, rule-based methods show 
high precisions but they have a weak point that it is 
hard to maintain a system when new words are 
continuously added to the system (Goh, 2003). Sta-
tistical methods guarantee reasonable perfor-
mances but they need large-scale language 
resource and complex floating point operations. 
Therefore, it is not suitable to apply previous tradi-
tional approaches to mobile devices with many 
hardware limitations. To resolve this problem, we 
propose a statistical model based on syllable n-
grams, as shown in Figure 3. 
 
 
Figure 3. Statistical information extraction 
14
 The extraction of named instances has two kinds of 
problems; a instance boundary detection problem 
and a category assigning problem. If we can use a 
conventional morphological analyzer, the instance 
boundary detection problem is not big. However, it 
is not easy to use a morphological analyzer in a 
mobile device because of hardware limitations and 
users? writing habitations. Users often ignore word 
spacing and this habitation lowers the performance 
of the morphological analyzer. To resolve this 
problem, we adopt a syllable n-gram model that 
performs well in word boundary detection for lan-
guages like Chinese with no spacing between 
words (Goh, 2003; Ha, 2004). We first define 9 
labels that represent boundaries of named instance 
candidates by adopting BIO (begin, inner, and out-
er) annotation scheme, as shown in Table 1 (Hong, 
2005; Uchimoto, 2000).  
 
Tag Description Tag Description 
LB Begin of a location TB Begin of a topic 
LI Inner of  a location TI Inner of a topic 
LE End of a location TE End of a topic 
LS A single-syllable loca-
tion 
TS A single-syllable 
topic 
OT Other syllable   
Table 1. The definition of instance boundary labels 
 
Then, based on a modified HMM (hidden Markov 
model), the proposed system assigns boundary la-
bels to each syllable in an input message, as fol-
lows. 
Let 
nS ,1  denote a message which consists of a 
sequence of n syllable, 
nsss ,...,, 21 , and let nL ,1  de-
note the boundary label sequence, 
nlll ,...,, 21 , of  nS ,1 . 
Then, the label annotation problem can be formally 
defined as finding 
nL ,1  which is the result of Equa-
tion (1). 
 
),(maxarg            
)(
),(
maxarg            
)|(maxarg)(
,1,1
,1
,1,1
,1,1,1
,1
,1
,1
nn
L
n
nn
L
nn
L
def
n
SLP
SP
SLP
SLPSL
n
n
n
=
=
=
  (1) 
 
In Equation (1), we dropped )(
,1 nSP  as it is constant 
for all 
nL ,1 . Next, we break Equation (1) into bite-
size pieces about which we can collect statistics, as 
shown in Equation (2). 
 
?
=
???
=
n
i
iiiiiinn sllPslsPSLP
1
1,11,11,1,1,1,1 ),|(),|(),(  (2) 
 
We simplify Equation (2) by making the following 
two assumptions: one is that the current boundary 
label is only dependent on the previous boundary 
label, and the other is that current boundary label is 
affected by its contextual features.  
 
?
=
?
=
n
i
iiiinn llPlsPSLP
1
1
*
,1,1 )|()|(),(   (3) 
 
In Equation (3), )|(* ii lsP  is a modified observation 
probability that is adopted from a class probability 
in na?ve Bayesian classification (Zheng, 1998) as 
shown in Equation (4). The reason why we modify 
an original observation probability )|( ii lsP  in 
HMM is its sparseness that is caused by a size li-
mitation of training corpus in a mobile environ-
ment. 
 
?
=
=
f
j
iijiii lsPlPZ
lsP
1
* )|()(1)|(   (4) 
 
In Equation (4), f  is the number of contextual fea-
tures, and ijs  is the jth feature of the ith syllable. Z  
is a normalizing factor. Table 2 shows the contex-
tual features that the proposed system uses. 
 
Feature Composition Meaning 
1is  is  The current syllable 
2is  ii ss 1?  
The previous syllable and the 
current syllable 
3is  1+ii ss  
The current syllable and the 
next syllable 
Table 2. The composition of contextual features 
 
In Equation (1), the max scores are calculated by 
using the well-known Viterbi algorithm (Forney, 
1973). 
After performing instance boundary labeling, 
the proposed system extracts syllable sequences 
labeled with the same named categories. For ex-
ample, if a syllable sequence is labeled with ?TS 
OT LB LI LI?, the proposed system extracts the 
sub-sequence of syllables labeled with ?LB LI LI?, 
15
as a location candidate. Then, the proposed system 
ranks the extracted instance candidates by using 
some information such as position, length, and a 
degree of completion, as shown in Equation (5). 
 
iiii CompletionLengthPosition)Rank(NI ?+?+?= ???  (5) 
 
In Equation (5), iPosition  means the distance from 
the beginning of input message to the ith named 
instance candidate iNI . In Korean, important words 
tend to appear in the latter part of a message. 
Therefore, we assume that the latter part an in-
stance candidate appears in, the more important the 
instance candidate is. iLength  means the length of an 
instance candidate. We assume that the longer an 
instance candidate include is, the more informative 
the instance candidate is. iCompletion  means whether 
a sequence of instance boundary labels is complete. 
We assume that instance candidates with complete 
label sequences are more informative. To check the 
degree of completion, the proposed system uses 
FSA, as shown in Figure 4. In the training corpus, 
every transition is legal. Therefore most of candi-
dates were satisfied the completion condition. 
However, sometimes the completion condition is 
not satisfied, when the candidate was extracted 
from the boundary of a sentence. Accordingly the 
condition gave an effect to the rank.  
 
 
Figure 4. The FSA for checking label completion 
 
In the experiments, we set ? , ? ,  and ?  to 1, 2, 
and 10, respectively. 
2.3 Normalization of temporal instances 
It is inadequate for the proposed system to use the 
extracted temporal instances as database instances 
without any processing because the temporal in-
stances consist of various forms of human-readable 
strings like ?January 24, 2008?. Therefore, the pro-
posed system should normalize the temporal in-
stances into machine-manageable forms like 
?20080124?. However, the normalization is not 
easy because temporal instances often include the 
relative information like ?this Sunday? and ?after 
two days?. To resolve this problem, the proposed 
system converts relative temporal instances into 
absolute temporal instances by using a message 
arrival time. For example, if a message includes 
the temporal instance ?after two days?, the pro-
posed system checks arrival time information of 
the message. Then, the proposed system adds a 
date in the arrival time information to two days. 
Figure 5 shows an example of date normalization. 
 
 
Figure 5. An example of date normalization 
3 Evaluation 
3.1 Data sets and experimental settings 
We collected 6,190 short messages simulated in an 
appointment scheduling domain. These messages 
contain 4,686 locations and 4,836 topics.  Each 
message is manually annotated with the boundary 
labels in Table 1. The manual annotation was done 
by 2 graduate students majoring in natural lan-
guage processing and post-processed by a student 
in a doctoral course for consistency. In order to 
experiment the proposed system, we divided the 
annotated messages into the training corpus and 
the testing corpus by a ratio of four (4,952 messag-
es) to one (1,238 messages). Then, we performed 
5-fold cross validation and used a precision, a re-
call rate, and a F1-measure as performance meas-
ures. In this paper, we did not evaluate the 
performances on the temporal instance extraction 
because performances of the proposed method are 
fully dependent on the coverage of pre-constructed 
FSA. 
3.2 Experimental results 
16
To choose the proper size of language models in a 
mobile environment, we evaluated performance 
variations of the proposed system, as shown in 
Figure 6. 
 
 
Figure 6. The performance variations according to the 
size of language models  
 
As shown in Figure 6, the system using syllable 
unigrams showed much lower performances than 
the systems using syllable bigrams or syllable tri-
grams.  
 
 Bigram Trigram 
# of features 54,711 158,525 
Size of DB 1.33M 2.83M 
Table 3. Space requirements of language models. 
 
However, as shown in the Table 3, although the 
number of syllable trigrams was three times larger 
than the number of syllable bigrams, the difference 
of performances between the system using syllable 
bigrams and the system using syllable trigrams was 
not big (about 1%). Based on this experimental 
result, we conclude that the combination of sylla-
ble unigrams and syllable bigrams, as shown in 
Table 2, is the most suitable language model for 
mobile devices with low hardware resources. 
To evaluate the proposed system, we calculated 
two types of performances. One is boundary labe-
ling performances that measure whether the pro-
posed system can correctly annotate a test corpus 
with boundary labels in Table 1. The other is ex-
traction performances that measure whether the 
proposed system can correctly extract named in-
stances from a test corpus by using Equation (5). 
Table 4 shows the boundary labeling performances 
of the proposed system in comparisons with those 
of representative classifiers. 
 
Model Precision Recall 
rate 
F1-
measure 
NB 62.74% 75.17% 68.34% 
SVM 67.29% 67.58% 67.37% 
CRF 70.98% 66.27% 68.45% 
Proposed 
system 74.81% 77.20% 75.91% 
Table 4. The comparison of boundary labeling perfor-
mances 
 
In Table 4, NB is a classifier using na?ve Bayesian 
statistics, and SVM is a classifier using a support 
vector machine. CRF is a classifier using condi-
tional random fields. As shown in Table 4, the 
proposed system outperformed the comparative 
models in all measures. Based on this fact, we 
think that the modified HMM may be more effec-
tive in a labeling sequence problem.  
Table 5 shows the extraction performances of the 
proposed system. In Table 5, the reason why the 
performances on the topic extraction are lower is 
that topic instances can consist of more various 
syllables (e.g. the topic instance, ?a meeting in 
Samsung Research Center?, includes the location, 
?Samsung Research Center?).  
 
Category Precision Recall rate F1-
measure 
Location 79.37% 76.33% 77.78% 
Topic 58.54% 55.20% 56.72% 
Table 5. The extraction performances 
 
Table 6 shows performance variations according as 
the parameters in Equation (5) are changed. As 
shown in Table 6, the differences between perfor-
mances are not big, and the proposed model 
showed the best performance at (?=1, ?=2, ?=5) or 
(?=1, ?=2, ?=10). On the basis of this experiments, 
we set ?, ?, and ? to 1, 2, and 5, respectively. 
 
(?,?,?) Precision of Location 
Recall rate 
of Location 
F1-measure 
of Location 
(1,1,1) 79.23% 76.20% 77.65% 
(1,1,5) 79.28% 76.24% 77.69% 
17
(1,1,10) 79.30% 76.26% 77.71% 
(1,2,5) 79.37% 76.33% 77.78% 
(1,2,10) 79.37% 76.33% 77.78% 
(?,?,?) Precision of Topic 
Recall rate 
of Topic  
F1-measure 
of Topic 
(1,1,1) 58.09% 54.76% 56.28% 
(1,1,5) 58.09% 54.76% 56.28% 
(1,1,10) 58.11% 54.78% 56.30% 
(1,2,5) 58.54% 55.20% 56.72% 
(1,2,10) 58.54% 55.20% 56.72% 
Table 6. The performance variations according to para-
meter changes 
 
To evaluate usefulness of the proposed model in a 
real mobile phone environment, we measured an 
average response time of 100 short messages in a 
mobile phone with XSCALE PXA270 CPU, 
51.26MB memory, and Windows mobile 5.0. We 
obtained an average response time of 0.0532 
seconds.  
4 Conclusion 
We proposed an information extraction system for 
a mobile device in an appointment management 
domain. The proposed system efficiently extracts 
temporal instances with limited numbers of surface 
forms by using FSA. To effectively extract various 
surface forms of named instances with low hard-
ware resources, the proposed system uses a mod-
ified HMM based on syllable n-grams. In the 
experiment on instance boundary labeling, the pro-
posed system outperformed traditional classifiers 
that showed good performances in a labeling se-
quence problem. On the experimental basis, we 
think that the proposed method is very suitable for 
information extraction applications with many 
hardware limitations. 
Acknowledgments 
This research (paper) was funded by Samsung 
Electronics.  
5 Reference  
Chooi Ling Goh, Masayuki Asahara, Yuji Matsumoto. 
2003. Chinese unknown word identification using 
character-based tagging and chunking. Proceedings 
of ACL-2003 Interactive Posters and Demonstrations, 
197-200. 
G. David Forney, JR. 1973. The Viterbi Algorithm Pro-
ceedings of the IEEE, 61(3):268-278.  
Hong Shen, Anoop Sarkar. 2005. Voting Between Mul-
tiple Data Representations for Text Chunking. Cana-
dian Conference on AI 2005. 389-400. 
In-Su Kang, Seung-Hoon Na, Jong-Hyeok Lee, Gijoo 
Yang. 2004. Lightweight Natural Language Database 
Interfaces. Proceedings of the 9th International Con-
ference on Application of Natural Language to In-
formation Systems. 76-88. 
Juhong Ha, Yu Zheng, Byeongchang Kim, Gary Geun-
bae Lee, Yoon-Suk Seong. 2004. High Speed Un-
known Word Prediction Using Support Vector 
Machine for Chinese Text-to-Speech Systems. 
IJCNLP:509-517 
Kiyotaka Uchimoto, Qing Ma, Masaki Murata, Hiromi 
Ozaku, and Hitoshi Isahara. Named Entity Extraction 
Based on A Maximum Entropy Model and Trans-
formation Rules. In Proceedings of the 38th Annual 
Meeting of Association for Computational Linguis-
tics 
Nancy A. Chinchor. 1998. MUC-7 named entity task 
definition, Proceedings of the Seventh Message Un-
derstanding Conference. 
Richard Cooper, Sajjad Ali, Chenlan Bi, 2005. Extract-
ing Information from Short Messages, NLDB 2005, 
LNCS 3513, pp. 388-391.  
Rohini Srihari, Cheng Niu, Wei Li. 2001. A hybrid ap-
proach for named entity and sub-type tagging. In 
Proc. 6th Applied Natural Language Processing Con-
ference. 
Zijian Zheng. Naive Bayesian classifier committees. 
Proceedings of the 10th European Conference on 
Machine Learning. Berlin: Springet-Verlag (1998) 
196-207. 
SVM_light: http://svmlight.joachims.org/ 
CRF++: http://crfpp.sourceforge.net/ 
18
