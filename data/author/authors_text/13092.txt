Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 34?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Supporting the Adaptation of Texts for Poor Literacy Readers: a Text 
Simplification Editor for Brazilian Portuguese 
 
Arnaldo Candido Jr., Erick Maziero, Caroline Gasperin, Thiago A. S. Pardo, Lucia Specia, and Sandra M. Aluisio  
 
Center of Computational Linguistics (NILC) / Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
arnaldoc@icmc.usp.br, egmaziero@gmail.com, {cgasperin,taspardo,lspecia,sandra}@icmc.usp.br 
 
Abstract 
In this paper we investigate the task of text 
simplification for Brazilian Portuguese. Our 
purpose is three-fold: to introduce a 
simplification tool for such language and its 
underlying development methodology, to 
present an on-line authoring system of 
simplified text based on the previous tool, and 
finally to discuss the potentialities of such 
technology for education. The resources and 
tools we present are new for Portuguese and 
innovative in many aspects with respect to 
previous initiatives for other languages.  
1 Introduction 
In Brazil, according to the index used to measure 
the literacy level of the population (INAF - National 
Indicator of Functional Literacy), a vast number of 
people belong to the so called rudimentary and basic 
literacy levels. These people are only able to find 
explicit information in short texts (rudimentary 
level) or process slightly longer texts and make 
simple inferences (basic level). INAF reports that 
68% of the 30.6 million Brazilians between 15 and 
64 years who have studied up to 4 years remain at 
the rudimentary literacy level, and 75% of the 31.1 
million who studied up to 8 years remain at the 
rudimentary or basic levels. 
Reading comprehension entails three elements: 
the reader who is meant to comprehend; the text that 
is to be comprehended and the activity in which 
comprehension is a part of (Snow, 2002). In 
addition to the content presented in the text, the 
vocabulary load of the text and its linguistic 
structure, discourse style, and genre interact with the 
reader?s knowledge. When these factors do not 
match the reader?s knowledge and experience, the 
text becomes too complex for the comprehension to 
occur. In this paper we will focus on the text and the 
aspects of it that make reading difficult or easy. One 
solution to ease the syntactic structure of a text is 
via Text Simplification (TS) facilities.  
TS aims to maximize the comprehension of 
written texts through the simplification of their 
linguistic structure. This may involve simplifying 
lexical and syntactic phenomena, by substituting 
words that are only understood by a few people with 
words that are more usual, and by breaking down 
and changing the syntactic structure of the sentence, 
respectively. As a result, it is expected that the text 
can be more easily understood both by humans and 
computer systems (Mapleson, 2006; Siddharthan, 
2003, Max, 2006). TS may also involve dropping 
parts or full sentences and adding some extra 
material to explain a difficult point. This is the case, 
for example, of the approach presented by Petersen 
and Ostendorf (2007), in which abridged versions of 
articles are used in adult literacy learning. 
It has already been shown that long sentences, 
conjoined sentences, embedded clauses, passives, 
non-canonical word order, and use of low-frequency 
words, among other things, increase text complexity 
for language-impaired readers (Siddharthan, 2002; 
Klebanov et al, 2004; Devlin and Unthank, 2006). 
The Plain English initiative makes available 
guidelines to make texts easier to comprehend: the 
Plain Language1. In principle, its recommendations 
can be applied to any language. Although some of 
them are directly useful for TS systems (e.g., 
subject-verb-object order and active voice), others 
are difficult to specify (e.g., how simple each 
syntactic construction is and which words are 
simple). 
In this paper we present the results of a study of 
syntactic simplification for Brazilian Portuguese 
(BP) and a rule-based syntactic simplification 
system for this language that was developed based 
on this study ? the first of this kind for BP. We also 
present an on-line authoring tool for creating 
simplified texts. One possible application of this 
tool is to help teachers to produce instructional texts 
                                                 
1
 http://www.plainlanguage.gov 
34
to be used in classrooms. The study is part of the 
PorSimples project2 (Simplification of Portuguese 
Text for Digital Inclusion and Accessibility), which 
aims at producing text simplification tools for 
promoting digital inclusion and accessibility for 
people with different levels of literacy, and possibly 
other kinds of reading disabilities. 
This paper is organized as follows. In Section 2 
we present related approaches for text simplification 
with educational purposes. In Section 3 we describe 
the proposed approach for syntactic simplification, 
which is used within an authoring tool described in 
Section 4. In Section 5 we discuss possible uses of 
text simplification for educational purposes.  
2 Related work 
Burstein (2009) presents an NLP-based application 
for educational purposes, named Text Adaptor, 
which resembles our authoring tool. It includes 
complex sentence highlighting, text elaboration 
(word substitutions by easier ones), text 
summarization and translation. The system does not 
perform syntactic simplification, but simply 
suggests, using a shallow parser, that some 
sentences might be too complex. Specific hints on 
the actual source of complexity are not provided. 
Petersen (2007) addresses the task of text 
simplification in the context of second-language 
learning. A data-driven approach to simplification is 
proposed using a corpus of paired articles in which 
each original sentence does not necessarily have a 
corresponding simplified sentence, making it 
possible to learn where writers have dropped or 
simplified sentences. A classifier is used to select 
the sentences to simplify, and Siddharthan?s 
syntactic simplification system (Siddharthan, 2003) 
is used to split the selected sentences. In our 
approach, we do not drop sentences, since we 
believe that all the content must be kept in the text. 
Siddharthan proposes a syntactic simplification 
architecture that relies on shallow text analysis and 
favors time performance. The general goal of the 
architecture is to make texts more accessible to a 
broader audience; it has not targeted any particular 
application. The system treats apposition, relative 
clauses, coordination and subordination. Our 
method, on the other hand, relies on deep parsing 
(Bick, 2000). We treat the same phenomena as 
                                                 
2
 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
Siddharthan, but also deal with Subject-Verb-Object 
ordering (in Portuguese sentences can be written in 
different orders) and passive to active voice 
conversion. Siddharthan's system deals with non-
finite clauses which are not handled by our system 
at this stage. 
Lal and Ruger?s (2002) created a bayesian 
summarizer with a built-in lexical simplification 
module, based on WordNet and MRC psycho-
linguistic database3. The system focuses on 
schoolchildren and provides background 
information about people and locations in the text, 
which are retrieved from databases. Our rule-based 
simplification system only replaces discourse 
markers for more common ones using lexical 
resources built in our project, instead of inserting 
additional information in the text. 
Max (2005, 2006) applies text simplification in 
the writing process by embedding an interactive text 
simplification system into a word processor. At the 
user?s request, an automatic parser analyzes an 
individual sentence and the system applies 
handcrafted rewriting rules. The resulting suggested 
simplifications are ranked by a score of syntactic 
complexity and potential change of meaning. The 
writer then chooses their preferred simplification. 
This system ensures accurate output, but requires 
human intervention at every step. Our system, on 
the other hand, is autonomous, even though the user 
is able to undo any undesirable simplification or to 
choose alternative simplifications. These alternative 
simplifications may be produced in two cases: i) to 
compose a new subject in simplifications involving 
relatives and appositions and ii) to choose among 
one of the coordinate or subordinate simplifications 
when there is ambiguity regarding to conjunctions. 
Inui et al (2003) proposes a rule-based system 
for text simplification aimed at deaf people. The 
authors create readability assessments based on 
questionnaires answered by teachers about the deaf. 
With approximately one thousand manually created 
rules, the authors generate several paraphrases for 
each sentence and train a classifier to select the 
simpler ones. Promising results are obtained, 
although different types of errors on the paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency. In our work we 
produce alternative simplifications only in the two 
cases explained above. 
                                                 
3
 http://www.psych.rl.ac.uk/ 
35
Caseli et al (2009) developed an annotation 
editor to support the building of parallel corpora of 
original and simplified texts in Brazilian 
Portuguese. The tool was used to build a corpus of 
simplified texts aimed at people with rudimentary 
and basic literacy levels. We have used the parallel 
corpus to evaluate our rule-based simplification 
system. The on-line authoring system presented in 
this paper evolved from this annotation editor. 
There are also commercial systems like Simplus4 
and StyleWriter5, which aim to support Plain 
English writing.  
3 A rule-based syntactic simplification 
system 
Our text simplification system comprises seven 
operations (see Sections 3.1 and 3.2), which are 
applied to a text in order to make its syntactic 
structure simpler. These operations are applied 
sentence by sentence, following the 3-stage 
architecture proposed by Siddharthan (2002), which 
includes stages of analysis, transformation and 
regeneration. In Siddharthan?s work, the analysis 
stage performs the necessary linguistic analyses of 
the input sentences, such as POS tagging and 
chunking; the transformation stage applies 
simplification rules, producing simplified versions 
of the sentences; the regeneration stage performs 
operations on the simplified sentences to make them 
readable, like referring expressions generation, cue 
words rearrangement, and sentence ordering. 
Differently from such architecture, currently our 
regeneration stage only includes the treatment of 
cue words and a surface forms (GSF) generator, 
which is used to adjust the verb conjugation and 
regency after some simplification operations. 
As a single sentence may contain more than 
one complex linguistic phenomenon, simplification 
operations are applied in cascade to a sentence, as 
described in what follows. 
3.1 Simplification cases and operations 
As result of a study on which linguistic phenomena 
make BP text complex to read and how these 
phenomena could be simplified, we elaborated a 
manual of BP syntactic simplification (Aluisio et al, 
2008). The rule-based text simplification system 
                                                 
4
 http://www.linguatechnologies.com/english/home.html  
5
 http://www.editorsoftware.com/writing-software 
developed here is based on the specifications in this 
manual. According to this manual, simplification 
operations should be applied when any of the 22 
linguistic phenomena presented in Table 1 is 
detected. 
The possible operations suggested to be applied 
in order to simplify these phenomena are: (a) split 
the sentence, (b) change a discourse marker by a 
simpler and/or more frequent one (the indication is 
to avoid the ambiguous ones), (c) change passive to 
active voice, (d) invert the order of the clauses, (e) 
convert to subject-verb-object ordering, (f) change 
topicalization and detopicalization of adverbial 
phrases and (g) non-simplification.  
Table 1 shows the list of all simplification 
phenomena covered by our manual, the clues used 
to identify the phenomena, the simplification 
operations that should be applied in each case, the 
expected order of clauses in the resulting sentence, 
and the cue phrases (translated here from 
Portuguese) used to replace complex discourse 
markers or to glue two sentences. In column 2, we 
consider the following clues: syntactic information 
(S), punctuation (P), and lexicalized clues, such as 
conjunctions (Cj), relative pronouns (Pr) and 
discourse markers (M), and semantic information 
(Sm, and NE for named entities). 
3.2 Identifying simplification cases and 
applying simplification rules 
Each sentence is parsed in order to identify cases for 
simplification. We use parser PALAVRAS (Bick, 
2000) for Portuguese. This parser provides lexical 
information (morphology, lemma, part-of-speech, 
and semantic information) and the syntactic trees for 
each sentence. For some operations, surface 
information (such as punctuation or lexicalized cue 
phrases) is used to identify the simplification cases, 
as well as to assist simplification process. For 
example, to detect and simplify subjective non-
restrictive relative clauses (where the relative 
pronoun is the subject of the relative clause), the 
following steps are performed: 
1. The presence of a relative pronoun is verified. 
2. Punctuation is verified in order to distinguish it 
from restrictive relative clauses: check if the 
pronoun occurs after a comma or semicolon.  
3. Based on the position of the pronoun, the next 
punctuation symbol is searched to define the 
boundaries of the relative clause. 
36
4. The first part of the simplified text is generated, 
consisting of the original sentence without the 
embedded relative clause. 
5. The noun phrase in the original sentence to 
which the relative clause refers is identified. 
6. A second simplified sentence is generated, 
consisting of the noun phrase (as subject) and 
the relative clause (without the pronoun). 
The identification of the phenomena and the 
application of the operations are prone to errors 
though. Some of the clues that indicate the 
occurrence of the phenomena may be ambiguous. 
For example, some of the discourse markers that are 
used to identify subordinate clauses can indicate 
more than one type of these: for instance, ?como? 
(in English ?like?, ?how? or ?as?) can indicate 
reason, conformative or concessive subordinate 
clauses. Since there is no other clue that can help us 
disambiguate among those, we always select the 
case that occurs more frequently according to a 
corpus study of discourse markers and the rhetoric 
relations that they entitle (Pardo and Nunes, 2008). 
However, we can also treat all cases and let the user 
decide the simplifications that is most appropriate. 
 
Phenomenon Clues Op Clause Order Cue phrase Comments 
1.Passive voice S c   Verb may have to be adapted 
2.Embedded appositive S a Original/ 
App. 
 Appositive: Subject is the head of original + 
to be in present tense + apposition 
3.Asyndetic coordinate clause S a Keep order   New sentences: Subjects are the head of the 
original subject 
4.Additive coordinate clause S, Cj a Keep order Keep marker Marker appears in the beginning of the new 
sentence 
5.Adversative coordinate clause M a, b Keep order But  
6.Correlated coordinate clause M a, b Keep order Also Original markers disappear 
7.Result coordinate clause S, M a, b Keep order As a result  
8.Reason coordinate clause S, M a, b Keep order This happens 
because 
May need some changes in verb 
9.Reason subordinate clause M a, b, 
d 
Sub/Main With this To keep the ordering cause, result 
M a, b Main/Sub Also Rule for such ... as, so ... as markers  10.Comparative subordinate clause 
M g   Rule for the other markers or short sentences 
M a, b, 
d 
Sub/Main But ?Clause 1 although clause 2? is changed to 
?Clause 2. But clause 1? 
11.Concessive subordinate clause 
M a, b Main/Sub This happens 
even if 
Rule for hypothetical sentences 
12.Conditional subordinate clause S, M d Sub/Main  Pervasive use in simple accounts 
13. Result subordinate clause M a, b Main/Sub Thus May need some changes in verb 
14.Final/Purpose subordinate clause S, M a, b Main/Sub The goal is  
15.Confirmative subordinate clause M a, b, 
d 
Sub/Main Confirms 
that 
May need some changes in verb 
M a Sub/Main  May need some changes in verb 16.Time subordinate clause 
M a, b  Then Rule for markers: after that, as soon as  
17. Proportional Subordinate Clause M g    
18. Non-finite subordinate clause S g    
19.Non-restrictive relative clause S, P, Pr a Original/ 
Relative 
 Relative: Subject is the head of original + 
relative (subjective relative clause) 
20.Restrictive relative clause S, Pr a Relative/ 
Original 
 Relative: Subject is the head of original + 
relative  (subjective relative clause) 
21.Non Subject-Verb-Object order S e   Rewrite in Subject-Verb-Object order 
22. Adverbial phrases in theme 
position 
S, NE, 
Sm  
f In study  In study 
Table 1: Cases, operations, order and cue phrases 
Every phenomenon has one or more 
simplification steps associated with it, which are 
applied to perform the simplification operations. 
Below we detail each operation and discuss the 
challenges involved and our current limitations in 
their implementing. 
a) Splitting the sentence - This operation is the 
most frequent one. It requires finding the split point 
37
in the original sentence (such as the boundaries of 
relative clauses and appositions, the position of 
coordinate or subordinate conjunctions) and the 
creation of a new sentence, whose subject 
corresponds to the replication of a noun phrase in 
the original sentence. This operation increases the 
text length, but decreases the length of the 
sentences. With the duplication of the term from the 
original sentence (as subject of the new sentence), 
the resulting text contains redundant information, 
but it is very helpful for people at the rudimentary 
literacy level. 
When splitting sentences due to the presence of 
apposition, we need to choose the element in the 
original sentence to which it is referring, so that this 
element can be the subject of the new sentence. At 
the moment we analyze all NPs that precede the 
apposition and check for gender and number 
agreement. If more than one candidate passes the 
agreement test, we choose the closest one among 
these; if none does, we choose the closest among all 
candidates. In both cases we can also pass the 
decision on to the user, which we do in our 
authoring tool described in Section 4. 
For treating relative clauses we have the same 
problem as for apposition (finding the NP to which 
the relative clause is anchored) and an additional 
one: we need to choose if the referent found should 
be considered the subject or the object of the new 
sentence. Currently, the parser indicates the 
syntactic function of the relative pronoun and that 
serves as a clue. 
b) Changing discourse marker - In most cases 
of subordination and coordination, discourse 
markers are replaced by most commonly used ones, 
which are more easily understood. The selection of 
discourse markers to be replaced and the choice of 
new markers (shown in Table 1, col. 4) are done 
based on the study of Pardo and Nunes (2008). 
c) Transformation to active voice - Clauses in 
the passive voice are turned into active voice, with 
the reordering of the elements in the clause and the 
modification of the tense and form of the verb. Any 
other phrases attached to the object of the original 
sentence have to be carried with it when it moves to 
the subject position, since the voice changing 
operation is the first to be performed. For instance, 
the sentence: 
?More than 20 people have been bitten by gold piranhas 
(Serrasalmus Spilopleura), which live in the waters of the 
Sanchuri dam, next to the BR-720 highway, 40 km from 
the city.? 
is simplified to: 
?Gold piranhas (Serrasalmus Spilopleura), which live in 
the waters of the Sanchuri dam, next to the BR-720 
highway, 40 km from the city, have bitten more than 20 
people.? 
After simplification of the relative clause and 
apposition, the final sentence is: 
?Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. Gold piranhas 
are Serrasalmus Spilopleura.? 
d) Inversion of clause ordering - This operation 
was primarily designed to handle subordinate 
clauses, by moving the main clause to the beginning 
of the sentence, in order to help the reader 
processing it on their working memory (Graesser et 
al., 2004). Each of the subordination cases has a 
more appropriate order for main and subordinate 
clauses (as shown in Table 1, col. 3), so that 
?independent? information is placed before the 
information that depends on it. In the case of 
concessive subordinate clauses, for example, the 
subordinate clause is placed before the main clause. 
This gives the sentence a logical order of the 
expressed ideas. See the example below, in which 
there is also a change of discourse marker and 
sentence splitting, all operations assigned to 
concessive subordinate clauses:  
?The building hosting the Brazilian Consulate was also 
evacuated, although the diplomats have obtained 
permission to carry on working.? 
Its simplified version becomes:  
?The diplomats have obtained permission to carry on 
working. But the building hosting the Brazilian Consulate 
was also evacuated.? 
e) Subject-Verb-Object ordering - If a sentence 
is not in the form of subject-verb-object, it should be 
rearranged. This operation is based only on 
information from the syntactic parser. The example 
below shows a case in which the subject is after the 
verb (translated literally from Portuguese, 
preserving the order of the elements): 
?On the 9th of November of 1989, fell the wall that for 
almost three decades divided Germany.? 
Its simplified version is: 
?On the 9th of November of 1989, the wall that for almost 
three decades divided Germany fell.? 
Currently the only case we are treating is the non-
canonical order Verb-Object-Subject. We plan to 
treat other non-canonical orderings in the near 
future. Besides that, we still have to define how to 
deal with elliptic subjects and impersonal verbs 
(which in Portuguese do not require a subject). 
38
When performing this operation and the previous 
one, a generator of surface forms (GSF) is used to 
adjust the verb conjugation and regency. The GSF is 
compiled from the Apertium morphological 
dictionaries enhanced with the entries of Unitex-BP 
(Muniz et al, 2005), with an extra processing to 
map the tags of the parser to those existing in 
morphological dictionaries (Caseli et al, 2007) to 
obtain an adjusted verb in the modified sentence. 
f) Topicalization and detopicalization - This 
operation is used to topicalize or detopicalize an 
adverbial phrase. We have not implemented this 
operation yet, but have observed that moving 
adverbial phrases to the end or to the front of 
sentences can make them simpler in some cases. For 
instance, the sentence in the last example would 
become: 
?The wall that for almost three decades divided Germany fell 
on the 9th of November of 1989.? 
We are still investigating how this operation 
could be applied, that is, which situations require 
(de)topicalization. 
3.3 The cascaded application of the rules 
As previously mentioned, one sentence may contain 
several phenomena that could be simplified, and we 
established the order in which they are treated. The 
first phenomenon to be treated is passive voice. 
Secondly, embedded appositive clauses are 
resolved, since they are easy to simplify and less 
prone to errors. Thirdly, subordinate, non-restrictive 
and restrictive relative clauses are treated, and only 
then the coordinate clauses are dealt with.  
As the rules were designed to treat each case 
individually, it is necessary to apply the operations 
in cascade, in order to complete the simplification 
process for each sentence. At each iteration, we (1) 
verify the phenomenon to be simplified following 
the standard order indicated above; (2) when a 
phenomenon is identified, its simplification is 
executed; and (3) the resulting simplified sentence 
goes through a new iteration. This process continues 
until there are no more phenomena. The cascade 
nature of the process is crucial because the 
simplified sentence presents a new syntactic 
structure and needs to be reparsed, so that the 
further simplification operations can be properly 
applied. However, this process consumes time and 
is considered the bottleneck of the system.  
3.4 Simplification evaluation 
We have so far evaluated the capacity of our rule-
based simplifier to identify the phenomena present 
in each sentence, and to recommend the correct 
simplification operation. We compared the 
operations recommended by the system with the 
ones performed manually by an annotator in a 
corpus of 104 news articles from the Zero Hora 
newspaper, which can be seen in our Portal of 
Parallel Corpora of Simplified Texts6. Table 2 
presents the number of occurrences of each 
simplification operation in this corpus. 
Simplification Operations # Sentences 
Non-simplification 2638 
Subject-verb-object ordering 44 
Transformation to active voice 154 
Inversion of clause ordering 265 
Splitting sentences 1103 
Table 2. Statistics on the simplification operations 
The performance of the system for this task is 
presented in Table 3 in terms of precision, recall, 
and F-measure for each simplification operation.  
Operation P R F 
Splitting sentences 64.07 82.63 72.17 
Inversion of clause ordering 15.40 18.91 16.97 
Transformation to active voice 44.29 44.00 44.14 
Subject-verb-object ordering 1.12 4.65 1.81 
ALL 51.64 65.19 57.62 
Non-simplification 64.69 53.58 58.61 
Table 3. Performance on defining simplification 
operations according to syntactic phenomena 
These results are preliminary, since we are still 
refining our rules. Most of the recall errors on the 
inversion of clause ordering are due to the absence 
of a few discourse markers in the list of markers that 
we use to identify such cases. The majority of recall 
errors on sentence splitting are due to mistakes on 
the output of the syntactic parser and to the number 
of ordering cases considered and implemented so 
far. The poor performance for subject-verb-object 
ordering, despite suffering from mistakes of the 
parser, indicates that our rules for this operation 
need to be refined. The same applies to inversion of 
clause ordering. 
We did not report performance scores related to 
the ?changing discourse marker? operation because 
in our evaluation corpus this operation is merged 
with other types of lexical substitution. However, in  
                                                 
6
 http://caravelas.icmc.usp.br/portal/index.php 
39
order to assess if the sentences were correctly 
simplified, it is necessary to do a manual evaluation, 
since it is not possible to automatically compare the 
output of the rule-based simplifier with the 
annotated corpus, as the sentences in the corpus 
have gone through operations that are not performed 
by the simplifier (such as lexical substitution). We 
are in the process of performing such manual 
evaluation. 
4 Simplifica editor: supporting authors 
We developed Simplifica7 (Figure 1), an authoring 
system to help writers to produce simplified texts. It 
employs the simplification technology described in 
the previous section. It is a web-based WYSIWYG 
editor, based on TinyMCE web editor8.  
The user inputs a text in the editor, customizes 
the simplification settings where one or more 
simplifications can be chosen to be applied in the 
text and click on the ?simplify? button. This triggers 
the syntactic simplification system, which returns an 
XML file containing the resulting text and tags 
indicating the performed simplification operations. 
After that, the simplified version of the text is 
shown to the user, and he/she can revise the 
automatic simplification. 
4.1 The XML representation of simplification 
operations 
Our simplification system generates an XML file 
                                                 
7
 http://www.nilc.icmc.usp.br/porsimples/simplifica/ 
8
 http://tinymce.moxiecode.com/ 
describing all simplification operations applied to a 
text. This file can be easily parsed using standard 
XML parsers. Table 5 presents the XML annotation 
to the ?gold piranhas? example in Section 3.2. 
  
<simplification type="passive"> 
<simplification type="appositive"> 
<simplification type="relative"> 
Gold piranhas have bitten more than 20 people. Gold 
piranhas live in the waters of the Sanchuri dam, next to 
the BR-720 highway, 40 km from the city. 
</simplification> 
Gold piranhas are Serrasalmus Spilopleura. 
</simplification> 
</simplification> 
Table 5. XML representation of a simplified text 
In our annotation, each sentence receives a 
<simplification> tag which describes the simplified 
phenomena (if any); sentences that did not need 
simplification are indicated with a <simplification 
type=?no?> tag. The other simplification types refer 
to the eighteen simplification cases presented in 
Table 1. Nested tags indicate multiple operations 
applied to the same sentence. 
4.2 Revising the automatic simplification 
Once the automatic simplification is done, a review 
screen shows the user the simplified text so that 
he/she can visualize all the modifications applied 
and approve or reject them, or select alternative 
simplifications. Figure 1 shows the reviewing screen 
and a message related to the simplification 
performed below the text simplified. 
The user can revise simplified sentences one at a 
time; the selected sentence is automatically 
highlighted. The user can accept or reject a 
 
Figure 1: Interface of the Simplifica system 
40
simplified sentence using the buttons below the text. 
In the beginning of the screen ?Mais op??es?, 
alternative simplifications for the sentence are 
shown: this facility gives the user the possibility to 
resolve cases known to be ambiguous (as detailed in 
Sections 2 and 3.2) for which the automatic 
simplification may have made a mistake. In the 
bottom of the same screen we can see the original 
sentence (?Senten?a original?) to which the 
highlighted sentence refers.  
For the example in Figure 1, the tool presents 
alternative simplifications containing different 
subjects, since selecting the correct noun phrase to 
which an appositive clause was originally linked 
(which becomes the subject of the new sentence) 
based on gender and number information was not 
possible.  
At the end of the process, the user returns to the 
initial screen and can freely continue editing the text 
or adding new information to it. 
5 Text Simplification for education 
Text simplification can be used in several 
applications. Journalists can use it to write simple 
and straightforward news texts. Government 
agencies can create more accessible texts to a large 
number of people. Authors of manuals and technical 
documents can also benefit from the simplification 
technology. Simplification techniques can also be 
used in an educational setting, for example, by a 
teacher who is creating simplified texts to students. 
Classic literature books, for example, can be quite 
hard even to experienced readers. Some genres of 
texts already have simplified versions, even though 
the simplification level can be inadequate to a 
specific target audience. For instance, 3rd and 7th 
grade students have distinct comprehension levels. 
In our approach, the number and type of 
simplification operations applied to sentences 
determine its appropriateness to a given literacy 
level, allowing the creation of multiple versions of 
the same text, with different levels of complexity, 
targeting special student needs. 
The Simplifica editor allows the teacher to adopt 
any particular texts to be used in the class, for 
example, the teacher may wish to talk about current 
news events with his/her students, which would not 
be available via any repository of simplified texts. 
The teacher can customize the text generating 
process and gradually increase the text complexity 
as his/her students comprehension skills evolve. The 
use of the editor also helps the teacher to develop a 
special awareness of the language, which can 
improve his/her interaction with the students.  
Students can also use the system whenever they 
have difficulties to understand a text given in the 
classroom. After a student reads the simplified text, 
the reading of the original text becomes easier, as a 
result of the comprehension of the simplified text. In 
this scenario, reading the original text can also help 
the students to learn new and more complex words 
and syntactic structures, which would be harder for 
them without reading of the simplified text. 
6 Conclusions 
The potentialities of text simplification systems for 
education are evident. For students, it is a first step 
for more effective learning. Under another 
perspective, given the Brazilian population literacy 
levels, we consider text simplification a necessity. 
For poor literacy people, we see text simplification 
as a first step towards social inclusion, facilitating 
and developing reading and writing skills for people 
to interact in society. The social impact of text 
simplification is undeniable. 
In terms of language technology, we not only 
introduced simplification tools in this paper, but also 
investigated which linguistic phenomena should be 
simplified and how to simplify them. We also 
developed a representation schema and designed an 
on-line authoring system. Although some aspects of 
the research are language dependent, most of what 
we propose may be adapted to other languages. 
Next steps in this research include practical 
applications of such technology and the 
measurement of its impact for both education and 
social inclusion. 
Acknowledgments 
We thank the Brazilian Science Foundation FAPESP 
and Microsoft Research for financial support. 
References 
Alu?sio, S.M., Specia, L., Pardo, T.A.S., Maziero, E.G., 
Fortes, R. 2008. Towards Brazilian Portuguese 
Automatic Text Simplification Systems. In the 
Proceedings of the 8th ACM Symposium on Document 
Engineering, pp. 240-248. 
Bick, E. 2000. The parsing system ?Palavras?: 
41
Automatic grammatical analysis of Portuguese in a 
constraint grammar framework. PhD Thesis 
University of ?rhus, Denmark. 
Burstein, J. 2009. Opportunities for Natural Language 
Processing Research in Education. In the  Proceedings 
of CICLing, pp. 6-27.  
Caseli, H., Pereira, T.F., Specia, L., Pardo, T.A.S., 
Gasperin, C., Aluisio, S. 2009. Building a Brazilian 
Portuguese Parallel Corpus of Original and Simplified 
Texts. In the Proceedings of CICLing. 
Caseli, H.M.; Nunes, M.G.V.; Forcada, M.L. 2008. 
Automatic induction of bilingual resources from 
aligned parallel corpora: application to shallow-
transfer machine translation. Machine Translation, V. 
1, p. 227-245. 
Devlin, S., Unthank, G. 2006. Helping aphasic people 
process online information. In the Proceedings of the 
ACM SIGACCESS Conference on Computers and 
Accessibility, pp. 225-226. 
Graesser, A., McNamara, D. S., Louwerse, M., Cai, Z. 
2004. Coh-Metrix: Analysis of text on cohesion and 
language. Behavioral Research Methods, Instruments, 
and Computers, V. 36, pp. 193-202. 
Inui, K., Fujita, A., Takahashi, T., Iida, R., Iwakura, T. 
2003. Text Simplification for Reading Assistance: A 
Project Note. In the Proceedings of the Second 
International Workshop on Paraphrasing, 9 -16.  
Klebanov, B., Knight, K., Marcu, D. 2004. Text 
Simplification for Information-Seeking Applications. 
On the Move to Meaningful Internet Systems. LNCS, 
V.. 3290, pp. 735-747. 
Lal, P., Ruger, S. 2002. Extract-based summarization with 
simplification. In the Proceedings of DUC.  
Mapleson, D.L. 2006. Post-Grammatical Processing for 
Discourse Segmentation. PhD Thesis. School of 
Computing Sciences, University of East Anglia, 
Norwich. 
Max, A. 2005. Simplification interactive pour la 
production de textes adapt es aux personnes souffrant 
de troubles de la compr ehension. In the Proceedings 
of Traitement Automatique des Langues Naturelles 
(TALN).  
Max, A. 2006. Writing for language-impaired readers. In 
the  Proceedings of CICLing, pp. 567-570.  
Muniz, M.C., Laporte, E. Nunes, M.G.V. 2005. UNITEX-
PB, a set of flexible language resources for Brazilian 
Portuguese. In Anais do III Workshop em Tecnologia 
da Informa??o e da Linguagem Humana, V. 1, pp. 1-
10. 
Pardo, T.A.S.  and Nunes, M.G.V. 2008. On the 
Development and Evaluation of a Brazilian 
Portuguese Discourse Parser. Journal of Theoretical 
and Applied Computing, V. 15, N. 2, pp. 43-64. 
Petersen, S.E. 2007. Natural Language Processing Tools 
for Reading Level Assessment and Text Simplification 
for Bilingual Education. PhD Thesis, University of 
Washington.  
Petersen, S.E. and Ostendorf, M. 2007. Text 
Simplification for Language Learners: A Corpus 
Analysis. In the Proceedings of the Speech and 
Language Technology for Education Workshop, pp. 
69-72. 
Specia, L., Alu?sio, S.M., Pardo, T.A.S. 2008. Manual de 
simplifica??o sint?tica para o portugu?s. Technical 
Report NILC-TR-08-06, NILC. 
Siddharthan, A. 2002. An Architecture for a Text 
Simplification System. In the Proceedings of the 
Language Engineering Conference, pp. 64-71. 
Siddharthan, A. 2003. Syntactic Simplification and Text 
Cohesion. PhD Thesis. University of Cambridge. 
Snow, C. 2002. Reading for understanding: Toward an 
R&D program in reading comprehension. Santa 
Monica, CA. 
 
42
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 41?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
SIMPLIFICA: a tool for authoring simplified texts in  
Brazilian Portuguese guided by readability assessments 
 
 
Carolina  Scarton, Matheus de Oliveira,  Arnaldo Candido Jr.,  
Caroline Gasperin and Sandra Maria Alu?sio  
Department of Computer Sciences, University of S?o Paulo 
Av. Trabalhador S?o-Carlense, 400. 13560-970 - S?o Carlos/SP, Brazil 
{carolina@grad,matheusol@grad,arnaldoc@,cgasperin@,sandra@}icmc.usp.br 
 
  
 
 
Abstract 
SIMPLIFICA is an authoring tool for produc-
ing simplified texts in Portuguese. It provides 
functionalities for lexical and syntactic simpli-
fication and for readability assessment. This 
tool is the first of its kind for Portuguese; it 
brings innovative aspects for simplification 
tools in general, since the authoring process is 
guided by readability assessment based on the 
levels of literacy of the Brazilian population. 
1 Introduction 
In order to promote digital inclusion and accessi-
bility for people with low levels of literacy, partic-
ularly access to documents available on the web, it 
is important to provide textual information in a 
simple and easy way. Indeed, the Web Content 
Accessibility Guidelines (WCAG) 2.01 establishes 
a set of guidelines that discuss accessibility issues 
and provide accessibility design solutions. WCAG 
requirements address not only structure and tech-
nological aspects, but also how the content should 
be made available to users. However, Web devel-
opers are not always responsible for content prepa-
ration and authoring in a Website. Moreover, in the 
context of Web 2.0 it becomes extremely difficult 
to develop completely WCAG conformant Web-
sites, since users without any prior knowledge 
about the guidelines directly participate on the con-
tent authoring process of Web applications.  
                                                        
1 http://www.w3.org/TR/WCAG20/ 
In Brazil, since 2001, the INAF index (National 
Indicator of Functional Literacy) has been com-
puted annually to measure the levels of literacy of 
the Brazilian population. The 2009 report pre-
sented a still worrying scenario: 7% of the individ-
uals were classified as illiterate; 21% as literate at 
the rudimentary level; 47% as literate at the basic 
level; and only 25% as literate at the advanced lev-
el (INAF, 2009). These literacy levels are defined 
as: (1) Illiterate: individuals who cannot perform 
simple tasks such as reading words and phrases; 
(2) Rudimentary: individuals who can find expli-
cit information in short and familiar texts (such as 
an advertisement or a short letter); (3) Basic: indi-
viduals who can read and understand texts of aver-
age length, and find information even when it is 
necessary to make some inference; and (4) Ad-
vanced/Fully: individuals who can read longer 
texts, relating their parts, comparing and interpret-
ing information, distinguish fact from opinion, 
make inferences and synthesize. 
We present in this paper the current version of 
an authoring tool named SIMPLIFICA. It helps 
authors to create simple texts targeted at poor lite-
rate readers. It extends the previous version pre-
sented in Candido et al (2009) with two new mod-
ules: lexical simplification and the assessment of 
the level of complexity of the input texts. The 
study is part of the PorSimples project2 (Simplifi-
cation of Portuguese Text for Digital Inclusion and 
Accessibility) (Aluisio et al, 2008).  
This paper is organized as follows. In Section 2 
                                                        
2 http://caravelas.icmc.usp.br/wiki/index.php/Principal 
41
we describe SIMPLIFICA and the underlying 
technology for lexical and syntactic simplification, 
and for readability assessment. In Section 3 we 
summarize the interaction steps that we propose to 
show in the demonstration session targeting texts 
for low-literate readers of Portuguese. Section 4 
presents final remarks with emphasis on why de-
monstrating this system is relevant.  
2 SIMPLIFICA authoring tool  
SIMLIFICA is a web-based WYSIWYG editor, 
based on TinyMCE web editor3. The user inputs a 
text in the editor and customizes the simplification 
settings, where he/she can choose: (i) strong sim-
plification, where all the complex syntactic phe-
nomena (see details in Section 2.2) are treated for 
each sentence, or customized simplification, where 
the user chooses one or more syntactic simplifica-
tion phenomena to be treated for each sentence, 
and (ii) one or more thesauri to be used in the syn-
tactic and lexical simplification processes. Then 
the user activates the readability assessment mod-
ule to predict the complexity level of a text. This 
module maps the text to one of the three levels of 
literacy defined by INAF: rudimentary, basic or 
advanced. According to the resulting readability 
level the user can trigger the lexical and/or syntac-
tic simplifications modules, revise the automatic 
simplification and restart the cycle by checking the 
readability level of the current version of the text.  
Figure 1 summarizes how the three modules are 
integrated and below we describe in more detail 
the SIMPLIFICA modules. 
 
Figure 1. Steps of the authoring process. 
 
                                                        
3 http://tinymce.moxiecode.com/ 
2.1 Lexical Simplification  
Basically, the first part of the lexical simplification 
process consists of tokenizing the original text and 
marking the words that are considered complex. In 
order to judge a word as complex or not, we use 3 
dictionaries created for the PorSimples project: one 
containing words common to youngsters, a second 
one composed by frequent words extracted from 
news texts for children and nationwide newspa-
pers, and a third one containing concrete words.  
The lexical simplification module also uses the 
Unitex-PB dictionary4 for finding the lemma of the 
words in the text, so that it is possible to look for it 
in the simple words dictionaries. The problem of 
looking for a lemma directly in a dictionary is that 
there are ambiguous words and we are not able to 
deal with different word senses. For dealing with 
part-of-speech (POS) ambiguity, we use the 
MXPOST POS tagger5 trained over NILC tagset6. 
After the text is tagged, the words that are not 
proper nouns, prepositions and numerals are se-
lected, and their POS tags are used to look for their 
lemmas in the dictionaries. As the tagger has not a 
100% precision and some words may not be in the 
dictionary, we look for the lemma only (without 
the tag) when we are not able to find the lemma-
tag combination in the dictionary. Still, if we are 
not able to find the word, the lexical simplification 
module assumes that the word is complex and 
marks it for simplification. 
The last step of the process consists in providing 
simpler synonyms for the marked words. For this 
task, we use the thesauri for Portuguese TeP 2.07 
and the lexical ontology for Portuguese PAPEL8. 
This task is carried out when the user clicks on a 
marked word, which triggers a search in the the-
sauri for synonyms that are also present in the 
common words dictionary. If simpler words are 
found, they are listed in order, from the simpler to 
the more complex ones. To determine this order, 
we used Google API to search each word in the 
web: we assume that the higher a word frequency, 
the simpler it is. Automatic word sense disambigu-
ation is left for future work. 
                                                        
4 http://www.nilc.icmc.usp.br/nilc/projects/unitex-pb/web 
/dicionarios.html 
5 http://sites.google.com/site/adwaitratnaparkhi/home 
6 www.nilc.icmc.usp.br/nilc/TagSet/ManualEtiquetagem.htm  
7 http://www.nilc.icmc.usp.br/tep2/ 
8 http://www.linguateca.pt/PAPEL/ 
42
2.2 Syntactic Simplification 
Syntactic simplification is accomplished by a rule-
based system, which comprises seven operations 
that are applied sentence-by-sentence to a text in 
order to make its syntactic structure simpler.  
Our rule-based text simplification system is 
based on a manual for Brazilian Portuguese syntac-
tic simplification (Specia et al, 2008). According 
to this manual, simplification operations should be 
applied when any of the 22 linguistic phenomena 
covered by our system (see Candido et al (2009) 
for details) is detected. Our system treats apposi-
tive, relative, coordinate and subordinate clauses, 
which had already been addressed by previous 
work on text simplification (Siddharthan, 2003). 
Additionally, we treat passive voice, sentences in 
an order other than Subject-Verb-Object (SVO), 
and long adverbial phrases. The simplification op-
erations available to treat these phenomena are: 
split sentence, change particular discourse markers 
by simpler ones, change passive to active voice, 
invert the order of clauses, convert to subject-verb-
object ordering, and move long adverbial phrases. 
Each sentence is parsed in order to identify syn-
tactic phenomena for simplification and to segment 
the sentence into portions that will be handled by 
the operations. We use the parser PALAVRAS 
(Bick, 2000) for Portuguese. Gasperin et al (2010) 
present the evaluation of the performance of our 
syntactic simplification system.  
Since our syntactic simplifications are conserva-
tive, the simplified texts become longer than the 
original ones due to sentence splitting. We ac-
knowledge that low-literacy readers prefer short 
texts, and in the future we aim to provide summa-
rization within SIMPLIFICA (see (Watanabe et al, 
2009)). Here, the shortening of the text is a respon-
sibility of the author. 
2.3 Readability assessment 
With our readability assessment module, we can 
predict the readability level of a text, which cor-
responds to the literacy level expected from the 
target reader: rudimentary, basic or advanced.  
We have adopted a machine-learning classifier 
to identify the level of the input text; we use the 
Support Vector Machines implementation from 
Weka9 toolkit (SMO). We have used 7 corpora 
                                                        
9 http://www.cs.waikato.ac.nz/ml/weka/ 
within 2 different genres (general news and popu-
lar science articles) to train the classifier. Three of 
these corpora contain original texts published in 
online newspapers and magazines. The other cor-
pora contain manually simplified versions of most 
of the original texts. These were simplified by a 
linguist, specialized in text simplification, accord-
ing to the two levels of simplification proposed in 
our project, natural and strong, which result in 
texts adequate for the basic and rudimentary litera-
cy levels, respectively. 
Our feature set is composed by cognitively-
motivated features derived from the Coh-Metrix-
PORT tool10, which is an adaptation for Brazilian 
Portuguese of Coh-Metrix 2.0 (free version of 
Coh-Metrix (Graesser et al 2003)) also developed 
in the context of the PorSimples project. Coh-
Metrix-PORT implements the metrics in Table 1. 
 
Categories Subcategories Metrics 
Shallow 
Readabili-
ty metric 
- Flesch Reading Ease index 
for Portuguese. 
Words and 
textual 
informa-
tion 
Basic counts Number of words, sen-
tences, paragraphs, words 
per sentence, sentences per 
paragraph, syllables per 
word, incidence of verbs, 
nouns, adjectives and ad-
verbs. 
Frequencies Raw frequencies of content 
words and minimum fre-
quency of content words. 
Hyperonymy Average number of hyper-
nyms of verbs. 
Syntactic 
informa-
tion 
Constituents Incidence of nominal 
phrases, modifiers per noun 
phrase and words preced-
ing main verbs. 
Pronouns, 
Types and 
Tokens 
Incidence of personal pro-
nouns, number of pronouns 
per noun phrase, types and 
tokens. 
Connectives Number of connectives, 
number of positive and 
negative additive connec-
tives, causal / temporal / 
logical positive and nega-
tive connectives. 
Logical 
operators 
- Incidence of the particles 
?e? (and), ?ou? (or), ?se? 
(if), incidence of negation 
and logical operators. 
Table 1. Metrics of Coh-Metrix-PORT. 
 
                                                        
10 http://caravelas.icmc.usp.br:3000/ 
43
We also included seven new metrics to Coh-
Metrix-PORT: average verb, noun, adjective and 
adverb ambiguity, incidence of high-level constitu-
ents, content words and functional words.  
We measured the performance of the classifier 
on identifying the levels of the input texts by a 
cross-validation experiment. We trained the clas-
sifier on our 7 corpora and reached 90% F-measure 
on identifying texts at advanced level, 48% at basic 
level, and 73% at rudimentary level. 
 
3. A working session at SIMPLIFICA  
 
In the NAACL demonstration section we aim to 
present all functionalities of the tool for authoring 
simple texts, SIMPLIFICA. We will run all steps 
of the authoring process ? readability assessment, 
lexical simplification and syntactic simplification ? 
in order to demonstrate the use of the tool in pro-
ducing a text for basic and rudimentary readers of 
Portuguese, regarding the lexical and the syntactic 
complexity of an original text.  
We outline a script of our demonstration at 
http://www.nilc.icmc.usp.br/porsimples/demo/dem
o_script.htm. In order to help the understanding by 
non-speakers of Portuguese we provide the transla-
tions of the example texts shown. 
 
4. Final Remarks 
 
A tool for authoring simple texts in Portuguese is 
an innovative software, as are all the modules that 
form the tool. Such tool is extremely important in 
the construction of texts understandable by the ma-
jority of the Brazilian population. SIMPLIFICA?s 
target audience is varied and includes: teachers that 
use online text for reading practices; publishers; 
journalists aiming to reach poor literate readers; 
content providers for distance learning programs; 
government agencies that aim to communicate to 
the population as a whole; companies that produce 
technical manuals and medicine instructions; users 
of legal language, in order to facilitate the under-
standing of legal documents by lay people; and 
experts in language studies and computational lin-
guistics for future research. 
Future versions of SIMPLIFICA will also pro-
vide natural simplification, where the target sen-
tences for simplifications are chosen by a machine 
learning classifier (Gasperin et al, 2009). 
 
Acknowledgments 
We thank FAPESP and Microsoft Research for 
supporting the PorSimples project 
References  
Sandra Alu?sio, Lucia Specia, Thiago Pardo, Erick Ma-
ziero and Renata Fortes. 2008. Towards Brazilian 
Portuguese Automatic Text Simplification Systems. In 
Proceedings of The Eight ACM Symposium on Doc-
ument Engineering (DocEng 2008),  240-248, S?o 
Paulo, Brasil. 
Eckhard Bick. 2000. The Parsing System "Palavras": 
Automatic Grammatical Analysis of Portuguese in a 
Constraint Grammar Framework. PhD thesis. Aa-
rhus University. 
Arnaldo Candido Junior, Erick Maziero, Caroline Gas-
perin, Thiago Pardo, Lucia Specia and Sandra M. A-
luisio. 2009. Supporting the Adaptation of Texts for 
Poor Literacy Readers: a Text Simplification Editor 
for Brazilian Portuguese. In the Proceedings of the 
NAACL HLT Workshop on Innovative Use of NLP 
for Building Educational Applications, pages 34?42, 
Boulder, Colorado, June 2009. 
Caroline Gasperin; Lucia Specia; Tiago Pereira and  
Sandra Alu?sio. 2009. Learning When to Simplify 
Sentences for Natural Text Simplification. In: Pro-
ceedings of ENIA 2009,  809-818. 
Caroline Gasperin, Erick Masiero and Sandra M. Alui-
sio. 2010. Challenging choices for text simplifica-
tion. Accepted for publication in Propor 2010 
(http://www.inf.pucrs.br/~propor2010/). 
Arthur Graesser, Danielle McNamara, Max  Louwerse 
and Zhiqiang Cai. 2004. Coh-Metrix: Analysis of 
text on cohesion and language. In: Behavioral Re-
search Methods, Instruments, and Computers, 36, 
p?ginas 193-202. 
INAF. 2009. Instituto P. Montenegro and A??o Educa-
tiva. INAF Brasil - Indicador de Alfabetismo Funcio-
nal - 2009. Online available at http://www.ibope. 
com.br/ipm/relatorios/relatorio_inaf_2009.pdf  
Advaith Siddharthan. 2003. Syntactic Simplification and 
Text Cohesion. PhD Thesis. University of 
Cambridge. 
Lucia Specia, Sandra Aluisio and Tiago Pardo. 2008. 
Manual de Simplifica??o Sint?tica para o Portugu?s. 
Technical Report NILC-TR-08-06, 27 p. Junho 2008, 
S?o Carlos-SP. 
Willian Watanabe, Arnaldo Candido Junior, Vin?cius 
Uz?da, Renata Fortes, Tiago Pardo and Sandra Alu?-
sio. 2009. Facilita: reading assistance for low-
literacy readers. In Proceedings of the 27th ACM In-
ternational Conference on Design of Communication. 
SIGDOC '09. ACM, New York, NY, 29-36.  
44
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 137?147,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Towards an on-demand Simple Portuguese Wikipedia
Arnaldo Candido Junior Ann Copestake
Institute of Mathematics and Computer Sciences Computer Laboratory 
University of S?o Paulo University of Cambridge 
arnaldoc at icmc.usp.br Ann.Copestake at cl.cam.ac.uk 
Lucia Specia Sandra Maria Alu?sio
Research Group in Computational Linguistics Institute of Mathematics and Computer Sciences 
University of Wolverhampton University of S?o Paulo
l.specia at wlv.ac.uk sandra at icmc.usp.br
Abstract
The  Simple  English Wikipedia  provides  a 
simplified  version  of  Wikipedia's  English 
articles  for  readers  with  special  needs. 
However,  there are fewer efforts  to make 
information  in  Wikipedia  in  other 
languages  accessible  to  a  large  audience. 
This work proposes the use of a syntactic 
simplification  engine  with  high  precision 
rules  to  automatically  generate  a  Simple 
Portuguese Wikipedia on demand, based on 
user interactions with the main Portuguese 
Wikipedia.  Our  estimates  indicated that  a 
human  can  simplify  about  28,000 
occurrences  of  analysed  patterns  per 
million  words,  while  our  system  can 
correctly simplify 22,200 occurrences, with 
estimated f-measure 77.2%. 
1 Introduction
The Simple English Wikipedia1 is an effort to make 
information  in  Wikipedia2 accessible  for  less 
competent  readers  of  English  by  using  simple 
words and grammar.  Examples of  intended users 
include  children  and  readers  with  special  needs, 
such as users with learning disabilities and learners 
of English as a second language. 
Simple English (or Plain English), used in this 
version  of  Wikipedia,  is  a  result  from the  Plain 
English movement that occurred in Britain and the 
United States in the late 1970?s as a reaction to the 
unclear language used in government and business 
forms and documents. Some recommendations on 
how  to  write  and  organize  information  in  Plain 
1 http://simple.wikipedia.org/
2 http://www.wikipedia.org/
Language (the set of guidelines to write simplified 
texts) are related to both syntax and lexical levels: 
use short sentences; avoid hidden verbs; use active 
voice; use concrete, short, simple words. 
A number of resources, such as lists of common 
words3,  are available for the English language to 
help users write in Simple English. These include 
lexical  resources  like  the  MRC  Psycholinguistic 
Database4 which  helps  identify  difficult  words 
using  psycholinguistic  measures.  However, 
resources as such do not exist for Portuguese. An 
exception is a small list of simple words compiled 
as part  of  the  PorSimples project  (Aluisio et  al., 
2008).
Although  the  guidelines  from  the  Plain 
Language  can  in  principle  be  applied  for  many 
languages and text genres, for Portuguese there are 
very  few  efforts  using  Plain  Language  to  make 
information accessible to a large audience. To the 
best  of  our  knowledge,  the  solution  offered  by 
Portugues  Claro5 to  help  organizations  produce 
European  Portuguese  (EP)  documents  in  simple 
language is the only commercial option in such a 
direction.  For  Brazilian  Portuguese  (BP),  a 
Brazilian  Law (10098/2000)  tries  to  ensure  that 
content  in  e-Gov sites  and  services  is  written  in 
simple  and  direct  language  in  order  to  remove 
barriers in communication and to ensure citizens' 
rights  to  information  and communication  access. 
However,  as  it  has  been  shown  in  Martins  and 
Filgueiras  (2007),  content  in  such  websites  still 
needs  considerable  rewriting  to  follow the  Plain 
Language guidelines. 
A few efforts from the research community have 
recently  resulted  in  natural  language  processing 
3 http://simple.wiktionary.org/
4 http://www2.let.vu.nl/resources/elw/resource/mrc.html
5  http://www.portuguesclaro.pt/
137
systems to simplify and make Portuguese language 
clearer. ReEscreve (Barreiro and Cabral, 2009) is a 
multi-purpose  paraphraser that  helps  users  to 
simplify their EP texts by reducing its ambiguity, 
number  of  words  and  complexity.  The  current 
linguistic phenomena paraphrased are support verb 
constructions,  which  are  replaced  by  stylistic 
variants.  In  the  case  of  BP,  the  lack  of 
simplification  systems  led  to  development  of 
PorSimples project (Alu?sio and Gasperin, 2010). 
This  project  uses  simplification  in  different 
linguistic levels to provide simplified text to poor 
literacy readers.
For  English,  automatic  text  simplification  has 
been  exploited  for  helping  readers  with  poor 
literacy (Max, 2006) and readers with other special 
needs,  such  as  aphasic  people  (Devlin  and 
Unthank,  2006;  Carroll  et  al.  1999).  It  has  also 
been used in bilingual education (Petersen, 2007) 
and  for  improving  the  accuracy  of  Natural 
Language Processing (NLP) tasks (Klebanov et al, 
2004; Vickrey and Koller, 2008).
Given the general scarcity of human resources to 
manually simplify large content  repositories such 
as Wikipedia, simplifying texts automatically can 
be  the  only  feasible  option.  The  Portuguese 
Wikipedia,  for  example,  is  the  tenth  largest 
Wikipedia (as of May 2011), with 683,215 articles 
and approximately 860,242 contributors6. 
In  this  paper  we  propose  a  new  rule-based 
syntactic simplification system to create a Simple 
Portuguese Wikipedia  on demand,  based on user 
interactions with the main Portuguese Wikipedia. 
We use a simplification engine to change passive 
into active voice and to break down and change the 
syntax of subordinate clauses.  We focus on these 
operations  because  they  are  more  difficult  to 
process  by  readers  with  learning  disabilities  as 
compared  to  others  such  as  coordination  and 
complex noun phrases (Abedi et al, 2011; Jones et 
al.,  2006;  Chappell,  1985).  User  interaction with 
Wikipedia can be performed by a system like the 
Facilita7 (Watanabe et al, 2009), a browser plug-in 
developed  in  the  PorSimples  project  to  allow 
automatic adaptation (summarization and syntactic 
simplification) of any web page in BP.
This  paper  is  organized  as  follows.  Section  2 
presents related work on syntactic  simplification. 
6 http://meta.wikimedia.org/wiki/List_of_Wikipedias#
Grand Total
7 http://nilc.icmc.usp.br/porsimples/facilita/
Section  3 presents the methodology to build and 
evaluate the simplification engine for BP. Section 4 
presents  the  results  of  the  engine  evaluation. 
Section  5 presents  an  analysis  on  simplification 
issues  and  discusses  possible  improvements. 
Section 6 contains some final remarks.
2 Related work
Given the dependence of  syntactic  simplification 
on  linguistic  information,  successful  approaches 
are  mostly  based  on  rule-based  systems. 
Approaches using operations learned from corpus 
have  not  shown  to  be  able  to  perform  complex 
operations  such  the  splitting  of  sentences  with 
relative clauses (Chandrasekar and Srinivas, 1997; 
Daelemans  et  al.,  2004;  Specia,  2010).  On  the 
other hand. the use of machine learning techniques 
to predict when to simplify a sentence, i.e. learning 
the properties of language that distinguish simple 
from normal  texts,  has  achieved relative  success 
(Napoles and Dredze, 2010). Therefore, most work 
on syntactic simplification still relies on rule-based 
systems to simplify a set of syntactic constructions. 
This is also the approach we follow in this paper. 
In what follows we review some relevant and work 
on syntactic simplification.
The seminal work of Chandrasekar and Srinivas 
(1997) investigated the induction of syntactic rules 
from a corpus annotated with part-of-speech tags 
augmented  by  agreement  and  subcategorization 
information.  They  extracted  syntactic 
correspondences  and  generated  rules  aiming  to 
speed up parsing and improving its accuracy, but 
not  working  on  naturally  occurring  texts. 
Daelemans et  al.  (2004)  compared both machine 
learning  and  rule-based  approaches  for  the 
automatic generation of TV subtitles for hearing-
impaired  people.  In  their  machine  learning 
approach,  a  simplification model  is  learned from 
parallel  corpora  with  TV  programme  transcripts 
and the associated subtitles. Their method used a 
memory-based learner and features such as words, 
lemmas,  POS tags,  chunk tags,  relation tags  and 
proper  name  tags,  among  others  features  (30  in 
total). However, this approach did not perform as 
well  as  the  authors  expected,  making errors  like 
removing sentence subjects or deleting a part of a 
multi-word  unit.   More  recently,  Specia  (2010) 
presented a new approach for text simplification, 
based  on  the  framework  of  Statistical  Machine 
138
Translation. Although the results are promising for 
lexical simplification, syntactic rewriting was not 
captured  by  the  model  to  address  long-distance 
operations,  since  syntactic  information  was  not 
included into the framework.
Inui et al (2003) proposed a rule-based system 
for text simplification aimed at deaf people. Using 
about  one  thousand  manually  created  rules,  the 
authors  generate  several  paraphrases  for  each 
sentence and train a classifier to select the simpler 
ones.  Promising  results  were  obtained,  although 
different  types  of  errors  on  the  paraphrase 
generation are encountered, such as problems with 
verb conjugation and regency.  Our work aims at 
making  Portuguese  Wikipedia  information 
accessible  to  a  large  audience  and  instead  of 
generating  several  possible  outputs  we  generate 
only one based on rules taken from a manual of 
simplification for BP.
Siddharthan  (2006)  proposed  a  syntactic 
simplification  architecture  that  relies  on  shallow 
parsing. The general goal of the architecture is to 
make texts more accessible to a broader audience 
instead of targeting any particular application. The 
system  simplifies  apposition,  relative  clauses, 
coordination  and  subordination.  Our  method,  on 
the other hand, relies on deep parsing (Bick, 2000) 
and focuses  on  changing passive  to  active voice 
and  changing  the  syntax  of  relative  clauses  and 
subordinate sentences.
Max  (2006)  applied  text  simplification  in  the 
writing process by embedding the simplifier into a 
word  processor.  Although  this  system  ensures 
accurate  output,  it  requires  manual  choices.  The 
suggested simplifications are ranked by a score of 
syntactic  complexity  and  potential  change  of 
meaning.  The writer  then chooses their  preferred 
simplification.  Our  method,  on  the  other  hand, 
offers the user only one simplification since it uses 
several  rules  to  better  capture  each  complex 
phenomenon. 
Inspired  by  Siddharthan  (2006),  Jonnalagadda 
and  Gonzalez  (2009)  present  an  approach  to 
syntactic  simplification  addressing  also  the 
problem of accurately determining the grammatical 
correctness  of  the  simplified  sentences.  They 
propose  the  combination  of  the  number  of  null 
links  and  disjunct  cost  (the  level  of 
inappropriateness,  caused  by  using  less  frequent 
rules in the linkage) from the cost vector returned 
by a Link Grammar8 parser. Their motivation is to 
improve the performance of systems for extracting 
Protein-Protein  Interactions  automatically  from 
biomedical  articles  by  automatically  simplifying 
sentences.  Besides  treating  the  syntactic 
phenomena described in Siddharthan (2006), they 
remove  describing  phrases  occurring  at  the 
beginning  of  the  sentences,  like  ?These  results 
suggest that? and ?As reported previously?. While 
they  focus  on  the  scientific  genre,  our  work  is 
focused on the encyclopedic genre.
In order to obtain a text easier to understand by 
children,  De  Belder  and  Moens  (2010)  use  the 
Stanford parser9 to select the following phenomena 
to syntactically simplify the sentences: appositions, 
relative  clauses,  prefix  subordination  and  infix 
subordination  and  coordination.  After  sentence 
splitting, they try to apply the simplification rules 
again to both of the new sentences. However, they 
conclude that  with the set  of  simplification rules 
used,  it  was  not  possible  to  reduce  the  reading 
difficulty for children and foresee the use of other 
techniques for this purpose, such as summarization 
and elaborations for difficult words.
3 Simplification engine
3.1 Engine development
The  development  of  a  syntactic  simplification 
engine  for  a  specific  task  and  audience  can  be 
divided  into  five  distinct  phases:  (a)  target 
audience analysis; (b) review of complex syntactic 
phenomena for such an audience; (c) formulation 
of simplification guidelines; (d) refinement of rules 
based  on  evidence  from  corpora;  and  (e) 
programming and evaluation of rules.
In this paper we focus on the last two phases. 
We  use  the  simplification  guidelines  from  the 
PorSimples  project,  but  these  are  based  on 
grammar  studies  and  corpora  analysis  for  a 
different  text  genre  (news).  Therefore  additional 
corpora  evidence  proved  to  be  necessary.  This 
resulted  in  the  further  refinement  of  the  rules, 
covering  different  cases  for  each  syntactic 
phenomenon.
The Simplification engine relies on the output of 
the  Palavras  Parser  (Bick,  2000)  to  perform 
constituent tree transformations (for example, tree 
8 http://www.abisource.com/projects/link-grammar/
9 http://nlp.stanford.edu/software/lex-parser.shtml
139
splitting).  Each  node  of  a  sentence  tree  is  fed 
(breadth-first  order)  to  the  simplification 
algorithms,  which can simplify the node (and its 
sub-tree) or skip it when the node does not meet 
the simplification prerequisites. Breadth-first order 
is chosen because several operations affect the root 
of a (sub)tree, while none of them affect leaves.
A development  corpus containing examples  of 
cases analysed for each syntactic phenomenon is 
used  to  test  and  refine  the  rules.  The  current 
version of the corpus has 156 sentences extracted 
from news text. The corpus includes negative and 
positive  examples  for  each  rule.  Negative 
examples  should  not  be  simplified.  They  were 
inserted  into  the  corpus  to  avoid  unnecessary 
simplifications. Each rule is first tested against its 
own positive and negative examples.  This test  is 
called  local  test.  After reaching a good precision 
on the local test, the rule is then tested against all 
the  sentences  in  the  corpus,  global  test.  In  the 
current corpus, the global test identified sentences 
correctly   simplified by  at  least  one  rule  (66%), 
sentences incorrectly simplified due to major errors 
in  parsing/rules  (7%)  (ungrammatical  sentences) 
and  non-simplified  sentences  (27%).  The  last 
includes  mainly  negative  examples,  but  also 
includes  sentences  not  selected  due  to  parsing 
errors, sentences from cases not yet implemented, 
and sentences from cases ignored due to ambiguity.
3.2 Passive voice
The default case for dealing with passive voice in 
our simplification engine is illustrated by the pair 
of  original-simplified sentences  in  example10 (1). 
Sentences  belonging  to  this  case  have  a  non-
pronominal subject and a passive agent. Also, the 
predicator has two verbs, the verb  to be followed 
by  a  verb  in  the  past  participle  tense.  The 
simplification consists  in  reordering the sentence 
components,  turning  the  agent  into  subject 
(removing the  by preposition), turning the subject 
into direct  object and adjusting the predicator by 
removing the verb to be and re-inflecting the main 
verb. The new tense of the main verb is the same 
as  the  one  of  the  to  be  verb  and  its  number  is 
defined according to the new subject.
10 Literal translations from Portuguese result in some 
sentences appearing ungrammatical in English. 
O: As[The] transfer?ncias[transfers] 
foram[were:plural] feitas[made] pela[by the] 
empresa[company]. (1)
S: A[The] empresa[company] fez[made:sing] 
as[the] transfer?ncias[transfers].
Other correctly processed cases vary according 
the  number  of  verbs  (three  or  four),  special 
subjects, and special agents. For cases comprising 
three or four verbs, the simplification rule must re-
inflect11 two verbs (2) (one of them should agree 
with  the  subject  and  the  other  receives  its  tense 
from  the  verb  to  be).   There  are  two  cases  of 
special subjects. In the first case, a hidden subject 
is turned into a pronominal direct object (3). In the 
second  case,  a  pronominal  subject  must  be 
transformed to oblique case pronoun and then to 
direct  object.  Special  agents  also  represent  two 
cases. In the first one, oblique case pronouns must 
be  transformed before  turning  the  agent  into  the 
subject. In the second case (4), a non-existent agent 
is turned into an undetermined subject (represented 
here by ?they?).
O: A[The] porta[door] deveria[should] ter[have] 
sido[been] trancada[locked:fem] por[by] John. (2)S: John deveria[should] ter[have] 
trancado[locked:masc] a[the] porta[door].
O: [I] fui[was] encarregado[entrusted] por[by] 
minha[my] fam?lia[family]. (3)S: Minha[My] fam?lia[family] 
encarregou[entrusted] me[me].
O: O[The] ladr?o[thief] foi[was] pego[caught]. (4)
S: [They] pegaram[caught] o[the] ladr?o[thief].
Two cases  are  not  processed because they are 
already considered easy enough: the syndetic voice 
and passive in non-root sentences. In those cases, 
the  proposed  simplification  is  generally  less 
understandable  than  the  original  sentence. 
Sentences  with  split  predicator  (as  in  ?the 
politician was very criticized by his electors?) are 
not  processed  for  the  time  being,  but  should  be 
incorporated in the pipeline in the future.
Table  1 presents the algorithm used to process 
the  default  case  rule  and  verb  case  rules. 
Simplification rules are applied against all nodes in 
constituent tree, one node at a time, using breadth-
first traversing.
11 Some reinflections may not be visible on example 
translation.
140
Step Description
1 Validate these prerequisites or give up:
1.1     Node must be root
1.2     Predictor must have an inflection of auxiliary   
    verb to be
1.3     Main verb has to be in past participle
2 Transform subject into direct object
3 Fix the predicator
3.1 If main verb is finite then:
    main verb gets mode and tense from to be
    main verb gets person according to agent
3.2 Else:
    main verb gets mode and tense from verb to be
    finite verb gets person according to agent
3.3 Remove verb to be
4 Transform passive agent into a new subject
Table 1: Algorithm for default and verb cases
3.3 Subordination
Types of subordinate clauses are presented in Table 
2. Two clauses are not processed: comparative and 
proportional.  Comparative  and  proportional 
clauses will be addressed in future work.
id Clause type Processed
d Relative Restrictive ?
e Relative  Non-restrictive ?
f Reason ?
g Comparative  
h Concessive ?
i Conditional ?
j Result ?
k Confirmative ?
l Final Purpose ?
m Time ?
w Proportional
Table 2: Subordinate clauses
Specific  rules  are  used  for  groups  of  related 
subordinate cases. At least one of two operations 
can  be  found  in  all  rules:  component  reordering 
and sentence splitting. Below, letter codes are used 
to  describe  rules  involving  these  two  and  other 
common operations:
A additional processing
M splitting-order main-subordinate 
P Also processes non-clause phrases and/or non-
finite clauses
R component reordering 
S splitting-order subordinate-main 
c clone subject or turn object of a clause into 
subject in another if it is necessary
d marker deletion
m marker replacement
v verb reinflection
[na] not simplified due ambiguity
[nf] not simplified, future case
[np] not simplified due parsing problems
2...8 covered cases (when more than one applies)
Table  3 presents the marker information. They 
are used to select sentences for simplification, and 
several  of  them  are  replaced  by  easier  markers. 
Cases  themselves  are  not  detailed since they are 
too  numerous  (more  than  40  distinct  cases). 
Operation  codes  used  for  each  marker  are 
described in column ?Op?. It is important to notice 
that  multi-lexeme  markers  also  face  ambiguities 
due to co-occurrence of its component lexemes12. 
The  list  does  not  cover  all  possible  cases,  since 
there  may  be  additional  cases  not  seen  in  the 
corpus. As relative clauses (d and e) require almost 
the same processing, they are grouped together.
Several  clauses  require  additional  processing. 
For  example,  some  conditional  clauses  require 
negating the main clause. Other examples include 
noun phrases replacing clause markers and clause 
reordering, both for relative clauses, as showed in 
(5). The marker  cujo (whose) in the example can 
refer to Northbridge or to the building. Additional 
processing  is  performed  to  try  to  solve  this 
anaphora13,  mostly  using  number  agreement 
between the each possible co-referent and the main 
verb in the subordinate clause. The simplification 
engine can give up in ambiguous cases (focusing 
on  precision)  or  elect  a  coreferent  (focusing  on 
recall),  depending  on  the  number  of  possible 
coreferents  and  on  a  confidence  threshold 
parameter, which was not used in this paper.
O: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge 
cujo[whose] desabamento[landslide] 
matou[killed] 16 pessoas[people].
(5)S: Ele[He] deve[should] visitar[visit] o[the] 
pr?dio[building] em[in] Northbridge. O[The] 
desabamento[landslide] do[of  the] 
pr?dio[building] em[in] Northbridge 
matou[killed] 16 pessoas[people].
12 For example, words ?de?, ?sorte? and ?que? can be 
adjacent  to each other without the meaning of ?de sorte 
que? marker (?so that?).
13 We opted to solve this kind of anaphora instead of using 
pronoun insertion in order to facilitate the reading of the 
text.
141
3.4 Evaluation in the development corpus
Figure 1 provides statistics from the of processing 
all  identified  cases  in  the  development  corpus. 
These statistics cover number of cases rather than 
the  number  of  sentences  containing  cases.  The 
cases  ?incorrect  selection?  and  ?incorrect 
simplification?  affect  precision  by  generating 
ungrammatical  sentences.  The  former  refers  to 
sentences  that  should  not  be  selected  for  the 
simplification  process,  while  the  latter  refers  to 
sentences  correctly  selected  but  wrongly 
simplified.  There  are  three  categories  affecting 
recall, classified according to their priority in the 
simplification  engine.  Pending cases  are 
considered  to  be  representative,  with  higher 
priority.  Possible cases  are  considered  to  be 
unrepresentative. Having less priority, they can be 
handled in future versions of the engine.  Finally, 
Skipped cases  will  not  be  implemented,  mainly 
because  of  ambiguity,  but  also  due  to  low 
representativeness.  It  is  possible  to  observe  that 
categories  reducing  precision  (incorrect  selection 
and simplification) represent a smaller number of 
cases (5%) than categories reducing recall (45%). 
It  is  worth  noticing  that  our  approach  focus  on 
precision  in  order  to  make  the  simplification  as 
automatic  as  possible,  minimizing  the  need  for 
human interaction.
Figure 1: Performance on the development 
corpus
There are some important remarks regarding the 
development corpus used during the programming 
phase.  First,  some  cases  are  not  representative, 
therefore  the  results  are  expected  to  vary 
significantly in real texts. Second, a few cases are 
not orthogonal: i.e.,  there are sentences that can be 
classified  in  more  than  one  case.  Third,  several 
errors  refer  to  sub-cases  of  cases  being  mostly 
correctly  processed,  which are  expected to occur 
less frequently. Fourth, incorrect parsed sentences 
were not take in account in this phase. Although 
there may exist other cases not identified yet, it is 
plausible to estimate that only 5% of known cases 
are affecting the precision negatively.
id Marker Op id Marker Op id Markers Op
de que [that/which] 8MRAdv h se bem que [albeit] Mmv j tanto ? que [so ? that] [nf]
de o qual [which]* 8MRAdv h ainda que [even if] 2Mm j tal ? que [such ? that] [nf]
de como [as] [na] h mesmo que [even if] 2Mm j tamanho ? que [so ? that]* [nf]
de onde [where] [nf] h nem que [even if] 2Mm k conforme [as/according] 3PRAcm
de quando [when] [na] h por mais que [whatever] 2Mm k consoante [as/according] 3PRAcm
de quem [who/whom] [nf] h mas [but] [np] k segundo [as/according] 3PRAcm
de quanto [how much] [nf] i contanto que [provided that] 2Rmv k como [as] [na]
de cujo [whose]* MAd i caso [case] 2Rmv l a fim de [in order to] 2PMcm
de o que [what/which] Sd i se [if/whether] 2Rmv l a fim de que [in order that] 2PMcm
f j? que [since] Scm i a menos que [unless] 2RAmv l para que [so that] 2PMcm
f porquanto [in view of] Scm i a n?o ser que [unless] 2RAmv l porque [because] [na]
f uma vez que [since] Scm i exceto se [unless] 2RAmv m assim que [as soon as] 5PMAcvr
f visto que [since] Scm i salvo se [unless] 2RAmv m depois de [after] 5PMAcvr
f como [for] [na] i antes que [before] Rmv m depois que [after] 5PMAcvr
f porque [because] [na] i sem que [without] Rmv m logo que [once] 5PMAcvr
f posto que [since] [na] i desde que [since] RAmv m antes que [before] PSAcvr
f visto como [seen as] [na] j de forma que [so] 5Mmv m apenas [only] [na]
f pois que [since] [nf] j de modo que [so] 5Mmv m at? que [until] [na]
h apesar de que [although] Mmv j de sorte que [so that] 5Mmv m desde que [since] [na]
h apesar que [despite] Mmv j tamanho que [so that]* 5Mmv m cada vez que [every time] [nf]
h conquanto [although] Mmv j tal que [such that] 5Mmv m sempre que [whenever] [nf]
h embora [albeit] Mmv j tanto que [so that] (1)* [na] m enquanto [while] [nf]
h posto que [since] Mmv j tanto que [so that] (2) [na] m mal [just] [na]
h por muito que [although] Mmv j t?o ? que [so ? that] [nf] m quando [when] [na]
* gender and/or number variation
Table 3: Marker processing
correct 45%
incorrect
simplification 4% incorrectselection 1%
pending 17%
possible 7%
skipped 25%
142
4 Engine evaluation
4.1 Evaluation patterns
The  evaluation  was  performed  on  a  sample  of 
sentences  extracted  from Wikipedia's  texts  using 
lexical patterns. These patterns allows to filter the 
texts,  extracting  only  relevant  sentences  for 
precision and recall evaluation. They were created 
to  cover  both  positive  and  negative  sentences. 
They are applied before parsing or Part of Speech 
(PoS)  analysis.  For  passive  voice  detection,  the 
pattern is  defined as a sequence of  two or  more 
possible verbs (no PoS in use) in which at least one 
of them could be an inflection of verb to be. For 
subordination detection, the pattern is equivalent to 
the  discourse  markers  associated  with  each 
subordination type, as shown in Table 3. 
The  patterns  were  applied  against  featured 
articles  appearing  in  Wikipedia's  front  page  in 
2010 and 2011, including featured articles planned 
to be featured, but not featured yet. A maximum of 
30 sentences resulting from each pattern matching 
were then submitted to the simplification engine. 
Table 4 presents statistics from featured articles. 
texts 165
sentences 83,656
words 1,226,880
applied patterns 57,735
matched sentences 31,080
Table 4: Wikipedia's featured articles (2010/2011)
The number of applied patterns represents both 
patterns to be simplified (s-patterns) and patterns 
not  to  be  simplified  (n-patterns).  N-patterns 
represent both non-processable patterns due to high 
ambiguity (a-patterns) and pattern extraction false 
negatives. We observed a few, but very frequent, 
ambiguous patterns introducing noise, particularly 
se and  como.  In  fact,  these  two  markers  are  so 
noisy that  we were not  be able  to  provide good 
estimations  on  their  true  positives  distribution 
given the 30 sentences limit per pattern. Similarly 
to the number of applied patterns, the number of 
matched sentences correspond to both sentences to 
be simplified and not to be simplified.
Table  5 presents  additional  statistics  about 
characters,  words  and  sentences  calculated  in  a 
sample of 32 articles where the 12 domains of the 
Portuguese Wikipedia are balanced. The number of 
automatic simplified sentence is also presented. In 
Table  5,  simple  words refers  to  percentage  of 
words  which  are  listed  on  our  simple  word  list, 
supposed to be common to youngsters,  extracted 
from the dictionary described in (Biderman, 2005), 
containing 5,900 entries.  Figure 2 presents clause 
distribution per sentence in  the balanced sample. 
Zero  clauses refers  to  titles,  references,  figure 
labels, and other pieces of text without a verb. We 
observed  60%  of  multi-clause  sentences  in  the 
sample.
characters per word 5.22
words per sentence 21.17
words per text 8,476
simple words 75.52%
sentences per text 400.34
passive voice 15.11%
total sentences 13,091
simplified sentences 16,71%
Table 5: Statistics from the balanced text sample
Figure 2: Clauses per sentence in the sample
4.2 Simplification analysis
We manually analysed and annotated all sentences 
in  our  samples.  These  samples  were  used  to 
estimate  several statistics, including the number of 
patterns  per  million  words,  the  system precision 
and  recall  and  the  noise  rate.  We  opted  for 
analysing  simplified  patterns  per  million  words 
instead of  per simplified sentences. First, because 
an analysis based on sentences can be misleading, 
since  there  are  cases  of  long  coordinations  with 
many patterns, as well as succinct sentences with 
no patterns.  Moreover,  one incorrectly  simplified 
marker in a sentence could hide useful statistics of 
correctly  simplified  patterns  and  even  of  other 
incorrectly simplified patterns. 
The samples are composed by s-patterns and n-
patterns  (including  a-patterns).  In  total  1,243 
patterns were annotated.  Table  6 presents pattern 
estimates per million words. 
0 1 2 3 4 5 6 7 8
0,0000
0,0500
0,1000
0,1500
0,2000
0,2500
0,3000
clauses
dis
trib
uti
on
 [0
-1]
>7
143
Total patterns 70,834
Human s-patterns 33,906
Selection s-patterns 27,714
Perfect parser s-patterns 23,969
Obtained s-patterns 22,222
Table 6: Patterns per million words
Total patterns refers to the expected occurrences 
of  s-patterns  and  n-patterns  in  a  corpus  of  one 
million  words.  This  is  the  only  information 
extracted from the full corpus, while the remaining 
figures are estimates from the sample corpus.
Human s-patterns is an estimate of the number 
patterns that a human could simplify in the corpus. 
Unlike  other  s-pattern  estimates,  a-patterns  are 
included, since a human can disambiguate them. In 
other words, this is the total of positive patterns. 
The estimate  does  not  include very rare  (sample 
size equals to zero) or very noisy markers (patterns 
presenting 30 noisy sentences in its sample).
Selection  s-patterns are  an  estimate  of  the 
number  of  patterns  correctly  selected  for 
simplification,  regardless  of  whether  the  pattern 
simplification is correct or incorrect.  Precision and 
recall derived from this measure (Table 7) consider 
incorrectly simplified patterns,  and do not include 
patterns with parsing problems.  Its  purpose is  to 
evaluate how well the selection for simplification 
is performed. Rare or noisy patterns, whose human 
s-patterns  per  sample  is  lower  than  7,  are  not 
included.
Perfect  parser  s-patterns is  an  estimate  very 
similar to selection s-patterns, but considering only 
correctly  simplified  patterns.  As  in  selection  s-
patterns,  incorrect  parsed  sentences  are  not 
included in calculations. This is useful to analyse 
incorrect simplifications due to simplification rule 
problems, ignoring errors originating from parsing.
Finally,  obtained  s-patterns refers  to  the 
estimate of correct simplified patterns,  similar to 
perfect  parser  s-patterns,  but  including 
simplification  problems  caused  by  parsing.  This 
estimate  represents  the  real  performance  to  be 
expected from the system on Wikipedia's texts.
It is important to note that the real numbers of 
selection  s-patterns,  perfect  s-patterns  and 
obtained s-patterns  is expected to be bigger than 
the estimates,  since noisy and rare  pattern could 
not used be used in calculations (due the threshold 
of  7  human  s-patterns  per  sample).  The  data 
presented on Table 6 is calculated using estimated 
local precisions for each pattern. Table  7 presents 
global  precision,  recall  and  f-measure  related  to 
selection,  perfect  parser  and  obtained s-patterns. 
The  real  values  of  the  estimates  are  expected to 
variate up to +/- 2.48% .
Measures Precision Recall F-measure
Selection 99.05% 82.24% 89.86%
Perfect parser 85.66% 82.24% 83.92%
Obtained 79.42% 75.09% 77.20%
Table 7: Global estimated measures
Although the precision of the selection seems to 
be  impressive,  this  result  is  expected,  since  our 
approach  focus  on  the  processing  of  mostly 
unambiguous  markers,  with  sufficient  syntactic 
information. It is also due to the the threshold of 7 
human s-patterns  and the fact  that  a-patterns  are 
not  included.  Due to  these two restrictions,  only 
approximately 31.5% of unique patterns could be 
used for the calculations in Table  7. Interestingly, 
these unique patterns correspond to 82.5% of the 
total estimated human s-patterns. The majority of 
the 17.5%  remaining s-patterns refers to patterns 
too  noisy  to  be  analysed  and  to  a-patterns  (not 
processed  due  ambiguity),  and  also  others  n-
patterns which presented a low representativeness 
in  the  corpus.  The  results  indicate  good 
performance in rule formulation, covering the most 
important (and non-ambiguous) markers, which is 
also confirmed by the ratio between both selection 
s-patterns  and  human  s-patterns  previously 
presented on Table 6. 
An  alternative  analysis,  including  a-patterns, 
lowers recall and f-measure, but not precision (our 
focus in this work). In this case, recall drops from 
75.09% to  62.18%,  while  f-measure  drops  from 
77.20% to 70.18%.
Figure 3: Pattern distribution
Figure  3 presents  the  distribution  of  patterns 
according to their frequency per million words and 
their purity (1 - noisy rate). This data is useful to 
2,0 20,0 200,0 2000,0 20000,0 200000,0
0,0000
0,2000
0,4000
0,6000
0,8000
1,0000
1,2000
b-passiva
de-que
l-a_fim_de
j-tal_que
J-tanto_*_que
Frequency PMW
Pu
rity
144
identify  most  frequent  patterns  (such  as  passive 
voice in  b-passiva)  and patterns with medium to 
high  frequency,  which  are  easy  to  process  (not 
ambiguous), such as l-a_fim_de.
5 Issues on simplification quality
This analysis aims at identifying factors affecting 
the quality of simplifications considered as correct. 
Hence, factors affecting the overall simplified text 
quality  are  also  presented.  In  contrast,  the 
quantitative  analysis  presented  on  Section  4.2 
covered  the  ratio  between  incorrect  and  correct 
simplifications.
Three cases of clause disposition were identified 
as  important  factors  affecting  the  simplified 
sentence  readability.  These  cases  are  presented 
using  the  following  notation:  clauses  are 
represented  in  uppercase  letters;  clause 
concatenation represents coordination; parentheses 
represent  subordination;  c1 and  c2 represent 
clause/sentence  connectors  (including  markers); 
the  entailment  operator  (?)  represents  the 
simplification rule transforming clauses.
? ?A(B(c1 C)) ? A(B). c2 C?: the vertical case. 
In this scenario it is more natural to read c2 as 
connecting  C to the main clause  A, while  c1 
connects  C to  B,  as seen in (6). This is still 
acceptable for several sentences analysed, but 
we  are  considering to  simplify only level  2 
clauses in the future, splitting C  from B only 
if another rule splits A and B first.
? ?A(B)CD  ?  ACD.  c1 B?:  the  horizontal 
case. In this scenario, c1 correctly connects A 
and B, but long coordinations following A can 
impact  negatively on text  reading,  since the 
target  audience  may  forget  about  A when 
starting  to  read  B.  In  this  scenario, 
coordination  compromise  subordination 
simplification,  showing  the  importance  of 
simplifying coordination as well, even though 
they  are  considered  easier  to  read  than 
subordination.
? Mixed  case:  this  scenario  combines  the 
potential problems of horizontal and vertical 
cases.  It  may  occur  in  extremely  long 
sentences.
Besides  clause  disposition  factors,  clause 
inversions can also lead to  problems in sentence 
readability.  In  our  current  system,  inversion  is 
mainly used to produce simplified sentences in the 
cause-effect  order  or  condition-action  order. 
Reordering, despite using more natural orders, can 
transform  anaphors  into  cataphors.  A  good 
anaphora resolution system would be necessary to 
avoid  this  issue.  Another  problem  is  moving 
sentence connectors as in ?A. c1 BC. ? A. B. c2 c1 
C?,  while  ?A.  c1 B.  c2 C?  is  more  natural 
(maintaining c1 position). 
O: Ela[She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group] de_modo_que[so that] seu[their] 
desempenho[performance] melhore[improves] (6)S: [He/She] dissertou[talked] sobre[about] 
como[how] motivar[to motive] o[the] 
grupo[group]. Thus, seu[their] 
desempenho[performance] melhore[improves]
We  have  observed  some  errors  in  sentence 
parsing,  related  to  clause  attachment,  generating 
truncated ungrammatical text. As a result, a badly 
simplified key sentence can compromise the text 
readability more than several  correctly simplified 
sentences  can  improve  it,  reinforcing  the 
importance  of  precision  rather  than  recall  in 
automated text simplification.
Experienced  readers  analysed  the  simplified 
versions of the articles and considered them easier 
to read than the original ones in most cases, despite 
simplification  errors.  Particularly,  the  readers 
considered  that  the  readability  would  improve 
significantly  if  cataphor  and horizontal  problems 
were  addressed.  Evaluating  the  simplifications 
with readers from the target audience is left  as a 
future work, after  improvements in the identified 
issues.
6 Conclusions
We  have  presented  a  simplification  engine  to 
process texts from the Portuguese Wikipedia. Our 
quantitative  analysis  indicated  a  good  precision 
(79.42%),  and  reasonable  number  of  correct 
simplifications  per  million  words  (22,222). 
Although our focus was on the encyclopedic genre 
evaluation,  the  proposed  system  can  be  used  in 
other genres as well.
Acknowledgements
We thank FAPESP (p.  2008/08963-4)  and CNPq 
(p. 201407/2010-8) for supporting this work.
145
References
J.  Abedi,  S.  Leon,  J.  Kao,  R.  Bayley,  N.  Ewers,  J. 
Herman and K. Mundhenk. 2011. Accessible Reading 
Assessments for Students with Disabilities: The Role 
of  Cognitive,  Grammatical,  Lexical,  and 
Textual/Visual  Features.  CRESST  Report  785. 
National  Center  for  Research  on  Evaluation, 
Standards,  and  Student  Testing,  University  of 
California, Los Angeles.
S.  M.  Alu?sio,   C.  Gasperin.  2010.  Fostering  Digital 
Inclusion and Accessibility: The PorSimples project 
for Simplification of Portuguese Texts.  Proceedings 
of  the  NAACL  HLT  2010  Young  Investigators 
Workshop  on  Computational  Approaches  to 
Languages of the Americas. : ACL, New York, USA. 
v. 1. p. 46-53.
A.  Barreiro,  L.  M.  Cabral.  2009.  ReEscreve:  a 
translator-friendly  multi-purpose  paraphrasing 
software  tool.  The  Proceedings  of  the  Workshop 
Beyond  Translation  Memories:  New  Tools  for 
Translators,  The  Twelfth  Machine  Translation 
Summit. Ontario, Canada, pp. 1-8. 
E.  Bick.  2006.   The  parsing  system  ?Palavras?: 
Automatic grammatical  analysis of  Portuguese in a 
constraint  grammar  framework.  Thesis  (PhD). 
University of ?rhus, Aarhus, Denmark.
M.  T.  C.  Biderman.   2005.  Dicion?rio  Ilustrado  de 
Portugu?s. Editora ?tica. 1a. ed. S?o Paulo 
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin 
and  J.  Tait.  1999.  Simplifying  Text  for  Language-
Impaired  Readers,.  In  Proceedings  of  the  9th 
Conference  of  the  European  Chapter  of  the 
Association for Computational  Linguistics  (EACL), 
269-270.
R.  Chandrasekar  and  B.  Srinivas.  1997.  Automatic 
Induction  of  Rules  for  Text  Simplification. 
Knowledge-Based Systems, 10, 183-190.
G. E. Chappell.  1985. Description and assessment of 
language disabilities of junior high school students. 
In:  Communication  skills  and  classroom  success: 
Assessment  of  language-learning  disabled  students. 
College- Hill Press,  San Diego,  pp. 207-239.
W.  Daelemans,  A.  Hothker  and  E.  T.  K.  Sang.  2004. 
Automatic Sentence Simplification for Subtitling in 
Dutch  and  English.  In:  Proceedings  of  the  4th 
Conference on Language Resources and Evaluation, 
Lisbon, Portugal 1045-1048.
J. De Belder and M. Moens. 2010.  Text simplification 
for children. Proceedings of the SIGIR Workshop on 
Accessible Search Systems, pp.19-26.
S.  Devlin  and  G.  Unthank.  2006.  Helping  aphasic 
people process online information. In: Proceedings of 
the  ACM  SIGACCESS  2006,  Conference  on 
Computers  and  Accessibility.  Portland,  Oregon, 
USA , 225-226.
K. Inui, A. Fujita, T. Takahashi, R. Iida and T. Iwakura. 
2003.  Text Simplification for Reading Assistance: A 
Project  Note.  In  the  Proceedings  of  the  Second 
International Workshop on Paraphrasing, 9-16.
S.  Jonnalagadda  and  G.  Gonzalez.  2009.  Sentence 
Simplification  Aids  Protein-Protein  Interaction 
Extraction.  Proceedings  of  the  3rd  International 
Symposium on Languages in Biology and Medicine, 
Short  Papers,  pages  109-114,  Jeju  Island,  South 
Korea, 8-10 November 2009.
F.  W.  Jones,  K.  Long  and  W.  M.  L.  Finlay.  2006. 
Assessing the reading comprehension of adults with 
learning disabilities. Journal of Intellectual Disability 
Research, 50(6), 410-418. 
B.  Klebanov,  K.  Knight  and  D.  Marcu.  2004.  Text 
Simplification for Information-Seeking Applications. 
In:  On  the  Move  to  Meaningful  Internet  Systems. 
Volume  3290,  Springer-Verlag,  Berlin  Heidelberg 
New York, 735-747.
S. Martins, L. Filgueiras. 2007. M?todos de Avalia??o 
de Apreensibilidade das Informa??es Textuais:  uma 
Aplica??o  em  S?tios  de  Governo  Eletr?nico.  In 
proceeding  of  Latin  American  Conference  on 
Human-Computer Interaction (CLIHC 2007). Rio de 
Janeiro, Brazil.
A. Max. 2006. Writing for Language-impaired Readers. 
In: Proceedings of Seventh International Conference 
on  Intelligent  Text  Processing  and  Computational 
Linguistics. Mexico City, Mexico. Berlin Heidelberg 
New York, Springer-Verlag, 567-570.
C.  Napoles  and  M.  Dredze.  2010.  Learning  simple 
Wikipedia:  a  cogitation  in  ascertaining  abecedarian 
language. In the  Proceedings of the NAACL HLT 
2010  Workshop  on  Computational  Linguistics  and 
Writing:  Writing  Processes  and  Authoring  Aids 
(CL&W '10), 42-50.
S.  E.  Petersen.  2007.  Natural  Language  Processing 
Tools  for  Reading  Level  Assessment  and  Text 
Simplification for  Bilingual  Education.  PhD thesis. 
University of Washington.
A. Siddharthan. 2006. Syntactic simplification and text 
cohesion.  Research  on  Language  &  Computation, 
4(1):77-109.
L.  Specia.   2010.  Translating  from  Complex  to 
Simplified  Sentences.  9th  International  Conference 
146
on  Computational  Processing  of  the  Portuguese 
Language.  Lecture  Notes  in  Artificial  Intelligence, 
Vol. 6001, Springer, pp. 30-39.
D. Vickrey and D. Koller. 2008. Sentence Simplification 
for Semantic Role Labelling. In: Proceedings of the 
ACL-HLT. 344-352.
W. M. Watanabe,  A. Candido Jr, V. R. Uzeda, R. P. M. 
Fortes,  T.  A.  S.  Pardo  and  S.  M.  Alu?sio.  2009. 
Facilita:  Reading  Assistance  for  Low-literacy 
Readers.  In:  ACM  International  Conference  on 
Design of Communication (SIGDOC 2009), volume 
1, Bloomington, US,   29-36. 
147
