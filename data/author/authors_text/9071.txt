  
 
Automatic Linguistic Analysis for Language Teachers:  
The Case of Zeros 
 
MITSUKO YAMURA-TAKEI 
Graduate School of Information Sciences 
Hiroshima City University  
3-4-1 Ozuka-higashi, Asaminami-ku,  
Hiroshima, JAPAN 731-3194 
yamuram@nlp.its.hiroshima-cu.ac.jp 
 
MIHO FUJIWARA 
Department of Japanese and Chinese 
Willamette University 
900 State Street, Salem,  
OR. USA 97301 
mfujiwar@willamette.edu 
MAKOTO YOSHIE 
Graduate School of Information Sciences 
Hiroshima City University  
yoshie@nlp.its.hiroshima-cu.ac.jp 
TERUAKI AIZAWA 
Faculty of Information Sciences 
Hiroshima City University  
aizawa@its.hiroshima-cu.ac.jp 
 
 
 
Abstract 
This paper presents the Natural Language 
Processing-based linguistic analysis tool that 
we have developed for Japanese as a Second 
Language teachers.  This program, Zero De-
tector (ZD), aims to promote effective instruc-
tion of zero anaphora, on the basis of a hy-
pothesis about ideal conditions for second 
language acquisition, by making invisible 
zeros visible.  ZD takes Japanese written 
narrative discourse as input and provides the 
zero-specified texts and their underlying 
structures as output.  We evaluated ZD?s 
performance in terms of its zero detecting 
accuracy.  We also present an experimental 
report of its validity for practical use.  As a 
result, ZD has proven to be pedagogically 
feasible in terms of its accuracy and its impact 
on effective instruction. 
 
Introduction 
Natural Language Processing (NLP) is an 
emerging technology with a variety of real-world 
applications.  Computer-Assisted Language 
Learning/Teaching (CALL/CALT) is one area 
that NLP techniques can contribute to.  Such 
techniques range from indexing and concor-
dancing to morphological processing with 
on-demand dictionary look-ups and syntactic 
processing with diagnostic error analysis, to 
name a few.  But little work has been done on 
discourse-level phenomena, including anaphora. 
Zero anaphora or zero pronouns (henceforth 
zeros) are referential noun phrases (NPs) that are 
not overtly expressed in Japanese discourse.  
These NPs can be omitted if they are recoverable 
from a given context or relevant knowledge.  
The use of zeros is common in Japanese and this 
poses a challenge for Japanese as a Second Lan-
guage (JSL) learners for their accurate compre-
hension and natural-sounding production of 
Japanese discourse with zeros.  Some learners 
fail to understand a passage correctly because of 
the difficulty of identifying zeros and/or their 
antecedents.  Other learners produce grammati-
cally correct but still unnatural-sounding Japa-
nese due to overuse or underuse of zeros. 
Yet, very few textbooks provide systematic 
instruction or intensive exercises to overcome 
these difficulties with zeros.  Consequently 
many Japanese language teachers rely on their 
intuitions when explaining zeros.  Intuition is a 
conventional tool in teaching one?s native lan-
guage, but from a student?s perspective, a 
well-developed systematic method of instruction 
can be more convincing.  Also from a teacher?s 
standpoint, such analysis will be helpful in pre-
paring teaching materials and evaluating stu-
dents? performance. 
  
 
Analysis of zeros can be divided into three 
phases: zero identification, zero interpretation 
and zero production.  This paper focuses on the 
first phase and proposes a method of systemati-
cally identifying the presence of zeros in order 
that teachers might provide effective instruction 
of zeros, based on some pedagogical principles 
from relevant second language acquisition (SLA) 
theory.  We regard teachers as primary users of 
the program and aim to help them enhance their 
instruction.  We implemented the program and 
evaluated its potential benefits for language 
teachers. 
In Sections 1 and 2 we discuss the peda-
gogical assumptions from SLA theory that moti-
vate our program design, and present the linguis-
tic assumptions from which our heuristics were 
drawn. Section 3 provides an overview of our 
system implementation.  In Section 4, we pre-
sent the results of evaluation from the viewpoints 
of both the accuracy and the empirical validity of 
the program.  We conclude with a discussion of 
possible future work. 
1 Pedagogical Assumptions 
There have been many studies about how people 
learn foreign languages and what is responsible 
for successful language learning. 
Recent SLA theory progresses beyond 
Krashen (e.g., 1982)?s emphasis on automatic 
processes of acquisition.  Empirical research 
has shown that learners? consciousness-raising 
through explicit instruction does contribute to 
successful second language learning (see Norris 
& Ortega, 2000 for comprehensive review). 
Chapelle (1998) reviewed seven hypotheses 
about ideal SLA conditions that are relevant for 
CALL program design.  At the top of her list is 
that ?the linguistic characteristics of target lan-
guage input need to be made salient? (p. 23).  
Effective input enhancement, by prompting 
learners to notice particular learning items, with 
highlighting for example, plays a significant role 
in facilitating acquisition.  We conjecture that 
this salience effect can also be realized by mak-
ing zeros visible. 
2 Linguistic Assumptions 
Japanese is a head-final language.  A sentence 
or a clause is headed by a predicate, which takes 
a set of arguments and adjuncts.  Predicates in 
Japanese include verbs, adjectives, nominal ad-
jectives and copula, and usually consist of a core 
predicate and some auxiliary elements.  Argu-
ments are classified into three types: Topic 
Phrase (TP), headed by a topic marker wa, Focus 
Phrase (FP), headed by focus particles mo, koso, 
dake, sae, shika, etc., and Kase Phrase (KP), 
headed by case particles ga, wo, ni, e, to, yori, de, 
kara, and made.  We regard adjuncts as 
non-particle-headed phrases. 
We define zeros as unexpressed obligatory 
arguments of a core predicate.  What is 
?obligatory? is the next question to arise.  
Obligatoriness is a controversial issue, and there 
is no set agreement among linguists on its 
definition.  Somers (1984) proposed a six-level 
scale of valency binding that reflects the degree 
of closeness of an element to the predicate.  The 
levels are (i) integral complements, (ii) 
obligatory complements, (iii) optional 
complements, (iv) middles, (v) adjuncts and (vi) 
extraperipherals.  Ishiwata (1999) suggests that 
in Japanese group (i) is often treated as part of 
idioms and is not omissible, and Japanese 
nominative ?ga and accusative ?wo fall into the 
category (ii), while dative ?ni belongs to (iii).  
In light of this, we assume that obligatory 
arguments that can be zero-pronominalized are 
phrases headed by nominative-case particle ga 
and accusative wo, and ni, excluding dative ni in 
an indirect object position. 
3 Zero Detector 
Zero Detector (henceforth ZD) is an automatic 
zero identifying tool, which takes Japanese writ-
ten narrative texts as input and provides the 
zero-specified texts and their underlying struc-
tures as output.  This aims to draw learners? and 
teachers? attention to zeros, by making these 
invisible elements visible in effectively enhanced 
formats. 
3.1 System Overview 
ZD employs a rule-based approach, with theo-
retically sound heuristics.  Our heuristics are 
drawn from the linguistic assumptions described 
in Section 2. 
ZD reuses and integrates two existing natu-
ral language analysis tools and an electronic dic-
tionary, none of which were intended for a lan-
guage learning purpose, into its architecture, 
attempting to make the best possible use of their 
  
 
capabilities for our purpose.  Morphological 
analysis is done by ChaSen 2.2.8 (NAIST, Ma-
tsumoto, Y. et al, 2001), and dependency struc-
ture analysis by CaboCha 0.21 (NAIST, Kudo, 
K., 2001).  The Goi-Taikei Valency Dictionary 
(hereafter GTVD; Ikehara et al, 1997) serves as 
a source for valency pattern search. 
The flow of the system is illustrated in Fig-
ure 1. 
 
 
Clause Splitter
Morphological Analysis
Clause Splitting
(Manual Correction)
Revised Split Clauses
Zero Detector
Dependency Structure Analysis
Zero Detection Valency
Dictionary
Zero Insertion
OUTPUT(B):
Clause Structure Frames
OUTPUT(C):
Predicate-Argument
Structures with Zeros
OUTPUT(D):
Zero-inserted Text
Morphological Analysis
OUTPUT(A):
Split Clauses
INPUT: Text
 
Figure 1: Flow diagram of zero detecting processes 
 
 
3.2 ZD Output 
As shown in Figure 1, ZD produces four differ-
ent types of output: (A) split clauses, (B) clause 
structure frames, (C) predicate-argument struc-
tures with zeros, and (D) zero-inserted texts.  
We will show how these outputs are structured 
using the example text in Figure 2. 
 
 
komatta   Satsuki-wa     sassoku  
in trouble  Satsuki-TOP    immediately 
 
gennin-wo   shirabe-sase-ta. 
cause-ACC  investigate-CAUSATIVE-PAST 
 
 
?Satsuki, who was in trouble, immediately had 
(someone) investigate its cause.? 
 
Figure 2: An example input text 
 
First, output (A) provides a text divided into 
clauses, each consisting of one and only one 
predicate and its arguments.  Some predicates 
are simplex, while others are complex, consisting 
of more than one core predicate (i.e., verb, adjec-
tive).  Several complex predicates (e.g., ta-
beta-koto-ga-aru ate-experience-subject marker- 
have, ?have eaten?) are predefined as simplex to 
avoid excessive clause splitting.  The clauses 
are labelled with their clause types: independent 
(main), dependent (coordinated/subordinated) or 
embedded (relative/nominal/quoted).  A clause 
serves as the basic unit for the zero detecting 
operation.  In this study, embedded clauses are 
excluded from this operation and are left within 
their superordinate clauses.  An example output 
(A) is given in Figure 3 (next page). 
 
 
 
 
  
 
 
komatta EC(RC)] Satsuki-wa sassoku  
 
gennin-wo shirabe-sase-mashita. IC] 
 
Figure 3: Split clauses1 
 
Once the text is split into clauses, each 
clause is analysed for its dependency structure 
and then converted into its clause structure frame.  
The noun phrases which depend on the predicate 
are extracted, and then classified into phrase 
types (TP, FP and KP) according to their accom-
panying particles.  An example of this frame, 
i.e., output (B), is given in Figure 4. 
 
 
Input: komatta Satsuki-wa sassoku gennin-wo 
shirabe-sase-ta. 
 
Paragraph#: 2 
Sentence#: 4 
Clause#: 5 
Clause Type: Independent with EC(RC) 
  ----------------------------------------------------- 
  [Predicate] : shirabe-sase-ta. 
    Core:     shiraberu   verb 
    Auxiliary:  saseru   verb 
  ta   auxiliary verb 
.  
    Voice: causative 
    Empathy: 
    Conjunction: 
  ----------------------------------------------------- 
  [Argument] : 
    Topic Phrase:  komatta Satsuki-wa 
      Topic-Case:  N1-ga 
    Focus Phrase:  <none> 
      Focus-Case:  <none> 
    Kase Phrase:  gennin-wo 
    Pre-copula: <none> 
  [Adjunct] :  sassoku 
 
Figure 4: A clause structure frame 
 
This frame also includes the result of 
valency checking, as in Figure 5, and zero iden-
tifying processes, as in Figure 6, at the bottom. 
 
                                                     
1 Here, we use the acronyms: IC for Independent 
Clause, EC for Embedded Clause, and RC for 
Relative Clause. 
 
Valency Selected: N1 ga  N2 wo 
 
Valency Obligatory: N1 ga  N2 wo 
 
Valency Changed: N1 ga   N2 wo  N3 ni 
 
Figure 5: Valency checking 
 
A core predicate is checked against GTVD 
to search for its syntactic valency pattern.  
GTVD is a semantic valency dictionary, origi-
nally designed for transfer-based Japa-
nese-to-English machine translation, so it in-
cludes as many valency pattern entries for each 
predicate as are necessary for effective transfer.  
The entries are ordered according to expected 
frequency of occurrence.  We took the na?ve 
approach of selecting the first-ranking entry from 
the listing for each core predicate (i.e.,?Valency 
Selected? in Figure 5). 
The next step is to apply the definition of 
?obligatoriness? described in Section 2 to refine 
the selected valency pattern (?Valency Obliga-
tory? in Figure 5).  If non-ga, wo, or ni cases are 
within the first three case slots of the selected 
valency pattern, they are excluded.  If a ni-case 
still remains in the third case slot, it is also de-
leted.  These operations leave us two valency 
patterns: (i) N1-ga N2-wo, and (ii) N1-ga N2-ni, 
in most cases. 
Then, a valency changing operation is done 
in the case of causatives or passives.  When an 
auxiliary verb is added to the core predicate in 
the causative or passive construction, the verb 
then requires three arguments.  In the causative 
case, these are a ga-marked causer, a wo-marked 
object and a ni-marked causee.  The valency 
changing operation adds the boxed valent, N3 ni, 
in Figure 5 (Valency Changed) because the voice 
slot is marked as causative in Figure 4. 
 
 
Valency Selected: N1 ga  N2 wo 
 
Valency Obligatory: N1 ga  N2 wo 
 
Valency Changed: N1 ga  N2 wo  N3 ni 
 
Zero: N3 ni 
 
Figure 6: Zero identifying 
 
  
 
Now that the valency pattern for the given 
predicate is assigned, it is checked against overt 
arguments listed in the frame.  The valent N2 is 
matched with the overt argument gennin-wo and 
removed from the zero candidates, as shown in 
Figure 6. 
Case-less elements, such as TP and FP, also 
need to have their canonical case markers re-
stored.  This is done by assigning the first re-
maining valent to TP and/or FP.  This is based 
on the linguistic fact that subjects are more likely 
to be topicalized or focused than objects.  In the 
example, TP, Satsuki-wa, is assigned ga case.  
The assigned case slot N1-ga is then matched 
with Satsuki-wa (ga) and is also deleted. 
Finally, the remaining valent, if any, is as-
sumed to be a zero (i.e., N3 ni in Figure 6). 
Once zeros are identified, ZD decides where 
to insert the identified zeros in the original text, 
by keeping canonical ordering as listed in the 
valency pattern.  An example of the predicate- 
(obligatory) argument structure from Figure 6, 
with the identified zero, is presented in Figure 7.  
This is output (C).  Here, the restored case 
marking particle is presented in parentheses. 
 
 
*komatta Satsuki-wa (ga) 
 
*gennin-wo 
 
*[   ni] 
 
*shirabe-sase-ta. 
 
Figure 7: Predicate-argument structure with zeros 
 
Finally, ZD outputs the original series of 
clauses with zeros inserted in the most plausible 
positions, along with adjuncts, output (D), as in 
Figure 8. 
 
 
komatta Satsuki-wa sassoku gennin-wo [   ni] 
 
shirabe-sase-ta. 
 
Figure 8: Zero-specified text 
 
These outputs can later be converted into 
the form of a slide presentation or hard-copy 
handouts, etc., depending on how they are used 
by teachers. 
4 Evaluation 
The purpose of the evaluation was to assess the 
validity of ZD output for practical use in a lan-
guage learning/teaching setting.  In the follow-
ing subsections, we evaluate ZD?s performance 
in terms of its accuracy and then present an ex-
perimental report of its validity for educational 
use. 
4.1 Performance 
First, we compared the ZD output with human 
judgements.  The test corpus consisted of two 
reading selections from a JSL textbook and one 
student written narrative monologue, all of which 
were representative samples for lower intermedi-
ate level Japanese.  Five subjects (native speak-
ers of Japanese and trained natural language re-
searchers) served as our human zero detectors.  
They were asked to intuitively identify missing 
arguments in each clause.  We used average 
human performance as a baseline against which 
to evaluate ZD output.  Here, zeros detected by 
three or more, out of five, subjects were regarded 
as average human performance. 
As Table 1 shows, ZD achieved a 73% 
per-clause matching rate with human output.  
That number represents the ratio of the number 
of exact matches between the two outputs over 
the total number of clauses. 
 
Table 1: Per-clause matching rates 
 # of clauses # of matched 
Reading (1) 30 22 (73%) 
Reading (2) 25 18 (72%) 
Writing 23 17 (74%) 
Total 78 57 (73%) 
 
A closer examination of each case element 
(ga, wo, ni) is given in Table 2 (next page).  The 
level ?matched? includes both cases where ZD 
and human detect a zero and cases where neither 
detects it.  The accuracy (89% average) is high 
enough for the ZD output to be put into practical 
use as a learning aid, without an excessive load 
on teachers for post-editing output errors.  Re-
leasing teachers from having to spend enormous 
amount of time on the tedious work of analysing 
educational materials is one of the biggest ad-
vantages of computerization of linguistic analy-
sis. 
 
  
 
Table 2: Per-case element matching rates 
? ga ? wo ? ni  
Human ZD Human ZD Huma
n 
ZD 
Detected 35 32 5 4 5 2 
Not Detected 43 39 73 68 73 63 
 
Matched 
Total 78 71 (91%) 78 72 (92%) 78 65 (83%) 
Under-detected 3 1 3 
Over-detected 4 5 10 
 
Not 
Matched Total 
 
7 (9%) 
 
6 (8%) 
 
13 (17%) 
 
 
Also, we analysed ?not matched? cases to 
improve future performance.  There were 26 
cases of both underproduction and overproduc-
tion of zeros.  Nearly half of them, 12 out of 26, 
were caused by our na?ve valency selection algo-
rithm, which selects the first entry from the 
GTVD valency pattern listing for each predicate.  
Three were caused by our canonical-case- 
marker-restoring heuristics, which assign a first 
available case marker from ga and wo in its 
preference order.  They sometimes do not 
function properly when accusatives or adjuncts 
are topicalized (or focused).  These are two 
major areas for future enhancement.  Four cases 
were affected by morphological/sysntactic 
analyses.  Also, our definition of obligatory 
arguments, which excludes dative ?ni, produced 
three ?not matched? cases.  This definition is 
also an issue for further consideration. 
What should be noted here, on the other 
hand, is that there were six ZD produced zeros 
which did not match our human zero detectors? 
decision but whose validity was later confirmed 
by a JSL teacher who carefully examined the 
result from an instructional point of view.  This 
implies that human-recognized zeros and 
linguistically/pedagogically plausible zeros do 
not always match, and demonstrates the potential 
of ZD to fill this gap. 
4.2 Experiment 
In order to verify the pedagogical effectiveness 
of ZD, the output files were experimentally used 
in a university-level intermediate JSL classroom, 
through digital presentation.  The aim of this 
lesson was to familiarize the students with zeros 
by making these invisible elements visible in 
texts and presenting their underlying structures. 
In their post-lesson feedback, the students 
showed a positive reaction to this analytic in-
struction.  They described this approach as ?in-
novative?, ?effective?, ?clear? and ?easy? for 
understanding zeros, in contrast to their past ?just 
guessing or being lost? experiences. 
The teacher who conducted this experimen-
tal lesson also acknowledged the impact of ZD 
on effective instruction.  She pointed out the 
following benefits for students: 
 
(i) The valency checking segment of 
output (B) helps students realize that 
each predicate has its own valency 
pattern, and as a consequence, clarifies 
when to use what particles, 
(ii) the predicate-argument structures with 
zeros, output (C), help students realize 
that locating zeros is not a random op-
eration, but a canonical designation, 
and 
(iii) the clause-by-clause parallel arrange-
ment in output (D) facilitates realizing 
zero distributions in discourse and 
tracking down antecedents for each 
zero. 
 
These include positive side effects that we ini-
tially did not foresee. 
From a teaching point of view, ZD helps 
teachers predict the difficulties with zeros that 
students might encounter, by analysing text in 
advance.  This leads to the careful selection of 
teaching materials and the well-thought-out crea-
tion of reading comprehension questions and 
tests.  Also, ZD output will be helpful in ex-
plaining the illegal use of zeros and particles 
found in students? writing. 
  
 
Conclusions and Future Work 
We have developed an automatic zero detecting 
program that is intended mainly to serve as 
teacher support.  The program has proven to be 
pedagogically feasible in terms of its accuracy 
and its impact on effective instruction.  The 
great contribution of ZD is to introduce consis-
tency and systematic analysis into an area where 
human intuitions play a dominant, but not always 
accurate and effective, role. 
ZD is currently a purely syntactic-based tool 
that utilizes only surface-level heuristics, ex-
cluding any semantic cues.  As our error analy-
sis in Section 4 indicates, more accuracy can be 
achieved in a semantically enhanced version, 
which in fact is our next project goal.  
Valency-pattern-selecting (from GTVD) and 
canonical-case-marker-restoring (from TP and 
FP) algorithms are two major areas to which 
semantic information can greatly contribute. 
Also, ZD has been designed as a teaching 
aid in a teacher-controlled class instruction mode.  
To extend its use to a self-study mode, as some 
students suggested, clear guidance and a 
user-friendly interface will be required to replace 
teachers? explanation. 
ZD is a part of the CALL program for JSL 
learners, Zero Checker, which supports reading 
comprehension and writing revision process with 
a focus on zeros.  Thus, ZD will also serve as a 
pre-processing module for the models of resolv-
ing and generating zeros, created within the cen-
tering framework (e.g., Grosz et al, 1995). 
 
References  
Chapelle, Carol A. (1998). Multimedia CALL: 
Lessons to be learned from research on instructed 
SLA. Language Learning and Technology, vol.2, 
no.1, pp.22-34. 
Grosz, B. J., A. K. Joshi and S. Weinstein. (1995). 
Centering: A framework for modelling the local 
coherence of discourse. Computational Linguis-
tics, 21/2, pp. 203-225. 
Ikehara, S., M. Miyazaki, S. Shirai, A. Yokoo, H. 
Nakaiwa, K. Ogura and Y. Hayashi (1997). 
Goi-Taikei ? A Japanese Lexicon, 5 volumes, 
Iwanami Shoten, Tokyo. 
Krashen, S. (1982). Principles and Practice in 
Second Language Acquisition. Pergamon, Ox-
ford. 
NAIST, Kudo, K. (2001). CaboCha 0.21. 
http://cl.aist-nara.ac.jp/~taku-ku/software/caboch
a/ 
NAIST, Matsumoto, Y. et al (2001). ChaSen 2.2.8. 
http://chasen.aist-nara.ac.jp/ 
Ishiwata, T. (1999). Gendai GengoRiron to Kaku, 
Hituzi Shobo, Tokyo. 
Norris, J. M. and L. Ortega (2000). Effectiveness of 
L2 instruction: A research synthesis and quantita-
tive meta-analysis. Language Learning 50 (3), 
pp.417-528. 
Somers, H. L. (1984). On the validity of the comple-
ment-adjunct distinction in valency grammar. Lin-
guistics 22, pp. 507-53. 
Approaches to Zero Adnominal Recognition 
Mitsuko Yamura-Takei 
Graduate School of Information Sciences 
Hiroshima City University 
Hiroshima, JAPAN 
yamuram@nlp.its.hiroshima-cu.ac.jp 
 
Abstract 
This paper describes our preliminary at-
tempt to automatically recognize zero ad-
nominals, a subgroup of zero pronouns, in 
Japanese discourse.  Based on the corpus 
study, we define and classify what we call 
?argument-taking nouns (ATNs),? i.e., 
nouns that can appear with zero adnomi-
nals.  We propose an ATN recognition al-
gorithm that consists of lexicon-based 
heuristics, drawn from the observations of 
our analysis.  We finally present the result 
of the algorithm evaluation and discuss 
future directions. 
1 Introduction 
(1) Zebras always need to watch out for lions.  
Therefore, even while eating grass, so that able 
to see behind, eyes are placed at face-side.  
 
This is a surface-level English translation of a 
naturally occurring ?unambiguous? Japanese dis-
course.  By ?unambiguous,? we mean that Japa-
nese speakers find no difficulty in interpreting this 
discourse segment, including whose eyes are being 
talked about.  Moreover, Japanese speakers find 
this segment quite ?coherent,? even though there 
seems to be no surface level indication of who is 
eating or seeing, or whose eyes are being men-
tioned in this four-clause discourse segment. 1  
However, this is not always the case with Japanese 
as a Second Language (JSL) learners.2 
What constitutes ?coherence? has been studied 
by many researchers.  Reference is one of the lin-
guistic devices that create textual unity, i.e., cohe-
                                                          
1 This was verified by an informal poll conducted on 15 native 
speakers of Japanese. 
2 Personal communication with a JSL teacher. 
sion (Halliday and Hasan, 1976).  Reference also 
contributes to the semantic continuity and content 
connectivity of a discourse, i.e., coherence.  Co-
herence represents the natural and reasonable con-
nections between utterances that make for easy 
understanding, and thus lower inferential load for 
hearers. 
The Japanese language uses ellipsis as its major 
type of referential expression.  Certain elements 
are ellipted when they are recoverable from a given 
context or from relevant knowledge.  These ellip-
ses may include verbals and nominals; the missing 
nominals have been termed ?zero pronouns,? ?zero 
pronominals,? ?zero arguments,? or simply ?zeros? 
by researchers. 
How many zeros are contained in (1), for ex-
ample, largely depends on how zeros are defined.  
In the literature, zeros are usually defined as ele-
ments recoverable from the valency requirements 
of the predicate with which they occur.  However, 
does this cover all the zeros in Japanese?  Does this 
explain all the content connectivity created by 
nominal ellipsis in Japanese? 
In this paper, we introduce a subgroup of zeros, 
what we call ?zero adnominals,? in contrast to 
other well-recognized ?zero arguments? and inves-
tigate possible approaches to recognizing these 
newly-defined zeros, in an attempt to incorporate 
them in an automatic zero detecting tool for JSL 
teachers that aims to promote effective instruction 
of zeros.  In section 2, we provide the definition of 
zero adnominals, and present the results of their 
manual identification in the corpus. Section 3 de-
scribes the theoretical and pedagogical motivations 
for this study.    Section 4 illustrates the syntac-
tic/semantic classification of the zero adnominal 
examples found in the corpus.  Based on the classi-
fication results, we propose lexical information-
based heuristics, and present a preliminary evalua-
tion.  In the final two sections, we present related 
work, and discuss possible future directions. 
2 Zero Adnominals 
2.1 Definition 
Recall the discourse segment in (1).  Its original 
Japanese is analyzed in (2). 
 
(2)  a. simauma-wa  raion ni   itumo  
          zebra-TOP     lion-DAT  always 
ki-o-tuke-nakereba-narimasen. 
watch-out-for-need-to 
?Zebras always need to watch out for lions.? 
 
b. desukara,  ? kusa-o  tabete-ite-mo, 
so      ?-NOM grass-ACC eating-even-while 
?So even while (they) are eating grass,? 
 
c. ? ? usiro-no-ho-made             mieru-yo-ni 
?-NOM ?-ADN-behind-even  see-can-for  
?so that (they) can see even what is  
behind (them),? 
 
d. ? me-ga                 ? kao-no-yoko-ni    
?-ADN-eye-NOM ?-ADN-face-side LOC 
      tuite-imasu. 
placed-be 
?(their)eyes are on the sides of (their) faces.? 
  
Zero arguments are unexpressed elements that are 
predictable from the valency requirements of their 
heads, i.e., a given predicate of the clause.  Zero 
nominatives in (2b) and (2c) are of this type.  Zero 
adnominals, analogously, are missing elements that 
can be inferred from some features specified by 
their head nouns.  A noun for body-part, me ?eyes? 
in (2d) usually calls hearers? attention to ?of-
whom? information and hearers recover that in-
formation in the flow of discourse.  That missing 
information can be supplied by a noun phrase (NP) 
followed by an adnominal particle no, i.e., si-
mauma-no ?zebras?(= their)? in the case of (2d) 
above.  Hence, as a first approximation, we define 
a zero adnominal as an unexpressed ?NP no? in the 
NP no NP (a.k.a., A no B) construction. 
2.2 The Corpus 
Before we proceed, we will briefly describe the 
corpus that we investigated.  The corpus consists 
of a collection of 83 written narrative texts taken 
from seven different JSL textbooks with levels 
ranging from beginning to intermediate.  Thus, it is 
a representative sample of naturally-occurring, but 
maximally canonical, free-from-deviation, and co-
herent narrative discourse. 
2.3 Identification 
Our primary goal is to identify relevant informa-
tion for recognizing zero adnominals.  Since such 
information is unavailable in the surface text, the 
identification of missing adnominal elements and 
their referents in the corpus was based on the na-
tive speaker intuitions and the linguistic expertise 
of the author, who used the definition in 2.1, with 
occasional consultation with a JSL teaching ex-
pert/linguist.  As a result, we located a total of 320 
zero adnominals.  These adnominals serve as the 
zero adnominal samples on which our later analy-
sis is based. 
3 Theoretical/Pedagogical Motivations 
3.1 Centering Analysis 
One discourse account that models the perceived 
degree of coherence of a given discourse in rela-
tion to local focus of attention and the choice of 
referring expressions is centering (e.g., Grosz, 
Joshi and Weinstein, 1995).   
The investigation of zeros behavior in our cor-
pus, within the centering framework, shows that 
zero adnominals make a considerable contribution 
to center continuity in discourse by realizing the 
central entity in an utterance (called Cb) just as 
well-acknowledged zero arguments do. 
Recall example (2). Its center data structure is 
given in (3).  The Cf (forward-looking center) list 
is a set of discourse entities that appear in each 
utterance (Ui).  The Cb (backward-looking center) 
is a special member of the Cf list, and is meant to 
represent the entity that the utterance is most cen-
trally about; it is the most highly ranked element of 
the Cf (Ui-1) that is realized in Ui. 
 
(3) a. Cb: none   [Cf: zebra, lion] 
      b.  Cb: zebra  [Cf: zebra, grass] 
      c. Cb: zebra [Cf: zebra, what is behind] 
      d.  Cb: zebra [Cf: zebra, eye, face-side] 
 
In (3b) and (3c), the Cb is realized as a zero nomi-
native, and in (3d), it is realized by the same entity 
(zebra) as a zero adnominal, maintaining the 
CONTINUE transition that by definition is maxi-
mally coherent.  This matches the intuitively per-
ceived degree of coherence in the utterance.  Our 
corpus contains a total of 138 zero adnominals that 
refer to previously mentioned entities (15.56% of 
all the zero Cbs), and realize the Cb of the utter-
ance in which they occur, as in (3d=2d).  
Our corpus study shows that discourse coher-
ence can be more accurately characterized, in the 
centering account, by recognizing the role of zero 
adnominals as a valid realization of Cbs (see Ya-
mura-Takei et al, ms. for detailed discussion).  
This is our first motivation towards zero adnominal 
recognition. 
3.2 Zero Detector 
Yamura-Takei et al (2002) developed an auto-
matic zero identifying tool.  This program, Zero 
Detector (henceforth, ZD) takes Japanese written 
narrative texts as input and provides the zero-
specified texts and their underlying structures as 
output.  This aims to draw learners? and teachers? 
attention to zeros, on the basis of a hypothesis 
about ideal conditions for second language acquisi-
tion, by making invisible zeros visible.  ZD regards 
teachers as its primary users, and helps them pre-
dict the difficulties with zeros that students might 
encounter, by analyzing text in advance.  Such dif-
ficulties often involve failure to recognize dis-
course coherence created by invisible referential 
devices, i.e., the center continuity maintained by 
the use of various types of zeros. 
As our centering analysis above indicates, in-
clusion of zero adnominals into ZD?s detecting 
capability enables a more comprehensive coverage 
of the zeros that contributes to discourse coherence.  
This is our project goal. 
4 Towards Zero Adnominal Recognition 
4.1 Semantic Classification 
Unexpressed elements need to be predicted from 
other expressed elements.  Thus, we need to char-
acterize B nouns (which are overt) in the (A no) B 
construction, assuming that zero adnominals (A) 
are triggered by their head nouns (B) and that cer-
tain types of NPs tend to take implicit (A) argu-
ments.  Our first approach is to use an existing A 
no B classification scheme.  We adopted, from 
among many A no B works, a classification mod-
eled on Shimazu, Naito and Nomura (1985, 1986, 
and 1987) because it offers the most comprehen-
sive classification (Fais and Yamura-Takei, ms).  
Table 1 below describes the five main groups that 
we used to categorize (A no) B phrases. 
4.2 Results 
We classified our 320 ?(A no) B? examples into 
the five groups described in the previous section.  
Group V comprised the vast majority, while ap-
proximately the same percentage of examples was 
included in Groups I, II and III.  There were no 
Group IV examples.  The number and percentage 
of examples of each group are presented in Table 2. 
 
Group # of examples 
I  33 (10.31%) 
II  23 (  7.19%) 
III  35 (10.94%) 
IV   0 (  0.00%) 
V 229 (71.56%) 
Total 320    (100%) 
Table 2: Distribution of semantic types 
Group # Definition Example from Shimazu et al (1986) 
I A: argument B: nominalized verbal element 
kotoba no rikai 
?word-no-understanding? 
II A: noun denoting an entity B: abstract relational noun 
biru no mae 
?building-no-front? 
III A: noun denoting an entity B: abstract attribute noun 
hasi no nagasa 
?bridge-no-length? 
IV A: nominalized verbal element B: argument 
kenka no hutari 
?argument-no-two people? 
V A: noun expressing attribute B: noun denoting an entity 
ningen no atama 
?human-no-head? 
Table 1: (A no) B classification scheme 
We conjecture that certain nouns are more 
likely to take zero adnominals than others, and that 
the head nouns which take zero adnominals, ex-
tracted from our corpus, are representative samples 
of this particular group of nouns.  We call them 
?argument-taking nouns (ATNs).?  ATNs syntacti-
cally require arguments and are semantically de-
pendent on their arguments. We use the term ATN 
only to refer to a particular group of nouns that can 
take implicit arguments (i.e., zero adnominals). 
We closely examined the 127 different ATN 
tokens among the 320 cases of zero adnominals 
and classified them into the four types that corre-
spond to Groups I, II, III and V in Table 1.  We 
then listed their syntactic/semantic properties 
based on the syntactic/semantic properties pre-
sented in the Goi-Taikei Japanese Lexicon (hereaf-
ter GT, Ikehara, Miyazaki, Shirai, Yokoo, Nakaiwa, 
Ogura, Oyama, and Hayashi, 1997).    GT is a se-
mantic feature dictionary that defines 300,000 
nouns based on an ontological hierarchy of ap-
proximately 2,800 semantic attributes.  It also uses 
nine part-of-speech codes for nouns.  Table 3 lists 
the syntactic/semantic characterizations of the 
nouns in each type and the number of examples in 
the corpus.  What bold means in the table will be 
explained later in section 4.3. 
Type Syntactic properties Semantic properties # Examples 
Human activity 21 zikosyokai ?self-introduction? I Nominalized verbal, de-
rived  (from verb) noun, 
common noun 
phenomenon 3 entyo ?extension? 
Location 13 mae ?front? II formal noun, common 
noun Time 1 yokuzitu ?next day? 
Amount 9 sintyo ?height? 
Value 2 nedan ?price? 
Emotion 1 kimoti ?feeling? 
Material phenomenon 1 nioi ?smell? 
Name 1 namae ?name? 
III Derived (from verb/ad-
jective) noun, suffix 
noun, common noun 
Order 1 ichiban ?first? 
Human (kinship) 14 haha ?mother? 
Animate (body-part) 14 atama ?head? 
Organization 7 kaisya ?company? 
Housing (part) 7 doa ?door? 
Human (profession) 4 sensei ?teacher? 
Human (role) 4 dokusya ?reader? 
Human (relationship) 3 dooryoo ?colleague? 
Clothing 3 kutu ?shoes? 
Tool 2 saihu ?purse? 
Human (biological feature) 2 zyosei ?woman? 
Man-made 2 kuruma ?car? 
Facility 1 byoin ?hospital? 
Building 1 niwa ?garden? 
Housing (body) 1 gareeji ?garage? 
Housing (attachment) 1 doa ?door? 
Creative work 1 sakuhin ?work? 
Substance 1 kuuki ?air? 
Language 1 nihongo ?Japanese? 
Document 1 pasupooto ?passport? 
Chart 1 chizu ?map? 
Animal 1 petto ?pet? 
V Common noun 
? (unregistered) 2 hoomusutei ?homestay? 
 Total 127  
Table 3: Subtypes of ATNs
When we examine these four types, we see that 
they partially overlap with some particular types of 
nouns studied theoretically in the literature. Tera-
mura (1991) subcategorizes locative relational 
nouns like mae ?front?, naka ?inside?, and migi 
?right? as ?incomplete nouns? that require elements 
to complete their meanings; these are a subset of 
Type II.  Iori (1997) argues that certain nouns are 
categorized as ?one-place nouns,? in which he 
seems to include Type I and some of Type V nouns.  
Kojima (1992) examines so-called ?low-
independence nouns? and categorizes them into 
three types, according to their syntactic behaviors 
in Japanese copula expressions. These cover sub-
sets of our Type I, II, III and V.   In computational 
work, Bond, Ogura, and Ikehara (1995) extracted 
205 ?trigger nouns? from a corpus aligned with 
English. These nouns trigger the use of possessive 
pronouns when they are machine-translated into 
English.  They seem to correspond mostly to our 
Type V nouns.  Our result offers a comprehensive 
coverage which subsumes all of the types of nouns 
discussed in these accounts. 
Next, let us more closely look at the properties 
expressed by our samples. The most prevalent 
ATNs (21 in number) are nominalized verbals in 
the semantic category of human activity.  The next 
most common are kinship nouns (14 in number) 
and body-part nouns (14), both in the common 
noun category; location nouns (13), either in the 
common noun or formal noun category; and nouns 
that express amount (9) whose syntactic category 
is either common or de-adjectival.  The others in-
clude some ?human? subcategories, etc.  
The part-of-speech subcategory, ?nominalized 
verbal? (sahen-meishi) is a reasonably accurate 
indicator of Type 1 nouns.  So is ?formal noun? 
(keishiki-meishi) for Type II, although this does not 
offer a full coverage of this type.  Numeral noun 
and counter suffix noun compounds also represent 
a major subset of Type III. 
Semantic properties, on the other hand, seem 
helpful to extract certain groups such as location 
(Type II), amount (Type III), kinship, body-part, 
organization, and some human subcategories (Type 
V).  But other low-frequency ATN samples are 
problematic for determining an appropriate level of 
categorization in GT?s semantic hierarchy tree.   
4.3 Algorithm 
Our goal is to build a system that can identify the 
presence of zero adnominals.  In this section, we 
propose an ATN (hence zero adnominal) recogni-
tion algorithm.  The algorithm consists of a set of 
lexicon-based heuristics, drawn from the observa-
tions in section 4.2. 
The algorithm takes morphologically-analyzed 
text as input and provides ATN candidates as out-
put.  The process consists of the following three 
phases: (i) bare noun extraction, (ii) syntactic cate-
gory (part-of-speech) checking, and (iii) semantic 
category checking. 
Zero adnominals usually co-occur with ?bare 
nouns.? Bare nouns, in our definition, are nouns 
without any pre-nominal modifiers, including de-
monstratives, explicit adnominal phrases, relative 
clauses, and adjectives.3 Bare nouns are often sim-
plex as in (4a), and sometimes are compound (e.g., 
numeral noun + counter suffix noun) as in (4b).  
These are immediately followed by case-marking, 
topic/focus-marking or other particles (e.g., ga, o, 
ni, wa, mo).   
 
(4)  a. atama-ga   head-NOM 
       b. 70-paasento-o  70-percent-ACC  
 
The extracted nouns under this definition are initial 
candidates for ATNs. 
Once bare nouns are identified, they are 
checked against our syntactic-property- (i.e., part-
of-speech, POS) based-, followed by semantic-
attribute (SEM) based-heuristics.  For semantic 
filtering, we decided to use the noun groups of 
high frequency (more than two tokens categorized 
in the same group; indicated in bold in Table 3 
above) to minimize a risk of over-generalization.  
The algorithm checks the following two condi-
tions, for each bare noun, in this order: 
 
[1] If POS = [nominalized verval, derived noun, 
formal noun, numeral + counter suffix com-
pound], label it as ATN. 
 
[2] If SEM = [2610: location, 2585: amount, 
362: organization, 552: animate (part), 111: hu-
man (relation), 224: human (profession), 72: 
                                                          
3 Japanese do not use determiners for its nouns. 
human (kinship), 866: housing (part), 813: cloth-
ing], label it as ATN. 4 
 
Therefore, nouns that pass condition [1] are labeled 
as ATNs, without checking their semantic proper-
ties.  A noun that fails to pass condition [1] and 
passes condition [2] is labeled as ATN.  A noun 
that fails to match both [1] and [2] is labeled as 
non-ATN.  Consider the noun sintyo ?height? for 
example.  Its POS code in GT is common noun, so 
it fails condition [1] and goes to [2].  This noun is 
categorized in the ?2591: measures? group which 
is under the ?2585: amount? node in the hierarchy 
tree, so it is labeled as ATN.  In this way, the algo-
rithm labels each bare noun as either ATN or non-
ATN. 
4.4 Evaluation 
To assess the performance of our algorithm, we ran 
it by hand on a sample text.5  The test corpus con-
tains a total of 136 bare nouns.  We then matched 
the result against our manually-extracted ATNs (34 
in number).  The result is shown in Table 4 below, 
with recall and precision metrics.  As a baseline 
measurement, we give the accuracy for classifying 
every bare noun as ATN.  For comparison, we also 
provide the results when only either POS-based or 
semantic-based heuristics are applied. 
 
 Recall Precision 
Baseline 34/34    (100%) 34/136 (25.00%) 
POS only 2/34 (  5.88%) 2/6 (33.33%) 
Semantic only 30/34 (88.23%) 30/35 (85.71%) 
POS/Semantic 32/34 (94.11%) 32/41 (78.04%) 
Table 4: Algorithm evaluation 
 
Semantic categories make a greater contribution 
to identifying ATNs than POS.  However, the 
POS/Semantic algorithm achieved a higher recall 
but a lower precision than the semantic-only algo-
rithm did. This is mainly because the former pro-
duced more over-detected errors.  Closer 
examination of those errors indicates that most of 
them (8 out of 9 cases) involve verbal idiomatic 
expressions that contain ATN candidate nouns, as 
example (5) shows. 
                                                          
4 These numbers indicate the numbers assigned to each seman-
tic category in Goi-Taikei Japanese Lexicon (GT). 
5 This is taken from the same genre as our corpus for the initial 
analysis, i.e., another JSL textbook. 
(5) me-o-samasu   eye-ACC-wake   ?wake up? 
 
Although me ?eye? is a strong ATN candidate, as in 
example (2) above, case (5) should be treated as 
part of an idiomatic expression rather than as a 
zero adnominal expression.6  Thus, we decided to 
add another condition, [0] below, before we apply 
the POS/SEM checks.  The revised algorithm is as 
follows: 
 
[0] If part of idiom in [idiom list],7 label it as 
non-ATN. 
 
[1] If POS = [nominalized verval, derived noun, 
formal noun, numeral + counter suffix com-
pound], label it as ATN. 
 
[2] If SEM = [2610: location, 2585: amount, 
362: organization, 552: animate (part), 111: hu-
man (relation), 224: human (profession), 72: 
human (kinship), 866: housing (part), 813: cloth-
ing], label it as ATN. 
 
When a noun matches condition [0], it will not be 
checked against [1] and [2].  When this applies, the 
evaluation result is now as shown below. 
 
 Recall Precision 
POS only 2/34 (  5.88%) 2/4 (50.00%) 
Semantic only 30/34 (88.23%) 31/35 (88.57%) 
POS/Semantic 32/34 (94.11%) 32/33 (96.96%) 
Table 5: Revised-algorithm evaluation 
 
The revised algorithm, with both syntac-
tic/semantic heuristics and the additional idiom-
filtering rule, achieved a precision of 96.96%.  The 
result still includes some over/under-detecting er-
rors, which will require future attention. 
5 Related Work 
Associative anaphora (e.g., Poesio and Vieira, 
1998) and indirect anaphora (e.g., Murata and Na-
gao, 2000) are virtually the same phenomena that 
this paper is concerned with, as illustrated in (6). 
 
                                                          
6 Vieira and Poesio (2000) also list ?idiom? as one use of defi-
nite descriptions (English equivalent to Japanese bare nouns), 
along with same head/associative anaphora, etc. 
7 The list currently includes eight idiomatic samples from the 
test data, but it should of course be expanded in the future. 
(6) a. a house ? the roof  
      b. ie ?house? ? yane ?roof? 
      c. ie ?house? ? (?-no) yane ?(??s) roof? 
 
We take a zero adnominal approach, as in (6c), 
because we assume, for our pedagogical purpose 
discussed in section 3.2, that zero adnominals, by 
making them visible, more effectively prompt peo-
ple to notice referential links than lexical relations, 
such as meronymy in (6a) and (6b).  
However, insights from other approaches are 
worth attention.  There is a strong resemblance 
between bare nouns (that zero adnominals co-occur 
with) in Japanese and definite descriptions in Eng-
lish in their behaviors, especially in their referen-
tial properties (Sakahara, 2000).  The task of 
classifying several different uses of definite de-
scriptions (Vieira and Poesio, 2000; Bean and 
Riloff, 1999) is somewhat analogous to that for 
bare nouns.  Determining definiteness of Japanese 
noun phrases (Heine, 1998; Bond et al, 1995; Mu-
rata and Nagao, 1993)8 is also relevant to ATN 
(which is definite in nature) recognition.   
6 Future Directions 
We have proposed an ATN (hence zero adnomi-
nal) recognition algorithm, with lexicon-based heu-
ristics that were inferred from our corpus 
investigation.  The evaluation result shows that the 
syntactic/semantic feature-based generalization 
(using GT) is capable of identifying potential 
ATNs.   The evaluation on a larger corpus, of 
course, is essential to verify this claim.  Implemen-
tation of the algorithm is also in our future agenda. 
This approach has its limitations, too, as is 
pointed out by Kurohashi et al (1999).  One limi-
tation is illustrated by a pair of Japanese nouns, 
sakusya ?author? and sakka ?writer,? which fall un-
der the same GT semantic property group (at the 
deepest level).9  These nouns have an intuitively 
different status for their valency requirements; the 
former requires ?of-what work? information, while 
the latter does not. 10   We risk over- or under-
generation when we designate certain semantic 
properties, no matter how fine-grained they might 
                                                          
8 Their interests are in machine-translation of Japanese into 
languages that require determiners for their nouns. 
9 This example pair is taken from Iori (1997). 
10 This intuition was verified by an informal poll conducted on 
seven native speakers of Japanese.   
be.  We proposed the idiom-filtering rule to solve 
one case of over-detection.  A larger-scale evalua-
tion of the algorithm and its error analysis might 
lead to additional rules that refine extracted ATN 
candidates.  Insights from the works presented in 
the previous section could also be incorporated. 
Determining an appropriate level of generaliza-
tion is a significant factor for this type of approach, 
and this was done, in this study, according to our 
introspective judgments.  More systematic methods 
should be explored. 
A related issue is the notoriously hard-to-define 
argument-adjunct distinction for nouns, which is 
closely related to the distinction between ATNs 
and non-ATNs. We experimentally tested seven 
native-Japanese-speaking subjects in distinguish-
ing these two.  We presented 26 nouns in the same 
GT semantic category (at the deepest level): ?per-
sons who write.? There were six nouns which all 
the subjects agreed on categorizing as ATNs, in-
cluding sakusha ?author.?  Five nouns, including 
sakka ?writer,? on the other hand, were judged as 
non-ATNs by all the subjects.   For the remaining 
15 nouns, however, their judgments varied widely.  
As Somers (1984) suggests for verbs, binary dis-
tinction does not work well for nouns, either.  This 
distinction might largely depend on the context in 
some cases.  This is also something we will need to 
address.   
In this study, we focused on ?implicit argu-
ment-taking nouns.?   There may be a line (al-
though it may be very thin) between nouns which 
take explicit arguments and those which take im-
plicit arguments.  This distinction also needs fur-
ther investigation in the corpus. 
 
Acknowledgements 
Some of the foundation work for this paper was 
done while the author was at NTT Communication 
Science Laboratories, NTT Corporation, Japan, as 
a research intern.  The author would like to thank 
Laurel Fais and Miho Fujiwara for their support, 
and anonymous reviewers for their insightful 
comments and suggestions that helped elaborate an 
earlier draft into this paper. 
 
 
References 
Bean, David L. and Ellen Riloff. 1999. Corpus-based 
identification of non-anaphoric noun phrases. In Pro-
ceedings of the 37th Annual Meeting of the ACL, 373-
380. 
Bond, Francis, Kentaro Ogura, and Satoru Ikehara. 1995. 
Possessive pronouns as determiners in Japanese-to-
English machine translation.  In Proceedings of the 
2nd Pacific Association for Computational Linguistics 
conference. 
Bond, Francis, Kentaro Ogura, and Tsukasa Kawaoka. 
1995. Noun phrase reference in Japanese-to-English 
machine translation. In Proceedings of the 6th Inter-
national Conference on Theoretical and Methodo-
logical Issues in Machine Translation, 1-14. 
Fais, Laurel and Mitsuko Yamura-Takei (under review). 
Salience ranking in centering: The case of a Japanese 
complex nominal. ms. 
Grosz, Barbara J., Aravind Joshi, and Scott Weinstein. 
1995. Centering: a framework for modeling the local 
coherence of discourse. Computational Linguistics, 
21(2), 203-225. 
Halliday, M.A.K. and Ruqaiya Hasan. 1976. Cohesion 
in English. Longman, New York. 
Heine, Julia E. 1998. Definiteness prediction for Japa-
nese noun phrases. In Proceedings of the 
COLING/ACL?98, Quebec, 519-525. 
Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shirai, 
Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura, and 
Yoshifumi Oyama, editors. 1997. Goi-Taikei ? Japa-
nese Lexicon. Iwanami Publishing, Tokyo. 
Iori, Isao. 1997. Aspects of Cohesion in Japanese Texts. 
Unpublished PhD dissertation, Osaka University (in 
Japanese). 
Kojima, Sachiko. 1992. Low-independence nouns and 
copula expressions. In IPA Technical Report No. 3-
125, 175-198 (in Japanese). 
Kurohashi and Sakai. 1999. Semantic analysis of Japa-
nese noun phrases: A new approach to dictionary-
based understanding. In Proceedings of the 37th An-
nual Meeting of the ACL, 481-488. 
Murata, Masaki and Makoto Nagao. 1993. Determina-
tion of referential property and number of nouns in 
Japanese sentences for machine translation into Eng-
lish.  In Proceedings of the 5th International Confer-
ence on Theoretical and Methodological Issues in 
Machine Translation, 218-225. 
 
Murata, Masaki and Makoto Nagao. 2000.  Indirect ref-
erence in Japanese sentences. In Botley, S. and 
McEnerry, A. (eds.) Corpus-based and Computa-
tional Approaches to Discourse Anaphora, 189-212.  
John Benjamins, Amsterdam/Philadelphia. 
Poesio, Massimo and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2): 183-216. 
Sakahara, Shigeru. 2000. Advances in Cognitive Lin-
guistics. Hituzi Syobo Publishing, Tokyo, Japan (in 
Japanese). 
Shimazu, Akira, Shozo Naito, and Hirosato Nomura. 
1985. Classification of semantic structures in Japa-
nese sentences with special reference to the noun 
phrase (in Japanese).  In Information Processing So-
ciety of Japan, Natural Language Special Interest 
Group Technical Report, No. 47-4. 
Shimazu, Akira, Shozo Naito, and Hirosato Nomura. 
1986. Analysis of semantic relations between nouns 
connected by a Japanese particle ?no.? Mathematical 
Linguistics, 15(7), 247-266 (in Japanese). 
Shimazu, Akira, Shozo Naito, and Hirosato Nomura. 
1987. Semantic structure analysis of Japanese noun 
phrases with adnominal particles. In Proceedings of 
the 25th Annual Meeting of the ACL, Stanford, 123-
130. 
Somers, Harold L. 1984. On the validity of the comple-
ment-adjunct distinction in valency grammar. Lin-
guistics 22, 507-53. 
Teramura, Hideo. 1991. Japanese Syntax and Meaning 
II. Kurosio Publishers, Tokyo (in Japanese). 
Vieira, Renata and Massimo Poesio. 2000. An empiri-
cally based system for processing definite descrip-
tions. Computational Linguistics, 26(4): 525-579. 
Yamura-Takei, Mitsuko, Laurel Fais, Miho Fujiwara 
and Teruaki Aizawa. 2003. Forgotten referential 
links in Japanese discourse and centering. ms. 
Yamura-Takei, Mitsuko, Miho Fujiwara, Makoto Yo-
shie, and Teruaki Aizawa. 2002.  Automatic linguis-
tic analysis for language teachers: The case of zeros.  
In Proceedings of the 19th International Conference 
on Computational Linguistics (COLING), Taipei, 
1114-1120. 
 
