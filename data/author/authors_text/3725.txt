Proceedings of the Second Workshop on Statistical Machine Translation, pages 193?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multi-Engine Machine Translation
with an Open-Source Decoder for Statistical Machine Translation
Yu Chen1, Andreas Eisele1,2, Christian Federmann2,
Eva Hasler3, Michael Jellinghaus1, Silke Theison1
(authors listed in alphabetical order)
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
3: University of Cologne, Germany
Abstract
We describe an architecture that allows
to combine statistical machine translation
(SMT) with rule-based machine translation
(RBMT) in a multi-engine setup. We use a
variant of standard SMT technology to align
translations from one or more RBMT sys-
tems with the source text. We incorporate
phrases extracted from these alignments into
the phrase table of the SMT system and use
the open-source decoder Moses to find good
combinations of phrases from SMT training
data with the phrases derived from RBMT.
First experiments based on this hybrid archi-
tecture achieve promising results.
1 Introduction
Recent work on statistical machine translation has
led to significant progress in coverage and quality of
translation technology, but so far, most of this work
focuses on translation into English, where relatively
simple morphological structure and abundance of
monolingual training data helped to compensate for
the relative lack of linguistic sophistication of the
underlying models. As SMT systems are trained on
massive amounts of data, they are typically quite
good at capturing implicit knowledge contained in
co-occurrence statistics, which can serve as a shal-
low replacement for the world knowledge that would
be required for the resolution of ambiguities and the
insertion of information that happens to be missing
in the source text but is required to generate well-
formed text in the target language.
Already before, decades of work went into the im-
plementation of MT systems (typically rule-based)
for frequently used language pairs1, and these sys-
tems quite often contain a wealth of linguistic
knowledge about the languages involved, such as
fairly complete mechanisms for morphological and
syntactic analysis and generation, as well as a large
number of bilingual lexical entries spanning many
application domains.
It is an interesting challenge to combine the differ-
ent types of knowledge into integrated systems that
could then exploit both explicit linguistic knowledge
contained in the rules of one or several conventional
MT system(s) and implicit knowledge that can be
extracted from large amounts of text.
The recently started EuroMatrix2 project will ex-
plore this integration of rule-based and statistical
knowledge sources, and one of the approaches to
be investigated is the combination of existing rule-
based MT systems into a multi-engine architecture.
The work described in this paper is one of the
first incarnations of such a multi-engine architec-
ture within the project, and a careful analysis of the
results will guide us in the choice of further steps
within the project.
2 Architectures for multi-engine MT
Combinations of MT systems into multi-engine ar-
chitectures have a long tradition, starting perhaps
with (Frederking and Nirenburg, 1994). Multi-
engine systems can be roughly divided into simple
1See (Hutchins et al, 2006) for a list of commercial MT
systems
2See http://www.euromatrix.net
193
Figure 1: Architecture for multi-engine MT driven
by a SMT decoder
architectures that try to select the best output from a
number of systems, but leave the individual hypothe-
ses as is (Tidhar and Ku?ssner, 2000; Akiba et al,
2001; Callison-Burch and Flournoy, 2001; Akiba et
al., 2002; Nomoto, 2004; Eisele, 2005) and more so-
phisticated setups that try to recombine the best parts
from multiple hypotheses into a new utterance that
can be better than the best of the given candidates,
as described in (Rayner and Carter, 1997; Hogan and
Frederking, 1998; Bangalore et al, 2001; Jayaraman
and Lavie, 2005; Matusov et al, 2006; Rosti et al,
2007).
Recombining multiple MT results requires find-
ing the correspondences between alternative render-
ings of a source-language expression proposed by
different MT systems. This is generally not straight-
forward, as different word order and errors in the
output can make it hard to identify the alignment.
Still, we assume that a good way to combine the var-
ious MT outcomes will need to involve word align-
ment between the MT output and the given source
text, and hence a specialized module for word align-
ment is a central component of our setup.
Additionally, a recombination system needs a way
to pick the best combination of alternative building
blocks; and when judging the quality of a particu-
lar configuration, both the plausibility of the build-
ing blocks as such and their relation to the context
need to be taken into account. The required opti-
mization process is very similar to the search in a
SMT decoder that looks for naturally sounding com-
binations of highly probable partial translations. In-
stead of implementing a special-purpose search pro-
cedure from scratch, we transform the information
contained in the MT output into a form that is suit-
able as input for an existing SMT decoder. This has
the additional advantage that resources used in stan-
dard phrase-based SMT can be flexibly combined
with the material extracted from the rule-based MT
results; the optimal combination can essentially be
reduced to the task of finding good relative weights
for the various phrase table entries.
A sketch of the overall architecture is given in
Fig. 1, where the blue (light) parts represent the
modules and data sets used in purely statistical MT,
and the red (dark) parts are the additional modules
and data sets derived from the rule-based engines. It
should be noted that this is by far not the only way
to combine systems. In particular, as this proposed
setup gives the last word to the SMT decoder, we
risk that linguistically well-formed constructs from
one of the rule-based engines will be deteriorated in
the final decoding step. Alternative architectures are
under exploration and will be described elsewhere.
3 MT systems and other knowledge
sources
For the experiments, we used a set of six rule-based
MT engines that are partly available via web inter-
faces and partly installed locally. The web based
systems are provided by Google (based on Systran
for the relevant language pairs), SDL, and ProMT
which all deliver significantly different output. Lo-
cally installed systems are OpenLogos, Lucy (a re-
cent offspring of METAL), and translate pro by lin-
genio (only for German? English). In addition to
these engines, we also used the scripts included in
the Moses toolkit (Koehn et al, 2006)3 to generate
phrase tables from the training data. We enhanced
the phrase tables with information on whether a
given pair of phrases can also be derived via a third,
intermediate language. We assume that this can be
useful to distinguish different degrees of reliability,
but due to lack of time for fine-tuning we could not
yet show that it indeed helps in increasing the overall
quality of the output.
3see http://www.statmt.org/moses/
194
4 Implementation Details
4.1 Alignment of MT output
The input text and the output text of the MT systems
was aligned by means of GIZA++ (Och and Ney,
2003), a tool with which statistical models for align-
ment of parallel texts can be trained. Since training
new models on merely short texts does not yield very
accurate results, we applied a method where text can
be aligned based on existing models that have been
trained on the Europarl Corpus (Koehn, 2005) be-
forehand. This was achieved by using a modified
version of GIZA++ that is able to load given mod-
els.
The modified version of GIZA++ is embedded
into a client-server setup. The user can send two
corresponding files to the server, and specify two
models for both translation directions from which
alignments should be generated. After generating
alignments in both directions (by running GIZA++
twice), the system also delivers a combination of
these alignments which then serves as input to the
following steps described below.
4.2 Phrase tables from MT output
We then concatenated the phrase tables from the
SMT baseline system and the phrase tables obtained
from the rule-based MT systems and augmented
them by additional columns, one for each system
used. With this additional information it is clear
which of the MT systems a phrase pair stems from,
enabling us to assign relative weights to the con-
tributions of the different systems. The optimal
weights for the different columns can then be as-
signed with the help of minimum error rate training
(Och, 2003).
5 Results
We compared the hybrid system to a purely statis-
tical baseline system as well as two rule-based sys-
tems. The only differences between the baseline sys-
tem and our hybrid system are the phrase table ? the
hybrid system includes more lexical entries than the
baseline ? and the weights obtained from minimum
error rate training.
For a statistical system, lexical coverage becomes
an obstacle ? especially when the bilingual lexical
entries are trained on documents from different do-
mains. However, due to the distinct mechanisms
used to generate these entries, rule-based systems
and statistical systems usually differ in coverage.
Our system managed to utilize lexical entries from
various sources by integrating the phrase tables de-
rived from rule-based systems into the phrase table
trained on a large parallel corpus. Table 1 shows
Systems Token #
Ref. 2091 (4.21%)
R-I 3886 (7.02%)
R-II 3508 (6.30%)
SMT 3976 (7.91%)
Hybrid 2425 (5.59%)
Table 1: Untranslated tokens (excl. numbers and
punctuations) in output for news commentary task
(de-en) from different systems
a rough estimation of the number of untranslated
words in the respective output of different systems.
The estimation was done by counting ?words? (i.e.
tokens excluding numbers and punctuations) that ap-
pear in both the source document and the outputs.
Note that, as we are investigating translations from
German to English, where the languages share a lot
of vocabulary, e.g. named entities such as ?USA?,
there are around 4.21% of words that should stay the
same throughout the translation process. In the hy-
brid system, 5.59% of the words remain unchanged,
which is is the lowest percentage among all systems.
Our baseline system (SMT in Table 1), not compris-
ing additional phrase tables, was the one to produce
the highest number of such untranslated words.
Baseline Hybrid
test 18.07 21.39
nc-test 21.17 22.86
Table 2: Performance comparison (BLEU scores)
between baseline and hybrid systems, on in-domain
(test) and out-of-domain (nc-test) test data
Higher lexical coverage leads to better perfor-
mance as can be seen in Table 2, which compares
BLEU scores of the baseline and hybrid systems,
both measured on in-domain and out-of-domain test
data. Due to time constraints these numbers reflect
195
results from using a single RBMT system (Lucy);
using more systems would potentially further im-
prove results.
6 Outlook
Due to lack of time for fine-tuning the parameters
and technical difficulties in the last days before de-
livery, the results submitted for the shared task do
not yet show the full potential of our architecture.
The architecture described here places a strong
emphasis on the statistical models and can be seen
as a variant of SMT where lexical information from
rule-based engines is used to increase lexical cover-
age. We are currently also exploring setups where
statistical alignments are fed into a rule-based sys-
tem, which has the advantage that well-formed syn-
tactic structures generated via linguistic rules can-
not be broken apart by the SMT components. But
as rule-based systems typically lack mechanisms for
ruling out implausible results, they cannot easily
cope with errors that creep into the lexicon due to
misalignments and similar problems.
7 Acknowledgements
This research has been supported by the European
Commission in the FP6-IST project EuroMatrix. We
also want to thank Teresa Herrmann for helping us
with the Lucy system.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of MT
Summit VIII, Santiago de Compostela, Spain.
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to select
the best among outputs from multiple mt systems. In
COLING.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In ASRU, Italy.
Chris Callison-Burch and Raymond S. Flournoy. 2001.
A program for automatically selecting the best output
from multiple machine translation engines. In Proc. of
MT Summit VIII, Santiago de Compostela, Spain.
Andreas Eisele. 2005. First steps towards multi-engine
machine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, June.
Robert E. Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In ANLP, pages 95?100.
Christopher Hogan and Robert E. Frederking. 1998. An
evaluation of the multi-engine MT architecture. In
Proceedings of AMTA, pages 113?123.
John Hutchins, Walter Hartmann, and Etsuo Ito. 2006.
IAMT compendium of translation software. Twelfth
Edition, January.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. of EAMT, Budapest, Hungary.
P. Koehn, M. Federico, W. Shen, N. Bertoldi, O. Bo-
jar, C. Callison-Burch, B. Cowan, C. Dyer, H. Hoang,
R. Zens, A. Constantin, C. C. Moran, and E. Herbst.
2006. Open source toolkit for statistical machine trans-
lation: Factored translation models and confusion net-
work decoding. Final Report of the 2006 JHU Summer
Workshop.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of the MT
Summit.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In In Proc. EACL, pages 33?40.
Tadashi Nomoto. 2004. Multi-engine machine translation
with voted language model. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Manny Rayner and David M. Carter. 1997. Hybrid lan-
guage processing in the spoken language translator. In
Proc. ICASSP ?97, pages 107?110, Munich, Germany.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Dan Tidhar and Uwe Ku?ssner. 2000. Learning to select a
good translation. In COLING, pages 843?849.
196
Proceedings of the Third Workshop on Statistical Machine Translation, pages 179?182,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Moses to Integrate Multiple Rule-Based Machine Translation Engines
into a Hybrid System
Andreas Eisele1,2, Christian Federmann2, Herve? Saint-Amand1,
Michael Jellinghaus1, Teresa Herrmann1, Yu Chen1
1: Saarland University, Saarbru?cken, Germany
2: DFKI GmbH, Saarbru?cken, Germany
Abstract
Based on an architecture that allows to com-
bine statistical machine translation (SMT)
with rule-based machine translation (RBMT)
in a multi-engine setup, we present new results
that show that this type of system combination
can actually increase the lexical coverage of
the resulting hybrid system, at least as far as
this can be measured via BLEU score.
1 Introduction
(Chen et al, 2007) describes an architecture that
allows to combine statistical machine translation
(SMT) with one or multiple rule-based machine
translation (RBMT) systems in a multi-engine setup.
It uses a variant of standard SMT technology to align
translations from one or more RBMT systems with
the source text and incorporated phrases extracted
from these alignments into the phrase table of the
SMT system. Using this approach it is possible to
employ a vanilla installation of the open-source de-
coder Moses1 (Koehn et al, 2007) to find good com-
binations of phrases from SMT training data with
the phrases derived from RBMT. A similar method
was presented in (Rosti et al, 2007).
This setup provides an elegant solution to the
fairly complex task of integrating multiple MT re-
sults that may differ in word order using only stan-
dard software modules, in particular GIZA++ (Och
and Ney, 2003) for the identification of building
blocks and Moses for the recombination, but the
authors were not able to observe improvements in
1see http://www.statmt.org/moses/
terms of BLEU score. A closer investigation re-
vealed that the experiments had suffered from a cou-
ple of technical difficulties, such as mismatches in
character encodings generated by different MT en-
gines and similar problems. This motivated us to
re-do these experiments in a somewhat more sys-
tematic way for this year?s shared translation task,
paying the required attention to all the technical de-
tails and also to try it out on more language pairs.
2 System Architecture
For conducting the translations, we use a multi-
engine MT approach based on a ?vanilla? Moses
SMT system with a modified phrase table as a cen-
tral element. This modification is performed by aug-
menting the standard phrase table with entries ob-
tained from translating the data with several rule-
based MT systems. The resulting phrase table thus
combines statistically gathered phrase pairs with
phrase pairs generated by linguistic rules.
Basing its decision about the final translation on
the obtained ?combined? phrase table, the SMT de-
coder searches for the best translation by recombin-
ing the building blocks that have been contributed by
the different RBMT systems and the original SMT
system trained on Europarl data.
A sketch of the overall architecture is given in
Fig. 1, where the lighter parts represent the mod-
ules and data sets used in purely statistical MT,
and the darker parts are the additional modules and
data sets derived from the rule-based engines. The
last word in the proposed setup is thus given to the
SMT decoder, which can recombine (and potentially
also tear apart) linguistically well-formed constructs
179
ModelLanguagePhrasetable
Combined
Alignment,PhraseExtraction
DecoderSMT
Rule?basedMT engines
ParallelCorpus
SourceText
TargetText
MonolingualCorpus
Hypotheses
CountingSmoothing
Figure 1: Hybrid architecture of the system
from the rule-based engines? output.
2.1 The Combined Phrase Table
The combined phrase table is built from the orig-
inal Moses phrase table and separate phrase tables
for each of the RBMT systems that are used in our
setup. Since the original phrase table is created
during the training process of the Moses decoder
with the Europarl bilingual corpus as training ma-
terial, it comprises general knowledge about typical
constructions and vocabulary from the Europarl do-
main. Therefore, a standard Moses SMT system is,
in principle, well adapted for input from this do-
main. However, it will have problems in dealing
with vocabulary and structures that did not occur in
the training data. The additional phrase tables are
generated separately for each RBMT system from
the source text and its translation by the respective
system. By using a combined phrase table that in-
cludes the original Moses phrase table as well as the
phrase tables from the RBMT systems, the hybrid
system can both handle a wider range of syntactic
constructions and exploit knowledge that the RBMT
systems possess about the particular vocabulary of
the source text.
3 Implementation
3.1 MT Systems and Knowledge Sources
Apart from the Moses SMT system, we used a
set of six rule-based MT engines that are partly
available via web interfaces and partly installed lo-
cally. The web interfaces are provided by Al-
tavista Babelfish (based on Systran), SDL, ProMT
and Lucy (a recent offspring of METAL). All of
them deliver significantly different output trans-
lations. Locally installed systems are OpenLo-
gos (for German?English, English?Spanish and
English?French) and translatePro by lingenio (for
German?English). The language model for our pri-
mary setup is based on the Europarl corpus whereas
the English Gigaword corpus served as training data
for a contrastive setup that was created for the trans-
lation direction German?English only.
3.2 Alignment of RBMT output
As already mentioned above, the construction of the
RBMT system specific phrase tables is a major part
of the overall system architecture. Such an RBMT
phrase table is generated from a bilingual corpus
consisting of the input text and its translation by
the respective RBMT system. Because this corpus
has the mere size of the text to be translated, it usu-
ally is not big enough to ensure the statistical meth-
ods for phrase table building of the Moses system to
work. Therefore, we create the alignments between
the RBMT input and output with help of another tool
(Theison, 2007) that is based on knowledge learned
in a previously conducted training phase with an ap-
propriately bigger corpus. On the basis of the align-
ments created in this manner, the Moses training
script provides a phrase table that consists of the
source text vocabulary. These steps are carried out
for each one of the six RBMT systems leading to
six source text specific phrase tables which are then
combined with the original Moses phrase table.
3.3 Combination of Phrase Tables
The combination process basically consists of the
concatenation of the Moses phrase table and the pre-
viously created RBMT phrase tables with one mi-
nor adjustment: The phrase table resulting from this
combination now also features additional columns
indicating which system each phrase table entry
originated from. For each new source text, the
RBMT phrase tables have to be created from scratch
and incorporated into a new combined phrase table.
3.4 Tuning
The typical process for creating an SMT system with
the Moses toolkit includes a tuning step in which
180
Europarl NewsCommentary
de-en en-de fr-en en-fr es-en en-es de-en en-de fr-en en-fr es-en en-es
SMT 22.81 19.78 24.18 21.62 31.68 24.46 14.24 9.75 11.60 12.24 17.27 14.48
Hybrid 27.85 20.75 28.12 28.82 33.15 32.31 17.36 13.57 17.66 20.71 22.16 22.55
RBMT1? 13.34 11.09 ?? 17.19 ?? 18.63 14.90 12.34 ?? 15.11 ?? 17.13
RBMT2 16.19 12.06 ?? ?? ?? ?? 16.66 13.64 ?? ?? ?? ??
RBMT3 16.32 10.88 18.18 20.38 19.32 20.89 16.88 12.53 17.20 18.82 19.00 19.98
RBMT4 15.58 12.09 19.00 22.20 18.99 21.69 17.41 13.93 17.73 20.85 19.14 21.70
RBMT5 15.58 9.54 21.36 12.98 18.47 20.59 15.99 11.05 18.65 19.49 20.50 20.02
RBMT6 13.96 9.44 17.16 18.91 18.01 19.18 15.08 10.41 16.86 17.82 18.70 19.97
Table 1: Performance of baseline SMT system, our system and RBMT systems (BLEU scores)
the system searches for the best weight configura-
tion for the columns in the phrase table while given
a development set to be translated, and correspond-
ing reference translations. In our hybrid setup, it is
equally essential to conduct tuning since the com-
bined phrase table we use contains 7 more columns
than the original Moses phrase table. All these
columns are given the same default weight initially
and thus still need be to be tuned to more meaning-
ful values. From this year?s Europarl development
data the first 200 sentences of each of the data sets
dev2006, test2006, test2007 and devtest2006 were
concatenated to build our development set. This set
of 800 sentences was used for Minimum Error Rate
Training (Och, 2003) to tune the weights of our sys-
tem with respect to BLEU score.
4 Results
In order to be able to evaluate our hybrid approaches
in contrast to stand-alone rule-based approaches, we
also calculated BLEU scores for the translations
conducted by the RBMT systems used in the hy-
brid setup. Our hybrid system is compared to a SMT
baseline and all the 6 RBMT systems that we used.
Table 1 shows the evaluation of all the systems in
terms of BLEU score (Papineni et al, 2002) with the
best score highlighted. The empty cells in the table
indicate the language pairs which are not available
in the corresponding systems2. The SMT system is
the one upon which we build the hybrid system. Ac-
cording to the scores, the hybrid system produces
better results than the baseline SMT system in all
2The identities of respective RBMT systems are not revealed
in this paper. RBMT1 is evaluated on the partial results pro-
duced due to some technical problems.
cases. The difference between our system and the
baseline is more significant for out-of-domain tests,
where gaps in the lexicon tend to be more severe.
Figure 2 illustrates an example of how the hy-
brid system differs from the baseline SMT system
and how it benefits from the RBMT systems. The
example lists the English translations of the same
German sentence (from News Commentary test set)
from different systems involved in our experiment.
Neither the word ?Pentecost? nor its German trans-
lation ?Pfingsten? has appeared in the training cor-
pus. Therefore, the SMT baseline system cannot
translate the word and chooses to leave the word
as it is whereas all the RBMT systems translate the
word correctly. The hybrid system appears to have
the corresponding lexicon gap covered by the ex-
tra entries produced by the RBMT systems. On the
other side, these additional entries may not always
be helpful. The errors in RBMT outputs can be sig-
nificant noise that destroys the correct information
in the SMT system. In the example translation pro-
duced by the hybrid system, there is a comma miss-
ing after ?in addition?, which appears to be frequent
in the RBMT outputs.
5 Outlook
The results reported in this paper are still somewhat
preliminary in the sense that many possible (includ-
ing some desirable) variants of the setup could not
be tried out due to lack of time. In particular, we
think that the full power of our approach on out-
of-domain test data can only be exploited with the
help of large language models trained on out-of-
domain text, but could not yet try this systematically.
Furthermore, the presence of multiple instances of
181
Source Daru?ber hinaus gibt es je zwei Feiertage zu Ostern, Pfingsten, und Weihnachten.
Reference In addition, Easter, Pentecost, and Christmas are each two-day holidays.
Moses In addition, there are two holidays, pfingsten to Easter, and Christmas.
Hybrid In addition there are the two holidays to Easter, Pentecost and Christmas.
RBMT1 Furthermore there are two holidays to Easter, Pentecost and Christmas .
RBMT2 Furthermore there are two holidays each at Easter, Pentecost and Christmas.
RBMT3 In addition there are each two holidays to Easters, Whitsun, and Christmas.
RBMT4 In addition, there is two holidays to Easter, Pentecost, and Christmas.
RBMT5 Beyond that there are ever two holidays to Easter, Whitsuntide, and Christmas.
RBMT6 In addition it gives two holidays apiece to easter, Pentecost, and Christmas.
Figure 2: German-English translation examples
the same phrase pair (with different weight) in the
combined phrase table causes the decoder to gen-
erate many instances of identical results in differ-
ent ways, which increases computational effort and
significantly decreases the number of distinct cases
that are considered during MERT. We suspect that a
modification of our scheme that avoids this problem
will be able to achieve better results, but experiments
in this direction are still ongoing.
The approach presented here combines the
strengths of multiple systems and is different from
recent work on post-correction of RBMT output as
presented in (Simard et al, 2007; Dugast et al,
2007), which focuses on the improvement of a sin-
gle RBMT system by correcting typical errors via
SMT techniques. These ideas are independent and a
suitable combination of them could give rise to even
better results.
Acknowledgments
This work was supported by the EuroMatrix project
funded by the European Commission (6th Frame-
work Programme). We thank Martin Kay, Hans
Uszkoreit, and Silke Theison for interesting discus-
sions and practical help, and two anonymous re-
viewers for hints to improve the paper.
References
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison. 2007.
Multi-engine machine translation with an open-source
SMT decoder. In Proceedings of WMT07, pages 193?
196, Prague, Czech Republic, June. Association for
Computational Linguistics.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of WMT07, pages
220?223, Prague, Czech Republic, June. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL Demo and Poster Sessions, pages 177?180, Jun.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, Mar.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of ACL,
Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie J. Dorr.
2007. Combining translations from multiple machine
translation systems. In Proceedings of the Conference
on Human Language Technology and North American
chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT-NAACL?2007), pages 228?
235, Rochester, NY, April 22-27.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with
statistical phrase-based post-editing. In Proceedings
of WMT07, pages 203?206, Prague, Czech Republic,
June. Association for Computational Linguistics.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical methods.
Diploma thesis, Saarland University.
182
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 42?46,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Combining Multi-Engine Translations with Moses
Yu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2,
Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,2
1: Universita?t des Saarlandes, Saarbru?cken, Germany
2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
{yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de
{eisele,cfedermann,uszkoreit}@dfki.de
Abstract
We present a simple method for generating
translations with the Moses toolkit (Koehn
et al, 2007) from existing hypotheses pro-
duced by other translation engines. As
the structures underlying these translation
engines are not known, an evaluation-
based strategy is applied to select sys-
tems for combination. The experiments
show promising improvements in terms of
BLEU.
1 Introduction
With the wealth of machine translation systems
available nowadays (many of them online and
for free), it makes increasing sense to investigate
clever ways of combining them. Obviously, the
main objective lies in finding out how to integrate
the respective advantages of different approaches:
Statistical machine translation (SMT) and rule-
based machine translation (RBMT) systems of-
ten have complementary characteristics. Previous
work on building hybrid systems includes, among
others, approaches using reranking, regeneration
with an SMT decoder (Eisele et al, 2008; Chen
et al, 2007), and confusion networks (Matusov et
al., 2006; Rosti et al, 2007; He et al, 2008).
The approach by (Eisele et al, 2008) aimed
specifically at filling lexical gaps in an SMT sys-
tem with information from a number of RBMT
systems. The output of the RBMT engines was
word-aligned with the input, yielding a total of
seven phrase tables which where simply concate-
nated to expand the phrase table constructed from
the training corpus. This approach differs from the
confusion network approaches mainly in that the
final hypotheses do not necessarily follow any of
the input translations as the skeleton. On the other
hand, it emphasizes that the additional translations
should be produced by RBMT systems with lexi-
cons that cannot be learned from the data.
The present work continues on the same track
as the paper mentioned above but implements a
number of important changes, most prominently
a relaxation of the restrictions on the number and
type of input systems. These differences are de-
scribed in more detail in Section 2. Section 3 ex-
plains the implementation of our system and Sec-
tion 4 its application in a number of experiments.
Finally, Section 5 concludes this paper with a sum-
mary and some thoughts on future work.
2 Integrating Multiple Systems of
Unknown Type and Quality
When comparing (Eisele et al, 2008) to the
present work, our proposal is more general in a
way that the requirement for knowledge about the
systems is minimum. The types and the identities
of the participated systems are assumed unknown.
Accordingly, we are not able to restrict ourselves
to a certain class of systems as (Eisele et al, 2008)
did. We rely on a standard phrase-based SMT
framework to extract the valuable pieces from the
system outputs. These extracted segments are also
used to improve an existing SMT system that we
have access to.
While (Eisele et al, 2008) included translations
from all of a fixed number of RBMT systems
and added one feature to the translation model for
each system, integrating all given system outputs
in this way in our case could expand the search
space tremendously. Meanwhile, we cannot rely
on the assumption that all candidate systems ac-
tually have the potential to improve our baseline.
This implies the need for a first step of system se-
lection where the best candidate systems are iden-
tified and a limited number of them is chosen to be
included in the combination. Our approach would
not work without a small set of tuning data being
available so that we can evaluate the systems for
later selection and adjust the weights of our sys-
tems. Such tuning data is included in this year?s
42
task.
In this paper, we use the Moses decoder to con-
struct translations from the given system outputs.
We mainly propose two slightly different ways:
One is to construct translation models solely from
the given translations and the other is to extend
an existing translation model with these additional
translations.
3 Implementation
Despite the fact that the output of current MT sys-
tems is usually not comparable in quality to hu-
man translations, the machine-generated transla-
tions are nevertheless ?parallel? to the input so
that it is straightforward to construct a translation
model from data of this kind. This is the spirit
behind our method for combining multiple trans-
lations.
3.1 Direct combination
Clearly, for the same source sentence, we expect
to have different translations from different trans-
lation systems, just like we would expect from hu-
man translators. Also, every system may have its
own advantages. We break these translations into
smaller units and hope to be able to select the best
ones and form them into a better translation.
One single translation of a few thousand sen-
tences is normally inadequate for building a re-
liable general-purpose SMT system (data sparse-
ness problem). However, in the system combina-
tion task, this is no longer an issue as the system
only needs to translate sentences within the data
set.
When more translation engines are available,
the size of this set becomes larger. Hence,
we collect translations from all available systems
and pair them with the corresponding input text,
thus forming a medium-sized ?hypothesis? cor-
pus. Our system starts processing this corpus
with a standard phrase-based SMT setup, using the
Moses toolkit (Koehn et al, 2007).
The hypothesis corpus is first tokenized and
lowercased. Then, we run GIZA++ (Och and
Ney, 2003) on the corpus to obtain word align-
ments in both directions. The phrases are extracted
from the intersection of the alignments with the
?grow? heuristics. In addition, we also generate
a reordering model with the default configuration
as included in the Moses toolkit. This ?hypothe-
sis? translation model can already be used by the
Moses decoder together with a language model to
perform translations over the corresponding sen-
tence set.
3.2 Integration into existing SMT system
Sometimes, the goal of system combination is not
only to produce a translation but also to improve
one of the systems. In this paper, we aim at incor-
porating the additional system outputs to improve
an out-of-domain SMT system trained on the Eu-
roparl corpus (Koehn, 2005). Our hope is that the
additional translation hypotheses could bring in
new phrases or, more generally, new information
that was not contained in the Europarl model. In
order to facilitate comparisons, we use in-domain
LMs for all setups.
We investigate two alternative ways of integrat-
ing the additional phrases into the existing SMT
system: One is to take the hypothesis translation
model described in Section 3.1, the other is to
construct system-specific models constructed with
only translations from one system at a time.
Although the Moses decoder is able to work
with two phrase tables at once (Koehn and
Schroeder, 2007), it is difficult to use this method
when there is more than one additional model.
The method requires tuning on at least six more
features, which expands the search space for the
translation task unnecessarily. We instead inte-
grate the translation models from multiple sources
by extending the phrase table. In contrast to the
prior approach presented in (Chen et al, 2007) and
(Eisele et al, 2008) which concatenates the phrase
tables and adds new features as system markers,
our extension method avoids duplicate entries in
the final combined table.
Given a set of hypothesis translation models
(derived from an arbitrary number of system out-
puts) and an original large translation model to be
improved, we first sort the models by quality (see
Section 3.3), always assigning the highest priority
to the original model. The additional phrase tables
are appended to the large model in sorted order
such that only phrase pairs that were never seen
before are included. Lastly, we add new features
(in the form of additional columns in the phrase ta-
ble) to the translation model to indicate each pair?s
origin.
3.3 System evaluation
Since both the system translations and the ref-
erence translations are available for the tuning
43
set, we first compare each output to the reference
translation using BLEU (Papineni et al, 2001)
and METEOR (Banerjee and Lavie, 2005) and a
combined scoring scheme provided by the ULC
toolkit (Gimenez and Marquez, 2008). In our ex-
periments, we selected a subset of 5 systems for
the combination, in most cases, based on BLEU.
On the other hand, some systems may be de-
signed in a way that they deliver interesting unique
translation segments. Therefore, we also measure
the similarity among system outputs as shown in
Table 2 in a given collection by calculating aver-
age similarity scores across every pair of outputs.
de-en fr-en es-en en-de en-fr en-es
Num. 20 23 28 15 16 9
Median 19.87 26.55 22.50 13.78 24.76 23.70
Range 16.37 17.06 9.74 4.75 11.05 13.94
Top 5 de-en fr-en es-en en-de en-fr en-es
Median 22.26 27.93 26.43 15.21 26.62 26.61
Range 4.31 4.76 5.71 1.71 0.68 5.56
Table 1: Statistics of system outputs? BLEU scores
The range of BLEU scores cannot indicate the
similarity of the systems. The direction with the
most systems submitted is Spanish-English but
their respective performances are very close to
each other. As for the selected subset, the English-
French systems have the most similar performance
in terms of BLEU scores. The French-English
translations have the largest range in BLEU but the
similarity in this group is not the lowest.
de-en fr-en es-en en-de en-fr en-es
All 34.09 46.48 61.83 31.74 44.95 38.11
Selected 36.65 56.16 56.06 33.92 52.78 57.25
Table 2: Similarity of the system outputs
Ideally, we should select systems with highest
quality scores and lowest similarity scores. For
German-English, we selected the three with the
highest METEOR scores and another two with
high METEOR scores but low similarity scores to
the first three. For the other language directions,
we chose five systems from different institutions
with the highest scores.
3.4 Language models
We use a standard n-gram language model for
each target language using the monolingual train-
ing data provided in the translation task. These
LMs are thus specific to the same domain as the
input texts. Moreover, we also generate ?hypoth-
esis? LMs solely based on the given system out-
puts, that is, LMs that model how the candidate
systems convey information in the target language.
These LMs do not require any additional training
data. Therefore, we do not require any training
data other than the given system outputs by using
the ?hypothesis? language model and the ?hypoth-
esis? translation model.
3.5 Tuning
After building the models, it is essential to tune
the SMT system to optimize the feature weights.
We use Minimal Error Rate Training (Och, 2003)
to maximize BLEU on the complete development
data. Unlike the standard tuning procedure, we do
not tune the final system directly. Instead, we ob-
tain the weights using models built from the tuning
portion of the system outputs.
For each combination variant, we first train
models on the provided outputs corresponding to
the tuning set. This system, called the tuning sys-
tem, is also tuned on the tuning set. The initial
weights of any additional features not included in
the standard setting are set to 0. We then adapt the
weights to the system built with translations cor-
responding to the test set. The procedure and the
settings for building this system must be identical
to that of the tuning system.
4 Experiments
The purpose of this exercise is to understand the
nature of the system combination task in prac-
tice. Therefore, we restrict ourselves to the train-
ing data and system translations provided by the
shared task. The types of the systems that pro-
duced the translations are assumed to be unknown.
We report results for six translation directions be-
tween four languages.
4.1 Data and baseline
We build an SMT system from release v4 of the
Europarl corpus (Koehn, 2005), following a stan-
dard routine using the Moses toolkit. The sys-
tem also includes 5-gram language models trained
on in-domain corpora of the respective target lan-
guages using SRILM (Stolcke, 2002).
The systems in this paper, including the base-
line, are all tuned on the same 501-sentence tuning
set. Note also that the provided n-best outputs are
excluded in our experiments.
44
4.2 Results
The experiments include three different setups for
direct system combination, involving only hypoth-
esis translation models. System S0, the baseline
for this group, uses a hypothesis translation model
built with all available system translations and a
hypothesis LM (also from the machine-generated
outputs). S1 differs from S0 in that the LM in S1 is
generated from a large news corpus. S2 consists of
translation models built with only the five selected
systems. The BLEU scores of these systems are
shown in Table 3.
de-en fr-en es-en en-de en-fr en-es
Top 1 21.16 30.91 28.54 14.96 26.55 27.84
Mean 17.29 23.78 21.39 12.76 22.96 21.43
S0 20.46 27.50 23.35 13.95 27.29 25.59
S1 21.76 28.05 25.49 15.16 27.70 26.09
S2 21.71 24.98 27.26 15.62 24.28 25.22
Table 3: BLEU scores of direct system combina-
tion
When all outputs are included, the combined
system can always produce translations better than
most of the systems. When only a hypothesis LM
is used, the BLEU scores are always higher than
the average BLEU scores of the outputs. It even
outperforms the top system for English-French.
This simple setup (S0) is certainly a feasible so-
lution when no additional data is available and no
system evaluation is possible. This approach ap-
pears to be more effective on typically difficult
language pairs that involve German.
As for the systems with normal language mod-
els, neither of the systems ensure better transla-
tions. The translation quality is not completely
determined by the number of included translations
and their quality. On the other hand, the output
set with higher diversity (Table 2) usually leads
to better combination results. This observation is
consistent with the results from the system inte-
gration experiments shown in Table 4.
de-en fr-en es-en en-de en-fr en-es
Bas 19.13 25.07 24.55 13.59 23.67 23.67
Med 17.99 24.56 20.70 13.19 24.19 22.12
All 21.40 28.00 27.75 15.21 27.20 26.41
Top5 21.70 26.01 28.53 15.52 27.87 27.92
Table 4: BLEU scores of integrated SMT systems
(Bas: Baseline, Med: Median)
There are two variants in our experiments on
system integration. All in Table 4 represents the
system that integrates the complete hypothesis
translation model with the Europarl model, while
Top 5 refers to the system that incorporates the five
system-specific models separately. Both setups re-
sult in an improvement over the baseline Europarl-
based SMT system. BLEU scores increase by up
to 4.25 points. The integrated SMT system some-
times produces translations better than the best
system (7 out of 12 cases).
5 Conclusion
This work uses the Moses toolkit to combine
translations from multiple engines in a simple way.
The experiments on six translation directions show
interesting results: The final translations are al-
ways better than the majority of the given systems,
while the combination performs better than the
best system in half the cases. A similar approach
was applied to improve an existing SMT system
which was built in a domain different from the test
task. We achieved improvements in all cases.
There are many possible future directions to
continue this work. As we have shown, the qual-
ity of the combined system is more related to the
diversity of the involved systems than to the num-
ber of the systems or their quality. Hand-picked
systems lead to better combinations than those se-
lected by BLEU scores. It would be interesting
to develop a more comprehensive system selection
strategy.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Yu Chen, Andreas Eisele, Christian Federmann, Eva
Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an
open-source SMT decoder. In Proceedings of
45
WMT07, pages 193?196, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to integrate mul-
tiple rule-based machine translation engines into a
hybrid system. In Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 179?
182, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Jesus Gimenez and Lluis Marquez. 2008. A smor-
gasbord of features for automatic MT evaluation.
In Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 195?198, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 98?107, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbs. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of Annual meet-
ing of the Association for Computation Linguis-
tics (acl), demonstration session, pages 177?180,
Prague, Czech, June.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit 2005.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33?40, Trento, Italy, April.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Antti-Veikko I. Rosti, Spyridon Matsoukas, and
Richard M. Schwartz. 2007. Improved word-level
system combination for machine translation. In
ACL.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
46
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 70?74,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Translation Combination using Factored Word Substitution
Christian Federmann1, Silke Theison2, Andreas Eisele1,2, Hans Uszkoreit1,2,
Yu Chen2, Michael Jellinghaus2, Sabine Hunsicker2
1: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany
2: Universita?t des Saarlandes, Saarbru?cken, Germany
{cfedermann,eisele,uszkoreit}@dfki.de, {sith,yuchen,micha,sabineh}@coli.uni-sb.de
Abstract
We present a word substitution approach
to combine the output of different machine
translation systems. Using part of speech
information, candidate words are deter-
mined among possible translation options,
which in turn are estimated through a pre-
computed word alignment. Automatic
substitution is guided by several decision
factors, including part of speech, local
context, and language model probabili-
ties. The combination of these factors
is defined after careful manual analysis
of their respective impact. The approach
is tested for the language pair German-
English, however the general technique it-
self is language independent.
1 Introduction
Despite remarkable progress in machine transla-
tion (MT) in the last decade, automatic translation
is still far away from satisfactory quality. Even the
most advanced MT technology as summarized by
(Lopez, 2008), including the best statistical, rule-
based and example-based systems, produces out-
put rife with errors. Those systems may employ
different algorithms or vary in the linguistic re-
sources they use which in turn leads to different
characteristic errors.
Besides continued research on improving MT
techniques, one line of research is dedicated to bet-
ter exploitation of existing methods for the com-
bination of their respective advantages (Macherey
and Och, 2007; Rosti et al, 2007a).
Current approaches for system combination in-
volve post-editing methods (Dugast et al, 2007;
Theison, 2007), re-ranking strategies, or shal-
low phrase substitution. The combination pro-
cedure applied for this pape tries to optimize
word-level translations within a ?trusted? sentence
frame selected due to the high quality of its syntac-
tic structure. The underlying idea of the approach
is the improvement of a given (original) translation
through the exploitation of additional translations
of the same text. This can be seen as a simplified
version of (Rosti et al, 2007b).
Considering our submission from the shared
translation task as the ?trusted? frame, we add
translations from four additional MT systems that
have been chosen based on their performance in
terms of automatic evaluation metrics. In total, the
combination system performs 1,691 substitutions,
i.e., an average of 0.67 substitutions per sentence.
2 Architecture
Our system combination approach computes a
combined translation from a given set of machine
translations. Below, we present a short overview
by describing the different steps in the derivation
of a combined translation.
Compute POS tags for translations. We apply
part-of-speech (POS) tagging to prepare the
selection of possible substitution candidates.
For the determination of POS tags we use the
Stuttgart TreeTagger (Schmid, 1994).
Create word alignment. The alignment between
source text and translations is needed to
identify translation options within the differ-
ent systems? translations. Word alignment
is computed using the GIZA++ toolkit (Och
and Ney, 2003), only one-to-one word align-
ments are employed.
Select substitution candidates. For the shared
task, we decide to substitute nouns, verbs
and adjectives based on the available POS
tags. Initially, any such source word is con-
sidered as a possible substitution candidate.
As we do not want to require substitution can-
70
didates to have exactly the same POS tag as
the source, we use groups of ?similar? tags.
Compute decision factors for candidates. We
define several decision factors to enable an
automatic ranking of translation options.
Details on these can be found in section 4.
Evaluate the decision factors and substitute.
Using the available decision factors we
compute the best translation and substitute.
The general combination approach is language
independent as it only requires a (statistical) POS
tagger and GIZA++ to compute the word align-
ments. More advanced linguistic resources are not
required. The addition of lexical resources to im-
prove the extracted word alignments has been con-
sidered, however the idea was then dropped as we
did not expect any short-term improvements.
3 System selection
Our system combination engine takes any given
number of translations and enables us to compute
a combined translation out of these. One of the
given system translations is chosen to provide the
?sentence skeleton?, i.e. the global structure of the
translation, thus representing the reference system.
All other systems can only contribute single words
for substitution to the combined translation, hence
serve as substitution sources.
3.1 Reference system
Following our research on hybrid translation try-
ing to combine the strengths of rule-based MT
with the virtues of statistical MT, we choose our
own (usaar) submission from the shared task to
provide the sentence frame for our combination
system. As this translation is based upon a rule-
based MT system, we expect the overall sentence
structure to be of a sufficiently high quality.
3.2 Substitution sources
For the implementation of our combination sys-
tem, we need resources of potential substitution
candidates. As sources for possible substitution,
we thus include the translation results of the fol-
lowing four systems:
? Google (google)1
1The Google submission was translated by the Google
MT production system offered within the Google Language
Tools as opposed to the qualitatively superior Google MT
research system.
? University of Karlsruhe (uka)
? University of Maryland (umd)
? University of Stuttgart (stuttgart)
The decision to select the output of these par-
ticular MT systems is based on their performance
in terms of different automatic evaluation metrics
obtained with the IQMT Framework by (Gime?nez
and Amigo?, 2006). This includes BLEU, BLEU1,
TER, NIST, METEOR, RG, MT06, and WMT08.
The results, listing only the three best systems per
metric, are given in table 1.
metric best three systems
BLEU1 google uka systran
0.599 0.593 0.582
BLEU google uka umd
0.232 0.231 0.223
TER umd rwth.c3 uka
0.350 0.335 0.332
NIST google umd uka
6.353 6.302 6.270
METEOR google uka stuttgart
0.558 0.555 0.548
RG umd uka google
0.527 0.525 0.520
MT06 umd google stuttgart
0.415 0.413 0.410
WMT08 stuttgart rbmt3 google
0.344 0.341 0.336
Table 1: Automatic evaluation results.
On grounds of these results we anticipate the
four above named translation engines to perform
best when being combined with our hybrid ma-
chine translation system. We restrict the substi-
tution sources to the four potentially best systems
in order to omit bad substitutions and to reduce
the computational complexity of the substitution
problem. It is possible to choose any other num-
ber of substitution sources.
4 Substitution
As mentioned above, we consider nouns, verbs
and adjectives as possible substitution candidates.
In order to allow for automatic decision making
amongst several translation options we define a set
of factors, detailed in the following. Furthermore,
we present some examples in order to illustrate the
use of the factors within the decision process.
71
4.1 Decision factors
The set of factors underlying the decision proce-
dure consists of the following:
A: Matching POS. This Boolean factor checks
whether the target word POS tag matches the
source word?s POS category. The factor com-
pares the source text to the reference trans-
lation as we want to preserve the sentential
structure of the latter.
B: Majority vote. For this factor, we compute
an ordered list of the different translation op-
tions, sorted by decreasing frequency. A con-
sensus between several systems may help to
identify the best translation.
Both the reference system and the Google
submission receive a +1 bonus, as they ap-
peared to offer better candidates in more
cases within the small data sample of our
manual analysis.
C: POS context. Further filtering is applied de-
termining the words? POS context. This is
especially important as we do not want to de-
grade the sentence structure maintained by
the translation output of the reference system.
In order to optimize this factor, we conduct
trials with the single word, the ?1 left, and
the +1 right context. To reduce complex-
ity, we shorten POS tags to a single character,
e.g. NN ? N or NPS ? N .
D: Language Model. We use an English lan-
guage model to score the different translation
options. As the combination system only re-
places single words within a bi-gram context,
we employ the bi-gram portion of the English
Gigaword language model.
The language model had been estimated us-
ing the SRILM toolkit (Stolcke, 2002).
4.2 Factor configurations
To determine the best possible combination of our
different factors, we define four potential factor
configurations and evaluate them manually on a
small set of sentences. The configurations differ
in the consideration of the POS context for factor
C (strict including ?1 left context versus relaxed
including no context) and in the usage of factor A
Matching POS (+A). Table 2 shows the settings of
factors A and C for the different configurations.
configuration Matching POS POS context
strict disabled ?1 left
strict+A enabled ?1 left
relaxed disabled single word
relaxed+A enabled single word
Table 2: Factor configurations for combination.
Our manual evaluation of the respective substi-
tution decisions taken by different factor combi-
nation is suggestive of the ?relaxed+A? configura-
tion to produce the best combination result. Thus,
this configuration is utilized to produce sound
combined translations for the complete data set.
4.3 Factored substitution
Having determined the configuration of the dif-
ferent factors, we compute those for the complete
data set, in order to apply the final substitution step
which will create the combined translation.
The factored substitution algorithm chooses
among the different translation options in the fol-
lowing way:
(a) Matching POS? If factor A is activated for
the current factor configuration (+A), sub-
stitution of the given translation options can
only be possible if the factor evaluates to
True. Otherwise the substitution candidate is
skipped.
(b) Majority vote winner? If the majority vote
yields a unique winner, this translation option
is taken as the final translation.
Using the +1 bonuses for both the reference
system and the Google submission we intro-
duce a slight bias that was motivated by man-
ual evaluation of the different systems? trans-
lation results.
(c) Language model. If several majority vote
winners can be determined, the one with the
best language model score is chosen.
Due to the nature of real numbers this step
always chooses a winning translation option
and thus the termination of the substitution
algorithm is well-defined.
Please note that, while factors A, B, and D are
explicitly used within the substitution algorithm,
factor C POS context is implicitly used only when
computing the possible translation options for a
given substitution candidate.
72
configuration substitutions ratio
strict 1,690 5.714%
strict+A 1,347 4.554%
relaxed 2,228 7.532%
relaxed+A 1,691 5.717%
Table 3: Substitutions for 29,579 candidates.
Interestingly we are able to obtain best results
without considering the ?1 left POS context, i.e.
only checking the POS tag of the single word
translation option for factor C.
4.4 Combination results
We compute system combinations for each of the
four factor configurations defined above. Table
3 displays how many substitutions are conducted
within each of these configurations.
The following examples illustrate the perfor-
mance of the substitution algorithm used to pro-
duce the combined translations.
?Einbruch?: the reference translation for ?Ein-
bruch? is ?collapse?, the substitution sources
propose ?slump? and ?drop?, but also ?col-
lapse?, all three, considering the context,
forming good translations. The majority vote
rules out the suggestions different to the ref-
erence translation due to the fact that 2 more
systems recommend ?collapse? as the correct
translation.
?Ru?ckgang?: the reference system translates this
word as ?drop? while all of the substitution
sources choose ?decline? as the correct trans-
lation. Since factor A evaluates to True, i.e.
the POS tags are of the same nature, ?de-
cline? is clearly selected as the best transla-
tion by factor B Majority vote and thus re-
places ?drop? in the final combined transla-
tion result.
?Tagesgescha?fte?: our reference system trans-
lates ?Tagesgescha?fte? with ?requirements?,
while two of the substitution systems indi-
cate ?business? to be a better translation. Due
to the +1 bonus for our reference translation
a tie between the two possible translations
emerges, leaving the decision to the language
model score, which is higher for ?business?.
4.5 Evaluation results
Table 4 shows the results of the manual evaluation
campaign carried out as part of the WMT09 shared
task. Randomly chosen sentences are presented
to the annotator, who then has to put them into
relative order. Note that each annotator is shown a
random subset of the sentences to be evaluated.
system relative rank data points
google -2.74 174
uka -3.00 217
umd -3.03 170
stuttgart -2.89 163
usaar -2.78 186
usaar-combo -2.91 164
Table 4: Relative ranking results from the WMT09
manual evalution campaign.
Interestingly, our combined system is not able
to outperform the baseline, i.e., additional data
did not improve translation results. However the
evaluation is rather intransparent since it does not
allow for a strict comparison between sentences.
5 Conclusion
Within the system described in this paper, we ap-
proach a hybrid translation technique combining
the output of different MT systems. Substituting
particular words within a well-structured transla-
tion frame equips us with considerably enhanced
translation output. We obtain promising results
providing substantiated proof that our approach is
going in the right direction.
Further steps in the future will include machine
learning methods to optimize the factor selection.
This was, due to limited amount of time and data,
not feasible thus far. We will also investigate the
potential of phrase-based substitution taking into
account multi-word alignments instead of just sin-
gle word mappings. Additionally, we would like
to continue work on the integration of lexical re-
sources to post-correct the word alignments ob-
tained by GIZA++ as this will directly improve the
overall system performance.
Acknowledgments
This work was supported by the EuroMatrix
project (IST-034291) which is funded by the
European Community under the Sixth Frame-
work Programme for Research and Technological
Development.
73
References
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
220?223, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
framework for automatic machine translation eval-
uation. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC?06).
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Wolfgang Macherey and Franz J. Och. 2007. An em-
pirical study on computing consensus translations
from multiple machine translation systems. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 986?995, Prague, Czech Republic,
June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system
combination for machine translation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 312?319,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In the 7th International
Conference on Spoken Language Processing (IC-
SLP) 2002, Denver, Colorado.
Silke Theison. 2007. Optimizing rule-based machine
translation output with the help of statistical meth-
ods. Master?s thesis, Saarland University, Computa-
tional Linguistics department.
74
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 110?114,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Exodus ? Exploring SMT for EU Institutions
Michael Jellinghaus1,2, Alexandros Poulis1, David Kolovratn??k3
1: European Parliament, Luxembourg
2: Saarland University, Saarbru?cken, Germany
3: Charles University in Prague, Czech Republic
micha@coli.uni-sb.de, apoulis@europarl.europa.eu, david@kolovratnik.net
Abstract
In this paper, we describe Exodus, a joint
pilot project of the European Commission?s
Directorate-General for Translation (DGT)
and the European Parliament?s Directorate-
General for Translation (DG TRAD) which
explores the potential of deploying new ap-
proaches to machine translation in European
institutions. We have participated in the
English-to-French track of this year?s WMT10
shared translation task using a system trained
on data previously extracted from large in-
house translation memories.
1 Project Background
1.1 Translation at EU Institutions
The European Union?s policy on multilingualism1 re-
quires enormous amounts of documents to be trans-
lated into the 23 official languages (which yield 506
translation directions). To cope with this task, the EU
has the biggest translation service in the world, em-
ploying almost 5000 internal staff as translators (out of
which 1750 at the European Commission (EC) and 760
at the European Parliament (EP) alone), backed up by
more than 2000 support staff. In 2009, the total output
of the Commission?s Directorate-General for Transla-
tion (DGT) and the Parliament?s Directorate-General
for Translation (DG TRAD) together was more than 3
million translated pages. Thus, it is not surprising that
the cost of all translation and interpreting services of
all the EU institutions amounts to 1% of the annual EU
budget (2008 figures). According to our estimations,
this is more than e 1 billion per year.
1.2 Machine Translation and Other Translation
Technologies at EU Institutions
In order to make the translators? work more efficient so
that they can translate more pages in the same time,
a number of tools like terminology databases, bilin-
gual concordancers, and, most importantly, translation
memories are at their disposition, most of which are
heavily used.
1http://ec.europa.eu/education/
languages/eu-language-policy/index en.htm
In real translation production scenarios, Machine
Translation is usually used to complement transla-
tion memory tools (TM tool). Translation memories
are databases that contain text segments (usually sen-
tences) that are stored together with their translations.
Each such pair of source and target language segments
is called a translation unit. Translation units also con-
tain useful meta-data (creation date, document type,
client, etc.) that allow us to filter the data both for trans-
lation and machine translation purposes.
A TM tool tries to match the segments within a doc-
ument that needs to be translated with segments in the
translation memory and propose translations. If the
memory contains an identical string then we have a so-
called exact or 100% match which yields a very reliable
translation. Approximate or partial matches are called
fuzzy matches and usually, the minimum value of a
fuzzy match is set to 65%?70%. Lower matches are
not considered as usable since they demand more edit-
ing time than typing a translation from scratch. First
experiments have shown that the quality of SMT out-
put for certain language pairs is equal or similar to 70%
fuzzy matches.
Consequently, the cases where machine translation
can play a helpful role in this context is when, for a seg-
ment to be translated, there is no exact match and the
available fuzzy matches do not exceed a certain thresh-
old. This threshold in our case is expected to be 85% or
lower. To this end, there exists a system called ECMT
(European Commission Machine Translation; also ac-
cessible to other European institutions) which is a rule-
based system.
However, only certain translation directions are cov-
ered by ECMT, and its maintenance is quite compli-
cated and requires quite a lot of dedicated and special-
ized human resources. In the light of these facts and
with the addition of the languages of (prospective) new
member states, statistical approaches to machine trans-
lation seem to offer a viable alternative.
First of all, SMT is data-driven, i.e. it exploits par-
allel corpora of which there are plenty at the EU in-
stitutions in the form of translation memories. Trans-
lation memories have two main advantages over other
parallel corpora. First of all, they contain almost ex-
clusively perfectly aligned segments, as each segment
is stored together with its translation, and secondly,
110
they contain cleaner data since their content is regu-
larly maintained by linguists and database administra-
tors. SMT systems are quicker to develop and easier
to maintain than rule-based systems. The availability
of free, open-source software like Moses2 (Koehn et
al., 2007), GIZA++3 (Och and Ney, 2003) and the like
constitutes a further argument in their favor.
Early experiments with Moses were started by mem-
bers of DGT?s Portuguese Language Department as
early as summer 2008 (Leal Fontes and Machado,
2009), then turned into a wider interinstitutional project
with the codename Exodus, currently combining re-
sources from European Commission?s DGT and Euro-
pean Parliament?s DGTRAD. Exodus is the first joint
project of the interinstitutional Language Technology
Watch group where a number of EU institutions join
forces in the field of language technology.
2 Participation in WMT 2010 Shared
Task
After the English-Portuguese experiments, the first lan-
guage pair for which we developed a system with
a sizeable amount of training data was English-to-
French. This system has been developed for testing
at the European Parliament. As English-to-French is
also one of the eight translation directions evaluated in
this year?s shared translation task, we decided to partic-
ipate. The reasons behind this decision are manifold:
We would like to
? know where we stand in comparison to other sys-
tems,
? learn about what system adaptations are the most
beneficial,
? make our project known to potential collaborators,
? compare the WMT10 evaluation results to the out-
come of our in-house evaluation.
There is, however, one major difference between the
evaluation as carried out in WMT10 and our in-house
evaluation: The test data of WMT10 consists exclu-
sively of news articles and is thus out-of-domain for
our system intended for use within the European Parlia-
ment. This means that the impact of training our system
on the in-domain data we obtain from our translation
memories cannot be assessed properly, i.e. taking into
consideration our specific translation production needs.
Therefore, we would like to invite other interested
groups to also translate our in-domain test data with
the goal of seeing how our translation scenario could
benefit from their setups. Due to legal issues, however,
we unfortunately cannot provide our internal training
data at this moment.
2http://www.statmt.org/moses/
3http://www.fjoch.com/GIZA++.html
3 Data Used
To build our English-to-French MT system, we did
not use any of the data provided by the organizers of
the WMT10 shared translation task. Instead, we used
data that was extracted from the translation memories
at the core of EURAMIS (European Advanced Multi-
lingual Information System; (Theologitis, 1997; Blatt,
1998)) which are the fruit of thousands of man-years
contributed by translators at EU institutions who, each
day, upload the majority of the segments they translate.
Initially (before pre-processing), our EN-FR cor-
pus contained 10,446,450 segments and included doc-
uments both from the Commission and the EP from
common legislative procedures. These segments were
extracted in November 2009 from 7 translation memo-
ries hosted in Euramis. Currently, we do not have in-
formation about the exact document types coming from
the Commission?s databases. The Parliament?s docu-
ment types used include, among others:
? legislative documents such as draft reports, final
reports, amendments, opinions, etc.,
? documents for the plenary such as questions, res-
olutions or session amendments,
? committee and delegation documents,
? documents concerning the ACP4 and the EMPA5,
? internal documents such as budget estimates, staff
regulations, rules of procedure, etc.,
? calls for tender.
Any sensitive or classified documents or
Commission-internal documents that do not be-
long to common legislative procedures have been
excluded from the data.
In terms of preprocessing, we performed several
steps. First, we obtained translation memory exchange
(TMX) files from EURAMIS and converted them to
UTF-8 text as the Euramis native character encoding
is UCS-2. Then we removed certain control charac-
ters which otherwise would have halted processing, we
extracted the two single-language corpora into a plain-
text file, tokenized and lowercased the data. Finally,
we separated the corpus into training data (9,300,682
segments), and data for tuning and testing ? 500 seg-
ments each. These segments did not exceed a max-
imum length of 60 tokens and were excluded from
the preparation of the translation and language models.
The models were then trained on the remaining seg-
ments. The maximum length of 60 tokens was applied
here as well.
4African, Caribbean and Pacific Group of States
5Euro-Mediterranean Parliamentary Assembly
111
Metric Score
BLEU 18.8
BLEU-cased 16.9
TER 0.747
Table 1: Automatic scores calculated for Exodus in
WMT10
4 Building the Models and Decoding
The parallel data described above was used to train an
English-to-French translation model and a French tar-
get language model. This was done on a server running
Sun Solaris with 64 GB of RAM and 8 double core
CPU?s @1800 Mhz (albeit shared with other processes
running simultaneously).
In general, we simply used a vanilla Moses instal-
lation at this point, leaving the integration of more so-
phisticated features to a later moment, i.e. after a thor-
ough analysis of the results of the present evaluation
campaign when we will know which adaptations yield
the most significant improvements.
For the word alignments, we chose MGIZA (Gao
and Vogel, 2008), using seven threads per MGIZA in-
stance, with the parallel option, i.e. one MGIZA in-
stance per pair direction running in parallel. The target
language model is a 7-gram, binarized IRSTLM (Fed-
erico et al, 2008). The weights of the distortion, trans-
lation and language models were optimized with re-
spect to BLEU scores (Papineni et al, 2002) on a given
held-out set of sentences with Minimum Error Rate
Training (MERT; (Och, 2003)) in 15 iterations.
After the actual translation with Moses, an additional
recasing ?translation? model was applied in the same
manner. Finally, the translation output underwent min-
imal automatic postprocessing based on regular expres-
sion replacements. This was mainly undertaken in or-
der to fix the distribution of whitespace and some re-
maining capitalization issues.
5 Results
5.1 WMT10 Evaluation
In one of the tasks of the WMT10 human evaluation
campaign, people were asked to rank competing trans-
lations. From each 1-through-5 ranking of a set of 5
system outputs, 10 pairwise comparisons are extracted.
Then, for each system, a score is computed that tells
how often it was ranked equally or better than the other
system. For our system, this score is 32.35%, meaning
it ranked 17th out of 19 systems for English-to-French.
A number of automatic scores were also calculated and
appear in Table 1.
5.2 Evaluation at the European Parliament
As the goal behind building our system has been to pro-
vide a tool to translators at EU institutions, we have
also had it evaluated by two of our colleagues, both
Evaluator A Evaluator B Overall
Reference 1.75 2.06 1.97
ECMT 3.34 3.31 3.32
Google 3.59 3.28 3.37
Exodus 3.52 3.45 3.47
Table 2: Average relative rank (on a scale from 1 to 5)
OK Edited Bad
Reference 29 30 2
ECMT 8 57 2
Google 7 33 5
Exodus 13 62 12
Table 3: Results of Editing Task (?OK? means ?No cor-
rections needed?; ?Bad? means ?Unable to correct?)
native speakers of French and working as professional
translators of the French Language Unit at the Parlia-
ment?s DG TRAD.
For this purpose, we had 1742 sentences of in-house
documents translated by our system as well as by
the rule-based ECMT and the statistics-based Google
Translate.6,7 We developed an online evaluation tool
based on the one used by the WMT evaluation cam-
paign in the last years (Callison-Burch et al, 2009)
where we asked the evaluators to perform three differ-
ent tasks.
In the first one, they were shown the three automatic
translations plus a human reference in random order
and asked to rank the four versions relative to each
other on a scale from 1 to 5. The average relative ranks
can be seen in Table 2.
The second task consisted of post-editing a given
translation. Again, the sentence might come from one
of three MT systems, or be a human translation. The
absolute number of items that did not need any correc-
tions, had to be edited, or were impossible to correct
are shown in Table 3.
For the third and last task, only translations of our
own system were displayed. Here, the evaluators
should simply assign them to one of four quality cat-
egories as proposed by (Roturier, 2009), and addition-
ally tick boxes standing for the presence of 13 different
types of errors in the sentence concerning word order,
punctuation, or different types of syntactic/semantic
problems. A total of 150 segments were judged. For
the categorization results, see Tables 4 and 5.
5.3 Evaluation at the European Commission
On a side note, the Portuguese Language Department
also performed a manual evaluation (Leal Fontes and
Machado, 2009) which involved 14 of their managers
and translators, comparing their Moses-based system to
6http://translate.google.com
7As about a third of the source documents are not public,
we could not send those to Google Translate.
112
Items Proportion
Excellent 28 18.6%
Good 42 28%
Medium 45 30%
Poor 35 23.3%
Table 4: Results of Categorization Task: Quality Cate-
gories
Error type Occurrences
Word order
Single word 11
Sequence of words 42
Incorrect word(s)
Wrong lexical choice 51
Wrong terminology choice 6
Incorrect form 77
Extra word(s) 21
Missing word(s) 14
Style 44
Idioms 1
Untranslated word(s) 5
Punctuation 24
Letter case 7
Other 5
Table 5: Results of Categorization Task: Error Types
ECMT and Google. Table 6 shows how many people
considered Moses better, similar, or worse compared to
ECMT and Google, respectively.
Moses-based SMT did well in fields where ECMT
is systematically used (e.g. Justice and Home Affairs
and Trade) and showed a big improvement over ECMT
in terminology-intensive domains (e.g. Fisheries). As
of early 2009, more than half of their translators (58%)
now already use ECMT systematically in production,
i.e. for all English and French originals. 85% use it for
specific language combinations or for certain domains
only. On a voluntary basis, they have been replacing
ECMT with Moses-based SMT for the translation of
day-to-day incoming documents. Over a three-month
period, more than 2500 pages have been translated in
this manner, and the translators of the Portuguese de-
partment declared themselves ready to switch over to
an SMT system as soon as it should become available.
Compared to Better Similar Worse
ECMT 7 5 2
Google 5 5 3
Table 6: Portuguese Language Department evaluation
results of Moses-based MT system
6 Discussion of Results
As expected, our system did not rank among the top
competitors in the WMT10 shared task. This is mainly
due to the data we trained on, which is of a very spe-
cific domain (common legislative procedures of Eu-
ropean Institutions) and relatively small in size when
compared to what others used for this language combi-
nation. In addition, we more or less used Moses out-of-
the-box with no sophisticated add-ons or optimization.
In the internal evaluation, our system beat neither
Google Translate nor ECMT overall but it did show a
similar performance. This is all the more encourag-
ing as Exodus has been built within less than a month,
while ECMT has been developed and maintained in ex-
cess of 30 years, and while Google Translate is based
on manpower and computing resources that a public
administration body usually cannot provide.
Finally, the successful trials of SMT software at the
EC?s Portuguese department seem to indicate that such
a system holds enormous potential, especially when a
serious adaptation to specific language combinations
and domains is taken into consideration.
7 Outlook
Further use and development of SMT at EU institutions
depends on the outcome of internal evaluations, among
other factors. We plan to extend our activities to other
language pairs, an English-to-Greek machine transla-
tion project already having started. Given a continu-
ation of the currently promising results, Exodus will
eventually be integrated into the CAT (computer-aided
translation) tools used by EU translators.8 Further-
more, we would like to release an extended EuroParl
corpus not only containing parliamentary proceedings
but also other types of public documents. We estimate
that such a step should foster research to the benefit of
both EU institutions and machine translation in gen-
eral.
8 Conclusions
We have presented Exodus, a joint pilot project of
the European Commission?s Directorate-General for
Translation (DGT) and the European Parliament?s
Directorate-General for Translation (DG TRAD) with
the aim of exploring the potential of deploying new
approaches to machine translation in European insti-
tutions.
Our system is based on a fairly vanilla Moses instal-
lation and trained on data extracted from large in-house
translation memories covering a range of EU docu-
ments. The obtained models use 7-grams.
We applied the Exodus system to this year?s WMT10
shared English-to-French translation task. As the test
8However, speed issues will have to be addressed before
as the current system is not able to provide translations in real
time.
113
data stems from a different domain than the one tar-
geted by our system, we did not outperform the com-
petitors. However, results from in-house evaluation are
promising and indicate the big potential of SMT for
European Institutions.
Acknowledgments
We would very much like to thank (in alphabetical or-
der) Manuel Toma?s Carrasco Ben??tez, Dirk De Paepe,
Alfons De Vuyst, Peter Hjorts?, Herman Jenne?, Hila?rio
Leal Fontes, Maria Jose? Machado, Spyridon Pilos, Joa?o
Rosas, Helmut Spindler, Filiep Spyckerelle, and Ange-
lika Vaasa for their invaluable help and support.
David Kolovratn??k was supported by the Czech Sci-
ence Foundation under contract no. 201/09/H057 and
by the Grant Agency of Charles University under con-
tract no. 100008/2008.
References
A. Blatt. 1998. EURAMIS : Added value by integra-
tion. In T&T Terminologie et Traduction, 1.1998,
pages 59?73.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit for
Handling Large Scale Language Models. In Pro-
ceedings of Interspeech, Brisbane, Australia.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Jun.
Hila?rio Leal Fontes and Maria Jose? Machado. 2009.
Contribution of the Portuguese Langauge Depart-
ment to the Evaluation of Moses Machine Transla-
tion System. Technical report, Portuguese Language
Department, DGT, European Commission, Decem-
ber.
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of ACL, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In ACL ?02:
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 311?
318, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Johann Roturier. 2009. Deploying novel MT tech-
nology to raise the bar for quality: A review of key
advantages and challenges. In The twelfth Machine
Translation Summit, Ottawa, Canada, August. Inter-
national Association for Machine Translation.
D. Theologitis. 1997. EURAMIS, the platform of the
EC translator. In EAMT Workshop, pages 17?32.
114
