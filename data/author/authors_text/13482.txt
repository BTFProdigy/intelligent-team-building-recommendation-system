Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2280?2290, Dublin, Ireland, August 23-29 2014.
A Probabilistic Co-Bootstrapping Method for Entity Set Expansion 
 
 
Bei Shi,    Zhengzhong Zhang Le Sun,    Xianpei Han 
Institute of Software, 
 Chinese Academy of Sciences, 
 Beijing, China 
State Key Laboratory of Computer Science, 
Institute of Software,  
Chinese Academy of Sciences,  
Beijing, China 
{shibei, zhenzhong, sunle, xianpei}@nfs.iscas.ac.cn 
 
 
Abstract 
Entity Set Expansion (ESE) aims at automatically acquiring instances of a specific target category. 
Unfortunately, traditional ESE methods usually have the expansion boundary problem and the semantic 
drift problem. To resolve the above two problems, this paper proposes a probabilistic Co-Bootstrapping 
method, which can accurately determine the expansion boundary using both the positive and the 
discriminant negative instances, and resolve the semantic drift problem by effectively maintaining and 
refining the expansion boundary during bootstrapping iterations. Experimental results show that our 
method can achieve a competitive performance. 
1 Introduction 
Entity Set Expansion (ESE) aims at automatically acquiring instances of a specific target category 
from text corpus or Web. For example, given the capital seeds {Rome, Beijing, Paris}, an ESE system 
should extract all other capitals from Web, such as Ottawa, Moscow and London. ESE system has 
been used in many applications, e.g., dictionary construction (Cohen and Sarawagi, 2004), word sense 
disambiguation (Pantel and Lin, 2002), query refinement (Hu et al., 2009), and query suggestion (Cao 
et al., 2008). 
Due to the limited supervision provided by ESE (in most cases only 3-5 seeds are given), traditional 
ESE systems usually employ bootstrapping methods (Cucchiarelli and Velardi, 2001; Etzioni et al., 
2005; Pasca, 2007; Riloff and Jones, 1999; Wang and Cohen, 2008). That is, the entity set is 
iteratively expanded through a pattern generation step and an instance extraction step. Figure 1(a) 
demonstrates a simple bootstrapping process.? 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
                                                        
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 
Rome 
Beijing 
Paris 
Milan 
Tokyo 
Shanghai 
London 
* is the city of 
 
at the embassy in * 
* is the capital of 
at the hotel in * 
Chicago 
Berlin 
Pattern Generation Instance Extraction 
Rome 
Beijing 
Paris 
Milan 
Tokyo 
Shanghai 
London 
Sydney 
Boston 
* is the city of 
 
at the embassy in * 
 
* is the capital of 
 
to cities such as * 
 
at the hotel in * 
Chicago 
Nokia * the official web site 
 
New York 
Pattern Generation Instance Co-Extraction 
Negative Positive 
 
(a) (b) 
Figure 1: A demo of Bootstrapping (a) and Co-Bootstrapping (b) 
2280
However, the traditional bootstrapping methods have two main drawbacks:  
1) The expansion boundary problem. That is, using only positive seeds (i.e., some example 
entities from the category we want to expand), it is difficult to represent which entities we want to 
expand and which we don?t want. For example, starting from positive seeds {Rome, Beijing, Paris}, 
we can expand entities at many different levels, e.g., all capitals, all cities, or even all locations. And 
all these explanations are reasonable.  
2) The semantic drift problem. That is, the expansion category may change gradually when noisy 
instances/patterns are introduced during the bootstrapping iterations. For example, in Figure 1 (a), the 
instance Rome will introduce a pattern ?* is the city of?, which will introduce many noisy city 
instances such as Milan and Chicago for the expansion of Capital. And these noisy cities in turn will 
introduce more city patterns and instances, and finally will lead to a semantic drift from Capital to 
City. 
In recent years, some methods (Curran et al, 2007; Pennacchiotti and Pantel, 2011) have exploited 
mutual exclusion constraint to resolve the semantic drift problem. These methods expand multiple 
categories simultaneously, and will determine the expansion boundary based on the mutually 
exclusive property of the pre-given categories. For instance, the exclusive categories Fruit and 
Company will be jointly expanded and the expansion boundary of {Apple, Banana, Cherry} will be 
limited by the expansion boundary of {Google, Microsoft, Apple Inc.}. These methods, however, still 
have the following two drawbacks: 
1) These methods require that the expanded categories should be mutually exclusive. However, in 
many cases the mutually exclusive assumption does not hold. For example, many categories hold a 
hyponymy relation (e.g., the categories City and Capital, because the patterns for Capital are also the 
patterns for City) or a high semantic overlap (e.g., the categories Movies and Novels, because some 
movies are directly based on the novels of the same title.). 
2) These methods require the manually determination of the mutually exclusive categories. 
Unfortunately, it is often very hard for even the experts to determine the categories which can define 
the expansion boundaries for each other. For example, in order to expand the category Chemical 
Element, it is difficult to predict its semantic drift towards Color caused by the ambiguous instances 
{Silver, Gold}. 
In this paper, to resolve the above problems, we propose a probabilistic Co-Bootstrapping method. 
The first advantage of our method is that we propose a method to better define the expansion boundary 
using both the positive and the discriminant negative seeds, which can both be automatically populated 
during the bootstrapping process. For instance, in Figure 1(b), in order to expand Capital, the 
Co-Bootstrapping algorithm will populate both positive instances from the positive seeds {Rome, 
Beijing, Paris}, and negative instances from the negative seeds {Boston, Sydney, New York}. In this 
way the expansion boundary of Capital can be accurately determined. 
The second advantage of our method is that we can maintain and refine the expansion boundary 
during bootstrapping iterations, so that the semantic drift problem can be effectively resolved. 
Specifically, we propose an effective scoring algorithm to estimate the probability that an extracted 
instance belongs to the target category. Based on this scoring algorithm, this paper can effectively 
select positive instances and discriminant negative instances. Therefore the expansion boundary can be 
maintained and refined through the above jointly expansion process. 
We have evaluated our method on the expansion of thirteen categories of entities. The experimental 
results show that our method can achieve 6%~15% P@200 performance improvement over the 
baseline methods. 
This paper is organized as follows. Section 2 briefly reviews related work. Section 3 defines the 
problem and proposes a probabilistic Co-Bootstrapping approach. Experiments are presented in 
Section 4. Finally, we conclude this paper and discuss some future work in Section 5. 
2 Related Work 
In recent years, ESE has received considerable attentions from both research (An et al., 2003; 
Cafarella et al., 2005; Pantel and Ravichandran, 2004; Pantel et al., 2009; Pasca, 2007; Wang and 
Cohen, 2008) and industry communities (e.g., Google Sets). Till now, most ESE systems employ 
bootstrapping methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), etc. 
2281
The main drawbacks of the traditional bootstrapping methods are the expansion boundary problem 
and the semantic drift problem. Currently, two strategies have been exploited to resolve the semantic 
drift problem. The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar et 
al., 2008), which select highly confident patterns and instances through a ranking algorithm, with the 
assumption that high-ranked instances will be more likely to be the instances of the target category. 
The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh and 
Curran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), which 
expand multiple categories simultaneously and determine the expansion boundary based on the 
mutually exclusive property of the pre-given categories. 
3 The Co-Bootstrapping Method 
3.1 The Framework of Probabilistic Co-Bootstrapping 
Given the initial positive seeds and negative seeds, the goal of our method is to extract instances of a 
specific target semantic category. For demonstration, we will describe our method through the running 
example shown in Figure 1(b). 
Specifically, Figure 2 shows the framework of our method. The central tasks of our 
Co-Bootstrapping method are as follows: 
 
Figure 2: The framework of probabilistic Co-Bootstrapping 
1) Pattern Generation and Evaluation. This step generates and evaluates patterns using the 
statistics of the positive and the negative instances. Specifically, we propose three measures of pattern 
quality: the Generality (GE), the Precision of Extracted Instances (PE) and the Precision of Not 
Extracted Instances (PNE). 
2) Instance Co-Extraction. This step co-extracts the positive and the negative instances using 
highly confident patterns. Specifically, we propose an effective scoring algorithm to estimate the 
probability that an extracted instance belongs to the target category based on the statistics and the 
quality of the patterns which extract it. 
3) Seed Selection. This step selects the high ranked positive instances and discriminant negative 
instances to refine the expansion boundary by measuring how well a new instance can be used to 
define the expansion boundary. 
The above three steps will iterate until the number of extracted entities reaches a predefined 
threshold. We describe these steps as follows. 
3.2 Pattern Generation and Evaluation 
In this section, we describe the pattern generation and evaluation step. In this paper, each pattern is a 
4-grams lexical context of an entity. We use the Google Web 1T corpus?s (Brants and Franz, 2006) 
5-grams for both the pattern generation and the instance co-extraction in ESE. Our method generates 
patterns through two steps: 1) Generate candidate patterns by matching seeds with the 5-grams. 2) 
Evaluate the quality of the patterns. 
For the first step, we simply match each seed instance with all 5-grams, then we replace the 
matching instance with wildcard ?*? to generate the pattern. 
Extracted Positive (ep) London 
Extracted Negative (en) Shanghai, Milan 
Not Extracted Positive (nep) Tokyo 
Not Extracted Negative (nen) Chicago, Nokia 
 
Table 1: (a) shows the four classes of instances according to polarity and extraction. (b) shows the four 
classes of the instances given ?to cities such as *? 
Count Positive Negative 
Extracted Extracted Positive (ep) Extracted Negative (en) 
Not Extracted 
Not Extracted and Positive 
(nep) 
Not Extracted and Negative 
(nen) 
Pattern Generation and Evaluation 
Initial   
Seeds 
Pattern 
Positive Instance 
Discriminant Negative Instance    
Positive Instance  
Negative Instance 
Instance Co-Extraction 
Seeds Evaluation and Selection 
(a) (b) 
2282
For the second step, we propose three measures to evaluate the quality of a pattern, correspondingly 
the Generality (GE), the Precision of Extracted Instances (PE), and the Precision of Not Extracted 
Instances (PNE). Specifically, given a pattern, we observed that all instances can be categorized into 
four classes, according to whether they belong to the target category and whether they can be extracted 
by the pattern (shown in Table 1(a)). For example, given the pattern ?to cities such as *? in Figure 
1(b), the instances under its four classes are shown in Table 1 (b). 
The proposed three measures of the quality of a pattern can be computed as follows (In most cases, 
we cannot get the accurate number of ep, en, nep and nen. So this paper uses the corresponding known 
instances in the previous iteration to approximately compute ep, en, nep and nen): 
1) Generality (GE). The Generality of a pattern measures how many entities can be extracted by it. 
A more general pattern will cover more entities than a more specific pattern. Specifically, the GE of a 
pattern is computed as: 
 
That is, the proportion of the instances which can be extracted by the pattern in the previous iteration. 
2) Precision of Extracted Instances (PE). The PE measures how likely an instance extracted by a 
pattern will be positive. That is, a pattern with higher PE will be more likely to extract positive 
instances than a lower PE pattern. The PE is computed as: 
 
That is, the proportion of positive instances within all instances which can be extracted by the 
pattern in the previous iteration. 
3) Precision of Not Extracted Instances (PNE). The PNE measures how likely a not extracted 
instance is positive. Instances not extracted by a high PNE pattern will be more likely to be positive. 
PNE is computed as: 
 
Because the number of negative instances is usually much larger than the number of positive 
instances, we normalize the number of positive and negative instances in the formula. 
Table 2 shows these measures of some selected patterns evaluated using the Google Web 1T corpus. 
We can see that the above measures can effectively evaluate the quality of patterns. For instance, 
GE(?* is the city of?)=0.566 is larger than GE(?at the embassy in *?)=0.340, which is consistent with 
our intuition that the pattern ?* is the city of? is more general than ?at the embassy in *?. PE(?* is the 
capital of?)=0.928 is larger than PE(?* is the city of?)=0.269, which is consistent with our intuition 
that the instances extracted by ?* is the capital of? are more likely Capital than by?* is the city of?. 
 GE PE PNE 
at the embassy in * 0.340 0.833 0.312 
* is the capital of 0.321 0.928 0.224 
to cities such as * 0.426 0.875 0.566 
at the hotel in * 0.333 0.192 0.571 
* is the city of 0.566 0.269 0.592 
* the official web site 0.218 0.230 0.607 
Table 2: The GE, PE and PNE of some selected patterns 
3.3 Instance Co-Extraction 
In this section, we describe how to co-extract positive instances and discriminant negative instances. 
Given the generated patterns, the central task of this step is to measure the likelihood of an instance to 
be positive. The higher the likelihood, the more likely the instance belongs to the target category. To 
resolve the task, we propose a probabilistic method which predicts the probability of an instance to be 
positive, i.e., the Instance Positive Probability and we denote it as P+. Generally, the P+ is determined 
by both the statistics and the quality of patterns. We start with the observation that: 
2283
1) If an instance is extracted by a pattern with a high PE, the instance will have a high P+. 
2) If an instance is not extracted by a high PNE pattern, the instance will have a high P+. 
3) If an instance is extracted by many patterns with high PE and not extracted by many patterns 
with high PNE, the instance will have a high P+, and vice versa. 
Based on the above observations, the computation of P+ is as follows: 
The Situation of One Pattern 
For the situation that only one pattern exists, the P+ of an instance can be simply computed as: 
 
where e denotes an extracted instance and p denotes a pattern which extracts e. This formula means 
that if the instance is extracted by a pattern, the P+ is determined by the PE of the pattern. For 
example, in Figure 3 (a), the instance Tokyo is only extracted by the pattern ?at the embassy in *? and 
the P+ is determined by the PE of ?at the embassy in *?, i.e., P+(Tokyo)=PE(?at the embassy in *?). 
The above formula also means when the instance cannot be extracted by the only pattern, the P+ 
will be determined by the PNE of the pattern. For example, in Figure 3 (b), the instance Tokyo is not 
extracted by the only pattern ?at the hotel in *? and the P+ is only determined by the PNE of ?at the 
hotel in *?, that is, P+(Tokyo)=PNE(?at the hotel in *?). 
 
 
 
 
Figure 3: (a) Tokyo is extracted by ?at the embassy in *?. (b) Tokyo is not extracted by ?at the hotel 
in *?. (c) London is extracted by ?at the embassy in *? and not extracted by ?to cities such as *?. 
The Situation of Multiple Patterns 
In this section, we describe how to compute P+ in the situation of multiple patterns. Specifically, we 
assume that an instance is extracted by different patterns independently. Therefore, given all the 
pattern-instance relations (i.e., whether a specific pattern extracts a specific instance), the likelihood 
for an instance e being positive is computed as: 
 
where R+ is all the patterns which extract e, and R- is all the patterns which do not extract e. I+ is the 
set of all positive instances.  is the probability of the event ?pattern p extracts 
instance e and e is positive?. Using Bayes rule, this probability can be computed as: 
 
where  is the probability of the event ?p extracts an instance e?, its value is GE(p); 
 is the conditional probability that e is positive under the condition ?p extracts e?, 
and its value is PE(p). Finally  is computed as: 
 
 is the probability of the event ?p does not extract e and e is positive?, which can 
be computed as: 
 
 is the probability of p not extracting an instance e, and its value is 1-GE(p). 
 is the conditional probability that e is positive under the condition ?p does not 
extract e?, and its value is PNE(p). Then  is finally computed as: 
 
Tokyo at the embassy in * Tokyo  at the hotel in * London 
at the embassy in * 
to cities such as * 
(a) (b) (c) 
2284
For example, in Figure 3 (c), the instance London is extracted by the pattern ?at the embassy in *? 
and not extracted by the pattern ?to cities such as *?. In this situation, PosLikelihood(London)= 
[GE(?at the embassy in *?) ? PE(?at the embassy in?)] ? [(1-GE(?to cities such as *?)) ? PNE(?to 
cities such as *?)]. 
Using the same intuition and the same method, the likelihood of an instance being negative is 
computed as: 
 
where  is the probability of the event ?p extracts e and e is negative?, which is 
computed as: 
 
 is the probability of the event ?p does not extract e and e is negative?, which is 
computed as: 
 
For instance, in Figure 3 (c), NegLikelihood(London) = [GE(?at the embassy in *?) ? (1-PE(?at the 
embassy in?))] ? [(1-GE(?to cities such as *?)) ? (1-PNE(?to cities such as *?))]. 
Finally, the Instance Positive Probability, P+, is computed as:  
 
3.4 Seed Selection 
In this section, we describe how to select positive and discriminant negative instances at each iteration. 
To determine whether an instance is positive, we use a threshold of P+ to determine the polarity of 
instances, which can be empirically estimated from data. The instances which have much higher P+ 
than the threshold will be added to the set of positive instances. For example, London and Tokyo in 
Figure 1 (b) are selected as positive instances. 
To select discriminant negative instances, we observed that not all negative instances are the same 
useful for the expansion boundary determination. Intuitively, the discriminant negative instances are 
those negative instances which are highly overlapped with the positive instances. For instance, due to 
the lower overlap between categories Fruit and Capital, Apple is not a discriminant negative instance 
since it provides little information for the expansion boundary determination. Therefore, the instances 
near the threshold are used as the discriminant negative instances in the next iteration. (Notice that, the 
computation of GE, PE and PNE still uses all positive and negative instances, rather than only 
discriminant negative instances). For example, in Figure 1(b), Shanghai, Milan and Chicago are 
selected as discriminate negative instances, and Nokia will be neglected. Finally the boundary between 
Capital and City can be determined by the positive instances and the discriminant negative instances. 
4 Experiments 
4.1 Experimental Settings 
Category Description Category Description 
CAP Place: capital name FAC Facilities: names of man-made structures 
ELE chemical element ORG Organization: e.g. companies, governmental 
FEM Person: female first name GPE Place: Geo-political entities 
MALE Person: male first name LOC Locations other than GPEs 
LAST Person: last name DAT Reference to a date or period 
TTL Honorific title LANG Any named language 
NORP Nationality, Religion, Political(adjectival)   
Table 3: Target categories 
Corpus: In our experiments, we used the Google Web 1T corpus (Brants and Franz, 2006) as our 
expansion corpus. Specifically, we use the open source package LIT-Indexer (Ceylan and Mihalcea, 
2011) to support efficient wildcard querying for pattern generation and instance extraction. 
2285
Target Expansion Categories: We conduct our experiments on thirteen categories, which are shown 
in Table 3. Eleven of them are from Curran et al. (2007). Besides the eleven categories, to evaluate 
how well ESE systems can resolve the semantic drift problem, we use two additional categories 
(Capital and Chemical Element) which are high likely to drift into other categories. 
Evaluation Criteria: Following Curran et al (2007), we use precision at top n (P@N) as the 
performance metrics, i.e., the percentage of correct entities in the top n ranked entities for a given 
category. In our experiments, we use P@10, P@20, P@50, P@100 and P@200. Since the output is a 
ranked list of extracted entities, we also choose the average precision (AP) as the evaluation metric. In 
our experiments, the correctness of all extracted entities is manually judged. In our experiments, we 
present results to 3 annotators, and an instance will be considered as positive if 2 annotators label it as 
positive. We also provide annotators some supporting resources for better evaluation, e.g., the entity 
list of target type collected from Wikipedia. 
4.2 Experimental Results 
In this section, we analyze the effect of negative instances, categories boundaries, and seed selection 
strategies. We compare our method with the following two baseline methods: i) Only_Pos (POS): 
This is an entity set expansion system which uses only positive seeds. ii) Mutual_Exclusion (ME): 
This is a mutual exclusion bootstrapping based ESE method, whose expansion boundary is determined 
by the exclusion of the categories. 
We implement our method using two different settings: i) Hum_Co-Bootstrapping (Hum_CB): 
This is the proposed Co-Bootstrapping method in which the initial negative seeds are manually given. 
Specifically, we randomly select five positive seeds from the list of the category?s instances while the 
initial negative seeds are manually provided. ii) Feedback_Co-Bootstrapping (FB_CB): This is our 
proposed probabilistic Co-Bootstrapping method with two steps of selecting initial negative seeds:   
1) Expand the entity set using only the positive seeds for only first iteration. Return the top ten 
instances. 2) Select the negative instances in the top ten results of the first iteration as negative seeds. 
4.2.1. Overall Performance 
Several papers have shown that the experimental performance may vary with different seed choices 
(Kozareva and Hovy, 2010; McIntosh and Curran, 2009; Vvas et al., 2009). Therefore, we input the 
ESE system with five different positive seed settings for each category. Finally we average the 
performance on the five settings so that the impact of seed selection can be reduced. 
 P@10 P@20 P@50 P@100 P@200 MAP 
POS 0.84 0.74 0.55 0.41 0.34 0.42 
ME 0.83(0.90) 0.79(0.87) 0.68(0.78) 0.58(0.67) 0.51(0.59) - 
Hum_CB 0.97 0.95 0.83 0.71 0.57 0.78 
FB_CB 0.97 0.96 0.90 0.79 0.66 0.85 
Table 4: The overall experimental results 
Table 4 shows the overall experimental results. The results in parentheses are the known results of 
eleven categories (without CAP and ELE) shown in (Curran et al., 2007). MAP of ME is missed 
because there are no available results in (Curran et al., 2007). From Table 4, we can see that: 
1) Our method can achieve a significant performance improvement: Compared with the 
baseline POS, our method Hum_CB and FB_CB can respectively achieve a 23% and 32% 
improvement on P@200; Compared with the baseline ME, our method Hum_CB and FB_CB can 
respectively improve P@200 by 6% and 15%. 
2) By explicitly representing the expansion boundary, the expansion performance can be 
increased: Compared with the baseline POS, ME can achieve a 17% improvement on P@200, and our 
method Hum_CB can achieve a 23% improvement on P@200. 
3) The negative seeds can better determine the expansion boundary than mutually exclusive 
categories. Compared with ME, Hum_CB and FB_CB can respectively achieve a 6% and 15% 
improvement on P@200. We believe this is because using negative instances is a more accurate and 
more robust way for defining and maintaining the expansion boundary than mutually exclusive 
categories. 
2286
4) The system?s feedback is useful for selecting negative instances: Compared with Hum_CB, 
FB_CB method can significantly improve the P@200 by 9.0%. We believe this is because that the 
system?s feedback is a good indicator of the semantic drift direction. In contrast, it is usually difficult 
for human to determine which directions the bootstrapping will drift towards. 
4.2.2. Detailed Analysis: Expansion Boundary 
In Table 5, we show the top 20 positive and negative Capital instances (FB_CB setting). From Table 5, 
we can make the following observations: 1) Our method can effectively generate negative instances. 
In Table 5, the negative instances contain cities, states, countries and general terms, all of which have 
a high semantic overlap with Capital category. 2) The positive instances and negative instances 
generated by our Co-Bootstrapping method can discriminately determine the expansion boundary. For 
instance, the negative instances Kyoto can distinguish Capital from City; Australia and China can 
distinguish Capital from Country; 
Positive Instances 
London,  Paris,  Moscow,  Beijing,  Madrid,  Amsterdam,  Washington,  Tokyo,  Berlin,  Rome,  
Vienna,  Baghdad,  Athens,  Bangkok,  Cairo,  Dublin,  Brussels,  Prague,  San,  Budapest 
Negative Instances 
(with categories)  
City Kyoto,  Kong,  Newcastle,  Zurich,  Lincoln,  Albany,  Lyon,  LA,  Shanghai 
Country China,  Australia 
General downtown,  April 
State Hawaii,  Oklahoma,  Manhattan 
Other Hollywood,  DC,  Tehran,  Charlotte 
Table 5: Top 20 positive instances and negative instances (True positive instances are in bold) 
4.2.3. Detailed Analysis: Semantic Drift Problem 
POS 
Stockholm,  Tampa,  East,  West,  Springfield,  Newport, Cincinnati,  Dublin,  Chattanooga,  Savannah,  
Omaha,  Cambridge,  Memphis,  Providence,  Panama,  Miami,  Cape,  Victoria,  Milan,  Berlin 
ME 
London,  Prague,  Newport,  Cape,  Dublin,  Savannah,  Chattanooga,  Beijing,  Memphis,  Athens,  
Berlin,  Miami,  Plymouth,  Victoria,  Omaha,  Tokyo,  Portland,  Troy,  Anchorage,  Bangkok 
Hum_CB 
London,  Rome,  Berlin,  Paris,  Athens,  Moscow,  Tokyo,  Beijing,  Prague,  Madrid,  Vienna,  
Dublin,  Budapest,  Amsterdam,  Bangkok,  Brussels,  Sydney,  Cairo,  Washington,  Barcelona 
FB_CB 
London,  Paris,  Moscow,  Beijing,  Washington,  Tokyo,  Berlin,  Rome,  Vienna,  Baghdad,  
Athens,  Bangkok,  Cairo,  Brussels,  Prague,  San,  Budapest,  Amsterdam,  Dublin,  Madrid 
Table 6: Top 20 instances of all methods (True positive instances are in bold) 
To analyze how our method can resolve the semantic drift problem, Table 6 shows the top 20 positive 
Capital instances of different methods. From Table 6, we can make the following observations: i) 
Different methods can resolve the semantic drift problem to different extent: ME is better than POS, 
with 50% instances being positive, and our method is better than ME, with 95% instances being 
positive. ii) The Co-Bootstrapping method can effectively resolve the semantic drift problem: 25% of 
POS?s top 20 instances and 50% of ME?s top 20 instances are positive. In contrast, 90% of Hum_CB?s 
top 20 instances and 95% of FB_CB?s top 20 instances are positive respectively. It proves that 
Co-Bootstrapping method can better resolve the semantic drift problem than POS and ME. 
4.3 Parameter Optimization 
 
Figure 4: The MAP vs. threshold of P+ 
Our method has only one parameter: threshold of P+, which determines the instance?s polarity. 
Intuitively, a larger threshold of P+ will improve the precision of the positive instances but will regard 
some positive instances as negative instances mistakenly. As shown in Figure 4, our method can 
achieve the best MAP performance when the value of the threshold is 0.6. 
0
0.2
0.4
0.6
0.8
1
0.0 0.2 0.4 0.6 0.8 1.0
MAP
Threshold of P+ 
 
MAP 
2287
4.4 Comparison with State-of-the-Art Systems 
We also compare our method with three state-of-the-art systems: Google Sets1-- an ESE application 
provided by Google, SEAL2 -- a state-of-the-art ESE method proposed by Wang and Cohen (2008), 
and WMEB -- a state-of-the-art mutual exclusion based system proposed in McIntosh and Curran 
(2008). To make a fair comparison, we directly use the results before the adjustment which miss 
P@10 and P@50 in their original paper (McIntosh and Curran, 2008) and compared the performance 
of these systems on nine categories in (McIntosh and Curran, 2008). For each system, we conduct the 
experiment five times to reduce the impact of seeds selection. The average P@10, P@50, P@100 and 
P@200 are shown in Figure 5. 
 
Figure 5: The results compared with three state-of-the-art systems 
From the results shown in Figure 5, we can see that our probabilistic Co-Bootstrapping method can 
achieve state-of-the-art performance on all metrics: Compared with the well-known baseline Google 
Sets, our method can get a 42.0% improvement on P@200; Compared with the SEAL baseline, our 
method can get a 35.0% improvement on P@200; Compared with the WMEB method, our method can 
achieve a 6.2% improvement on P@100 and a 3.1% improvement on P@200. 
5 Conclusion and Future Work 
In this paper, we proposed a probabilistic Co-Bootstrapping method for entity set expansion. By 
introducing negative instances to define and refine the expansion boundary, our method can 
effectively resolve the expansion boundary problem and the semantic drift problem. Experimental 
results show that our method achieves significant performance improvement over the baselines, and 
outperforms three state-of-the-art ESE systems. Currently, our method did not take into account the 
long tail entity expansion, i.e., the instances which appear only a few times in the corpus, such as 
Saipan, Roseau and Suva for the Capital category. For future work, we will resolve the long tail 
entities in our Co-Bootstrapping method by taking the sparsity of instances/patterns into consideration. 
6 Acknowledgements 
We would like to thank three anonymous reviewers for invaluable comments and suggestions to 
improve our paper. This work is supported by the National Natural Science Foundation of China under 
Grants no. 61100152 and 61272324, and the National High Technology Development 863 Program of 
China under Grants no. 2013AA01A603. 
References 
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting Relations from Large Plain-Text Collections. 
In: Proceedings of the fifth ACM conference on Digital libraries (DL-00), Pages 85-94. 
Joohui An, Seungwoo Lee, and Gary Geunbae Lee. 2003. Automatic acquisition of named entity tagged corpus 
from world wide web. In: Proceedings of ACL-03, Pages 165-168, Volume 2. 
Thorsten Brants and Alex Franz. 2006. Web 1t-5gram version1. http://www.ldc.upenn.edu/Catalog/ 
catalogEntry.jsp?catalogId=LDC2006T13 
                                                        
1 https://docs.google.com/spreadsheet/ 
2 http://www.boowa.com/ 
0.978 0.909 
0.848 
0.773 
0
0.2
0.4
0.6
0.8
1
P@10 P@50 P@100 P@200
Google Sets SEAL WMEB Co-Bootstrapping
2288
Sergey Brin. 1998. Extracting patterns and relations from the World Wide Web. In: Proceedings of the 
Workshop at the 6th International Conference on Extending Database Technology, Pages 172-183. 
Michael J. Cafarella, Doug Downey, Stephen Soderland, and Oren Etzioni. 2005. KnowItNow: Fast, Scalable 
Information Extraction from the Web. In: Proceedings of EMNLP-05, Pages 563-570. 
Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao, Enhong Chen, and Hang Li. 2008. Context-aware 
query suggestion by mining click-through and session data. In Proceedings of KDD-08, pages 875?883. 
Hakan Ceylan and Rada Mihalcea. 2011. An Efficient Indexer for Large N-Gram Corpora. In: Proceedings of 
System Demonstrations of ACL-11, Pages 103-108. 
William W. Cohen and Sunita Sarawagi. 2004. Exploiting dictionaries in named entity extraction: combining 
semi-Markov extraction processes and data integration methods. In: Proceedings of KDD-04, Pages 89-98. 
Alessandro Cucchiarelli and Paola Velardi. 2001. Unsupervised Named Entity Recognition Using Syntactic and 
Semantic Contextual Evidence. In: Computational Linguistics, Pages 123-131, Volume 27. 
James R. Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with Mutual Exclusion 
Bootstrapping. In: Proceedings of the 10th Conference of the Pacific Association for Computational 
Linguistics, Pages 172?180. 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. 
Weld, and Alexander Yates. 2005. Unsupervised Named-Entity Extraction from the Web: An Experimental 
Study. In: Artificial Intelligence, Pages 91-134, Volume 165. 
Jian Hu, Gang Wang, Fred Lochovsky, Jiantao Sun, and Zheng Chen. 2009. Understanding user?s query intent 
with Wikipedia. In Proceedings of WWW-09, Pages 471?480. 
Zornitsa Kozareva and Eduard Hovy. 2010. Learning arguments and supertypes of semantic relations using 
recursive patterns. In: Proceedings of ACL-10, Pages 1482?1491. 
Tara McIntosh and James R. Curran. 2008. Weighted mutual exclusion bootstrapping for domain independent 
lexicon and template acquisition. In: Proceedings of the Australasian Language Technology Association 
Workshop, Pages 97-105. 
Tara McIntosh and James R. Curran. 2009. Reducing semantic drift with bagging and distributional similarity. 
In: Proceedings of ACL-09, Pages 396-404. 
Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In: Proceedings of KDD-08, Pages 
613-619. 
Patrick Pantel and Deepak Ravichandran. 2004. Automatically Labeling Semantic Classes. In: Proceedings of 
HLT/NAACL, Pages 321-328, Volume 4. 
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically 
Harvesting Semantic Relations. In: Proceedings of ACL-06, Pages 113?120. 
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu and Vishnu Vyas. 2009. Web-Scale 
Distributional Similarity and Entity Set Expansion. In: Proceedings of EMNLP-09, Pages 938-947. 
Marius Pasca. 2007. Weakly-supervised discovery of named entities using web search queries. In: Proceedings of 
CIKM-07, Pages 683-690. 
Marco Pennacchiotti, Patrick Pantel. 2011. Automatically building training examples for entity extraction. In: 
Proceedings of CoNLL-11, Pages 163-171. 
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction using multi-level 
bootstrapping. In: Proceedings of AAAI-99, Pages 474-479. 
Partha P. Talukdar, Joseph Reisinger, Marius Pasca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 
2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In: Proceedings of 
EMNLP-08, Pages 582-590. 
Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction 
pattern contexts. In: Proceedings of ACL-02, Pages 214-221. 
Richard C. Wang and William W. Cohen. 2008. Iterative Set Expansion of Named Entities using the Web. In: 
Proceedings of ICDM-08, Pages 1091-1096. 
2289
Richard C. Wang and William W. Cohen. 2009. Automatic Set Instance Extraction using the Web. In: 
Proceedings of ACL-09, Pages 441-449. 
Vishnu Vvas, Patrick Pantel and Eric Crestan. 2009. Helping editors choose better seed sets for entity set 
expansion. In: Proceedings of CIKM-09, Pages 225-234 
Roman Yangarber, Winston Lin and Ralph Grishman. 2002. Unsupervised learning of generalized names. In: 
Proceedings of COLING-02, Pages 1-7. 
2290
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 105?115, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
An Entity-Topic Model for Entity Linking 
Xianpei Han        Le Sun 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{xianpei, sunle}@nfs.iscas.ac.cn 
 
 
Abstract 
Entity Linking (EL) has received 
considerable attention in recent years. 
Given many name mentions in a document, 
the goal of EL is to predict their referent 
entities in a knowledge base. Traditionally, 
there have been two distinct directions of 
EL research: one focusing on the effects of 
mention?s context compatibility, assuming 
that ?the referent entity of a mention is 
reflected by its context?; the other dealing 
with the effects of document?s topic 
coherence, assuming that ?a mention?s 
referent entity should be coherent with the 
document?s main topics?. In this paper, we 
propose a generative model ? called entity-
topic model, to effectively join the above 
two complementary directions together. By 
jointly modeling and exploiting the context 
compatibility, the topic coherence and the 
correlation between them, our model can 
accurately link all mentions in a document 
using both the local information (including 
the words and the mentions in a document) 
and the global knowledge (including the 
topic knowledge, the entity context 
knowledge and the entity name knowledge). 
Experimental results demonstrate the 
effectiveness of the proposed model. 
1 Introduction 
Entity Linking (EL) has received considerable 
research attention in recent years (McNamee & 
Dang, 2009; Ji et al2010). Given many name 
mentions in a document, the goal of EL is to 
predict their referent entities in a given knowledge 
base (KB), such as the Wikipedia1. For example, as 
                                                          
1 www.wikipedia.org 
shown in Figure 1, an EL system should identify 
the referent entities of the three mentions WWDC, 
Apple and Lion correspondingly are the entities 
Apple Worldwide Developers Conference, Apple 
Inc. and Mac OS X Lion in KB. The EL problem 
appears in many different guises throughout the 
areas of natural language processing, information 
retrieval and text mining. For instance, in many 
applications we need to collect all appearances of a 
specific entity in different documents, EL is an 
effective way to resolve such an information 
integration problem. Furthermore, EL can bridge 
the mentions in documents with the semantic 
information in knowledge bases (e.g., Wikipedia 
and Freebase 2 ), thus can provide a solid 
foundation for knowledge-rich methods. 
 
Figure 1. A Demo of Entity Linking 
Unfortunately, the accurate EL is often hindered 
by the name ambiguity problem, i.e., a name may 
refer to different entities in different contexts. For 
example, the name Apple may refer to more than 
20 entities in Wikipedia, such as Apple Inc., Apple 
(band) and Apple Bank. Traditionally, there have 
been two distinct directions in EL to resolve the 
name ambiguity problem: one focusing on the 
effects of mention?s context compatibility and the 
other dealing with the effects of document?s topic 
coherence. EL methods based on context 
                                                          
2 www.freebase.com 
At the WWDC 
conference, 
Apple  
introduces its 
new operating 
system release - 
Lion. 
Document Knowledge Base
Apple Inc. 
MAC OS X Lion 
Steve Jobs 
iPhone
Apple Worldwide 
Developers 
Conference 
California
105
compatibility assume that ?the referent entity of a 
mention is reflected by its context?(Mihalcea & 
Cosomai, 2007; Zhang et al2010; Zheng et al
2010; Han & Sun, 2011; Kataria et al2011; Sen 
2012). For example, the context compatibility 
based methods will identify the referent entity of 
the mention Lion in Figure 1 is the entity Mac OS 
X Lion, since this entity is more compatible with its 
context words operating system and release than 
other candidates such as Lion(big cats) or 
Lion(band). EL methods based on topic coherence 
assume that ?a mention?s referent entity should be 
coherent with document?s main topics? (Medelyan 
et al2008; Milne & Witten, 2008; Kulkarni et al
2009; Han et al2011). For example, the topic 
coherence based methods will link the mention 
Apple in Figure 1 to the entity Apple Inc., since it 
is more coherent with the document?s topic MAC 
OS X Lion Release than other referent candidates 
such as Apple (band) or Apple Bank. 
In recent years, both of the above two EL 
directions have shown their effectiveness to some 
extent, and obviously they are complementary to 
each other. Therefore we believe that bring the 
above two directions together will enhance the EL 
performance. Traditionally, the above two 
directions are usually be brought together using a 
hybrid method (Zhang and Sim, 2011; Ratinov et 
al., 2011; Han et al2011), i.e., the context 
compatibility and the topic coherence are first 
separately modeled, then their EL evidence are 
combined through an additional model. For 
example, Zhang and Sim (2011) first models the 
context compatibility as a context similarity and 
the topic coherence as a similarity between the 
underlying topics of documents and KB entries, 
then these two similarities are combined through 
an additional SVM classifier for the final EL 
decision. 
The main drawback of these hybrid methods, 
however, is that they model the context 
compatibility and the topic coherence separately, 
which makes it difficult to capture the mutual 
reinforcement effect between the above two 
directions. That is, the topic coherence and the 
context compatibility are highly correlated and 
their evidence can be used to reinforce each other 
in EL decisions. For example, in Figure 1, if the 
context compatibility gives a high likelihood the 
mention Apple refers to the entity Apple Inc., then 
this likelihood will give more evidence for this 
document?s topic is about MAC OS X Lion, and it 
in turn will reinforce the topic coherence between 
the entity MAC OS X Lion and the document. In 
reverse, once we known the topic of this document 
is about MAC OS X Lion, the context compatibility 
between the mention Apple and the entity Apple 
Inc. can be improved as the importance of the 
context words operating system and release will be 
increased using the topic knowledge. In this way, 
we believe that modeling the above two directions 
jointly, rather than separately, will further improve 
the EL performance by capturing the mutual 
reinforcement effect between the context 
compatibility and the topic coherence. 
In this paper, we propose a method to jointly 
model and exploit the context compatibility, the 
topic coherence and the correlation between them 
for better EL performance. Specifically, we 
propose a generative probabilistic model ? called 
entity-topic model, which can uniformly model the 
text compatibility and the topic coherence as the 
statistical dependencies between the mentions, the 
words, the underlying entities and the underlying 
topics of a document by assuming that each 
document is generated according to the following 
two assumptions: 
1) Topic coherence assumption: All entities 
in a document should be centered around the main 
topics of the document. For example, the entity 
Apple Inc. tends to occur in documents about IT, 
but the entity Apple Bank  will more likely to occur 
in documents about bank or investment. 
2) Context compatibility assumption: The 
context words of a mention should be centered on 
its referent entity. For example, the words 
computer, phone and music tends to occur in the 
context of the entity Apple Inc., meanwhile the 
words loan, invest and deposit will more likely to 
occur in the context of the entity Apple Bank. 
In this way, the entity-topic model uniformly 
models the context compatibility, the topic 
coherence and the correlation between them as the 
dependencies between the observed information 
(the mentions and the words) in a document and 
the hidden information we want to know (the 
underlying topics and entities) through the global 
knowledge (including the topic knowledge, the 
entity name knowledge and the entity context 
knowledge). And the EL problem can now be 
decomposed into the following two inference tasks: 
106
1) Predicting the underlying topics and the 
underlying entities of a document based on the 
observed information and the global knowledge. 
We call such a task the prediction task; 
2) Estimating the global knowledge from data. 
Notice that the topic knowledge, the entity 
name knowledge and the entity context 
knowledge are all not previously given, thus we 
need to estimate them from data. We call such a 
task the knowledge discovery task. 
Because the accurate inference of the above two 
tasks is intractable in our entity-topic model, this 
paper also develops an approximate inference 
algorithm ? the Gibbs sampling algorithm to solve 
them. 
Contributions. The main contributions of this 
paper are summarized below: 
y We propose a generative probabilistic 
model, the entity-topic model, which can jointly 
model and exploit the context compatibility, the 
topic coherence and the correlation between them 
for better EL performance; 
y We develop a Gibbs sampling algorithm to 
solve the two inference tasks of our model: 1) 
Discovering the global knowledge from data; and 2) 
Collectively making accurate EL decisions. 
This paper is organized as follows. Section 2 
describes the proposed entity-topic model. Section 
3 demonstrates the Gibbs sampling algorithm. The 
experimental results are presented and discussed in 
Section 4. The related work is reviewed in Section 
5. Finally we conclude this paper in Section 6. 
2 The Entity-Topic Model for Entity 
Linking 
In this section, we describe the proposed entity-
topic model. In following we first demonstrate how 
to capture the context compatibility, the topic 
coherence and the correlation between them in the 
document generative process, then we incorporate 
the global knowledge generation into our model 
for knowledge estimation from data. 
2.1 Document Generative Process 
As shown in Section 1, we jointly model the 
context compatibility and the topic coherence as 
the statistical dependencies in the entity-topic 
model by assuming that all documents are 
generated in a topical coherent and context 
compatible way. In following we describe the 
document generative process. 
In our model, each document d is assumed 
composed of two types of information, i.e., the 
mentions and the words. Formally, we represent a 
document as: 
A document is a collection of M mentions and 
N words, denoted as d = {m1, ?, mM; w1, ?, 
wN}, with mi the ith mention and wj the jth word. 
For example, the document in Figure 1 is 
represented as d = {WWDC, Apple, Lion;   at, the, 
conference, ?}, where WWDC, Apple, Lion are 
the three mentions and the other are the words. 
To generate a document, our model relies on 
three types of global knowledge, including: 
y Topic Knowledge ?  (The entity 
distribution of topics): In our model, all entities in 
a document are generated based on its underlying 
topics, with each topic is a group of semantically 
related entities. Statistically, we model each topic 
as a multinomial distribution of entities, with the 
probability indicating the likelihood an entity to be 
extracted from this topic. For example, we may 
have a topic ?Apple Inc:= {Steve Jobs0.12, iPhone0.07, 
iPod0.08, ?}, indicating the likelihood of the entity 
Steve Jobs be extracted from this topic is 0.12, etc. 
y Entity Name Knowledge ?  (The name 
distribution of entities): In our model, all name 
mentions are generated using the name knowledge 
of its referent entity. Specifically, we model the 
name knowledge of an entity as a multinomial 
distribution of its names, with the probability 
indicating the likelihood this entity is mentioned 
by the name. For example, the name knowledge of 
the entity Apple Inc. may be ?Apple Inc: = {Apple0.51, 
Apple Computer Inc.0.10, Apple Inc.0.07, ?}, indicating 
that the entity Apple Inc. is mentioned by the name 
Apple with probability 0.51, etc. 
y Entity Context Knowledge ? (The context 
word distribution of entities): In our model, all 
context words of an entity?s mention are generated 
using its context knowledge. Concretely, we model 
the context knowledge of an entity as a 
multinomial distribution of words, with the 
probability indicating the likelihood a word 
appearing in this entity?s context. For example, we 
may have ?Apple Inc:= {phone0.07, computer0.10, IT0.06, 
phone0.002, ?}, indicating that the word computer 
appearing in the context of the entity Apple Inc. 
with probability 0.1, etc. 
107
 
Figure 2. The document generative process, with 
Dir(:), Mult(:) and Unif(:) correspondingly 
Dirichlet, Multinomial and Uniform distribution 
Given the entity list E = {e1, e2, ?, eE} in the 
knowledge base, the word list V = {w1, w2, ?, wv}, 
the entity name list K = {n1, n2, ?, nK} and the 
global knowledge described in above, the 
generation process of a document collection 
(corpus) D = {d1, d2, ?, dD} is shown in Figure 2.  
To demonstrate the generation process, we also 
demonstrate how the document in Figure 1 can be 
generated using our model in following steps: 
Step 1: The model generates the topic 
distribution of the document as ?d = {Apple Inc.0.45, 
Operating System(OS)0.55}; 
Step 2: For the three mentions in the document: 
i. According to the topic distribution  ?d, the 
model generates their topic assignments as 
z1=Apple Inc., z2 = Apple Inc., z3 = OS; 
ii. According to the topic knowledge ?Apple Inc. , 
?OS  and the topic assignments z1, z2, z3, the model 
generates their entity assignments as e1 = Apple 
Worldwide Developers Conference, e2 = Apple Inc., 
e3 = Mac OS X Lion; 
iii. According to the name knowledge of the 
entities Apple Worldwide Developers Conference, 
Apple Inc. and Mac OS X Lion, our model 
generates the three mentions as m1=WWDC, m2 = 
Apple, m3 = Lion; 
Step 3: For all words in the document: 
i. According to the referent entity set in 
document ed = {Apple Worldwide Developers 
Conference, Apple Inc., Mac OS X Lion}, the 
model generates the target entity they describes as 
a3=Apple Worldwide Developers Conference and 
a4=Apple Inc.; 
ii. According to their target entity and the 
context knowledge of these entities, the model 
generates the context words in the document. For 
example, according to the context knowledge of 
the entities Apple Worldwide Developers 
Conference, the model generates its context word 
w3 =conference, and according to the context 
knowledge of the entity Apple Inc., the model 
generates its context word w4 = introduces. 
Through the above generative process, we can 
see that all entities in a document are extracted 
from the document?s underlying topics, ensuring 
the topic coherence; and all words in a document 
are extracted from the context word distributions 
of its referent entities, resulting in the context 
compatibility. Furthermore, the generation of 
topics, entities, mentions and words are highly 
correlated, thus our model can capture the 
correlation between the topic coherence and the 
context compatibility. 
2.2 Global Knowledge Generative Process 
The entity-topic model relies on three types of 
global knowledge (including the topic knowledge, 
the entity name knowledge and the entity context 
knowledge) to generate a document. Unfortunately, 
all three types of global knowledge are unknown 
and thus need to be estimated from data. In this 
paper we estimate the global knowledge through 
Bayesian inference by also incorporating the 
knowledge generation process into our model. 
Specifically, given the topic number T, the entity 
number E, the name number K and the word 
number V, the entity-topic model generates the 
global knowledge as follows: 
1) ?j? ? Dir(?) 
For each topic z, our model samples its entity 
distribution ?z from an E-dimensional Dirichlet 
distribution with hyperparameter ?. 
2) ?j? ? Dir(?) 
For each entity e, our model samples its name 
distribution ?e from a K-dimensional Dirichlet 
distribution with hyperparameter ?. 
3) ?j? ? Dir(?) 
Given the topic knowledge ? , the entity name 
knowledge ? and the entity context knowledge ?: 
1. For each doc d in D, sample its topic distribution
?d ? Dir(?); 
2. For each of the Md mentions mi in doc d: 
a) Sample a topic assignment zi ? Mult(?d); 
b) Sample an entity assignment ei ? Mult(?zi);
c) Sample a mention mi ? Mult(?ei); 
3. For each of the Nd words wi in doc d: 
a) Sample a target entity it describes from d?s 
referent entities ai ? Unif(em1 ; em2 ;? ? ? ; emd);
b) Sample a describing word using ai?s context 
word distribution wi ? Mult(?ai). 
108
For each entity e, our model samples its context 
word distribution ?e  from a V-dimensional 
Dirichlet distribution with hyperparameter ?. 
Finally, the full entity-topic model is shown in 
Figure 3 using the plate representation. 
?
?
 
Figure 3. The plate representation of the entity-
topic model 
2.3 The Probability of a Corpus 
Using the entity-topic model, the probability of 
generating a corpus D={d1, d2, ?, dD} given 
hyperparameters ?, ?, ? and ? can be expressed as: 
P (D;?; ?; ?; ?) =
Y
d
P (md;wd;?; ?; ?; ?)
=
Y
d
X
ed
P (edj?; ?)P (mdjed; ?)P (wdjed; ?)
=
Z
?
P (?j?)
Z
?
P (?j?)
Y
d
X
ed
P (mdjed; ?)
?
Z
?
P (?j?)
X
ad
P (adjed)P (wdjad; ?)
?
Z
?
P (?j?)P (edj?; ?)d?d?d?d? (2:1)
 
where md  and ed  correspondingly the set of 
mentions and their entity assignments in document 
d, wd and ad correspondingly the set of words and 
their entity assignments in document d. 
3 Inference using Gibbs Sampling 
In this section, we describe how to resolve the 
entity linking problem using the entity-topic model. 
Overall, there were two inference tasks for EL: 
1) The prediction task. Given a document d, 
predicting its entity assignments (ed for mentions 
and ad  for words) and topic assignments ( zd ). 
Notice that here the EL decisions are just the 
prediction of per-mention entity assignments (ed). 
2) The knowledge discovery task. Given a 
corpus D={d1, d2, ?, dD}, estimating the global 
knowledge (including the entity distribution of 
topics ?, the name distribution ? and the context 
word distribution ? of entities) from data. 
Unfortunately, due to the heaven correlation 
between topics, entities, mentions and words (the 
correlation is also demonstrated in Eq. (2.1), where 
the integral is intractable due to the coupling 
between ? , ?, ? and ? ), the accurate inference of 
the above two tasks is intractable. For this reason, 
we propose an approximate inference algorithm ? 
the Gibbs sampling algorithm for the entity-topic 
model by extending the well-known Gibbs 
sampling algorithm for LDA (Griffiths & Steyvers, 
2004). In Gibbs sampling, we first construct the 
posterior distribution P (z; e;ajD) , then this 
posterior distribution is used to: 1) estimate ?, ?, ? 
and ?; and 2) predict the entities and the topics of 
all documents in D. Specifically, we first derive the 
joint posterior distribution from Eq. (2.1) as: 
P (z; e; ajD) / P (z)P (ejz)P (mje)P (aje)P (wja) 
where 
P (z) = (?(T?)?(?)T )
D
DY
d=1
Q
t ?(? +CDTdt )
?(T? + CDTd? )
       (3.1) 
is the probability of the joint topic assignment z to 
all mentions m in corpus D, and 
P (ejz) = (?(E?)?(?)E )
T
TY
t=1
Q
e ?(? + CTEte )
?(E? + CTEt? )
     (3.2) 
is the conditional probability of the joint entity 
assignments e to all mentions m in corpus D given 
all topic assignments z, and 
P (mje) = (?(K?)?(?)K )
E
EY
e=1
Q
m ?(? + CEMem )
?(K? + CEMe? )
   (3.3) 
is the conditional probability of all mentions m 
given all per-mention entity assignments e, and 
P (aje) =
DY
d=1
Y
e?ed
?CDEde
CDEd?
?CDAde              (3.4) 
is the conditional probability of the joint entity  
assignments a to all words w in corpus D given all 
per-mention entity assignments e, and 
109
P (wja) = (?(V ?)?(?)V )
E
EY
e=1
Q
w ?(? + CEWew )
?(V ? + CEWe? )
    (3.5) 
is the conditional probability of all words w given 
all per-word entity assignments a . In all above 
formulas, ?(:) is the Gamma function, CDTdt  is the 
times topic t has been assigned for all mentions in 
document d, CDTd? =
P
t CDTdt  is the topic number 
in document d, and CTEte , CEMem ,CDEde , CDAde , C
EW
ew  
have similar explanation. 
Based on the above joint probability, we 
construct a Markov chain that converges to the 
posterior distribution P (z; e;ajD) and then draw 
samples from this Markov chain for inference. For 
entity-topic model, each state in the Markov chain 
is an assignment (including topic assignment to a 
mention, entity assignment to a mention and entity 
assignment to a word). In Gibbs sampling, all 
assignments are sequentially sampled conditioned 
on all the current other assignments. So here we 
only need to derive the following three fully 
conditional assignment distributions: 
1) P (zi = tjz?i; e;a;D): the topic assignment 
distribution to a mention given the current 
other topic assignments z?i , the current 
entity assignments e and a; 
2) P (ei = ejz; e?i;a;D) : the entity 
assignment distribution to a mention given 
the current entity assignments of all other 
mentions e?i, the current topic assignments 
z  and the current entity assignments of 
context words a; 
3) P (ai = ejz; e; a?i;D) : the entity 
assignment distribution to a context word 
given the current entity assignments of all 
other context words a?i, the current topic 
assignments  z  and the current entity 
assignments e of mentions. 
Using the Formula 3.1-3.5, we can derive the 
above three conditional distributions as (where mi 
is contained in doc d): 
P (zi = tjz?i;e;a;D) /
CDT(?i)dt + ?
CDT(?i)d? + T?
?
CTE(?i)te + ?
CTE(?i)t? +E?
 
where the topic assignment to a mention is 
determined by the probability this topic appearing 
in doc d (the 1st term) and the probability the 
referent entity appearing in this topic (the 2nd term); 
P (ei = ejz; e?i;a;D) /
CTE(?i)te + ?
CTE(?i)t? +E?
?
CEM(?i)em + ?
CEM(?i)e? +K?
?
?CDE(?i)de + 1
CDE(?i)de
?CDAde  
where the entity assignment to a mention is 
determined by the probability this entity extracted 
from the assigned topic (the 1st term), the 
probability this entity is referred by the name m 
(the 2nd term) and the contextual words describing 
this entity in doc d (the 3rd term); 
P (ai = ejz; e;a?i;D) /
CDEde
CDEd?
?
CEW(?i)ew + ?
CEW(?i)e? + V ?
 
where the entity assignment to a word is 
determined by the number of times this entity has 
been assigned to mentions in doc d (the 1st term) 
and the probability the word appearing in the 
context of this entity (the 2nd term). 
Finally, using the above three conditional 
distributions, we iteratively update all assignments 
of corpus D until coverage, then the global 
knowledge is estimated using the final assignments, 
and the final entity assignments are used as the 
referents of their corresponding mentions. 
Inference on Unseen Documents. When 
unseen documents are given, we predict its entities 
and topics using the incremental Gibbs sampling 
algorithm described in (Kataria et al2011), i.e., 
we iteratively update the entity assignments and 
the topic assignments of an unseen document as 
the same as the above inference process, but with 
the previously learned global knowledge fixed. 
Hyperparameter setting. One still problem 
here is the setting of the hyperparameters ?, ?, ? 
and ?. For ? and ? , this paper empirically set the 
value of them to ? = 50=T  and ? = 0:1  as in 
Griffiths & Steyvers(2004). For ?, we notice that 
K? is the number of pseudo names added to each 
entity, when ? = 0  our model only mentions an 
entity using its previously used names. Observed 
that an entity typically has a fixed set of names, we 
set ? to a small value by setting K? = 1:0. For ?, 
we notice that V ? is the number of pseudo words 
added to each entity, playing the role of smoothing 
its context word distribution. As there is typically a 
relatively loose correlation between an entity and 
its context words, we set ?  to a relatively large 
value by fixing the total smoothing words added to 
each entity, a typical value is V ? = 2000. 
110
4 Experiments 
In this section, we evaluate our method and 
compare it with the traditional EL methods. We 
first explain the experimental settings in Section 
4.1-4.4, then discuss the results in Section 4.5. 
4.1 Knowledge Base 
In our experiments, we use the Jan. 30, 2010 
English version of Wikipedia as the knowledge 
base, which contains over 3 million entities. Notice 
that we also take the general concepts in Wikipedia 
(such as Apple, Video, Computer, etc.) as entities, 
so the entity in this paper may not strictly follow 
its definition. 
4.2 Data Sets 
There are two standard data sets for EL: IITB3 and 
TAC 2009 EL data set (McNamee & Dang, 2009), 
where IITB focuses on aggressive recall EL and 
TAC 2009 focuses on EL on salient mentions. Due 
to the collective nature of our method, we mainly 
used the IITB as the primary data set as the same 
as Kulkarni et al009) and Han et al011). But 
we also give the EL accuracies on the TAC 2009 in 
Sect. 4.5.4 as auxiliary results.  
Overall, the IITB data set contains 107 web 
documents. For each document, the name 
mentions? referent entities in Wikipedia are 
manually annotated to be as exhaustive as possible. 
In total, 17,200 name mentions are annotated, with 
161 name mentions per document on average. In 
our experiments, we use only the name mentions 
whose referent entities are contained in Wikipedia. 
4.3 Evaluation Criteria 
This paper adopted the same performance metrics 
used in the Kulkarni et al2009), which includes 
Recall, Precision and F1. Let M* be the golden 
standard set of the EL results (each EL result is a 
pair (m, e), with m the mention and e its referent 
entity), M be the set of EL results outputted by an 
EL system, then these metrics are computed as: 
Precision = jM\M
?j
jMj  
Recall = jM\M
?j
jM?j  
where two EL results are considered equal if and 
only if both their mentions and referent entities are 
equal. As the same as Kulkarni et al009), 
                                                          
3 http://www.cse.iitb.ac.in/~soumen/doc/QCQ/ 
Precision and Recall are averaged across 
documents and overall F1 is used as the primary 
performance metric by computing from average 
Precision and Recall. 
4.4 Baselines 
We compare our method with five baselines which 
are described as follows: 
Wikify!. This is a context compatibility based 
EL method using vector space model (Mihalcea & 
Csomai, 2007). Wikify! computes the context 
compatibility using the word overlap between the 
mention?s context and the entity?s Wikipedia entry. 
EM-Model. This is a statistical context 
compatibility based EL method described in Han 
& Sun(2011), which computes the compatibility by 
integrating the evidence from the entity popularity, 
the entity name knowledge and the context word 
distribution of entities. 
M&W. This is a relational topic coherence based 
EL method described in Milne & Witten(2008). 
M&W measures an entity?s topic coherence to a 
document as its average semantic relatedness to the 
unambiguous entities in the document. 
CSAW. This is an EL method which combines 
context compatibility and topic coherence using a 
hybrid method (Kulkarni et al2009), where 
context compatibility and topic coherence are first 
separated modeled as context similarity and the 
sum of all pair-wise semantic relatedness between 
the entities in the document, then the entities which 
can maximize the weighted sum of the context 
compatibility and the topic coherence are identified 
as the referent entities of the document. 
EL-Graph. This is a graph based hybrid EL 
method described in Han et al2011), which first 
models the context compatibility as text similarity 
and the topic coherence of an entity as its node 
importance in a referent graph which captures all 
mention-entity and entity-entity relations in a 
document, then a random walk algorithm is used to 
collectively find all referent entities of a document. 
Except for CSAW and EL-Graph, all other 
baselines are designed only to link the salient name 
mentions (i.e., key phrases) in a document. In our 
experiment, in order to compare the EL 
performances on also the non-salient name 
mentions, we push these systems? recall by 
reducing their respective importance thresholds of 
linked mentions. 
111
4.5 Experimental Results 
4.5.1 Overall Performance 
We compared our method with all the above five 
baselines. For our method, we estimate the global 
knowledge using all the articles in the Jan. 30, 
2010 English version of Wikipedia, and totally 
there were 3,083,158 articles. For each article, the 
mentions within it are detected using the methods 
described in Medelyan et al008) and all terms in 
an article are used as context words, so a term may 
both be a mention and a context word. The topic 
number of our model is T = 300  (will be 
empirically set in Sect 4.5.2). To train the entity-
topic model, we run 500  iterations of our Gibbs 
sampling algorithm to converge. The training time 
of our model is nearly one week on our server 
using 20 GB RAM and one core of 3.2 GHz CPU. 
Since the training can be done offline, we believe 
that the training time is not critical to the real-
world usage as the online inference on new 
document is very quick. Using the above settings, 
the overall results are shown in Table 1. 
 Precision Recall F1 
Wikify! 0.55 0.28 0.37 
EM-Model 0.82 0.48 0.61 
M&W 0.80 0.38 0.52 
CSAW 0.65 0.73 0.69 
EL-Graph 0.69 0.76 0.73 
Our Method 0.81 0.80 0.80 
Table 1. The overall results on IITB data set 
From the overall results in Table 1, we can see that: 
1) By jointly modeling and exploiting the 
context compatibility and the topic coherence, our 
method can achieve competitive performance: ?1  
compared with the context compatibility baselines 
Wikify! and EM-Model, our method 
correspondingly gets 43% and 19% F1 
improvement; ?2  compared with the topic 
coherence baselines M&W, our method achieves 
28% F1 improvement; ?  compared with the 
hybrid baselines CSAW and EL-Graph, our method 
correspondingly achieves 11% and 7% F1 
improvement. 
2) Compared with the context compatibility 
only and the topic coherence only methods, the 
main advantage of our method is that, rather than 
only achieved high entity linking precision on 
salient mentions, it can also effectively link the 
non-salient mentions in a document: this is 
demonstrated in our method?s significant Recall 
improvement: a 32~52% Recall improvement over 
baselines Wikify!, EM-Model and M&W. We 
believe this is because a document usually contains 
little evidence for EL decisions on non-salient 
mentions, so with either only context compatibility 
or only topic coherence the evidence is not enough 
for EL decisions on these non-salient mentions, 
and bring these two directions together is critical 
for the accurate EL on these mentions. 
3) Compared with the hybrid methods, the 
main advantage of our method is the improvement 
of EL precision (a 11~16% improvement over 
baselines CSAW and EL-Graph), we believe this is 
because: ?1 Our method can further capture the 
mutual reinforcement effect between the context 
compatibility and the topic coherence; ?2 The 
traditional hybrid methods usually determine the 
topic coherence of an entity to a document using 
all entities in the document, in comparison our 
method uses only the entities in the same topic, we 
believe this is more reasonable for EL decisions. 
4.5.2 Parameter Tuning 
One still parameter of our method is the topic 
number T. An appropriate T will distribute entities 
into well-organized topics, in turn it will capture 
the co-occurrence information of entities. Figure 4 
plots the F1 at different T values. We can see that 
the F1 is not very sensitive to the topic number and 
with T = 300  our method achieves its best F1 
performance. 
0 200 400 600 800 1000
0.770
0.775
0.780
0.785
0.790
0.795
0.800
F1
T  
Figure 4. The F1 vs. the topic number T 
4.5.3 Detailed Analysis 
In this section we analyze why and how our 
method works well in detail. Generally, we believe 
the main advantages of our method are: 
1) The effects of topic knowledge. One main 
advantage of our model is that the topic knowledge 
112
can provide a document-specific entity prior for EL. 
Concretely, using the topic knowledge and the 
topic distribution of documents, the prior for an 
entity appearing in a document d is highly related 
to the document?s topics: 
P (ejd) =Pz P (zjd)P (ejz) 
This prior is obviously more reasonable than the 
?information less prior? (i.e., all entities have equal 
prior) or ?a global entity popularity prior? (Han & 
Sun, 2011). To demonstrate, Table 2-3 show the 3 
topics where the Apple Inc. and the fruit Apple 
have the largest generation probability P(e|z) from 
these topics. We can see that the topic knowledge 
can provide a reasonable prior for entities 
appearing in a document: the Apple Inc. has a large 
prior in documents about Computer, Video and 
Software, and the fruit Apple has a large prior in 
documents about Wine, Food and Plant. 
Topic(Computer) Topic(Video) Topic(Software) 
Computer 
CPU 
Hardware 
Personal computer 
Video 
Mobile phone 
Mass media 
Music 
Computer software
Microsoft Windows
Linux 
Web browser 
Computer memory Television Operating system
Table 2. The 3 topics where the Apple Inc. has the 
largest P(e|z) 
Topic(Wine) Topic(Food) Topic(Plant) 
Wine 
Grape 
Vineyard 
Winery 
Food 
Restaurant 
Meat 
Cheese 
Plant 
Flower 
Leaf 
Tree 
Apple Vegetable Fruit 
Table 3. The 3 topics where the fruit Apple has the 
largest P(e|z) 
2) The effects of a fine-tuned context model. 
The second advantage of our model is that it 
provides a statistical framework for fine-tuning the 
context model from data. To demonstrate such an 
effect, Table 4 compares the EL performance of  
? the entity-topic model with no context model is 
used (No Context), i.e., we determine the referent 
entity of a mention by deleting the 3rd term of the 
formula P (ei = ejz;e?i;a;D) in Section 3; ? with 
the context model estimated using the entity?s 
Wikipedia page (Article Content), ? with the 
context model estimated using the 50 word 
window of all its mentions in Wikipedia (Mention 
Context) and; ?  with the context model in the 
original entity-topic model (Entity-Topic Model). 
From Table 4 we can see that a fine-tuned context 
model will result in a 2~7% F1 improvement. 
Context Model F1 
No Context 0.73 
Article Content 
Mention Context 
Entity-Topic Model 
0.75 
0.78 
0.80 
Table 4. The F1 using different context models 
3) The effects of joint model. The third 
advantage of our model is that it jointly model the 
context compatibility and the topic coherence, 
which bring two benefits: ?  the mutual 
reinforcement between the two directions can be 
captured in our model; ? the context compatibility 
and the topic coherence are uniformly modeled and 
jointly estimated, which makes the model more 
accurate for EL. 
4.5.4 EL Accuracies on TAC 2009 dataset 
We also compare our method with the top 5 EL 
systems in TAC 2009 and the two state-of-the-art 
systems (EM-Model and EL-Graph) on TAC 2009 
data set in Figure 5 (For EL-Graph and our method, 
a NIL threshold is used to detect whether the 
referent entity is contained in the knowledge base, 
if the knowledge base not contains the referent 
entity, we assign the mention to a NIL entity). 
From Figure 5, we can see that our method is 
competitive: 1) Our method can achieve a 3.4% 
accuracy improvement over the best system in 
TAC 2009; 2) Our method, EM-Model and EL-
Graph get very close accuracies (0.854, 0.86 and 
0.838 correspondingly), we believe this is because: 
?1  The mentions to be linked in TAC data set are 
mostly salient mentions; ?2  The influence of the 
NIL referent entity problem, i.e., the referent entity 
is not contained in the given knowledge base: Most 
referent entities (67.5%) on TAC 2009 are NIL 
entity and our method has no special handling on 
this problem, rather than other methods such as the 
EM-Model, which affects the overall performance 
of our method. 
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
0.88
 
Figure 5. The EL accuracies on TAC 2009 dataset 
113
5 Related Work 
In this section, we briefly review the related work 
of EL. Traditionally, the context compatibility 
based methods link a mention to the entity which 
has the largest compatibility with it. Cucerzan 
(2007) modeled the compatibility as the cosine 
similarity between the vector space representation 
of mention?s context and of entity?s Wikipedia 
entry. Mihalcea & Csomai (2007), Bunescu & 
Pasca (2006), Fader et al2009), Gottipati et 
al.(2011) and Zhang et al011) extended the 
vector space model with more information such as 
the entity category and the acronym expansion, etc. 
Han & Sun (2011) proposed a generative model 
which computes the compatibility using the 
evidences from entity?s popularity, name 
distribution and context word distribution. Kataria 
et al011) and Sen (2012) used a latent topic 
model to learn the context model of entities. Zheng 
et al2010), Dredze et al2010), Zhang et al
(2010), Zhou et al2010) and Ji & Chen(2011) 
employed the ranking techniques to further take 
relations between candidate entities into account. 
On the other side, the topic coherence based 
methods link a mention to the entity which are 
most coherent to the document containing it. 
Medelyan et al2008) measured the topic 
coherence of an entity to a document as the 
weighted average of its relatedness to the 
unambiguous entities in the document. Milne and 
Witten (2008) extended Medelyan et al2008)?s 
coherence by incorporating commonness and 
context quality. Bhattacharya and Getoor (2006) 
modeled the topic coherence as the likelihood an 
entity is generated from the latent topics of a 
document. Sen (2012) modeled the topic coherence 
as the groups of co-occurring entities. Kulkarni et 
al. (2009) modeled the topic coherence as the sum 
of all pair-wise relatedness between the referent 
entities of a document. Han et al011) and 
Hoffart et al011) modeled the topic coherence of 
an entity as its node importance in a graph which 
captures all mention-entity and entity-entity 
relations in a document. 
6 Conclusions and Future Work 
This paper proposes a generative model, the entity-
topic model, for entity linking. By uniformly 
modeling context compatibility, topic coherence 
and the correlation between them as statistical 
dependencies, our model provides an effective way 
to jointly exploit them for better EL performance. 
In this paper, the entity-topic model can only 
link mentions to the previously given entities in a 
knowledge base. For future work, we want to 
overcome this limit by incorporating an entity 
discovery ability into our model, so that it can also 
discover and learn the knowledge of previously 
unseen entities from a corpus for linking name 
mentions to these entities. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no.  
90920010 and 61100152. Moreover, we sincerely 
thank the reviewers for their valuable comments. 
References 
Adafre, S. F. & de Rijke, M. 2005. Discovering missing 
links in Wikipedia. In: Proceedings of the 3rd 
international workshop on Link discovery. 
Bhattacharya, I. and L. Getoor. 2006. A latent dirichlet 
model for unsupervised entity resolution. In: 
Proceedings of SIAM International Conference on 
Data Mining. 
Blei, D. M. and A. Y. Ng, et al2003). Latent dirichlet 
allocation. In: The Journal of Machine Learning 
Research 3: 993--1022. 
Bunescu, R. & Pasca, M. 2006. Using encyclopedic 
knowledge for named entity disambiguation. In: 
Proceedings of EACL, vol. 6. 
Brown,  P., Pietra, S. D.,  Pietra, V. D., and Mercer, R.  
1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2), 263-31. 
Chen, S. F. & Goodman, J. 1999. An empirical study of 
smoothing techniques for language modeling.  In 
Computer Speech and Language, London; Orlando: 
Academic Press, c1986-, pp. 359-394. 
Cucerzan, S. 2007. Large-scale named entity 
disambiguation based on Wikipedia data. In:  
Proceedings of EMNLP-CoNLL, pp. 708-716. 
De Beaugrande, R. A. and W. U. Dressler. 1981. 
Introduction to text linguistics, Chapter V, Longman 
London. 
Dredze, M., McNamee, P., Rao, D., Gerber, A. & Finin, 
T. 2010. Entity Disambiguation for Knowledge Base 
Population. In: Proceedings of the 23rd International 
Conference on Computational Linguistics. 
114
Fader, A., Soderland, S., Etzioni, O. & Center, T. 2009. 
Scaling Wikipedia-based named entity 
disambiguation to arbitrary web text. In: Proceedings 
of  Wiki-AI Workshop at IJCAI, vol. 9. 
Gottipati, S., Jiang, J. 2011. Linking Entities to a 
Knowledge Base with Query Expansion. In: 
Proceedings of EMNLP. 
Griffiths, T. L. and M. Steyvers. 2004. Finding 
scientific topics. In: Proceedings of the National 
Academy of Sciences of the United States of 
America. 
Han, X., Sun, L. and Zhao J. 2011. Collective Entity 
Linking in Web Text: A Graph-Based Method. In: 
Proceedings of 34th Annual ACM SIGIR Conference. 
Han, X. and Sun, L. 2011. A Generative Entity-Mention 
Model for Linking Entities with Knowledge Base. In: 
Proceedings of ACL-HLT. 
Hoffart, J., Yosef, M. A., et al011. Robust 
Disambiguation of Named Entities in Text. In: 
Proceedings of EMNLP. 
Jelinek, Frederick and Robert L. Mercer. 1980. 
Interpolated estimation of Markov source parameters 
from sparse data. In: Proceedings of the Workshop 
on Pattern Recognition in Practice. 
Kataria, S. S., Kumar, K. S. and Rastogi, R. 2011. Entity 
Disambiguation with Hierarchical Topic Models. In: 
Proceedings of KDD. 
Kulkarni, S., Singh, A., Ramakrishnan, G. & 
Chakrabarti, S. 2009. Collective annotation of 
Wikipedia entities in web text. In: Proceedings of the 
15th ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 457-466. 
Li, X., Morie, P. & Roth, D. 2004. Identification and 
tracing of ambiguous names: Discriminative and 
generative approaches. In: Proceedings of the 
National Conference on Artificial Intelligence, pp. 
419-424. 
McNamee, P. & Dang, H. T. 2009.  Overview of the 
TAC 2009 Knowledge Base Population Track. In: 
Proceeding of Text Analysis Conference. 
Ji, H., et al010. Overview of the TAC 2010 knowledge 
base population track. In: Proceedings of Text 
Analysis Conference. 
Ji, H. and Chen, Z. 2011. Collaborative Ranking: A 
Case Study on Entity Linking. In: Proceedings of 
EMNLP. 
Milne, D. & Witten, I. H. 2008. Learning to link with 
Wikipedia. In: Proceedings of the 17th ACM 
conference on Conference on information and 
knowledge management. 
Milne, D., et al2006. Mining Domain-Specific 
Thesauri from Wikipedia: A case study. In Proc. of 
IEEE/WIC/ACM WI. 
Medelyan, O., Witten, I. H. & Milne, D. 2008. Topic 
indexing with Wikipedia. In: Proceedings of the 
AAAI WikiAI workshop. 
Mihalcea, R. & Csomai, A. 2007. Wikify!: linking 
documents to encyclopedic knowledge. In: 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp. 233-242. 
Pedersen, T., Purandare, A. & Kulkarni, A. 2005. Name 
discrimination by clustering similar contexts. 
Computational Linguistics and Intelligent Text 
Processing, pp. 226-237. 
Ratinov, L. and D. Roth, et al011. Local and Global 
Algorithms for Disambiguation to Wikipedia. In: 
Proceedings of ACL. 
Sen, P. 2012. Collective context-aware topic models for 
entity disambiguation. In Proceedings of WWW '12, 
New York, NY, USA, ACM. 
Zhang, W., Su, J., Tan, Chew Lim & Wang, W. T. 2010. 
Entity Linking Leveraging Automatically Generated 
Annotation. In: Proceedings of the 23rd International 
Conference on Computational Linguistics (Coling 
2010). 
Zheng, Z., Li, F., Huang, M. & Zhu, X. 2010. Learning 
to Link Entities with Knowledge Base. In: The 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL. 
Zhou, Y., Nie, L., Rouhani-Kalleh, O., Vasile, F. & 
Gaffney, S. 2010. Resolving Surface Forms to 
Wikipedia Topics. In: Proceedings of the 23rd 
International Conference on Computational 
Linguistics (Coling 2010),  pp. 1335-1343. 
Zhang, W. and Sim, Y. C., et al2011. Entity Linking 
with Effective Acronym Expansion, Instance 
Selection and Topic Modeling?. In: Proceedings of 
IJCAI. 
 
115
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 50?59,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Structural Semantic Relatedness: A Knowledge-Based Method to 
Named Entity Disambiguation 
 
Xianpei Han        Jun Zhao? 
National Laboratory of Pattern Recognition  
 Institute of Automation, Chinese Academy of Sciences 
Beijing 100190, China  
{xphan,jzhao}@nlpr.ia.ac.cn 
 
  
 
                                                 
? Corresponding author 
Abstract 
 
Name ambiguity problem has raised urgent 
demands for efficient, high-quality named ent-
ity disambiguation methods. In recent years, 
the increasing availability of large-scale, rich 
semantic knowledge sources (such as Wikipe-
dia and WordNet) creates new opportunities to 
enhance the named entity disambiguation by 
developing algorithms which can exploit these 
knowledge sources at best. The problem is that 
these knowledge sources are heterogeneous 
and most of the semantic knowledge within 
them is embedded in complex structures, such 
as graphs and networks. This paper proposes a 
knowledge-based method, called Structural 
Semantic Relatedness (SSR), which can en-
hance the named entity disambiguation by 
capturing and leveraging the structural seman-
tic knowledge in multiple knowledge sources. 
Empirical results show that, in comparison 
with the classical BOW based methods and 
social network based methods, our method can 
significantly improve the disambiguation per-
formance by respectively 8.7% and 14.7%. 
1 Introduction 
Name ambiguity problem is common on the Web. 
For example, the name ?Michael Jordan? 
represents more than ten persons in the Google 
search results. Some of them are shown below: 
Michael (Jeffrey) Jordan, Basketball Player 
Michael (I.) Jordan, Professor of Berkeley 
Michael (B.) Jordan, American Actor 
The name ambiguity has raised serious prob-
lems in many relevant areas, such as web person 
search, data integration, link analysis and know-
ledge base population. For example, in response 
to a person query, search engine returns a long, 
flat list of results containing web pages about 
several namesakes. The users are then forced 
either to refine their query by adding terms, or to 
browse through the search results to find the per-
son they are seeking. Besides, an ever-increasing 
number of question answering and information 
extraction systems are coming to rely on data 
from multi-sources, where name ambiguity will 
lead to wrong answers and poor results. For ex-
ample, in order to extract the birth date of the 
Berkeley professor Michael Jordan, a system 
may return the birth date of his popular name-
sakes, e.g., the basketball player Michael Jordan. 
So there is an urgent demand for efficient, 
high-quality named entity disambiguation me-
thods. Currently, the common methods for 
named entity disambiguation include name ob-
servation clustering (Bagga and Baldwin, 1998) 
and entity linking with knowledge base (McNa-
mee and Dang, 2009). In this paper, we focus on 
the method of name observation clustering. Giv-
en a set of observations O = {o1, o2, ?, on} of the 
target name to be disambiguated, a named entity 
disambiguation system should group them into a 
set of clusters C = {c1, c2, ?, cm}, with each re-
sulting cluster corresponding to one specific enti-
ty. For example, consider the following four ob-
servations of Michael Jordan: 
1) Michael Jordan is a researcher in Computer 
Science. 
2) Michael Jordan plays basketball in Chicago Bulls. 
3) Michael Jordan wins NBA MVP. 
4) Learning in Graphical Models: Michael Jordan. 
A named entity disambiguation system should 
group the 1st and 4th Michael Jordan observations 
into one cluster for they both refer to the Berke-
50
ley professor Michael Jordan, meanwhile group 
the other two Michael Jordan into another clus-
ter as they refer to another person, the Basketball 
Player Michael Jordan. 
To a human, named entity disambiguation is 
usually not a difficult task as he can make deci-
sions depending on not only contextual clues, but 
also the prior background knowledge. For exam-
ple, as shown in Figure 1, with the background 
knowledge that both Learning and Graphical 
models are the topics related to Machine learning, 
while Machine learning is the sub domain of 
Computer science, a human can easily determine 
that the two Michael Jordan in the 1st and 4th ob-
servations represent the same person. In the same 
way, a human can also easily identify that the 
two Michael Jordan in the 2nd and 3rd observa-
tions represent the same person. 
 
Figure 1. The exploitation of knowledge in human 
named entity disambiguation 
The development of systems which could rep-
licate the human disambiguation ability, however, 
is not a trivial task because it is difficult to cap-
ture and leverage the semantic knowledge as 
humankind. Conventionally, the named entity 
disambiguation methods measure the similarity 
between name observations using the bag of 
words (BOW) model (Bagga and Baldwin (1998); 
Mann and Yarowsky (2006); Fleischman and 
Hovy (2004); Pedersen et al (2005)), where a 
name observation is represented as a feature vec-
tor consisting of the contextual terms. This mod-
el measures similarity based on only the co-
occurrence statistics of terms, without consider-
ing all the semantic relations like social related-
ness between named entities, associative related-
ness between concepts, and lexical relatedness 
(e.g., acronyms, synonyms) between key terms. 
 
Figure 2. Part of the link structure of Wikipedia 
Fortunately, in recent years, due to the evolu-
tion of Web (e.g., the Web 2.0 and the Semantic 
Web) and many research efforts for the construc-
tion of knowledge bases, there is an increasing 
availability of large-scale knowledge sources, 
such as Wikipedia and WordNet. These large-
scale knowledge sources create new opportuni-
ties for knowledge-based named entity disam-
biguation methods as they contain rich semantic 
knowledge. For example, as shown in Figure 2, 
the link structure of Wikipedia contains rich se-
mantic relations between concepts. And we be-
lieve that the disambiguation performance can be 
greatly improved by designing algorithms which 
can exploit these knowledge sources at best. 
The problem of these knowledge sources is 
that they are heterogeneous (e.g., they contain 
different types of semantic relations and different 
types of concepts) and most of the semantic 
knowledge within them is embedded in complex 
structures, such as graphs and networks. For ex-
ample, as shown in Figure 2, the semantic rela-
tion between Graphical Model and Computer 
Science is embedded in the link structure of the 
Wikipedia. In recent years, some research has 
investigated to exploit some specific semantic 
knowledge, such as the social connection be-
tween named entities in the Web (Kalashnikov et 
al. (2008), Wan et al (2005) and Lu et al 
(2007)), the ontology connection in DBLP (Has-
sell et al, 2006) and the semantic relations in 
Wikipedia (Cucerzan (2007), Han and Zhao 
(2009)). These knowledge-based methods, how-
ever, usually are specialized to the knowledge 
sources they used, so they often have the know-
ledge coverage problem. Furthermore, these me-
thods can only exploit the semantic knowledge to 
a limited extent because they cannot take the 
structural semantic knowledge into consideration. 
To overcome the deficiencies of previous me-
thods, this paper proposes a knowledge-based 
method, called Structural Semantic Relatedness 
(SSR), which can enhance the named entity dis-
ambiguation by capturing and leveraging the 
structural semantic knowledge from multiple 
knowledge sources. The key point of our method 
is a reliable semantic relatedness measure be-
tween concepts (including WordNet concepts, 
NEs and Wikipedia concepts), called Structural 
Semantic Relatedness, which can capture both 
the explicit semantic relations between concepts 
and the implicit semantic knowledge embedded 
in graphs and networks. In particular, we first 
extract the semantic relations between two con-
cepts from a variety of knowledge sources and 
Computer Science
Machine learning
Statistics 
Graphical model Learning
Mathematic 
Probability Theory 
2) Michael Jordan plays basketball in Chicago Bulls. 
1) Michael Jordan is a researcher in Computer Science.
4) Learning in Graphical Models: Michael Jordan
3) Michael Jordan wins NBA MVP. 
Machine learning 
51
represent them using a graph-based model, se-
mantic-graph. Then based on the principle that 
?two concepts are semantic related if they are 
both semantic related to the neighbor concepts of 
each other?, we construct our Structural Seman-
tic Relatedness measure. In the end, we leverage 
the structural semantic relatedness measure for 
named entity disambiguation and evaluate the 
performance on the standard WePS data sets. 
The experimental results show that our SSR me-
thod can significantly outperform the traditional 
methods. 
This paper is organized as follows. Section 2 
describes how to construct the structural seman-
tic relatedness measure. Next in Section 3 we 
describe how to leverage the captured knowledge 
for named entity disambiguation. Experimental 
results are demonstrated in Sections 4. Section 5 
briefly reviews the related work. Section 6 con-
cludes this paper and discusses the future work. 
2 The Structural Semantic Relatedness 
Measure 
In this section, we demonstrate the structural se-
mantic relatedness measure, which can capture 
the structural semantic knowledge in multiple 
knowledge sources. Totally, there are two prob-
lems we need to address: 
1) How to extract and represent the seman-
tic relations between concepts, since there are 
many types of semantic relations and they may 
exist as different patterns (the semantic know-
ledge may exist as explicit semantic relations or 
be embedded in complex structures). 
2) How to capture all the extracted seman-
tic relations between concepts in our semantic 
relatedness measure. 
To address the above two problems, in follow-
ing we first introduce how to extract the semantic 
relations from multiple knowledge sources; then 
we represent the extracted semantic relations us-
ing the semantic-graph model; finally we build 
our structural semantic relatedness measure. 
2.1 Knowledge Sources 
We extract three types of semantic relations (se-
mantic relatedness between Wikipedia concepts, 
lexical relatedness between WordNet concepts 
and social relatedness between NEs) correspon-
dingly from three knowledge sources: Wikipedia, 
WordNet and NE Co-occurrence Corpus. 
1. Wikipedia1, a large-scale online encyc-
lopedia, its English version includes more than 
3,000,000 concepts and new articles are added 
quickly and up-to-date. Wikipedia contains rich 
semantic knowledge in the form of hyperlinks 
between Wikipedia articles, such as Polysemy 
(disambiguation pages), Synonym (redirect pages) 
and Associative relation (hyperlinks between 
Wikipedia articles). In this paper, we extract the 
semantic relatedness sr between Wikipedia con-
cepts using the method described in Milne and 
Witten(2008): 
log(max( )) log( )
( , ) 1
log( ) log(min( , ))
A B A B
sr a b
W A B
?= ? ?
??
 
where a and b are the two concepts of interest, A 
and B are the sets of all the concepts that are re-
spectively linked to a and b, and W is the entire 
Wikipedia. For demonstration, we show the se-
mantic relatedness between four selected con-
cepts in Table 1. 
 Statistics Basketball
Machine learning 0.58 0.00 
MVP 0.00 0.45 
Table 1. The semantic relatedness table of four se-
lected Wikipedia concepts 
2. WordNet 3.02 (Fellbaum et al, 1998), a 
lexical knowledge source includes over 110,000 
WordNet concepts (word senses about English 
words). Various lexical relations are recorded 
between WordNet concepts, such as hyponyms, 
holonym and synonym. The lexical relatedness lr 
between two WordNet concepts are measured 
using the Lin (1998)?s WordNet semantic simi-
larity measure. Table 2 shows some examples of 
the lexical relatedness. 
 school science 
university 0.67 0.10 
research 0.54 0.39 
Table 2. The lexical relatedness table of four selected 
WordNet concepts 
3. NE Co-occurrence Corpus, a corpus of 
documents for capturing the social relatedness 
between named entities. According to the fuzzy 
set theory (Baeza-Yates et al, 1999), the degree 
of named entities co-occurrence in a corpus is a 
measure of the relatedness between them. For 
example, in Google search results, the ?Chicago 
Bulls? co-occurs with ?NBA? in more than 
                                                 
1 http://www.wikipedia.org/ 
2 http:// wordnet.princeton.edu/ 
52
7,900,000 web pages, while only co-occurs with 
?EMNLP? in less than 1,000 web pages. So the 
co-occurrence statistics can be used to measure 
the social relatedness between named entities. In 
this paper, given a NE Co-occurrence Corpus D, 
the social relatedness scr between two named 
entities ne1 and ne2 is measured using the Google 
Similarity Distance (Cilibrasi and Vitanyi, 2007): 
1 2 1 2
1 2
1 2
log(max( , )) log( )
( , ) 1
log( ) log(min( , ))
D D D D
scr ne ne
D D D
?= ? ?
?  
where D1 and D2 are the document sets corres-
pondingly containing ne1 and ne2. An example of 
social relatedness is shown in Table 3, which is 
computed using the Web corpus through Google. 
 ACL NBA 
EMNLP 0.61 0.00 
Chicago Bulls 0.19 0.55 
Table 3. The social relatedness table of four selected 
named entities 
2.2 The Semantic-Graph Model 
In this section we present a graph-based repre-
sentation, called semantic-graph, to model the 
extracted semantic relations as a graph within 
which the semantic relations are interconnected 
and transitive. Concretely, the semantic-graph is 
defined as follows: 
A semantic-graph is a weighted graph G = (V, 
E), where each node represents a distinct con-
cept; and each edge between a pair of nodes 
represents the semantic relation between the 
two concepts corresponding to these nodes, 
with the edge weight indicating the strength of 
the semantic relation. 
For demonstration, Figure 3 shows a semantic-
graph which models the semantic knowledge 
extracted from Wikipedia for the Michael Jordan 
observations in Section 1. 
 
Figure 3. An example of semantic-graph 
Given a set of name observations, the con-
struction of semantic-graph takes two steps: con-
cept extraction and concept connection. In the 
following we respectively describe each step. 
1) Concept Extraction. In this step we ex-
tract all the concepts in the contexts of name ob-
servations and represent them as the nodes in the 
semantic-graph. We first gather all the N-grams 
(up to 8 words) and identify whether they corres-
pond to semantically meaningful concepts: if a 
N-gram is contained in the WordNet, we identify 
it as a WordNet concept, and use its primary 
word sense as its semantic meaning; to find 
whether a N-gram is a named entity, we match it 
to the named entity list extracted using the open-
Calais API3, which contains more than 30 types 
of named entities, such as Person, Organization 
and Award; to find whether a N-gram is a Wiki-
pedia concept, we match it to the Wikipedia anc-
hor dictionary, then find its corresponding Wiki-
pedia concept using the method described in 
(Medelyan et al 2008). After concept identifica-
tion, we filter out all the N-grams which do not 
correspond to the semantic meaningful concepts, 
such as the N-grams ?learning in? and ?wins 
NBA MVP?. The retained N-grams are identified 
as concepts, corresponding with their semantic 
meanings (a concept may have multiple semantic 
meaning explanation, e.g., the ?MVP? has three 
semantic meaning, as ?most valuable player, 
MVP? in WordNet, as the ?Most Valuable Play-
er? in Wikipedia and as a named entity of Award 
type). 
2) Concept Connection. In this step we 
represent the semantic relations as the edges be-
tween nodes. That is, for each pair of extracted 
concepts, we identify whether there are semantic 
relations between them: 1) If there is only one 
semantic relation between them, we connect 
these two concepts with an edge, where the edge 
weight is the strength of the semantic relation; 2) 
If there is more than one semantic relations be-
tween them, we choose the most reliable seman-
tic relation, i.e., we choose the semantic relation 
in the knowledge sources according to the order 
of WordNet, Wikipedia and NE Co-concurrence 
corpus (Suchanek et al, 2007). For example, if 
both Wikipedia and WordNet provide the seman-
tic relation between MVP and NBA, we choose 
the semantic relation provided by WordNet. 
                                                 
3 http://www.opencalais.com/ 
Researcher Graphical  
Model 
Learning 
NBA
MVP
Basketball 
Chicago Bulls 
Computer 
Science 
0.32 0.28 
0.48 
0.41 
0.58 
0.76 
0.45 
0.71 
0.71 0.57 
53
2.3 The Structural Semantic Relatedness 
Measure 
In this section, we describe how to capture the 
semantic relations between the concepts in se-
mantic-graph using a semantic relatedness meas-
ure. Totally, the semantic knowledge between 
concepts is modeled in two forms: 
1) The edges of semantic-graph. The 
edges model the direct semantic relations be-
tween concepts. We call this form of semantic 
knowledge as explicit semantic knowledge. 
2) The structure of semantic-graph. Ex-
cept for the edges, the structure of the semantic-
graph also models the semantic knowledge of 
concepts. For example, the neighbors of a con-
cept represent all the concepts which are explicit-
ly semantic-related to this concept; and the paths 
between two concepts represent all the explicit 
and implicit semantic relations between them. 
We call this form of semantic knowledge as 
structural semantic knowledge, or implicit se-
mantic knowledge. 
Therefore, in order to deduce a reliable seman-
tic relatedness measure, we must take both the 
edges and the structure of semantic-graph into 
consideration. Under the semantic-graph model, 
the measurement of semantic relatedness be-
tween concepts equals to quantifying the similar-
ity between nodes in a weighted graph. To simpl-
ify the description, we assign each node in se-
mantic-graph an integer index from 1 to |V| and 
use this index to represent the node, then we can 
write the adjacency matrix of the semantic-graph 
G as A, where A[i,j] or Aij is the edge weight be-
tween node i and node j. 
The problem of quantifying the relatedness be-
tween nodes in a graph is not a new problem, e.g., 
the structural equivalence and structural similar-
ity (the SimRank in Jeh and Widom (2002) and 
the similarity measure in Leicht et al (2006)). 
However, these similarity measures are not suit-
able for our task, because all of them assume that 
the edges are uniform so that they cannot take 
edge weight into consideration. 
In order to take both the graph structure and 
the edge weight into account, we design the 
structural semantic relatedness measure by ex-
tending the measure introduced in Leicht et al 
(2006). The fundamental principle behind our 
measure is ?a node u is semantically related to 
another node v if its immediate neighbors are 
semantically related to v?. This definition is natu-
ral, for example, as shown in Figure 3, the con-
cept Basketball and its neighbors NBA and Chi-
cago Bulls are all semantically related to MVP. 
This definition is recursive, and the starting point 
we choose is the semantic relatedness in the edge. 
Thus our structural semantic relatedness has two 
components: the neighbor term of the previous 
recursive phase which captures the graph struc-
ture and the semantic relatedness which captures 
the edge information. Thus, the recursive form of 
the structural semantic relatedness Sij between 
the node i and the node j can be written as: 
i
il
ij lj ij
l N i
A
S S A
d
? ?
?
= +?  
where ?  and ?  control the relative importance 
of the two components and 
Ni={j | Aij > 0} is the set of the immediate 
neighbors of node i; 
j Ni
d Aiji ?
?= is the degree of node i. 
In order to solve this formula, we introduce the 
following two notations: 
T: The relatedness transition matrix, where 
T[i,j]=Aij/di, indicating the transition rate of re-
latedness from node j to its neighbor i. 
S: The structural semantic relatedness matrix, 
where S[i,j]=Sij. 
Now we can turn our first form of structural se-
mantic relatedness into the matrix form: 
S TS A? ?= +  
By solving this equation, we can get: 
1( )S I T A? ? ?= ?  
where I is the identity matrix. Since ?  is a pa-
rameter which only contributes an overall scale 
factor to the relatedness value, we can ignore it 
and get the final form of the structural semantic 
relatedness as: 
1( )S I T A? ?= ?  
Because the S is asymmetric, the finally related-
ness between node i and node j is the average of 
Sij and Sji. 
The meaning of ? : The last question of our 
structural semantic relatedness measure is how to 
set the free parameter ? . To understand the 
meaning of ? , let us expand the similarity as a 
power series thus: 
2 2( ... ...)k kS I T T T A? ? ?= + + + + +  
Noting that the [Tk]ij element is the relatedness 
transition rate from node i to node j with path 
length k, we can view the ?  as a penalty factor 
for the transition path length: by setting the ?  
with a value within (0, 1), a longer graph path 
will contribute less to the final relatedness value. 
The optimal value of ?  is 0.6 through a learning 
54
process shown in Section 4. For demonstration, 
Table 4 shows some structural semantic related-
ness values of the Semantic-graph in Figure 3 
(CS represents computer science and GM 
represents Graphical model). From Table 4, we 
can see that the structural semantic relatedness 
can successfully capture the semantic knowledge 
embedded in the structure of semantic-graph, 
such as the implicit semantic relation between 
Researcher and Learning. 
 Researcher CS GM Learning
Researcher --- 0.50 0.27 0.31 
CS 0.50 --- 0.62 0.73 
GM 0.27 0.62 --- 0.80 
Learning 0.31 0.73 0.80 --- 
Table 4. The structural semantic relatedness of the 
semantic-graph shown in Figure 3 
3 Named Entity Disambiguation by Le-
veraging Semantic Knowledge 
In this section we describe how to leverage the 
semantic knowledge captured in the structural 
semantic relatedness measure for named entity 
disambiguation. Because the key problem of 
named entity disambiguation is to measure the 
similarity between name observations, we inte-
grate the structural semantic relatedness in the 
similarity measure, so that it can better reflect the 
actual similarity between name observations. 
Concretely, our named entity disambiguation 
system works as follows: 1) Measuring the simi-
larity between name observations; 2) Grouping 
name observations using the clustering algorithm. 
In the following we describe each step in detail. 
3.1 Measuring the Similarity between Name 
Observations 
Intuitively, if two observations of the target name 
represent the same entity, it is highly possible 
that the concepts in their contexts are closely re-
lated, i.e., the named entities in their contexts are 
socially related and the Wikipedia concepts in 
their contexts are semantically related. In con-
trast, if two name observations represent differ-
ent entities, the concepts within their contexts 
will not be closely related. Therefore we can 
measure the similarity between two name obser-
vations by summarizing all the semantic related-
ness between the concepts in their contexts. 
To measure the similarity between name ob-
servations, we represent each name observation 
as a weighted vector of concepts (including 
named entities, Wikipedia concepts and Word-
Net concepts), where the concepts are extracted 
using the same method described in Section 2.2, 
so they are just the same concepts within the se-
mantic-graph. Using the same concept index as 
the semantic-graph, a name observation oi is then 
represented as 1 2{ , ,..., }i i i ino w w w= , where wik is 
the kth concept?s weight in observation oi, com-
puted using the standard TFIDF weight model, 
where the DF is computed using the Google 
Web1T 5-gram corpus4. Given the concept vec-
tor representation of two name observations oi 
and oj, their similarity is computed as: 
( , )i j il jk lk il jk
l k l k
SIM o o w w S w w=?? ??  
which is the weighted average of all the structur-
al semantic relatedness between the concepts in 
the contexts of the two name observations. 
3.2 Grouping Name Observations through 
Hierarchical Agglomerative Clustering 
Given the computed similarities, name observa-
tions are disambiguated by grouping them ac-
cording to their represented entities. In this paper, 
we group name observations using the hierar-
chical agglomerative clustering(HAC) algorithm, 
which is widely used in prior disambiguation 
research and evaluation task (WePS1 and 
WePS2). The HAC produce clusters in a bottom-
up way as follows: Initially, each name observa-
tion is an individual cluster; then we iteratively 
merge the two clusters with the largest similarity 
value to form a new cluster until this similarity 
value is smaller than a preset merging threshold 
or all the observations reside in one common 
cluster. The merging threshold can be deter-
mined through cross-validation. We employ the 
single-link method to compute the similarity be-
tween two clusters, which has been applied wide-
ly in prior research (Bagga and Baldwin (1998); 
Mann and Yarowsky (2003)). 
4 Experiments 
To assess the performance of our method and 
compare it with traditional methods, we conduct 
a series of experiments. In the experiments, we 
evaluate the proposed SSR method on the task of 
personal name disambiguation, which is the most 
common type of named entity disambiguation. In 
the following, we first explain the general expe-
rimental settings in Section 4.1, 4.2 and 4.3; then 
evaluate and discuss the performance of our me-
thod in Section 4.4. 
                                                 
4 www.ldc.upenn.edu/Catalog/docs/LDC2006T13/ 
55
4.1 Disambiguation Data Sets 
We adopted the standard data sets used in the 
First Web People Search Clustering Task 
(WePS1) (Artiles et al, 2007) and the Second 
Web People Search Clustering Task (WePS2) 
(Artiles et al, 2009). The three data sets we used 
are WePS1_training data set, WePS1_test data 
set, and WePS2_test data set. Each of the three 
data sets consists of a set of ambiguous personal 
names (totally 109 personal names); and for each 
name, we need to disambiguate its observations 
in the web pages of the top N (100 for WePS1 
and 150 for WePS2) Yahoo! search results. 
The experiment made the standard ?one per-
son per document? assumption, which is widely 
used in the participated systems in WePS1 and 
WePS2, i.e., all the observations of the same 
name in a document are assumed to represent the 
same entity. Based on this assumption, the fea-
tures within the entire web page are used to dis-
ambiguate personal names. 
4.2 Knowledge Sources 
There were three knowledge sources we used for 
our experiments: the WordNet 3.0; the Sep. 9, 
2007 English version of Wikipedia; and the Web 
pages of each ambiguous name in WePS datasets 
as the NE Co-occurrence Corpus. 
4.3 Evaluation Criteria 
We adopted the measures used in WePS1 to eva-
luate the performance of name disambiguation. 
These measures are: 
Purity (Pur): measures the homogeneity of 
name observations in the same cluster; 
Inverse purity (Inv_Pur): measures the com-
pleteness of a cluster; 
F-Measure (F): the harmonic mean of purity 
and inverse purity. 
The detailed definitions of these measures can 
be found in Amigo, et al (2008). We use F-
measure as the primary measure just liking 
WePS1 and WePS2. 
4.4 Experimental Results 
We compared our method with four baselines: (1) 
BOW: The first one is the traditional Bag of 
Words model (BOW) based methods: hierarchic-
al agglomerative clustering (HAC) over term 
vector similarity, where the features including 
single words and NEs, and all the features are 
weighted using TFIDF. This baseline is also the 
state-of-art method in WePS1 and WePS2. (2) 
SocialNetwork: The second one is the social 
network based methods, which is the same as the 
method described in Malin et al (2005): HAC 
over the similarity obtained through random 
walk over the social network built from the web 
pages of the top N search results. (3)SSR-
NoKnowledge: The third one is used as a base-
line for evaluating the efficiency of semantic 
knowledge: HAC over the similarity computed 
on semantic-graph with no knowledge integrated, 
i.e., the similarity is computed as: 
( , )i j il jl il jk
l l k
SIM o o w w w w=? ??  
(4) SSR-NoStructure: The fourth one is used as 
a baseline for evaluating the efficiency of the 
semantic knowledge embedded in complex struc-
tures: HAC over the similarity computed by only 
integrating the explicit semantic relations, i.e., 
the similarity is computed as: 
( , )i j il jk lk il jk
l k l k
SIM o o w w A w w=?? ??  
4.4.1 Overall Performance 
We conducted several experiments on all the 
three WePS data sets: the four baselines, the pro-
posed SSR method and the proposed SSR me-
thod with only one special type knowledge added, 
respectively SSR-NE, SSR-WordNet and SSR-
Wikipedia. All the optimal merging thresholds 
used in HAC were selected by applying leave-
one-out cross validation. The overall perfor-
mance is shown in Table 5. 
Method 
WePS1_training
Pur Inv_Pur F
BOW 0.71 0.88 0.78
SocialNetwork 0.66 0.98 0.76
SSR-NoKnowledge 0.79 0.89 0.81
SSR-NoStructure 0.87 0.83 0.83
SSR-NE 0.80 0.86 0.82
SSR-WordNet 0.80 0.91 0.83
SSR-Wikipedia 0.82 0.90 0.84
SSR 0.82 0.92 0.85
WePS1_test 
Pur Inv_Pur F
BOW 0.74 0.87 0.74
SocialNetwork 0.83 0.63 0.65
SSR-NoKnowledge 0.80 0.74 0.75
SSR-NoStructure 0.80 0.78 0.78
SSR-NE 0.73 0.80 0.74
SSR-WordNet 0.81 0.77 0.77
SSR-Wikipedia 0.88 0.77 0.81
SSR 0.85 0.83 0.84
WePS2_test 
Pur Inv_Pur F
BOW 0.80 0.80 0.77
SocialNetwork 0.62 0.93 0.70
SSR-NoKnowledge 0.84 0.80 0.80
SSR-NoStructure 0.84 0.83 0.81
SSR-NE 0.78 0.88 0.80
SSR-WordNet 0.85 0.82 0.83
SSR-Wikipedia 0.84 0.81 0.82
SSR 0.89 0.84 0.86
Table 5. Performance results of baselines and SSR 
methods 
56
From the performance results in Table 5, we 
can see that: 
1) The semantic knowledge can greatly im-
prove the disambiguation performance: com-
pared with the BOW and the SocialNetwork 
baselines, SSR respectively gets 8.7% and 14.7% 
improvement on average on the three data sets. 
2) By leveraging the semantic knowledge 
from multiple knowledge sources, we can obtain 
a better named entity disambiguation perfor-
mance: compared with the SSR-NE?s 0% im-
provement, the SSR-WordNet?s 2.3% improve-
ment and the SSR-Wikipedia?s 3.7% improve-
ment, the SSR gets 6.3% improvement over the 
SSR-NoKnowledge baseline, which is larger than 
all the SSR methods with only one type of se-
mantic knowledge integrated. 
3) The exploitation of the structural seman-
tic knowledge can further improve the disambig-
uation performance: compared with SSR-
NoStructure, our SSR method achieves 4.3% im-
provement. 
 
Figure 4. The F-Measure vs. ?  on three data sets 
4.4.2 Optimizing Parameters 
There is only one parameter ?  needed to be con-
figured, which is the penalty factor for the rela-
tedness transition path length in the structural 
semantic relatedness measure. Usually a smaller 
?  will make the structural semantic knowledge 
contribute less in the resulting relatedness value. 
Figure 4 plots the performance of our method 
corresponding to the special ?  settings. As 
shown in Figure 4, the SSR method is not very 
sensitive to the ?  and can achieve its best aver-
age performance when the value of ?  is 0.6. 
4.4.3 Detailed Analysis 
To better understand the reasons why our SSR 
method works well and how the exploitation of 
structural semantic knowledge can improve per-
formance, we analyze the results in detail. 
The Exploitation of Semantic Knowledge. The 
primary advantage of our method is the exploita-
tion of semantic knowledge. Our method exploits 
the semantic knowledge in two directions: 
1) The Integration of Multiple Semantic 
Knowledge Sources. Using the semantic-graph 
model, our method can integrate the semantic 
knowledge extracted from multiple knowledge 
sources, while most traditional knowledge-based 
methods are usually specialized to one type of 
knowledge. By integrating multiple semantic 
knowledge sources, our method can improve the 
semantic knowledge coverage. 
2) The exploitation of Semantic Knowledge 
embedded in complex structures. Using the struc-
tural semantic relatedness measure, our method 
can exploit the implicit semantic knowledge em-
bedded in complex structures; while traditional 
knowledge-based methods usually lack this abili-
ty. 
The Rich Meaningful Features. One another 
advantage of our method is the rich meaningful 
features, which is brought by the multiple seman-
tic knowledge sources. With more meaningful 
features, our method can better describe the 
name observations with less information loss. 
Furthermore, unlike the traditional N-gram fea-
tures, the features enriched by semantic know-
ledge sources are all semantically meaningful 
units themselves, so little noisy features will be 
added. The effect of rich meaningful features can 
also be shown in Table 5: by adding these fea-
tures, the SSR-NoKnowledge respectively 
achieves 2.3% and 9.7% improvement over the 
BOW and the SocialNetwork baseline. 
5 Related Work 
In this section, we briefly review the related 
work. Totally, the traditional named entity dis-
ambiguation methods can be classified into two 
categories: the shallow methods and the know-
ledge-based methods. 
Most of previous named entity disambiguation 
researches adopt the shallow methods, which are 
mostly the natural extension of the bag of words 
(BOW) model. Bagga and Baldwin (1998) 
represented a name as a vector of its contextual 
words, then two names were predicted to be the 
same entity if their cosine similarity is above a 
threshold. Mann and Yarowsky (2003) and Niu 
et al (2004) extended the vector representation 
with extracted biographic facts. Pedersen et al 
(2005) employed significant bigrams to represent 
57
a name observation. Chen and Martin (2007) ex-
plored a range of syntactic and semantic features. 
In recent years some research has investigated 
employing knowledge sources to enhance the 
named entity disambiguation. Bunescu and Pasca 
(2006) disambiguated the names using the cate-
gory information in Wikipedia. Cucerzan (2007) 
disambiguated the names by combining the BOW 
model with the Wikipedia category information. 
Han and Zhao (2009) leveraged the Wikipedia 
semantic knowledge for computing the similarity 
between name observations. Bekkerman and 
McCallum (2005) disambiguated names based 
on the link structure of the Web pages between a 
set of socially related persons. Kalashnikov et al 
(2008) and Lu et al (2007) used the co-
occurrence statistics between named entities in 
the Web. The social network was also exploited 
for named entity disambiguation, where similari-
ty is computed through random walking, such as 
the work introduced in Malin (2005), Malin and 
Airoldi (2005), Yang et al(2006) and Minkov et 
al. (2006). Hassell et al (2006) used the relation-
ships from DBLP to disambiguate names in re-
search domain. 
6 Conclusions and Future Works 
In this paper we demonstrate how to enhance the 
named entity disambiguation by capturing and 
exploiting the semantic knowledge existed in 
multiple knowledge sources. In particular, we 
propose a semantic relatedness measure, Struc-
tural Semantic Relatedness, which can capture 
both the explicit semantic relations and the im-
plicit structural semantic knowledge. The expe-
rimental results on the WePS data sets demon-
strate the efficiency of the proposed method. For 
future work, we want to develop a framework 
which can uniformly model the semantic know-
ledge and the contextual clues for named entity 
disambiguation. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no. 
60875041 and 60673042, and the National High 
Technology Development 863 Program of China 
under Grants no. 2006AA01Z144. 
References  
Amigo, E., Gonzalo, J., Artiles, J. and Verdejo, F. 
2008. A comparison of extrinsic clustering evalua-
tion metrics based on formal constraints. Informa-
tion Retrieval. 
Artiles, J., Gonzalo, J. & Sekine, S. 2007. The Se-
mEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. In 
SemEval. 
Artiles, J., Gonzalo, J. and Sekine, S. 2009. WePS2 
Evaluation Campaign: Overview of the Web 
People Search Clustering Task. In WePS2, WWW 
2009. 
Baeza-Yates, R., Ribeiro-Neto, B., et al 1999. Mod-
ern information retrieval. Addison-Wesley Reading, 
MA. 
Bagga, A. & Baldwin, B. 1998. Entity-based cross-
document coreferencing using the vector space 
model. Proceedings of the 17th international confe-
rence on Computational linguistics-Volume 1, pp. 
79-85. 
Bekkerman, R. & McCallum, A. 2005. Disambiguat-
ing web appearances of people in a social network. 
Proceedings of the 14th international conference on 
World Wide Web, pp. 463-470. 
Bunescu, R. & Pasca, M. 2006. Using encyclopedic 
knowledge for named entity disambiguation. Pro-
ceedings of EACL, vol. 6.  
Chen, Y. & Martin, J. 2007. Towards robust unsuper-
vised personal name disambiguation. Proceedings 
of EMNLP and CoNLL, pp. 190-198. 
Cilibrasi, R. L., Vitanyi, P. M. & CWI, A. 2007. The 
google similarity distance, IEEE Transactions on 
knowledge and data engineering, vol. 19, no. 3, pp. 
370-383. 
Cucerzan, S. 2007, Large-scale named entity disam-
biguation based on Wikipedia data. Proceedings of 
EMNLP-CoNLL, pp. 708-716. 
Fellbaum, C., et al 1998. WordNet: An electronic 
lexical database. MIT press Cambridge, MA. 
Fleischman, M. B. & Hovy, E. 2004. Multi-document 
person name resolution. Proceedings of ACL, Ref-
erence Resolution Workshop. 
Han, X. & Zhao, J. 2009. Named entity disambigua-
tion by leveraging Wikipedia semantic knowledge. 
Proceeding of the 18th ACM conference on Infor-
mation and knowledge management, pp. 215-224. 
Hassell, J., Aleman-Meza, B. & Arpinar, I. 2006. On-
tology-driven automatic entity disambiguation in 
unstructured text. Proceedings of The 2006 ISWC, 
pp. 44-57. 
Jeh, G. & Widom, J. 2002. SimRank: A measure of 
structural-context similarity, Proceedings of the 
eighth ACM SIGKDD international conference on 
Knowledge discovery and data mining, p. 543.  
58
Kalashnikov, D. V., Nuray-Turan, R. & Mehrotra, S. 
2008. Towards Breaking the Quality Curse. A 
Web-Querying Approach to Web People Search. In 
Proc. of SIGIR. 
Leicht, E. A.,  Petter Holme,  & M. E. J. Newman. 
2006. Vertex similarity in networks. Physical Re-
view E , vol. 73, no. 2, p. 26120. 
Lin., D. 1998. An information-theoretic definition of 
similarity. In Proc. of ICML. 
Lu, Y. & Nie , Z. et al 2007. Name Disambiguation 
Using Web Connection. In Proc. of AAAI. 
Malin, B. 2005. Unsupervised name disambiguation 
via social network similarity. SIAM SDM Work-
shop on Link Analysis, Counterterrorism and Secu-
rity. 
Malin, B., Airoldi, E. & Carley, K. M. 2005. A net-
work analysis model for disambiguation of names 
in lists. Computational & Mathematical Organiza-
tion Theory, vol. 11, no. 2, pp. 119-139. 
Mann, G. S. & Yarowsky, D. 2003. Unsupervised 
personal name disambiguation, Proceedings of the 
seventh conference on Natural language learning at 
HLT-NAACL 2003-Volume 4, p. 40. 
McNamee, P. & Dang, H. Overview of the TAC 2009 
Knowledge Base Population Track. In Proceedings 
of Text Analysis Conference (TAC-2009), 2009. 
Medelyan, O., Witten, I. H. & Milne, D. 2008. Topic 
indexing with Wikipedia. Proceedings of the AAAI 
WikiAI workshop. 
Milne, D., Medelyan, O. & Witten, I. H. 2006. Min-
ing domain-specific thesauri from wikipedia: A 
case study. IEEE/WIC/ACM International Confe-
rence on Web Intelligence, pp. 442-448. 
Minkov, E., Cohen, W. W. & Ng, A. Y. 2006. Con-
textual search and name disambiguation in email 
using graphs, Proceedings of the 29th annual inter-
national ACM SIGIR conference on Research and 
development in information retrieval, pp. 27-34. 
Niu C., Li W. and Srihari, R. K. 2004. Weakly Super-
vised Learning for Cross-document Person Name 
Disambiguation Supported by Information Extrac-
tion. Proceedings of ACL, pp. 598-605. 
Pedersen, T., Purandare, A. & Kulkarni, A. 2005. 
Name discrimination by clustering similar contexts. 
Computational Linguistics and Intelligent Text 
Processing, pp. 226-237. 
Strube, M. & Ponzetto, S. P. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia, Pro-
ceedings of the National Conference on Artificial 
Intelligence, vol. 21, no. 2, p. 1419. 
Suchanek, F. M., Kasneci, G. & Weikum, G. 2007. 
Yago: a core of semantic knowledge, Proceedings 
of the 16th international conference on World 
Wide Web, p. 706. 
Wan, X., Gao, J., Li, M. & Ding, B. 2005. Person 
resolution in person search results: Webhawk. Pro-
ceedings of the 14th ACM international conference 
on Information and knowledge management, p. 
170. 
Witten, D. M. & Milne, D. 2008. An effective, low-
cost measure of semantic relatedness obtained from 
Wikipedia links. Proceeding of AAAI Workshop 
on Wikipedia and Artificial Intelligence: an Evolv-
ing Synergy, AAAI Press, Chicago, USA, pp. 25-
30. 
Yang, K. H., Chiou, K. Y., Lee, H. M. & Ho, J. M. 
2006. Web appearance disambiguation of personal 
names based on network motif. Proceedings of the 
2006 IEEE/WIC/ACM International Conference on 
Web Intelligence, pp. 386-389. 
 
59
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 945?954,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Generative Entity-Mention Model for Linking Entities with 
Knowledge Base 
Xianpei Han        Le Sun 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{xianpei, sunle}@nfs.iscas.ac.cn 
 
Abstract 
Linking entities with knowledge base (entity 
linking) is a key issue in bridging the textual 
data with the structural knowledge base. Due to 
the name variation problem and the name 
ambiguity problem, the entity linking decisions 
are critically depending on the heterogenous 
knowledge of entities. In this paper, we propose 
a generative probabilistic model, called entity-
mention model, which can leverage 
heterogenous entity knowledge (including 
popularity knowledge, name knowledge and 
context knowledge) for the entity linking task. 
In our model, each name mention to be linked 
is modeled as a sample generated through a 
three-step generative story, and the entity 
knowledge is encoded in the distribution of 
entities in document P(e), the distribution of 
possible names of a specific entity P(s|e), and 
the distribution of possible contexts of a 
specific entity P(c|e). To find the referent entity 
of a name mention, our method combines the 
evidences from all the three distributions P(e), 
P(s|e) and P(c|e). Experimental results show 
that our method can significantly outperform 
the traditional methods. 
1 Introduction 
In recent years, due to the proliferation of 
knowledge-sharing communities like Wikipedia 1 
and the many research efforts for the automated 
knowledge base population from Web like the 
Read the Web2 project, more and more large-scale 
knowledge bases are available. These knowledge 
bases contain rich knowledge about the world?s 
entities, their semantic properties, and the semantic 
relations between each other. One of the most 
notorious examples is Wikipedia: its 2010 English 
                                                        
1 http://www.wikipedia.org/ 
2 http://rtw.ml.cmu.edu/ 
version contains more than 3 million entities and 
20 million semantic relations. Bridging these 
knowledge bases with the textual data can facilitate 
many different tasks such as entity search, 
information extraction and text classification. For 
example, as shown in Figure 1, knowing the word 
Jordan in the document refers to a basketball 
player and the word Bulls refers to a NBA team 
would be helpful in classifying this document into 
the Sport/Basketball class. 
After a standout career at the University,
joined the in 1984.
Michael Jeffrey Jordan
NBA Player
Basketball Player
Chicago Bulls
NBA
Sport Organization
NBA Team
Knowledge Base
Employer-of
IS-A
IS-A IS-A
IS-A
IS-A
Part-of
Jordan
Bulls
 
Figure 1. A Demo of Entity Linking 
A key issue in bridging the knowledge base with 
the textual data is linking the entities in a 
document with their referents in a knowledge base, 
which is usually referred to as the Entity Linking 
task. Given a set of name mentions M = {m1, 
m2, ?, mk} contained in documents and a 
knowledge base KB containing a set of entities E = 
{e1, e2, ?, en}, an entity linking system is a 
function : M E? ?  which links these name 
mentions to their referent entities in KB. For 
example, in Figure 1 an entity linking system 
should link the name mention Jordan to the entity 
Michael Jeffrey Jordan and the name mention 
Bulls to the entity Chicago Bulls. 
The entity linking task, however, is not trivial 
due to the name variation problem and the name 
ambiguity problem. Name variation means that an 
entity can be mentioned in different ways such as 
full name, aliases, acronyms and misspellings. For 
945
example, the entity Michael Jeffrey Jordan can be 
mentioned using more than 10 names, such as 
Michael Jordan, MJ and Jordan. The name 
ambiguity problem is related to the fact that a 
name may refer to different entities in different 
contexts. For example, the name Bulls can refer to 
more than 20 entities in Wikipedia, such as the 
NBA team Chicago Bulls, the football team Belfast 
Bulls and the cricket team Queensland Bulls. 
Complicated by the name variation problem and 
the name ambiguity problem, the entity linking 
decisions are critically depending on the 
knowledge of entities (Li et al, 2004; Bunescu & 
Pasca, 2006; Cucerzan, 2007; Milne & Witten, 
2008 and Fader et al, 2009). Based on the previous 
work, we found that the following three types of 
entity knowledge can provide critical evidence for 
the entity linking decisions: 
? Popularity Knowledge. The popularity 
knowledge of entities tells us the likelihood of an 
entity appearing in a document. In entity linking, 
the entity popularity knowledge can provide a 
priori information to the possible referent entities 
of a name mention. For example, without any other 
information, the popularity knowledge can tell that 
in a Web page the name ?Michael Jordan? will 
more likely refer to the notorious basketball player 
Michael Jeffrey Jordan, rather than the less 
popular Berkeley professor Michael I. Jordan. 
? Name Knowledge. The name knowledge 
tells us the possible names of an entity and the 
likelihood of a name referring to a specific entity. 
For example, we would expect the name 
knowledge tells that both the ?MJ? and ?Michael 
Jordan? are possible names of the basketball 
player Michael Jeffrey Jordan, but the ?Michael 
Jordan? has a larger likelihood. The name 
knowledge plays the central role in resolving the 
name variation problem, and is also helpful in 
resolving the name ambiguity problem. 
? Context Knowledge. The context 
knowledge tells us the likelihood of an entity 
appearing in a specific context. For example, given 
the context ?__wins NBA MVP?, the name 
?Michael Jordan? should more likely refer to the 
basketball player Michael Jeffrey Jordan than the 
Berkeley professor Michael I. Jordan. Context 
knowledge is crucial in solving the name 
ambiguities. 
Unfortunately, in entity linking system, the 
modeling and exploitation of these types of entity 
knowledge is not straightforward. As shown above, 
these types of knowledge are heterogenous, 
making it difficult to be incorporated in the same 
model. Furthermore, in most cases the knowledge 
of entities is not explicitly given, making it 
challenging to extract the entity knowledge from 
data. 
To resolve the above problems, this paper 
proposes a generative probabilistic model, called 
entity-mention model, which can leverage the 
heterogeneous entity knowledge (including 
popularity knowledge, name knowledge and 
context knowledge) for the entity linking task. In 
our model, each name mention is modeled as a 
sample generated through a three-step generative 
story, where the entity knowledge is encoded in 
three distributions: the entity popularity knowledge 
is encoded in the distribution of entities in 
document P(e), the entity name knowledge is 
encoded in the distribution of possible names of a 
specific entity P(s|e), and the entity context 
knowledge is encoded in the distribution of 
possible contexts of a specific entity P(c|e). The 
P(e), P(s|e) and P(c|e) are respectively called the 
entity popularity model, the entity name model and 
the entity context model. To find the referent entity 
of a name mention, our method combines the 
evidences from all the three distributions P(e), 
P(s|e) and P(c|e). We evaluate our method on both 
Wikipedia articles and general newswire 
documents. Experimental results show that our 
method can significantly improve the entity linking 
accuracy. 
Our Contributions. Specifically, the main 
contributions of this paper are as follows: 
1) We propose a new generative model, the 
entity-mention model, which can leverage 
heterogenous entity knowledge (including 
popularity knowledge, name knowledge and 
context knowledge) for the entity linking task; 
2) By modeling the entity knowledge as 
probabilistic distributions, our model has a 
statistical foundation, making it different from 
most previous ad hoc approaches. 
This paper is organized as follows. The entity-
mention model is described in Section 2. The 
model estimation is described in Section 3. The 
experimental results are presented and discussed in 
Section 4. The related work is reviewed in Section 
5. Finally we conclude this paper in Section 6. 
946
2 The Generative Entity-Mention Model 
for Entity Linking 
In this section we describe the generative entity-
mention model. We first describe the generative 
story of our model, then formulate the model and 
show how to apply it to the entity linking task. 
2.1 The Generative Story 
In the entity mention model, each name mention is 
modeled as a generated sample. For demonstration, 
Figure 2 shows two examples of name mention 
generation. As shown in Figure 2, the generative 
story of a name mention is composed of three steps, 
which are detailed as follows: 
(i) Firstly, the model chooses the referent 
entity e of the name mention from the given 
knowledge base, according to the distribution of 
entities in document P(e). In Figure 2, the model 
chooses the entity ?Michael Jeffrey Jordan? for the 
first name mention, and the entity ?Michael I. 
Jordan? for the second name mention; 
(ii) Secondly, the model outputs the name s of 
the name mention according to the distribution of 
possible names of the referent entity P(s|e). In 
Figure 2, the model outputs ?Jordan? as the name 
of the entity ?Michael Jeffrey Jordan?, and the 
?Michael Jordan? as the name of the entity 
?Michael I. Jordan?; 
(iii) Finally, the model outputs the context c of 
the name mention according to the distribution of 
possible contexts of the referent entity P(c|e). In 
Figure 2, the model outputs the context ?joins 
Bulls in 1984? for the first name mention, and the 
context ?is a professor in UC Berkeley? for the 
second name mention. 
2.2 Model 
Based on the above generative story, the 
probability of a name mention m (its context is c 
and its name is s) referring to a specific entity e 
can be expressed as the following formula (here we 
assume that s and c are independent given e): 
( , , )P(m,e)= P s c e = P(e)P(s |e)P(c|e) 
This model incorporates the three types of entity 
knowledge we explained earlier: P(e) corresponds 
to the popularity knowledge, P(s|e) corresponds to 
the name knowledge and P(c|e) corresponds to the 
context knowledge. 
Knowledge Base
Michael Jeffrey Jordan
Michael I. Jordan
Jordan Michael Jordan
Jordan joins Bulls in
1984.
Michael Jordan is a
professor in UC Berkeley.
Entity
Name
Mention
 
Figure 2.  Two examples of name mention 
generation 
Given a name mention m, to perform entity 
linking, we need to find the entity e which 
maximizes the probability P(e|m). Then we can 
resolve the entity linking task as follows: 
( , )e argmax argmax ( ) ( | ) ( | )( ) ee
P m e P e P s e P c eP m? ?
 
Therefore, the main problem of entity linking is to 
estimate the three distributions P(e), P(s|e) and 
P(c|e), i.e., to extract the entity knowledge from 
data. In Section 3, we will show how to estimate 
these three distributions. 
Candidate Selection. Because a knowledge base 
usually contains millions of entities, it is time-
consuming to compute all P(m,e) scores between a 
name mention and all the entities contained in a 
knowledge base. To reduce the time required, the 
entity linking system employs a candidate selection 
process to filter out the impossible referent 
candidates of a name mention. In this paper, we 
adopt the candidate selection method of 
NLPR_KBP system (Han and Zhao, 2009), the 
main idea of which is first building a name-to-
entity dictionary using the redirect links, 
disambiguation pages, anchor texts of Wikipedia, 
then the candidate entities of a name mention are 
selected by finding its name?s corresponding entry 
in the dictionary. 
3 Model Estimation  
Section 2 shows that the entity mention model can 
decompose the entity linking task into the 
estimation of three distributions P(e), P(s|e) and 
P(c|e). In this section, we describe the details of the 
estimation of these three distributions. We first 
947
introduce the training data, then describe the 
estimation methods. 
3.1 Training Data 
In this paper, the training data of our model is a set 
of annotated name mentions M = {m1, m2, ?, mn}. 
Each annotated name mention is a triple m={s, e, 
c}, where s is the name, e is the referent entity and 
c is the context. For example, two annotated name 
mentions are as follows: 
? Jordan | Michael Jeffrey Jordan | ? wins his first NBA 
MVP in 1991. 
? NBA | National Basketball Association | ? is the pre-
eminent men's professional basketball league. 
In this paper, we focus on the task of linking 
entities with Wikipedia, even though the proposed 
method can be applied to other resources. We will 
only show how to get the training data from 
Wikipedia. In Wikipedia, a hyperlink between two 
articles is an annotated name mention (Milne & 
Witten, 2008): its anchor text is the name and its 
target article is the referent entity. For example, in 
following hyperlink (in Wiki syntax), the NBA is 
the name and the National Basketball Association 
is the referent entity. 
?He won his first [[National Basketball Association | 
NBA]] championship with the Bulls?  
Therefore, we can get the training data by 
collecting all annotated name mentions from the 
hyperlink data of Wikipedia. In total, we collected 
more than 23,000,000 annotated name mentions. 
3.2 Entity Popularity Model 
The distribution P(e) encodes the popularity 
knowledge as a distribution of entities, i.e., the 
P(e1) should be larger than P(e2) if e1 is more 
popular than e2. For example, on the Web the 
P(Michael Jeffrey Jordan) should be higher than 
the P(Michael I. Jordan). In this section, we 
estimate the distribution P(e) using a model called 
entity popularity model. 
Given a knowledge base KB which contains N 
entities, in its simplest form, we can assume that 
all entities have equal popularity, and the 
distribution P(e) can be estimated as: 
( ) 1P e N?  
However, this does not reflect well the real 
situation because some entities are obviously more 
popular than others. To get a more precise 
estimation, we observed that a more popular entity 
usually appears more times than a less popular 
entity in a large text corpus, i.e., more name 
mentions refer to this entity. For example, in 
Wikipedia the NBA player Michael Jeffrey Jordan 
appears more than 10 times than the Berkeley 
professor Michael I. Jordan. Based on the above 
observation, our entity popularity model uses the 
entity frequencies in the name mention data set M 
to estimate the distribution P(e) as follows: 
( ) 1( ) Count eP e M N
?? ?
 
where Count(e) is the count of the name mentions 
whose referent entity is e, and the |M| is the total 
name mention size. The estimation is further 
smoothed using the simple add-one smoothing 
method for the zero probability problem. For 
illustration, Table 1 shows three selected entities? 
popularity. 
Entity Popularity 
National Basketball Association 1.73*10-5 
Michael Jeffrey Jordan(NBA player) 8.21*10-6 
Michael I. Jordan(Berkeley Professor) 7.50*10-8 
Table 1. Three examples of entity popularity 
3.3 Entity Name Model 
The distribution P(s|e) encodes the name 
knowledge of entities, i.e., for a specific entity e, 
its more frequently used name should be assigned a 
higher P(s|e) value than the less frequently used 
name, and a zero P(s|e) value should be assigned 
to those never used names. For instance, we would 
expect the P(Michael Jordan|Michael Jeffrey 
Jordan) to be high, P(MJ|Michael Jeffrey Jordan) 
to be relative high and P(Michael I. 
Jordan|Michael Jeffrey Jordan) to be zero. 
Intuitively, the name model can be estimated by 
first collecting all (entity, name) pairs from the 
name mention data set, then using the maximum 
likelihood estimation: 
( , )( | ) ( , )
s
Count e sP s e Count e s??
 
where the Count(e,s) is the count of the name 
mentions whose referent entity is e and name is s. 
However, this method does not work well because 
it cannot correctly deal with an unseen entity or an 
unseen name. For example, because the name 
?MJ? doesn?t refer to the Michael Jeffrey Jordan in 
Wikipedia, the name model will not be able to 
identify ?MJ? as a name of him, even ?MJ? is a 
popular name of Michael Jeffrey Jordan on Web. 
948
To better estimate the distribution P(s|e), this 
paper proposes a much more generic model, called 
entity name model, which can capture the 
variations (including full name, aliases, acronyms 
and misspellings) of an entity's name using a 
statistical translation model. Given an entity?s 
name s, our model assumes that it is a translation 
of this entity?s full name f using the IBM model 1 
(Brown, et al, 1993). Let ?  be the vocabulary 
containing all words may be used in the name of 
entities, the entity name model assumes that a 
word in ? can be translated through the following 
four ways: 
1) It is retained (translated into itself); 
2) It is translated into its acronym; 
3) It is omitted(translated into the word NULL); 
4) It is translated into another word (misspelling 
or alias). 
In this way, all name variations of an entity are 
captured as the possible translations of its full 
name. To illustrate, Figure 3 shows how the full 
name ?Michael Jeffrey Jordan? can be transalted 
into its misspelling name ?Micheal Jordan?. 
 
Figure 3. The translation from Michael Jefferey 
Jordan to Micheal Jordan 
Based on the translation model, P(s|e) can be 
written as: 
01
( | )( 1)
fs
s
ll
i jl
ijf
P(s |e) t s fl
?
??
? ? ??
 
where ? is a normalization factor, f is the full name 
of entity e, lf is the length of f, ls is the length of the 
name s, si the i
th word of s, fj is the j
th word of f and 
t(si|fj) is the lexical translation probability which 
indicates the probability of a word fj in the full 
name will be written as si in the output name. 
Now the main problem is to estimate the lexical 
translation probability t(si|fj). In this paper, we first 
collect the (name, entity full name) pairs from all 
annotated name mentions, then get the lexical 
translation probability by feeding this data set into 
an IBM model 1 training system (we use the 
GIZA++ Toolkit3). 
Table 2 shows several resulting lexical 
translation probabilities through the above process. 
                                                        
3 http://fjoch.com/GIZA++.html 
We can see that the entity name model can capture 
the different name variations, such as the acronym 
(Michael?M), the misspelling (Michael?Micheal) 
and the omission (St. ? NULL). 
Full name word Name word Probability 
Michael Michael 0.77 
Michael M 0.008 
Michael Micheal 2.64*10
-4
 
Jordan Jordan 0.96 
Jordan J 6.13*10
-4
 
St. NULL 0.14 
Sir NULL 0.02 
Table 2. Several lexical translation probabilities 
3.4 Entity Context Model 
The distribution P(c|e) encodes the context 
knowledge of entities, i.e., it will assign a high 
P(c|e) value if the entity e frequently appears in the 
context c, and will assign a low P(c|e) value if the 
entity e rarely appears in the context c. For 
example, given the following two contexts: 
C1: __wins NBA MVP. 
C2: __is a researcher in machine learning. 
Then P(C1|Michael Jeffrey Jordan) should be high 
because the NBA player Michael Jeffrey Jordan 
often appears in C1 and the P(C2|Michael Jeffrey 
Jordan) should be extremely low because he rarely 
appears in C2. 
__ wi s NBA MVP.
__is a professor in UC
Berkeley.
Michael Jeffrey Jordan
(NBA Player)
NBA=0.03
MVP=0.008
Basketball=0.02
player=0.005
win=0.00008
professor=0
...
Michael I. Jordan
(Berkeley Professor)
professor=0.003
Berkeley=0.002
machine learning=0.1
researcher = 0.006
NBA = 0
MVP=0
...
 
Figure 4. Two entity context models 
To estimate the distribution P(c|e), we propose a 
method based on language modeling, called entity 
context model. In our model, the context of each 
name mention m is the word window surrounding 
m, and the window size is set to 50 according to 
the experiments in (Pedersen et al, 2005). 
Specifically, the context knowledge of an entity e 
is encoded in an unigram language model: 
{ ( )}e eM P t?  
where Pe(t) is the probability of the term t 
appearing in the context of e. In our model, the 
term may indicate a word, a named entity 
(extracted using the Stanford Named Entity 
Michael Jeffrey Jordan 
Micheal Jordan NULL 
Full Name 
Name 
949
Recognizer 4 ) or a Wikipedia concept (extracted 
using the method described in (Han and Zhao, 
2010)). Figure 4 shows two entity context models 
and the contexts generated using them. 
Now, given a context c containing n terms 
t1t2?tn, the entity context model estimates the 
probability P(c|e) as: 
1 2 1 2( | ) ( ... | ) ( ) ( ).... ( )n e e e e nP c e P t t t M P t P t P t? ? 
So the main problem is to estimate Pe(t), the 
probability of a term t appearing in the context of 
the entity e. 
Using the annotated name mention data set M, 
we can get the maximum likelihood estimation of 
Pe(t) as follows: 
_
( )( ) ( )
e
e ML
e
t
Count tP t Count t??
 
where Counte(t) is the frequency of occurrences of 
a term t in the contexts of the name mentions 
whose referent entity is e. 
Because an entity e?s name mentions are usually 
not enough to support a robust estimation of Pe(t) 
due to the sparse data problem (Chen and 
Goodman, 1999), we further smooth Pe(t) using the 
Jelinek-Mercer smoothing method (Jelinek and 
Mercer, 1980): 
_( ) ( ) (1 ) ( )e e ML gP t P t P t? ?? ? ?
 
where Pg(t) is a general language model which is 
estimated using the whole Wikipedia data, and the 
optimal value of ? is set to 0.2 through a learning 
process shown in Section 4. 
3.5 The NIL Entity Problem 
By estimating P(e), P(s|e) and P(c|e), our method 
can effectively link a name mention to its referent 
entity contained in a knowledge base. 
Unfortunately, there is still the NIL entity problem 
(McNamee and Dang, 2009), i.e., the referent 
entity may not be contained in the given 
knowledge base. In this situation, the name 
mention should be linked to the NIL entity. 
Traditional methods usually resolve this problem 
with an additional classification step (Zheng et al 
2010): a classifier is trained to identify whether a 
name mention should be linked to the NIL entity. 
Rather than employing an additional step, our 
entity mention model seamlessly takes into account 
the NIL entity problem. The start assumption of 
                                                        
4 http://nlp.stanford.edu/software/CRF-NER.shtml 
our solution is that ?If a name mention refers to a 
specific entity, then the probability of this name 
mention is generated by the specific entity?s model 
should be significantly higher than the probability 
it is generated by a general language model?. 
Based on the above assumption, we first add a 
pseudo entity, the NIL entity, into the knowledge 
base and assume that the NIL entity generates a 
name mention according to the general language 
model Pg, without using any entity knowledge; 
then we treat the NIL entity in the same way as 
other entities: if the probability of a name mention 
is generated by the NIL entity is higher than all 
other entities in Knowledge base, we link the name 
mention to the NIL entity. Based on the above 
discussion, we compute the three probabilities of 
the NIL entity: P(e), P(s|e) and P(c|e) as follows: 
1P(NIL) M N? ?
 
( )gt sP(s | NIL) P t???
 
( )gt cP(c | NIL) P t???
 
4 Experiments 
In this section, we assess the performance of our 
method and compare it with the traditional 
methods. In following, we first explain the 
experimental settings in Section 4.1, 4.2 and 4.3, 
then evaluate and discuss the results in Section 4.4. 
4.1 Knowledge Base 
In our experiments, we use the Jan. 30, 2010 
English version of Wikipedia as the knowledge 
base, which contains over 3 million distinct entities. 
4.2 Data Sets 
To evaluate the entity linking performance, we 
adopted two data sets: the first is WikiAmbi, which 
is used to evaluate the performance on Wikipedia 
articles; the second is TAC_KBP, which is used to 
evaluate the performance on general newswire 
documents. In following, we describe these two 
data sets in detail. 
WikiAmbi: The WikiAmbi data set contains 1000 
annotated name mentions which are randomly 
selected from Wikipedia hyperlinks data set (as 
shown in Section 3.1, the hyperlinks between 
Wikipedia articles are manually annotated name 
mentions). In WikiAmbi, there were 207 distinct 
950
names and each name contains at least two 
possible referent entities (on average 6.7 candidate 
referent entities for each name) 5 . In our 
experiments, the name mentions contained in the 
WikiAmbi are removed from the training data. 
TAC_KBP: The TAC_KBP is the standard data 
set used in the Entity Linking task of the TAC 
2009 (McNamee and Dang, 2009). The TAC_KBP 
contains 3904 name mentions which are selected 
from English newswire articles. For each name 
mention, its referent entity in Wikipedia is 
manually annotated. Overall, 57% (2229 of 3904) 
name mentions?s referent entities are missing in 
Wikipedia, so TAC_KBP is also suitable to 
evaluate the NIL entity detection performance. 
The above two data sets can provide a standard 
testbed for the entity linking task. However, there 
were still some limitations of these data sets: First, 
these data sets only annotate the salient name 
mentions in a document, meanwhile many NLP 
applications need all name mentions are linked. 
Second, these data sets only contain well-formed 
documents, but in many real-world applications the 
entity linking often be applied to noisy documents 
such as product reviews and microblog messages. 
In future, we want to develop a data set which can 
reflect these real-world settings. 
4.3 Evaluation Criteria 
We adopted the standard performance metrics used 
in the Entity Linking task of the TAC 2009 
(McNamee and Dang, 2009). These metrics are: 
? Micro-Averaged Accuracy (Micro-
Accuracy): measures entity linking accuracy 
averaged over all the name mentions; 
? Macro-Averaged Accuracy (Macro-
Accuracy): measures entity linking accuracy 
averaged over all the target entities. 
As in TAC 2009, we used Micro-Accuracy as the 
primary performance metric. 
4.4 Experimental Results 
We compared our method with three baselines: (1) 
The first is the traditional Bag of Words based 
method (Cucerzan, 2007): a name mention?s 
referent entity is the entity which has the highest 
cosine similarity with its context ? we denoted it as 
BoW; (2) The second is the method described in 
                                                        
5 This is because we want to create a highly ambiguous test 
data set 
(Medelyan et al, 2008), where a name mention?s 
referent entity is the entity which has the largest 
average semantic relatedness with the name 
mention?s unambiguous context entities ? we 
denoted it as TopicIndex. (3) The third one is the 
same as the method described in (Milne & Witten, 
2008), which uses learning techniques to balance 
the semantic relatedness, commoness and context 
quality ? we denoted it as Learning2Link. 
4.4.1 Overall Performance 
We conduct experiments on both WikiAmbi and 
TAC_KBP datasets with several methods: the 
baseline BoW; the baseline TopicIndex; the 
baseline Learning2Link; the proposed method 
using only popularity  knowledge (Popu), i.e., the 
P(m,e)=P(e); the proposed method with one 
component of the model is ablated(this is used to 
evaluate the independent contributions of the three 
components), correspondingly Popu+Name(i.e., 
the P(m,e)=P(e)P(s|e)), Name+Context(i.e., the 
P(m,e)=P(c|e)P(s|e)) and Popu+Context (i.e., the 
P(m,e)=P(e)P(c|e)); and the full entity mention 
model (Full Model). For all methods, the 
parameters were configured through 10-fold cross 
validation. The overall performance results are 
shown in Table 3 and 4. 
 Micro-Accuracy Macro-Accuracy 
BoW 0.60 0.61 
TopicIndex 0.66 0.49 
Learning2Link 0.70 0.54 
Popu 0.39 0.24 
Popu + Name 0.50 0.31 
Name+Context 0.70 0.68 
Popu+Context 0.72 0.73 
Full Model 0.80 0.77 
Table 3. The overall results on WikiAmbi dataset 
 Micro-Accuracy Macro-Accuracy 
BoW 0.72 0.75 
TopicIndex 0.80 0.76 
Learning2Link 0.83 0.79 
Popu 0.60 0.53 
Popu + Name 0.63 0.59 
Name+Context 0.81 0.78 
Popu+Context 0.84 0.83 
Full Model 0.86 0.88 
Table 4. The overall results on TAC-KBP dataset 
From the results in Table 3 and 4, we can make the 
following observations: 
1) Compared with the traditional methods, 
our entity mention model can achieve a significant 
951
performance improvement: In WikiAmbi and 
TAC_KBP datasets, compared with the BoW 
baseline, our method respectively gets 20% and 
14% micro-accuracy improvement; compared with 
the TopicIndex baseline, our method respectively 
gets 14% and 6% micro-accuracy improvement; 
compared with the Learning2Link baseline, our 
method respectively gets 10% and 3% micro-
accuracy improvement. 
2) By incorporating more entity knowledge, 
our method can significantly improve the entity 
linking performance: When only using the 
popularity knowledge, our method can only 
achieve 49.5% micro-accuracy. By adding the 
name knowledge, our method can achieve 56.5% 
micro-accuracy, a 7% improvement over the Popu. 
By further adding the context knowledge, our 
method can achieve 83% micro-accuracy, a 33.5% 
improvement over Popu and a 26.5% improvement 
over Popu+Name. 
3) All three types of entity knowledge 
contribute to the final performance improvement, 
and the context knowledge contributes the most: 
By respectively ablating the popularity knowledge, 
the name knowledge and the context knowledge, 
the performance of our model correspondingly 
reduces 7.5%, 5% and 26.5%. 
NIL Entity Detection Performance. To 
compare the performances of resolving the NIL 
entity problem, Table 5 shows the micro-
accuracies of different systems on the TAC_KBP 
data set (where All is the whole data set, NIL only 
contains the name mentions whose referent entity 
is NIL, InKB only contains the name mentions 
whose referent entity is contained in the 
knowledge base). From Table 5 we can see that our 
method can effectively detect the NIL entity 
meanwhile retaining the high InKB accuracy. 
 All NIL InKB 
BoW 0.72 0.77 0.65 
TopicIndex 0.80 0.91 0.65 
Learning2Link 0.83 0.90 0.73 
Full Model 0.86 0.90 0.79 
Table 5.  The NIL entity detection performance on 
the TAC_KBP data set 
4.4.2 Optimizing Parameters 
Our model needs to tune one parameter: the 
Jelinek-Mercer smoothing parameter ? used in the 
entity context model. Intuitively, a smaller ? 
means that the general language model plays a 
more important role. Figure 5 plots the tradeoff.  In 
both WikiAmbi and TAC_KBP data sets,  Figure 5 
shows that a ? value 0.2 will result in the best 
performance. 
 
Figure 5. The micro-accuracy vs. ? 
4.4.3 Detailed Analysis 
To better understand the reasons why and how the 
proposed method works well, in this Section we 
analyze our method in detail.  
The Effect of Incorporating Heterogenous 
Entity Knowledge. The first advantage of our 
method is the entity mention model can 
incorporate heterogeneous entity knowledge. The 
Table 3 and 4 have shown that, by incorporating 
heterogenous entity knowledge (including the 
name knowledge, the popularity knowledge and 
the context knowledge), the entity linking 
performance can obtain a significant improvement. 
 
Figure 6. The performance vs. training mention 
size on WikiAmbi data set 
The Effect of Better Entity Knowledge 
Extraction. The second advantage of our method 
is that, by representing the entity knowledge as 
probabilistic distributions, our model has a 
statistical foundation and can better extract the 
entity knowledge using more training data through 
the entity popularity model, the entity name model 
and the entity context model. For instance, we can 
train a better entity context model P(c|e) using 
more name mentions. To find whether a better 
952
entity knowledge extraction will result in a better 
performance, Figure 6 plots the micro-accuray 
along with the size of the training data on name 
mentions for P(c|e) of each entity e.  From Figure 
6, we can see that when more training data is used, 
the performance increases. 
4.4.4 Comparision with State-of-the-Art 
Performance 
We also compared our method with the state-of-
the-art entity linking systems in the TAC 2009 
KBP track (McNamee and Dang, 2009). Figure 7 
plots the comparison with the top five 
performances in TAC 2009 KBP track. From 
Figure 7, we can see that our method can 
outperform the state-of-the-art approaches: 
compared with the best ranking system, our 
method can achieve a 4% performance 
improvement. 
 
Figure 7.  A comparison with top 5 TAC 2009 
KBP systems 
5 Related Work 
In this section, we briefly review the related work. 
To the date, most entity linking systems employed 
the context similarity based methods. The essential 
idea was to extract the discriminative features of an 
entity from its description, then link a name 
mention to the entity which has the largest context 
similarity with it. Cucerzan (2007) proposed a Bag 
of Words based method, which represents each 
target entity as a vector of terms, then the 
similarity between a name mention and an entity 
was computed using the cosine similarity measure. 
Mihalcea & Csomai (2007), Bunescu & Pasca 
(2006), Fader et al (2009) extended the BoW 
model by incorporating more entity knowledge 
such as popularity knowledge, entity category 
knowledge, etc.  Zheng et al (2010), Dredze et al 
(2010), Zhang et al (2010) and Zhou et al (2010) 
employed the learning to rank techniques which 
can further take the relations between candidate 
entities into account. Because the context 
similarity based methods can only represent the 
entity knowledge as features, the main drawback of 
it was the difficulty to incorporate heterogenous 
entity knowledge. 
Recently there were also some entity linking 
methods based on inter-dependency. These 
methods assumed that the entities in the same 
document are related to each other, thus the 
referent entity of a name mention is the entity 
which is most related to its contextual entities.  
Medelyan et al (2008) found the referent entity of 
a name mention by computing the weighted 
average of semantic relatedness between the 
candidate entity and its unambiguous contextual 
entities. Milne and Witten (2008) extended 
Medelyan et al (2008) by adopting learning-based 
techniques to balance the semantic relatedness, 
commoness and context quality. Kulkarni et al 
(2009) proposed a method which collectively 
resolves the entity linking tasks in a document as 
an optimization problem. The drawback of the 
inter-dependency based methods is that they are 
usually specially designed to the leverage of 
semantic relations, doesn?t take the other types of 
entity knowledge into consideration.  
6 Conclusions and Future Work 
This paper proposes a generative probabilistic 
model, the entity-mention model, for the entity 
linking task. The main advantage of our model is it 
can incorporate multiple types of heterogenous 
entity knowledge. Furthermore, our model has a 
statistical foundation, making the entity knowledge 
extraction approach different from most previous 
ad hoc approaches. Experimental results show that 
our method can achieve competitive performance. 
In our method, we did not take into account the 
dependence between entities in the same document. 
This aspect could be complementary to those we 
considered in this paper. For our future work, we 
can integrate such dependencies in our model. 
Acknowledgments 
The work is supported by the National Natural 
Science Foundation of China under Grants no. 
60773027, 60736044, 90920010, 61070106 and 
61003117, and the National High Technology 
Development 863 Program of China under Grants 
no. 2008AA01Z145. Moreover, we sincerely thank 
the reviewers for their valuable comments. 
953
References  
Adafre, S. F. & de Rijke, M. 2005. Discovering missing 
links in Wikipedia. In: Proceedings of the 3rd 
international workshop on Link discovery. 
Bunescu, R. & Pasca, M. 2006. Using encyclopedic 
knowledge for named entity disambiguation. In: 
Proceedings of EACL, vol. 6. 
Brown,  P., Pietra, S. D.,  Pietra, V. D., and Mercer, R.  
1993. The mathematics of statistical machine 
translation: parameter estimation. Computational 
Linguistics, 19(2), 263-31. 
Chen, S. F. & Goodman, J. 1999. An empirical study of 
smoothing techniques for language modeling.  In 
Computer Speech and Language, London; Orlando: 
Academic Press, c1986-, pp. 359-394. 
Cucerzan, S. 2007. Large-scale named entity 
disambiguation based on Wikipedia data. In:  
Proceedings of EMNLP-CoNLL, pp. 708-716. 
Dredze, M., McNamee, P., Rao, D., Gerber, A. & Finin, 
T. 2010. Entity Disambiguation for Knowledge Base 
Population. In: Proceedings of the 23rd International 
Conference on Computational Linguistics. 
Fader, A., Soderland, S., Etzioni, O. & Center, T. 2009. 
Scaling Wikipedia-based named entity 
disambiguation to arbitrary web text. In: Proceedings 
of  Wiki-AI Workshop at IJCAI, vol. 9. 
Han, X. & Zhao, J. 2009. NLPR_KBP in TAC 2009 
KBP Track: A Two-Stage Method to Entity Linking. 
In: Proceeding of Text Analysis Conference. 
Han, X. & Zhao, J. 2010. Structural semantic 
relatedness: a knowledge-based method to named 
entity disambiguation. In: Proceedings of the 48th 
Annual Meeting of the Association for 
Computational Linguistics. 
Jelinek, Frederick and Robert L. Mercer. 1980. 
Interpolated estimation of Markov source parameters 
from sparse data. In: Proceedings of the Workshop 
on Pattern Recognition in Practice. 
Kulkarni, S., Singh, A., Ramakrishnan, G. & 
Chakrabarti, S. 2009. Collective annotation of 
Wikipedia entities in web text. In: Proceedings of the 
15th ACM SIGKDD international conference on 
Knowledge discovery and data mining, pp. 457-466. 
Li, X., Morie, P. & Roth, D. 2004. Identification and 
tracing of ambiguous names: Discriminative and 
generative approaches. In: Proceedings of the 
National Conference on Artificial Intelligence, pp. 
419-424. 
McNamee, P. & Dang, H. T. 2009.  Overview of the 
TAC 2009 Knowledge Base Population Track. In: 
Proceeding of Text Analysis Conference. 
Milne, D. & Witten, I. H. 2008. Learning to link with 
Wikipedia. In: Proceedings of the 17th ACM 
conference on Conference on information and 
knowledge management. 
Milne, D., et al  2006. Mining Domain-Specific 
Thesauri from Wikipedia: A case study. In Proc. of 
IEEE/WIC/ACM WI. 
Medelyan, O., Witten, I. H. & Milne, D. 2008. Topic 
indexing with Wikipedia. In: Proceedings of the 
AAAI WikiAI workshop. 
Mihalcea, R. & Csomai, A. 2007. Wikify!: linking 
documents to encyclopedic knowledge. In: 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp. 233-242. 
Pedersen, T., Purandare, A. & Kulkarni, A. 2005. Name 
discrimination by clustering similar contexts. 
Computational Linguistics and Intelligent Text 
Processing, pp. 226-237. 
Zhang, W., Su, J., Tan, Chew Lim  & Wang, W. T. 
2010. Entity Linking Leveraging Automatically 
Generated Annotation. In: Proceedings of the 23rd 
International Conference on Computational 
Linguistics (Coling 2010). 
Zheng, Z., Li, F., Huang, M. & Zhu, X. 2010. Learning 
to Link Entities with Knowledge Base. In: The 
Proceedings of the Annual Conference of the North 
American Chapter of the ACL. 
Zhou, Y., Nie, L., Rouhani-Kalleh, O., Vasile, F. & 
Gaffney, S. 2010. Resolving Surface Forms to 
Wikipedia Topics. In: Proceedings of the 23rd 
International Conference on Computational 
Linguistics (Coling 2010),  pp. 1335-1343. 
954
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 61?67,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
A Feature-Enriched Tree Kernel for Relation Extraction 
 
          Le Sun      and      Xianpei Han 
State Key Laboratory of Computer Science 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{sunle, xianpei}@nfs.iscas.ac.cn 
  
 
Abstract 
Tree kernel is an effective technique for rela-
tion extraction. However, the traditional syn-
tactic tree representation is often too coarse or 
ambiguous to accurately capture the semantic 
relation information between two entities. In 
this paper, we propose a new tree kernel, 
called feature-enriched tree kernel (FTK), 
which can enhance the traditional tree kernel 
by: 1) refining the syntactic tree representation 
by annotating each tree node with a set of dis-
criminant features; and 2) proposing a new 
tree kernel which can better measure the syn-
tactic tree similarity by taking all features into 
consideration. Experimental results show that 
our method can achieve a 5.4% F-measure im-
provement over the traditional convolution 
tree kernel. 
1 Introduction 
Relation Extraction (RE) aims to identify a set of 
predefined relations between pairs of entities in 
text. In recent years, relation extraction has re-
ceived considerable research attention. An effec-
tive technique is the tree kernel (Zelenko et al, 
2003; Zhou et al, 2007; Zhang et al, 2006; Qian 
et al, 2008), which can exploit syntactic parse tree 
information for relation extraction. Given a pair of 
entities in a sentence, the tree kernel-based RE 
method first represents the relation information 
between them using a proper sub-tree (e.g., SPT ? 
the sub-tree enclosed by the shortest path linking 
the two involved entities). For example, the three 
syntactic tree representations in Figure 1. Then the 
similarity between two trees are computed using a 
tree kernel, e.g., the convolution tree kernel pro-
posed by Collins and Duffy (2001). Finally, new 
relation instances are extracted using kernel based 
classifiers, e.g., the SVM classifier. 
Unfortunately, one main shortcoming of the 
traditional tree kernel is that the syntactic tree rep-
resentation usually cannot accurately capture the 
 
Figure 1. The ambiguity of possessive structure 
relation information between two entities. This is 
mainly due to the following two reasons: 
1) The syntactic tree focuses on representing 
syntactic relation/structure, which is often too 
coarse or ambiguous to capture the semantic re-
lation information. In a syntactic tree, each node 
indicates a clause/phrase/word and is only labeled 
with a Treebank tag (Marcus et al, 1993). The 
Treebank tag, unfortunately, is usually too coarse 
or too general to capture semantic information. 
For example, all the three trees in Figure 1 share 
the same possessive syntactic structure, but ex-
press quite different semantic relations: where 
?Mary?s brothers? expresses PER-SOC Family 
relation, ?Mary?s toys? expresses Possession rela-
tion, and ?New York?s airports? expresses PHYS-
Located relation. 
2) Some critical information may lost during 
sub-tree representation extraction. For example, 
in Figure 2, when extracting SPT representation, 
all nodes outside the shortest-path will be pruned, 
such as the nodes [NN plants] and [POS ?s] in tree 
T1. In this pruning process, the critical infor-
mation ?word town is the possessor of the posses-
sive phrase the town?s plants? will be lost, which 
in turn will lead to the misclassification of the 
DISC relation between one and town. 
This paper proposes a new tree kernel, referred 
as feature-enriched tree kernel (FTK), which can 
effectively resolve the above problems by enhanc-
ing the traditional tree kernel in following ways: 
1) We refine the syntactic tree representa-
tion by annotating each tree node with a set of dis-
criminant features. These features are utilized to 
NP
NP NN
NN POS
Mary 's
brothers
(a) (b) (c)
NP
NP NN
NN POS
Mary 's
toys
NP
NP NN
NN POS
NY 's
airports
61
better capture the semantic relation information 
between two entities. For example, in order to dif-
ferentiate the syntactic tree representations in Fig-
ure 1, FTK will annotate them with several fea-
tures indicating ?brother is a male sibling?, ?toy 
is an artifact?, ?New York is a city?, ?airport is 
facility?, etc. 
2) Based on the refined syntactic tree repre-
sentation, we propose a new tree kernel ? feature-
enriched tree kernel, which can better measure the 
similarity between two trees by also taking all fea-
tures into consideration. 
 
Figure 2. SPT representation extraction 
We have experimented our method on the ACE 
2004 RDC corpus. Experimental results show that 
our method can achieve a 5.4% F-measure im-
provement over the traditional convolution tree 
kernel based method. 
This paper is organized as follows. Section 2 
describes the feature-enriched tree kernel. Section 
3 presents the features we used. Section 4 dis-
cusses the experiments. Section 5 briefly reviews 
the related work. Finally Section 6 concludes this 
paper. 
2 The Feature-Enriched Tree Kernel 
In this section, we describe the proposed feature-
enriched tree kernel (FTK) for relation extraction. 
2.1 Refining Syntactic Tree Representation 
As described in above, syntactic tree is often too 
coarse or too ambiguous to represent the semantic 
relation information between two entities. To re-
solve this problem, we refine the syntactic tree 
representation by annotating each tree node with 
a set of discriminant features. 
 
Figure 3. Syntactic tree enriched with features 
Specifically, for each node  in a syntactic tree 
, we represent it as a tuple: 
 
where  is its phrase label (i.e., its Treebank tag), 
and  is a feature vector which indicates the 
characteristics of node , which is represented as: 
 
where fi is a feature and is associated with a weight 
. The feature we used includes charac-
teristics of relation instance, phrase properties and 
context information (See Section 3 for details). 
For demonstration, Figure 3 shows the feature-
enriched version of tree T2 and tree T4 in Figure 
2. We can see that, although T2 and T4 share the 
same syntactic structure, the annotated features 
can still differentiate them. For example, the NP5 
node in tree T2 and the NP5 node in tree T4 are 
differentiated using their features Possessive-
Phrase and PPPhrase, which indicate that NP5 in 
T2 is a possessive phrase, meanwhile NP5 in T4 is 
a preposition phrase. 
2.2 Feature-Enriched Tree Kernel 
This section describes how to take into account 
the annotated features for a better tree similarity. 
In Collins and Duffy?s convolution tree kernel 
(CTK), the similarity between two trees T1 and T2 
is the number of their common sub-trees: 
 
Using this formula, CTK only considers whether 
two enumerated sub-trees have the identical syn-
tactic structure (the indicator  is 1 if the 
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
town
E2
DT
the
POS
's
NN
plants
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
town
E2
DT
the
Prune
NP
NP PP
CD
one
IN
E1
of
NP
NP
DT NN
the teams
E2
PP
IN
in USA
NP
NN
Prune
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
teams
E2
DT
the
(T1)
(T2)
(T3) (T4)
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
town
E2
DT
the
(T2)
PossessivePhrase, RootPath:NP-PP,
Contain_Arg2_GPE, ...
Possessor, Contain_Arg2_GPE,
RootPath:NP-PP-NP,
EndWithPOS, ...
EntType:GPE, MentionType:NOM,
RootPath:NP-PP-NP-NP, ...
WN:town, WN:district, WN:region,
WN:location, Match_Arg2_GPE ...
PPPhrase, RootPath:NP-PP,
Contain_Arg2_ORG, ...
PP_Head, RootPath:NP-PP-NP,
Contain_Arg2_ORG,...
EntType:ORG, MentionType:NOM,
RootPath:NP-PP-NP-NP, ...
WN:team, WN:social_unit,
WN:group, WN:organization,
Match_Arg2_ORG ...
Feature Vector
1
2 3
4
5
6 7 8
9 10 11
12
14
13
15
NP
NP PP
CD
one
IN
E1
of
NP
NP
NN
team
E2
DT
the
(T4) 1
2 3
4
5
6 7 8
9 10
12
14
11
13
15
62
two sub-trees  and  have the identical syntac-
tic structure and 0 otherwise). Such an assumption 
makes CTK can only capture the syntactic struc-
ture similarity between two trees, while ignoring 
other useful information. 
To resolve the above problem, the feature-en-
riched tree kernel (FTK) compute the similarity 
between two trees as the sum of the similarities 
between their common sub-trees: 
 
where  is the similarity between enumer-
ated sub-trees  and , which is computed as: 
 
where  is the same indicator function as in 
CTK; is a pair of aligned nodes between 
 and , where  and  are correspondingly in 
the same position of tree  and ;  is the 
set of all aligned node pairs;  is the 
feature vector similarity between node  and , 
computed as the dot product between their feature 
vectors  and . 
Notice that, if all nodes are not annotated with 
features,  will be equal to . In this 
perspective, we can view  as a similarity 
adjusted version of , i.e.,  only 
considers whether two nodes are equal, in contrast 
 further considers the feature similarity 
 between two nodes. 
The Computation of FTK. As the same as 
CTK, FTK can be efficiently computed as: 
 
where  is the set of nodes in tree , and 
 evaluates the sum of the similarities of 
common sub-trees rooted at node  and node , 
which is recursively computed as follows: 
1) If the production rules of  and  are differ-
ent,  = 0; 
2) If both  and  is pre-terminal nodes, 
; 
Otherwise go to step 3; 
3) Calculate  recursively as: 
?(n1;n2) = ?? (1 + sim(n1; n2))
?
#ch(n1)X
k=1
(1 + ?(ch(n1; k); ch(n2; k))
 
3 Features for Relation Extraction 
This section presents the features we used to en-
rich the syntactic tree representation. 
3.1 Instance Feature 
Relation instances of the same type often share 
some common characteristics. In this paper, we 
add the following instance features to the root 
node of a sub-tree representation: 
1) Syntactico-Semantic structure. A fea-
ture indicates whether a relation instance has the 
following four syntactico-semantic structures in 
(Chan & Roth, 2011) ? Premodifiers, Possessive, 
Preposition, Formulaic and Verbal. 
2) Entity-related information of argu-
ments. Features about the entity information of 
arguments, including: a) #TP1-#TP2: the concat 
of the major entity types of arguments; b) #ST1-
#ST2: the concat of the sub entity types of argu-
ments; c) #MT1-#MT2: the concat of the mention 
types of arguments. 
3) Base phrase chunking features. Fea-
tures about the phrase path between two argu-
ments and the phrases? head before and after the 
arguments, which are the same as the phrase 
chunking features in (Zhou, et al, 2005). 
3.2 Phrase Feature 
As discussed in above, the Treebank tag is too 
coarse to capture the property of a phrase node. 
Therefore, we enrich each phrase node with fea-
tures about its lexical pattern, its content infor-
mation, and its lexical semantics: 
1) Lexical Pattern. We capture the lexical 
pattern of a phrase node using the following fea-
tures: a) LP_Poss: A feature indicates the node is 
a possessive phrase; b) LP_PP: A feature indi-
cates the node is a preposition phrase; c) LP_CC: 
A feature indicates the node is a conjunction 
phrase; d) LP_EndWithPUNC: A feature indicates 
the node ends with a punctuation; e) LP_EndWith-
POSS: A feature indicates the node ends with a 
possessive word. 
2) Content Information. We capture the 
property of a node?s content using the following 
features: a) MB_#Num: The number of mentions 
contained in the phrase; b) MB_C_#Type: A fea-
ture indicates that the phrase contains a mention 
with major entity type #Type; c) MW_#Num: The 
number of words within the phrase. 
3) Lexical Semantics. If the node is a pre-
terminal node, we capture its lexical semantic by 
adding features indicating its WordNet sense in-
formation. Specifically, the first WordNet sense 
of the terminal word, and all this sense?s hyponym 
senses will be added as features. For example, 
WordNet senses {New York#1, city#1, district#1, 
63
region#1, ?} will be added as features to the [NN 
New York]  node in Figure 1. 
3.3 Context Information Feature 
The context information of a phrase node is criti-
cal for identifying the role and the importance of 
a sub-tree in the whole relation instance. This pa-
per captures the following context information: 
1) Contextual path from sub-tree root to 
the phrase node. As shown in Zhou et al (2007), 
the context path from root to the phrase node is an 
effective context information feature. In this paper, 
we use the same settings in (Zhou et al, 2007), i.e., 
each phrase node is enriched with its context paths 
of length 1, 2, 3. 
2) Relative position with arguments. We 
observed that a phrase?s relative position with the 
relation?s arguments is useful for identifying the 
role of the phrase node in the whole relation in-
stance. To capture the relative position infor-
mation, we define five possible relative positions 
between a phrase node and an argument, corre-
sponding match, cover, within, overlap and other. 
Using these five relative positions, we capture the 
context information using the following features: 
a) #RP_Arg1Head_#Arg1Type: a feature in-
dicates the relative position of a phrase node with 
argument 1?s head phrase, where #RP is the rela-
tive position (one of match, cover, within, overlap, 
other), and #Arg1Type is the major entity type of 
argument 1. One example feature may be 
Match_Arg1Head_LOC. 
b) #RP_Arg2Head_#Arg2Type: The relative 
position with argument 2?s head phrase; 
c) #RP_Arg1Extend_#Arg1Type: The rela-
tive position with argument 1?s extended phrase; 
d) #PR_Arg2Extend_#Arg2Type: The rela-
tive position with argument 2?s extended phrase. 
Feature weighting. Currently, we set al fea-
tures with an uniform weight , which is 
used to control the relative importance of the fea-
ture in the final tree similarity: the larger the fea-
ture weight, the more important the feature in the 
final tree similarity. 
4 Experiments 
4.1 Experimental Setting 
To assess the feature-enriched tree kernel, we 
evaluate our method on the ACE RDC 2004 cor-
pus using the same experimental settings as (Qian 
et al, 2008). That is, we parse all sentences using 
the Charniak?s parser (Charniak, 2001), relation 
instances are generated by iterating over all pairs 
of entity mentions occurring in the same sentence. 
In our experiments, we implement the feature-en-
riched tree kernel by extending the SVMlight (Joa-
chims, 1998) with the proposed tree kernel func-
tion (Moschitti, 2004). We apply the one vs. oth-
ers strategy for multiple classification using SVM. 
For SVM training, the parameter C is set to 2.4 for 
all experiments, and the tree kernel parameter ? is 
tuned to 0.2 for FTK and 0.4 (the optimal param-
eter setting used in Qian et al(2008)) for CTK. 
4.2 Experimental Results 
4.2.1 Overall performance 
We compare our method with the standard convo-
lution tree kernel (CTK) on the state-of-the-art 
context sensitive shortest path-enclosed tree rep-
resentation (CSPT, Zhou et al, 2007). We exper-
iment our method with four different feature set-
tings, correspondingly: 1) FTK with only instance 
features ? FTK(instance); 2) FTK with only 
phrase features ? FTK(phrase); 3) FTK with only 
context information features ? FTK(context); and 
4) FTK with all features ? FTK. The overall per-
formance of CTK and FTK is shown in Table 1, 
the F-measure improvements over CTK are also 
shown inside the parentheses. The detailed perfor-
mance of FTK on the 7 major relation types of 
ACE 2004 is shown in Table 2. 
 P(%) R(%) F 
CTK 77.1 61.3 68.3 (-------) 
FTK(instance) 78.5 64.6 70.9 (+2.6%) 
FTK(phrase) 78.3 64.2 70.5 (+2.2%) 
FTK(context) 80.1 67.5 73.2 (+4.9%) 
FTK 81.2 67.4 73.7 (+5.4%) 
Table 1. Overall Performance 
Relation Type P(%) R(%) F Impr 
EMP-ORG 84.7 82.4 83.5 5.8% 
PER-SOC 79.9 70.7 75.0 1.0% 
PHYS 73.3 64.4 68.6 7.0% 
ART 83.6 57.5 68.2 1.7% 
GPE-AFF 74.7 56.6 64.4 4.3% 
DISC 81.6 48.0 60.5 6.6% 
OTHER-AFF 74.2 36.8 49.2 1.0% 
Table 2. FTK on the 7 major relation types and 
their F-measure improvement over CTK 
From Table 1 and 2, we can see that: 
1) By refining the syntactic tree with discri-
minant features and incorporating these features 
into the final tree similarity, FTK can significantly 
improve the relation extraction performance: 
compared with the convolution tree kernel base-
line CTK, our method can achieve a 5.4% F-meas-
ure improvement. 
64
2) All types of features can improve the per-
formance of relation extraction: FTK can corre-
spondingly get 2.6%, 2.2% and 4.9% F-measure 
improvements using instance features, phrase fea-
tures and context information features. 
3) Within the three types of features, context 
information feature can achieve the highest F-
measure improvement. We believe this may be-
cause: ?  The context information is useful in 
providing clues for identifying the role and the im-
portance of a sub-tree; and ? The context-free as-
sumption of CTK is too strong, some critical in-
formation will lost in the CTK computation. 
4) The performance improvement of FTK 
varies significantly on different relation types: in 
Table 2, most performance improvement gains 
from the EMP-ORG, PHYS, GPE-AFF and DISC 
relation types. We believe this may because the 
discriminant features will better complement the 
syntactic tree for capturing EMP-ORG, PHYS, 
GPE-AFF and DISC relation. On contrast the fea-
tures may be redundant to the syntactic infor-
mation for other relation types. 
System P(%) R(%) F 
Qian et al, (2008): composite kernel 83.0 72.0 77.1 
Zhou et al, (2007): composite kernel 82.2 70.2 75.8 
Ours: FTK with CSPT 81.2 67.4 73.7 
Zhou et al, (2007): context sensitive 
CTK with CSPT 
81.1 66.7 73.2 
Ours: FTK with SPT 81.1 66.2 72.9 
Jiang & Zhai (2007): MaxEnt classi-
fier with features 
74.6 71.3 72.9 
Zhang et al, (2006): composite kernel  76.1 68.4 72.1 
Zhao & Grishman, (2005): Composite 
kernel 
69.2 70.5 70.4 
Zhang et al, (2006): CTK with SPT 74.1 62.4 67.7 
Table 3. Comparison of different systems on the 
ACE RDC 2004 corpus 
4.2.2 Comparison with other systems 
Finally, Table 3 compares the performance of our 
method with several other systems. From Table 3, 
we can see that FTK can achieve competitive per-
formance: ? It achieves a 0.8% F-measure im-
provement over the feature-based system of Jiang 
& Zhai (2007); ? It achieves a 0.5% F-measure 
improvement over a state-of-the-art tree kernel: 
context sensitive CTK with CSPT of Zhou et al, 
(2007); ? The F-measure of our system is slightly 
lower than the current best performance on ACE 
2004 (Qian et al, 2008) ? 73.7 vs. 77.1, we believe 
this is because the system of (Qian et al, 2008) 
adopts two extra techniques: composing tree ker-
nel with a state-of-the-art feature-based kernel and 
using a more proper sub-tree representation. We 
believe these two techniques can also be used to 
further improve the performance of our system. 
5 Related Work 
This section briefly reviews the related work. A 
classical technique for relation extraction is to 
model the task as a feature-based classification 
problem (Kambhatla, 2004; Zhou et al, 2005; 
Jiang & Zhai, 2007; Chan & Roth, 2010; Chan & 
Roth, 2011), and feature engineering is obviously 
the key for performance improvement. As an al-
ternative, tree kernel-based method implicitly de-
fines features by directly measuring the similarity 
between two structures (Bunescu and Mooney, 
2005; Bunescu and Mooney, 2006; Zelenko et al 
2003; Culotta and Sorensen, 2004; Zhang et al, 
2006). Composite kernels were also be used (Zhao 
and Grishman, 2005; Zhang et al, 2006). 
The main drawback of the current tree kernel is 
that the syntactic tree representation often cannot 
accurately capture the relation information. To re-
solve this problem, Zhou et al (2007) took the an-
cestral information of sub-trees into consideration; 
Reichartz and Korte (2010) incorporated depend-
ency type information into a tree kernel; Plank and 
Moschitti (2013) and Liu et al (2013) embedded 
semantic information into tree kernel. Bloehdorn 
and Moschitti (2007a, 2007b) proposed Syntactic 
Semantic Tree Kernels (SSTK), which can cap-
ture the semantic similarity between leaf nodes. 
Moschitti (2009) proposed a tree kernel which 
specify a kernel function over any pair of nodes 
between two trees, and it was further extended and 
applied in other tasks in (Croce et al, 2011; Croce 
et al, 2012; Mehdad et al, 2010). 
6 Conclusions and Future Work 
This paper proposes a feature-enriched tree kernel, 
which can: 1) refine the syntactic tree representa-
tion; and 2) better measure the similarity between 
two trees. For future work, we want to develop a 
feature weighting algorithm which can accurately 
measure the relevance of a feature to a relation in-
stance for better RE performance. 
Acknowledgments 
This work is supported by the National Natural 
Science Foundation of China under Grants no. 
61100152 and 61272324, and the Open Project of 
Beijing Key Laboratory of Internet Culture and 
Digital Dissemination Research under Grants no. 
ICDD201204.  
65
References 
Agichtein, E. and Gravano, L. 2000. Snowball: Ex-
tracting relations from large plain-text collections. 
In: Proceedings of the 5th ACM Conference on Dig-
ital Libraries, pp. 85?94. 
Plank, B. and Moschitti, A. 2013. Embedding Semantic 
Similarity in Tree Kernels for Domain Adaptation of 
Relation Extraction?. In: Proceedings of ACL 2013. 
Banko, M., Cafarella, M. J., Soderland, S., Broadhead, 
M. and Etzioni, O. 2007. Open information extrac-
tion from the Web. In: Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence, 
pp. 2670?2676. 
Bunescu, R. and Mooney, R. 2005. A shortest path de-
pendency kernel for relation extraction. In: Pro-
ceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods 
in Natural Language Processing, pp.724?731. 
Bloehdorn, S. and Moschitti, A. 2007a. Combined Syn-
tactic and Semantic Kernels for Text Classification. 
In: Proceedings of the 29th European Conference on 
Information Retrieval (ECIR). 
Bloehdorn, S. and Moschitti, A. 2007b. Structure and 
semantics for expressive text kernels. In: Proceeding 
of ACM 16th Conference on Information and 
Knowledge Management (CIKM). 
Bunescu, R. and Mooney. R., 2006. Subsequence ker-
nels for relation extraction. In: Advances in Neural 
Information Processing Systems 18, pp. 171?178. 
Charniak, E., 2001. Immediate-head parsing for lan-
guage models. In: Proceedings of the 39th Annual 
Meeting on Association for Computational Linguis-
tics, pp. 124-131. 
Chan, Y. S. and Roth, D. 2010. Exploiting background 
knowledge for relation extraction. In: Proceedings 
of the 23rd International Conference on Computa-
tional Linguistics, pp. 152?160. 
Chan, Y. S. and Roth, D. 2011. Exploiting syntactico-
semantic structures for relation extraction. In: Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics, pp. 551?560. 
Croce, D., Moschitti, A. and Basili, R. 2011. Struc-
tured lexical similarity via convolution kernels on 
dependency trees. In: Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language 
Processing, pp. 1034?1046. 
Croce, D., Moschitti, A., Basili, R. and Palmer, M. 
2012. Verb Classification using Distributional Sim-
ilarity in Syntactic and Semantic Structures. In: Pro-
ceedings of ACL 2012, pp. 263-272. 
Culotta, A. and Sorensen, J. 2004. Dependency tree 
kernels for relation extraction. In: Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics, pp. 423?429. 
Grishman, R. and Sundheim, B. 1996. Message under-
standing conference-6: A brief history. In: Proceed-
ings of the 16th International Conference on Com-
putational Linguistics, pp. 466?471. 
Collins, M. and Duffy, N., 2001. Convolution Kernels 
for Natural Language. In: Proceedings of NIPS 
2001. 
Liu, D., et al 2013. Incorporating lexical semantic 
similarity to tree kernel-based Chinese relation ex-
traction. In: Proceedings of Chinese Lexical Seman-
tics 2013. 
Jiang, J. and Zhai, C. 2007. A systematic exploration of 
the feature space for relation extraction. In: Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pp. 113?120. 
Joachims, T.  1998.  Text  Categorization  with  Su 
pport Vector  Machine:  learning  with  many  rele-
vant  features. ECML-1998: 137-142. 
Kambhatla, N. 2004. Combining lexical, syntactic, and 
semantic features with maximum entropy models for 
extracting relations. In: the Proceedings of 42st An-
nual Meeting of the Association for Computational 
Linguistics, pp. 178?181. 
Krause, S., Li, H., Uszkoreit, H., & Xu, F. 2012. Large-
scale learning of relation-extraction rules with dis-
tant supervision from the web. In: Proceedings of 
ISWC 2012, pp. 263-278. 
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. 
1993. Building a large annotated corpus of English: 
The Penn Treebank. Computational linguistics, 
19(2), 313-330. 
Moschitti, A. 2004. A study on Convolution Kernels for 
Shallow Semantic Parsing. In: Proceedings of the 
42-th Conference on Association for Computational 
Linguistic (ACL-2004). 
Moschitti, A. 2009. Syntactic and semantic kernels for 
short text pair categorization. In: Proceedings of the 
12th Conference of the European Chapter of the 
ACL (EACL 2009), pp. 576?584. 
Mehdad, Y., Moschitti, A. and Zanzotto, F. 2010. Syn-
tactic/Semantic Structures for Textual Entailment 
Recognition. In: Proceedings of Human Language 
Technology - North American chapter of the Asso-
ciation for Computational Linguistics. 
Mintz, M., Bills, S., Snow, R. and Jurafsky D. 2009. 
Distant supervision for relation extraction without 
labeled data. In: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association 
for Computational Linguistics and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pp. 1003?1011. 
66
Qian L., Zhou G., Kong F., Zhu Q., and Qian P., 2008. 
Exploiting constituent dependencies for tree kernel 
based semantic relation extraction. In: Proceedings 
of the 22nd International Conference on Computa-
tional Linguistics, pp. 697-704. 
Reichartz, F. and H. Korte, et al  2010. Semantic rela-
tion extraction with kernels over typed dependency 
trees. In: Proceedings of the 16th ACM SIGKDD 
international conference on Knowledge discovery 
and data mining. 
Zelenko, D., Aone, C., and Richardella, A. 2003. Ker-
nel methods for relation extraction. Journal of Ma-
chine Learning Research, 3:1083?1106. 
Zhang, M., Zhang, J., and Su, J. 2006. Exploring syn-
tactic features for relation extraction using a convo-
lution tree kernel. In: Proceedings of the Human 
Language Technology Conference and the North 
American Chapter of the Association for Computa-
tional Linguistics, pages 288?295. 
Zhang, M., Zhang, J., Su, J. and Zhou, G. 2006. A com-
posite kernel to extract relations between entities 
with both flat and structured features. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meeting 
of the Association for Computational Linguistics, 
pages 825?832. 
Zhao, S. and Grishman, R. 2005. Extracting relations 
with integrated information using kernel methods. 
In Proceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, pages 
419?426. 
Zhou, G., Su, J., Zhang, J., and Zhang, M. 2005. Ex-
ploring various knowledge in relation extraction. In 
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 427?
434. 
Zhou, G. and Zhang M. 2007. Extracting relation in-
formation from text documents by exploring various 
types of knowledge. Information Processing & Man-
agement 43(4): 969--982. 
Zhou, G., et al 2007. Tree kernel-based relation ex-
traction with context-sensitive structured parse tree 
information. In: Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, pp. 728?736. 
67
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 718?724,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Semantic Consistency: A Local Subspace Based Method for Distant 
Supervised Relation Extraction 
 
 Xianpei Han    and       Le Sun 
State Key Laboratory of Computer Science 
Institute of Software, Chinese Academy of Sciences 
HaiDian District, Beijing, China. 
{xianpei, sunle}@nfs.iscas.ac.cn 
  
Abstract 
One fundamental problem of distant supervi-
sion is the noisy training corpus problem. In 
this paper, we propose a new distant supervi-
sion method, called Semantic Consistency, 
which can identify reliable instances from 
noisy instances by inspecting whether an in-
stance is located in a semantically consistent 
region. Specifically, we propose a semantic 
consistency model, which first models the lo-
cal subspace around an instance as a sparse 
linear combination of training instances, then 
estimate the semantic consistency by exploit-
ing the characteristics of the local subspace. 
Experimental results verified the effectiveness 
of our method. 
1 Introduction 
Relation extraction aims to identify and categorize 
relations between pairs of entities in text. Due to 
the time-consuming annotation process, one criti-
cal challenge of relation extraction is the lack of 
training data. To address this limitation, a promis-
ing approach is distant supervision (DS), which 
can automatically gather labeled data by heuristi-
cally aligning entities in text with those in a 
knowledge base (Mintz et al, 2009). The under-
lying assumption of distant supervision is that 
every sentence that mentions two entities is likely 
to express their relation in a knowledge base. 
Relation Instance Label 
S1: Jobs was the founder of Apple Founder-of, CEO-of 
S2: Jobs joins Apple Founder-of, CEO-of 
Figure 1. Labeled instances by distant supervi-
sion, using relations CEO-of(Steve Jobs, Apple 
Inc.) and Founder-of(Steve Jobs, Apple Inc.) 
The distant supervision assumption, unfortu-
nately, can often fail and result in a noisy training 
corpus. For example, in Figure 1 DS assumption 
will wrongly label S1 as a CEO-of instance and S2 
as instance of Founder-of and CEO-of. The noisy 
training corpus in turn will lead to noisy extrac-
tions that hurt extraction accuracy (Riedel et al, 
2010). 
 
Figure 2. The regions the two instances in Figure 
1 located, where: 1) S1 locates in a semantically 
consistent region; and 2) S2 locates in a semanti-
cally inconsistent region 
To resolve the noisy training corpus problem, 
this paper proposes a new distant supervision 
method, called Semantic Consistency, which can 
effectively identify reliable instances from noisy 
instances by inspecting whether an instance is lo-
cated in a semantically consistent region. Figure 2 
shows two intuitive examples. We can see that, 
semantic consistency is an effective way to iden-
tify reliable instances. For example, in Figure 2 S1 
is highly likely a reliable Founder-of instance be-
cause its neighbors are highly semantically con-
sistent, i.e., most of them express the same rela-
tion type ? Founder-of. On contrast S2 is highly 
likely a noisy instance because its neighbors are 
semantically inconsistent, i.e., they have a diverse 
relation types. The problem now is how to model 
the semantic consistency around an instance. 
To model the semantic consistency, this paper 
proposes a local subspace based method. Specifi-
cally, given sufficient training instances, our 
method first models each relation type as a linear 
subspace spanned by its training instances. Then, 
the local subspace around an instance is modeled 
and characterized by seeking the sparsest linear 
combination of training instances which can re-
construct the instance. Finally, we estimate the se-
mantic consistency of an instance by exploiting 
the characteristics of its local subspace. 
+
+
+
+
+
?
??
?
?
S2
? S1 ?
+:  CEO-of
?:  Founder-of
+
?
+
?
? ?
+    :  Manager-of
   :  CTO-of
718
This paper is organized as follows. Section 2 
reviews related work. Section 3 describes the pro-
posed method. Section 4 presents the experiments. 
Finally Section 5 concludes this paper. 
2 Related Work 
This section briefly reviews the related work. Cra-
ven and Kumlien (1999), Wu et al (2007) and 
Mintz et al(2009) were several pioneer work of 
distant supervision. One main problem of DS as-
sumption is that it often will lead to false positives 
in training data. To resolve this problem, Bunescu 
and Mooney (2007), Riedel et al (2010) and Yao 
et al (2010) relaxed the DS assumption to the at-
least-one assumption and employed multi-in-
stance learning techniques to identify wrongly la-
beled instances. Takamatsu et al (2012) proposed 
a generative model to eliminate noisy instances. 
Another research issue of distant supervision is 
that a pair of entities may participate in more than 
one relation. To resolve this problem, Hoffmann 
et al (2010) proposed a method which can com-
bine a sentence-level model with a corpus-level 
model to resolve the multi-label problem. 
Surdeanu et al (2012) proposed a multi-instance 
multi-label learning approach which can jointly 
model all instances of an entity pair and all their 
labels. Several other research issues also have 
been addressed. Xu et al (2013), Min et al (2013) 
and Zhang et al (2013) try to resolve the false 
negative problem raised by the incomplete 
knowledge base problem. Hoffmann et al (2010) 
and Zhang et al (2010) try to improve the extrac-
tion precision by learning a dynamic lexicon. 
3 The Semantic Consistency Model for 
Relation Extraction 
In this section, we describe our semantic con-
sistency model for relation extraction. We first 
model the subspaces of all relation types in the 
original feature space, then model and character-
ize the local subspace around an instance, finally 
estimate the semantic consistency of an instance 
and exploit it for relation extraction. 
3.1 Testing Instance as a Linear Combina-
tion of Training Instances 
In this paper, we assume that there exist k distinct 
relation types of interest and each relation type is 
represented with an integer index from 1 to k. For 
ith relation type, we assume that totally ni training 
instances Vi = fvi;1;vi;2; :::;vi;nig have been 
collected using DS assumption. And each instance 
is represented as a weighted feature vector, such 
as the features used in (Mintz, 2009) or (Surdeanu 
et al, 2012), with each feature is TFIDF weighted 
by taking each instance as an individual document. 
To model the subspace of ith relation type in 
the original feature space, a variety of models 
have been proposed to discover the underlying 
patterns of Vi. In this paper, we make a simple and 
effective assumption that the instances of a single 
relation type can be represented as the linear 
combination of other instances of the same rela-
tion type. This assumption is well motived in rela-
tion extraction, because although there is nearly 
unlimited ways to express a specific relation, in 
many cases basic principles of economy of ex-
pression and/or conventions of genre will ensure 
that certain systematic ways will be used to ex-
press a specific relation (Wang et al, 2012). For 
example, as shown in (Hearst, 1992), the IS-A re-
lation is usually expressed using several regular 
patterns, such as ?such NP as {NP ,}* {(or | and)} 
NP? and ?NP {, NP}* {,} or other NP?. 
Based on the above assumption, we hold many 
instances for each relation type and directly use 
these instances to model the subspace of a relation 
type. Specifically, we represent an instance y of 
ith type as the linear combination of training in-
stances associated with ith type: 
y = ?i;1vi;1 + ?i;2vi;2 + ::: + +?i;nivi;ni   (1) 
for some scalars , with j = 1, 2, ?,ni. For ex-
ample, we can represent the CEO-of instance 
?Jobs was the CEO of Apple? as the following lin-
ear combination of CEO-of instances: 
? 0.8: Steve Ballmer is the CEO of Microsoft 
? 0.2: Rometty was served as the CEO of IBM 
For simplicity, we arrange the given ni training in-
stances of ith relation type as columns of a matrix 
Ai = [vi;1;vi;2; :::;vi;ni], then we can write the 
matrix form of Formula 1 as: 
y = Aixi                           (2) 
where xi = [?i;1; :::; ?i;ni] is the coefficient vec-
tor. In this way, the subspace of a relation type is 
the linear subspace spanned by its training in-
stances, and if we can find a valid xi, we can ex-
plain y as a valid instance of ith relation type. 
3.2 Local Subspace Modeling 
via Sparse Representation 
Based on the above model, the local subspace of 
an instance is modeled as the linear combination 
of training instances which can reconstruct the in-
stance. Specifically, to model the local subspace, 
we first concatenate the n training instances of all 
k relation types: 
A = [A1;A2; :::; Ak] 
719
Then the local subspace around y is modeled by 
seeking the solution of the following formula: 
y = Ax                           (3) 
However, because of the redundancy of train-
ing instances, Formula 3 usually has more than 
one solution. In this paper, following the idea in 
(Wright et al, 2009) for robust face recognition, 
we use the sparsest solution (i.e., how to recon-
struct an instance using minimal training in-
stances), which have been shown is both discrimi-
nant and robust to noisiness. Concretely, we seek 
the sparse linear combination of training instances 
to reconstruct y by solving: 
(l1) : x? = arg min kxk1 s.t. kAx?yk2 ? "  (4) 
where x= [?1;1; :::;?1;n1; :::;?i;1;?i;2; :::;?i;ni; :::] 
is a coefficient vector which identifies the span-
ning instances of y?s local subspace, i.e., the in-
stances whose ??,? ? 0 . In practice, the training 
corpus may be too large to direct solve Formula 4. 
Therefore, this paper uses the K-Nearest-Neigh-
bors (KNN) of y (1000 nearest neighbors in this 
paper) to construct the training instance matrix A 
for each y, and KNN can be searched very effi-
ciently using specialized algorithms such as the 
LSH functions in (Andoni & Indyk, 2006). 
Through the above semantic decomposition, 
we can see that, the entries of x can encode the 
underlying semantic information of instance y. 
For ith relation type, let  be a new vector 
whose only nonzero entries are the entries in x that 
are associated with ith relation type, then we can 
compute the semantic component corresponding 
to ith relation type as . In this way a 
testing instance y will be decomposed into k se-
mantic components, with each component corre-
sponds to one relation type (with an additional 
noise component ): 
y= y1 + :::+yi + :::+yk + ?        (5) 
S1 = 0:8?
2
6
4
was
co-founder
of
...
3
7
5+ 0:2?
2
6
4
Jobs
Apple
the
...
3
7
5
 
 
S2 = 0:1
?join
...
?
+ 0:1
?join
...
?
+ 0:1
?join
...
?
+ ...
 
Figure 3. The semantic decomposition of the two 
instances in Figure 1 
Figure 3 shows an example of semantic decom-
position. We can see that, the semantic decompo-
sition can effectively summarize the semantic 
consistency information of y?s local subspace: if 
the instances around an instance have diverse re-
lation types (S2 for example), its information will 
be scattered on many different semantic compo-
nents. On contrast if the instances around an in-
stance have consistent relation types (S1 for ex-
ample), most of its information will concentrate 
on the corresponding relation type. 
3.3 Semantic Consistency based 
Relation Extraction 
This section describes how to estimate and exploit 
the semantic consistency for relation extraction. 
Specifically, given y?s semantic decomposition: 
y= y1 + :::+yi + :::+yk + ? 
we observe that if instance y locates at a semantic 
consistent region, then all its information will con-
centrate on a specific component yi, with all other 
components equal to zero vector 0. However, 
modeling errors, expression ambiguity and noisy 
features will lead to small nonzero components. 
Based on the above discussion, we define the se-
mantic consistency of an instance as the semantic 
concentration degree of its decomposition: 
Definition 1(Semantic Consistency). For an in-
stance y, its semantic consistency with ith relation 
type is: 
Consistency(y; i) = kyik2P
i kyik2 + k?k2
 
where Consistency(y, i)  and will be 1.0 if 
all information of y is consistent with ith relation 
type; on contrast it will be 0 if no information in y 
is consistent with ith relation type. 
Semantic Consistency based Relation Ex-
traction. To get accurate extractions, we deter-
mine the relation type of y based on both: 1) How 
much information in y is related to ith type; and 2) 
its semantic consistency score with ith type, i.e., 
whether y is a reliable instance of ith type. 
To measure how much information in y is re-
lated to ith relation type, we compute the propor-
tio  of common information between y and yi: 
sim(y;yi) = y ? yiy ? y
                    (6) 
Then the likelihood for a testing instance y ex-
pressing ith relation type is scored by summariz-
ing both its information and semantic consistency: 
rel(y; i) = sim(y;yi)?Consistency(y; i) 
and y will be classified into ith relation type if its 
likelihood is larger than a threshold: 
rel(y; i) ? ?i                       (7) 
where  is a relation type specific threshold 
learned from training dataset. 
Founder-of CEO-of 
Founder-of noise 
CTO-of 
720
Multi-Instance Evidence Combination. It is 
often that an entity pair will match more than one 
sentence. To exploit such redundancy for more 
confident extraction, this paper first combines the 
evidence from different instances by combing 
their underlying components. That is, given the 
matched m instances Y={y1, y2, ?, ym} for an en-
tity pair (e1, e2), we first decompose each instance 
as yj = yj1 + ::: + yjk + ?, then the entity-pair 
level decomposition y = y1 + :::+yk + ? is ob-
tained by summarizing semantic components of 
different instances: yi =P1?j?myji. Finally, the 
likelihood of an entity pair expressing ith relation 
type is scored as: 
rel(Y; i) = sim(y;yi)Consistency(y; i)log(m+1) 
where  is a score used to encourage 
extractions with more matching instances. 
3.4 One further Issue for Distant Supervi-
sion: Training Instance Selection 
The above model further provides new insights 
into one issue for distant supervision: training in-
stance selection. In this paper, we select informa-
tive training instances by seeking a most compact 
subset of instances which can span the whole sub-
space of a relation type. That is, all instances of 
ith type can be represented as a linear combination 
of these selected instances. 
However, finding the optimal subset of training 
instances is difficult, as there exist 2N possible so-
lutions for a relation type with N training instances. 
Therefore, this paper proposes an approximate 
training instance selection algorithm as follows: 
1) Computing the centroid of ith relation type as                
vi = P1?j?ni vi;j 
2) Finding the set of training instances which 
can most compactly span the centroid by 
solving: 
(l1) : xi = arg min kxk1 s.t. kAix? vik2 ? " 
3) Ranking all training instances according to 
their absolute coefficient weight value ; 
4) Selecting top p percent ranked instances as 
final training instances. 
The above training instance selection has two 
benefits. First, it will select informative instances 
and remove redundant instances: an informative 
instance will receive a high  value because 
many other instances can be represented using it; 
and if two instances are redundant, the sparse so-
lution will only retain one of them. Second, most 
of the wrongly labeled training instances will be 
filtered, because these instances are usually not 
regular expressions of ith type, so they appear 
only a few times and will receive a small . 
4 Experiments 
In this section, we assess the performance of our 
method and compare it with other methods. 
Dataset. We assess our method using the KBP 
dataset developed by Surdeanu et al (2012). The 
KBP is constructed by aligning the relations from 
a subset of English Wikipedia infoboxes against a 
document collection that merges two distinct 
sources: (1) a 1.5 million documents collection 
provided by the KBP shared task(Ji et al, 2010; Ji 
et al, 2011); and (2) a complete snapshot of the 
June 2010 version of Wikipedia. Totally 183,062 
training relations and 3,334 testing relations are 
collected. For tuning and testing, we used the 
same partition as Surdeanu et al (2012): 40 que-
ries for development and 160 queries for formal 
evaluation. In this paper, each instance in KBP is 
represented as a feature vector using the features 
as the same as in (Surdeanu et al, 2012). 
Baselines. We compare our method with four 
baselines as follows: 
? Mintz++. This is a traditional DS assump-
tion based model proposed by Mintz et al(2009).  
? Hoffmann. This is an at-least-one as-
sumption based multi-instance learning method 
proposed by Hoffmann et al (2011). 
? MIML. This is a multi-instance multi-la-
bel model proposed by Surdeanu et al (2012).  
? KNN. This is a classical K-Nearest-
Neighbor classifier baseline. Specifically, given 
an entity pair, we first classify each matching in-
stance using the labels of its 5 (tuned on training 
corpus) nearest neighbors with cosine similarity, 
then all matching instances? classification results 
are added together. 
Evaluation. We use the same evaluation set-
tings as Surdeanu et al (2012). That is, we use the 
official KBP scorer with two changes: (a) relation 
mentions are evaluated regardless of their support 
document; and (b) we score only on the subset of 
gold relations that have at least one mention in 
matched sentences. For evaluation, we use 
Mintz++, Hoffmann, and MIML implementation 
from Stanford?s MIMLRE package (Surdeanu et 
al., 2012) and implement KNN by ourselves. 
4.1 Experimental Results 
4.1.1 Overall Results 
We conduct experiments using all baselines and 
our semantic consistency based method. For our 
721
method, we use top 10% weighted training in-
stances. All features occur less than 5 times are 
filtered. All l1-minimization problems in this pa-
per are solved using the augmented Lagrange 
multiplier algorithm (Yang et al, 2010), which 
has been proven is accurate, efficient, and robust. 
To select the classification threshold  for ith re-
lation type, we use the value which can achieve 
the best F-measure on training dataset (with an ad-
ditional restriction that precision should > 10%). 
 
Figure 4. Precision/recall curves in KBP dataset 
System Precision Recall F1 
Mintz++ 0.260 0.250 0.255 
Hoffmann 0.306 0.198 0.241 
MIML 0.249 0.314 0.278 
KNN 0.261 0.295 0.277 
Our method 0.286 0.342 0.311 
Table 1.  The best F1-measures in KBP dataset 
Figure 4 shows the precision/recall curves of 
different systems, and Table 1 shows their best 
F1-measures. From these results, we can see that: 
1) The semantic consistency based method 
can achieve robust and competitive performance: 
in KBP dataset, our method correspondingly 
achieves 5.6%, 7%, 3.3% and 3.4% F1 improve-
ments over the Mintz++, Hoffmann, MIML and 
KNN baselines. We believe this verifies that the 
semantic consistency around an instance is an ef-
fective way to identify reliable instances. 
2) From Figure 4 we can see that our method 
achieves a consistent improvement on the high-re-
call region of the KBP curves (when recall > 0.1). 
We believe this is because by modeling the se-
mantic consistency using the local subspace 
around each testing instance, our method can bet-
ter solve the classification of long tail instances 
which are not expressed using salient patterns. 
3) The local subspace around an instance 
can be effectively modeled as a linear subspace 
spanned by training instances. From Table 1 we 
can see that both our method and KNN baseline 
(where the local subspace is spanned using its k 
nearest neighbors) achieve competitive perfor-
mance: even the simple KNN baseline can achieve 
a competitive performance (0.277 in F1). This re-
sult shows: a) the effectiveness of instance-based 
subspace modeling; and b) by partitioning sub-
space into many local subspaces, the subspace 
model is more adaptive and robust to model prior. 
4) The sparse representation is an effective 
way to model the local subspace using training in-
stances. Compared with KNN baseline, our 
method can achieve a 3.4% F1 improvement. We 
believe this is because: (1) the discriminative na-
ture of sparse representation as shown in (Wright 
et al, 2009); and (2) the sparse representation 
globally seeks the combination of training in-
stances to characterize the local subspace, on con-
trast KNN uses only its nearest neighbor in the 
training data, which is more easily affected by 
noisy training instances(e.g., false positives). 
4.1.2 Training Instance Selection Results 
To demonstrate the effect of training instance se-
lection, Table 2 reports our method?s performance 
using different proportions of training instances. 
Proportion 5% 10% 20% 100% 
Best F1 0.282 0.311 0.305 0.280 
Table 2. The best F1-measures using different 
proportions of top weighted training instances 
From Table 2, we can see that: ? Our training in-
stance selection algorithm is effective: our method 
can achieve performance improvement using only 
top weighted instances. ? The training instances 
are highly redundant: using only 10% weighted 
instances can achieve a competitive performance. 
5 Conclusion and Future Work 
This paper proposes a semantic consistency 
method, which can identify reliable instances 
from noisy instances for distant supervised rela-
tion extraction. For future work, we want to de-
sign a more effective instance selection algorithm 
and embed it into our extraction framework. 
Acknowledgments 
This work is supported by the National Natural 
Science Foundation of China under Grants no. 
61100152 and 61272324, and the National High 
Technology Development 863 Program of China 
under Grants no. 2013AA01A603. 
722
Reference 
Andoni, Alexandr, and Piotr Indyk . 2006. Near-opti-
mal hashing algorithms for approximate nearest 
neighbor in high dimensions. In: Foundations of 
Computer Science, 2006,  pp. 459-468. 
Bunescu, Razvan, and Raymond Mooney. 2007. 
Learning to extract relations from the web using 
minimal supervision. In: ACL 2007, pp. 576. 
Craven, Mark, and Johan Kumlien. 1999. Constructing 
biological knowledge bases by extracting infor-
mation from text sources. In : Proceedings of AAAI 
1999. 
Downey, Doug, Oren Etzioni, and Stephen Soderland. 
2005. A probabilistic model of redundancy in infor-
mation extraction, In: Proceeding of IJCAI 2005. 
Gupta, Rahul, and Sunita Sarawagi. 2011. Joint train-
ing for open-domain extraction on the web: exploit-
ing overlap when supervision is limited. In: Pro-
ceedings of WSDM 2011, pp. 217-226. 
Hearst, Marti A. 1992. Automatic acquisition of hypo-
nyms from large text corpora. In: Proceedings of 
COLING 1992, pp. 539-545. 
Hoffmann, Raphael, Congle Zhang, and Daniel S. 
Weld. 2010. Learning 5000 relational extractors. In: 
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, 2010, pp. 
286-295. 
Hoffmann, Raphael, Congle Zhang, Xiao Ling, Luke 
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-
based weak supervision for information extraction 
of overlapping relations. In: Proceedings of ACL 
2011, pp. 541-550. 
Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2010. Overview of the TAC 2010 
knowledge base population track. In: Proceedings 
of the Text Analytics Conference. 
Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-
fitt, and Joe Ellis. 2011. Overview of the TAC 2011 
knowledge base population track.  In Proceedings of 
the Text Analytics Conference. 
Krause, Sebastian, Hong Li, Hans Uszkoreit, and Feiyu 
Xu. 2012. Large-Scale learning of relation-extrac-
tion rules with distant supervision from the web. In: 
ISWC 2012, pp. 263-278. 
Mintz, Mike, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In: Proceedings ACL- 
AFNLP 2009, pp. 1003-1011. 
Min, Bonan, Ralph Grishman, Li Wan, Chang Wang, 
and David Gondek. 2013. Distant Supervision for 
Relation Extraction with an Incomplete Knowledge 
Base. In: Proceedings of NAACL-HLT 2013,pp. 
777-782. 
Min, Bonan, Xiang Li, Ralph Grishman, and Ang Sun. 
2012. New york university 2012 system for kbp slot 
filling. In: Proceedings of TAC 2012. 
Nguyen, Truc-Vien T., and Alessandro Moschitti. 
2011. Joint distant and direct supervision for rela-
tion extraction. In: Proceedings of IJCNLP 2011, pp. 
732-740. 
Riedel, Sebastian, Limin Yao, and Andrew McCallum. 
2010. Modeling relations and their mentions with-
out labeled text. In: Machine Learning and 
Knowledge Discovery in Databases, 2010, pp. 148-
163. 
Riedel, Sebastian, Limin Yao, Andrew McCallum, and 
Benjamin M. Marlin. 2013. Relation Extraction 
with Matrix Factorization and Universal Schemas. 
In: Proceedings of NAACL-HLT 2013, pp. 74-84. 
Roth, Benjamin, and Dietrich Klakow. 2013. Combin-
ing Generative and Discriminative Model Scores for 
Distant Supervision. In: Proceedings of ACL 2013, 
pp. 24-29. 
Surdeanu, Mihai, Julie Tibshirani, Ramesh Nallapati, 
and Christopher D. Manning. 2012. Multi-instance 
multi-label learning for relation extraction. In: Pro-
ceedings of EMNLP-CoNLL 2012, pp. 455-465. 
Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. 
2012. Reducing wrong labels in distant supervision 
for relation extraction. In: ACL 2012,pp. 721-729. 
Wang, Chang, Aditya Kalyanpur, James Fan, Branimir 
K. Boguraev, and D. C. Gondek. 2012. Relation ex-
traction and scoring in DeepQA. In: IBM Journal of 
Research and Development, 56(3.4), pp. 9-1. 
Wang, Chang, James Fan, Aditya Kalyanpur, and Da-
vid Gondek. 2011. Relation extraction with relation 
topics. In: Proceedings of EMNLP 2011, pp. 1426-
1436. 
Wright, John, Allen Y. Yang, Arvind Ganesh, Shankar 
S. Sastry, and Yi Ma. 2009. Robust face recognition 
via sparse representation. In: Pattern Analysis and 
Machine Intelligence, IEEE Transactions on, 31(2), 
210-227 
Wu, Fei, and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In: Proceedings of CIKM 
2007,pp. 41-50. 
Xu, Wei, Raphael Hoffmann Le Zhao, and Ralph 
Grishman. 2013. Filling Knowledge Base Gaps for 
Distant Supervision of Relation Extraction. In: Pro-
ceedings of Proceedings of 2013, pp. 665-670. 
Yang, Allen Y., Shankar S. Sastry, Arvind Ganesh, and 
Yi Ma. 2010. Fast l1-Minimization Algorithms and 
An Application in Robust Face Recognition: A Re-
view. In: Proceedings of ICIP 2010. 
Yao, Limin, Sebastian Riedel, and Andrew McCallum. 
2010. Collective cross-document relation extraction 
723
without labelled data. In: Proceedings of EMNLP 
2010, pp. 1013-1023. 
Zhang, Congle, Raphael Hoffmann, and Daniel S. 
Weld. 2012. Ontological smoothing for relation ex-
traction with minimal supervision. In: Proceedings 
of AAAI 2012, pp. 157-163. 
Zhang, Xingxing, Zhang, Jianwen, Zeng, Junyu, Yan, 
Jun, Chen, Zheng and Sui, Zhifang. 2013. Towards 
Accurate Distant Supervision for Relational Facts 
Extraction. In: Proceedings of ACL 2013, pp. 810-
815. 
724
