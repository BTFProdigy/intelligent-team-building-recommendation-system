A Compact Architecture for Dialogue Management Based on 
Scripts ond Meta-Outputs 
MAnny Rayner ,  Beth  Ann Hockey ,  F rAnk ie  J~mes  
Research Inst i tute for Advanced Computer  Science 
Mail  Stop 19-39, NASA Ames Research Center  
Moffett  Field, CA  94035-1000 
{ mauny, bahockey, l~ ames} ~r iacs.edu 
Abst rac t  
We describe an architecture for spoken dialogue 
interfaces to semi-autonomous systems that trans- 
forms speech signals through successive r presenta- 
tions of linguistic, dialogue, and domain knowledge. 
Each step produces an output, and a meta-output 
describing the transformation, with an executable 
program in a simple scripting language as the fi- 
nal result. The output/meta-output distinction per- 
mits perspicuous treatment of diverse tasks such as 
resolving pronouns, correcting user misconceptions, 
and optimizing scripts. 
1 In t roduct ion  
The basic task we consider in this paper is that of 
using spoken language to give commands to a semi- 
autonomous robot or other similar system. As ev- 
idence of the importance of this task in the NLP 
community note that the early, influential system 
SHRDLU (Winograd, 1973) was intended to address 
just this type of problem. More recent work on spo- 
ken language interfaces to semi-antonomous robots 
include SRrs Flakey robot (Konolige et al, 1993) 
and NCARArs InterBOT project (Perzanowski et 
al., 1998; Perzanowski et al, 1999). A number of 
other systems have addressed part of the task. Com- 
mandTalk (Moore et al, 1997), Circuit Fix-It Shop 
(Smith, 1997) and TRAINS-96 (Traum and Allen, 
1994; Tranm and Andersen, 1999) are spoken lan- 
guage systems but they interface to simulation or 
help facilities rather than semi-autonomous agents. 
Jack's MOOse Lodge (Badler et al, 1999) takes text 
rather than speech as natural language input and the 
avatars being controlled are not semi-autonomous. 
Other researchers have considered particular aspects 
of the problem such as accounting for various aspects 
of actions (Webber, 1995; Pyre et al, 1995). In most 
of this and other related work the treatment is some 
variant of the following. If there is a speech inter- 
face, the input speech signal is converted into text. 
Text either from the recognizer or directly input by 
the user is then converted into some kind of logi- 
cal formula, which abstractly represents the user's 
intended command; this formula is then fed into a 
command interpreter, which executes the command. 
We do not think the standard treatment outlined 
above is in essence incorrect, but we do believe that, 
as it stands, it is in need of some modification. This 
paper will in particular make three points. First, we 
suggest that the output representation should not be 
regarded as a logical expression, but rather as a pro- 
gram in some kind of scripting language. Second, we 
argue that it is not merely the case that the process 
of converting the input signal to the final represen- 
tation can sometimes go wrong; rather, this is the 
normal course of events, and the interpretation pro- 
cess should be organized with that assumption in 
mind. Third, we claim, perhaps urprisingly, that 
the first and second points are related. These claims 
are elaborated in Section 2. 
The remainder of the paper describes an archi- 
tecture which addresses the issues outlined above, 
and which has been used to implement a prototype 
speech interface to a simulated semi-autonomous 
robot intended for deployment on the International 
Space Station. Sections 3 and 4 present an overview 
of the implemented interface, focussing on represen- 
tational issues relevant to dialogue management. Il-
lustrative xamples of interactions with the system 
are provided in Section 5. Section 6 concludes. 
2 Theoret i ca l  Ideas  
2.1 Scripts vs Logical Forms 
Let's first look in a little more detail at the question 
of what the output representation f a spoken lan- 
guage interface to a semi-autonomous robot/agent 
should be. In practice, there seem to be two main 
choices: atheoreticai representations, or some kind 
of logic. 
Logic is indeed an excellent way to think 
about representing static relationships like database 
queries, but it is much less clear that it is a good way 
to represent commands. In real life, when people 
wish to give a command to a computer, they usu- 
ally do so via its operating system; a complex com- 
mand is an expression in a scripting language like 
CSHELL, Perl, or VBScript. These languages are 
related to logical formalisms, but cannot be mapped 
112 
onto them in a simple way. Here are some of the 
obvious differences: 
? A scripting language is essentially imperative, 
rather than relational. 
? The notion of temporal sequence is fundamental 
to the language. "Do P and then Q" is not the 
same as "Make the goals P and Q true"; it is 
explicitly stated that P is to be done first. Simi- 
larly, "For each X in the list (A B C), do P(X)" 
is not the same as "For all X, make P(X) true"; 
once again, the scripting language defines an or: 
der, but not the logical anguage 1. 
? Scripting languages assume that commands do 
not always succeed. For example, UNIX-based 
scripting languages like CSHELL provide each 
script with the three predefined streams tdin, 
stdout and sl;derr. Input is read from std in 
and written to sCdout; error messages, warn- 
ings and other comments are sent to stderr .  
We do not think that these properties of scripting 
language are accidental. They have evolved as the 
result of strong selectional pressure from real users 
with real-world tasks that need to be carried out, 
and represent a competitive way to meet said users' 
needs. We consequently hink it is worth taking seri- 
ously the idea that a target representation produced 
by a spoken language interface should share many of 
these properties. 
2.2 Fall|ble Interpretat ion:  Outputs  and 
Meta-outputs  
We now move on to the question of modelling the in- 
terpretation process, that is to say the process that 
converts the input (speech) signal to the output (ex- 
ecutable) representation. As already indicated, we 
think it is important to realize that interpretation 
is a process which, like any other process, may suc- 
ceed more or less well in achieving its intended goals. 
Users may express themselves unclearly or incom- 
pletely, or the system may more or less seriously 
fail to understand exactly what they mean. A good 
interpretation architecture will keep these consider- 
ations in mind. 
Taking our lead from the description of scripting 
languages sketched above, we adapt the notion of 
the "error stream" to the interpretation process. In 
the course of interpreting an utterance, the system 
translates it into successively "deeper" levels of rep- 
resentation. Each translation step has not only an 
input (the representation consumed) and an output 
1In cases like these, the theorem prover or logic program- 
ruing interpreter used to evaluate he logical formula typically 
assigns a conventional order to the conjuncts; note however 
that this is part of the procedural semantics ofthe theorem 
prover/interpreter, and does not follow from the declarative 
semantics of the logical formalism. 
(the representation produced), but also something 
we will refer to as a "meta-output": his provides in- 
formation about how the translation was performed. 
At a high level of abstraction, our architecture will 
be as follows. Interpretation proceeds as a series 
of non-deterministic translation steps, each produc- 
ing a set of possible outputs and associated meta- 
outputs. The final translation step produces an ex- 
ecutable script. The interface attempts to simulate 
execution of each possible script produced, in or- 
der to determine what would happen if that script 
were selected; simulated execution can itself produce 
further meta-outputs. Finally, the system uses the 
meta-output information to decide what to do with 
the various possible interpretations it has produced. 
Possible actions include selection and execution of 
an output script, paraphrasing meta-output infor- 
mation back to the user, or some combination ofthe 
two. 
In the following section, we present a more de- 
tailed description showing how the output/meta- 
output distinction works in a practical system. 
3 A P ro to type  Imp lementat ion  
The ideas sketched out above have been realized as 
a prototype spoken language dialogue interface to a 
simulated version of the Personal Satellite Assistant 
(PSA; (PSA, 2000)). This section gives an overview 
of the implementation; i  the following section, we 
focus on the specific aspects of dialogue management 
which are facilitated by the output/meta-output ar- 
chitecture. 
3.1 Leve ls  o f  Representat ion  
The real PSA is a miniature robot currently being 
developed at NASA Ames Research Center, which 
is intended for deployment on the Space Shuttle 
and/or International Space Station. It will be ca- 
pable of free navigation in an indoor micro-gravity 
environment, and will provide mobile sensory capac- 
ity as a backup to a network of fixed sensors. The 
PSA will primarily be controlled by voice commands 
through a hand-held or head-mounted microphone, 
with speech and language processing being handled 
by an offboard processor. Since the speech process- 
ing units are not in fact physically connected to the 
PSA we envisage that they could also be used to con- 
trol or monitor other environmental functions. In 
particular, our simulation allows voice access to the 
current and past values of the fixed sensor eadings. 
The initial PSA speech interface demo consists of 
a simple simulation of the Shuttle. State parame- 
ters include the PSA's current position, some envi- 
ronmental variables uch as local temperature, pres- 
sure and carbon dioxide levels, and the status of the 
Shuttle's doors (open/closed). A visual display gives 
direct feedback on some of these parameters. 
113 
The speech and language processing architecture 
is based on that of the SRI CommandTalk sys- 
tem (Moore et al, 1997; Stent et al, 1999). The sys- 
tem comprises a suite of about 20 agents, connected 
together using the SPd Open Agent Architecture 
(OAA; (Martin et al, 1998)). Speech recognition 
is performed using a version of the Nuance recog- 
nizer (Nuance, 2000). Initial language processing is
carried out using the SRI Gemini system (Dowding 
et al, 1993), using a domain~independent unification 
grammar and a domain-specific lexicon. The lan- 
guage processing rammar is compiled into a recog- 
nition grarnm~kr using the methods of (Moore et al, 
1997); the net result is that only grammatically well- 
formed utterances can be recognized. Output from 
the initial language-processing step is represented 
in a version of Quasi Logical Form (van Eijck and 
Moore, 1992), and passed in that form to the dia- 
logue manager. We refer to these as linguistic level 
representations. 
The aspects of the system which are of primary in- 
terest here concern the dialogue manager (DM) and 
related modules. Once a linguistic level represen- 
tation has been produced, the following processing 
steps occur: 
? The linguistic level representation is converted 
into a discourse level representation. This pri- 
marily involves regularizing differences in sur- 
face form: so, for example, "measure the pres- 
sure" and '~hat is the pressure?" have differ- 
ent representations at the linguistic level, but 
the same representation at the discourse level. 
? If necessary, the system attempts to resolve in- 
stances of ellipsis and anaph*oric reference. For 
example, if the previous command was "mea- 
sure temperature at flight deck", then the new 
command "lower deck" will be resolved to an 
expression meaning "measure temperature at 
lower deck". Similarly, if the previous command 
was "move to the crew hatch", then the com- 
mand "open it" will be resolved to "open the 
crew hatch". We call the output of this step a 
resolved iscourse level representation. 
? The resolved discourse level representation is 
converted into an executable script in a lan- 
guage essentially equivalent o a subset of 
CSHELL. This involves two sub-steps. First, 
quantified variables are given scope: for exam- 
ple, "go to the flight deck and lower deck and 
measure pressure" becomes omething approxi- 
mately equivalent to the script 
foreach x ( f l ight_deck lower_deck) 
go_to $x 
measure  pressure 
end 
The point to note here is that the foreach has 
scope over both the go_to and the meeusmre ac- 
tions; an alternate (incorrect) scoping would be 
fo reachx  ( f l ight_deck lower_deck) 
go_to $x 
end 
measure  pressure 
The second sub-step is to attempt o optimize 
the plan. In the current example, this can 
be done by reordering the list ( f l ight .deck 
louer_deck). For instance, if the PSA is al- 
ready at the lower deck, reversing the list will 
mean that the robot only makes one trip, in- 
stead of two. 
The final step in the interpretation process is 
plan evaluation: the system tries to work out 
what will happen if it actually executes the 
plan. (The relationship between plan evaluation 
and plan execution is described in more detail 
in Section 4.1). Among other things, this gives 
the dialogue manager the possibility of compar- 
ing different interpretations of the original com- 
mand, and picking the one which is most effi- 
cient. 
3.2 How Meta-outputs Participate in the 
Tr---qlation , 
The above sketch shows how context-dependent 
interpretation is arranged as a series of non- 
deterministic translation steps; in each case, we have 
described the input and the output for the step in 
question. We now go back to the concerns of Sec- 
tion 2. First, note that each translation step is in 
general fallible. We give several examples: 
One of the most obvious cases arises when the 
user simply issues an invalid command, such as 
requesting the PSA to open a door D which is 
already open. Here, one of the meta-outputs 
issued by the plan evaluation step will be the 
term 
presupposition_failure(already_open(D)); 
the DM can decide to paraphrase this back to 
the user as a surface string of the form "D is 
already open". Note that plan evaluation does 
not involve actually executing the final script, 
which can be important. For instance, if the 
command is "go to the crew hatch and open it" 
and the crew hatch is already open, the interface 
has the option of informing the user that there 
is a problem without first carrying out the "go 
to" action. 
The resolution step can give rise to similar kinds 
of metaooutput. For example, a command may 
114 
include a referring expression that has no deno- 
tation, or an ambiguous denotation; for exam- 
ple, the user might say "both decks", presum- 
ably being unaware that there are in fact three 
of them. This time, the meta-output produced 
is 
presupposition_failure ( 
incorrect_size_of_set (2,3)) 
representing the user's incorrect belief about 
the number of decks. The DM then has the pos- 
sibility of informingthe user of this misconcelfi 
tion by realizing the meta-output term as the 
surface string "in fact there are three of them". 
Ambiguous denotation occurs when a descrip- 
tion is under-specified. For instance, the user 
might say "the deck" in a situation where there 
is no clearly salient deck, either in the discourse 
situation or in the simulated world: here, the 
meta-output will be 
presupposition_failure ( 
underspecif ied_def inite (deck)) 
which can be realized as the clarification ques- 
tion "which deck do you mean?" 
? A slightly more complex case involves plan 
costs. During plan evaluation, the system simu- 
lates execution of the output script while keep- 
ing track of execution cost. (Currently, the cost 
is just an estimate of the time required to exe- 
cute the script). Execution costs are treated as 
meta-outputs of the form 
cost (C) 
and passed back through the interpreter so that 
the plan optimization step can make use of 
them. 
? Finally, we consider what happens when the 
system receives incorrect input from the speech 
recognizer. Although the recognizer's language 
model is constrained so that it can only pro- 
duce grammatical utterances, it can still misrec- 
ognize one grammatical string as another one. 
Many of these cases fall into one of a small 
number of syntactic patterns, which function as 
fairly reliable indicators of bad recognition. A 
typical example is conjunction involving a pro- 
noun: if the system hears "it and flight deck", 
this is most likely a misrecognition fsomething 
like "go to flight deck". 
During the processing phase which translates 
linguistic level representations into discourse 
level representations, the system attempts to 
match each misrecognition pattern against he 
input linguistic form, and if successful produces 
a meta-output of the form 
presupposi t ion_fa i lure ( 
dubious_If (<Type>)) 
These meta-outputs are passed down to the 
DM, which in the absence of sufficiently com- 
pelling contrary evidence will normally issue a 
response of the form "I'm sorry, I think I mis- 
heard you". 
4 A Compact  Arch i tec ture  for  
D ia logue  Management  Based  on  
Scr ip ts  and  Meta -Outputs  
None of the individual functionalities outlined above 
are particularly novel in themselves. What we find 
new and interesting is the fact that they can all 
be expressed in a uniform way in terms of the 
script output/meta-output architecture. This sec- 
tion presents three examples illustrating how the ar- 
chitecture can be used to simplify the overall orga- 
nization of the system. 
4.1 Integration of plan evaluation, plan 
execution and dialogue management 
Recall that the DM simulates evaluation of the plan 
before running it, in order to obtain relevant meta- 
information. At plan execution time, plan actions 
result in changes to the world; at plan evaluation 
time, they result in simulated changes to the world 
and/or produce meta-outputs. 
Conceptualizing plans as scripts rather than log- 
icai formulas permits an elegant reatment of the 
execution/evaluation dichotomy. There is one script 
interpreter, which functions both as a script exec- 
utive and a script evaluator, and one set of rules 
which defines the procedural semantics of script ac- 
tions. Rules are parameterized by execution type 
which is either "execute" or "evaluate". In "evalu- 
ate" mode, primitive actions modify a state vector 
which is threaded through the interpreter; in "ex- 
ecute" mode, they result in commands being sent 
to (real or simulated) effector agents. Conversely, 
"meta-information" actions, such as presupposition 
failures, result in output being sent to the meta- 
output stream in "evaluate" mode, and in a null ac- 
tion in "execute" mode. The upshot is that a simple 
semantics can be assigned to rules like the following 
one, which defines the action of attempting to open 
a door which may already be open: 
procedure ( 
open_door (D), 
i f_then_else (status (D, open_closed, open), 
presupposi t ion_fa i lure (already_open(D)), 
change_status (D, open_closed, open) ) ) 
4.2 Using meta-outputs to choose between 
interpretations 
As described in the preceding section, the resolution 
step is in general non-deterministic and gives rise to 
115 
meta-outputs which describe the type of resolution 
carried out. For example, consider a command in- 
volving a definite description, like "open the door". 
Depending on the preceding context, resolution will 
produce a number of possible interpretations; "the 
door" may be resolved to one or more contextually 
available doors, or the expression may be left un- 
resolved. In each case, the type of resolution used 
appears as a meta-output, and is available to the di- 
alogue manager when it decides which interpretation 
is most felicitous. By default, the DM's strategy isto 
attempt to supply antecedents for referring expre~.. 
sious, preferring the most recently occurring sortally 
appropriate candidate. In some cases, however, it is 
desirable to allow the default strategy to be over- 
ridden: for instance, it may result in a script which 
produces a presupposition failure during plan eval- 
uation. Treating resolution choices and plan evalu- 
ation problems as similar types of objects makes it 
easy to implement this kind of idea. 
4.3 Us ing meta-outputs  to choose between 
dialogue management  moves 
Perhaps the key advantage ofour architecture is that 
collecting together several types of information as a 
bag of meta-outputs simplifies the top-level struc- 
ture of the dialogue manager. In our application, 
the critical choice of dialogue move comes after the 
dialogue manager has selected the most plausible in- 
terpretation. It now has to make two choices. First, 
it must decide whether or not to paraphrase any of 
the meta-outputs back to the user; for example, if
resolution was unable to fill some argument posi- 
tion or find an antecedent for a pronoun, it may be 
appropriate to paraphrase the corresponding meta- 
output as a question, e.g. "where do you mean?", or 
"what do you mean by 'it' ?' .  Having all the meta- 
outputs available together means that the DM is 
able to plan a coherent response: so if there are sev- 
eral recta-outputs which could potentially be worth 
paraphrasing, it typically realizes only the most im- 
portant one. 
Second, if interpretation was able to produce a 
well-formed plan, the DM currently has the three 
options of executing it, paraphrasing it back to the 
user as a confirmation question, or doing nothing. 
Once again, this decision often requires global infor- 
mation about what has happened uring the inter- 
pretation process. For example, knowing that plan 
evaluation showed that the plan would take signif- 
icant time to execute makes it more plausible that 
the user would prefer to receive a confirmation. 
5 Examples  
This section presents a commented sample dialogue 
with the implemented PSA interface demo chosen 
to illustrate the points discussed in the previous ec- 
tions. We focus in particular on the role that meta- 
outputs play in interpretation. Text preceded by 
USER represents spoken utterances from the user. 
Text preceded by PSA represents synthesized spoken 
responses from the PSA. Text inside square brackets 
describes the actions carried out by the PSA. 
1. The PSA can move to various locations and 
measure nvironmental f ctors. In the first ex- 
change, reference resolution is used to interpret 
the phrase "all three decks" as referring to the 
lower deck, mid deck and flight deck. Plan op- 
timiT.ation finds the most efficient way to or- 
der the locations, and paraphrases the resulting 
plan back to the user. 
USER: Go to all three decks and measure car- 
bon dioxide. 
PSA: I will move to flight deck, mid deck and 
then lower deck and I will measure carbon 
dioxide level, okay? 
USER: Okay. \[PSA moves to flight deck\] 
PSA: The carbon dioxide level at the flight 
deck is one percent. \[PSA moves to mid 
deck\] 
PSA: The carbon dioxide level at the mid deck 
is one percent. \[PSA moves to lower deck\] 
PSA: The carbon dioxide level at the lower 
deck is one percent. 
2. The next example involves resolution of ellip- 
sis. The relevant context is recovered from the 
preceding dialogue. 
USER: Do the same for the pilot's seat. 
PSA: I will move to the pilot's seat and I will 
measure carbon dioxide level, okay? 
USER: Right. \[PSA moves to pilot's seat\] 
PSA: The carbon dioxide level at the pilot's 
seat is one percent. 
3. Resolution of ellipsis can involve filling in un- 
specified semantic parameters toa syntactically 
complete command. It is not meaningful to talk 
about a sensor eading without reference to the 
place at which that reading was carried out; in 
the following example, resolution assumes that 
the location in question is the one most recently 
mentioned in the dialogue. 
Note also that since the PSA does not have to 
move in order to carry out the command, and 
hence can execute it immediately, the DM rea- 
sonably decides to skip the confirmation ques- 
tion. This decision is made based on the meta- 
output encoding the estimated cost of executing 
the script. 
116 
USER: What was the carbon dioxide level at 
fifteen oh five according to the fixed sen- 
sors? 
PSA: According to the fixed sensors, at fifteen 
oh five the carbon dioxide level at the pi- 
lot's seat was one percent. 
4. The system encounters a user misconception, 
which results in a "presupposition failure" 
meta-output being produced uring reference 
resolution. It consequently decides to alert the 
user to this, rather than attempt to carry out 
the command. 
USER: Close both doors. 
PSA: There are in fact three of them. 
5. The user utterance includes a pronoun, "it". 
The system resolves "it" to "the crew hatch", 
and issues an appropriate paraphrase in order 
to confirm that this is what the user meant. 
USER: Go to crew hatch and close it. 
PSA: I will go to crew hatch and I will close 
crew hatch, okay? 
USER: Yeah. \[PSA moves to crew hatch and 
closes it\] 
6. The user asks the system to close the door, but 
it is unclear which door they are referring to. 
An "ambiguous reference" meta-output is pro- 
duced by reference resolution, resulting in the 
DM issuing a clarification question. 
USER: Close the door. 
PSA: Which door do you mean? 
7. The user response is successfully interpreted by 
reference resolution as an instance of ellipsis. 
Trying to close the crew hatch a second time 
however causes a presupposition failure meta- 
output o be generated during plan evaluation. 
The system informs the user of this and takes 
no action. 
USER: The crew hatch. 
PSA: The crew hatch is already closed. 
8. The user presents three locations in a subopti- 
mal order. Plan evaluation finds a more efficient 
ordering. 
USER: Move to storage lockers, commander's 
seat and flight deck and measure tempera- 
ture. 
PSA: I will move to flight deck, commander's 
seat and then storage lockers and I will 
measure temperature, okay? 
USER: Sure. 
\[PSA moves to flight deck, commander's 
seat and storage lockers in that order and 
reports temperature at each location\] 
9. The user asks the system to repeat he previous 
command. Since the PSA is now starting from 
a different position, its most efficient plan is no 
longer the same, and it informs the user of this. 
USER: Do that again. 
PSA: I will move to storage lockers, comman- 
der's seat and then flight deck and I will 
measure temperature, okay? 
USER: Yes. \[PSA moves to storage lockers\] 
PSA: The temperature at the storage lockers 
is 19.9 degrees Celsius. \[PSA starts moving 
to commander's seat\] 
10. The user interrupts the PSA in the middle of the 
plan, and asks it to go back to its last stopping 
place. Since "stop" may involve a potentially 
hazardous condition, the interface obeys imme- 
diately, without attempting to confirm first. 
USER: Stop. \[PSA stops\] 
USER: Go back. \[PSA returns to storage lock- 
ers/ 
6 Summary  
We have described a generic architecture for spoken 
language dialogue interfaces to semi-autonomous 
agents, based on the standard notion of translating 
to successively deeper levels of representation. The 
novel aspects of our scheme center around two ideas: 
first, that the final output representations are best 
conceptualized not as logical expressions but rather 
as programs in a scripting language; second, that 
steps in the translation process hould produce not 
only a simple output, but also meta-information de-
scribing how the output was produced. We have pre- 
sented examples suggesting how several apparently 
diverse types of dialogue behavior can be captured 
simply within our framework, and outlined a proto- 
type implementation f the scheme. 
References  
N. Badler, R. Bindiganavale, J. Bourne, J. Allbeck, 
J. Shi, and M. Palmer. 1999. Real time virtual 
humans. In International Conference on Digital 
Media Futures. 
J. Dowding, M. Gawron, D. Appelt, L. Cherny, 
R. Moore, and D. Moran. 1993. Gemini: A nat- 
ural language system for spoken language un- 
derstanding. In Proceedings of the Thirty-First 
Annual Meeting of the Association for Computa- 
tional Linguistics. 
117 
K. Konolige, K. Myers, E. Ruspini, and A. Saf- 
fiotti. 1993. Flakey in action: The 1992 AAAI  
robot competition. Technical Report SRI Techni- 
cal Note 528, SKI, AI Center, SKI International, 
333 Ravenswood Ave., Menlo Park, CA  94025. 
D. Martin, A. Cheyer, and D. Moran. 1998. Build- 
ing distributed software systems with the open 
agent architecture. In Proceedings of the Third 
International Conference on the Practical Appli- 
cation of Intelligent Agenta nd Multi-Agent Tech- 
nalogy. 
R. Moore, J. Dowding, H. Bratt, J. Gawron~- 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A spoken-language interface for battlefield simu- 
lations. In Proceedings ofthe Fifth Conference on 
Applied Natural Language Processing, pages 1-7. 
Nuance, 2000. Nuance Communications, Inc. 
http://www.nuance.com. As of 9 March 2000. 
D. Perzanowski, A. Schnltz, and W. Adams. 1998. 
Integrating natural language and gesture in a 
robotics domain. In IEEE International Sympo- 
sium on Intelligent Control." ISIC/CIRA/ISAS 
Joint Conference, pages 247-252, Gaithersburg, 
MD: National Institute of Standards and Tech- 
nology. 
D. Perzanowski, A. Schnltz, W. Adams, and 
E. Marsh. 1999. Goal tracking in s natural lan- 
guage interface: Towards achieving adjustable au- 
tonomy. In ISIS/CIRA99 Conference, Monterey, 
CA. IEEE. 
PSA, 2000. Personal Satellite Assistant (PSA) 
Project. http://ic.arc.nasa.gov/ic/psa/. As of 9 
March 2000. 
D. Pyre, L. Pryor, and D. Murphy. 1995. Actions 
as processes: a position on planning. In Working 
Notes, AAAI Symposium on Eztending Theories 
of Action, pages 169-173. 
R. W. Smith. 1997. An evaluation of strategies for 
selective utterance verification for spoken atural 
language dialog. In Proceedings of the Fifth Con- 
\]erence on Applied Natural Language Processing, 
pages 41-48. 
A. Stent, J. Dowding, J. Gawron, E. Bratt, and 
R. Moore. 1999. The CommandTalk spoken di- 
alogue system. In Proceedings of the Thirty- 
Seventh Annual Meeting of the Association for 
Computational Linguistics, pages 183-190. 
D. R. Tranm and J. Allen. 1994. Discourse obliga- 
tions in dialogue processing. In Proceedings ofthe 
Thirty-Second Annual Meetiitg of the Association 
for Computational Linguistics, pages 1-8. 
D. R. Traum and C. F. Andersen. 1999. Represen- 
tations of dialogue state for domain and task inde- 
pendent meta-dialogue. In Proceedings of the IJ- 
CAI'gg Workshop on Knowledge and Reasoning 
in Practical Dialogue Systems, pages 113-120. 
J. van Eijck and R. Moore. 1992. Semantic rules 
for English. In H. Alshawi, editor, The Core Lan- 
guage Engine. MIT Press. 
B. Webber. 1995. Instructing animated agents: 
Viewing language inbehavioral terms. In Proceed- 
ings of the International Conference on Coopera- 
tive Multi-modal Communication. 
T. A. Winograd. 1973. A procedural model of lan- 
guage understanding. In R. C. Shank and K. M. 
Colby, editors, Computer Models of Thought and 
Language. Freeman, San Francisco, CA. 
118 
Compi l ing  Language Mode ls  f rom a L ingu is t i ca l ly  Mot ivated  
Un i f i ca t ion  Grammar  
Manny Rayner t>, Beth Ann Hockey t, Frankie James t 
Elizabeth Owen Bratt ++, Sharon Goldwater ++ and Jean Mark Gawron ~ 
tResea.rch Inst i tute for 
Advanced Computer  Science 
Mail Stop 19-39 
NASA Ames Research Center 
Moffett Field, CA 94035-1000 
Abstract 
Systems now exist which are able to con:pile 
unification gralmnars into language models that 
can be included in a speech recognizer, but it 
is so far unclear whether non-trivial linguisti- 
cally principled gralnlnars can be used for this 
purpose. We describe a series of experiments 
which investigate the question empirica.lly, by 
incrementally constructing a grammar and dis- 
covering what prot)lems emerge when succes- 
sively larger versions are compiled into finite 
state graph representations and used as lan- 
guage models for a medium-vocabulary recog- 
nition task. 
1 Introduction ~ 
Construction of speech recognizers for n:ediuln- 
vocabulary dialogue tasks has now becolne an 
important I)ractical problem. The central task 
is usually building a suitable language model, 
and a number of standard methodologies have 
become established. Broadly speaking, these 
fall into two main classes. One approach is 
to obtain or create a domain corpus, and froln 
it induce a statistical anguage model, usually 
some kind of N-gram grammar; the alternative 
is to manually design a grammar which specifies 
the utterances the recognizer will accept. There 
are many theoretical reasons to prefer the first 
course if it is feasible, but in practice there is of- 
ten no choice. Unless a substantial domain cor- 
pus is available, the only method that stands a 
chance of working is hand-construction f an ex- 
i The majority of the research reported was performed 
at I{IACS under NASA Cooperative Agreement~ Number 
NCC 2-1006. The research described in Section 3 was 
supported by the Defense Advanced Research Projects 
Agency under Con~racl~ N66001-94 C-6046 with the 
Naval Command, Control, and Ocean Surveillance Cen- 
ter. 
SRI International  
333 Ravenswood Ave 
Menlo Park, CA 94025 
*netdecisions 
Well ington House 
East Road 
Cambr idge CB1 1BH 
England 
plicit grammar based on the grammar-writer's 
intuitions. 
If the application is simple enough, experi- 
ence shows that good grammars of this kind 
can be constructed quickly and efficiently using 
commercially available products like ViaVoice 
SDK (IBM 1999) or the Nuance Toolkit (Nu- 
ance 1999). Systems of this kind typically al- 
low specification of some restricted subset of the 
class of context-free grammars, together with 
annotations that permit the grammar-writer to
associate selnantic values with lexical entries 
and rules. This kind of framework is fl:lly ad- 
equate for small grammars. As the gran:mars 
increase in size, however, the limited expres- 
sive power of context-free language notation be- 
conies increasingly burdensome. The grainn:a,r 
tends to beconie large and unwieldy, with many 
rules appearing in multiple versions that con- 
stantly need to be kept in step with each other. 
It represents a large developn:ent cost, is hard 
to maintain, and does not usually port well to 
new applications. 
It is tempting to consider the option of mov- 
ing towards a :::ore expressive grammar tbrmal- 
isln, like unification gramnm.r, writing the orig- 
inal grammar in unification grammar form and 
coml)iling it down to the context-free notation 
required by the underlying toolkit. At least 
one such system (Gemilfi; (Moore ct al 1997)) 
has been implemented and used to build suc- 
cessful and non-trivial applications, most no- 
tably ComnmndTalk (Stent ct al 1999). Gem- 
ini accepts a slightly constrained version of the 
unification grammar formalism originally used 
in the Core Language Engine (Alshawi 1992), 
and compiles it into context-free gran:nmrs in 
the GSL formalism supported by the Nuance 
Toolkit. The Nuance Toolkit con:piles GSL 
gran:mars into sets of probabilistic finite state 
670 
gra.phs (PFSGs), which form the final bmguage 
model. 
The relative success of the Gemilfi system 
suggests a new question. Ulfification grammars 
ha.re been used many times to build substantial 
general gramlnars tbr English and other na.tu- 
ra\[ languages, but the language model oriented 
gra.mln~rs o far developed fi)r Gemini (includ- 
ing the one for ColnmandTalk) have a.ll been 
domain-sl)ecific. One naturally wonders how 
feasible it is to take yet another step in the di- 
rection of increased genera.lity; roughly, what 
we want to do is start with a completely gen- 
eral, linguistically motivated gramma.r, combine 
it with a domain-specific lexicon, and compile 
the result down to a domain-specitic context- 
free grammar that can be used as a la.nguage 
model. If this 1)tetra.mine can be rea.lized, it is 
easy to believe that the result would 1)e a.n ex- 
tremely useful methodology tbr rapid construc- 
tion of la.nguage models. It is i lnportant o note 
tha.t there are no obvious theoretical obstacles 
in our way. The clailn that English is context- 
free has been respectable since a.t least the early 
8(Is (Pullum and Gazda.r 1982) 'e, and the idea. 
of using unification grammar as a. compact wa 5, 
of tel)resenting an ulMerlying context-fl'e~e, lan- 
guage is one of the main inotivations for GPSG 
(Gazdar et al1985) and other formalislns based 
on it. The real question is whether the goal is 
practically achievable, given the resource limi- 
tations of current technology. 
In this l)a.1)er, we describe work aimed at the 
target outlined above, in which we used the 
Gemini system (described in more detail in Sec- 
tion 2) to a.ttempt o compile a. va.riety of lin- 
guistically principled unification gralnlna.rs into 
la.ngua.ge lnodels. Our first experiments (Sec- 
tion 3) were pertbrmed on a. large pre-existing 
unification gramlna.r. These were unsuccessful, 
for reasons that were not entirely obvious; in 
order to investigate the prol)lem more system- 
atically, we then conducted a second series of 
experilnents (Section 4), in which we increlnen- 
tally 1)uilt up a smMler gra.lnlna.r. By monitor- 
ing; the behavior of the compilation process and 
the resulting langua.ge model as the gra.lmnar~s 
2~1e m'e aware l, hal, this claim is most~ 1)robably not 
l;rue for natural languages ill gelmraI (lh'csnall cl al 
1987), but furl~hcr discussion of t.his point is beyond I.he 
scope of t, llC paper. 
cover~ge was expanded, we were a.ble to iden- 
tit~ the point a,t which serious problems began 
to emerge (Section 5). In the fina.1 section, we 
summarize and suggest fltrther directions. 
2 Tile Genfini Language Model  
Compi le r  
To lnake the paper nlore self-contained, this sec- 
tion provides some background on the method 
used by Gemini to compile unifica.tion grain- 
mars into CFGs, and then into language mod- 
els. The ha.sic idea. is the obvious one: enu- 
mera.te all possible instantiations of the feal;ures 
in the grammar rules and lexicon entries, and 
thus tra.nsform esch rule and entry in the ()rig- 
inal unification grammar into a set of rules in 
the derived CFG. For this to be possible, the 
relevant fe~ttul'es Inust be constrained so that 
they can only take values in a finite predefined 
range. The finite range restriction is inconve- 
nient for fea.tures used to build semantic repre- 
sentations, and the tbrmalism consequently dis- 
tinguishes syntactic and semantic features; se- 
lmmtic features axe discarded a.t the start of the 
compilation process. 
A naive iml)lelnentation of the basic lnethod 
would be iml)raetical for any but the small- 
est a.nd simplest grammars, and considera.ble 
ingemfity has been expended on various opti- 
mizations. Most importantly, categories axe ex- 
panded in a demand-driven fa.shion, with infer  
lnatiotl being percolated 1)oth t)otton>up (from 
the lexicon) and top-down (fl'om the grammar's 
start symbol). This is done in such a. way 
that potentially valid colnl)inations of feature 
instantiations in rules are successively filtered 
out if they are not licensed by the top-down 
and bottom-ul) constra.ints. Ranges of feature 
values are also kept together when possible, so 
that sets of context-free rules produced by the 
mdve algorithm may in these cases be merged 
into single rules. 
By exploiting the structure of the gram- 
mar a.nd lexicon, the demand-driven expansion 
lnethod can often effect substa.ntial reductions 
in the size of the derived CFG. (For the type 
of grammar we consider in this paper, the re- 
duction is typically by ,~ fa.etor of over 102?). 
The downside is that even an app~trently slnall 
cha.nge in the syntactic t>atures associated with 
a. rule may have a large eIfect on the size of 
671 
the CFG, if it opens up or blocks an impor- 
tant percolation path. Adding or deleting lexi- 
con entries can also have a significant effect on 
the size of the CFG, especially when there are 
only a small number of entries in a given gram- 
matical category; as usual, entries of this type 
behave from a software ngineering standpoint 
like grammar ules. 
The language model compiler also performs 
a number of other non-trivial transformations. 
The most important of these is related to the 
fact that Nuance GSL grammars are not al- 
lowed to contain left-recursive rules, and left- 
recursive unification-grammar rules must con- 
sequently be converted into a non-left-recursive 
fort::. Rules of this type do not however occur 
in the gramlnars described below, and we conse- 
quently omit further description of the method. 
3 Initial Experiments 
Our initial experiments were performed on a 
recent unification grammar in the ATIS (Air 
Travel Information System) domain, developed 
as a linguistically principled grammar with a 
domain-specific lexicon. This grammar was 
cre~ted for an experiment COl::t)aring cover- 
age and recognition performance of a hand- 
written grammar with that of a.uto:::atically de- 
rived recognition language models, as increas- 
ing amounts of data from the ATIS corpus 
were made available for each n:ethod. Exam- 
ples of sentences covered by this gralnlnar are 
"yes", "on friday", "i want to fly from boston 
to denver on united airlines on friday septem- 
ber twenty third", "is the cheapest one way 
fare from boston to denver a morning flight", 
and "what flight leaves earliest from boston to 
san francisco with the longest layover in den- 
ver". Problems obtaining a working recognition 
grammar from the unification grammar ended 
our original experiment prematurely, and led 
us to investigate the factors responsible for the 
poor recognition performance. 
We explored several ikely causes of recogni- 
tion trouble: number of rules, ::umber of vocab- 
ulary items, size of node array, perplexity, and 
complexity of the grammar, measured by aver- 
age and highest number of transitions per graph 
in the PFSG form of the grammar. 
We were able to in:mediately rule out sim- 
ple size metrics as the cause of Nuance's diffi- 
culties with recognition. Our smallest air travel 
grammar had 141 Gemini rules and 1043 words, 
producing a Nuance grammar with 368 rules. 
This compares to the Con:mandTalk grammar, 
which had 1231 Gemini rules and 1771 words, 
producing a Nuance gran:n:ar with 4096 rules. 
To determine whether the number of the 
words in the grammar or the structure of 
the phrases was responsible for the recognition 
problems, we created extreme cases of a Word+ 
grammar (i.e. a grammar that constrains the 
input to be any sequence of the words in the 
vocabulary) and a one-word-per-category gram- 
mar. We found that both of these variants 
of our gralmnar produced reasonable recogni- 
tion, though the Word+ grammar was very in- 
accurate. However, a three-words-per-category 
grammar could not produce snccessflfl speech 
recognition. 
Many thature specifications can lnake a gram- 
mar ::tore accurate, but will also result in a 
larger recognition grammar due to multiplica- 
tion of feature w~lues to derive the categories 
of the eontext-fl'ee grammar. We experimented 
with various techniques of selecting features to 
be retained in the recognition grammar. As de- 
scribed in the previous ection, Gemini's default 
method is to select only syntactic features and 
not consider semantic features in the recogni- 
tion grammar. We experimented with selecting 
a subset of syntactic features to apply and with 
applying only se:nantic sortal features, and no 
syntactic features. None of these grammars pro- 
duced successful speech recognition. 
/.Fro::: these experiments, we were unable to 
isolate any simple set of factors to explain which 
grammars would be problematic for speech 
recognition. However, the numbers of transi- 
tions per graph in a PFSG did seem suggestive 
of a factor. The ATIS grammar had a high of 
1184 transitions per graph, while the semantic 
grammar of CommandTalk had a high of 428 
transitions per graph, and produced very rea- 
sonable speech recognition. 
Still, at; the end of these attempts, it beca.me 
clear that we did not yet know the precise char- 
acteristic that makes a linguistically motivated 
grammar intractable for speech recognition, nor 
the best way to retain the advantages of the 
hand-written grammar approach while provid- 
ing reasonable speech recognition. 
672 
4 Incrementa l  Grammar  
Deve lopment  
In our second series of experiments, we in- 
crelnenta.lly developed a. new grammar front 
s('ra.tch. The new gra.mma.r is basica.lly a s('a.led- 
down and a.dapted version of tile Core Lan- 
guage Engine gramme\ for English (Puhnan 
1!)92; Rayner 1993); concrete development work 
a.nd testing were organized a.round a. speech in- 
terfa c(; to a. set; of functionalities oflhred by a 
simple simula,tion of the Space Shuttle (Rather, 
Hockey gll(l James 2000). Rules and lexical 
entries were added in sma.ll groups, typically 
2-3 rules or 5 10 lexical entries in one incre- 
ment. After each round of exl)a.nsion , we tested 
to make sure that the gramlnar could still 1)e 
compiled into a. usa.bh; recognizer, a.nd a.t sev- 
ere.1 points this suggested changes in our iln- 
1)\]ementation strategy. The rest of this section 
describes tile new grmmnar in nlore detail. 
4.1 Overv iew of  Ru les  
The current versions of the grammar and lexi- 
con contain 58 rules a.nd 30J. Ulfinflectesl entries 
respectively. They (:over the tbllowing phenom- 
el i  :~IZ 
1. Top-level utl;er~tnces: declarative clauses, 
WH-qtlestions, Y-N questions, iml)erat;ives, 
etlil)tical NPs and I)Ps, int(;rject.ions. 
~.. / \ ]  9 \,~ H-lnovement of NPs and PPs. 
3. The fbllowing verb types: intr~nsi- 
tive, silnple transitive, PP con:plen-mnt, 
lnodaJ/a.uxiliary, -ing VP con-q)len:ent, par- 
ticleq-NP complement, sentential comple- 
lnent, embedded question complement. 
4. PPs: simple PP, PP with postposition 
("ago")~ PP lnodifica,tion of VP and NP. 
5. Relat;ive clauses with both relative NP pro- 
1101111 ("tit(; telnperature th,tt I measured )
and relative PP ("the (loci: where I am"). 
6. Numeric determiners, time expressions, 
and postmodification of NP 1)y nun:eric ex- 
pressions. 
7. Constituent conjunction of NPs and 
cl~ulses. 
Tilt following examl)le sentences illustrate 
current covera,ge: 3 '-. , ':how ~d)out scenario 
three.?", "wha, t is the temperature?", "mea- 
sure the pressure a,t flight deck", "go to tile 
crew ha.tch a.nd (:lose it", "what were ten:per- 
a.tttt'e a, nd pressure a.t iifteen oh five?", "is the 
telnpera.ture going ttp'. ~', "do the fi?ed sensors 
sa.y tha.t the pressure is decreasing. , "find out 
when the pressure rea.ched fifteen p s i . . . .  wh~t 1 
is the pressure that you mea.sured?", "wha.t is 
the tempera.lure where you a.re?", ?~(:a.n you find 
out when the fixed sensors ay the temperature 
at flight deck reached thirty degrees celsius?". 
4.2 Unusua l  Features  o f  the  Grammar  
Most of the gramn:~u', as already sta.ted, is 
closely based on the Core Language Eng!ne 
gra.nlnla.r. \?e briefly sllnllna.rize the main di- 
vergences between the two gramnlars. 
4.2.1 I nvers ion  
The new gramlna, r uses a. novel trea.tment of 
inversion, which is p~trtly designed to simplify 
the l)l'ocess of compiling a, fea,ture gl'anllna, r into 
context-free form. The CLE grammar's trea.t- 
l l tent of invers ion uses a, movement account, in 
which the fronted verb is lnoved to its notional 
pla.ce in the VP through a feature. So, tbr 
example, the sentence "is pressure low?" will 
in the origina.1 CLE gramma.r ha.re the phrase- 
structure 
::\[\[iS\]l" \ [p ressure \ ]N / ,  \[\[\]V \[IO\V\]AI),\]\]V'\]'\],'g" 
in whk:h the head of th(, VP is a V gap coin- 
dexed with tile fronted main verb 1,~ . 
Our new gra.mn:ar, in contrast, hal:dles in- 
version without movement, by making the con> 
bination of inverted ver\]) and subject into a. 
VBAR constituent. A binary fea.ture invsubj  
picks o:ll; these VBARs, a.nd there is a. question- 
forma,tion rule of tilt form 
S --> VP : E invsub j=y\ ]  
Continuing the example, the new gram- 
mar a.ssigns this sentence tilt simpler phrase- 
structure 
"\[\[\[is\] v \[press:ire\] N*'\] v .A .  \[\[low\] J\] V.\] S" 
4.2.2 Sorta l  Const ra in ts  
Sortal constra,ints are coded into most gr~un:nnr 
rules as synta.ctic features in a straight-forward 
lna.nner, so they are available to the compilation 
673 
process which constructs the context-free gram- 
mar, ~nd ultimately tile language model. The 
current lexicon allows 11 possible sortal values 
tbr nouns, and 5 for PPs. 
We have taken the rather non-standard step 
of organizing tile rules for PP modification so 
that a VP or NP cannot be modified by two 
PPs  of the same sortal type. The principal mo- 
tivation is to tighten the language model with 
regard to prepositions, which tend to be pho- 
netically reduced and often hard to distinguish 
from other function words. For example, with- 
out this extra constraint we discovered that an 
utterance like 
measure temperature at flight deck 
and lower deck 
would frequently be misrecognized as 
measure temperature at flight deck in 
lower deck 
5 Exper iments  with Incremental  
G r am 111 ar  S 
Our intention when developing the new gram- 
mar was to find out just when problems began 
to emerge with respect to compilation of tan- 
gm~ge models. Our initial hypothesis was that 
these would l)robably become serious if the rules 
for clausal structure were reasonably elaborate; 
we expected that the large number of possible 
ways of combining modal and auxiliary verbs, 
question forlnation, movement, and sentential 
complements would rapidly combine to produce 
an intractably loose language model. Interest- 
ingly, this did not prove to be the case. In- 
stead, the rules which appear to be the primary 
ca.use of difficulties are those relating to relative 
clauses. We describe the main results in Sec- 
tion 5.1; quantitative results on recognizer per- 
tbrmance are presented together in Section 5.2. 
5.1 Main Findings 
We discovered that addition of the single rule 
which allowed relative clause modification of an 
NP had a dr~stic effect on recognizer perfor- 
lnance. The most obvious symptoms were that 
recognition became much slower and the size of 
the recognition process much larger, sometimes 
causing it to exceed resource bounds. The false 
reject rate (the l)roportion of utterances which 
fell below the recognizer's mininmnl confidence 
theshold) also increased substantially, though 
we were surprised to discover no significant in- 
crea.se in the word error rate tbr sentences which 
did produce a recognition result. To investi- 
gate tile cause of these effects, we examined the 
results of perfornfing compilation to GSL and 
PFSG level. The compilation processes are such 
that symbols retain mnemonic names, so that it 
is relatively easy to find GSL rules and gral)hs 
used to recognize phrases of specified gralnmat- 
ical categories. 
At the GSL level, addition of the relative 
clause rule to the original unification grammar 
only increased the number of derived Nuance 
rules by about 15%, from 4317 to 4959. The av- 
erage size of the rules however increased much 
more a. It, is easiest o measure size at the level of 
PFSGs, by counting nodes and transitions; we 
found that the total size of all the graphs had in- 
creased from 48836 nodes and 57195 tra.nsitions 
to 113166 nodes and 140640 transitions, rather 
more than doubling. The increase was not dis- 
tributed evenly between graphs. We extracted 
figures for only the graphs relating to specific 
grammatical categories; this showed that, the 
number of gra.1)hs fbr NPs had increased from 
94 to 258, and lnoreover that the average size 
of each NP graph had increased fronl 21 nodes 
and 25.5 transitions to 127 nodes and 165 tra.nsi- 
tions, a more than sixfold increase. The graphs 
for clause (S) phrases had only increased in 
number froln 53 to 68. They ha.d however also 
greatly increased in average size, from 171 nodes 
and 212 transitions to 445 nodes and 572 tran- 
sitions, or slightly less than a threefold increase. 
Since NP and S are by far the most important 
categories in the grammar, it is not strange that 
these large changes m~tke a great difference to 
the quality of the language model, and indi- 
rectly to that of speech recognition. 
Colnparing the original unification gramlnar 
and the compiled CSL version, we were able to 
make a precise diagnosis. The problem with the 
relative clause rules are that they unify feature 
values in the critical S and NP subgralnlnars; 
this means that each constrains the other, lead- 
ing to the large observed increase in the size 
and complexity of the derived Nuance grammar. 
aGSL rules are written in all notat ion which allows 
disjunction and Klccne star. 
674 
Specifically, agreement ilffbrmation and sortal 
category are shared between the two daugh- 
ter NPs in the relative clause modification rule, 
which is schematically as follows: 
Igp: \[agr=A, sort=S\]  --+ 
NP: \[agr=A, sort=S\] 
REL:\[agr=A, sort=S\]  
These feature settings ~re needed in order to get 
tile right alternation in pairs like 
the robot that *measure/measures 
the teml)erature \[agr\] 
the *deck/teml)era.ture tha.t you 
measured \[sort\] 
We tested our hypothesis by colnlnenting ()lit 
the agr and sor t  features in the above rule. 
This completely solves the main 1)robh;in of ex- 
1)lesion in the size of the PFSG representation; 
tile new version is only very slightly larger than 
tile one with no relative clause rule (50647 nodes 
and 59322 transitions against 48836 nodes and 
57195 transitions) Most inL1)ortantty, there is 
no great increase in the number or average size 
of the NP and S graphs. NP graphs increase in 
number froin 94 to 130, and stay constant in a.v- 
era ge size.; S graphs increase in number f}om 53 
to 64, and actually decrease, in aa;erage size to 
13,5 nodes and 167 transitions. Tests on st)eech 
(l~t;a. show that recognition quality is nea~rly :lie 
sa.me as for the version of the recognizer which 
does not cover relative clauses. Although speed 
is still significantly degraded, the process size 
has been reduced sufficiently that the 1)roblen:s 
with resource bounds disappear. 
It would be rea.sonal)le 1:o expect tim: remov- 
ing the explosion in the PFSG ret)resentation 
would result in mL underconstrained language 
model for the relative clause paxt of the gram- 
mar, causing degraded 1)erformance on utter- 
ances containing a, relative clause. Interestingly, 
this does not appear to hapl)en , though recog- 
nition speed under the new grammar is signif- 
icaatly worse for these utterances COml)ared to 
utterances with no relative clause. 
5.2 Recogn i t ion  Resu l ts  
This section summarizes our empirical recog- 
nition results. With the help of the Nuance 
Toolkit batchrec  tool, we evah:ated three ver- 
sions of the recognizer, which differed only with 
respect to tile language model, no_re ls  used 
the version of the language model derived fl'onI a 
granLn:a.r with the relative clause rule removed; 
re l s  is the version derived from the fltll gram- 
lnar; and un l inked  is the colnl)romise version, 
which keeps the relative clause rule but removes 
the critical features. We constructed a corpus 
of 41 utterances, of mean length 12.1 words. 
The utterances were chosen so that the first, 31 
were within the coverage of all three versions 
of the grammar; the last 10 contained relative 
clauses, and were within the coverage of re :s  
and un: inked but :tot of no_rels .  Each utter- 
anee was recorded by eight different subjects, 
none of whom had participated in development 
of the gra.mmar or recognizers. Tests were run 
on a dual-processor SUN Ultra60 with 1.5 GB 
of RAM. 
The recognizer was set, to reject uttera.nces if 
their a.ssociated confidence measure fell under 
the default threshold. Figures 1 and 2 sum- 
marize the re.suits for the first 31 utterances 
(no relative clauses) and the last 10 uttera:Lces 
(relative clauses) respectively. Under '?RT', 
we give inean recognition speed (averaged over 
subjects) e?pressed as a multiple of real time; 
'PRe.j' gives the false reject rate, the :heart l)er - 
centage of utterances which were reiected ue to 
low confidence measures; 'Me:n' gives the lnean 
1)ercentage of uttera.nces which fhiled due to the. 
recognition process exceeding inemory resource 
bounds; and 'WER,' gives the mean word er- 
ror rate on the sentences that were neither re- 
jected nor failed due to resource bound prob- 
lems. Since the distribution was highly skewed, 
all mea.ns were calculated over the six subjects 
renm.i:fing after exclusion of the extreme high 
and low values. 
Looking first at Figure 1, we see that re l s  is 
clearly inferior to no_re ls  on tile subset of the 
corpus which is within the coverage of both ver- 
sions: nea.rly twice as many utterances are re- 
jected due to low confidence values or resource 
1)roblems, and recognition speed is about five 
times slower, un l inked  is in contrast :tot sig- 
nificantly worse than no_re ls  in terms of recog- 
nition performance, though it is still two and a 
half times slower. 
Figure 2 compares re l s  and un l inked on the 
utterances containing a relative clause. It seems 
reasona.ble to say that recognition performance 
675 
I C4ran"nar I I FR .i I IWER 1 
no_rels 1.04 9.0% - 6.0% 
re l s  4.76 16.1% 1.1% 5.7% 
un l inked  2.60 9.6% - 6.5% 
Figure 1: Evaluation results for 31 utterances 
not containing relative clauses, averaged across 
8 subjects excluding extreme values. 
Grammar xRT FRej Men: WER\ ]  
re l s  4.60 26.7% 1.6% 3.5%\] 
un l inked 5.29 20.0% - 5.4%J 
Figure 2: Evaluation results for i0 utter~mces 
containing relative clauses, averaged across 8 
subjects excluding extreme values. 
is comparable for the two versions: rels has 
lower word error rate, but also rqjects more 
utterances. Recognition speed is marginally 
lower for unl inked,  though it is not clear to us 
whether the difference is significant given the 
high variability of the data. 
6 Conc lus ions  and  Fur ther  
D i rec t ions  j 
We found the results presented above surpris- 
ing and interesting. When we 1)egal: our pro- 
gramme of attempting to compile increasingly 
larger linguistically based unification grammars 
into language models, we had expected to see a 
steady combinatorial increase, which we guessed 
would be most obviously related to complex 
clause structure. This did not turn out to be the 
case. Instead, the serious problems we encoun- 
tered were caused by a small number of crit- 
ical rules, of which the one for relative clause 
modification was by the far the worst. It was 
not immediately obvious how to deal with the 
problem, but a careful analysis revealed a rea- 
sonable con:promise solution, whose only draw- 
back was a significant but undisastrous degra- 
dation in recognition speed. 
It seems optimistic to hope that the rela- 
tive clause problem is the end of the story; the 
obvious way to investigate is by continuing to 
expand the gramlnar in the same incremental 
fashion, and find out what happens next. We 
intend to do this over the next few months, and 
expect in due course to be able to l)resent fur- 
ther results. 
References  
H. Alshawi. 1992. The Core Language Engine. 
Cambridge, Massachusetts: The MIT Press. 
J. Bresnan, R.M. Kapla.n, S. Peters and A. Za- 
enen. Cross-Serial Dependencies in Dutch. 
1987. In W. J. Savitch et al(eds.), The For- 
real Complexity of Natural Languagc, Reidel, 
Dordrecht, pages 286-319. 
G. Gazdar, E. Klein, G. Pullum and I. Sag. 
1985. Generalized Phrase Structure Gram- 
mar Basil Blackwell. 
IBM. 1999. ViaVoice SDK tbr Windows, ver- 
sion 1.5. 
R. Moore, J. Dowding, H. Bratt, J.M. Gawron, 
Y. Gorfl:, and A. Cheyer. 1997. Com- 
mandTalk: A Spoken-Language Interface 
tbr Battlefield Simulations. Proceedings 
of the Fifth Conference on Applied Nat- 
uraI Languagc Processing, pages 1-7, 
Washington, DC. Available online from 
http ://www. ai. sri. com/natural-language 
/project s/arpa-sl s / commandt alk. html. 
Nuance Communications. 1999. Nuance Speech 
Recognition System Developer's Manv, aI, Ver- 
sion 6.2 
G. Pullum and G. Gazdar. 1982. Natural Lan- 
guages and Context-Free Languages. Lin- 
guistics and Philosophy, 4, pages 471-504. 
S.G. Puhnan. 1992. Unification-Based Synta.c- 
tic Analysis. In (Alshawi 1992) 
M. Rayner. 1993. English Linguistic Coverage. 
In M.S. Agn~s et al 1993. Spoken Language 
Translator: First Year Report. SRI Techni- 
cal Report CRC-043. Available online from 
http ://www. sri. com. 
M. Rayner, B.A. Hockey and F. James. 2000. 
Turning Speech into Scripts. To appear in 
P~vceedings of the 2000 AAAI Spring Sym- 
posium on Natural Language Dialogues with 
Practical Robotic Devices 
A. Stent, J. Dowding, J.M. Gawron, E.O. 
Bratt, and R. Moore. 1999. The Coin- 
mandTalk Spoken Dialogue System. P'rv- 
cecdings of the 37th Annual Meeting of the 
ACL, pages 183-190. Available online from 
ht tp  ://www. a i .  s r i .  com/natura l - language 
/p ro jec t  s /a rpa-s  :s  / commandt a:k.  html. 
676 
A Compact  Arch i tec ture  for D ia logue  Management  Based  on 
Scr ip ts  and  Meta -Outputs  
Manny Rayner,  Beth  Ann  Hockey~ Frankie James .: : 
Research Institute for Advanced Computer Science 
Mail Stop 19-39, NASA Ames Research Center 
Moffett Field, CA 94035-1000 
{manny, bahockey, fjames} @riacs.edu 
Abstract  
We describe an architecture for spoken dialogue 
interfaces to semi-autonomous systems that trans- 
forms speech~signals through successive representa- 
tions of linguistic, dialogue, and domain knowledge. 
Each step produces an output, and a meta-output 
describing the transformation, with an executable 
program in a simple scripting language as the fi- 
nal result. The output/meta-output distinction per- 
mits perspicuous treatment of diverse tasks such as 
resolving pronouns, correcting user misconceptions, 
and optimizing scripts. 
1 Introduction 1 
The basic task we consider in this paper is that of 
using spoken language to give commands to a semi- 
autonomous robot or other similar system. As ev- 
idence of the importance of this ta~k in the NLP 
community note that the early, influential system 
SHRDLU (Winograd, 1973) was intended to address 
just this type of problem. More recent work on spo- 
ken language interfaces to semi-autonomous robots 
include SRI's Flakey robot (Konolige et al, 1993) 
and NCARAI's InterBOT project (Perzanowski et 
al., 1998; Perzanowski et al, 1999). A number of 
other systems have addressed part of the task. Com- 
mandTalk (Moore et al, 1997), Circuit Fix-It Shop 
(Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 
1994; Traum and Andersen, 1999) are spoken lan- 
guage systems but they interface to simulation or 
help facilities rather than semi-autonomous agents. 
Jack's MOOse Lodge (Badler et al, 1999) takes text 
rather than speech as natural anguage input and the 
avatars being controlled are not semi-autonomous. 
Other researchers have considered particular aspects 
of the problem such as accounting for various aspects 
of actions (Webber, 1995; Pym et al, 1995). In most 
of this and other related work the treatment is some 
variant of the following. If there is a speech inter- 
face, the input speech signal is converted into text. 
Text either from the recognizer or directly input by 
IThis paper also appears in the proceedings of the Sixth 
International Conference on Applied Natural Language Pro- 
cessing~ Seattle, WA, April 2000. 
the user is then converted into some kind of logi- 
cal formula, which abstractly represents the user's 
intended command; this formula is.then fed into a 
command interpreter, which execdtes the command. 
We do not think the standard treatment outlined 
above is in essence incorrect, but we do believe that, 
as it stands, it is in-need of some modification. This 
paper will in particular make three points. First, we 
suggest that the output representation should not be 
regarded as a logical expression, but: rather as a pro- 
gram in some kind of scripting language. Second, we 
argue that it is not merely the case that the process 
of converting the input signal to the final represen- 
tation can sometimes go wrong; rather, this is the 
normal course of events, and the inferpretatiofi pro- 
cess should be organized with that assumption in 
mind. Third, we claim, perhaps urprisingly, that 
the first and second points are related. These claims 
are elaborated in Section 2. 
The remainder of the paper describes an archi- 
tecture which addresses the issues outlined above, 
and which has been used to implement a prototype 
speech interface to a simulated semi-autonomous 
robot intended for deployment on the International 
Space Station. Sections 3 and 4 present an overview 
of the implemented interface, focussingon represen- 
tational issues relevant o dialogue management. Il-
lustrative xamples of interactions with the system 
are provided in Section 5. Section 6 concludes. 
2 Theoretical Ideas, 
2.1 Scripts vs Logical Forms 
Let's first look in a little more detail at the question 
of what the output representation f a spoken lan- 
guage interface to a semi-autonomous robot/agent 
should be. In practice, there seem to be two main 
choices: atheoretical representations, or some kind 
of logic. 
Logic is indeed an excellent way to  think 
about representing static relationships like database 
queries, but it is much less clear that it is a good way 
to represent commands. In real life, when people 
wish to give a command to a computer, they usu- 
ally do so via its operating system; a complex com- 
54 
mand is an expression in a scripting language like 
CSHELL, Perl, or VBScript. These languages are 
related to logical formalisms, but cannot be mapped 
onto them in a simple way. Here are some of the 
obvious differences: 
? A scripting language is essentially imperative, 
rather than relational. 
? The notion of temporal sequence is fundamental 
to the language. "Do P and then Q" is not the 
same as "Make the goals P and Q true"; it is 
explicitly stated that P is to be done first. Simi~ 
larly, "For each X in the list (A B C), do P (X)"  
is not the same as "For all X, make P(X) true"; 
once again, the scripting language defines an or- 
der, but no~ the logical language 2.
? Scripting languages assume that commands do 
not always succeed. For example, UNIX-based 
scripting languages like CSHELL provide each 
script with the three predefined streams td in ,  
s tdout  and s tder r .  Input is read from s td in  
and written to stdout;  error messages, warn- 
ings and other comments are sent to s tder r .  
Wedo not think that these properties of scripting 
language are accidental. They have evolved as the 
result of strong selectional pressure from real users 
with real-world tasks that need to be carried out, 
and represent a competitive way to meet said users' 
needs. We consequently think it is worth taking seri- 
ously the idea that a target representation produced 
by a spoken language interface should share many of 
these properties. 
2.2 Fall ible In terpretat ion :  Outputs  and 
Meta -outputs  
We now move on to the question of modelling the in- 
terpretation process, that is to say the process that 
converts the input (speech) signal to the output (ex- 
ecutable) representation. As already indicated, we 
think it is important o realize that interpretation 
is a process which, like any other process, may suc- 
ceed more or less well in achieving its intended goals. 
Users may express themselves unclearly or incom- 
pletely, or the system may more or less seriously 
fail to understand exactly what they mean. A good 
interpretation architecture will keep these consider- 
ations in mind. 
Taking our lead from the description of scripting 
languages ketched above, we adapt the notion of 
the "error stream" to the interpretation process. In 
the course of  interpreting an utterance, the system 
2In cases like these, the theorem prover or logic program- 
ming interpreter used to evaluate the logical formula typically 
assigns a conventional order to the conjuncts; note however 
that this is part of the procedural semantics of the theorem 
prover/interpreter, and does not follow from the declarative 
semantics of the logical formalism. 
55 
translates it into successively "deeper" levels of rep- 
resentation. Each translation step has not only an 
input (the representation consumed) and an output 
(the representation produced), but also something 
we will refer to as a "meta-output':  this provides in- 
formation about how the translation was performed. 
At a high level of abstraction, our architecture will 
be as follows. Interpretation proceeds as a series 
of non-deterministic ranslation steps, each produc- 
ing a set of possible outputs and associated meta- 
outputs. The final translation step produces an ex- 
ecutable script. The interface attempts to simulate 
execution of each possible script produced, in or- 
der to determine what would happen if that script 
were selected; simulated execution can itself produce 
further meta-outputs. Finally, the system uses the 
meta-output information to decide what to do with 
the various possible interpretations it has produced. 
Possible actions incl~tde selection and execution of 
an output script, paraphrasing meta-output infor- 
mation back to the user, or some combination of the 
two. 
In the following section, we present a more de- 
tailed description showing how the output/meta- 
output'distinction works in a practical system. 
3 A Prototype Implementation 
The ideas sketched out above have been realized as 
a prototype spoken language dialogue interface to a 
simulated version of the Personal Satellite Assistant 
(PS i ;  (PSA, 2000)). This section gives an overview 
of the implementation; in the following section, we 
focus on the specific aspects of dialogue management 
which are facilitated by the output/meta-output ar- 
chitecture. 
3.1 Levels of  Representat ion  
The real PSA is a miniature robot currently being 
developed at NASA Ames Research Center, which 
is intended for deployment on the Space Shuttle 
and/or International Space Station. It will be ca- 
pable of free navigation in an indoor micro-gravity 
environment, and will provide mobile sensory capac- 
ity as a backup to a network of fixed sensors. The 
PSA will primarily be controlled by voice commands 
through a hand-held or head-mounted microphone, 
with speech and language processing being handled 
by an offboard processor. Since the speech process- 
ing units are not in fact physically connected to the 
PSA we envisage that they could also be used to con- 
trol or monitor other environmental functions. In 
particular, our simulation allows voice access to the 
current and past values of the fixed sensor eadings. 
The initial PSA speech interface demo consists of 
a simple simulation of the Shuttle. State parame- 
ters include the PSA's current position, some envi- 
ronmental variables uch as local temperature, pres- 
sure and carbon dioxide levels, and the status of the 
Shuttle's doors (open/closed). A visual display gives 
direct feedback on some of these parameters. 
The speech and language processing architecture 
is based on that of the SRI CommandTalk sys- 
tem (Moore et al, 1997; Stent et al, 1999). The sys- 
tem comprises a suite of about 20 agents, connected 
together using the SRI Open Agent Architecture 
(OAA; (Martin et al, 1998)). Speech recognition 
is performed using a version of the Nuance recog- 
nizer (Nuance, 2000). Initial language processing is
carried out Using the SRI Gemini system (Dowding 
et al, 1993), using a domain-independent u ification. 
grammar and a domain-specific lexicon. The lan- 
guage processing rammar is compiled into a recog- 
nition grammar using the methods of (Moore et al, 
1997); the n~ resnlt is that only grammatically well- 
formed utterances Gan be recognized. Output from 
the initial language-processing step is represented 
in a version of Quasi Logical Form (van Eijck and 
Moore, 1992), and passed in that form to the dia- 
logue manager. We refer to these as linguistic level 
representations. 
The aspects of the system which are of primary in- 
terest here concern the dialogue manager (DM) and 
related modules. Once a linguistic level represen- 
tation has been produced, the following processing 
steps occur: 
? The linguistic level representation is converted 
into a discourse level representation. This pri- 
marily involves regularizing differences in sur- 
face form: so, for example, "measure the pres- 
sure" and "what is the pressure?" have differ- 
ent representations at the linguistic level, but 
the same representation at the discourse level. 
? If necessary, the system attempts to resolve in- 
stances of ellipsis and anaphoric reference. For 
example, if the previous command was "mea- 
sure temperature at flight deck", then the new 
command "lower deck" will be resolved to an 
expression meaning "measure temperature at 
lower deck". Similarly, if the previous command 
was "move to the crew hatch", then the com- 
mand "open it" will be resolved to "open the 
crew hatch". We call the output of this step a 
resolved discourse level representation. 
? The resolved discourse level representation is 
converted into an executable script in a lan- 
guage essentially equivalent o a subset of 
CSHELL. This involves two sub-steps. First, 
quantified variables are given scope: for exam- 
ple, "go to the flight deck and lower deck and 
measure pressure" becomes omething approxi- 
mately equivalent to the script 
foreach x ( f l ight_deck  lower_deck) 
go_to Sx 
measure  pressure  
end 
The point to note here is that :the foreach has 
scope over both the go_to and the measure ac- 
tions; an alternate (incorrect) sCoping would be 
foreach x (flight_deck lower_deck) 
go_to Sx 
end 
measure  pressure 
The second sub-step is to attempt o optimize 
the plan. In the current example, this can 
be done by reordering the list ( f l ight_deck 
lowerAeck). For instance, if the PSA is al- 
ready at the lower deck, reversing the list will 
mean that the robot only makes one trip, in- 
stead of two. ,, 
The final step in the 'interlJretation process is 
plan evaluation: the syStem tries to work out 
what will happen if it actually executes the 
plan. (The relationship between plan evaluation 
and plan execution is described in more detail 
in Section 4.1). Among other things, this gives 
the dialogue manager the possibility of compar- 
ing different interpretations of the original com- 
mand, and picking the one which is most effi- 
cient. 
3.2 How Meta-outputs  Part ic ipate in the 
Translation 
The above sketch shows how context-dependent 
interpretation is arranged as a series of non- 
deterministic translation stepS; in each case, we have 
described the input and the output for the step in 
question. We now go back to the concerns of Sec- 
tion 2. First, note that each translation step is in 
general fallible. We give severalexamples: 
? One of the most obvious cases arises when the 
user simply issues an invalid command, such as 
requesting the PSA to open a door D which is 
already open. Here, one of the meta~outputs 
issued by the plan evaluation step will be the 
term 
presupposition_failure(already_open(D)); 
the DM can decide to paraphrase this back to 
the user as a surface string of the form "D is 
already open". Note that plan evaluation does 
not involve actually executing the final script, 
which can be important. For instance, if the 
command is "go to the crew hatch and open it" 
and the crew hatch is already open, the interface 
has the option of informing the user that there 
is a problem without-first carrying out the "go 
to" action. 
56 
? The resolution step can give rise to similar kinds 
of meta-output. For example, a command may 
include a referring expression that has no deno- 
tation, or an ambiguous denotation; for exam- 
ple, the user might say "both decks", presum- 
ably being unaware that there are in fact three 
of them. This time, the meta-output produced 
is 
presupposition_failure ( 
incorrect_size_of_set (2,3)) 
representing the user'sincorrect belief abou\[  
the number of decks. The DM then has the pos- 
sibility of informing the user of this misconcep- 
tion by realizipg the meta-output term as the 
surface stung "in. fact there are three of them". 
Ambiguous denotation occurs when a descrip- 
tion is under-specified. For instance, the user 
might say "the deck" in a situation where there 
is no clearly salient deck, either in the discourse 
situation :or in the simulated world: here, the 
meta-output will be 
presupposition_failure ( 
under specif ied_def inite (deck)) 
which can be realized as the clarification ques- 
tion "which deck do you mean?" 
? A slightly more complex case involves plan 
costs. During plan evaluation, the system simu- 
lates execution of the output script while keep- 
ing track of execution cost. (Currently, the cost 
is just an estimate of the time required to exe- 
cute the script). Execution costs are treated as 
meta-outputs of the form 
cost(C) 
and passed back through the interpreter so that 
the plan optimization step can make use of 
them. 
Finally,. we consider what happens when the 
system receives incorrect input from the speech 
recognizer. Although the recognizer's language 
model is constrained so that it can only pro- 
duce grammatical utterances, it can still misrec- 
ognize one grammatical string as another one. 
Many of these cases fall into one of a small 
number of syntactic patterns, which function as 
fairly reliable indicators of bad recognition. A 
typical example is conjunction i volving a pro- 
noun: if the system hears "it and flight deck", 
this is most likely a misrecognition f something 
like "go to flight deck". 
During the processing phase which translates 
linguistic level representations into discourse 
level representations, the system attempts to 
match each misrecognition pattern against he 
input linguistic form, and if successful produces 
a meta-output of the form 
presupposition_failure ( 
dubious_if (<Type>)) 
These mete-outputs are passed down to the 
DM, which in the absence of sufficiently com- 
pelling contrary evidence will normally issue a 
response of the form "I'm sorry, I think I mis- 
heard you". 
4 A Compact  Arch i tec ture  for 
D ia logue  Management  Based  on  
Scr ip ts  and  Meta -Outputs  
None of the individual functionalities outlined above 
are particularly novel in themselves. What we find 
new and interesting-is the fact that they can all 
be expressed in a uniform way in terms of the 
script output/meta-output architecture. This sec- 
tion presents three examples illustrating how the ar- 
chitecture can be used to simplify the overall orga- 
nization of the system. 
4.1 Integrat ion of plan evaluation, plan 
execution and dialogue management .  
Recall that the DM simulates evaluation of the plan 
before running it, in order to obtain relevant meta- 
information. At plan execution time, plan actions 
result in changes to the world; at plan evaluation 
time, they result in simulated changes to the world 
and/or produce meta-outputs. 
Conceptualizing plans as scripts rather than log- 
ical formulas permits an elegant reatment of the 
execution/evaluation dichotomy. There is one script 
interpreter, which functions both as a script exec- 
utive and a script evaluator, and one set of rules 
which defines the procedural semantics of script ac- 
tions. Rules are parameterized by execution type 
which is either "execute" or "evaluate". In "evalu- 
ate" mode, primitive actions modify a state vector 
which is threaded through the interpreter; in "ex- 
ecute" mode, they result in commands being sent 
to (real or simulated) effector agents. Conversely, 
"meta-information" actions, such as presupposition 
failures, result in output being sent to the meta- 
output stream in "evaluate" mode, and in a null ac- 
tion in "execute" mode. The upshot is that a simple 
semantics can be assigned to rules like the following 
one, which defines the action of attempting to open 
a door which may already be open: 
procedure ( 
open_door (D) , 
if_then_else (status (D, open_closed, open), 
presupposition_failure (already_open(D)), 
change_status (D, open_closed, open))) 
57 
4.2 Using meta-outputs  o choose between 
interpretat ions 
As described in the preceding section, the resolution 
step is in general non-deterministic and gives rise to 
meta-outputs which describe the type of resolution 
carried out. For example, consider a command in- 
volving a definite description, like "open the door". 
Depending on the preceding context, resolution will 
produce a number of possible interpretations; "the 
door" may be resolved to one or more contextually 
available doors, or the expression may be left un- 
resolved. In each case, the type of resolution used 
appears as a meta-output, and is available to the di- 
alogue manager when it decides which interpretation 
is most felicitous. By default, the DM's strategy is to 
attempt to~pp ly  antecedents for referring expres- 
sions, preferring h~ most recently occurring sortally 
appropriate candidate. In some cases, however, it is 
desirable to allow the default strategy to be over- 
ridden: for instance, it may result in a script which 
produces a presupposition failure during plan eval- 
uation. Treating resolution choices and plan evalu- 
ation problems as similar types of objects makes it 
easy to implement this kind of idea. 
4.3 Using meta-outputs  o choose between 
dialogue management  moves 
Perhaps the key advantage ofour architecture is that 
collecting together several types of information as a 
bag of meta-outputs simplifies the top-level struc- 
ture of the dialogue manager. In our application, 
the critical choice of dialogue move comes after the 
dialogue manager has selected the most plausible in- 
terpretation. It now has to make two choices. First, 
it must decide whether or not to paraphrase any of 
the meta-outputs back to the user; for example, if 
resolution was unable to fill some argument posi- 
tion or find an antecedent for a pronoun, it may be 
appropriate to paraphrase the corresponding meta- 
output as a question, e.g. "where do you mean?", or 
"what do you mean by 'it' ?". Having all the meta- 
outputs available together means that the DM is 
able to plan a coherent response: so if there are sev- 
eral meta-outputs which could potentially be worth 
paraphrasing, it typically realizes only the most im- 
portant one. 
Second, if interpretation was able to produce a 
well-formed plan, the DM currently has the three 
options of executing it, paraphrasing it back to the 
user as a confirmation question, or doing nothing. 
Once again, this decision often requires global infor- 
mation about what has happened uring the inter- 
pretation process. For example, knowing that plan 
evaluation showed that the plan would take signif- 
icant time to execute makes it more plausible that 
the user would prefer to receive a confirmation. 
58 
5 Examples  
This section presents a Commented sample dialogue 
with the implemented PSA interface demo chosen 
to illustrate the points discussed inthe previous ec- 
tions. We focus in particular On ,the :role that meta- 
outputs play in interpretation. Text preceded by 
USER represents spoken utterances from the user. 
Text preceded by PSA represents synthesized spoken 
responses from the PSA. Text inside square brackets 
describes the actions carried out bythe PSA. 
1. The PSA can move to various locations and 
measure nvironmental f ctors. In the first ex- 
zhange, reference resolution is used to interpret 
the phrase "all three decks" as referring to the 
lower deck, mid deck and flight deck. Plan op- 
timization finds the most efficient way to or- 
der the locations, an d paraphrases the resulting 
plan back to the user. 
USER: Go to all three decks and measure car- 
bon dioxide. ~ ' 
PSA: I will move to flight,deck, mid deck and 
then lower deck and I will measure carbon 
' dioxide level, okay? 
USER: Okay. \[PSA moves to flight deck\] 
PSA: The carbon dioxide level at the flight 
deck is one percent. \[PSA: moves to mid 
deck\] 
PSA.: The carbon dioxide level at the mid deck 
is one percent. \[PSA moves to lower deck\] 
PSA: The carbon dioxide level at the lower 
deck is one percent. '
2. The next example involves resolution of ellip- 
sis. The relevant context is recovered from the 
preceding dialogue. 
USER: Do the same for the pilot's seat. 
PSA: I will move to the pilot's seat and I will 
measure carbon dioxide level, okay? 
USER: Right. \[PSA moves to pilot's seat\] 
PSA: The carbon dioxide level at the pilot's 
seat is one percent. 
3. Resolution of ellipsis can involve filling in un- 
specified semantic parameters toa syntactically 
complete command. It is not meaningful to talk 
about a sensor eading without reference to the 
place at which that reading was carried out; in 
the following example, resolution assumes that 
the location in question is the one most recently 
mentioned in the dialogue. 
Note also that since the PSA does not have to 
move in order to carry out the command, and 
. 
. 
. 
. 
. 
hence can execute it immediately, the DM rea- 
sonably decides to skip the confirmation ques- 
tion. This decision is made based on the meta- 
Output encoding the estimated cost of executing 
the :script. 
' i  
USER: What was the carbon dioxide level at 
fifteen oh five according to the fixed sen- 
sors? 
PSA: According to the fixed sensors, at fifteen 
oh five the carbon dioxide level at the pi- 
lot's seat was one percent. -. 
The system encounters a user misconception, 
which results in a "presupposition failure" 
meta-output being produced during reference 
resolutiom It ?onsequently decides to alert the 
user to this, rather than attempt o carry out 
the command. 
USER: Close both doors. 
PSA: There are in fact three of them. 
The user utterance includes a pronoun, "it". 
The system resolves "it" to. "the crew hatch", 
and issues an appropriate paraphrase in order 
to confirm that this is what the user meant. 
USER: Go to crew hatch and close it. 
PSA: I will go to crew hatch and I will close 
crew hatch, okay? 
USER: Yeah. \[PSA moves to crew hatch and 
closes it\] 
The user asks the system to close the door, but 
it is unclear which door they are referring to. 
An "ambiguous reference" meta-output is pro- 
duced by reference resolution, resulting in the 
DM issuing a clarification question. 
USER: Close the door. 
PSA: Which door do you mean? 
The user response is successfully interpreted by 
reference resolution as an instance of ellipsis. 
Trying to close the crew hatch a second time 
however causes a presupposition failure meta- 
output o be generated during plan evaluation. 
The system informs the user of this and takes 
no action. 
USER: The crew hatch. 
PSA: The crew hatch is already closed. 
The user presents three locations in a subopti- 
mal order. Plan evaluation finds a more efficient 
ordering. 
USER: Move to storage lockers, commander's 
seat and flight deck and measure tempera- 
ture. 
PSA: I will move to flight deck, commander's 
seat and then storage lockers and I will 
measure temperature, okay? 
USER: Sure. 
\[PSA moves to flight deck, commander's 
seat and storage lockers in that order and 
reports temperature at each location\] 
9. The user asks the system to repeat he previous 
command. Since the PSA is now starting from 
a different position, its most efficient plan is no 
longer the same, and it informs the user of this. 
USER: Do that again. 
' PSA:  I will move to storage lockers, comman- 
der's seat and then flight deck and I will 
measure temperature, okay? 
USER: Yes. \[PSA moves to storage lockers\] 
PSA: The temperature at the storage lockers 
is 19.9 degrees Celsius. \[PSA starts moving 
to commander's seat\] 
10. The user interrupts the PSA in the middle of the 
plan, and asks it to go back to its last stopping 
place. Since "stop" may involve a potentially 
hazardous condition, the interface obeys imme- 
diately, without attempting to confirm first. 
USER: Stop. \[PSA stops\] 
USER: Go back. \[PSA returns to storage lock- 
ers\] 
6 Summary  
We have described a generic architecture for spoken 
language dialogue interfaces to semi-autonomous 
agents, based on the standard notion of translating 
to successively deeper levels of representation. The 
novel aspects of our scheme center around two ideas: 
first, that the final output representations are best 
conceptualized not as logical expressions but rather 
as programs in a scripting language; second, that 
steps in the translation process hould produce not 
only a simple output, but also meta-information de-
scribing how the output was produced. We have pre- 
sented examples uggesting how several apparently 
diverse types of dialogue behavior can be captured 
simply within our framework, and outlined a proto- 
type implementation f the scheme. 
References 
N. Badler, R. Bindiganavale, J. Bourne, J. Allbeck, 
J. Shi, and M. Palmer. 1999. Real time virtual 
humans. In International Conference on Digital 
Media Futures. 
J. Dowding, M. Gawron, D. Appelt, L. Cherny, 
R. Moore, and D. Moran. 1993. Gemini: A nat- 
ural language system for spoken language un- 
derstanding. In Proceedings of the Thirty-First 
59 
Annual Meeting of the Association for Computa- 
tional Linguistics. 
K. Konolige, K. Myers, E. Ruspini, and A. Saf- 
fiotti. 1993. Flakey in action: The 1992 AAAI 
robot competition. Technical Report SRI Techni- 
cal Note 528, SRI, AI Center, SRI International, 
333 Ravenswood Ave., Menlo Park, CA 94025. 
D. Martin, A. Cheyer, and D. Moran. 1998. Build- 
ing distributed software systems with the open 
agent architecture. In Proceedings of the Third 
International Conference on the Practical Appli- 
cation of Intelligent Agents and Multi-Agent Tech~ 
nology. 
R. Moore, J. Dowding, H. Bratt, J. Gawron, 
Y. Gorfu, and A. Cheyer. 1997. CommandTalk: 
A spoken:language interface for battlefield simu- 
lations. In ProCeedings of the Fifth Conference on 
Applied NaturaiLanguage Processing, pages 1-7. 
Nuance, 2000. Nuance Communications, Inc. 
http://www.nuance.com. As of 9 March 2000. 
D. Perzanowski, A. Schultz, and W. Adams. 1998. 
Integrating natural language and gesture in a 
robotics domain. In IEEE International Sympo- 
sium on Intelligent Control: ISIC/CIRA//ISAS 
Joint Conference, pages 247-252, Gaithersburg, 
MD: National Institute of Standards and Tech- 
nology. 
D. Perzanowski, A. Schultz, W. Adams, and 
E. Marsh. 1999. Goal tracking in a natural an- 
guage interface: Towards achieving adjustable au- 
tonomy. In ISIS/CIRA99 Conference, Monterey, 
CA. IEEE. 
PSA, 2000. Personal Satellite Assistant (PSA) 
Project. http://ic.arc.nasa.gov/ic/psa/. As of 9 
March 2000. 
D. Pym, L. Pryor, and D. Murphy. 1995. Actions 
as processes: a position on planning. In Working 
Notes, AAAI Symposium on Extending Theories 
of Action, pages 169-173. 
R. W. Smith. 1997. An evaluation of strategies for 
selective utterance verification for spoken natural 
language dialog. In Proceedings ofthe Fifth Con- 
ference on Applied Natural Language Processing, 
pages 41-48. 
A. Stent, J. Dowding, J. Gawron, E. Bratt, and 
R. Moore. 1999. The CommandTalk spoken di- 
alogue system. In Proceedings of the Thirty- 
Seventh Annual Meeting of the Association for 
Computational Linguistics, pages 183-190. 
D. R. Traum and J. Allen. 1994. Discourse obliga- 
tions in dialogue processing. In Proceedings ofthe 
Thirty-Second Annual Meeting of the Association 
for Computational Linguistics, pages 1-8. 
D. R. Traum and C. F. Andersen. 1999. Represen- 
tations of dialogue state for domain and task inde- 
pendent meta-dialogue. In Proceedings of the I J- 
CAI'99 Workshop on KnowTedge and Reasoning 
in Practical Dialogue Systems, pages 113-120. 
J. van Eijck and R. Moore. 1992. Semantic rules 
for English. In H. Alshawi, editor, The Core Lan- 
guage Engine. MIT Press. ~ 
B. Webber. 1995. Instructing animated agents: 
Viewing language in behavioral terms. In Proceed- 
ings of the International Conference on Coopera- 
tive Multi-modal Communication. 
T. A. Winograd. 1973. A procedural model of lan- 
guage understanding. In R. C. Shank and K. M. 
Colby, editors, Computer Models of Thought and 
Language. Freeman, San Francisco, CA. 
i 
60 
