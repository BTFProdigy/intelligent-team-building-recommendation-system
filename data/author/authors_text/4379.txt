Proceedings of SPEECHGRAM 2007, pages 49?52,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Development Environment
for Building Grammar-Based Speech-Enabled Applications
Elisabeth Kron1, Manny Rayner1,2, Marianne Santaholma1, Pierrette Bouillon1
1 University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
elisabethkron@yahoo.co.uk
Marianne.Santaholma@eti.unige.ch
Pierrette.Bouillon@issco.unige.ch
2 Powerset, Inc.
475 Brannan Street
San Francisco, CA 94107
manny@powerset.com
Abstract
We present a development environment for
Regulus, a toolkit for building unification
grammar-based speech-enabled systems, fo-
cussing on new functionality added over the
last year. In particular, we will show an
initial version of a GUI-based top-level for
the development environment, a tool that
supports graphical debugging of unification
grammars by cutting and pasting of deriva-
tion trees, and various functionalities that
support systematic development of speech
translation and spoken dialogue applications
built using Regulus.
1 The Regulus platform
The Regulus platform is a comprehensive toolkit
for developing grammar-based speech-enabled sys-
tems that can be run on the commercially avail-
able Nuance recognition environment. The plat-
form has been developed by an Open Source con-
sortium, the main partners of which have been
NASA Ames Research Center and Geneva Uni-
versity, and is freely available for download from
the SourceForge website1. Regulus has been used
to build several large systems, including Geneva
University?s MedSLT medical speech translator
(Bouillon et al, 2005) and NASA?s Clarissa proce-
dure browser (Rayner et al, 2005b)2.
Regulus is described at length in
(Rayner et al, 2006), the first half of which consists
of an extended tutorial introduction. The release
1http://sourceforge.net/projects/regulus/
2http://ic.arc.nasa.gov/projects/clarissa/
also includes extensive online documentation,
including several example applications.
The core functionality offered by Regulus is com-
pilation of typed unification grammars into parsers,
generators, and Nuance-formatted CFG language
models, and hence also into Nuance recognition
packages. Small unification grammars can be com-
piled directly into executable forms. The central
idea of Regulus, however, is to base as much of
the development work as possible on large, domain-
independent resource grammars. A resource gram-
mar for English is available from the Regulus web-
site; similar grammars for several other languages
have been developed under the MedSLT project at
Geneva University, and can be downloaded from the
MedSLT SourceForge website3.
Large resource grammars of this kind are over-
general as they stand, and it is not possible to com-
pile them directly into efficient recognisers or gener-
ators. The platform, however, provides tools, driven
by small corpora of examples, that can be used to
create specialised versions of these general gram-
mars using the Explanation Based Learning (EBL)
algorithm. We have shown in a series of exper-
iments that suitably specialised grammars can be
compiled into efficient executable forms. In particu-
lar, recognisers built in this way are very competitive
with ones created using statistical training methods
(Rayner et al, 2005a).
The Regulus platform also supplies a framework
for using the compiled resources ? parsers, gen-
erators and recognisers ? to build speech transla-
tion and spoken dialogue applications. The envi-
ronment currently supports 75 different commands,
3http://sourceforge.net/projects/medslt
49
which can be used to carry out a range of func-
tions including compilation of grammars into var-
ious forms, debugging of grammars and compiled
resources, and testing of applications. The environ-
ment exists in two forms. The simpler one, which
has been available from the start of the project, is a
command-line interface embedded within the SICS-
tus Prolog top-level. The focus will however be on
a new GUI-based environment, which has been un-
der development since late 2006, and which offers
a more user-friendly graphical/menu-based view of
the underlying functionality.
In the rest of the paper, we outline how Regulus
supports development both at the level of grammars
(Section 2), and at the level of the applications that
can be built using the executable forms derived from
them (Section 3).
2 Developing unification grammars
The Regulus grammar development toolset borrows
ideas from several other systems, in particular the
SRI Core Language Engine (CLE) and the Xerox
Language Engine (XLE). The basic functionalities
required are uncontroversial. As usual, the Regulus
environment lets the user parse example sentences
to create derivation trees and logical forms; in the
other direction, if the grammar has also been com-
piled into a generator, the user can take a logical
form and use it to generate a surface string and an-
other derivation tree. Once a derivation tree has been
created, either through parsing or through genera-
tion, it is possible to examine individual nodes to
view the information associated with each one. Cur-
rently, this information consists of the syntactic fea-
tures, the piece of logical form built up at the node,
and the grammar rule or lexical entry used to create
it.
The Regulus environment also provides a more
elaborate debugging tool, which extends the ear-
lier ?grammar stepper? implemented under the CLE
project. Typically, a grammar development problem
has the following form. The user finds a bad sen-
tence B which fails to get a correct parse; however,
there are several apparently similar or related sen-
tences G1...Gn which do get correct parses. In most
cases, the explanation is that some rule which would
appear in the intended parse for B has an incorrect
feature-assignment.
A simple strategy for investigating problems of
this kind is just to examine the structures of B and
G1...Gn by eye, and attempt to determine what the
crucial difference is. An experienced developer,
who is closely familiar with the structure of the
grammar, will quite often be able to solve the prob-
lem in this way, at least in simple cases. ?Solving
by inspection? is not, however, very systematic, and
with complex rule bugs it can be hard even for ex-
perts to find the offending feature assignment. The
larger the grammar becomes, especially in terms of
the average number of features per category, the
more challenging the ad hoc debugging approach
becomes.
A more systematic strategy was pioneered in the
CLE grammar stepper. The developer begins by
looking at the working examples G1...Gn, to de-
termine what the intended correct structure would
be for B. They then build up the corresponding
structure for the bad example, starting at the bot-
tom with the lexical items and manually selecting
the rules used to combine them. At some point, a
unification will fail, and this will normally reveal the
bad feature assignment. The problem is that manual
bottom-up construction of the derivation tree is very
time-consuming, since even quite simple trees will
usually have at least a dozen nodes.
The improved strategy used in the Regulus gram-
mar stepper relies on the fact that the G1...Gn can
usually be constructed to include all the individual
pieces of the intended derivation tree for B, since in
most cases the feature mis-match arises when com-
bining two subtrees which are each internally con-
sistent. We exploit this fact by allowing the devel-
oper to build up the tree for B by cutting up the trees
for G1...Gn into smaller pieces, and then attempting
to recombine them. Most often, it is enough to take
two of the Gi, cut an appropriate subtree out of each
one, and try to unify them together; this means that
the developer can construct the tree for B with only
five operations (two parses, two cuts, and a join),
rather than requiring one operation for each node in
B, as in the bottom-up approach.
A common pattern is that B and G1 are identical,
except for one noun-phrase constituent NP , and G2
consists of NP on its own. To take an example from
the MedSLT domain, B could be ?does the morning
50
Figure 1: Example of using the grammar stepper to discover a feature mismatch. The window on the
right headed ?Stepper? presents the list of available trees, together with the controls. The windows headed
?Tree 1? and ?Tree 4? present the trees for item 1 (?does red wine give you headaches?) and item 4 (?the
morning?). The popup window on the lower right presents the feature mismatch information.
give you headaches??, G1 the similar sentence ?does
red wine give you headaches?? and G2 the single
NP ?the morning?. We cut out the first NP subtree
from G1 to produce what is in effect a tree with an
NP ?slash category?, that can be rendered as ?does
NP give you headaches??; call this G?1. We then cut
out the single NP subtree (this accounts for most,
but not all, of the derivation) from G2, to produce
G?2. By attempting to unify G?2 with the NP ?hole?
left in G?1, we can determine the exact nature of the
feature mismatch. We discover that the problem is
in the sortal features: the value of the sortal feature
on G?2 is time, but the corresponding feature-value
in the NP ?hole? is action\/cause.
Figure 1 contains a screenshot of the development
environment in the example above, showing the state
when the feature mismatch is revealed. A detailed
example, including screenshots for each step, is in-
cluded in the online Regulus GUI tutorial4.
4Available in the file doc/RegulusGUITutorial.pdf from the
SourceForge Regulus website
3 Developing applications
The Regulus platform contains support for both
speech translation and spoken dialogue applications.
In each case, it is possible to run the development
top-loop in a mode appropriate to the type of appli-
cation, including carrying out systematic regression
testing using both text and speech input. For both
types of application, the platform assumes a uniform
architecture with pre-specified levels of representa-
tion.
Due to shortage of space, and because it is the
better-developed of the two, we focus on speech
translation. The framework is interlingua-based,
and also permits simple context-based translation
involving resolution of ellipsis5. Processing goes
through the following sequence of representations:
1. Spoken utterance in source language.
2. Recognised words in source language.
5Although it is often possible to translate ellipsis as ellipsis
in closely related language pairs, this is usually not correct in
more widely separated ones.
51
3. Source logical form. Source logical form and
all other levels of representation are (almost)
flat lists of attribute/value pairs.
4. ?Source discourse representation?. A regu-
larised version of the source logical form, suit-
able for carrying out ellipsis resolution.
5. ?Resolved source discourse representation?.
The output resulting from carrying out any nec-
essary ellipsis processing on the source dis-
course representation. Typically this will add
material from the preceding context represen-
tation to create a representation of a complete
clause.
6. Interlingua. A language-independent version
of the representation.
7. Target logical form.
8. Surface words in target language.
The transformations from source logical form
to source discourse representation, from resolved
source discourse representation to interlingua, and
from interlinga to target logical form are defined
using translation rules which map lists of at-
tribute/value pairs to lists of attribute/value pairs.
The translation trace includes all the levels of rep-
resentation listed above, the translation rules used at
each stage, and other information omitted here. The
?translation mode? window provided by the devel-
opment environment makes all these fields available
in a structured form which allows the user to select
for display only those that are currently of interest.
The framework for spoken dialogue systems is simi-
lar, except that in the last three steps ?Interlingua? is
replaced by ?Dialogue move?, ?Target logical form?
by ?Abstract response?, and ?Surface words in target
language? by ?Concrete response?.
The platform contains tools for performing sys-
tematic regression testing of both speech translation
and spoken dialogue applications, using both text
and speech input. Input in the required modality is
taken from a specified file and passed through all
stages of processing, with the output being written
to another file. The user is able to annotate the re-
sults with respect to correctness (the GUI presents
a simple menu-based interface for doing this) and
save the judgements permanently, so that they can
be reused for future runs.
The most interesting aspects of the framework
involve development of spoken dialogue systems.
With many other spoken dialogue systems, the ef-
fect of a dialogue move is distributed throughout the
program state, and true regression testing is very dif-
ficult. Here, our side-effect free approach to dia-
logue management means that the DM can be tested
straightforwardly as an isolated component, since
the context is fully encapsulated as an object. The
theoretical issues involved are explored further in
(Rayner and Hockey, 2004).
References
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Rayner and Hockey2004] M. Rayner and B.A. Hockey.
2004. Side effect free dialogue management in a voice
enabled procedure browser. In Proceedings of the 8th
International Conference on Spoken Language Pro-
cessing (ICSLP), Jeju Island, Korea.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. Methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the international
space station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
52
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 32?35
Manchester, August 2008
The 2008 MedSLT System
Manny Rayner1, Pierrette Bouillon1, Jane Brotanek2, Glenn Flores2
Sonia Halimi1, Beth Ann Hockey3, Hitoshi Isahara4, Kyoko Kanzaki4
Elisabeth Kron5, Yukie Nakao6, Marianne Santaholma1
Marianne Starlander1, Nikos Tsourakis1
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon,Nikolaos.Tsourakis}@issco.unige.ch
{Sonia.Halimi,Marianne.Santaholma,Marianne.Starlander}@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
4 NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
{isahara,kanzaki}@nict.go.jp
5 3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
6 University of Nantes, LINA, 2, rue de la Houssinie`re, BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
MedSLT is a grammar-based medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several dif-
ferent subdomains and multiple language
pairs. Vocabulary ranges from about 350 to
1000 surface words, depending on the lan-
guage and subdomain. We will demo three
different versions of the system: an any-
to-any multilingual version involving the
languages Japanese, English, French and
Arabic, a bidirectional English ? Span-
ish version, and a mobile version run-
ning on a hand-held PDA. We will also
demo the Regulus development environ-
ment, focussing on features which sup-
port rapid prototyping of grammar-based
speech translation systems.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
MedSLT is a medium-vocabulary grammar-based
medical speech translation system built on top of
the Regulus platform (Rayner et al, 2006). It is
intended for use in doctor-patient diagnosis dia-
logues, and provides coverage of several subdo-
mains and a large number of different language-
pairs. Coverage is based on standard examina-
tion questions obtained from physicians, and fo-
cusses primarily on yes/no questions, though there
is also support for WH-questions and elliptical ut-
terances.
Detailed descriptions of MedSLT can be found
in earlier papers (Bouillon et al, 2005; Bouil-
lon et al, 2008)1. In the rest of this note, we
will briefly sketch several versions of the system
that we intend to demo at the workshop, each of
which displays new features developed over the
last year. Section 2 describes an any-language-to-
any-language multilingual version of the system;
Section 3, a bidirectional English ? Spanish ver-
sion; Section 4, a version running on a mobile PDA
1All MedSLT publications are available on-line
at http://www.issco.unige.ch/projects/
medslt/publications.shtml.
32
platform; and Section 5, the Regulus development
environment.
2 A multilingual version
During the last few months, we have reorganised
the MedSLT translation model in several ways2. In
particular, we give a much more central role to the
interlingua; we now treat this as a language in its
own right, defined by a normal Regulus grammar,
and using a syntax which essentially amounts to
a greatly simplified form of English. Making the
interlingua into another language has made it easy
to enforce tight constraints on well-formedness of
interlingual semantic expressions, since checking
well-formedness now just amounts to performing
generation using the interlingua grammar.
Another major advantage of the scheme is that
it is also possible to systematise multilingual de-
velopment, and only work with translation from
source language to interlingua, and from interlin-
gua to target language; here, the important point
is that the human-readable interlingua surface syn-
tax makes it feasible in practice to evaluate transla-
tion between normal languages and the interlingua.
Development of rules for translation to interlingua
is based on appropriate corpora for each source
language. Development of rules for translating
from interlingua uses a corpus which is formed by
merging together the results of translating each of
the individual source-language corpora into inter-
lingua.
We will demonstrate our new capabilities in
interlingua-based translation, using a version of
the system which translates doctor questions in the
headache domain from any language to any lan-
guage in the set {English, French, Japanese, Ara-
bic}. Table 1 gives examples of the coverage of the
English-input headache-domain version, and Ta-
ble 2 summarises recognition performance in this
domain for the three input languages where we
have so far performed serious evaluations. Differ-
ences in the sizes of the recognition vocabularies
are primarily due to differences in use of inflec-
tion.
3 A bidirectional version
The system from the preceding section is unidi-
rectional; all communication is in the doctor-to-
patient direction, the expectation being that the pa-
2The ideas in the section are described at greater length in
(Bouillon et al, 2008).
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognis-
ers. ?Vocab? = number of surface words in source
language recogniser vocabulary; ?WER? = Word
Error Rate for source language recogniser, on in-
coverage material; ?SemER? = semantic error rate
for source language recogniser, on in-coverage
material.
tient will respond non-verbally. Our second demo,
an early version of which is described in (Bouillon
et al, 2007), supports bidirectional translation for
the sore throat domain, in the English ? Spanish
pair. Here, the English-speaking doctor typically
asks WH-questions, and the Spanish-speaking pa-
tient responds with elliptical utterances, which are
translated as full sentence responses. A short ex-
ample dialogue is shown in Table 3.
Doctor: Where is the pain?
?Do?nde le duele?
Patient: En la garganta.
I experience the pain in my throat.
Doctor: How long have you had a pain
in your throat?
?Desde cua?ndo le duele la garganta?
Patient: Ma?s de tres d??as.
I have experienced the pain in my
throat for more than three days.
Table 3: Short dialogue with bidirectional English
? Spanish version. System translations are in ital-
ics.
4 A mobile platform version
When we have shown MedSLT to medical profes-
sionals, one of the most common complaints has
been that a laptop is not an ideal platform for use
in emergency medical situations. Our third demo
shows an experimental version of the system us-
ing a client/server architecture. The client, which
contains the user interface, runs on a Nokia Linux
N800 Internet Tablet; most of the heavy process-
ing, including in particular speech recognition, is
hosted on the remote server, with the nodes com-
municating over a wireless network. A picture of
33
Where? Is the pain above your eye?
When? Have you had the pain for more than a month?
How long? Does the pain typically last a few minutes?
How often? Do you get headaches several times a week?
How? Is it a stabbing pain?
Associated symptoms? Do you vomit when you get the headaches?
Why? Does bright light make the pain worse?
What helps? Does sleep make the pain better?
Background? Do you have a history of sinus disease?
Table 1: Examples of English MedSLT coverage
the tablet, showing the user interface, is presented
in Figure 1. The sentences appearing under the
back-translation at the top are produced by an on-
line help component, and are intended to guide the
user into the grammar?s coverage (Chatzichrisafis
et al, 2006).
The architecture is described further in
(Tsourakis et al, 2008), which also gives perfor-
mance results for another Regulus applications.
These strongly suggest that recognition perfor-
mance in the client/server environment is no
worse than on a laptop, as long as a comparable
microphone is used.
5 The development environment
Our final demo highlights the new Regulus devel-
opment environment (Kron et al, 2007), which has
over the last few months acquired a large amount
of new functionality designed to facilitate rapid
prototyping of spoken language applications3 . The
developer initially constructs and debugs her com-
ponents (grammar, translation rules etc) in a text
view. As soon as they are consistent, she is able
to compile the source-language grammar into a
recogniser, and combine this with other compo-
nents to run a complete speech translation system
within the development environment. Connections
between components are defined by a simple con-
fig file. Figure 2 shows an example.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
3This work is presented in a paper currently under review.
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Tsourakis, N., M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
34
Figure 1: Mobile version of the MedSLT system, running on a Nokia tablet.
Figure 2: Speech to speech translation from the development environment, using a Japanese to Arabic
translator built from MedSLT components. The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
35
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 9?16
Manchester, August 2008
Making Speech Look Like Text
in the Regulus Development Environment
Elisabeth Kron
3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
Manny Rayner, Marianne Santaholma, Pierrette Bouillon, Agnes Lisowska
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Marianne.Santaholma@eti.unige.ch
Pierrette.Bouillon@issco.unige.ch
Agnes.Lisowska@issco.unige.ch
Abstract
We present an overview of the de-
velopment environment for Regulus, an
Open Source platform for construction of
grammar-based speech-enabled systems,
focussing on recent work whose goal has
been to introduce uniformity between text
and speech views of Regulus-based appli-
cations. We argue the advantages of be-
ing able to switch quickly between text and
speech modalities in interactive and offline
testing, and describe how the new func-
tionalities enable rapid prototyping of spo-
ken dialogue systems and speech transla-
tors.
1 Introduction
Sex is not love, as Madonna points out at the be-
ginning of her 1992 book Sex, and love is not
sex. None the less, even people who agree with
Madonna often find it convenient to pretend that
these two concepts are synonymous, or at least
closely related. Similarly, although text is not
speech, and speech is not text, it is often conve-
nient to pretend that they are both just different as-
pects of the same thing.
In this paper, we will explore the similarities and
differences between text and speech, in the con-
crete setting of Regulus, a development environ-
ment for grammar based spoken dialogue systems.
Our basic goal will be to make text and speech
processing as similar as possible from the point
of view of the developer. Specifically, we arrange
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
things so that the developer is able to develop her
system using a text view; she will write text-based
rules, and initially test the system using text in-
put and output. At any point, she will be able to
switch to a speech view, compiling the text-based
processing rules into corresponding speech-based
versions, and test the resulting speech-based sys-
tem using speech input and output.
Paradoxically, the reason why it is so important
to be able to switch seamlessly between text and
speech viewpoints is that text and speech are in
fact not the same. For example, a pervasive prob-
lem in speech recognition is that of easily confus-
able pairs of words. This type of problem is of-
ten apparent after just a few minutes when running
the system in speech mode (the recogniser keeps
recognising one word as the other), but is invis-
ible in text mode. More subtly, some grammar
problems can be obvious in text mode, but hard
to see in speech mode. For instance, articles like
?the? and ?a? are short, and usually pronounced
unstressed, which means that recognisers can be
reasonably forgiving about whether or not to hy-
pothesise them when they are required or not re-
quired by the recognition grammar. In text mode, it
will immediately be clear if the grammar requires
an article in a given NP context: incorrect vari-
ants will fail to parse. In speech mode, the symp-
toms are far less obvious, and typically amount to
no more than a degradation in recognition perfor-
mance.
The rest of the paper is structured as follows.
Sections 2 and 3 provide background on the Reg-
ulus platform and development cycle respectively.
Section 4 describes speech and text support in the
interactive development environment, and 5 de-
scribes how the framework simplifies the task of
9
switching between modalities in regression testing.
Section 6 concludes.
2 The Regulus platform
The Regulus platform is a comprehensive toolkit
for developing grammar-based speech-enabled
systems that can be run on the commercially avail-
able Nuance recognition environment. The plat-
form has been developed by an Open Source con-
sortium, the main partners of which have been
NASA Ames Research Center and Geneva Univer-
sity, and is freely available for download from the
SourceForge website1. In terms of ideas (though
not code), Regulus is a descendent of SRI Inter-
national?s CLE and Gemini platforms (Alshawi,
1992; Dowding et al, 1993); other related systems
are LKB (Copestake, 2002), XLE (Crouch et al,
2008) and UNIANCE (Bos, 2002).
Regulus has already been used to build sev-
eral large applications. Prominent examples
are Geneva University?s MedSLT medical speech
translator (Bouillon et al, 2005), NASA?s Clarissa
procedure browser (Rayner et al, 2005) and Ford
Research?s experimental SDS in-car spoken dia-
logue system, which was awarded first prize at
the 2007 Ford internal demo fair. Regulus is de-
scribed at length in (Rayner et al, 2006), the first
half of which consists of an extended tutorial in-
troduction. The release includes a command-line
development environment, extensive online docu-
mentation, and several example applications.
The core functionality offered by Regulus is
compilation of typed unification grammars into
parsers, generators, and Nuance-formatted CFG
language models, and hence also into Nuance
recognition packages. These recognition packages
produced by Regulus can be invoked through the
Regulus SpeechServer (?Regserver?), which pro-
vides an interface to the underlying Nuance recog-
nition engine. The value added by the Regserver
is to provide a view of the recognition process
based on the Regulus unification grammar frame-
work. In particular, recognition results, originally
produced in the Nuance recognition platform?s in-
ternal format, are reformatted into the semantic no-
tation used by the Regulus grammar formalism.
There is extensive support within the Regulus
toolkit for development of both speech translation
and spoken dialogue applications. Spoken dia-
1http://sourceforge.net/projects/
regulus/
logue applications (Rayner et al, 2006, Chapter 5)
use a rule-based side-effect free state update model
similar in spirit to that described in (Larsson and
Traum, 2000). Very briefly, there are three types of
rules: state update rules, input management rules,
and output management rules. State update rules
take as input the current state, and a ?dialogue
move?; they produce as output a new state, and an
?abstract action?. Dialogue moves are abstract rep-
resentations of system inputs; these inputs can ei-
ther be logical forms produced by the grammar, or
non-speech inputs (for example, mouse-clicks in a
GUI). Similarly, abstract actions are, as the name
suggests, abstract representations of the concrete
actions the dialogue system will perform, for ex-
ample speaking or updating a visual display. Input
management rules map system inputs to dialogue
moves; output management rules map abstract ac-
tions to system outputs.
Speech translation applications are also rule-
based, using an interlingua model (Rayner et al,
2006, Chapter 6). The developer writes a second
grammar for the target language, using Regulus
tools to compile it into a generator; mappings from
source representation to interlingua, and from in-
terlingua to target representation, are defined by
sets of translation rules. The interlingua itself is
specified using a third Regulus grammar (Bouillon
et al, 2008).
To summarise, the core of a Regulus application
consists of several different linguistically oriented
rule-sets, some of which can be interpreted in ei-
ther a text or a speech modality, and all of which
need to interact correctly together. In the next sec-
tion, we describe how this determines the nature of
the Regulus development cycle.
3 The Regulus development cycle
Small unification grammars can be compiled di-
rectly into executable forms. The central idea
of Regulus, however, is to base as much of
the development work as possible on large,
domain-independent, linguistically motivated re-
source grammars. A resource grammar for En-
glish is available from the Regulus website; similar
grammars for several other languages have been
developed under the MedSLT project at Geneva
University, and can be downloaded from the Med-
SLT SourceForge website2. Regulus contains
2http://sourceforge.net/projects/
medslt
10
an extensive set of tools that permit specialised
domain-specific grammars to be extracted from the
larger resource grammars, using example-based
methods driven by small corpora (Rayner et al,
2006, Chapter 7). At the beginning of a project,
these corpora can consist of just a few dozen exam-
ples; for a mature application, they will typically
have grown to something between a few hundred
and a couple of thousand sentences. Specialised
grammars can be compiled by Regulus into effi-
cient recognisers and generators.
As should be apparent from the preceding de-
scription, the Regulus architecture is designed to
empower linguists to the maximum possible ex-
tent, in terms of increasing their ability directly
to build speech enabled systems; the greater part
of the core development teams in the large Reg-
ulus projects mentioned in Section 1 have indeed
come from linguistics backgrounds. Experience
with Regulus has however shown that linguists are
not quite as autonomous as they are meant to be,
and in particular are reluctant to work directly with
the speech view of the application. There are sev-
eral reasons.
First, non-toy Regulus projects require a range
of competences, including both software engineer-
ing and linguistics. In practice, linguist rule-
writers have not been able to test their rules in
the speech view without writing glue code, scripts,
and other infrastructure required to tie together the
various generated components. These are not nec-
essarily things that they want to spend their time
doing. The consequence can easily be that the lin-
guists end up working exclusively in the text view,
and over-refine the text versions of the rule-sets.
From a project management viewpoint, this results
in bad prioritisation decisions, since there are more
pressing issues to address in the speech view.
A second reason why linguist rule-writers have
been unhappy working in the speech view is the
lack of reproducibility associated with speech in-
put. One can type ?John loves Mary? into a text-
processing system any number of times, and ex-
pect to get the same result. It is much less reason-
able to expect to get the same result each time if
one says ?John loves Mary? to a speech recogniser.
Often, anomalous results occur, but cannot be de-
bugged in a systematic fashion, leading to general
frustration. The result, once again, is that linguists
have preferred to stick with the text view, where
they feel at home.
Yet another reason why rule-writers tend to
limit themselves to the text view is simply the
large number of top-level commands and inter-
mediate compilation results. The current Regulus
command-line environment includes over 110 dif-
ferent commands, and compilation from the initial
resource grammar to the final Nuance recognition
package involves creating a sequence of five com-
pilation steps, each of which requires the output
created by the preceding one. This makes it diffi-
cult for novice users to get their bearings, and in-
creases their cognitive load. Additionally, once the
commands for the text view have been mastered,
there is a certain temptation to consider that these
are enough, since the text and speech views can
reasonably be perceived as fairly similar.
In the next two sections, we describe an en-
hanced development environment for Regulus,
which addresses the key problems we have just
sketched. From the point of view of the linguist
rule-writer, we want speech-based development to
feel more like text-based development.
4 Speech and text in the online
development environment
The Regulus GUI (Kron et al, 2007) is intended
as a complete redesign of the development envi-
ronment, which simultaneously attacks all of the
central issues. Commands are organised in a struc-
tured set of functionality-based windows, each of
which has an appropriate set of drop-down menus.
Following normal GUI design practice (Dix et al,
1998, Chapters 3 and 4); (Jacko and Sears, 2003,
Chapter 13), only currently meaningful commands
are executable in each menu, with the others shown
greyed out.
Both compile-time and run-time speech-related
functionality can be invoked directly from the
command menus, with no need for external scripts,
Makefiles or glue code. Focussing for the moment
on the specific case of developing a speech transla-
tion application, the rule-writer will initially write
and debug her rules in text mode. She will be able
to manipulate grammar rules and derivation trees
using the Stepper window (Figure 1; cf. also (Kron
et al, 2007)), and load and test translation rules
in the Translate window (Figure 2). As soon as
the grammar is consistent, it can at any point be
compiled into a Nuance recognition package us-
ing the command menus. The resulting recogniser,
together with other speech resources (license man-
11
Figure 1: Using the Stepper window to browse trees in the Toy1 grammar from (Rayner et al, 2006,
Chapter 4). The upper left window shows the analysis tree for ?switch on the light in the kitchen?; the
lower left window shows one of the subtrees created by cutting the first tree at the higher NP node. Cut
subtrees can be recombined for debugging purposes (Kron et al, 2007).
Figure 2: Using the Translate window to test the toy English ? French translation application from
(Rayner et al, 2006, Chapter 6). The to- and from-interlingua rules used in the example are shown in the
two pop-up windows at the top of the figure.
12
regulus_config(regulus_grammar,
[toy1_grammars(toy1_declarations),
toy1_grammars(toy1_rules),
toy1_grammars(toy1_lexicon)]).
regulus_config(top_level_cat, ?.MAIN?).
regulus_config(nuance_grammar, toy1_runtime(recogniser)).
regulus_config(to_interlingua_rules,
toy1_prolog(?eng_to_interlingua.pl?)).
regulus_config(from_interlingua_rules,
toy1_prolog(?interlingua_to_fre.pl?)).
regulus_config(generation_rules, toy1_runtime(?generator.pl?)).
regulus_config(nuance_language_pack,
?English.America?).
regulus_config(nuance_compile_params, [?-auto_pron?, ?-dont_flatten?]).
regulus_config(translation_rec_params,
[package=toy1_runtime(recogniser), grammar=?.MAIN?]).
regulus_config(tts_command,
?vocalizer -num_channels 1 -voice juliedeschamps -voices_from_disk?).
Figure 3: Config file for a toy English ? French speech translation application, showing items relevant
to the speech view. Some declarations have been omitted for expositional reasons.
ager, TTS engine etc), can then be started using a
single menu command.
In accordance with the usual Regulus design
philosophy of declaring all the resources associ-
ated with a given application in its config file, the
speech resources are also specified here. Figure 3
shows part of the config file for a toy translation
application, in particular listing all the declara-
tions relevant to the speech view. If we needed to
change the speech resources, this would be done
just by modifying the last four lines. For example,
the config file as shown specifies construction of
a recogniser using acoustic models appropriate to
American English. We could change this to British
English by replacing the entry
regulus_config(nuance_language_pack,
?English.America?).
with
regulus_config(nuance_language_pack,
?English.UK?).
When the speech resources have been loaded,
the Translate window can take input equally easily
in text or speech mode; the Translate button pro-
cesses written text from the input pane, while the
Recognise button asks for spoken input. In each
case, the input is passed through the same process-
ing stages of source-to-interlingua and interlingua-
to-target translation, followed by target-language
generation. If a TTS engine or a set of recorded
target language wavfiles is specified, they are used
to realise the final result in spoken form (Figure 4).
Every spoken utterance submitted to recognition
is logged as a SPHERE-headed wavfile, in a time-
stamped directory started at the beginning of the
current session; this directory also contains a meta-
data file, which associates each recorded wavfile
with the recognition result it produced. The Trans-
late window?s History menu is constructed using
the meta-data file, and allows the user to select any
recorded utterance, and re-run it through the sys-
tem as though it were a new speech input. The
consequence is that speech input becomes just as
reproducible as text, with corresponding gains for
interactive debugging in speech mode.
5 Speech and text in regression testing
In earlier versions of the Regulus development
environment (Rayner et al, 2006, ?6.6), regres-
sion testing in speech mode was all based on
Nuance?s batchrec utility, which permits of-
fline recognition of a set of recorded wavfiles.
A test suite for spoken regression testing conse-
quently consisted of a list of wavfiles. These
were first passed through batchrec; outputs
were then post-processed into Regulus form, and
finally passed through Regulus speech understand-
ing modules, such as translation or dialogue man-
agement.
As Regulus applications grow in complexity,
this model has become increasingly inadequate,
since system input is very frequently not just a
list of monolingual speech events. In a multi-
modal dialogue system, input can consist of either
speech or screen events (text/mouse-clicks); con-
text is generally important, and the events have to
be processed in the order in which they occurred.
Dialogue systems which control real or simulated
robots, like the Wheelchair application of (Hockey
13
Figure 4: Speech to speech translation from the GUI, using a Japanese to Arabic translator built from
MedSLT components (Bouillon et al, 2008). The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
and Miller, 2007) will also receive asynchronous
inputs from the robot control and monitoring pro-
cess; once again, all inputs have to be processed in
the appropriate temporal order. A third example is
contextual bidirectional speech translation (Bouil-
lon et al, 2007). Here, the problem is slightly
different ? we have only speech inputs, but they
are for two different languages. The basic issue,
however, remains the same, since inputs have to be
processed in the right order to maintain the correct
context at each point.
With examples like these in mind, we have also
effected a complete redesign of the Regulus envi-
ronment?s regression testing facilities. A test suite
is now allowed to consist of a list of items of any
type ? text, wavfile, or non-speech input ? in any
order. Instead of trying to fit processing into the
constraints imposed by the batchrec utility, of-
fline processing now starts up speech resources in
the same way as the interactive environment, and
submits each item for appropriate processing in the
order in which it occurs. By adhering to the prin-
ciple that text and speech should be treated uni-
formly, we arrive at a framework which is simpler,
less error-prone (the underlying code is less frag-
ile) and above all much more flexible.
6 Summary and conclusions
The new functionality offered by the redesigned
Regulus top-level is not strikingly deep. In the
context of any given application, it could all
have been duplicated by reasonably simple scripts,
which linked together existing Regulus compo-
nents. Indeed, much of this new functionality is
implemented using code derived precisely from
such scripts. Our observation, however, has been
that few developers have actually taken the time
to write these scripts, and that when they have
been developed inside one project they have usu-
ally not migrated to other ones. One of the things
we have done, essentially, is to generalise previ-
ously ad hoc application-dependent functionality,
and make it part of the top-level development en-
vironment. The other main achievements of the
new Regulus top-level are to organise the existing
functionality in a more systematic way, so that it is
easier to find commands, and to package it all as a
normal-looking Swing-based GUI.
Although none of these items sound dramatic,
they make a large difference to the platform?s over-
14
all usability, and to the development cycle it sup-
ports. In effect, the Regulus top-level becomes
a generic speech-enabled application, into which
developers can plug their grammars, rule-sets and
derived components. Applications can be tested in
the speech view much earlier, giving a correspond-
ingly better chance of catching bad design deci-
sions before they become entrenched. The mecha-
nisms used to enable this functionality do not de-
pend on any special properties of Regulus, and
could readily be implemented in other grammar-
based development platforms, such as Gemini and
UNIANCE, which support compilation of feature
grammars into grammar-based language models.
At risk of stating the obvious, it is also worth
pointing out that many users, particularly younger
ones who have grown up using Windows and Mac
environments, expect as a matter of course that de-
velopment platforms will be GUI-based rather than
command-line. Addressing this issue, and sim-
plifying the transition between text- and speech-
based, views has the pleasant consequence of im-
proving Regulus as a vehicle for introducing lin-
guistics students to speech technology. An initial
Regulus-based course at the University of Santa
Cruz, focussing on spoken dialogue systems, is de-
scribed in (Hockey and Christian, 2008); a similar
one, but oriented towards speech translation and
using the new top-level described here, is currently
under way at the University of Geneva. We expect
to present this in detail in a later paper.
References
Alshawi, H., editor. 1992. The Core Language Engine.
MIT Press, Cambridge, Massachusetts.
Bos, J. 2002. Compilation of unification grammars
with compositional semantics to speech recognition
packages. In Proceedings of the 19th International
Conference on Computational Linguistics, Taipei,
Taiwan.
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Copestake, A. 2002. Implementing Typed Feature
Structure Grammars. CSLI Press, Chicago.
Crouch, R., M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman, 2008. XLE Documenta-
tion. http://www2.parc.com/isl/groups/nltt/xle/doc.
As of 29 Apr 2008.
Dix, A., J.E. Finlay, G.D. Abowd, and R. Beale, edi-
tors. 1998. Human Computer Interaction. Second
ed. Prentice Hall, England.
Dowding, J., M. Gawron, D. Appelt, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proceedings of the Thirty-First Annual Meeting of
the Association for Computational Linguistics.
Hockey, B.A. and G. Christian. 2008. Zero to spoken
dialogue system in one quarter: Teaching computa-
tional linguistics to linguists using regulus. In Pro-
ceedings of the Third ACL Workshop on Teaching
Computational Linguistics (TeachCL-08), Colum-
bus, OH.
Hockey, B.A. and D. Miller. 2007. A demonstration of
a conversationally guided smart wheelchair. In Pro-
ceedings of the 9th international ACM SIGACCESS
conference on Computers and accessibility, pages
243?244, Denver, CO.
Jacko, J.A. and A. Sears, editors. 2003. The
human-computer interaction handbook: Fundamen-
tals, evolving technologies and emerging applica-
tions. Lawerence Erlbaum Associates, Mahwah,
New Jersey.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Larsson, S. and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Di-
alogue Systems Engineering, pages 323?340.
Rayner, M., B.A. Hockey, J.M. Renders,
N. Chatzichrisafis, and K. Farrell. 2005. A
voice enabled procedure browser for the interna-
tional space station. In Proceedings of the 43rd
15
Annual Meeting of the Association for Compu-
tational Linguistics (interactive poster and demo
track), Ann Arbor, MI.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
16
