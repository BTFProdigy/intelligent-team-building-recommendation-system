Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 107?115,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Modeling the Use of Graffiti Style Features to Signal Social Relations 
within a Multi-Domain Learning Paradigm 
Mario Piergallini1, A. Seza Do?ru?z2, Phani Gadde1, David Adamson1, Carolyn P. Ros?1,3  
1Language Technologies 
Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
{mpiergal,pgadde, 
dadamson}@cs.cmu.edu 
2Tilburg University, TSH, 
5000 LE Tilburg, The 
Netherlands/ 
Language Technologies 
Institute, Carnegie Mellon 
University, 5000 Forbes 
Ave.,Pittsburgh PA 15213 
a.s.dogruoz@gmail.com 
3Human-Computer 
Interaction Institute 
Carnegie Mellon University 
5000 Forbes Avenue, 
Pittsburgh PA, 15213 
cprose@cs.cmu.edu 
 
Abstract 
In this paper, we present a series of 
experiments in which we analyze the usage of 
graffiti style features for signaling personal 
gang identification in a large, online street 
gangs forum, with an accuracy as high as 83% 
at the gang alliance level and 72% for the 
specific gang.  We then build on that result in 
predicting how members of different gangs 
signal the relationship between their gangs 
within threads where they are interacting with 
one another, with a predictive accuracy as high 
as 66% at this thread composition prediction 
task.  Our work demonstrates how graffiti 
style features signal social identity both in 
terms of personal group affiliation and 
between group alliances and oppositions.  
When we predict thread composition by 
modeling identity and relationship 
simultaneously using a multi-domain learning 
framework paired with a rich feature 
representation, we achieve significantly higher 
predictive accuracy than state-of-the-art 
baselines using one or the other in isolation. 
1 Introduction 
Analysis of linguistic style in social media has 
grown in popularity over the past decade.  
Popular prediction problems within this space 
include gender classification (Argamon et al., 
2003), age classification (Argamon et al., 2007), 
political affiliation classification (Jiang & 
Argamon, 2008), and sentiment analysis (Wiebe 
et al., 2004).  From a sociolinguistic perspective, 
this work can be thought of as fitting within the 
area of machine learning approaches to the 
analysis of style (Biber & Conrad, 2009), 
perhaps as a counterpart to work by variationist 
sociolinguists in their effort to map out the space 
of language variation and its accompanying 
social interpretation (Labov, 2010; Eckert & 
Rickford, 2001).  One aspiration of work in 
social media analysis is to contribute to this 
literature, but that requires that our models are 
interpretable.  The contribution of this paper is an 
investigation into the ways in which stylistic 
features behave in the language of participants of 
a large online community for street gang 
members.  We present a series of experiments 
that reveal new challenges in modeling stylistic 
variation with machine learning approaches.  As 
we will argue, the challenge is achieving high 
predictive accuracy without sacrificing 
interpretability. 
 Gang language is a type of sociolect that has 
so far not been the focus of modeling in the area 
of social media analysis.  Nevertheless, we argue 
that the gangs forum we have selected as our 
data source provides a strategic source of data for 
exploring how social context influences stylistic 
language choices, in part because it is an area 
where the dual goals of predictive accuracy and 
interpretability are equally important. In 
particular, evidence that gang related crime may 
account for up to 80% of crime in the United 
States attests to the importance of understanding 
the social practices of this important segment of 
society (Johnsons, 2009).  Expert testimony 
attributing meaning to observed, allegedly gang-
related social practices is frequently used as 
evidence of malice in criminal investigations 
(Greenlee, 2010).  Frequently, it is police officers 
who are given the authority to serve as expert 
witnesses on this interpretation because of their 
routine interaction with gang members.  
107
Nevertheless, one must consider their lack of 
formal training in forensic linguistics (Coulthard 
& Johnson, 2007) and the extent to which the 
nature of their interaction with gang members 
may subject them to a variety of cognitive biases 
that may threaten the validity of their 
interpretation (Kahneman, 2011).   
 Gang-related social identities are known to be 
displayed through clothing, tattoos, and language 
practices including speech, writing, and gesture 
(Valentine, 1995), and even dance (Philips, 
2009).  Forensic linguists have claimed that these 
observed social practices have been over-
interpreted and inaccurately interpreted where 
they have been used as evidence in criminal trials 
and that they may have even resulted in 
sentences that are not justified by sufficient 
evidence (Greenlee, 2010).  Sociolinguistic 
analysis of language varieties associated with 
gangs and other counter-cultural groups attests to 
the challenges in reliable interpretation of such 
practices (Bullock, 1996; Lefkowitz, 1989).  If 
we as a community can understand better how 
stylistic features behave due to the choices 
speakers make in social contexts, we will be in a 
better position to achieve high predictive 
accuracy with models that are nevertheless 
interpretable.  And ultimately, our models may 
offer insights into usage patterns of these social 
practices that may then offer a more solid 
empirical foundation for interpretation and use of 
language as evidence in criminal trials. 
 In the remainder of the paper we describe our 
annotated corpus.  We then motivate the 
technical approach we have taken to modeling 
linguistic practices within the gangs forum.  
Next, we present a series of experiments 
evaluating our approach and conclude with a 
discussion of remaining challenges. 
2 The Gangs Forum Corpus 
The forum that provides data for our experiments 
is an online forum for members of street gangs. 
The site was founded in November, 2006. It was 
originally intended to be an educational resource 
compiling knowledge about the various gang 
organizations and the street gang lifestyle. Over 
time, it became a social outlet for gang members. 
There are still traces of this earlier focus in that 
there are links at the top of each page to websites 
dedicated to information about particular gangs. 
At the time of scraping its contents, it had over a 
million posts and over twelve thousand active 
users.   Our work focuses on analysis of stylistic 
choices that are influenced by social context, so 
it is important to consider some details about the 
social context of this forum.  Specifically, we 
discuss which gangs are present in the data and 
how the gangs are organized into alliances and 
rivalries.  Users are annotated with their gang 
identity at two levels of granularity, and threads 
are annotated with labels that indicate which 
gang dominates and how the participating gangs 
relate to one another.   
2.1 User-Level Annotations 
At the fine-grained level, we annotated users 
with the gang that they indicated being affiliated 
with,  including Bloods, Crips, Hoovers, 
Gangster Disciples, other Folk Nation, Latin 
Kings, Vice Lords, Black P. Stones, other People 
Nation, Trinitarios, Norte?os, and Sure?os.  
There was also an Other category for the smaller 
gangs.  For a coarser grained annotation of gang 
affiliation, we also noted the nation, otherwise 
known as gang alliance, each gang was 
associated with.   
For our experiments, a sociolinguist with 
significant domain expertise annotated the gang 
identity of 3384 users.  Information used in our 
annotation included the user?s screen name, their 
profile, which included a slot for gang affiliation, 
and the content of their posts.  We used regular 
expressions to find gang names or other 
identifiers occurring within the gang affiliation 
field and the screen names and annotated the 
users that matched.  If the value extracted for the 
two fields conflicted, we marked them as 
claiming multiple gangs.  For users whose 
affiliation could not be identified automatically, 
we manually checked their profile to see if their 
avatar (an image that accompanies their posts) or 
other fields there contained any explicit 
information.  Otherwise, we skimmed their posts 
for explicit statements of gang affiliation.   
Affiliation was unambiguously identified 
automatically for 56% of the 3384 users from 
their affiliation field.  Another 36% were 
identified automatically based on their screen 
name.  Manual inspection was only necessary in 
9% of the cases.  Users that remained ambiguous, 
were clearly fake or joke accounts, or who 
claimed multiple gangs were grouped together in 
an ?Other? category, which accounts for 6.2% of 
the total.  Thus, 94% of the users were classified 
into the 12 specific gangs mentioned above. 
108
At a coarse-grained level, users were also 
associated with a nation.  The nation category 
was inspired by the well-known gang alliances 
known as the People Nation and Folks Nation, 
which are city-wide alliances of gangs in 
Chicago. We labeled the Crips and Hoovers as a 
nation since they are closely allied gangs.  
Historically, the Hoovers began breaking away 
from the Crips and are rivals with certain subsets 
of Crips, but allies with the majority of other 
Crips gangs.  The complex inner structure of the 
Crips alliance will be discussed in Section 5 
where we interpret our quantitative results. 
There are a large number of gangs that 
comprise the People and Folks Nations. The 
major gangs within the People Nation are the 
Latin Kings, Vice Lords and Black P. Stones. 
The Folks Nation is dominated by the Gangster 
Disciples with other Folks Nation gangs being 
significantly smaller. The People Nation, Blood 
and Norte?os gangs are in a loose, national 
alliance against the opposing national alliance of 
the Folks Nation, Crips and Sure?os. Remaining 
gangs were annotated as other, such as the 
Trinitarios, that don't fit into this national 
alliance system nor even smaller alliances.   
2.2 Thread-Level Annotations 
In addition to person-level annotations of gang 
and nation, we also annotated 949 threads with 
dominant gang as well as thread composition, by 
which we mean whether the users who 
participated on the thread were only from allied 
gangs, included opposing gangs, or contained a 
mix of gangs that were neither opposing nor 
allied.  These 949 threads were ones where a 
majority of the users who posted were in the set 
of 3384 users annotated with a gang identity. 
For the dominant gang annotation at the 
gang level, we consider only participants on the 
thread for whom there was an annotated gang 
affiliation. If members of a single gang produced 
the majority of the posts in the thread, then that 
was annotated as the dominant gang of the thread. 
If no gang had a majority in the thread, it was 
instead labeled as Mixed. For dominant gang at 
the nation level, the same procedure was used, 
but instead of looking for which gang accounted 
for more of the members, we looked for which 
gang alliance accounted for the majority of users. 
For the thread composition annotation, we 
treated the Bloods, People Nation, and Norte?os 
as allied with each other as the ?Red set?.  We 
treated Crips, Hoovers, Folks Nation, and 
Sure?os as allies with each other as the ?Blue 
set?.  The Red and Blue sets oppose one another.  
The Latin Kings and Trinitarios also oppose one 
another.  Thread composition was labeled as 
Allied, Mixed or Opposing depending on the 
gangs that appeared in the thread. As with the 
dominant gang annotation, only annotated users 
were considered. If all of the posts were by users 
of the same gang or allied gangs, the thread was 
labeled as Allied.  If there were any posts from 
rival gangs, it was labeled as Opposing. 
Otherwise, it was labeled as Mixed. If the users 
were all labeled with Other as their gang it was 
also labeled as Mixed.  
3 Modeling Language Practices at the 
Feature Level 
In this section, we first describe the rich feature 
representation we developed for this work.  
Finally, we discuss the motivation for employing 
a multi-domain learning framework in our 
machine-learning experiments. 
3.1 Feature Space Design: Graffiti Style 
Features 
While computational work modeling gang-
related language practices is scant, we can learn 
lessons from computational work on other types 
of sociolects that may motivate a reasonable 
approach.  Gender prediction, for example, is a 
problem where there have been numerous 
publications in the past decade (Corney et al., 
2002; Argamon et al., 2003; Schler et al., 2005; 
Schler, 2006; Yan & Yan, 2006; Zhang et al., 
2009).  Because of the complex and subtle way 
gender influences language choices, it is a 
strategic example to motivate our work. 
 Gender-based language variation arises from 
multiple sources. Among these, it has been noted 
that within a single corpus comprised of samples 
of male and female language that the two 
genders do not speak or write about the same 
topics. This is problematic because word-based 
features such as unigrams and bigrams, which 
are very frequently used, are highly likely to pick 
up on differences in topic (Schler, 2006) and 
possibly perspective. Thus, in cases where 
linguistic style variation is specifically of 
interest, these features do not offer good 
generalizability (Gianfortoni et al., 2011). 
Similarly, in our work, members of different 
109
gangs are located in different areas associated 
with different concerns and levels of 
socioeconomic status.  Thus, in working to 
model the stylistic choices of gang forum 
members, it is important to consider how to 
avoid overfitting to content-level distinctions. 
 Typical kinds of features that have been used 
in gender prediction apart from unigram features 
include part-of-speech (POS) ngrams (Argamon 
et al., 2003), word-structure features that cluster 
words according to endings that indicate part of 
speech (Zhang et al., 2009), features that indicate 
the distribution of word lengths within a corpus 
(Corney et al., 2002), usage of punctuation, and 
features related to usage of jargon (Schler et al., 
2005). In Internet-based communication, 
additional features have been investigated such 
as usage of internet specific features including 
?internet speak? (e.g., lol, wtf, etc.), emoticons, 
and URLs (Yan & Yan, 2006).   
Transformation Origin or meaning 
b^, c^, h^, p^ ?Bloods up? Positive towards 
Bloods, Crips, Hoovers, 
Pirus, respectively 
b ? bk, c ? ck 
h ? hk, p ? pk 
Blood killer, Crip killer 
Hoover killer, Piru killer 
ck ? cc, kc Avoid use of ?ck? since it 
represents Crip killer 
o ? x, o ? ? Represents crosshairs, 
crossing out the ?0?s in a 
name like Rollin? 60s Crips 
b ? 6 Represents the six-pointed 
star. Symbol of Folk Nation 
and the affiliated Crips. 
e ? 3 Various. One is the trinity in 
Trinitario. 
s ? 5 Represents the five-pointed 
star. Symbol of People 
Nation and the affiliated 
Bloods. 
Table 1: Orthographical substitutions from gang 
graffiti symbolism 
 
 In order to place ourselves in the best position 
to build an interpretable model, our space of 
graffiti style features was designed based on a 
combination of qualitative observations of the 
gangs forum data and reading about gang 
communication using web accessible resources 
such as informational web pages linked to the 
forum and other resources related to gang 
communication (Adams & Winter, 1997; Garot, 
2007).  Specifically, in our corpus we observed 
gang members using what we refer to as graffiti 
style features to mark their identity.  Gang 
graffiti employs shorthand references to convey 
affiliation or threats (Adams & Winter, 
1997).  For example, the addition of a <k> after a 
letter representing a rival gang stands for ?killer.? 
So, writing <ck> would represent ?crip killer.? A 
summary of these substitutions can be seen in 
Table 1.  Unfortunately, only about 25% of the 
users among the 12,000 active users employ 
these features in their posts, which limits their 
ability to achieve a high accuracy, but 
nevertheless offers the opportunity to model a 
frequent social practice observed in the corpus.  
 The graffiti style features were extracted 
using a rule-based algorithm that compares 
words against a standard dictionary as well as 
using some phonotactic constraints on the 
position of certain letters.  The dictionary was 
constructed using all of the unique words found 
in the AQUAINT corpus (Graff, 2002).  If a 
word in a post did not match any word from the 
AQUAINT corpus, we tested it against each of 
the possible transformations in Table 1.  
Transformations were applied to words using 
finite state transducers.  If some combination 
transformations from that table applied to the 
observed word could produce some term from 
the AQUAINT corpus, then we counted that 
observed word as containing the features 
associated with the applied transformations. 
 The transformations were applied in the order 
of least likely to occur in normal text to the most 
likely. Since ?bk? only occurs in a handful of 
obscure words, for example, almost any 
occurrence of it can be assumed to be a 
substitution and the ?k? can safely be removed 
before the next step. By contrast, ?cc? and ?ck? 
occur in many common words so they must be 
saved for last to ensure that the final dictionary 
checks have any simultaneous substitutions 
already removed. 
 When computing values for the graffiti style 
features for a text, the value for each feature was 
computed as the number of words (tokens) that 
contained the feature divided by the total number 
of words (tokens) in the document.  We used a 
set of 13 of these features, chosen on the basis of 
how frequently they occurred and how strongly 
they distinguished gangs from one another (for 
example, substituting ?$? for ?s? was a 
transformation that was common across gangs in 
110
our qualitative analysis, and thus did not seem 
beneficial to include).  
Transformation Freq. False 
Positive 
rate 
False 
Negative 
rate 
b^, c^, h^, p^ 15103 0% 0% 
b ? bk 26923 1% 0% 
c ? ck 16144 25% 8% 
h ? hk 10053 1% 0% 
p ? pk 5669 3% 0% 
ck ? cc, kc 72086 2% 0% 
o ? x, o ? ? 13646 15% 5% 
b ? 6 2470 16% 0% 
e ? 3 8628 28% 1% 
s ? 5 13754 6% 0% 
Table 2: Evaluation of extraction of graffiti style 
features over the million post corpus 
 
 The feature-extraction approach was 
developed iteratively. After extracting the 
features over the corpus of 12,000 active users, 
we created lists of words where the features were 
detected, sorted by frequency. We then manually 
examined the words to determine where we 
observed errors occurring and then made some 
minor adjustments to the extractors.  Table 2 
displays a quantitative evaluation of the accuracy 
of the graffiti style feature extraction. 
 Performance of the style features was 
estimated for each style-feature rule.  For each 
rule, we compute a false positive and false 
negative rate.  For false positive rate, we begin 
by retrieving the list of words marked by the 
feature extraction rule containing the associated 
style marking. From the full set of words that 
matched a style feature rule, we selected the 200 
most frequently occurring word types.  We 
manually checked that complete set of word 
tokens and counted the number of misfires.  The 
false positive rate was then calculated for each 
feature by dividing the number of tokens that 
were misfires over the total number of tokens in 
the set. In all cases, we ensured that at least 55% 
of the total word tokens were covered, so 
additional words may have been examined.  
 In the case of false negatives, we started with 
the set of word types that did not match any word 
in the dictionary and also did not trigger the style 
feature rule.  Again we sorted word types in this 
list by frequency and selected the top 200 most 
frequent.  We then manually checked for missed 
instances where the associated style feature was 
used but not detected.  The false negative rate 
was then the total number of word tokens within 
this word type set divided by the total number of 
word tokens in the complete set of word types. 
 Another type of feature we used referenced 
the nicknames gangs used for themselves and 
other gangs, which we refer to as Names features.  
The intuition behind this is simple: someone who 
is a member of the Crips gang will talk about the 
Crips more often. The measure is simply how 
often a reference to a gang occurs per document. 
Some of these nicknames we included were 
gang-specific insults, with the idea that if 
someone uses insults for Crips often, they are 
likely not a Crip. The last type of reference is 
words that refer to gang alliances like the People 
Nation and Folks Nation. Members of those 
Chicago-based gangs frequently refer to their 
gang as the ?Almighty [gang name] Nation?. 
Gang Positive/Neutral 
Mentions 
Insults 
Crips crip, loc crab, ckrip, ck 
Bloods blood, damu, 
piru, ubn 
slob, bklood, 
pkiru, bk, pk 
Hoovers hoover, groover, 
crim, hgc, hcg 
snoover, 
hkoover, hk 
Gangster 
Disciples 
GD, GDN, 
Gangster 
Disciple 
gk, dk, nigka 
Folks 
Nations 
folk, folknation, 
almighty, nation 
 
People 
Nation 
people, 
peoplenation, 
almighty, nation 
 
Latin 
Kings 
alkqn, king, 
queen 
 
Black P. 
Stones 
stone, abpsn, 
moe, black p. 
 
Vice 
Lords 
vice, lord, vl, 
avln, foe, 4ch 
 
Table 3: Patterns used for gang name features.  For all 
gangs listed in the table, there are slang terms used as 
positive mentions of the gang.  For some gangs there 
are also typical insult names. 
 
We used regular expressions to capture 
occurrences of these words and variations on 
them such as the use of the orthographic 
substitutions mentioned previously, plurals, 
feminine forms, etc. Additionally, in the Blood 
and Hoover features, they sometimes use 
numbers to replace the ?o?s representing the 
street that their gang is located on. So the Bloods 
from 34th Street, say, might write ?Bl34d?. 
111
3.2 Computational Paradigm: Multi-
domain learning 
The key to training an interpretable model in our 
work is to pair a rich feature representation with 
a model that enables accounting for the structure 
of the social context explicitly.  Recent work in 
the area of multi-domain learning offers such an 
opportunity (Arnold, 2009; Daum? III, 2007; 
Finkel & Manning, 2009).  In our work, we treat 
the dominant gang of a thread as a domain for 
the purpose of detecting thread composition.  
This decision is based on the observation that 
while it is a common practice across gangs to 
express their attitudes towards allied and 
opposing gangs using stylistic features like the 
Graffiti style features, the particular features that 
serve the purpose of showing affiliation or 
opposition differ by gang.  Thus, it is not the 
features themselves that carry significance, but 
rather a combination of who is saying it and how 
it is being said. 
 As a paradigm for multi-domain learning, we 
use Daume?s Frustratingly Easy Domain 
Adaptation approach (Daum? III, 2007) as 
implemented in LightSIDE (Mayfield & Ros?, 
2013). In this work, Daum? III proposes a very 
simple ?easy adapt? approach, which was 
originally proposed in the context of adapting to 
a specific target domain, but easily generalizes to 
multi-domain learning. The key idea is to create 
domain-specific versions of the original input 
features depending on which domain a data point 
belongs to. The original features represent a 
domain-general feature space. This allows any 
standard learner to appropriately optimize the 
weights of domain-specific and domain-general 
features simultaneously.  In our work, this allows 
us to model how different gangs signal within-
group identification and across-group animosity 
or alliance using different features.  The resulting 
model will enable us to identify how gangs differ 
in their usage of style features to display social 
identity and social relations. 
 It has been noted in prior work that style is 
often expressed in a topic-specific or even 
domain-specific way (Gianfortoni et al., 2011).  
What exacerbates these problems in text 
processing approaches is that texts are typically 
represented with features that are at the wrong 
level of granularity for what is being 
modeled.  Specifically, for practical reasons, the 
most common types of features used in text 
classification tasks are still unigrams, bigrams, 
and part-of-speech bigrams, which are highly 
prone to over-fitting. When text is represented 
with features that operate at too fine-grained of a 
level, features that truly model the target style are 
not present within the model.  Thus, the trained 
models are not able to capture the style itself and 
instead capture features that correlate with that 
style within the data (Gianfortoni et al., 2011). 
 This is particularly problematic in cases 
where the data is not independent and identically 
distributed (IID), and especially where instances 
that belong to different subpopulations within the 
non-IID data have different class value 
distributions.  In those cases, the model will tend 
to give weight to features that indicate the 
subpopulation rather than features that model the 
style.   Because of this insight from prior work, 
we contrast our stylistic features with unigram 
features and our multi-domain approach with a 
single-domain approach wherever appropriate in 
our experiments presented in Section 4. 
4 Prediction Experiments 
In this section we present a series of prediction 
experiments using the annotations described in 
Section 2.  We begin by evaluating our ability to 
identify gang affiliation for individual users.  
Because we will use dominant gang as a domain 
feature in our multi-domain learning approach to 
detect thread composition, we also present an 
evaluation of our ability to automatically predict 
dominant gang for a thread.  Finally, we evaluate 
our ability to predict thread composition.  All of 
our experiments use L1 regularized Logistic 
regression. 
4.1 Predicting Gang Affiliation per User 
The first set of prediction experiments we ran 
was to identify gang affiliation.  For this 
experiment, the full set of posts contributed by a 
user was concatenated together and used as a 
document from which to extract text features.  
We conducted this experiment using a 10-fold 
cross-validation over the full set of users 
annotated for gang affiliation. Results contrasting 
alternative feature spaces at the gang level and 
nation level are displayed in Table 4.  We begin 
with a unigram feature space as the baseline.  We 
contrast this with the Graffiti style features 
described above in Section 3.1.  Because all of 
the Graffiti features are encoded in words as 
pairs of characters, we contrast the carefully 
extracted Graffiti style features with character 
112
bigrams.  Next we test the nickname features 
also described in Section 3.1.  Finally, we test 
combinations of these features.   
 Gang Nation 
Unigrams 70% 81% 
Character Bigrams 64% 76% 
Graffiti Features 44% 68% 
Name Features 63% 78% 
Name + Graffiti 67% 81% 
Unigrams + Name 70% 82% 
Unigrams + Character 
Bigrams 
71% 82% 
Unigrams + Graffiti 71% 82% 
Unigrams + Name  + 
Graffiti 
72% 83% 
Unigrams + Name  + 
Character Bigrams 
72% 79% 
Table 4: Results (percent accuracy) for gang 
affiliation prediction at the gang and nation level. 
  
     We note that the unigram space is a 
challenging feature space to beat, possibly 
because only about 25% of the users employ the 
style features we identified with any regularity.  
The character bigram space actually significantly 
outperforms the Graffiti features, in part because 
it captures aspects of both the Graffiti features, 
the name features, and also some other gang 
specific jargon.  When we combine the stylistic 
features with unigrams, we start to see an 
advantage over unigrams alone.  The best 
combination is Unigrams, Graffiti style features, 
and Name features, at 72% accuracy (.65 Kappa) 
at the gang level and 83% accuracy (.69 Kappa) 
at the nation level.  Overall the accuracy is 
reasonable and offers us the opportunity to 
expand our analysis of social practices on the 
gangs forum to a much larger sample in our 
future work than we present in this first foray. 
4.2 Predicting Dominant Gang per Thread 
In Section 4.3 we present our multi-domain 
learning approach to predicting thread 
composition.  In that work, we use dominant 
gang on a thread as a domain.  In those 
experiments, we contrast results with hand-
annotated dominant gang and automatically-
predicted dominant gang.  In order to compute an 
automatically-identified dominant gang for the 
949 threads used in that experiment, we build a 
model for gang affiliation prediction using data 
from the 2689 users who did not participate on 
any of those threads as training data so there is 
no overlap in users between train and test. 
     The feature space for that classifier included 
unigrams, character bigrams, and the gang name 
features since this feature space tied for best 
performing at the gang level in Section 4.1 and 
presents a slightly lighter weight solution than 
Unigrams, graffiti style features, and gang name 
features. We applied that trained classifier to the 
users who participated on the 949 threads.  From 
the automatically-predicted gang affiliations, we 
computed a dominant gang using the gang and 
nation level for each thread using the same rules 
that we applied to the annotated user identities 
for the annotated dominant gang labels described 
in Section 2.2.  We then evaluated our 
performance by comparing the automatically-
identified dominant gang with the more carefully 
annotated one.  Our automatically identified 
dominant gang labels were 73.3% accurate (.63 
Kappa) at the gang level and 76.6% accurate (.72 
Kappa) at the nation level. This experiment is 
mainly important as preparation for the 
experiment presented in Section 4.3. 
4.3 Predicting Thread Composition 
Our final and arguably most important prediction 
experiments were for prediction of thread 
composition.  This is where we begin to 
investigate how stylistic choices reflect the 
relationships between participants in a 
discussion.  We conducted this experiment twice, 
specifically, once with the annotated dominant 
gang labels (Table 5) and once with the 
automatically predicted ones (Table 6).  In both 
cases, we evaluate gang and nation as alternative 
domain variables.  In both sets of experiments, 
the multi-domain versions significantly 
outperform the baseline across a variety of 
feature spaces, and the stylistic features provide 
benefit above the unigram baseline.  In both 
tables the domain and nation variables are hand-
annotated. * indicates the results are significantly 
better than the no domain unigram baseline.  
Underline indicates best result per column.  And 
bold indicates overall best result.  
     The best performing models in both cases 
used a multi-domain model paired with a stylistic 
feature space rather than a unigram space.  Both 
models performed significantly better than any of 
the unigram models, even the multi-domain 
versions with annotated domains. Where gang 
was used as the domain variable and Graffiti 
style features were the features used for 
prediction, we found that the high weight 
features associated with Allied threads were 
113
either positive about gang identity for a variety 
of gangs other than their own (like B^ in a Crips 
dominated thread) or protective (like CC in a 
Bloods dominated thread).   
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 58%* 60%* 
Character 
Bigrams 
49% 55% 56% 
Graffiti 
Features 
53% 54% 61%* 
Name 
Features 
54% 63%* 66%* 
Name + 
Graffiti 
54% 61%* 65%* 
Unigrams 
+ Name 
52% 58%* 61%* 
Unigrams 
+ Graffiti 
53% 57% 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 61%* 65%* 
Table 5: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches, one 
with dominant gang as the domain variables, and the 
other with dominant nation as the domain variable. In 
this case, the domain variables are annotated. 
 
 No 
Domain 
Dominant 
Gang 
Dominant 
Nation 
Unigrams 53% 57% 57% 
Character 
Bigrams 
49% 53% 55% 
Graffiti 
Features 
53% 65%* 58%* 
Name 
Features 
54% 61%* 59%* 
Name + 
Graffiti 
54% 60%* 59%* 
Unigrams 
+ Name 
52% 56% 56% 
Unigrams 
+ Graffiti 
53% 58%* 57% 
Unigrams 
+ Name  
+ Graffiti 
54% 60%* 59%* 
Table 6: Results (percent accuracy) for thread 
composition prediction, contrasting a single domain 
approach with two multi-domain approaches with 
predicted domain variables, one with dominant gang 
as the domain variables, and the other with dominant 
nation as the domain variable.  
 
Crips-related features were the most frequent 
within this set, perhaps because of the complex 
social structure within the Crips alliance, as 
discussed above.  We saw neither features 
associated with negative attitudes of the gang 
towards others nor other gangs towards them in 
these Allied threads, but in opposing threads, we 
see both, for example, PK in Crips threads or BK 
in Bloods threads.  Where unigrams are used as 
the feature space, the high weight features are 
almost exclusively in the general space rather 
than the domain space, and are generally 
associated with attitude directly rather than gang 
identity.  For example, ?lol,? and ?wtf.? 
5 Conclusions  
We have presented a series of experiments in 
which we have analyzed the usage of stylistic 
features for signaling personal gang 
identification and between gang relations in a 
large, online street gangs forum.  This first foray 
into modeling the language practices of gang 
members is one step towards providing an 
empirical foundation for interpretation of these 
practices.  In embarking upon such an endeavor, 
however, we must use caution.  In machine-
learning approaches to modeling stylistic 
variation, a preference is often given to 
accounting for variance over interpretability, 
with the result that interpretability of models is 
sacrificed in order to achieve a higher prediction 
accuracy.  Simple feature encodings such as 
unigrams are frequently chosen in a (possibly 
misguided) attempt to avoid bias.  As we have 
discussed above, however, rather than cognizant 
introduction of bias informed by prior linguistic 
work, unknown bias is frequently introduced 
because of variables we have not accounted for 
and confounding factors we are not aware of, 
especially in social data that is rarely IID. Our 
results suggest that a strategic combination of 
rich feature encodings and structured modeling 
approach leads to high accuracy and 
interpretability.  In our future work, we will use 
our models to investigate language practices in 
the forum at large rather than the subset of users 
and threads used in this paper1. 
                                                          
1 An appendix with additional analysis and the 
specifics of the feature extraction rules can be found 
at http://www.cs.cmu.edu/~cprose/Graffiti.html. This 
work was funded in part by ARL 
000665610000034354.   
114
References  
Adams, K. & Winter, A. (1997). Gang graffiti as a 
discourse genre, Journal of Sociolinguistics 1/3. Pp 
337-360. 
Argamon, S., Koppel, M., Fine, J., & Shimoni, A. 
(2003). Gender, genre, and writing style in formal 
written texts, Text, 23(3), pp 321-346. 
Argamon, S., Koppel, M., Pennebaker, J., & Schler, J. 
(2007). Mining the blogosphere: age, gender, and 
the varieties of self-expression. First Monday 
12(9). 
Arnold, A. (2009). Exploiting Domain And Task 
Regularities For Robust Named Entity 
Recognition. PhD thesis, Carnegie Mellon 
University, 2009. 
Biber, D. & Conrad, S. (2009). Register, Genre, and 
Style, Cambridge University Press 
Bullock, B. (1996). Derivation and Linguistic Inquiry: 
Les Javnais, The French Review 70(2), pp 180-191. 
Corney, M., de Vel, O., Anderson, A., Mohay, G. 
(2002). Gender-preferential text mining of e-mail 
discourse, in the Proceedings of the 18th Annual 
Computer Security Applications Conference. 
Coulthard, M. & Johnson, A. (2007). An Introduction 
to Forensic Linguistics: Language as Evidence, 
Routledge 
Daum? III, H. (2007). Frustratingly Easy Domain 
Adaptation. In Proceedings of the 45th Annual 
Meeting of the Association of Computational 
Linguistics, pages 256-263. 
Eckert, P. & Rickford, J. (2001). Style and 
Sociolinguistic Variation, Cambridge: University 
of Cambridge Press. 
Finkel, J. & Manning, C. (2009). Hierarchical 
Bayesian Domain Adaptation. In Proceedings of 
Human Language Technologies: The 2009 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics. 
Garot, R. (2007). ?Where You From!?: Gang Identity 
as Performance, Journal of Contemporary 
Ethnography, 36, pp 50-84. 
Gianfortoni, P., Adamson, D. & Ros?, C. P. (2011).  
Modeling Stylistic Variation in Social Media with 
Stretchy Patterns, in Proceedings of First 
Workshop on Algorithms and Resources for 
Modeling of Dialects and Language Varieties, 
Edinburgh, Scottland, UK, pp 49-59. 
Graff, D. (2002).  The AQUAINT Corpus of English 
News Text, Linguistic Data Consortium, 
Philadelphia 
Greenlee, M. (2010).  Youth and Gangs, in M. 
Coulthard and A. Johnson (Eds.). The Routledge 
Handbook of Forensic Linguistics, Routledge. 
Jiang, M. & Argamon, S. (2008). Political leaning 
categorization by exploring subjectivities in 
political blogs. In Proceedings of the 4th 
International Conference on Data Mining, pages 
647-653. 
Johnsons, K. (2009).  FBI: Burgeoning gangs behind 
up to 80% of U.S. Crime, in USA Today, January 
29, 2009. 
Kahneman,  D. (2011).  Thinking Fast and Slow, 
Farrar, Straus, and Giroux 
Krippendorff, K. (2013). Content Analysis: An 
Introduction to Its Methodology (Chapter 13), 
SAGE Publications 
Labov, W. (2010). Principles of Linguistic Change: 
Internal Factors (Volume 1), Wiley-Blackwell. 
Lefkowitz, N. (1989).  Talking Backwards in French, 
The French Review 63(2), pp 312-322. 
Mayfield, E. & Ros?, C. P. (2013). LightSIDE: Open 
Source Machine Learning for Text Accessible to 
Non-Experts, in The Handbook of Automated 
Essay Grading, Routledge Academic Press.        
http://lightsidelabs.com/research/ 
Philips, S. (2009).  Crip Walk, Villian Dance, Pueblo 
Stroll: The Embodiment of Writing in African 
American Gang Dance, Anthropological Quarterly 
82(1), pp69-97. 
Schler, J., Koppel, M., Argamon, S., Pennebaker, J. 
(2005). Effects of Age and Gender on Blogging, 
Proceedings of AAAI Spring Symposium on 
Computational Approaches for Analyzing Weblogs. 
Schler, J. (2006). Effects of Age and Gender on 
Blogging. Artificial Intelligence, 86, 82-84. 
Wiebe, J., Bruce, R., Martin, M., Wilson, T., & Ball, 
M. (2004). Learning Subjective Language, 
Computational Linguistics, 30(3). 
Yan, X., & Yan, L. (2006). Gender classification of 
weblog authors. AAAI Spring Symposium Series 
Computational Approaches to Analyzing Weblogs 
(p. 228?230). 
Zhang, Y., Dang, Y., Chen, H. (2009). Gender 
Difference Analysis of Political Web Forums : An 
Experiment on International Islamic Women?s 
Forum, Proceedings of the 2009 IEEE international 
conference on Intelligence and security 
informatics, pp 61-64. 
 
115
Proceedings of the Second Workshop on Metaphor in NLP, pages 1?10,
Baltimore, MD, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Conversational Metaphors in Use: Exploring the Contrast between
Technical and Everyday Notions of Metaphor
Hyeju Jang, Mario Piergallini, Miaomiao Wen, and Carolyn Penstein Ros
?
e
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{hyejuj, mpiergal, mwen, cprose}@cs.cmu.edu
Abstract
Much computational work has been done
on identifying and interpreting the mean-
ing of metaphors, but little work has been
done on understanding the motivation be-
hind the use of metaphor. To computation-
ally model discourse and social position-
ing in metaphor, we need a corpus anno-
tated with metaphors relevant to speaker
intentions. This paper reports a corpus
study as a first step towards computa-
tional work on social and discourse func-
tions of metaphor. We use Amazon Me-
chanical Turk (MTurk) to annotate data
from three web discussion forums cover-
ing distinct domains. We then compare
these to annotations from our own anno-
tation scheme which distinguish levels of
metaphor with the labels: nonliteral, con-
ventionalized, and literal. Our hope is that
this work raises questions about what new
work needs to be done in order to address
the question of how metaphors are used to
achieve social goals in interaction.
1 Introduction
Our goal is to understand and characterize
the ways that nonliteral language, especially
metaphors, play a role in a variety of conversa-
tional strategies. In contrast to the large body
of work on uncovering the intended propositional
meaning behind metaphorical expressions, we are
most interested in the illocutionary and perlocu-
tionary force of the same contributions.
People use metaphorical expressions in a vari-
ety of ways in order to position themselves so-
cially and express attitudes, as well as to make
their point more effective, attractive, and convinc-
ing. Metaphors can be used to describe unfa-
miliar situations and feelings when the speaker
feels that literal description is inadequate. They
can also be used to display the speaker?s creativ-
ity and wit. They can further be used as a tac-
tic for persuasion or manipulation by foreground-
ing aspects that would not ordinarily be relevant.
Cameron (2007) shows that we can understand
social interactions and their contexts better by
closely looking at these patterns of metaphor use.
Metaphors can vary in how conventionalized
they are, from those which have lost their orig-
inal concrete meanings to completely novel and
vivid metaphors. Intuitively, it also makes sense
that metaphors which are more conventional and
less obviously metaphorical will be used with
less conscious thought than more novel or vivid
metaphors. There are thus reasons to suspect
that distinguishing between levels of metaphoric-
ity could give insight into patterns of use.
In this paper, we are interested in where we
can draw a line between levels of metaphoricity.
As a first step towards our long-term goal, we
present a corpus study in three web discussion
forums including a breast cancer support group,
a Massive Open Online Course (MOOC), and
a forum for street gang members, which cover
distinctly different domains and have differing
community structure. First, we investigate how
laypeople intuitively recognize metaphor by con-
ducting Amazon Mechanical Turk (MTurk) ex-
periments. Second, we introduce a new annota-
tion scheme for metaphorical expressions. In our
annotation scheme, we try to map the metaphor
spectrum of nonliteralness to three types of lan-
guage: nonliteral, conventionalized, and literal.
Our hope is that this distinction provides some
benefit in examining the social and discourse
functions of metaphor. Next, we compare MTurk
1
results with our annotations. Different people will
place the dividing line between literal language
and metaphorical language in different places. In
this work we have the opportunity to gauge how
much everyday conceptions of metaphoricity di-
verge from theoretical perspectives and therefore
how much models of metaphoricity may need to
be adapted in order to adequately characterize
metaphors in strategic use.
The paper is organized as follows. Section 2
relates our work to prior work on annotation and
a corpus study. Section 3 describes the data used
for annotation. Section 4 illustrates the functions
metaphor serves in discourse through a qualitative
analysis of our data. Section 5 explains our anno-
tation scheme. Section 6 presents our annotation
and MTurk experiments. Section 7 discusses the
results. Section 8 concludes the paper.
2 Relation to Prior Work
In this section, we introduce the two main bodies
of relevant prior work on metaphor in language
technologies: computational metaphor processing
and metaphor annotation.
2.1 Computational Work on Metaphor
Much of of the computational work on metaphor
can be classified into two tasks: automatic identi-
fication and interpretation of metaphors.
Metaphor identification has been done using
different approaches: violation of selectional pref-
erences (Fass, 1991), linguistic cues (Goatly,
1997), source and target domain words (Stefanow-
itsch and Gries, 2006), clustering (Birke and
Sarkar, 2006; Shutova et al., 2010), and lexi-
cal relations in WordNet (Krishnakumaran and
Zhu, 2007). Gedigian et al. (2006) and Li and
Sporleder (2010) distinguished the literal and non-
literal use of a target expression in text. In addi-
tion, Mason (2004) performed source-target do-
main mappings.
Metaphor interpretation is another large part
of the computational work on metaphor. Start-
ing with Martin (1990), a number of re-
searchers including Narayanan (1999), Barn-
den and Lee (2002), Agerri et al. (2007),
and Shutova (2010) have worked on the task.
Metaphor identification and interpretation was
performed simultaneously in (Shutova, 2013;
Shutova et al., 2013b).
As we have seen so far, much of the com-
putation work has focused on detecting and un-
covering the intended meaning behind metaphor-
ical expressions. On the other hand, Klebanov
and Flor (2013) paid attention to motivations be-
hind metaphor use, specifically metaphors used
for argumentation in essays. They showed a
moderate-to-strong correlation between percent-
age of metaphorically used words in an essay and
the writing quality score. We will introduce their
annotation protocol in Section 2.2.
However, to the best of our knowledge, not
much computational work has been done on
understanding the motivation behind the use
of metaphor besides that of Klebanov and
Flor (2013). Our work hopefully lays additional
foundation for the needed computational work.
2.2 Metaphor Annotation
One of the main challenges in computational work
on metaphor is the lack of annotated datasets. An-
notating metaphorical language is nontrivial be-
cause of a lack of consensus regarding annotation
schemes and clear definitions. In this section, we
introduce some work dedicated to metaphor anno-
tation and a corpus study.
Wallington et al. (2003) conducted experiments
to investigate what identifies metaphors. Two dif-
ferent teams annotated the same text with differ-
ent instructions, one asked to label ?interesting
stretches? and the other ?metaphorical stretches?.
They also asked annotators to tag words or phrases
that indicated a metaphor nearby, in order to inves-
tigate signals of metaphoricity.
Pragglejaz Group (2007) presented a metaphor
annotation scheme, called the Metaphor Identifi-
cation Procedure (MIP), which introduced a sys-
tematic approach with clear decision rules. In this
scheme, a word is considered to be metaphorical if
it is not used according to its most basic concrete
meaning, and if its contextual meaning can be un-
derstood in comparison with the most basic con-
crete meaning. This method is relatively straight-
forward and can give high inter-reliability. De-
pending on how one decides upon the basic mean-
ing of words, this scheme can be used for different
applications. However, defining the basic mean-
ing of a word is nontrivial, and following the def-
2
inition of basic meaning introduced in the paper
tends to result in a large proportion of words be-
ing annotated as metaphor. Many of the annotated
words would not be considered to be metaphors
by a layperson due to their long and widespread
usage.
Later works by Steen (2010), Shutova and
Teufel (2010), and Shutova et al. (2013a) ex-
panded upon MIP. Steen (2010) discussed the
strengths and weaknesses of MIP, and intro-
duced the Metaphor Identification Procedure VU
University Amsterdam (MIPVU). Shutova and
Teufel (2010) and and Shutova et al. (2013a)
added a procedure for identifying underlying con-
ceptual mappings between source and target do-
mains.
So far, these presented schemes do not distin-
guish between degrees of metaphoricity, and were
not specifically designed for considering moti-
vations behind metaphor use. Unlike the anno-
tation schemes described above, Klebanov and
Flor (2013) built a metaphor annotation proto-
col for metaphors relevant to arguments in essays.
They were interested in identifying metaphors that
stand out and are used to support the writer?s ar-
gument. Instead of giving a formal definition of
a literal sense, the annotators were instructed to
mark words they thought were used metaphori-
cally, and to write down the point being made
by the metaphor, given a general definition of
metaphor and examples. Our work is similar to
this work in that both corpus studies pay attention
to motivations behind metaphor use. However,
our work focuses on more conversational discus-
sion data whereas they focused on essays, which
are more well-formed.
3 Data
We conducted experiments using data from three
different web forums including a Massive Open
Online Course (MOOC), a breast cancer support
group (Breastcancer), and a forum for street gang
members (Gang). We randomly sampled 21 posts
(100 sentences) from MOOC, 8 posts (103 sen-
tences) from Breastcancer and 44 posts (111 sen-
tences) from Gang.
We chose these three forums because they all
offer conversational data and they all differ in
terms of the social situation. The forums dif-
fer significantly in purpose, demographics and
the participation trajectory of members. There-
fore, we expect that people will use language dif-
ferently in the three sets, especially related to
metaphorical expressions.
MOOC: This forum is used primarily for task-
based reasons rather than socializing. People par-
ticipate in the forum for a course, and leave when
the course ends. As a result, the forum does
not have continuity over time; participants do not
spend long time with the same people.
Breastcancer: People join this forum for both
task-based and social reasons: to receive informa-
tional and emotional support. People participate
in the forum after they are diagnosed with cancer,
and may leave the forum when they recover. This
forum is also used episodically by many users, but
a small percentage of users stay for long periods
of time (2 or more years). Thus, continuity al-
lows shared norms to develop over years centered
around an intense shared experience.
Gang: In this forum, members belong to a dis-
tinct subculture prior to joining, whereas Breast-
cancer and MOOC members have less shared
identity before entering the forum. This forum
is purely social. There is no clear endpoint for
participation; members leave the forum whenever
they are not interested in it any more. Users may
stay for a week or two, or for years.
4 Qualitative Analysis
Metaphors can be used for a number of conver-
sational purposes such as increasing or decreas-
ing social distance or as a tactic of persuasion or
manipulation (Ritchie, 2013). In this section, we
perform a qualitative analysis on how metaphor
functions in our data. We illustrate some exam-
ples from each domain with an analysis of how
some functions of social positioning are observed.
The choice of metaphor may reflect something
about the attitude of the speaker. For example,
journey is a metaphor frequently used in the breast
cancer support discussion forum
1
as seen in exam-
ples (2) ? (5) from the Breastcancer forum. Peo-
ple compare chemotherapy to a journey by using
metaphors such as journey, road and moves along.
A journey has a beginning and a goal one trav-
els towards, but people may take different paths.
1
http:breastcancer.org
3
This conveys the experience of cancer treatment
as a process of progressing along a path, strug-
gling and learning, but allows for each person?s
experience to differ without judgment of personal
success or failure (Reisfield and Wilson, 2004).
By contrast, another common metaphor compares
cancer treatment to battles and war. This metaphor
instead conveys an activity rather than passivity, a
struggle against a defined foe, which can be won
if one fights hard enough. But it also creates neg-
ative connotations for some patients, as forgoing
treatment could then be seen as equivalent to sur-
render (ibid.).
(1) Hello Ladies! I was supposed to
start chemo in January, ... I cant
start tx until that is done. So I will
be joining you on your journey this
month. I AM SICK OF the ANXI-
ETY and WAITING.
(2) So Ladies, please add another
member to this club. Looks like we
well all be leaning on each other.
But I promise to pick you up if you
fall if you can catch me once in a
while!
(3) The road seems long now but it re-
ally moves along fast.
(4) I split this journey into 4 stages and
I only deal with one.
In addition, using metaphors can have an ef-
fect of increasing empathetic understanding be-
tween the participants (Ritchie, 2013). We can
see this in examples (1) ? (4), where participants
in the same thread use similar metaphors relat-
ing chemotherapy to a journey. Reusing each
other?s metaphors reduces emotional distance and
helps to build empathic understanding and bond-
ing through a shared perception of their situations.
Metaphor also serves to suggest associations
between things that one would not normally asso-
ciate. Example (5) from the MOOC forum frames
participation in discussions as stepping into an
arena, which refers to an area for sports or com-
petition. By making such an analogy, it conveys
an environment of direct competition in front of a
large audience. It suggests that a student may be
afraid of contributing to discussion because they
may make a wrong statement or weak argument
and another person could counter their contribu-
tions, and they will be embarrassed in front of
their classmates.
(5) Hi, Vicki, great point ? I do wish
that teachers in my growing up
years had been better facilitators
of discussion that allowed EVERY-
one to practice adn become skill-
ful at speaking...I think in the early
years some of us need some hand-
holding in stepping into the arena
and speaking
Metaphors can also be used simply as a form of
wordplay, to display one?s wit and creativity. This
can be seen in the exchange in examples (6) ? (8),
from the Gang forum. A common metaphor used
on that forum is to refer to someone as food to
mean that they are weak and unthreatening. The
writer in (6) expands on this metaphor to suggest
that the other person is especially weak by calling
him dessert, while the writer in (7) then challenges
him to fight by exploiting the meaning of hungry
as ?having a desire for food?. The first writer (8)
then dismisses him as not worth the effort to fight,
as he does not eat vegetables.
(6) So If She Is Food That Must Make
U Desert
(7) if u hungry nigga why wait?
(8) I Dont Eat Vegatables.
5 Our Annotation Scheme
When we performed qualitative analysis as in Sec-
tion 4, we found that more noticeable metaphors
such as ?journey?, ?pick you up?, and ?fall? in (1)
and (2) seem more indicative of speaker attitude
or positioning than metaphors such as ?point? in
(5). This might suggest the degree of metaphoric-
ity affects how metaphors function in discourse.
In this section, we describe our metaphor anno-
tation scheme, which tries to map this variation
among metaphors to a simpler three-point scale of
nonliteralness: nonliteral, conventionalized, and
literal.
5.1 Basic Conditions
Our annotation scheme targets language satisfying
the following three conditions:
4
1. the expression needs to have an original es-
tablished meaning.
2. the expression needs to be used in context to
mean something significantly different from
that original meaning.
3. the difference in meaning should not
be hyperbole, understatement, sarcasm or
metonymy
These conditions result in metaphorical ex-
pressions including simile and metaphorical id-
ioms. We consider simile to be a special case of
metaphor which makes an explicit comparison us-
ing words such as ?like?. We include metaphor-
ical idioms because they are obviously nonliteral
and metaphorical despite the fact that they have
lost their source domains.
Have an original meaning: The expression or
the words within the expression need to have orig-
inal established meanings. For example, in the
sentence ?I will be joining you on your journey
this month? of (1) in Section 4, the word ?journey?
refers to chemotherapy given the context, but has
a clear and commonly known original meaning of
a physical journey from one place to another.
Alter the original and established meanings
of the words: The usage needs to change the orig-
inal meaning of the expression in some way. The
intended meaning should be understood through
a comparison to the original meaning. For the
same example, in ?I will be joining you on your
journey this month?, the intended meaning can be
understood through a comparison to some char-
acteristics of a long voyage. For metaphorical id-
ioms such as ?he kicked the bucket,? the nonliteral
meaning of ?he died? is far from the literal mean-
ing of ?he struck the bucket with his foot.?
Should not merely be hyperbole, understate-
ment, sarcasm, or metonymy: To reduce the
scope of our work, the usage needs to alter the
original meaning of the expression but should not
simply be a change in the intensity or the polar-
ity of the meaning, nor should it be metonymy.
Language uses like hyperbole and understatement
may simply change the intensity of the meaning
without otherwise altering it. For sarcasm, the
intended meaning is simply the negation of the
words used. Metonymy is a reference by asso-
ciation rather than a comparison. For example, in
?The White House denied the rumor?, the White
House stands in for the president because it is as-
sociated with him, rather than because it is being
compared to him. Note that metaphorical expres-
sions used in conjunction with these techniques
will still be coded as metaphor.
5.2 Decision Steps
To apply the basic conditions to the actual annota-
tion procedure, we come up with a set of decision
questions (Table 1). The questions rely on a va-
riety of other syntactic and semantic distinctions
serving as filtering questions. An annotator fol-
lows the questions in order after picking a phrase
or word in a sentence he or she thinks might be
nonliteral language. We describe some of our de-
cisions below.
Unit: The text annotators think might be non-
literal is considered for annotation. We allow a
word, a phrase, a clause, or a sentence as the
unit for annotation as in (Wallington et al., 2003).
We request that annotators include as few words
as necessary to cover each metaphorical phrase
within a sentence.
Category: We request that annotators code a
candidate unit as nonliteral, conventionalized, or
literal. We intend the nonliteral category to in-
clude nonliteral language usage within our scope,
namely metaphors, similes, and metaphorical id-
ioms. The conventionalized category is intended
to cover the cases where the nonliteralness of the
expression is unclear because of its extensive us-
age. The literal category is assigned to words that
are literal without any doubt.
Syntactic forms: We do not include prepo-
sitions or light verbs. We do not consider
phrases that consist of only function words such
as modals, auxiliaries, prepositions/particles or
infinitive markers. We restrict the candidate
metaphorical expressions to those which contain
content words.
Semantic forms: We do not include single
compound words, conventional terms of address,
greeting or parting phrases, or discourse markers
such as ?well?. We also do not include termi-
nology or jargon specific to the domain being an-
notated such as ?twilight sedation? in healthcare,
since this may be simply borrowing others? words.
5
No. Question Decision
1 Is the expression using the primary or most concrete meanings of the words? Yes = L
2 Does the expression include a light verb that can be omitted without changing
the meaning, as in ?I take a shower? ? ?I shower?? If so, the light verb
expression as a whole is literal.
Yes = L
3 Is the metaphor composed of a single compound word, like ?painkiller?, used
in its usual meaning?
Yes = L
4 Is the expression a conventional term of address, greeting, parting phrase or a
discourse marker?
Yes = L
5 Is the expression using terminology or jargon very common in this domain or
medium?
Yes = L
6 Is the expression merely hyperbole/understatement, sarcasm or metonymy? Yes = L
7 Is the expression a fixed idiom like ?kick the bucket? that could have a very
different concrete meaning?
Yes = N
8 Is the expression a simile, using ?like? or ?as? to make a comparison between
unlike things?
Yes = N
9 Is the expression unconventional/creative and also using non-concrete mean-
ings?
Yes = N
10 Is there another common way to say it that would convey all the same nuances
(emotional, etc.)? Or, is this expression one of the only conventional ways of
conveying that meaning?
If yes to
the latter
= C
11 If you cannot otherwise make a decision between literal and nonliteral, just
mark it as C.
Table 1: Questions to annotate (N: Nonliteral, C: Conventionalized, L: Literal).
6 Experiment
In this section, we present our comparative study
of the MTurk annotations and the annotations
based on our annotation scheme. The purpose
of this experiment is to explore (1) how laypeo-
ple perceive metaphor, (2) how valid the anno-
tations from crowdsourcing can be, and (3) how
metaphors are different in the three different do-
mains.
6.1 Experiment Setup
We had two annotators who were graduate stu-
dents with some linguistic knowledge. Both were
native speakers of English. The annotators were
asked to annotate the data using our annotation
scheme. We will call the annotators trained an-
notators from now on.
In addition, we used Amazon?s Mechanical
Turk (MTurk) crowdsourcing marketplace to col-
lect laypeople?s recognition of metaphors. We
employed MTurk workers to annotate each sen-
tence with the metaphorical expressions. Each
sentence was given along with the full post it came
from. MTurkers were instructed to copy and paste
all the metaphors appearing in the sentence to
given text boxes. They were given a simple def-
inition of metaphor from Wikipedia along with a
few examples to guide them. Each sentence was
labeled by seven different MTurk workers, and we
paid $0.05 for annotating each sentence. To con-
trol annotation quality, we required that all work-
ers have a United States location and have 98%
or more of their previous submissions accepted.
We monitored the annotation job and manually
filtered out annotators who submitted uniform or
seemingly random annotations.
6.2 Results
To evaluate the reliability of the annotations, we
used weighted Kappa (Cohen, 1968) at the word
level, excluding stop words. The weighted Kappa
value for annotations following our annotation
scheme was 0.52, and the percent agreement was
95.68%. To measure inter-reliability between two
annotators per class, we used Cohen?s Kappa (Co-
6
hen, 1960). Table 2 shows the Kappa values for
each dataset and each class. Table 4 shows the
corpus statistics.
Dataset N C N+C Weighted
all 0.44 0.20 0.49 0.52
breastcancer 0.69 0.20 0.63 0.71
Gang 0.26 0.28 0.39 0.34
MOOC 0.41 0.13 0.47 0.53
Table 2: Inter-reliability between two trained an-
notators for our annotation scheme.
To evaluate the reliability of the annotations by
MTurkers, we calculated Fleiss?s kappa (Fleiss,
1971). Fleiss?s kappa is appropriate for assessing
inter-reliability when different items are rated by
different judges. We measured the agreement at
the word level, excluding stop words as in com-
puting the agreement between trained annotators.
The annotation was 1 if the MTurker coded a word
as a metaphorical use, otherwise the annotation
was 0. The Kappa values are listed in Table 3.
Dataset Fleiss?s Kappa
all 0.36
breastcancer 0.41
Gang 0.35
MOOC 0.30
Table 3: Inter-reliability among MTurkers.
We also measured the agreement between the
annotations based on our scheme and MTurk an-
notations to see how they agree with each other.
First, we made a gold standard after discussing
the annotations of trained annotators. Then, to
combine the seven MTurk annotations, we give
a score for an expression 1 if the majority of
MTurkers coded it as metaphorically used, other-
wise the score is 0. Then, we computed Kappa
value between trained annotators and MTurkers.
The agreement between trained annotators and
MTurkers was 0.51 for N and 0.40 for N + C. We
can see the agreement between trained annotators
and MTurkers is not that bad especially for N.
Figure 1 shows the percentage of words labeled
as N, C or L according to the number of MTurk-
ers who annotated the word as metaphorical. As
seen, the more MTurkers who annotated a word,
Dataset N N+ C
all 0.51 0.40
breastcancer 0.64 0.47
Gang 0.36 0.39
MOOC 0.65 0.36
Table 5: Inter-reliability between trained annota-
tors and MTurkers.
the more likely it was to be annotated as N or C
by our trained annotators. The distinction between
Nonliteral and Conventionalized, however, is a bit
muddier, although it displays a moderate trend to-
wards more disagreement between MTurkers for
the Conventionalized category. The vast majority
of words (>90%) were considered to be literal, so
the sample size for comparing the N and C cate-
gories is small.
Figure 1: Correspondence between MTurkers and
trained annotators. X-axis: the number of MTuck-
ers annotating a word as metaphor.
7 Discussion
In this section, we investigate the disagreements
between annotators. A problem inherent to the an-
notation of metaphor is that the boundary between
literal and nonliteral language is fuzzy. Different
annotators may draw the line in different places
even when it comes to phrases they are all famil-
iar with. It is also true that each person will have
a different life history, and so some phrases which
are uninteresting to one person will be strikingly
metaphorical to another. For example, someone
who is unfamiliar with the internet will likely find
the phrase ?surf the web? quite metaphorical.
Since we did not predefine the words or phrases
that annotators could consider, there were often
cases where one person would annotate just the
7
Dataset Posts Sent. Words Content Words N C N/Sent. C/Sent.
MOOC 21 100 2005 982 23 59 0.23 0.59
Breastcancer 8 103 1598 797 27 41 0.26 0.4
Gang 44 111 1403 519 30 51 0.27 0.46
Table 4: Data statistics.
noun and another might include the entire noun
phrase. If it was part of a conventional multi-word
expression, MTurkers seemed likely to include the
entire collocation, not merely the metaphorical
part. Boundaries were an issue to a lesser extent
with our trained annotators.
One of our datasets, the Gang forum, uses a lot
of slang and non-standard grammar and spellings.
One of our trained annotators is quite familiar with
this forum and the other is not. This was the set
they had the most disagreement on. For exam-
ple, the one annotator did not recognize names of
certain gangs and rap musicians, and thought they
were meant metaphorically. Similarly, the MTurk-
ers had trouble with many of the slang expressions
in this data.
Another issue for the MTurkers is the distinc-
tion between metaphor and other forms of nonlit-
eral language such as metonymy and hyperbole.
For example, in the Gang data, the term ?ass? is
used to refer to a whole person. This is a type
metonymy (synecdoche) using a part to refer to
the whole. MTurkers were likely to label such
expressions as metaphor. Hyperbolic expressions
like ?never in a million years? were also marked
by some MTurkers.
In a few cases, the sentence may have required
more context to decipher, such as previous posts
in the same thread. Another minor issue was that
some data had words misspelled as other words or
grammatical errors, which some MTurkers anno-
tated as metaphors.
Certain categories of conventionalized
metaphors that would be annotated in the
original presentation of MIP (Pragglejaz-Group,
2007) were never or almost never annotated by
MTurkers. These included light verbs such as
?make? or ?get? when used as causatives or
the passive ?get?, verbs of sensation used for
cognitive meanings, such as ?see? meaning ?un-
derstand?, and demonstratives and prepositions in
themselves. This may indicate something about
the relevance of these types of metaphors for
certain applications.
8 Conclusion
We annotated data from three distinct conver-
sational online forums using both MTurks and
our annotation scheme. The comparison between
these two annotations revealed a few things. One
is that MTurkers did not show high agreement
among themselves, but showed acceptable agree-
ment with trained annotators for the N category.
Another is that domain-specific knowledge is im-
portant for accurate identification of metaphors.
Even trained annotators will have difficulty if they
are not familiar with the domain because they may
not even understand the meaning of the language
used.
Our annotation scheme has room for improve-
ment. For example, we need to distinguish be-
tween the Conventionalized and Nonliteral cate-
gories more clearly. We will refine the coding
scheme further as we work with more annotators.
We also think there may be methods of pro-
cessing MTurk annotations to improve their cor-
respondence with annotations based on our cod-
ing scheme. This could address issues such as in-
consistent phrase boundaries or distinguishing be-
tween metonymy and metaphor. This could make
it possible to use crowdsourcing to annotate the
larger amounts of data required for computational
applications in a reasonable amount of time.
Our research is in the beginning phase work-
ing towards the goal of computational modeling
of social and discourse uses of metaphor. Our next
steps in that direction will be to work on develop-
ing our annotated dataset and then begin to investi-
gate the differing contexts that metaphors are used
in. Our eventual goal is to be able to apply compu-
tational methods to interpret metaphor at the level
of social positioning and discourse functions.
8
Acknowledgments
This work was supported by NSF grant IIS-
1302522, and Army research lab grant W911NF-
11-2-0042.
References
Rodrigo Agerri, John Barnden, Mark Lee, and Alan
Wallington. 2007. Metaphor, inference and domain
independent mappings. In Proceedings of RANLP,
pages 17?23. Citeseer.
John A Barnden and Mark G Lee. 2002. An artifi-
cial intelligence approach to metaphor understand-
ing. Theoria et Historia Scientiarum, 6(1):399?412.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In EACL.
Lynne J Cameron. 2007. Patterns of metaphor
use in reconciliation talk. Discourse & Society,
18(2):197?222.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or par-
tial credit. Psychological bulletin, 70(4):213.
Dan Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378?382.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the Third Workshop on Scalable Natu-
ral Language Understanding, pages 41?48. Associ-
ation for Computational Linguistics.
Andrew Goatly. 1997. Language of Metaphors: Lit-
eral Metaphorical. Routledge.
Beata Beigman Klebanov and Michael Flor. 2013.
Argumentation-relevant metaphors in test-taker es-
says. Meta4NLP 2013, pages 11?20.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
approaches to Figurative Language, pages 13?20.
Association for Computational Linguistics.
Linlin Li and Caroline Sporleder. 2010. Using gaus-
sian mixture models to detect figurative language in
context. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
James H Martin. 1990. A computational model of
metaphor interpretation. Academic Press Profes-
sional, Inc.
Zachary J Mason. 2004. Cormet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Srinivas Narayanan. 1999. Moving right along: A
computational model of metaphoric reasoning about
events. In AAAI/IAAI, pages 121?127.
Pragglejaz-Group. 2007. Mip: A method for iden-
tifying metaphorically used words in discourse.
Metaphor and symbol, 22(1):1?39.
Gary M Reisfield and George R Wilson. 2004. Use
of metaphor in the discourse on cancer. Journal of
Clinical Oncology, 22(19):4024?4027.
SL. David Ritchie. 2013. Metaphor (Key Topics in
Semantics and Pragmatics). Cambridge university
press.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source-target do-
main mappings. In LREC.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics,
pages 1002?1010. Association for Computational
Linguistics.
Ekaterina Shutova, BarryJ. Devereux, and Anna Ko-
rhonen. 2013a. Conceptual metaphor theory meets
the data: a corpus-based human annotation study.
Language Resources and Evaluation, 47(4):1261?
1284.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013b. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301?353.
Ekaterina Shutova. 2010. Automatic metaphor inter-
pretation as a paraphrasing task. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1029?1037.
Association for Computational Linguistics.
Ekaterina Shutova. 2013. Metaphor identification as
interpretation. Atlanta, Georgia, USA, page 276.
9
Gerard J Steen, Aletta G Dorst, J Berenike Herrmann,
Anna Kaal, Tina Krennmayr, and Trijntje Pasma.
2010. A method for linguistic metaphor identifica-
tion: From MIP to MIPVU, volume 14. John Ben-
jamins Publishing.
Anatol Stefanowitsch and Stefan Th Gries. 2006.
Corpus-based approaches to metaphor and
metonymy, volume 171. Walter de Gruyter.
AM Wallington, JA Barnden, P Buchlovsky, L Fel-
lows, and SR Glasbey. 2003. Metaphor annota-
tion: A systematic study. COGNITIVE SCIENCE
RESEARCH PAPERS-UNIVERSITY OF BIRMING-
HAM CSRP.
10
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 21?31,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Towards Identifying the Resolvability of Threads in MOOCs
Diyi Yang, Miaomiao Wen, Carolyn Rose
Language Technologies Institute, Carnegie Mellon University
5000 Forbes Ave, Pittsburgh, 15213
{diyiy,mwen,cprose}@cs.cmu.edu
Abstract
One important function of the discussion
forums of Massive Open Online Courses
(MOOCs) is for students to post problems
they are unable to resolve and receive help
from their peers and instructors. There are
a large proportion of threads that are not
resolved to the satisfaction of the students
for various reasons. In this paper, we
attack this problem by firstly constructing
a conceptual model validated using a
Structural Equation Modeling technique,
which enables us to understand the factors
that influence whether a problem thread
is satisfactorily resolved. We then demon-
strate the robustness of these findings using
a predictive model that illustrates how ac-
curately those factors can be used to predict
whether a thread is resolved or unresolved.
Experiments conducted on one MOOC
show that thread resolveability connects
closely to our proposed five dimensions and
that the predictive ensemble model gives
better performance over several baselines.
1 Introduction
Massive Open Online Courses (MOOCs), run by
organizations such as Coursera, have been among
the most news worthy social media environments
in the past year. While usage of social media
affordances such as discussion forums in such
courses is small relative to usage of videos or
assignments, participation in the discussion forums
is an important predictor of commitment to the
course (Yang et al., 2013). We hypothesize that
supporting a positive experience in such forums
has the potential to increase retention in such
courses. In this paper, we specifically study the
behavior of students in a MOOC course for learning
Python programming. We present empirical work
that elucidates an important problem in existing
MOOC discussion forums, propose a practical
solution, and offer promising results in a corpus
based evaluation.
MOOCs for programming skills can be seen as
important resources for the professional develop-
ment of programmers and programmers in training.
While MOOCs for learning programming are a
recent phenomenon, they are not the first web
accessible resources for development of such skills.
In recent years, a plethora of question/answer
sites for programming have become available that
have grown into thriving communities of practice
for programmers. In these online communities,
programmers can get mentoring from those who
are more expert than them and offer mentoring to
programmers who are less expert than them. For
example, StackOverflow
1
has become a forum not
only for getting specific questions answered, but for
negotiating the pros and cons of alternative ways
of solving technical problems. The code proposed
as part of alternative solutions remains as part of
the community memory, which is then accessible
for those who come later with similar concerns.
Where StackOverflow falls short is in providing
an appropriate environment for the active involve-
ment of very novice programmers. When such
novices come to a forum like StackOverflow and
present their naive questions, they are frequently
met with sarcastic responses if they get a response
at all.
MOOCs for learning programming skills fill a
gap left open by such environments, in that they
welcome the very novice and provide forums where
naive questions are not shunned. Nevertheless,
discussion forums that only include such novice
programmers would be akin to the blind leading the
blind were it not for the involvement of a few more
expert students and the teaching staff. This does not
fully solve the problem, however. Many threads are
1
http://stackoverflow.com/
21
still left without a satisfactory resolution. Currently,
it is challenging for the teaching staff and expert
participants to know where in the massive amount
of communication to look for opportunities where
their support is most needed. This is the problem
we aim to address in this paper, i.e. automatically
identify whether a thread is resolved and provide
potential for better allocation of instructor and
student resources.
In the remainder of the paper we first survey
related work. Next we describe the formulation
of the problem. We then present a series of
two experiments, the later one building on the
successful findings and results from the former.
The results conducted on one MOOC show that
our proposed model of thread resolveability better
captures the difference between resolved and
unresolved threads and that the ensemble logistic
model outperforms several baselines. We conclude
the paper with a discussion of the limitations of
this work and next steps.
2 Related Work
MOOCs have received more and more attention
recently, with the promise of providing many of the
benefits of traditional classroom learning but not
limited by time, location or finances. Much prior
work has focused on analysis of such platforms
to motivate the design of better student learning
experiences. In various ways, the issue of students
needing support from instructors and students has
been addressed (Lieberman, 1995).
An important component in the Coursera envi-
ronment is the discussion forums, which students
can use to learn new knowledge from each other
and from the teaching staff when they participate.
In support of the importance of the discussion
forums in connection with major problems like
attrition, models are proposed to predict student
dropout based both on their video watching be-
havior and also discussion forum posting behavior,
such as how many posts a student has made (Balakr-
ishnan, 2013). Student behavior in the discussion
forum is also focused by other prior works (Yang
et al., 2013). Yang et al. analyze drop out along the
way, demonstrating the predictive power of features
extracted within time windows of student behavior
within the forums. The results of their work suggest
that interaction with other students is important
for keeping students motivated, which is further
confirmed by many works (Yang et al., 2014; Ros?e
et al., 2014). Besides, linguistic reflections are also
crucial for students engagement (Wen et al., 2014).
Other work highlights the importance of interac-
tion in the form of feedback during participation in
MOOCs. For example, some prior work (Piech et
al., 2013) has explored peer grading, especially
in helping grading of open ended assignments,
in courses with thousands or tens of thousands
of students. Other work takes a more holistic
approach to assessment of student behavior. For
example, in one such example (Kizilcec et al.,
2013), instead of looking at students? assignments,
students were classified based on their patterns
of interaction with video lectures and assessment
activities. This behavior trace was processed using
a simple and scalable classification method that
could identify a small number of longitudinal
engagement trajectories that potentially provide
the impetus for tailored feedback or mentoring.
Outside of MOOC discussion forums, there has
also been work investigating the conditions under
which questions receive appropriate feedback in
more general Question Answering (QA) sites. In
particular, this work has been framed as research
on thread resolveability in QA sites. It can
be conceived as the human counterpart to fully
automated question answering systems (Prager et
al., 2000; Perera, 2012; Jeon et al., 2006; Agichtein
et al., 2008). Much of this work has emphasized the
importance of having effective features to model
question and answer processes.
In some of this prior work, the focus has been
on identifying whether a thread is answered given
a question and a set of potential answers (Sung
et al., 2013; Tian et al., 2013). The prior
work (Anderson et al., 2012) has focused on
understanding the dynamics of the surrounding
community activity, like the process through which
answers and voters arrive over time. Based on
understanding of such factors, a prediction can be
made about the long term value for the community
of a question being answered. Similarly, Agichtein
and colleagues (Agichtein et al., 2009) presented
a general prediction model of information seeker
satisfaction in community question answering,
and developed content, structure and community
focused features for the question answering task. A
collection of other related work (Liu and Agichtein,
2008) has developed personalized models of asker
satisfaction to predict whether a particular question
starter will be satisfied with the answers given
22
by others. This is solved by exploring content,
structure and interaction features using standard
prediction models.
Work on automated question answering systems
can also be seen as relevant since questions that can
be answered automatically do not need a human
response, and therefore might reduce the load
on available human effort. Instead of predicting
whether a problem is answered, strategies for
predicting are explored when a question answering
system is likely to give an incorrect answer (Brill
et al., 2002). To further understand how a question
is answered, researchers (Yih et al., 2013) have
studied the answer sentence selection problem
for question answering and improves the model
performance by using lexical semantic resources.
That is, they construct semantic matches between
question and answers. In terms of the extent
to which the question is answered, Shah and
colleagues (Shah and Pomerantz, 2010) evaluated
answer quality by manually rating the quality
of each answer. Then they extracted various
features to train classifiers to select the best answer
for that question. Liu et al. (Liu et al., 2011)
proposed to use a mutual reinforcement based
propagation algorithm to predict question quality
based. The model makes its prediction based on
the connection between askers and topics, and how
those connections predict differences in quality.
The above question answering work is all
about general discussion forums (Qu et al., 2009;
Kabutoya et al., 2010), such as Yahoo! Answers
2
.
In our work, in addition to taking advantage of
existing QA work, we also adopt a linguistic
perspective (Jansen et al., 2014) and take semantic
matching into account using a latent semantic
approach. To the best of our knowledge, this is
the first work on thread resolvability analysis in a
MOOC context.
3 Research Problem Introduction
In this section, we focus on how to identify the
resolveability of threads in the MOOC forums. We
firstly introduce the research context and dataset,
then we formulate our resolveability problem.
3.1 Research Context and Dataset
In programming MOOCs, when students encounter
problems working on the programming assign-
ments, or when something is not clear from the
2
http://answers.yahoo.com/
readings or lectures, students have the opportunity
to initiate a thread in the course forum, in order
to engage other students in the class as well as
the teaching staff. For example, if a student
were confused about the distinction between an
argument and a parameter in Python, he/she
would post the question to the variables subforum,
marking it unresolved at the same time. In the
ideal case, another participant would reply to
this question with some detailed explanation and
example, which would solve that problem. When
the student who initiated the thread receives the
response, assuming it is adequate, that student
may mark it as resolved. Others may join in as
well, and individual posts may be rated through
upvotes and downvotes. In contrast to existing QA
sites, no best answer option is available. Thus,
the resolved/unresolved button provides the closest
equivalent groundtruth.
The data for this paper was crawled from a
Python language course. Our focus was specifically
to investigate the inner workings of threads related
to getting answers to questions or help with
programming difficulty. In order to avoid including
threads in our dataset that are off-topic or otherwise
irrelevant, we limited the set of forums to the
subforums that focus strongly on course content,
including those indicated to focus on lectures,
exercises and assignments as well as the final exam.
That is, we discarded posts in the study groups,
social discussion, and other discussion areas that
do not have unresolved buttons. In the final dataset,
there were 2508 threads (1244 resolved threads) in
total, and 2896 users (12 instructors and staffs) who
had at least one post. Each question is associated
with a label indicating whether it is resolved or not.
3.2 Problem Formulation
Work on the related problem of analysis of QA
websites has grown in popularity in recent years.
However, due to differences in how MOOCs work
as temporary online communities, it is necessary to
consider how findings from prior work in these
areas may or may not generalize to this new
context as we formulate our research problem. In
particular, MOOCs are different from existing QA
websites, such as Yahoo! Q&A, Stack Overflow.
The purpose of QA sites is primarily for people
to get answers. While people may learn from
their interactions on such sites, those sites are not
designed in particular to support learning. Thus,
23
0200
400
600
800
1000
1200
1400
C
n
t 
N
u
m
b
e
r
Post Number
Standby Less Active Active Super Star
(a) Question Post
0
200
400
600
800
1000
1200
1400
0 3 6 9
1
2
1
5
1
8
2
1
2
4
2
7
31 34 39 4
4
52 6
2
73 8
8
96
1
2
7
2
1
3
1
0
0
1
C
n
t 
N
u
m
b
e
r
Reply Number
Standby Less Active Active Super Star
(b) Reply Devotee
0
200
400
600
800
1000
1200
1400
0 1 2 3 4 5 6 7 10 12 28 226
C
n
t 
N
u
m
b
e
r
Resolved Number
Standby Less Active Active Super Star
(c) Resolved Favor
0
200
400
600
800
1000
1200
1400
0 3 6 9
1
2
1
5
1
8
2
1
2
4
2
9
37 4
0
4
4
53 79
1
1
8
1
30
2
1
2
798
C
n
t 
N
u
m
b
e
r
UpVotes
Standby Less Active Active Super Star
(d) UpVotes
Figure 1: Starter Influence Statistics. Each Figure has two curves; the below one indicates how many
users have made the associated number of posts/reply. The above one is the cumulative version of the
same.
different characteristics are needed in the MOOCs
discussion threads. One implication is that the
discussions in MOOCs may need to be more
interactive than those found in environments such
as StackOverflow. Students who post problems
can be expected to be less capable of fully
comprehending an answer even if it is a good
one. This demands more effort from those with the
ability to offer helpful responses. In order for the
discussions to be effective, the threads must include
a balance of naive participants and participants with
more knowledge. A related issue is that it is not yet
ubiquitous for participants in MOOCs to have the
opportunity to earn a reputation score for offering
useful answers and other instructional support. In
other QA sites, this is both a valuable motivator as
well as an important predictor of resolved versus
unresolved question threads (Anderson et al., 2012).
Thus, students who post questions may need to
sell their problem in order to attract those who
can offer help. Taking these interrelated issues
into account, an important aspect of our modeling
work is in recognizing the different roles that
users play in the community. Related to this, we
will describe below how we develop models that
include latent variables related to the propensity
of users to initiate problem threads that attract
useful responses, and the propensity of others to
contribute useful responses in such contexts. We
refer to these complementary variables as starter
influence and expert participation respectively.
Secondly, all are welcome to learn in a MOOC
and participate actively even if they have no
prior knowledge. In an educational context, it
would not be appropriate to meet a naive question
with a sarcastic response. In contrast, in Stack
Overflow, it would be treated as unremarkable for
a naive question to get a sarcastic response. While
naive participants may not enjoy such responses,
they learn to expect them. Since approaching
posted problems with patience and friendliness is
important for avoiding discouraging new learners,
we include a variable called friendliness that
represents friendly and polite discussion behavior.
None of these would ultimately result in thread
resolution if the answers that are offered were not
targeted to the problems raised by the students
who initiated the threads. This is one place where
our work is very aligned with earlier work on QA
sites. And thus we adopt a similar practice where
we include in our model an estimate of answer
appropriateness in a latent variable we refer to as
content matching.
Now we define important terms used in our
discussion. First, we define roles within discussion
threads that are relevant for our work. For a
given thread, the user who initialized the thread
is called the Starter; the teaching staff including
both official course instructors and TAs are referred
to as Instructors; and any other users who
have replied or commented in the thread are
referred to as Participants. We count a thread
in our dataset as resolved only if the thread
starter personally changes the Unresolved button to
Resolved. Otherwise, we count the as unresolved.
We are interested in the conditions under which
a thread is marked as resolved or unresolved:
Thread Resolveability: Given a thread with
its associated question and set of replies, which
may not have been explicitly marked as resolved,
identify whether it should have been marked as
resolved or not.
4 Latent Variable Modeling
We laid the foundation for a conceptual model
above to understand the factors associated with
resolved versus unresolved threads and introduced
five latent factors we referred to as Starter Influence,
Expert Participation, Thread Popularity, Friendli-
ness, and Content Matching. In this section, we
24
further formalize these latent factors by specifying
associated sets of observed variables that will
ultimately enable us to evaluate our conceptual
model. All latent and observed variables are
enumerated in Table 1.
4.1 Starter Influence
The person who serves as the Thread starter
is responsible for formulating the question that
is addressed, and therefore the focus of that
discussion. Some participants post many questions
and are very adept at formulating their questions in
ways that engage the attention of people who have
the ability to provide answers. If the starter posts a
lot and his/her questions often get resolved, this can
be taken as an indication that this person is popular.
Questions contributed by him/her may be more
likely to attract attention and receive replies. This
simple indication of popularity, which can be easily
computed, may in some way compensate for the
lack of an established badge system where they are
not in use. We propose to measure this form of user
influence by using the following four indicators.
(1) Question Devotee x
Pst
, indicates how many
threads this question starter has proposed in this
forum. Based on Figure 1(a), we divide users in
this discussion forum into four types to indicate the
propensity to post, i.e. post number ranges from
1-2 as standbys, 3-5 as less active, 6-14 as active,
40-489 as superstars. Similar partition method is
adopted for all the following indicators. (2) Reply
Devotee x
Rep
, means how many times a person acts
as a Participant in a thread posted by other students
as shown in Figure 1(b). If he/she usually replies
to others, then it is possible that his/her question
will be paid more attention in return. (3) Resolved
Favor x
Res
, means in how many threads the person
acts as the Starter in threads that get resolved. (4)
Praised Responder x
Uvt
, indicates the proportion
of all the posts this starter makes in the forum that
received upvotes, as displayed in Figure 1(d). This
connects to how others recognize this starter and to
what degree.
4.2 Expert Participation
Who participates a discussion is as important as
who initiates the discussion. Students with some
expertise in the related content can often provide
quality replies (Anderson et al., 2012). Since user
reputation score information is not available in
this MOOC, it is necessary to for us to identify
observable indicators. We define a person as Expert
x
Exp
in our forum as follows. A person is an
Expert if and only if he/she is one of the instructors
or his/her reputation score as we compute it is
ranked in the top 1% among all students. The
reputation score of student u is computed based
on his/her question devotee u
Pst
, reply devotee
u
Rep
, resolved favor u
Res
, and praised recognition
u
Uvt
as we defined in the previous section. The
contribution of each factor to reputation score is
controlled using parameters ?, ?, ?.
score(u) =?u
Pst
+ ?u
Rep
+ ?u
Res
+ (1? ?? ? ? ?)u
Uvt
(1)
4.3 Thread Popularity
How much attention is paid to a question may be
linked to the attractiveness of the thread based
on how it is presented to the community. Thus
modeling thread popularity may be valuable for
accounting for variation in level of participation
across threads. In particular, a reply is given
upvotes when others think it is informative or
good. Thus upvotes could indicate how others
evaluate the replies in connection with the question.
We design three observable factors here that
may contribute to a model of thread popularity.
The Total UpVotes x
Tvt
and Max UpVotes x
Mvt
are used to represent the credit this thread has
received and how others recognize the current
discussion. Based on our analysis, people rarely
give a downvote to others? posts. The Question
Votes x
Svt
indicates whether the starter formulates
a problem that wins recognition from others. For
Total Upvotes, we find that in resolved threads, it is
6.10 compared to 3.15 in unresolved thread. Thus,
intuitively, thread popularity has the potential to
give a useful prediction of thread resolveability.
4.4 Friendliness
Friendliness (Danescu-Niculescu-Mizil et al.,
2013; Burke and Kraut, 2008) concerns whether
the current conversation is conducive for others
to discuss ideas. This has not been considered
in existing question answering work, and we
thus discuss our operationalization of politeness
here. We hypothesize that resolved threads posses
more polite words, such as ?thank?. For example,
a resolved thread might end with gratitude to
thank others for providing help, and indeed we
see this. Thus, we specify a set of observed
indicators that may be useful in a latent variable
model of politeness. (1) Start with Thanks: x
Stx
,
25
Var T Description Var T Description Var T Description
Pae N Please Count Qa1 N 1st Match Score Svt N Question Votes
Thx N Thanks Count Qa2 N 2nd Match Score Mvt N Max Votes
Dfe N Deference Qa3 N 3rd Match Score Uvt N User Votes
Etx B End with Thx Len N Max Length Rep N Reply Number
Stx B Start with Thx Sim N Similarity Res N Resolved Count
Exp B Expert Join Tvt N Total Votes Pst N Post Number
Sin - Starter Influence Epr - Expert Participation Con - Content Matching
Pop - Thread Popularity Fen - Friendliness Label B Resolved or not
Table 1: Variables used in the Structural Equation Model (SEM). Var is the factor variable that is used,
which also corresponds to Figure 2. T indicates what type of values a variable can take. B is short for
Binary. N is short for Numeric. ?-? means it is a latent unobserved variable.
indicates whether this starter shows politeness
when he/she posted the question. (2) End with
Thanks: x
Eth
, stands for whether the starter says
thanks after receiving others? help. (3) Thanks
Count: x
Thx
, measures overall friendliness in the
current discussion. We evaluate this by counting
the thanking related words. (4) Deference: x
Dfe
, is
a count of positive polite words occurring in the dis-
cussion, such as using the words ?Nice?,?Great?, or
?Awesome?, as in prior work (Danescu-Niculescu-
Mizil et al., 2013). Such words are used as markers
to conduct counting. (5) Please: x
Pae
, captures
whether friendly question asking words were used,
i.e. how many times words such as ?Please?, ?Will?,
occur in current conversation.
4.5 Content Matching
Matches between the content of a thread and its
replies indicate whether replies are relevant to
answering the question instead of some off-topic
discussion. In order to estimate this, we build
an Eigenword bipartite graph to capture semantic
similarities. Each node in the bipartite graph is the
corresponding Eigenword
3
of a given word, with
the left side representing the words that occurred
in the thread starter, and the right side representing
the words in a given reply. The edge is a similarity
score computed by using the cosine similarity
metric. In order to better identify whether a reply is
discussing the content of the question, a semantic
match between the thread question and its replies is
needed. The top 3 matching scores are denoted as
x
Qa1
, x
Qa2
, x
Qa3
. Additionally, TF-IDF similarity
x
Sim
is computed (the correlation between x
Sim
and Qa1, Qa2, Qa3 are 0.3280, 0.3572, 0.3569
separately) and the maximum answer length x
Len
3
http://www.cis.upenn.edu/ ungar/eigenwords/
is used to assist in computing the matching score.
5 Experimental Investigation
In the above section, we described five latent fac-
tors we hypothesize are important in distinguishing
resolved and unresolved threads along with sets
of associated observed variables. In this section,
we conduct two studies on thread resolveability,
including validating the influence of each latent
factor on thread resolution using a Structural Equa-
tion Model (SEM), and evaluating the generality
of the identification of the resolveability using a
predictive model. Experiments are conducted on
the Python dataset with performance measurement
under different evaluation metrics.
5.1 Conceptual SEM Validation
Our conceptual model is implemented as a Struc-
tural Equation Model (SEM) and is introduced as
an evaluations of the effect of each latent factor on
thread resolveability, as shown in Figure 2.
5.1.1 Conceptual SEM Model
A Structural Equation Model (Bollen, 1987), is
a statistical technique for testing and estimating
correlational (and sometimes causal) relations in
cross sectional datasets. To explore the influence of
our five latent factors, we take advantage of SEM
to formalize the conceptual structure in order to
measure what contributes to thread resolveability.
The designed latent factors are specified as latent
variables within the model, with the associated
observed variables discussed above. We define the
conceptual structure of how a thread gets resolved
as well as a mathematical expression of each latent
variable in Equation 2.
Related variables are explained above and
26
Figure 2: SEM Model Factor Analysis Result. Each directed edge indicates the predictive relationship. Weight on each
directed edge is the estimated influence strength of one node to another. Table 1 illustrates the denotation. Only significant node
influences whose p-value (p < 0.05) are presented. Circles stand for latent variables while rectangles signify observed variable.
summarized in Table 1. Label refers to the
label of a unknown thread, taking the value of
Resolved or Unresolved. Label (L) is a linear
combination of each latent factor set. For each
variable in a latent factor set, it is associated with a
weight parameter ? in the SEM. Specifically, this
conceptual structure of how a thread gets resolved
relates to five aspects, i.e. (1) whether the thread
starter has enough influence on others, (2) whether
the relevant experts participated at least once in
the discussion, (3) whether the thread polite and
conducive to encouraging others to be willing to
provide help, (4) whether the thread is popular,
and (5) whether replies aim at answering questions
instead of off topic discussion.
Con = ?
ci
3
?
i=1
x
Qai
+ ?
c4
x
Sim
+ ?
c5
x
Len
Fen = ?
p1
x
Stx
+ ?
p2
x
Etx
+ ?
p3
x
Thx
+ ?
p4
x
Dfe
+ ?
p5
x
Pae
Sin = ?
u1
x
Rep
+ ?
u2
x
Pst
+ ?
u3
x
Res
+ ?
u4
x
Uvt
Pop = ?
t1
x
Cmt
+ ?
t2
x
Tvt
+ ?
t3
x
Mvt
+ ?
t4
x
Svt
Epr = ?
a0
x
Exp
Label = ?
1
Con+ ?
2
Fen+ ?
3
Sin
+ ?
4
Pop+ ?
5
Epr
(2)
5.1.2 SEM Result Analysis
In this section, we discuss what we learn from the
SEM about the influence of each factor within the
model. We adopt the Structural Equation Model in
R (Rosseel, 2012) to conduct the validation, and
evaluate it by looking at the Comparative Fit Index
(CFI), Root Mean Square Error of Approximation
(RMSEA) and Standardized Root Mean Square
Residual (SRMR) (Barrett, 2007). Figure 2 shows
the influence of each observed variable on its
corresponding latent variable, and in turn the latent
variable on the resolved label. The weights on
each directed edge represent the standard estimated
parameter for measuring the influence. For the
model fitting, we get a RMSEA of 0.09 and SRMR
of 0.06, with a CFI of 0.89. The fit is not extremely
high, but it is moderate, and it is within the range
one would expect from a good fitting model when
a large set of variables is considered.
Based on Figure 2, firstly, starter influence
and expert participation contribute a lot to thread
resolveability, with a standard estimated parameter
of 0.619 and 0.587. This makes sense that who
posts the question and who gives replies matter
a lot in identifying whether a thread is resolved.
Next, content matching contributes 0.178 to the
resolving of a thread, which means matching
between question and replies does differentiate
between resolved and unresolved threads, but less
so than who participates, perhaps because the
observed variables are very shallow indicators
of relevance. Friendliness is not very strongly
predictive of resolvability. Similarly, Thread
popularity contributes only 0.051 to the prediction,
without significant influence compared to the other
four latent variables, which are all significant.
Thus we conclude that starter influence, expert
participation, and content matching are strong
factors while friendliness and thread popularity
could help us separate resolved and unresolved
27
threads, but less so than the other two.
5.2 Resolveability Prediction
The influences of five latent factors on thread
resolveability are demonstrated as above. In this
part, we build an ensemble logistic regression
model to leverage those findings to predict whether
a given thread is resolved or not.
5.2.1 Ensemble Regression Model
An ensemble logistic regression model is proposed
to deal with the prediction of whether a thread
is resolved or not. That is, given the question
and a set of potential replies, as well as the five
latent variables and associated observed variables,
we want to predict whether a question has been
answered. Our ensemble logistic model works
in the following way. Firstly we train a separate
logistic model for each of the five aspects defined
above, i.e. five sub logistic model of how each
aspect predicts the resolved property. Then those
sub-models are included together in an ensemble
in order to contribute to a final logistic model,
which takes those results as the input features.
Similar to generalized boosting (Friedman et al.,
1998), this regression model integrates five weak
predictors that capture five different aspects of
thread resolveability, and construct a two layer
logistic ensemble, which is distinct from a linear
voting strategy. Our ensemble model relaxes
the assumption of linearity and thus offers more
flexibility in finding an effective predictive model.
This process is formalized below.
?
R
j
=
1
1 + e
?
?
k
i=1
?
i
?
?
R
ij
(3)
Here, k refers to the number of latent aspects.
?
R
j
is the predicted resolved score for thread j; if it
is larger than a threshold, the prediction of that
thread question is resolved, otherwise it remains
unresolved.
?
R
ij
is the predicted resolved score
of latent factor set i on thread j, trained on the
corresponding latent factor set.
5.2.2 Prediction Results
To demonstrate the predictive abilities of the five
latent factors, we use our ensemble regression
model to predict thread resolution. 10-fold cross
validation is used, and the prediction results will be
evaluated using the metrics Recall, Precision, and
AUC (Area under Curve). For baselines, we begin
with the simplest model EndThx, which simply
Single Model Precision Recall AUC
Si 0.697 0.696 0.791
Ep 0.602 0.590 0.572
Ct 0.626 0.616 0.647
Tp 0.594 0.579 0.626
Fr 0.639 0.633 0.685
Table 2: Prediction Result of Single Latent Factor
Model Precision Recall AUC
EndThx 0.629 0.612 0.593
Si + Ep 0.803 0.802 0.857
Si+Ep+Ct 0.819 0.815 0.884
Si+Ep+Ct+Fr 0.823 0.823 0.893
ALL-Linear 0.826 0.826 0.894
ALL-Ensemble 0.831 0.831 0.896
Table 3: Prediction Result
bases the prediction on whether the current thread
ends up with a gratitude sentence. This makes
sense because it is natural that students will express
their gratitude after receiving others? help. One
simple baseline is the Majority, which predicts the
testing thread as the majority status (unresolved in
our dataset), leading to a accuracy of 0.503; Si+Ep
is a combination of the latent aspect of starter
influence and expert participation; and Si+Ep+Ct
adds the content matching latent set on Si+Ep;
Si+Ep+Ct+Fr is defined similarly. ALL-Linear
is adding all five latent factor sets and predicts the
resolved or not using a linear logistic regression.
Comparably, ALL-Ensemble is trained using the
nonlinear ensemble logistic regression model. The
combination results as well as a comparison are
summarized in Table 3. For the influence of each
single latent aspect on the same prediction task, we
present them correspondingly in Table 2, where
Si, Ep, Ct, Tp, and Fr represent Student Influence,
Expert Participation, Content Matching, Thread
Populratiy, and Friendliness respectively.
Looking at the five latent aspects, (1) we con-
clude that, starter influence has the most powerful
influence on thread resolution. It improves a lot
on the Precision metric, and 50.25% on AUC
compared to the EndThx. It makes sense that,
if a user posts a lot, and often helps answer others?
questions, it is more likely that his/her question will
get a lot attention; (2) Thread Popularity, by itself
works better than the baseline under the metric of
AUC. The features in this set are not so directly
28
connected to thread resolution from a conceptual
standpoint compared to whether a thread ends with
thanks. However, it unexpectedly achieves an AUC
of 0.626, which is higher than the baseline. (3) For
content matching, the precision is similar to that
of EndThx, but in contrast, this model achieves
a good improvement on AUC. Content matching
describes the similarities between a question and
a reply, which is a direct indication of whether
the reply is trying to answer the question. (4)
Friendliness has a significant predictive ability in
connection with thread resolution. For the AUC, it
offers about a 13% improvement over the baseline.
It is reasonable that a resolved thread tends to
be more polite, which means people use ?please?,
?thanks? more than in other unresolved threads.
To build the ensemble models, we combine the
latent factor sets in the order of their strength
of estimated influence on resolveability. We
firstly integrate the starter influence and expert
participation, as we can see, it achieves significant
improvement over the simpler baselines, with 28%
higher on Precision, 31% on Recall and 45%
on AUC. It even performs better on the three
metrics than any of the single models in Table2.
Si+Ep+Ct also gives a substantial increase on
the metrics and when adding semantic content
matching, Si+Ep+Ct+Fr is about 3% better than
Si+Ep on precision and recall. This indicates that
friendliness and content matching are capturing
different aspects of the thread resolveability from
starter influence and expert participation. Besides,
the ALL-Linear performs best among all one layer
regression models. This shows that even though
thread popularity contributes least to resolved or
not based on the SEM result, it gives a different
perspective of the thread resolveability and is not
to be ignored. When we applied our proposed
ensemble regression model ALL-Ensemble using
the five latent factor sets, we find that it outperforms
all one layer logistic regressors, especially in Recall
and Precision. This demonstrates that the two-
layer ensemble logistic regression model?s added
representational power is needed for this problem.
6 Conclusions and Future Research
In this paper, we have focused on improving the
thread resolveability in MOOC discussion forums.
Our investigation is divided into two separate
studies that leverage a common conceptual model
involving five latent factors that are associated with
thread resolution. Our first study validates the
five latent variable structures using a SEM model,
which helps us to validate our assumptions and
hone in on those factors that are most promising
to leverage in subsequent work. It enables us
to assess the relative strength of each factor?s
influence on thread resolveability, and provides
a foundation for the other study. The second
study?s focus is predicting thread resolution based
on the first phase?s findings. In addition to
serving as a test of generality from trained data to
unseen data, the predictive model may also have a
practical benefit. In particular, thread resoveability
identification could provide the potential to achieve
a better allocation of valuable human resources to
work on unresolved threads, which increases the
potential for students to get their support needs
met in Massive Open Online Courses. Our work
is contenxtualized in the specifics of MOOCs
as an online context including the particulars of
interaction practices within those contexts. Thus,
in addition to building on existing QA work in
our feature engineering, we also introduce new
directions, such as the linguistic modeling of
speaker politeness, and conduct forms of latent
semantic matching that have proven effective in
dialogue systems.
However, we believe there is a need for further
modeling in order to fully understand thread
resolveability. A limitation of the current work is
that it was conducted in only one course. Thus,
we will be in a stronger position for moving
forward if we explicitly address the question of
generalizability across courses with further corpus
based investigation. Besides, how to transfer
the prediction models from forums with resolved
buttons to ones that have no such affordances,
which may be challenging because of differences
in the distribution of behaviors.
Acknowledgement
This research was funded in part by NSF grants
IIS-1320064 and OMA-0836012 and funding from
Google.
References
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceedings
of the 2008 International Conference on Web Search
and Data Mining, WSDM ?08, pages 183?194, New
York, NY, USA. ACM.
29
Eugene Agichtein, Yandong Liu, and Jiang Bian.
2009. Modeling information-seeker satisfaction in
community question answering. ACM Trans. Knowl.
Discov. Data, 3(2):10:1?10:27, April.
Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg,
and Jure Leskovec. 2012. Discovering value from
community activity on focused question answering
sites: A case study of stack overflow. In Pro-
ceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data
Mining, KDD ?12, pages 850?858, New York, NY,
USA. ACM.
Girish Balakrishnan. 2013. Predicting student reten-
tion in massive open online courses using hidden
markov models. Master?s thesis, EECS Department,
University of California, Berkeley, May.
Paul Barrett. 2007. Structural equation modelling:
Adjudging model fit. Personality and Individual
Differences, 42(5):815?824.
Kenneth A Bollen. 1987. Total, direct, and indirect
effects in structural equation models. Sociological
methodology, 17(1):37?69.
Eric Brill, Susan Dumais, and Michele Banko. 2002.
An analysis of the askmsr question-answering sys-
tem. In Proceedings of the ACL-02 Conference on
Empirical Methods in Natural Language Processing
- Volume 10, EMNLP ?02, pages 257?264, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Moira Burke and Robert Kraut. 2008. Mind your
ps and qs: The impact of politeness and rudeness
in online communities. In Proceedings of the
2008 ACM Conference on Computer Supported
Cooperative Work, CSCW ?08, pages 281?284, New
York, NY, USA. ACM.
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In ACL (1), pages 250?
259.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
1998. Additive logistic regression: a statistical view
of boosting. Annals of Statistics, 28:2000.
Peter Jansen, Mihai Surdeanu, and Peter Clark. 2014.
Discourse complements lexical semantics for non-
factoid answer reranking. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics, ACL ?13=4. Association for
Computational Linguistics.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and
Soyeon Park. 2006. A framework to predict the
quality of answers with non-textual features. In
Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?06, pages 228?235,
New York, NY, USA. ACM.
Yutaka Kabutoya, Tomoharu Iwata, Hisako Shiohara,
and Ko Fujimura. 2010. Effective question recom-
mendation based on multiple features for question
answering communities. In ICWSM.
Ren?e F Kizilcec, Chris Piech, and Emily Schneider.
2013. Deconstructing disengagement: analyzing
learner subpopulations in massive open online cours-
es. In Proceedings of the Third International
Conference on Learning Analytics and Knowledge,
pages 170?179. ACM.
Ann Lieberman. 1995. Practices that support
teacher development: Transforming conceptions of
professional learning. Innovating and Evaluating
Science Education: NSF Evaluation Forums, 1992-
94, page 67.
Yandong Liu and Eugene Agichtein. 2008. You?ve
got answers: Towards personalized models for pre-
dicting success in community question answering.
In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics on
Human Language Technologies: Short Papers, HLT-
Short ?08, pages 97?100, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Qiaoling Liu, Eugene Agichtein, Gideon Dror, Evgeniy
Gabrilovich, Yoelle Maarek, Dan Pelleg, and Idan
Szpektor. 2011. Predicting web searcher satisfac-
tion with existing community-based answers. In
Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ?11, pages 415?424, New
York, NY, USA. ACM.
Rivindu Perera. 2012. Ipedagogy: Question answering
system based on web information clustering. In
Technology for Education (T4E), 2012 IEEE Fourth
International Conference on, pages 245?246. IEEE.
Chris Piech, Jonathan Huang, Zhenghao Chen, Chuong
Do, Andrew Ng, and Daphne Koller. 2013. Tuned
models of peer assessment in MOOCs. In Pro-
ceedings of The 6th International Conference on
Educational Data Mining (EDM 2013).
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by predictive
annotation. In Proceedings of the 23rd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
?00, pages 184?191, New York, NY, USA. ACM.
Mingcheng Qu, Guang Qiu, Xiaofei He, Cheng Zhang,
Hao Wu, Jiajun Bu, and Chun Chen. 2009.
Probabilistic question recommendation for question
answering communities. In Proceedings of the
18th International Conference on World Wide Web,
WWW ?09, pages 1229?1230, New York, NY, USA.
ACM.
Carolyn Penstein Ros?e, Ryan Carlson, Diyi Yang,
Miaomiao Wen, Lauren Resnick, Pam Goldman,
and Jennifer Sherer. 2014. Social factors that
30
contribute to attrition in moocs. In Proceedings
of the first ACM conference on Learning@ scale
conference, pages 197?198. ACM.
Yves Rosseel. 2012. lavaan: An r package for
structural equation modeling. Journal of Statistical
Software, 48(2):1?36, 5.
Chirag Shah and Jefferey Pomerantz. 2010. Evalu-
ating and predicting answer quality in community
qa. In Proceedings of the 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?10, pages 411?418,
New York, NY, USA. ACM.
Juyup Sung, Jae-Gil Lee, and Uichin Lee. 2013.
Booming up the long tails: Discovering potentially
contributive users in community-based question an-
swering services. In ICWSM.
Qiongjie Tian, Peng Zhang, and Baoxin Li. 2013.
Towards predicting the best answers in community-
based question-answering services. In Emre Kici-
man, Nicole B. Ellison, Bernie Hogan, Paul Resnick,
and Ian Soboroff, editors, ICWSM. The AAAI Press.
Miaomiao Wen, Diyi Yang, and Carolyn Penstein Ros?e.
2014. Linguistic reflections of student engagement
in massive open online courses. In Proceedings of
the International Conference on Weblogs and Social
Media.
Diyi Yang, Tanmay Sinha, David Adamson, and
Carolyn Penstein Rose. 2013. turn on, tune
in, drop out: Anticipating student dropouts in
massive open online courses. In Workshop on Data
Driven Education, Advances in Neural Information
Processing Systems 2013.
Diyi Yang, Miaomiao Wen, and Carolyn Rose. 2014.
Peer influence on attrition in massive open online
courses. In Proceedings of Educational Data
Mining.
Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and
Andrzej Pastusiak. 2013. Question answering using
enhanced lexical semantic models. In Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 1744?1753, Sofia, Bulgaria, August.
Association for Computational Linguistics.
31
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 39?41,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Shared Task on Prediction of Dropout Over Time in Massively Open 
Online Courses 
 
Carolyn P. Ros? 
Language Technologies Institute  
and Human-Computer Interaction Institute 
Carnegie Mellon University 
5000 Forbes Avenue, Pittsburgh, PA 15213 
cprose@cs.cmu.edu 
George Siemens 
Center for Distributed Education  
University of Texas at Arlington 
701 South Nedderman Drive, Arlington, TX 
76019 
gsiemens@uta.edu 
 
  
 
  
Abstract 
The shared task on Prediction of Dropout 
Over Time in MOOCs involves analysis 
of data from 6 MOOCs offered through 
Coursera.  Data from one MOOC with ap-
proximately 30K students was distributed as 
training data and consisted of discussion fo-
rum data (in SQL) and clickstream data (in 
JSON format). The prediction task was Pre-
dicting Attrition Over Time. Based on behav-
ioral data from a week?s worth of activity in a 
MOOC for a student, predict whether the stu-
dent will cease to actively participate after 
that week.  This paper describes the task.  
A full write up of the results is published 
separately (Ros? & Siemens, 2014). 
 
1 Overview 
Research on Massively Open Online Courses 
(MOOCs)1 is an emerging area for real world impact 
of technology for analysis of social media at a large 
scale (Breslow et al., 2013).  Modeling user experi-
ence in MOOCs supports research towards under-
standing user needs better so that experiences that are 
more conducive to learning can be offered.  Beyond 
that, automated analyses enable adaptive technology 
to tailor the experience of users in real time (Ros? et 
al., 2014a).  This paper describes a shared task de-
signed to enlist the involvement of the language tech-
nologies community in this endeavor and to identify 
what value expertise within the field might bring. 
 
                                                 
1 http://www.moocresearch.com/reports 
One area for impact of natural language processing in 
the MOOC space is in modeling behavior within the 
threaded discussion forums.   In a typical MOOC, 
between 5% and 10% of students actively participate 
in the threaded discussion forums.  Previously pub-
lished research demonstrates that characteristics of 
posting behavior are predictive of dropout along the 
way (Ros? et al., 2014b; Wen et al., 2014a; Wen et 
al., 2014b; Yang et al., 2013; Yang et al., 2014).  
However, ideally, we would like to make predictions 
for the other 90% to 95% of students who do not post.  
Thus, in this shared task, we challenge participants to 
use models of social interaction as displayed through 
the text-based interaction between students in the 
threaded discussions (from the minority of students 
who participate in them) to make meaning from the 
clickstream data we have from all students.  If the 
discussion data can be thus leveraged to make more 
effective models of the clickstream data, then a mean-
ingful prediction about drop out along the way can 
also be made about the students who do not post to 
the discussion forums.   
 
One of the biggest challenges in the shared task is that 
the participants were only given data from one 
Coursera MOOC as training and development data.  
Their task was to produce a predictive model that 
could be applied to data from other MOOCs they did 
not have access to.  A separate report describes a de-
tailed analysis of the results applying submitted mod-
els to each of 5 test MOOCs (Ros? & Siemens, 2014).   
 
12 research teams signed up for the shared task, in-
cluding an international assortment of academic and 
industrial teams.  Out of these 12 teams, only 4 sub-
mitted final models (Sinha et al., 2014; Sharkey & 
Sanders, 2014; Amnueypornsakul et al., 2014; Kloft 
et al., 2014 ).  
 
In the remainder of this paper we describe the shared 
task in greater detail and discuss plans for future re-
lated research. 
39
2 Shared Task 
Participants in the shared task were given a complete 
SQL dump and clickstream dump from one Coursera 
MOOC as training data.  The student-week was the 
unit of analysis.  In other words, a prediction was 
made for each student for each week of their active 
participation to predict whether that week was the last 
week of their active participation.  Scripts were pro-
vided to parse the data into a form that could be used 
for the task, e.g., aggregating entries per user per 
week.  Scripts were also provided for running a test of 
the trained model on test data.  The purpose of the 
scripts was to standardize the way in which each 
team?s work would later be evaluated on the test 
MOOCs that participants did not have access to.   
 
A major part of the work in doing the task is in de-
termining what an effective representation would be 
of the behavior trace associated with each student-
week that would enable making an accurate predic-
tion.  In other words, the question is what are the dan-
ger signs that a student is especially vulnerable to 
drop out? The rules of the task were such that the in-
formation the model was allowed to use for making 
the prediction could be extracted from the whole par-
ticipation history of all training students (including 
both the SQL data and the clickstream data) up to and 
including the week a prediction was being made for.   
 
Each of the four finalist teams submitted a final model 
trained on the training MOOC and a write up include-
ing result trained on a designated subset of students 
from the training MOOC and tested on the remaining 
students.  Results were presented in terms of preci-
sion, recall, and fmeasure for the held out users. 
 
We recommend that participants make use of the text 
data to bootstrap effective models that use only click-
stream data.  However, participants were welcome to 
leverage either type of data in the models they submit-
ted.  In our evaluation presented separately (Ros? & 
Siemens, 2014), we evaluated the models on the test 
MOOCs in three different ways: First, an evaluation 
was conducted on data from students who actively 
participated in the discussion forums.  Second, an 
evaluation was conducted on data from students who 
never participated in the discussion forums.  And fi-
nally, and evaluation was conducted on the set of stu-
dents that includes both types of students. 
 
Each submission consisted of a write up describing 
the technical approach and a link to a downloadable 
zip file containing the trained model and code and/or 
a script for using the trained model to make predic-
tions about the test sets.  The code was required to be 
runnable by launching a single script in Ubuntu 12.04.  
A code stub for streamlining the preparation of the 
submission was distributed with the data.  The follow-
ing programming languages were acceptable: R 3.1, 
C++ 4.7, Java 1.6, or Python 2.7.  The script was re-
quired to be able to run within 24 hours on a 2400 
MHz machine with 6 cores.   
3 Looking Forward 
Computational modeling of massive scale social in-
teraction (as in MOOCs and other environments for 
learning at scale) has the potential to yield new 
knowledge about the inner-workings of interaction in 
such environments so that support for healthy com-
munity formation can be designed and built.  Howev-
er, the state-of-the-art in graphical models applied to 
large scale social data provides representations of the 
data that are challenging to interpret in light of specif-
ic questions that may be asked from a learning scienc-
es or social psychological perspective.  What is need-
ed are new methodologies for development and inter-
pretation of models that bridge expertise from ma-
chine learning and language technologies on one side 
and  learning sciences, sociolinguistics, and social 
psychology on the other side.  The field of language 
technologies has the human capital to take leadership 
in making these breakthroughs.   
 
The shared task described in this paper is the first one 
like it where a data set from a Coursera MOOC has 
been made publically available so that a wide range of 
computational modeling techniques can be evaluated 
side by side (Ros? & Siemens, 2014).  However, there 
is recognition that such shared tasks may play an im-
portant role in shaping the future of the field of Learn-
ing Analytics going forward (Pea, 2014). 
 
One of the major challenges in running a shared task 
like this is ensuring the protection of privacy of the 
MOOC participants.  Such concerns have been the 
focus of much discussion in the area of learning at 
scale (Asilomar Convention, 2014).   
 
Data sharing ethics were carefully considered in the 
design of this shared task.  In particular, all of the 
students who participated in the MOOC that produced 
the training data were told that their data would be 
used for research purposes.  The data was carefully 
preprocessed to remove personal identifiers about the 
students and the university that hosted the course.  All 
of the workshop participants who got access to the 
data were required to participate in human subjects 
training and to agree to use the data only for this 
workshop, and not to share it beyond their team.  Data 
was shared through a secure web connection.  Ap-
proval for use of the data in this fashion was approved 
by the Institutional Review Board of the hosting uni-
versity as well as the university that ran the MOOC. 
 
It was a goal in development of this shared task to 
serve as a forerunner in what we hope will become a 
more general practice of community wide collabora-
tion on large scale learning analytics (Suthers et al., 
2013). 
40
Acknowledgements 
The authors would like to thank Norman Bier for as-
sistance in working through the data sharing logistics.  
This work was funded in part by NSF Grant OMA-
0836012. 
References 
Amnueypornsakul, B., Bhat, S., & Chinprutthiwong, 
P. (2014).  Predicting Attrition Along the Way: 
The UIUC Model, in Proceedings of the 2014 Em-
pirical Methods in Natural Language Processing 
Workshop on Modeling Large Scale Social Interac-
tion in Massively Open Online Courses, Qatar, Oc-
tober 2014. 
Asilomar Convention (2014).  The Asilomar Conven-
tion for Learning Research in Higher Education, 
June 13, 2014. 
Breslow, L., Pritchard, D., De Boer, J., Stump, G., 
Ho, A., & Seaton, D. (2013).  Studying Learning in 
the Worldwide Classroom : Research into edX?s 
First MOOC, Research & Practice in Assessment 
(8). 
Kloft, M., Stiehler, F., Zheng, Z., & Pinkward, N. 
(2014). Predicting MOOC Dropout over Weeks 
Using Machine Learning Methods, in Proceedings 
of the 2014 Empirical Methods in Natural Lan-
guage Processing Workshop on Modeling Large 
Scale Social Interaction in Massively Open Online 
Courses, Qatar, October 2014. 
Pea, R. (2014).  The Learning Analytics Workgroup: 
A Report on Building the Field of Learning Analyt-
ics for Personalized Learning at Scale, Stanford 
University. 
Ros?, C. P. & Siemens, G. (2014). Shared Task Re-
port : Results of the EMNLP 2014 Shared Task on 
Predictions of Dropout Over Time in MOOCs, 
Langauge Technologies Institute Technical Report. 
Ros?, C. P., Goldman, P., Sherer, J. Z., Resnick, L. 
(2014a).  Supportive Technologies for Group Dis-
cussion in MOOCs, Current Issues in Emerging 
eLearning, Special issue on MOOCs, December 
2014. 
Ros?, C. P., Carlson, R., Yang, D., Wen, M., Resnick, 
L., Goldman, P. & Sherer, J. (2014b).Social Fac-
tors that Contribute to Attrition in MOOCs, in Pro-
ceedings of the First ACM Conference on Learning 
@ Scale. 
Sinha, T., Li, N., Jermann, P., & Dillenbourg, P. 
(2014).  Capturing ?attrition intensifying? structural 
traits from didactic interaction sequences of 
MOOC learners, in Proceedings of the 2014 Em-
pirical Methods in Natural Language Processing 
Workshop on Modeling Large Scale Social Interac-
tion in Massively Open Online Courses, Qatar, Oc-
tober 2014. 
Sharkey, M. & Sanders, R. (2014).  A Process for 
Predicting MOOC Attrition, in Proceedings of the 
2014 Empirical Methods in Natural Language 
Processing Workshop on Modeling Large Scale 
Social Interaction in Massively Open Online 
Courses, Qatar, October 2014. 
Suthers, D., Lund, K., Ros?, C. P., Teplovs, C., Law, 
N. (2013).  Productive Multivocality in the Analy-
sis of Group Interactions, edited volume, Springer. 
Wen, M., Yang, D., & Ros?, C. P. (2014b). Linguistic 
Reflections of Student Engagement in Massive 
Open Online Courses, in Proceedings of the Inter-
national Conference on Weblogs and Social Media  
Wen, M., Yang, D., & Ros?, C. P. (2014a). Sentiment 
Analysis in MOOC Discussion Forums: What does 
it tell us? in Proceedings of Educational Data Min-
ing. 
Yang, D., Sinha, T., Adamson, D., & Ros?, C. P. 
(2013). Turn on, Tune in, Drop out: Anticipating 
student dropouts in Massive Open Online Courses, 
in NIPS Data-Driven Education Workshop. 
Yang, D., Wen, M., & Ros?, C. P. (2014). Peer Influ-
ence on Attrition in Massively Open Online Cours-
es, in Proceedings of Educational Data Mining. 
 
41
