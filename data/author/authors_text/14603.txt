Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513?523,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Title Generation with Quasi-Synchronous Grammar
Kristian Woodsend, Yansong Feng and Mirella Lapata
School of Informatics, University of Edinburgh
Edinburgh EH8 9AB, United Kingdom
k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
The task of selecting information and render-
ing it appropriately appears in multiple con-
texts in summarization. In this paper we
present a model that simultaneously optimizes
selection and rendering preferences. The
model operates over a phrase-based represen-
tation of the source document which we ob-
tain by merging PCFG parse trees and depen-
dency graphs. Selection preferences for in-
dividual phrases are learned discriminatively,
while a quasi-synchronous grammar (Smith
and Eisner, 2006) captures rendering prefer-
ences such as paraphrases and compressions.
Based on an integer linear programming for-
mulation, the model learns to generate sum-
maries that satisfy both types of preferences,
while ensuring that length, topic coverage and
grammar constraints are met. Experiments on
headline and image caption generation show
that our method obtains state-of-the-art per-
formance using essentially the same model for
both tasks without any major modifications.
1 Introduction
Summarization is the process of condensing a source
text into a shorter version while preserving its infor-
mation content. Humans summarize on a daily ba-
sis and effortlessly, yet the automatic production of
high-quality summaries remains a challenge.
Most work today focuses on extractive summa-
rization, where a summary is created by identifying
and subsequently concatenating the most important
sentences in a document. The advantage of this ap-
proach is that it does not require a great deal of lin-
guistic analysis to generate grammatical sentences,
assuming the source document was well written.
Unfortunately, extracts generated this way are often
documents of low readability and text quality, and
contain much redundant information. The concise-
ness can be improved when sentence extraction is
interfaced with sentence compression, where words
and clauses are deleted based on rules typically op-
erating over parsed input (Jing, 2000; Daume? III
and Marcu, 2002; Lin, 2003; Daume? III, 2006; Zajic
et al, 2007; Martins and Smith, 2009).
An alternative abstractive or ?bottom-up? ap-
proach involves identifying high-interest words and
phrases in the source text, and combining them into
new sentences guided by a language model (Banko
et al, 2000; Soricut and Marcu, 2007). This ap-
proach has the potential to work well, breaking out
of the single-sentence paradigm. Unfortunately, the
resulting summaries are not always coherent ? indi-
vidual constituent phrases are often combined with-
out any semantic constraints ? or grammatical be-
yond the n-gram horizon imposed by the language
model.
Constituent deletion and recombination are
merely two of the many rewrite operations profes-
sional editors and abstractors employ when creating
summaries (Jing, 2002). Additional operations in-
clude truncating sentences, aggregating them, and
paraphrasing at word or syntax level. Furthermore,
professionals write summaries in a task-specific
style. News headlines for example are typically
short (three to six words), written in the present
tense and active voice, and often leave out forms of
the verb be. There are also different ways of writing
a headline either directly by stating what the docu-
513
ment is about or indirectly by raising a question in
the reader?s mind, which the document answers.
The automatic generation of summaries similar to
those produced by human abstractors is challenging
because of the many constraints imposed by the task:
the summary must be maximally informative and
minimally redundant, grammatical, coherent, adhere
to a pre-specified length and stylistic conventions.
Importantly, these constraints are conflicting; the
deletion of certain phrases may avoid redundancy
but result in ungrammatical output and information
loss.
In this paper we propose a model for summariza-
tion that attempts to capture and optimize these con-
straints jointly. We learn both how to select the
most important information (the content), and how
to render it appropriately (the style). Selection pref-
erences are learned discriminatively, while a quasi-
synchronous grammar (QG, Smith and Eisner 2006)
captures rendering preferences such as paraphrases
and compressions. The entire solution space of
possible extractions and QG-generated paraphrases
is searched efficiently through use of integer lin-
ear programming. The ILP framework allows us to
model naturally as constraints, additional require-
ments such as sentence length, overall summary
length, topic coverage and, importantly, grammati-
cality.
We argue that QG is attractive for describ-
ing rewrite operations common in summarization.
Rather than assuming a strictly synchronous struc-
ture over the source and target sentences, QG iden-
tifies a ?sloppy? alignment of parse trees assuming
that the target tree is in some way ?inspired by? the
source tree. A key insight in our approach is to
formulate the summarization problem at the phrase
level: both QG rules and information extraction op-
erate over individual phrases rather than (as is the
norm) sentences. At this smaller unit level, QG
rules become more widely applicable and compres-
sion falls naturally because only phrases deemed im-
portant should appear in the summary.
We evaluate the proposed model on headline gen-
eration and the related task of image caption gen-
eration. However, there is nothing inherent in our
formulation that is specific to those two tasks; it is
possible for the model to generate longer or shorter
summaries, for a single or multiple documents. Ex-
perimental results show that our method obtains
state-of-the-art performance, both in terms of gram-
maticality and informativeness for both tasks using
the same summarization model.
2 Related work
Much effort in automatic summarization has been
devoted to sentence extraction which is often for-
malized as a classification task (Kupiec et al, 1995).
Given appropriately annotated training data, a bi-
nary classifier learns to predict for each document
sentence if it is worth extracting. A few previ-
ous approaches have attempted to interface sentence
compression with summarization. A straightforward
way to achieve this is by adopting a two-stage ar-
chitecture (e.g., Lin 2003) where the sentences are
first extracted and then compressed or the other way
round.
Other work implements a joint model where
words are deleted and sentences selected from a doc-
ument simultaneously (Daume? III and Marcu, 2002;
Martins and Smith, 2009; Woodsend and Lapata,
2010). ILP models have also been developed for
sentence rather than document compression (Clarke
and Lapata, 2008). Dras (1999) discusses the appli-
cation of ILP to reluctant paraphrasing, i.e., the task
of choosing between paraphrases while conforming
to length, readability, or style constraints. Again,
the aim is to rewrite text without, however, con-
tent selection. Rewrite operations other than dele-
tion tend to be hand-crafted and domain specific
(Jing and McKeown, 2000). Notable exceptions are
Cohn and Lapata (2008) and Zhao et al (2009) who
present a model that can both compress and para-
phrase individual sentences without however gener-
ating document-level summaries.
Headline generation is a well-studied task within
single-document summarization, due to its promi-
nence in the DUC-03 and DUC-04 evaluation com-
petitions.1 Many approaches identify the most infor-
mative sentence in a given document (typically the
first sentence for the news genre) and subsequently
apply a form of sentence compression such that
the headline meets some length requirement (Dorr
1Approaches to headline generation are too numerous to list
in detail; see the proceedings of DUC-03 and DUC-04 for an
overview.
514
et al, 2003). The compressed sentence may also be
?padded? with important content words or phrases
to ensure that the topic of the document is covered
(Zajic et al, 2004). Other work generates headlines
in a bottom-up fashion starting from important, indi-
vidual words and phrases, that are glued together to
create a fluent sentence. For example, Banko et al
(2000) draw inspiration from Machine Translation
and generate headlines using statistical models for
content selection and sentence realization.
Relatively little work has focused on caption gen-
eration, a task related to headline generation. The
aim here is to create a short, title-like description of
an image embedded in a news article. Like head-
lines, captions have to be short and informative. In
addition, a good caption must clearly identify the
subject of the picture and establish its relevance to
the article. Feng and Lapata (2010a) develop ex-
tractive and abstractive caption generation models
that operate over the output of a probabilistic im-
age annotation model that preprocesses the pictures
and suggests keywords to describe their content.
Their best model is an extension of Banko et al?s
(2000) word-based model for headline generation to
phrases.
Our own work develops an ILP-based summariza-
tion model with rewrite operations that are not lim-
ited to deletion, are defined over phrases, and en-
coded in quasi-synchronous grammar. The QG for-
malism has been previously applied to parser adap-
tation and projection (Smith and Eisner, 2009), para-
phrase identification (Das and Smith, 2009), and
question answering (Wang et al, 2007); however
the use of QG in summarization is novel to our
knowledge. Unlike most synchronous grammar for-
malisms, QG does not posit a strict isomorphism be-
tween a source sentence and its target translation; it
only loosely links the syntactic structure of the two,
and is therefore well suited to describing the rela-
tionship between a document and its abstract. We
propose an ILP formulation which not only allows
to efficiently search through the space of many QG
rules but also to incorporate constraints relating to
content, style, and the task at hand.
3 Modeling
There are three components to our model. Content
selection is performed discriminatively; an SVM
learns which information in the source document
should be in the summary, and gives a real-valued
salience score for each phrase. QG rules are used
to generate compressions and paraphrases of the
source sentences. An ILP model combines the out-
put of these two components into an output sum-
mary, while optimizing content selection and surface
realization preferences jointly.
3.1 Document Representation
Our model operates on documents annotated with
syntactic information which we obtain by parsing
every sentence twice, once with a phrase structure
parser and once with a dependency parser. The out-
put from the two representations is combined into a
single data structure, by mapping the dependencies
to the edges of the phrase structure tree. The proce-
dure is described in detail in Woodsend and Lapata
(2010). However, we do not merge the leaf nodes
into phrases here, but keep the full tree structure,
as we will apply compression to phrases through
the QG. In our experiments, we obtain this com-
bined representation from the output of the Stan-
ford parser (Klein and Manning, 2003) but any other
broadly similar parser could be used instead.
3.2 Quasi-synchronous grammar
Given an input sentence S1 or its parse tree T1, the
QG constructs a monolingual grammar for parsing,
or generating, the possible translation (or here, para-
phrase) trees T2. A grammar node in the target tree
T2 is modeled on a subset of nodes in the source tree,
with a rather loose alignment between the trees.
In our approach, the process of learning the gram-
mar is unsupervised. Each sentence of the source
document is compared to each sentence in the target
document ? headline or caption, depending on the
task. Using the combined PCFG-dependency tree
representation described above, we build up a list of
leaf node alignments based on lexical identity, after
stemming and removing stop words. We align direct
parent nodes where more than one child node aligns.
A grammar rule is created if the all the nodes in the
target tree can be explained using nodes from the
515
(a) NP
NNP/nn
Saudi
JJ/amod
dissident
NNP/nn
Osama
NNP/nn
bin
NNP/?
Laden
NP
NNP/nn
bin
NNP/?
Laden
(b) PP/prep in
IN/?
in
DT/det
the
JJ/amod
disputed
NN/?
territory
PP/prep of
IN/?
of
NNP/nn
East
NNP/?
Timor
PP/prep in
IN/?
in
NNP/nn
East
NNP/?
Timor
(c) NP/dobj
DT/det
the
NN/?
extradition
PP/prep of
IN/?
of
NNP/nn
Kurdish
NN/nn
leader
NNP/?
Ocalan
NP/dobj
NP/poss
Ocalan?s
NN/?
extradition
Figure 1: Examples of QG alignments between
source node (left) and target node (right). (a) align-
ment of child nodes, involving compression through
deletion; (b) rewriting involving child and grand-
child nodes; (c) reordering of child nodes (with fur-
ther compression through applying other QG rules
on children). Nodes bear phrase and dependency la-
bels. Dotted lines show alignments in the grammar
between source and target child nodes. Examples
are taken from the QG rules discovered in the DUC-
03 data set of headlines.
source; this helps to improve the quality in what is
inherently a noisy process. Finally, QG rules are cre-
ated from aligned nodes above the leaf node level,
recording the phrase and dependency label of nodes,
and the alignment of child nodes.
Unlike previous work involving QG which has
used dependency graphs exclusively (e.g., Wang
et al 2007; Das and Smith 2009), our approach op-
erates over a combined PCFG-dependency represen-
tation. As a result, some configurations in Smith and
Eisner (2006) are not so relevant here ? instead,
we found that deletions, reorderings, flattening of
nodes, and the addition of text elements were im-
CHOICE/?
PP/prep in
IN/?
in
DT/det
the
JJ/amod
disputed
NN/?
territory
PP/prep of
IN/?
of
NNP/nn
East
NNP/?
Timor
PP/prep in
IN/?
in
NNP/nn
East
NNP/?
Timor
Figure 2: Alternative paraphrases are represented as
a CHOICE sub-tree.
portant operations for the grammar.
Figure 1 shows some example alignments that are
captured by the QG, with the source node on the
left and the target node on the right. Leaf nodes
have their original text, while other nodes have a
combined phrase and dependency label that they ob-
tain in the merged representation described in Sec-
tion 3.1 above (e.g., NP/dobj is a noun phrase and a
direct object, NNP/nn is a proper noun and a nomi-
nal modifier, whereas NN/? is a head noun). Align-
ments between the children are shown by dotted
lines. In Figure 1(a), some child nodes are aligned
while others are not present in the target tree. This
type of rule is common in our training data, and typ-
ically arises from the compression of names in noun
phrases. Another frequent compression, shown in
Figure 1(b), is flattening the tree structure by in-
corporating grand-child elements at the child level.
Figure 1(c) shows a rule involving the reordering
of child nodes, and where additional rules are ap-
plied recursively to achieve further compression and
a transformation in the phrase constituency.
Paraphrases are created from source sentence
parse trees by applying suitable rules recursively.
Suitable rules have matching structure in terms of
phrase and dependency label, for both the parent and
child nodes. Additionally, the proposed paraphrase
sub-tree must be suitable for the target tree being
created (i.e., the root node of the paraphrase must
match the phrase and dependency label of the corre-
sponding node in the target tree). Where more than
one paraphrase is possible, the alternatives are incor-
porated into the target parse tree under a CHOICE
node, as is shown in Figure 2. Note that unlike pre-
516
vious QG approaches, we do not use the probability
model proposed by Smith and Eisner (2006); instead
the QG is used to represent rewrite operations, and
we simply record a frequency count for how often
each rule is encountered in the training data.
3.3 ILP model
The objective of our model is to create the most in-
formative text possible, subject to constraints which
can be tailored to the specific task. These relate to
sentence length, overall summary length, the inclu-
sion of specific topics, and grammaticality. These
constraints are global in their scope, and cannot be
adequately satisfied by optimizing each one of them
individually. Our approach therefore uses an ILP
formulation which will provide a globally optimal
solution, and which can be efficiently solved using
standard optimization tools. Specifically, the model
selects phrases and paraphrases from which to form
the output sentence. Here, we focus on a single
sentence as this is most appropriate for title gener-
ation. However, multi-sentence output can be easily
generated by setting a summary length constraint.
The model operates over the merged phrase struc-
ture trees described in Section 3.1, augmented with
paraphrase choice nodes such as shown in Figure 2
rather than raw text.
Let S be the set of sentences in a document, P be
the set of phrases, and Ps ? P be the set of phrases
in each sentence s ? S . Let the sets Di ? P , ?i ? P
capture the phrase dependency information for each
phrase i, where each set Di contains the phrases that
depend on the presence of i. In a similar fashion,
C ? P is the set of choice nodes throughout the doc-
ument, which represent nodes in the tree where more
than one QG rule can be applied; Ci ? P , i ? C are
the sets of phrases that are direct children of each
choice node, in other words they are the individual
alternative paraphrases. Let li be the length of each
phrase i, in tokens.
For caption generation, the model has as addi-
tional input a list of tags (keywords drawn from the
source document) that correspond to the image, and
we refer to this set of tags as T . Pt ? P is the set of
phrases containing the tag t ? T . We use the proba-
bilistic image annotation model of Feng and Lapata
(2010a) to generate the list of keywords. The lat-
ter highlight the objects depicted in the image and
should be in all likelihood included in the caption.
The model is cast as an integer linear program:
max
x ?
i?P
( fi +?gi)xi (1a)
s.t. ?
i?P
lixi ? Lmax (1b)
?
i?P
lixi ? Lmin (1c)
?
i?Pt ,t?T
xi ? Tmin (1d)
x j? xi ?i ? P , j ?Di (1e)
?
j?Ci
x j = xi ?i ? C , j ? Ci (1f)
xi? ys ?s ? S , i ? Ps (1g)
?
s?S
ys ? NS (1h)
xi ? {0,1} ?i ? P (1i)
ys ? {0,1} ?s ? S . (1j)
A vector of binary variables x? {0,1}|P | indicates
if each phrase is to be part of the output. The vector
of auxiliary binary variables y ? {0,1}|S | indicates
from which sentences the chosen phrases come, see
Equation (1g).
Our objective function (1a) is the weighted sum of
two components for each phrase: a salience score,
and a measure of how frequently the QG rule was
seen in the training data. Let fi denote the salience
score for phrase i, determined by the machine learn-
ing algorithm. We apply a paraphrase penalty gi to
each phrase,
gi = log
(
nr
Nr
)
,
where nr is a count of the number of times this par-
ticular QG rule r was seen in the training data, and
Nr is the number of times all suitable rules for this
phrase node were seen. If no suitable rules exist,
we set gi = 0. The intuition here is that common
paraphrases should be more trustworthy, and thus
are given a smaller penalty than rare ones. Para-
phrase penalties are weighted by the constant param-
eter ?. which controls the amount of paraphrasing
we allow in the output. The objective function is
the sum of the salience scores and paraphrase penal-
ties of all the phrases chosen to form the output of a
given document, subject to the constraints in Equa-
517
tions (1b)?(1j). The latter provide a natural way of
describing the requirements the output must meet.
Constraints (1b) and (1c) ensure that the gener-
ated output stays within the acceptable length range
of (Lmin,Lmax) tokens. Equation (1d) is a set-
covering constraint, requiring that at least Tmin words
in T appear in the output. This is important where
we want to focus on some aspect of the source doc-
ument, for instance on the subject of an image.
Constraint (1e) ensures that the phrase dependen-
cies are respected and thus enforces grammatical
correctness. Phrases that depend on phrase i are con-
tained in the set Di. Variable xi is true, and therefore
phrase i will be included, if any of its dependents
x j ?Di are true. The phrase dependency constraints,
contained in the set Di and enforced by (1e), are the
result of three principles based on the typed depen-
dency information:
1. Where the QG provides alternative para-
phrases, it makes sense of course to select only
one. This is controlled by constraint (1f), and
by placing all paraphrases in the set Di for the
choice node i.
2. Where there are no applicable QG rules to
guide the model, in general we require all child
nodes j of the current node i to be included in
the summary if node i is included. As excep-
tions, we allow the subtree represented by node
j to be deleted if the dependency label for the
connecting edge i? j is of type advcl (adver-
bial clause) or some form of conj (conjunction).
3. In general, we force the parent node p of the
current node i to be included in the output if i
is, resulting in all ancestors up to the root node
being included. We allow a break, and the sub-
tree at i to be used as a stand-alone sentence, if
the PCFG parser has marked i with an S (sen-
tence) label.
Constraint (1g) tells the ILP to output a sentence if
one of its constituent phrases is chosen. Finally, (1h)
limits the output to a maximum of NS sentences.
4 Experimental Set-up
As mentioned earlier we evaluated the performance
of our model on two title generation tasks, namely
headline and caption generation. In this section we
give details on the corpora and grammars we used,
model parameters and features. We also describe the
baselines used for comparison with our approach,
and explain how system output was evaluated.
Training We obtained phrase-based salience
scores using a supervised machine learning algo-
rithm. For the headline generation task, the full
DUC-03 (Task 1) corpus was used for training;
it contains 500 documents and 4 headline-style
summaries per document. For the captions, training
data was gathered from the CNN news website.2
We used 200 documents and their corresponding
captions. Sentences were first tokenized to separate
words and punctuation, and then parsed to obtain
phrases and dependencies as described in Section 3
using the Stanford parser (Klein and Manning,
2003). Document phrases were marked as positive
or negative automatically. If there was a unigram
overlap (excluding stop words) between the phrase
and any of the original title or caption, we marked
this phrase with a positive label. Non-overlapping
phrases were given negative labels.
Our feature set comprised surface features such as
sentence and paragraph position information, POS
tags, and whether high-scoring tf.idf words were
present in the phrase. Additionally, the caption train-
ing set contained features for unigram and bigram
overlap with the title. We learned the feature weights
with a linear SVM, using the software SVM-OOPS
(Woodsend and Gondzio, 2009). This tool gave us
directly the feature weights as well as support vec-
tor values, and it allowed different penalties to be
applied to positive and negative misclassifications,
enabling us to compensate for the unbalanced data
set. The penalty hyper-parameters chosen were the
ones that gave the best F-scores, using 10-fold vali-
dation.
For each of the two tasks, QG rules were extracted
from the same data used to train the SVM, resulting
in 2,910 distinct rules for headlines and 2,757 rules
for the captions. Table 1 shows that for both tasks,
the majority of rules apply to PP and NP phrases.
Both tasks involve considerable compression, but
the proportions of the rewrite operations involved in-
dicate differences in style between them. Compared
2See http://edition.cnn.com/.
518
Label Prop?n Proportion for Label
of set Unmod Del Ins Re-ord
PP 40% 5% 93% 12% 6%
NP 31% 5% 87% 14% 7%
S 20% 1% 96% 15% 7%
SBAR 6% 4% 95% 28% 6%
(a) Headlines
Label Prop?n Proportion for Label
of set Unmod Del Ins Re-ord
PP 30% 17% 81% 7% 4%
NP 29% 17% 76% 11% 3%
S 27% 10% 84% 16% 6%
SBAR 10% 13% 80% 16% 3%
(b) Captions
Table 1: QG rules generated for (a) headline and
(b) caption tasks (top 4 labels shown). The columns
show label of root node, proportion of the full rule-
set, then the proportions of rules for this label in-
volving no modification, deletions, insertions and
re-orderings.
to headlines, captions involve slightly less deletion
and a higher proportion of the phrases are unmod-
ified. The QG learning mechanism also discovers
more alignments between source sentences and cap-
tions than it does for the headline task.
Title generation For the headline generation task,
we evaluated our model on a testing partition from
the DUC-04 corpus (75 documents, Task 1). For the
caption task, we used the test set (240 documents)
described in Feng and Lapata (2010a). Their corpus
was downloaded from the BBC news site and con-
tains documents, images, and their captions.3
We created and solved an ILP for each docu-
ment. For each phrase, features were extracted and
salience scores calculated from the feature weights
determined through SVM training. The distance
from the SVM hyperplane represents the salience
score. Parameters for the ILP models for the two
tasks are shown in Table 2. The ? parameter was
set to 0.2 to ensure that paraphrases were included;
other parameters were chosen to capture the prop-
3Available from http://homepages.inf.ed.ac.uk/
s0677528/data.html.
Parameter Headlines Captions
Min length Lmin 8 8
Max length Lmax 16 20
Min keywords Tmin 0 2
Max sentences NS 5 1
Paraphrase ? 0.2 0.1
Table 2: ILP model parameters for the two tasks.
erties seen in the majority of the training set. Note
the maximum number of sentences allowed to form
a headline is set to 5 as some of the headlines in the
DUC dataset contained multiple sentences.
To solve the ILP model we used the ZIB Opti-
mization Suite software (Achterberg, 2007; Koch,
2004). The solution was converted into a sentence
by removing nodes not chosen from the tree rep-
resentation, then concatenating the remaining leaf
nodes in order.
Model Comparison For the headline task, we
compared our model to the DUC-04 standard base-
line of the first sentence, truncated at the first word
boundary after 75 characters; and the output of the
Topiary system (Zajic et al, 2004), which came top
in almost all measures in the DUC-04 evaluation.
In order to generate a headline, Topiary first com-
presses the lead sentence using linguistically moti-
vated heuristics and then enhances it with topic key-
words. For the captions, we compared our model
against the highest-scoring document sentence ac-
cording to the SVM and against the probabilistic
model presented in Feng and Lapata (2010a). The
latter estimates the probability of a phrase appear-
ing in the caption given the same phrase appearing
in the corresponding document and uses a language
model to select among many different surface real-
izations. The language model is adapted with prob-
abilities from an image annotation model (Feng and
Lapata, 2010b).
Evaluation We evaluated the quality of the head-
lines using ROUGE (Lin and Hovy, 2003). The
DUC-04 dataset provides four reference head-
lines per document. We report unigram overlap
(ROUGE-1) and bigram overlap (ROUGE-2) as a
means of assessing informativeness, and the longest
common subsequence (ROUGE-L) as a means of as-
519
sessing fluency. Original DUC-04 ROUGE parame-
ters were used. We also use ROUGE to evaluate the
automatic captions with the original BBC captions
as reference.
In addition, we evaluated the generated headlines
by eliciting human judgments. Participants were
presented with a news article and its correspond-
ing headline and were asked to rate the latter along
two dimensions: informativeness (does the headline
capture the article?s most important information?),
and grammaticality (is it fluent and easy to under-
stand?). The subjects used a seven point rating scale;
an ideal system would receive high numbers for
both measures. We randomly selected twelve docu-
ments from the test set and generated headlines with
our model. We also included the output of Topiary
and the human written DUC-04 headlines as a gold
standard. We thus obtained ratings for 48 (12 ? 4)
document-highlights pairs.
We elicited judgments for the generated captions
in a similar fashion. Participants were presented
with a document, an associated image, and its cap-
tion, and asked to rate the latter (using a 1?7 rating
scale) with respect to grammaticality and informa-
tiveness (does it describe succinctly the content of
the image and document?). Again, we randomly se-
lected 12 document-image pairs from the test set and
generated captions for them using the highest scor-
ing document sentence according to the SVM, our
ILP-based model, and the output of Feng and Lap-
ata?s (2010a) system. We also included the original
BBC captions as an upper bound. Both studies were
conducted over the Internet using WebExp (Keller
et al, 2009). 80 unpaid volunteers rated the head-
lines and 65 the captions, all self reported native En-
glish speakers.
5 Results
We report results on the headline generation task in
Figure 3, with ROUGE-1, ROUGE-2 and ROUGE-
L. In ROUGE-1 and ROUGE-L measures, the best
scores are obtained by the Topiary system, slightly
better than the lead sentence baseline, while for
ROUGE-2 the ordering is reversed. Our model does
not outperform the lead sentence or Topiary. Note
that the 95% confidence level intervals reported by
ROUGE are so large that no results are statistically
Lead The chances for a new, strictly secular government in
Turkey faded Wednesday.
Topiary TURKEY YILMAZ PARTY ECEVIT chances strictly
secular government faded.
ILP Bulent Ecevit needs Turkey?s two-center right parties to
hammer together secular coalition.
DUC Chance for new, secular, Turkish government fades; what
will Ecevit do now?
Source Premier-designate Bulent Ecevit needs Turkey?s two-
center right parties to hammer together a secular coali-
tion, but Tansu Ciller, the ex-premier who commands 99
votes in parliament, rebuffed him Wednesday.
Lead U.S. President Bill Clinton won South Korea?s support
Saturday for confronting.
Topiary NUCLEAR U.S. President Bill Clinton won for con-
fronting North Korea.
ILP North Koreans have denied construction site has nuclear
purpose.
DUC U.S. warns N. Korea not to waste chance for peace over
alleged nuclear site.
Source The North Koreans have denied the underground con-
struction site has any nuclear purpose, and it has de-
manded a dlrs 300 million payment for proving that.
Lead By only one vote, the center-left prime minister of Italy,
Romano Prodi.
Topiary PRODI By only one vote center left prime minister and
toppled from power.
ILP Political system changes, Italy is condemned to political
instability.
DUC Prodi loses confidence vote; will stay as caretaker until
new government.
Source ?Unless the Italian political system changes, Italy is con-
demned to political instability,? said Sergio Romano, a
former diplomat and political science professor.
Table 3: Example headline output.
F&L The former paramedic training officer stood at the next
general election.
ILP The majority are now believing that war in Iraq was
wrong.
BBC L/Cpl Thomas Keys was shot 18 times, his inquest heard.
Source The majority of people in this country are now believing
that the war in Iraq was wrong, and I do believe we will
get support.
F&L The state government of Victoria take as those tests for
cannabis.
ILP Police in Victoria have begun randomly testing drivers for
the drug ecstasy.
BBC Police say drugs like Ecstasy can be as dangerous as al-
cohol for drivers.
Source Police in the Australian state of Victoria have begun ran-
domly testing drivers for the drug ecstasy.
F&L The US Government Professor Holdren called for more
than a year.
ILP ?We are experiencing dangerous human disruption of
global climate,? Professor Holdren said.
BBC Sea levels could rise by 4m over the coming century, he
warns.
Source ?We are experiencing dangerous human disruption of the
global climate and we?re going to experience more,? Pro-
fessor Holdren said.
Table 4: Example caption output.
520
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
Lead-1 Topiary ILP
Sc
or
e
Rouge-1
Rouge-L
Rouge-2
Figure 3: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the DUC-04 headlines for our ILP model,
the lead sentence baseline and Topiary.
 0
 0.05
 0.1
 0.15
 0.2
SVM F&L ILP
Sc
or
e
Rouge-1
Rouge-L
Rouge-2
Figure 4: ROUGE-1, ROUGE-2 and ROUGE-L re-
sults on the BBC captions for our ILP model, the
sentence baseline chosen by the SVM, and Feng and
Lapata?s (2010) model.
significant. We also investigated using an ILP model
with just the QG rules or just dependency label in-
formation (see constraint (1e) in Section 3.3). Both
settings gave less compressed output, and the result-
ing ROUGE scores were lower on all measures. The
ROUGE results for the caption generation task fol-
low a similar pattern (see Figure 4). Our model is
slightly better than the best sentence baseline but
performs worse than Feng and Lapata (2010a). Ta-
bles 3 and 4 show example output for the ILP model
and the baselines on the headline and caption tasks
respectively. In the tables, Source refers to the sen-
tence chosen by the ILP, but before any paraphrasing
is applied. We can see that deletion rules dominate,
and a more compressive style of paraphrasing has
been learned for the headline task.
The results of our human evaluation study for
the DUC-04 headlines are summarized in Table 5.
Means differences were compared using a Post-hoc
Model Grammaticality Importance
Lead-1 4.95 3.30
Topiary 3.03 3.43
ILP 5.36 4.94
Reference 5.12 5.17
Table 5: Average human ratings of DUC-04 head-
lines, for our ILP model, the lead sentence baseline,
the output of Topiary and the human-written refer-
ence.
Model Grammaticality Importance
SVM 5.24 5.01
F&L 4.42 4.74
ILP 5.49 5.25
Reference 5.61 5.18
Table 6: Average human ratings of captions, for
our ILP model, the sentence baseline chosen by the
SVM, Feng and Lapata?s (2010) model and the ref-
erence BBC caption.
Tukey test. The headlines created by our model
were considered significantly more important and
more grammatical than those of the Topiary sys-
tem (? < 0.01), despite the better overlap of Topi-
ary with the reference headlines as indicated in the
Rouge results above. Compared to the lead sentence
of the article (the DUC-04 baseline), our model was
also rated significantly higher in terms of importance
(? < 0.01) but not grammaticality.
Table 6 summarizes the results of our second
judgment elicitation study. The captions generated
by our model are significantly more grammatical
than those of Feng and Lapata (2010a) (? < 0.01).
The SVM, ILP model and reference captions do not
differ significantly in terms of grammaticality. In
terms of importance, the ILP model is significantly
better than the SVM (? < 0.01) and Feng and Lap-
ata (? < 0.01) and comparable to the reference.
The human ratings are more favorable to our
model than ROUGE for both tasks. There are two
reasons for this. Firstly, the model is not bi-
ased towards selecting the lead sentence as a head-
line/caption and is disadvantaged in ROUGE evalua-
tions as professional abstractors often reuse the lead
or parts of it to create a title. Secondly, the model
often generates an appropriate title that is lexically
521
distinct from the reference even though it expresses
similar meaning.
6 Conclusions
In this paper we proposed a joint content se-
lection and surface realization model for single-
document summarization. The model operates over
a syntax-rich representation of the source docu-
ment and learns which phrases should be in the
summary. Content selection preferences are cou-
pled with a quasi-synchronous grammar whose rules
encode surface realization preferences (e.g., para-
phrases and compressions). Both types of prefer-
ences are optimized simultaneously in an integer lin-
ear program subject to grammaticality, length and
coverage constraints. Importantly, the QG allows
the model to adapt to the writing and stylistic con-
ventions of different tasks. The results of our hu-
man studies show that our system creates grammati-
cal and informative summaries whilst outperforming
several competitive baselines.
The model itself is relatively simple and achieves
good performance without any task-specific modifi-
cation. One potential stumbling block may be the
availability of parallel data for acquiring the QG.
The Internet provides a large repository of news
documents with headlines, images and captions. In
some cases news articles are even accompanied with
?story highlights? which could be used as training
data for longer summaries.4 For other domains ob-
taining such data may be more difficult. However,
our experiments have shown that relatively small
parallel corpora (in the range of 200?500 pairs) suf-
fice to learn many of the writing conventions for a
given task.
In the future, we plan to explore how to inte-
grate more sophisticated QG rules in the generation
process. Currently we consider deletions, reorder-
ings and insertions. Ideally, we would also like to
model arbitrary substitutions between words but also
larger constituents (e.g., subclauses, sentence aggre-
gation). Beyond summarization, we would also like
to apply our model to other generation tasks, such as
paraphrasing and text simplification.
4On-line CNN news articles are prefaced by story
highlights?three or four short sentences that are written by hu-
mans and give a brief overview of the article.
Acknowledgments We are grateful to David Chi-
ang and Noah Smith for their input on earlier ver-
sions of this work. We would also like to thank
Andreas Grothey and members of ICCS at the
School of Informatics for valuable discussions and
comments. We acknowledge the support of EP-
SRC through project grants EP/F055765/1 and
GR/T04540/01.
References
Achterberg, Tobias. 2007. Constraint Integer Program-
ming. Ph.D. thesis, Technische Universita?t Berlin.
Banko, Michele, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In Proceedings of the 38th ACL. Hong
Kong, pages 318?325.
Clarke, James and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research 31:399?429.
Cohn, Trevor and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd COLING. Manchester, UK, pages 137?144.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the ACL-IJCNLP.
Suntec, Singapore, pages 468?476.
Daume? III, Hal. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th ACL. Philadelphia, PA, pages 449?456.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In Proceedings of the HLT-
NAACL 2003 Text Summarization Workshop and Doc-
ument Understanding Conference. Edmondon, Al-
berta, pages 1?8.
Dras, Mark. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text.. Ph.D. thesis, Macquarie
University.
Feng, Yansong and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, Uppsala, Sweden, pages 1239?1249.
Feng, Yansong and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Pro-
522
ceedings of the NAACL HLT . Association for Com-
putational Linguistics, Los Angeles, California, pages
831?839.
Jing, Hongyan. 2000. Sentence reduction for automatic
text summarization. In Proceedings of the 6th ANLP.
Seattle, WA, pages 310?315.
Jing, Hongyan. 2002. Using hidden Markov modeling to
decompose human-written summaries. Computational
Linguistics 28(4):527?544.
Jing, Hongyan and Kathleen McKeown. 2000. Cut
and paste summarization. In Proceedings of the 1st
NAACL. Seattle, WA, pages 178?185.
Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of web experi-
ments: A case study using the WebExp software pack-
age. Behavior Research Methods 41(1):1?12.
Klein, Dan and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st ACL.
Sapporo, Japan, pages 423?430.
Koch, Thorsten. 2004. Rapid Mathematical Prototyping.
Ph.D. thesis, Technische Universita?t Berlin.
Kupiec, Julian, Jan O. Pedersen, and Francine Chen.
1995. A trainable document summarizer. In Proceed-
ings of SIGIR-95. Seattle, WA, pages 68?73.
Lin, Chin-Yew. 2003. Improving summarization perfor-
mance by sentence compression ? a pilot study. In
Proceedings of the 6th International Workshop on In-
formation Retrieval with Asian Languages. Sapporo,
Japan, pages 1?8.
Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT NAACL. Edmonton,
Canada, pages 71?78.
Martins, Andre? and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing. Boulder, Colorado, pages 1?9.
Smith, David and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings on the Workshop on Sta-
tistical Machine Translation. Association for Compu-
tational Linguistics, New York City, pages 23?30.
Smith, David A. and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the EMNLP. Suntec, Sin-
gapore, pages 822?831.
Soricut, R. and D. Marcu. 2007. Abstractive head-
line generation using WIDL-expressions. Information
Processing and Management 43(6):1536?1548. Text
Summarization.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings of the
EMNLP-CoNLL. Prague, Czech Republic, pages 22?
32.
Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector ma-
chine training. Computational Optimization and Ap-
plications Published online.
Woodsend, Kristian and Mirella Lapata. 2010. Automatic
generation of story highlights. In Sandra Carberry and
Stephen Clark, editors, Proceedings of the 48th ACL.
Uppsala, Sweden, pages 565?574.
Zajic, David, Bonnie Dorr, and Richard Schwartz. 2004.
BBN/UMD at DUC-2004: Topiary. In Proceedings
of the NAACL Workshop on Document Understanding.
Boston, MA, pages 112?119.
Zajic, David, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing Management Special Is-
sue on Summarization 43(6):1549?1570.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP. Suntec, Singapore, pages 834?842.
523
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 409?420,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning to Simplify Sentences with Quasi-Synchronous Grammar and
Integer Programming
Kristian Woodsend and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
k.woodsend@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Text simplification aims to rewrite text into
simpler versions, and thus make information
accessible to a broader audience. Most pre-
vious work simplifies sentences using hand-
crafted rules aimed at splitting long sentences,
or substitutes difficult words using a prede-
fined dictionary. This paper presents a data-
driven model based on quasi-synchronous
grammar, a formalism that can naturally
capture structural mismatches and complex
rewrite operations. We describe how such a
grammar can be induced from Wikipedia and
propose an integer linear programming model
for selecting the most appropriate simplifica-
tion from the space of possible rewrites gen-
erated by the grammar. We show experimen-
tally that our method creates simplifications
that significantly reduce the reading difficulty
of the input, while maintaining grammaticality
and preserving its meaning.
1 Introduction
Sentence simplification is perhaps one of the oldest
text rewriting problems. Given a source sentence,
the goal is to create a grammatical target that is
easier to read with simpler vocabulary and syntac-
tic structure. An example is shown in Table 1 in-
volving a broad spectrum of rewrite operations such
as deletion, substitution, insertion, and reordering.
The popularity of the simplification task stems from
its potential relevance to various applications. Ex-
amples include the development of reading aids for
people with aphasia (Carroll et al, 1999), non-native
Also contributing to the firmness in copper, the an-
alyst noted, was a report by Chicago purchasing
agents, which precedes the full purchasing agents re-
port that is due out today and gives an indication of
what the full report might hold.
Also contributing to the firmness in copper, the an-
alyst noted, was a report by Chicago purchasing
agents. The Chicago report precedes the full purchas-
ing agents report. The Chicago report gives an indica-
tion of what the full report might hold. The full report
is due out today.
Table 1: Example of a source sentence (top) and its sim-
plification (bottom).
speakers (Siddharthan, 2003) and more generally in-
dividuals with low literacy (Watanabe et al, 2009).
A simplification component could be also used as
a preprocessing step to improve the performance
of parsers (Chandrasekar et al, 1996), summarizers
(Beigman Klebanov et al, 2004) and semantic role
labelers (Vickrey and Koller, 2008).
Simplification is related to, but different from
paraphrase extraction (Barzilay, 2003). We must not
only have access to paraphrases (i.e., rewrite rules),
but also be able to combine them to generate new
text, in a simpler language. The task is also dis-
tinct from sentence compression as it aims to ren-
der a sentence more accessible while preserving its
meaning. On the contrary, compression unavoidably
leads to some information loss as it creates shorter
sentences without necessarily reducing complexity.
In fact, one of the commonest simplification oper-
ations is sentence splitting which usually produces
longer rather than shorter output! Moreover, mod-
409
els developed for sentence compression have been
mostly designed with one rewrite operation in mind,
namely word deletion, and are thus unable to model
consistent syntactic effects such as reordering, sen-
tence splitting, changes in non-terminal categories,
and lexical substitution (but see Cohn and Lapata
2008 and Zhao et al 2009 for notable exceptions).
In this paper we propose a sentence simplification
model that is able to handle structural mismatches
and complex rewriting operations. Our approach is
based on quasi-synchronous grammar (QG, Smith
and Eisner 2006), a formalism that is well suited for
text rewriting. Rather than postulating a strictly syn-
chronous structure over the source and target sen-
tences, QG identifies a ?sloppy? alignment of parse
trees assuming that the target tree is in some way
?inspired by? the source tree. Specifically, our model
is formulated as an integer linear program and uses
QG to capture the space of all possible rewrites.
Given a source tree, it finds the best target tree li-
censed by the grammar subject to constraints such
as sentence length and reading ease. Our model is
conceptually simple and computationally efficient.
Furthermore, it finds globally optimal simplifica-
tions without resorting to heuristics or approxima-
tions during the decoding process.
Contrary to most previous approaches (see the
discussion in Section 2) which rely heavily on
hand-crafted rules, our model learns simplifi-
cation rewrites automatically from examples of
source-target sentences. Our work joins others in us-
ing Wikipedia to extract data appropriate for model
training (Yamangil and Nelken, 2008; Yatskar et al,
2010; Zhu et al, 2010). Advantageously, the Sim-
ple English Wikipedia (henceforth SimpleEW) pro-
vides a large repository of simplified language; it
uses fewer words and simpler grammar than the or-
dinary English Wikipedia (henceforth MainEW) and
is aimed at non-native English speakers, children,
translators, people with learning disabilities or low
reading proficiency. We exploit Wikipedia and cre-
ate a (parallel) simplification corpus in two ways:
by aligning MainEW sentences to their SimpleEW
counterparts, and by extracting training instances
from SimpleEW revision histories, thus leveraging
Wikipedia?s collaborative editing process.
Our experimental results demonstrate that a sim-
plification model can be learned from Wikipedia
data alone without any manual effort. Perhaps un-
surprisingly, the quality of the QG grammar rules
greatly improves when these are learned from re-
vision histories which are less noisy than sentence
alignments. When compared against current state-
of-the-art methods (Zhu et al, 2010) our model
yields significantly simpler output that is both gram-
matical and meaning preserving.
2 Related Work
Sentence simplification has attracted a great deal
of attention due to its potential impact on society.
The literature is rife with attempts to simplify text
using mostly hand-crafted syntactic rules aimed at
splitting long and complicated sentences into sev-
eral simpler ones (Carroll et al, 1999; Chandrasekar
et al, 1996; Siddharthan, 2004; Vickrey and Koller,
2008). Other work focuses on lexical simplifications
and substitutes difficult words by more common
WordNet synonyms or paraphrases found in a pre-
defined dictionary (Devlin, 1999; Inui et al, 2003;
Kaji et al, 2002).
More recently, Yatskar et al (2010) explore
data-driven methods to learn lexical simplifications
from Wikipedia revision histories. A key idea in
their work is to utilize SimpleEW edits, while rec-
ognizing that these may serve other functions, such
as vandalism removal or introduction of new con-
tent. Zhu et al (2010) also use Wikipedia to learn
a sentence simplification model which is able to
perform four rewrite operations, namely substitu-
tion, reordering, splitting, and deletion. Inspired
by syntax-based SMT (Yamada and Knight, 2001),
their model consists of three components: a lan-
guage model P(s) whose role is to guarantee that the
simplification output is grammatical, a direct trans-
lation model P(s|c) capturing the probability that the
target sentence s is a simpler version of the source c,
and a decoder which searches for the simplifica-
tion s which maximizes P(s)P(s|c). The translation
model is the product of the aforementioned four
rewrite operations whose probabilities are estimated
from a parallel corpus of MainEW and SimpleEW
sentences using an expectation maximization algo-
rithm. Their decoder translates sentences into sim-
pler alternatives by greedily selecting the branch in
the source tree with the highest probability.
410
Our own work formulates sentence simplification
in the framework of Quasi-synchronous grammar
(QG, Smith and Eisner 2006). QG allows to describe
non-isomorphic tree pairs (the grammar rules can
comprise trees of arbitrary depth, and fragments can
be mapped) and is thus suited to text-rewriting tasks
which typically involve a number of local modifi-
cations to the input text. We use quasi-synchronous
grammar to learn a wide range of rewrite opera-
tions capturing both lexical and structural simplifi-
cations naturally without any additional rule engi-
neering. In contrast to Yatskar et al (2010) and Zhu
et al (2010), simplification operations (e.g., substi-
tution or splitting) are not modeled explicitly; in-
stead, we leave it up to our grammar extraction algo-
rithm to learn appropriate rules that reflect the train-
ing data. Compared to Zhu et al, our model is con-
ceptually simpler and more general. The proposed
ILP formulation not only allows to efficiently search
through the space of many QG rules but also to in-
corporate constraints relating to grammaticality and
the task at hand without the added computational
cost of integrating a language model. Furthermore,
our learning framework is not limited to simplifi-
cation and could be easily adapted to other rewrit-
ing tasks. Indeed, the QG formalism has been pre-
viously applied to parser adaptation and projection
(Smith and Eisner, 2009), paraphrase identification
(Das and Smith, 2009), question answering (Wang
et al, 2007), and title generation (Woodsend et al,
2010).
Finally, our work relates to a large body of recent
literature on Wikipedia and its potential for a wide
range of NLP tasks. Beyond text rewriting, examples
include semantic relatedness (Ponzetto and Strube,
2007), information extraction (Wu and Weld, 2010),
ontology induction (Nastase and Strube, 2008), and
the automatic creation of overview articles (Sauper
and Barzilay, 2009).
3 Sentence Simplification Model
Our model takes a single sentence as input and cre-
ates a version that is simpler to read. This may
involve rendering syntactically complex structures
simpler (e.g., through sentence splitting), or sub-
stituting rare words with more common words or
phrases (e.g., such that a second language learner
may be familiar with), or deleting elements of the
original text in order to produce a relatively sim-
pler and shallower syntactic structure. In addition,
the output must be grammatical and coherent. These
constraints are global in their scope, and cannot be
adequately satisfied by optimizing each one of them
individually. Our approach therefore uses an ILP
formulation which will provide a globally optimal
solution. Given an input sentence, our model decon-
structs it into component phrases and clauses, each
of which is simplified (lexically and structurally)
through QG rewrite rules. We generate all possible
simplifications for a given input and use the ILP to
find the best target subject to grammaticality con-
straints. In what follows we first detail how we ex-
tract QG rewrite rules as these form the backbone of
our model and then formulate the ILP proper.
3.1 Quasi-synchronous Grammar
Phrase alignment Our model operates on indi-
vidual sentences annotated with syntactic informa-
tion i.e., phrase structure trees. In our experiments,
we obtain this information from the Stanford parser
(Klein and Manning, 2003) but any other broadly
similar parser could be used instead. Given an input
sentence S1 or its parse tree T1, the QG constructs
a monolingual grammar for parsing, or generating,
possible translation trees T2. A grammar node in the
target tree T2 is modeled on a subset of nodes in the
source tree, with a rather loose alignment between
the trees.
We take aligned sentence pairs represented as
phrase structure trees and build up a list of leaf node
alignments based on lexical identity. We align direct
parent nodes where more than one child node aligns.
QG rules are created from aligned nodes above the
leaf node level if the all the nodes in the target tree
can be explained using nodes from the source. This
helps to improve the quality in what is inherently a
noisy process, and it is largely responsible for a rel-
atively small resulting grammar (see Table 2). Ex-
amples of phrase alignments (indicated with dotted
lines) are shown in Figure 1.
Syntactic simplification rules Each QG rule de-
scribes the transformations required from source to
target phrase sub-trees. It allows child (and possi-
bly grand-child) constituents to be deleted or re-
411
ST
.
.
VP
VP 2
NP 4
NNP
Mary
VBD 3
met
ADVP
RB
afterwards
CC
and
VP 1
NP
NN
dog
PRP$
his
VBD
walked
NP
NNP
Smith
NNP
John
ST (main)
.
.
VP 1
NP
NN
dog
PRP$
his
VBD
walked
NP
NNP
Smith
NNP
John
ST (aux)
.
.
VP 2
ADVP
RB
later
NP 4
NNP
Mary
VBD 3
met
NP
PRP
He
Rule involving lexical substitution:
?VP, VP? ? ?[ADVP [RB afterwards] VBD 3 NP 4 ], [VBD 3 NP 4 ADVP [RB later]]?
Rule for splitting into main constituent and auxiliary sentence:
?VP, VP, ST? ? ?[VP 1 and VP 2 ], [VP 1 ], [NP [PRP He] VP 2 .]?
Figure 1: A source sentence (upper tree) is split into two sentences. Dotted lines show word alignments, while boxed
subscripts show aligned nodes used to form QG rules. Below, two QG rules learned from this data.
ordered, and for nodes to be flattened. In addition,
we allow insertion of punctuation and some func-
tion words, identified by a small set of POS tags. To
distinguish sentences proper (which have final punc-
tuation) from clauses, we modify the output of the
parser, changing the root sentence parse tag from S
to ST (a ?top-level sentence?); this allows clauses to
be extracted and rewritten as stand-alone sentences.
Lexical simplification rules Lexical substitutions
are an important part of simplification. We learn
them from aligned sub-trees, in the same way as
described above for syntax rules, by allowing a
small number of lexical substitutions to be present
in the rules, and provided they do not include proper
nouns. The resulting QG rules could be applied
by matching the syntax of the whole sub-tree sur-
rounding the substitution, but this approach is overly
restrictive and suffers from data sparsity. Indeed,
Yatskar et al (2010) learn lexical simplifications
without taking syntactic context into account. We
therefore add a post-processing stage to the learning
process. For rules where the syntactic structures of
the source and target sub-trees match, and the only
difference is a lexical substitution, we construct a
more general rule by extracting the words and cor-
responding POS tags involved in the substitution.
Then at the generation stage, identifying suitable
rules depends only on the substitution words, rather
than the surrounding syntactic context. An example
of a lexical substitution rule is shown in Figure 1.
Sentence splitting rules Another important sim-
plification technique is to split syntactically compli-
cated sentences into several shorter ones. To learn
QG rules for this operation, the source sentence is
aligned with two consecutive target sentences.
Rather than expecting to discover a split point in
the source sentence, we attempt to identify a node
in the source parse tree that contributes to both of
the two target sentences. Our intuition is that one
of the target sentences will follow the general syn-
tactic structure of the source sentence. We designate
this as the main sentence. A node in the source sen-
tence parse tree will be aligned with a (similar but
simpler) node in the main target sentence, but at the
same time it will fully explain the other target sen-
tence, which we term the auxiliary sentence. It is
412
possible for the auxiliary sentence to come before or
after the main sentence. In the learning procedure,
we try both possible orderings, and record the order
in any QG rules successfully produced.
The resulting QG rule is a tuple of three phrase
structure elements: the source node, the node in the
target main sentence (the top level of this node is
typically the same as that of the source node), and
the phrase structure of the entire auxiliary sentence.1
In addition, there is a flag to indicate if the auxiliary
sentence comes before or after the main sentence.
This formalism is able to capture the operations re-
quired to split sentences containing coordinate or
subordinate clauses, parenthetical content, relative
clauses and apposition. An example of a sentence
splitting rule is illustrated in Figure 1.
3.2 ILP-based Generation
We cast the problem of finding a suitable target sim-
plification given a source sentence as an integer lin-
ear program (ILP). Specifically, simplified text is
created from source sentence parse trees by identi-
fying and applying QG grammar rules. These will
have matching structure and may also require lexical
matching (shown using italics in the example rules
in Figure 1). The generation process starts at the root
node of the parse tree, applying QG rules to sub-
trees until leaf nodes are reached. We do not use the
Bayesian probability model proposed by Smith and
Eisner (2006) to identify the best sequence of sim-
plification rules. Instead, where there is more than
one matching rule, and so more than one simplifi-
cation is possible, the alternatives are all generated
and incorporated into the target phrase structure tree.
The ILP model operates over this phrase structure
tree and selects the phrase nodes from which to form
the target output.
Applying the QG rules on the source sentence
generates a number of auxiliary sentences. Let S be
this set of sentences. Let P be the set of nodes in the
phrase structure trees of the auxiliary sentences, and
Ps ? P be the set of nodes in each sentence s ? S .
Let the sets Di ? P , ?i ? P capture the phrase de-
pendency information for each node i, where each
set Di contains the nodes that depend on the pres-
1Note that the target component comprises the second and
third elements as a pair, and variables from the source compo-
nent are split between them.
ence of i. In a similar fashion, the sets Ai? S , ?i?P
capture the indices of any auxiliary sentences that
depend on the presence of node i. C ? P is the set
of nodes involving a choice of alternative simplifi-
cations (nodes in the tree where more than one QG
rewrite rule can be applied, as mentioned above);
Ci ? P , i ? C are the sets of nodes that are direct
children of each such node, in other words they are
the individual simplifications. Let l(w)i be the length
of each node i in words, and l(sy)i its length in syl-
lables. As we shall see below counts of words and
syllables are important cues in assessing readability.
The model is cast as an binary integer linear
program. A vector of binary decision variables
x ? {0,1}|P | indicates if each node is to be part of
the output. A vector of auxiliary binary variables
y ? {0,1}|S | indicates which (auxiliary) sentences
have been chosen.
maxx ?i?P gixi +hw +hsy (1a)
s.t. x j? xi ?i ? P , j ?Di (1b)
xi? ys ?i ? P ,s ? Ai (1c)
xi? ys ?s ? S , i ? Ps (1d)
?
j?Ci
x j = xi ?i ? C , j ? Ci (1e)
?
s?S
yi ? 1 (1f)
xi ? {0,1} ?i ? P (1g)
ys ? {0,1} ?s ? S . (1h)
Our objective function, given in Equation (1a),
is the summation of local and global compo-
nents. Each phrase is locally given a rewrite
penalty gi, where common lexical substitutions,
rewrites and simplifications are penalized less (as
we trust them more), compared to rarer QG rules.
The penalty is a simple log-probability measure,
gi = log
(
nrNr
)
, where nr is the number of times the
QG rule r was seen in the training data, and Nr
the number of times all suitable rules for this
phrase node were seen. If no suitable rules exist, we
set gi = 0.
The other two components of the objective,
hw and hsy, are global in nature, and guide the ILP
413
towards simpler language. They draw inspiration
from existing measures of readability (the ease with
which a document can be read and understood).
The primary aim of readability formulas is to assess
whether texts or books are suitable for students at
particular grade levels or ages (see Mitchell 1985 for
an overview). Intuitively, texts ought to be simpler if
they correspond to low reading levels. A commonly
used reading level measure is the Flesch-Kincaid
Grade Level (FKGL) index which estimates read-
ability as a combination of the average number of
syllables per word and the average number of words
per sentence. Unfortunately, this measure is non-
linear2 and cannot be incorporated directly into the
objective of the ILP. Instead, we propose a linear ap-
proximation. We provide the ILP with targets for the
average number of words per sentence (wps), and
syllables per word (spw). hw(x,y) then measures the
number of words below this target level that the ILP
has achieved:
hw(x,y) = wps??
i?S
yi??
i?P
l(w)i xi.
When positive, this indicates that sentences are
shorter than target, and contributes positively to the
readability objective whilst encouraging the appli-
cation of sentence splitting and deletion-based QG
rules. Similarly, hsy(x,y) measures the number of
syllables below that expected, from the target aver-
age and the number of words the ILP has chosen:
hsy(x) = spw??
i?P
l(w)i xi??
i?P
l(sy)i xi.
This component of the objective encourages the
deletion or lexical substitution of complex words.
We can use the two target parameters (wps and spw)
to control how much simplification the ILP should
apply.
Constraint (1b) enforces grammatical correctness
by ensuring that the phrase dependencies are re-
spected and the resulting structure is a tree. Phrases
that depend on phrase i are contained in the set Di.
Variable xi is true, and therefore phrase i will be
included in the target output, if any of its depen-
dents x j ?Di are true.3 Constraint (1c) links main
2FKGL = 0.39
( total wordstotal sentences
)
+1.8
( total syllables
total words
)
?15.59
3Constraints (1b), (1c) and (1d) are shown as dependencies
for clarity, but they were implemented as inequalities in the ILP.
phrases to auxiliary sentences, so that the latter can
only be included in the output if the main phrase
has also been chosen. This helps to control coher-
ence within the output text. Despite seeming similar
to (1c), the role of constraint (1d) is quite different.
It links phrase variables x to sentence variables y, to
ensure the logical integrity of the model is correct.
Where the QG provides alternative simplifications,
it makes sense of course to select only one. This is
controlled by constraint (1e), and by placing all al-
ternatives in the set Di for the node i.
With these constraints alone, and faced with a
source sentence that is particularly difficult to sim-
plify, it is possible for the ILP solver to return a ?triv-
ial? solution of no output at all, as all other avail-
able solutions result in a negative objective value.
It is therefore necessary to impose a global mini-
mum output constraint (1f). In combination with the
dependency relations in (1c), this constraint ensures
that at least an element of the root sentence is present
in the output. Global maximum length constraints
are a frequently occurring aspect of ILP models used
in NLP applications. We decided not to incorporate
any such constraints into our model, as we did not
want to place limitations on the simplification of
original content.
4 Experimental Setup
In this section we present our experimental setup
for assessing the performance of the simplification
model described above. We give details on the cor-
pora and grammars we used, model parameters, the
systems used for comparison with our approach, and
explain how the output was evaluated.
Grammar Extraction QG rules were learned
from revision histories and an aligned simplifica-
tion corpus, which we obtained from snapshots4 of
MainEW and SimpleEW. Wiki-related mark-up and
meta-information was removed to extract the plain
text from the articles.
SimpleEW revisions not only simplify the text of
existing articles, they may also introduce new con-
tent, vandalize or remove vandalism, or perform nu-
merous automatic ?house-keeping? modifications.
4The snapshots for MainEW (enwiki) and SimpleEW (sim-
plewiki dated 2010-09-16 and 2010-09-13, respectively (both
available from http://download.wikimedia.org/).
414
Corpora Syntactic Lexical Splitting
Revision 316 269 184
Aligned 312 96 254
Table 2: Number of QG rules extracted (after removing
singletons) from revision-based and aligned corpora.
We identified suitable revisions for simplification by
selecting those where the author had mentioned a
keyword (such as simple, clarification or grammar)
in the revision comments. Each selected revision
was compared to the previous version. Because the
entire article is stored at each revision, we needed to
identify and align modified sentences. We first iden-
tified modified sections using the Unix diff pro-
gram, and then individual sentences within the sec-
tions were aligned using the program dwdiff5. This
resulted in 14,831 paired sentences. With regard to
the aligned simplification corpus, we paired 15,000
articles from SimpleEW and MainEW following the
language link within the snapshot files. Within the
paired articles, we identified aligned sentences us-
ing macro alignment (at paragraph level) then mi-
cro alignment (at sentence level), using tf.idf scores
to measure similarity (Barzilay and Elhadad, 2003;
Nelken and Schieber, 2006).
All source-target sentences (resulting from revi-
sions or alignments) were parsed with the Stanford
parser (Klein and Manning, 2003) in order to la-
bel the text with syntactic information. QG rules
were created by aligning nodes in these sentences
as described earlier. A breakdown of the number
and type of rules we obtained from the revision
and aligned corpora (after removing rules appear-
ing only once) is given in Table 2. Examples of the
most frequently learned QG rules are shown in Ta-
ble 3. Rules (1)?(3) involve syntactic simplification
and rules (4)?(6) involve sentence splitting. Exam-
ples of common lexical simplifications found by our
grammar are: ?discovered? ? ?found?, ?defeated?
? ?won against?, ?may refer to?? ?could mean?,
?original?? ?first?, ?requires?? ?needs?.
Sentence generation We generated simplified
versions of MainEW sentences. For each (parsed)
source sentence, we created and solved an ILP (see
Equation (1)) parametrized as follows: the number
5http://os.ghalkes.nl/dwdiff.html
1. ?S, ST? ? ?[NP 1 VP 2 ], [NP 1 VP 2 .]?
2. ?S, ST? ? ?[VP 1 ], [This VP 1 .]?
3. ?NP, ST? ? ?[NP 1 , NP 2 ], [NP 1 was VP 2 .]?
4. ?ST, ST, ST? ? ?[S 1 , and S 2 ], [ST 1 ], [ST 2 ]?
5. ?ST, ST, ST? ? ?[S 1 : S 2 ], [ST 1 ], [ST 2 ]?
6. ?ST, ST, ST? ? ?[S 1 , but S 2 ], [ST 1 ], [ST 2 ]?
Table 3: Examples of QG rules involving syntactic sim-
plification (1)?(3) and sentence division (4)?(6). The lat-
ter are shown as the tuple ?source, target, aux?. The trans-
form of nodes from S to ST (for example) rely on the
application of syntactic simplification rules rules. Boxed
subscripts show aligned nodes.
of target words per sentence (wps) was set to 8, and
syllables per word (spw) to 1.5. These two param-
eters were empirically tuned on the training set. To
solve the ILP model we used the ZIB Optimization
Suite software (Achterberg, 2007; Koch, 2004). The
solution was converted into a sentence by removing
nodes not chosen from the tree representation, then
concatenating the remaining leaf nodes in order.
Evaluation We evaluated our model on the same
dataset used in Zhu et al (2010), an aligned cor-
pus of MainEW and SimpleEW sentences. The cor-
pus contains 100/131 source/target sentences and
was created automatically. Sentences from this cor-
pus (and their revisions) were excluded from train-
ing. We evaluated two versions of our model, one
with rewrite rules acquired from revision histories
of simplified documents and another one with rules
extracted from MainEW-SimpleEW aligned sen-
tences. These models were compared against Zhu
et al (2010)6 who also learn simplification rules
from Wikipedia, and a simple baseline that uses
solely lexical simplifications7 provided by the Sim-
pleEW editor ?SpencerK? (Spencer Kelly). An obvi-
ous idea would be to treat sentence simplification as
an English-to-English translation problem and use
an off-the-shelf system like Moses8 for the task.
However, we refrained from doing so as Zhu et al
(2010) show that Moses performs poorly, it cannot
model rewrite operations that split sentences or drop
words and in most cases generates output identical
6We are grateful to Zhemin Zhu for providing us with his
test set and the output of his system.
7http://www.spencerwaterbed.com/soft/simple/
8http://www.statmt.org/moses/
415
MainEW Wonder has recorded several critically acclaimed albums and hit singles, and writes and produces songs
for many of his label mates and outside artists as well.
Zhu et alWonder has recorded several praised albums and writes and produces songs. Many of his label mates
and outside artists as well.
AlignILP Wonder has recorded several critically acclaimed albums and hit singles. He produces songs for many
of his label mates and outside artists as well. He writes.
RevILP Wonder has recorded many critically acclaimed albums and hit singles. He writes. He makes songs for
many of his label mates and outside artists as well.
SimpleEW He has recorded 23 albums and many hit singles, and written and produced songs for many of his label
mates and other artists as well.
MainEW The London journeys In 1790, Prince Nikolaus died and was succeeded by a thoroughly unmusical
prince who dismissed the entire musical establishment and put Haydn on a pension.
Zhu et alThe London journeys in 1790, prince Nikolaus died and was succeeds by a son became prince. A son
became prince told the entire musical start and put he on a pension.
AlignILP The London journeys In 1790, Prince Nikolaus died. He was succeeded by a thoroughly unmusical
prince. He dismissed the entire musical establishment. He put Haydn on a pension.
RevILP The London journeys In 1790, Prince Nikolaus died. He was succeeded by a thoroughly unmusical
prince. He dismissed the whole musical establishment. He put Haydn on a pension.
SimpleEW The London journeys In 1790, Prince Nikolaus died and his son became prince. Haydn was put on a
pension.
Table 4: Example simplifications produced by the systems in this paper (RevILP, AlignILP) and Zhu et al?s (2010)
model, compared to real Wikipedia text (MainEW: input source, SimpleEW: simplified target).
to the source.
We evaluated model output in two ways, using au-
tomatic evaluation measures and human judgments.
Intuitively, readability measures ought to be suit-
able for assessing the output of simplification sys-
tems. We report results with the well-known Flesch-
Kincaid Grade Level index (FKGL). Experiments
with other readability measures such as the Flesch
Reading Ease and the Coleman-Liau index obtained
similar results. In addition, we also assessed how the
system output differed from the human SimpleEW
gold standard by computing BLEU (Papineni et al,
2002) and TERp (Snover et al, 2009). Both mea-
sures are commonly used to automatically evaluate
the quality of machine translation output. BLEU9
scores the target output by counting n-gram matches
with the reference, whereas TERp is similar to word
error rate, the only difference being that it allows
shifts and thus can account for word order differ-
ences. TERp also allows for stem, synonym, and
paraphrase substitutions which are common rewrite
operations in simplification.
In line with previous work on text rewriting
(e.g., Knight and Marcu 2002) we also evaluated
9We calculated single-reference BLEU using the mteval-
v13a script (with the default settings).
system output by eliciting human judgments. We
conducted three experiments. In the first experi-
ment participants were presented with a source sen-
tence and its target simplification and asked to rate
whether the latter was easier to read compared to the
source. In the second experiment, they were asked
to rate the grammaticality of the simplified output.
In the third experiment, they judged how well the
simplification preserved the meaning of the source.
In all experiments participants used a five point rat-
ing scale where a high number indicates better per-
formance. We randomly selected and automatically
simplified 64 sentences from Zhu et al?s (2010) test
corpus using the four models described above. We
also included gold standard simplifications. Our ma-
terials thus consisted of 320 (64 ? 5) source-target
sentences.10 We collected ratings from 45 unpaid
volunteers, all self reported native English speakers.
The studies were conducted over the Internet using
a custom built web interface. Examples of our ex-
perimental items are given in Table 4 (we omit the
output of SpencerK as this is broadly similar to the
source sentence, modulo lexical substitutions).
10A Latin square design ensured that subjects did not see two
different simplifications of the same sentence.
416
Models FKGL BLEU TERP
MainEW 15.12 ? ?
SimpleEW 11.25 ? ?
SpencerK 14.67 0.47 0.51
Zhu et al9.41 0.38 0.59
RevILP 10.92 0.42 0.60
AlignILP 12.36 0.34 0.85
Table 5: Model performance using automatic evaluation
measures.
5 Results
The results of our automatic evaluation are summa-
rized in Table 5. The first column reports the FKGL
readability index of the source sentences (MainEW),
of their target simplifications (SimpleEW) and the
output of four models: a simple baseline that re-
lies on lexical substitution (SpencerK), Zhu et al?s
(2010) model, and two versions of our model, one
trained on revision histories (RevILP) and another
one trained on the MainEW-SimpleEW aligned cor-
pus (AlignILP). As can be seen, the source sentences
have the highest reading level. Zhu et al?s system
has the lowest reading level followed by our own
models and SpencerK. All models are significantly11
different in reading level from SimpleEW with the
exception of RevILP (using a one-way ANOVA with
post-hoc Tukey HSD tests). SpencerK is not signif-
icantly different in readability from MainEW; Re-
vILP is significantly different from Zhu et al and
AlignILP. In sum, these results indicate that RevILP
is the closest to SimpleEW and that the provenance
of the QG rules has an impact on the model?s perfor-
mance.
Table 5 also shows BLEU and TERp scores with
SimpleEW as the reference. These scores can be
used to examine how close to the gold standard our
models are. SpencerK has the highest BLEU and
lowest TERp scores.12 This is expected as this base-
line performs only a very limited type of rewriting,
namely lexical substitution. AlignILP is most differ-
ent from the reference, followed by Zhu et al (2010)
and RevILP. Taken together these results indicate
11All significance differences reported throughout this paper
are with a level less than 0.01.
12The perfect BLEU score is one and the perfect TERp score
is zero.
Models Simplicity Grammaticality Meaning
SimpleEW 3.74 4.89 4.41
SpencerK 1.41 4.87 4.84
Zhu et al2.92 3.43 3.44
RevILP 3.64 4.55 4.19
AlignILP 2.69 4.03 3.98
Table 6: Average human ratings for gold standard Sim-
pleEW sentences, a simple baseline (SpencerK) based on
lexical substitution, Zhu et al?s 2010 model, and two ver-
sions of our ILP model (RevILP and AlignILP).
Zhu et alAlignILP RevILP SimpleEW
SpencerK 2?4 2?4 24 24
Zhu et al?4 2?4 2?4
AlignILP 2?N 2?4
RevILP N
Table 7: 2/: is/not sig. diff. wrt simplicity; ?/: is/not
sig. diff. wrt grammaticality; 4/N: is/not sig. diff. wrt
meaning.
that the ILP models perform a fair amount of rewrit-
ing without simply rehashing the source sentence.
We now turn to the results of our judgment elic-
itation study. Table 6 reports the average ratings
for Simplicity (is the target sentence simpler than
the source?), Grammaticality (is the target sentence
grammatical?), and Meaning (does the target pre-
serve the meaning of the source?). With regard to
simplicity, our participants perceive the gold stan-
dard (SimpleEW) to be the simplest, followed by
RevILP, Zhu et al and AlignILP. SpencerK is the
least simple model and the most grammatical one
as lexical substitutions do not change the structure
of the sentence. Interestingly, RevILP and AlignILP
are also rated highly with regard to grammaticality.
Zhu et al (2010) is the least grammatical model.
Finally, RevILP preserves the meaning of the tar-
get as well as SimpleEW, whereas Zhu et al yields
the most distortions. Again SpencerK is rated highly
amongst the other models as it is does not substan-
tially simplify and thus change the meaning of the
source.
Table 7 reports on pairwise comparisons between
all models and their statistical significance (again us-
ing a one-way ANOVA with post-hoc Tukey HSD
tests). RevILP is not significantly different from
SimpleEW on any dimension (Simplicity, Grammat-
417
Original story: There was once a sweet little maid who lived with her father and mother in a pretty little
cottage at the edge of the village. At the further end of the wood was another pretty cottage and in it lived
her grandmother. Everybody loved this little girl, her grandmother perhaps loved her most of all and gave
her a great many pretty things. Once she gave her a red cloak with a hood which she always wore, so people
called her Little Red Riding Hood.
Generated simplification: There was once a sweet little maid. She lived with her father and mother in
a pretty little cottage at the edge of the village. At the further end of the wood it lived her grandmother.
Everybody loved this little girl. Her grandmother perhaps loved her most of all. She gave her a great many
pretty things. Once she gave her a red cloak with a hood, so persons called her Little Red Riding Hood.
Table 8: Excerpt of Little Red Riding Hood simplified by the RevILP model. Modifications to the original story are
highlighted in italics.
icality, Meaning), whereas Zhu et al differs signif-
icantly from RevILP and SimpleEW on all dimen-
sions. It is also significantly different from Alig-
nILP in terms of grammaticality and meaning but
not simplicity. RevILP is significantly more simple
and grammatical than AlignILP but performs com-
parably with respect to preserving the meaning of
the source.
In sum, our results show that RevILP is the best
performing model. It creates sentences that are sim-
ple, grammatical and adhere to the meaning of
the source. The QG rules obtained from the revi-
sion histories produce better output compared to the
aligned corpus. As revision histories are created by
Wikipedia contributors, they tend to be a more ac-
curate data source than aligned sentences which are
obtained via an automatic and unavoidably noisy
procedure. Our results also show that a more gen-
eral model not restricted to specific rewrite opera-
tions like Zhu et al (2010) obtains superior results
and has better coverage.
We also wanted to see whether a simplification
model trained on Wikipedia could be applied to an-
other domain. To this end, we used RevILP to sim-
plify five children stories from the Gutenburg13 col-
lection. The model simplified one sentence at a time
and was ran with the Wikipedia settings without any
modification. The mean FKGL on the simplified sto-
ries was 3.78. compared to 7.04 for the original ones.
An example of our system?s output on Little Red
Riding Hood is shown in Table 8.
Possible extensions and improvements to the cur-
rent model are many and varied. We have presented
an all-purpose simplification model without a target
13http://www.gutenberg.org
audience or application in mind. An interesting re-
search direction would be to simplify text accord-
ing to readability levels or text genres (e.g., news-
paper vs literary text). We could do this by incorpo-
rating readability-specific constraints to the ILP or
by changing the objective function (e.g., by favoring
more domain-specific rules). Finally, we would like
to extend the current model so as to simplify entire
documents both in terms of style and content.
Acknowledgments We are grateful to Lillian Lee
whose invited talk at CoNLL-2010 inspired this re-
search. We would also like to thank the members of
the Probabilistic Models of Language group at the
School of Informatics for valuable discussions and
comments. We acknowledge the support of EPSRC
through project grant EP/F055765/1.
References
Achterberg, Tobias. 2007. Constraint Integer Pro-
gramming. Ph.D. thesis, Technische Universita?t
Berlin.
Barzilay, Regina. 2003. Information Fusion for
Multi-Document Summarization: Paraphrasing
and Generation. Ph.D. thesis, Columbia Univer-
sity.
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Sapporo, Japan, pages 25?32.
Beigman Klebanov, Beata, Kevin Knight, and
Daniel Marcu. 2004. Text simplification for
information-seeking applications. In Proceed-
ings of Ontologies, Dabases, and Applications of
Semantics (ODBASE) International Conference.
418
Springer, Agia Napa, Cyprus, volume 3290 of
Lecture Notes in Computer Science, pages 735?
747.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and John Tait.
1999. Simplifying text for language-impaired
readers. In Proceedings of the 9th Conference of
the European Chapter of the ACL. Bergen, Nor-
way, pages 269?270.
Chandrasekar, Raman, Christine Doran, and Ban-
galore Srinivas. 1996. Motivations and meth-
ods for text simplification. In Proceedings of the
16th International Conference on Computational
Linguistics. Copenhagen, Denmark, pages 1041?
1044.
Cohn, Trevor and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Pro-
ceedings of the 22nd International Conference
on Computational Linguistics. Manchester, UK,
pages 137?144.
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of the ACL-IJCNLP.
Suntec, Singapore, pages 468?476.
Devlin, Siobhan. 1999. Simplifying Natural Lan-
guage for Aphasic Readers. Ph.D. thesis, Univer-
sity of Sunderland.
Inui, Kentaro, Atsushi Fujita, Tetsuro Takahashi,
Ryu Iida, and Tomoya Iwakura. 2003. Text sim-
plification for reading assistance: A project note.
In Proceedings of the Second International Work-
shop on Paraphrasing. Association for Computa-
tional Linguistics, Sapporo, Japan, pages 9?16.
Kaji, Nobuhiro, Daisuke Kawahara, Sadao Kuro-
hashi, and Satoshi Sato. 2002. Verb paraphrase
based on case frame alignment. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics. Association for Compu-
tational Linguistics, Philadelphia, Pennsylvania,
USA, pages 215?222.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Annual Meeting of the Association of
Computational Linguistics. Sapporo, Japan, pages
423?430.
Knight, Kevin and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artificial
Intelligence 139(1):91?107.
Koch, Thorsten. 2004. Rapid Mathematical Pro-
totyping. Ph.D. thesis, Technische Universita?t
Berlin.
Mitchell, James V. 1985. The Ninth Mental Mea-
surements Year-book. University of Nebraska
Press, Lincoln, Nebraska.
Nastase, Vivi and Michael Strube. 2008. Decoding
Wikipedia categories for knowledge acquisition.
In Proceedings of the 23rd Conference on Artifi-
cial Intelligence. pages 1219?1224.
Nelken, Rani and Stuart Schieber. 2006. Towards
robust context-sensitive sentence alignment for
monolingual corpora. In Proceedings of the 11th
Conference of the European Chapter of the As-
sociation for Computational Linguistics. Trento,
Italy, pages 161?168.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th ACL. Philadelphia, PA, pages
311?318.
Ponzetto, Simone Paolo and Michael Strube. 2007.
Knowledge derived from Wikipedia for comput-
ing semantic relatedness. Journal of Artificial In-
telligence Research 30:181?212.
Sauper, Christina and Regina Barzilay. 2009. Au-
tomatically generating Wikipedia articles: A
structure-aware approach. In Proceedings of the
Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Con-
ference on Natural Language Processing of the
AFNLP. Association for Computational Linguis-
tics, Suntec, Singapore, pages 208?216.
Siddharthan, Advaith. 2003. Syntactic Simplifica-
tion and Text Cohesion. Ph.D. thesis, University
of Cambridge, University of Cambridge.
Siddharthan, Advaith. 2004. Syntactic simplifica-
tion and text cohesion. in research on language
and computation. Research on Language and
Computation 4(1):77?109.
Smith, David and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft pro-
jection of syntactic dependencies. In Proceedings
on the Workshop on Statistical Machine Transla-
419
tion. Association for Computational Linguistics,
New York City, pages 23?30.
Smith, David A. and Jason Eisner. 2009. Parser
adaptation and projection with quasi-synchronous
grammar features. In Proceedings of the EMNLP.
Suntec, Singapore, pages 822?831.
Snover, Matthew, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or HTER? Exploring different human judgments
with a tunable MT metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion. Athens, Greece, pages 259?268.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT . Association for Com-
putational Linguistics, Columbus, Ohio, pages
344?352.
Wang, Mengqiu, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In Proceedings
of the EMNLP-CoNLL. Prague, Czech Republic,
pages 22?32.
Watanabe, Willian Massami, Arnaldo Candido Ju-
nior, Vin??cius Rodriguez de Uze?da, Renata Pon-
tin de Mattos Fortes, Thiago Alexandre Salgueiro
Pardo, and Sandra Maria Alu?sio. 2009. Facilita:
reading assistance for low-literacy readers. In
Proceedings of the 27th ACM International Con-
ference on Design of Communication. Blooming-
ton, IN.
Woodsend, Kristian, Yansong Feng, and Mirella
Lapata. 2010. Title generation with quasi-
synchronous grammar. In Proceedings of the
2010 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Com-
putational Linguistics, Cambridge, MA, pages
513?523.
Wu, Fei and Daniel S. Weld. 2010. Open infor-
mation extraction using Wikipedia. In Proceed-
ings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics. Association
for Computational Linguistics, Uppsala, Sweden,
pages 118?127.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceed-
ings of 39th Annual Meeting of the Association
for Computational Linguistics. Toulouse, France,
pages 523?530.
Yamangil, Elif and Rani Nelken. 2008. Mining
Wikipedia revision histories for improving sen-
tence compression. In Proceedings of ACL-08:
HLT, Short Papers. Association for Computa-
tional Linguistics, Columbus, Ohio, pages 137?
140.
Yatskar, Mark, Bo Pang, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2010. For
the sake of simplicity: Unsupervised extrac-
tion of lexical simplifications from Wikipedia.
In Proceedings of the Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics. pages 365?368.
Zhao, Shiqi, Xiang Lan, Ting Liu, and Sheng Li.
2009. Application-driven statistical paraphrase
generation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP. Singapore,
pages 834?842.
Zhu, Zhemin, Delphine Bernhard, and Iryna
Gurevych. 2010. A monolingual tree-based trans-
lation model for sentence simplification. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics. Beijing, China, pages
1353?1361.
420
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 233?243, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Multiple Aspect Summarization Using Integer Linear Programming
Kristian Woodsend and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
k.woodsend@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
Multi-document summarization involves
many aspects of content selection and sur-
face realization. The summaries must be
informative, succinct, grammatical, and obey
stylistic writing conventions. We present a
method where such individual aspects are
learned separately from data (without any
hand-engineering) but optimized jointly
using an integer linear programme. The
ILP framework allows us to combine the
decisions of the expert learners and to select
and rewrite source content through a mixture
of objective setting, soft and hard constraints.
Experimental results on the TAC-08 data set
show that our model achieves state-of-the-art
performance using ROUGE and signifi-
cantly improves the informativeness of the
summaries.
1 Introduction
Automatic summarization has enjoyed wide popu-
larity in natural language processing (see the pro-
ceedings of the Document Understanding and Text
Analysis conferences) due to its potential for prac-
tical applications but also because it incorporates
many important aspects of both natural language un-
derstanding and generation. Of the many summa-
rization paradigms that have been identified over the
years (see Sparck Jones (1999) and Mani (2001) for
comprehensive overviews), multi-document sum-
marization ? the task of producing summaries from
clusters of thematically related documents ? has
consistently attracted attention.
Despite considerable research effort, the auto-
matic generation of multi-document summaries that
resemble those written by humans remains chal-
lenging. This is primarily due to the task itself
which is complex and subject to several constraints:
the summary must be maximally informative and
minimally redundant, grammatical, coherent, adhere
to a pre-specified length and stylistic conventions.
An ideal model would learn to output summaries
that simultaneously meet alhese constraints from
data (i.e., document clusters and their correspond-
ing summaries). This global inference problem is,
however, hard ? the solution space is large and the
lack of easily accessible datasets an obstacle to joint
learning. It is thus no surprise that previous work has
focused on specific aspects of joint learning.
Initial global formulations of the multi-document
summarization task focused on extractive summa-
rization and used approximate greedy algorithms for
finding the sentences of the summary. Goldstein et
al. (2000) search for the set of sentences that are
both relevant and non-redundant, whereas Filatova
and Hatzivassiloglou (2004) model multi-document
summarization as an instance of the maximum cov-
erage set problem.1 More recent work improves on
the search problem by considering exact solutions
and permits a limited amount of rewriting. McDon-
ald (2007) proposes an integer linear programming
formulation that maximizes the sum of relevance
scores of the selected sentences penalized by the
1Given C, a finite set of weighted elements, a collection T of
subsets of C, and an integer k, find those k sets that maximize the
total number of elements in the union of T ?s members (Hochba,
1997).
233
sum of redundancy scores of all pairs of selected
sentences. Gillick et al2008) develop an exact so-
lution for a model similar to Filatova and Hatzivas-
siloglou (2004) under the assumption that the value
of a summary is the sum of values of the unique con-
cepts (approximated by bigrams) it contains. Subse-
quent work (Gillick et al2009; Berg-Kirkpatrick et
al., 2011) extends this model to allow sentence com-
pression in the form of word or constituent deletion.
In this paper we propose a model for multi-
document summarization that attempts to cover
many different aspects of the task such as content se-
lection, surface realization, paraphrasing, and stylis-
tic conventions. These aspects are learned separately
using specific ?expert? predictors, but are optimized
jointly using an integer linear programming model
(ILP) to generate the output summary.2 All experts
are learned from data without requiring additional
annotation over and above the summaries written
for each document cluster. Our predictors include
the use of unique bigram information to model con-
tent and avoid redundancy, positional information to
model important and poor locations of content, and
language modeling to capture stylistic conventions.
Learning each predictor separately gives better gen-
eralization, while the ILP framework allows us to
combine the decisions of the expert learners through
the use of objectives, hard and soft constraints.
The experts work collaboratively to rewrite the
content using rules extracted from document clusters
and model summaries. We adopt the synchronous
tree substitution grammar (STSG) formalism (Eis-
ner, 2003) which can model non-isomorphic tree
structures (the grammar rules can comprise trees of
arbitrary depth) and is thus suited to text-rewriting
tasks which typically involve a number of local mod-
ifications to the input text. Specifically, we pro-
pose quasi-synchronous tree substitution grammar
(QTSG) as a flexible formalism to learn general tree-
edits from loosely-aligned phrase structure trees.
We evaluate our model on the 100-word ?non-
2Our task is standard multi-document summarization and
should not be confused with ?guided? summarization where
system and human summarizers are given a list of important
aspects to cover in the summary. Our usage of the term aspects
broadly refers to the different types of constraints (e.g., relating
to content or style) a summary must meet, but these are learned
rather than specified in advance.
update? summarization task as defined in the the
Text Analysis Conference (TAC 2008). Experimen-
tal results show that our method obtains perfor-
mance comparable and in some cases superior to
state-of-the-art, in terms of ROUGE and human rat-
ings of summary grammaticality and informative-
ness. Importantly, there is nothing inherent in our
model that is specific to this particular summariza-
tion task. As all of the different experts are learned
from data, it could easily adapt to other summariza-
tion styles or conventions as needed.
2 Related work
Recent years have seen increased interest in global
inference methods for summarization. ILP-based
models have been developed for several subtasks
ranging from sentence compression (Clarke and La-
pata, 2008), to single- and multi-document sum-
marization (McDonald, 2007; Martins and Smith,
2009; Gillick and Favre, 2009; Woodsend and Lap-
ata, 2010; Berg-Kirkpatrick et al2011), and head-
line generation (Deshpande et al2007; Wood-
send et al2010). Most of these approaches are ei-
ther purely extractive or implement a single rewrite
operation, namely word deletion. Although it is
well-known that hand-written summaries often ex-
hibit additional edits and sentence recombinations
(Jing, 2002), the challenges involved in acquiring
the rewrite rules, interfacing them with inference,
and ensuring grammatical output make the develop-
ment of abstractive models non-trivial.
Our work is closest to Gillick et al2008) who
also develop an ILP model for multi-document sum-
marization. A key assumption in their model which
we also follow is that input documents contain a
variety of concepts, each of which are allocated a
value, and the goal of a good summary is to max-
imize the sum of these values subject to the length
constraint. The authors use bigrams as concepts and
their frequency in the input documents as a proxy
for their value. This model can also perform sen-
tence compression (see also Gillick et al2009)),
however, the deletion rules are hand-coded. Berg-
Kirkpatrick et al2011) build on this work by re-
casting it as a structured prediction problem. They
essentially combine the same bigram content scor-
ing system with features relating to the parse tree
234
which they learn using a maximum-margin SVM
trained on annotated gold-standard compressions.
Our multi-document summarization model jointly
optimizes different aspects of the task involving both
content selection and surface realization. Each indi-
vidual aspect has its own dedicated expert, which we
argue is advantageous as it renders inference simpler
and affords flexibility (e.g., additional aspects can be
incorporated into the model or trained separately on
different datasets). Our work differs from Gillick et
al. (2009) and Berg-Kirkpatrick et al2011) in three
important respects. Firstly, we develop a genuinely
abstractive model that is not limited to deletion.
Our rewrite rules are encoded in quasi-synchronous
tree substitution grammar and learned automatically
from source documents and their summaries. Un-
like previous applications of STSG to sentence com-
pression (Cohn and Lapata, 2009; Cohn and Lap-
ata, 2008) our quasi-synchronous TSG does not at-
tempt to learn the complete translation from source
to target sentence; it only loosely links the syntactic
structure of the two (Smith and Eisner, 2006), and
is therefore well suited to describing the relation-
ship between documents and their abstracts. Sec-
ondly, our content selection component extends to
features beyond the bigram horizon, as we learn to
identify important concepts based on syntactic and
positional information. We also learn which words
are unlikely to appear in a summary. Thirdly, unlike
Berg-Kirkpatrick et al2011) our model does not
try to learn all the parameters (e.g., content, rewrite
rules, style) of the summarization problem jointly;
although decoupling learning from inference is per-
haps less elegant from a modeling perspective, the
learning process is more robust and reliable.
3 Modeling
There are many aspects to producing a good sum-
mary of multiple documents. The important con-
tent needs to be captured, typically key facts in
each individual document, and information seen
across the cluster. Stylistic features may be differ-
ent in the summary from original documents. For
instance, summaries tend to use more concise lan-
guage, sources are not attributed as they are in news
articles, and relative dates are not included. In addi-
tion, the summary must be fluent, coherent, and re-
spect a pre-specified maximum length requirement.
We present an approach where elements of all the
above considerations are learned from training data
by separate dedicated components, and then com-
bined in an integer linear programme. Content se-
lection is performed partly through identifying the
most salient topics (bigrams); an additional compo-
nent learns to identify which information from the
source documents should be in the summary based
on positional information. Meanwhile, in terms of
surface realization, a language model identifies the
words that should not be in the output summaries,
whereas a separate component learns to exclude
sentences that are poor candidates for summaries.
QTSG rules, learned from the training corpus, are
used to generate alternative compressions and para-
phrases of the source sentences, in the style suit-
able for the summaries. Finally, an ILP model com-
bines the output of these components into a sum-
mary, jointly optimizing content selection and sur-
face realization preferences, and providing the flexi-
bility to treat some components as soft while others
as hard constraints.
3.1 Document Representation
Given an input sentence, our approach deconstructs
it into component phrases and clauses, typical of a
phrase structure parser. In our experiments, we ob-
tain this representation from the output of the Stan-
ford parser (Klein and Manning, 2003) but any other
broadly similar parser could be used instead. Nodes
in the parse tree represent points where QTSG rules
can be applied (and paraphrases generated), and they
also represent decision points for the ILP. In the fol-
lowing, we will refer to these decision nodes as the
set N , and decisions for each node using the binary
variable zi, i ?N .
3.2 Content Selection Using Bigrams
We follow Gillick et al2008) in modeling the infor-
mation content of the summary as the weighted sum
of the individual information units it contains. We
represent information units as the set of bigrams B
seen in the source documents. The weight w of each
bigram is calculated from the number of source doc-
uments where the bigram was seen. The summary is
thus given the score fB(z), i.e., the weighted sum of
235
its information units:
fB(z) = ?
j?B
w jb j (1)
where w j is the weight of concept j, b j a binary vari-
able to indicate if concept j is present in the sum-
mary, and j ? B .
Importantly, each information unit is counted only
once; this encourages wide coverage of the source
documents, and removes any drive towards redun-
dant information without actively discouraging it,
contrary to other global formulations where redun-
dancy measures form part of the objective (McDon-
ald, 2007). The counting mechanism is achieved by
linking the variables z indicating nodes in the parse
tree and b indicating bigrams:
b j ? ?
i?N : j?Bi
zi ? j ? B (2)
where Bi ? B is the subset of bigrams that are con-
tained in node i. A drawback of the global nature
of this counting mechanism, however, is that it can-
not be integrated with local features such as those
described below; our approach takes local features
into account but these are weighted by other compo-
nents.
3.3 Content Selection Using Salience
The bigram approach is a powerful method for
identifying important concepts within the document
cluster. It works particularly well in the sentence ex-
traction paradigm. However, additional elements are
known to be good predictors of important informa-
tion. Examples include the position of a sentence
in the document (e.g., first sentences often con-
tain salient information), whether it contains proper
nouns, numbers, pronouns, mentions of money, and
so on. We decided to learn which of these elements
(represented as nodes in the parse tree) are infor-
mative from training data. Specifically, sentences
in the cluster documents were aligned to sentences
from corresponding human summaries. Alignment
was based rather simply on identifying the sentence
pairs with the highest number of overlapping bi-
grams, without compensating for sentence length, or
matching the sequence of information in the sum-
maries and source documents (Nelken and Schieber,
Weight Feature
1.21 From first sentence in document
0.73 Contains proper nouns
0.68 Contains nouns
0.57 From first paragraph
0.53 From first three sentences
0.51 Contains numbers
-0.50 Contains pronouns
0.32 Contains money
Table 1: Weights and features of SVM that predicts the
salience of summary content. Negative weights indicate
information that should not be included in the summary.
2006). Matched sentences in the source documents
were given positive labels, while unaligned sen-
tences were given negative labels. These labels were
then propagated to phrase structure nodes.
We trained an SVM on this data (tree nodes and
their labels) using surface features that do not over-
lap with bigram information: sentence and para-
graph position, POS-tag information. Table 1 shows
the most important features learned by the model as
predictors of salient content.
The summary can be given a salience score fS (z)
using the raw SVM prediction scores of the individ-
ual parse tree nodes:
fS (z) = ?
i?N
(?(i) ??)zi (3)
where ?(i) is the feature vector for node i, and ? the
weights learned by the SVM.
3.4 Surface Realization Using Style
Some sentences in the source documents will make
poor summary sentences, despite the information
they contain, and therefore contrary to the predic-
tions of the content selection indicators described
above. This may be because the source sentence is
very short, or is expressed as a quotation, or con-
tains many pronouns that will not be resolved when
the sentence is extracted.
Our idea is to learn which sentences are poor from
a stylistic perspective using again aligned training
data. We train a second SVM on the aligned sen-
tences and their labels using surface features at the
sentence level, such as sentence length and POS-tag
information. The most important features learned by
236
Weight Feature
-1.04 Word count less than 10
-0.83 Word count less than 20
-0.30 Question
-0.30 Quotation
-0.14 Personal pronouns
Table 2: Weights and features of SVM that predicts poor
candidate sentences.
the model as predictors of poor sentences, and the
weights assigned to them, are shown in Table 2.
The predictions of the SVM are incorporated into
the ILP as a hard constraint, by forcing all parse tree
nodes within those sentences predicted as poor (the
set N ?) to be zero:
zi = 0 ?i ?N ?. (4)
3.5 Surface Realization Using Lexical
Preferences
Human-written summaries differ from the source
news articles in a number of ways. They delete ex-
traneous information, merge material from several
sentences, employ paraphrases and syntactic trans-
formations, change the order of the source sentences
and replace phrases or clauses with more general
or specific descriptions. We could attempt to learn
the ?language of summaries? with a language model
which we could then use to guide the generation
process (e.g., by producing maximally probable out-
put). Aside from the logistics of gathering training
data large enough to provide robust estimates, we
believe that a more compelling approach is to focus
on the words that are unlikely to appear in the sum-
mary despite appearing in the source documents.
A comparison of the language models generated
from the source documents and model summaries,
even at the unigram level, is revealing. Table 3 shows
lexemes that appear in both source and summary
documents, but where the likelihood of the lexeme
appearing in the summary is much less than that
of it appearing the document, taking into account
that the summary is much shorter anyway. The fi-
nal column shows the log10-ratio (L(w)) between
the two probabilities. We can see that least prob-
able words are those that correspond to attribut-
ing information sources (e.g., said, told, according
Lexeme w Source Summary L(w)
count count
say 5670 88 -1.63
go 638 11 -1.52
last 616 9 -1.69
get 543 15 -1.05
tell 512 8 -1.62
come 488 12 -1.17
know 404 9 -1.27
monday 391 8 -1.35
think 382 7 -1.46
next 239 7 -0.99
spokesman 197 4 -1.36
Table 3: Counts of lexemes in the source news articles and
summaries, and measure of the ratio of their probabilities
(for most common lexemes with ratio <?0.95).
to, spokesman), dates described relatively (e.g., last
Monday), and events that are in the process of hap-
pening (e.g., coming, going).
As the amount of training data tends to be lim-
ited ? there are usually only a few human-written
summaries available per document cluster ? we
use a unigram language model, but conceivably a
longer-range n-gram could be employed in the same
vein. We incorporate preferences about summary
language into the model as a soft constraint. The
log-ratio values fLR (z) are included in the objective
and defined at the tree node level:
fLR (z) = ?
i?N
?
w?Wi
L(w)zi (5)
where L(w), w ?Wi is the log-ratio value for an in-
dividual word w:
L(w) = log10
Psrc(w)
Psum(w)
,
Psrc(w) and Psum(w) are the probabilities of word w
appearing in the source and summary documents re-
spectively, and Wi is the set of words at parse tree
node i. Importantly, we include only those those lex-
emes with negative L(w) values. This guides the
model away from the kind of phrases described
above, but not towards any particular language pref-
erences.
237
3.6 Quasi-synchronous Tree Substitution
Grammar
Rewrite rules involving substitutions, deletions and
reorderings are captured in our model using a quasi-
synchronous tree substitution grammar. Given an in-
put (source) sentence S1 or its parse tree T1, the
QTSG contains rules for generating possible trans-
lation trees T2. A grammar node in the target tree T2
is modeled on a subset of nodes in the source tree,
with a rather loose alignment between the trees.
We extract QTSG rules from aligned source
and summary sentence pairs represented by their
phrase structure trees. Our algorithm builds up a
list of leaf node alignments based on lexical iden-
tity. Direct parent nodes are aligned where more
than one child node aligns. This quasi-synchronous
?bottom-up? process gives us better ability to match
non-isomorphic structures. We do not assume an
alignment between source and target root nodes, nor
do we require a surjective alignment of all target
nodes to the source tree. QTSG rules are then cre-
ated from aligned nodes above the leaf node level if
all the nodes in the target tree can be explained us-
ing nodes from the source. Individual rewrite rules
describe the mapping of source tree fragments into
target tree fragments, and so the grammar represents
the space of valid target trees that can be produced
from a given source tree (Eisner, 2003; Cohn and
Lapata, 2009).
Examples of the most frequent QTSG rules
learned by the above process are shown in Figure 1.
Many of the rules relate to the compression of noun
phrases through deletion, and examples are shown
in the upper box. Others capture the compression of
verb phrases (middle box). An important rewrite op-
eration is the abstraction of a sentence from a more
complex source sentence, adding final punctuation if
necessary (lower box).
At generation, paraphrases are created from
source sentence parse trees by identifying and ap-
plying QTSG rules with matching structure. The
transduction process starts at the root node of the
parse tree, applying QTSG rules to sub-trees un-
til leaf nodes are reached. Note that we do not use
the Bayesian probability model normally associated
with quasi-synchronous grammars (Smith and Eis-
ner, 2006); instead, we ask the QTSG to provide
?NP, NP? ? ?[NP 1 PP], [NP 1 ]?
?NP, NP? ? ?[NP 1 VP], [NP 1 ]?
?NP, NP? ? ?[NP 1 SBAR], [NP 1 ]?
?NP, NP? ? ?[NP 1 , NP ,], [NP 1 ]?
?NP, NP? ? ?[NP 1 CC NP], [NP 1 ]?
?NP, NP? ? ?[NNP NNP 1 ], [NNP 1 ]?
?NP, NP? ? ?[DT 1 JJ NN 2 ], [DT 1 NN 2 ]?
?VP, VP? ? ?[VP 1 CC VP], [VP 1 ]?
?VP, VP? ? ?[VP CC VP 1 ], [VP 1 ]?
?VP, VP? ? ?[VP 1 , CC VP], [VP 1 ]?
?S, S? ? ?[NP 1 VP 2 ], [NP 1 VP 2 .]?
?S, S? ? ?[ADVP , NP 1 VP 2 .], [NP 1 VP 2 .]?
Figure 1: Examples of most frequently learned QTSG
rules. Boxed subscripts show aligned nodes.
paraphrases that are acceptable rather than probable,
and generate all paraphrases licensed by the QTSG.
The alternative paraphrases are incorporated into
the target phrase structure tree as choices that the
ILP can make. We use the set C ? N to be the
set of nodes where a choice of paraphrases is avail-
able, and Ci ?N , i ? C to be the actual paraphrases
of i. Where there are alternatives, it makes sense of
course to select only one, which we implement using
the constraint:
?
j?Ci
z j = zi ?i ? C , j ? Ci (6)
More generally, we need to constrain the output to
ensure that a parse tree structure is maintained. For
each node i ?N , the set Di ?N contains the list of
dependent nodes (both ancestors and descendants)
of node i, so that each set Di contains the nodes that
depend on the presence of i. We introduce a con-
straint to force node i to be present if any of its de-
pendent nodes are chosen:
z j? zi ?i ?N , j ?Di (7)
3.7 The ILP Objective
The model we propose for generating a multi-
document summary is expressed as an integer linear
programme and incorporates the content selection
and surface realization preferences, as well as the
238
soft and hard constraints described in the preceding
sections. The objective of the optimization problem
is to maximize the score contributed by the various
elements of content selection ( fB(z) and fS (z)) and
soft surface realization constraints ( fLR (z)) :
max
z
fB(z)+ fS (z)+ fLR (z) (8)
This objective is subject to the constraints (2), (4),
(6), and (7) that represent hard constraint decisions,
or maintain the logical integrity of the model. An
overall length constraint completes the model:
?
i?N
lizi ? lmax (9)
where li is the number of words generated by choos-
ing node i, and lmax is the global word length limit.
Note that the scores in the objective are for each
tree node and not each sentence. This affords the
model flexibility: the content selection elements are
generally not competing with each other to give a
decision on a sentence (see McDonald (2007)). In-
stead, components are marking positive and nega-
tive nodes. The ILP is implicitly searching the gram-
mar rules for ways to rewrite the sentence, with the
aim of including the salient nodes while removing
negative-scoring nodes (deleting them increases the
score of the node to zero). Figure 2 shows an exam-
ple of a source sentence where the bigram, salience
and language preference components of the ILP
work together to score nodes in the parse tree. The
nodes NP 1 , VP 3 and VP 4 all have positive scores,
while ?said Tuesday? is negative. As a rewrite pos-
sibility, the rewrite rule shown bottom left is avail-
able, which will remove the negative node. Further
rewrite rules allow VP 2 to be compressed. The out-
put actually generated by the model used sub-trees
(b) and (d) ? the final text is included in Table 6.
4 Experimental Set-up
Data Our model was evaluated on the TAC non-
update multi-document summarization task which
involves generating a 100-word-limited summary
from a cluster of 10 related input documents; ad-
ditionally, TAC provides a set of four model sum-
maries for each cluster, written by human experts.
We used the 44 document clusters from TAC-2009
as training data, to learn the different elements of
the model. The 48 document clusters of TAC-2008
were reserved for the generation of test summaries.3
Training The two components described in Sec-
tions 3.3 and 3.4 were trained using binary SVM
classifiers, with labels inferred automatically via
alignment. The salience classifier was trained on
102,754 node instances (16,042 positive and 86,712
negative). The style classifier was trained on 20,443
sentence instances (2,083 positive and 18,360 neg-
ative). We learned the feature weights with a linear
SVM, using the software SVM-OOPS (Woodsend
and Gondzio, 2009). Because of the high compres-
sion rate in this task, sentence alignment leads to an
unbalanced data set. We compensated for this by us-
ing different SVM hyper-parameters C+ and C? as
the loss multiplier for misclassification of positive
and negative training samples respectively. SVM
hyper-parameters were chosen that gave the high-
est F1 values using 10-fold cross-validation. The
salience SVM obtained a precision of 0.28 and re-
call of 0.43. Precision for the style SVM was 0.20
and recall 0.63, respectively. The classifiers on their
own would thus not be great predictors of salience
or style, but in practice they were useful for break-
ing ties in bigram scores.
Aligned sentences from the training data were
also used to learn the quasi-synchronous tree sub-
stitution grammar, using the process described in
Section 3.6. Rules seen fewer than 3 times were re-
moved, resulting in a total of 339 QTSG rules. Two
unigram language models (see Section 3.5) were
trained on the source articles and summaries, respec-
tively. Their probabilities were compared to give the
word list shown in Table 3. We removed words with
a source count less than 50, providing a list of 60 lex-
emes. The resulting integer linear programmes were
solved using SCIP,4 and it took 55 seconds on aver-
age to read in and solve a document cluster problem.
Evaluation We compared our model against two
systems. As a baseline, we used the ICSI-1 extrac-
tive system (Gillick et al2008) which is also based
on ILP and was highly ranked in the TAC-2008
evaluation. We also compared against the ?learned
phrase compression? system of Berg-Kirkpatrick et
3This split follows Berg-Kirkpatrick et al2011).
4http://scip.zib.de/
239
(a) S
.
.
VP
said Tuesday
NP
a top space
official
,
,
S
VP 2
SBAR 4
if its maiden unmanned spacecraft Chandrayaan-1,
slated to be launched by 2008, is successful in
mapping the lunar surface
VP 3
will launch more mis-
sions to the moon
NP 1
India
(b) S
.VP 2NP 1
(c) VP 2
VP 3
(d) VP 2
SBAR 4VP 3
?S, S? ?
?[NP 1 VP 2 ], [NP 1 VP 2 .]?
?VP 2 , VP 2 ? ?
?[VP 3 SBAR], [VP 3 ]?
?VP 2 , VP 2 ? ?
?[VP 3 SBAR 4 ], [VP 3 SBAR 4 ]?
Figure 2: Sentence representation provided to the ILP. (a) The source sentence representation (child nodes condensed
for space reasons). Bigrams are shown in bold, slanted text indicates phrases with high salience scores fS , while said
Tuesday is penalized by fLR . Alternative sub-trees (b), (c) and (d) are created using QTSG rules (dashed lines). The
output sentence (see Table 6) was generated from sub-trees (b) and (d).
al. (2011) (henceforth B-K), which has the highest
reported ROUGE scores that we are aware of.5 In
addition to the full model described in Section 3, we
also produced outputs where each of the five compo-
nents described in Sections 3.2?3.6 were removed,
to assess their individual contribution.
We evaluated the output summaries in two ways,
using automatic measures and human judgements.
Automatic evaluation was performed with ROUGE
(Lin and Hovy, 2003) using TAC-2008 parame-
ter settings. We report bigram overlap (ROUGE-2)
and skip-bigram (ROUGE-SU4) recall values. We
also used Translation Edit Rate (TER, Snover et al
(2006)) to examine the systems? rewrite potential.
TER is defined as the minimum number of edits
(insertions, deletions, substitutions, and shifts) re-
quired to change the system output so that it exactly
matches a reference (here, the reference is the most
closely aligning source sentence). The perfect TER
score is 0, however note that it can be higher than 1
due to insertions.
Our judgement elicitation study was conducted
as follows. We randomly selected ten document
5We are grateful to Taylor Berg-Kirkpatrick for making his
system output available to us.
clusters from the test set and generated summaries
with our model (and its lesser variations). We also
included the corresponding ICSI-1 and B-K sum-
maries, and one randomly-selected model summary.
The study was conducted over the Internet using
Mechanical Turk and was completed by 54 volun-
teers, all self reported native English speakers. Par-
ticipants were first asked to read the documents in
each cluster. Next, they were asked a few compre-
hension questions to ensure they had understood and
processed the documents. Finally, they were pre-
sented with a summary and asked to rate it along
two dimensions: grammaticality (is the summary
fluent and grammatical?), and informativeness (are
the main topics captured in the summary?). The sub-
jects used a 1?5 rating scale, with half-points al-
lowed. Participants who declared themselves as non-
native English speakers, did not answer the compre-
hension questions correctly or took only a few min-
utes to complete the task were eliminated.
5 Results
Our results are summarized in Table 4. Let us first
discuss those obtained using ROUGE-2 (2-R) and
ROUGE-SU4 (SU4-R) recall values. As can be seen
240
Models ROUGE TER (%) Sentences
2-R SU4-R Ins Del Sub Shift Count CR (%) Mod (%)
ICSI-1 11.03 13.96 ? ? ? ? 200 ? ?
B-K 11.71 14.47 0.2 26.2 2.3 0.4 216 74.0 63.9
MA-ILP 11.37 14.47 0.7 11.6 5.3 0.6 191 89.1 61.8
ILP w/o bigrams 9.24 12.66 0.8 15.4 11.8 1.2 205 85.4 80.0
ILP w/o salience 11.38 14.71 1.1 19.1 12.0 1.3 233 82.1 92.3
ILP w/o style 11.83 15.09 1.4 17.4 18.9 1.7 271 84.1 86.3
ILP w/o log-ratio 11.41 14.70 1.2 16.9 12.5 1.5 223 84.3 90.1
ILP w/o QTSG 10.32 13.68 0 0 0 0 163 100.0 0
Table 4: Performance of the multiple-aspect ILP model against comparison systems using ROUGE and the four com-
ponents of TER (insertion, deletion, substitution, shifts). In the lower section, performance of our model without (w/o)
each component in turn. The final columns show the number of source sentences, the average compression ratio, and
the proportion of sentences modified.
from the upper section of Table 4, the systems incor-
porating some form of rewriting gain slightly higher
ROUGE scores than ICSI-1. The multiple aspects
ILP system (MA-ILP) yields ROUGE scores simi-
lar to B-K, despite performing rewriting operations
which increase the scope for error and without re-
quiring any hand-crafted compression rules or man-
ually annotated training data. Indeed, the outputs of
the two systems are not significantly different under
ROUGE (using a paired t-test, p > 0.5).
In the lower section of Table 4, we show the per-
formance of our model when each of the contribut-
ing components described in Section 3 are removed.
Clearly the bigram content indicators are an impor-
tant element for the ROUGE scores, as their removal
yields a reduction of 2.46 points (see the row ILP
w/o bigrams in Table 4). The model without QTSG
rules (ILP w/o QTSG) is effectively limited to sen-
tence extraction, and removing rewrite rules also
lowers ROUGE scores to levels similar to ICSI-1.
ROUGE scores are increased by allowing the model
to select ?poor quality? sentences (ILP w/o style),
higher indeed than those of the B-K system. The
inclusion of non-summary language (ILP w/o log-
ratio) does not affect ROUGE scores to the same ex-
tent that bigrams and QTSG do.
Table 4 includes a break-down of the systems?
rewrite operations as measured by TER. We also
show the number of source sentences (Count), the
average compression ratio (CR %) and the propor-
tion of sentences modified (Mod %) by each system.
As can be seen, MA-ILP draws on fewer sentences,
Models Grammar Inform
ICSI-1 4.68 2.55
B-K 4.40 2.70
MA-ILP 4.68 3.90
ILP w/o style 3.30 2.67
Gold 4.90 4.75
Table 5: Mean ratings on system output output.
performs less deletion and more rewriting than B-K.
The number of deletions increases when individual
ILP components are removed and so does the num-
ber of substitutions. All the subsystems are more ag-
gressive in their rewriting than when used in com-
bination (higher TER, higher compression rate and
a larger number of sentences are modified). Expect-
edly, when removing the QTSG rules, the ILP is lim-
ited to a pure extractive system (last row in Table 4).
The results of our human evaluation study are
shown in Table 5. We elicited grammaticality and in-
formativeness ratings for a randomly selected model
summary, ICSI-1, B-K, the multiple aspect ILP
(MA-ILP), and the ILP w/o style which we in-
cluded in this study as it performed best under
ROUGE. ICSI-1, B-K, and MA-ILP are rated highly
on the grammaticality dimension. MA-ILP is in-
distinguishable from the sentence extraction sys-
tem (ICSI-1). Both systems are significantly more
grammatical than B-K (?< 0.05, using a Post-hoc
Tukey test). Notice that summaries created by the
ILP w/o style are rated poorly by humans, contrary
to ROUGE. The style component stops very short
241
Florida?s Governor Jeb Bush asked the US
Supreme Court to intervene to keep a comatose
woman alive, over the wishes of her husband,
who wants to disconnect the feeding tube that
has sustained her for 14 years. Her husband,
Michael Schiavo, and her parents, Robert and
Mary Schindler, have conflicts of interest that pre-
vent them from fairly deciding whether to keep
her alive. Some doctors have testified that Terri
Schiavo is in a persistent vegetative state with
no hope for recovery. The state House in Florida
passed a bill Thursday to extend life support for a
brain-damaged woman.
The space agencies of India and France signed an
agreement to cooperate in launching a satellite in
four years that will help make climate predictions
more accurate. The Indian Space Research Orga-
nization (ISRO) has short-listed experiments from
five nations including the United States, Britain
and Germany, for a slot on India?s unmanned
moon mission Chandrayaan-1 to be undertaken
by 2006-2007, the Press Trust of India (PTI) re-
ported Monday. India will launch more missions
to the moon if its maiden unmanned spacecraft
Chandrayaan-1, slated to be launched by 2008, is
successful in mapping the lunar surface.
Table 6: Example summaries generated by the multiple
aspects model (MA-ILP).
sentences and quotations from being included in the
summary even if they have quite high bigram or
content scores. Without it, the model tends to gen-
erate summaries that are fragmentary and lacking
proper context, resulting in lower grammaticality
(and informativeness) when judged by humans. The
MA-ILP system obtains the highest rating with re-
spect to information content. It is significantly better
(?< 0.05) than ICSI-1 and B-K. This is not entirely
surprising as our model includes additional content
selection elements over and above the bigram units.
There is still a significant gap from all systems to the
gold-standard human-authored summaries. Example
output summaries of the full ILP model are shown in
Table 6.
Overall, we obtain best results when considering
the contributions from the individual model experts
collectively. This suggests that additional improve-
ments could be obtained with more experts. It is also
possible that optimizing the relative weightings of
experts in the ILP objective would improve output.
The TER analysis shows that the experts have a tem-
pering effect on each other, resulting in less aggres-
sive, but qualitatively better, rewriting than when
used individually. Generally, experts work together
to shape an output sentence, but they can also com-
pete. In the future, we also plan to test the ability
of the model to adapt to other multi-document sum-
marization tasks, where the location of summary in-
formation is not as regular as it is in news articles.
We would also like interface our model with sen-
tence ordering and more generally with some notion
of the coherence of the generated summary.
Acknowledgments We are grateful to Micha El-
sner for his input on earlier versions of this work.
We would also like to thank members of the ILCC
at the School of Informatics for valuable discus-
sions and comments. We acknowledge the support
of EPSRC through project grants EP/I032916/1 and
EP/I017127/1.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 481?490, Portland, Ore-
gon.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
22nd International Conference on Computational Lin-
guistics, pages 137?144, Manchester, UK.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Pawan Deshpande, Regina Barzilay, and David Karger.
2007. Randomized decoding for selection-and-
ordering problems. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
242
Proceedings of the Main Conference, pages 444?451,
Rochester, New York.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
ACL Interactive Poster/Demonstration Sessions, pages
205?208, Sapporo, Japan.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the 20th
International Conference on Computational Linguis-
tics, pages 397?403, Geneva, Switzerland.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18, Boulder, Colorado.
Dan Gillick, Benoit Favre, and Dilek Hakkani-tu?r. 2008.
The ICSI summarization system at TAC 2008. In Pro-
ceedings of the Text Analysis Conference.
Dan Gillick, Benoit Favre, Dilek Hakkani-tu?r, Berndt
Bohnet, Yang Liu, and Shasha Xie. 2009. The
ICSI/UTD summarization system at TAC 2009. In
Proceedings of the Text Analysis Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization
by sentence extraction. In Proceedings of the 2000
NAACL?ANLP Workshop on Automatic Summariza-
tion, pages 40?48, Seattle, Washington.
Dorit S. Hochba. 1997. Approximating covering
and packing problems: Set cover, vertex cover, in-
dependent set, and related problems. In Dorit S.
Hochba, editor, Approximation Algorithms for NP-
Hard Problems, pages 94?143. PWS Publishing Com-
pany, Boston, MA.
Honyang Jing. 2002. Using Hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?544.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st As-
sociation for Computational Linguistics, pages 423?
430, Sapporo, Japan.
Chin-Yew Lin and Eduard H. Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of HLT?NAACL, pages 71?
78, Edmonton, Canada.
Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Pub Co.
Andre? Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on In-
teger Linear Programming for Natural Language Pro-
cessing, pages 1?9, Boulder, Colorado.
Ryan McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceed-
ings of the 29th European conference on IR Research,
pages 557?564, Rome, Italy.
Rani Nelken and Stuart Schieber. 2006. Towards ro-
bust context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 161?168, Trento, Italy.
David Smith and Jason Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of Workshop on Statis-
tical Machine Translation, pages 23?30, NYC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas, pages
223?231, Cambridge.
Karen Sparck Jones. 1999. Automatic summarizing:
Factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization, pages 1?33. MIT Press, Cambridge.
Kristian Woodsend and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector ma-
chine training. Computational Optimization and Ap-
plications.
Kristian Woodsend and Mirella Lapata. 2010. Automatic
generation of story highlights. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 565?574, Uppsala, Sweden.
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Title generation with quasi-synchronous gram-
mar. In Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing, pages
513?523, Cambridge, MA.
243
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 407?413,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Composition of Word Representations
Improves Semantic Role Labelling
Michael Roth and Kristian Woodsend
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
{mroth,kwoodsen}@inf.ed.ac.uk
Abstract
State-of-the-art semantic role labelling
systems require large annotated corpora to
achieve full performance. Unfortunately,
such corpora are expensive to produce and
often do not generalize well across do-
mains. Even in domain, errors are often
made where syntactic information does
not provide sufficient cues. In this pa-
per, we mitigate both of these problems
by employing distributional word repre-
sentations gathered from unlabelled data.
While straight-forward word representa-
tions of predicates and arguments improve
performance, we show that further gains
are achieved by composing representa-
tions that model the interaction between
predicate and argument, and capture full
argument spans.
1 Introduction
The goal of semantic role labelling (SRL) is to
discover the relations that hold between a pred-
icate and its arguments in a given input sen-
tence (e.g., ?who? did ?what? to ?whom?, ?when?,
?where?, and ?how?). This semantic knowl-
edge at the predicate-argument level is required
by inference-based NLP tasks in order to iden-
tify meaning-preserving transformations, such as
active/passive, verb alternations and nominaliza-
tions. Several manually-build semantic resources,
including FrameNet (Ruppenhofer et al., 2010)
and PropBank (Palmer et al., 2005), have been
developed with the goal of documenting and pro-
viding examples of such transformations and how
they preserve semantic role information. Given
that labelled corpora are inevitably restricted in
size and coverage, and that syntactic cues are not
by themselves unambiguous or sufficient, the suc-
cess of systems that automatically provide corre-
sponding analyses has been limited in practice.
Recent work on SRL has explored approaches
that can leverage unlabelled data, following a
semi-supervised (F?urstenau and Lapata, 2012;
Titov and Klementiev, 2012) or unsupervised
learning paradigm (Abend et al., 2009; Titov and
Klementiev, 2011). Unlabelled data provides ad-
ditional statistical strength and can lead to more
consistent models. For instance, latent representa-
tions of words can be computed, based on distri-
butional similarity or language modelling, which
can be used as additional features during tradi-
tional supervised learning. Although we would
expect that extra features would improve classifier
performance, this seems in part counter-intuitive.
Just because one word has a specific representa-
tion does not mean that it should be assigned a
specific argument label. Instead, one would ex-
pect a more complex interplay between predicate,
argument and the context they appear in.
In this paper, we investigate the impact of dis-
tributional word representations for SRL. Initially,
we augment the feature space with word repre-
sentations for a predicate and its argument head.
Furthermore, we use a compositional approach to
model a representation of the full argument, by
composing a joint representation of all words in
the argument span, and we also investigate the in-
teraction between predicate and argument, using
a compositional representation of the dependency
path. We demonstrate the benefits of these com-
positional features using a state-of-the-art seman-
tic role labeller, which we evaluate on the English
part of the CoNLL-2009 data set.
2 Related Work
Research into using distributional information
in SRL dates back to Gildea and Jurafsky
(2002), who used distributions over verb-object
co-occurrence clusters to improve coverage in ar-
gument classification. The distribution of a word
over these soft clusters assignments was added as
407
features to their classifier. The SRL system by
Croce et al. (2010) combines argument clustering
based on co-occurrence frequencies with a lan-
guage model. Collobert et al. (2011) used dis-
tributional word representations in a neural net-
work model that can update representations dur-
ing training. Zapirain et al. (2013) suggested dis-
tributional information as a basis for a selectional
preference model that can be used as a single addi-
tional feature for classifying potential arguments.
Most recently, Hermann et al. (2014) used distri-
butional word representations within pre-defined
syntactic contexts as input to a classifier which
learns to distinguish different predicate senses.
A complementary line of research explores the
representation of sequence information. Promi-
nent examples are the works by Deschacht and
Moens (2009) and Huang and Yates (2010) who
learned and applied Hidden Markov Models to
assign state variables to words and word spans,
which serve as supplementary features for classifi-
cation. One drawback of this approach is that state
variables are discrete and the number of states
(i.e., their granularity) has to be chosen in advance.
The popularity of distributional methods for
word representation has been a motivation for de-
veloping representations of larger constructions
such as phrases and sentences, and there have
been several proposals for computing the meaning
of word combinations in vector spaces. Mitchell
and Lapata (2010) introduced a general frame-
work where composition is formulated as a func-
tion f of two vectors u and v. Depending on
how f is chosen, different composition models
arise, the simplest being an additive model where
f(u, v) = u + v. To capture relational functions,
Baroni and Zamparelli (2010) expanded on this
approach by representing verbs, adjectives and ad-
verbs by matrices which can modify the properties
of nouns (represented by vectors). Socher et al.
(2012) combined word representations with syn-
tactic structure information, through a recursive
neural network that learns vector space represen-
tations for multi-word phrases and sentences. An
empirical comparison of these composition meth-
ods was provided in (Blacoe and Lapata, 2012).
In this work, we use type-based continuous rep-
resentations of words to compose representations
of multiple word sequences and spans, which can
then be incorporated directly as features into SRL
systems.
Distributional Feature Computation
Argument a ~a
Predicate p ~p
Predicate-argument Interaction ~a + ~p
Argument Span w
1
. . . w
n
?
i
~w
i
Dependency Path from a to p ?
w?path(a,p)
~w
Table 1: Features based on distributional word
representations and additive composition. Vector
~w denotes the representation of word w.
3 Method
Following the set-up of the CoNLL shared task
in 2009, we consider predicate-argument struc-
tures that consist of a verbal or nominal pred-
icate p and PropBank-labelled arguments a
i
?
{a
1
. . . a
n
}, where each a
i
corresponds to the head
word of the phrase that constitutes the respective
argument. Traditional semantic role labelling ap-
proaches compute a set of applicable features on
each pair ?p, a
i
?, such as the observed lemma type
of a word and the grammatical relation to its head,
that serve as indicators for a particular role label.
The disadvantage of this approach lies in the
fact that indicator features such as word and
lemma type are often sparse in training data and
hence do not generalize well across domains. In
contrast, features based on distributional represen-
tations (e.g., raw co-occurrence frequencies) can
be computed for every word, given that it occurs
in some unlabelled corpus. In addition to this ob-
vious advantage for out-of-domain settings, dis-
tributional representations can provide a more ro-
bust input signal to the classifier, for instance by
projecting a matrix of co-occurrence frequencies
to a lower-dimensional space. We hence hypoth-
esize that such features enable the model to be-
come more robust out-of-domain, while providing
higher precision in-domain.
Although simply including the components of
a word representation as features to a classifier
can lead to immediate improvements in SRL per-
formance, this observation seems in part counter-
intuitive. Just because one word has a specific
representation does not mean that it should be as-
signed a specific argument label. In fact, one
would expect a more complex interplay between
the representation of an argument a
i
and the con-
text it appears in. To model aspects of this inter-
play, we define an extended set of features that
408
further includes representations for the combina-
tion of p and a
i
, the set of words in the depen-
dency path between p and a
i
, and the set of words
in the full span of a
i
. We compute additive com-
positional representations of multiple words, us-
ing the simplest method of Mitchell and Lapata
(2010) where the composed representation is the
uniformly weighted sum of each single represen-
tation. Our full set of feature types based on distri-
butional word representations is listed in Table 1.
4 Experimental Setup
We evaluate the impact of different types of fea-
tures by performing experiments on a benchmark
dataset for semantic role labelling. To assess the
gains of distributional representations realistically,
we incorporate the features described in Section 3
into a state-of-the-art SRL system. The follow-
ing paragraphs summarize the details of our ex-
perimental setup.
Semantic Role Labeller. In all our experi-
ments, we use the publicly available system by
Bj?orkelund et al. (2010).
1
This system com-
bines the first-ranked SRL system and the first-
ranked syntactic parser in the CoNLL 2009 shared
task for English (Bj?orkelund et al., 2009; Bohnet,
2010). To the best of our knowledge, this
combination represents the current state-of-the-art
for semantic role labelling following the Prop-
Bank/NomBank paradigm (Palmer et al., 2005;
Meyers et al., 2004). To re-train and evaluate mod-
els with different feature sets, we use the same
training, development and test sets as provided
in the CoNLL shared task (Haji?c et al., 2009).
Although the employed system features a full
syntactic-semantic parsing pipeline, we only mod-
ify the feature sets of the two components directly
related to the actual role labelling task, namely ar-
gument identification and argument classification.
Word Representations. As a baseline, we sim-
ply added as features the word representations of
the predicate and argument head involved in a
classification decision (first two lines in Table 1).
We experimented with a range of publicly avail-
able sets of word representations, including em-
beddings from various neural language models
1
http://code.google.com/p/mate-tools/
2
http://metaoptimize.com/projects/wordreprs/
3
http://ai.stanford.edu/%7eehhuang/
4
http://lebret.ch/words/
5
http://www.cis.upenn.edu/%7eungar/eigenwords/
Development dims P R F
1
None ? 86.1 81.0 83.5
Brown clusters
2
320 86.2 81.3 83.7
Neural LM
2
50 86.2 81.4 83.7
Neural LM+Global
3
50 86.2 81.4 83.7
HLBL
2
50 86.3 81.3 83.7
H-PCA
4
50 86.2 81.3 83.7
Eigenwords
5
50 86.2 81.3 83.6
Table 2: Results on the CoNLL-2009 develop-
ment set, using off-the-shelf word representations
for predicates and argument as additional features.
Performance numbers in percent.
(Mnih and Hinton, 2009; Collobert et al., 2011;
Huang et al., 2012), eigenvectors (Dhillon et al.,
2011), Brown clusters (Brown et al., 1992), and
post-processed co-occurrence counts (Lebret and
Collobert, 2014). Results on the development set
for various off-the-shelf representations are shown
in Table 2. The numbers reveal that any kind of
word representation can be employed to improve
results. We choose to perform all follow-up exper-
iments using the 50-dimensional embeddings in-
duced by Turian et al. (2010), using the method by
Collobert et al., as they led to slightly better results
in F
1
-score than other representations. No signif-
icant differences were observed, however, using
other types of representations or vector sizes.
5 Results
We evaluate our proposed set of additional fea-
tures on the CoNLL-2009 in-domain and out-of-
domain test sets, using the aforementioned SRL
system and word representations. All results are
computed using the system?s built-in preprocess-
ing pipeline and re-trained models for argument
identification and classification. We report la-
belled precision, recall and semantic F1-score as
computed by the official scorer.
The upper part of Table 3 shows SRL perfor-
mance on the in-domain CoNLL-2009 test set,
with and without (Original) additional features
based on distributional representations. The re-
sults reveal that any type of additional feature
helps to improve precision and recall in this setting
(from 85.2% F
1
-score up to 85.5%), with signifi-
cant gains for 4 of the 5 additional features (com-
puted using a randomization test; cf. Yeh, 2000).
Interestingly, we find that the features do not seem
409
In-domain P R F
1
Original 87.4 83.1 85.2
Original + Argument 87.6 83.3 85.4**
Original + Predicate 87.4 83.2 85.2
Original + Interaction 87.5 83.3 85.3**
Original + Span 87.6 83.5 85.5**
Original + Path 87.5 83.4 85.4**
Original + All 87.6 83.4 85.5**
Out-of-domain P R F
1
Original 76.9 71.7 74.2
Original + Argument 77.4 71.9 74.5
Original + Predicate 77.3 72.2 74.7*
Original + Interaction 77.2 72.0 74.5
Original + Span 77.3 72.3 74.7*
Original + Path 77.2 72.3 74.7*
Original + All 77.5 73.0 75.2**
Table 3: Results on both CoNLL-2009 test sets.
All numbers in percent. Significant differences
from Original in terms of F
1
-score are marked by
asterisks (* p<0.05, ** p<0.01).
to have a cumulative effect here, as indicated by
the results with all features (+All, 85.5% F
1
). We
conjecture that this is due to the high volume of
existing in-domain training data, which renders
our full feature set redundant. To test this conjec-
ture, we further assess performance on the out-of-
domain test set of the CoNLL-2009 shared task.
The results for the out-of-domain experiment
are summarized in the lower part of Table 3.
We again observe that each single feature type
improves classification, with absolute gains be-
ing slightly higher than in the in-domain setting.
More interestingly though, we find that the com-
plete feature set boosts performance even further,
achieving an overall gain in precision and recall
of 0.6 and 1.3 percentage points, respectively. The
resulting F
1
-score of 75.2 lies even higher than the
top score for this particular data set reported in the
CoNLL shared task (Zhao et al., 2009; 74.6 F
1
).
We next investigate the benefits of compo-
sitional representations over features for single
words by assessing their impact on the overall re-
sult in an ablation study. Table 4 shows results
of ablation tests performed for the three composi-
tional feature types Interaction, Span and Path
on the out-of-domain test set. The results reveal
Out-of-domain P R F
1
Original 76.9 71.7 74.2
Full (Original+All) 77.5 73.0 75.2
Full ?Interaction 77.2 72.5 74.8
Full ?Span 77.2 72.3 74.7
Full ?Path 77.6 72.3 74.8
Table 4: Results of an ablation study over features
based on compositional representations. All num-
bers in percent.
a considerable loss in recall, indicating the impor-
tance of including compositional word represen-
tations and confirming our intuition that they can
provide additional gains over simple type-level
representations. In the next section, we discuss
this result in more detail and provide examples of
improved classification decisions.
6 Discussion
As a more detailed qualitative analysis, we exam-
ined the impact of word representations on SRL
performance with respect to different argument la-
bels and predicate types. Results on the in-domain
data set, shown in the upper part of Table 5, sug-
gest that most improvements in terms of preci-
sion are gained for verbal predicates, while nom-
inal predicates primarily benefit from higher re-
call. One reason for the latter observation might
be that arguments of nominal predicates are gen-
erally much harder to identify for the Original
model, as the cues provided by indicator features
on words and syntax are often inconclusive. For
verbal predicates, the word representations mainly
provide reinforcing signals to the classifier, im-
proving its precision at a slight cost of recall.
The results on the out-of-domain data set pro-
vide more insights regarding the suitability of
word representations for generalization. As shown
in the lower half of Table 5, the additional features
on average have a positive impact on precision and
recall. For verbal predicates, we observe only one
case, namely A0, in which improvements in recall
came with a decline in precision. Regarding nomi-
nal predicates, the trend is similar to what we have
seen in the in-domain setting, with most gains be-
ing achieved in terms of recall.
Apart from assessing quantitative effects, we
further examined cases that directly show the qual-
itative gains of the compositional features defined
410
Sentence with predicate and [gold argument
label
] Original Features required for correction
(1) He did not resent [their
A0
] supervision A1 Interaction
(2) [He
A1
] is getting plenty of rest no label Interaction, Path
(3) [He
A0
] rose late and went down to have breakfast. no label Path
(4) He was able to sit [for hours
AM-TMP
]. A2 Span
(5) Because he had spoken [too softly
AM-MNR
]. AM-TMP Span
Table 6: Example sentences in which distributional features compensated for errors made by Original.
In-domain verbal nominal
Label P R P R
A0 +0.4 +0.4 ?0.1 +2.4
A1 +0.2 ?0.4 +0.6 +1.5
A2 +1.7 ?1.5 ? +2.5
AM-ADV +0.8 +0.2 ?9.9 ?3.1
AM-DIS +0.3 ?3.2 ? ?
AM-LOC +0.8 +1.1 +0.6 +3.0
AM-MNR ?0.5 ?1.2 +2.7 +0.3
AM-TMP ?1.2 ?0.7 ?1.9 +3.3
Out-of-domain verbal nominal
Label P R P R
A0 ?0.9 +2.5 ?2.5 ?0.4
A1 +1.7 +0.8 +1.0 +3.7
A2 +1.4 +0.7 ?2.5 +3.2
AM-ADV +5.6 +0.7 ? ?
AM-DIS +7.3 ? ? ?
AM-LOC +0.7 +2.4 ? +15.0
AM-MNR +6.4 +10.5 +9.7 +10.7
AM-TMP +1.6 +1.8 ?6.7 +1.1
Table 5: Differences in precision and recall per
argument label and predicate word category. All
numbers represent absolute percentage points.
in Section 3. Table 6 lists examples from the
out-of-domain data set that were misclassified by
the Original model but could be correctly pre-
dicted using our enhanced feature set. As illus-
trated by Examples (1) and (2), the Interaction
feature seems to help recall by guiding classifica-
tion decisions towards more meaningful and com-
plete structures.
Improvements using the Path feature can be ob-
served in cases where nested syntactic structures
need to be processed, as required in Example (2).
In another instance, Example (3), the following
path is predicted between argument and predicate:
He
SBJ
?? rose
COORD
????and
CONJ
???went
OPRD
??? to
IM
??have.
Such cases are particularly problematic for the
Original model because long and potentially er-
roneous paths are sparse in the training data.
Further gains in performance are achieved using
the Span feature, which enables the model to bet-
ter handle infrequent and out-of-vocabulary words
occurring in an argument span, including ?hours?
and ?softly? in Example (4) and (5), respectively.
7 Conclusions
In this paper, we proposed to enhance the feature
space of a state-of-the-art semantic role labeller by
applying and composing distributional word rep-
resentations. Our results indicate that combining
such features with standard syntactic cues leads to
more precise and more robust models, with sig-
nificant improvements both in-domain and out-of-
domain. Ablation tests on an out-of-domain data
set have shown that gains in recall are mostly due
to features based on composed representations.
Given the novelty of these features for SRL, we
believe that this insight is remarkable and deserves
further investigation. In future work, we plan to
apply more sophisticated models of composition-
ality to better represent predicate-argument struc-
tures and to guide classification decisions towards
outcomes that are semantically more plausible.
We anticipate that this line of research will also be
of interest for a range of related tasks beyond tra-
ditional SRL, including predicate-argument struc-
ture alignment (Roth and Frank, 2012) and im-
plicit argument linking (Gerber and Chai, 2012).
Acknowledgements
This work has been supported by the FP7 Col-
laborative Project S-CASE (Grant Agreement No
610717) funded by the European Commission
(Michael Roth), and by EPSRC (EP/K017845/1)
in the framework of the CHIST-ERA READERS
project (Kristian Woodsend).
411
References
Omri Abend, Roi Reichart, and Ari Rappoport. 2009.
Unsupervised argument identification for semantic
role labeling. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 28?36, Sun-
tec, Singapore, August.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association
for Computational Linguistics.
Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 43?48. Association for Computational Lin-
guistics.
Anders Bj?orkelund, Bernd Bohnet, Love Hafdell, and
Pierre Nugues. 2010. A high-performance syn-
tactic and semantic dependency parser. In Coling
2010: Demonstration Volume, pages 33?36, Beijing,
China, August. Coling 2010 Organizing Committee.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 546?556, Jeju Island, Korea,
July. Association for Computational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Danilo Croce, Cristina Giannone, Paolo Annesi, and
Roberto Basili. 2010. Towards open-domain se-
mantic role labeling. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 237?246, Uppsala, Sweden, July.
Association for Computational Linguistics.
Koen Deschacht and Marie-Francine Moens. 2009.
Semi-supervised semantic role labeling using the
Latent Words Language Model. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 21?29, Singapore,
August.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
CCA. In Advances in Neural Information Process-
ing Systems, pages 199?207.
Hagen F?urstenau and Mirella Lapata. 2012. Semi-
supervised semantic role labeling via structural
alignment. Computational Linguistics, 38(1):135?
171.
Matthew Gerber and Joyce Chai. 2012. Semantic Role
Labeling of Implicit Arguments for Nominal Predi-
cates. Computational Linguistics, 38(4):755?798.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, et al. 2009. The conll-2009
shared task: Syntactic and semantic dependencies
in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame
identification with distributed word representations.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics, pages
1448?1458, Baltimore, Maryland, June. Association
for Computational Linguistics.
Fei Huang and Alexander Yates. 2010. Open-domain
semantic role labeling by modeling word spans. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 968?
978, Uppsala, Sweden, July.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882, Jeju Island,
Korea, July.
R?emi Lebret and Ronan Collobert. 2014. Word em-
beddings through hellinger pca. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, pages
482?490, Gothenburg, Sweden, April. Association
for Computational Linguistics.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The nombank project: An interim report. In
A. Meyers, editor, HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May.
412
Jeff Mitchell and Mirella Lapata. 2010. Composition
in Distributional Models of Semantics. Cognitive
Science, 34(8):1388?1429.
Andriy Mnih and Geoffrey Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
volume 21, pages 1081?1088.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Michael Roth and Anette Frank. 2012. Aligning
predicate argument structures in monolingual com-
parable texts: A new corpus for a new task. In
Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics, pages 218?227,
Montreal, Canada, June.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211, Jeju Island, Korea, July. Association for
Computational Linguistics.
Ivan Titov and Alexandre Klementiev. 2011. A
bayesian model for unsupervised semantic parsing.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 1445?1455, Port-
land, Oregon, USA, June.
Ivan Titov and Alexandre Klementiev. 2012. Semi-
supervised semantic role labeling: Approaching
from an unsupervised perspective. In Proceedings
of COLING 2012, pages 2635?2652, Mumbai, In-
dia, December.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 384?394, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Alexander Yeh. 2000. More accurate tests for
the statistical significance of result differences.
In Proceedings of the 18th International Confer-
ence on Computational Linguistics, pages 947?953,
Saarbr?ucken, Germany, August.
Be?nat Zapirain, Eneko Agirre, Llu??s M`arquez, and Mi-
hai Surdeanu. 2013. Selectional preferences for se-
mantic role classification. Computational Linguis-
tics, 39(3):631?663.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 61?66, Boulder,
Colorado, USA, June.
413
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565?574,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Automatic Generation of Story Highlights
Kristian Woodsend and Mirella Lapata
School of Informatics, University of Edinburgh
Edinburgh EH8 9AB, United Kingdom
k.woodsend@ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we present a joint con-
tent selection and compression model
for single-document summarization. The
model operates over a phrase-based rep-
resentation of the source document which
we obtain by merging information from
PCFG parse trees and dependency graphs.
Using an integer linear programming for-
mulation, the model learns to select and
combine phrases subject to length, cover-
age and grammar constraints. We evalu-
ate the approach on the task of generat-
ing ?story highlights??a small number of
brief, self-contained sentences that allow
readers to quickly gather information on
news stories. Experimental results show
that the model?s output is comparable to
human-written highlights in terms of both
grammaticality and content.
1 Introduction
Summarization is the process of condensing a
source text into a shorter version while preserving
its information content. Humans summarize on
a daily basis and effortlessly, but producing high
quality summaries automatically remains a chal-
lenge. The difficulty lies primarily in the nature
of the task which is complex, must satisfy many
constraints (e.g., summary length, informative-
ness, coherence, grammaticality) and ultimately
requires wide-coverage text understanding. Since
the latter is beyond the capabilities of current NLP
technology, most work today focuses on extractive
summarization, where a summary is created sim-
ply by identifying and subsequently concatenating
the most important sentences in a document.
Without a great deal of linguistic analysis, it
is possible to create summaries for a wide range
of documents. Unfortunately, extracts are of-
ten documents of low readability and text quality
and contain much redundant information. This is
in marked contrast with hand-written summaries
which often combine several pieces of informa-
tion from the original document (Jing, 2002) and
exhibit many rewrite operations such as substitu-
tions, insertions, deletions, or reorderings.
Sentence compression is often regarded as a
promising first step towards ameliorating some of
the problems associated with extractive summa-
rization. The task is commonly expressed as a
word deletion problem. It involves creating a short
grammatical summary of a single sentence, by re-
moving elements that are considered extraneous,
while retaining the most important information
(Knight and Marcu, 2002). Interfacing extractive
summarization with a sentence compression mod-
ule could improve the conciseness of the gener-
ated summaries and render them more informative
(Jing, 2000; Lin, 2003; Zajic et al, 2007).
Despite the bulk of work on sentence compres-
sion and summarization (see Clarke and Lapata
2008 and Mani 2001 for overviews) only a handful
of approaches attempt to do both in a joint model
(Daume? III and Marcu, 2002; Daume? III, 2006;
Lin, 2003; Martins and Smith, 2009). One rea-
son for this might be the performance of sentence
compression systems which falls short of attaining
grammaticality levels of human output. For ex-
ample, Clarke and Lapata (2008) evaluate a range
of state-of-the-art compression systems across dif-
ferent domains and show that machine generated
compressions are consistently perceived as worse
than the human gold standard. Another reason is
the summarization objective itself. If our goal is
to summarize news articles, then we may be bet-
ter off selecting the first n sentences of the docu-
ment. This ?lead? baseline may err on the side of
verbosity but at least will be grammatical, and it
has indeed proved extremely hard to outperform
by more sophisticated methods (Nenkova, 2005).
In this paper we propose a model for sum-
565
marization that incorporates compression into the
task. A key insight in our approach is to formulate
summarization as a phrase rather than sentence
extraction problem. Compression falls naturally
out of this formulation as only phrases deemed
important should appear in the summary. Ob-
viously, our output summaries must meet addi-
tional requirements such as sentence length, over-
all length, topic coverage and, importantly, gram-
maticality. We combine phrase and dependency
information into a single data structure, which al-
lows us to express grammaticality as constraints
across phrase dependencies. We encode these con-
straints through the use of integer linear program-
ming (ILP), a well-studied optimization frame-
work that is able to search the entire solution space
efficiently.
We apply our model to the task of generat-
ing highlights for a single document. Examples
of CNN news articles with human-authored high-
lights are shown in Table 1. Highlights give a
brief overview of the article to allow readers to
quickly gather information on stories, and usually
appear as bullet points. Importantly, they repre-
sent the gist of the entire document and thus of-
ten differ substantially from the first n sentences
in the article (Svore et al, 2007). They are also
highly compressed, written in a telegraphic style
and thus provide an excellent testbed for models
that generate compressed summaries. Experimen-
tal results show that our model?s output is compa-
rable to hand-written highlights both in terms of
grammaticality and informativeness.
2 Related work
Much effort in automatic summarization has been
devoted to sentence extraction which is often for-
malized as a classification task (Kupiec et al,
1995). Given appropriately annotated training
data, a binary classifier learns to predict for
each document sentence if it is worth extracting.
Surface-level features are typically used to sin-
gle out important sentences. These include the
presence of certain key phrases, the position of
a sentence in the original document, the sentence
length, the words in the title, the presence of
proper nouns, etc. (Mani, 2001; Sparck Jones,
1999).
Relatively little work has focused on extraction
methods for units smaller than sentences. Jing and
McKeown (2000) first extract sentences, then re-
move redundant phrases, and use (manual) recom-
bination rules to produce coherent output. Wan
and Paris (2008) segment sentences heuristically
into clauses before extraction takes place, and
show that this improves summarization quality.
In the context of multiple-document summariza-
tion, heuristics have also been used to remove par-
enthetical information (Conroy et al, 2004; Sid-
dharthan et al, 2004). Witten et al (1999) (among
others) extract keyphrases to capture the gist of the
document, without however attempting to recon-
struct sentences or generate summaries.
A few previous approaches have attempted to
interface sentence compression with summariza-
tion. A straightforward way to achieve this is by
adopting a two-stage architecture (e.g., Lin 2003)
where the sentences are first extracted and then
compressed or the other way round. Other work
implements a joint model where words and sen-
tences are deleted simultaneously from a docu-
ment. Using a noisy-channel model, Daume? III
and Marcu (2002) exploit the discourse structure
of a document and the syntactic structure of its
sentences in order to decide which constituents to
drop but also which discourse units are unimpor-
tant. Martins and Smith (2009) formulate a joint
sentence extraction and summarization model as
an ILP. The latter optimizes an objective func-
tion consisting of two parts: an extraction com-
ponent, essentially a non-greedy variant of max-
imal marginal relevance (McDonald, 2007), and
a sentence compression component, a more com-
pact reformulation of Clarke and Lapata (2008)
based on the output of a dependency parser. Com-
pression and extraction models are trained sepa-
rately in a max-margin framework and then inter-
polated. In the context of multi-document summa-
rization, Daume? III?s (2006) vine-growth model
creates summaries incrementally, either by start-
ing a new sentence or by growing already existing
ones.
Our own work is closest to Martins and Smith
(2009). We also develop an ILP-based compres-
sion and summarization model, however, several
key differences set our approach apart. Firstly,
content selection is performed at the phrase rather
than sentence level. Secondly, the combination of
phrase and dependency information into a single
data structure is new, and important in allowing
us to express grammaticality as constraints across
phrase dependencies, rather than resorting to a lan-
566
Most blacks say MLK?s vision fulfilled, poll finds
WASHINGTON (CNN) ? More than two-thirds of African-
Americans believe Martin Luther King Jr.?s vision for race
relations has been fulfilled, a CNN poll found ? a figure up
sharply from a survey in early 2008.
The CNN-Opinion Research Corp. survey was released
Monday, a federal holiday honoring the slain civil rights
leader and a day before Barack Obama is to be sworn in as
the first black U.S. president.
The poll found 69 percent of blacks said King?s vision has
been fulfilled in the more than 45 years since his 1963 ?I have
a dream? speech ? roughly double the 34 percent who agreed
with that assessment in a similar poll taken last March.
But whites remain less optimistic, the survey found.
? 69 percent of blacks polled say Martin Luther King Jr?s
vision realized.
? Slim majority of whites say King?s vision not fulfilled.
? King gave his ?I have a dream? speech in 1963.
9/11 billboard draws flak from Florida Democrats, GOP
(CNN) ? A Florida man is using billboards with an image of
the burning World Trade Center to encourage votes for a Re-
publican presidential candidate, drawing criticism for politi-
cizing the 9/11 attacks.
?Please Don?t Vote for a Democrat? reads the type over the
picture of the twin towers after hijacked airliners hit them on
September, 11, 2001.
Mike Meehan, a St. Cloud, Florida, businessman who paid to
post the billboards in the Orlando area, said former President
Clinton should have put a stop to Osama bin Laden and al
Qaeda before 9/11. He said a Republican president would
have done so.
? Billboards use image from 9/11 to encourage GOP votes.
? 9/11 image wrong for ad, say Florida political parties.
? Floridian praises President Bush, says ex-President Clin-
ton failed to stop al Qaeda.
Table 1: Two example CNN news articles, showing the title and the first few paragraphs, and below, the
original highlights that accompanied each story.
guage model. Lastly, our model is more com-
pact, has fewer parameters, and does not require
two training procedures. Our approach bears some
resemblance to headline generation (Dorr et al,
2003; Banko et al, 2000), although we output sev-
eral sentences rather than a single one. Head-
line generation models typically extract individual
words from a document to produce a very short
summary, whereas we extract phrases and ensure
that they are combined into grammatical sentences
through our ILP constraints.
Svore et al (2007) were the first to foreground
the highlight generation task which we adopt as an
evaluation testbed for our model. Their approach
is however a purely extractive one. Using an al-
gorithm based on neural networks and third-party
resources (e.g., news query logs and Wikipedia en-
tries) they rank sentences and select the three high-
est scoring ones as story highlights. In contrast,
we aim to generate rather than extract highlights.
As a first step we focus on deleting extraneous ma-
terial, but other more sophisticated rewrite opera-
tions (e.g., Cohn and Lapata 2009) could be incor-
porated into our framework.
3 The Task
Given a document, we aim to produce three or four
short sentences covering its main topics, much like
the ?Story Highlights? accompanying the (online)
CNN news articles. CNN highlights are written by
humans; we aim to do this automatically.
Documents Highlights
Sentences 37.2 ? 39.6 3.5 ? 0.5
Tokens 795.0 ? 744.8 47.0 ? 9.6
Tokens/sentence 22.4 ? 4.2 13.3 ? 1.7
Table 2: Overview statistics on the corpus of doc-
uments and highlights (mean and standard devia-
tion). A minority of documents are transcripts of
interviews and speeches, and can be very long; this
accounts for the very large standard deviation.
Two examples of a news story and its associ-
ated highlights, are shown in Table 1. As can be
seen, the highlights are written in a compressed,
almost telegraphic manner. Articles, auxiliaries
and forms of the verb be are often deleted. Com-
pression is also achieved through paraphrasing,
e.g., substitutions and reorderings. For example,
the document sentence ?The poll found 69 percent
of blacks said King?s vision has been fulfilled.? is
rephrased in the highlight as ?69 percent of blacks
polled say Martin Luther King Jr?s vision real-
ized.?. In general, there is a fair amount of lexi-
cal overlap between document sentences and high-
lights (42.44%) but the correspondence between
document sentences and highlights is not always
one-to-one. In the first example in Table 1, the sec-
ond paragraph gives rise to two highlights. Also
note that the highlights need not form a coherent
summary, each of them is relatively stand-alone,
and there is little co-referencing between them.
567
(a)
S
S
CC
But
NP
NNS
whites
VP
VBP
remain
ADJP
RBR
less
JJ
optimistic
,
,
NP
DT
the
NN
survey
VP
VBD
found
.
.
(b)
TOP
found
optimistic
whites
ns
ub
j
remain
co
p
less
advmod
cc
om
p
survey
the
de
t
nsubj
Figure 1: An example phrase structure (a) and dependency (b) tree for the sentence ?But whites remain
less optimistic, the survey found.?.
In order to train and evaluate the model pre-
sented in the following sections we created a cor-
pus of document-highlight pairs (approximately
9,000) which we downloaded from the CNN.com
website.1 The articles were randomly sampled
from the years 2007?2009 and covered a wide
range of topics such as business, crime, health,
politics, showbiz, etc. The majority were news
articles, but the set alo contained a mixture of
editorials, commentary, interviews and reviews.
Some overview statistics of the corpus are shown
in Table 2. Overall, we observe a high degree of
compression both at the document and sentence
level. The highlights summary tends to be ten
times shorter than the corresponding article. Fur-
thermore, individual highlights have almost half
the length of document sentences.
4 Modeling
The objective of our model is to create the most in-
formative story highlights possible, subject to con-
straints relating to sentence length, overall sum-
mary length, topic coverage, and grammaticality.
These constraints are global in their scope, and
cannot be adequately satisfied by optimizing each
one of them individually. Our approach therefore
uses an ILP formulation which will provide a glob-
ally optimal solution, and which can be efficiently
solved using standard optimization tools. Specif-
ically, the model selects phrases from which to
form the highlights, and each highlight is created
from a single sentence through phrase deletion.
The model operates on parse trees augmented with
1The corpus is available from http://homepages.inf.
ed.ac.uk/mlap/resources/index.html.
dependency labels. We first describe how we ob-
tain this representation and then move on to dis-
cuss the model in more detail.
Sentence Representation We obtain syntactic
information by parsing every sentence twice, once
with a phrase structure parser and once with a
dependency parser. The phrase structure and
dependency-based representations for the sen-
tence ?But whites remain less optimistic, the sur-
vey found.? (from Table 1) are shown in Fig-
ures 1(a) and 1(b), respectively.
We then combine the output from the two
parsers, by mapping the dependencies to the edges
of the phrase structure tree in a greedy fashion,
shown in Figure 2(a). Starting at the top node of
the dependency graph, we choose a node i and a
dependency arc to node j. We locate the corre-
sponding words i and j on the phrase structure
tree, and locate their nearest shared ancestor p. We
assign the label of the dependency i? j to the first
unlabeled edge from p to j in the phrase structure
tree. Edges assigned with dependency labels are
shown as dashed lines. These edges are important
to our formulation, as they will be represented by
binary decision variables in the ILP. Further edges
from p to j, and all the edges from p to i, are
marked as fixed and shown as solid lines. In this
way we keep the correct ordering of leaf nodes.
Finally, leaf nodes are merged into parent phrases,
until each phrase node contains a minimum of two
tokens, shown in Figure 2(b). Because of this min-
imum length rule, it is possible for a merged node
to be a clause rather than a phrase, but in the sub-
sequent description we will use the term phrase
rather loosely to describe any merged leaf node.
568
(a)
S
S
CC
But
NP
NNS
whites
ns
ub
j
VP
VBP
remain
co
p
ADJP
RBR
less
adv
mod
JJ
optimistic
ccomp
,
,
NP
DT
the
de
t
NN
survey
nsubj
VP
VBD
found
.
.
(b)
S
S
But whites remain
less optimistic
cc
om
p
,
,
NP
the survey
nsubj
VBD
found .
Figure 2: Dependencies are mapped onto phrase structure tree (a) and leaf nodes are merged with parent
phrases (b).
ILP model The merged phrase structure tree,
such as shown in Figure 2(b), is the actual input to
our model. Each phrase in the document is given
a salience score. We obtain these scores from the
output of a supervised machine learning algorithm
that predicts for each phrase whether it should be
included in the highlights or not (see Section 5 for
details). Let S be the set of sentences in a docu-
ment, P be the set of phrases, and Ps ? P be the
set of phrases in each sentence s ? S . T is the set
of words with the highest tf.idf scores, and Pt ? P
is the set of phrases containing the token t ? T .
Let fi denote the salience score for phrase i, deter-
mined by the machine learning algorithm, and li is
its length in tokens.
We use a vector of binary variables x ? {0,1}|P |
to indicate if each phrase is to be within a high-
light. These are either top-level nodes in our
merged tree representation, or nodes whose edge
to the parent has a dependency label (the dashed
lines). Referring to our example in Figure 2(b), bi-
nary variables would be allocated to the top-level S
node, the child S node and the NP node. The vec-
tor of auxiliary binary variables y ? {0,1}|S | in-
dicates from which sentences the chosen phrases
come (see Equations (1i) and (1j)). Let the sets
Di ? P , ?i ? P capture the phrase dependency in-
formation for each phrase i, where each set Di
contains the phrases that depend on the presence
of i. Our objective function function is given in
Equation (1a): it is the sum of the salience scores
of all the phrases chosen to form the highlights
of a given document, subject to the constraints
in Equations (1b)?(1j). The latter provide a nat-
ural way of describing the requirements the output
must meet.
max
x ?
i?P
fixi (1a)
s.t. ?
i?P
lixi ? LT (1b)
?
i?Ps
lixi ? LMys ?s ? S (1c)
?
i?Ps
lixi ? Lmys ?s ? S (1d)
?
i?Pt
xi ? 1 ?t ? T (1e)
x j? xi ?i ? P , j ?Di (1f)
xi? ys ?s ? S , i ? Ps (1g)
?
s?S
ys ? NS (1h)
xi ? {0,1} ?i ? P (1i)
ys ? {0,1} ?s ? S . (1j)
Constraint (1b) ensures that the generated high-
lights do not exceed a total budget of LT tokens.
This constraint may vary depending on the appli-
cation or task at hand. Highlights on a small screen
device would presumably be shorter than high-
lights for news articles on the web. It is also possi-
ble to set the length of each highlight to be within
the range [Lm,LM]. Constraints (1c) and (1d) en-
force this requirement. In particular, these con-
straints stop highlights formed from sentences at
the beginning of the document (which tend to have
569
high salience scores) from being too long. Equa-
tion (1e) is a set-covering constraint, requiring that
each of the words in T appears at least once in
the highlights. We assume that words with high
tf.idf scores reveal to a certain extent what the doc-
ument is about. Constraint (1e) ensures that some
of these words will be present in the highlights.
We enforce grammatical correctness through
constraint (1f) which ensures that the phrase de-
pendencies are respected. Phrases that depend on
phrase i are contained in the set Di. Variable xi is
true, and therefore phrase i will be included, if any
of its dependents x j ?Di are true. The phrase de-
pendency constraints, contained in the set Di and
enforced by (1f), are the result of two rules based
on the typed dependency information:
1. Any child node j of the current node i,
whose connecting edge i ? j is of type
nsubj (nominal subject), nsubjpass (passive
nominal subject), dobj (direct object), pobj
(preposition object), infmod (infinitival mod-
ifier), ccomp (clausal complement), xcomp
(open clausal complement), measure (mea-
sure phrase modifier) and num (numeric
modifier) must be included if node i is in-
cluded.
2. The parent node p of the current node i must
always be included if i is, unless the edge
p? i is of type ccomp (clausal complement)
or advcl (adverbial clause), in which case it
is possible to include i without including p.
Consider again the example in Figure 2(b).
There are only two possible outputs from this sen-
tence. If the phrase ?the survey? is chosen, then
the parent node ?found? will be included, and from
our first rule the ccomp phrase must also be in-
cluded, which results in the output: ?But whites
remain less optimistic, the survey found.? If, on
the other hand, the clause ?But whites remain less
optimistic? is chosen, then due to our second rule
there is no constraint that forces the parent phrase
?found? to be included in the highlights. Without
other factors influencing the decision, this would
give the output: ?But whites remain less opti-
mistic.? We can see from this example that encod-
ing the possible outputs as decisions on branches
of the phrase structure tree provides a more com-
pact representation of many options than would be
possible with an explicit enumeration of all possi-
ble compressions. Which output is chosen (if any)
depends on the scores of the phrases involved, and
the influence of the other constraints.
Constraint (1g) tells the ILP to create a highlight
if one of its constituent phrases is chosen. Finally,
note that a maximum number of highlights NS can
be set beforehand, and (1h) limits the highlights to
this maximum.
5 Experimental Set-up
Training We obtained phrase-based salience
scores using a supervised machine learning algo-
rithm. 210 document-highlight pairs were chosen
randomly from our corpus (see Section 3). Two
annotators manually aligned the highlights and
document sentences. Specifically, each sentence
in the document was assigned one of three align-
ment labels: must be in the summary (1), could be
in the summary (2), and is not in the summary (3).
The annotators were asked to label document sen-
tences whose content was identical to the high-
lights as ?must be in the summary?, sentences
with partially overlapping content as ?could be in
the summary? and the remainder as ?should not
be in the summary?. Inter-annotator agreement
was .82 (p < 0.01, using Spearman?s ? rank corre-
lation). The mapping of sentence labels to phrase
labels was unsupervised: if the phrase came from
a sentence labeled (1), and there was a unigram
overlap (excluding stop words) between the phrase
and any of the original highlights, we marked this
phrase with a positive label. All other phrases
were marked negative.
Our feature set comprised surface features such
as sentence and paragraph position information,
POS tags, unigram and bigram overlap with the
title, and whether high-scoring tf.idf words were
present in the phrase (66 features in total). The
210 documents produced a training set of 42,684
phrases (3,334 positive and 39,350 negative). We
learned the feature weights with a linear SVM,
using the software SVM-OOPS (Woodsend and
Gondzio, 2009). This tool gave us directly the fea-
ture weights as well as support vector values, and
it allowed different penalties to be applied to pos-
itive and negative misclassifications, enabling us
to compensate for the unbalanced data set. The
penalty hyper-parameters chosen were the ones
that gave the best F-scores, using 10-fold valida-
tion.
Highlight generation We generated highlights
for a test set of 600 documents. We created and
570
solved an ILP for each document. Sentences were
first tokenized to separate words and punctuation,
then parsed to obtain phrases and dependencies as
described in Section 4 using the Stanford parser
(Klein and Manning, 2003). For each phrase, fea-
tures were extracted and salience scores calcu-
lated from the feature weights determined through
SVM training. The distance from the SVM hyper-
plane represents the salience score. The ILP model
(see Equation (1)) was parametrized as follows:
the maximum number of highlights NS was 4,
the overall limit on length LT was 75 tokens, the
length of each highlight was in the range of [8,28]
tokens, and the topic coverage set T contained the
top 5 tf.idf words. These parameters were chosen
to capture the properties seen in the majority of
the training set; they were also relaxed enough to
allow a feasible solution of the ILP model (with
hard constraints) for all the documents in the test
set. To solve the ILP model we used the ZIB Opti-
mization Suite software (Achterberg, 2007; Koch,
2004; Wunderling, 1996). The solution was con-
verted into highlights by concatenating the chosen
leaf nodes in order. The ILP problems we created
had on average 290 binary variables and 380 con-
straints. The mean solve time was 0.03 seconds.
Summarization In order to examine the gen-
erality of our model and compare with previous
work, we also evaluated our system on a vanilla
summarization task. Specifically, we used the
same model (trained on the CNN corpus) to gen-
erate summaries for the DUC-2002 corpus2. We
report results on the entire dataset and on a subset
containing 140 documents. This is the same parti-
tion used by Martins and Smith (2009) to evaluate
their ILP model.3
Baselines We compared the output of our model
to two baselines. The first one simply selects
the ?leading? three sentences from each document
(without any compression). The second baseline
is the output of a sentence-based ILP model, sim-
ilar to our own, but simpler. The model is given
in (2). The binary decision variables x ? {0,1}|S |
now represent sentences, and fi the salience score
for each sentence. The objective again is to max-
imize the total score, but now subject only to
tf.idf coverage (2b) and a limit on the number of
2http://www-nlpir.nist.gov/projects/duc/
guidelines/2002.html
3We are grateful to Andre? Martins for providing us with
details of their testing partition.
highlights (2c) which we set to 3. There are no
sentence length or grammaticality constraints, as
there is no sentence compression.
max
x ?
i?S
fixi (2a)
s.t. ?
i?St
xi ? 1 ?t ? T (2b)
?
i?S
xi ? NS (2c)
xi ? {0,1} ?i ? S . (2d)
The SVM was trained with the same features used
to obtain phrase-based salience scores, but with
sentence-level labels (labels (1) and (2) positive,
(3) negative).
Evaluation We evaluated summarization qual-
ity using ROUGE (Lin and Hovy, 2003). For the
highlight generation task, the original CNN high-
lights were used as the reference. We report un-
igram overlap (ROUGE-1) as a means of assess-
ing informativeness and the longest common sub-
sequence (ROUGE-L) as a means of assessing flu-
ency.
In addition, we evaluated the generated high-
lights by eliciting human judgments. Participants
were presented with a news article and its corre-
sponding highlights and were asked to rate the lat-
ter along three dimensions: informativeness (do
the highlights represent the article?s main topics?),
grammaticality (are they fluent?), and verbosity
(are they overly wordy and repetitive?). The sub-
jects used a seven point rating scale. An ideal
system would receive high numbers for grammat-
icality and informativeness and a low number for
verbosity. We randomly selected nine documents
from the test set and generated highlights with our
model and the sentence-based ILP baseline. We
also included the original highlights as a gold stan-
dard. We thus obtained ratings for 27 (9 ? 3)
document-highlights pairs.4 The study was con-
ducted over the Internet using WebExp (Keller
et al, 2009) and was completed by 34 volunteers,
all self reported native English speakers.
With regard to the summarization task, follow-
ing Martins and Smith (2009), we used ROUGE-1
and ROUGE-2 to evaluate our system?s output.
We also report results with ROUGE-L. Each doc-
ument in the DUC-2002 dataset is paired with
4A Latin square design ensured that subjects did not see
two different highlights of the same document.
571
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
Recall Precision
Rouge-1
F-score Recall Precision
Rouge-L
F-score
S
co
re
Leading-3
ILP sentence
ILP phrase
Figure 3: ROUGE-1 and ROUGE-L results for
phrase-based ILP model and two baselines, with
error bars showing 95% confidence levels.
a human-authored summary (approximately 100
words) which we used as reference.
6 Results
We report results on the highlight generation task
in Figure 3 with ROUGE-1 and ROUGE-L (error
bars indicate the 95% confidence interval). In
both measures, the ILP sentence baseline has the
best recall, while the ILP phrase model has the
best precision (the differences are statistically sig-
nificant). F-score is higher for the phrase-based
system but not significantly. This can be at-
tributed to the fact that the longer output of the
sentence-based model makes the recall task easier.
Average highlight lengths are shown in Table 3,
and the compression rates they represent. Our
phrase model achieves the highest compression
rates, whereas the sentence-based model tends to
select long sentences even in comparison to the
lead baseline. The sentence ILP model outper-
forms the lead baseline with respect to recall but
not precision or F-score. The phrase ILP achieves
a significantly better F-score over the lead baseline
with both ROUGE-1 and ROUGE-L.
The results of our human evaluation study are
summarized in Table 4. There was no sta-
tistically significant difference in the grammat-
icality between the highlights generated by the
phrase ILP system and the original CNN high-
lights (means differences were compared using a
Post-hoc Tukey test). The grammaticality of the
sentence ILP was significantly higher overall as
no compression took place (? < 0.05). All three
s toks/s C.R.
Articles 36.5 22.2 ? 4.0 100%
CNN highlights 3.5 13.3 ? 1.7 5.8%
ILP phrase 3.8 18.0 ? 2.9 8.4%
Leading-3 3.0 25.1 ? 7.4 9.3%
ILP sentence 3.0 31.3 ? 7.9 11.6%
Table 3: Comparison of output lengths: number
of sentences, tokens per sentence, and compres-
sion rate, for CNN articles, their highlights, the
ILP phrase model, and two baselines.
Model Grammar Importance Verbosity
CNN highlights 4.85 4.88 3.14
ILP sentence 6.41 5.47 3.97
ILP phrase 5.53 5.05 3.38
Table 4: Average human ratings for original CNN
highlights, and two ILP models.
systems performed on a similar level with respect
to importance (differences in the means were not
significant). The highlights created by the sen-
tence ILP were considered significantly more ver-
bose (? < 0.05) than those created by the phrase-
based system and the CNN abstractors. Overall,
the highlights generated by the phrase ILP model
were not significantly different from those written
by humans. They capture the same content as the
full sentences, albeit in a more succinct manner.
Table 5 shows the output of the phrase-based sys-
tem for the documents in Table 1.
Our results on the complete DUC-2002 cor-
pus are shown in Table 6. Despite the fact that
our model has not been optimized for the original
task of generating 100-word summaries?instead
it is trained on the CNN corpus, and generates
highlights?the results are comparable with the
best of the original participants5 in each of the
ROUGE measures. Our model is also significantly
better than the lead sentences baseline.
Table 7 presents our results on the same
DUC-2002 partition (140 documents) used by
Martins and Smith (2009). The phrase ILP model
achieves a significantly better F-score (for both
ROUGE-1 and ROUGE-2) over the lead baseline,
the sentence ILP model, and Martins and Smith.
We should point out that the latter model is not a
straw man. It significantly outperforms a pipeline
5The list of participants is on page 12 of the slides
available from http://duc.nist.gov/pubs/2002slides/
overview.02.pdf.
572
? More than two-thirds of African-Americans believe
Martin Luther King Jr.?s vision for race relations has
been fulfilled.
? 69 percent of blacks said King?s vision has been ful-
filled in the more than 45 years since his 1963 ?I have a
dream? speech.
? But whites remain less optimistic, the survey found.
? A Florida man is using billboards with an image of the
burning World Trade Center to encourage votes for a
Republican presidential candidate, drawing criticism.
? ?Please Don?t Vote for a Democrat? reads the type over
the picture of the twin towers.
? Mike Meehan said former President Clinton should
have put a stop to Osama bin Laden and al Qaeda be-
fore 9/11.
Table 5: Generated highlights for the stories in Ta-
ble 1 using the phrase ILP model.
Participant ROUGE-1 ROUGE-2 ROUGE-L
28 0.464 0.222 0.432
19 0.459 0.221 0.431
21 0.458 0.216 0.426
29 0.449 0.208 0.419
27 0.445 0.209 0.417
Leading-3 0.416 0.200 0.390
ILP phrase 0.454 0.213 0.428
Table 6: ROUGE results on the complete
DUC-2002 corpus, including the top 5 original
participants. For all results, the 95% confidence
interval is ?0.008.
approach that first creates extracts and then com-
presses them. Furthermore, as a standalone sen-
tence compression system it yields state of the art
performance, comparable to McDonald?s (2006)
discriminative model and superior to Hedge Trim-
mer (Zajic et al, 2007), a less sophisticated deter-
ministic system.
7 Conclusions
In this paper we proposed a joint content selection
and compression model for single-document sum-
marization. A key aspect of our approach is the
representation of content by phrases rather than
entire sentences. Salient phrases are selected to
form the summary. Grammaticality, length and
coverage requirements are encoded as constraints
in an integer linear program. Applying the model
to the generation of ?story highlights? (and sin-
gle document summaries) shows that it is a vi-
able alternative to extraction-based systems. Both
ROUGE scores and the results of our human study
ROUGE-1 ROUGE-2 ROUGE-L
Leading-3 .400 ? .018 .184 ? .015 .374 ? .017
M&S (2009) .403 ? .076 .180 ? .076 ?
ILP sentence .430 ? .014 .191 ? .015 .401 ? .014
ILP phrase .445 ? .014 .200 ? .014 .419 ? .014
Table 7: ROUGE results on DUC-2002 cor-
pus (140 documents). ?: only ROUGE-1 and
ROUGE-2 results are given in Martins and Smith
(2009).
confirm that our system manages to create sum-
maries at a high compression rate and yet maintain
the informativeness and grammaticality of a com-
petitive extractive system. The model itself is rel-
atively simple and knowledge-lean, and achieves
good performance without reference to any re-
sources outside the corpus collection.
Future extensions are many and varied. An ob-
vious next step is to examine how the model gen-
eralizes to other domains and text genres. Al-
though coherence is not so much of an issue for
highlights, it certainly plays a role when generat-
ing standard summaries. The ILP model can be
straightforwardly augmented with discourse con-
straints similar to those proposed in Clarke and
Lapata (2007). We would also like to generalize
the model to arbitrary rewrite operations, as our
results indicate that compression rates are likely
to improve with more sophisticated paraphrasing.
Acknowledgments
We would like to thank Andreas Grothey and
members of ICCS at the School of Informatics for
the valuable discussions and comments through-
out this work. We acknowledge the support of EP-
SRC through project grants EP/F055765/1 and
GR/T04540/01.
References
Achterberg, Tobias. 2007. Constraint Integer Programming.
Ph.D. thesis, Technische Universita?t Berlin.
Banko, Michele, Vibhu O. Mittal, and Michael J. Witbrock.
2000. Headline generation based on statistical translation.
In Proceedings of the 38th ACL. Hong Kong, pages 318?
325.
Clarke, James and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings of
EMNLP-CoNLL. Prague, Czech Republic, pages 1?11.
Clarke, James and Mirella Lapata. 2008. Global inference
for sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Research
31:399?429.
Cohn, Trevor and Mirella Lapata. 2009. Sentence compres-
sion as tree transduction. Journal of Artificial Intelligence
Research 34:637?674.
573
Conroy, J. M., J. D. Schlesinger, J. Goldstein, and D. P.
O?Leary. 2004. Left-brain/right-brain multi-document
summarization. In DUC 2004 Conference Proceedings.
Daume? III, Hal. 2006. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D. thesis,
University of Southern California.
Daume? III, Hal and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of the
40th ACL. Philadelphia, PA, pages 449?456.
Dorr, Bonnie, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to headline
generation. In Proceedings of the HLT-NAACL 2003
Workshop on Text Summarization. pages 1?8.
Jing, Hongyan. 2000. Sentence reduction for automatic text
summarization. In Proceedings of the 6th ANLP. Seattle,
WA, pages 310?315.
Jing, Hongyan. 2002. Using hidden Markov modeling to de-
compose human-written summaries. Computational Lin-
guistics 28(4):527?544.
Jing, Hongyan and Kathleen McKeown. 2000. Cut and paste
summarization. In Proceedings of the 1st NAACL. Seattle,
WA, pages 178?185.
Keller, Frank, Subahshini Gunasekharan, Neil Mayo, and
Martin Corley. 2009. Timing accuracy of web experi-
ments: A case study using the WebExp software package.
Behavior Research Methods 41(1):1?12.
Klein, Dan and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st ACL. Sap-
poro, Japan, pages 423?430.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach to sen-
tence compression. Artificial Intelligence 139(1):91?107.
Koch, Thorsten. 2004. Rapid Mathematical Prototyping.
Ph.D. thesis, Technische Universita?t Berlin.
Kupiec, Julian, Jan O. Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proceedings of SIGIR-
95. Seattle, WA, pages 68?73.
Lin, Chin-Yew. 2003. Improving summarization performance
by sentence compression ? a pilot study. In Proceed-
ings of the 6th International Workshop on Information Re-
trieval with Asian Languages. Sapporo, Japan, pages 1?8.
Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT NAACL. Edmonton, Canada, pages
71?78.
Mani, Inderjeet. 2001. Automatic Summarization. John Ben-
jamins Pub Co.
Martins, Andre? and Noah A. Smith. 2009. Summarization
with a joint model for sentence extraction and compres-
sion. In Proceedings of the Workshop on Integer Linear
Programming for Natural Language Processing. Boulder,
Colorado, pages 1?9.
McDonald, Ryan. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy.
McDonald, Ryan. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceedings
of the 29th ECIR. Rome, Italy.
Nenkova, Ani. 2005. Automatic text summarization of
newswire: Lessons learned from the Document Under-
standing Conference. In Proceedings of the 20th AAAI.
Pittsburgh, PA, pages 1436?1441.
Siddharthan, Advaith, Ani Nenkova, and Kathleen McKe-
own. 2004. Syntactic simplification for improving con-
tent selection in multi-document summarization. In Pro-
ceedings of the 20th International Conference on Compu-
tational Linguistics (COLING 2004). pages 896?902.
Sparck Jones, Karen. 1999. Automatic summarizing: Factors
and directions. In Inderjeet Mani and Mark T. Maybury,
editors, Advances in Automatic Text Summarization, MIT
Press, Cambridge, pages 1?33.
Svore, Krysta, Lucy Vanderwende, and Christopher Burges.
2007. Enhancing single-document summarization by
combining RankNet and third-party sources. In Proceed-
ings of EMNLP-CoNLL. Prague, Czech Republic, pages
448?457.
Wan, Stephen and Ce?cile Paris. 2008. Experimenting with
clause segmentation for text summarization. In Proceed-
ings of the 1st TAC. Gaithersburg, MD.
Witten, Ian H., Gordon Paynter, Eibe Frank, Carl Gutwin, and
Craig G. Nevill-Manning. 1999. KEA: Practical automatic
keyphrase extraction. In Proceedings of the 4th ACM
International Conference on Digital Libraries. Berkeley,
CA, pages 254?255.
Woodsend, Kristian and Jacek Gondzio. 2009. Exploiting
separability in large-scale linear support vector machine
training. Computational Optimization and Applications .
Wunderling, Roland. 1996. Paralleler und objektorientierter
Simplex-Algorithmus. Ph.D. thesis, Technische Univer-
sita?t Berlin.
Zajic, David, Bonnie J. Door, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks.
Information Processing Management Special Issue on
Summarization 43(6):1549?1570.
574
