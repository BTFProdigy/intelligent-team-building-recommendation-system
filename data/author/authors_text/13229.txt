Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 82?93, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Dialogue Decisions Using Information Density for
Interactive Systems
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
School of Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh, Scotland
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Incremental processing allows system design-
ers to address several discourse phenomena
that have previously been somewhat neglected
in interactive systems, such as backchannels
or barge-ins, but that can enhance the re-
sponsiveness and naturalness of systems. Un-
fortunately, prior work has focused largely
on deterministic incremental decision mak-
ing, rendering system behaviour less flexible
and adaptive than is desirable. We present a
novel approach to incremental decision mak-
ing that is based on Hierarchical Reinforce-
ment Learning to achieve an interactive op-
timisation of Information Presentation (IP)
strategies, allowing the system to generate
and comprehend backchannels and barge-ins,
by employing the recent psycholinguistic hy-
pothesis of information density (ID) (Jaeger,
2010). Results in terms of average rewards
and a human rating study show that our learnt
strategy outperforms several baselines that are
not sensitive to ID by more than 23%.
1 Introduction
Recent work on incremental systems has shown
that adapting a system?s turn-taking behaviour to be
more human-like can improve the user?s experience
significantly, based on incremental models of auto-
matic speech recognition (ASR) (Baumann et al
2011), dialogue management (Buss et al2010), and
speech generation (Skantze and Hjalmarsson, 2010).
All of these approaches are based on the same gen-
eral abstract architecture of incremental processing
(Schlangen and Skantze, 2011). While this archi-
tecture offers inherently incremental mechanisms to
update and revise input hypotheses, it is affected
by a number of drawbacks, shared by determinis-
tic models of decision making in general: they rely
on hand-crafted rules which can be time-consuming
and expensive to produce, they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, they are unable to gener-
alise and adapt flexibly to unseen situations, and
they do not use automatic optimisation. Statisti-
cal approaches to incremental processing that ad-
dress some of these problems have been suggested
by Raux and Eskenazi (2009), who use a cost matrix
and decision theoretic principles to optimise turn-
taking in a dialogue system under the constraint that
users prefer no gaps and no overlap at turn bound-
aries. Also, DeVault et al2009) use maximum en-
tropy classification to support responsive overlap in
an incremental system by predicting the completions
of user utterances. Selfridge et al2011) use logis-
tic regression models to predict the stability and ac-
curacy of incremental speech recognition results to
enhance performance without causing delay. For re-
lated work on (deterministic) incremental language
generation, please see (Kilger and Finkler, 1995;
Purver and Otsuka, 2003).
Recent years have seen a number of data-driven
approaches to interactive systems that automatically
adapt their decisions to the dialogue context us-
ing Reinforcement Learning (Levin et al2000;
Walker, 2000; Young, 2000; Singh et al2002;
Pietquin and Dutoit, 2006; Henderson et al2008;
Cuaya?huitl et al2010; Thomson, 2009; Young et
al., 2010; Lemon, 2011; Janarthanam and Lemon,
2010; Rieser et al2010; Cuaya?huitl and Dethlefs,
82
2011; Dethlefs and Cuaya?huitl, 2011). While these
approaches have been shown to enhance the perfor-
mance and adaptivity of interactive systems, unfor-
tunately none of them has yet been combined with
incremental processing.
In this paper, we present a novel approach to in-
cremental decision making for output planning that
is based on Hierarchical Reinforcement Learning
(HRL). In particular, we address the problem of op-
timising IP strategies while allowing the system to
generate and comprehend backchannels and barge-
ins based on a partially data-driven reward func-
tion. Generating backchannels can be beneficial for
grounding in interaction. Similarly, barge-ins can
lead to more efficient interactions, e.g. when a sys-
tem can clarify a bad recognition result immediately
before acting based on a misrecognition.
A central concept to our approach is Information
Density (ID) (Jaeger, 2010), a psycholinguistic hy-
pothesis that human utterance production is sensitive
to a uniform distribution of information across the
utterance. This hypothesis has also been adopted for
low level output planning recently, see e.g. Rajku-
mar and White (2011). Our results in terms of av-
erage rewards and a human rating study show that a
learning agent that is sensitive to ID can learn when
it is most beneficial to generate feedback to a user,
and outperforms several other agents that are not
sensitive to ID.
2 Incremental Information Presentation
2.1 Information Presentation Strategies
Our example domain of application is the Infor-
mation Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al2010). This previous work
incrementally constructs IP strategies according to
the predicted user reaction, whereas our approach
focuses on whether and when to generate backchan-
nels and barge-ins and how to react to user barge-
ins in the context of dynamically changing input hy-
potheses. We therefore implement a simplified ver-
sion of Rieser et al model. Their system distin-
guished two steps: the selection of an IP strategy
and the selection of attributes to present to the user.
We assume here that the choice of attributes is deter-
mined by matching the types specified in the user in-
put, so that our system only needs to choose a strat-
egy for presenting its results. Attributes include cui-
sine, food quality, location, price range and service
quality of a restaurant. The system then performs a
database lookup and chooses among three main IP
strategies summary, comparison, recommendation
and several ordered combinations of these. Please
see Rieser et al2010) for details. Table 1 shows
examples of the main types of IP strategies that we
generate.
2.2 Backchannels and Barge-ins
An important advantage of incremental processing
can be the increased reactiveness of systems. In this
paper, we focus on the phenomena of backchannels
and barge-ins that can act as feedback in an interac-
tion for both user and system. Figure 1 shows some
examples. Backchannels can often be interpreted as
signals of grounding. Coming from the user, the sys-
tem may infer that the user is following the presenta-
tion of information or is confirming a piece of infor-
mation without trying to take the turn. Similarly, we
can allow a system to generate backchannels to the
user to confirm that it understands the user?s prefer-
ences, i.e. receives high confidence scores from the
ASR module. An important decision for a dialogue
system is then when to generate a backchannel?
Barge-ins typically occur in different situations.
The user may barge-in on the system to correct an
ASR error (such as ?Italian? instead of ?Indian? in
Figure 1) or the system may want to barge-in on the
user to confirm a low-confidence ASR hypothesis so
as to be able to start an immediate database look up
for results. In the former case, the user barging-in
on the system, we assume that the system has two
choices: yielding the turn to the user, or trying to
keep the turn. In the latter case, the system barging-
in on the user, the system would have to decide if and
when it would be beneficial to barge-in on a user ut-
terance. In the following sections, we will develop
a model of dialogue optimisation that can address
these question based on Hierarchical RL that opti-
mises system behaviour based on trade-offs defined
in terms of ID.
83
Type Example
Comparison The restaurant Roma is in the medium price range, but does not serve excellent food.
The restaurants Firenze and Verona both have great food but are more expensive. The
restaurant Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expen-
sive, but has great food and service.
Summary I found 24 Italian restaurants in the city centre that match your query. 11 of them are
in the medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
Backchannel 1 (the system backchannels)
USR I want Italian food [500 ms] in the city centre. . .
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range, but does not
have great food. The restaurants Firenze and Verona . . .
Backchannel 2 (the user backchannels)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 central Italian restaurants . . .
USR OK.
SYS The restaurant Verona has great food but is also a bit
expensive. The Roma is cheaper, but not as central as Verona . . .
Barge-ins 1 (the user barges-in on system)
USR I want Italian food in the centre of town . . .
SYS I found 35 Indian . . .
USR Not Indian, I want Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Barge-ins 2 (the system barges-in on user)
USR I need an Italian restaurant that is located . . .
SYS I?m sorry. Did you say
Indian or Italian?
USR I said Italian. And in the centre of town please.
SYS OK, let me see. I have 24 Italian restaurants . . .
Figure 1: Example phenomena generated with the learnt
policy. The agent has learnt to produce backchannels
and barge-ins at the appropriate moment and alternative
strategies to deal with user barge-ins.
3 Information Theory
Information Theory as introduced by Shannon
(1948) is based on two main concepts: a communi-
cation channel through which information is trans-
ferred in bits and the information gain, i.e. the in-
formation load that each bit carries. For natural lan-
guage, the assumption is that people aim to com-
municate according to the channel?s capacity, which
corresponds to the hearer?s capacity in terms of cog-
nitive load. If they go beyond that, the cognitive load
of the listener gets too high. If they stay (far) below,
too little information is transferred per bit (i.e., the
utterance is inefficient or uninformative). The in-
formation gain of each word, which is indicative of
how close we are to the channel?s capacity, can be
computed using entropy measures.
3.1 Information Density
Psycholinguistic research has presented evidence for
users distributing information across utterances uni-
formly, so that each word is carrying roughly the
same amount of information. This has been ob-
served for phonetic phenomena based on words
(Bell et al2003) and syllables (Aylett and Turk,
2004), and for syntactic phenomena (Levy and
Jaeger, 2007; Jaeger, 2010). Relating ID to likeli-
hood, we can say that the less frequent a word is, the
more information it is likely to carry (Jaeger, 2010).
For example the word ?the? often has a high corpus
frequency but a low ID.
The ID is defined as the log-probability of an
event (i.e. a word) (Shannon, 1948; Levy and Jaeger,
2007), so that for an utterance u consisting of the
word sequence w1 . . . wi?1, we can compute the ID
at each point during the utterance as:
log 1P (u) =
n
?
i=1
log 1P (wi|w1 . . . wi?1)
(1)
While typically the context of a word is given by
all preceding words of the utterance, we follow Gen-
zel and Charniak (2002) in restricting our computa-
tion to tri-grams for computability reasons. Given a
84
language model of the domain, we can therefore op-
timise ID in system-generated discourse, where we
treat ID as ?an optimal solution to the problem of
rapid yet error-free communication in a noisy envi-
ronment? (Levy and Jaeger (2007), p.2). We will
now transfer the notion of ID to IP and investigate
the distribution of information over user restaurant
queries.
3.2 Information Density in User Utterances
We aim to use ID for incremental IP in two ways:
(1) to estimate the best moment for generating
backchannels or barge-ins to the user, and (2) to de-
cide whether to yield or keep the current system turn
in case of a user barge-in. While we do not have spe-
cific data on human barge-in behaviour, we know
from the work of (Jaeger, 2010), e.g., that ID influ-
ences human language production. We therefore hy-
pothesise a relationship between ID and incremen-
tal phenomena. A human-human data collection is
planned for the near future.
To compute the ID of user and system utterances
at each time step, we estimated an n?gram lan-
guage model (using Kneser-Ney smoothing) based
on a transcribed corpus of human subjects interact-
ing with a system for restaurant recommendations of
Rieser et al2011).1 The corpus contained user ut-
terances as exemplified in Figure 1 and allowed us to
compute the ID at any point during a user utterance.2
In this way, we can estimate points of low density
which may be eligible for a barge-in or a backchan-
nel. Figure 2 shows some example utterances drawn
from the corpus and their ID including the first sen-
tence from Figure 1. These examples were typical
for what could generally be observed from the cor-
pus. We see that while information is transmitted
with varying amounts of density, the main bits of in-
formation are transmitted at a scale between 2 and
7.
Due to a lack of human data for the system utter-
ances, we use the same corpus data to compute the
ID of system utterances.3 The learning agent can use
1Available at http://www.macs.hw.ac.uk/
ilabarchive/classicproject/data/login.php.
2Note that our model does not currently handle out-of-
domain words. In future work, we will learn when to seek clar-
ification.
3We plan a data collection of such utterances for the future,
1 2 3 4 5 6 7 8 9 10 11 12
0
2
4
6
8
10
12
14
16
18
20
In
fo
rm
at
io
n 
De
ns
ity
Time 
 
 
I want Italian food in the city centre.
Yes, I need a moderately priced restaurant in the New Chesterton area.
I need the address of a Thai restaurant.
Figure 2: Information Density for example utterances,
where peaks indicate places of high density.
this information to consider the trade-off of yielding
a current turn to the user or trying to keep it, e.g., in
case of a user barge-in given the ID of its own turn
and of the user?s incoming turn. Such decisions will
be made incrementally in our domain given dynam-
ically changing hypotheses of user input.
4 Incremental Utterance Optimisation
To optimise incremental decision making for an in-
teractive system given the optimisation measure of
ID, we formalise the dialogue module as a Hierar-
chical Reinforcement Learning agent and learn an
optimal action policy by mapping states to actions
and optimising a long-term reward signal. The di-
alogue states can be seen as representing the sys-
tem?s knowledge about the task, the user and the
environment. The dialogue actions correspond to
the system?s capabilities, such as present the re-
sults or barge-in on the user. They also handle in-
cremental updates in the system. In addition, we
need a transition function that specifies the way
that actions change the environment (as expressed
in the state representation) and a reward function
which specifies a numeric value for each action
taken. In this way, decision making can be seen
as a finite sequence of states, actions and rewards
{s0, a0, r1, s1, a1, ..., rt?1, st}, where the goal is to
induce an optimal strategy automatically using Rein-
forcement Learning (RL) (Sutton and Barto, 1998).
We used Hierarchical RL, rather than flat RL, be-
cause the latter is affected by the curse of dimen-
sionality, the fact that the state space grows expo-
nentially according to the state variables taken into
account. This affects the scalability of flat RL agents
but for now make the assumption that using the corpus data is
informative since they are from the same domain.
85
and limits their application to small-scale problems.
Since timing is crucial for incremental approaches,
where processing needs to be fast, we choose a hi-
erarchical setting for better scalability. We denote
the hierarchy of RL agents as M ij where the in-
dexes i and j only identify an agent in a unique
way, they do not specify the execution sequence of
subtasks, which is subject to optimisation. Each
agent of the hierarchy is defined as a Semi-Markov
Decision Process (SMDP) consisting of a 4-tuple
< Sij , Aij , T ij , Rij >. Here, Sij denotes the set of
states, Aij denotes the set of actions, and T ij is a
probabilistic state transition function that determines
the next state s? from the current state s and the per-
formed action a. Rij(s?, ? |s, a) is a reward function
that specifies the reward that an agent receives for
taking an action a in state s lasting ? time steps
(Dietterich, 1999). Since actions in SMDPs may
take a variable number of time steps to complete,
the variable ? represents this number of time steps.
The organisation of the learning process into dis-
crete time steps allows us to define incremental hy-
pothesis updates as state updates and transitions in
an SMDP. Whenever conditions in the learning en-
vironment change, such as the recogniser?s best hy-
pothesis of the user input, we represent them as tran-
sitions from one state to another. At each time step,
the agent checks for changes in its state represen-
tation and takes the currently best action according
to the new state. The best action in an incremental
framework can also include generating a backchan-
nel to the user to indicate the status of grounding
or barging-in to confirm an uncertain piece of infor-
mation. Once information has been presented to the
user, it is committed or realised. Realised informa-
tion is represented in the agent?s state, so that it can
monitor its own output.
Actions in a Hierarchical Reinforcement learner
can be either primitive or composite. The former
are single-step actions that yield single rewards, and
the latter are multi-step actions that correspond to
SMDPs and yield cumulative rewards. Decision
making occurs at any time step of an SMDP: after
each single-step action, we check for any updates
of the environment that require a system reaction or
change of strategy. If no system action is required
(e.g. because the user is speaking), the system can
decide to do nothing. The goal of each SMDP is to
find an optimal policy pi? that maximises the reward
for each visited state, according to
pi?ij(s) = argmaxa?A Q
?i
j(s, a), (2)
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing pi?. We use HSMQ-Learning to induce dia-
logue policies, see (Cuaya?huitl, 2009), p. 92.
5 Experimental Setting
5.1 Hierarchy of Learning Agents
The HRL agent in Figure 3 shows how the tasks of
(1) dealing with incrementally changing input hy-
potheses, (2) choosing a suitable IP strategy and (3)
presenting information, are connected. Note that
we focus on a detailed description of models M10...3
here, which deal with barge-ins and backchannels
and are the core of this paper. Please see Dethlefs et
al. (2012) for details of an RL model that deals with
the remaining decisions.
Briefly, model M00 deals with dynamic input hy-
potheses. It chooses when to listen to an incoming
user utterance (M13 ) and when and how to present
information (M10...2) by calling and passing control
to a child subtask. The variable ?incrementalStatus?
characterises situations in which a particular (incre-
mental) action is triggered, such as a floor holder ?let
me see?, a correction or self-correction. The variable
?presStrategy? indicates whether a strategy for IP has
been chosen or not, and the variable ?userReaction?
shows the user?s reaction to an IP episode. The
?userSilence? variable indicates whether the user is
speaking or not. The detailed state and action space
of the agents is given in Figure 4. We distinguish ac-
tions for Information Presentation (IP), actions for
attribute presentation and ordering (Slot-ordering),
and incremental actions (Incremental).
Models M10...2 correspond to different ways of
presenting information to the user. They perform
attribute selection and ordering and then call the
child agents M20...4 for attribute realisation. When-
ever a user barges in over the system, these agents
will decide to either yield the turn to the user or to
try and keep the turn based on information density.
The variables representing the status of the cuisine,
86
Root
Summary Comparison Recommendation
Observe
 User
Present
Cuisine
Present
 Food
Present
Location
Present
  Price
Present
Service
Figure 3: Hierarchy of learning agent for incremental In-
formation Presentation and Slot Ordering.
food, location, price and service of restaurants indi-
cate whether the slot is of interest to the user (we as-
sume that 0 means that the user does not care about
this slot), and what input confidence score is cur-
rently associated with the value of the slot. For ex-
ample, if our current best hypothesis is that the user
is interested in Indian restaurants, the variable ?sta-
tusCuisine? will have a value between 1-3 indicating
the strength of this hypothesis. Once slots have been
presented to the user, they are realised and can only
be changed through a correction or self-correction.
Model M13 is called whenever the user is speak-
ing. The system?s main choice here is to remain
silent and listen to the user or barge-in to request
the desired cuisine, location, or price range of a
restaurant. This can be beneficial in certain situa-
tions, such as when the system is able to increase its
confidence for a slot from ?low? to ?high? through
barging-in with a direct clarification request, e.g.
?Did you say Indian?? (and thereby saving sev-
eral turns that may be based on a wrong hypoth-
esis). This can also be harmful in certain situa-
tions, though, assuming that users have a general
preference for not being barged-in on. The learning
agent will need to learn to distinguish these situa-
tions. This agent is also responsible for generating
backchannels and will over time learn the best mo-
ments to do this.
Models M20...4 choose surface forms for presenta-
tion to the user from hand-crafted templates. They
are not the focus of this paper, however, and there-
fore not presented in detail. The state-action space
size of this agent is roughly 1.5 million.4 The agent
4Note that a flat RL agent, in contrast, would need 8? 1025
million state-actions to represent this problem.
States M00
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
observeUser {0=unfilled,1=filled}
presStrategy {0=unfilled,1=filled}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions M00
IP: compare M11 , recommend M12 , summarise M10 , sum-
mariseCompare, summariseRecommend, summariseCompar-
eRecommend,
Incremental: correct, selfCorrect, holdFloor, observeUser
Goal State M00 0, 1, 1, 0, ?
States M10...2
IDSystem={0=low,1=medium, 2=high}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusQuality {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
turnType {0=holding, 1=resuming, 2=keeping, 3=yielding}
userBargeIn {0=false, 1=true}
Actions M10...2
Slot-ordering: presentCuisine M20 , presentQuality M21 ,
presentLocation M22 , presentPrice M23 , presentService M24 ,
Incremental: yieldTurn, keepTurn
Goal State M10...2 ?, ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, ?, ?
States M13
bargeInOnUser={0=undecided,1=yes, 2=no}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
Actions M13
Incremental: doNotBargeIn, bargeInCuisine, bargeInLocation,
bargeInPrice, backchannel
Goal State M13 >0, ?, 0 ? 4, 0 ? 4, 0 ? 4
States M20...4
IDSystem={0=low,1=medium, 2=high}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
surfaceForm {0=unrealised,1=realised}
Actions M20...4
Surface Realisation: [alternative surface realisations]
e.g. ?$number$ restaurants serve $cuisine$ food?, ?$number$
places are located in $area$, etc.
Goal State M20...4 ?, ?, 1
Figure 4: The state and action space of the HRL agent.
The goal state is reached when all items (that the user
specified in the search query) have been presented. Ques-
tion marks mean that a variable does not affect the goal
state, which can be reached regardless of the variable?s
value.
reaches its goal state (defined w.r.t. the state vari-
87
ables in Fig. 4) when an IP strategy has been chosen
and all information has been presented.
5.2 The Simulated Environment
For a policy to converge, a learning agent typically
needs several thousand interactions in which it is ex-
posed to a multitude of different circumstances. For
our domain, we designed a simulated environment
with three main components addressing IP, incre-
mental input hypotheses and ID. Using this simula-
tion, we trained the agent for 10 thousand episodes,
where one episode corresponds to one recommenda-
tion dialogue.
5.2.1 Information Presentation
To learn a good IP strategy, we use a user simula-
tion5 by Rieser et al2010) which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user reac-
tion at time t to the system?s IP strategy IPs,t in state
s at time t. We distinguish the user reactions of se-
lect a restaurant, addMoreInfo to the current query to
constrain the search, and other. The last category is
usually considered an undesirable user reaction that
the system should learn to avoid. The simulation
uses linear smoothing to account for unseen situa-
tions. In this way, we can predict the most likely
user reaction to each system action. Even though
previous work has shown that n-gram-based simu-
lations can lead to dialogue inconsistencies, we as-
sume that for the present study this does not present
a problem, since we focus on generating single utter-
ances and on obtaining user judgements for single,
independent utterances.
5.2.2 Input Hypothesis Updates
While the IP strategies can be used for incremen-
tal and non-incremental dialogue, the second part of
the simulation deals explicitly with the dynamic en-
vironment updates that the system will need to be
sensitive to in an incremental setting. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores for each value are
5The simulation data are available from www.
classic-project.org.
shown in Table 2. A score of 0 means that the user
does not care about the attribute, 1 means that the
system?s confidence in the attribute?s value is low, 2
that the confidence is medium, and 3 means that the
confidence is high. A value of 4 means that the at-
tribute has already been realised, i.e. communicated
to the user. At the beginning of a learning episode,
we assign each attribute a possible value and con-
fidence score with equal probability. For food and
service quality, we assume that the user is never in-
terested in bad food or service. Subsequently, con-
fidence scores can change at each time step. In fu-
ture work these transition probabilities will be esti-
mated from a data collection, though the following
assumptions are realistic based on our experience.
We assume that a confidence score of 0 changes to
any other value with a likelihood of 0.05. A confi-
dence score of 1 changes with a probability of 0.3,
a confidence score of 2 with a probability of 0.1
and a confidence score of 3 with a probability of
0.03. Once slots have been realised, their value is
set to 4. They cannot be changed then without an ex-
plicit correction. We also assume that realised slots
change with a probability of 0.1. If they change,
we assume that half of the time, the user is the ori-
gin of the change (because they changed their mind)
and half of the time the system is the origin of the
change (because of an ASR or interpretation error).
Each time a confidence score is changed, it has a
probability of 0.5 for also changing its value. The
resulting input to the system are data structures of
the form present(cuisine=Indian), confidence=low.
The probability of observing this data structure in
our simulation is 0.1 (for Indian) ? 0.2 (for low
confidence) = 0.02. Its probability of changing
to present(cuisine=italian), confidence=high is 0.1
(for changing from low to medium) ? 0.05 (for
changing from Indian to Italian) = 0.005.
5.2.3 Information Density Updates
We simulate ID of user utterances based on proba-
bilistic context-free grammars (PCFG) that were au-
tomatically induced from the corpus data in Section
3.2 using the ABL algorithm (van Zaanen, 2000).
This algorithm takes a set of strings as input and
computes a context-free grammar as output by align-
ing strings based on Minimum Edit Distance. We
use the n?gram language models trained earlier to
88
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Quality bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, good-price-for-value,
expensive, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
add probabilities to grammar rules. We use these
PCFGs to simulate user utterances to which the sys-
tem has to react. They can be meaningful utter-
ances such as ?Show me restaurants nearby? or less
meaningful fragments such as ?um let me see, do
you. . . hm?. The former type is more frequent in
the data, but both types can be simulated along with
their ID (clearly, the first type is more dense than the
second).
In addition to simulating user utterances, we
hand-crafted context-free grammars of system ut-
terances and augmented them with probabilities es-
timated using the same user corpus data as above
(where again, we make the assumption that this is
to some extent feasible given the shared domain).
We use the simulated system utterances to compute
varying degrees of ID for the system.
Both measures, the ID of user and system utter-
ances, can inform the system during learning to bal-
ance the trade-off between them for generating and
receiving backchannels and barge-ins.
5.3 A Reward Function for Incremental
Dialogue Based on Information Density
To train the HRL agent, we use a partially data-
driven reward function. For incremental IP, we use
rewards that are based on human intuition. The
agent receives
R =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+100 if the user selects an item,
0 if the user adds further con-
straints to the search,
-100 if the user does something else
or a self-correction,
-0.5 for the system holding a turn,
-1 otherwise.
The agent is encouraged to choose those sequences
of actions that lead to the user selecting a restaurant
as quickly as possible. If the agent is not sure what to
say (because planning has not finished), it can gen-
erate a floor holding marker, but should in any case
avoid a self-correction due to having started speak-
ing too early.
The remaining rewards are based on ID scores
computed incrementally during an interaction. The
agent receives the following rewards, where info-
Density(Usr) and infoDensity(Sys) refer to the ID of
the current user and system utterance, respectively,
as defined in Equation 1.
R =
?
?
?
?
?
?
?
-infoDensity(Usr) for keeping a turn,
barging-in or
a backchannel,
-infoDensity(Sys) for yielding a turn.
These two measures encourage the agent to consider
the trade-offs between its own ID and the one trans-
mitted by an incoming user utterance. Barging-in
on a user utterance at a low ID point then yields a
small negative reward, whereas barging-in on a user
utterance at a high ID point yields a high negative
reward. Both rewards are negative because barging-
in on the user always contains some risk. Similarly,
keeping a turn over a non-dense user utterance re-
ceives a smaller negative reward than keeping it over
a dense user utterance. A reward of ?2 is assigned
for barging-in over a user utterance fragment with a
falling ID to reflect results from a qualitative study
of our corpus data: humans tend to barge-in between
information peaks, so that a barge-in to clarify a low-
confidence slot appears immediately before the ID is
rising again for a new slot. The exact best moment
for barge-ins and backchannels to occur will be sub-
ject to optimisation.
89
6 Experimental Results
The agent learns to barge-in or generate backchan-
nels to users at points where the ID is low but rising.
In particular, the agent learns to barge-in right before
information density peaks in an incoming user utter-
ance to clarify or request slots that are still open from
the previous information density peak. If a user has
specified their desired cuisine type but the system
has received a low ASR confidence score for it, it
may barge-in to clarify the slot. This case was illus-
trated in the last example in Figure 1, where the sys-
tem clarified the previous (cuisine) slot (which is as-
sociated with a high ID) just before the user specifies
the location slot (which again would have a high ID).
The main benefit the system can gain through clar-
ification barge-ins is to avoid self-corrections when
having acted based on a low ASR confidence, lead-
ing to more efficient interactions.
The system learns to generate backchannels after
information peaks to confirm newly acquired slots
that have a high confidence. An example is shown
in the first dialogue fragment in Figure 1.
In addition, the system learns to yield its current
turn to a user that is barging-in if its own ID is low,
falling or rising, or if the ID of the incoming user
utterance is high. If the system?s own ID is high, but
the user?s is not, it will try to keep the turn.6 This is
exemplified in the third dialogue fragment in Figure
1.
We compare our learnt policy against two base-
lines. Baseline 1 was designed to always generate
barge-ins after an information peak in a user utter-
ance, i.e. when ID has just switched from high to
falling. We chose this baseline to confirm that users
indeed prefer barge-ins before information peaks
rather than at any point of low ID. Baseline 1 yields
a turn to a user barge-in if its own ID is low and tries
to keep it otherwise. Baseline 2 generates barge-ins
and backchannels randomly and at any point during
a user utterance. The decision of yielding or keeping
a turn in case of a user barge-in is also random. Both
baselines also use HRL to optimise their IP strategy.
We do not compare different IP strategies, which has
been done in detail by Rieser et al2010). All re-
6Incidentally, this also helps to prevent the system yielding
its turn to a user backchannel; cf. Example 2 in Fig. 1.
101 102 103 104
?120
?100
?80
?60
?40
?20
0
20
40
60
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Learnt
Baseline1
Baseline2
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the HRL agent and its baselines.
sults are summarised in Table 3.
6.1 Average Rewards over Time
Figure 5 shows the performance of all systems in
terms of average rewards in simulation. The learnt
policy outperforms both baselines. While the learnt
policy and Baseline 1 appear to achieve similar per-
formance, an absolute comparison of the last 1000
episodes of each behaviour shows that the improve-
ment of the HRL agent over Baseline 1 corresponds
to 23.42%. The difference between the learnt policy
and its baselines is significant at p < 0.0001 accord-
ing to a paired t-test and has a high effect size of
r = 0.85.
The main reason for these different performances
is the moment each system will barge-in. Since
Baseline 1 barges-in on users after an information
peak, when ID may still be high, it continuously re-
ceives a negative reward reflecting the user prefer-
ence for late barge-ins. As a result of this contin-
uous negative reward, the agent will then learn to
avoid barge-ins altogether, which may in turn lead
to less efficient interactions because low confidence
ASR scores are clarified only late in the interaction.
The main problem of the random barge-ins of
Baseline 2 is that users may often have to restart
a turn because the system barged-in too early or
in the middle of an information peak. In addition,
Baseline 2 needs to occasionally self-correct its own
utterances because it started to present information
too early, when input hypotheses were not yet stable
enough to act upon them.
90
Policy Average Reward User Rating (%)
Learnt 55.54??,? 43%??
Baseline 1 45.0?? 26%
Baseline 2 1.47 31%
Table 3: Comparison of policies in terms of average re-
wards and user ratings. ? indicates a significant improve-
ment over Baseline 1 and ?? over Baseline 2.
6.2 Human Rating Study
To confirm our simulation-based results, we con-
ducted a user rating study on the CrowdFlower
crowd sourcing platform.7 Participants were
shown user utterances along with three options of
barging-in over them. For example: | I want
[OPTION 1] Italian food [OPTION 2] in the
city [OPTION 3] centre|, where OPTION 1 cor-
responds to the learnt policy, OPTION 2 to Baseline
2 and OPTION 3 to Baseline 1.
Users were asked to choose one option which they
considered the best moment for a barge-in. Partici-
pants in the study rated altogether 144 utterances.
They preferred the learnt system 63 times (43%),
Baseline 1 37 times (26%) and Baseline 2 44 times
(31%). This is statistically significant at p < 0.02
according to a Chi-Square test (?2 = 7.542, df =
2). In a separate test, directly comparing the learnt
policy and Baseline 1, learnt was chosen signifi-
cantly more often than Baseline 1; i.e. 79% of the
time (for 127 utterances, using a 1-tailed Sign test,
p < 0.0001). Finally, learnt was directly compared
to Baseline 2 and shown to be significantly more of-
ten chosen; i.e. 59% of the time (138 utterances, 1-
tailed Sign test, p < 0.025). These results provide
evidence that an optimisation of the timing of gener-
ating barge-ins and backchannels in incremental di-
alogue can be sensitive to fine-grained cues in evolv-
ing ID and therefore achieve a high level of adaptiv-
ity. Such sensitivity is difficult to hand-craft as can
be concluded w.r.t. the performance of Baseline 1,
which received similar rewards to learnt in simula-
tion, but is surprisingly beaten by the random Base-
line 2 here. This indicates a strong human dislike
for late barge-ins. The bad performance of Base-
line 2 in terms of average rewards was due to the
random barge-ins leading to less efficient dialogues.
7www.crowdflower.com
Regarding user ratings however, Baseline 2 was pre-
ferred over Baseline 1. This is most likely due to the
timing of barge-ins: since Baseline 2 has a chance
of barging-in at earlier occasions than Baseline 1,
it may have received better ratings. The evaluation
shows that humans care about timing of a barge-in
regarding the density of information that is currently
conveyed and dislike late barge-ins. ID is then useful
in determining when to barge-in. We can therefore
further conclude that ID can be a feasible optimisa-
tion criterion for incremental decision making.
7 Conclusion and Future Work
We have presented a novel approach to incremen-
tal dialogue decision making based on Hierarchical
RL combined with the notion of information den-
sity. We presented a learning agent in the domain of
IP for restaurant recommendations that was able to
generate backchannels and barge-ins for higher re-
sponsiveness in interaction. Results in terms of av-
erage rewards and a human rating study have shown
that a learning agent that is optimised based on a
partially data-driven reward function that addresses
information density can learn to decide when and if
it is beneficial to barge-in or backchannel on user
utterances and to deal with backchannels and barge-
ins from the user. Future work can take several di-
rections. Given that ID is a measure influencing
human language production, we could replace our
template-based surface realiser by an agent that op-
timises the information density of its output. Cur-
rently we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situa-
tions could be achieved if the agent was able to learn
from ongoing interactions. Finally, we can confirm
the human results obtained from an overhearer-style
evaluation in a real interactive setting and explicitly
extend our language model to discourse phenomena
such as pauses or hesitations to take them into ac-
count in measuring ID.
Acknowledgements
The research leading to this work has received funding
from EC?s FP7 programmes: (FP7/2011-14) under grant
agreement no. 287615 (PARLANCE); (FP7/2007-13) un-
der grant agreement no. 216594 (CLASSiC); (FP7/2011-
14) under grant agreement no. 270019 (SPACEBOOK);
91
and (FP7/2011-16) under grant agreement no. 269427
(STAC). Many thanks to Michael White for discussion
of the original idea of using information density as an op-
timisation metric.
References
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for
the relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31?56.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Alan Bell, Dan Jurafsky, Eric Fossler-Lussier, Cynthia
Girand, Michelle Gregory, and Daniel Gildea. 2003.
Effects of disfluencies, predictability, and utterance
position on word form variation in english conver-
sation. Journal of the Acoustic Society of America,
113(2):1001?1024.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proceedings of 11th An-
nual SIGdial Meeting on Discourse and Dialogue.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011.
Spatially-aware Dialogue Control Using Hierarchi-
cal Reinforcement Learning. ACM Transactions on
Speech and Language Processing (Special Issue on
Machine Learning for Robust and Adaptive Spoken
Dialogue System), 7(3).
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395?429.
Heriberto Cuaya?huitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD Thesis,
University of Edinburgh, School of Informatics.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the International
Conference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Pro-
ceedings of the 10th Annual SigDial Meeting on Dis-
course and Dialogue, Queen Mary University, UK.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227?303.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
Rate Constancy in Text. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 199?206.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid Reinforcement/Supervised Learning of
Dialogue Policies from Fixed Data Sets. Computa-
tional Linguistics, 34(4):487?511.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology, 61:23?62.
Srini Janarthanam and Oliver Lemon. 2010. Learning
to Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Oliver Lemon. 2011. Learning What to Say and How to
Say It: Joint Optimization of Spoken Dialogue Man-
agement and Natural Language Generation.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A Stochastic Model of Computer-Human Inter-
action for Learning Dialogue Strategies. IEEE Trans-
actions on Speech and Audio Processing, 8:11?23.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
Advances in Neural Information Processing Systems,
19.
Olivier Pietquin and Dutoit. 2006. A Probabilis-
tic Framework for Dialogue Simulation and Optimal
Strategy Learning. IEEE Transactions on Speech and
Audio Processing, 14(2):589?599.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Rajakrishnan Rajkumar and Michael White. 2011. Lin-
guistically Motivated Complementizer Choice in Sur-
face Realization. In Proceedings of the EMNLP-11
Workshop on Using Corpora in NLG, Edinburgh, Scot-
land.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proceedings of the 10th Conference of the
92
North American Chapter of the Association for Com-
putational Linguistics?Human Language Technolo-
gies (NAACL-HLT), Boulder, Colorado.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
David Schlangen and Gabriel Skantze. 2011. A General,
Abstract Model of Incremental Dialogue Processing.
Dialogue and Discourse, 2(1).
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in In-
cremental Speech Recognition. In Proceedings of the
12th Annual SigDial Meeting on Discourse and Dia-
logue, Portland, Oregon.
Claude Shannon. 1948. A Mathematical Theory of
Communications. Bell Systems Technical Journal,
27(4):623?656.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105?133.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proceedings of the 11th Annual SigDial Meeting on
Discourse and Dialogue, Tokyo, Japan.
Richard Sutton and Andrew Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA.
Blaise Thomson. 2009. Statistical Methods for Spo-
ken Dialogue Management. Ph.D. thesis, University
of Cambridge.
Menno van Zaanen. 2000. Bootstrapping Syntax and
Recursion using Alignment-Based Learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning, ICML ?00, pages 1063?1070.
Marilyn Walker. 2000. An Application of Reinforcement
Learning to Dialogue Strategy Selection in a Spoken
Dialogue System for Email. Journal of Artificial In-
telligence Research (JAIR), 12:387?416.
Steve Young, Milica Gasic, Simon Keizer, Francois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The Hidden Information State Model: A
Practical Framework for POMDP-based Spoken Dia-
logue Management. Computer Speech and Language,
24(2):150?174.
Steve Young. 2000. Probabilistic Methods in Spoken
Dialogue Systems. Philosophical Transactions of the
Royal Society (Series A), 358(1769):1389?1402.
93
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702?711,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Cluster-based Prediction of User Ratings for Stylistic Surface Realisation
Nina Dethlefs, Heriberto Cuaya?huitl, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot-Watt University, Mathematical and Computer Sciences, Edinburgh
n.s.dethlefs@hw.ac.uk
Abstract
Surface realisations typically depend on
their target style and audience. A challenge
in estimating a stylistic realiser from data is
that humans vary significantly in their sub-
jective perceptions of linguistic forms and
styles, leading to almost no correlation be-
tween ratings of the same utterance. We ad-
dress this problem in two steps. First, we
estimate a mapping function between the
linguistic features of a corpus of utterances
and their human style ratings. Users are
partitioned into clusters based on the sim-
ilarity of their ratings, so that ratings for
new utterances can be estimated, even for
new, unknown users. In a second step, the
estimated model is used to re-rank the out-
puts of a number of surface realisers to pro-
duce stylistically adaptive output. Results
confirm that the generated styles are recog-
nisable to human judges and that predictive
models based on clusters of users lead to
better rating predictions than models based
on an average population of users.
1 Introduction
Stylistic surface realisation aims not only to find
the best realisation candidate for a semantic input
based on some underlying trained model, but also
aims to adapt its output to properties of the user,
such as their age, social group, or location, among
others. One of the first systems to address stylis-
tic variation in generation was Hovy (1988)?s
PAULINE, which generated texts that reflect dif-
ferent speaker attitudes towards events based on
multiple, adjustable features. Stylistic variation
in such contexts can often be modelled systemat-
ically as a multidimensional variation space with
several continuous dimensions, so that varying
stylistic scores indicate the strength of each di-
mension in a realisation candidate. Here, we fo-
cus on the dimensions of colloquialism, politeness
and naturalness. Assuming a target score on one
or more dimensions, candidate outputs of a data-
driven realiser can then be ranked according to
their predicted affinity with the target scores.
In this paper, we aim for an approach to stylis-
tic surface realisation which is on the one hand
based on natural human data so as to reflect stylis-
tic variation that is as natural as possible. On the
other hand, we aim to minimise the amount of
annotation and human engineering that informs
the design of the system. To this end, we esti-
mate a mapping function between automatically
identifiable shallow linguistic features character-
istic of an utterance and its human-assigned style
ratings. In addition, we aim to address the high
degree of variability that is often encountered in
subjective rating studies, such as assessments of
recommender systems (O?Mahony et al., 2006;
Amatriain et al., 2009), sentiment analysis (Pang
and Lee, 2005), or surface realisations, where user
ratings have been shown to differ significantly
(p<0.001) for the same utterance (Walker et al.,
2007). Such high variability can affect the per-
formance of systems which are trained from an
average population of user ratings. However, we
are not aware of any work that has addressed this
problem principally by estimating ratings for both
known users, for whom ratings exists, and un-
known users, for whom no prior ratings exist. To
achieve this, we propose to partition users into
clusters of individuals who assign similar ratings
to linguistically similar utterances, so that their
ratings can be estimated more accurately than
702
based on an average population of users. This is
similar to Janarthanam and Lemon (2014), who
show that clustering users and adapting to their
level of domain expertise can significantly im-
prove task success and user ratings. Our resulting
model is evaluated with realisers not originally
built to deal with stylistic variation, and produces
natural variation recognisable by humans.
2 Architecture and Domain
We aim to with generating restaurant recommen-
dations as part of an interactive system. To do
this, we assume that a generator input is provided
by a preceding module, e.g. the interaction man-
ager, and that the task of the surface realiser is
to find a suitable stylistically appropriate realisa-
tion. An example input is inform(food=Italian,
name=Roma), which could be expressed as The
restaurant Roma serves Italian food. A further
aspect is that users are initially unknown to the
system, but that it should adapt to them over time
by discovering their stylistic preferences. Fu-
ture work involves integrating the surface realiser
into the PARLANCE1 (Hastie et al., 2013) spo-
ken dialogue system with a method for triggering
the different styles. Here, we leave the question
of when different styles are appropriate as future
work and focus on being able to generate them.
The architecture of our model is shown in Fig-
ure 1. Training of the regression model from sty-
listically-rated human corpora is shown in the top-
left box (grey). Utterance ratings from human
judges are used to extract shallow linguistic fea-
tures as well as to estimate user clusters. Both
types of information inform the resulting stylis-
tic regression model. For surface realisation (top-
right box, blue), a semantic input from a preced-
ing model is given as input to a surface realiser.
Any realiser is suitable that returns a ranked list of
output candidates. The resulting list is re-ranked
according to stylistic scores estimated by the re-
gressor, so that the utterance which most closely
reflects the target score is ranked highest. The re-
ranking process is shown in the lower box (red).
3 Related Work
3.1 Stylistic Variation in Surface Realisation
Our approach is most closely related to work by
Paiva and Evans (2005) and Mairesse and Walker
1http://parlance-project.eu
User Clusters
Regressor Surface Realisation
Ranking + 
Evaluation
Figure 1: Architecture of stylistic realisation model.
Top left: user clusters are estimated from corpus ut-
terances described by linguistic features and ratings.
Top right: surface realisation ranks a list of output can-
didates based on a semantic input. These are ranked
stylistically given a trained regressor.
(2011), discussed in turn here. Paiva and Evans
(2005) present an approach that uses multivari-
ate linear regression to map individual linguistic
features to distinguishable styles of text. The ap-
proach works in three steps. First, a factor anal-
ysis is used to determine the relevant stylistic di-
mensions from a corpus of human text using shal-
low linguistic features. Second, a hand-crafted
generator is used to produce a large set of ut-
terances, keeping traces of each generator deci-
sion, and obtaining style scores for each output
based on the estimated factor model. The result
is a dataset of <generator decision, style score>
pairs which can be used in a correlation analy-
sis to identify the predictors of particular output
styles. During generation, the correlation equa-
tions inform the generator at each choice point so
as to best express the desired style. Unfortunately,
no human evaluation of the model is presented so
that it remains unclear to what extent the gener-
ated styles are perceivable by humans.
Closely related is work by Mairesse and Walker
(2011) who present the PERSONAGE system,
which aims to generate language reflecting par-
ticular personalities. Instead of choosing genera-
tor decisions by considering their predicted style
scores, however, Mairesse and Walker (2011) di-
rectly predict generator decisions based on tar-
get personality scores. To obtain the generator,
the authors first generate a corpus of utterances
which differ randomly in their linguistic choices.
All utterances are rated by humans indicating the
703
extent to which they reflect different personality
traits. The best predictive model is then chosen in
a comparison of several classifiers and regressors.
Mairesse and Walker (2011) are the first to evalu-
ate their generator with humans and show that the
generated personalities are indeed recognisable.
Approaches on replicating personalities in re-
alisations include Gill and Oberlander (2002) and
Isard et al. (2006). Porayska-Pomsta and Mellish
(2004) and Gupta et al. (2007) are approaches to
politeness in generation, based on the notion of
face and politeness theory, respectively.
3.2 User Preferences in Surface Realisation
Taking users? individual content preferences into
account for training generation systems can
positively affect their performance (Jordan and
Walker, 2005; Dale and Viethen, 2009). We are
interested in individual user perceptions concern-
ing the surface realisation of system output and
the way they relate to different stylistic dimen-
sions. Walker et al. (2007) were the first to show
that individual preferences exist for the perceived
quality of realisations and that these can be mod-
elled in trainable generation. They train two ver-
sions of a rank-and-boost generator, a first version
of which is trained on the average population of
user ratings, whereas a second one is trained on
the ratings of individual users. The authors show
statistically that ratings from different users are
drawn from different distributions (p<0.001) and
that significantly better performance is achieved
when training and testing on data of individual
users. In fact, training a model on one user?s rat-
ings and testing it on another?s performs as badly
as a random baseline. However, no previous work
has modelled the individual preferences of unseen
users?for whom no training data exists.
4 Estimation of Style Prediction Models
4.1 Corpora and Style Dimensions
Our domain of interest is the automatic generation
of restaurant recommendations that differ with re-
spect to their colloquialism and politeness and are
as natural as possible. All three stylistic dimen-
sion were identified from a qualitative analysis of
human domain data. To estimate the strength of
each of them in a single utterance, we collect user
ratings for three data sets that were collected un-
der different conditions and are freely available.
Corpus Colloquial Natural Polite
LIST 3.38 ? 1.5 4.06 ? 1.2 4.35 ? 0.8
MAI 3.95 ? 1.2 4.32 ? 1.0 4.27 ? 0.8
CLASSIC 4.29 ? 1.1 4.20 ? 1.2 3.64 ? 1.3
Table 1: Average ratings with standard deviations.
Ratings between datasets (except one) differ signifi-
cantly at p<0.01, using the Wilcoxon signed-rank test.
? LIST is a corpus of restaurant recommenda-
tions from the website The List.2 It consists
of professionally written reviews. An exam-
ple is ?Located in the heart of Barnwell, Bel-
uga is an excellent restaurant with a smart
menu of modern Italian cuisine.?
? MAI is a dataset collected by Mairesse et
al. (2010),3 using Amazon Mechanical Turk.
Turkers typed in recommendations for vari-
ous specified semantics; e.g. ?I recommend
the restaurant Beluga near the cathedral.?
? CLASSIC is a dataset of transcribed spoken
user utterances from the CLASSiC project.4
The utterances consist of user queries for
restaurants, such as ?I need an Italian
restaurant with a moderate price range.?
Our joint dataset consists of 1, 361 human ut-
terances, 450 from the LIST, 334 from MAI,
and 577 from CLASSIC. We asked users on the
CrowdFlower crowdsourcing platform5 to read
utterances and rate their colloquialism, politeness
and naturalness on a 1-5 scale (the higher the bet-
ter). The following questions were asked.
? Colloquialism: The utterance is colloquial,
i.e. could have been spoken.
? Politeness: The utterance is polite / friendly.
? Naturalness: The utterance is natural, i.e.
could have been produced by a human.
The question on naturalness can be seen as a gen-
eral quality check for our training set. We do
not aim to generate unnatural utterances. 167
users took part in our rating study leading to a
rated dataset of altogether 3, 849 utterances. All
users were from the USA. The average ratings per
dataset and stylistic dimension are summarised
in Table 1. From this, we can see that LIST ut-
terances were perceived as the least natural and
2http://www.list.co.uk/
3http://people.csail.mit.edu/francois/
research/bagel/
4http://www.classic-project.org/
5http://crowdflower.com/
704
colloquial, but as the most polite. CLASSIC ut-
terances were perceived as the most colloquial,
but the least polite, and MAI utterances were rated
as the most natural. Differences between ratings
for each dimension and dataset are significant at
p<0.01, using the Wilcoxon signed-rank test, ex-
cept the naturalness for MAI and CLASSIC.
Since we are mainly interested in the lexical
and syntactic features of utterances here, the fact
that CLASSIC utterances are spoken, whereas the
other two corpora are written, should not affect
the quality of the resulting model. Similarly, some
stylistic categories may seem closely related, such
as colloquialism and naturalness, or orthogonal
to each other, such as politeness and colloqui-
alism. However, while ratings for colloquialism
and naturalness are very close for the CLASSIC
dataset, they vary significantly for the two other
datasets (p<0.01). Also, the ratings for colloqui-
alim and politeness show a weak positive corre-
lation of 0.23, i.e. are not perceived as orthogo-
nal by users. These results suggest that all in all
our three stylistic categories are perceived as suf-
ficiently different from each other and suitable for
training to predict a spectrum of different styles.
Another interesting aspect is that individual
user ratings vary significantly, leading to a high
degree of variability for identical utterances. This
will be the focus of the following sections.
4.2 Feature Estimation
Table 2 shows the feature set we will use in our
regression experiments. We started from a larger
subset including 45 lexical and syntactic features
as well as unigrams and bigrams, all of which
could be identified from the corpus without man-
ual annotation. The only analysis tool we used
was the Stanford Parser,6 which identified certain
types of words (pronouns, wh-words) or the depth
of syntactic embedding. A step-wise regression
analysis was then carried out to identify those
features that contributed significantly (at p<0.01)
to the overall regression equation obtained per
stylistic dimension. Of all lexical features (uni-
grams and bigrams), the word with was the only
contributor. A related feature was the average tf-
idf score of the content words in an utterance.
6http://nlp.stanford.edu/software/
lex-parser.shtml
Feature Type
Length of utterance num
Presence of personal pronouns bool
Presence of WH words bool
with cue word bool
Presence of negation bool
Average length of content words num
Ave tf-idf score of content words num
Depth of syntactic embedding num
Table 2: Features used for regression, which were
identified as significant contributors (p<0.01) from a
larger feature set in a step-wise regression analysis.
4.3 Regression Experiments
Based on the features identified in Section 4.2, we
train a separate regressor for each stylistic dimen-
sion. The task of the regressor is to predict, based
on the extracted linguistic features of an utterance,
a score in the range of 1-5 for colloquialism, po-
liteness and naturalness. We compare: (1) a mul-
tivariate multiple regressor (MMR), (2) an M5P
decision tree regressor, (3) a support vector ma-
chine (SVM) with linear kernel, and (4) a ZeroR
classifier, which serves as a majority baseline. We
used the R statistics toolkit7 for the MMR and the
Weka toolkit8 for the remaining models.
Average User Ratings The regressors were first
trained to predict the average user ratings of an ut-
terance and evaluated in a 10-fold cross validation
experiment. Table 3 shows the results. Here, r
denotes the Pearson correlation coefficient, which
indicates the correlation between the predicted
and the actual user scores; R2 is the coefficient of
determination, which provides a measure of how
well the learnt model fits the data; and RMSE
refers to the Root Mean Squared Error, the error
between the predicted and actual user ratings.
We can observe that MMR achieves the best
performance for predicting colloquialism and nat-
uralness, whereas M5P best predicts politeness.
Unfortunately, all regressors achieve at best a
moderate correlation with human ratings. Based
on these results, we ran a correlation analysis for
all utterances for which more than 20 original
user ratings were available. The purpose was to
find out to what extent human raters agree with
each other. The results showed that user agree-
ment in fact ranges from a high positive corre-
7http://www.r-project.org/
8http://www.cs.waikato.ac.nz/ml/weka/
705
Model r R2 RMSE
Colloquial
MMR 0.50 0.25 0.85
SVM 0.47 0.22 0.86
M5P 0.48 0.23 0.85
ZeroR -0.08 0.006 0.97
Natural
MMR 0.30 0.09 0.78
SVM 0.24 0.06 0.81
M5P 0.27 0.07 0.78
ZeroR -0.09 0.008 0.81
Polite
MMR 0.33 0.11 0.71
SVM 0.31 0.09 0.73
M5P 0.42 0.18 0.69
ZeroR -0.09 0.008 0.76
Table 3: Comparison of regression models per dimen-
sion using average user ratings. The best model is
indicated in bold-face for the correlation coefficient.
Model r R2 RMSE
Colloquial
MMR 0.61 0.37 1.05
SVM 0.36 0.13 1.3
M5P 0.56 0.31 1.07
ZeroR -0.06 0.004 1.3
Natural
MMR 0.55 0.30 0.96
SVM 0.36 0.13 1.13
M5P 0.49 0.24 0.99
ZeroR -0.08 0.06 1.13
Polite
MMR 0.69 0.48 0.76
SVM 0.54 0.30 0.92
M5P 0.71 0.50 0.73
ZeroR -0.04 0.002 1.04
Table 4: Comparison of regression models per dimen-
sion using individual user ratings. The best model is
indicated in bold-face for the correlation coefficient.
lation of 0.79 to a moderate negative correlation
of ?0.55. The average is 0.04 (SD=0.95), i.e.
indicating no correlation between user ratings,
even for the same utterance. This observation is
partially in line with related work that has found
high diversity in subjective user ratings. Yeh and
Mellish (1997) report only 70% agreement of hu-
man judges on the best choice of referring ex-
pression. Amatriain et al. (2009) report incon-
sistencies in user ratings in recommender systems
with an RMSE range of 0.55 to 0.81 and argue
that this constitutes a lower bound for system per-
formance. This inconsistency is exacerbated by
raters recruited via crowdsourcing platforms as
in our study (Koller et al., 2010; Rieser et al.,
2011). However, while crowdsourced data have
been shown to contain substantially more noise
than data collected in a lab environment, they do
tend to reflect the general tendency of their more
controlled counterparts (Gosling et al., 2004).
Individual User Ratings Given that individual
preferences exist for surface realisation (Walker
et al., 2007), we included the user?s ID as a re-
gression feature and re-ran the experiments. The
hypothesis was that if users differ in their pref-
erences for realisation candidates, they may also
differ in terms of their perceptions of linguistic
styles. The results shown in Table 4 support this:
the obtained correlations are significantly higher
(p<0.001, using the Fisher r-to-z transformation)
than those without the user?s ID (though we are
still not able to model the full variation observed
in ratings). Importantly, this shows that user rat-
ings are intrinsically coherent (not random) and
that variation exists mainly for inter-user agree-
ment. This model performs satisfactorily for a
known population of users. However, it does not
allow the prediction of ratings of unknown users,
who we mostly encounter in generation.
5 Clustering User Rating Behaviour
5.1 Spectral Clustering
The goal of this section is to find a number of k
clusters which partition our data set of user rat-
ings in a way that users in one cluster rate ut-
terances with particular linguistic properties most
similarly to each other, while rating them most
dissimilarly to users in other clusters. We as-
sume a set of n data points x
1
. . . x
n
, which
in our case correspond to an individual user or
group of users, characterised in terms of word
bigrams, POS tag bigrams, and assigned rat-
ings of the utterance they rated. An example
is Beluga NNP serves VBZ Italian JJ food NN;
[col=5.0, nat=5.0, pol=4.0]. Features were cho-
sen as a subset of relevant features from the larger
set used for regression above.
Using spectral clustering (von Luxburg, 2007),
clusters can be identified from a set of eigenvec-
tors of an affinity matrix S derived from pair-wise
similarities between data points s
ij
= s(x
i
, x
j
)
using a symmetric and non-negative similarity
function. To do that, we use a cumulative simi-
larity based on the Kullback-Leibler divergence,
D(P,Q) =
?
i
p
i
log
2
(
p
i
q
i
) +
?
j
q
j
log
2
(
q
j
p
j
)
2
,
where P is a distribution of words, POS tags or
ratings in data point x
i
; and Q a similar distribu-
tion in data point x
j
. The lower the cumulative di-
706
0.
3
0.
4
0.
5
0.
6
Number of Clusters
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 3 5 7 9 20 40 60 80 100 167
Individual
Clusters
Average
Figure 2: Average correlation coefficient for different
numbers of clusters. For comparison, results from av-
erage and individual user ratings are also shown.
vergence between two data sets, the more similar
they are. To find clusters of similar users from the
affinity matrix S, we use the algorithm described
in Ng et al. (2001). It derives clusters by choosing
the k largest eigenvectors u
1
, u
2
, . . . , u
k
from the
Laplacian matrix L = D1/2?SD1/2 (where D is
a diagonal matrix), arranging them into columns
in a matrix U = [u
1
u
2
. . . u
k
] and then normalis-
ing them for length. The result is a new matrix T ,
obtained through t
ij
= u
ij
/(
?
k
u
2
ik
)
1/2
. The set
of clusters C
1
, . . . C
k
can then be obtained from T
using the K-means algorithm, where each row in
T serves as an individual data point. Finally, each
original data point x
i
(row i of T ) is assigned to a
cluster C
j
. In comparison to other clustering algo-
rithms, experiments by Ng et al. (2001) show that
spectral clustering is robust for convex and non-
convex data sets. The authors also demonstrate
why using K-means only is often not sufficient.
The main clusters obtained describe surface
realisation preferences by particular groups of
users. An example is the realisation of the loca-
tion of a restaurant as a prepositional phrase or as
a relative clause as in restaurant in the city centre
vs. restaurant located in the city centre; or the re-
alisation of the food type as an adjective, an Ital-
ian restaurant, vs. a clause, this restaurant serves
Italian food. Clusters can then be characterised as
different combinations of such preferences.
5.2 Results: Predicting Stylistic Ratings
Figure 2 shows the average correlation coefficient
r across dimensions in relation to the number
of clusters, in comparison to the results obtained
with average and individual user ratings. We can
see that the baseline without user information is
outperformed with as few as three clusters. From
30 clusters on, a medium correlation is obtained
until another performance jump occurs around 90
clusters. Evidently, the best performance would
be achieved by obtaining one cluster per user, i.e.
167 clusters, but nothing would be gained in this
way, and we can see that useful generalisations
can be made from much fewer clusters. Based on
the clusters found, we will now predict the ratings
of known and unknown users.
Known Users For known users, first of all, Fig-
ure 3 shows the correlations between the predicted
and actual ratings for colloquialism, politeness
and naturalness based on 90 user clusters. Cor-
relation coefficients were obtained using an MMR
regressor. We can see that a medium correlation is
achieved for naturalness and (nearly) strong cor-
relations are achieved for politeness and colloqui-
alism. This confirms that clustering users can help
to better predict their ratings than based on shal-
low linguistic features alone, but that more gener-
alisation is achieved than based on individual user
ratings that include the user?s ID as a regression
feature. The performance gain in comparison to
predicting average ratings is significant (p<0.01)
from as few as three clusters onwards.
Unknown Users We initially sort unknown
users into the majority cluster and then aim to
make more accurate cluster allocations as more
information becomes available. For example, af-
ter a user has assigned their first rating, we can
take it into account to re-estimate their cluster
more accurately. Clusters are re-estimated with
each new rating, based on our trained regression
model. While estimating a user cluster based on
linguistic features alone yields an average corre-
lation of 0.38, an estimation based on linguistic
features and a single rating alone already yields an
average correlation of 0.45. From around 30 rat-
ings, the average correlation coefficients achieved
are as good as for known users. More importantly,
though, estimations based on a single rating alone
significantly outperform ratings based on the av-
707
(a)
1 2 3 4 5
2
3
4
5
Correlation: Colloquialism
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(b)
1 2 3 4 5
1
2
3
4
5
Correlation: Naturalness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(c)
1 2 3 4 5
1
2
3
4
5
Correlation: Politeness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
Figure 3: Correlations per dimension between actual and predicted user ratings based on 90 user clusters: (a)
Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001).
erage population of users (p<0.001). Fig. 4 shows
this process. It shows the correlation between pre-
dicted and actual user ratings for unknown users
over time. This is useful in interactive scenarios,
where system behaviour is refined as more infor-
mation becomes available (Cuaya?huitl and Deth-
lefs, 2011; Gas?ic? et al., 2011), or for incremental
systems (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012b; Dethlefs et al., 2012a).
0.
3
0.
4
0.
5
0.
6
Number of Ratings
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 2 3 4 5 6 7 8 9 10 15 20 30
90 Clusters
Ratings
Average
Figure 4: Average correlation coefficient for unknown
users with an increasing number of ratings. Results
from 90 clusters and average ratings are also shown.
6 Evaluation: Stylistically-Aware
Surface Realisation
To evaluate the applicability of our regression
model for stylistically-adaptive surface realisa-
tion, this section describes work that compares
four different surface realisers, which were not
originally developed to produce stylistic variation.
To do that, we first obtain the cluster for each in-
put sentence s: c? = argmin
c?C
?
x
D(P
x
s
|Q
x
c
),
where x refers to n-grams, POS tags or ratings
(see Section 5.1); P refers to a discrete probability
distribution of sentence s; and Q refers to a dis-
crete probability distribution of cluster c. The best
cluster is used to compute the style score of sen-
tence s using: score(s) =
?
n
i
?
i
f
i
(s), c
?
? F ,
where ?
i
are the weights estimated by the regres-
sor, and f
i
are the features of sentence s; see Table
2. The idea is that if well-phrased utterances can
be generated, whose stylistic variation is recog-
nisable to human judges, then our regressor can
be used in combination with any statistical sur-
face realiser. Note however that the stylistic vari-
ation observed depends on the stylistic spectrum
that each realiser covers. Here, our goal is mainly
to show that whatever stylistic variation exists in
a realiser can be recognised by our model.
6.1 Overview of Surface Realisers
In a human rating study, we compare four surface
realisers (ordered alphabetically), all of which
are able to return a ranked list of candidate re-
alisations for a semantic input. Please refer to
the references given for details of each system.
The BAGEL and SPaRKy realisers were compared
based on published ranked output lists.9
? BAGEL is a surface realiser based on dy-
namic Bayes Nets originally trained using
Active Learning by Mairesse et al. (2010).
It was shown to generate well-phrased utter-
ances from unseen semantic inputs.
? CRF (global) treats surface realisation as a
9Available from http://people.csail.mit.
edu/francois/research/bagel and http://
users.soe.ucsc.edu/
?
maw/downloads.html.
708
System Utterance
BAGEL Beluga is a moderately priced
restaurant in the city centre area.
Col = 4.0, Pol = 4.0, Nat = 4.0
CRF (global) Set in the city centre, Beluga is a
moderately priced location for the
celebration of the Italian spirit.
Col = 2.0, Pol = 5.0, Nat = 2.0
pCRU Beluga is located in the city centre
and serves cheap Italian food.
Col = 4.0, Pol = 3.0, Nat = 5.0
SPaRKy Beluga has the best overall quality
among the selected restaurants
since this Italian restaurant has
good decor, with good service.
Col = 3.0, Pol = 4.0, Nat = 5.0
Table 5: Example utterances for the BAGEL, CRF
(global), pCRU and SPaRKy realisers shown to users.
Sample ratings from individual users are also shown.
sequence labelling task: given a set of (ob-
served) linguistic features, it aims to find the
best (hidden) sequence of phrases realising a
semantic input (Dethlefs et al., 2013).
? pCRU is based on probabilistic context-
free grammars and generation is done using
Viterbi search, sampling (used here), or ran-
dom search. It is based on Belz (2008).
? SPaRKy is based on a rank-and-boost ap-
proach. It learns a mapping between the lin-
guistic features of a target utterance and its
predicted user ratings and ranks candidates
accordingly (Walker et al., 2007).
6.2 Results: Recognising Stylistic Variation
242 users from the USA took part in a rating study
on the CrowdFlower platform and rated altogether
1, 702 utterances, from among the highest-ranked
surface realisations above. For each utterance
they read, they rated the colloquialism, natura-
less and politeness based on the same questions
as in Section 4.1, used to obtain the training data.
Based on this, we compare the perceived strength
of each stylistic dimension in an utterance to the
one predicted by the regressor. Example utter-
ances and ratings are shown in Table 5. Results
are shown in Table 6 and confirm our observa-
tions: ratings for known users can be estimated
with a medium (or high) correlation based on
clusters of users who assign similar ratings to ut-
terances with similar linguistic features. We can
also see that such estimations do not depend on a
particular data set or realiser.
System Colloquial Polite Natural
BAGEL 0.78 0.66 0.69
CRF global 0.58 0.63 0.63
pCRU 0.67 0.42 0.77
SPaRKy 0.87 0.56 0.81
Table 6: Correlation coefficients between subjective
user ratings and ratings predicted by the regressor for
known users across data-driven surface realisers.
A novel aspect of our technique in compari-
son to previous work on stylistic realisation is
that it does not depend on the time- and resource-
intensive design of a hand-coded generator, as in
Paiva and Evans (2005) and Mairesse and Walker
(2011). Instead, it can be applied in conjunc-
tion with any system designer?s favourite realiser
and preserves the realiser?s original features by
re-ranking only its top n (e.g. 10) output candi-
dates. Our method is therefore able to strike a
balance between highly-ranked and well-phrased
utterances and stylistic adaptation. A current lim-
itation of our model is that some ratings can still
not be predicted with a high correlation with hu-
man judgements. However, even the medium cor-
relations achieved have been shown to be signif-
icantly better than estimations based on the aver-
age population of users (Section 5.2).
7 Conclusion and Future Work
We have presented a model of stylistic realisation
that is able to adapt its output along several stylis-
tic dimensions. Results show that the variation is
recognisable by humans and that user ratings can
be predicted for known as well as unknown users.
A model which clusters individual users based
on their ratings of linguistically similar utterances
achieves significantly higher performance than a
model trained on the average population of rat-
ings. These results may also play a role in other
domains in which users display variability in their
subjective ratings, e.g. recommender systems,
sentiment analysis, or emotion generation. Future
work may explore the use of additional cluster-
ing features as a more scalable alternative to re-
ranking. It also needs to determine how user feed-
back can be obtained during an interaction, where
asking users for ratings may be disruptive. Possi-
bilities include to infer user ratings from their next
dialogue move, or from multimodal information
such as hesitations or eye-tracking.
709
Acknowledgements This research was funded
by the EC FP7 programme FP7/2011-14 under
grant agreements no. 270019 (SPACEBOOK)
and no. 287615 (PARLANCE).
References
Xavier Amatriain, Josep M. Pujol, and Nuria Oliver.
2009. I like It... I Like It Not: Evaluating User Rat-
ings Noise in Recommender Systems. In In the 17th
International Conference on User Modelling, Adap-
tation, and Personalisation (UMAP), pages 247?
258, Trento, Italy. Springer-Verlag.
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language En-
gineering, 14(4):431?455.
Penelope Brown and Stephen Levinson. 1987. Some
Universals in Language Usage. Cambridge Univer-
sity Press, Cambridge, UK.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011. Op-
timizing Situated Dialogue Management in Un-
known Environments. In INTERSPEECH, pages
1009?1012.
Robert Dale and Jette Viethen. 2009. Referring
Expression Generation Through Attribute-Based
Heuristics. In Proceedings of the 12th Euro-
pean Workshop on Natural Language Generation
(ENLG), Athens, Greece.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the Interna-
tional Conference on Natural Language Generation
(INLG), Chicago, Illinois, USA.
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl,
and Oliver Lemon. 2013. Conditional Random
Fields for Responsive Surface Realisation Using
Global Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Sofia, Bulgaria.
Michael Fleischman and Eduard Hovy. 2002. Emo-
tional Variation in Speech-Based Natural Language
Generation. In Proceedings of the 2nd International
Natural Language Generation Conference.
Milica Gas?ic?, Filip Jurc???c?ek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-Line Policy Optimi-
sation of Spoken Dialogue Systems via Interaction
with Human Subjects. In Proceedings of the IEEE
Automatic Speech Recognition and Understanding
(ASRU) Workshop.
Alastair Gill and Jon Oberlander. 2002. Taking Care
of the Linguistic Features of Extraversion. In Pro-
ceedings of the 24th Annual Conference of the Cog-
nitive Science Society, pages 363?368, Fairfax, VA.
Samuel Gosling, Simine Vazire, Sanjay Srivastava,
and Oliver John. 2004. Should We Trust Web-
Based Studies? A Comparative Analysis of Six Pre-
conceptions About Internet Questionnaires. Ameri-
can Psychologist, 59(2):93?104.
Swati Gupta, Marilyn Walker, and Daniela Romano.
2007. How Rude Are You? Evaluating Politeness
and Affect in Interaction. In Proceedings of the
2nd International Conference on Affective Comput-
ing and Intelligent Interaction.
Helen Hastie, Marie-Aude Aufaure, Panos Alex-
opoulos, Heriberto Cuayhuitl, Nina Dethlefs,
James Henderson Milica Gasic, Oliver Lemon,
Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis,
Yves Vanrompay, Boris Villazon-Terrazas, and
Steve Young. 2013. Demonstration of the PAR-
LANCE System: A Data-Driven, Incremental, Spo-
ken Dialogue System for Interactive Search. In Pro-
ceedings of the 14th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIG-
dial).
Eduard Hovy. 1988. Generating Natural Language
under Pragmatic Constraints. Lawrence Erlbaum
Associates, Hillsdale, NJ.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and Alignment in Generated
Dialogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG),
Sydney, Australia.
Srini Janarthanam and Oliver Lemon. 2014. Adaptive
generation in dialogue systems using dynamic user
modeling. Computational Linguistics. (in press).
Pamela Jordan and Marilyn Walker. 2005. Learning
Content Selection Rules for Generating Object De-
scriptions in Dialogue. Journal of Artificial Intelli-
gence Research, 24:157?194.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, and Johanna Moore.
2010. The First Challenge on Generating Instruc-
tions in Virtual Environments. In M. Theune and
E. Krahmer, editors, Empirical Methods in Natu-
ral Language Generation, pages 337?361. Springer
Verlag, Berlin/Heidelberg.
Franc?ois Mairesse and Marilyn Walker. 2011. Con-
trolling User Perceptions of Linguistic Style: Train-
able Generation of Personality Traits. Computa-
tional Linguistics, 37(3):455?488, September.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Si-
mon Keizer, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Phrase-based statistical language
generation using graphical models and active learn-
ing. In Proceedings of the Annual Meeting of the
710
Association for Computational Linguistics (ACL),
pages 1552?1561.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001.
On Spectral Clustering: Analysis and an Algorithm.
In Advances in Neural Information Processing Sys-
tems, pages 849?856. MIT Press.
Michael O?Mahony, Neil Hurley, and Gue?nole? Sil-
vestre. 2006. Detecting Noise in Recommender
System Databases. In Proceedings of the Inter-
national Conference on Intelligent User Interfaces
(IUI)s. ACM Press.
Daniel Paiva and Roger Evans. 2005. Empirically-
Based Control of Natural Language Generation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
Ann Arbor, Michigan, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-
ing Class Relationships for Sentiment Categoriza-
tion with Respect to Rating Scales. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL).
Kaska Porayska-Pomsta and Chris Mellish. 2004.
Modelling Politness in Natural Language Gener-
ation. In Proceedings of the 3rd International
Natural Language Generation Conference (INLG),
Brighton, UK.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual Sig-
Dial Meeting on Discourse and Dialogue, Tokyo,
Japan.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17(4).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
Ching-long Yeh and Chris Mellish. 1997. An Empir-
ical Study on the Generation of Anaphora in Chi-
nese. Computational Linguistics, 23:169?190.
711
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 636?640,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Comparing HMMs and Bayesian Networks for Surface Realisation
Nina Dethlefs
Heriot-Watt University
Edinburgh, Scotland
n.s.dethlefs@hw.ac.uk
Heriberto Cuaya?huitl
German Research Centre for Artificial Intelligence
Saarbru?cken, Germany
heriberto.cuayahuitl@dfki.de
Abstract
Natural Language Generation (NLG) systems
often use a pipeline architecture for sequen-
tial decision making. Recent studies how-
ever have shown that treating NLG decisions
jointly rather than in isolation can improve the
overall performance of systems. We present
a joint learning framework based on Hierar-
chical Reinforcement Learning (HRL) which
uses graphical models for surface realisation.
Our focus will be on a comparison of Bayesian
Networks and HMMs in terms of user satis-
faction and naturalness. While the former per-
form best in isolation, the latter present a scal-
able alternative within joint systems.
1 Introduction
NLG systems have traditionally used a pipeline ar-
chitecture which divides the generation process into
three distinct stages. Content selection chooses
?what to say? and constructs a semantic form. Ut-
terance planning organises the message into sub-
messages and surface realisation maps the seman-
tics onto words. Recently, a number of studies
have pointed out that many decisions made at these
distinct stages require interrelated, rather than iso-
lated, optimisations (Angeli et al, 2010; Lemon,
2011; Cuaya?huitl and Dethlefs, 2011a; Dethlefs and
Cuaya?huitl, 2011a). The key feature of a joint archi-
tecture is that decisions of all three NLG stages share
information and can be made in an interrelated fash-
ion. We present a joint NLG framework based on
Hierarchical RL and focus, in particular, on the sur-
face realisation component of joint NLG systems.
We compare the user satisfaction and naturalness
of surface realisation using Hidden Markov Models
(HMMs) and Bayesian Networks (BNs) which both
have been suggested as generation spaces?spaces
of surface form variants for a semantic concept?
within joint NLG systems (Dethlefs and Cuaya?huitl,
2011a; Dethlefs and Cuaya?huitl, 2011b) and in iso-
lation (Georgila et al, 2002; Mairesse et al, 2010).
2 Surface Realisation for Situated NLG
We address the generation of navigation instruc-
tions, where e.g. the semantic form (path(target =
end of corridor) ? (landmark = lif t ? dir =
left)) can be expressed as ?Go to the end of the
corridor?, ?Head to the end of the corridor past the
lift on your left? and many more. The best realisa-
tion depends on the space (types and properties of
spatial objects), the user (position, orientation, prior
knowledge) and decisions of content selection and
utterance planning. These can be interrelated with
surface realisation, for example:
(1) ?Follow this corridor and go past the lift on your
left. Then turn right at the junction.?
(2) ?Pass the lift and turn right at the junction.?
Here, (1) is appropriate for a user unfamiliar with the
space and a high information need, so that more in-
formation should be given. For a familiar user, how-
ever, who may know where the lift is, it is redundant
and (2) is preferable, because it is more efficient. An
unfamiliar user may get confused with just (2).
In this paper, we distinguish navigation of des-
tination (?go back to the office?), direction (?turn
left?), orientation (?turn around?), path (?follow the
636
corridor?) and straight? (?go forward?) in the GIVE
corpus (Gargett et al, 2010). Users can react to an
instruction by performing the action, performing an
undesired action, hesitating or requesting help.
3 Jointly Learnt NLG: Hierarchical RL
with Graphical Models
In a joint framework, each subtask of content selec-
tion, utterance planning and surface realisation has
knowledge of the decisions made in the other two
subtasks. In an isolated framework, this knowledge
is absent. In the joint case, the relationship between
hierarchical RL and graphical models is that the lat-
ter provide feedback to the former?s surface realisa-
tion decisions according to a human corpus.
Hierarchical RL Our HRL agent consists of a
hierarchy of discrete-time Semi-Markov Decision
Processes, or SMDPs, M ij defined as 4-tuples <
Sij, Aij , T ij , Rij >, where i and j uniquely identify
a model in the hierarchy. These SMDPs represent
generation subtasks, e.g. generating destination in-
structions. Sij is a set of states, Aij is a set of ac-
tions, and T ij is a probabilistic state transition func-
tion that determines the next state s? from the current
state s and the performed action a. Rij(s?, ? |s, a) is
a reward function that specifies the reward that an
agent receives for taking an action a in state s last-
ing ? time steps. Since actions in SMDPs may take
a variable number of time steps to complete, the ran-
dom variable ? represents this number of time steps.
Actions can be either primitive or composite. The
former yield single rewards, the latter correspond to
SMDPs and yield cumulative rewards. The goal of
each SMDP is to find an optimal policy pi? that max-
imises the reward for each visited state, according
to pi?ij(s) = argmaxa?A Q?ij(s, a), where Qij(s, a)
specifies the expected cumulative reward for execut-
ing action a in state s and then following pi?. Please
see (Dethlefs and Cuaya?huitl, 2011b) for details on
the design of the hierarchical RL agent and the inte-
gration of graphical models for surface realisation.
Hidden Markov Models Representing surface re-
alisation as an HMM can be roughly defined as the
converse of POS tagging. While in POS tagging we
map an observation string of words onto a hidden
sequence of POS tags, in NLG we face the oppo-
.
.
.
go
walk
into
to
point
room room room room
point point point
to to to
into into into
walk walk walk
go go go
process
spatial 
relation relatum detail
. . .
direc. direc. direc. direc.
Figure 1: Example trellis for an HMM for destination
instructions (not all states and transitions are shown).
Dashed arrows show paths that occur in the corpus.
site scenario. Given an observation sequence of se-
mantic symbols, we want to map it onto a hidden
most likely sequence of words. We treat states as
representing surface realisations for (observed) se-
mantic classes, so that a sequence of states s0...sn
represents phrases or sentences. An observation se-
quence o0...on consists of a finite set of semantic
symbols specific to an instruction type. Each symbol
has an observation likelihood bs(o)t giving the prob-
ability of observing o in state s at time t. We created
the HMMs and trained the transition and emission
probabilities from the GIVE corpus using the Baum-
Welch algorithm. Please see Fig. 1 for an example
HMM and (Dethlefs and Cuaya?huitl, 2011a) for de-
tails on using HMMs for surface realisation.
Bayesian Networks Representing a surface re-
aliser as a BN, we can model the dynamics between
semantic concepts and their realisations. A BN mod-
els a joint probability distribution over a set of ran-
dom variables and their dependencies based on a di-
rected acyclic graph, where each node represents a
variable Yj with parents pa(Yj). Due to the Markov
condition, each variable depends only on its parents,
resulting in a unique joint probability distribution
p(Y ) = ?p(Yj|pa(Yj)), where every variable is as-
sociated with a conditional probability distribution
637
Destination
Verb
Destination
Direction
Values: {left/right,
straight, empty} Values: {go, keep going,walk, continue, return,
get, you need, you want,
empty, ... }
Information
Values: {high, low}
Destination
Preposition
Values:{into, in,
to, towards, until,
empty, ...}
Destination
Relatum
Values:{landmark,
room}
Need
Figure 2: BN for generating destination instructions.
p(Yj|pa(Yj)). The meaning of random variables
corresponds to semantic symbols. The values of ran-
dom variables correspond to surface variants of a se-
mantic symbol. Figure 2 shows an example BN with
two main dependencies. First, the random variable
?information need? influences the inclusion of op-
tional semantic constituents and the process of the
utterance (?destination verb?). Second, a sequence
of dependencies spans from the verb to the end of
the utterance (?destination relatum?). The first de-
pendency is based on the intuition that more detail
is needed in an instruction for users with high infor-
mation need (e.g. with little prior knowledge).1 The
second dependency is based on the hypothesis that
the value of one constituent can be estimated based
on the previous constituent. In the future, we may
compare different configurations and effects of word
order. Given the word sequence represented by lex-
ical and syntactic variables Y0...Yn, and situation-
based variables Yn+1...Ym, we can compute the pos-
terior probability of a random variable Yj . The pa-
rameters of the BNs were estimated using MLE.
Please see (Dethlefs and Cuaya?huitl, 2011b) for de-
tails on using BNs for surface realisation within a
joint learning framework.
4 Experimental Setting
We compare instructions generated with the
HMMs and BNs according to their user sat-
isfaction and their naturalness. The learn-
1This is key to the joint treatment of content selection and
surface realisation: if an utterance is not informative in terms
of content, it will receive bad rewards, even with good surface
realisation choices (and vice versa).
ing agent is trained using the reward function
Reward = User satisfaction ? P (w0 . . . wn) ?
CAS.2 User satisfaction is a function of task
success and the number of user turns based on
the PARADISE framework3 (Walker et al, 1997)
and CAS refers to the proportion of repetition
and variation in surface forms. Our focus in
this short paper is on P (w0 . . . wn) which rewards
the agent for having generated a surface form se-
quence w0 . . . wn. In HMMs, this corresponds to
the forward probability?obtained from the For-
ward algorithm?of observing the sequence in the
data. In BNs, P (w0 . . . wn) corresponds to P (Yj =
vx|pa(Yj) = vy), the posterior probability given the
chosen values vx and vy of random variables and
their dependencies. We assign a reward of ?1 for
each action to prevent loops.
5 Experimental Results
User satisfaction Our trained policies learn the
same content selection and utterance planning be-
haviour reported by (Dethlefs and Cuaya?huitl,
2011b). These policies contribute to the user sat-
isfaction of instructions. BNs and HMMs however
differ in their surface realisation choices. Figure
3 shows the performance in terms of average re-
wards over time for both models within the joint
learning framework and in isolation.4 For ease of
comparison, a learning curve using a greedy policy
is also shown. It always chooses the most likely
surface form according to the human corpus with-
out taking other tradeoffs into account. Within the
joint framework, both BNs and HMMs learn to gen-
erate context-sensitive surface forms that balance
the tradeoffs of the most likely sequence (accord-
ing to the human corpus) and the one that best cor-
responds to the user?s information need (e.g., using
nick names of rooms for familiar users). The BNs
2This reward function, the simulated environment and train-
ing parameters were adapted from (Dethlefs and Cuaya?huitl,
2011b) to allow a comparison with related work in using graph-
ical models for surface realisation. Simulation is based on uni-
and bigrams for the spatial setting and Naive Bayes Classifica-
tion for user reactions to system instructions.
3See (Dethlefs et al, 2010) for evidence of the correlation
between user satisfaction, task success and dialogue length.
4In the isolated case, subtasks of content selection, utterance
planning and surface realisation are blind regarding the deci-
sions made by other subtasks, but in the joint case they are not.
638
103 104 105
?20
?19
?18
?17
?16
?15
?14
?13
?12
?11
?10
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
BNs Joint
BNs Isolated
HMMs Joint
HMMs Isolated
Greedy
Figure 3: Performance of HMMs, BNs and a greedy base-
line in conjunction and isolation of the joint framework.
reach an average reward5 of ?11.53 and outper-
form the HMMs (average ?11.64) only marginally
by less than one percent. BNs and HMMs improve
the greedy baseline by 6% (p < 0.0001, r = 0.90).
While BNs reach the same performance in isola-
tion of the joint framework, the performance of
HMMs deteriorates significantly to an average re-
ward of ?12.12. This corresponds to a drop of 5%
(p < 0.0001, r = 0.79) and is nearly as low as the
greedy baseline. HMMs thus reach a comparable
performance to BNs as a result of the joint learning
architecture: the HRL agent will discover the non-
optimal behaviour that is caused by the HMM?s lack
of context-awareness (due to their independence as-
sumptions) and learn to balance this drawback by
learning a more comprehensive policy itself. For the
more context-aware BNs this is not necessary.
Naturalness We compare the instructions gener-
ated with HMMs and BNs regarding their human-
likeness based on the Kullback-Leibler (KL) diver-
gence. It computes the difference between two prob-
ability distributions. For evidence of its usefulness
for measuring naturalness, cf. (Cuaya?huitl, 2010).
We compare human instructions (based on strings)
drawn from the corpus against strings generated by
the HMMs and BNs to see how similar both are to
human authors. Splitting the human instructions in
half and comparing them to each other indicates how
similar human authors are to each other. It yields a
KL score of 1.77 as a gold standard (the lower the
better). BNs compared with human data obtain a
score of 2.83 and HMMs of 2.80. The difference in
5The average rewards of agents have negative values due to
the negative reward of ?1 the agent receives for each action.
terms of similarity with humans for HMMs and BNs
in a joint NLG model is not significant.
Discussion While HMMs reach comparable user
satisfaction and naturalness to BNs in a joint system,
they show a 5% lower performance in isolation. This
is likely caused by their conditional independence
assumptions: (a) the Markov assumption, (b) the
stationary assumption, and (c) the observation inde-
pendence assumption. Even though these can make
HMMs easier to train and scale than more structured
models such as BNs, it also puts them in a disadvan-
tage concerning context-awareness and accuracy as
shown by our results. In contrast, the random vari-
ables of BNs allow them to keep a structured model
of the space, user, and relevant content selection and
utterance planning choices. BNs are thus able to
compute the posterior probability of a surface form
based on all relevant properties of the current situa-
tion (not just the occurrence in a corpus). While BNs
also place independence assumptions on their vari-
ables, they usually overcome the problem of lacking
context-awareness by their dependencies across ran-
dom variables. However, BNs also face limitations.
Given the dependencies they postulate, they are typ-
ically more data intensive and less scalable than less
structured models such as HMMs. This can be prob-
lematic for large domains such as many real world
applications. Regarding their application to surface
realisation, we can argue that while BNs are the best
performing model in isolation, HMMs represent a
cheap and scalable alternative especially for large-
scale problems in a joint NLG system.
6 Conclusion and Future Work
We have compared the user satisfaction and natural-
ness of instructions generated with HMMs and BNs
in a joint HRL model for NLG. Results showed that
while BNs perform best in isolation, HMMs repre-
sent a cheap and scalable alternative within the joint
framework. This is particularly attractive for large-
scale, data-intensive systems. While this paper has
focused on instruction generation, the hierarchical
approach in our learning framework helps to scale
up to larger NLG tasks, such as text or paragraph
generation. Future work could test this claim, com-
pare other graphical models, such as dynamic BNs,
and aim for a comprehensive human evaluation.
639
Acknowledgements This research was funded by
the European Commission?s FP7 programmes under
grant agreement no. 287615 (PARLANCE) and no.
ICT-248116 (ALIZ-E).
References
Angeli, G., Liang, P. and D. Klein (2010). A simple
domain-independent probabilistic approach to gener-
ation , Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP) .
Cuaya?huitl, H., Renals, S., Lemon, O. and H. Shimodaira
(2010). Evaluation of a Hierarchical Reinforcement
Learning Spoken Dialogue System, Computer Speech
and Language 24.
Cuaya?huitl, H., and N. Dethlefs (2011a). Spatially-
Aware Dialogue Control Using Hierarchical Rein-
forcement Learning, ACM Transactions on Speech
and Language Processing (Special Issue on Machine
Learning for Robust and Adaptive Spoken Dialogue
Systems 7(3).
Dethlefs, N. and H. Cuaya?huitl, 2011. Hierarchical Re-
inforcement Learning and Hidden Markov Models for
Task-Oriented Natural Language Generation, In Proc.
of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL-HLT).
Dethlefs, N. and H. Cuaya?huitl, 2011. Combining Hi-
erarchical Reinforcement Learning and Bayesian Net-
works for Natural Language Generation in Situated
Dialogue, In Proceedings of the 13th European Work-
shop on Natural Language Generation (ENLG).
Dethlefs, N., Cuaya?huitl, H., Richter, K.-F., Andonova,
E. and J. Bateman, 2010. Evaluating Task Success in
a Dialogue System for Indoor Navigation, In Proceed-
ings of the Workshop on the Semantics and Pragmatics
of Dialogue (SemDial).
Gargett, A., Garoufi, K., Koller, A. and K. Striegnitz
(2010). The GIVE-2 Corpus of Giving Instructions
in Virtual Environments, Proc. of the 7th International
Conference on Language Resources and Evaluation.
Georgila, K., Fakotakis, N. and Kokkinakis, G. (2002).
Stochastic Language Modelling for Recognition and
Generation in Dialogue Systems. TAL (Traitement au-
tomatique des langues) Journal, Vol. 43(3).
Lemon, O. (2011). Learning what to say and how to say
it: joint optimization of spoken dialogue management
and Natural Language Generation, Computer Speech
and Language 25(2).
Mairesse, F., Gas?ic?, M., Jurc???c?ek, F., Keizer, S., Thom-
son, B., Yu, K. and S. Young (2010). Phrase-based
statistical language generation using graphical models
and active learning, Proc. of the 48th Annual Meeting
of the Association for Computational Linguistics.
Walker, M., Litman, D., Kamm, C. and A. Abella (1997).
PARADISE: A Framework for Evaluating Spoken Di-
alogue Agents, Proceedings of the Annual Meeting of
the Association for Computational Linguistic (ACL).
640
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 654?659,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Hierarchical Reinforcement Learning and Hidden Markov Models for
Task-Oriented Natural Language Generation
Nina Dethlefs
Department of Linguistics,
University of Bremen
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
German Research Centre for Artificial Intelligence
(DFKI), Saarbru?cken
heriberto.cuayahuitl@dfki.de
Abstract
Surface realisation decisions in language gen-
eration can be sensitive to a language model,
but also to decisions of content selection. We
therefore propose the joint optimisation of
content selection and surface realisation using
Hierarchical Reinforcement Learning (HRL).
To this end, we suggest a novel reward func-
tion that is induced from human data and is
especially suited for surface realisation. It is
based on a generation space in the form of
a Hidden Markov Model (HMM). Results in
terms of task success and human-likeness sug-
gest that our unified approach performs better
than greedy or random baselines.
1 Introduction
Surface realisation decisions in a Natural Language
Generation (NLG) system are often made accord-
ing to a language model of the domain (Langkilde
and Knight, 1998; Bangalore and Rambow, 2000;
Oh and Rudnicky, 2000; White, 2004; Belz, 2008).
However, there are other linguistic phenomena, such
as alignment (Pickering and Garrod, 2004), consis-
tency (Halliday and Hasan, 1976), and variation,
which influence people?s assessment of discourse
(Levelt and Kelter, 1982) and generated output (Belz
and Reiter, 2006; Foster and Oberlander, 2006).
Also, in dialogue the most likely surface form may
not always be appropriate, because it does not cor-
respond to the user?s information need, the user is
confused, or the most likely sequence is infelicitous
with respect to the dialogue history. In such cases, it
is important to optimise surface realisation in a uni-
fied fashion with content selection. We suggest to
use Hierarchical Reinforcement Learning (HRL) to
achieve this. Reinforcement Learning (RL) is an at-
tractive framework for optimising a sequence of de-
cisions given incomplete knowledge of the environ-
ment or best strategy to follow (Rieser et al, 2010;
Janarthanam and Lemon, 2010). HRL has the ad-
ditional advantage of scaling to large and complex
problems (Dethlefs and Cuaya?huitl, 2010). Since
an HRL agent will ultimately learn the behaviour
it is rewarded for, the reward function is arguably
the agent?s most crucial component. Previous work
has therefore suggested to learn a reward function
from human data as in the PARADISE framework
(Walker et al, 1997). Since PARADISE-based re-
ward functions typically rely on objective metrics,
they are not ideally suited for surface realisation,
which is more dependend on linguistic phenomena,
e.g. frequency, consistency, and variation. However,
linguistic and psychological studies (cited above)
show that such phenomena are indeed worth mod-
elling in an NLG system. The contribution of this
paper is therefore to induce a reward function from
human data, specifically suited for surface genera-
tion. To this end, we train HMMs (Rabiner, 1989)
on a corpus of grammatical word sequences and use
them to inform the agent?s learning process. In addi-
tion, we suggest to optimise surface realisation and
content selection decisions in a joint, rather than iso-
lated, fashion. Results show that our combined ap-
proach generates more successful and human-like
utterances than a greedy or random baseline. This is
related to Angeli et al (2010), who also address in-
terdependent decision making, but do not use an opt-
misation framework. Since language models in our
approach can be obtained for any domain for which
corpus data is available, it generalises to new do-
mains with limited effort and reduced development
654
Utterance
string=?turn around and go out?, time=?20:54:55?
Utterance type
content=?orientation,destination? [straight, path, direction]
navigation level=?low? [high]
User
user reaction=?perform desired action?
[perform undesired action, wait, request help]
user position=?on track? [off track]
Figure 1: Example annotation: alternative values for at-
tributes are given in square brackets.
time. For related work on using graphical models
for language generation, see e.g., Barzilay and Lee
(2002), who use lattices, or Mairesse et al (2010),
who use dynamic Bayesian networks.
2 Generation Spaces
We are concerned with the generation of navigation
instructions in a virtual 3D world as in the GIVE
scenario (Koller et al, 2010). In this task, two peo-
ple engage in a ?treasure hunt?, where one partici-
pant navigates the other through the world, pressing
a sequence of buttons and completing the task by
obtaining a trophy. The GIVE-2 corpus (Gargett et
al., 2010) provides transcripts of such dialogues in
English and German. For this paper, we comple-
mented the English dialogues of the corpus with a
set of semantic annotations,1 an example of which
is given in Figure 1. This example also exempli-
fies the type of utterances we generate. The input to
the system consists of semantic variables compara-
ble to the annotated values, the output corresponds
to strings of words. We use HRL to optimise deci-
sions of content selection (?what to say?) and HMMs
for decisions of surface realisation (?how to say it?).
Content selection involves whether to use a low-, or
high-level navigation strategy. The former consists
of a sequence of primitive instructions (?go straight?,
?turn left?), the latter represents contractions of se-
quences of low-level instructions (?head to the next
room?). Content selection also involves choosing a
level of detail for the instruction corresponding to
the user?s information need. We evaluate the learnt
content selection decisions in terms of task success.
For surface realisation, we use HMMs to inform
the HRL agent?s learning process. Here we address
1The annotations are available on request.
the one-to-many relationship arising between a se-
mantic form (from the content selection stage) and
its possible realisations. Semantic forms of instruc-
tions have an average of 650 surface realisations,
including syntactic and lexical variation, and deci-
sions of granularity. We refer to the set of alterna-
tive realisations of a semantic form as its ?generation
space?. In surface realisation, we aim to optimise the
tradeoff between alignment and consistency (Picker-
ing and Garrod, 2004; Halliday and Hasan, 1976) on
the one hand, and variation (to improve text quality
and readability) on the other hand (Belz and Reiter,
2006; Foster and Oberlander, 2006) in a 50/50 dis-
tribution. We evaluate the learnt surface realisation
decisions in terms of similarity with human data.
Note that while we treat content selection and
surface realisation as separate NLG tasks, their op-
timisation is achieved jointly. This is due to a
tradeoff arising between the two tasks. For exam-
ple, while surface realisation decisions that are opti-
mised solely with respect to a language model tend
to favour frequent and short sequences, these can
be inappropriate according to the user?s information
need (because they are unfamiliar with the naviga-
tion task, or are confused or lost). In such situa-
tions, it is important to treat content selection and
surface realisation as a unified whole. Decisions of
both tasks are inextricably linked and we will show
in Section 5.2 that their joint optimisation leads to
better results than an isolated optimisation as in, for
example, a two-stage model.
3 NLG Using HRL and HMMs
3.1 Hierarchical Reinforcement Learning
The idea of language generation as an optimisa-
tion problem is as follows: given a set of genera-
tion states, a set of actions, and an objective reward
function, an optimal generation strategy maximises
the objective function by choosing the actions lead-
ing to the highest reward for every reached state.
Such states describe the system?s knowledge about
the generation task (e.g. content selection, naviga-
tion strategy, surface realisation). The action set
describes the system?s capabilities (e.g. ?use high
level navigation strategy?, ?use imperative mood?,
etc.). The reward function assigns a numeric value
for each action taken. In this way, language gen-
655
Figure 2: Hierarchy of learning agents (left), where shaded agents use an HMM-based reward function. The top three
layers are responsible for content selection (CS) decisions and use HRL. The shaded agents in the bottom use HRL
with an HMM-based reward function and joint optimisation of content selection and surface realisation (SR). They
provide the observation sequence to the HMMs. The HMMs represent generation spaces for surface realisation. An
example HMM, representing the generation space of ?destination? instructions, is shown on the right.
eration can be seen as a finite sequence of states,
actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},
where the goal is to find an optimal strategy auto-
matically. To do this we use RL with a divide-and-
conquer approach to optimise a hierarchy of genera-
tion policies rather than a single policy. The hierar-
chy of RL agents consists of L levels and N models
per level, denoted as M ij , where j ? {0, ..., N ? 1}
and i ? {0, ..., L ? 1}. Each agent of the hierar-
chy is defined as a Semi-Markov Decision Process
(SMDP) consisting of a 4-tuple < Sij, Aij , T ij , Rij >.
Sij is a set of states, Aij is a set of actions, T ij is
a transition function that determines the next state
s? from the current state s and the performed ac-
tion a, and Rij is a reward function that specifies
the reward that an agent receives for taking an ac-
tion a in state s lasting ? time steps. The random
variable ? represents the number of time steps the
agent takes to complete a subtask. Actions can be
either primitive or composite. The former yield sin-
gle rewards, the latter correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optimal policy that max-
imises the reward for each visited state, according to
pi?ij(s) = arg maxa?Aij Q
?i
j(s, a), where Q?ij (s, a)
specifies the expected cumulative reward for execut-
ing action a in state s and then following policy pi?ij .
We use HSMQ-Learning (Dietterich, 1999) to learn
a hierarchy of generation policies.
3.2 Hidden Markov Models for NLG
The idea of representing the generation space of
a surface realiser as an HMM can be roughly de-
fined as the converse of POS tagging, where an in-
put string of words is mapped onto a hidden se-
quence of POS tags. Our scenario is as follows:
given a set of (specialised) semantic symbols (e.g.,
?actor?, ?process?, ?destination?),2 what is the most
likely sequence of words corresponding to the sym-
bols? Figure 2 provides a graphic illustration of this
idea. We treat states as representing words, and se-
quences of states i0...in as representing phrases or
sentences. An observation sequence o0...on consists
of a finite set of semantic symbols specific to the in-
struction type (i.e., ?destination?, ?direction?, ?orien-
tation?, ?path?, ?straight?). Each symbol has an ob-
servation likelihood bi(ot), which gives the proba-
bility of observing o in state i at time t. The tran-
sition and emission probabilities are learnt during
training using the Baum-Welch algorithm. To de-
sign an HMM from the corpus data, we used the
ABL algorithm (van Zaanen, 2000), which aligns
strings based on Minimum Edit Distance, and in-
duces a context-free grammar from the aligned ex-
amples. Subsequently, we constructed the HMMs
from the CFGs, one for each instruction type, and
trained them on the annotated data.
2Utterances typically contain five to ten semantic categories.
656
3.3 An HMM-based Reward Function Induced
from Human Data
Due to its unique function in an RL framework, we
suggest to induce a reward function for surface re-
alisation from human data. To this end, we create
and train HMMs to represent the generation space
of a particular surface realisation task. We then use
the forward probability, derived from the Forward
algorithm, of an observation sequence to inform the
agent?s learning process.
r =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0 for reaching the goal state
+1 for a desired semantic choice or
maintaining an equal distribution
of alignment and variation
-2 for executing action a and remain-
ing in the same state s = s?
P (w0...wn) for for reaching a goal state corres-
ponding to word sequence w0...wn
-1 otherwise.
Whenever the agent has generated a word sequence
w0...wn, the HMM assigns a reward corresponding
to the likelihood of observing the sequence in the
data. In addition, the agent is rewarded for short
interactions at maximal task success3 and optimal
content selection (cf. Section 2). Note that while re-
ward P (w0...wn) applies only to surface realisation
agents M30...4, the other rewards apply to all agents
of the hierarchy.
4 Experimental Setting
We test our approach using the (hand-crafted) hierar-
chy of generation subtasks in Figure 2. It consists of
a root agent (M00 ), and subtasks for low-level (M20 )
and high-level (M21 ) navigation strategies (M11 ), and
for instruction types ?orientation? (M30 ), ?straight?
(M31 ), ?direction? (M32 ), ?path? (M33 ) and destina-
tion? (M34 ). Models M30...4 are responsible for sur-
face generation. They will be trained using HRL
with an HMM-based reward function induced from
human data. All other agents use hand-crafted re-
wards. Finally, subtask M10 can repair a previous
system utterance. The states of the agent contain
all situational and linguistic information relevant to
its decision making, e.g., the spatial environment,
3Task success is addressed by that each utterance needs to
be ?accepted? by the user (cf. Section 5.1).
discourse history, and status of grounding.4 Due to
space constraints, please see Dethlefs et al (2011)
for the full state-action space. We distinguish prim-
itive actions (corresponding to single generation de-
cisions) and composite actions (corresponding to
generation subtasks (Fig. 2)).
5 Experiments and Results
5.1 The Simulated Environment
The simulated environment contains two kinds of
uncertainties: (1) uncertainty regarding the state of
the environment, and (2) uncertainty concerning the
user?s reaction to a system utterance. The first as-
pect is represented by a set of contextual variables
describing the environment, 5 and user behaviour.6
Altogether, this leads to 115 thousand different con-
textual configurations, which are estimated from
data (cf. Section 2). The uncertainty regarding
the user?s reaction to an utterance is represented by
a Naive Bayes classifier, which is passed a set of
contextual features describing the situation, mapped
with a set of semantic features describing the utter-
ance.7 From these data, the classifier specifies the
most likely user reaction (after each system act) of
perform desired action, perform undesired action, wait
and request help.8 The classifier was trained on the
annotated data and reached an accuracy of 82% in a
cross-corpus validation using a 60%-40% split.
5.2 Comparison of Generation Policies
We trained three different generation policies. The
learnt policy optimises content selection and sur-
face realisation decisions in a unified fashion, and
is informed by an HMM-based generation space
reward function. The greedy policy is informed
only by the HMM and always chooses the most
4An example for the state variables of model M11 are the
annotation values in Fig. 1 which are used as the agent?s
knowledge base. Actions are ?choose easy route?, ?choose short
route?, ?choose low level strategy?, ?choose high level strategy?.
5previous system act, route length, route status
(known/unknown), objects within vision, objects within
dialogue history, number of instructions, alignment(proportion)
6previous user reaction, user position, user wait-
ing(true/false), user type(explorative/hesitant/medium)
7navigation level(high / low), abstractness(implicit / ex-
plicit), repair(yes / no), instruction type(destination / direction /
orientation / path / straight)
8User reactions measure the system?s task success.
657
likely sequence independent of content selection.
The valid sequence policy generates any grammat-
ical sequence. All policies were trained for 20000
episodes.9 Figure 3, which plots the average re-
wards of all three policies (averaged over ten runs),
shows that the ?learnt? policy performs best in terms
of task success by reaching the highest overall re-
wards over time. An absolute comparison of the av-
erage rewards (rescaled from 0 to 1) of the last 1000
training episodes of each policy shows that greedy
improves ?any valid sequence? by 71%, and learnt
improves greedy by 29% (these differences are sig-
nificant at p < 0.01). This is due to the learnt policy
showing more adaptation to contextual features than
the greedy or ?valid sequence? policies. To evaluate
human-likeness, we compare instructions (i.e. word
sequences) using Precision-Recall based on the F-
Measure score, and dialogue similarity based on the
Kulback-Leibler (KL) divergence (Cuaya?huitl et al,
2005). The former shows how the texts generated by
each of our generation policies compare to human-
authored texts in terms of precision and recall. The
latter shows how similar they are to human-authored
texts. Table 1 shows results of the comparison of
two human data sets ?Real1? vs ?Real2? and both of
them together against our different policies. While
the greedy policy receives higher F-Measure scores,
the learnt policy is most similar to the human data.
This is due to variation: in contrast to greedy be-
haviour, which always exploits the most likely vari-
ant, the learnt policy varies surface forms. This leads
to lower F-Measure scores, but achieves higher sim-
ilarity with human authors. This ultimately is a de-
sirable property, since it enhances the quality and
naturalness of our instructions.
6 Conclusion
We have presented a novel approach to optimising
surface realisation using HRL. We suggested to
inform an HRL agent?s learning process by an
HMM-based reward function, which was induced
9For training, the step-size parameter ? (one for each
SMDP) , which indicates the learning rate, was initiated with
1 and then reduced over time by ? = 11+t , where t is the time
step. The discount rate ?, which indicates the relevance of fu-
ture rewards in relation to immediate rewards, was set to 0.99,
and the probability of a random action ? was 0.01. See Sutton
and Barto (1998) for details on these parameters.
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
x 104
?250
?200
?150
?100
?50
0
50
Av
er
ag
e 
Re
wa
rd
s
Episodes
 
 
Valid Sequence
Greedy
Learnt
Figure 3: Performance of ?learnt?, ?greedy?, and ?any
valid sequence? generation behaviours (average rewards).
Compared Policies F-Measure KL-Divergence
Real1 - Real2 0.58 1.77
Real - ?Learnt? 0.40 2.80
Real - ?Greedy? 0.49 4.34
Real - ?Valid Seq.? 0.0 10.06
Table 1: Evaluation of generation behaviours with
Precision-Recall and KL-divergence.
from data and in which the HMM represents the
generation space of a surface realiser. We also
proposed to jointly optimise surface realisation
and content selection to balance the tradeoffs of
(a) frequency in terms of a language model, (b)
alignment/consistency vs variation, (c) properties
of the user and environment. Results showed that
our hybrid approach outperforms two baselines in
terms of task success and human-likeness: a greedy
baseline acting independent of content selection,
and a random ?valid sequence? baseline. Future
work can transfer our approach to different domains
to confirm its benefits. Also, a detailed human
evaluation study is needed to assess the effects
of different surface form variants. Finally, other
graphical models besides HMMs, such as Bayesian
Networks, can be explored for informing the surface
realisation process of a generation system.
Acknowledgments
Thanks to the German Research Foundation DFG
and the Transregional Collaborative Research Cen-
tre SFB/TR8 ?Spatial Cognition? and the EU-FP7
project ALIZ-E (ICT-248116) for partial support of
this work.
658
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 502?512.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th Conference on Computa-
tional Linguistics (ACL) - Volume 1, pages 42?48.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 164?171.
Anja Belz and Ehud Reiter. 2006. Comparing Automatic
and Human Evaluation of NLG Systems. In Proc. of
the European Chapter of the Association for Compu-
tational Linguistics (EACL), pages 313?320.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 1:1?26.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-Computer Dia-
logue Simulation Using Hidden Markov Models. In
Proc. of ASRU, pages 290?295.
Nina Dethlefs and Heriberto Cuaya?huitl. 2010. Hi-
erarchical Reinforcement Learning for Adaptive Text
Generation. Proceeding of the 6th International Con-
ference on Natural Language Generation (INLG).
Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.
2011. Optimising Natural Language Generation De-
cision Making for Situated Dialogue. In Proc. of the
12th Annual SIGdial Meeting on Discourse and Dia-
logue.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227?303.
Mary Ellen Foster and Jon Oberlander. 2006. Data-
driven generation of emphatic facial displays. In Proc.
of the European Chapter of the Association for Com-
putational Linguistic (EACL), pages 353?360.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The GIVE-2 corpus of
giving instructions in virtual environments. In LREC.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In Proc. of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 69?78.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010. The first challenge on generat-
ing instructions in virtual environments. In M. The-
une and E. Krahmer, editors, Empirical Methods
on Natural Language Generation, pages 337?361,
Berlin/Heidelberg, Germany. Springer.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
704?710.
W J M Levelt and S Kelter. 1982. Surface form and
memory in question answering. Cognitive Psychol-
ogy, 14.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1552?1561.
Alice H. Oh and Alexander I. Rudnicky. 2000. Stochas-
tic language generation for spoken dialogue systems.
In Proceedings of the 2000 ANLP/NAACL Workshop
on Conversational systems - Volume 3, pages 27?32.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistc psychology of dialog. Behavioral and
Brain Sciences, 27.
L R Rabiner. 1989. A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition.
In Proceedings of IEEE, pages 257?286.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proc. of the Annual Meeting of
the Association for Computational Lingustics (ACL),
pages 1009?1018.
Richard S Sutton and Andrew G Barto. 1998. Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA.
Menno van Zaanen. 2000. Bootstrapping syntax and
recursion using alginment-based learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning (ICML), pages 1063?1070, San
Francisco, CA, USA.
Marilyn A. Walker, Diane J. Litman, Candace A. Kamm,
and Alicia Abella. 1997. PARADISE: A framework
for evaluating spoken dialogue agents. In Proc. of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 271?280.
Michael White. 2004. Reining in CCG chart realization.
In Proc. of the International Conference on Natural
Language Generation (INLG), pages 182?191.
659
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1254?1263,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Conditional Random Fields for Responsive Surface Realisation using
Global Features
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl and Oliver Lemon
Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh
n.s.dethlefs | h.hastie | h.cuayahuitl | o.lemon@hw.ac.uk
Abstract
Surface realisers in spoken dialogue sys-
tems need to be more responsive than con-
ventional surface realisers. They need to
be sensitive to the utterance context as well
as robust to partial or changing generator
inputs. We formulate surface realisation as
a sequence labelling task and combine the
use of conditional random fields (CRFs)
with semantic trees. Due to their extended
notion of context, CRFs are able to take
the global utterance context into account
and are less constrained by local features
than other realisers. This leads to more
natural and less repetitive surface realisa-
tion. It also allows generation from partial
and modified inputs and is therefore ap-
plicable to incremental surface realisation.
Results from a human rating study confirm
that users are sensitive to this extended no-
tion of context and assign ratings that are
significantly higher (up to 14%) than those
for taking only local context into account.
1 Introduction
Surface realisation typically aims to produce out-
put that is grammatically well-formed, natural and
cohesive. Cohesion can be characterised by lexical
or syntactic cues such as repetitions, substitutions,
ellipses, or connectives. In automatic language
generation, such properties can sometimes be dif-
ficult to model, because they require rich context-
awareness that keeps track of all (or much) of what
was generated before, i.e. a growing generation
history. In text generation, cohesion can span over
the entire text. In interactive settings such as gen-
eration within a spoken dialogue system (SDS), a
challenge is often to keep track of cohesion over
several utterances. In addition, since interactions
are dynamic, generator inputs from the dialogue
manager can sometimes be partial or subject to
subsequent modification. This has been addressed
by work on incremental processing (Schlangen
and Skantze, 2009). Since dialogue acts are passed
on to the generation module as soon as possible,
this can sometimes lead to incomplete generator
inputs (because the user is still speaking), or in-
puts that are subject to later modification (because
of an initial ASR mis-recognition).
In this paper, we propose to formulate surface
realisation as a sequence labelling task. We use
conditional random fields (Lafferty et al, 2001;
Sutton and McCallum, 2006), which are suitable
for modelling rich contexts, in combination with
semantic trees for rich linguistic information. This
combination is able to keep track of dependen-
cies between syntactic, semantic and lexical fea-
tures across multiple utterances. Our model can
be trained from minimally labelled data, which re-
duces development time and may (in the future)
facilitate an application to new domains.
The domain used in this paper is a pedestrian
walking around a city looking for information and
recommendations for local restaurants from an
SDS. We describe here the module for surface re-
alisation. Our main hypothesis is that the use of
global context in a CRF with semantic trees can
lead to surface realisations that are better phrased,
more natural and less repetitive than taking only
local features into account. Results from a human
rating study confirm this hypothesis. In addition,
we compare our system with alternative surface
realisation methods from the literature, namely, a
rank and boost approach and n-grams.
Finally, we argue that our approach lends itself
1254
to surface realisation within incremental systems,
because CRFs are able to model context across
full as well as partial generator inputs which may
undergo modifications during generation. As a
demonstration, we apply our model to incremen-
tal surface realisation in a proof-of-concept study.
2 Related Work
Our approach is most closely related to Lu et
al. (2009) who also use CRFs to find the best
surface realisation from a semantic tree. They
conclude from an automatic evaluation that using
CRF-based generation which takes long-range de-
pendencies into account outperforms several base-
lines. However, Lu et al?s generator does not take
context beyond the current utterance into account
and is thus restricted to local features. Further-
more, their model is not able to modify generation
results on the fly due to new or updated inputs.
In terms of surface realisation from graphical
models (and within the context of SDSs), our ap-
proach is also related to work by Georgila et al
(2002) and Dethlefs and Cuaya?huitl (2011b), who
use HMMs, Dethlefs and Cuaya?huitl (2011a) who
use Bayes Nets, and Mairesse et al (2010) who
use Dynamic Bayes Nets within an Active Learn-
ing framework. The last approach is also con-
cerned with generating restaurant recommenda-
tions within an SDS. Specifically, their system op-
timises its performance online, during the interac-
tion, by asking users to provide it with new textual
descriptions of concepts, for which it is unsure of
the best realisation. In contrast to these related
approaches, we use undirected graphical models
which are useful when the natural directionality
between the input variables is unknown.
In terms of surface realisation for SDSs, Oh and
Rudnicky (2000) present foundational work in us-
ing an n-gram-based system. They train a surface
realiser based on a domain-dependent language
model and use an overgeneration and ranking ap-
proach. Candidate utterances are ranked accord-
ing to a penalty function which penalises too long
or short utterances, repetitious utterances and ut-
terances which either contain more or less infor-
mation than required by the dialogue act. While
their approach is fast to execute, it has the dis-
advantage of not being able to model long-range
dependencies. They show that humans rank their
output equivalently to template-based generation.
Further, our approach is related to the SPaRKy
sentence generator (Walker et al, 2007). SPaRKy
was also developed for the domain of restaurant
recommendations and was shown to be equivalent
to or better than a carefully designed template-
based generator which had received high human
ratings in the past (Stent et al, 2002). It generates
sentences in two steps. First, it produces a ran-
domised set of alternative realisations, which are
then ranked according to a mapping from sentence
plans to predicted human ratings using a boosting
algorithm. As in our approach, SPaRKy distin-
guishes local and global features. Local features
take only information of the current tree node into
account, including its parents, siblings and chil-
dren, while global features take information of the
entire utterance into account. While SPaRKy is
shown to reach high output quality in compari-
son to a template-based baseline, the authors ac-
knowledge that generation with SPaRKy is rather
slow when applied in a real-time SDS. This could
present a problem in incremental settings, where
generation speed is of particular importance.
The SPaRKy system is also used by Rieser et
al. (2011), who focus on information presentation
strategies for restaurant recommendations, sum-
maries or comparisons within an SDS. Their sur-
face realiser is informed by the highest ranked
SPaRKy outputs for a particular information pre-
sentation strategy and will constitute one of our
baselines in the evaluation.
More work on trainable realisation for SDSs
generally includes Bulyko and Ostendorf (2002)
who use finite state transducers, Nakatsu and
White (2006) who use supervised learning, Varges
(2006) who uses chart generation, and Konstas
and Lapata (2012) who use weighted hypergraphs,
among others.
3 Cohesion across Utterances
3.1 Tree-based Semantic Representations
The restaurant recommendations we generate can
include any of the attributes shown in Table 1.
It is then the task of the surface realiser to find
the best realisation, including whether to present
them in one or several sentences. This often is
a sentence planning decision, but in our approach
it is handled using CRF-based surface realisation.
The semantic forms underlying surface realisation
can be produced in many ways. In our case, they
are produced by a reinforcement learning agent
which orders semantic attributes in the tree ac-
1255
Timing and Ordering
Surface Realisation
User
Interaction 
Micro-turn dialogue 
act, inform(food=Thai)
Semantic tree
String of words
intervening modules
speech
semantics of 
user utterance
(synthesised)
Manager
Figure 1: Architecture of our SDS with a focus on
the NLG components. While the user is speaking,
the dialogue manager sends dialogue acts to the
NLG module, which uses reinforcement learning
to order semantic attributes and produce a seman-
tic tree (see Dethlefs et al (2012b)). This paper fo-
cuses on surface realisation from these trees using
a CRF as shown in the surface realisation module.
Slot Example
ADDRESS The venue?s address is . . .
AREA It is located in . . .
FOOD The restaurant serves . . . cuisine.
NAME The restaurant?s name is . . .
PHONE The venue?s phone number is . . .
POSTCODE The postcode is . . .
QUALITY This is a . . . venue.
PRICE It is located in the . . . price range.
SIGNATURE The venue specialises in . . .
VENUE This venue is a . . .
Table 1: Semantic slots required for our domain
along with example realisations. Attributes can be
combined in all possible ways during generation.
cording to their confidence in the dialogue. This
is because SDSs can often have uncertainties with
regard to the user?s actual desired attribute values
due to speech recognition inaccuracies. We there-
fore model all semantic slots as probability distri-
butions, such as inform(food=Indian, 0.6) or in-
form(food=Italian, 0.4) and apply reinforcement
learning to finding the optimal sequence for pre-
sentation. Please see Dethlefs et al (2012b) for
details. Here, we simply assume that a semantic
form has been produced by a previous processing
module.
As shown in the architecture diagram in Fig-
ure 1, a CRF surface realiser takes a semantic
tree as input. We represent these as context-free
trees which can be defined formally as 4-tuples
Lexical
features
Syntactic
features
Semantic
features
The Beluga is a great Italian restaurant
y0 y1 y2
root
inform(
name=
Beluga)
The Beluga
root
inform(
venue=
Restaurant)
is a great Italian
inform(
type=
Italian)
root
restaurant
(a)
(b)
The 
Beluga
is a great
Italian
restaurant
other
phrases
(c)
Figure 2: (a) Graphical representation of a linear-
chain Conditional Random Field (CRF), where
empty nodes correspond to the labelled sequence,
shaded nodes to linguistic observations, and dark
squares to feature functions between states and ob-
servations; (b) Example semantic trees that are up-
dated at each time step in order to provide linguis-
tic features to the CRF (only one possible surface
realisation is shown and parse categories are omit-
ted for brevity); (c) Finite state machine of phrases
(labels) for this example.
{S, T,N,H}, where S is a start symbol, typically
the root node of the tree; T = {t0, t1, t2 . . . t|T |}
is a set of terminal symbols, corresponding to sin-
gle phrases; N = {n0, n1, n2 . . . n|N |} is a set of
non-terminal symbols corresponding to semantic
categories, and H = {h0, h1, h2 . . . h|H|} is a set
of production rules of the form n ? ?, where
n ? N , ? ? T ? N . The production rules rep-
resent alternatives at each branching node where
the CRF is consulted for the best available expan-
sion from the subset of possible ones. All nodes
in the tree are annotated with a semantic concept
(obtained from the semantic form) as well as their
parse category.
3.2 Conditional Random Fields for
Phrase-Based Surface Realisation
The main idea of our approach is to treat surface
realisation as a sequence labelling task in which a
sequence of semantic inputs needs to be labelled
with appropriate surface realisations. The task is
therefore to find a mapping between (observed)
1256
lexical, syntactic and semantic features and a (hid-
den) best surface realisation.
We use the linear-chain Conditional Random
Field (CRF) model for statistical phrase-based sur-
face realisation, see Figure 2 (a). This probabilis-
tic model defines the posterior probability of la-
bels (surface realisation phrases) y={y1, . . . , y|y|}
given features x={x1, . . . , x|x|} (informed by a se-
mantic tree, see Figure 2 (b)), as
P (y|x) = 1Z(x)
T?
t=1
exp
{ K?
k=1
?k?k(yt, yt?1, xt)
}
,
where Z(x) is a normalisation factor over all pos-
sible realisations (i.e. labellings) of x such that the
sum of all terms is one. The parameters ?k are
weights corresponding to feature functions ?k(.),
which are real values describing the label state y
at time t based on the previous label state yt?1 and
features xt. For example: from Figure 2 (c), ?k
might have the value ?k = 1.0 for the transition
from ?The Beluga? to ?is a great Italian?, and 0.0
elsewhere. The parameters ?k are set to maximise
the conditional likelihood of phrase sequences in
the training data set. They are estimated using the
gradient ascent algorithm.
After training, labels can be predicted for new
sequences of observations. The most likely phrase
sequence is expressed as
y ? = argmax
y
P (y|x),
which is computed using the Viterbi algorithm.
We use the Mallet package1 (McCallum, 2002) for
parameter learning and inference.
3.3 Feature Selection and Training
The following features define the generation con-
text used during training of the CRF. The genera-
tion context includes everything that has been gen-
erated for the current utterance so far. All features
can be obtained from a semantic input tree.
? Lexical items of parents and siblings,
? Semantic types in expansion,
? Semantic types of parents and siblings,
? Parse category of expansion,
? Parse categories of parents and siblings.
We use the StanfordParser2 (Marneffe et al, 2006)
to obtain the parse category for each tree node.
1http://mallet.cs.umass.edu/
2http://nlp.stanford.edu/software/
lex-parser.shtml
The semantics for each node are derived from the
input dialogue acts (these are listed in Table 1) and
are associated with nodes. The lexical items are
present in the generation context and are mapped
to semantic tree nodes.
As an example, for generating an utterance (la-
bel sequence) such as The Beluga is a great restau-
rant. It is located in the city centre., each gen-
eration step needs to take the features of the en-
tire generation history into account. This includes
all individual lexical items generated, the seman-
tic types used and the parse categories for each
tree node involved. For the first constituent, The
Beluga, this corresponds to the features {? BE-
GIN NAME} indicating the beginning of a sentence
(where empty features are omitted), the beginning
of a new generation context and the next semantic
slot required. For the second constituent, is a great
restaurant, the features are {THE BELUGA NAME
NP VENUE}, i.e. including the generation history
(with lexical items and parse category added for
the first constituent) and the semantics of the next
required slot, VENUE. In this way, a sequence of
surface form constituents is generated correspond-
ing to latent states in the CRF.
Since global utterance features capture the full
generation context (i.e. beyond the current ut-
terance), we are also able to model phenomena
such as co-references and pronouns. This is useful
for longer restaurant recommendations which may
span over more than one utterance. If the genera-
tion history already contains a semantic attribute,
e.g. the restaurant name, the CRF may afterwards
choose a pronoun, e.g. it, which has a higher like-
lihood than using the proper name again. Simi-
larly, the CRF may decide to realise a new attribute
as constituents of different order, such as a sen-
tence or PP, depending on the length, number and
parse categories of previously generated output. In
this way, our approach implicitly treats sentence
planning decisions such as the distribution of con-
tent over a set of messages in the same way as (or
as part of) surface realisation. A further capabil-
ity of our surface realiser is that it can generate
complete phrases from full as well as partial dia-
logue acts. This is useful in interactive contexts,
where we need as much robustness as possible. A
demonstration of this is given in Section 5 in an
application to incremental surface realisation.
To train the CRF, we used a data set of 552
restaurant recommendations from the website The
1257
List.3 The data contains recommendations such as
Located in the city centre, Beluga is a stylish yet
laid-back restaurant with a smart menu of modern
European cuisine.
3.4 Grammar Induction
The grammar g of surface realisation candidates
is obtained through an automatic grammar induc-
tion algorithm which can be run on unlabelled
data and requires only minimal human interven-
tion. This grammar defines the surface realisa-
tion space for the CRFs. We provide the human
corpus of restaurant recommendations from Sec-
tion 3.3 as input to grammar induction. The al-
gorithm is shown in Algorithm 1. It first identi-
fies all semantic attributes of interest in an utter-
ance, in our case those specified in Table 1, and re-
places them by a variable. These attributes include
food types, such as Mexican, Chinese, particular
parts of town, prices, etc. About 45% of them can
be identified based on heuristics. The remainder
needs to be hand-annotated at the moment, which
includes mainly attributes like restaurant names or
quality attributes, such as delicate, exquisite, etc.
Subsequently, all utterances are parsed using the
Stanford parser to obtain constituents and are inte-
grated into the grammar under construction. The
non-terminal symbols are named after the auto-
matically annotated semantic attributes contained
in their expansion, e.g. NAME QUALITY ? The
$name$ is of $quality$ quality. In this way, each
non-terminal symbol has a semantic representa-
tion and an associated parse category. In total, our
induced grammar contains more than 800 rules.
4 Evaluation
To evaluate our approach, we focus on a sub-
jective human rating study which aims to deter-
mine whether CRF-based surface realisation that
takes the full generation context into account,
called CRF (global), is perceived better by human
judges than one that uses a CRF but just takes local
context into account, called CRF (local). While
CRF (global) uses features from the entire genera-
tion history, CRF (local) uses only features from
the current tree branch. We assume that cohe-
sion can be identified by untrained judges as natu-
ral, well-phrased and non-repetitive surface forms.
To examine differences in methodology between
3http://www.list.co.uk
Algorithm 1 Grammar Induction.
1: function FINDGRAMMAR(utterances u, semantic at-
tributes a) return grammar
2: for each utterance u do
3: if u contains a semantic attribute from a, such as
venue, cuisine, etc. then
4: Find and replace the attribute by its semantic
variable, e.g. $venue$.
5: end if
6: Parse the sentence and induce a set of rules ??
?, where ? is a semantic variable and ? is its parse.
7: Traverse the parse tree in a top-down, depth-first
search and
8: if expansion ? exists then
9: continue
10: else if non-terminal ? exists then
11: add new expansion ? to ?.
12: else write new rule ?? ?.
13: end if
14: Write grammar.
15: end for
16: end function
CRFs and other state-of-the-art methods, we also
compare our system to two other baselines:
? CLASSiC corresponds to the system re-
ported in Rieser et al (2011),4 which gen-
erates restaurant recommendations based on
the SPaRKy system (Walker et al, 2007), and
has received high ratings in the past. SPaRKy
uses global utterance features.
? n-grams represents a simple 5-gram baseline
that is similar to Oh and Rudnicky (2000)?s
system. We will sample from the most likely
slot realisations that do not contain a repeti-
tion and include exactly the required slot val-
ues. Local context only is taken into account.
4.1 Human Rating Study
We carried out a user rating study on the Crowd-
Flower crowd sourcing platform.5 Each partici-
pant was shown part of a real human-system dia-
logue that emerged as part of the CLASSiC project
evaluation (Rieser et al, 2011). All dialogues
and data are freely available from http://www.
classic-project.org. Each dialogue contained
two variations for one of the utterances. These
variations were generated from two out of the four
systems described above. The order that these
were presented to the participant was counterbal-
anced. Table 2 gives an example of a dialogue seg-
ment presented to the participants.
4In Rieser et al (2011), this system is referred to as the
TIP system, which generates summaries, comparisons or rec-
ommendations for restaurants. For the present study, we com-
1258
SYS Thank you for calling the Cambridge Information
system. Your call will be recorded for research pur-
poses. You may ask for information about a place
to eat, such as a restaurant, a pub, or a cafe. How
may I help you?
USR I want to find an American restaurant which is in
the very expensive area.
SYS
A
The restaurant Gourmet Burger is an outstanding,
expensive restaurant located in the central area.
SYS
B
Gourmet Burger is a smart and welcoming restau-
rant. Gourmet Burger provides an expensive dining
experience with great food and friendly service. If
you?re looking for a central meal at an expensive
price.
USR What is the address and phone number?
SYS Gourmet Burger is on Regent Street and its phone
number is 01223 312598.
USR Thank you. Good bye.
Table 2: Example dialogue for participants to
compare alternative outputs in italics, USR=user,
SYS A=CRF (global), SYS B=CRF(local).
System Natural Phrasing Repetit.
CRF global 3.65 3.64 3.65
CRF local 3.10? 3.19? 3.13?
CLASSiC 3.53? 3.59 3.48?
n-grams 3.01? 3.09? 3.32?
Table 3: Subjective user ratings. Significance with
CRF (global) at p<0.05 is indicated as ?.
44 participants gave a total of 1,830 ratings of
utterances produced across the four systems. Flu-
ent speakers of English only were requested and
the participants were from the United States. They
were asked to rate each utterance on a 5 point Lik-
ert scale in response to the following questions
(where 5 corresponds to totally agree and 1 cor-
responds to totally disagree):
? The utterance was natural, i.e. it could have
been produced by a human. (Natural)
? The utterance was phrased well. (Phrasing)
? The utterance was repetitive. (Repetitive)
4.2 Results
We can see from Table 3 that across all the cate-
gories, the CRF (global) gets the highest overall
ratings. This difference is significant for all cat-
egories compared with CRF (local) and n-grams
(using a 1-sided Mann Whitney U-test, p < 0.001).
pare only with the subset of recommendations.
5http://www.crowdflower.com
Possibly this is because the local context taken
into account by both systems was not enough to
ensure cohesion across surface phrases. It is not
possible, e.g., to cover co-references within a lo-
cal context only or discourse markers that refer be-
yond the current utterance. This can lead to short
and repetitive phrases, such as Make your way to
Gourmet Burger. The food quality is outstanding.
The prices are expensive. generated by the n-gram
baseline.
The CLASSiC baseline, based on SPaRKy, was
the most competitive system in our comparison.
None-the-less CRF (global) is rated higher across
categories and significantly so for Natural (p <
0.05) and Repetitive (p < 0.005). For Phrasing,
there is a trend but not a significant difference (p
< 0.16). All comparisons are based on a 1-sided
Mann Whitney U-test. A qualitative comparison
between the CRF (global) and CLASSiC outputs
showed the following. CLASSiC utterances tend
to be longer and contain more sentences than CRF
(global) utterances. While CRF (global) often de-
cides to aggregate attributes into one sentence,
such as the Beluga is an outstanding restaurant
in the city centre, CLASSiC tends to rely more on
individual messages, such as The Beluga is an out-
standing restaurant. It is located in the city cen-
tre. A possible reason is that while CRF (global)
is able to take features beyond an utterance into
account, CLASSiC/SPaRKy is restricted to global
features of the current utterance.
We can further compare our results with Rieser
et al (2011) and Mairesse et al (2010) who also
generate restaurant recommendations and asked
similar questions to participants as we did. Rieser
et al (2011)?s system received an average rating
of 3.586 in terms of Phrasing which compares to
our 3.64. This difference is not significant, and
in line with the user ratings we observed for the
CLASSiC system above (3.59). Mairesse et al
(2010) achieved an average score of 4.05 in terms
of Natural in comparison to our 3.65. This differ-
ence is significant at p<0.05. Possibly their better
performance is due to the data set being more ?in
domain? than ours. They collected data from hu-
mans that was written specifically for the task that
the system was tested on. In contrast, our system
was trained on freely available data that was writ-
ten by professional restaurant reviewers. Unfortu-
nately, we cannot compare across other categories,
6This was rescaled from a 1-6 scale.
1259
USR1 I?m looking for a nice restaurant in the centre.
SYS1 inform(area=centre [0.2], food=Thai [0.3])
inform(name=Bangkok [0.3])
So you?re looking for a Thai . . .
USR2 [barges in] No, I?m looking for a restaurant
with good quality food.
SYS2 inform(quality=good [0.6], name=Beluga [0.6])
Oh sorry, so a nice restaurant located . . .
USR3 [barges in] . . . in the city centre.
SYS3 inform(area=centre [0.8])
Table 4: Example dialogue where the dialogue
manager needs to send incremental updates to the
NLG. Incremental surface realisation from seman-
tic trees for this dialogue is shown in Figure 3.
because the authors tested only for Phrasing and
Natural, respectively.
5 Incremental Surface Realisation
Recent years have seen increased interest in
incremental dialogue processing (Skantze and
Schlangen, 2009; Schlangen and Skantze, 2009).
The main characteristic of incremental architec-
tures is that instead of waiting for the end of a user
turn, they begin to process the input stream as soon
as possible, updating their processing hypotheses
as more information becomes available. From a
dialogue perspective, they can be said to work on
partial rather than full dialogue acts.
With respect to surface realisation, incremen-
tal NLG systems have predominantly relied on
pre-defined templates (Purver and Otsuka, 2003;
Skantze and Hjalmarsson, 2010; Dethlefs et al,
2012a), which limits the flexibility and quality of
output generation. Buschmeier et al (2012) have
presented a system which systematically takes
the user?s acoustic understanding problems into
account by pausing, repeating or re-phrasing if
necessary. Their approach is based on SPUD
(Stone et al, 2003), a constraint satisfaction-based
NLG architecture and marks important progress
towards more flexible incremental surface realisa-
tion. However, given the human labour involved in
constraint specification, cohesion is often limited
to a local context. Especially for long utterances
or such that are separated by user turns, this may
lead to surface form increments that are not well
connected and lack cohesion.
5.1 Application to Incremental SR
This section will discuss a proof-of-concept appli-
cation of our approach to incremental surface re-
alisation. Table 4 shows an example dialogue be-
tween a user and system that contains a number
of incremental phenomena that require hypothe-
sis updates, system corrections and user barge-
ins. Incremental surface realisation for this dia-
logue is shown in Figure 3, where processing steps
are indicated as bold-face numbers and are trig-
gered by partial dialogue acts that are sent from
the dialogue manager, such as inform(area=centre
[0.2]). The numbers in square brackets indicate
the system?s confidence in the attribute-value pair.
Once a dialogue act is observed by the NLG sys-
tem, a reinforcement learning agent determines the
order of attributes and produces a semantic tree, as
described in Section 3.1. Since the semantic forms
are constructed incrementally, new tree nodes can
be attached to and deleted from an existing tree,
depending on what kind of update is required.
In the dialogue in Table 4, the user first asks
for a nice restaurant in the centre. The dialogue
manager constructs a first attribute-value slot, in-
form(area=centre [0.2], . . . ), and passes it on to
NLG.7 In Figure 3, we can observe the corre-
sponding NLG action, a first tree is created with
just a root node and a node representing the area
slot (step 1). In a second step, the semantically
annotated node gets expanded into a surface form
that is chosen from a set of candidates (shown in
curly brackets). The CRF is responsible for this
last step. Since there is no preceding utterance, the
best surface form is chosen based on the semantics
alone. Active tree nodes, i.e. those currently under
generation, are indicated as asterisks in Figure 3.
Currently inactive nodes are shown as circles.
Step 3 then further expands the current tree
adding a node for the food type and the name of
a restaurant that the dialogue manager had passed.
We see here that attributes can either be primitive
or complex. Primitive attributes contain a single
semantic type, such as area, whereas complex at-
tributes contain multiple types, such as food, name
and need to be decomposed in a later processing
step (see steps 4 and 6). Step 5 again uses the CRF
7Note here that the information passed on to the NLG is
distinct from the dialogue manager?s own actions. In the ex-
ample, the NLG is asked to generate a recommendation, but
the dialogue manager actually decides to clarify the user?s
preferences due to low confidence. This scenario is an exam-
ple of generator inputs that may get revised afterwards.
1260
root
(1) inform
(area=centre)
(2) Right in the city centre,
{located in $area$, if 
you're looking to eat 
in $area$, in $area$, ...} 
inform(area=
centre)
(3) inform(food=Thai
        name=Bangkok)
Right in the city centre, 
root
(6) inform
(food=Thai)
(4) inform(name=
              Bangkok)
(5) Bangkok
{the $name$, 
it is called $name$,  ...}
root
inform(area=
centre)
Right in the city centre, 
inform(food=Thai, 
name=Bangkok)
root
inform(area=
centre)
Right in the city centre, 
(7) inform(quality=very
good, name=Beluga)
inform(name=
        Bangkok)
inform
(food=Thai)
Bangkok
root
inform(area=
centre)
inform(quality=nice, 
name=Beluga)
Right in the city centre, 
(8) inform(name=
Beluga)
(10) inform(quality=
very good)
(9) the Beluga
{$name$, the venue 
called $name$, ...}
(11) is of very good quality. 
{is a $quality$ venue, if you want $quality$ 
food, $quality$, a $quality$ place ...}
*
*
*
*
*
* *
**
*
**
*
Figure 3: Example of incremental surface realisation, where each generation step is indicated by a num-
ber. Active generation nodes are shown as asterisks and deletions are shown as crossed out. Lexical and
semantic features are associated with their respective nodes. Syntactic information in the form of parse
categories are also taken into account for surface realisation, but have been omitted in this figure.
to obtain the next surface realisation that connects
with the previous one (so that a sequence of real-
isation ?labels? appears: Right in the city centre
and Bangkok). It takes the full generation context
into account to ensure a globally optimal choice.
This is important, because the local context would
otherwise be restricted to a partial dialogue act,
which can be much smaller than a full dialogue
act and thus lead to short, repetitive sentences.
The dialogue continues as the system implicitly
confirms the user?s preferred restaurant (SYS1).
At this point, we encounter a user barge-in correct-
ing the desired choice. As a consequence, the dia-
logue manager needs to update its initial hypothe-
ses and communicate this to NLG. Here, the last
three tree nodes need to be deleted from the tree
because the information is no longer valid. This
update and the deletion is shown in step 7. After-
wards, the dialogue continues and NLG involves
mainly expanding the current tree into a full se-
quence of surface realisations for partial dialogue
acts which come together into a full utterance.
This example illustrates three incremental pro-
cessing steps: expansions, updates and deletions.
Expansions are the most frequent operation. They
add new partial dialogue acts to the semantic tree.
They also consult the CRF for the best surface
realisation. Since CRFs are not restricted by the
Markov condition, they are less constrained by lo-
cal context than other models and can take non-
local dependencies into account. For our applica-
tion, the maximal context is 9 semantic attributes
(for a surface form that uses all possible 10 at-
tributes). While their extended context aware-
ness can often make CRFs slow to train, they are
fast at execution and therefore very applicable to
the incremental scenario. For applications involv-
ing longer-spanning alternatives, such as texts or
paragraphs, the context of the CRF would likely
have to be constrained. Updates are triggered by
the hypothesis updates of the dialogue manager.
Whenever a new attribute comes in, it is checked
against the generator?s existing knowledge. If it
is inconsistent with previous knowledge, an up-
date is triggered and often followed by a deletion.
Whenever generated output needs to be modified,
old expansions and surface forms are deleted first,
before new ones can be expanded in their place.
5.2 Updates and Processing Speed Results
Since fast responses are crucial in incremental sys-
tems, we measured the average time our system
took for a surface realisation. The time is 100ms
on a MacBook Intel Core 2.6 Duo with 8GB in
1261
RAM. This is slightly better than other incremen-
tal systems (Skantze and Schlangen, 2009) and
much faster than state-of-the-art non-incremental
systems such as SPaRKy (Walker et al, 2007).
In addition, we measured the number of neces-
sary generation updates in comparison to a non-
incremental setting. Since updates take effect di-
rectly on partial dialogue acts, rather than the full
generated utterance, we require around 50% less
updates as if generating from scratch for every
changed input hypothesis. A qualitative analysis
of the generated outputs showed that the quality is
comparable to the non-incremental case.
6 Conclusion and Future Directions
We have presented a novel technique for surface
realisation that treats generation as a sequence la-
belling task by combining a CRF with tree-based
semantic representations. An essential property
of interactive surface realisers is to keep track of
the utterance context including dependencies be-
tween linguistic features to generate cohesive ut-
terances. We have argued that CRFs are well
suited for this task because they are not restricted
by independence assumptions. In a human rating
study, we confirmed that judges rated our output
as better phrased, more natural and less repetitive
than systems that just take local features into ac-
count. This also holds for a comparison with state-
of-the-art rank and boost or n-gram approaches.
Keeping track of the global context is also impor-
tant for incremental systems since generator inputs
can be incomplete or subject to modification. In a
proof-of-concept study, we have argued that our
approach is applicable to incremental surface real-
isation. This was supported by preliminary results
on the speed, number of updates and quality dur-
ing generation. As future work, we plan to test
our model in a task-based setting using an end-to-
end SDS in an incremental and non-incremental
setting. This study will contain additional evalu-
ation categories, such as the understandability or
informativeness of system utterances. In addition,
we may compare different sequence labelling al-
gorithms for surface realisation (Nguyen and Guo,
2007) or segmented CRFs (Sarawagi and Cohen,
2005) and apply our method to more complex sur-
face realisation domains such as text generation or
summarisation. Finally, we would like to explore
methods for unsupervised data labelling so as to
facilitate portability across domains further.
Acknowledgements
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE).
References
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533?550.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Incremental Language Generation and Incremental
Speech Synthesis. In Proceedings of the 13th An-
nual SigDial Meeting on Discourse and Dialogue
(SIGdial), Seoul, South Korea.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011a. Com-
bining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Genera-
tion in Situated Dialogue. In Proceedings of the 13th
European Workshop on Natural Language Genera-
tion (ENLG), Nancy, France.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011b.
Hierarchical Reinforcement Learning and Hidden
Markov Models for Task-Oriented Natural Lan-
guage Generation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), Portland, Oregon, USA.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of the International Con-
ference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
Kallirroi Georgila, Nikos Fakotakis, and George
Kokkinakis. 2002. Stochastic Language Modelling
for Recognition and Generation in Dialogue Sys-
tems. TAL (Traitement automatique des langues)
Journal, 43(3):129?154.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text Generation via Discriminative Reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378, Jeju Island, Korea.
John D. Lafferty, Andrew McCallum, and Fer-
nando C.N. Pereira. 2001. Conditional Random
1262
Fields: Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing (ICML), pages 282?289.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009.
Natural Language Generation with Tree Conditional
Random Fields. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
Franc?ois Mairesse, Filip Jurc???c?ek, Simon Keizer,
Blaise Thomson, Kai Yu, and Steve Young. 2010.
Phrase-Based Statistical Language Generation Us-
ing Graphical Models and Active Learning. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL), Uppsala,
Sweden.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Crystal Nakatsu and Michael White. 2006. Learning
to Say It Well: Reranking Realizations by Predicted
Synthesis Quality. In In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL) 2006, pages 1113?1120,
Sydney, Australia.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of Sequence Labeling Algorithms and Extensions.
In Proceedings of the International Conference on
Machine Learning (ICML), Corvallis, OR, USA.
Alice Oh and Alexander Rudnicky. 2000. Stochas-
tic Language Generation for Spoken Dialogue Sys-
tems. In Proceedings of the ANLP/NAACL Work-
shop on Conversational Systems, pages 27?32, Seat-
tle, Washington, USA.
Matthew Purver and Masayuki Otsuka. 2003. In-
cremental Generation by Incremental Parsing. In
In Proceedings of the 6th UK Special-Interesting
Group for Computational Linguistics (CLUK) Col-
loquium.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Sunita Sarawagi and William Cohen. 2005. Semi-
Markov Conditional Random Fields for Information
Extraction. Advances in Neural Information Pro-
cessing.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual SigDial
Meeting on Discourse and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Athens, Greece.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored Generation
for Spoken Dialogue: An Experiment. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with Communicative Intentions: The SPUD
System. Computational Intelligence, 19:311?381.
Charles Sutton and Andrew McCallum. 2006. Intro-
duction to Conditional Random Fields for Relational
Learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Sebastian Varges. 2006. Overgeneration and Ranking
for Spoken Dialogue Systems. In Proceedings of the
Fourth International Natural Language Generation
Conference (INLG), Sydney, Australia.
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
1263
Hierarchical Reinforcement Learning for Adaptive Text Generation
Nina Dethlefs
University of Bremen, Germany
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
University of Bremen, Germany
heriberto@uni-bremen.de
Abstract
We present a novel approach to natural lan-
guage generation (NLG) that applies hierar-
chical reinforcement learning to text genera-
tion in the wayfinding domain. Our approach
aims to optimise the integration of NLG tasks
that are inherently different in nature, such
as decisions of content selection, text struc-
ture, user modelling, referring expression gen-
eration (REG), and surface realisation. It
also aims to capture existing interdependen-
cies between these areas. We apply hierar-
chical reinforcement learning to learn a gen-
eration policy that captures these interdepen-
dencies, and that can be transferred to other
NLG tasks. Our experimental results?in a
simulated environment?show that the learnt
wayfinding policy outperforms a baseline pol-
icy that takes reasonable actions but without
optimization.
1 Introduction
Automatic text generation involves a number of sub-
tasks. (Reiter and Dale, 1997) list the following as
core tasks of a complete NLG system: content se-
lection, discourse planning, sentence planning, sen-
tence aggregation, lexicalisation, referring expres-
sion generation and linguistic realisation. However,
decisions made for each of these core tasks are not
independent of each other. The value of one gen-
eration task can change the conditions of others,
as evidenced by studies in corpus linguistics, and
it can therefore be undesirable to treat them all as
isolated modules. In this paper, we focus on inter-
related decision making in the areas of content se-
lection, choice of text structure, referring expression
and surface form. Concretely, we generate route in-
structions that are tailored specifically towards dif-
ferent user types as well as different environmental
features. In addition, we aim to balance the degree
of variation and alignment in texts and produce lex-
ical and syntactic patterns of co-occurrence that re-
semble those of human texts of the same domain.
Evidence for the importance of this is provided by
(Halliday and Hasan, 1976) who note the way that
lexical cohesive ties contribute to text coherence as
well as by the theory of interactive alignment. Ac-
cording to (Pickering and Garrod, 2004) we would
expect significant traces of lexical and syntactic self-
alignment in texts.
Approaches to NLG in the past have been ei-
ther rule-based (Reiter and Dale, 1997) or statisti-
cal (Langkilde and Knight, 1998). However, the for-
mer relies on a large number of hand-crafted rules,
which makes it infeasible for controlling a large
number of interrelated variables. The latter typi-
cally requires training on a large corpus of the do-
main. While these approaches may be better suitable
for larger domains, for limited domains such as our
own, we propose to overcome these drawbacks by
applying Reinforcement Learning (RL)?with a hi-
erarchical approach. Previous work that has used RL
for NLG includes (Janarthanam and Lemon, 2009)
who employed it for alignment of referring expres-
sions based on user models. Also, (Lemon, 2008;
Rieser and Lemon, 2009) used RL for optimising
information presentation styles for search results.
While both approaches displayed significant effects
of adaptation, they focused on a single area of opti-
misation. For larger problems, however, such as the
one we are aiming to solve, flat RL will not be appli-
cable due to the large state space. We therefore sug-
gest to divide the problem into a number of subprob-
lems and apply hierarchical reinforcement learning
(HRL) (Barto and Mahadevan, 2003) to solve it.
We describe our problem in more detail in Sec-
tion 2, our proposed HRL architecture in Sections
3 and 4 and present some results in Section 5. We
show that our learnt policies outperform a baseline
that does not adapt to contextual features.
2 Generation tasks
Our experiments are all drawn from an indoor
navigation dialogue system which provides users
with route instructions in a university building and
is described in (Cuaya?huitl et al, 2010). We aim
to optimise generation within the areas of content
selection, text structure, referring expression gener-
ation and surface realisation.
Content Selection Content selection decisions
are subject to different user models. We distin-
guish users who are familiar with the navigation
environment and users who are not. In this way,
we can provide different routes for these users
corresponding to their particular information need.
Specifically, we provide more detail for unfamiliar
than familiar users by adding any or several of
the following: (a) landmarks at decision points,
(b) landmarks lying on long route segments, (c)
specifications of distance.
Text Structure Depending on the type of user
and the length of the route, we choose among three
different text generation strategies to ease the cogni-
tive load of the user. Examples of all strategies are
displayed in Table 1. All three types resulted from
an analysis of a corpus of 24 human-written driving
route instructions. We consider the first type (se-
quential) most appropriate for long or medium-long
routes and both types of user. The second type (tem-
poral) is appropriate for unfamiliar users and routes
of short or medium length. It divides the route into
an explicit sequence of consecutive actions. The
third type (schematic) is used in the remaining cases.
Referring Expression Generation We dis-
tinguish three types of referring expressions:
common names, familiar names and descriptions.
In this way, entities can be named according to
the users? prior knowledge. For example, one
and the same room can be called either ?the
student union room?, ?room A3530? or ?the room
right at the corner beside the entrance to the terrace?.
Surface Realisation For surface realisation, we
aim to generate texts that display a natural balance
of (self-)alignment and variation. While it is a rule
of writing that texts should typically contain varia-
tion of surface forms in order not to appear repetitive
and stylistically poor, there is evidence that humans
also get influenced by self-alignment processes dur-
ing language production. Specifically, (Garrod and
Anderson, 1987; Pickering and Garrod, 2004) ar-
gue that the same mental representations are used
during language production and comprehension, so
that alignment occurs regardless of whether the last
utterance was made by another person or by the
speaker him- or herself (for experimental evidence
see (Branigan et al, 2000; Bock, 1986)). We can
therefore hypothesise that coherent texts will, be-
sides variation, also display a certain degree of self-
alignment. In order to determine a proper balance
of alignment and variation, we computed the degree
of lexical repetition from our corpus of 24 human
route descriptions. This analysis was based on (Hirst
and St-Onge, 1998) who retrieve lexical chains from
texts by identifying a number of relations between
lexical items. We focus here exclusively on Hirst
& St-Onge?s ?extra-strong? relations, since these can
be computed from shallow properties of texts and do
not require a large corpus of the target domain. In
order to make a fair comparison between the human
texts and our own, we used a part-of-speech (POS)
tagger (Toutanova and Manning, 2000)1 to extract
those grammatical categories that we aim to control
within our framework, i.e. nouns, verbs, preposi-
tions, adjectives and adverbs. Based on these cat-
egories, we compute the proportion of tokens that
are members in lexical chains, the ?alignment score?
(AS), according to the following equation:
AS = Lexical tokens in chains
Total number of tokens ? 100. (1)
We obtained an average alignment score of 43.3%
for 24 human route instructions. In contrast, the
1http://nlp.stanford.edu/software/tagger.shtml
Table 1: Different text generation strategies for the same underlying route.
Type 1: Sequential Type 2: Temporal Type 3: Schematic
Turn around, and go straight First, turn around. Second, - Turn around.
to the glass door in front of go straight to the glass door - Go straight until the glass door in front
you. Turn right, then follow in front of you. Third, turn of you. (20 m)
the corridor until the lift. It right. Fourth, follow the - Turn right
will be on your left-hand corridor until the lift. It will - Follow the corridor until the lift. (20 m)
side. be on your left-hand side. - It will be on your left-hand side.
same number of instructions generated by Google
Maps yielded 78.7%, i.e. an almost double amount
of repetition. We will therefore train our agent
to generate texts with an about medium alignment
score.
3 Hierarchical Reinforcement Learning
for NLG
The idea of text generation as an optimization
problem is as follows: given a set of genera-
tion states, a set of actions, and an objective
reward function, an optimal generation strategy
maximizes the objective function by choosing the
actions leading to the highest reward for every
reached state. Such states describe the system?s
knowledge about the generation task (e.g. con-
tent selection, text structure, REG, surface realiza-
tion). The action set describes the system?s ca-
pabilities (e.g. expand sequential aggregation, ex-
pand schematic aggregation, expand lexical items,
etc.). The reward function assigns a numeric value
for each taken action. In this way, text generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, where
the goal is to find an optimal strategy automatically.
To do that we use hierarchical reinforcement learn-
ing in order to optimize a hierarchy of text genera-
tion policies rather than a single policy.
The hierarchy of RL agents consists of L lev-
els and N models per level, denoted as M = M ij ,
where j ? {0, ..., N ? 1} and i ? {0, ..., L ? 1}.
Each agent of the hierarchy is defined as a Semi-
Markov Decision Process (SMDP) consisting of a
4-tuple < Sij, Aij , T ij , Rij >. Sij is a set of states, Aij
is a set of actions, and T ij is a transition function that
determines the next state s? from the current state
s and the performed action a with a probability of
P (s?|s, a). Rij(s?, ? |s, a) is a reward function that
specifies the reward that an agent receives for taking
an action a in state s at time ? . Since SMDPs allow
for temporal abstraction, that is, actions may take a
variable number of time steps to complete, the ran-
dom variable ? represents this number of time steps.
Actions can be either primitive or composite. The
former yield single rewards, the latter (executed us-
ing a stack mechanism) correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optional policy ?? that max-
imises the reward for each visited state, according to
??ij(s) = arg maxa?A Q
?i
j(s, a). (2)
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing ??. For learning a generation policy, we
use hierarchical Q-Learning (HSMQ) (Dietterich,
1999). The dynamics of SMDPs are as follows:
when an SMDP terminates its execution, it is popped
off the stack of models to execute, and control is
transferred to the next available SMDP in the stack,
and so on until popping off the root SMDP. An
SMDP terminates when it reaches one of its termi-
nal states. This algorithm is executed until the Q-
values of the root agent stabilize. The hierarchical
decomposition allows to find context-independent
policies with the advantages of policy reuse and fa-
cilitation for state-action abstraction. This hierarchi-
cal approach has been applied successfully to dia-
logue strategy learning (Cuayahuitl et al, 2010).
4 Experimental Setting
4.1 Hierarchy of SMDPs
The hierarchy consists of 15 agents. It is depicted
in Figure 1. The root agent is responsible for deter-
Figure 1: Hierarchy of agents for learning adaptive text generation strategies in the wayfinding domain
mining a route instruction type for a navigation situ-
ation. We distinguish turning, passing, locating, go-
ing and following instructions. It also chooses a text
generation strategy and the information structure of
the clause (i.e., marked or unmarked theme (Hall-
iday and Matthiessen, 2004)). Leaf agents are re-
sponsible for expanding constituents in which varia-
tion or alignment can occur, e.g. the choice of verb
or prepositional phrase.
4.2 State and action sets
We distinguish three kinds of state representations,
displayed in Table 2. The first (M010 and M10 ) en-
codes information on the spatial environment and
user type so that texts can be tailored towards these
variables. These variables play a major part in our
simulated environment (Section 5.1). The second
representation (M11 - M15 and M23 ) controls sentence
structure and ensures that all required constituents
for a message have been realised. The third (all re-
maining models) encodes variants of linguistic sur-
face structure and represents the degree of alignment
of all variants. We address the way that these align-
ment values are computed in Section 4.4. Actions
can be either primitive or composite. Whereas the
former expand a logical form directly, the latter cor-
respond to SMDPs at different levels of the hierar-
chy. All parent agents have both types of actions,
only the leaf agents have exclusively primitive ac-
tions. The set of primitive actions is displayed in Ta-
ble 2, all composite actions, corresponding to mod-
els, are shown in Figure 1. The average number of
state-action pairs for a model is |S ? A| = 77786.
While in the present work, the action set was de-
termined manually, future work can aim at learning
hierarchies of SMDPs automatically from data.
4.3 Prior Knowledge
Agents contain prior knowledge of two sorts. First,
the root agents and agents at the first level of the hi-
erarchy contain prior probabilities of executing cer-
tain actions. For example, given an unfamiliar user
and a long route, model M10 , text strategy, is initi-
ated with a higher probability of choosing a sequen-
tial text strategy than a schematic or temporal strat-
egy. Second, leaf agents of the hierarchy are initi-
ated with values of a hand-crafted language model.
These values indicate the probabilities of occurrence
of the different surface forms of the leaf agents listed
in Table 2. Both types of prior probabilities are used
by the reward functions described below.
4.4 Reward functions
We use two types of reward function, both of which
are directly motivated by the principles we stated in
Section 2. The first addresses interaction length (the
shorter the better) and the choice of actions tailored
towards the user model and spatial environment.
R =
?
?
?
0 for reaching the goal state
-10 for an already invoked subtask
p(a) otherwise
(3)
p(a) corresponds to the probability of the last ac-
tion given the current state, described above as prior
knowledge. The second reward function addresses
Table 2: State and action sets for learning adaptive text generation strategies in the wayfinding domain
Model State Variables Action Set
M00 text strategy (FV), info structure (FV), instruction (FV), expand text strategy (M10 ), turning (M23 ),
slot in focus(0=action, 1=landmark), user type(0=unfamiliar, going (M21 ), passing (M25 ), following (M22 ),
1=familiar) subtask termination(0=continue, 1=halt) locating instr.(M24 ), expand unmarked theme
M10 end(0=continue, 1=halt), text strategy (FV), route length expand schematic aggregation, expand sequen-
(0=short, 1=medium, 2=long), user type(0=unfam., 1=fam.) ce aggregation, expand temporal aggregation
M11 going vp (FV), limit (FV), SV expand going vp (M20 ), expand limit
M12 following vp (FV), SV, limit (FV) expand following vp (M21 ), expand limit
M13 turning location (FV), turning vp (FV), expand turning vp (M32 ), expand turning loc.,
SV, turning direction (FV) expand turning direction (M34 )
M14 np locatum (FV), locating vp (FV), expand np locatum, expand locating vp (M25 ),
static direction (FV), SV expand static dir. (M26 )
M15 np locatum (FV), passing vp (FV), SV, static direction (FV) expand pass. vp (M26 ), expand static dir. (M26 )
M20 vp go straight ahead, vp go straight, vp move straight ahead, Actions correspond to expansions of
vp walk straight ahead, vp walk straight (all AS) lexemes
M21 vp follow, vp go over, vp walk down, vp go down, Actions correspond to expansions of
vp go up, vp walk up, vp walk over (all AS) lexemes
M22 vp walk, vp veer , vp hang, vp bear (all AS), vp go, vp head, Actions correspond to expansions of
vp turn (all AS) lexemes
M23 identifiability(0=not id.,1=id.), user type(0=un-, expand relatum id., expand relatum, not id.,
fam.,1=fam,, relatum identifiability (FV), relatum name (FV) expand descriptive, expand common name
M24 pp nonphoric, pp nonphoric handedness, Actions correspond to expansions of
pp nonphoric poss, pp phoric pp nonphoric side (all AS) lexemes
M25 vp be, vp be located at, vp get to, vp see (all AS) Actions correspond to expansions of lexemes
M26 direction on, direction poss, direction to (all AS) Actions correspond to expansions of lexemes
M27 vp move past, vp pass, vp pass by, vp walk past (all AS) Actions correspond to expansions of lexemes
(FV = filling status): 0=unfilled, 1=filled. (SV = shared variables): the variables np actor (FV), relatum (FV),
sentence (FV) and information need (0=low, 1=high) are shared by several subagents; the same applies to their
corresponding expansion actions. (AS = alignment score): 0=unaligned, 1=low AS, 2=medium AS, 3=high AS.
the tradeoff between alignment and variation:
R =
?
?
?
0 for reaching the goal state
p(a) for medium alignment
-0.1 otherwise
(4)
Whilst the former reward function is used by the root
and models M10 - M15 and M22 , the latter is used by
models M20 - M21 and M23 - M27 . It rewards the agent
for a medium alignment score, which corresponds
to the score of typical human texts we computed
in Section 2. The alignment status of a constituent
is computed by the Constituent Alignment Score
(CAS) as follows, where MA stands for ?medium
alignment?.
CAS(a) = Count of occurrences(a)Occurences of a without MA (5)
From this score, we can determine the degree of
alignment of a constituent by assigning ?no align-
ment? for a constituent with a score of less than
0.25, ?low alignment? for a score between 0.25 and
0.5, ?medium alignment? for a score between 0.5 and
0.75 and ?high alignment? above. On the whole thus,
the agent?s task consists of finding a balance be-
tween choosing the most probable action given the
language model and choosing an action that aligns
with previous utterances.
5 Experiments and Results
5.1 Simulated Environment
The simulated environment encodes information on
the current user type (un-/familiar with the environ-
ment) and corresponding information need (low or
high), the length of the current route (short, medium-
long, long), the next action to perform (turn, go
straight, follow a path, pass a landmark or take note
of a salient landmark) and the current focus of at-
tention (the action to be performed or some salient
landmark nearby). Thus, there are five different state
variables with altogether 120 combinations, sam-
pled from a uniform distribution. This simple form
of stochastic behaviour is used in our simulated en-
vironment. Future work can consider inducing a
learning environment from data.
5.2 Comparison of learnt and baseline policies
In order to test our framework, we designed a sim-
ulated environment that simulates different naviga-
tional situations, routes of different lengths and dif-
ferent user types. We trained our HRL agent for
10.000 episodes with the following learning param-
eters: the step-size parameter ? was initiated with 1
and then reduced over time by ? = 11+t , t being the
time step. The discount rate parameter ? was 0.99
and the probability of random action ? was 0.01 (see
(Sutton and Barto, 1998) for details on these param-
eters). Figure 2 compares the learnt behaviour of
our agent with a baseline (averaged over 10 runs)
that chooses actions at random in models M10 and
M20 - M27 (i.e., the baseline does not adapt its text
strategy to user type or route length and neither per-
forms adaptation of referring expressions or align-
ment score). The user study reported in (Cuaya?huitl
et al, 2010) provided users with instruction using
this baseline generation behaviour. The fact that
users had a user satisfaction score of 90% indicates
that this is a sensible baseline, producing intelligi-
ble instructions. We can observe that after a certain
number of episodes, the performance of the trained
agent begins to stabilise and it consistently outper-
forms the baseline.
6 Example of generation
As an example, Figure 3 shows in detail the genera-
tion steps involved in producing the clause ?Follow
102 103 104
?65
?60
?55
?50
?45
?40
?35
?30
?25
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Learnt Behaviour
Baseline
Figure 2: Comparison of learnt and baseline behaviour in
the generation of route descriptions
the corridor until the copyroom? for an unfamiliar
user and a route of medium length. Generation starts
with the root agent in state (0,0,0,0,0,0), which in-
dicates that text strategy, info structure and instruc-
tion are unfilled slots, the slot in focus of the sen-
tence is an action, the status of subtask termination
is ?continue? and the user type is unfamiliar. After
the primitive action expand unmarked theme was
executed, the state is updated to (0,1,0,0,0,0), in-
dicating the filled slot. Next, the composite action
text strategy is executed, corresponding to model
M10 . The initial state (1,0,0) indicates a route
of medium length, an unfilled text strategy slot
and an unfamiliar user. After the primitive ac-
tion expand sequential text was chosen, the ter-
minal state is reached and control is returned to
the root agent. Here, the next action is follow-
ing instruction corresponding to model M12 . The
initial state (0,1,0,0,0,0) here indicates unfilled slots
for following vp, np actor, sentence, path, limit
and relatum, as well as a high information need
of the current user. The required constituents
are expanded in turn. First, the primitive actions
expand limit, expand np actor, expand s and ex-
pand path cause their respective slots in the state
representation to be filled. Next, the composite ac-
tion expand relatum is executed with an initial state
(0,1,0,0) representing an identifiable landmark, un-
filled slots for a determiner and a referring expres-
sion for the landmark and an unfamiliar user. Two
primitive actions, expand relatum identifiable and
expand relatum common name, cause the agent to
reach its terminal state. The generated referring ex-
pression thus treats the referenced entity as either
known or easily recoverable. Finally, model M21
executes the composite action expand following vp,
which is initialised with a number of variables cor-
responding to the alignment status of different verb
forms. Since this is the first time this agent is called,
none of them shows traces of alignment (i.e., all val-
ues are 0). Execution of the primitive action ex-
pand following vp causes the respective slot to be
updated and the agent to terminate. After this sub-
task, model M12 has also reached its terminal state
and control is returned to the root agent.
As a final step towards surface generation, all cho-
sen actions are transformed into an SPL (Kasper,
1989). The type ?following instruction? leads to the
initialisation of a semantically underspecified scaf-
fold of an SPL, all other actions serve to supplement
this scaffold to preselect specific syntactic structures
or lexical items. For example, the choice of ?ex-
pand following vp? leads to the lexical item ?fol-
low? being inserted. Similarly, the choice of ?ex-
pand path? leads to the insertion of ?the corridor?
into the SPL to indicate the path the user should fol-
low. ?expand limit?, in combination with the choice
of referring expression, leads to the insertion of the
PP ?until the copy room?. For generation of more
than one instruction, aggregation has to take place.
This is done by iterating over all instructions of a
text and inserting them into a larger SPL that re-
alises the aggregation. Finally, the constructed SPL
is passed to the KPML surface generator (Bateman,
1997) for string realisation.
7 Discussion
We have argued in this paper that HRL is an es-
pecially suited framework for generating texts that
are adaptive to different users, to environmental fea-
tures and properties of surface realisation such as
alignment and variation. While the former tasks ap-
pear intuitively likely to contribute to users? com-
prehension of texts, it is often not recognised that
the latter task can have the same effect. Differing
surface forms of identical concepts in texts without
motivation can lead to user confusion and deterio-
rate task success. This is supported by Clark?s ?prin-
ciple of contrast? (Clark, 1987), according to which
new expressions are only introduced into an interac-
tion when the speaker wishes to contrast them with
other entities already present in the discourse. Si-
miliarly, a study by (Clark and Wilkes-Gibbs, 1986)
showed that interlocutors tend to align their referring
expressions and thereby achieve more efficient and
successful dialogues. We tackled the integration of
different NLG tasks by applying HRL and presented
results, which showed to be promising. As an al-
ternative to RL, other machine learning approaches
may be conceivable. However, supervised learning
requires a large amount of training data, which may
not always be available, and may also produce un-
predictable behaviour in cases where a user deviates
from the behaviour covered by the corpus (Levin
et al, 2000). Both arguments are directly trans-
ferable to NLG. If an agent is able to act only on
grounds of what it has observed in a training cor-
pus, it will not be able to react flexibly to new state
representations. Moreover, it has been argued that
a corpus for NLG cannot be regarded as an equiv-
alent gold standard to the ones of other domains of
NLP (Belz and Reiter, 2006; Scott and Moore, 2006;
Viethen and Dale, 2006). The fact that an expres-
sion for a semantic concept does not appear in a cor-
pus does not mean that it is an unsuited or impos-
sible expression. Another alternative to pure RL is
to apply semi-learnt behaviour, which can be help-
ful for tasks with very large state-action spaces. In
this way, the state-action space is reduced to only
sensible state-action pairs by providing the agent
with prior knowledge of the domain. All remain-
ing behaviour continues to be learnt. (Cuaya?huitl,
2009) suggests such an approach for learning dia-
logue strategies, but again the principle is transfer-
able to NLG. While there is room for exploration
of different RL methods, it is clear that neither tra-
ditional rule-based accounts of generation, nor n-
gram-based generators can achieve the same flexible
generation behaviour given a large, and partially un-
known, number of state variables. Since state spaces
are typically very large, specifying rules for each
single condition is at best impractical. Especially for
tasks such as achieving a balanced alignment score,
as we have shown in this paper, decisions depend on
very fine-grained textual cues such as patterns of co-
occurrence which are hard to pin down accurately
by hand. On the other hand, statistical approaches
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M00 (0, 0, 0, 0, 0, 0)
{action = expand unmarked
theme}
M00 (0, 1, 0, 0, 0, 0)
{action = text strategy}
?
?
M10 (1, 0, 0)
{action = expand sequential text}
M10 (1, 1, 0), (terminalstate)
?
?
M00 , (1, 1, 0, 0, 0, 0),
{action = following instruction}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M21 (0, 1, 0, 0, 0, 0)
{action = expand limit}
M21 (0, 1, 1, 0, 0, 0)
{action = expand np actor}
M21 (0, 1, 1, 1, 0, 0)
{action = expand s}
M21 (0, 1, 1, 0, 1, 0)
{action = expand path}
M21 (0, 1, 1, 1, 1, 0)
{action = expand relatum}
?
?
?
?
?
?
?
?
?
M23 (0, 0, 0, 0)
{action = expand relatum
identifiable}
M23 (0, 1, 0, 0)
{action = expand relatum
common name}
M23 (0, 1, 1, 0), (terminalstate)
?
?
?
?
?
?
?
?
?
M21 (0, 1, 1, 1, 1, 0)
{action = expand following
vp}
?
?
?
?
M21 (0, 0, 0, 0, 0, 0, 0, 0, 0)
{action = follow}
M21 (1, 0, 0, 0, 0, 0, 0, 0, 0)
(terminalstate)
?
?
?
?
M21 (1, 1, 1, 1, 1, 1)(terminalstate)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
M00 (1, 1, 1, 0, 1, 0)(terminalstate)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Example of generation for the clause ?Follow the corridor until the copy room?. This example shows decision
making for a single instruction, adaptation and alignment occurs over longer sequences of text.
to generation that are based on n-grams focus on the
frequency of constructions in a corpus without tak-
ing contextual variables such as user type or environ-
mental properties into account. Further, they share
the problem of supervised learning approaches dis-
cussed above, namely, that it can act only on grounds
of what it has observed in the past, and are not well
able to adapt to novel situations. For a more de-
tailed account of statistical and trainable approaches
to NLG as well as their advantages and drawbacks,
see (Lemon, 2008).
8 Conclusion
We presented a novel approach to text generation
that applies hierarchical reinforcement learning to
optimise the following interrelated NLG tasks: con-
tent selection, choice of text structure, referring ex-
pressions and surface structure. Generation deci-
sions in these areas were learnt based on three differ-
ent variables: the type of user, the properties of the
spatial environment and the proportion of alignment
and variation in texts. Based on a simulated envi-
ronment, we compared the results of different poli-
cies and demonstrated that the learnt policy outper-
forms a baseline that chooses actions without taking
contextual variables into account. Future work can
transfer our approach to different domains of appli-
cation or to other NLG tasks. In addition, our pre-
liminary simulation results should be confirmed in
an evaluation study with real users.
Acknowledgements
This work was partly supported by DFG SFB/TR8
?Spatial Cognition?.
References
Barto, A. G. and Mahadevan, S. (2003). Recent Ad-
vances in Hierarchical Reinforcement Learning. Dis-
crete Event Dynamic Systems, 13:2003.
Bateman, J. A. (1997). Enabling technology for multi-
lingual natural language generation: the KPML devel-
opment environment. Natural Language Engineering,
3(1):15?55.
Belz, A. and Reiter, E. (2006). Comparing automatic and
human evaluation of nlg systems. In In Proc. EACL06,
pages 313?320.
Bock, K. (1986). Syntactic persistence in language pro-
duction. Cognitive Psychology, 18.
Branigan, H. P., Pickering, M. J., and Cleland, A. (2000).
Syntactic coordination in dialogue. Cognition, 75.
Clark, E. (1987). The principle of contrast: A constraint
on language acquisition. In MacWhinney, B., edi-
tor, Mechanisms of Language Acquisition, pages 1?33.
Lawrence Erlbaum Assoc., Hillsdale, NJ.
Clark, H. H. and Wilkes-Gibbs, D. (1986). Referring as
a colloborative process. Cognition, 22.
Cuaya?huitl, H. (2009). Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD thesis,
School of Informatics, University of Edinburgh.
Cuaya?huitl, H., Dethlefs, N., Richter, K.-F., Tenbrink, T.,
and Bateman, J. (2010). A dialogue system for indoor
wayfinding using text-based natural language. In-
ternational Journal of Computational Linguistics and
Applications, ISSN 0976-0962.
Cuayahuitl, H., Renals, S., Lemon, O., and Shimodaira,
H. (2010). Evaluation of a hierarchical reinforcement
learning spoken dialogue system. Computer Speech
and Language, 24(2):395?429.
Dietterich, T. G. (1999). Hierarchical reinforcement
learning with the maxq value function decomposition.
Journal of Artificial Intelligence Research, 13:227?
303.
Garrod, S. and Anderson, A. (1987). Saying What You
Mean in Dialogue: A Study in conceptual and seman-
tic co-ordination. Cognition, 27.
Halliday, M. A. K. and Hasan, R. (1976). Cohesion in
English. Longman, London.
Halliday, M. A. K. and Matthiessen, C. M. I. M. (2004).
An Introduction to Functional Grammar. Edward
Arnold, London, 3rd edition.
Hirst, G. and St-Onge, D. (1998). Lexical chains as rep-
resentations of context for the detection and correction
of malapropisms. In Fellbaum, C., editor, WordNet:
An Electronic Database and Some of its Applications,
pages 305?332. MIT Press.
Janarthanam, S. and Lemon, O. (2009). Learning lexi-
cal alignment policies for generating referring expres-
sions in spoken dialogue systems. In ENLG ?09: Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 74?81, Morristown, NJ,
USA.
Kasper, R. (1989). SPL: A Sentence Plan Language for
text generation. Technical report, USC/ISI.
Langkilde, I. and Knight, K. (1998). Generation that ex-
ploits corpus-based statistical knowledge. In ACL-36:
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
pages 704?710.
Lemon, O. (2008). Adaptive Natural Language Gener-
ation in Dialogue using Reinforcement Learning. In
SemDial.
Levin, E., Pieraccini, R., and Eckert, W. (2000). A
stochastic model of computer-human interaction for
learning dialogue strategies. IEEE Transactions on
Speech and Audio Processing, 8.
Pickering, M. J. and Garrod, S. (2004). Toward a mecha-
nistc psychology of dialog. Behavioral and Brain Sci-
ences, 27.
Reiter, E. and Dale, R. (1997). Building applied natural
language generation systems. Natural Language En-
gineering, 3(1):57?87.
Rieser, V. and Lemon, O. (2009). Natural language gen-
eration as planning under uncertainty for spoken dia-
logue systems. In EACL ?09: Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 683?691,
Morristown, NJ, USA.
Scott, D. and Moore, J. (2006). An NLG evaluation com-
petition? eight reasons to be cautious. Technical re-
port.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA, USA.
Toutanova, K. and Manning, C. D. (2000). Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natural
language processing and very large corpora, pages
63?70, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Viethen, J. and Dale, R. (2006). Towards the evaluation
of referring expression generation. In In Proceedings
of the 4th Australiasian Language Technology Work-
shop, pages 115?122.
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 78?87,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Optimising Natural Language Generation Decision Making
For Situated Dialogue
Nina Dethlefs
Department of Linguistics,
University of Bremen
dethlefs@uni-bremen.de
Heriberto Cuaya?huitl
German Research Centre
for Artificial Intelligence (DFKI)
heriberto.cuayahuitl@dfki.de
Jette Viethen
Centre for Language Technology
acquarie University
jviethen@ics.mq.edu.au
Abstract
Natural language generators are faced with a
multitude of different decisions during their
generation process. We address the joint opti-
misation of navigation strategies and referring
expressions in a situated setting with respect to
task success and human-likeness. To this end,
we present a novel, comprehensive framework
that combines supervised learning, Hierarchi-
cal Reinforcement Learning and a hierarchical
Information State. A human evaluation shows
that our learnt instructions are rated similar
to human instructions, and significantly better
than the supervised learning baseline.
1 Introduction
Natural Language Generation (NLG) systems are
typically faced with a multitude of decisions dur-
ing their generation process due to nondeterminacy
between a semantic input to a generator and its re-
alised output. This is especially true in situated set-
tings, where sudden changes of context can occur
at anytime. Sources of uncertainty include (a) the
situational context, such as visible objects, or task
complexity, (b) the user, including their behaviour
and reactions, and (c) the dialogue history, includ-
ing shared knowledge or patterns of linguistic con-
sistency (Halliday and Hasan, 1976) and alignment
(Pickering and Garrod, 2004).
Previous work on context-sensitive generation in
situated domains includes Stoia et al (2006) and
Garoufi and Koller (2010). Stoia et al present a
supervised learning approach for situated referring
expression generation (REG). Garoufi and Koller
use techniques from AI planning for the combined
generation of navigation instructions and referring
expressions (RE). More generally, the NLG prob-
lem of non-deterministic decision making has been
addressed from many different angles, including
PENMAN-style choosers (Mann and Matthiessen,
1983), corpus-based statistical knowledge (Langk-
ilde and Knight, 1998), tree-based stochastic models
(Bangalore and Rambow, 2000), maximum entropy-
based ranking (Ratnaparkhi, 2000), combinatorial
pattern discovery (Duboue and McKeown, 2001),
instance-based ranking (Varges, 2003), chart gen-
eration (White, 2004), planning (Koller and Stone,
2007), or probabilistic generation spaces (Belz,
2008) to name just a few.
More recently, there have been several approaches
towards using Reinforcement Learning (RL) (Rieser
et al, 2010; Janarthanam and Lemon, 2010) or Hi-
erarchical Reinforcement Learning (HRL) (Deth-
lefs and Cuaya?huitl, 2010) for NLG decision mak-
ing. All of these approaches have demonstrated that
HRL/RL offers a powerful mechanism for learn-
ing generation policies in the absence of complete
knowledge about the environment or the user. It
overcomes the need for large amounts of hand-
crafted knowledge or data in rule-based or super-
vised learning accounts. On the other hand, RL
can have difficulties to find an optimal policy in a
large search space, and is therefore often limited to
small-scale applications. Pruning the search space
of a learning agent by including prior knowledge is
therefore attractive, since it finds solutions faster, re-
duces computational demands, incorporates expert
knowledge, and scales to complex problems. Sug-
78
gestions to use such prior knowledge include Lit-
man et al (2000) and Singh et al (2002), who
hand-craft rules of prior knowledge obvious to the
system designer. Cuaya?huitl (2009) suggests us-
ing Hierarchical Abstract Machines to partially pre-
specify dialogue strategies, and Heeman (2007) uses
a combination of RL and Information State (IS)
to also pre-specify dialogue strategies. Williams
(2008) presents an approach of combining Partially-
Observable Markov Decision Processes with con-
ventional dialogue systems. The Information State
approach is well-established in dialogue manage-
ment (e.g., Bohlin et al (1999) and Larsson and
Traum (2000)). It allows the system designer to
specify dialogue strategies in a principled and sys-
tematic way. A disadvantage is that random design
decisions need to be made in cases where the best
action, or sequence of actions, is not obvious.
The contribution of this paper consists in a com-
prehensive account of constrained Hierarchical Re-
inforcement Learning through a combination with
a hierarchical Information State (HIS), which is in-
formed by prior knowledge induced from decision
trees. We apply our framework to the generation
of navigation strategies and referring expressions in
a situated setting, jointly optimised for task suc-
cess and linguistic consistency. An evaluation shows
that humans prefer our learnt instructions to the su-
pervised learning-based instructions, and rate them
equal to human instructions. Simulation-based re-
sults show that our semi-learnt approach learns more
quickly than the fully-learnt baseline, which makes
it suitable for large and complex problems. Our ap-
proach differs from Heeman?s in that we transfer it
to NLG and to a hierarchical setting. Although Hee-
man was able to show that his combined approach
learns more quickly than pure RL, it is limited to
small-scale systems. Our ?divide-and-conquer? ap-
proach, on the other hand, scales up to large search
spaces and allows us to address complex problems.
2 The Generation Tasks
2.1 The GIVE-2 Domain
Our domain is the generation of navigation instruc-
tions and referring expressions in a virtual 3D world
in the GIVE scenario (Koller et al, 2010). In this
task, two people engage in a ?treasure hunt?, where
an instruction giver (IG) navigates an instruction fol-
lower (IF) through the world, pressing a sequence of
buttons and completing the task by obtaining a tro-
phy. Pairs take part in three dialogues (in three dif-
ferent worlds); after the first dialogue, they switch
roles. The GIVE-2 corpus (Gargett et al, 2010) pro-
vides transcripts of such dialogues in English and
German. For this paper, we complemented the En-
glish dialogues of the corpus with a set of seman-
tic annotations.1 The feature set is organised in five
groups (Table 1). The first two groups cover manip-
ulation instructions (i.e., instructions to press a but-
ton), including distractors2 and landmarks (Gargett
et al, 2010). The third group describes high- and
low-level navigation, the fourth group describes the
user. The fifth group finally contains grammatical
information.
2.2 Navigation and Manipulation Instructions
Navigation instructions can take many forms, even
for the same route. For example, a way to another
room can be described as ?go to the room with the
lamp?, ?go left and through the door?, or ?turn 90
degrees, left, straight?. Choosing among these vari-
ants is a highly context- and speaker-dependent task.
Figure 1 shows the six user strategies we identified
from the corpus based on an analysis of the combi-
nation of navigation level (?high? vs. ?low?) and con-
tent (?destination?, ?direction?, ?orientation?, ?path?,
?straight?). User models are based on the navigation
level and content decisions made in a sequence of in-
structions, so that different sequences, with a certain
distribution, lead to different user model classifica-
tions. The proportions are shown in Figure 1. We
found that 75% of all speakers use the same strat-
egy in consecutive rounds/games. 62.5% of pairs
are consistent over all three dialogues, indicating
inter-speaker alignment. These high measures of
human consistency suggest that this phenomenon
is worth modelling in a learning agent, and there-
fore provides the motivation of including linguis-
tic consistency in our agent?s behaviour. Manipula-
tion instructions were treated as an REG task, which
needs to be sensitive to the properties of the referent
and distractors (e.g, size, colour, or spatial relation
1The annotations are available on request.
2Distractors are objects of the same type as the referent.
79
ID Feature Type Description
f1 absolute property(referent) boolean Is the colour of the referent mentioned?
f2 absolute property(distractor) boolean Is the colour of the distractor mentioned?
f3 discriminative colour(referent) boolean Is the colour of the referent discriminating?
f4 discriminative colour(distractor) boolean Is the colour of the distractor discriminating?
f5 mention(distractor) boolean Is a distractor mentioned?
f6 first mention(referent) boolean Is this the first reference to the referent?
f7 mention(macro landmark) boolean Is a macro (non-movable) landmark mentioned?
f8 mention(micro landmark) boolean Is a micro (movable) landmark mentioned?
f9 num(distractors) integer How many distractors are present?
f10 num(micro landmarks) integer How many micro landmarks are present?
f11 spatial rel(referent,obj) string Which spatial relation(s) are used in the RE?
f12 taxonomic property(referent) boolean Is the type of the distractor mentioned?
f13 within field of vision(referent) boolean Is the referent within the user?s field of vision?
f14 mention(colour, lm) boolean Is the colour of a macro- / micro lm mentioned?
f15 mention(size, lm) boolean Is the size of a macro- / micro lm mentioned?
f16 abstractness(nav instruction) string Is the instruction explicit or implicit?
f17 content(nav instruction) string Vals: destination, direction, orientation, path, straight
f18 level(nav instruction) string Is the instruction high- or low-level?
f19 position(user) string Is the user on track or off track?
f20 reaction(user) string Vals: take action, take wrong action, wait, req help
f21 type(user) string Vals: likes waiting, likes exploring, in between
f22 waits(user) boolean Is the user waiting for the next instruction?
f23 model(user) string User model/navig. strategy used (cf. Fig.1)?
f24 actor(instruction) boolean Is the actor of the instruction inserted?
f25 mood(instruction) boolean Is the mood of the instruction inserted?
f26 process(instruction) boolean Is the process of the instruction inserted?
f27 locational phrase(instruction) boolean Is the loc. phrase (path, straight, etc.) inserted?
Table 1: Corpus annotation features that were used as knowledge of the learning agent and the Information State. Fea-
tures are presented in groups, describing the properties of referents in the environment (f1...f13) and their distractors
(f14...f15), features of high- and low-level navigation (f16...f18), the user (f19...f23), and grammatical information
about constituents (f24...f27).
with respect to the referent) to be natural and dis-
tinguishing. We also considered the visual salience
of objects, and the type of spatial relation involved,
since recent studies indicate the potential relevance
of these features (Viethen and Dale, 2008). Given
these observations, we aim to optimise the task suc-
cess and linguistic consistency of instructions. Task
success is measured from user reactions after each
instruction (Section 5.1). Linguistic consistency is
achieved by rewarding the agent for generating in-
structions that belong to the same user model as the
previous one. The agent has the same probability
for choosing any pattern, but is then rewarded for
consistency. Table 3 (in Section 5.2) presents an ex-
ample dialogue generated by our system.
3 Constrained Hierarchical Reinforcement
Learning for NLG
3.1 Hierarchical Reinforcement Learning
Our idea of language generation as an optimisa-
tion problem is as follows: given a set of genera-
tion states, a set of actions, and an objective reward
function, an optimal generation strategy maximises
the objective function by choosing the actions lead-
ing to the highest reward for every reached state.
Such states describe the system?s knowledge about
80
Figure 1: Decision tree for the classification of user
models (UM) defined by the use of navigation level and
content. UM 0=high-level, UM 1=low-level (LL), UM
2=orientation-based LL, UM 3=orientation-based mix-
ture (M), UM 4=path-based M, UM 5=pure M.
the generation task (e.g. navigation strategy, or re-
ferring expressions). The action set describes the
system?s capabilities (e.g. ?use high level naviga-
tion strategy?, ?mention colour of referent?, etc.).
The reward function assigns a numeric value for
each action taken. In this way, language generation
can be seen as a finite sequence of states, actions
and rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, where
the goal is to find an optimal strategy automatically.
To do this we use RL with a divide-and-conquer ap-
proach in order to optimise a hierarchy of generation
policies rather than a single policy. The hierarchy of
RL agents consists of L levels and N models per
level, denoted as M ij , where j ? {0, ..., N ? 1}
and i ? {0, ..., L ? 1}. Each agent of the hierar-
chy is defined as a Semi-Markov Decision Process
(SMDP) consisting of a 4-tuple < Sij, Aij , T ij , Rij >.
Sij is a set of states, Aij is a set of actions, T ij is
a transition function that determines the next state
s? from the current state s and the performed ac-
tion a, and Rij is a reward function that specifies
the reward that an agent receives for taking an ac-
tion a in state s lasting ? time steps. The random
variable ? represents the number of time steps the
agent takes to complete a subtask. Actions can be
either primitive or composite. The former yield sin-
gle rewards, the latter correspond to SMDPs and
yield cumulative discounted rewards. The goal of
each SMDP is to find an optimal policy that max-
imises the reward for each visited state, according to
??ij(s) = arg maxa?Aij Q
?i
j(s, a), where Q?ij (s, a)
specifies the expected cumulative reward for exe-
cuting action a in state s and then following pol-
icy ??ij . We use HSMQ-Learning (Dietterich, 1999)
for learning a hierarchy of generation policies. This
hierarchical approach has been applied successfully
to dialogue strategy learning by Cuaya?huitl et al
(2010).
3.2 Information State
The notion of an Information State has traditionally
been applied to dialogue, where it encodes all infor-
mation relevant to the current state of the dialogue.
This includes, for example, the context of the in-
teraction, participants and their beliefs, and the sta-
tus of grounding. An IS consists of a set of infor-
mational components, encoding the information of
the dialogue, formal representations of these com-
ponents, a set of dialogue moves leading to the up-
date of the IS, a set of update rules which govern the
update, and finally an update strategy, which speci-
fies which update rule to apply in case more than one
applies (Larsson and Traum (2000), p. 2-3). In this
paper, we apply the theory of IS to language gener-
ation. For this purpose we define the informational
components of an IS to represent the (situational and
linguistic) knowledge of the generator (Section 4.2).
Update rules are triggered by generator actions, such
as the decision to insert a new constituent into the
current logical form, or the decision to prefer one
word order sequence over another. We use the DIP-
PER toolkit (Bos et al, 2003)3 for our implementa-
tion of the IS.
3.3 Combining Hierarchical Reinforcement
Learning and Information State
Previous work has suggested the HSMQ-Learning
algorithm for optimizing text generation strategies
(Dethlefs and Cuaya?huitl, 2010). Because such an
algorithm uses all available actions in each state,
an important extension is to constrain the actions
available with some prior expert knowledge, aim-
ing to combine behaviour specified by human de-
signers and behaviour automatically inferred by re-
inforcement learning agents. To that end, we sug-
3http://www.ltg.ed.ac.uk/dipper
81
Figure 2: (Left:) Hierarchy of learning agents executed from top to bottom for generating instructions. (Right:) State
representations for the agents shown in the hierarchy on the left. The features f1...f27 refer back to the features used
in the annotation given in the first column of Table 1. Note that agents can share information across levels.
gest combining the Information State approach with
hierarchical reinforcement learning. We therefore
re-define the characterisation of each Semi-Markov
Decision Process (SMDP) in the hierarchy as a 5-
tuple model M ij =< Sij, Aij , T ij , Rij , Iij >, where
Sij , Aij , T ij and Rij are as before, and the additional
element Iij is an Information State used as knowl-
edge base and rule-based decision maker. In this ex-
tended model, action selection is based on a con-
strained set of actions provided by the IS update
rules. We assume that the names of update rules
in Iij represent the agent actions Aij . The goal of
each SMDP is then to find an optimal policy that
maximises the reward for each visited state, accord-
ing to ??ij(s) = arg maxa?Aij?Iij Q
?i
j(s, a), where
Q?ij (s, a) specifies the expected cumulative reward
for executing constrained action a in state s and then
following ??ij thereafter. For learning such poli-
cies we use a modified version of HSMQ-Learning.
This algorithm receives subtask M ij and Information
State Iij used to initialise state s, performs similarly
to Q-Learning for primitive actions, but for compos-
ite actions it invokes recursively with a child sub-
task. In contrast to HSMQ-Learning, this algorithm
chooses actions from a subset derived by applying
the IS update rules to the current state of the world.
When the subtask is completed, it returns a cumu-
lative reward rt+? , and continues its execution until
finding a goal state for the root subtask. This process
iterates until convergence occurs to optimal context-
independent policies, as in HSMQ-Learning.
4 Experimental Setting
4.1 Hierarchy of Agents
Figure 2 shows a (hand-crafted) hierarchy of learn-
ing agents for navigating and acting in a situated en-
vironment. Each of these agents represents an indi-
vidual generation task. Model M00 is the root agent
and is responsible for ensuring that a set of naviga-
tion instructions guide the user to the next referent,
where an RE is generated. Model M10 is responsible
for the generation of the RE that best describes an
intended referent. Subtasks M20 ... M22 realise sur-
face forms of possible distractors, or macro- / micro
landmarks. Model M12 is responsible for the gener-
ation of navigation instructions which smoothly fit
into the linguistic consistency pattern chosen. Part
of this task is choosing between a low-level (model
M23 ) and a high-level (model M24 ) instruction. Sub-
tasks M30 ...M34 realise the actual instructions, des-
tination, direction, orientation, path, and ?straight?,
respectively.4 Finally, model M11 can repair previ-
ous system utterances.
4Note that navigation instructions and REs correspond to se-
quences of actions, not to a single one.
82
Model(s) Actions
M00 navigation, manipulation, confirmation, stop, repair system act, repair no system act
M10 insert distractor, insert no distractor, insert no absolute property, insert micro relatum, insert macro relatum
insert no taxonomic property, insert absolute property, insert no macro relatum, insert taxonomic property
M12 choose high level, choose low level, get route, choose easy route, choose short route
M20 ... M22 exp head, exp no head, insert colour, insert no colour, insert size, insert no size, exp spatial relation
M23 choose explicit abstractness, choose implicit abstractness, destination instruction, path instruction
M24 choose explicit abstractness, choose implicit abstractness, direction instr, orientation instr, straight instr
M30 ... M34 exp actor, exp no actor, exp mood, exp loc phrase, exp no loc phrase, exp process, exp no process
Table 2: Action set of the learning agents and Information States.
4.2 State and Action Sets
The HRL agent?s knowledge base consists of all sit-
uational and linguistic knowledge the agent needs
for decision making. Figure 2 shows the hierarchy
of learning agents together with the knowledge base
of the learning agent with respect to the semantic
features shown in Table 1 that were used for the an-
notation of the GIVE-2 corpus dialogues. The first
column of the table in Figure 2 indicates the respec-
tive model, also referred to as agent, or subtask, and
the second column refers to the knowledge variable
it uses (in the form of the feature index given in the
first column of Table 1). In the agent, boolean values
and strings were represented as integers. The HIS
shares all information of the learning agent, but has
an additional set of relational feature-value pairs for
each slot. For example, if the agent knows that the
slot content(nav instruction) has value 1 (mean-
ing ?filled?), the HIS knows also which value it was
filled with, such as path. Such additional knowledge
is required for the supervised learning baseline (Sec-
tion 5). The action set of the hierarchical learning
agent and the hierarchical information state is given
in Table 2. The state-action space size of a flat learn-
ing agent would be |S ?A| = 1011, the hierarchical
setting has a state-action space size of 2.4 ? 107.
The average state-action space size of all subtasks is
|S ? A|/14 = 1.7 ? 107. Generation actions can
be primitive or composite. While the former corre-
spond to single generation decisions, the latter rep-
resent separate generation subtasks (Fig. 2).
4.3 Prior Knowledge
Prior knowledge can include decisions obvious to
the system designer, expert knowledge, or general
intuitions. In our case, we use a supervised learn-
ing approach to induce prior knowledge into our
HRL agent. We trained decision trees on our anno-
tated corpus data using Weka?s (Witten and Frank,
2005) J48 decision tree classifer. A separate tree
was trained for each semantic attribute (cf. Table
1). The obtained decision trees represent our super-
vised learning baseline. They achieved an accuracy
of 91% in a ten-fold cross-validation. For our semi-
learnt combination of HRL and HIS, we performed a
manual analysis of the resulting rules to assess their
impact on a learning agent.5 In the end, the fol-
lowing rules were used to constrain the agent?s be-
haviour: (1) In REs, always use a referent?s colour,
except in cases of repair when colour is not discrim-
inating; (2) mention a distractor or micro landmark,
if the colour of the referent is not discriminating;
(3) in navigation, always make orientation instruc-
tions explicit. All remaining behaviour was subject
to learning.
4.4 Reward Function
We use the following reward function to train the hi-
erarchy of policies of our HRL agent. It aims to re-
duce discourse length at maximal task success6 us-
ing a consistent navigation strategy.
R =
?
?
?
?
?
?
?
?
?
0 for reaching the goal state
-2 for an already invoked subtask
+1 for generating instruction u con-
sistent with instruction u?1
-1 otherwise.
5We excluded rules that always choose the same value, since
they would work against our aim of generating consistent, but
variable instructions.
6Task success is addressed by that the user has to ?accept?
each instruction for a state transition.
83
The third reward that encourages consistency of in-
structions rewards a sequence of actions that allow
the last generated instruction to be classified as be-
longing to the same navigation strategy/user model
as the previously generated instruction (cf. 2.2).
5 Experiments and Results
5.1 The Simulated Environment
The simulated environment contains two kinds of
uncertainties: (1) uncertainty regarding the state of
the environment, and (2) uncertainty concerning the
user?s reaction to a system utterance. The first aspect
is represented by a set of contextual variables de-
scribing the environment, 7 and user behaviour.8 Al-
together, this leads to 115 thousand different contex-
tual configurations, which are estimated from data
(cf. Section 2.1). The uncertainty regarding the
user?s reaction to an utterance is represented by a
Naive Bayes classifier, which is passed a set of
contextual features describing the situation, mapped
with a set of semantic features describing the utter-
ance.9 From these data, the classifier specifies the
most likely user reaction (after each system act) of
perform desired action, perform undesired action, wait
and request help.10 The classifier was trained on the
annotated data and reached an accuracy of 82% in a
ten-fold cross validation.
5.2 Learnt Policies
With respect to REs, the fully-learnt policy (only
HRL) uses colour when it is discriminating, and a
distractor or micro landmark otherwise. The semi-
learnt policy (HRL with HIS) behaves as defined in
Section 4.3. The supervised learning policy (only
HIS) uses the rules learnt by the decision trees. Both
learnt policies learn to maximise task success, and
to generate consistent navigation strategies.11 The
7previous system act, route length, route status
(known/unknown), objects within vision, objects within
dialogue history, number of instructions, alignment(proportion)
8previous user reaction, user position, user wait-
ing(true/false), user type(explorative/hesitant/medium)
9navigation level(high / low), abstractness(implicit / ex-
plicit), repair(yes / no), instruction type(destination / direction /
orientation / path / straight)
10User reactions measure the system?s task success.
11They thereby also learn to adapt their semantic choices to
those most frequently made by humans.
101 102 103 104
?80
?70
?60
?50
?40
?30
?20
?10
0
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Deterministic
Semi?Learnt
Fully?Learnt
Figure 3: Comparison of fully-learnt, semi-learnt, and su-
pervised learning (deterministic) behaviours.
supervised learning policy generates successful in-
structions from the start. Note that we are not ac-
tually learning dialogue strategies, but rather gen-
eration strategies using dialogue features. There-
fore the described policies, fully-learnt, semi-learnt
and supervised-learning, exclusively guide the sys-
tem?s behaviour in the interaction with the simulated
user. An example dialogue is shown in Table 3. We
can observe that the agent starts using a low level
navigation strategy, and then switches to high level.
When the user gets confused, the system temporar-
ily switches back to low level. For referring expres-
sions, it first attempts to locate the referent by ref-
erence to a distractor, and then repairs by using a
micro landmark. The surface forms of instructions
were realised from templates, since the NLG system
so far only generates a sequence of content selection
decisions. We address surface realisation in Dethlefs
and Cuaya?huitl (2011).
We compared our semi-learnt policy against a
fully-learnt, and a supervised learning baseline. All
policies were trained for 40 thousand episodes. For
training, the step-size parameter ?, which indicates
the learning rate, was initiated with 1 and then re-
duced over time by ? = 11+t , where t is the time step.
The discount rate ?, which indicates the relevance of
future rewards in relation to immediate rewards, was
set to 0.99, and the probability of a random action ?
was 0.01. See (Sutton and Barto, 1998) for details
on these parameters.
5.3 Comparison of Policies
Figure 3 compares the rewards averaged over 10
runs. In terms of learning speed, we can see that
84
Agent Policy Action (composite in italics) Utterance
Usr request route How do I get to the trophy?
Sys ?00, ?12 root, navigation, dontRepair, orientation
?23, ?30 low level navigation, add direction+degrees First, turn 90 degrees left.
?23, ?31 low level navigation, ?straight? Then go straight ahead.
Usr perform desired action
Sys ?00 confirmation OK.
?24, ?34, ?32 high level navigation, destination, direction,
?24, ?34, ?32 addLocation Turn left at the end of the hallway.
Usr wait
Sys ?11 repair system act, change navigation strategy
?23, ?32 low level navigation, direction Turn right
?24, ?33 high level navigation, path and go down the hallway.
Usr perform desired action
Sys ?23, ?32 low level navigation, direction Now turn left.
Usr perform desired action
Sys ?00 confirmation OK.
?24, ?34 high level navigation, destination,
?24, ?34 addLocation Enter to the room in front of you.
Usr perform desired action
Sys ?10, ?21 manipulation, use colour, use distractor Press the red button beside the blue.
Usr request help
Sys ?11 repair system act, change granularity
?24, ?10 high level navigation, manipulation
?34, ?22 destination, use colour, use micro landmark Go to the red button beside the chair.
Usr perform desired action
Sys ?10 manipulation Press it.
Usr perform desired action
Sys ?00 confirmation Well done.
Table 3: Sample dialogue in the GIVE-2 scenario showing the dynamics of generation policies. See Figure 2 for the
corresponding hierarchy models, and Table 2 for the action set. See Section 5.2 for an explantation of the dialogue.
while the semi-learnt behaviour is able to follow a
near-optimal policy from the beginning, the fully-
learnt policy takes about 40 thousand episodes to
reach the same performance. In terms of simulated
task success, we see that while the supervised learn-
ing behaviour follows a good policy from the start,
it is eventually beaten by the learnt policies.
5.4 Human Evaluation Study
We asked 11 participants12 to rate altogether 132
sets of instructions, where each set contained a spa-
tial graphical scene containing a person, mapped
with one human, one learnt, and one supervised
126 female, 5 male with an age average of 26.4.
learning instruction. Instructions consisted of a nav-
igation instruction followed by a referring expres-
sion. Subjects were asked to rate instructions on a
1-5 Likert scale (where 5 is the best) for their help-
fulness on guiding the displayed person from its ori-
gin to pressing the intended button. We selected
six different scenarios for the evaluation: (a) only
one button is present, (b) two buttons are present,
the referent and a distractor of the same colour as
the referent, (c) two buttons are present, the referent
and a distractor of a different colour than the refer-
ent, (d) one micro landmark is present and one dis-
tractor of the same colour as the referent, (e) one
micro landmark is present and one distractor of a
different colour than the referent. All scenarios oc-
85
Figure 4: Example scenario of the human evaluation study.
curred twice in each evaluation sheet, their specific
instances were drawn from the GIVE-2 corpus at
random. Scenes and instructions were presented in
a randomised order. Figure 4 presents an example
evaluation scene. Finally, we asked subjects to cir-
cle the object they thought was the intended refer-
ent. Subjects rated the human instructions with an
average of 3.82, the learnt instructions with an aver-
age of 3.55, and the supervised learning instructions
with an average of 2.39. The difference between hu-
man and learnt is not significant. The difference be-
tween learnt and supervised learning is significant at
p < 0.003, and the difference between human and
supervised learning is significant at p < 0.0002. In
96% of all cases, users were able to identify the in-
tended referent.
6 Conclusion and Discussion
We have presented a combination of HRL with a hi-
erarchical IS, which was informed by prior knowl-
edge from decision trees. Such a combined frame-
work has the advantage that it allows us to system-
atically pre-specify (obvious) generation strategies,
and thereby find solutions faster, reduce computa-
tional demands, scale to complex domains, and in-
corporate expert knowledge. By applying HRL to
the remaining (non-obvious) action set, we are able
to learn a flexible, generalisable NLG policy, which
will take the best action even under uncertainty. As
an application of our approach and its generalisabil-
ity across domains, we have presented the joint op-
timisation of two separate NLG tasks, navigation in-
structions and referring expressions, in situated dia-
logue under the aspects of task success and linguis-
tic consistency. Based on an evaluation in a simu-
lated environment estimated from data, we showed
that our semi-learnt behaviour outperformed a fully-
learnt baseline in terms of learning speed, and a su-
pervised learning baseline in terms of average re-
wards. Human judges rated our instructions signif-
icantly better than the supervised learning instruc-
tions, and close to human quality. The study re-
vealed a task success rate of 96%. Future work
can transfer our approach to different applications to
confirm its benefits, and induce the agent?s reward
function from data to test in a more realistic setting.
Acknowledgments
Thanks to the German Research Foundation DFG
and the Transregional Collaborative Research Cen-
tre SFB/TR8 ?Spatial Cognition? and the EU-FP7
project ALIZ-E (ICT-248116) for partial support of
this work. Also, thanks to John Bateman for com-
ments on an earlier draft of this paper.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th conference on Computa-
tional linguistics - Volume 1, pages 42?48.
Anja Belz. 2008. Automatic generation of
weather forecast texts using comprehensive probabilis-
tic generation-space models. Natural Language Engi-
neering, 1:1?26.
86
Peter Bohlin, Robin Cooper, Elisabet Engdahl, and
Staffan Larsson. 1999. Information states and di-
alogue move engines. In IJCAI-99 Workshop on
Knowledge and Reasoning in Practical Dialogue Sys-
tems.
Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi Oka.
2003. DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Architec-
ture. In 4th SIGDial Workshop on Discourse and Dia-
logue, pages 115?124.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395?429.
Heriberto Cuaya?huitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. thesis,
School of Informatics, University of Edinburgh.
Nina Dethlefs and Heriberto Cuaya?huitl. 2010. Hi-
erarchical Reinforcement Learning for Adaptive Text
Generation. Proceedings of INLG ?10.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hier-
archical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Genera-
tion. In Proceedings of ACL-HLT 2011, Portland, OR.
Thomas G. Dietterich. 1999. Hierarchical reinforce-
ment learning with the maxq value function decom-
position. Journal of Artificial Intelligence Research,
13:227?303.
Pablo A. Duboue and Kathleen R. McKeown. 2001. Em-
pirically estimating order constraints for content plan-
ning in generation. In ACL ?01, pages 172?179.
Andrew Gargett, Konstantina Garoufi, Alexander Koller,
and Kristina Striegnitz. 2010. The give-2 corpus of
giving instructions in virtual environments. In LREC.
Konstantina Garoufi and Alexander Koller. 2010. Au-
tomated planning for situated natural language gener-
ation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1573?1582, July.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Peter Heeman. 2007. Combining reinforcement learning
with information-state update rules. In Human Tech-
nology Conference (HLT), pages 268?275.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In ACL ?10,
pages 69?78.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of ACL-07.
Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-
tine Cassell, Robert Dale, Johanna Moore, and Jon
Oberlander. 2010. The first challenge on generat-
ing instructions in virtual environments. In M. The-
une and E. Krahmer, editors, Empirical Methods
on Natural Language Generation, pages 337?361,
Berlin/Heidelberg, Germany. Springer.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
ACL-36, pages 704?710.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Nat. Lang. Eng., 6(3-
4):323?340.
Diane J. Litman, Michael S. Kearns, Satinder Singh, and
Marilyn A. Walker. 2000. Automatic optimization of
dialogue management. In Proceedings of the 18th con-
ference on Computational linguistics, pages 502?508.
William Mann and Christian M I M Matthiessen. 1983.
NIGEL: A systemic grammar for text generation.
Technical report, ISI/RR-85-105.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistc psychology of dialog. Behavioral and
Brain Sciences, 27.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL, pages 194?201.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In ACL ?10, pages 1009?1018.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105?133.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase gen-
eration for situated dialogs. In Proceedings of INLG
?06, pages 81?88.
Richard S Sutton and Andrew G Barto. 1998. Reinforce-
ment Learning: An Introduction. MIT Press, Cam-
bridge, MA, USA.
Sebastian Varges. 2003. Instance-based Natural Lan-
guage Generation. Ph.D. thesis, School of Informat-
ics, University of Edinburgh.
Jette Viethen and Robert Dale. 2008. The use of spatial
relations in referring expression generation. In Pro-
ceedings of INLG ?08, INLG ?08, pages 59?67.
Michael White. 2004. Reining in CCG chart realization.
In In Proc. INLG-04, pages 182?191.
Jason Williams. 2008. The best of both worlds: Uni-
fying conventional dialog systems and POMDPs. In
Interspeech, Brisbane.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. 2. edi-
tion.
87
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49?58,
Utica, May 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Generation for Spoken Dialogue Systems:
Reducing the Need for Fillers
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot Watt University
EH14 4AS, Edinburgh
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Recent studies have shown that incremental
systems are perceived as more reactive, nat-
ural, and easier to use than non-incremental
systems. However, previous work on incre-
mental NLG has not employed recent ad-
vances in statistical optimisation using ma-
chine learning. This paper combines the two
approaches, showing how the update, revoke
and purge operations typically used in in-
cremental approaches can be implemented as
state transitions in a Markov Decision Process.
We design a model of incremental NLG that
generates output based on micro-turn inter-
pretations of the user?s utterances and is able
to optimise its decisions using statistical ma-
chine learning. We present a proof-of-concept
study in the domain of Information Presen-
tation (IP), where a learning agent faces the
trade-off of whether to present information as
soon as it is available (for high reactiveness)
or else to wait until input ASR hypotheses are
more reliable. Results show that the agent
learns to avoid long waiting times, fillers and
self-corrections, by re-ordering content based
on its confidence.
1 Introduction
Traditionally, the smallest unit of speech processing
for interactive systems has been a full utterance with
strict, rigid turn-taking. Components of these inter-
active systems, including NLG systems, have so far
treated the utterance as the smallest processing unit
that triggers a module into action. More recently,
work on incremental systems has shown that pro-
cessing smaller ?chunks? of user input can improve
the user experience (Skantze and Schlangen, 2009;
Buss et al, 2010; Skantze and Hjalmarsson, 2010;
Baumann et al, 2011). Incrementality in NLG sys-
tems enables the system designer to model several
dialogue phenomena that play a vital role in hu-
man discourse (Levelt, 1989) but have so far been
absent from NLG systems. These include more
natural turn-taking through rapid system responses,
grounding through the generation of backchannels
and feedback, and barge-ins (from both user and sys-
tem). In addition, corrections and self-corrections
through constant monitoring of user and system ut-
terances play an important role, enabling the system
to recover smoothly from a recognition error or a
change in the user?s preferences. Some examples of
the phenomena we are targeting are given in Fig. 1.
Skantze and Hjalmarsson (2010) present a model
of incremental speech generation in which input pro-
cessing and output planning are parallel processes
and the system can self-monitor its own genera-
tion process. In an evaluation with human users
they showed that their incremental system started
to speak significantly faster than a non-incremental
system (roughly 600 ms) and was perceived as sig-
nificantly more polite and efficient. Users also in-
dicated that they knew better when to start speaking
themselves. Alternative approaches to incremental
NLG include Kilger and Finkler (1995) who present
an early approach based on Tree-Adjoining Gram-
mar, and Purver and Otsuka (2003) who define an
incremental generator based on Dynamic Syntax.
Both of these generators can monitor their own out-
put and initiate corrections if necessary.
Over recent years, adaptive and data-driven ap-
49
Self-correction (the system made a mistake)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 Indian restaurants . . .
USR No, I want Italian.
SYS oh sorry . . .
SYS I have 24 Italian restaurants in the city centre . . .
Correction (the user changed their mind)
USR I want Indian food in the centre of town . . .
SYS There are 35 Indian . . .
USR Oh sorry, I meant Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Re-ordering (from high to low confidence)
USR I want Italian food . . .
SYS [waits]
USR in the city centre.
SYS I have 120 places in the city centre . . .
USR I love Italian . . .
SYS 24 of them Italian . . .
Holding the floor
USR I want cheap Italian food . . .
SYS ok let me see
SYS I have 3 cheap Italian places . . .
Figure 1: Example phenomena generated with the trained
policy. The agent has learnt to produce backchannels
when the user pauses, monitor and (self-)correct its out-
put, and present information according to its confidence.
proaches to NLG have also been developed and
shown to outperform the previous (handcrafted,
rule-based) methods for specific problems (Rieser et
al., 2010; Janarthanam and Lemon, 2010; Dethlefs
and Cuaya?huitl, 2011). This work has established
that NLG can fruitfully be treated as a data-driven
statistical planning process, where the objective is
to maximise expected utility of the generated utter-
ances (van Deemter, 2009), by adapting them to the
context and user. Statistical approaches to sentence
planning and surface realisation have also been ex-
plored (Stent et al, 2004; Belz, 2008; Mairesse et
al., 2010; Angeli et al, 2010). The advantages of
data-driven methods are that NLG is more robust in
the face of noise, can adapt to various contexts and,
trained on real data, can produce more natural and
desirable variation in system utterances.
This paper describes an initial investigation into a
novel NLG architecture that combines incremental
processing with statistical optimisation. In order to
move away from conventional strict-turn taking, we
have to be able to model the complex interactions
observed in human-human conversation. Doing this
in a deterministic fashion through hand-written rules
would be time consuming and potentially inaccu-
rate, with no guarantee of optimality. In this paper,
we demonstrate that it is possible to learn incremen-
tal generation behaviour in a reward-driven fashion.
2 Previous Work: Incremental Processing
Architectures
The smallest unit of processing in incremental sys-
tems is called incremental unit (IU). Its instantia-
tion depends on the particular processing module. In
speech recognition, IUs can correspond to phoneme
sequences that are mapped onto words (Baumann
and Schlangen, 2011). In dialogue management, IUs
can correspond to dialogue acts (Buss et al, 2010).
In speech synthesis, IUs can correspond to speech
unit sequences which are mapped to segments and
speech plans (Skantze and Hjalmarsson, 2010). IUs
are typically linked to other IUs by two types of rela-
tions: same-level links connect IUs sequentially and
express relationships at the same level; grounded-in
links express hierarchical relations between IUs.
2.1 Buffer-Based Incremental Processing
A general abstract model of incremental process-
ing based on buffers and a processor was devel-
oped by Schlangen and Skantze (2009) and is illus-
trated in Figure 2. It assumes that the left buffer
of a module, such as the NLG module, receives
IUs from one or more other processing modules,
such as the dialogue manager. These input IUs are
then passed on to the processor, where they are
mapped to corresponding (higher-level) IUs. For
an NLG module, this could be a mapping from the
dialogue act present(cuisine=Indian) to the realisa-
tion ?they serve Indian food?. The resulting IUs are
passed on to the right buffer which co-incides with
the left buffer of another module (for example the
speech synthesis module in our example). Same-
level links are indicated as dashed arrows in Figure
2 and grounded-in links as stacked boxes of IUs.
The figure also shows that the mapping between
IUs can be a one-to-many mapping (IU1 and IU2
are mapped to IU3) or a one-to-one mapping (IU3 is
50
IU1 IU2
IU1 IU2
IU3
IU3 IU3
IU4
IU4
Left buffer Processor Right buffer
Left buffer Processor Right buffer
Figure 2: The buffer-based model showing two connected
modules (from Skantze and Hjalmarsson (2010).
IU1
IU2 IU3 IU4 IU5
IU6 IU7 IU8 IU9 . . .
Figure 3: The ISU-model for incremental processing
(adapted from Buss and Schlangen (2011)).
mapped to IU4). The model distinguishes four op-
erations that handle information processing: update,
revise, purge and commit. Whenever new IUs en-
ter the module?s left buffer, the module?s knowledge
base is updated to reflect the new information. Such
information typically corresponds to the current best
hypothesis of a preceding processing module. As
a property of incremental systems, however, such
hypotheses can be revised by the respective preced-
ing module and, as a result, the knowledge bases of
all subsequent modules need to be purged and up-
dated to the newest hypothesis. Once a hypothesis
is certain to not be revised anymore, it is commit-
ted. For concrete implementations of this model, see
Skantze and Schlangen (2009), Skantze and Hjal-
marsson (2010), Baumann and Schlangen (2011).
An implementation of an incremental dialogue
manager is based on the Information State Update
(ISU) model (Buss et al, 2010; Buss and Schlangen,
2011). The model is related in spirit to the buffer-
based architecture, but all of its input processing and
output planning is realised by ISU rules. This is true
for the incremental ?house-keeping? actions update,
revise, etc. and all types of dialogue acts. The in-
cremental ISU model is shown in Figure 3. Note
that this hierarchical architecture transfers well to
the ?classical? division of NLG levels into utterance
(IU1), content selection (IU2 - IU5) and surface re-
alisations (IU6 - IU9, etc.).
2.2 Beat-Driven Incremental Processing
In contrast to the buffer-based architectures, alterna-
tive incremental systems do not reuse previous par-
tial hypotheses of the user?s input (or the system?s
best output) but recompute them at each process-
ing step. We follow Baumann et al (2011) in call-
ing them ?beat-driven? systems. Raux and Eskenazi
(2009) use a cost matrix and decision theoretic prin-
ciples to optimise turn-taking in a dialogue system
under the constraint that users prefer no gaps and no
overlap at turn boundaries. DeVault et al (2009) use
maximum entropy classification to support respon-
sive overlap in an incremental system by predicting
the completions of user utterances.
2.3 Decision-making in Incremental Systems
Some of the main advantages of the buffer- and ISU-
based approaches include their inherently incremen-
tal mechanisms for updating and revising system hy-
potheses. They are able to process input of varying
size and type and, at the same time, produce arbi-
trarily complex output which is monitored and can
be modified at any time. On the other hand, current
models are based on deterministic decision making
and thus share some of the same drawbacks that non-
incremental systems have faced: (1) they rely on
hand-written rules which are time-consuming and
expensive to produce, (2) they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, and (3) they are unable to
generalise and adapt flexibly to unseen situations.
For NLG in particular, we have seen that incre-
mentality can enhance the responsiveness of sys-
tems and facilitate turn-taking. However, this ad-
vantage was mainly gained by the system produc-
ing semantically empty fillers such as um, let me
see, well, etc. (Skantze and Hjalmarsson, 2010). It
is an open research question whether such markers
of planning or turn-holding can help NLG systems,
but for now it seems that they could be reduced to
a minimum by optimising the timing and order of
Information Presentation. In the following, we de-
velop a model for incremental NLG that is based on
reinforcement learning (RL). It learns the best mo-
ment to present information to the user, when faced
with the options of presenting information as soon
as it becomes available or else waiting until the in-
51
Type Example
Comparison The restaurant Roma is in the medium price range, but does not have great food. The Firenze
and Verona both have great food but are more expensive. The Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expensive,
but has great food and service.
Summary I have 43 Italian restaurants in the city centre that match your query. 10 of them are in the
medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
put hypotheses of the system are more stable. This
also addresses the general trade-off that exists in in-
cremental systems between the processing speed of
a system and the output quality.
3 Information Presentation Strategies
Our domain of application will be the Informa-
tion Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al (2010), (see also Walker et
al. (2004) for an alternative approach). Rieser et
al. incrementally construct IP strategies according
to the predicted user reaction, whereas our approach
focuses on timing and re-ordering of information
according to dynamically changing input hypothe-
ses. We therefore implement a simplified version
of Rieser et al?s model. Their system distinguished
two steps: the selection of an IP strategy and the
selection of attributes to present to the user. We as-
sume here that the choice of attributes is determined
by matching the types specified in the user input,
so that our system only needs to choose a strategy
for presenting its results (in the future, though, we
will include attribute selection into the decision pro-
cess). Attributes include cuisine, food quality, lo-
cation, price range and service quality of a restau-
rant. The system then performs a database lookup
and chooses among three main IP strategies sum-
mary, comparison, recommendation and several or-
dered combinations of these. Please see Rieser et al
(2010) for details. Table 1 shows examples of the
main types of presentation strategies we address.
4 Optimising Incremental NLG
To optimise the NLG process within an incremen-
tal model of dialogue processing, we define an RL
agent with incremental states and actions for the IP
task. An RL agent is formalised as a Markov De-
cision Process, or MDP, which is characterised as a
four-tuple < S,A, T,R >, where S is a set of states
representing the status of the NLG system and all in-
formation available to it, A is a set of NLG actions
that combine strategies for IP with handling incre-
mental updates in the system, T is a probabilistic
transition function that determines the next state s?
from the current state s and the action a according
to a conditional probability distribution P (s?|s, a),
and R is a reward function that specifies the reward
(a numeric value) that an agent receives for taking
action a in state s. Using such an MDP, the NLG
process can be seen as a finite sequence of states,
actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},
where t is the time step. Note that a learning episode
falls naturally into a number of time steps at each of
which the agent observes the current state of the en-
vironment st, takes an action at and makes a tran-
sition to state st+1. This organisation into discrete
time steps, and the notion of a state space that is ac-
cessible to the learning agent at any time allows us to
implement the state update, revoke and purge opera-
tions typically assumed by incremental approaches
as state updates and transitions in an MDP. Any
change in the environment, such as a new best hy-
pothesis of the recogniser, can thus be represented
as a transition from one state to another. At each
time step, the agent then takes the currently best ac-
tion according to the new state. The best action in
an incremental framework can include correcting a
previous output, holding the floor as a marker of
planning, or to wait until presenting information.1
1We treat these actions as part of NLG content selection
here, but are aware that in alternative approaches, they could
52
States
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
presStrategy {0=unfilled,1=filled}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusFood {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions
IP: compare, recommend, summarise, summariseCompare,
summariseRecommend, summariseCompareRecommend,
Slot-ordering: presentCuisine, presentFood, presentLocation,
presentPrice, presentService,
Incremental: backchannel, correct, selfCorrect, holdFloor,
waitMore
Goal State 0, 1, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 1, 0 ? 1
Figure 4: The state and action space of the learning agent.
The goal state is reached when all items (that the user may
be interested in) have been presented.
Once information has been presented to the user,
it is committed or realised. We again represent re-
alised IUs in the agent?s state representation, so that
it can monitor its own output. The goal of an MDP
is to find an optimal policy pi? according to which
the agent receives the maximal possible reward for
each visited state. We use the Q-Learning algorithm
(Watkins, 1989) to learn an optimal policy according
to pi?(s) = argmaxa?A Q?(s, a), where Q? speci-
fies the expected reward for executing action a in
state s and then following policy pi?.
5 Experimental Setting
5.1 The State and Action Space
The agent?s state space needs to contain all infor-
mation relevant for choosing an optimal IP strat-
egy and an optimal sequence of incremental ac-
tions. Figure 4 shows the state and action space
of our learning agent. The states contain infor-
mation on the incremental and presentation sta-
tus of the system. The variable ?incrementalSta-
tus? characterises situations in which a particular
(incremental) action is triggered. For example, a
holdFloor is generated when the user has finished
speaking, but the system has not yet finished its
database lookup. A correction is needed when
also be the responsibility of a dialogue manager.
the system has to modify already presented infor-
mation (because the user changed their preferences)
and a selfCorrection is needed when previously
presented information is modified because the sys-
tem made a mistake (in recognition or interpreta-
tion). The variable ?presStrategy? indicates whether
a strategy for IP has been chosen. It is ?filled? when
this is the case, and ?unfilled? otherwise. The vari-
ables representing the status of the cuisine, food, lo-
cation, price and service indicate whether the slot
is of interest to the user (0 means that the user does
not care about it), and what input confidence score is
currently associated with its value. Once slots have
been presented, they are realised and can only be
changed through a correction or self-correction.
The variable ?userReaction? shows the user?s re-
action to an IP episode. The user can select a restau-
rant, provide more information to further constrain
the search or do something else. The ?userSilence?
variable indicates whether the user is speaking or
not. This can be relevant for holding the floor or
generating backchannels. The action set comprises
IP actions, actions which enable us to learn the or-
dering of slots, and actions which allow us to cap-
ture incremental phenomena. The complete state-
action space size of this agent is roughly 3.2 mil-
lion. The agent reaches its goal state (defined w.r.t.
the state variables in Figure 4) when an IP strategy
has been chosen and all relevant attributes have been
presented.
5.2 The Simulated Environment
Since a learning agent typically needs several thou-
sand interactions to learn a reasonable policy, we
train it in a simulated environment with two compo-
nents. The first one deals with different IP strategies
generally (not just for the incremental case), and the
second one focuses on incrementally updated user
input hypothesis during the interaction.
To learn a good IP strategy, we use a user simula-
tion by Rieser et al (2010),2 which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user re-
action at time t to the system?s IP strategy IPs,t in
state s at time t. We distinguish the user reactions of
2The simulation data are available from http://www.
classic-project.org/.
53
select a restaurant, addMoreInfo to the current query
to constrain the search, and other. The last category
is considered an undesired user reaction that the sys-
tem should learn to avoid. The simulation uses lin-
ear smoothing to account for unseen situations. In
this way, we can then predict the most likely user
reaction to each system action.
While the IP strategies can be used for incremen-
tal and non-incremental NLG, the second part of the
simulation deals explicitly with the dynamic envi-
ronment updates during an interaction. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores are shown in Table 2
and denote the same as described in Section 5.1.
At the beginning of a learning episode, we as-
sign each attribute a possible value and confidence
score with equal probability. For food and service
quality, we assume that the user is never interested
in bad food or service. Subsequently, confidence
scores can change at each time step. (In future work
these transition probabilities will be estimated from
a data collection, though the following assumptions
are realistic, based on our experience.) We assume
that a confidence score of 0 changes to any other
value with a likelihood of 0.05. A confidence score
of 1 changes with a probability of 0.3, a confidence
score of 2 with a probability of 0.1 and a confidence
score of 3 with a probability of 0.03. Once slots
have been realised, their value is set to 4. They
cannot be changed then without an explicit correc-
tion. We also assume that realised slots change with
a probability of 0.1. If they change, we assume
that half of the time, the user is the origin of the
change (because they changed their mind) and half
of the time the system is the origin of the change
(because of an ASR or interpretation error). Each
time a confidence score is changed, it has a proba-
bility of 0.5 to also change its value. The resulting
input to the NLG system are data structures of the
form present(cuisine=Indian), confidence=low.
5.3 The Reward Function
The main trade-off to optimise for IP in an incre-
mental setting is the timing and order of presenta-
tion. The agent has to decide whether to present
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Food bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, expensive, good-price-
for-value, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
information as soon as it becomes available or else
wait until confidence for input hypotheses is more
stable. Alternatively, it can reorder information to
account for different confidence scores. We assign
the following rewards3: +100 if the user selects
an item, 0 if the user adds more search constraints,
?100 if the user does something else or the sys-
tem needs to self-correct,?0.5 for holding the floor,
and ?1 otherwise. In addition, the agent receives
an increasing negative reward for the waiting time,
waiting time2 (to the power of two), in terms of the
number of time steps passed since the last item was
presented. This reward is theoretically ??. The
agent is thus penalised stronger the longer it delays
IP. The rewards for user reactions are assigned at the
end of each episode, all other rewards are assigned
after each time step. One episode stretches from the
moment that a user specifies their initial preferences
to the moment in which they choose a restaurant.
The agent was trained for 10 thousand episodes.
6 Experimental Results
After training, the RL agent has learnt the following
incremental IP strategy. It will present information
slots as soon as they become available if they have
a medium or high confidence score. The agent will
then order attributes so that those slots with the high-
est confidence scores are presented first and slots
with lower confidence are presented later (by which
time they may have achieved higher confidence). If
no information is known with medium or high con-
3Handcrafted rewards are sufficient for this proof-of-
concept study, and can be learned from data for future models
(Rieser and Lemon, 2011).
54
101 102 103 104
?100
?80
?60
?40
?20
0
20
40
60
80
100
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
RL
Base1
Base2
Base3
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the RL agent and its baselines.
fidence, the agent will hold the floor or wait. In this
way, it can prevent self-corrections and minimise
waiting time?both of which yield negative rewards.
It can thus start speaking very early (avoiding long
pauses or semantically empty utterances) and still
has a low likelihood of having to self-correct.
For a comparison of the learnt policy with possi-
ble hand-crafted policies (because current incremen-
tal NLG systems are rule-based), we designed three
baselines. Baseline 1 always presents information
as soon as it is available, i.e. never waits. Base-
line 2 always waits until all information is known
with high confidence (i.e. until all confidence scores
are 3). Baseline 3 was chosen to be more ambi-
tious. It always presents information as soon as
possible, using a decreasing order of confidence to
minimise self-corrections (i.e. very similar to the
learnt policy). It chooses randomly among slots with
equal confidence. All baseline policies have an op-
timised IP strategy (recommend/summary etc.) and
differ only in their incremental processing strategies.
Baseline 1 is most similar to the current approach
used in spoken dialogue systems, where the Dia-
logue Manager triggers an NLG component as soon
as a task-relevant user utterance is processed.
Here we do not compare the different strategies
for IP generally because this has been done by
Rieser et al (2010), even if not for incremental
NLG. Figure 5 shows the performance of all be-
haviours in terms of average rewards. Baseline 1
obtains on average roughly 100 reward points less
than the RL agent. This corresponds to the (neg-
ative) reward of one self-correction (?100). Since
information is always presented as soon as it is avail-
able, this baseline needs to produce on average one
Baseline 1 (present information as soon as
it becomes available)
USR Please show me Italian places
(cuisine = Indian;conf score = 0.2)
SYS I have found 35 Indian . . .
USR No, ITALIAN
(cuisine = Italian; conf score = 0.8)
SYS Oh, you meant Italian. I have 104 Italian places in Edin-
burgh. 24 in the city centre, 21 in the New Town, 14 in . . .
USR Show me some in the city centre
(location = centre; conf score = 0.6)
SYS OK. I found 24 Italian restaurants in the city centre . . .
Baseline 2 (always wait until confidence is high)
USR Do you have Italian restaurants in the centre of town?
cuisine = Italian; conf score = 0.4
location = centre; conf score = 0.2
SYS waits
USR Italian in the centre.
cuisine = Italian, conf score = 0.7
location = centre, conf score = 0.5
SYS I have 104 Italian restaurants.
USR waits
SYS waits
USR city centre please
location = centre, conf score = 0.7
SYS I have 24 Italian restaurants in the city centre . . .
Baseline 3 (present information in decreasing
order of confidence)
USR I want Italian food . . .
cuisine = Indian, conf score = 0.2
location = centre, conf score = 0.3
SYS hmm (holding turn) . . .
USR in the centre of town
location = centre, conf score = 0.9
SYS In the centre, let me see, Indian . . .
USR Italian, please.
cuisine = Italian, conf score = 0.7
SYS Oh I see. I have 24 Italian places in the centre . . .
Figure 6: Example dialogues generated with the baseline
policies for a user who wants Italian food in the city cen-
tre. Confidence scores for cuisine and location variables
for the restaurants are shown as updated.
self-correction per episode. Baseline 2 needs to wait
until all information is known with high confidence
and obtains on average 125 to 130 rewards less than
the RL agent. This corresponds to approximately
11 time steps of waiting (for input to reach higher
confidence) before presentation since 11 is (approxi-
mately) the square root of 130. Baseline 3 is roughly
a reward of ?10 worse than the RL agent?s be-
55
haviour, which is due to a combination of more self-
corrections, even if they just occur occasionally, and
a higher number of turn holding markers. The latter
is due to the baseline starting to present as soon as
possible, so that whenever all confidence scores are
too low to start presenting, a turn holding marker
is generated. The learning agent learns to outper-
form all baselines significantly, by presenting infor-
mation slots in decreasing order of confidence, com-
bined with waiting and holding the floor at appro-
priate moments. Anticipating the rewards for wait-
ing vs. holding the floor at particular moments is the
main reason that the learnt policy outperforms Base-
line 3. Subtle moments of timing as in this case are
difficult to hand-craft and more appropriately bal-
anced using optimisation. An absolute comparison
of the last 1000 episodes of each behaviour shows
that the improvement of the RL agent corresponds
to 126.8% over Baseline 1, to 137.7% over Baseline
2 and to 16.76% over Baseline 3. All differences are
significant at p < 0.001 according to a paired t-test
and have a high effect size r > 0.9. The high per-
centage improvement of the learnt policy over Base-
lines 1 and 2 is mainly due to the high numeric val-
ues chosen for the rewards as can be observed from
their qualitative behaviour. Thus, if the negative nu-
meric values of, e.g., a self-correction were reduced,
the percentage reward would reduce, but the pol-
icy would not change qualitatively. Figure 1 shows
some examples of the learnt policy including several
incremental phenomena. In contrast, Figure 6 shows
examples generated with the baselines.
7 Conclusion and Future Directions
We have presented a novel framework combining in-
cremental and statistical approaches to NLG for in-
teractive systems. In a proof-of-concept study in the
domain of Information Presentation, we optimised
the timing and order of IP. The learning agent op-
timises the trade-off of whether to present informa-
tion as soon as it becomes available (for high respon-
siveness) or else to wait until input hypotheses were
more stable (to avoid self-corrections). Results in a
simulated environment showed that the agent learns
to avoid self-corrections and long waiting times, of-
ten by presenting information in order of decreas-
ing confidence. It outperforms three hand-crafted
baselines due to its enhanced adaptivity. In pre-
vious work, incremental responsiveness has mainly
been implemented by producing semantically empty
fillers such as um, let me see, well, etc. (Skantze and
Hjalmarsson, 2010). Our work avoids the need for
these fillers by content reordering.
Since this paper has focused on a proof-of-
concept study, our goal has not been to demonstrate
the superiority of automatic optimisation over hand-
crafted behaviour. Previous studies have shown
the advantages of optimisation (Janarthanam and
Lemon, 2010; Rieser et al, 2010; Dethlefs et al,
2011). Rather, our main goal has been to demon-
strate that incremental NLG can be phrased as an op-
timisation problem and that reasonable action poli-
cies can be learnt so that an application within an
incremental framework is feasible. This observation
allows us to take incremental systems, which so far
have been restricted to deterministic decision mak-
ing, one step further in terms of their adaptability
and flexibility. To demonstrate the effectiveness of
a synergy between RL and incremental NLG on a
large scale, we would like to train a fully incremental
NLG system from human data using a data-driven
reward function. Further, an evaluation with human
users will be required to verify the advantages of dif-
ferent policies for Information Presentation.
Regarding the scalability of our optimisation
framework, RL systems are known to suffer from the
curse of dimensionality, the problem that their state
space grows exponentially according to the number
of variables taken into account. While the appli-
cation of flat RL is therefore limited to small-scale
problems, we can use RL with a divide-and-conquer
approach, hierarchical RL, which has been shown to
scale to large-scale NLG applications (Dethlefs and
Cuaya?huitl, 2011), to address complex NLG tasks.
Future work can take several directions. Cur-
rently, we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situ-
ations could be achieved if the agent was able to
learn from ongoing interactions using online learn-
ing. In addition, current NLG systems tend to as-
sume that the user?s goals and situational circum-
stances are known with certainty. This is often an
unrealistic assumption that future work could ad-
dress using POMDPs (Williams and Young, 2007).
56
Acknowledgements
The research leading to this work has received fund-
ing from EC?s FP7 programmes: (FP7/2011-14)
under grant agreement no. 287615 (PARLANCE);
(FP7/2007-13) under grant agreement no. 216594
(CLASSiC); (FP7/2011-14) under grant agreement
no. 270019 (SPACEBOOK); (FP7/2011-16) under
grant agreement no. 269427 (STAC).
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proc. of EMNLP, pages 502?512.
Timo Baumann and David Schlangen. 2011. Predict-
ing the Micro-Timing of User Input for an Incremental
Spoken Dialogue System that Completes a User?s On-
going Turn. In Proc. of 12th Annual SIGdial Meeting
on Discourse and Dialogue, Portland, OR.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language Engi-
neering, 14(4):431?455.
Okko Buss and David Schlangen. 2011. DIUM?An In-
cremental Dialogue Manager That Can Produce Self-
Corrections. In Proc. of the Workshop on the Seman-
tics and Pragmatics of Dialogue (SemDIAL / Los An-
gelogue), Los Angeles, CA.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proc. of 11th Annual SIG-
dial Meeting on Discourse and Dialogue.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proc. of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.
2011. Optimising Natural Language Generation Deci-
sion Making for Situated Dialogue. In Proceedings of
the 12th Annual Meeting on Discourse and Dialogue
(SIGdial), Portland, Oregon, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Proc.
of the 10th Annual SigDial Meeting on Discourse and
Dialogue, Queen Mary University, UK.
Srini Janarthanam and Oliver Lemon. 2010. Learning to
Adapt to Unknown Users: Referring Expression Gen-
eration in Spoken Dialogue Systems. In Proc. of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Willem Levelt. 1989. Speaking: From Intenion to Artic-
ulation. MIT Press.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1552?1561.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proc. of the 10th Conference of the North
American Chapter of the Association for Compu-
tational Linguistics?Human Language Technologies
(NAACL-HLT), Boulder, Colorado.
Verena Rieser and Oliver Lemon. 2011. Reinforcement
Learning for Adaptive Dialogue Systems: A Data-
driven Methodology for Dialogue Management and
Natural Language Generation. Book Series: The-
ory and Applications of Natural Language Processing,
Springer, Berlin/Heidelberg.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Dia-
logue Systems. In Proc. of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
Uppsala, Sweden.
David Schlangen and Gabriel Skantze. 2009. A General,
Abstract Model of Incremental Dialogue Processing.
In Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proc. of the 11th Annual SigDial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialogue systems. In
57
Proc. of the Annual Meeting of the Association for
Computational Linguistics.
Kees van Deemter. 2009. What game theory can do for
NLG: the case of vague language. In 12th European
Workshop on Natural Language Generation (ENLG).
Marilyn Walker, Steve Whittaker, Amanda Stent, Pre-
taam Maloor, Johanna Moore, and G Vasireddy.
2004. Generation and Evaluation of User Tailored Re-
sponses in Multimodal Dialogue. Cognitive Science,
28(5):811?840.
Chris Watkins. 1989. Learning from Delayed Rewards.
PhD Thesis, King?s College, Cambridge, UK.
Jason Williams and Steve Young. 2007. Partially
Observable Markov Decision Processes for Spoken
Dialog Systems. Computer Speech and Language,
21(2):393?422.
58
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 7?8,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Dialogue Systems Using Online Learning: Beyond Empirical Methods ?
Heriberto Cuaya?huitl
German Research Center for Artificial Intelligence
Saarbru?cken, Germany
hecu01@dfki.de
Nina Dethlefs
Heriot-Watt University
Edinburgh, Scotland
n.s.dethlefs@hw.ac.uk
Abstract
We discuss a change of perspective for train-
ing dialogue systems, which requires a shift
from traditional empirical methods to online
learning methods. We motivate the applica-
tion of online learning, which provides the
benefit of improving the system?s behaviour
continuously often after each turn or dialogue
rather than after hundreds of dialogues. We
describe the requirements and advances for di-
alogue systems with online learning, and spec-
ulate on the future of these kinds of systems.
1 Motivation
Important progress has been made in empirical
methods for training spoken or multimodal dialogue
systems over the last decade. Nevertheless, a differ-
ent perspective has to be embraced if we want dia-
logue systems to learn on the spot while interacting
with real users. Typically, empirical methods op-
erate cyclically as follows: collect data, provide the
corresponding annotations, train a statistical or other
machine learning model, evaluate the performance
of the learned model, and if satisfactory, deploy the
trained model in a working system. The disadvan-
tage of this approach is that while data is still be-
ing collected subsequent to deployment, the system
does not optimize its behaviour anymore (cf. step-
wise learning, the solid blue line in Fig. 1). In con-
trast, dialogue systems with online learning tackle
this limitation by learning a machine learning model
?This research was funded by the EC?s FP7 programmes
under grant agreement no. ICT-248116 (ALIZ-E) and under
grant agreement no. 287615 (PARLANCE).
Tr
ai
ni
ng
 a
ct
iv
ity
Online
learning
Offline
learning
Collected dialogues
Figure 1: Learning approaches for dialogue systems.
Whilst offline learning aims for discontinuous learning,
online learning aims for continuous learning while inter-
acting with users in a real environment.
continuously often from unlabeled or minimally la-
beled data (cf. dotted red line in Fig. 1). So whilst
empirical methods train models after hundreds of di-
alogues, online learning methods refine the system
models after each user turn or each dialogue. In the
rest of the paper we discuss the requirements, ad-
vances and potential future of these kind of systems.
2 Online Learning Systems: Requirements
Several requirements arise for the development of
successful online learning systems. First of all, they
need to employ methods that are scalable for real-
world systems and the modelling of knowledge in
sufficient detail. Second, efficient learning is a pre-
requisite for learning from an ongoing interaction
without causing hesitations or pauses for the user.
Third, learnt models should satisfy a stability crite-
rion that guarantees that the learning agent?s perfor-
mance does not deteriorate over time, e.g. over the
course of a number of interactions, due to the newly
accumulated knowledge and behaviours. Fourth,
7
systems should employ a knowledge transfer ap-
proach in which they master new tasks they are con-
fronted with over their life span by transferring gen-
eral knowledge gathered in previous tasks. Fifth, on-
line learning sytems should adopt a lifelong learn-
ing approach, arguably without stopping learning.
This implies making use of large data sets, which
can be unlabeled or partially labeled due to the costs
that they imply. Finally, in the limit of updating the
learned models after every user turn, the online and
offline learning methods could be the same as long
as they meet the first three requirements above.
3 Online Learning Systems: Advances
Several authors have recognised the potential bene-
fits of online learning methods in previous work.
Thrun (1994) presents a robot for lifelong learn-
ing that learns to navigate in an unknown office en-
vironment by suggesting to transfer general purpose
knowledge across tasks. Bohus et al (2006) de-
scribe a spoken dialogue system that learns to op-
timise its non-understanding recovery strategies on-
line through interactions with human users based on
pre-trained logistic regression models. Cuaya?huitl
and Dethlefs (2011) present a dialogue system in the
navigation domain that is based on hierarchical rein-
forcement learning and Bayesian Networks and re-
learns its behaviour after each user turn, using indi-
rect feedback from the user?s performance. Gas?ic? et
al. (2011) present a spoken dialogue system based
on Gaussian Process-based Reinforcement Learn-
ing. It learns directly from binary feedback that
users assign explicitly as rewards at the end of each
dialogue and that indicate whether users were happy
or unhappy with the system?s performance. From
these previous investigations, we can observe that
online learning systems can take both explicit and/or
implicit feedback to refine their trained models.
4 Online Learning Systems: Future
While previous work has made important steps, the
problem of lifelong learning for spoken dialogue
systems is far from solved. Especially the follow-
ing challenges will need to receive attention: (a) fast
learning algorithms that can retrain behaviours after
each user turn with stable performance; and (b) scal-
able methods for optimizing multitasked behaviours
at different levels and modalities of communication.
In addition, we envision online learning systems
with the capability of transfering knowledge across
systems and domains. For example: a dialogue act
classifier, an interaction strategy, or a generation
strategy can be made transferable to similar tasks.
This could involve reasoning mechanisms to infer
what is known/unknown based on past experiences.
The idea of learning from scratch every time a new
system is constructed will thus be avoided. In this
regard, the role of the system developer in these
kinds of systems is not only to specify the system?s
tasks and learning environment, but to constrain and
bootstrap the system behaviour for faster learning.
All of these capabilities will be possible using on-
line learning with a lifelong learning perspective.
5 Tools and Data
Currently there are software tools for training mod-
els but they are more suitable for offline learning.1
Software tools for online learning remain to be de-
veloped and shared with the community. In addi-
tion, since building a dialogue system typically re-
quires a tremendous amount of effort, researchers
working on learning approaches should agree on
standards to facilitate system development. Finally,
since dialogue data is an often lacking resource in
the community, the online learning perspective may
contribute towards reducing the typical chicken and
egg problem, due to dialogue knowledge being more
readily transferable across domains, subject to on-
line adaption towards particular domains.
References
Dan Bohus, Brian Langner, Antoine Raux, Alan Black,
Maxine Eskenazi, and Alexander Rudnicky. 2006.
Online Supervised Learning of Non-Understanding
Recovery Policies. In Proc. IEEE SLT.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011. Optimiz-
ing Situated Dialogue Management in Unknown Envi-
ronments. In Proc. INTERSPEECH.
Milica Gas?ic?, Filip Jurc???c?ek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-line policy optimisation
of spoken dialogue systems via interaction with human
subjects. In Proc. IEEE ASRU.
Sebastian Thrun. 1994. A Lifelong Learning Perspective
for Mobile Robot Control. In Proc. IEEE/RSJ/GI.
1www.cs.waikato.ac.nz/ml/weka/
8
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 15?16,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Incremental Spoken Dialogue Systems: Tools and Data
Helen Hastie, Oliver Lemon, Nina Dethlefs
The Interaction Lab, School of Mathematics and Computer Science
Heriot-Watt University, Edinburgh, UK EH14 4AS
h.hastie, o.lemon, n.s.dethlefs@hw.ac.uk
Abstract
Strict-turn taking models of dialogue do not
accurately model human incremental process-
ing, where users can process partial input and
plan partial utterances in parallel. We discuss
the current state of the art in incremental sys-
tems and propose tools and data required for
further advances in the field of Incremental
Spoken Dialogue Systems.
1 Incremental Spoken Dialogue Systems
For Spoken Dialogue Systems (SDS) to be more fre-
quently adopted, advances in the state-of-the-art are
necessary to enable highly responsive and conversa-
tional systems. Traditionally, the unit of speech has
been a whole utterance with strict, rigid turn-taking
determined by a voice-activity detector. However,
a large body of psycholinguistic literature indicates
that human-human interaction is in fact incremen-
tal (Tanenhaus and Brown-Schmidt, 2008; Levelt,
1989). Using a whole utterance as the unit of choice
makes dialogues longer, unnatural and stilted and ul-
timately interferes with a user?s ability to focus on
their goal (Allen et al, 2001).
A new generation of Incremental SDS (ISDS) are
being developed that deal with ?micro-turns? (sub-
utterance processing units) resulting in dialogues
that are more fluid and responsive. Recent work
has shown that processing smaller ?chunks? of input
and output can improve the user experience (Aist et
al., 2007; Skantze and Schlangen, 2009; Buss et al,
2010; Baumann et al, 2011; Selfridge et al, 2011).
Incrementality enables the system designer to model
several dialogue phenomena that play a vital role
in human discourse (Levelt, 1989) but have so far
been absent from systems. These include more
natural turn-taking through rapid system responses,
grounding through the generation of backchannels
and feedback, and barge-ins (from both user and sys-
tem). In addition, corrections and self-corrections
through constant monitoring of user and system ut-
terances play an important role, enabling the system
to recover smoothly from a recognition error or a
change in user?s preferences. Some examples of the
phenomena we are targeting are given in Figure 1.
Parlance, a FP7 EC project1, is currently develop-
ing incremental systems for English and Mandarin.
The goal of Parlance is to develop mobile, interac-
tive, ?hyper-local? search through speech. Recent
trends in Information Retrieval are towards incre-
mental, interactive search. Spoken dialogue systems
can provide a truly natural medium for this type of
search, in particular for people on the move.
2 Tools and Data
The emphasis of the Parlance project is on data-
driven techniques for ISDS, thereby addressing the
problem of a lack of data for system develop-
ment. Although incremental dialogue phenomena
described in Figure 1 have been observed in human-
human dialogue, more task-based data is needed. It
is challenging to fabricate a situation where users
produce incremental discourse phenomena as in Fig-
ure 1 frequently and in a natural manner. Wizard-
1http://www.parlance-project.eu
15
Backchannels (when the user pauses)
USR I want Italian food [500 ms] in the centre of town . . .
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range,. . .
Self-correction (the system made a mistake)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 Indian restaurants . . .
USR No, I want Italian.
SYS oh sorry . . .
SYS I have 24 Italian restaurants in the city centre . . .
Holding the floor
USR I want cheap Italian food . . .
SYS ok let me see
SYS I have 3 cheap Italian places . . .
Figure 1: Incremental phenomena observed in human-
human dialogue that systems should be able to model.
of-Oz experiments can be used to collect data from
the system side, but user-initiated phenomena, such
as the user changing his/her mind are more difficult
to instigate. Therefore, data collections of naturally
occurring incremental phenomena in human-human
settings will be essential for further development of
incremental systems. Such data can inform user sim-
ulations which provide means of training stochastic
SDS with less initial data and can compensate for
data sparsity. For example, in Dethlefs et al (2012)
the user simulation can change its mind and react to
different NLG strategies such as giving information
with partial input or waiting for complete input from
the user. Both the academic community and industry
would benefit from open access data, such as will be
collected in the Parlance project and made available
to the dialogue community2. There would also need
to be a clear path from academic research on ISDS
to industry standards such as VoiceXML to facilitate
adoption.
Various components and techniques of ISDS are
needed to handle ?micro-turns?. Challenges here
include recognizing and understanding partial user
input and back-channels; micro-turn dialogue man-
agement that can decide when to back-channel, self-
correct and hold-the-floor; incremental NLG that
can generate output while the user is still talking;
2As was done for CLASSiC project data at:
http://www.macs.hw.ac.uk/iLabArchive/CLASSiCProject/Data/login.php
and finally more flexible TTS that can handle barge-
in and understand when it has been interrupted.
In summary, in order to achieve highly natural,
responsive incremental systems, we propose using
data-driven techniques, for which the main issue is
lack of data. Carefully crafted task-based human-
human data collection and WoZ studies, user simu-
lations, shared data archives, and upgraded industry
standards are required for future work in this field.
Acknowledgments
The research leading to this work has received fund-
ing from the EC?s FP7 programme: (FP7/2011-14)
under grant agreement no. 287615 (PARLANCE).
References
Gregory Aist, James Allen, Ellen Campana, Lucian
Galescu, Carlos Gomez Gallo, Scott Stoness, Mary
Swift, and Michael Tanenhaus. 2007. Software ar-
chitectures for incremental understanding of human
speech. In Proceedings of SemDial / DECALOG.
James Allen, George Ferguson, and Amanda Stent. 2001.
An Architecture For More Realistic Conversational
Systems. In Proc. of Intelligent User Interfaces.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proc. of SIGDIAL.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need for
Fillers. In Proc of INLG, Chicago, Illinois, USA.
Willem Levelt. 1989. Speaking: From Intenion to Artic-
ulation. MIT Press.
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in Incre-
mental Speech Recognition. In Proc. of SigDial.
Gabriel Skantze and David Schlangen. 2009. Incremen-
tal Dialogue Processing in a Micro-Domain. In Proc.
of EACL, Athens, Greece.
M.K. Tanenhaus and S. Brown-Schmidt. 2008. Lan-
guage processing in the natural world. In B.C.M
Moore, L.K. Tyler, and W.D. Marslen-Wilson, edi-
tors, The perception of speech: from sound to meaning,
pages 1105?1122.
16
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the SIGDIAL 2013 Conference, pages 314?318,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Impact of ASR N-Best Information on Bayesian Dialogue Act Recognition
Heriberto Cuaya?huitl, Nina Dethlefs, Helen Hastie, Oliver Lemon
School of Mathematical and Computer Sciences,
Heriot-Watt University, Edinburgh, UK
{h.cuayahuitl,n.s.dethlefs,h.hastie,o.lemon}@hw.ac.uk
Abstract
A challenge in dialogue act recognition
is the mapping from noisy user inputs to
dialogue acts. In this paper we describe
an approach for re-ranking dialogue act
hypotheses based on Bayesian classifiers
that incorporate dialogue history and Au-
tomatic Speech Recognition (ASR) N-best
information. We report results based on
the Let?s Go dialogue corpora that show
(1) that including ASR N-best information
results in improved dialogue act recogni-
tion performance (+7% accuracy), and (2)
that competitive results can be obtained
from as early as the first system dialogue
act, reducing the need to wait for subse-
quent system dialogue acts.
1 Introduction
The primary challenge of a Dialogue Act Recog-
niser (DAR) is to find the correct mapping be-
tween a noisy user input and its true dialogue
act. In standard ?slot-filling? dialogue sys-
tems a dialogue act is generally represented as
DialogueActType(attribute-value pairs), see Sec-
tion 3. While a substantial body of research has
investigated different types of models and meth-
ods for dialogue act recognition in spoken dia-
logue systems (see Section 2), here we focus on
re-ranking the outputs of an existing DAR for eval-
uation purposes. In practice the re-ranker should
be part of the DAR itself. We propose to use mul-
tiple Bayesian classifiers to re-rank an initial set
of dialogue act hypotheses based on information
from the dialogue history as well as ASR N-best
lists. In particular the latter type of information
helps us to learn mappings between dialogue acts
and common mis-recognitions. We present exper-
imental results based on the Let?s Go dialogue cor-
pora which indicate that re-ranking hypotheses us-
ing ASR N-best information can lead to improved
recognition. In addition, we compare the recogni-
tion accuracy over time and find that high accuracy
can be obtained with as little context as one system
dialogue act, so that there is often no need to take
a larger context into account.
2 Related Work
Approaches to dialogue act recognition from spo-
ken input have explored a wide range of meth-
ods. (Stolcke et al, 2000) use HMMs for dialogue
modelling, where sequences of observations cor-
respond to sequences of dialogue act types. They
also explore the performance with decision trees
and neural networks and report their highest ac-
curacy at 65% on the Switchboard corpus. (Zim-
mermann et al, 2005) also use HMMs in a joint
segmentation and classification model. (Grau et
al., 2004) use a combination of Naive Bayes and
n-grams with different smoothing methods. Their
best models achieve an accuracy of 66% on En-
glish Switchboard data and 89% on a Spanish cor-
pus. (Sridhar et al, 2009; Wright et al, 1999)
both use a maximum entropy classifier with n-
grams to classify dialogue acts using prosodic fea-
tures. (Sridhar et al, 2009) report an accuracy of
up to 74% on Switchboard data and (Wright et al,
1999) report an accuracy of 69% on the DCIEM
Maptask Corpus. (Bohus and Rudnicky, 2006)
maintain an N-best list of slot values using logis-
tic regression. (Surendran and Levow, 2006) use
a combination of linear support vector machines
(SVMs) and HMMs. They report an accuracy of
65.5% on the HCRC MapTask corpus and con-
clude that SVMs are well suited for sparse text and
dense acoustic features. (Gamba?ck et al, 2011)
use SVMs within an active learning framework.
They show that while passive learning achieves an
accuracy of 77.8% on Switchboard data, the ac-
tive learner achieves up to 80.7%. (Henderson et
al., 2012) use SVMs for dialogue act recognition
from ASR word confusion networks.
314
Speech 
Recognizer
Dialogue Act 
Recognizer Dialogue Act Re-Ranker
n-best 
list
n-best 
list
n-best 
list
(e.g. Let's Go parser)
speech
scored re-scored
  Following 
Components
Figure 1: Pipeline architecture for dialogue act recognition and re-ranking component. Here, the input
is a list of dialogue acts with confidence scores, and the output is the same list of dialogue acts but with
recomputed confidence scores. A dialogue act is represented as DialogueActType(attribute-value pairs).
Several authors have presented evidence in
favour of Bayesian methods. (Keizer and op den
Akker, 2007) have shown that Bayesian DARs
can outperform baseline classifiers such as deci-
sion trees. More generally, (Ng and Jordan, 2001)
show that generative classifiers (e.g. Naive Bayes)
reach their asymptotic error faster than discrimina-
tive ones. As a consequence, generative classifiers
are less data intensive than discriminative ones.
In addition, several authors have investigated
dialogue belief tracking. While our approach
is related to belief tracking, we focus here on
spoken language understanding under uncertainty
rather than estimating user goals. (Williams, 2007;
Thomson et al, 2008) use approximate inference
to improve the scalability of Bayes nets for be-
lief tracking and (Lison, 2012) presents work on
improving their scalability through abstraction.
(Mehta et al, 2010) model user intentions through
the use of probabilistic ontology trees.
Bayes nets have also been applied to other
dialogue-related tasks, such as surface realisa-
tion within dialogue (Dethlefs and Cuaya?huitl,
2011) or multi-modal dialogue act recognition
(Cuaya?huitl and Kruijff-Korbayova?, 2011). In the
following, we will explore a dialogue act recogni-
tion technique based on multiple Bayesian classi-
fiers and show that re-ranking with ASR N-best in-
formation can improve recognition performance.
3 Re-Ranking Dialogue Acts Using
Multiple Bayesian Networks
Figure 1 shows an illustration of our dialogue act
re-ranker within a pipeline architecture. Here, pro-
cessing begins with the user?s speech being inter-
preted by a speech recogniser, which produces a
first N-best list of hypotheses. These hypotheses
are subsequently passed on and interpreted by a
dialogue act recogniser, which in our case is rep-
resented by the Let?s Go parser. The parser pro-
duces a first set of dialogue act hypotheses, based
on which our re-ranker becomes active. A full
dialogue act in our scenario consists of three el-
ements: dialogue act types, attributes (or slots),
and slot values. An example dialogue act is in-
form(from=Pittsburgh Downtown). The dialogue
act re-ranker thus receives a list of hypotheses
in the specified form (triples) from its preceding
module (a DAR or in our case the Let?s Go parser)
and its task is to generate confidence scores that
approximate true label (i.e. the dialogue act really
spoken by a user) as closely as possible.
We address this task by using multiple Bayesian
classifiers: one for classifying a dialogue act type,
one for classifying a set of slots, and the rest for
classifying slot values. The use of multiple classi-
fiers is beneficial for scalability purposes; for ex-
ample, assuming 10 dialogue act types, 10 slots,
10 values per slot, and no other dialogue con-
text results in a joint distribution of 1011 parame-
ters. Since a typical dialogue system is required to
model even larger joint distributions, our adopted
approach is to factorize them into multiple inde-
pendent Bayesian networks (with combined out-
puts). A multiple classifier system is a power-
ful solution to complex classification problems in-
volving a large set of inputs and outputs. This
approach not only decreases training time but has
also been shown to increase the performance of
classification (Tax et al, 2000).
A Bayesian Network (BN) models a joint prob-
ability distribution over a set of random variables
and their dependencies, see (Bishop, 2006) for
an introduction to BNs. Our motivation for us-
ing multiple BNs is to incorporate a fairly rich di-
alogue context in terms of what the system and
user said at lexical and semantic levels. In con-
trast, using a single BN for all slots with rich di-
alogue context faces scalability issues, especially
for slots with large numbers of domain values,
and is therefore not an attractive option. We
denote our set of Bayesian classifiers as ? =
{?dat, ?att, ..., ?val(i)}, where BN ?dat is used to
rank dialogue act types, BN ?att is used to rank
attributes, and the other BNs (?val(i)) are used to
315
rank values for each slot i. The score of a user
dialogue act (< d, a, v >) is computed as:
P (d, a, v) = 1Z
?
P (d|pad)P (a|paa)P (v|pav),
where d is a dialogue act type, a is an attribute
(or slot), v is a slot value, pax is a parent random
variable, andZ is a normalising constant. This im-
plies that the score of a dialogue act is the product
of probabilities of dialogue act type and slot-value
pairs. For dialogue acts including multiple slot-
value pairs, the product above can be extended ac-
cordingly. The best and highest ranked hypothesis
(from space H) can be obtained according to:
< d, a, v >?= arg max
<d,a,v>?H
P (d, a, v).
In the following, we describe our experimental
setting. Here, the structure and parameters of our
classifiers will be estimated from a corpus of spo-
ken dialogues, and we will use the equations above
for re-ranking user dialogue acts. Finally, we re-
port results comparing Bayesian classifiers that
make use of ASR N-best information and dialogue
context against Bayesian classifiers that make pre-
dictions based on the dialogue context alone.
4 Experiments and Results
4.1 Data
Our experiments are based on the Let?s Go corpus
(Raux et al, 2005). Let?s Go contains recorded in-
teractions between a spoken dialogue system and
human users who make enquiries about the bus
schedule in Pittsburgh. Dialogues are driven by
system-initiative and query the user sequentially
for five slots: an optional bus route, a departure
place, a destination, a desired travel date, and a
desired travel time. Each slot needs to be explic-
itly (or implicity) confirmed by the user. Our anal-
yses are based on a subset of this data set contain-
ing 779 dialogues with 7275 turns, collected in the
Summer of 2010. From these dialogues, we used
70% for training our classifiers and the rest for
testing (with 100 random splits). Briefly, this data
set contains 12 system dialogue act types1, 11 user
dialogue act types2, and 5 main slots with varia-
tions3. The number of slot values ranges between
1ack, cant help, example, expl conf, go back, hello,
impl conf, more buses, request, restart, schedule, sorry.
2affirm, bye, go back, inform, negate, next bus, prevbus,
repeat, restart, silence, tellchoices.
3date.absday, date.abmonth, date.day, date.relweek, from,
route, time.ampm, time.arriveleave, time.hour, time.minute,
time.rel, to.
*
Figure 2: Bayesian network for probabilistic rea-
soning of locations (variable ?from desc?), which
incorporates ASR N-best information in the vari-
able?from desc nbest? and dialogue history in-
formation in the remaining random variables.
102 and 103 so that the combination of all possi-
ble dialogue act types, attributes and values leads
to large amounts of triplets. While the majority
of user inputs contain one user dialogue act, the
average number of system dialogue acts per turn
is 4.2. Note that for the user dialogue act types,
we also model silence explicitly. This is often not
considered in dialogue act recognisers: since the
ASR will always try to recognise something out
of any input (even background noise), typical dia-
logue act recognisers will then try to map the ASR
output onto a semantic interpretation.
4.2 Bayesian Networks
We trained our Bayesian networks in a supervised
learning manner and used 43 discrete features (or
random variables) plus a class label (also discrete).
The feature set is described by three main subsets:
25 system-utterance-level binary features4 derived
from the system dialogue act(s) in the last turn; 17
user-utterance-level binary features5 derived from
(a) what the user heard prior to the current turn,
or (b) what keywords the system recognised in its
4System utterance features: heardAck, heardCantHelp,
heardExample, heardExplConf, heardGoBackDAT, heard-
Hello, heardImplConf, heardMoreBuses, heardRequest,
heardRestartDAT, heardSchedule, heardSorry, heardDate,
heardFrom, heardRoute, heardTime, heardTo, heardNext,
heardPrevious, heardGoBack, heardChoices, heardRestart,
heardRepeat, heardDontKnow, lastSystemDialActType.
5User utterance features: hasRoute, hasFrom, hasTo, has-
Date, hasTime, hasYes, hasNo, hasNext, hasPrevious, has-
GoBack, hasChoices, hasRestart, hasRepeat, hasDontKnow,
hasBye, hasNothing, duration in secs. (values=0,1,2,3,4,>5).
316
list of speech recognition hypotheses; and 1 word-
level non-binary feature (* nbest) corresponding
to the slot values in the ASR N-best lists.
Figure 2 shows the Bayes net corresponding to
the classifier used to rank location names. The
random variable from desc is the class label, the
random variable from desc nbest (marked with an
asterisk) incorporates slot values from the ASR
N-best lists, and the remaining variables model
dialogue history context. The structure of our
Bayesian classifiers were derived from the K2 al-
gorithm6, and their parameters were derived from
maximum likelihood estimation. In addition, we
performed probabilistic inference using the Junc-
tion tree algorithm7. Based on these data and
tools, we trained 14 Bayesian classifiers: one for
scoring dialogue act types, one for scoring at-
tributes (slots), and the rest for scoring slot values.
4.3 Experimental Results
We compared 7 different dialogue act recognisers
in terms of classification accuracy. The compar-
ison was made against gold standard data from
a human-labelled corpus. (Semi-Random) is a
recogniser choosing a random dialogue act from
the Let?s Go N-best parsing hypotheses. (Inci) is
our proposed approach considering a context of i
system dialogue acts, and (Ceiling) is a recogniser
choosing the correct dialogue act from the Let?s
Go N-best parsing hypotheses. The latter was used
as a gold standard from manual annotations, which
reflects the proportion of correct labels in the N-
best parsing hypotheses.
We also assessed the impact of ASR N-best in-
formation on probabilistic inference. To this end,
we compared Bayes nets with a focus on the ran-
dom variable ?* nbest?, which in one case con-
tains induced distributions from data and in the
other case contains an equal distribution of slot
values. Our hypothesis is that the former setting
will lead to better performance.
Figure 3 shows the classification accuracy of
our dialogue act recognisers. The first point to no-
tice is that the incorporation of ASR N-best infor-
mation makes an important difference. The per-
formance of recogniser IncK (K being the num-
ber of system dialogue acts) is 66.9% without
ASR N-best information and 73.9% with ASR N-
best information (the difference is significant8 at
6www.cs.waikato.ac.nz/ml/weka/
7www.cs.cmu.edu/?javabayes/Home/8Based on a two-sided Wilcoxon Signed-Rank test.
Semi?Random Inc0 Inc1 Inc2 Inc3 IncK Ceiling40
45
50
55
60
65
70
75
80
85
90
Dialogue Act Recogniser
Clas
sifica
tion 
Accu
racy
 (%)
 
 
Without ASR N?Best InformationWith ASR N?Best Information
Figure 3: Bayesian dialogue act recognisers show-
ing the impact of ASR N-best information.
p < 0.05). The latter represents a substantial im-
provement over the semi-random baseline (62.9%)
and Lets Go dialogue act recognizer (69%), both
significant at p < 0.05. A second point to notice is
that the differences between Inci (? i>0) recognis-
ers were not significant. We can say that the use of
one system dialogue act as context is as competi-
tive as using a larger set of system dialogue acts.
This suggests that dialogue act recognition carried
out at early stages (e.g. after the first dialogue act)
in an utterance does not degrade recognition per-
formance. The effect is possibly domain-specific
and generalisations remain to be investigated.
Generally, we were able to observe that more
than half of the errors made by the Bayesian clas-
sifiers were due to noise in the environment and
caused by the users themselves, which interfered
with ASR results. Detecting when users do not
convey dialogue acts to the system is therefore still
a standing challenge for dialogue act recognition.
5 Conclusion and Future Work
We have described a re-ranking approach for user
dialogue act recognition. Multiple Bayesian clas-
sifiers are used to rank dialogue acts from a set of
dialogue history features and ASR N-best infor-
mation. Applying our approach to the Let?s Go
data we found the following: (1) that including
ASR N-best information results in improved di-
alogue act recognition performance; and (2) that
competitive results can be obtained from as early
as the first system dialogue act, reducing the need
to include subsequent ones.
Future work includes: (a) a comparison of our
317
Bayesian classifiers with other probabilistic mod-
els and forms of training (for example by us-
ing semi-supervised learning), (b) training dia-
logue act recognisers in different (multi-modal and
multi-task) domains, and (c) dealing with random
variables that contain very large domain values.
6 Acknowledgements
This research was funded by the EC FP7 pro-
gramme under grant agreement no. 287615 (PAR-
LANCE) and no. 270019 (SPACEBOOK).
Sample Re-Ranked User Inputs
User input: ?forty six d?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(route=46a) 3.33E-4 1.9236763E-6
inform(route=46b) 1.0E-6 1.5243509E-16
inform(route=46d) 0.096107 7.030841E-4
inform(route=46k) 0.843685 4.9941495E-10
silence() NA 0
User input: ?um jefferson hills to mckeesport?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(from=mill street) 7.8E-4 3.5998527E-16
inform(from=mission street) 0.015577 3.5998527E-16
inform(from=osceola street) 0.0037 3.5998527E-16
inform(from=robinson township) 0.007292 3.5998527E-16
inform(from=sheraden station) 0.001815 3.1346254E-8
inform(from=brushton) 2.45E-4 3.5998527E-16
inform(from=jefferson) 0.128727 0.0054255757
inform(from=mckeesport) 0.31030 2.6209198E-4
silence() NA 0
References
[Bishop2006] Christopher M. Bishop. 2006. Pattern Recog-
nition and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secaucus, NJ,
USA.
[Bohus and Rudnicky2006] D. Bohus and A. Rudnicky.
2006. A k hypotheses + other? belief updating model. In
AAAI Workshop on Statistical and Empirical Approaches
to Spoken Dialogue Systems.
[Cuaya?huitl and Kruijff-Korbayova?2011] H. Cuaya?huitl and
I. Kruijff-Korbayova?. 2011. Learning human-robot di-
alogue policies combining speech and visual beliefs. In
IWSDS, pages 133?140.
[Dethlefs and Cuaya?huitl2011] Nina Dethlefs and Heriberto
Cuaya?huitl. 2011. Combining Hierarchical Reinforce-
ment Learning and Bayesian Networks for Natural Lan-
guage Generation in Situated Dialogue. In ENLG, Nancy,
France.
[Gamba?ck et al2011] Bjo?rn Gamba?ck, Fredrik Olsson, and
Oscar Ta?ckstro?m. 2011. Active Learning for Dialogue
Act Classification. In INTERSPEECH, pages 1329?1332.
[Grau et al2004] Sergio Grau, Emilio Sanchis, Maria Jose
Castro, and David Vilar. 2004. Dialogue Act Classifi-
cation Using a Bayesian Approach. In SPECOM.
[Henderson et al2012] Matthew Henderson, Milica Gasic,
Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve
Young. 2012. Discriminative spoken language under-
standing using word confusion networks. In SLT, pages
176?181.
[Keizer and op den Akker2007] Simon Keizer and Rieks
op den Akker. 2007. Dialogue Act Recognition Under
Uncertainty Using Bayesian Networks. Natural Language
Engineering, 13(4):287?316.
[Lison2012] Pierre Lison. 2012. Probabilistic dialogue mod-
els with prior domain knowledge. In SIGDIAL Confer-
ence, pages 179?188.
[Mehta et al2010] Neville Mehta, Rakesh Gupta, Antoine
Raux, Deepak Ramachandran, and Stefan Krawczyk.
2010. Probabilistic ontology trees for belief tracking in
dialog systems. In SIGDIAL Conference, pages 37?46.
[Ng and Jordan2001] Andrew Y. Ng and Michael I. Jordan.
2001. On discriminative vs. generative classifiers: A com-
parison of logistic regression and naive bayes. In NIPS,
pages 841?848.
[Raux et al2005] Antoine Raux, Brian Langner, Dan Bohus,
Alan W. Black, and Maxine Eskenazi. 2005. Let?s
go public! Taking a Spoken Dialog System to the Real
World. In INTERSPEECH, pages 885?888.
[Sridhar et al2009] Vivek Kumar Rangarajan Sridhar, Srini-
vas Bangalore, and Shrikanth Narayanan. 2009. Com-
bining Lexical, Syntactic and Prosodic Cues for Improved
Online Dialog Act Tagging. Computer Speech & Lan-
guage, 23(4):407?422.
[Stolcke et al2000] Andreas Stolcke, Klaus Ries, Noah Coc-
caro, Elizabeth Shriberg, Rebecca A. Bates, Daniel Juraf-
sky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialog Act Modeling for Auto-
matic Tagging and Recognition of Conversational Speech.
Computational Linguistics, 26(3):339?373.
[Surendran and Levow2006] Dinoj Surendran and Gina-Anne
Levow. 2006. Dialog Act Tagging with Support Vec-
tor Machines and Hidden Markov Models. In INTER-
SPEECH.
[Tax et al2000] David M. Tax, Martijn van Breukelen,
Robert P. Duin, and Josef Kittler. 2000. Combining mul-
tiple classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475?1485, September.
[Thomson et al2008] Blaise Thomson, Jost Schatzmann, and
Steve Young. 2008. Bayesian update of dialogue state for
robust dialogue systems. In ICASSP, pages 4937?4940.
[Williams2007] Jason D. Williams. 2007. Using particle fil-
ters to track dialogue state. In ASRU, pages 502?507.
[Wright et al1999] H. Wright, Massimo Poesio, and Stephen
Isard. 1999. Using high level dialogue information for
dialogue act recognition using prosodic features. In Pro-
ceedings of an ESCA Tutorial and Research Workshop on
Dialogue and Prosody, pages 139?143, Eindhoven, The
Netherlands.
[Zimmermann et al2005] Matthias Zimmermann, Yang Liu,
Elizabeth Shriberg, and Andreas Stolcke. 2005. Toward
Joint Segmentation and Classification of Dialog Acts in
Multiparty Meetings. In MLMI, pages 187?193.
318
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
