Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 313?320
Manchester, August 2008
Tracking the Dynamic Evolution of Participant Salience in a Discussion
Ahmed Hassan
University of Michigan
hassanam@umich.edu
Anthony Fader
University of Michigan
afader@umich.edu
Michael H. Crespin
University of Georgia
crespin@uga.edu
Kevin M. Quinn
Harvard University
kquinn@fsa.harvard.edu
Burt L. Monroe
Pennsylvania State University
burtmonroe@psu.edu
Michael Colaresi
Michigan State University
colaresi@msu.edu
Dragomir R. Radev
University of Michigan
radev@umich.edu
Abstract
We introduce a technique for analyzing the
temporal evolution of the salience of par-
ticipants in a discussion. Our method can
dynamically track how the relative impor-
tance of speakers evolve over time using
graph based techniques. Speaker salience
is computed based on the eigenvector cen-
trality in a graph representation of partici-
pants in a discussion. Two participants in a
discussion are linked with an edge if they
use similar rhetoric. The method is dy-
namic in the sense that the graph evolves
over time to capture the evolution inher-
ent to the participants salience. We used
our method to track the salience of mem-
bers of the US Senate using data from the
US Congressional Record. Our analysis
investigated how the salience of speakers
changes over time. Our results show that
the scores can capture speaker centrality
in topics as well as events that result in
change of salience or influence among dif-
ferent participants.
1 Introduction
There are several sources of data that record
speeches or participations in debates or discus-
sions among a group of speakers or participants.
Those include parliamentary records, blogs, and
news groups. This data represents a very important
and unexploited source of information that con-
tains several trends and ideas. In any debate or
discussion, there are certain types of persons who
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
influence other people and pass information or ad-
vice to them. Those persons are often regarded
as experts in the field or simply influential peo-
ple and they tend to affect the ideas and rhetoric
of other participants. This effect can be tracked
down by tracking the similarity between different
speeches. We can then imagine a debate with many
people arguing about many different things as a
network of speeches or participations interacting
with each other. We can then try to identify the
most salient or important participants by identify-
ing the most central speeches in this network and
associating them with their speakers. When we
have a large dataset of debates and conversations
that expand over a long period of time, the salience
of participants becomes a dynamic property that
changes over time. To capture this dynamic nature
of the process, the graph of speeches must evolve
over time such that we have a different graph at
each instance of time that reflects the interaction
of speeches at this instant.
We apply our method to the US Congressional
Record. The US Congressional Record documents
everything said and done in the US Congress
House and Senate. The speeches in this data set
are made by a large number of people over a long
period of time. Using political speeches as test
data for the proposed method adds an extra layer
of meaning onto the measure of speakers salience.
Speaker salience of the Congress members can re-
flect the importance or influence in the US leg-
islative process. The way salience scores evolve
over time can answer several interesting issues like
how the influence of the speakers vary with major-
ity status and change of party control. It can also
study the dynamics of the relative distribution of
attention to each topic area in different time peri-
ods.
313
The rest of this paper will proceed as follows.
Section 2 reviews some related work. In Section 3,
we describe how the data can be clustered into dif-
ferent topic clusters. In Section 4, we describe
our method for computing the salience of different
participant in a discussion, we also describe how
to the network of speakers varies over time. Sec-
tion 5 describes the experimental setup. Finally,
we present the conclusions in Section 6.
2 Related Work
Several methods have been proposed for identify-
ing the most central nodes in a network. Degree
centrality, closeness, and betweenness (Newman,
2003) are among the most known methods for
measuring centrality of nodes in a network. Eigen-
vector centrality is another powerful method that
that has been applied to several types of networks.
For example it has been used to measure cen-
trality in hyperlinked web pages networks (Brin
and Page, 1998; Kleinberg, 1998), lexical net-
works (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Kurland and Lee, 2005; Kurland and
Lee, 2006), and semantic networks (Mihalcea et
al., 2004).
The interest of applying natural language pro-
cessing techniques in the area of political science
has been recently increasing.
(Quinn et al, 2006) introduce a multinomial
mixture model to cluster political speeches into
topics or related categories. In (Porter et al, 2005),
a network analysis of the members and committees
of the US House of Representatives is performed.
The authors prove that there are connections link-
ing some political positions to certain committees.
This suggests that there are factors affecting com-
mittee membership and that they are not deter-
mined at random. In (Thomas et al, 2006), the au-
thors try to automatically classify speeches, from
the US Congress debates, as supporting or oppos-
ing a given topic by taking advantage of the voting
records of the speakers. (Fader et al, 2007) in-
troduce MavenRank , which is a method based on
lexical centrality that identifies the most influen-
tial members of the US Senate. It computes a sin-
gle salience score for each speaker that is constant
over time.
In this paper, we introduce a new method for
tracking the evolution of the salience of partici-
pants in a discussion over time. Our method is
based on the ones described in (Erkan and Radev,
2004; Mihalcea and Tarau, 2004; Fader et al,
2007), The objective of this paper is to dynami-
cally rank speakers or participants in a discussion.
The proposed method is dynamic in the sense that
the computed importance varies over time.
3 Topic Clusters
Before applying the proposed method to a data
set with speeches in multiple topics, we first need
to divide the speech documents into topic clus-
ters. We used the model described in (Quinn et al,
2006) for this purpose. The model presented in this
paper assumes that the probabilities of a document
belonging to a certain topic varies smoothly over
time and the words within a given document have
exactly the same probability of being drawn from
a particular topic (Quinn et al, 2006). These two
properties make the model different than standard
mixture models (McLachlan and Peel, 2000) and
the latent Dirichlet alocation model of (Blei et al,
2003). The model of (Quinn et al, 2006) is most
closely related to the model of (Blei and Lafferty,
2006), who present a generalization of the model
used by (Quinn et al, 2006).
The output from the topic model is a D ? K
matrix Z where D is the number of speeches , K
is the number of topics and the element z
dk
repre-
sents the probability of the dth speech being gen-
erated by topic k. We then assign each speech d
to the kth cluster where k = argmax
j
z
dj
. If the
maximum value is not unique, one of the clusters
having the maximum value is arbitrary selected.
4 Speaker Centrality
In this section we describe how to build a network
of speeches and use it to identify speaker centrality.
We also describe how to generate different projec-
tions of the network at different times, and how
to use those projection to get dynamic salience
scores.
4.1 Computing Speaker Salience
The method we used is similar to the methods de-
scribed in (Erkan and Radev, 2004; Mihalcea and
Tarau, 2004; Kurland and Lee, 2005), which were
originally used for ranking sentences and docu-
ments in extractive summarization and information
retrieval systems.
A collection of speeches can be represented as
a network where similar speeches are linked to
each other. The proposed method is based on
314
the premise that important speeches tend to be
lexically similar to other important speeches, and
important speeches tend to belong to important
speakers. Hence given a collection of speeches and
a similarity measure, we can build a network and
define the centrality score of a speech recursively
in terms of the scores of other similar speeches.
Later, we can compute the salience of a speaker
as the sum of the centrality measure of all his
speeches.
To measure the similarity between two
speeches, we use the bag-of-words model to repre-
sent each sentence as an N-dimensional vector of
tf-idf scores, where N is the number of all possible
words in the target language. The similarity
between two speeches is then computed using the
cosine similarity between the two vectors.
A vector of term frequencies is used to represent
each speech. Those term frequencies are weighted
according to the relative importance of the given
term in the cluster.
The vectors representing speeches contain term
frequencies (or tf), which are weighted according
to their inverse document frequencies to account
for the relative importance of the given term in the
cluster. The inverse document frequency of a term
w is given by (Sparck-Jones, 1972)
idf(w) = log
(
N
n
w
)
(1)
where n
w
is the number of speeches in the clus-
ter containing the term w, and N is the number of
documents in the cluster. We calculated idf values
specific to each topic, rather than to all speeches.
We preferred to use topic-specific idf values be-
cause the relative importance of words may vary
from one topic to the other.
The tf-idf cosine similarity measure is computed
as the cosine of the angle between the tf-idf vec-
tors. It is defined as follows:
P
w?u,v
tf
u
(w) tf
v
(w) idf(w)
2
?
P
w?u
(tf
u
(w) idf(w))
2
?
P
w?v
(tf
v
(w) idf(w))
2
, (2)
The choice of tf-idf scores to measure speech
similarity is an arbitrary choice. Some other possi-
ble similarity measures are edit distance, language
models (Kurland and Lee, 2005), or generation
probabilities (Erkan, 2006).
The recursive definition of the score of any
speech s in the speeches network is given by
p(s) =
?
t?adj[s]
p(t)
deg(t)
(3)
where deg(t) is the degree of node t, and adj[s] is
the set of all speeches adjacent to s in the network.
This can be rewritten in matrix notation as:
p = pB (4)
where p = (p(s
1
), p(s
2
), . . . , p(s
N
)) and the ma-
trix B is the row normalized similarity matrix of
the graph
B(i, j) =
S(i, j)
?
k
S(i, k)
(5)
where S(i, j) = sim(s
i
, s
j
). Equation (4) shows
that the vector of salience scores p is the left eigen-
vector of B with eigenvalue 1.
The matrix B can be thought of as a stochastic
matrix that acts as transition matrix of a Markov
chain. An element X(i, j) of a stochastic matrix
specifies the transition probability from state i to
state j in the corresponding Markov chain. And
the whole process can be seen as a Markovian ran-
dom walk on the speeches graph. To help the ran-
dom walker escape from periodic or disconnected
components, (Brin and Page, 1998) suggests re-
serving a small escape probability at each node
that represents a chance of jumping to any node
in the graph, making the Markov chain irreducible
and aperiodic, which guarantees the existence of
the eigenvector.
Equation (4) can then be rewritten, assuming a
uniform escape probability, as:
p = p[dU+ (1 ? d)B] (6)
where N is the total number of nodes, U is a
square matrix with U(i, j) = 1/N for all i, j, and
d is the escape probability chosen in the interval
[0.1, 0.2] (Brin and Page, 1998).
4.2 Dynamic Salience Scores
We use the time stamps associated with the data to
compute dynamic salience scores p
T
(u) that iden-
tify central speakers at some time T . To do this,
we create a speech graph that evolves over time.
Let T be the current date and let u and v be two
speech documents that occur on days t
u
and t
v
.
Our goal is to discount the lexical similarity of u
and v based on how far apart they are. One way
to do this is by defining a new similarity measure
s(u, v;T ) as:
s(u, v;T ) = tf-idf-cosine(u, v) ? f(u, v;T ) (7)
315
where f(u, v;T ) is a function taking values in
[0, 1].
If f(u, v;T ) = 1 for all u, v, and T , then time is
ignored when calculating similarity and p
T
(u) =
p(u). On the other hand, suppose we let
f(u, v;T ) =
{
1 if t
u
= t
v
= T ,
0 else.
(8)
This removes all edges that link a speech, occur-
ring at some time T , to all other speeches occur-
ring at some time other than T and the ranking al-
gorithm will be run on what is essentially the sub-
graph of documents restricted to time T (although
the isolated speech documents will receive small
non-zero scores because of the escape probability
from Section 4.1). These two cases act as the ex-
treme boundaries of possible functions f : in the
first case time difference has no effect on document
similarity, while in the second case two documents
must occur on the same day to be similar.
We use the following time weight functions in
our experiments. In each case, we assume that the
speeches represented by speech documents u and v
have already occurred, that is, t
u
, t
v
? T . We will
use the convention that f(u, v;T ) = 0 if t
u
> T
or t
v
> T for all time weight functions, which
captures the idea that speeches that have not yet
occurred have no influence on the graph at time T .
Also define
age(u, v;T ) = T ? min{t
u
, t
v
} (9)
which gives the age of the oldest speech document
from the pair u, v at time T .
? Exponential: Given a parameter a > 0, define
f
exp,a
(u, v;T ) = e
?a age(u,v;T )
. (10)
This function will decrease the impact of sim-
ilarity as time increases in an exponential
fashion. a is a parameter that controls how
fast this happens, where a larger value of a
makes earlier speeches have a small impact
on current scores and a smaller value of a
means that earlier speeches will have a larger
impact on current scores.
? Linear: Given b > 0, define
f
lin,d
(u, v;T ) =
?
?
?
?
?
1 ?
1
b
age(u, v;T )
if age(u, v;T ) ? b
0 if age(u, v;T ) > b
(11)
Figure 1: The Dynamic boundary cases for Sena-
tor Santorum.
This function gives speech documents that
occur at time T full weight and then decreases
their weight linearly towards time T + b,
where it becomes 0.
? Boundary: Given d ? 0, define
f
bnd,d
(u, v;T ) =
{
1 if age(u, v;T ) ? d
0 if age(u, v;T ) > d
(12)
This function gives speech documents occur-
ring within d days of T the regular tf-idf sim-
ilarity score, but sets the similarity of speech
documents occurring outside of d days to 0.
The case when d = 0 is one of the boundary
cases explained above.
Figure 1 gives an example of different time
weighting functions for Senator Rick Santorum
(R - Pennsylvania) on topic 22 (Abortion) during
1997, the first session of the 105th Congress. The
dashed line shows the case when time has no ef-
fect on similarity (his score is constant over time),
while the solid line shows the case where only
speeches on the current day are considered simi-
lar (his score spikes only on days where he speaks
and is near zero otherwise). The dotted line shows
the case when the influence of older speeches de-
creases exponentially, which is more dynamic than
the first case but smoother than the second case.
5 Experiments and Results
5.1 Data
We used the United States Congressional Speech
corpus (Monroe et al, 2006) in our experiment.
316
This corpus is in XML formatted version of the
electronic United States Congressional Record
from the Library of Congress
1
. The Congressional
Record is a verbatim transcript of the speeches
made in the US House of Representatives and Sen-
ate and includes tens of thousands of speeches per
year (Monroe et al, 2006). The data we used cover
the period from January 2001 to January 2003.
5.2 Experimental Setup
We used results from (Quinn et al, 2006) to get
topic clusters from the data, as described in Sec-
tion 3. The total number of topics was 42. The
average sized topic cluster had several hundred
speech documents (Quinn et al, 2006).
We set up a pipeline using a Perl implementa-
tion of the proposed method We ran it on the topic
clusters and ranked the speakers based on the cen-
trality scores of their speeches. The graph nodes
were speech documents. A speaker?s score was
determined by the average of the scores of the
speeches given by that speaker. After comparing
the different time weighting function as shown in
Figure 1, we decided to use the exponential time
weight function for all the experiments discussed
below. Exponential time weighting function de-
creases the impact of similarity as time increases
in an exponential fashion. It also allows us to con-
trol the rate of decay using the parameter a.
5.3 Baseline
We compare the performance of our system to
a simple baseline that calculates the salience of
a speaker as a weighted count of the number of
times he has spoken. The baseline gives high
weight to recent speeches . The weight decreases
as the speeches gets older. The salience score of a
speaker is calculate as follows:
BS(i) =
?
d
?
d
0
?d
? S
i
d
(13)
Where BS(i) is the baseline score of speaker i,
? is the discounting factor, d
0
is the current date,
and S
i
d
is the number of speeches made by speaker
i at date d. We used ? = 0.9 for all our experi-
ments.
5.4 Results
One way to evaluate the dynamic salience scores,
is to look at changes when party control of the
1
http://thomas.loc.gov
chamber switches. Similar to (Hartog and Mon-
roe, 2004), we exploit the party switch made by
Senator Jim Jeffords of Vermont and the result-
ing change in majority control of the Senate dur-
ing the 107th Congress as a quasi-experimental
design. In short, Jeffords announced his switch
on May 24, 2001 from Republican to Independent
status, effective June 6, 2001. Jeffords stated that
he would vote with the Democrats to organize the
Senate, giving the Democrats a one-seat advantage
and change control of the Senate from the Repub-
licans back to the Democrats. This change of ma-
jority status during the 107th Congress allows us
to ignore many of the factors that could potentially
influence dynamic salience scores at the start of a
new congress.
On average, we expect committee chairs or a
member of the majority party to be the most im-
portant speaker on each topic followed by ranking
members or a member of the minority party. If
our measure is capturing dynamics in the central-
ity of Senators, we expect Republicans to be more
central before the Jeffords switch and Democrats
becoming central soon afterwards, assuming the
topic is being discussed on the Senate floor. We
show that the proposed technique captures several
interesting events in the data and also show that the
baseline explained above fails to capture the same
set of events.
Figure 2(a) shows the dynamic salience scores
over time for Senator John McCain (R - Arizona)
and Senator Carl Levin (D - Michigan) on topic
5 (Armed Forces 2) for the 107th Senate. Mc-
Cain was the most salient speaker for this topic
until June 2001. Soon after the change in major-
ity status a switch happened and Levin, the new
chair of Senate Armed Services, replaced McCain
as the most salient speaker. On the other hand,
Figure 2(b) shows the baseline scores for the same
topic and same speakers. We notice here that the
baseline failed to capture the switch of salience
near June 2001.
We can also observe similar behavior in Fig-
ure 3(a). This figure shows how Senate Majority
Leader Trent Lott (R - Mississippi) was the most
salient speaker on topic 35 (Procedural Legisla-
tion) until July 2001. Topic 35 does not map to
a specific committee but rather is related to ma-
neuvering bills through the legislative process on
the floor, a job generally delegated to members in
the Senate leadership. Just after his party gained
317
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2 (Infra
structure
)??, Expo
nential, a
=0.02, th
=
MCCA
IN
LEVIN
CARL
(a) Dynamic Lexrank
 
0
 
2
 
4
 
6
 
8
 
10 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
MCCA
IN
LEVIN
CARL
(b) Baseline
Figure 2: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2).
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Proce
dural 4
 (Legisla
ton 2)??, E
xponenti
al, a=0.0
2, th= REID LOTT
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Proced
ural 4 (L
egislaton
 2)??
REID LOTT
(b) Baseline
Figure 3: The Switch of Speakers Salience near Jun 2001 for Topic 35(Procedural Legislation).
majority status, Senator Harry Reid (D - Nevada)
became the most salient speaker for this topic. This
is consistent with Reid?s switch from Assistant mi-
nority Leader to Assistant majority Leader. Again
the baseline scores for the same topic and speakers
in Figure 3(b) fails to capture the switch.
An even more interesting test would be to check
whether the Democrats in general become more
central than Republicans after the Jeffords switch.
Figure 4(a) shows the normalized sum of the
scores of all the Democrats and all the Republicans
on topic 5 (Armed Forces 2) for the 107th Senate.
The figure shows how the Republicans were most
salient until soon after the Jeffords switch when the
Democrats regained the majority and became more
salient. We even discovered similar behavior when
we studied how the average salience of Democrats
and Republicans change across all topics. This is
shown in Figure 5(a) where we can see that the
Republicans were more salient on average for all
topics until June 2001. Soon after the change in
majority status, Democrats became more central.
Figures 4(b) and 5(b) show the same results using
the baseline system. We notice that the number of
speeches made by the Democrats and the Repub-
licans is very similar in most of the times. Even
when one of the parties has more speeches than
the other, it does not quite reflect the salience of
the speakers or the parties in general.
An alternative approach to evaluate the dynamic
scores is to exploit the cyclical nature of the leg-
islative process as some bills are re-authorized on
a fairly regular time schedule. For example, the
farm bill comes due about every five years. As a
new topic is coming up for debate, we expect the
saliency scores for relevant legislators to increase.
Figure 6 shows the dynamic scores of Senator
Thomas Harkin (D - Iowa), and Senator Richard
318
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2??
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20
 
25
 
30 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
Democ
rates
Repub
licans
(b) Baseline
Figure 4: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2), Republicans vs
Democrats.
 
10
 
12
 
14
 
16
 
18
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
100
 
200
 
300
 
400
 
500
 
600
 
700 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107
Democ
rates
Repub
licans
(b) Baseline
Figure 5: The Switch of Speakers Salience near Jun 2001 for All Topics, Republicans vs Democrats.
Lugar (R - Indiana) during the 107th senate on
topic 24 (Agriculture). The two senators were
identified, by the proposed method, as the most
salient speakers for this topic, as expected, since
they both served as chairmen of the Senate Com-
mittee on Agriculture, Nutrition, and Forestry
when their party was in the majority during the
107th Senate. This committee was in charge of
shepherding the Farm Bill through the Senate. The
scores of both senators on the agriculture topic sig-
nificantly increased starting late 2001 until June
2002. The debate began on the bill starting in
September of 2001 and it was not passed until May
2002.
6 Conclusion
We presented a graph based method for analyz-
ing the temporal evolution of the salience of par-
ticipants in a discussion. We used this method to
track the evolution of salience of speakers in the
US Congressional Record. We showed that the
way salience scores evolve over time can answer
several interesting issues. We tracked how the in-
fluence of the speakers vary with majority status
and change of party control. We also show how
a baseline system that depends on the number of
speeches fails to capture the interesting events cap-
tured by the proposed system. We also studied the
dynamics of the relative distribution of attention to
each topic area in different time periods and cap-
tured the cyclical nature of the legislative process
as some bills are re-authorized on a fairly regular
time schedule.
319
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Agric
ulture??
, Expon
ential, 
a=0.02
, th= LUG
AR HARKI
N
Figure 6: The Farm Bill Discussions on the Rela-
tive Distribution of Attention to Topic 24 (Agricul-
ture).
Acknowledgments
This paper is based upon work supported by
the National Science Foundation under Grant No.
0527513, ?DHB: The dynamics of Political Rep-
resentation and Political Rhetoric?. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
References
Blei, David and John Lafferty. 2006. Dynamic topic
models. In ICML 2006.
Blei, David, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
CNIS, 30(1?7):107?117.
Erkan, G?unes? and Dragomir Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Erkan, Gunes. 2006. Language model-based document
clustering using random walks. In HLT/NAACL
2006, pages 479?486. Association for Computa-
tional Linguistics.
Fader, Anthony, Dragomir Radev, Michael Crespin,
Burt Monroe, Kevin Quinn, and Michael Colaresi.
2007. Mavenrank: Identifying influential members
of the us senate using lexical centrality. In EMNLP
2007.
Hartog, Chris Den and Nathan Monroe. 2004. The
value of majority status: The effect of jeffords?s
switch on asset prices of republican and democratic
firms. Legislative Studies Quarterly, 33:63?84.
Kleinberg, Jon. 1998. Authoritative sources in a hyper-
linked environment. In the ACM-SIAM Symposium
on Discrete Algorithms, pages 668?677.
Kurland, Oren and Lillian Lee. 2005. PageRank with-
out hyperlinks: Structural re-ranking using links in-
duced by language models. In SIGIR 2005, pages
306?313.
Kurland, Oren and Lillian Lee. 2006. Respect my au-
thority! HITS without hyperlinks, utilizing cluster-
based language models. In SIGIR 2006, pages 83?
90.
McLachlan, Geoffrey and David Peel. 2000. Finite
Mixture Models. New York: Wiley.
Mihalcea, Rada and Paul Tarau. 2004. TextRank:
Bringing order into texts. In EMNLP 2004.
Mihalcea, Rada, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application
to word sense disambiguation. In COLING 2004,
pages 1126?1132.
Monroe, Burt, Cheryl Monroe, Kevin Quinn, Dragomir
Radev, Michael Crespin, Michael Colaresi, Anthony
Fader, Jacob Balazer, and Steven Abney. 2006.
United states congressional speech corpus. Depart-
ment of Political Science, The Pennsylvania State
University.
Newman, Mark. 2003. A measure of betweenness
centrality based on random walks. Technical Report
cond-mat/0309045, Arxiv.org.
Porter, Mason, Peter Mucha, Miark Newman, and
Casey Warmbrand. 2005. A network analysis of
committees in the U.S. House of Representatives.
PNAS, 102(20).
Quinn, Kevin, Burt Monroe, Michael Colaresi, Michael
Crespin, and Dragomir Radev. 2006. An automated
method of topic-coding legislative speech over time
with application to the 105th-108th U.S. senate. In
Midwest Political Science Association Meeting.
Sparck-Jones, Karen. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 28(1):11?20.
Thomas, Matt, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In EMNLP
2006, pages 327?335.
320
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 584?592,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Citations to Generate Surveys of Scientific Paradigms
Saif Mohammad??, Bonnie Dorr???, Melissa Egan??, Ahmed Hassan?,
Pradeep Muthukrishan?, Vahed Qazvinian?, Dragomir Radev??, David Zajic?
Institute for Advanced Computer Studies? and Computer Science?, University of Maryland.
Human Language Technology Center of Excellence?. Center for Advanced Study of Language.
{saif,bonnie,mkegan,dmzajic}@umiacs.umd.edu
Department of Electrical Engineering and Computer Science?
School of Information?, University of Michigan.
{hassanam,mpradeep,vahed,radev}@umich.edu
Abstract
The number of research publications in var-
ious disciplines is growing exponentially.
Researchers and scientists are increasingly
finding themselves in the position of having
to quickly understand large amounts of tech-
nical material. In this paper we present the
first steps in producing an automatically gen-
erated, readily consumable, technical survey.
Specifically we explore the combination of
citation information and summarization tech-
niques. Even though prior work (Teufel et
al., 2006) argues that citation text is unsuitable
for summarization, we show that in the frame-
work of multi-document survey creation, cita-
tion texts can play a crucial role.
1 Introduction
In today?s rapidly expanding disciplines, scientists
and scholars are constantly faced with the daunting
task of keeping up with knowledge in their field. In
addition, the increasingly interconnected nature of
real-world tasks often requires experts in one dis-
cipline to rapidly learn about other areas in a short
amount of time.
Cross-disciplinary research requires scientists in
areas such as linguistics, biology, and sociology
to learn about computational approaches and appli-
cations, e.g., computational linguistics, biological
modeling, social networks. Authors of journal ar-
ticles and books must write accurate surveys of pre-
vious work, ranging from short summaries of related
research to in-depth historical notes.
Interdisciplinary review panels are often called
upon to review proposals in a wide range of areas,
some of which may be unfamiliar to panelists. Thus,
they must learn about a new discipline ?on the fly?
in order to relate their own expertise to the proposal.
Our goal is to effectively serve these needs by
combining two currently available technologies: (1)
bibliometric lexical link mining that exploits the
structure of citations and relations among citations;
and (2) summarization techniques that exploit the
content of the material in both the citing and cited
papers.
It is generally agreed upon that manually written
abstracts are good summaries of individual papers.
More recently, Qazvinian and Radev (2008) argue
that citation texts are useful in creating a summary
of the important contributions of a research paper.
The citation text of a target paper is the set of sen-
tences in other technical papers that explicitly refer
to it (Elkiss et al, 2008a). However, Teufel (2005)
argues that using citation text directly is not suitable
for document summarization.
In this paper, we compare and contrast the use-
fulness of abstracts and of citation text in automati-
cally generating a technical survey on a given topic
from multiple research papers. The next section pro-
vides the background for this work, including the
primary features of a technical survey and also the
types of input that are used in our study (full pa-
pers, abstracts, and citation texts). Following this,
we describe related work and point out the advances
of our work over previous work. We then describe
how citation texts are used as a new input for multi-
document summarization to produce surveys of a
given technical area. We apply four different sum-
marization techniques to data in the ACL Anthol-
584
ogy and evaluate our results using both automatic
(ROUGE) and human-mediated (nugget-based pyra-
mid) measures. We observe that, as expected, ab-
stracts are useful in survey creation, but, notably, we
also conclude that citation texts have crucial survey-
worthy information not present in (or at least, not
easily extractable from) abstracts. We further dis-
cover that abstracts are author-biased and thus com-
plementary to the broader perspective inherent in ci-
tation texts; these differences enable the use of a
range of different levels and types of information in
the survey?the extent of which is subject to survey
length restrictions (if any).
2 Background
Automatically creating technical surveys is sig-
nificantly distinct from that of traditional multi-
document summarization. Below we describe pri-
mary characteristics of a technical survey and we
present three types of input texts that we used for
the production of surveys.
2.1 Technical Survey
In the case of multi-document summarization, the
goal is to produce a readable presentation of mul-
tiple documents, whereas in the case of technical
survey creation, the goal is to convey the key fea-
tures of a particular field, basic underpinnings of the
field, early and late developments, important con-
tributions and findings, contradicting positions that
may reverse trends or start new sub-fields, and ba-
sic definitions and examples that enable rapid un-
derstanding of a field by non-experts.
A prototypical example of a technical survey is
that of ?chapter notes,? i.e., short (50?500 word)
descriptions of sub-areas found at the end of chap-
ters of textbook, such as Jurafsky and Martin (2008).
One might imagine producing such descriptions au-
tomatically, then hand-editing them and refining
them for use in an actual textbook.
We conducted a human analysis of these chapter
notes that revealed a set of conventions, an outline
of which is provided here (with example sentences
in italics):
1. Introductory/opening statement: The earliest
computational use of X was in Y, considered by
many to be the foundational work in this area.
2. Definitional follow up: X is def ined as Y.
3. Elaboration of definition (e.g., with an exam-
ple): Most early algorithms were based on Z.
4. Deeper elaboration, e.g., pointing out issues
with initial approaches: Unfortunately, this
model seems to be wrong.
5. Contrasting definition: Algorithms since then...
6. Introduction of additional specific instances /
historical background with citations: Two clas-
sic approaches are described in Q.
7. References to other summaries: R provides a
comprehensive guide to the details behind X.
The notion of text level categories or zoning
of technical papers?related to the survey compo-
nents enumerated above?has been investigated pre-
viously in the work of Nanba and Kan (2004b) and
Teufel (2002). These earlier works focused on the
analysis of scientific papers based on their rhetori-
cal structure and on determining the portions of pa-
pers that contain new results, comparisons to ear-
lier work, etc. The work described in this paper fo-
cuses on the synthesis of technical surveys based on
knowledge gleaned from rhetorical structure not un-
like that of the work of these earlier researchers, but
perhaps guided by structural patterns along the lines
of the conventions listed above.
Although our current approach to survey creation
does not yet incorporate a fully pattern-based com-
ponent, our ultimate objective is to apply these pat-
terns to guide the creation and refinement of the final
output. As a first step toward this goal, we use cita-
tion texts (closest in structure to the patterns iden-
tified by convention 7 above) to pick out the most
important content for survey creation.
2.2 Full papers, abstracts, and citation texts
Published research on a particular topic can be sum-
marized from two different kinds of sources: (1)
where an author describes her own work and (2)
where others describe an author?s work (usually in
relation to their own work). The author?s descrip-
tion of her own work can be found in her paper. How
others perceive her work is spread across other pa-
pers that cite her work. We will refer to the set of
sentences that explicitly mention a target paper Y as
the citation text of Y.
585
Traditionally, technical survey generation has
been tackled by summarizing a set of research pa-
pers pertaining to the topic. However, individual re-
search papers usually come with manually-created
?summaries??their abstracts. The abstract of a pa-
per may have sentences that set the context, state the
problem statement, mention how the problem is ap-
proached, and the bottom-line results?all in 200 to
500 words. Thus, using only the abstracts (instead
of full papers) as input to a summarization system is
worth exploring.
Whereas the abstract of a paper presents what the
authors think to be the important contributions of a
paper, the citation text of a paper captures what oth-
ers in the field perceive as the contributions of the
paper. The two perspectives are expected to have
some overlap in their content, but the citation text
also contains additional information not found in ab-
stracts (Elkiss et al, 2008a). For example, how a
particular methodology (described in one paper) was
combined with another (described in a different pa-
per) to overcome some of the drawbacks of each.
A citation text is also an indicator of what contri-
butions described in a paper were more influential
over time. Another distinguishing feature of citation
texts in contrast to abstracts is that a citation text
tends to have a certain amount of redundant informa-
tion. This is because multiple papers may describe
the same contributions of a target paper. This redun-
dancy can be exploited to determine the important
contributions of the target paper.
Our goal is to test the hypothesis that an ef-
fective technical survey will reflect information on
research not only from the perspective of its au-
thors but also from the perspective of others who
use/commend/discredit/add to it. Before describ-
ing our experiments with technical papers, abstracts,
and citation texts, we first summarize relevant prior
work that used these sources of information as input.
3 Related work
Previous work has focused on the analysis of cita-
tion and collaboration networks (Teufel et al, 2006;
Newman, 2001) and scientific article summarization
(Teufel and Moens, 2002). Bradshaw (2003) used
citation texts to determine the content of articles and
improve the results of a search engine. Citation
texts have also been used to create summaries of sin-
gle scientific articles in Qazvinian and Radev (2008)
and Mei and Zhai (2008). However, there is no pre-
vious work that uses the text of the citations to pro-
duce a multi-document survey of scientific articles.
Furthermore, there is no study contrasting the qual-
ity of surveys generated from citation summaries?
both automatically and manually produced?to sur-
veys generated from other forms of input such as the
abstracts or full texts of the source articles.
Nanba and Okumura (1999) discuss citation cate-
gorization to support a system for writing a survey.
Nanba et al (2004a) automatically categorize cita-
tion sentences into three groups using pre-defined
phrase-based rules. Based on this categorization a
survey generation tool is introduced in Nanba et al
(2004b). They report that co-citation (where both
papers are cited by many other papers) implies sim-
ilarity by showing that the textual similarity of co-
cited papers is proportional to the proximity of their
citations in the citing article.
Elkiss et al (2008b) conducted several exper-
iments on a set of 2,497 articles from the free
PubMed Central (PMC) repository.1 Results from
this experiment confirmed that the cohesion of a ci-
tation text of an article is consistently higher than
the that of its abstract. They also concluded that ci-
tation texts contain additional information are more
focused than abstracts.
Nakov et al (2004) use sentences surrounding ci-
tations to create training and testing data for seman-
tic analysis, synonym set creation, database cura-
tion, document summarization, and information re-
trieval. Kan et al (2002) use annotated bibliogra-
phies to cover certain aspects of summarization and
suggest using metadata and critical document fea-
tures as well as the prominent content-based features
to summarize documents. Kupiec et al (1995) use a
statistical method and show how extracts can be used
to create summaries but use no annotated metadata
in summarization.
Siddharthan and Teufel (2007) describe a new
reference task and show high human agreement as
well as an improvement in the performance of ar-
gumentative zoning (Teufel, 2005). In argumenta-
tive zoning?a rhetorical classification task?seven
1http://www.pubmedcentral.gov
586
classes (Own, Other, Background, Textual, Aim,
Basis, and Contrast) are used to label sentences ac-
cording to their role in the author?s argument.
Our aim is not only to determine the utility of cita-
tion texts for survey creation, but also to examine the
quality distinctions between this form of input and
others such as abstracts and full texts?comparing
the results to human-generated surveys using both
automatic and nugget-based pyramid evaluation
(Lin and Demner-Fushman, 2006; Nenkova and Pas-
sonneau, 2004; Lin, 2004).
4 Summarization systems
We used four summarization systems for our
survey-creation approach: Trimmer, LexRank, C-
LexRank, and C-RR. Trimmer is a syntactically-
motivated parse-and-trim approach. LexRank is a
graph-based similarity approach. C-LexRank and C-
RR use graph clustering (?C? stands for clustering).
We describe each of these, in turn, below.
4.1 Trimmer
Trimmer is a sentence-compression tool that extends
the scope of an extractive summarization system by
generating multiple alternative sentence compres-
sions of the most important sentences in target doc-
uments (Zajic et al, 2007). Trimmer compressions
are generated by applying linguistically-motivated
rules to mask syntactic components of a parse of a
source sentence. The rules can be applied iteratively
to compress sentences below a configurable length
threshold, or can be applied in all combinations to
generate the full space of compressions.
Trimmer can leverage the output of any con-
stituency parser that uses the Penn Treebank con-
ventions. At present, the Stanford Parser (Klein and
Manning, 2003) is used. The set of compressions
is ranked according to a set of features that may in-
clude metadata about the source sentences, details of
the compression process that generated the compres-
sion, and externally calculated features of the com-
pression.
Summaries are constructed from the highest scor-
ing compressions, using the metadata and maximal
marginal relevance (Carbonell and Goldstein, 1998)
to avoid redundancy and over-representation of a
single source.
4.2 LexRank
We also used LexRank (Erkan and Radev, 2004), a
state-of-the-art multidocument summarization sys-
tem, to generate summaries. LexRank first builds a
graph of all the candidate sentences. Two candidate
sentences are connected with an edge if the similar-
ity between them is above a threshold. We used co-
sine as the similarity metric with a threshold of 0.15.
Once the network is built, the system finds the most
central sentences by performing a random walk on
the graph.
The salience of a node is recursively defined on
the salience of adjacent nodes. This is similar to
the concept of prestige in social networks, where the
prestige of a person is dependent on the prestige of
the people he/she knows. However, since random
walk may get caught in cycles or in disconnected
components, we reserve a low probability to jump
to random nodes instead of neighbors (a technique
suggested by Langville and Meyer (2006)).
Note also that unlike the original PageRank
method, the graph of sentences is undirected. This
updated measure of sentence salience is called as
LexRank. The sentences with the highest LexRank
scores form the summary.
4.3 Cluster Summarizers: C-LexRank, C-RR
Two clustering methods proposed by Qazvinian and
Radev (2008)?C-RR and C-LexRank?were used
to create summaries. Both create a fully connected
network in which nodes are sentences and edges are
cosine similarities. A cutoff value of 0.1 is applied
to prune the graph and make a binary network. The
largest connected component of the network is then
extracted and clustered.
Both of the mentioned summarizers cluster the
network similarly but use different approaches to se-
lect sentences from different communities. In C-
RR sentences are picked from different clusters in
a round robin (RR) fashion. C-LexRank first calcu-
lates LexRank within each cluster to find the most
salient sentences of each community. Then it picks
the most salient sentence of each cluster, and then
the second most salient and so forth until the sum-
mary length limit is reached.
587
Most of work in QA and paraphrasing focused on folding paraphrasing knowledge into question analyzer or answer
locator Rinaldi et al 2003; Tomuro, 2003. In addition, number of researchers have built systems to take reading
comprehension examinations designed to evaluate children?s reading levels Charniak et al 2000; Hirschman et al
1999; Ng et al 2000; Riloff and Thelen, 2000; Wang et al 2000. so-called ? definition ? or ? other ?
questions at recent TREC evalua - tions Voorhees, 2005 serve as good examples. To better facilitate user
information needs, recent trends in QA research have shifted towards complex, context-based, and interactive
question answering Voorhees, 2001; Small et al 2003; Harabagiu et al 2005. [And so on.]
Table 1: First few sentences of the QA citation texts survey generated by Trimmer.
5 Data
The ACL Anthology is a collection of papers from
the Computational Linguistics journal, and proceed-
ings of ACL conferences and workshops. It has
almost 11,000 papers. To produce the ACL An-
thology Network (AAN), Joseph and Radev (2007)
manually parsed the references before automatically
compiling the network metadata, and generating ci-
tation and author collaboration networks. The AAN
includes all citation and collaboration data within
the ACL papers, with the citation network consist-
ing of 11,773 nodes and 38,765 directed edges.
Our evaluation experiments are on a set of papers
in the research area of Question Answering (QA)
and another set of papers on Dependency parsing
(DP). The two sets of papers were compiled by se-
lecting all the papers in AAN that had the words
Question Answering and Dependency Parsing, re-
spectively, in the title and the content. There were
10 papers in the QA set and 16 papers in the DP set.
We also compiled the citation texts for the 10 QA
papers and the citation texts for the 16 DP papers.
6 Experiments
We automatically generated surveys for both QA
and DP from three different types of documents: (1)
full papers from the QA and DP sets?QA and DP
full papers (PA), (2) only the abstracts of the QA
and DP papers?QA and DP abstracts (AB), and
(3) the citation texts corresponding to the QA and
DP papers?QA and DP citations texts (CT).
We generated twenty four (4x3x2) surveys,
each of length 250 words, by applying Trimmer,
LexRank, C-LexRank and C-RR on the three data
types (citation texts, abstracts, and full papers) for
both QA and DP. (Table 1 shows a fragment of one
of the surveys automatically generated from QA ci-
tation texts.) We created six (3x2) additional 250-
word surveys by randomly choosing sentences from
the citation texts, abstracts, and full papers of QA
and DP. We will refer to them as random surveys.
6.1 Evaluation
Our goal was to determine if citation texts do in-
deed have useful information that one will want to
put in a survey and if so, how much of this infor-
mation is not available in the original papers and
their abstracts. For this we evaluated each of the
automatically generated surveys using two separate
approaches: nugget-based pyramid evaluation and
ROUGE (described in the two subsections below).
Two sets of gold standard data were manually cre-
ated from the QA and DP citation texts and abstracts,
respectively:2 (1) We asked two impartial judges to
identify important nuggets of information worth in-
cluding in a survey. (2) We asked four fluent speak-
ers of English to create 250-word surveys of the
datasets. Then we determined how well the differ-
ent automatically generated surveys perform against
these gold standards. If the citation texts have only
redundant information with respect to the abstracts
and original papers, then the surveys of citation texts
will not perform better than others.
6.1.1 Nugget-Based Pyramid Evaluation
For our first approach we used a nugget-based
evaluation methodology (Lin and Demner-Fushman,
2006; Nenkova and Passonneau, 2004; Hildebrandt
et al, 2004; Voorhees, 2003). We asked three impar-
tial annotators (knowledgeable in NLP but not affil-
iated with the project) to review the citation texts
and/or abstract sets for each of the papers in the QA
and DP sets and manually extract prioritized lists
2Creating gold standard data from complete papers is fairly
arduous, and was not pursued.
588
of 2?8 ?nuggets,? or main contributions, supplied
by each paper. Each nugget was assigned a weight
based on the frequency with which it was listed by
annotators as well as the priority it was assigned
in each case. Our automatically generated surveys
were then scored based on the number and weight
of the nuggets that they covered. This evaluation ap-
proach is similar to the one adopted by Qazvinian
and Radev (2008), but adapted here for use in the
multi-document case.
The annotators had two distinct tasks for the QA
set, and one for the DP set: (1) extract nuggets for
each of the 10 QA papers, based only on the citation
texts for those papers; (2) extract nuggets for each
of the 10 QA papers, based only on the abstracts of
those papers; and (3) extract nuggets for each of the
16 DP papers, based only on the citation texts for
those papers.3
We obtained a weight for each nugget by revers-
ing its priority out of 8 (e.g., a nugget listed with
priority 1 was assigned a weight of 8) and summing
the weights over each listing of that nugget.4
To evaluate a given survey, we counted the num-
ber and weight of nuggets that it covered. Nuggets
were detected via the combined use of annotator-
provided regular expressions and careful human re-
view. Recall was calculated by dividing the com-
bined weight of covered nuggets by the combined
weight of all nuggets in the nugget set. Precision
was calculated by dividing the number of distinct
nuggets covered in a survey by the number of sen-
tences constituting that survey, with a cap of 1. F-
measure, the weighted harmonic mean of precision
and recall, was calculated with a beta value of 3 in
order to assign the greatest weight to recall. Recall
is favored because it rewards surveys that include
highly weighted (important) facts, rather than just a
3We first experimented using only the QA set. Then to show
that the results apply to other datasets, we asked human anno-
tators for gold standard data on the DP citation texts. Addi-
tional experiments on DP abstracts were not pursued because
this would have required additional human annotation effort to
establish a point we had already made with the QA set, i.e., that
abstracts are useful for survey creation.
4Results obtained with other weighting schemes that ig-
nored priority ratings and multiple mentions of a nugget by a
single annotator showed the same trends as the ones shown by
the selected weighting scheme, but the latter was a stronger dis-
tinguisher among the four systems.
Human Performance: Pyramid F-measureHuman1 Human2 Human3 Human4 Average
Input: QA citation surveysQA?CT nuggets 0.524 0.711 0.468 0.695 0.599QA?AB nuggets 0.495 0.606 0.423 0.608 0.533Input: QA abstract surveysQA?CT nuggets 0.542 0.675 0.581 0.669 0.617QA?AB nuggets 0.646 0.841 0.673 0.790 0.738Input: DP citation surveysDP?CT nuggets 0.245 0.475 0.378 0.555 0.413
Table 2: Pyramid F-measure scores of human-created
surveys of QA and DP data. The surveys are evaluated
using nuggets drawn from QA citation texts (QA?CT),
QA abstracts (QA?AB), and DP citation texts (DP?CT).
great number of facts.
Table 2 gives the F-measure values of the 250-
word surveys manually generated by humans. The
surveys were evaluated using the nuggets drawn
from the QA citation texts, QA abstracts, and DP ci-
tation texts. The average of their scores (listed in the
rightmost column) may be considered a good score
to aim for by the automatic summarization methods.
Table 3 gives the F-measure values of the surveys
generated by the four automatic summarizers, evalu-
ated using nuggets drawn from the QA citation texts,
QA abstracts, and DP citation texts. The table also
includes results for the baseline random summaries.
When we used the nuggets from the abstracts
set for evaluation, the surveys created from ab-
stracts scored higher than the corresponding surveys
created from citation texts and papers. Further, the
best surveys generated from citation texts outscored
the best surveys generated from papers. When we
used the nuggets from citation sets for evaluation,
the best automatic surveys generated from citation
texts outperform those generated from abstracts and
full papers. All these pyramid results demonstrate
that citation texts can contain useful information that
is not available in the abstracts or the original papers,
and that abstracts can contain useful information that
is not available in the citation texts or full papers.
Among the various automatic summarizers, Trim-
mer performed best at this task, in two cases ex-
ceeding the average human performance. Note also
that the random summarizer outscored the automatic
summarizers in cases where the nuggets were taken
from a source different from that used to generate
the survey. However, one or two summarizers still
tended to do well. This indicates a difficulty in ex-
589
System Performance: Pyramid F-measure
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT nuggets 0.321 0.434 0.268 0.295 0.616
QA?AB nuggets 0.305 0.388 0.349 0.320 0.543
Input: QA abstract surveys
QA?CT nuggets 0.452 0.383 0.480 0.441 0.404
QA?AB nuggets 0.623 0.484 0.574 0.606 0.622
Input: QA full paper surveys
QA?CT nuggets 0.239 0.446 0.299 0.190 0.199
QA?AB nuggets 0.294 0.520 0.387 0.301 0.290
Input: DP citation surveys
DP?CT nuggets 0.219 0.231 0.170 0.372 0.136
Input: DP abstract surveys
DP?CT nuggets 0.321 0.301 0.263 0.311 0.312
Input: DP full paper surveys
DP?CT nuggets 0.032 0.000 0.144 * 0.280
Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data. The surveys are evaluated using nuggets
drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).
* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).
Human Performance: ROUGE-2human1 human2 human3 human4 average
Input: QA citation surveysQA?CT refs. 0.1807 0.1956 0.0756 0.2019 0.1635QA?AB refs. 0.1116 0.1399 0.0711 0.1576 0.1201Input: QA abstract surveysQA?CT refs. 0.1315 0.1104 0.1216 0.1151 0.1197QA-AB refs. 0.2648 0.1977 0.1802 0.2544 0.2243Input: DP citation surveysDP?CT refs. 0.1550 0.1259 0.1200 0.1654 0.1416
Table 4: ROUGE-2 scores obtained for each of the manu-
ally created surveys by using the other three as reference.
ROUGE-1 and ROUGE-L followed similar patterns.
tracting the overlapping survey-worthy information
across the two sources.
6.1.2 ROUGE evaluation
Table 4 presents ROUGE scores (Lin, 2004) of
each of human-generated 250-word surveys against
each other. The average (last column) is what the au-
tomatic surveys can aim for. We then evaluated each
of the random surveys and those generated by the
four summarization systems against the references.
Table 5 lists ROUGE scores of surveys when the
manually created 250-word survey of the QA cita-
tion texts, survey of the QA abstracts, and the survey
of the DP citation texts, were used as gold standard.
When we use manually created citation text
surveys as reference, then the surveys gener-
ated from citation texts obtained significantly bet-
ter ROUGE scores than the surveys generated from
abstracts and full papers (p < 0.05) [RESULT 1].
This shows that crucial survey-worthy information
present in citation texts is not available, or hard to
extract, from abstracts and papers alone. Further,
the surveys generated from abstracts performed sig-
nificantly better than those generated from the full
papers (p < 0.05) [RESULT 2]. This shows that ab-
stracts and citation texts are generally denser in sur-
vey worthy information than full papers.
When we use manually created abstract sur-
veys as reference, then the surveys generated
from abstracts obtained significantly better ROUGE
scores than the surveys generated from citation texts
and full papers (p < 0.05) [RESULT 3]. Further, and
more importantly, the surveys generated from cita-
tion texts performed significantly better than those
generated from the full papers (p < 0.05) [RESULT
4]. Again, this shows that abstracts and citation texts
are richer in survey-worthy information. These re-
sults also show that abstracts of papers and citation
texts have some overlapping information (RESULT
2 and RESULT 4), but they also have a signifi-
cant amount of unique survey-worthy information
(RESULT 1 and RESULT 3).
Among the automatic summarizers, C-LexRank
and LexRank perform best. This is unlike the results
found through the nugget-evaluation method, where
Trimmer performed best. This suggests that Trim-
590
System Performance: ROUGE-2
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT refs. 0.11561 0.17013 0.09522 0.13501 0.16984
QA?AB refs. 0.08264 0.11653 0.07600 0.07013 0.10336
Input: QA abstract surveys
QA?CT refs. 0.04516 0.05892 0.06149 0.05369 0.04114
QA?AB refs. 0.12085 0.13634 0.12190 0.20311 0.13357
Input: QA full paper surveys
QA?CT refs. 0.03042 0.03606 0.03599 0.28244 0.03986
QA?AB refs. 0.04621 0.05901 0.04976 0.10540 0.07505
Input: DP citation surveys
DP?CT refs. 0.10690 0.13164 0.08748 0.04901 0.10052
Input: DP abstract surveys
DP?CT refs. 0.07027 0.07321 0.05318 0.20311 0.07176
Input: DP full paper surveys
DP?CT refs. 0.03770 0.02511 0.03433 * 0.04554
Table 5: ROUGE-2 scores of automatic surveys of QA and DP data. The surveys are evaluated by using human
references created from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT). These
results are obtained after Jack-knifing the human references so that the values can be compared to those in Table 4.
* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).
mer is better at identifying more useful nuggets of
information, but C-LexRank and LexRank are bet-
ter at producing unigrams and bigrams expected in
a survey. To some extent this may be due to the fact
that Trimmer uses smaller (trimmed) fragments of
source sentences in its summaries.
7 Conclusion
In this paper, we investigated the usefulness of di-
rectly summarizing citation texts (sentences that cite
other papers) in the automatic creation of technical
surveys. We generated surveys of a set of Ques-
tion Answering (QA) and Dependency Parsing (DP)
papers, their abstracts, and their citation texts us-
ing four state-of-the-art summarization systems (C-
LexRank, C-RR, LexRank, and Trimmer). We then
used two separate approaches, nugget-based pyra-
mid and ROUGE, to evaluate the surveys. The re-
sults from both approaches and all four summa-
rization systems show that both citation texts and
abstracts have unique survey-worthy information.
These results also demonstrate that, unlike single
document summarization (where citing sentences
have been suggested to be inappropriate (Teufel
et al, 2006)), multidocument summarization?
especially technical survey creation?benefits con-
siderably from citation texts.
We next plan to generate surveys using both cita-
tion texts and abstracts together as input. Given the
overlapping content of abstracts and citation texts,
discovered in the current study, it is clear that re-
dundancy detection will be an integral component of
this future work. Creating readily consumable sur-
veys is a hard task, especially when using only raw
text and simple summarization techniques. There-
fore we intend to combine these summarization and
bibliometric techniques with suitable visualization
methods towards the creation of iterative technical
survey tools?systems that present surveys and bib-
liometric links in a visually convenient manner and
which incorporate user feedback to produce even
better surveys.
Acknowledgments
This work was supported, in part, by the National
Science Foundation under Grant No. IIS-0705832
(iOPENER: Information Organization for PENning
Expositions on Research) and Grant No. 0534323
(Collaborative Research: BlogoCenter - Infrastruc-
ture for Collecting, Mining and Accessing Blogs),
in part, by the Human Language Technology Cen-
ter of Excellence, and in part, by the Center for Ad-
vanced Study of Language (CASL). Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the sponsors.
591
References
Shannon Bradshaw. 2003. Reference directed indexing:
Redeeming relevance for subject search in citation in-
dexes. In Proceedings of the 7th European Conference
on Research and Advanced Technology for Digital Li-
braries.
Jaime G. Carbonell and Jade Goldstein. 1998. The use
of mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
21st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 335?336, Melbourne, Australia.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008a. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008b. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Overview of the trec 2003 question-answering track.
In Proceedings of the 2004 Human Language Tech-
nology Conference and the North American Chapter
of the Association for Computational Linguistics An-
nual Meeting (HLT/NAACL 2004).
Mark Joseph and Dragomir Radev. 2007. Citation analy-
sis, centrality, and the ACL Anthology. Technical Re-
port CSE-TR-535-07, University of Michigan. Dept.
of Electrical Engineering and Computer Science.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics (2nd edition). Prentice-Hall.
Min-Yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 2002. Using the Annotated Bibliography as a
Resource for Indicative Summarization. In Proceed-
ings of LREC 2002, Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of ACL, pages 423?430.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95,
pages 68?73, New York, NY, USA. ACM.
Amy Langville and Carl Meyer. 2006. Google?s PageR-
ank and Beyond: The Science of Search Engine Rank-
ings. Princeton University Press.
Jimmy J. Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to complex
questions. Information Retrieval, 9(5):565?587.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the ACL
workshop on Text Summarization Branches Out.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL ?08, pages 816?824.
Preslav I. Nakov, Schwartz S. Ariel, and Hearst A. Marti.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In Workshop on Search and
Discovery in Bioinformatics.
Hidetsugu Nanba and Manabu Okumura. 1999. Towards
multi-paper summarization using reference informa-
tion. In IJCAI1999, pages 926?931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Okumura,
and Suguru Saito. 2004a. Bilingual presri: Integration
of multiple research paper databases. In Proceedings
of RIAO 2004, pages 195?211, Avignon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Okumura.
2004b. Classification of research papers using cita-
tion links and citation types: Towards automatic re-
view article generation. In Proceedings of the 11th
SIG Classification Research Workshop, pages 117?
134, Chicago, USA.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
Proceedings of EMNLP, pages 103?110, Australia.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and Af-
fect in Text: Theory and Applications, pages 159?170.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proceedings of the
Twelfth Text Retrieval Conference (TREC 2003).
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing and Management (Spe-
cial Issue on Summarization).
592
Language Independent Text Correction using Finite State Automata
Ahmed Hassan? Sara Noeman
IBM Cairo Technology Development Center
Giza, Egypt
hasanah,noemans,hanyh@eg.ibm.com
Hany Hassan
Abstract
Many natural language applications, like
machine translation and information extrac-
tion, are required to operate on text with
spelling errors. Those spelling mistakes
have to be corrected automatically to avoid
deteriorating the performance of such ap-
plications. In this work, we introduce a
novel approach for automatic correction of
spelling mistakes by deploying finite state
automata to propose candidates corrections
within a specified edit distance from the mis-
spelled word. After choosing candidate cor-
rections, a language model is used to assign
scores the candidate corrections and choose
best correction in the given context. The
proposed approach is language independent
and requires only a dictionary and text data
for building a language model. The ap-
proach have been tested on both Arabic and
English text and achieved accuracy of 89%.
1 Introduction
The problem of detecting and correcting misspelled
words in text has received great attention due to
its importance in several applications like text edit-
ing systems, optical character recognition systems,
and morphological analysis and tagging (Roche and
Schabes, 1995). Other applications, like machine
translation and information extraction, operate on
text that might have spelling errors. The automatic
detection, and correction of spelling erros should be
of great help to those applications.
The problem of detecting and correcting mis-
spelled words in text is usually solved by checking
whether a word already exists in the dictionary or
not. If not, we try to extract words from the dictio-
nary that are most similar to the word in question.
?Now with the University of Michigan Ann Arbor, has-
sanam@umich.edu
Those words are reported as candidate corrections
for the misspelled word.
Similarity between the misspelled word and dic-
tionary words is measured by the Levenshtein edit
distance (Levenshtein, 1966; Wagner and M.Fisher,
1974). The Levenshtein edit distance is usu-
ally calculated using a dynamic programming tech-
nique with quadratic time complexity (Wagner and
M.Fisher, 1974). Hence, it is not reasonable to com-
pare the misspelled word to each word in the dictio-
nary while trying to find candidate corrections.
The proposed approach uses techniques from fi-
nite state theory to detect misspelled words and to
generate a set of candidate corrections for each mis-
spelled word. It also uses a language model to select
the best correction from the set of candidate correc-
tions using the context of the misspelled word. Us-
ing techniques from finite state theory, and avoiding
calculating edit distances makes the approach very
fast and efficient. The approach is completely lan-
guage independent, and can be used with any lan-
guage that has a dictionary and text data to building
a language model.
The rest of this paper will proceed as follows.
Section 2 will present an overview of related work.
Section 3 will discuss the different aspects of the
proposed approach. Section 4 presents a perfor-
mance evaluation of the system. Finally a conclu-
sion is presented in section 5.
2 Related Work
Several solutions were suggested to avoid comput-
ing the Levenshtein edit distance while finding can-
didate corrections. Most of those solutions select
a number of dictionary words that are supposed to
contain the correction, and then measure the dis-
tance between the misspelled word and all selected
words. The most popular of those methods are the
similarity keys methods (Kukich, 1992; Zobel and
Dart, 1995; De Beuvron and Trigano, 1995). In
913
those methods, the dictionary words are divided into
classes according to some word features. The input
word is compared to words in classes that have sim-
ilar features only.
In addition to the techniques discussed above,
other techniques from finite state automata have
been recently proposed. (Oflazer, 1996) suggested
a method where all words in a dictionary are treated
as a regular language over an alphabet of letters. All
the words are represented by a finite state machine
automaton. For each garbled input word, an exhaus-
tive traversal of the dictionary automaton is initiated
using a variant of Wagner-Fisher algorithm (Wag-
ner and M.Fisher, 1974) to control the traversal of
the dictionary. In this approach Levenshtein dis-
tance is calculated several times during the traversal.
The method carefully traverses the dictionary such
that the inspection of most of the dictionary states
is avoided. (Schulz and Mihov, 2002) presents a
variant of Oflazers?s approach where the dictionary
is also represented as deterministic finite state au-
tomaton. However, they avoid the computation of
Levenshtein distance during the traversal of the dic-
tionary automaton. In this technique, a finite state
acceptor is constructed for each input word. This
acceptor accepts all words that are within an edit dis-
tance k from the input word. The dictionary automa-
ton and the Levenshtein-automaton are then tra-
versed in parallel to extract candidate corrections for
the misspelled word. The authors present an algo-
rithm that can construct a deterministic Levenshtein-
automaton for an arbitrary word of degrees 1, and
2 which corresponds to 1 or 2 errors only. They
suggest another algorithm that can construct a non-
deterministic Levenshtein-automaton for any other
degree. They report results using a Levenshtein-
automaton of degree 1(i.e. words having a single
insertion, substitution, or deletion) only.
The method we propose in this work also assumes
that the dictionary is represented as a determinis-
tic finite state automaton. However, we completely
avoid computing the Levenshtein-distance at any
step. We also avoid reconstructing a Levenshtein-
automaton for each input word. The proposed
method does not impose any constraints on the
bound k, where k is the edit distance between the
input word and the candidate corrections. The ap-
proach can adopt several constraints on which char-
Figure 1: An FSM representation of a word list
acters can substitute certain other characters. Those
constraints are obtained from a phonetic and spatial
confusion matrix of characters.
The purpose of context-dependent error correc-
tion is to rank a set of candidate corrections tak-
ing the misspelled word context into account. A
number of approaches have been proposed to tackle
this problem that use insights from statistical ma-
chine learning (Golding and Roth, 1999), lexical
semantics (Hirst and Budanitsky, 2005), and web
crawls (Ringlstetter et al, 2007).
3 Error Detection and Correction in Text
Using FSMs
The approach consists of three main phases: detect-
ing misspelled words, generating candidate correc-
tions for them, and ranking corrections. A detailed
description of each phase is given in the following
subsections.
3.1 Detecting Misspelled Words
The most direct way for detecting misspelled words
is to search the dictionary for each word, and report
words not found in the dictionary. However, we can
make use of the finite state automaton representation
of the dictionary to make this step more efficient.
In the proposed method, we build a finite state ma-
chine (FSM) that contains a path for each word in
the input string. This FSM is then composed with
the dictionary FSM. The result of the composition
is merely the intersection of the words that exist in
both the input string and the dictionary. If we calcu-
lated the difference between the FSM containing all
words and this FSM, we get an FSM with a path for
914
each misspelled word. Figure 1 illustrate an FSM
that contain all words in an input string.
3.2 Generating Candidate Corrections
The task of generating candidate corrections for mis-
spelled words can be divided into two sub tasks:
Generating a list of words that have edit distance
less than or equal k to the input word, and select-
ing a subset of those words that also exist in the dic-
tionary. To accomplish those tasks, we create a sin-
gle transducer(Levenshtein-transducer) that is when
composed with an FSM representing a word, gen-
erates all words withing any edit distance k from
the input word. After composing the misspelled
word with Levenshtein-transducer, we compose the
resulting FSM with the dictionary FSM to filter out
words that do not exist in the dictionary.
3.2.1 Levenshtein-transducers for primitive
edit distances
To generate a finite state automaton that con-
tain all words within some edit distance to the in-
put word, we use a finite state transducer that al-
lows editing its input according to the standard
Levenshtein-distance primitive operations: substitu-
tion, deletion, and insertion.
A finite-state transducers (FST) is a a 6-tuple
(Q,?1,?2, ?, i, F ), where Q is a set of states, ?1
is the input alphabet, ?2 is the output alphabet, i is
the initial state, F ? Q is a set of final states, and ?
is a transition function (Hopcroft and Ullman, 1979;
Roche and Shabes, 1997). A finite state acceptor is a
special case of an FST that has the same input/output
at each arc.
Figure 2 illustrates the Levenshtein-transducer for
edit distance 1 over a limited set of vocabulary (a,b,
and c). We can notice that we will stay in state zero
as long as the output is identical to the input. On
the other hand we can move from state zero, which
corresponds to edit distance zero, to state one, which
corresponds to edit distance one, with three different
ways:
? input is mapped to a different output (input is
consumed and a different symbol is emitted)
which corresponds to a substitution,
? input is mapped to an epsilon (input is con-
sumed and no output emitted) which corre-
sponds to a deletion, and
Figure 2: A Levenshtein-transducer (edit distance 1)
? an epsilon is mapped to an output (output is
emitted without consuming any input) which
corresponds to an insertion.
Once we reach state 1, the only possible transitions
are those that consume a symbol and emit the same
symbol again and hence allowing only one edit op-
eration to take place.
When we receive a new misspelled word, we rep-
resent it with a finite state acceptor that has a single
path representing the word, and then compose it with
the Levenshtein-transducer. The result of the com-
position is a new FSM that contains all words with
edit distance 1 to the input word.
3.2.2 Adding transposition
Another non-primitive edit distance operation that
is frequently seen in misspelled words is transposi-
tion. Transposition is the operation of exchanging
the order of two consecutive symbols (ab ? ba).
Transposition is not a primitive operation because
it can be represented by other primitive operations.
However, this makes it a second degree operation.
As transposition occurs frequently in misspelled
words, adding it to the Levenshtein-transducer as a
single editing operation would be of great help.
915
Figure 3: A Levenshtein-transducer for edit distance
1 with transposition
To add transposition, as a single editing opera-
tion, to the Levenshtein-transducer we add arcs be-
tween states zero and one that can map any symbol
sequence xy to the symbol sequence yx, where x,
and y are any two symbols in the vocabulary. Fig-
ure 3 shows the Levenshtein-transducer with degree
1 with transposition over a limited vocabulary (a and
b).
3.2.3 Adding symbol confusion matrices
Adding a symbol confusion matrix can help re-
duce the number of candidate corrections. The con-
fusion matrix determines for each symbol a set of
symbols that may have substituted it in the garbled
word. This matrix can be used to reduce the num-
ber of candidate corrections if incorporated into the
Levenshtein-transducer. For any symbol x, we add
an arc x : y between states zero, and one in the trans-
ducer where y ? Confusion Matrix(x) rather
than for all symbols y in the vocabulary.
The confusion matrix can help adopt the meth-
ods to different applications. For example, we can
build a confusion matrix for use with optical char-
acter recognition error correction that captures er-
rors that usually occur with OCRs. When used with
a text editing system, we can use a confusion ma-
trix that predicts the confused characters according
to their phonetic similarity, and their spatial location
on the keyboard.
Figure 4: A Levenshtein-transducer for edit distance
2 with transposition
3.2.4 Using degrees greater than one
To create a Levenshtein-transducer that can gen-
erate all words within edit distance two of the input
word, we create a new state (2) that maps to two edit
operations, and repeat all arcs that moves from state
0 to state 1 to move from state 1 to state 2.
To allow the Levenshtein-transducer of degree
two to produce words with edit distance 1 and 2 from
the input word, we mark both state 1, and 2 as final
states. We may also favor corrections with lower
edit distances by assigning costs to final states, such
that final states with lower number of edit operations
get lower costs. A Levenshtein-transducer of degree
2 for the limited vocabulary (a and b) is shown in
figure 4.
3.3 Ranking Corrections
To select the best correction from a set of candidate
corrections, we use a language model to assign a
probability to a sequence of words containing the
corrected word. To get that word sequence, we go
back to the context where the misspelled word ap-
peared, replace the misspelled word with the candi-
date correction, and extract n ngrams containing the
candidate correction word in all possible positions
in the ngram. We then assign a score to each ngram
using the language model, and assign a score to the
candidate correction that equals the average score of
all ngrams. Before selecting the best scoring cor-
rection, we penalize corrections that resulted from
higher edit operations to favor corrections with the
minimal number of editing operations.
916
Edit 1/with trans. Edit 1/no trans. Edit 2/with trans. Edit 2 / no trans.
word len. av. time av. correcs. av. time av. correcs. av. time av. correcs. av. time av. correcs.
3 3.373273 18.769 2.983733 18.197 73.143538 532.637 69.709387 514.174
4 3.280419 4.797 2.796275 4.715 67.864291 136.230 66.279842 131.680
5 3.321769 1.858 2.637421 1.838 73.718353 33.434 68.695935 32.461
6 3.590046 1.283 2.877242 1.277 75.465624 11.489 69.246055 11.258
7 3.817453 1.139 2.785156 1.139 78.231015 6.373 72.2057 6.277
8 4.073228 1.063 5.593761 1.062 77.096026 4.127 73.361455 4.066
9 4.321661 1.036 3.124661 1.036 76.991945 3.122 73.058418 3.091
10 4.739503 1.020 3.2084 1.020 75.427416 2.706 72.2143 2.685
11 4.892105 1.007 3.405101 1.007 77.045616 2.287 71.293116 2.281
12 5.052191 0.993 3.505089 0.993 78.616536 1.910 75.709801 1.904
13 5.403557 0.936 3.568391 0.936 81.145124 1.575 78.732955 1.568
Table 1: Results for English
Edit 1/with trans. Edit 1/no trans. Edit 2/with trans. Edit 2 / no trans.
word len. av. time av. correcs. av. time av. correcs. av. time av. correcs. av. time av. correcs.
3 5.710543 31.702 4.308018 30.697 83.971263 891.579 75.539547 862.495
4 6.033066 12.555 4.036479 12.196 80.481281 308.910 71.042372 296.776
5 7.060306 6.265 4.360373 6.162 79.320644 104.661 69.71572 100.428
6 9.08935 4.427 4.843784 4.359 79.878962 51.392 74.197127 48.991
7 8.469497 3.348 5.419919 3.329 82.231107 24.663 70.681298 23.781
8 10.078842 2.503 5.593761 2.492 85.32005 13.586 71.557569 13.267
9 10.127946 2.140 6.027077 2.136 83.788916 8.733 76.199034 8.645
10 11.04873 1.653 6.259901 1.653 92.671732 6.142 81.007893 6.089
11 12.060286 1.130 7.327353 1.129 94.726469 4.103 77.464609 4.084
12 13.093397 0.968 7.194902 0.967 95.35985 2.481 82.40306 2.462
13 13.925067 0.924 7.740105 0.921 106.66238 1.123 78.966914 1.109
Table 2: Results for Arabic
4 Experimental Setup
4.1 Time Performance
The proposed method was implemented in C++ on
a 2GHz processor machine under Linux. We used
11,000 words of length 3,4,..., and 13, 1,000 word
for each word length, that have a single error and
computed correction candidates. We report both the
average correction time, and the average number of
corrections for each word length. The experiment
was run twice on different test data, one with con-
sidering transposition as primitive operation, and the
other without. We also repeated the experiments for
edit distance 2 errors, and also considered the two
cases where transposition is considered as a primi-
tive operation or not. Table 1 shows the results for
an English dictionary of size 225,400 entry, and Ta-
ble 2 shows the results for an Arabic dictionary that
has 526,492. entries.
4.2 Auto-correction accuracy
To measure the accuracy of the auto-correction pro-
cess, we used a list of 556 words having common
spelling errors of both edit distances 1 and 2. We put
a threshold on the number of characters per word to
decide whether it will be considered for edit distance
1 or 2 errors. When using a threshold of 7, the spell
engine managed to correct 87% of the words. This
percentage raised to 89% when all words were con-
sidered for edit distance 2 errors. The small degra-
dation in the performance occured because in 2% of
the cases, the words were checked for edit distance 1
errors although they had edit distance 2 errors. Fig-
ure 6 shows the effect of varying the characters limit
on the correction accuracy.
Figure 5 shows the effect of varying the weight as-
signed to corrections with lower edit distances on the
accuracy. As indicated in the figure, when we only
consider the language model weight, we get accura-
cies as low as 79%. As we favor corrections with
lower edit distances the correction accuracy raises,
but occasionally starts to decay again when empha-
sis on the low edit distance is much larger than that
on the language model weights.
Finally, we repeated the experiments but with us-
917
Figure 5: Effect of increasing lower edit distance
favoring factor on accuracy
Figure 6: Effect of increasing Ed1/Ed2 char limits
on accuracy
ing a confusion matrix, as 3.2.3. We found out that
the average computation time dropped by 78% ( be-
low 1 ms for edit distance 1 errors) at the price of
losing only 8% of the correction accuracy.
5 Conclusion
In this work, we present a finite state automata based
spelling errors detection and correction method.
The new method avoids calculating the edit dis-
tances at all steps of the correction process. It also
avoids building a Levenshtein-automata for each in-
put word. The method is multilingual and may work
for any language for which we have an electronic
dictionary, and a language model to assign probabil-
ity to word sequences. The preliminary experimen-
tal results show that the new method achieves good
performance for both correction time and accuracy.
The experiments done in this paper can be extended
in several directions. First, there is still much room
for optimizing the code to make it faster especially
the FST composition process. Second, we can allow
further editing operations like splitting and merging.
References
Francois De Bertrand De Beuvron and Philippe Trigano.
1995. Hierarchically coded lexicon with variants. In-
ternational Journal of Pattern Recognition and Artifi-
cial Intelligence, 9:145?165.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine learning, 34:107?130.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11:87?111.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction to au-
tomata theory, languages, and computation. Reading,
Massachusetts: Addison-Wesley.
Karen Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys, pages
377?439.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
- Doklady.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?89.
Christoph Ringlstetter, Max Hadersbeck, Klaus U.
Schulz, and Stoyan Mihov. 2007. Text correction
using domain dependent bigram models from web
crawls. In Proceedings of the International Joint Con-
ference on Artificial Intelligence (IJCAI-2007) Work-
shop on Analytics for Noisy Unstructured Text Data.
Emmanuel Roche and Yves Schabes. 1995. Determinis-
tic part-of-speech tagging with finite-state transducers.
Computational Linguistics, 2:227253.
Emmanuel Roche and Yves Shabes. 1997. Finite-state
language processing. Cambridge, MA, USA: MIT
Press.
Klaus Schulz and Stoyan Mihov. 2002. Fast string cor-
rection with levenshtein-automata. International Jour-
nal of Document Analysis and Recognition (IJDAR),
5:67?85.
R.A. Wagner and M.Fisher. 1974. The string-to-string
correction problem. Journal of the ACM.
Justin Zobel and Philip Dart. 1995. Finding approximate
matches in large lexicons. Software Practice and Ex-
perience, 25:331?345.
918
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 501?508,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Information Extraction Approach Using Graph Mutual 
Reinforcement  
 
 
Hany Hassan Ahmed Hassan Ossama Emam 
 
IBM Cairo Technology Development Center 
Giza, Egypt 
P.O. Box 166 Al-Ahram 
 
hanyh@eg.ibm.com hasanah@eg.ibm.com emam@eg.ibm.com 
 
  
 
Abstract 
Information Extraction (IE) is the task of 
extracting knowledge from unstructured 
text. We present a novel unsupervised 
approach for information extraction 
based on graph mutual reinforcement. 
The proposed approach does not require 
any seed patterns or examples. Instead, it 
depends on redundancy in large data sets 
and graph based mutual reinforcement to 
induce generalized ?extraction patterns?. 
The proposed approach has been used to 
acquire extraction patterns for the ACE 
(Automatic Content Extraction) Relation 
Detection and Characterization (RDC) 
task. ACE RDC is considered a hard task 
in information extraction due to the ab-
sence of large amounts of training data 
and inconsistencies in the available data. 
The proposed approach achieves superior 
performance which could be compared to 
supervised techniques with reasonable 
training data.  
1 Introduction 
In this paper we propose a novel, and completely 
unsupervised approach for information extrac-
tion. We present a general technique; however 
we focus on relation extraction as an important 
task of Information Extraction. The approach 
depends on constructing generalized extraction 
patterns, which could match many instances, and 
deploys graph based mutual reinforcement to 
weight the importance of these patterns. The mu-
tual reinforcement is used to automatically iden-
tify the most informative patterns, where patterns 
that match many instances tend to be correct. 
Similarly, instances matched by many patterns 
tend to be correct. The intuition is that large un-
supervised data is redundant, i.e. different in-
stances of information could be found many 
times in different contexts and by different repre-
sentation. The problem can therefore be seen as 
hubs (instances) and authorities (patterns) prob-
lem which can be solved using the Hypertext 
Induced Topic Selection (HITS) algorithm 
(Kleinberg, 1998). 
HITS is an algorithmic formulation of the no-
tion of authority in web pages link analysis, 
based on a relationship between a set of relevant 
?authoritative pages? and a set of ?hub pages?. 
The HITS algorithm benefits from the following 
observation:  when a page (hub) links to another 
page (authority), the former confers authority 
over the latter.  
By analogy to the authoritative web pages 
problem, we could represent the patterns as au-
thorities and instances as hubs, and use mutual 
reinforcement between patterns and instances to 
weight the most authoritative patterns. Highly 
weighted patterns are then used in extracting in-
formation.  
The proposed approach does not need any 
seeds or examples. Human involvement is only 
needed in determining the entities of interest; the 
entities among which we are seeking relations. 
The paper proceeds as follows: in Section 2 
we discuss previous work followed by a brief 
definition of our general notation in Section 3. A 
detailed description of the proposed approach 
then follows in Section 4. Section 5 discusses the 
application of the proposed approach to the prob-
501
lem of detecting semantic relations from text. 
Section 6 discusses experimental results while 
the conclusion is presented in Section 7. 
2 Previous Work 
Most of the previous work on Information Ex-
traction (IE) focused on supervised learning. Re-
lation Detection and Characterization (RDC) was 
introduced in the Automatic Content Extraction 
Program (ACE) (ACE, 2004). The approaches 
proposed to the ACE RDC task such as kernel 
methods (Zelenko et al, 2002) and Maximum 
Entropy methods (Kambhatla, 2004) required the 
availability of large set of human annotated cor-
pora which are tagged with relation instances. 
However human annotated instances are limited, 
expensive, and time consuming to obtain, due to 
the lack of experienced human annotators and the 
low inter-annotator agreements. 
Some previous work adopted weakly super-
vised or unsupervised learning approaches. 
These approaches have the advantage of not 
needing large tagged corpora but need seed ex-
amples or seed extraction patterns. The major 
drawback of these approaches is their depend-
ency on seed examples or seed patterns which 
may lead to limited generalization due to de-
pendency on handcrafted examples. Some of 
these approaches are briefed here: 
 (Brin,98) presented an approach for extracting 
the authorship information as found in books de-
scription on the World Wide Web. This tech-
nique is based on dual iterative pattern relation 
extraction wherein a relation and pattern set is 
iteratively constructed. This approach has two 
major drawbacks: the use of handcrafted seed 
examples to extract more examples similar to 
these handcrafted seed examples and the use of a 
lexicon as the main source for extracting infor-
mation. 
(Blum and Mitchell, 1998) proposed an ap-
proach based on co-training that uses unlabeled 
data in a particular setting. They exploit the fact 
that, for some problems, each example can be 
described by multiple representations. 
(Riloff & Jones, 1999) presented the Meta-
Bootstrapping algorithm that uses an un-
annotated training data set and a set of seeds to 
learn a dictionary of extraction patterns and a 
domain specific semantic lexicon. Other works 
tried to exploit the duality of patterns and their 
extractions for the purpose of inferring the se-
mantic class of words like (Thelen & Riloff, 
2002) and (Lin et al 2003). 
(Muslea et al, 1999) introduced an inductive 
algorithm to generate extraction rules based on 
user labeled training examples. This approach 
suffers from the labeled data bottleneck. 
(Agichtein et. al, 2000) presented an approach 
using seed examples to generate initial patterns 
and to iteratively obtain further patterns. Then 
ad-hoc measures were deployed to estimate the 
relevancy of the patterns that have been newly 
obtained. The major drawbacks of this approach 
are:  its dependency on seed examples leads to 
limited capability of generalization, and the esti-
mation of patterns relevancy requires the de-
ployment of ad-hoc measures. 
(Hasegawa et. al. 2004) introduced unsuper-
vised approach for relation extraction depending 
on clustering context words between named enti-
ties; this approach depends on ad-hoc context 
similarity between phrases in the context and 
focused on certain types of relations. 
(Etzioni et al 2005) proposed a system for 
building lists of named entities found on the web. 
Their system uses a set of eight domain-
independent extraction patterns to generate can-
didate facts. 
All approaches, proposed so far, suffer from 
either requiring large amount of labeled data or 
the dependency on seed patterns (or examples) 
that result in limited generalization. 
3 General Notation 
In graph theory, a graph is a set of objects called 
vertices joined by links called edges. A bipartite 
graph, also called a bigraph, is a special graph 
where the set of vertices can be divided into two 
disjoint sets with no two vertices of the same set 
sharing an edge.  
The Hypertext Induced Topic Selection 
(HITS) algorithm is an algorithm for rating, and 
therefore ranking, web pages. The HITS algo-
rithm makes use of the following observation: 
when a page (hub) links to another page (author-
ity), the former confers authority over the latter. 
HITS uses two values for each page, the "author-
ity value" and the "hub value". "Authority value" 
and "hub value" are defined in terms of one an-
other in a mutual recursion. An authority value is 
computed as the sum of the scaled hub values 
that point to that authority. A hub value is the 
sum of the scaled authority values of the authori-
ties it points to. 
A template, as we define for this work, is a se-
quence of generic forms that could generalize 
502
over the given instances. An example template 
is:  
GPE POS  (PERSON)+ 
 
GPE: Geographical Political En-
tity 
POS: possessive ending 
PERSON: PERSON Entity 
 
This template could match the sentence: 
?France?s President Jacque Chirac...?.  This tem-
plate is derived from the representation of the 
Named Entity tags, Part-of-Speech (POS) tags 
and semantic tags. The choice of the template 
representation here is for illustration purpose 
only; any combination of tags, representations 
and tagging styles might be used.  
A pattern is more specific than a template. A 
pattern specifies the role played by the tags (first 
entity, second entity, or relation). An example of 
a pattern is:  
    
GPE(E2)  POS   (PERSON)+(E1) 
 
This pattern indicates that the word(s) with the 
tag GPE in the sentence represents the second 
en-tity (Entity 2) in the relation, while the 
word(s) tagged PERSON represents the first en-
tity (Entity 1) in this relation, the ?+? symbol 
means that the (PERSON) entity is repetitive (i.e. 
may consist of several tokens).  
A tuple, in our notation during this paper, is 
the result of the application of a pattern to un-
structured text. In the above example, one result 
of applying the pattern to some raw text is the 
following tuple: 
 
Entity 1: Jacque Chirac 
Entity 2: France 
Relation: EMP-Executive 
4 The Approach 
The unsupervised graph-based mutual rein-
forcement approach, we propose, depends on the 
construction of generalized ?extraction patterns? 
that could match many instances. The patterns 
are then weighted according to their importance 
by deploying graph based mutual reinforcement 
techniques. This duality in patterns and extracted 
information (tuples) could be stated that patterns 
could match different tuples, and tuples in turn 
could be matched by different patterns. The pro-
posed approach is composed of two main steps 
namely, initial patterns construction and pattern 
weighting or induction. Both steps are detailed in 
the next sub-sections. 
4.1 Initial Patterns Construction 
As shown in Figure 1, several syntactic, lexical, 
and semantic analyzers could be applied to the 
unstructured text. The resulting analyses could be 
employed in the construction of extraction pat-
terns. It is worth mentioning that the proposed 
approach is general enough to accommodate any 
pattern design; the introduced pattern design is 
for illustration purposes only. 
 
 
 
 
Initially, we need to start with some templates 
and patterns to proceed with the induction proc-
ess. Relatively large amount of text data is 
tagged with different taggers to produce the pre-
viously mentioned patterns styles. An n-gram 
language model is built on this data and used to 
construct weighted finite state machines.  
Paths with low cost (high language model 
probabilities) are chosen to construct the initial 
set of templates; the intuition is that paths with 
low cost (high probability) are frequent and 
could represent potential candidate patterns. 
The resulting initial set of templates is applied 
to a very large text data to produce all possible 
patterns. The number of candidate initial patterns 
could be reduced significantly by specifying the 
candidate types of entities; for example we might 
specify that the first entity could be PEROSN or 
PEOPLE while the second entity could be OR-
GANIZATION, LOCATION, COUNTRY and 
etc...  
The candidate patterns are then applied to the 
tagged stream and the unstructured text to collect 
a set of patterns and matched tuples pairs.  
The following procedure briefs the Initial Pat-
tern Construction Step: 
? Select a random set of text data. 
American vice President   Al Gore said today... 
PEOPLE    O         O       PERSON   O    O... 
ADJ     NOUN_PHRASE   NNP  VBD CD... 
PEOPLE NOUN_PHRASE  PERSON  VBD CD... 
Entities 
POS 
Tagged 
Stream 
Figure 1:  An example of the output of analys-
ers applied to the unstructured text  
 
503
? Apply various taggers on text data and con-
struct templates style. 
? Build n-gram language model on template 
style data. 
? Construct weighted finite state machines 
from the n-gram language model. 
? Choose n-best paths in the finite state ma-
chines. 
? Use best paths as initial templates. 
? Apply initial templates on large text data. 
? Construct initial patterns and associated tu-
ples sets. 
4.2 Pattern Induction 
The inherent duality in the patterns and tuples 
relation suggests that the problem could be inter-
preted as a hub authority problem. This problem 
could be solved by applying the HITS algorithm 
to iteratively assign authority and hub scores to 
patterns and tuples respectively. 
 
 
Patterns and tuples are represented by a bipar-
tite graph as illustrated in figure 2. Each pattern 
or tuple is represented by a node in the graph. 
Edges represent matching between patterns and 
tuples. The pattern induction problem can be 
formulated as follows: Given a very large set of 
data D containing a large set of patterns P which 
match a large set of tuples T, the problem is to 
identify P
~
, the set of patterns that match the set 
of the most correct tuples  T
~
. The intuition is 
that the tuples matched by many different pat-
terns tend to be correct and the patterns matching 
many different tuples tend to be good patterns. In 
other words; we want to choose, among the large 
space of patterns in the data, the most informa-
tive, highest confidence patterns that could iden-
tify correct tuples; i.e. choosing the most ?au-
thoritative? patterns in analogy with the hub au-
thority problem. However, both P
~
and T
~
are un-
known. The induction process proceeds as fol-
lows:  each pattern p in P is associated with a 
numerical authority weight av which expresses 
how many tuples match that pattern. Similarly, 
each tuple t in T has a numerical hub weight ht 
which expresses how many patterns were 
matched by this tuple. The weights are calculated 
iteratively as follows: 
( ) ( )( )
=
+
=
pT
u i
i
i
H
uhpa
1 )(
)(
)1(
 (1) 
( ) ( )( )
=
+
=
tP
u i
i
i
A
ua
th
1 )(
)(
)1(
 (2) 
where T(p) is the set of tuples matched by p, P(t) 
is the set of patterns matching t, ( )pa i )1( +  is the 
authoritative weight of pattern p  at iteration  
)1( +i , and ( )th i )1( +  is the hub weight of tuple t  
at iteration  )1( +i  . H(i) and A(i) are normaliza-
tion factors defined as: 
 
( )( ) 
= =
=
||
1 1
)()( P
p
pT
u
ii uhH  (3) 
( )( ) 
= =
=
||
1 1
)()( T
v
tP
u
ii uaA
 (4) 
 
Highly weighted patterns are identified and used 
for extracting relations. 
4.3 Tuple Clustering 
The tuple space should be reduced to allow more 
matching between pattern-tuple pairs. This space 
reduction could be accomplished by seeking a 
tuple similarity measure, and constructing a 
weighted undirected graph of tuples. Two tuples 
are linked with an edge if their similarity meas-
ure exceeds a certain threshold. Graph clustering 
algorithms could be deployed to partition the 
graph into a set of homogeneous communities or 
clusters. To reduce the space of tuples, we seek a 
matching criterion that group similar tuples to-
gether. Using WordNet, we can measure the se-
mantic similarity or relatedness between a pair of 
concepts (or word senses), and by extension, be-
tween a pair of sentences. We use the similarity 
P
P
P
P
P
T
T
T
T
T
P
P
T
T
Patterns Tuples
Figure 2: A bipartite graph represent-
ing patterns and tuples 
504
measure described in (Wu and Palmer, 1994) 
which finds the path length to the root  node 
from the least common subsumer (LCS) of the 
two word senses which is the most specific word 
sense they share as an ancestor. The similarity 
score of two tuples, ST, is calculated as follows: 
 
2
2
2
1 EET SSS +=    (5) 
 
where SE1, and SE2 are the similarity scores of the 
first entities in the two tuples, and their second 
entitles respectively. 
The tuple matching procedure assigns a simi-
larity measure to each pair of tuples in the data-
set. Using this measure we can construct an undi-
rected graph G. The vertices of G are the tuples. 
Two vertices are connected with an edge if the 
similarity measure between their underlying tu-
ples exceeds a certain threshold. It was noticed 
that the constructed graph consists of a set of 
semi isolated groups as shown in figure 3. Those 
groups have a very large number of inter-group 
edges and meanwhile a rather small number of 
intra-group edges. This implies that using a 
graph clustering algorithm would eliminate those 
weak intra-group edges and produce separate 
groups or clusters representing similar tuples. We 
used Markov Cluster Algorithm (MCL) for graph 
clustering (Dongen, 2000). MCL is a fast and 
scalable unsupervised clustering algorithm for 
graphs based on simulation of stochastic flow. 
 
 
 
Figure 3: Applying Clustering Algorithms to Tu-
ple graph  
 
An example of a couple of tuples that could be 
matched by this technique is: 
United Stated(E2) presi-
dent(E1) 
US(E2) leader(E1) 
  
A bipartite graph of patterns and tuple clusters 
is constructed. Weights are assigned to patterns 
and tuple clusters by iteratively applying the 
HITS algorithm and the highly ranked patterns 
are then used for relation extraction.  
5 Experimental Setup 
5.1 ACE Relation Detection and Charac-
terization 
In this section, we describe Automatic Content 
Extraction (ACE). ACE is an evaluation con-
ducted by NIST to measure Entity Detection and 
Tracking (EDT) and Relation Detection and 
Characterization (RDC). The EDT task is con-
cerned with the detection of mentions of entities, 
and grouping them together by identifying their 
coreference. The RDC task detects relations be-
tween entities identified by the EDT task. We 
choose the RDC task to show the performance of 
the graph based unsupervised approach we pro-
pose. To this end we need to introduce the notion 
of mentions and entities. Mentions are any in-
stances of textual references to objects like peo-
ple, organizations, geopolitical entities (countries, 
cities ?etc), locations, or facilities. On the other 
hand, entities are objects containing all mentions 
to the same object. Here, we present some exam-
ples of ACE entities and relations: 
Spain?s Interior Minister 
announced this evening the 
arrest of separatist organi-
zation Eta?s presumed leader 
Ignacio Garcia Arregui. Ar-
regui, who is considered to 
be the Eta organization?s 
top man, was arrested at 
17h45 Greenwich. The Spanish 
judiciary suspects Arregui 
of ordering a failed attack 
on King Juan Carlos in 1995. 
 
In this fragment, all the underlined phrases are 
mentions to ?Eta? organization, or to ?Garcia 
Arregui?. There is a management relation be-
tween ?leader? which references to ?Gar-
cia Arregui? and ?Eta?. 
5.2 Patterns Construction and Induction 
We used the LDC English Gigaword Corpus, 
AFE source from January to August 1996 as a 
source for unstructured text. This provides a total 
of 99475 documents containing 36 M words.  In 
the performed experiments, we focus on two 
types of relations EMP-ORG relations and GPE-
AFF relations which represent almost 50% of all 
relations in RDC ? ACE task. 
T
T T
T
T
T
T
T
T
T
TT
T T
T
T T
T
T
T
T
T
T
T
T
T
T T
Before Clustering After Clustering
505
POS (part of speech) tagger and mention tagger 
were applied to the data, the used pattern design 
consists of a mix between the part of speech 
(POS) tags and the mention tags for the words in 
the unsupervised data. We use the mention tag, if 
it exists; otherwise we use the part of speech tag. 
An example of the analyzed text and the pre-
sumed associated pattern is shown: 
 
Text: Eta?s presumed leader 
Arregui ? 
Pos: NNP POS JJ NN NNP 
Mention: ORG 0 0 0 PERSON 
Pattern: ORG(E2) POS JJ 
NN(R) PERSON(E1) 
 
An n-gram language model, 5-gram model and 
back off to lower order n-grams, was built on the 
data tagged with the described patterns? style. 
Weighted finite states machines were constructed 
with the language model probabilities. The n-best 
paths, 20 k paths, were identified and deployed 
as the initial template set. Sequences that do not 
contain the entities of interest, and hence cannot 
represent relations, were automatically filtered 
out. This resulted in an initial templates set of 
around 3000 element. This initial templates set 
was applied on the text data to establish initial 
patterns and tuples pairs. Graph based mutual 
reinforcement technique was deployed with 10 
iterations on the patterns and tuples pairs to 
weight the patterns. 
We conducted two groups of experiments, the 
first with simple syntactic tuple matching, and 
the second with semantic tuple clustering as de-
scribed in section 4.3 
6 Results and Discussion 
We compare our results to a state-of-the-art su-
pervised system similar to the system described 
in (Kambhatla, 2004). Although it is unfair to 
make a comparison between a supervised system 
and a completely unsupervised system, we chose 
to make this comparison to test the performance 
of the proposed unsupervised approach on a real 
task with defined test set and state-of-the-art per-
formance. The supervised system was trained on 
145 K words which contain 2368 instances of the 
two relation types we are considering. 
The system performance is measured using 
precision, recall and F-Measure with various 
amounts of induced patterns. Table 1 presents the 
precision, recall and F-measure for the two rela-
tions using the presented approach with the utili-
zation of different amount of highly weighted 
patterns. Table 2 presents the same results using 
semantic tuple matching and clustering, as de-
scribed in section 4.3.  
 
No. of  
Patterns Precision Recall F-Measure 
1500 35.9 66.3 46.58 
1000 41.2 59.7 48.75 
700 43.1 58.1 49.49 
500 46 56.5 50.71 
400 46.9 52.9 49.72 
200 50.1 44.9 47.36 
 
Table 1:  The effect of varying the number of 
induced patterns on the system performance 
(syntactic tuple matching) 
 
No. of  
Patterns Precision Recall F-Measure 
1500 36.1 67.2 46.97 
1000 43.7 59.6 50.43 
700 44.1 59.3 50.58 
500 46.3 57.2 51.18 
400 47.3 57.6 51.94 
200 48.1 45.9 46.97 
 
Table 2:  The effect of varying the number of 
induced patterns on the system performance (se-
mantic tuple matching) 
0
10
20
30
40
50
60
70
80
Sup 67.1 54.2 59.96
Unsup-Syn 46 56.5 50.71
Unsup-Sem 47.3 57.6 51.94
Precision Recall F Measure
 
 
Figure 4:  A comparison between the supervised 
system (Sup), the unsupervised system with syn-
tactic tuple matching (Unsup-Syn), and with se-
mantic tuple matching (Unsup-Sem) 
 
Best F-Measure is achieved using relatively 
small number of induced patterns (400 and  500 
patterns) while using more patterns increases the 
recall but degrades the precision. 
Table 2 indicates that the semantic clustering 
of tuples did not provide significant improve-
506
ment; although better performance was achieved 
with less number of patterns (400 patterns). We 
think that the deployed similarity measure and it 
needs further investigation to figure out the rea-
son for that. 
Figure 4 presents the comparison between the 
proposed unsupervised systems and the reference 
supervised system. The unsupervised systems 
achieves good results even in comparison to  a 
state-of-the-art supervised system. 
Sample patterns and corresponding matching 
text are introduced in Table 3 and Table 4. Table 
3 shows some highly ranked patterns while Table 
4 shows examples of low ranked patterns. 
 
Pattern Matches 
GPE (PERSON)+ Peruvian President Alberto Fu-jimori 
GPE (PERSON)+ Zimbabwean President Robert Mugabe 
GPE (PERSON)+ PLO leader Yasser Arafat 
GPE POS (PERSON)+ Zimbabwe 's President Robert Mugabe 
GPE JJ PERSON    American clinical neuropsy-
chologist 
GPE JJ PERSON    American diplomatic personnel 
PERSON IN JJ GPE candidates for local government 
ORGANIZATION PER-
SON Airways spokesman 
ORGANIZATION PER-
SON      Ajax players 
PERSON IN DT (OR-
GANIZATION)+  
chairman of the opposition par-
ties 
(ORGANIZATION)+ 
PERSON    opposition parties chairmans 
 
Table3: Examples of patterns with high weights 
 
Pattern Matches 
GPE CC (PERSON)+ Barcelona and Johan 
Cruyff 
GPE , CC PERSON Paris , but Riccardi 
GPE VBZ VBN PERSON Pyongyang has accepted 
Gallucci 
GPE VBZ VBN PERSON Russia has abandoned us 
GPE VBZ VBN P PER-
SON 
Rwanda 's defeated Hutu 
GPE VBZ VBN PERSON state has pressed Arafat 
GPE VBZ VBN TO VB 
PERSON 
Taiwan has tried to keep 
Lee 
(PERSON)+ VBD GPE 
ORGANIZATION 
Alfred Streim told Ger-
man radio 
(PERSON)+ VBD GPE 
ORGANIZATION 
Dennis Ross met Syrian 
army 
(PERSON)+ VBD GPE 
ORGANIZATION 
Van Miert told EU indus-
try 
 
Table4: Examples of patterns with low weights 
7 Conclusion and Future Work 
In this work, a general framework for unsuper-
vised information extraction based on mutual 
reinforcement in graphs has been introduced. We 
construct generalized extraction patterns and de-
ploy graph based mutual reinforcement to auto-
matically identify the most informative patterns. 
We provide motivation for our approach from a 
graph theory and graph link analysis perspective. 
Experimental results have been presented sup-
porting the applicability of the proposed ap-
proach to ACE Relation Detection and Charac-
terization (RDC) task, demonstrating its applica-
bility to hard information extraction problems. 
The proposed approach achieves remarkable re-
sults comparable to a state-of-the-art supervised 
system, achieving 51.94 F-measure compared to 
59.96 F-measure of the state-of-the-art super-
vised system which requires huge amount of hu-
man annotated data. The proposed approach 
represents a powerful unsupervised technique for 
information extraction in general and particularly 
for relations extraction that requires no seed pat-
terns or examples and achieves significant per-
formance. 
In our future work, we plan to focus on general-
izing the approach for targeting more NLP prob-
lems. 
8 Acknowledgements 
We would like to thank Salim Roukos for his 
invaluable suggestions and support. We would 
also like to thank Hala Mostafa for helping with 
the early investigation of this work. Finally we 
would like to thank the anonymous reviewers for 
their constructive criticism and helpful com-
ments. 
References 
ACE. 2004. The NIST ACE evaluation website. 
http://www.nist.gov/speech/tests/ace/ 
Eugene Agichtein and Luis Gravano. 2000.  Snow-
ball: Extracting Relations from Large Plain-Text 
Collections. Proceedings of the 5th ACM Confer-
ence on Digital Libraries (DL 2000). 
   Sergy Brin. 1998. Extracting Patterns and Relations 
from the World Wide Web. Proceedings of the 1998 
International Workshop on the Web and Data-
bases? 
Stijn van Dongen. 2000. A Cluster Algorithm for 
Graphs. Technical Report INS-R0010, National 
Research Institute for Mathematics and Computer 
Science in the Netherlands. 
507
Stijn van Dongen. 2000. Graph Clustering by Flow 
Simulation. PhD thesis, University of Utrecht 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll (prelimi-
nary results). In Proceedings of the 13th World 
Wide Web Conference, pages 100-109. 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised Named-Entity Extraction from the Web: 
An Experimental Study. Artificial Intelligence, 
2005. 
Radu Florian, Hany Hassan, Hongyan Jing, Nanda 
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and 
Salim Roukos. 2004. A Statistical Model for multi-
lingual entity detection and tracking. Proceedings 
of the Human Language Technologies Conference 
(HLT-NAACL 2004). 
Dayne Freitag, and Nicholas Kushmerick. 2000. 
Boosted wrapper induction. The 14th European 
Conference on Artificial Intelligence Workshop on 
Machine Learning for Information Extraction 
Rayid Ghani and Rosie Jones. 2002. A Comparison of 
Efficacy and Assumptions of Bootstrapping Algo-
rithms for Training Information Extraction Sys-
tems. Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated 
Data at the Linguistic Resources and Evaluation 
Conference (LREC 2002). 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman. 
2004. Discovering Relations among Named Enti-
ties from Large Corpora. Proceedings of The 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2004). 
Taher Haveliwala. 2002. Topic-sensitive PageRank. 
Proceedings of the 11th International World Wide 
Web Conference 
Thorsten Joachims. 2003. Transductive Learning via 
Spectral Graph Partitioning. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML 2003). 
Nanda Kambhatla. 2004. Combining Lexical, Syntac-
tic, and Semantic Features with Maximum Entropy 
Models for Information Extraction. Proceedings of 
The 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL 2004). 
John Kleinberg. 1998. Authoritative Sources in a Hy-
perlinked Environment. Proceedings of the 9th 
ACM-SIAM Symposium on Discrete Algorithms. 
N. Kushmerick, D.S. Weld, R.B. Doorenbos. 1997. 
Wrapper Induction for Information Extraction. 
Proceedings of the International Joint Conference 
on Artificial Intelligence.  
Winston Lin, Roman Yangarber, Ralph Grishman. 
2003. Bootstrapped Learning of Semantic Classes 
from Positive and Negative Examples. Proceedings 
of the 20th International Conference on Machine 
Learning (ICML 2003) Workshop on The Contin-
uum from Labeled to Unlabeled Data in Machine 
Learning and Data Mining. 
Ion Muslea, Steven Minton, and Craig 
Knoblock.1999.  A hierarchical approach to wrap-
per induction. Proceedings of the Third Interna-
tional Conference on Autonomous Agents. 
Ted Pedersen, Siddharth Patwardhan, and Jason 
Michelizzi. 2004, WordNet::Similarity - Measuring 
the Relatedness of Concepts. Proceedings of Fifth 
Annual Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(NAACL 2004) 
Ellen Riloff and Rosie Jones. 2003. Learning diction-
aries for information extraction by multilevel boot-
strapping. Proceedings of the Sixteenth national 
Conference on Artificial Intelligence (AAAI 1999). 
Michael Thelen and Ellen Riloff. 2002. A Bootstrap-
ping Method for Learning Semantic Lexicons using 
Extraction Pattern Contexts. Proceedings of the 
2002 Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2002). 
Scott White, and Padhraic Smyth. 2003. Algorithms 
for Discoveing Relative Importance in Graphs. 
Proceedings of Ninth ACM SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining. 
Zhibiao Wu, and Martha Palmer. 1994. Verb seman-
tics and lexical selection. Proceedings of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1994). 
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 
2003. Semi-supervised Learning using Gaussian 
Fields and Harmonic Functions. Proceedings of 
the 20th International Conference on Machine 
Learning (ICML 2003). 
 
 
508
Workshop on TextGraphs, at HLT-NAACL 2006, pages 9?16,
New York City, June 2006. c?2006 Association for Computational Linguistics
Graph Based Semi-Supervised Approach for Information Extraction 
 
 
Hany Hassan Ahmed Hassan Sara Noeman 
 
IBM Cairo Technology Development Center 
Giza, Egypt 
                                                              P.O. Box 166 Al-Ahram 
 
hanyh@eg.ibm.com hasanah@eg.ibm.com noemans@eg.ibm.com 
 
 
 
 
Abstract 
Classification techniques deploy supervised 
labeled instances to train classifiers for 
various classification problems. However 
labeled instances are limited, expensive, 
and time consuming to obtain, due to the 
need of experienced human annotators.  
Meanwhile large amount of unlabeled data 
is usually easy to obtain. Semi-supervised 
learning addresses the problem of utilizing 
unlabeled data along with supervised la-
beled data, to build better classifiers.  In 
this paper we introduce a semi-supervised 
approach based on mutual reinforcement in 
graphs to obtain more labeled data to en-
hance the classifier accuracy. The approach 
has been used to supplement a maximum 
entropy model for semi-supervised training 
of the ACE Relation Detection and Charac-
terization (RDC) task. ACE RDC is con-
sidered a hard task in information 
extraction due to lack of large amounts of 
training data and inconsistencies in the 
available data. The proposed approach pro-
vides 10% relative improvement over the 
state of the art supervised baseline system. 
1 Introduction 
Classification techniques use labeled data to train 
classifiers for various classification problems.  Yet 
they often face a shortage of labeled training data. 
Labeled instances are often difficult, expensive, 
and /or time consuming to obtain. Meanwhile large 
numbers of unlabeled instances are often available. 
Semi-supervised learning addresses the problem of 
how unlabeled data can be usefully employed, 
along with labeled data, to build better classifiers. 
In this paper we propose a semi-supervised ap-
proach for acquiring more training instances simi-
lar to some labeled instances. The approach 
depends on constructing generalized extraction 
patterns, which could match many instances, and 
deploying graph based mutual reinforcement to 
weight the importance of these patterns.  The mu-
tual reinforcement is used to automatically identify 
the most informative patterns; where patterns that 
match many instances tend to be correct. Similarly, 
instances matched by many patterns also tend to be 
correct. The labeled instances should have more 
effect in the mutual reinforcement weighting proc-
ess. The problem can therefore be seen as hubs 
(instances) and authorities (patterns) problem 
which can be solved using the Hypertext Induced 
Topic Selection (HITS) algorithm (Kleinberg, 
1998 ). 
HITS is an algorithmic formulation of the notion 
of authority in web pages link analysis, based on a 
relationship between a set of relevant ?authorita-
tive pages? and a set of ?hub pages?. The HITS 
algorithm benefits from the following observation:  
when a page (hub) links to another page (author-
ity), the former confers authority over the latter.  
By analogy to the authoritative web pages prob-
lem, we could represent the patterns as authorities 
and instances as hubs, and use mutual reinforce-
ment between patterns and instances to weight the 
most authoritative patterns. Instances from unsu-
9
pervised data matched with the highly weighted 
patterns are then used in retraining the system.  
The paper proceeds as follows: in Section 2 we 
discuss previous work followed by a brief defini-
tion of our general notation in Section 3. A detailed 
description of the proposed approach then follows 
in Section 4. Section 5 discusses the application of 
the proposed approach to the problem of detecting 
semantic relations from text. Section 6 discusses 
experimental results while the conclusion is pre-
sented in Section 7. 
2 Previous Work 
(Blum and Mitchell, 1998) proposed an approach 
based on co-training that uses unlabeled data in a 
particular setting. They exploit the fact that, for 
some problems, each example can be described by 
multiple representations. They develop a boosting 
scheme which exploits conditional independence 
between these representations.  
(Blum and Chawla, 2001) proposed  a general 
approach utilizing unlabeled data by constructing a 
graph on all the data points based on distance rela-
tionships among examples, and then to use the 
known labels to perform a graph partitioning using  
the minimum cut that agrees with the labeled data. 
(Zhu et al, 2003) extended this approach by pro-
posing a  cut based on the assumption that labels 
are generated according to a Markov Random 
Field on the graph , (Joachims, 2003) presented  an 
algorithm based on spectral graph partitioning. 
(Blum et al, 2004) extended the min-cut  approach 
by adding randomness to the graph structure, their 
algorithm addresses several shortcomings of the 
basic mincut approach, yet it may not help in cases 
where the graph does not have small cuts for a 
given classification problem. 
3 Background  
In graph theory, a graph is a set of objects called 
vertices joined by links called edges. A bipartite 
graph, also called a bigraph, is a special graph 
where the set of vertices can be divided into two 
disjoint sets with no two vertices of the same set 
sharing an edge.  
The Hypertext Induced Topic Selection (HITS) 
algorithm is an algorithm for rating, and therefore 
ranking, web pages. The HITS algorithm makes 
use of the following observation: when a page 
(hub) links to another page (authority), the former 
confers authority over the latter. HITS uses two 
values for each page, the "authority value" and the 
"hub value". "Authority value" and "hub value" are 
defined in terms of one another in a mutual recur-
sion. An authority value is computed as the sum of 
the scaled hub values that point to that authority. A 
hub value is the sum of the scaled authority values 
of the authorities it points to. 
A template, as we define for this work, is a se-
quence of generic forms that could generalize over 
the given training instance. An example template 
is:  
COUNTRY  NOUN_PHRASE PERSON 
VERB_PHRASE  
This template could represent the sentence: 
?American vice President Al Gore visited ...?.  
This template is derived from the representation of 
the Named Entity tags, Part-of-Speech (POS) tags 
and semantic tags. The choice of the template rep-
resentation here is for illustration purpose only; 
any combination of tags, representations and tag-
ging styles might be used.  
A pattern is more specific than a template. A 
pattern specifies the role played by the tags (first 
entity, second entity, or relation). An example of a 
pattern is: 
COUNTRY(E2) NOUN_PHRASE(R) PERSON(E1)   
VERB_PHRASE  
This pattern indicates that the word(s) with the 
tag COUNTRY in the sentence represents the sec-
ond entity (Entity 2) in the relation, while the 
word(s) tagged PERSON represents the first entity 
(Entity 1) in this relation. Finally, the word(s) with 
the tag NOUN_PHRASE represents the relation 
between the two previous entities.   
A tuple, in our notation during this paper, is the 
result of the application of a pattern to unstructured 
text. In the above example, one result of applying 
the pattern to some raw text is the following tuple: 
Entity 1:  Al Gore 
Entity 2: United States 
Relation: vice President 
4 The Approach 
The semi-supervised graph-based approach we 
propose depends on the construction of generalized 
extraction patterns that could match many training 
instances. The patterns are then weighted accord-
ing to their importance by deploying graph based 
10
mutual reinforcement techniques. Patterns derived 
from the supervised training instances should have 
a superior effect in the reinforcement weighting 
process. This duality in patterns and tuples relation 
could be stated that patterns could match different 
tuples, and tuples in turn could be matched by dif-
ferent patterns. The proposed approach is com-
posed of two main steps namely, pattern extraction 
and pattern weighting or induction. Both steps are 
detailed in the next subsections. 
4.1 Patterns Extraction 
As shown in Figure 1, several syntactic, lexical, 
and semantic analyzers could be applied to the 
training instances. The resulting analyses could be 
employed in the construction of extraction pat-
terns. Any extraction pattern could match different 
relations and hence could produce several tuples. 
As an example let?s consider the pattern depicted 
in figure 1: 
 
 
Figure 1:  An example of a pattern and its possible 
tuples. 
 
PEOPLE_Inhabitant(E2) NOUN_PHRASE(R) 
PERSON(E1) VERB_PHRASE  
This pattern could extract the tuple: 
Entity 1: Al Gore 
Entity 2: American  
Relation: vice President 
Another tuple that could be extracted by the same 
pattern is:  
Entity 1: Berlusconi 
Entity 2: Italian  
Relation: Prime Minister 
On the other hand, many other patterns could ex-
tract the same information in the tuple from differ-
ent contexts. It is worth mentioning that the 
proposed approach is general enough to accommo-
date any pattern design; the introduced pattern de-
sign is for illustration purposes only. 
To further increase the number of patterns that 
could match a single tuple, the tuple space might 
be reduced i.e. by grouping tuples conveying the 
same information content together into a single 
tuple. This will be detailed further in the experi-
mental setup section. 
4.2   Pattern Induction 
The inherent duality in the patterns and tuples rela-
tion suggests that the problem could be interpreted 
as a hub authority problem. This problem could be 
solved by applying the HITS algorithm to itera-
tively assign authority and hub scores to patterns 
and tuples respectively. 
 
Figure 2: A bipartite graph representing patterns 
and tuples 
 
Patterns and tuples are represented by a bipartite 
graph as illustrated in figure 2. Each pattern or tu-
ple is represented by a node in the graph. Edges 
represent matching between patterns and tuples.  
The pattern induction problem can be formu-
lated as follows: Given a very large set of data D 
containing a large set of patterns P which match a 
P
P
P
P
P
T
T
T
T
T
P
P
T
T
Patterns Tuples
American vice President   Al Gore said today... 
Word: American 
Entity: PEOPLE 
POS : ADJ 
Sem: Inhabitant 
Word: vice president 
Entity:  
POS: NOUN_PHRASE 
Sem:  
Word: Al Gore 
Entity: PERSON 
POS: 
Sem: 
PEOPLE_Inhabitant    NOUN_PHRASE        PERSON 
VERB_PHRASE 
 
Entity 1:  Al Gore 
Entity 2: American 
Relation: vice President 
American vice Presi-
dent   Al Gore said 
today? 
Italian Prime Minister 
Berlusconi  visited?.. 
Entity 1: Berlusconi  
Entity 2: Italian 
Relation: prime minister 
11
large set of tuples T, the problem is to identify P
~
, 
the set of patterns that match the set of the most 
correct tuplesT
~
. The intuition is that the tuples 
matched by many different patterns tend to be cor-
rect and the patterns matching many different tu-
ples tend to be good patterns. In other words; we 
want to choose, among the large space of patterns 
in the data, the most informative, highest confi-
dence patterns that could identify correct tuples; 
i.e. choosing the most ?authoritative? patterns in 
analogy with the hub authority problem. However, 
both P
~
andT
~
are unknown. The induction process 
proceeds as follows:  each pattern p in P is associ-
ated with a numerical authority weight av which 
expresses how many tuples match that pattern. 
Similarly, each tuple t in T has a numerical hub 
weight ht which expresses how many patterns were 
matched by this tuple. The weights are calculated 
iteratively as follows: 
( ) ( )( )
=
+
=
pT
u i
i
i
H
uhpa
1 )(
)(
)1(
 (1) 
( ) ( )( )
=
+
=
tP
u i
i
i
A
ua
th
1 )(
)(
)1(
 (2) 
where T(p) is the set of tuples matched by p, P(t) is 
the set of patterns matching t, ( )pa i )1( +  is the au-
thoritative weight of pattern p  at iteration  )1( +i , 
and ( )th i )1( +  is the hub weight of tuple t  at itera-
tion  )1( +i  . H(i) and A(i) are normalization fac-
tors defined as: 
 
( )( ) 
= =
=
||
1 1
)()( P
p
pT
u
ii uhH  (3) 
( )( ) 
= =
=
||
1 1
)()( T
v
tP
u
ii uaA
 (4) 
Patterns with weights lower than a predefined 
threshold are rejected, and examples associated 
with highly ranked patterns are then used in unsu-
pervised training. 
It is worth mentioning that both T and P contain 
supervised and unsupervised examples, however 
the proposed method could assign weights to the 
correct examples (tuples and patterns) in a com-
pletely unsupervised setup. For semi-supervised 
data some supervised examples are provided, 
which are associated in turn with tuples and pat-
terns.  
We adopt the HITS extension introduced in 
(White and Smyth, 2003) to extend HITS with Pri-
ors. By analogy, we handle the supervised exam-
ples as priors to the HITS induction algorithm.  
A prior probabilities vector pr ={pr1, . . . , prn}  
is defined such that the probabilities sum to 1,  
where prv denotes the relative importance (or 
?prior bias?) we attach to node v. A pattern Pi is 
assigned a prior pri=1/n if pattern Pi matches a 
supervised tuple, otherwise pri is set to zero, n is 
the total number of patterns that have a supervised 
match. We also define a ?back probability? 
 
, 0   
 
   1 which determines how often we bias the su-
pervised nodes: 
( ) ( ) ( )( ) ppTu i
i
i pr
H
uhpa *1
1 )(
)(
)1( ?? +



?= 
=
+
 (5) 
( ) ( ) ( )( ) ttPu i
i
i pr
A
ua
th *1
1 )(
)(
)1( ?? +



?= 
=
+
  (6) 
where T(p) is the set of tuples matched by p , P(t) 
is the set of patterns matching t, and H(i) and A(i) 
are normalization factors defined as in  equations 
(3) and (4) 
 
Thus each node in the graph (pattern or tuple) has 
an associated prior weight depending on its super-
vised data. The induction process proceeds to itera-
tively assign weights to the patterns and tuples. In 
the current work we used 5.0=? . 
5 Experimental Setup  
5.1 ACE Relation Detection and Characteri-
zation 
In this section, we describe Automatic Content 
Extraction (ACE). ACE is an evaluation conducted 
by NIST to measure Entity Detection and Tracking 
(EDT) and Relation Detection and Characteriza-
tion (RDC). The EDT task is concerned with the 
detection of mentions of entities, and grouping 
them together by identifying their coreference. The 
RDC task detects relations between entities identi-
fied by the EDT task. We choose the RDC task to 
show the performance of the graph based semi-
supervised information extraction approach we 
propose. To this end we need to introduce the no-
tion of mentions and entities. Mentions are any 
instances of textual references to objects like peo-
12
ple, organizations, geo-political entities (countries, 
cities ?etc), locations, or facilities. On the other 
hand, entities are objects containing all mentions to 
the same object. 
 
Type Subtype Number of Instances 
User-Owner 
Inventor ART 
Other 
331 
DISC DISC 143 
Employ-Exec 
Employ-Staff 
Employ-Undetermined 
Member-of-Group 
Subsidiary 
EMP-ORG 
Other 
1673 
Ethnic 
Ideology Other-AFF 
Other 
153 
Citizen-Resident 
Based-in GPE-AFF 
Other 
695 
Business 
Family PER-SOC 
Other 
358 
Located 
Near PHYS 
Part-Whole 
1411 
 
Table 1. Types and subtypes of ACE relations 
 
Table 1 lists the types and subtypes of relations 
for the ACE RDC task. Here, we present an exam-
ple for those relations: 
 
Spain?s Interior Minister an-
nounced this evening the ar-
rest of separatist 
organization Eta?s presumed 
leader Ignacio Garcia Ar-
regui. Arregui, who is con-
sidered to be the Eta 
organization?s top man, was 
arrested at 17h45 Greenwich. 
The Spanish judiciary sus-
pects Arregui of ordering a 
failed attack on King Juan 
Carlos in 1995. 
In this fragment, all the underlined phrases are 
mentions to Eta organization, or to ?Garcia Ar-
regui?. There is a management relation between 
leader which references to ?Garcia Arregui? and 
Eta. 
5.2 Baseline System 
The base line system uses a Maximum Entropy 
model that combines diverse lexical, syntactic and 
semantic features derived from text, like the sys-
tem described in (Nanda, 2004). The system was 
trained on the ACE training data provided by LDC. 
The training set contained 145K words, and 4764 
instances of relations, the number of instances cor-
responding to each relation is shown in Table 1. 
The test set contained around 41K words, and 
1097 instances of relations. The system was evalu-
ated using standard ACE evaluation procedure. 
ACE evaluation procedure assigns the system an 
ACE value for each relation type and a total ACE 
value. The ACE value is a standard NIST metric 
for evaluating relation extraction. The reader is 
referred to the ACE web site (ACE, 2004) for more 
details.  
5.3 Pattern Construction 
We used the baseline system described in the pre-
vious section to label a large amount of unsuper-
vised data. The data comes from LDC English 
Gigaword corpus, Agence France Press English 
Service (AFE). The data contains around 3M 
words, from which 80K instances of relations have 
been extracted. 
We start by extracting a set of patterns that rep-
resent the supervised and unsupervised data. We 
consider each relation type separately and extract a 
pattern for each instance in the selected relation. 
The pattern we used consists of a mix between the 
part of speech (POS) tags and the mention tags for 
the words in the training instance. We use the men-
tion tag, if it exists; otherwise we use the part of 
speech tag. An example of a pattern is: 
 
Text: Eta?s presumed leader 
Arregui ? 
Pos: NNP POS JJ NN NNP 
Mention: ORG 0 0 0 PERSON 
Pattern: ORG(E2) POS JJ NN(R) 
PERSON(E1) 
13
5.4 Tuples Clustering 
As discussed in the previous section, the tuple 
space should be reduced to allow more matching 
between pattern-tuple pairs. This space reduction 
could be accomplished by seeking a tuple similar-
ity measure, and constructing a weighted undi-
rected graph of tuples. Two tuples are linked with 
an edge if their similarity measure exceeds a cer-
tain threshold. Graph clustering algorithms could 
be deployed to partition the graph into a set of ho-
mogeneous communities or clusters. To reduce the 
space of tuples, we seek a matching criterion that 
group similar tuples together. Using WordNet, we 
can measure the semantic similarity or relatedness 
between a pair of concepts (or word senses), and 
by extension, between a pair of sentences. We use 
the similarity measure described in (Wu and 
Palmer, 1994) which finds the path length to the 
root  node from the least common subsumer (LCS) 
of the two word senses which is the most specific 
word sense they share as an ancestor. The similar-
ity score of two tuples, ST, is calculated as follows:. 
2
2
2
1 EET SSS +=   (9) 
where SE1, and SE2 are the similarity scores of the 
first entities in the two tuples, and their second en-
titles respectively. 
The tuple matching procedure assigns a similarity 
measure to each pair of tuples in the dataset. Using 
this measure we can construct an undirected graph 
G. The vertices of G are the tuples. Two vertices 
are connected with an edge if the similarity meas-
ure between their underlying tuples exceeds a cer-
tain threshold. It was noticed that the constructed 
graph consists of a set of semi isolated groups as 
shown in figure 3. Those groups have a very large 
number of inter-group edges and meanwhile a 
rather small number of intra-group edges. This im-
plies that using a graph clustering algorithm would 
eliminate those weak intra-group edges and pro-
duce separate groups or clusters representing simi-
lar tuples. We used Markov Cluster Algorithm 
(MCL) for graph clustering (Dongen, 2000). MCL 
is a fast and scalable unsupervised cluster algo-
rithm for graphs based on simulation of stochastic 
flow. 
A bipartite graph of patterns and tuple clusters is 
constructed. Weights are assigned to patterns and 
tuple clusters by iteratively applying the HITS with 
Priors? algorithm. Instances associated with highly 
ranked patterns are then added to the training data 
and the model is retrained. Samples of some highly 
ranked patterns and corresponding matching text 
are introduced in Table 2. 
 
 
Figure 3: Applying Clustering Algorithms to Tuple 
graph  
 
Pattern Matches 
GPE PERSON 
PERSON PERSON 
Zimbabwean President 
Robert Mugabe 
GPE POS PERSON 
PERSON 
Zimbabwe 's President 
Robert Mugabe 
GPE JJ PERSON American diplomatic per-
sonnel 
PERSON IN JJ GPE candidates for local gov-
ernment 
ORGANIZATION 
PERSON Airways spokesman 
ORGANIZATION 
PERSON      Ajax players 
PERSON IN DT JJ  
ORGANIZATION  
chairman of the opposition 
parties 
ORGANIZATION 
PERSON    parties chairmans 
 
Table 2: Examples of patterns with high weights 
6 Results and Discussion 
We train several models like the one described in 
section 5.2 on different training data sets. In all 
experiments, we use both the LDC ACE training 
data and the labeled unsupervised data induced 
with the graph based approach we propose. We use 
the ACE evaluation procedure and ACE test cor-
pus, provided by LDC, to evaluate all models. 
We incrementally added labeled unsupervised 
data to the training data to determine the amount of 
data after which degradation in the system per-
formance occurs. We sought this degradation point 
separately for each relation type. Figure 4 shows 
the effect of adding labeled unsupervised data on 
T
T T
T
T
T
T
T
T
T
TT
T T
T
T T
T
T
T
T
T
T
T
T
T
T T
Before Clustering After Clustering
14
the ACE value for each relation separately. We 
notice from figure 4 and table 1 that relations with 
a small number of training instances had a higher 
gain in performance compared to relations with a 
large number of training instances. This implies 
that the proposed approach achieves significant 
improvement when the number of labeled training 
instances is small but representative. 
. 
0
10
20
30
40
50
60
0 50 100 200 300 400 500
Number of Added Documents
A
CE
 
Va
lu
e
EMP-ORG
PER-SOC
ART
PHYS
GPE-AFF
OTHER-
AFF
 
 
Figure 4: The effect of adding labeled unsuper-
vised data on the ACE value for each relation. The 
average number of relations per document is 4. 
 
From figure 4, we determined the number of 
training instances resulting in the maximum boost 
in performance for each relation. We added the 
training instances corresponding to the maximum 
boost in performance for all relations to the super-
vised training data and trained a new model on 
them. Figure 5 compares the ACE values for each 
relation in the base line model and the final model 
The total system ACE value has been improved 
by 10% over the supervised baseline system. All 
relation types, except the DSC relation, had sig-
nificant improvement ranging from 7% to 30% 
over the baseline supervised system. The DISC 
relation type had a small degradation; noting that it 
already has a low ACE value with the baseline sys-
tem. We think this is due to the fact that the DISC 
relation has few and inconsistent examples in the 
supervised data set. 
To assess the usefulness of the smoothing 
method employing WordNet distance, we repeated 
the experiment on EMP-ORG relation without it. 
We found out that it contributed to almost 30% of 
the total achieved improvement. We also repeated 
the experiment but with considering hub scores 
instead of authority scores. We added the examples 
associated with highly ranked tuples to the training 
set. We noticed that using hub scores yielded very 
little variation in the ACE value (i.e. 0.1 point for 
EMP-ORG relation). 
0
10
20
30
40
50
AC
E 
Va
lu
e
BaseLine 36.7 6 33.1 22.3 23.6 42.2 26.4 30.5
Final Model 39.6 4.2 35.8 24.7 30.8 46.6 28.2 33.5
ART DISC EMP-ORG
GPE-
AFF
OTHE
R-AFF
PER-
SOC PHYS TOTAL
 
 
Figure 5: A comparison of base line ACE values, 
and final ACE values for each relation. 
 
To evaluate the quality and representativeness of 
the labeled unsupervised data, acquired using the 
proposed approach, we study the effect of replac-
ing supervised data with unsupervised data while 
holding the amount of training data fixed. Several 
systems have been built using mixture of the su-
pervised and the unsupervised data. In Figure 6, 
the dotted line shows the degradation in the system 
performance when using a reduced amount of su-
pervised training data only, while the solid line 
shows the effect of replacing supervised training 
data with unsupervised labeled data on the system 
performance. We notice from Figure 6 that the un-
supervised data could replace more than 50% of 
the supervised data without any degradation in the 
system performance. This is an indication that the 
induced unsupervised data is good for training the 
classifier.  
 
26
27
28
29
30
31
32
33
34
0% 25% 50% 75%
100% 75% 50% 25%
Percentage of Unsupervised/Supervised 
Data
A
CE
 
Va
lu
e Sup + Unsup
Data
Sup Data Only
Unsupervised
Supervised
 
 
Figure 6: The effect of removing portions of the 
supervised data on the ACE value. And the effect 
15
of replacing portions of the supervised data with 
labeled training data. 
7 Conclusion 
We introduce a general framework for semi-
supervised learning based on mutual reinforcement 
in graphs. We construct generalized extraction pat-
terns and deploy graph based mutual reinforcement 
to automatically identify the most informative pat-
terns. We provide motivation for our approach 
from a graph theory and graph link analysis per-
spective. 
We present experimental results supporting the 
applicability of the proposed approach to ACE Re-
lation Detection and Characterization (RDC) task, 
demonstrating its applicability to hard information 
extraction problems. Our approach achieves a sig-
nificant improvement over the base line supervised 
system especially when the number of labeled in-
stances is small. 
8 Acknowledgements 
We would like to thank Nanda Kambhatla for pro-
viding the ACE baseline system. We would also 
like to thank Salim Roukos for several invaluable 
suggestions and guidance. Finally we would like to 
thank the anonymous reviewers for their construc-
tive criticism and helpful comments. 
References  
ACE. 2004. The NIST ACE evaluation website. 
http://www.nist.gov/speech/tests/ace/ 
Avrim Blum, and Tom Mitchell. 1998. Combining La-
beled and Unlabeled data with Co-training. Proceed-
ings of the 11th Annual Conference on 
Computational Learning Theory. 
 Avrim Blum and Shuchi Chawla. 2001. Learning From 
Labeled and Unlabeled Data Using Graph Mincuts. 
Proceedings of International Conference on Machine 
Learning (ICML). 
Avrim Blum, John Lafferty, Mugizi Rwebangira, and 
Rajashekar Reddy. 2004. Semi-supervised Learning 
Using Randomized Mincuts. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML).  
Stijn van Dongen. 2000. A Cluster Algorithm for 
Graphs. Technical Report INS-R0010, National Re-
search Institute for Mathematics and Computer Sci-
ence in the Netherlands. 
Stijn van Dongen. 2000. Graph Clustering by Flow 
Simulation. PhD thesis, University of Utrecht 
Radu Florian, Hany Hassan, Hongyan Jing, Nanda 
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and 
Salim Roukos. 2004. A Statistical Model for multi-
lingual entity detection and tracking. Proceedings of 
the Human Language Technologies Conference 
(HLT-NAACL?04). 
Dayne Freitag, and Nicholas Kushmerick. 2000. 
Boosted wrapper induction. The 14th European Con-
ference on Artificial Intelligence Workshop on Ma-
chine Learning for Information Extraction 
Taher Haveliwala. 2002. Topic-sensitive PageRank. 
Proceedings of the 11th International World Wide 
Web Conference 
Thorsten Joachims. 2003. Transductive Learning via 
Spectral Graph Partitioning. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML). 
John Kleinberg. 1998. Authoritative Sources in a Hy-
perlinked Environment. Proceedings of the. 9th 
ACM-SIAM Symposium on Discrete Algorithms. 
Nanda Kambhatla. 2004. Combining Lexical, Syntactic, 
and Semantic Features with Maximum Entropy Mod-
els for Information Extraction. Proceedings of the 
42nd Annual Meeting of the Association for Compu-
tational Linguistics 
Ted  Pedersen, Siddharth Patwardhan, and Jason Mich-
elizzi, 2004, WordNet::Similarity - Measuring the 
Relatedness of Concepts. Proceedings of Fifth An-
nual Meeting of the North American Chapter of the 
Association for Computational Linguistics (NAACL-
2004) 
Scott White, and Padhraic Smyth. 2003. Algorithms for 
Discoveing Relative Importance in Graphs. Proceed-
ings of Ninth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining. 
Zhibiao Wu, and Martha Palmer. 1994. Verb semantics 
and lexical selection. Proceedings of the 32nd An-
nual Meeting of the Association for Computational 
Linguistics. 
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 
2003. Semi-supervised Learning using Gaussian 
Fields and Harmonic Functions. Proceedings of the 
20th International Conference on Machine Learning. 
16
BioNLP 2007: Biological, translational, and clinical language processing, pages 89?96,
Prague, June 2007. c?2007 Association for Computational Linguistics
BioNoculars: Extracting Protein-Protein Interactions from Biomedical Text
Amgad Madkour, *Kareem Darwish, Hany Hassan, Ahmed Hassan, Ossama Emam
Human Language Technologies Group
IBM Cairo Technology Development Center
P.O.Box 166 El-Ahram, Giza, Egypt
{amadkour,hanyh,hasanah,emam}@eg.ibm.com,*kareem@darwish.org
Abstract
The vast number of published medical doc-
uments is considered a vital source for rela-
tionship discovery. This paper presents a sta-
tistical unsupervised system, called BioNoc-
ulars, for extracting protein-protein interac-
tions from biomedical text. BioNoculars
uses graph-based mutual reinforcement to
make use of redundancy in data to construct
extraction patterns in a domain independent
fashion. The system was tested using MED-
LINE abstract for which the protein-protein
interactions that they contain are listed in the
database of interacting proteins and protein-
protein interactions (DIPPPI). The system
reports an F-Measure of 0.55 on test MED-
LINE abstracts.
1 Introduction
With the ever-increasing number of published
biomedical research articles and the dependency
of new research and previously published research,
medical researchers and practitioners are faced with
the daunting prospect of reading through hundreds
or possibly thousands of research articles to sur-
vey advances in areas of interest. Much work has
been done to ease access and discovery of articles
that match the interest of researchers via the use
of search engines such as PubMed, which provides
search capabilities over MEDLINE, a collection of
more than 15 million journal paper abstracts main-
tained by the National Library of Medicine (NLM).
However, with the addition of abstracts from more
than 5,000 medical journals to MEDLINE every
year, the number of articles containing information
that is pertinent to users needs has grown consider-
ably. These 5,000 journals constitute only a subset
of the published biomedical research. Further, med-
ical articles often contain redundant information and
only subsections of articles are typically of direct in-
terest to researchers. More advanced information
extraction tools have been developed to effectively
distill medical articles to produce key pieces of in-
formation from articles while attempting to elimi-
nate redundancy. These tools have focused on areas
such as protein-protein interaction, gene-disease re-
lationship, and chemical-protein interaction (Chun
et al, 2006). Many of these tools have been used
to extract key pieces of information from MED-
LINE. Most of the reported information extraction
approaches use sets of handcrafted rules in conjunc-
tion with manually curated dictionaries and ontolo-
gies.
This paper presents a fully unsupervised statisti-
cal technique to discover protein-protein interaction
based on automatically discoverable repeating pat-
terns in text that describe relationships. The paper
is organized as follows: section 2 surveys related
work; section 3 describes BioNoculars; Section 4
describes the employed experimental setup; section
5 reports and comments on experimental results; and
section 6 concludes the paper.
2 Background
The background will focus primarily on the tagging
of Biomedical Named Entities (BNE), such genes,
gene-products, proteins, and chemicals and the Ex-
89
traction of protein-protein interactions from text.
2.1 BNE Tagging
Concerning BNE tagging, the most common ap-
proaches are based on hand-crafted rules, statisti-
cal classifiers, or a hybrid of both (usually in con-
junction with dictionaries of BNE). Rule-based sys-
tems (Fukuda et al, 1998; Hanisch et al, 2003; Ya-
mamoto et al, 2003) that use dictionaries tend to
exhibit high precision in tagging named entities but
generally with lower tagging recall. They tend to
lag the latest published research and are sensitive
to the expression of the named entities. Dictionar-
ies of BNE are typically laborious and expensive to
build, and they are dependant on nomenclatures and
specific species. Statistical approaches (Collier et
al., 2000; Kazama et al, 2002; Settles, 2004) typ-
ically improve recall at the expense of precision,
but are more readily retargetable for new nomen-
clatures and organisms. Hybrid systems (Tanabe
and Wilbur, 2002; Mika and Rost, 2004) attempt to
take advantage of both approaches. Although these
approaches tend to generate acceptable recognition,
they are heavily dependent on the type of data on
which they are trained.
(Fukuda et al, 1998) proposed a rule-based pro-
tein name extraction system called PROPER (PRO-
tein Proper-noun phrase Extracting Rules) system,
which utilizes a set of rules based on the surface
form of text in conjunction with a Part-Of-Speech
(POS) tagging to identify what looks like a protein
without referring to any specific BNE dictionary.
They reported a 94.7% precision and a 98.84% re-
call for the identification of BNEs. The results that
they achieved seem to be too specific to their train-
ing and test sets.
(Hanisch et al, 2003) proposed a rule-based
protein and gene name extraction system called
ProMiner, which is based on the construction of a
general-purpose dictionary along with different dic-
tionaries of synonyms and an automatic curation
procedure based on a simple token model of protein
names. Results showed that their system achieved a
0.80 F-measure score in the name extraction task on
the BioCreative test set (BioCreative).
(Yamamoto et al, 2003) proposed the use of mor-
phological analysis to improve protein name tag-
ging. Their approach tags proteins based on mor-
pheme chunking to properly determine protein name
boundary. They used the GENIA corpus for training
and testing and obtained an F-measure score of 0.70
for protein name tagging.
(Collier et al, 2000) used a machine learning ap-
proach to protein name extraction based on a linear
interpolation Hidden Markov Model (HMM) trained
using bi-grams. They focused on finding the most
likely protein sequence classes (C) for a given se-
quence of words (W), by maximizing the probabil-
ity of C given W, P(C?W). Unlike traditional dic-
tionary based methods, the approach uses no manu-
ally crafted patterns. However, their approach may
misidentify term boundaries for phrases containing
potentially ambiguous local structures such as co-
ordination and parenthesis. They reported an F-
measure score of 0.73 for different mixtures of mod-
els tested on 20 abstracts.
(Kazama et al, 2002) proposed a machine learn-
ing approach to BNE tagging based on support vec-
tor machines (SVM), which was trained on the GE-
NIA corpus. Their preliminary results of the system
showed that the SVM with the polynomial kernel
function outperforms techniques of Maximum En-
tropy based systems.
Yet another BNE tagging system is ABNER (Set-
tles, 2005), which utilizes machine learning, namely
conditional random fields, with a variation of or-
thographic and contextual features and no seman-
tic or syntactic features. ABNER achieves an F-
measure score of 0.71 on the NLPA 2004 shared
task dataset corpus and 0.70 on the BioCreative cor-
pus.and scored an F1-measure of 51.8set.
(Tanabe and Wilbur, 2002) used a combination
of statistical and knowledge-based strategies, which
utilized automatically generated rules from transfor-
mation based POS tagging and other generated rules
from morphological clues, low frequency trigrams,
and indicator terms. A key step in their method is
the extraction of multi-word gene and protein names
that are dominant in the corpus but inaccessible to
the POS tagger. The advantage of such an approach
is that it is independent of any biomedical domain.
However, it can miss single word gene names that
do not occur in contextual gene theme terms. It
can also incorrectly tag compound gene names, plas-
mids, and phages.
(Mika and Rost, 2004) developed NLProt, which
90
combines the use of dictionaries, rules-based filter-
ing, and machine learning based on an SVM classi-
fier to tag protein names in MEDLINE. The NLProt
system used rules for pre-filtering and the SVM for
classification, and it achieved a precision of 75% and
recall 76%.
2.2 Relationship Extraction
As for the extraction of interactions, most efforts in
extraction of biomedical interactions between enti-
ties from text have focused on using rule-based ap-
proaches due to the familiarity of medical terms that
tend to describe interactions. These approaches have
proven to be successful with notably good results. In
these approaches, most researchers attempted to de-
fine an accurate set of rules to describe relationship
types and patterns and to build ontologies and dic-
tionaries to be consulted in the extraction process.
These rules, ontologies, and dictionaries are typi-
cally domain specific and are often not generalizable
to other problems.
(Blaschke et al, 1999) reported a domain spe-
cific approach for extracting protein-protein interac-
tions from biomedical text based on a set of pre-
defined patterns and words describing interactions.
Later work attempted to automatically extract inter-
actions, which are referenced in the database of in-
teracting proteins (Xenarios et al, 2000), from the
text mentioning the interactions (Blaschke and Va-
lencia, 2001). They achieved surprisingly low recall
(25%), which they attributed to problems in properly
identifying protein names in the text.
(Koike et al, 2005) developed a system called
PRIME, which was used to extract biological func-
tions of genes, proteins, and their families. Their
system used a shallow parser and sentence struc-
ture analyzer. They extracted so-called ACTOR-
OBJECT relationships from the shallow parsed sen-
tences using rule based sentence structure analysis.
The identification of BNEs was done by consulting
the GENA gene name dictionary and family name
dictionary. In extracting the biological functions of
genes and proteins, their system reported a recall of
64% and a precision of 94%.
Saric et al developed a system to extract gene
expression regulatory information in yeast as well
as other regulatory mechanisms such phosphoryla-
tion (Saric et al, 2004; Saric et al, 2006). They
used a rule based named entity recognition module,
which recognizes named entities via cascading finite
state automata. They reported a precision of 83-90%
and 86-95% for the extraction of gene expression
and phosphorylation regulatory information respec-
tively.
(Leroy and Chen, 2005) used linguistic parsers
and Concept Spaces, which use a generic co-
occurrence based technique that extracts relevant
medical phrases using a noun chunker. Their system
employed UMLS (Humphreys and Lindberg, 1993),
GO (Ashburner et al, 2000), and GENA (Koike and
Takagi, 2004) to further improve extraction. Their
main purpose was entity identification and cross ref-
erence to other databases to obtain more knowledge
about entities involved in the system.
Other extraction approaches such as the one re-
ported on by (Cooper and Kershenbaum, 2005) uti-
lized a large manually curated dictionary of many
possible combinations of gene/protein names and
aliases from different databases and ontologies.
They annotated their corpus using a dictionary-
based longest matching technique. In addition, they
used filtering with a maximum entropy based named
entity recognizer in order to remove the false posi-
tives that were generated from merging databases.
The problem with this approach is the resulting in-
consistencies from merging databases, which could
hurt the effectiveness of the system. They reported
a recall of 87.1 % and a precision of 78.5% in the
relationship extraction task.
Work by (Mack et al, 2004) used the Munich In-
formation Center for Protein Sequences (MIPS) for
entity identification. Their system was integrated in
the IBM Unstructured Information Management Ar-
chitecture (UIMA) framework (Ferrucci and Lally,
2004) for tokenization, identification of entities, and
extraction of relations. Their approach was based on
a combination of computational linguistics, statis-
tics, and domain specific rules to detect protein in-
teractions. They reported a recall of 61% and a pre-
cision of 97%.
(Hao et al, 2005) developed an unsupervised ap-
proach, which also uses patterns that were deduced
using minimum description lengths. They used pat-
tern optimization techniques to enhance the patterns
by introducing most common keywords that tend to
describe interactions.
91
(Jo?rg et. al., 2005) developed Ali Baba which
uses sequence alignments applied to sentences an-
notated with interactions and part of speech tags.It
also uses finite state automata optimized with a ge-
netic algorithm in its approach. It then matches the
generated patterns against arbitrary text to extract in-
teractions and their respective partners. The system
scored an F1-measure of 51.8% on the LLL?05 eval-
uation set.
The aforementioned systems used either rule-
based approaches, which require manual interven-
tion from domain experts, or statistical approaches,
either supervised or semi-supervised, which also re-
quire manually curated training data.
3 BioNoculars
BioNoculars is a relationship extraction system that
based on a fully unsupervised technique suggested
by (Hassan et al, 2006) to automatically extract
protein-protein interaction from medical articles. It
can be retargeted to different domains such as pro-
tein interactions in diseases. The only requirement
is to compile domain specific taggers and dictionar-
ies, which would aid the system in performing the
required task.
The approach uses an unsupervised graph-based
mutual reinforcement, which depends on the con-
struction of generalized extraction patterns that
could match instances of relationships (Hassan et
al., 2006). Graph-based mutual reinforcement is
similar to the idea of hubs and authorities in web
pages depicted by the HITS algorithm (Kleinberg,
1998). The basic idea behind the algorithm is that
the importance of a page increases when more and
more good pages link to it. The duality between pat-
terns and extracted information (tuples) leads to the
fact that patterns could express different tuples, and
tuples in turn could be expressed by different pat-
terns. Tuple in this context contains three elements,
namely two proteins and the type of interaction be-
tween them. The proposed approach is composed of
two main steps, namely initial pattern construction
and then pattern induction.
For pattern construction, the text is POS tagged
and BNE tagged. The tags of Noun Phrases or se-
quences of nouns that constitute a BNE are removed
and replaced with a BNE tag. Then, an n-gram lan-
guage model is built on the tagged text (using tags
only) and is used to construct weighted finite state
machines. Paths with low cost (high language model
probabilities) are chosen to construct the initial set
of patterns; the intuition is that paths with low cost
(high probability) are frequent and could represent
potential candidate patterns. The number of candi-
date initial patterns could be reduced significantly
by specifying the candidate types of entities of in-
terest. In the case of BioNoculars, the focus was
on relationships between BNEs of type PROTEIN.
The candidate patterns are then applied to the tagged
stream to produce in-sentence relationship tuples.
As for pattern induction, due to the duality in the
patterns and tuples relation, patterns and tuples are
represented by a bipartite graph as illustrated in Fig-
ure 1.
Figure 1: A bipartite graph representing patterns and
tuples
Each pattern or tuple is represented by a node in
the graph. Edges represent matching between pat-
terns and tuples. The pattern induction problem can
be formulated as follows: Given a very large set of
data D containing a large set of patterns P, which
match a large set of tuples T, the problem is to iden-
tify , which is the set of patterns that match the set
of the most correct tuples T. The intuition is that
the tuples matched by many different patterns tend
to be correct and the patterns matching many differ-
ent tuples tend to be good patterns. In other words,
BioNoculars attempts to choose from the large space
of patterns in the data the most informative, high-
est confidence patterns that could identify correct tu-
ples; i.e. choosing the most authoritative patterns in
analogy with the hub-authority problem. The most
authoritative patterns can then be used for extracting
relations from free text. The following pattern-tuple
pairs show how patterns can match tuples in the cor-
pus:
(protein) (verb) (noun) (prep.) (protein)
92
Cla4 induces phosphorylation of Cdc24
(protein) (I-protein) (Verb) (prep.) (protein)
NS5A interacts with Cdk1
The proposed approach represents an unsuper-
vised technique for information extraction in general
and particularly for relations extraction that requires
no seed patterns or examples and achieves signifi-
cant performance. Given enough domain text, the
extracted patterns can support many types of sen-
tences with different styles (such passive and active
voice) and orderings (the interaction of X and Y vs.
X interacts with Y).
One of the critical prerequisites of the above-
mentioned approach is the use of a POS tagger,
which is tuned for biomedical text, and a BNE tag-
ger to properly identify BNEs. Both are critical for
determining the types of relationships that are of in-
terest. For POS tagging, a decision tree based tagger
developed by (Schmid, 1994) was used in combi-
nation with a model, which was trained on a cor-
rected/revised GENIA corpus provided by (Saric et
al., 2004) and was reported to achieve 96.4% tagging
accuracy (Saric et al, 2006). This POS tagger will
be referred to as the Schmid tagger. For BNE tag-
ging, ABNER was used. The accuracy of ABNER
is approximately state of the art with precision and
recall of 74.5% and 65.9% respectively with training
done using the BioCreative corpora (BioCreative).
Nonetheless we still face entity identification prob-
lems such as missed identifications in the text which
in turn affects our results considerably. We do be-
lieve if we use a better identification method , we
would yield better results.
4 Experimental Setup
Experiments aimed at extracting protein-protein
interactions for Bakers yeast (Sacharomyces
Cerevesiae) to assess BioNoculars (Cherry et al,
1998). The experiments were performed using
109,440 MEDLINE abstracts that contained the
varying names of the yeast, namely Sacharomyces
cerevisiae, S. Cerevisiae, Bakers yeast, Brewers
yeast and Budding yeast. MEDLINE abstracts
typically summarize the important aspects of papers
possibly including protein-protein interactions if
they are of relevance to the article. The goal was
to deduce the most appropriate extraction patterns
that can be later used to extract relations from any
document. All the MEDLINE abstracts were used
for pattern extraction except for 70 that were set
aside for testing. There were no test documents in
the training set. To build ground-truth, the test set
was semi-manually POS and BNE tagged. They
were also annotated with the interactions that are
contained in the text. There was a condition that
all the abstracts that are used for testing must have
entries in the Database of Interacting Proteins and
Protein-Protein Interactions (DIPPPI), which is
a subset of the Database of Interacting Proteins
(DIP) (Xenarios et al, 2000) restricted to proteins
from yeast. DIPPPI lists the known protein-protein
interactions in the MEDLINE abstracts. There were
297 protein-protein interactions in the test set of 70
abstracts. One of the disadvantages of DIPPPI is
that the presence of interactions is indicated without
mentioning their types or from which sentences
they were extracted. Although BioNoculars is able
to guess the sentence from which an interaction was
extracted and the type of interaction, this informa-
tion was ignored when evaluating against DIPPPI.
Unfortunately, there is no standard test set for the
proposed task, and most of the evaluation sets are
proprietary. The authors hope that others can benefit
from their test set, which is freely available.
The abstracts used for pattern extraction were
POS tagged using the Schmid tagger and BNE tag-
ging was done using ABNER. The patterns were re-
stricted to only those with protein names. For extrac-
tion of interaction tuples, the test set was POS and
BNE tagged using the Schmid tagger and ABNER
respectively. A varying number of final patterns
were then used to extract tuples from the test set and
the average recall and precision were computed. An-
other setup was used in which the relationships were
filtered using preset keywords for relationships such
as inhibits, interacts, and activates to properly com-
pare BioNoculars to systems in the literature that use
such keywords. The keywords were obtained from
the (Hakenberg et al, 2005) and (Temkin and Gilder,
2003). One of the generated pattern-tuple pairs was
as follows:
(PROTEIN) (Verb) (Conjunction) (PROTEIN)
NS5A interacts with Cdk1
One consequence of tuple extraction is generation
of redundant tuples, which contain the same enti-
93
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.51 0.70 0.76 0.81 0.84 0.89 0.89 0.93
Precision 0.47 0.42 0.43 0.35 0.30 0.26 0.26 0.16
FMeasure 0.49 0.53 0.55 0.49 0.44 0.40 0.40 0.27
Table 1: Recall, Precision, and F-measure for extrac-
tion of tuples using a varying number of top rated
patterns
ties and relations. Consequently, all protein aliases
and full text names were resolved to a unified nam-
ing scheme and the unified scheme was used to re-
place all variations of protein names in patterns. All
potential protein-protein interactions that BioNocu-
lars extracted were compared to those in the DIPPPI
databases.
5 Results and Discussion
For the first set of experiments, the experimental
setup described above was used without modifica-
tion. Table 1 and Figure 2 report on the resulting
recall and precision when taking different number
of highest rated patterns. The highest rated 217 pat-
terns were divided on a linear scale into 8 clusters
based on their relative weights.
Figure 2: Recall, Precision, and F-measure for tuple
extraction using a varying number of top patterns
As expected, Figure 2 clearly shows an inverse
relationship between precision and recall. This is
because using more extraction patterns yields more
tuples thus increasing recall at the expense of pre-
cision. The F-measure (with ? = 1) peeks at 78
patterns, which seems to provide the best score
given that precision and recall are equally important.
However, the technique seems to favor recall, reach-
ing a recall of 93% when using all 217 patterns. The
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.31 0.44 0.46 0.48 0.64 0.73 0.74 0.78
Precision 0.31 0.36 0.35 0.34 0.39 0.35 0.35 0.37
FMeasure 0.31 0.40 0.40 0.40 0.48 0.47 0.48 0.50
Table 2: Recall, Precision, and Recall for extraction
of tuples using a varying number of top rated patters
keyword filtering
low precision levels warrant thorough investigation.
In the second set of experiments, extracted tuples
were filtered using preset keywords indicating inter-
actions. Table 2 and Figure 3 show the results of the
experiments.
Figure 3: Recall, Precision, and F-measure for tu-
ple extraction using a varying number of top patterns
with keyword filtering
The results show that filtering with keywords led
to lower recall, but precision remained fairly steady
as the number of patterns changed. Nonetheless, the
best precision in Figure 3 is lower than the best pre-
cision in Figure 2 and the maximum F-measure for
this set of experiments is lower than the maximum
F-measure when no filtering was used. The BioNoc-
ulars system with no filtering can be advantageous
for recall oriented applications. The use of no filter-
ing suggests that some interaction may be expressed
in more generic forms or patterns. An intermediate
solution would be to increase the size of the list of
most commonly occurring keywords to filter the ex-
tracted tuples further.
Currently, ABNER, which is used by the system,
has a precision of 75.4% and a recall of 65.9%. Per-
haps improved tagging may improve the extraction
effectiveness.
The effectiveness of BioNoculars needs to be
94
thoroughly compared to existing systems via the use
of standard test sets, which are not readily available.
Most of previously reported work has been tested
on proprietary test sets or sets that are not publicly
available. The creation of standard publicly avail-
able test set can prompt research in this area.
6 Conclusion and Future Work
This paper presented a system for extracting
protein-protein interaction from biomedical text call
BioNoculars. BioNoculars uses a statistical un-
supervised learning algorithm, which is based on
graph mutual reinforcement and data redundancy
to extract extraction patterns. The system is re-
call oriented and is able to properly extract 93% of
the interaction mentions from test MEDLINE ab-
stracts. Nonetheless, the systems precision remains
low. Precision can be enhanced by using keywords
that describe interactions to filter to the resulting in-
teraction, but this would be at the expense of recall.
As for future work, more attention should be fo-
cused on improving extraction patterns. Currently,
the system focuses on extracting interactions be-
tween exactly two proteins. Some of the issues that
need to be handled include complex relationship (X
and Y interact with A and B), linguistic variabil-
ity (passive vs. active voice; presence of superflu-
ous words such as modifiers, adjectives, and prepo-
sitional phrases), protein lists (W interacts with X,
Y, and Z), nested interactions (W, which interacts
with X, also interacts with Y). Resolving these is-
sues would require an investigation of how patterns
can be generalized in automatic or semi-automatic
ways. Further, the identification of proteins in the
text requires greater attention. Also, the BioNocu-
lars approach can be combined with other rule-based
approaches to produce better results.
References
Ashburner, M., C. A. Ball, J. A. Blake, D. Botstein, H.
Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S.
Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-
Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E.
Richardson, M. Ringwald, G. M. Rubin, and G. Sher-
lock. 2000. Gene ontology: tool for the unification of
biology. Nature Genetics,volume 25 pp.25-29.
BioCreative. 2004. [Online].
Blaschke C., M. A. Andrade, C. Ouzounis, and A. Valen-
cia. 1999. Automatic Extraction of Biological Infor-
mation from Scientific Text: Protein-Protein Interac-
tions. ISMB99, pp. 60-67.
Blaschke, C. and A. Valencia. 2001. Can Bibliographic
Pointers for Known Biological Protein Interactions
as a Case Study. Comparative and Functional Ge-
nomics,vol. 2: 196-206.
Cherry, J. M., C. Adler, C. Ball, S. A. Chervitz, S. S.
Dwight, E. T. Hester, Y. Jia, G. Juvik, T. Roe, M.
Schroeder, S. Weng, and D. Botstein. 1998. SGD:
Saccharomyces Genome Database. Nucleic Acids Re-
search, 26, 73-9.
Chun, H. W., Y. Tsuruka, J. D. Kim, R. Shiba, N. Nagata,
T. Hishiki, and J. Tsujii. 2006. Extraction of Gene-
Disease Relations from MEDLINE Using Domain Dic-
tionaries and Machine Learning. Pacific Symposium
on Biocomputing 11:4-15.
Collier, N., C. Nobata, and J. Tsujii. 2000. Extracting
the Names of Genes and Gene Products with a Hidden
Markov Model. COLING, 2000, pp. 201207.
Cooper, J. and A. Kershenbaum. 2005. Discovery of
protein-protein interactions using a combination of
linguistic, statistical and graphical information. BMC
Bioinformatics.
DIPPPI http://www2.informatik.hu-berlin.de/ haken-
ber/corpora. 2006.
Ferrucci, D. and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information processing
in the corporate research environment. Natural Lan-
guage Engineering 10, No. 3-4, 327-348.
Fukuda, K., T. Tsunoda, A. Tamura, and T. Takagi. 1998.
Toward information extraction: identifying protein
names from biological papers. PSB, pages 705716.
Hakenberg, J., C. Plake, U. Leser, H. Kirsch, and D.
Rebholz-Schuhmann. 2005. LLL?05 Challenge:
Genic Interaction Extraction with Alignments and Fi-
nite State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany.
Hanisch, D., J. Fluck, HT. Mevissen, and R. Zimmer.
2003. Playing biologys name game: identifying pro-
tein names in scientific text. PSB, pages 403414.
Hao, Y., X. Zhu, M. Huang, and M. Li. 2005. Discov-
ering patterns to extract protein-protein interactions
from the literature: Part II. Bioinformatics, Vol. 00
no. 0 2005 pages 1-7.
95
Hassan, H., A. Hassan, and O. Emam. 2006. Un-
supervised Information Extraction Approach Using
Graph Mutual Reinforcement. Proceedings of Em-
pirical Methods for Natural Language Processing (
EMNLP ).
Humphreys B. L. and D. A. B. Lindberg. 1993. The
UMLS project: making the conceptual connection be-
tween users and the information they need. Bulletin of
the Medical Library Association, 1993; 81(2): 170.
Jo?rg Hakenberg, Conrad Plake, Ulf Leser. 2005. Genic
Interaction Extraction with Alignments and Finite
State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany (August 2005)
Kazama, J., T. Makino, Y. Ohta, and J. Tsujii. 2002. Tun-
ing Support Vector Machines for Biomedical Named
Entity Recognition. ACL Workshop on NLP in
Biomedical Domain, pages 18.
Kleinberg, J. 1998. Authoritative sources in a hy-
perlinked environment. In Proc. Ninth Ann. ACM-
SIAM Symp. Discrete Algorithms, pages 668-677,
ACM Press, New York.
Koike A. and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. BioLINK
2004: Linking Biological Literature, Ontologies, and
Database, pp. 9-16.
Koike, A., Y. Niwa, and T. Takagi 2005. Automatic
extraction of gene/protein biological functions from
biomedical text. Bioinformatics, Vol. 21, No. 7.
Leroy, G. and H. Chen. 2005. Genescene: An Ontology-
enhanced Integration of Linguistic and Co-Occurance
based Relations in Biomedical Text. JASIST Special
Issue on Bioinformatics.
Mack, R. L., S. Mukherjea, A. Soffer, N. Uramoto, E. W.
Brown, A. Coden, J. W. Cooper, A. Inokuchi, B. Iyer,
Y. Mass, H. Matsuzawa, L. V. Subramaniam. 2004.
Text analytics for life science using the Unstructured
Information Management Architecture. IBM Systems
Journal 43(3): 490-515.
Mika, S. and B. Rost. 2004. NLProt: extracting pro-
tein names and sequences from papers. Nucleic Acids
Research, 32 (Web Server issue): W634W637.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2004. Extracting regulatory gene expression
networks from PUBMED. Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, pp.191-198.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2006. Extraction of regulatory gene/protein
networks from Medline. Bioinformatics Vol.22 no
6,pp. 645-650.
Schmid, H. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In the International Conference
on New Methods in Language Processing, Manch-
ester, UK.
Settles, B. 2004. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. In Proceedings of the International Joint Work-
shop on Natural Language Processing in Biomedicine
and its Applications (NLPBA), Geneva, Switzerland,
pages 104-107.
Settles, B. 2005. ABNER: an open source tool for au-
tomatically tagging genes, proteins, and other entity
names in text. Bioinformatics, 21(14): 3191-3192.
Tanabe L., and W. J. Wilbur. 2002. Tagging gene
and protein names in biomedical text. Bioinformatics,
18(8):11241132.
Temkin, J. M. and M. R. Gilder. 2003. Extraction
of protein interaction information from unstructured
text using a context-free grammar. Bioinformatics
19(16):2046-2053.
Xenarios I, Rice DW, Salwinski L, Baron MK, Marcotte
EM, Eisenberg D. 2000. DIP: the Database of Inter-
acting Proteins. Nucleic Acids Res 28: 289291.
Yamamoto, K., T. Kudo, A. Konagaya, Y. Matsumoto.
2003. Protein Name Tagging for Biomedical Annota-
tion in Text. Proceedings of the ACL 2003 Workshop
on Natural Language Processing in Biomedicine, pp.
65-72.
96
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1245?1255,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
What?s with the Attitude? Identifying Sentences with Attitude in Online
Discussions
Ahmed Hassan Vahed Qazvinian
University of Michigan Ann Arbor
Ann Arbor, Michigan, USA
hassanam,vahed,radev@umich.edu
Dragomir Radev
Abstract
Mining sentiment from user generated content
is a very important task in Natural Language
Processing. An example of such content is
threaded discussions which act as a very im-
portant tool for communication and collabo-
ration in the Web. Threaded discussions in-
clude e-mails, e-mail lists, bulletin boards,
newsgroups, and Internet forums. Most of the
work on sentiment analysis has been centered
around finding the sentiment toward products
or topics. In this work, we present a method
to identify the attitude of participants in an
online discussion toward one another. This
would enable us to build a signed network
representation of participant interaction where
every edge has a sign that indicates whether
the interaction is positive or negative. This
is different from most of the research on so-
cial networks that has focused almost exclu-
sively on positive links. The method is exper-
imentally tested using a manually labeled set
of discussion posts. The results show that the
proposed method is capable of identifying at-
titudinal sentences, and their signs, with high
accuracy and that it outperforms several other
baselines.
1 Introduction
Mining sentiment from text has a wide range of
applications from mining product reviews on the
Web (Morinaga et al, 2002; Turney and Littman,
2003) to analyzing political speeches (Thomas et al,
2006). Automatic methods for sentiment mining are
very important because manual extraction of them is
very costly, and inefficient. A new application of
sentiment mining is to automatically identify atti-
tudes between participants in an online discussion.
An automatic tool to identify attitudes will enable
us to build a signed network representation of par-
ticipant interaction in which the interaction between
two participants is represented using a positive or
a negative edge. Even though using signed edges
in social network studies is clearly important, most
of the social networks research has focused only on
positive links between entities. Some work has re-
cently investigated signed networks (Leskovec et al,
2010; Kunegis et al, 2009), however this work was
limited to a few number of datasets in which users
were allowed to explicitly add negative, as well as
positive, relations. This work will pave the way for
research efforts to examine signed social networks
in more detail. It will also allow us to study the re-
lation between explicit relations and the text under-
lying those relation.
Although similar, identifying sentences that dis-
play an attitude in discussions is different from iden-
tifying opinionated sentences. A sentence in a dis-
cussion may bear opinions about a definite target
(e.g., price of a camera) and yet have no attitude to-
ward the other participants in the discussion. For in-
stance, in the following discussion Alice?s sentence
has her opinion against something, yet no attitude
toward the recipient of the sentence, Bob.
Alice: ?You know what, he turned out to
be a great disappointment?
Bob: ?You are completely unqualified to
judge this great person?
However, Bob shows strong attitude toward Alice.
In this work, we look at ways to predict whether a
sentence displays an attitude toward the text recip-
ient. An attitude is the mental position of one par-
ticipant with regard to another participant. it could
be either positive or negative. We consider features
which takes into account the entire structure of sen-
tences at different levels or generalization. Those
1245
features include lexical items, part-of-speech tags,
and dependency relations. We use all those patterns
to build several pairs of models that represent sen-
tences with and without attitude.
The rest of the paper is organized as follows. In
Section 2 we review some of the related prior work
on identifying polarized words and subjectivity anal-
ysis. We explain the problem definition and discuss
our approach in Sections 3 & 4. Finally, in Sec-
tions 5 & 6 we introduce our dataset and discuss the
experimental setup. Finally, we conclude in Section
7.
2 Related Work
Identifying the polarity of individual words is a well
studied problem. In previous work, Hatzivassiloglou
and McKeown (1997) propose a method to iden-
tify the polarity of adjectives. They use a manu-
ally labeled corpus to classify each conjunction of
an adjective as ?the same orientation? as the adjec-
tive or ?different orientation?. Their method can
label simple in ?simple and well-received? as the
same orientation and simplistic in ?simplistic but
well-received? as the opposite orientation of well-
received. Although the results look promising, the
method would only be applicable to adjectives since
noun conjunctions may collocate regardless of their
semantic orientations (e.g., ?rise and fall?).
In other work, Turney and Littman (2003) use sta-
tistical measures to find the association between a
given word and a set of positive/negative seed words.
In order to get word co-occurrence statistics they use
the ?near? operator from a commercial search en-
gine on a given word and a seed word.
In more recent work, Takamura et al (2005) used
the spin model to extract word semantic orientation.
First, they construct a network of words using def-
initions, thesaurus, and co-occurrence statistics. In
this network, each word is regarded as an electron,
which has a spin and each spin has a direction tak-
ing one of two values: up or down. Then, they use
the energy point of view to propose that neighboring
electrons tend to have the same spin direction, and
therefore neighboring words tend to have the same
polarity orientations. Finally, they use the mean field
method to find the optimal solution for electron spin
directions.
Previous work has also used WordNet, a lexi-
cal database of English, to identify word polarity.
Specifically, Hu and Liu (2004) use WordNet syn-
onyms and antonyms to predict the polarity of any
given word with unknown polarity. They label each
word with the polarity of its synonyms and the op-
posite polarity of its antonyms. They continue in
a bootstrapping manner to label all unlabeled in-
stances. This work is very similar to (Kamps et al,
2004) in which a network of WordNet synonyms
is used to find the shortest path between any given
word, and the words ?good? and ?bad?. Kim and
Hovy (Kim and Hovy, 2004) used WordNet syn-
onyms and antonyms to expand two lists of positive
and negative seed words. Similarly, Andreevskaia
and Bergler (2006) used WordNet to expand seed
lists with fuzzy sentiment categories, in which words
could be more central to one category than the other.
Finally, Kanayama and Nasukawa (2006) used syn-
tactic features and context coherency, defined as the
tendency for same polarities to appear successively,
to acquire polar atoms.
All the work mentioned above focus on the task
of identifying the polarity of individual words. Our
proposed work is identifying attitudes in sentences
that appear in online discussions. Perhaps the most
similar work to ours is the prior work on subjectivity
analysis, which is to identify text that present opin-
ions as opposed to objective text that present fac-
tual information (Wiebe, 2000). Prior work on sub-
jectivity analysis mainly consists of two main cate-
gories: The first category is concerned with identify-
ing the subjectivity of individual phrases and words
regardless of the sentence and context they appear
in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;
Banea et al, 2008). In the second category, sub-
jectivity of a phrase or word is analyzed within its
context (Riloff and Wiebe, 2003; Yu and Hatzivas-
siloglou, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, ). A good study of the applications
of subjectivity analysis from review mining to email
classification is given in (Wiebe, 2000). Somasun-
daran et al (2007) develop genre-speci.c lexicons
using interesting function word combinations for de-
tecting opinions in meetings. Despite similarities,
our work is different from subjectivity analysis be-
cause the later only discriminates between opinions
and facts. A discussion sentence may display an
1246
opinion about some topic yet no attitude. The lan-
guage constituents considered in opinion detection
may be different from those used to detect attitude.
Moreover, extracting attitudes from online discus-
sions is different from targeting subjective expres-
sions (Josef Ruppenhofer and Wiebe, 2008; Kim
and Hovy, 2004). The later usually has a limited
set of targets that compete for the subjective expres-
sions (for example in movie review, targets could be:
director, actors, plot, and so forth). We cannot use
similar methods because we are working on an open
domain where anything could be a target. A very de-
tailed survey that covers techniques and approaches
in sentiment analysis and opinion mining could be
found in (Pang and Lee, 2008).
There is also some related work on mining on-
line discussions. Lin et al(2009) proposes a sparse
coding-based model simultaneously model seman-
tics and structure of threaded discussions. Shen
et al(2006) proposes three clustering methods for
exploiting the temporal information in the streams,
as well as an algorithm based on linguistic fea-
tures to analyze the discourse structure information.
Huang et al(2007) used an SVM classifier to extract
(thread-title, reply) pairs as chat knowledge from on-
line discussion forums to support the construction
of a chatbot for a certain domain. Other work has
focused on the structure of questions and question-
answer pairs in online forums and discussions (Ding
et al, 2008; Cong et al, 2008).
3 Problem Definition
Assume we have a set of sentences exchanged be-
tween participants in an online discussion. Our ob-
jective is to identify sentences that display an atti-
tude from the text writer to the text recepient from
those that do not. An attitude is the mental posi-
tion of one particpant with regard to another partic-
ipant. An attitude may not be directly observable,
but rather inferred from what particpants say to one
another. The attitude could be either positive or neg-
ative. Strategies for showing a positive attitude may
include agreement, and praise, while strategies for
showing a negative attitude may include disagree-
ment, insults, and negative slang. After identifying
sentences that display an attitude, we also predict the
sign (positive or negative) of that attitude.
4 Approach
In this section, we describe a model which, given a
sentence, predicts whether it carries an attitude from
the text writer toward the text recipient or not. Any
given piece of text exchanged between two partici-
pants in a discussion could carry an attitude toward
the text recipient, an attitude towards the topic, or
no attitude at all. As we are only interested in at-
titudes between participants, we limit our study to
sentences that use second person pronouns. Second
person pronouns are usually used in conversational
genre to indicate that the text writer is addressing the
text recipient. After identifying those sentences, we
do some pre-processing to extract the most relevant
fragments. We examine these fragments to to iden-
tify the polarity of every word in the sentence. Every
word could be assigned a semantic orientation. The
semantic orientation could be either positive, nega-
tive, or neutral. The existence of polarized words in
any sentence is an important indicator of whether it
carries an attitude or not.
The next step is to extract several patterns at
different levels of generalization representing any
given sentence. We use those patterns to build two
Markov models for every kind of patterns. The first
model characterizes the relation between different
tokens for all patterns that correspond to sentences
that have an attitude. The second model is similar to
the first one, but it uses all patterns that correspond
to sentences that do not have an attitude. Given a
new sentence, we extract the corresponding patterns
and estimate the likelihood of every pattern being
generated from the two corresponding models. We
then compare the likelihood of the sentence under
the two models and use this as a feature to predict
the existence of an attitude. A pair of models will
be built for every kind of patterns. If we have n dif-
ferent patterns, we will have n different likelihood
ratios that come from n pairs of models.
4.1 Word Polarity Identification
Identifying the polarity of words is an important step
for our method. Our word identification module is
similar to the work in (Annon, 2010). We construct
a graph where each node represent a word/part-of-
speech pair. Two nodes are linked if the words are
related. We use WordNet (Miller, 1995) to link re-
1247
lated words based on synonyms, hypernyms, and
similar to relations. For words that do not appear
in Wordnet, we used Wiktionary, a collaboratively
constructed dictionary. We also add some links
based on co-occurrence statistics between words as
from a large corpus. The resulting graph is a graph
G(W,E) where W is a set of word/part-of-speech
pairs, and E is the set of edges connecting related
words.
We define a random walk model on the graph,
where the set of nodes correspond to the state space
of the random walk. Transition probabilities are cal-
culated by normalizing the weights of the edges out
of every node. Let S+ and S? be two sets of ver-
tices representing seed words that are already la-
beled as either positive or negative respectively. We
used the list of labeled seeds from (Hatzivassiloglou
and McKeown, 1997) and (Stone et al, 1966). For
any given word w, we calculate the mean hitting
time betweenw, and the two seed sets h(w|S+), and
h(w|S?). The mean hitting time h(i|k) is defined as
the average number of steps a random walker, start-
ing in state i 6= k, will take to enter state k for the
first time (Norris, 1997). If h(w|S+) is greater than
h(w|S?), the word is classified as negative, oth-
erwise it is classified as positive. We also use the
method described in (Wilson et al, 2005) to deter-
mine the contextual polarity of the identified words.
The set of features used to predict contextual polar-
ity include word, sentence, polarity, structure, and
other features.
4.2 Identifying Relevant Parts of Sentences
The writing style in online discussion forums is very
informal. Some of the sentence are very long, and
punctuation marks are not always properly used. To
solve this problem, we decided to use the grammat-
ical structure of sentences to identify the most rele-
vant part of sentences that would be the subject of
further analysis. Figure 1 shows a parse tree repre-
senting the grammatical structure of a particular sen-
tence. If we closely examine the sentence, we will
notice that we are only interested in a part of the
sentence that includes the second person pronoun
?you?. We extract this part, by starting at the word
of interest , in this case ?you?, and go up in the hi-
erarchy till we hit the first sentence clause. Once,
we reach a sentence clause, we extract the corre-
sponding text if it is grammatical, otherwise we go
up one more level to the closest sentence clause. We
used the Stanford parser to generate the grammatical
structure of sentences (Klein and Manning, 2003).
Figure 1: An example showing how to identify the rele-
vant part of a sentence.
4.3 Sentences as Patterns
The fragments we extracted earlier are more rele-
vant to our task and are more suitable for further
analysis. However, these fragments are completely
lexicalized and consequently the performance of any
analysis based on them will be limited by data spar-
sity. We can alleviate this by using more general
representations of words. Those general representa-
tions can be used a long with words to generate a set
of patterns that represent each fragment. Each pat-
tern consists of a sequence of tokens. Examples of
such patterns could use lexical items, part-of-speech
(POS) tags, word polarity tags, and dependency re-
lations.
We use three different patterns to represent each
fragments:
? Lexical patterns: All polarized words are re-
places with the corresponding polarity tag, and
all other words are left as is.
? Part-of-speech patterns: All words are replaced
with their POS tags. Second person pronouns
are left as is. Polarized words are replaced with
their polarity tags and their POS tags.
? Dependency grammar patterns: the shortest
path connecting every second person pronoun
1248
to the closed polarized word is extracted. The
second person pronoun, the polarized word tag,
and the types of the dependency relations along
the path connecting them are used as a pat-
tern. It has been shown in previous work on
relation extraction that the shortest path be-
tween any two entities captures the the in-
formation required to assert a relationship be-
tween them (Bunescu and Mooney, 2005). Ev-
ery polarized word is assigned to the closest
second person pronoun in the dependency tree.
This is only useful for sentences that have po-
larized words.
Table 1 shows the different kinds of representa-
tions for a particular sentence. We use text, part-
of-speech tags, polarity tags, and dependency rela-
tions. The corresponding patterns for this sentence
are shown in Table 2.
4.4 Building the Models
Given a set of patterns representing a set of sen-
tences, we can build a graph G = V,E,w where
V is the set of all possible token that may appear
in the patterns. E = V ? V is the set of possible
transitions between any two tokens. w : E ? [0..1]
is a weighting function that assigns to every pair of
states (i, j) a weight w(i, j) representing the proba-
bility that we have a transition from state i to state
j.
This graph corresponds to a Markovian model.
The set of states are the vocabulary, and the the tran-
sition probabilities between states are estimated us-
ing Maximum Likelihood estimation as follows:
Pij =
Nij
Ni
whereNij is the number of times we saw a transition
from i to state j, and Ni is the total number of times
we saw state i in the training data. This is similar to
building a language model over the language of the
patterns.
We build two such models for every kind of pat-
terns. The first model is built using all sentences that
appeared in the training dataset and was labeled as
having an attitude, and the second model is built us-
ing all sentences in the training dataset that do not
have an attitude. If we have n kinds of patterns, we
will build one such pair for every kind of patterns.
Hence, we will end up with 2n models.
4.5 Identifying Sentences with Attitude
We split our training data into two splits; the first
containing all sentences that have an attitude and the
second containing all sentences that do not have an
attitude. Given the methodology described in the
previous section, we build n pairs of Markov mod-
els. Given any sentence, we extract the correspond-
ing patterns and estimate the log likelihood that this
sequence of tokens was generated from every model.
Given a model M , and sequence of tokens T =
(T1, T2, . . . TSn), the probability of this token se-
quence being generated from M is:
PM (T ) =
n?
i=2
P (Ti|T1, . . . , Ti?1) =
n?
i=2
W (Ti?1, Ti)
where n is the number of tokens in the pattern, and
W is the probability transition function.
The log likelihood is then defined as:
LLM (T ) =
n?
i=2
logW (Ti?1, Ti)
For every pair of models, we may use the ratio be-
tween the two likelihoods as a feature:
f =
LLMatt(T )
LLMnoatt(T )
where T is the token sequence, LLMatt(T ) is the log
likelihood of the sequence given the attitude model,
and LLMnoatt(T ) is the log likelihood of the pattern
given the no-attitude model.
Given the n kinds of patterns, we can calculate
three different features. A standard machine learn-
ing classifier is then trained using those features to
predict whether a given sentence has an attitude or
not.
4.6 Identifying the Sign of an Attitude
To determine the orientation of an attitude sentence,
we tried two different methods. The first method as-
sumes that the orientation of an attitude sentence is
directly related to the polarity of the words it con-
tains. If the sentence has only positive and neutral
1249
Table 1: Tags used for building patterns
Text That makes your claims so ignorant
POS That/DT makes/VBZ your/PRP$ claims/NNS so/RB ignorant/JJ
Polarity That/O makes/O your/O claims/O so/O ignorant/NEG
Dependency your
poss
? claims
nsubj
? ignorant
Table 2: Sample patterns
Lexical pattern That makes your claims so NEG
POS pattern DT VBZ your PRP$ NNS RB NEG JJ
Dependency pattern your poss nsubj NEG
words, it is classified as positive. If the sentence
has only negative and neutral words, it is classified
as negative. If the sentence has both positive and
negative words, we calculate the summation of the
polarity scores of all positive words and that of all
negative words. The polarity score of a word is an
indicator of how strong of a polarized word it is. If
the former is greater, we classify the sentence as pos-
itive,otherwise we classify the sentence as negative.
The problem with this method is that it assumes
that all polarized words in a sentence with an atti-
tude target the text recipient. Unfortunately, that is
not always correct. For example, the sentence ?You
are completely unqualified to judge this great per-
son? has a positive word ?great? and a negative word
?unqualified?. The first method will not be able to
predict whether the sentence is positive or negative.
To solve this problem, we use another method that
is based on the paths that connect polarized words to
second person pronouns in a dependency parse tree.
For every positive word w , we identify the shortest
path connecting it to every second person pronoun
in the sentence then we compute the average length
of the shortest path connecting every positive word
to the closest second person pronoun. We repeat for
negative words and compare the two values. The
sentence is classified as positive if the average length
of the shortest path connecting positive words to the
closest second person pronoun is smaller than the
corresponding value for negative words. Otherwise,
we classify the sentence as negative.
5 Data
Our data was randomly collected from a set of dis-
cussion groups. We collected a large number of
threads from the first quarter of 2009 from a set of
Usenet discussion groups. All threads were in En-
glish, and had 5 posts or more. We parsed the down-
loaded threads to identify the posts and senders. We
kept posts that have quoted text and discarded all
other posts. The reason behind that is that partici-
pants usually quote other participants text when they
reply to them. This restriction allows us to iden-
tify the target of every post, and raises the proba-
bility that the post will display an attitude from its
writer to its target. We plan to use more sophsticated
methods for reconstructing the reply structure like
the one in (Lin et al, 2009). From those posts, we
randomly selected approximately 10,000 sentences
that use second person pronouns. We explained ear-
lier how second person pronouns are used in discus-
sions genres to indicate the writer is targeting the
text recipient. Given a random sentence selected
from some random discussion thread, the probabil-
ity that the sentence does not have an attitude is sig-
nificantly larger than the probability that it will have
an attitude. Hence, restricting our dataset to posts
with quoted text and sentences with second person
pronouns is very important to make sure that we
will have a considerable amount of attitudinal sen-
tences. The data was tokenized, sentence-split, part-
of-speech tagged with the OpenNLP toolkit. It was
parsed with the Stanford dependency parser (Klein
and Manning, 2003).
5.1 Annotation Scheme
The goals of the annotation scheme are to distin-
guish sentences that display an attitude from those
that do not. Sentences could display either a neg-
ative or a positive attitude. Disagreement, insults,
and negative slang are indicators of negative attitude.
1250
A B C D
A - 82.7 80.6 82.1
B 81.0 - 81.9 82.9
C 77.8 78.2 - 83.8
D 78.3 77.7 78.6 -
Table 3: Inter-annotator agreement
Agreement, and praise are indicators of positive at-
titude. Our annotators were instructed to read every
sentence and assign two labels to it. The first speci-
fies whether the sentence displays an attitude or not.
The existence of an attitude was judged on a three
point scale: attitude, unsure, and no-attitude. The
second is the sign of the attitude. If an attitude ex-
ists, annotators were asked to specify whether the
attitude is positive or negative. To evaluate inter-
annotator agreement, we use the agr operator pre-
sented in (Wiebe et al, 2005). This metric measures
the precision and recall of one annotator using the
annotations of another annotator as a gold standard.
The process is repeated for all pairs of annotators,
and then the harmonic mean of all values is reported.
Formally:
agr(A|B) =
|A ?B|
|A|
(1)
where A, and B are the annotation sets produced by
the two reviewers. Table 3 shows the value of the
agr operator for all pairs of annotators. The har-
monic mean of the agr operator is 80%. The agr
operator was used over the Kappa Statistic because
the distribution of the data was fairly skewed.
6 Experiments
6.1 Experimental Setup
We performed experiments on the data described in
the previous section. The number of sentences with
an attitude was around 20% of the entire dataset.
The class imbalance caused by the small number of
attitude sentences may hurt the performance of the
learning algorithm (Provost, 2000). A common way
of addressing this problem is to artificially rebal-
ance the training data. To do this we down-sample
the majority class by randomly selecting, without
replacement, a number of sentences without an at-
titude that equals the number of sentences with an
attitude. That resulted in a balanced subset, approx-
imately 4000 sentences, that we used in our experi-
ments.
We used Support Vector Machines (SVM) as a
classifier. We optimized SVM separately for every
experiment. We used 10-fold cross validation for all
tests. We evaluate our results in terms of precision,
recall, accuracy, and F1. Statistical significance was
tested using a 2-tailed paired t-test. All reported re-
sults are statistically significant at the 0.05 level. We
compare the proposed method to several other base-
lines that will be described in the next subsection.
We also perform experiments to measure the perfor-
mance if we mix features from the baselines and the
proposed method.
6.2 Baselines
The first baseline is based on the hypothesis that the
existence of polarized words is a strong indicator
that the sentence has an attitude. As a result, we
use the number of polarized word in the sentence,
the percentage of polarized words to all other words,
and whether the sentences has polarized words with
mixed or same sign as features to train an SVM clas-
sifier to detect attitude.
The second baseline is based on the proximity be-
tween the polarized words and the second person
pronouns. We assume that every polarized word is
associated with the closest second person pronoun.
Let w be a polarized word and p(w) be the closes
second person pronoun, and surf dist(w, p(w)) be
the surface distance betweenw and p(w). This base-
line uses the minimum, maximum, and average of
surf dist(w, p(w)) for all polarized words as fea-
tures to train an SVM classifier to identify sentences
with attitude.
The next baseline uses the dependency tree dis-
tance instead of the surface distance. We assume that
every polarized word is associated to the second per-
son pronoun that is connected to it using the smallest
shortest path. The dep dist(w, p(w)) is calculated
similar to the previous baselines but using the de-
pendency tree distance. The minimum, maximum,
and average of this distance for all polarized words
are used as features to train an SVM classifier.
1251
Figure 2: Accuracy, Precision, and Recall for the Pro-
posed Approach and the Baselines.
0 10 20 30 40 50 60 70 80 90 1000
10
20
30
40
50
60
70
80
90
100
Recall
Prec
ision
 
 MMSurfDistDepDistPol
Figure 3: Precision Recall Graph.
6.3 Results and Discussion
Figure 2 compares the accuracy, precision, and re-
call of the proposed method (ML), the polarity based
classifier (POL), the surface distance based classi-
fier (Surf Dist), and the dependency distance based
classifier (Dep Dist). The values are selected to opti-
mize F1. The figure shows that the surface distance
based classifier behaves poorly with low accuracy,
precision, and recall. The two other baselines be-
have poorly as well in terms of precision and accu-
racy, but they do very well in terms of recall. We
looked at some of the examples to understand why
those two baselines achieve very high recall. It turns
out that they tend to predict most sentences that have
polarized words as sentences with attitude. This re-
sults in many false positives and low true negative
rate. Achieving high recall at the expense of losing
precision is trivial. On the other hand, we notice that
0 10 20 30 40 50 60 70 80 90 10075
76
77
78
79
80
81
82
Training Set Size (%)
Accu
racy
Figure 4: Accuracy Learning Curve for the Proposed
Method.
the proposed method results in very close values of
precision and recall at the optimum F1 point.
To better compare the performance of the pro-
posed method and the baseline, we study the the
precision-recall curves for all methods in Figure 3.
We notice that the proposed method outperforms all
baselines at all operating points. We also notice that
the proposed method provides a nice trade-off be-
tween precision and recall. This allows us some flex-
ibility in choosing the operating point. For example,
in some applications we might be interested in very
high precision even if we lose recall, while in other
applications we might sacrifice precision in order to
get high recall. On the other hand, we notice that
the baselines always have low precision regardless
of recall.
Table 4 shows the accuracy, precision, recall, and
F1 for the proposed method and all baselines. It also
shows the performance when we add features from
the baselines to the proposed method, or merge some
of the baselines. We see that we did not get any im-
provement when we added the baseline features to
the proposed method. We believe that the proposed
method captures all the information captured by the
baselines and more.
Our proposed method uses three different features
that correspond to the three types of patterns we use
to represent every sentence. To understand the con-
tributions of every feature, we measure the perfor-
mance of every feature by itself and also all possible
combinations of pairs of features. We compare that
1252
to the performance we get when using all features in
Table 5. We see that the part-of-speech patterns per-
forms better than the text patterns. This makes sense
because the former suffers from data sparsity. De-
pendency patterns performs best in terms of recall,
while part-of-speech patterns outperform all others
in terms of precision, and accuracy. All pairs of
features outperform any single feature that belong
to the corresponding pair in terms of F1. We also
notice that using the three features results in better
performance when compared to all other combina-
tions. This shows that every kind of pattern captures
slightly different information when compared to the
others. It also shows that merging the three features
improves performance.
One important question is how much data is re-
quired to the proposed model. We constructed a
learning curve, shown in Figure 4, by fixing the
test set size at one tenth of the data, and varying
the training set size. We carried out ten-fold cross
validation as with our previous experiments. We see
that adding more data continues to increase the accu-
racy, and that accuracy is quite sensitive to the train-
ing data. This suggests that adding more data to this
model could lead to even better results.
We also measured the accuracy of the two meth-
ods we proposed for predicting the sign of attitudes.
The accuracy of the first model that only uses the
count and scores of polarized words was 95%. The
accuracy of the second method that used depen-
dency distance was 97%.
6.4 Error Analysis
We had a closer look at the results to find out what
are the reasons behind incorrect predictions. We
found two main reasons. First, errors in predicting
word polarity usually propagates and results in er-
rors in attitude prediction. The reasons behind incor-
rect word polarity predictions is ambiguity in word
senses and infrequent words that have very few con-
nection in thesaurus. A possible solution to this type
of errors is to improve the word polarity identifica-
tion module by including word sense disambigua-
tion and adding more links to the words graph using
glosses or co-occurrence statistics. The second rea-
son is that some sentences are sarcastic in nature. It
is so difficult to identify such sentences. Identify-
ing sarcasm should be addressed as a separate prob-
Method Accuracy Precision Recall F1
ML 80.3 81.0 79.4 80.2
POL 73.1 66.4 93.9 77.7
ML+POL 79.9 77.9 83.4 80.5
SurfDist 70.2 67.1 79.2 72.7
DepDist 73.1 66.4 93.8 77.8
SurfDist+ 73.1 66.4 93.8 77.7
DepDist
ML+SurfDist 73.9 67.2 93.6 78.2
ML+DepDist 72.8 66.1 93.8 77.6
ML+SurfDist+ 74.0 67.2 93.4 78.2
DepDist
SurfDist+ 73.1 66.3 93.8 77.7
DepDist+POL
ML+SurfDist+ 73.0 66.2 93.8 77.6
DepDist+POL
Table 4: Precision, Recall, F1, and Accuracy for the pro-
posed method, the baselines, and different combinations
of proposed method and the baselines features
Method Accuracy Precision Recall F1
txt 75.5 74.1 78.6 76.2
pos 77.7 78.2 76.9 77.5
dep 74.7 70.4 85.1 77.0
txt+pos 77.8 77.0 79.4 78.1
txt+dep 79.4 79.6 79.2 79.4
pos+dep 80.4 79.1 82.5 80.7
txt+pos+dep 80.3 81.0 79.4 80.2
Table 5: Precision, Recall, F1, and Accuracy for different
combinations of the proposed method?s features.
lem. A method that utilizes holistic approaches that
takes context and previous interactions between dis-
cussion participants into consideration could be used
to address it.
7 Conclusions
We have shown that training a supervised Markov
model of text, part-of-speech, and dependecy pat-
terns allows us to identify sentences with attitudes
from sentences without attitude. This model is more
accurate than several other baselines that use fea-
tures based on the existence of polarized word, and
proximity between polarized words and second per-
son pronouns both in text and dependecy trees. This
method allows to extract signed social networks
from multi-party online discussions. This opens the
door to research efforts that go beyond standard so-
cial network analysis that is based on positve links
1253
only. It also allows us to study dynamics behind in-
teractions in online discussions, the relation between
text and social interactions, and how groups form
and break in online discussions.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL?06.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT ?05, pages 724?731, Morristown, NJ,
USA. Association for Computational Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ?08, pages 467?
474.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
ACL?08, pages 710?718.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD?04, pages 168?177.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion fo-
rums. In IJCAI?07, pages 423?428.
Swapna Somasundaran Josef Ruppenhofer and Janyce
Wiebe. 2008. Finding the sources and targets
of subjective expressions. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08).
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP?06, pages 355?363.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03, pages 423?430.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a so-
cial network with negative edges. In WWW?09, pages
741?750, New York, NY, USA.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010. Predicting positive and negative links in online
social networks. In WWW ?10, pages 641?650, New
York, NY, USA. ACM.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the web. In KDD?02, pages 341?349.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP?05.
Foster Provost. 2000. Machine learning from imbal-
anced data sets 101. In Proceedings of the AAAI Work-
shop on Imbalanced Data Sets.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
1254
meetings. In Proceedings of the SIGdial Workshop on
Discourse and Dialogue.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In EMNLP
2006, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
1(2):0.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP?05, Vancouver,
Canada.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
1255
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 59?70, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Detecting Subgroups in Online Discussions by Modeling Positive and
Negative Relations among Participants
Ahmed Hassan
Microsoft Research
Redmond, WA
hassanam@microsoft.com
Amjad Abu-Jbara
University of Michigan
Ann Arbor, MI
amjbara@umich.edu
Dragomir Radev
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
A mixture of positive (friendly) and nega-
tive (antagonistic) relations exist among users
in most social media applications. However,
many such applications do not allow users to
explicitly express the polarity of their interac-
tions. As a result most research has either ig-
nored negative links or was limited to the few
domains where such relations are explicitly
expressed (e.g. Epinions trust/distrust). We
study text exchanged between users in online
communities. We find that the polarity of the
links between users can be predicted with high
accuracy given the text they exchange. This
allows us to build a signed network represen-
tation of discussions; where every edge has
a sign: positive to denote a friendly relation,
or negative to denote an antagonistic relation.
We also connect our analysis to social psy-
chology theories of balance. We show that the
automatically predicted networks are consis-
tent with those theories. Inspired by that, we
present a technique for identifying subgroups
in discussions by partitioning singed networks
representing them.
1 Introduction
Most online communities involve a mixture of pos-
itive and negative relations between users. Positive
relations may indicate friendship, agreement, or ap-
proval. Negative relations usually indicate antago-
nism, opposition, or disagreement.
Most of the research on relations in social media
applications has almost exclusively focused on pos-
itive links between individuals (e.g. friends, fans,
followers, etc.). We think that one of the main rea-
sons, of why the interplay of positive and negative
links did not receive enough attention, is the lack of
a notion for explicitly expressing negative interac-
tions. Recently, this problem has received increas-
ing attention. However, all studies have been limited
to a handful of datasets from applications that allow
users to explicitly label relations as either positive or
negative (e.g. trust/distrust on Epinion (Leskovec et
al., 2010b) and friends/foes on Slashdot (Kunegis et
al., 2009)).
Predicting positive/negative relations between
discussants is related to another well studied prob-
lem, namely debate stance recognition. The ob-
jective of this problem is to identify which partic-
ipants are supporting and which are opposing the
topic being discussed. This line of work does not
pay enough attention to the relations between par-
ticipants, rather it focuses on participant?s stance to-
ward the topic. It also assumes that every partici-
pant either supports or opposes the topic being dis-
cussed. This is a simplistic view that ignore the
nature of complex topics that has many aspects in-
volved which may result in more than two subgroups
with different opinions.
In this work, we apply Natural Language Pro-
cessing techniques to text correspondences ex-
changed between individuals to identify the under-
lying signed social structure in online communities.
We present a method for identifying user attitude
and for automatically constructing a signed social
network representation of discussions. We apply
the proposed methods to a large set of discussion
posts. We evaluate the performance using a manu-
ally labeled dataset. We also conduct a large scale
evaluation by showing that predicted links are con-
sistent with the principals of social psychology the-
ories, namely the Structural Balance Theory (Hei-
der, 1946). The balance theory has been shown to
hold both theoretically (Heider, 1946) and empiri-
cally (Leskovec et al2010c) for a variety of social
community settings. Finally, we present a method
for identifying subgroups in online discussions by
identifying groups with high density of intra-group
positive relations and high density of inter-group
negative relations. This method is capable of identi-
fying subgroups even if the community splits into
more than two subgroups which is more general
than stance recognition which assumes that only two
groups exist.
59
B 
C 
D 
A E 
F G 
I H 
Positive Negative 
Source Target Sign Evidence from Text A E - I have to disagree with what you are saying.  G A - You are missing the entire point, he is putting lives at risk. D I - and you manufacture lies for what reason? E G + you have explained your position very well. C H + I am neutral on this, but I agree with your assessment! 
Figure 1: An example showing a signed social network
along with evidence from text that justifies edge signs.
The input to our algorithm is a set of text corre-
spondences exchanged between users (e.g. posts or
comments). The output is a signed network where
edges signify the existence of an interaction between
two users. The resulting network has polarity asso-
ciated with every edge. Edge polarity is a means for
indicating positive or negative affinity between two
individuals.
Figure 1 shows a signed network representation
for a subset of posts from a long discussion thread.
The thread discussed the November 2010 Wikileaks
cable release. We notice that participants split into
two groups, one supporting and one opposing the
leak. We also notice that most negative edges are
between groups, and most positive edges are within
groups. It is worth mentioning that networks gen-
erated from larger datasets (i.e. with thousands of
posts) have much more noise compared to this ex-
ample.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work
on mining sentiment from text, mining online dis-
cussions, extracting social networks from text, and
analyzing signed social networks. We define our
problem and explain our approach in Section 3. Sec-
tion 4 describes our dataset. Results and discussion
are presented in Section 5. We present a method for
identifying subgroups in online discussions in Sec-
tion 3.3. We conclude in Section 6.
2 Related Work
In this section, we survey several lines of research
that are related to our work.
2.1 Mining Sentiment from Text
Our general goal of mining attitude from one indi-
vidual toward another makes our work related to a
huge body of work on sentiment analysis. One such
line of research is the well-studied problem of iden-
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003; Kim and Hovy, 2004; Takamura et al2005).
Subjectivity analysis is yet another research line that
is closely related to our general goal of mining at-
titude. The objective of subjectivity analysis is to
identify text that presents opinion as opposed to ob-
jective text that presents factual information (Wiebe,
2000; Hatzivassiloglou and Wiebe, 2000; Banea et
al., 2008; Riloff and Wiebe, 2003). Our work is dif-
ferent from subjectivity analysis because we are not
only interested in discriminating between opinions
and facts. Rather, we are interested in identifying
the polarity of interactions between individuals. Our
method is not restricted to phrases or words, rather it
generalizes this to identifying the polarity of an in-
teraction between two individuals based on several
posts they exchange.
2.2 Stance Classification
Perhaps the closest work to this paper is the work on
stance classification. We notice that most of these
methods focus on the polarity of the written text as-
suming that anyone using positive text belongs to
one group and anyone using negative text belongs
to another. This works well for single-aspect topics
or entities like the ones used in (Tan et al2011)
(e.g. Obama, Sara Palin, Lakers, etc.). In this sim-
ple notion of topics, it is safe to assume that text
polarity is a good enough discriminator. This unfor-
tunately is not the case in online discussions about
complex topics having many aspects (e.g. abortion,
health care, etc.). In such complex topics, people use
positive and negative text targeting different aspects
of the topic, for example in the health care bill topic,
discussants expressed their opinion regarding many
aspects including: the enlarged coverage, the insur-
ance premiums, Obama, socialism, etc. This shows
that simply looking at text polarity is not enough to
identify groups.
Tan et al2011) studied how twitter following re-
lations can be used to improve stance classification.
Their main hypothesis is that connected users are
more likely to hold similar opinions. This may be
correct for the twitter following relations, but it is
not necessarily correct for open discussions where
60
no such relations exist. The only criterion that can be
used to connect discussants is how often they reply
to each other?s posts. We will show later that while
many people reply to people with similar opinions,
many others reply to people with different opinions
as well.
Thomas et al2006) address the same problem
of determining support and opposition as applied to
congressional floor-debates. They assess the agree-
ment/disagreement between different speakers by
training a text classifier and applying it to a win-
dow surrounding the names of other speakers. They
construct their training data by assuming that if two
speaker have the same vote, then every reference
connecting them is an agreement and vice versa.
We believe this will result in a very noisy train-
ing/testing set and hence we decided to recruit hu-
man annotators to create a training set. We found
out that many instances with references to other
discussants were labeled as neither agreement nor
disagreement regardless of whether the discussants
have similar or opposing positions. We will use this
system as a baseline and will show that the exis-
tence of positive/negative words close to a person
name does not necessarily show agreement or dis-
agreement with that person.
Hassan et al2010) use a language model based
approach for identifying agreement and disagree-
ment sentences in discussions. This work is limited
to sentences. It does not consider the overall rela-
tion between participants. It also does not consider
subgroup detection. We will use this method as a
baseline for one of our components and will show
that the proposed method outperforms it.
Murakami and Raymond (2010) present another
method for stance recognition. They use a small
number of hand crafted rules to identify agreement
and disagreement interactions. Hand crafted rules
usually result in systems with very low recall caus-
ing them to miss many agreement/disagreement in-
stances (they report 0.26 recall at the 0.56 preci-
sion level). We present a machine learning system
to solve this problem and achieve much better per-
formance. Park et al2011) propose a method for
finding news articles with different views on con-
tentious issues. Mohit et al2008) present a set
of heuristics for including disagreement informa-
tion in a minimum cut stance classification frame-
work. Galley et al2004) show the value of us-
ing durational and structural features for identify-
ing agreement and disagreement in spoken conver-
sational speech. They use features like duration of
spurts, speech rate, speaker overlap, etc. which are
not applicable to written language.
Our approach is different from agree-
ment/disagreement identification because we
not only study sentiment at the local sentiment
level but also at the global level that takes into
consideration many posts exchanged between
participants to build a signed network representation
of the discussion. Research on debate stance
recognition attempts to perform classification under
the ?supporting vs. opposing? paradigm. However
such simple view might not always be accurate
for discussions on more complex topics with
many aspects. After building the signed network
representation of discussions, we present a method
that can detect how the large group could split into
many subgroups (not necessarily two) with coherent
opinions.
2.3 Extracting Social Networks from Text
Little work has been done on the front of extracting
social relations between individuals from text. El-
son et al2010) present a method for extracting so-
cial networks from nineteenth-century British nov-
els and serials. They link two characters based on
whether they are in conversation or not. McCal-
lum et al2007) explored the use of structured data
such as email headers for social network construc-
tion. Gruzd and Hyrthonthwaite (2008) explored the
use of post text in discussions to study interaction
patterns in e-learning communities. Extracting so-
cial power relations from natural language (i.e. who
influences whom) has been studied in (Bramsen et
al., 2011; Danescu-Niculescu-Mizil et al2011).
Our work is related to this line of research because
we employ natural language processing techniques
to reveal embedded social structures. Despite sim-
ilarities, our work is uniquely characterized by the
fact that we extract signed social networks with both
positive and negative links from text.
2.4 Signed Social Networks
Most of the work on social networks analysis has
only focused on positive interactions. A few recent
papers have taken the signs of edges into account.
Brzozowski et al2008) study the positive and
negative relationships between users of Essembly.
Essembly is an ideological social network that dis-
tinguishes between ideological allies and nemeses.
Kunegis et al2009) analyze user relationships in
61
the Slashdot technology news site. Slashdot allows
users of the website to tag other users as friends or
foes, providing positive and negative endorsements.
Leskovec et al2010b) study signed social networks
generated from Slashdot, Epinions, and Wikipedia.
They also connect their analysis to theories of signed
networks from social psychology. A similar study
used the same datasets for predicting positive and
negative links given their context (Leskovec et al
2010a).
All this work has been limited to analyzing a
handful of datasets for which an explicit notion of
both positive and negative relations exists. Our work
goes beyond this limitation by leveraging the power
of natural language processing to automate the dis-
covery of signed social networks using the text em-
bedded in the network.
The research presented in this paper extends this
previous work in a number of ways: (i) we present
a method based on linguistic analysis that finds in-
stances of showing positive or negative attitude be-
tween participants (ii) we propose a technique for
representing discussions as signed networks where a
sign is associated with every edge to denote whether
the relation is friendly or antagonistic (iii) we eval-
uate the proposed methods using human annotated
data and also conduct a large scale evaluation based
on social psychology theories; (iv) finally we present
a method for identifying subgroups that globally
splits the community involved in the discussion by
utilizing the dynamics of the local interactions be-
tween participants.
3 Approach
3.1 Identifying Attitude from Text
To build a signed network representation of discus-
sants, we start by trying to identify sentences that
show positive or negative attitude from the writer to
the addressee. The first step toward identifying at-
titude is to identify words with positive/negative se-
mantic orientation. The semantic orientation or po-
larity of a word indicates the direction the word devi-
ates from the norm (Lehrer, 1974). We use Opinion-
Finder (Wilson et al2005a) to identify words with
positive or negative semantic orientation. The polar-
ity of a word is also affected by the context where
the word appears. For example, a positive word that
appears in a negated context should have a negative
polarity. Other polarized words sometimes appear as
neutral words in some contexts. To identify contex-
tual polarity of words, a large set of features is used
including words, sentences, structure, and other fea-
tures similar to the method described in (Wilson et
al., 2005b).
Our overall objective is to find the direct attitude
between participants. Hence after identifying the se-
mantic orientation of individual words, we move on
to predicting which polarized expressions target the
addressee and which do not.
Text polarity alone cannot be used to identify at-
titude between participants. Sentences that show
an attitude are different from subjective sentences.
Subjective sentences are sentences used to express
opinions, evaluations, and speculations (Riloff and
Wiebe, 2003). While every sentence that shows an
attitude is a subjective sentence, not every subjective
sentence shows an attitude toward the recipient.
In this method, we address the problem of iden-
tifying sentences with attitude as a relation detec-
tion problem in a supervised learning setting. We
study sentences that has mentions to the addressee
and polarized expressions (negative/positive words
or phrases). Mentions could either be names of other
participants or second person pronouns (you, your,
yours) used in text posted as a reply to another par-
ticipant. Reply structure (i.e. who replies to whom)
is readily available in many discussion forums. In
cases where reply structure is not available, we can
use a method like the one in (Lin et al2009) to re-
cover it.
We predict whether the mention is related to the
polarized expression or not. We regard the mention
and the polarized expression as two entities and try
to learn a classifier that predicts whether the two en-
tities are related or not.
The text connecting the two entities offers a very
condensed representation of the information needed
to assess whether they are related or not. For ex-
ample the two sentences ?you are completely un-
qualified? and ?you know what, he is unqualified ...?
show two different ways the words ?you?, and ?un-
qualified? could appear in a sentence. In the first
case the polarized word ?unqualified? refers to the
word ?you?. In the second case, the two words are
not related. The information in the shortest path
between two entities in a dependency tree can be
used to assert whether a relationship exists between
them (Bunescu and Mooney, 2005).
The sequence of words connecting the two enti-
ties is a very good predictor of whether they are re-
lated or not. However, these paths are completely
62
lexicalized and consequently their performance will
be limited by data sparseness. To alleviate this prob-
lem, we use higher levels of generalization to rep-
resent the path connecting the two tokens. These
representations are the part-of-speech tags, and the
shortest path in a dependency graph connecting the
two tokens. We represent every sentence with sev-
eral representations at different levels of generaliza-
tion. For example, the sentence ?your ideas are very
inspiring? will be represented using lexical, polar-
ity, part-of-speech, and dependency information as
follows:
LEX: ?YOUR ideas are very POS?
POS: ?YOUR NNS VBP RB JJ POS?
DEP: ?YOUR poss nsubj POS?
The set of features we use are the set of unigrams,
and bigrams representing the words, part-of-speech
tags, and dependency relations connecting the two
entities. For example the following features will be
set for the previous example:
YOUR ideas, YOUR NNS, YOUR poss,
poss nsubj, ...., etc.
We use Support Vector Machines (SVM) as a
learning system because it is good with handling
high dimensional feature spaces.
3.2 Extracting the Signed Network
In this subsection, we describe the procedure we
used to build the signed network given the compo-
nent we described in the previous subsection. This
procedure consists of two main steps. The first is
building the network without signs, and the second
is assigning signs to different edges.
To build the network, we parse our data to identify
different threads, posts and senders. Every sender is
represented with a node in the network. An edge
connects two nodes if there exists an interaction be-
tween the corresponding participants. We add a di-
rected edgeA? B, ifA replies toB?s posts at least
n times in m different threads. We set m, and n to
2 in all of our experiments. The interaction infor-
mation (i.e. who replies to whom) can be extracted
directly from the thread structure. Alternatively, as
mentioned earlier, we can use a method similar to
the one presented in (Lin et al2009) to recover the
reply structure if it is not readily available.
Once we build the network, we move to the more
challenging task in which we associate a sign with
Participant Features
Number of posts per month for A (B)
Percentage of positive posts per month for A (B)
Percentage of negative posts per month for A (B)
gender
Interaction Features
Percentage/number of positive (negative) sentences per post
Percentage/number of positive (negative) posts per thread
Discussion Domain (e.g. politics, science, etc.)
Table 1: Features used by the Interaction Sign Classifier.
every edge. We have shown in the previous section
how sentences with positive and negative attitude
can be extracted from text. Unfortunately the sign
of an interaction cannot be trivially inferred from the
polarity of sentences. For example, a single negative
sentence written by A and directed to B does not
mean that the interaction between A and B is neg-
ative. One way to solve this problem would be to
compare the number of negative sentences to posi-
tive sentences in all posts betweenA andB and clas-
sify the interaction according to the plurality value.
We will show later, in our experiments section, that
such a simplistic method does not perform well in
predicting the sign of an interaction.
As a result, we decided to pose the problem as a
classical supervised learning problem. We came up
with a set of features that we think are good predic-
tors of the interaction sign, and we trained a classi-
fier using those features on a labeled dataset. Our
features include numbers and percentages of pos-
itive/negative sentences per post, posts per thread,
and so on. A sentence is labeled as positive/negative
if a relation has been detected in this sentence be-
tween a mention referring to the addressee and a
positive/negative expression. A post is considered
positive/negative based on the majority of relations
detected in it. We use two sets of features. The first
set is related to A only or B only. The second set
is related to the interactions between A and B. The
features are summarized in Table 1.
3.3 Sub-Group Detection
In any discussion, different subgroups may emerge.
Members of every subgroup usually have a common
focus (positive or negative) toward the topic being
discussed. Each member of a group is more likely
to show positive attitude to members of the same
group, and negative attitude to members of opposing
groups. The signed network representation could
prove to be very useful for identifying those sub-
groups. To detect subgroups in a discussion thread,
63
we would like to partition the corresponding signed
network such that positive intra-group links and neg-
ative inter-group links are dense.
This problem is related to the constrained cluster-
ing (Wagstaff et al2001) and the correlation clus-
tering problem (Bansal et al2004). In constrained
clustering, a pairwise similarity metric (which is
not available in our domain), and a set of must-
link/cannot-link constraints are used with a standard
data clustering algorithm. Correlation clustering op-
erates in a scenario where given a signed graph
G = (V,E) where the edge label indicates whether
two nodes are similar (+) or different (-), the task
is to cluster the vertices so that similar objects are
grouped together. Bansal et. al (2004) proved NP-
hardness and gave constant-factor approximation al-
gorithms for the special case in which the graph
is complete (full information) and every edge has
weight +1 or -1 which is not the case in our network.
Alternatively, we can use a greedy optimization al-
gorithm to find partitions. A criterion function for
a local optimization partitioning procedure is con-
structed such that positive links are dense within
groups and negative links are dense between groups.
For any potential partition C, we seek to optimize
the following function: P (C) = ?
?
n +(1??)
?
p
where
?
n is the number of negative links between
nodes in the same subgroup,
?
p is the number of
positive links between nodes in different subgroups,
and ? is a trade factor that represents the importance
of the two terms. We set ? to 0.5 in all our experi-
ments.
Clusters are selected such that: C? =
argminP (C). A greedy optimization framework
is used to minimize P (C). Initially, nodes are ran-
domly partitioned into t different clusters and the
criterion function P is evaluated for that cluster. Ev-
ery cluster has a set of neighbors in the cluster space.
A neighbor cluster is obtained by moving one node
from one cluster to another, or by exchanging two
nodes in two different clusters. Neighbor partitions
are evaluated, and if one with a lower value for the
criterion function is found, it is set as the current
partition. This greedy procedure is repeated with
random restarts until a minimal solution is found.
To determine the number of subgroups t, we select
t that minimizes the optimization function P (C). In
all experiments we used an upper limit of t = 5.
This technique was able to identify the correct num-
ber of subgroups in 77% of the times. In the rest of
the cases, the number was different from the correct
number by at most 1 except for a single case where
it was 2.
4 Data
4.1 Signed Network Extraction
Our data consists of a large amount of discussion
threads collected from online discussion forums. We
collected around 41, 000 topics (threads) and 1.2M
posts from the period between the end of 2008 and
the end of 2010. All threads were in English and had
5 posts or more. They covered 11 different domains
including: politics, religion, science, etc. The aver-
age number of participants per domain is 1320 and
per topic is 52. The data was tokenized, sentence-
split, and part-of-speech tagged with the OpenNLP
toolkit. It was parsed with the Stanford parser (Klein
and Manning, 2003).
We randomly selected around 5300 posts (1000
interactions), and asked human annotators to label
them. Our annotators were instructed to read all the
posts exchanged between two participants and de-
cide whether the interaction between them is posi-
tive or negative. We used Amazon Mechanical Turk
for annotations. Following previous work (Callison-
Burch, 2009; Akkaya et al2010), we took sev-
eral precautions to maintain data integrity. We re-
stricted annotators to those based in the US to main-
tain an acceptable level of English fluency. We also
restricted annotators to those who have more than
95% approval rate for all previous work. Moreover,
we asked three different annotators to label every in-
teraction. The label was computed by taking the ma-
jority vote among the three annotators. We refer to
this data as the Interactions Dataset.
We ran a different annotation task where we se-
lected sentences including mentions referring to dis-
cussants (names or pronouns) and polarized expres-
sions. Annotators were asked to select sentences
where the polarized attribute is referring to the men-
tion and hence show a positive or negative attitude
toward other discussion participants. This resulted
in a set of 5000 manually annotated sentences. We
refer to this data as the Sentences Dataset.
We asked three different annotators to label ev-
ery instance. The kappa measure between the three
groups of annotations was 0.62 for the Interactions
Dataset and 0.64 for the Sentences Dataset. To bet-
ter assess the quality of the annotations, we asked a
trained annotator to label 10% of the data. We mea-
sured the agreement between the expert annotator
64
Logistic Reg.
Class Pos. Neg. Weigh. Avg.
Precision 0.848 0.724 0.809
Recall 0.884 0.657 0.812
F-Measure 0.866 0.689 0.81
Accuracy - - 0.812
SVM
Precision 0.906 0.71 0.844
Recall 0.847 0.809 0.835
F-Measure 0.875 0.756 0.838
Accuracy - - 0.835
Table 2: Interaction sign classifier performance.
Classifier Random Thresh-Num Thresh-Perc. SVM
Accuracy 65% 69% 71% 83.5%
Table 3: A comparison of different sign interaction clas-
sifiers.
and the majority label from Mechanical Turk. The
kappa measure was 0.69 for the Interactions Dataset
and 0.67 for the Sentences Dataset.
4.2 Sub-group Detection
We used a dataset of more than 42 topics and ap-
proximately 9000 posts collected from two political
forums (Createdebate1 and Politicalforum2). The fo-
rum administrators ran a poll asking participants to
select their stance from a set of possible answers
and hence the dataset was self-labeled with respect
to groups. We also used a set of discussions from
the Wikipedia discussion section. When a topic on
Wikipedia is disputed, the editors of that topic start a
discussion about it. We collected 117 Wikipedia dis-
cussion threads. The threads contain a total of 1,867
posts. The discussions were annotated by an expert
annotator (a professor in sociolinguistics, not an au-
thor of the paper) who was instructed to read each
of the Wikipedia discussion threads in its entirety
and determine whether the discussants split into sub-
groups, in which case he was asked to identify the
subgroup membership for each discussant. In to-
tal, we had 159 topics with an average of approxi-
mately 500 posts, 60 participants and 2.7 subgroups
per topic. Examples of the topics include: Arizona
immigration law, airport security, oil spill, evolution,
Ireland partitions, abortion and many others.
5 Results and Discussion
We performed experiments on the data described
in the previous section. We trained and tested the
sentence with the attitude detection classifiers de-
scribed in Section 3.1 using the Sentences Dataset.
1www.createdebate.com
2www.politicalforum.com
We also trained and tested the interaction sign clas-
sifier described in Section 3.2 using the Interactions
Dataset. We built one signed social network for ev-
ery domain (e.g. politics, economics, etc.). We de-
cided to build a network for every domain as op-
posed to one single network because the relation be-
tween any two individuals may vary across domains
(e.g. politics vs. science). In the rest of this section,
we will describe the experiments we did to assess the
performance of the sentences with attitude detection
and interaction sign prediction steps.
In addition to classical evaluation, we evaluate
our results using the structural balance theory which
has been shown to hold both theoretically (Heider,
1946) and empirically (Leskovec et al2010c). We
validate our results by showing that the automati-
cally extracted networks mostly agree with the the-
ory. We evaluated the approach using the structural
balance theory because it presents a global (pertain-
ing to relations between multiple edges) and large-
scale (used millions of posts and thousands of users)
evaluation of the results as opposed to traditional
evaluation which is local in nature (only considers
one edge at a time) and smaller in scale (used thou-
sands of posts).
5.1 Identifying Sentences with Attitude
We compare the proposed methods to two baselines.
The first baseline is based on the work of (Thomas
et al2006). We used the speaker agreement com-
ponent presented in (Thomas et al2006) as a base-
line. The speaker agreement component is one step
in their approach. In this component, they used
an SVM classifier trained using a window of text
surrounding references to other speakers to predict
agreement/disagreement between speakers.
We build an SVM text classifier trained on the
sentence at which the mention referring to the other
participant occurred. We refer to this baseline as
the Text Classification approach. The second base-
lines adopts the language model approach presented
in (Hassan et al2010). Two language models
are trained using a stream of words, part-of-speech
tags, and dependency relations, one for sentences
that show an attitude and one for sentences that do
not. New sentences are classified based on gener-
ation likelihoods. We refer to this baseline as the
Language Models approach.
We tested this component using the Sentences
Dataset described in Section 4. We compared the
performance of the proposed method and the two
65
Extracted Networks Random Networks
Domain (+++) (++?) (+??) (???) (+++) (++?) (+??) (???)
abortion 51.67 26.31 18.92 0.48 35.39 43.92 18.16 2.52
current-events 67.36 22.26 8.76 0.23 54.08 36.90 8.39 0.64
off-topic-chat 65.28 23.54 9.45 0.25 58.07 34.59 6.88 0.46
economics 72.68 18.30 7.77 0.00 66.50 29.09 4.22 0.20
political opinions 60.60 24.24 12.81 0.43 45.97 40.79 12.06 1.19
environment 47.46 32.54 17.26 0.30 37.38 43.61 16.89 2.12
latest world news 58.29 22.41 16.33 0.62 42.26 42.20 13.98 1.56
religion 47.17 25.89 22.56 1.42 39.68 42.94 15.51 1.87
science-technology 57.53 26.03 14.33 0.00 50.14 38.93 10.05 0.87
terrorism 64.96 23.36 9.46 0.73 41.54 42.42 14.36 1.68
Table 4: Percentage of different types of triangles in the extracted networks vs. the random networks.
Method Accuracy Precision Recall F1
Text Classification 60.4 61.1 60.2 60.6
Language Models 80.3 81.0 79.4 80.2
Relation Extraction 82.3 82.3 82.3 82.3
Table 5: Comparison of attitude identification methods.
baselines. Table 5 compares the precision, recall,
F1, and accuracy for the three methods. The text
classification based approach does much worse than
others. The reasons is that it ignores the structure
and uses much less information (part-of-speech tags
and dependency trees are not used) compared to the
other methods. Additionally, the short length of the
sentences compared to what is typical in text clas-
sification may have had a bad effect on the perfor-
mance. Both other models try to learn the char-
acteristics of the path connecting the mention and
the polarized expression. We notice that optimizing
the weights for unigram and bigrams features using
SVM results in a better performance compared to
language models because it does not have the con-
straints imposed by the former model on the learned
weights.
We evaluated the importance of the feature types
(i.e. dependency vs. pos tags vs words) by measur-
ing the chi-squared statistic for every feature with
respect to the class. Dependency features were most
helpful, but other types of features helped improve
the performance as well.
5.2 Interaction Sign Classifier
We used the relation detection classifier described in
Section 3.1 to find sentences with positive and nega-
tive attitude. The output of this classifier was used to
compute the features described in Section 3.2, which
were used to train a classifier that predicts the sign
of an interaction between any two individuals.
We used both Support Vector Machines (SVM)
and logistic regression to train the sign interaction
classifier. We report several performance metrics for
them in Table 2. We notice that the SVM classifier
performs better with an accuracy of 83.5% and an
F-measure of 81%. All results were computed using
10 fold cross validation on the labeled data. To bet-
ter assess the performance of the proposed classifier,
we compare it to a baseline that labels the relation as
negative if the percentage of negative sentences ex-
ceeds a particular threshold, otherwise it is labeled
as positive. The thresholds were empirically esti-
mated using a separate development set. The accu-
racy of this baseline is only 71%.
To better assess the performance of the proposed
classifier, we compare it to three baselines. The first
is a random baseline that predicts an interaction as
positive with probability p that equals the proportion
of positive instances to all instances in the training
set. The second classifier (Thresh-Num) labels the
edge as negative if the number of negative instances
exceeds a threshold Tn. The third classifier (Thresh-
Perc) labels the edge as negative if the percentage of
negative instances to all instances exceeds a thresh-
old Tp. The cutoff thresholds were estimated using
a separate development set.
The 3 baselines were tested using the entire la-
beled dataset. The SVM classifier was tested using
10 fold cross validation. The accuracy of the ran-
dom classifier, the two based on a cut off number
and percentage , and the SVM classifier are shown
in Table 3. We notice that the random classifier per-
forms worst, and the classifier based on percentage
cutoff outperforms the one based on number cut-
off. The SVM classifier significantly outperforms all
other classifiers. We tried to train a classifier using
both the number and percentage of negative and pos-
itive posts. The improvement over using the baseline
using the percentage of negative posts was not sta-
tistically significant.
We evaluated the importance of the features listed
66
in Table 1 by measuring the chi-squared statistic for
every feature with respect to the class. We found
out that the features describing the interaction be-
tween the two participants are more informative than
the ones describing individuals characteristics. The
later features are still helpful though and they im-
prove the performance by a statistically significant
amount. We also noticed that all features based on
percentages are more informative than those based
on counts. The most informative features are: per-
centage of negative posts per tread, percentage of
negative sentences per post, percentage of positive
posts per thread, number of negative posts, and dis-
cussion domain.
5.3 Structural Balance Theory
The structural balance theory is a psychological the-
ory that tries to explain the dynamics of signed so-
cial interactions. It has been shown to hold both the-
oretically (Heider, 1946) and empirically (Leskovec
et al2010c). In this section, we study the agree-
ment between the theory and our automatically ex-
tracted networks. The theory has its origins in the
work of Heider (1946). It was then formalized in
a graph theoretic form by (Cartwright and Harary,
1956). The theory is based on the principles that ?the
friend of my friend is my friend?, ?the enemy of my
friend is my enemy?, ?the friend of my enemy is
my enemy?, and variations on these. The structural
balance theory states that triangles that have an odd
number of positive signs (+ + + and + - -) are bal-
anced, while triangles that have an even number of
positive signs (- - - and + + -) are not.
In this section, we compare the predictions of
edge signs made by our system to the structural bal-
ance theory by counting the frequencies of differ-
ent types of triangles in the predicted network. Ta-
ble 4 shows the frequency of every type of trian-
gle for 10 different domains. To better understand
these numbers, we compare them to the frequencies
of triangles in a set of random networks. We shuf-
fle the signs for all edges on every network keeping
the fractions of positive and negative edges constant.
We repeat shuffling for 1000 times and report the av-
erage.
We find that the all-positive triangle (+ + +) is
overrepresented in the generated network compared
to chance across all domains. We also see that the
triangle with two positive edges (+ + ?), and the
all-negative triangle (? ? ?) are underrepresented
compared to chance across all domains. The tri-
angle with a single positive edge is slightly over-
represented in most but not all of the topics com-
pared to chance. This shows that the predicted net-
works mostly agree with the structural balance the-
ory. The slightly non standard behavior of the tri-
angle with one positive edge could be explained in
light of the weak balance theory. In this theory,
Davis (1967) states that this triangle, which corre-
sponds to the ?enemy of enemy is my friend? propo-
sition, holds only if the network can be partitioned
into exactly two subsets, but not when there are more
than two. In general, the percentage of balanced tri-
angles in the predicted networks is higher than in
the shuffled networks, and hence the balanced trian-
gles are significantly overrepresented compared to
chance showing that our automatically constructed
network is similar to explicit signed networks in that
they both mostly agree with the balance theory.
5.4 Sub-Group Detection
We compare the performance of the sub-group de-
tection method to three baselines. The first base-
line uses graph clustering (GC) to partition a net-
work based on the frequency of interaction between
participants. We build a graph where each node
represents a participant. Edges link participants if
they exchange posts, and edge weights are based on
the number of posts exchanged. The second base-
line (TC) is based on the premise that participants
with similar text are more likely to belong to the
same subgroup. We measure text similarity by com-
puting the cosine similarity between the tf-idf rep-
resentations of the text in a high dimensional vec-
tor space. We tried two methods for partitioning
those graphs: spectral partitioning (Luxburg, 2007)
and a hierarchical agglomeration algorithm which
works by greedily optimizing the modularity for
graphs (Clauset et al2004). The third baseline is
based on stance classification approaches (e.g. (Tan
et al2011)). In this baseline we put all the partic-
ipants who use more positive text in one subgroup
and the participants who use more negative text in
another subgroup. Text polarity is identified using
the method described in Section 3.1.
Table 6 shows the average purity (Purity), entropy
(Entropy), Normalizes Mutual Information (NMI),
and Rand Index (RandIndex) values of the method
based on signed networks and the baselines using
different partitioning algorithms. The differences in
the results shown in the table are statistically sig-
nificant at the 0.05 level (as indicated by a 2-tailed
67
Figure 2: A signed network representing participants in a discussion about the ?Health Care Reform Bill?. Blue (dark)
nodes represent participants with the bill, Yellow (light) nodes represent participants against the bill, red (solid) edges
represent negative attitude, while green (dashed) edges represent positive attitude.
Createdebate Politicalforum Wikipedia
Method Purity Entropy NMI RandIndex Purity Entropy NMI RandIndex Purity Entropy NMI RandIndex
GC - Spectral 0.50 0.85 0.28 0.40 0.50 0.88 0.27 0.39 0.49 0.89 0.33 0.35
GC - Hierarchical 0.48 0.86 0.30 0.41 0.47 0.89 0.31 0.40 0.49 0.87 0.38 0.39
TC - Spectral 0.50 0.85 0.31 0.43 0.48 0.90 0.30 0.45 0.51 0.87 0.40 0.46
TC - Hierarchical 0.49 0.90 0.35 0.46 0.48 0.91 0.33 0.49 0.53 0.80 0.40 0.49
Text Polarity 0.55 0.80 0.38 0.49 0.54 0.91 0.31 0.38 0.34 0.95 0.30 0.40
Signed Networks 0.64 0.74 0.46 0.59 0.58 0.80 0.43 0.55 0.65 0.54 0.51 0.60
Table 6: Comparison of the sub-group detection method to baseline systems
paired t-test).
We notice that partitioning the signed network
that was automatically extracted from text results in
significantly better partitions on the three datasets as
indicated by the higher Purity, NMI, and RandIndex
and the lower Entropy values it achieves. We believe
that the first two baselines performed poorly because
the interaction frequency and the text similarity are
not key factors in identifying subgroup structures.
Many people would respond to people they disagree
with more, while others would mainly respond to
people they agree with most of the time. Also, peo-
ple in opposing subgroups tend to use very similar
text when discussing the same topic and hence text
clustering does not work as well. The baseline that
classifies the stance of discussants based on the po-
larity of their text performed bad too because it over-
looks the fact that most of the discussed topics in our
datasets have multiple aspects and a discussant may
use both positive and negative text targeting differ-
ent aspects of the topic. An example of a signed net-
work and the corresponding subgtoups as extracted
from real data is showm in Figure 2.
6 Conclusions
In this paper, we have shown that natural language
processing techniques can be reliably used to extract
signed social networks from text correspondences.
We believe that this work brings us closer to un-
derstanding the relation between language use and
social interactions and opens the door to further re-
search efforts that go beyond standard social net-
work analysis by studying the interplay of positive
and negative connections. We rigorously evaluated
the proposed methods on labeled data and connected
our analysis to social psychology theories to show
that our predictions mostly agree with them. Finally,
we presented potential applications that benefit from
the automatically extracted signed network.
Acknowledgments
This research was funded in part by the Office of the
Director of National Intelligence, Intelligence Ad-
vanced Research Projects Activity. All statements
of fact, opinion or conclusions contained herein are
those of the authors and should not be construed as
representing the official views or policies of IARPA,
the ODNI or the U.S. Government
68
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, CSLDAMT ?10, pages 195?203.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation Clustering. Machine Learning, 56(1):89?
113.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label dis-
agreement in the min-cut classification framework. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011. Extracting social power rela-
tionships from natural language. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 773?782.
Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.
2008. Friends and foes: ideological social network-
ing. In Proceeding of the twenty-sixth annual SIGCHI
conference on Human factors in computing systems,
pages 817?820, New York, NY, USA.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, HLT ?05, pages 724?731,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 286?295.
Dorwin Cartwright and Frank Harary. 1956. Structure
balance: A generalization of heiders theory. Psych.
Rev., 63.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70:066111.
Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang,
and Jon M. Kleinberg. 2011. Echoes of power: Lan-
guage effects and power differences in social interac-
tion. CoRR.
J. A. Davis. 1967. Clustering and structural balance in
graphs. Human Relations, 20:181?187.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138?147,
Uppsala, Sweden, July.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech: use
of bayesian networks to model pragmatic dependen-
cies. In Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics, ACL ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social networks
from threaded discussions. In Proceedings of the In-
ternational Network of Social Network Analysis (IN-
SNA), St. Pete Beach, Florida.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Fritz Heider. 1946. Attitudes and cognitive organization.
Journal of Psychology, 21:107?112.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03, pages 423?430.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a so-
cial network with negative edges. In Proceedings of
the 18th international conference on World wide web,
pages 741?750, New York, NY, USA.
Adrienne Lehrer. 1974. Semantic fields and lezical struc-
ture. North Holland, Amsterdam and New York.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010a. Predicting positive and negative links in online
social networks. In Proceedings of the 19th interna-
tional conference on World wide web, pages 641?650,
New York, NY, USA.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010b. Signed networks in social media. In Proceed-
ings of the 28th international conference on Human
factors in computing systems, pages 1361?1370, New
York, NY, USA.
69
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010c. Signed networks in social media. In CHI 2010,
pages 1361?1370, New York, NY, USA. ACM.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
Ulrike Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17:395?416, December.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
Akiko Murakami and Rudy Raymond. 2010. Support or
oppose?: classifying positions in online debates from
reply activities and opinion expressions. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Posters, pages 869?875.
Souneil Park, KyungSoon Lee, and Junehwa Song. 2011.
Contrasting opposing views of news articles on con-
tentious issues. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, pages
340?349.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings
of the 17th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?11,
pages 1397?1405.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In In Proceedings
of EMNLP, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Kiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan
Schro?dl. 2001. Constrained k-means clustering with
background knowledge. In Proceedings of the Eigh-
teenth International Conference on Machine Learning,
pages 577?584.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjectiv-
ity analysis. In Proceedings of HLT/EMNLP on Inter-
active Demonstrations, HLT-Demo ?05, pages 34?35,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Vancou-
ver, Canada.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng., 19(10):1333?1348.
70
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1000?1010,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Identifying Web Search Query Reformulation using Concept based
Matching
Ahmed Hassan
Microsoft Research
One Microsoft Way
Redmond, WA 98053, USA
hassanam@microsoft.com
Abstract
Web search users frequently modify their
queries in hope of receiving better results.
This process is referred to as ?Query Refor-
mulation?. Previous research has mainly fo-
cused on proposing query reformulations in
the form of suggested queries for users. Some
research has studied the problem of predicting
whether the current query is a reformulation
of the previous query or not. However, this
work has been limited to bag-of-words models
where the main signals being used are word
overlap, character level edit distance and word
level edit distance. In this work, we show
that relying solely on surface level text sim-
ilarity results in many false positives where
queries with different intents yet similar top-
ics are mistakenly predicted as query reformu-
lations. We propose a new representation for
Web search queries based on identifying the
concepts in queries and show that we can sig-
nificantly improve query reformulation perfor-
mance using features of query concepts.
1 Introduction
Web search is a process of querying, learning and
reformulating queries to satisfy certain information
needs. When a user submits a search query, the
search engine attempts to return the best results to
that query. Oftentimes, users modify their search
queries in hope of getting better results. Typical
search users have low tolerance to viewing lowly
ranked search results and they prefer to reformu-
late the query rather than wade through result list-
ings (Jansen and Spink, 2006). Previous stud-
ies have also shown that 37% of search queries
are reformulations to previous queries (Jansen et
al., 2007) and that 52% of users reformulate their
queries (Jansen et al, 2005).
Understanding query reformulation behavior and
being able to accurately identify reformulation
queries have several benefits. One of these benefits
is learning from user behavior to better suggest au-
tomatic query refinements or query alterations. An-
other benefit is using query reformulation predic-
tion to identify boundaries between search tasks and
hence segmenting user activities into topically co-
herent units. Also, if we are able to accurately iden-
tify query reformulations, then we will be in a bet-
ter position to evaluate the satisfaction of users with
query results. For example, search satisfaction is
typically evaluated using clickthrough information
by assuming that if a user clicks on a result, and
possibly dwells for a certain amount of time, then
the user is satisfied. Identifying query reformulation
can be very useful for finding cases where the users
are not satisfied even after a click on a result that
may have seemed relevant given its title and sum-
mary but then turned out to be not relevant to the
user?s information need.
Previous work on query reformulation has either
focused on automatic query refinement by the search
system, e.g. (Jones et al, 2006; Boldi et al,
2008) or on defining taxonomies for query refor-
mulation strategies, e.g. (Lau and Horvitz, 1999;
Anick, 2003). Other work has proposed solutions
for the query reformulation prediction problem or
for the similar problem of task boundary identifi-
cation (Radlinski and Joachims, 2005; Jones and
Klinkner, 2008). These solutions have adopted the
1000
bag-of-words approach for representing queries and
mostly used features of word overlap or character
and word level edit distances. Take the queries ?ho-
tels in New York City? and ?weather in New York
City? as an example. The two queries are very likely
to have been issued by a user who is planning to
travel to New York City. The two queries have 5
words each, 4 of them are shared by the two queries.
Hence, most of the solutions proposed in previous
work for this problem will incorrectly assume that
the second query is a reformulation of the first due
to the high word overlap ratio and the small edit dis-
tance. In this work, we propose a method that goes
beyond the bag-of-words method by identifying the
concepts underlying these queries. In the previous
example, we would like our method to realize that
in the first query, the user is searching for ?hotels?
while for in the second query, she is searching for the
?weather? in New York City. Hence, despite similar
in terms of shared terms, the two queries have differ-
ent intents and are not reformulations of one another.
To this end, we conducted a study where we col-
lected thousands of consecutive queries and trained
judges to label them as either reformulations or
not. We then built a classifier to identify query
reformulation pairs and showed that the proposed
classifier outperforms the state-of-the-art methods
on identifying query reformulations. The proposed
method significantly reduces false positives (non-
reformulation pairs incorrectly classified as refor-
mulation) while achieving high recall and precision.
2 Related Work
There are three areas of work related to the research
presented in this paper: (i) query reformulation tax-
onomies, (ii) automatic query refinement, and (iii)
search tasks boundary identification. We cover each
of these areas in turn.
2.1 Query Reformulation Taxonomies
Existing research has studied how web search en-
gines can propose reformulations, but has given less
attention to how people perform query reformula-
tions. Most of the research on manual query re-
formulation has focused on building taxonomies of
query reformulation. These taxonomies are gener-
ally constructed by examining a small set of query
logs. Anick (2003) classified a random sample of
100 reformulations by hand into eleven categories.
Jensen et al (2007) identified 6 different kinds
of reformulation states (New, Assistance, Content
Change, Generalization, Reformulation, and Spe-
cialization) and provided heuristics for identifying
them. They also used them to predict when a user
is most receptive to automatic query suggestions.
The same categories were used in several other stud-
ies (Guo et al, 2008; Lau and Horvitz, 1999).
Huang and Efthimis (2010) proposed another re-
formulation taxonomy. Their taxonomy was lexi-
cal in nature (e.g., word reorder, adding words, re-
moving words, etc.). They also proposed the use of
regular expressions to identify them. While study-
ing re-finding behavior, Teevan et al (2006) con-
structed a taxonomy of query re-finding by manu-
ally examining query logs, and implemented algo-
rithms to identify repeat queries, equal click queries
and overlapping click queries. None of this work
has built an automatic classifier distinguishing refor-
mulation queries from other queries. Heuristics and
regular expressions have been used in (Huang et al,
2010) and (Jansen et al, 2007) to identify different
types of reformulations. This line of work is relevant
to our work because it studies query reformulation
strategies. Our work is different because we build a
machine-learned predictive model to identify query
reformulation while this line of work mainly focuses
on defining taxonomies for reformulation strategies.
2.2 Automatic Query Refinement
A close problem that has received most of the re-
search attention in this area is the problem of auto-
matically generating query refinements. These re-
finements are typically offered as query suggestions
to the users or used to alter the user query before
submitting it to the search engine.
Boldi et al (2008) introduced the concept of
the query-flow graph where every query is repre-
sented by a node and edges connect queries if it is
likely for users to move from one query to another.
Mei et al (2008) used random walks over a bipar-
tite graph of queries and URLs to find query refine-
ments. Query logs were used to suggest query re-
finements in (Baeza-Yates et al, 2005). Hierarchi-
cal agglomerative clustering was used to group sim-
ilar queries that can be used as suggestions for one
1001
another. Other research has adopted methods based
on query expansion (Mitra et al, 1998) or query
substitution (Jones et al, 2006). This line of work
is different from our work because it focuses on au-
tomatically generating query refinements while this
work focuses on identifying cases of manual query
reformulations.
2.3 Search Task Boundary Identification
The problem of classifying the boundaries of the
user search tasks within sessions in web search logs
has been widely addressed before. This problem is
closely related to the problem of identifying query
reformulation. A search task has been defined in
(Jones and Klinkner, 2008) as a single information
need that may result in one or more queries. Sim-
ilarly , Jansen et al (2007) defined a session as
a series of interactions by the user toward address-
ing a single information need. On the other hand, a
query reformulation is intended to modify a previ-
ous query in hope of getting better results to satisfy
the same information need. From these definitions,
it is clear how query reformulation and task bound-
ary detection are two sides of the same problem.
Boldi et al (2008) presented the concept of the
query-flow graph. A query-flow graph represents
chains of related queries in query logs. They use
this model for finding logical session boundaries and
query recommendation. Ozmutlu (2006) proposed
a method for identifying new topics in search logs.
He demonstrated that time interval, search pattern
and position of a query in a user session, are ef-
fective for shifting to a new topic. Radlinski and
Joachims (2005) study sequences of related queries
(query chains). They used that to generate new types
of preference judgments from search engine logs to
learn better ranked retrieval functions.
Arlitt (2000) found session boundaries using a
calculated timeout threshold. Murray et al (2006)
extended this work by using hierarchical clustering
to find better timeout values to detect session bound-
aries. Jones and Klinkner (2008) also addressed
the problem of classifying the boundaries of the
goals and missions in search logs. They showed
that using features like edit distance and common
words achieves considerably better results compared
to timeouts. Lucchese et al (Lucchese et al, 20011)
uses a similar set of features as (Jones and Klinkner,
2008), but uses clustering to group queries in the
same task together as opposed to identifying task
boundary as in (Jones and Klinkner, 2008). This
line of work is perhaps the closest to our work. Our
work is different because it goes beyond the bag of
words approach and tries to assess query similarity
based on the concepts represented in each query. We
compare our work to the state-of-the-art work in this
area later in this paper.
3 Problem Definition
We start by defining some terms that will be used
throughout the paper:
Definition: Query Reformulation is the act of
submitting a query Q2 to modify a previous search
query Q1 in hope of retrieving better results to sat-
isfy the same information need.
Definition: A Search Session is group of queries
and clicks demarcated with a 30-minute inactiv-
ity timeout, such as that used in previous work
(Downey et al, 2007; Radlinski and Joachims,
2005).
Search engines receive streams of queries from
users. In response to each query, the engine returns a
set of search results. Depending on these results, the
user may decide to click on one or more results, sub-
mit another query, or end the search session. In this
work, we focus on cases where the user submits an-
other query. Our objective is to solve the following
problem: Given a queryQ1, and the following query
Q2, predict whether Q2 is reformulation of Q1.
4 Approach
In this section, we propose methods for predicting
whether the current query has been issued by the
user to reformulate the previous query.
4.1 Query Normalization
We perform standard normalization where we re-
place all letters with their corresponding lower case
representation. We also replace all runs of whites-
pace characters with a single space and remove any
leading or trailing spaces. In addition to the stan-
dard normalization, we also break queries that do not
respect word boundaries into words. Word break-
ing is a well-studied topic that has proved to be
1002
Table 1 : Examples of queries, the corresponding segmentation, and the concept representation.
Phrases are separated by ?|? and different tokens in a keyword are separated by ? ?
Query Phrases and Keywords Concept Representation
hotels in new york city hotels in new york city Concept1 {head=?hotels?,
modifiers = ?new york city?}
hyundai roadside assistance hyundai roadside assistance Concept1 {head = ?roadside
phone number | phone number assistance?, modifiers = ?hyundai?},
Concept2{?phone number?}
kodak easyshare recharger chord kodak easyshare recharger cord Concept1{head =?recharger cord?,
modifiers ???kodak easyshare?}
user reviews for apple iphone user reviews for apple iphone Concept1{head=?user reviews?
, modifiers = ?apple iphone?}
user reviews for apple ipad user reviews for apple ipad Concept1{head =?user reviews?,
modifiers = ?apple ipad?}
tommy bhama rug tommy bhama rug Concept1{head =?rug?,
modifiers ???tommy bhama?}
tommy bhama perfume tommy bhama perfume Concept1{head =?perfume?,
modifiers ???tommy bhama?}
useful for many natural language processing appli-
cations. This becomes a frequent problems with
queries when users do not observe the correct word
boundaries (for example: ?southjerseycraigslist? for
?south jersey craiglist?) or when users are searching
for a part of a URL (for example ?quincycollege?
for ?quincy college?). We used a freely available
word breaker Web service that has been described at
(Wang et al, 2011).
4.2 Queries to Concepts
Lexical similarity between queries has been often
used to identify related queries (Jansen et al, 2007).
The problem with lexical similarity is that it intro-
duces many false negatives (e.g. synonyms) , but
this can be handled by other features as we will de-
scribe later. More seriously, it introduces many false
positives. Take the following query pair as an exam-
ple Q1: weather in new york city and Q2: ?hotels in
new york city?. Out of 5 words, 4 words are shared
between Q1 and Q2. Hence, any lexical similarity
feature would predict that the user submitted Q2 as
a reformulation of Q1. What we would like to do
is to have a query representation that recognizes the
difference between Q1 and Q2.
If we look closely at the two queries, we will no-
tice that in the first query, the user is looking for
the ?weather?, while in the second query the user
is looking for ?hotels?. We would like to recog-
nize ?weather?, and ?hotels? as the head keywords
of Q1 and Q2 respectively, while ?new york city? is
a modifier of the head keyword in both cases. To
build such a representation, we start by segmenting
each query into phrases. Query segmentation is the
process of taking a users search query and divid-
ing the tokens into individual phrases or semantic
units (Bergsma and Wang, 2007). Many approaches
to query segmentation have been presented in recent
research. Some of them pose the problem as a super-
vised learning problem (Bergsma and Wang, 2007;
Yu and Shi, 2009). Many of the supervised methods
though use expensive features that are difficult to re-
implement.
On the other hand, many unsupervised methods
for query segmentation have also been proposed
(Hagen et al, 2011; Hagen et al, 2010). Most
of these methods use only raw web n-gram fre-
quencies and are very easy to re-implement. Ad-
ditionally, Hagen et al (2010) have shown that
these methods can achieve segmentation accuracy
comparable to current state-of-the-art techniques us-
ing supervised learning. We opt for the unsuper-
vised techniques to perform query segmentation.
More specifically, we adopt the mutual information
1003
method (MI) used throughout the literature. A seg-
mentation for a query is obtained by computing the
pointwise mutual information score for each pair
of consecutive words. More formally, for a query
x = {x1, x2, ..., xn}
PMI(xi, xi+1) = log
p(xi, xi+1)
p(xi)p(xi+1)
(1)
where p(xi, xi+1) is the joint probability of occur-
rence of the bigram (xi, xi+1) and p(xi) and p(xi+1)
are the individual occurrence probabilities of the two
tokens xi and xi+1 .
A segment break is introduced whenever the point
wise mutual information between two consecutive
words drops below a certain threshold ? . The thresh-
old we used, ? = 0.895 , was selected to max-
imize the break accuracy (Jones et al, 2006) on
the Bergsma-Wang-Corpus (Bergsma and Wang,
2007). Furthermore, we do not allow a break to hap-
pen between a noun and a proposition (e.g. no break
can be introduced between ?hotels? and ?in? or ?in?
and ?new York? in the query ?hotels in new york
city?). We will shorty explain how we obtained the
part-of-speech tags.
In addition to breaking the query into phrases, we
were also interested in grouping multi-word key-
words together (e.g. ?new york?, ?Michael Jack-
son?, etc.). The intuition behind that is that a
query containing the keyword ?new york? and an-
other containing the keyword ?new mexico? should
not be awarded because they share the word ?new?.
We do that by adopting a hierarchical segmentation
technique where the same segmentation method de-
scribed above is reapplied to every resulting phrase
with a new threshold ?s < ? . We selected the
new threshold, ? = 1.91 , to maximize the break
accuracy over a set of a random sample of 10,000
Wikipedia title of persons, cities, countries and or-
ganizations and a random sample of bigrams and tri-
grams from Wikipedia text.
In our implementation, the probabilities for all
words and n-grams have been computed using the
freely available Microsoft Web N-Gram Service
(Huang et al, 2010).
Now that we have the phrases and keywords in
each query, we assume that every phrase corre-
sponds to a semantic unit. Every semantic unit
has a head and a zero or more modifiers. Depen-
dency parsing could be used to identify the head
and modifiers from every phrase. However, because
queries are typical short and not always well-formed
sentences, this may pose a challenge to the depen-
dency parser. But as we are mainly interested in
short noun phrases, we can apply a simple set of
rules to identify the head keyword of each phrase
using the part of speech tags of the words in the
phrase. A part-of-speech (POS) tagger assigns parts
of speech to each word in an input text, such as noun,
verb, adjective, etc. We used the Stanford POS tag-
ger, using Stanford CoreNLP, to assign POS tags to
queries (Toutanova et al, 2003). To identify the head
and attributes of every noun phrase, we use the fol-
lowing rules:
? For phrases with of the form: ?NNX+? (i.e. one
more nouns, where NNX could be NN: noun,
singular, NNS: noun, plural, NNP: proper
noun, singular or NNPS: proper noun, plural),
the head is the last noun keyword and all other
keywords are treated as attributes/modifiers.
? For the phrases of the form ?NNX+ IN NNX+?,
where IN denotes a preposition or a subordinat-
ing conjunction (e.g. ?in?, ?of?, etc.), the head
is the last noun keyword before the preposition.
Table 1 shows different examples of queries,
the corresponding phrases, keywords, and con-
cepts. For example the query ?kodak easyshare
recharger chord? consists of a single semantic
unit (phrase) and two keywords ?Kodak easyshare?
and ?recharger cord?. The head of this semantic
unit is the keyword ?recharger cord? and ?kodak
easyshare? is regarded as an attribute/modifier. An-
other example is the two queries ?tommy bhama
rug? and ?tommy bhama perfume?. The head of the
former is ?rug?, while the head of the latter is ?per-
fume?. Both share the attribute ?tommy bhama?.
This shows that the user had two different intents
even though most of the words in the two queries
are shared.
4.3 Matching Concepts
Phrases in two concepts may have full term over-
lap, partial term overlap, or no direct overlap yet are
semantically similar. To capture concept similarity,
1004
we define four different ways of matching concepts
ranked from the most to the least strict:
? Exact Match: The head and the attributes of the
two concepts match exactly.
? Approximate Match: To capture spelling vari-
ants and misspelling, we allow two keywords to
match if the Levenshtein edit distance between
them is less than 2.
? Lemma Match: Lemmatization is the process
of reducing an inflected spelling to its lexical
root or lemma form. We match two concepts if
the lemmas of their keywords can be matched.
? Semantic Match: We compute the concept sim-
ilarity by measuring the semantic similarity be-
tween the two phrases from which the concepts
where extracted. Let Q = {q1, ..., qI} be one
phrase and S = {s1, ..., sJ} be another, the
semantic similarity between these two phrases
can be measured by estimating the probabil-
ity of one of them being a translation of an-
other. The translation probabilities can be es-
timated using the IBM Model 1 (Brown et al,
1993; Berger and Lafferty, 1999). The model
was originally proposed to model the probabil-
ity of translating from one sequence of words
in one language to another. It has been also
used in different IR applications to estimate the
probability of translating from one sequence
of words to another sequence in the same lan-
guage (e.g. (Gao et al, 2012), (Gao et al,
2010) and (White et al, 2013)). More for-
mally, the similarity between two sequences of
words, Q = {q1, ..., qI} and S = {s1, ..., sJ},
can be defined as:
P (S|Q) =
I?
i=1
J?
j=1
P (si|qj)P (qj |Q) (2)
where P (q|Q) is the unigram probability of
word q in query Q. The word translation prob-
abilities P (s|q) are estimated using the query-
title pairs derived from the clickthrough search
logs, assuming that the title terms are likely to
be the desired alternation of the paired query.
The word translation probabilities P (s|q) (i.e.
the model parameters ?) are optimized by max-
imizing the probability of generating document
titles from queries over the entire training cor-
pus:
?? = argmax?
M?
i=1
P (S|Q, ?) (3)
where P (S|Q, ?) is defined as:
P (S|Q, ?) =

(J+!)I
I?
i=1
J?
j=1
P (si|qj) (4)
where  is a constant, I is the token length of S,
and J is the token length of Q. The query-title pairs
used for model training are sampled from one year
worth of search logs from a commercial search en-
gine. The search logs do not intersect with the search
logs where the data described in Section5.1 has been
sampled from. S and Q are considered a match if
P (S|Q, ?) > 0.5.
4.4 Features
4.4.1 Textual Features
Jones and Klinkner (2008) showed that word and
character edit features are very useful for identifying
same task queries. The intuition behind this is that
consecutive queries which have many words and/or
characters in common tend to be related. The fea-
tures they used are:
? normalized Levenshtein edit distance
? 1 if lev > 2, 0 otherwise
? Number of characters in common starting from
the left
? Number of characters in common starting from
the right
? Number of words in common starting from the
left
? Number of words in common starting from the
right
? Number of words in common
? Jaccard distance between sets of words
1005
4.4.2 Concept Features
As we explained earlier the word and character
edit features capture similarity between many pairs
of queries. However, they also tend to mis-classify
many other pairs especially when the two queries
share many words yet have different intents. We
used the conceptual representation of queries de-
scribed in the previous subsection to compute the
following set of features, notice that every feature
has two variants one at the concept level and the
other at the keyword (head or attribute) level:
? Number of ?exact match? concepts in common
? Number of ?approximate match? concepts in
common
? Number of ?lemma match? concepts in com-
mon
? Number of ?semantic match? concepts in com-
mon
? Number of concepts in Q1
? Number of concepts in Q2
? Number of concepts in Q1 but not in Q2
? Number of concepts in Q1 but not in Q2
? 1 if Q1 contains all Q2s concepts
? 1 if Q2 contains all Q1s concepts
? all above features recomputed for keywords in-
stead of concepts
4.4.3 Other Features
Other features, that have been also used in (Jones
and Klinkner, 2008), include temporal features:
? time between queries in seconds
? time between queries as a binary feature (5
mins, 10 mins, 20 mins, 30 mins, 60 mins, 120
mins)
and search results feature:
? cosine distance between vectors derived from
the first 10 search results for the query terms.
4.5 Predicting Reformulation Type
There are different strategies users use to reformu-
late a query which results in different types of query
reformulations:
? Generalization: A generalization reformula-
tion occurs when the second query is intended
to seek more general information compared to
the first query
? Specification: A specification reformulation
occurs when the second query is intended to
seek more specific information compared to the
first query
? Spelling: A spelling reformulation occurs
when the second query is intended to correct
one or more misspelled words in the first query
? Same Intent: A same intent reformulation oc-
curs when the second query is intended to ex-
press the same intent as the first query. This
can be the result of word substitution or word
reorder.
We used the following features to predict the
query reformulation type:
? Length (num. characters and num. words) of
Q1, Q2 and difference between them
? Number of out-of-vocabulary words in Q1, Q2
and the difference between them
? num. of ?exact match? concepts in common
? num. of ?approximate match? concepts in com-
mon
? num. of ?lemma match? concepts in common
? num. of ?semantic match? concepts in common
? num. of concepts in Q1, Q2 and the difference
between them
? num. of concepts in Q1 but not in Q2
? num. of concepts in Q1 but not in Q2
? 1 if Q1 contains all Q2s concepts
? 1 if Q2 contains all Q1s concepts
? all concept features above recomputed for key-
words instead of concepts
1006
5 Experiments and Results
5.1 Data
Our data consists of query pairs randomly sampled
from the queries submitted to a commercial search
engine during a week in mid-2012. Every record
in our data consisted of a consecutive query pair
(Qi,Qi+1) submitted to the search engine by the
same user and in the same session (i.e. within less
than 30 minutes of idle time, the 30 minutes thresh-
old has been frequently used in previous work, e.g.
(White and Drucker, 2007)). Identical queries were
excluded from the data because they are always la-
beled as reformulation and their label is very easy to
predict. Hence, when included, they result in unre-
alistically high estimates of the performance of the
proposed methods. All data in the session to which
the sampled query pair belongs were recorded. In
addition to queries, the data contained a timestamp
for each page view, all elements shown in response
to that query (e.g. Web results, answers, etc.), and
visited Web page or clicked answers. Intranet and
secure URL visits were excluded. Any personally
identifiable information was removed from the data
prior to analysis.
Annotators were instructed to exhaustively exam-
ine each session and ?re-enact? the user?s experi-
ence. The annotators inspected the entire search
results page for each of Qi and Qi+1, including
URLs, page titles, relevant snippets, and other fea-
tures. They were also shown clicks to aid them in
their judgments. Additionally, they were also shown
queries and clicks before and after the query pair of
interest. They were asked to then use their assess-
ment of the user?s objectives to determine whether
Qi+1 is a reformulation of Qi. Each query pair was
labeled by three judges and the majority vote among
judges was used. Because the number of positive
instances is much smaller than the number of neg-
ative instances, we used all positive instances and
an equal number of randomly selected negative in-
stances leaving us with approximately 6000 query
pairs.
Judges were also asked to classify reformulations
into one of four different categories: Generalization
(second query is intended to seek more general in-
formation), Specification (second query is intended
to seek more specific information), Spelling (second
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Prec
ision
Recall
HeurisitcTextualConceptsAll
Figure 1 Precision-Recall Curves for the Reformu-
lation Prediction Methods
 
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Same intent Spelling Generalization Specification
Figure 2 Distribution of Query Reformulation
Types
query is intended to correct spelling mistakes), and
Same Intent (second query is intended to express the
same intent in a different way).
5.2 Predicting Query Reformulation
In this section we describe the experiments we con-
ducted to evaluate the reformulation prediction clas-
sifier. We perform experiments using the data de-
scribed in the previous section. We compare the per-
formance of four different systems:
? The first one, Heuristic, simply computes the
similarity between two queries as the percent-
age of common words to the length of the
longer query in terms of the number of words.
1007
Table 2 : Heuristics vs. Textual vs. Concept Features for Reformulation Prediction
Accuracy Reform. F1 No-Reform. F1
Heuristics 77.10% 75.60% 69.07%
Textual 82.90% 71.75% 87.75%
Concepts 87.60% 81.20% 90.78%
All 89.02% 83.63% 91.75%
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Same intent Spelling Generalization Specification
Precision Recall
Figure 3 Precision and Recall for Query Reformu-
lation Type Prediction
When finding common words, it allows two
words to be matched if their Levenshtein edit
distance is less than or equals 2. The second
query is predicted to be a reformulation of the
first if similarity? ?sim and the time difference
? ?time minutes. The two thresholds were set
to 0.35 and 5 minutes respectively using grid
search to maximize accuracy over the training
data.
? The second system, Textual, uses the textual
features from previous work that have been de-
scribed in Section 4.4.1 and the temporal and
results features described in Section 4.4.3.
? The third system, Concepts, uses the concept
features that we presented in Section 4.4.2 and
the temporal and results features described in
Section 4.4.3.
? Finally, the last system, All, uses both the tex-
tual features, the conceptual features and the
temporal and results features.
For all methods, we used gradient boosted regres-
sion trees as a classifier with 10-fold cross valida-
tion. We also tried other classifiers like SVM and
logistic regression but we got the best performance
using the gradient boosted regression trees. All re-
ported differences are statistically significant at the
0.05 level according to a two-tailed student t-test.
The accuracy, positive (reformulation) F1, and
negative (non-reformulation) F1 for the four meth-
ods are shown in Table 2. The precision recall
curves for all methods are shown in Figure 1; the
heuristic method uses fixed thresholds resulting in
a single operating point. The results show that
the concept features outperform the textual features.
Combining them together results in a small gain
over using the concept features only. The concept
features were able to achieve higher precision rates
while not sacrificing recall because they were more
effective in eliminating false reformulation cases.
We examined the cases where the classifier failed
to predict the correct label to understand when the
classifier fails to work properly. We identified sev-
eral cases where this happens. For example, the
classifier failed to match some terms that have the
same semantic meaning. Many of these cases were
acronyms (e.g. ?AR? and ?Accelerated Reader?,
?GE? and ?General Electric?). These cases can
be handled by using a semantic matching method
that yields higher coverage especially in cases of
acronyms.
The classifier also failed in cases where the key-
word extractor and/or the POS tagger failed to cor-
rectly parse the queries (e.g. ?last to know?was
not recognized as a song name). These cases can
be handled by identifying named entities as a pre-
processing step and treating them accordingly when
identifying keywords or assigning POS tags to key-
words.
1008
Another dominant class of cases where the classi-
fier failed were cases where the dependency rules
failed to correctly identify the head keyword in a
query. In many such cases, the query was a non well-
formed sequence of words (e.g. ?dresses Christmas
toddler?). This is the hardest class to handle. Since
it is hard to correctly parse short text and it is even
harder when the text it is not well-formed.
5.3 Predicting Reformulation Type
We conducted another experiment to evaluate the
performance of the reformulation type classifier. We
performed experiments using the data described ear-
lier where judges were asked to select the type of
reformulation for every reformulation query. The
distribution of reformulations across types is shown
in Figure 2. The figure shows that most popular re-
formulations types are those where users move to
a more specific intent or express the same intent in
a different way. Reformulations with spelling sug-
gestions and query generalizations are less popular.
We conducted a one-vs-all experiment using gradi-
ent boosted regression trees with 10-fold cross val-
idation. The precision and recall of every type are
shown in Figure 3. The micro-averaged and macro-
averaged accuracy was 78.13% and 72.52% respec-
tively.
6 Conclusions
Identifying query reformulations is an interesting
and useful application in Information Retrieval. Re-
formulation identification is useful for automatic
query refinements, task boundary identification and
satisfaction prediction. Previous work on this prob-
lem has adopted a bag-of-words approach where
lexical similarity and word overlap are the key fea-
tures for identifying query reformulation. We pro-
posed a method for identifying concepts in search
queries and using them to identify query reformula-
tions. The proposed method outperforms previous
work because it can better represent the information
intent underlying the query and hence can better as-
sess query similarity. We showed that the proposed
method significantly outperforms the other methods.
We also showed that we can reliably predict the type
of the reformulation with high accuracy.
References
Peter Anick. 2003. Learning noun phrase query segmen-
tation. In Proceedings of the 26th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 88?95.
M. Arlitt. 2000. Characterizing web user sessions. ACM
SIGMETRICS Performance Eval Review, 28(2):50?
63.
Ricardo Baeza-Yates, Carlos Hurtado, Marcelo Men-
doza, and Georges Dupret. 2005. Modeling user
search behavior. In LA-WEB ?05: Proceedings of the
Third Latin American Web Congress, Washington, DC,
USA. IEEE Computer Society.
A. L. Berger and J. Lafferty. 1999. Information retrieval
as statistical translation. In Proceedings of the 22th
annual international ACM SIGIR conference on Re-
search and development in information retrieval (SI-
GIR 1999), pages 222?229.
S. Bergsma and I. Q. Wang. 2007. Learning noun phrase
query segmentation. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 816?826.
Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-
ora Donato, Aristides Gionis, and Sebastiano Vigna.
2008. The query-flow graph: model and applications.
In Proceeding of the 17th ACM conference on In-
formation and knowledge management (CIKM 2008),
pages 609?618.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
Doug Downey, Susan Dumais, and Eric Horvitz. 2007.
Models of searching and browsing: Languages, stud-
ies, and applications. Journal of the American Soci-
ety for Information Science and Technology (JASIST),
58(6):862?871.
J. Gao, X. He, and J. Nie. 2010. Clickthrough-based
translation models for web search: from word mod-
els to phrase models. In Proceeding of the ACM con-
ference on Information and knowledge management
(CIKM 2010), pages 1139?1148.
J. Gao, S. Xie, X. He, and A. Ali. 2012. Learning lexicon
models from search logs for query expansion. In Pro-
ceeding of the Conference on Emprical Methods for
Natural Language Processing (EMNLP 2012).
J. Guo, G. Xu, H. Li, and X. Cheng. 2008. A unified and
discriminative model for query refinement. In Pro-
ceedings of the annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 379?386.
M. Hagen, M. Potthast, B. Stein, and C. Brautigam.
2010. The power of naiv query segmentation. In Pro-
ceeding of the ACM Conference of the Special Interest
1009
Group on Information Retrieval (SIGIR 2010), pages
797?798,.
M. Hagen, M. Potthast, B. Stein, and C. Brautigam.
2011. Query segmentation revisited. In Proceeding of
the ACM World Wide Web Conference (WWW 2011),
pages 97?106.
J. Huang, J. Gao, J. Miao, X. Li, K. Wang, and F. Behr.
2010. Exploring web scale language models for search
query processing. In Proceeding of the ACM World
Wide Web Conference (WWW 2010), pages 451?460.
Bernard J. Jansen and Amanda Spink. 2006. How are we
searching the world wide web?: a comparison of nine
search engine transaction logs. Inf. Process. Manage.,
42:248?263, January.
B. J. Jansen, A. Spink, and J. Pedersen. 2005. A tem-
poral comparison of altavista web searching. Journal
of the American Society for Information Science and
Technology, 56:559?570.
B. J. Jansen, M. Zhang, and A. Spink. 2007. Patterns and
transitions of query reformulation during web search-
ing. International Journal of Web Information Sys-
tems.
Rosie Jones and Kristina Klinkner. 2008. Beyond the
session timeout: Automatic hierarchical segmentation
of search topics in query logs. In Proceedings of ACM
17th Conference on Information and Knowledge Man-
agement (CIKM 2008).
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substititions. In Pro-
ceedings of the Fifteenth International Conference on
the World-Wide Web (WWW06), pages 387?396.
Tessa Lau and Eric Horvitz. 1999. Patterns of search:
Analyzing and modeling web query refinement. In
ACM Press, editor, Proceedings of the Seventh Inter-
national Conference on User Modeling.
C. Lucchese, S. Orlando, R. Perego, F. Silvestri, and
G. Tolomei. 20011. Identifying task-based sessions
in search engine query logs. In Proceedings of ACM
Conference on Web Search and Data Mining(WSDM
2011).
Q. Mei, D. Zhou, and K. Church. 2008. Query sug-
gestion using hitting time. In Proceeding of the 17th
ACM conference on Information and knowledge man-
agement (CIKM 2008), pages 469?478.
M. Mitra, A. Singhal, and C. Buckley. 1998. Improv-
ing automatic query expansion. In Proceedings of
the 21th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 206?214.
G. V. Murray, J. Lin, and A. Chowdhury. 2006. Identifi-
cation of user sessions with hierarchical agglomerative
clustering. ASIST, 43(1):934?950.
Seda Ozmutlu. 2006. Automatic new topic identification
using multiple linear regression. Information Process-
ing and Management, 42(4):934?950.
Filip Radlinski and Thorsten Joachims. 2005. Query
chains: learning to rank from implicit feedback. In
Robert Grossman, Roberto Bayardo, and Kristin P.
Bennett, editors, KDD, pages 239?248. ACM.
Jamie Teevan, Eytan Adar, Rosie Jones, and Michael
Potts. 2006. History repeats itself: Repeat queries
in yahoo?s logs. In Proceedings of the 29th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 703?704.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceeding of the Hu-
man Language Technologies Conference and the An-
nual Meeting of the North American Association of
Computational Linguists (HLT-NAACL 2003), pages
252?259.
K. Wang, C. Thrasher, and B. Hsu. 2011. Web scale nlp:
A case study on url word breaking. In Proceeding of
the ACM World Wide Web Conference (WWW 2011),
pages 357?366.
Ryen W. White and Steven M. Drucker. 2007. Inves-
tigating behavioral variability in web search. In Pro-
ceedings of the 16th international conference on World
Wide Web.
Ryen W. White, Wei Chu, Ahmed Hassan, Xiaodong He,
Yang Song, and Hongning Wang. 2013. Enhancing
personalized search by mining and modeling task be-
havior. In Proceedings of the 22nd international con-
ference on World Wide Web, WWW ?13, pages 1411?
1420.
X. Yu and H. Shi. 2009. Query segmentation using con-
ditional random fields. In Proceedings of the Work-
shop on Keyword Search on Structured Data (KEYS),
pages 21?26.
1010
A Random Walk?Based Model for
Identifying Semantic Orientation
Ahmed Hassan?
Microsoft Research
Amjad Abu-Jbara??
University of Michigan
Wanchen Lu?
University of Michigan
Dragomir Radev?
University of Michigan
Automatically identifying the sentiment polarity of words is a very important task that has
been used as the essential building block of many natural language processing systems such as
text classification, text filtering, product review analysis, survey response analysis, and on-line
discussion mining. We propose a method for identifying the sentiment polarity of words that
applies a Markov random walk model to a large word relatedness graph, and produces a polarity
estimate for any given word. The model can accurately and quickly assign a polarity sign and
magnitude to any word. It can be used both in a semi-supervised setting where a training set of
labeled words is used, and in a weakly supervised setting where only a handful of seed words is
used to define the two polarity classes. The method is experimentally tested using a gold standard
set of positive and negative words from the General Inquirer lexicon. We also show how our
method can be used for three-way classification which identifies neutral words in addition to
positive and negative words. Our experiments show that the proposed method outperforms the
state-of-the-art methods in the semi-supervised setting and is comparable to the best reported
values in the weakly supervised setting. In addition, the proposed method is faster and does not
need a large corpus. We also present extensions of our methods for identifying the polarity of
foreign words and out-of-vocabulary words.
? Microsoft Research, Redmond, WA, USA. E-mail: hassanam@microsoft.com. This research was
performed while at the University of Michigan.
?? Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.
E-mail: amjbara@umich.edu.
? Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.
E-mail: wanchlu@umich.edu.
? Department of Electrical Engineering & Computer Science and School of Information, University of
Michigan, Ann Arbor, MI, USA. E-mail: radev@umich.edu.
Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication:
14 July 2013.
doi:10.1162/COLI a 00192
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Identifying emotions and attitudes from unstructured text has a variety of possible
applications. For example, there has been a large body of work for mining product
reputation on the Web (Morinaga et al. 2002; Turney 2002). Morinaga et al. (2002)
have shown how product reputation mining helps with marketing and customer re-
lation management. The Google products catalog and many on-line shopping sites
like Amazon.com provide customers not only with comprehensive information and
reviews about a product, but also with faceted sentiment summaries. Such systems are
all supported by a sentiment lexicon, some even in multiple languages.
Another interesting application is mining on-line discussions. An enormous num-
ber of discussion groups exist on the Web. Millions of users post content to these groups
covering pretty much every possible topic. Tracking a participant attitude toward differ-
ent topics and toward other participants is a very important task that makes use of sen-
timent lexicons. For example, Tong (2001) presented the concept of sentiment timelines.
His system classifies discussion posts about movies as either positive or negative. This
is used to produce a plot of the number of positive and negative sentiment messages
over time. All these applications would benefit from an automatic way of identifying
semantic orientation of words.
In this article, we study the task of automatically identifying the semantic orienta-
tion of any word by analyzing its relations to other words, Automatically classifying
words as positive, negative, or neutral enables us to automatically identify the polarity
of larger pieces of text. This could be a very useful building block for systems that
mine surveys, product reviews, and on-line discussions. We apply a Markov random
walk model to a large semantic relatedness graph, producing a polarity estimate for
any given word. Previous work on identifying the semantic orientation of words has
addressed the problem as both a semi-supervised (Takamura, Inui, and Okumura 2005)
and a weakly supervised (Turney and Littman 2003) learning problem. In the semi-
supervised setting, a training set of labeled words is used to train the model. In the
weakly supervised setting, only a handful of seeds are used to define the two polarity
classes.
Our proposed method can be used both in a semi-supervised and in a weakly
supervised setting. Empirical experiments on a labeled set of positive and negative
words show that the proposed method outperforms the state-of-the-art methods in the
semi-supervised setting. The results in the weakly supervised setting are comparable to
the best reported values. The proposed method has the advantages that it is faster and
does not need a large training corpus.
The rest of the article is structured as follows. In Section 2, we review related work
on word polarity and subjectivity classification and note applications of the random
walk and hitting times framework. Section 3 presents our method for identifying word
polarity. We describe how the proposed method can be extended to cover foreign
languages in Section 4, and out-of-vocabulary words in Section 5. Section 6 describes
our experimental set-up. We present our conclusions in Section 7.
2. Related Work
2.1 Identifying Word Polarity
Hatzivassiloglou and McKeown (1997) proposed a method for identifying the word
polarity of adjectives. They extract all conjunctions of adjectives from a given corpus
540
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
and then they classify each conjunctive expression as either the same orientation such
as ?simple and well-received? or different orientation such as ?simplistic but well-
received.? The result is a graph that they cluster into two subsets of adjectives. They
classify the cluster with the higher average frequency as positive. They created and
labeled their own data set for experiments. Their approach works only with adjectives
because there is nothing wrong with conjunctions of nouns or verbs with opposite
polarities (?war and peace?, ?rise and fall?, etc.).
Turney and Littman (2003) identify word polarity by looking at its statistical asso-
ciation with a set of positive/negative seed words. They use two statistical measures
for estimating association: Pointwise Mutual Information (PMI) and Latent Semantic
Analysis (LSA). To get co-occurrence statistics, they submit several queries to a search
engine. Each query consists of the given word and one of the seed words. They use the
search engine NEAR operator to look for instances where the given word is physically
close to the seed word in the returned document. They present their method as an un-
supervised method where a very small number of seed words are used to define
semantic orientation rather than train the model. One of the limitations of their method
is that it requires a large corpus of text to achieve good performance. They use sev-
eral corpora; the size of the best performing data set is roughly one hundred billion
words (Turney and Littman 2003).
Takamura et al. (2005) propose using spin models for extracting semantic orienta-
tion of words. They construct a network of words using gloss definitions, thesaurus, and
co-occurrence statistics. They regard each word as an electron. Each electron has a spin
and each spin has a direction taking one of two values: up or down. Two neighboring
spins tend to have the same orientation from an energy point of view. Their hypothesis
is that as neighboring electrons tend to have the same spin direction, neighboring words
tend to have similar polarity. They pose the problem as an optimization problem and
use the mean field method to find the best solution. The analogy with electrons leads
them to assume that each word should be either positive or negative. This assumption
is not accurate because most of the words in the language do not have any semantic ori-
entation. They report that their method could get misled by noise in the gloss definition
and their computations sometimes get trapped in a local optimum because of its greedy
optimization flavor.
Kamps et al. (2004) construct a network based on WordNet (Miller 1995) synonyms
and then use the shortest paths between any given word and the words ?good? and
?bad? to determine word polarity. They report that using shortest paths could be very
noisy. For example, ?good? and ?bad? themselves are closely related in WordNet with
a 5-long sequence ?good, sound, heavy, big, bad.? A given word w may be more
connected to one set of words (e.g., positive words); yet have a shorter path connecting
it to one word in the other set. Restricting seed words to only two words affects their
accuracy. Adding more seed words could help but it will make their method extremely
costly from the computation point of view. They evaluate their method using only
adjectives.
Hu and Liu (2004) propose another method that uses WordNet. They use WordNet
synonyms and antonyms to predict the polarity of words. For any word whose polarity
is unknown, they search WordNet and a list of seed labeled words to predict its polarity.
They check if any of the synonyms of the given word has known polarity. If so, they
label it with the label of its synonym. Otherwise, they check if any of the antonyms
of the given word has known polarity. If so, they label it with the opposite label of
the antonym. They continue in a bootstrapping manner until they label all possible
words.
541
Computational Linguistics Volume 40, Number 3
2.2 Building Sentiment Lexicons
A number of other methods try to build lexicons of polarized words. Esuli and
Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses
of the word as found in some dictionary. Then, a binary text classifier is trained using
the textual representation and applied to new words.
Kim and Hovy (2004) start with two lists of positive and negative seed words. Word-
Net is used to expand these lists. Synonyms of positive words and antonyms of negative
words are considered positive, and synonyms of negative words and antonyms of posi-
tive words are considered negative. A similar method is presented in Andreevskaia and
Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively
expand a list of seeds. The sentiment classes are treated as fuzzy categories where some
words are very central to one category, whereas others may be interpreted differently.
Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states that
overtly marked words such as dishonest, unhappy, and impure tend to have negative
semantic orientations whereas their unmarked counterparts (honest, happy, and pure)
tend to have positive semantic orientation. They use a set of 11 antonym-generating affix
patterns to generate overtly marked words and their counterparts from the Macquarie
Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the
sentiment lexicon using a Roget-like thesaurus. Their method does not require seed
sentiment words or WordNet, but still needs a comprehensive thesaurus. The idea of
the marking theory is language-dependent and cannot be applied from one language to
another.
Contrasting the dictionary based approaches that rely on resources such as Word-
Net, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons
semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic fea-
tures and context coherency (i.e., the tendency for same polarities to appear succes-
sively) to detect polar clauses.
2.3 Random Walk?Based Methods
Closest to our work in its methodology is probably the line of research on semi-
supervised graphical methods for sentiment classification. Rao and Ravichandran
(2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled
and labeled nodes, each node representing a word that can be either positive or neg-
ative, and each edge representing some semantic relatedness that can be constructed
using resources like WordNet or other thesaurus. They evaluate two semi-supervised
methods: Mincut (including its variant, Randomized Mincut) and label propagation.
The general idea of label propagation is defining a probability distribution over the
positive and negative classes for each node in the graph. A Markov random walk is
performed on the graph to recover this distribution for the unlabeled nodes.
Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al. (2008) use a
similar label propagation method on a lexical graph built from WordNet, where a small
set of words with known polarities are used as seeds. Brody and Elhadad (2010) use
label propagation over a graph constructed of adjectives only.
Velikovich et al. (2010) compare label propagation with a Web-based method and
conclude that label propagation is not suitable when the whole Web is used as a
background corpus, because the constructed graph is very noisy and contains many
dense subgraphs, unlike the lexical graph constructed from WordNet.
542
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Random walk?based methods have been studied in the context of many other NLP
tasks. For example, Kok and Brockett (2010) construct a graph from bilingual parallel
corpora, where each node represents a phrase and two nodes are connected by an edge
if they are aligned in a phrase table. Then they compute hitting time of random walks
to learn paraphrases.
Our work is different from previous random walk methods in that it uses the mean
hitting time as the criterion for assigning polarity labels. Our experiments showed that
this achieves better results than methods that use label propagation.
2.4 Subjectivity Analysis
Subjectivity analysis is another research line that is closely related to our work. The
main task in subjectivity analysis is to identify text that presents opinion as opposed to
objective text that present factual information (Wiebe 2000). Text could be either words,
phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of
subjectivity analysis such as classifying e-mails and mining reviews. For example, to
analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from
individual sentences as nodes to determine whether a sentence is subjective or objective.
Each node (sentence) has an individual subjectivity score obtained from a first-pass
classifier using sentence features and linguistic knowledge. Edges are weighted by a
similarity metric of how likely it is that the two sentences will be in the same subjectivity
class. All sentences to be classified are represented as unlabeled nodes and the only two
labeled nodes represent the subjective and objective classes. A Mincut algorithm is then
performed on the constructed graph to obtain the subjectivity classes for individual
sentences. The authors also integrate the subjectivity classification of isolated sentences
to document level sentiment analysis.
There are two main categories of work on subjectivity analysis. In the first cate-
gory, subjective words and phrases are identified without considering their context
(Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In
the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff
and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and
Mihalcea (2006a) studied the association of word subjectivity and word sense. They
showed that different subjectivity labels can be assigned to different senses of the same
word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles
from a wide variety of news sources manually annotated for opinions and other private
states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions
and emotions in language.
In addition, there has been a large body of work on labeling subjectivity of WordNet
words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or ob-
jective, utilizing the MPQA corpus. They show that subjectivity information for Word-
Net senses can improve word sense disambiguation tasks for subjectivity ambiguous
words.
Su and Markert (2009) propose a semi-supervised minimum cut framework to label
word sense entries in WordNet with subjectivity information. Their method requires
less training data other than the sense definitions and relational structure of WordNet.
2.5 Word Polarity Classification for Foreign Languages
Word sentiment and subjectivity has also been studied for languages other than English.
Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity
543
Computational Linguistics Volume 40, Number 3
lexicon based on an English lexicon, an on-line translation service, and Wordnet.
Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or a
parallel corpus to generate subjectivity analysis resources for foreign languages. Rao
and Ravichandran (2009) adapt their label propagation model to Hindi using Hindi
WordNet and French using a French thesaurus.
3. Approach
We use a Markov random walk model to identify the polarity of words. Assume that
we have a network of words, some of which are labeled as either positive or nega-
tive. In this network, two words are connected if they are related. Different sources
of information are used to decide whether two words are related. For example, the
synonyms of a word are all semantically related to it. The intuition behind connect-
ing semantically related words is that those words tend to have similar polarities.
Now imagine a random surfer walking along the network starting from an unlabeled
word w.
The random walk continues until the surfer hits a labeled word. If the word w is
positive then the probability that the random walk hits a positive word is higher, and if
w is negative then the probability that the random walk hits a negative word is higher.
Thus, if the word w is positive then the average time it takes a random walk starting at
w to hit a positive node should be much less than the average time it takes a random
walk starting at w to hit a negative node. If w doesn?t have a clear polarity and we would
like to say that it is neutral, we expect that the positive hitting time and negative hitting
time to not have a significant difference.
We describe how we construct a word relatedness graph in Section 3.1. The random
walk model is described in Section 3.2. Hitting time is defined in Section 3.3. Finally,
an algorithm for computing a sign and magnitude for the polarity of any given word
is described in Section 3.4.
3.1 Network Construction
We construct a network where two nodes are linked if they are semantically related.
Several sources of information are used as indicators of the relatedness of words. One
such source is WordNet (Miller 1995). WordNet is a large lexical database of English.
Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms
(synsets), each expressing a distinct concept (Miller 1995). Synsets are interlinked by
means of conceptual-semantic and lexical relations.
The simplest approach is to connect words that occur in the same WordNet synset.
We can collect all words in WordNet, and add links between any two words that
occur in the same synset. The resulting graph is a graph G(W, E) where W is a set
of word/part-of-speech (POS) pairs for all the words in WordNet. E is the set of
edges connecting each pair of synonymous words. Nodes represent word/POS pairs
rather than words because the part of speech tags are helpful in disambiguating
the different senses for a given word. For example, the word ?fine? has two dif-
ferent meanings, with two opposite polarities when used as an adjective and as a
noun.
Several other methods can be used to link words. For example, we can use other
WordNet relations: hypernyms, similar to, and so forth. Another source of links be-
tween words is co-occurrence statistics from a corpus. Following the method presented
544
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
in Hatzivassiloglou and McKeown (1997), we can connect words if they appear together
in a conjunction in the corpus. This method is only applicable to adjectives. If two
adjectives are connected by ?and,? it is highly likely that they have the same semantic
orientation. In all our experiments, we restricted the network to only WordNet relations.
We study the effect of using co-occurrence statistics to connect words later at the end of
our experiments. If more than one relation exists between any two words, the strength
of the corresponding edge is adjusted accordingly.
3.2 Random Walk Model
Imagine a random surfer walking along the word relatedness graph G. Starting from a
word with unknown polarity i, it moves to a node j with probability Pij after the first
step. The walk continues until the surfer hits a word with known polarity. Seed words
with known polarity act as an absorbing boundary for the random walk. If we repeat
the number of random walks N times, the percentage of times in which the walk ends at
a positive/negative word could be used as an indicator of its positive/negative polarity.
The average time a random walk starting at w takes to hit the set of positive/negative
nodes is also an indicator of its polarity. This view is closely related to the partially la-
beled classification with random walks approach in Szummer and Jaakkola (2002) and
the semi-supervised learning using harmonic functions approach in Zhu, Ghahramani,
and Lafferty (2003).
Let W be the set of words in our lexicon. We construct a graph whose nodes V are
all words in W. Edges E correspond to the relatedness between words. We define the
transition probability Pt+1|t( j|i) from i to j by normalizing the weights of the edges out
of node i, so:
Pt+1|t( j|i) = Wij/
?
k
Wik (1)
where k represents all nodes in the neighborhood of i. Pt+1|t( j|i) denotes the transition
probability from node i at step t to node j at time step t + 1. We note that the matrix of
weights Wij is symmetric whereas the matrix of transition probabilities Pt+1|t( j|i) is not
necessarily symmetric because of the node outdegree normalization.
3.3 First-Passage Time
The mean first-passage (hitting) time h(i|k) is defined as the average number of steps a
random walker, starting in state i 6= k, will take to enter state k for the first time (Norris
1997). Let G = (V, E) be a graph with a set of vertices V and a set of edges E. Consider
a subset of vertices S ? V. Consider a random walk on G starting at node i 6? S. Let Nt
denote the position of the random surfer at time t. Let h(i|S) be the average number of
steps a random walker, starting in state i 6? S, will take to enter a state k ? S for the first
time. Let TS be the first-passage for any vertex in S.
P(TS = t|N0 = i) =
?
j?V
pij ? P(TS = t? 1|N0 = j) (2)
545
Computational Linguistics Volume 40, Number 3
h(i|S) is the expectation of TS. Hence:
h(i|S) = E(TS|N0 = i)
=
??
t=1
t? P(TS = t|N0 = i)
=
??
t=1
t
?
j?V
pijP(TS = t? 1|N0 = j)
=
?
j?V
??
t=1
(t? 1)pijP(TS = t? 1|N0 = j)
+
?
j?V
??
t=1
pijP(TS = t? 1|N0 = j)
=
?
j?V
pij
??
t=1
tP(TS = t|N0 = j) + 1
=
?
j?V
pij ? h( j|S) + 1 (3)
Hence the first-passage (hitting) time can be formally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h( j|S) + 1 otherwise
(4)
3.4 Word Polarity Calculation
Based on the description of the random walk model and the first-passage (hitting)
time above, we now propose our word polarity identification algorithm. We begin by
constructing a word relatedness graph and defining a random walk on that graph as
described above. Let S+ and S? be two sets of vertices representing seed words that are
already labeled as either positive or negative, respectively.
For any given word w, we compute the hitting time h(w|S+) and h(w|S?) for the
two sets iteratively as described earlier. The ratio between the two hitting times is then
used as an indication of how positive/negative the given word is. This is useful in
case we need to provide a confidence measure for the prediction. This could be used
to allow the model to abstain from classifying words when the confidence level is low.
It also means that our method can be easily extended from two-way classification (i.e.,
positive or negative) to three-way classification (positive, negative, or neutral). This can
be done by setting a threshold ? on the ratio of positive and negative hitting time,
and classifying a word to positive or negative only when the two hitting times have
a significant difference; otherwise we classify it to neutral.
When the relatedness graph is very large, computing hitting time as described
earlier may be very time consuming. The graph constructed from the English WordNet
546
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Algorithm 1 3-class word polarity using random walks (parameter ? : 0 < ? < 1)
Require: A word relatedness graph G
1: Given a word w in V
2: Define a random walk on the graph. The transition probability between any two
nodes i, and j is defined as: Pt+1|t( j|i) = Wij/
?
k Wik
3: Start k independent random walks from w with a maximum number of steps m
4: Stop when a positive word is reached
5: Let h?(w|S+) be the estimated value for h(w|S+)
6: Repeat for negative words computing h?(w|S?)
7: if h?(w|S+) ? ?h?(w|S?) then
8: Classify w as positive
9: else if h?(w|S?) ? ?h?(w|S+) then
10: Classify w as negative
11: else
12: Classify w as neutral
13: end if
and synsets contains 155,000 nodes and 117,000 edges. To overcome this problem, we
propose a Monte Carlo?based algorithm (Algorithm 1) for estimating it.
In the case of binary classification, where each word must be either positive or
negative, if h(w|S+) is greater than h(w|S?), the word is classified as negative and
positive otherwise. This can be achieved by setting parameter ? = 1 in Algorithm 1.
4. Foreign Word Polarity
As we mentioned earlier, a large body of research has focused on identifying the
semantic orientation of words. This work has almost exclusively dealt with English and
uses several language-dependent resources. When we try to apply these methods to
other languages, we run into the problem of the lack of resources in other languages
when compared with English. For example, the General Inquirer lexicon (Stone et al.
1966) has thousands of English words labeled with semantic orientation. Most of the
literature has used it as a source of labeled seeds or for evaluation. Such lexicons are
not readily available in other languages.
As we showed earlier, WordNet (Miller 1995) has been used for this task. How-
ever, even though W have been built for other languages, their coverage is relatively
limited when compared to the English WordNet. The current release of English Word-
Net (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the
resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al.
2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan
et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in
Spanish, 15K in German, and 22K in French, among other European languages. In some
cases, accuracy was traded for coverage. For example, the current release of the Japanese
WordNet has 57K synsets but contains errors in as many as 5% of the entries.1
In this section, we show how we can extend the methods presented earlier to predict
the semantic orientation of foreign words. The proposed method is based on creating
1 http://nlpwww.nict.go.jp/wn-ja/index.en.html.
547
Computational Linguistics Volume 40, Number 3
a multilingual network of words that represents both English and foreign words. The
network has English?English connections, as well as Foreign?Foreign connections and
English?Foreign connections. This allows us to benefit from the richness of the resources
built for the English language and at the same time utilize resources specific to foreign
languages. We define a random walk model over the multilingual network and pre-
dict the semantic orientation of any given word by comparing the mean hitting time
of a random walk starting from it to a positive and a negative set of seed English
words.
We use Arabic and Hindi in our experiments. We compare the performance of sev-
eral methods using the foreign language resources only, and the multilingual network
that has both English and foreign words. We show that bootstrapping from languages
with dense resources such as English is useful for improving the performance on other
languages with limited resources.
4.1 Multilingual Word Network
We build a network G(V, E) where V = Ven ? Vfr is the union of the sets of English and
Foreign words. E is a set of edges connecting nodes in V. There are three types of connec-
tions: English?English connections, Foreign?Foreign connections, and English?Foreign
connections. For the English?English connections, we use the same methodology as in
Section 3.
Foreign?Foreign connections are created in a similar way to the English con-
nections. Some foreign languages have lexical resources based on the design of the
Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic Word-
Net (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al.
2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of
Hatzivassiloglou and McKeown (1997).
Finally, to connect foreign words to English words, we use a Foreign to English dic-
tionary. For every word in a list of foreign words, we look up its meaning in a dictionary
and add an edge between the foreign word and every other English word that appeared
as a possible meaning for it. If there is no comprehensive enough dictionary available,
constructing a multilingual word network like a translation graph (Etzioni et al. 2007)
may be a resolution.
4.2 Foreign Word Semantic Orientation Prediction
We use the multilingual network described previously to predict the semantic orien-
tation of words based on the mean hitting time to two sets of positive and negative
seeds. Given two lists of seed English words with known polarity, we define two sets
of nodes S+ and S? representing those seeds. For any given word w, we calculate the
mean hitting time between w and the two seed sets h(w|S+) and h(w|S?). If h(w|S+)
is greater than h(w|S?), the word is classified as negative; otherwise it is classified as
positive. We used the list of labeled seeds from Hatzivassiloglou and McKeown (1997)
and Stone et al. (1966).
5. Out-of-Vocabulary Words
We observed that a significant portion of the text used on-line in discussions, comments,
product reviews, and so on, contains words that are not defined in WordNet or in
548
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
standard dictionaries. We call these words Out-of-Vocabulary (OOV) words. Table 6
later in this article shows some OOV word examples. To show the importance of OOV
word polarity identification, we calculated the proportion of OOV words in three
corpora used for sentiment studies: a set of movie reviews, a set of on-line discussions
from a political forum, and a set of randomly sampled tweets. For each word in the
data, we look it up in two standard English dictionaries, together containing 160K
unique words. Table 1 shows the statistics.
OOV words have a high chance of being polarized because people tend to use
informal language or special acronyms to emphasize their attitudes or impress the
audience. Therefore, being able to automatically identify the polarity of OOV words
will essentially benefit real-world applications.
Consider the graph G(W, E) described in Section 3.1. So far, the only resource we use
to construct the graph is WordNet synsets. The first step in our approach to OOV word
polarity identification is to find the words in WordNet that are related to an OOV
word. Next, we add the OOV words to our graph by creating a new node for each OOV
word and adding an edge between each OOV word and each of its related words. Once
we have constructed the extended network, we use the random walk model described
in Section 3.2 to predict the polarity of each OOV word.
5.1 Mining OOV Word Relatedness from the Web
There are several alternative methods of linking words in the graph. Agirre et al. (2009)
studied the strengths and weaknesses of different approaches to term similarity and
relatedness. They noticed that lexicographical methods such as the WordNet suffer from
the limitation of lexicon coverage, which is the case here with OOV words. To overcome
this limitation, we use a Web-based distributional approach to find the set of related
words to each OOV word. We perform a Web search using the OOV word as a search
query and retrieve the top S search results. We extract the textual content of the retrieved
results and tokenize it. After removing all the stop words, we compute the number of
times each word co-occurs with the OOV word in the same document. We rank the
words based on their co-occurrence frequency and return the top R words as the set of
related words to the given OOV word.
We experimented with three different variants of this approach. In the first variant,
the frequency values of the co-occurring words are normalized by the lengths of the
Table 1
Proportion of OOV words in some corpora used for real world applications. (Numbers in
parentheses exclude words whose first letters are capitalized because they are likely to refer to
named entities.)
corpus source # of words Percentage
of OOV
Movie reviews 3, 411 customer reviews from IMDb for the
movie The Dark Knight (2008)
10.7 M (9.5 M) 5.3 (2.7)
Political forum 23K sentences from www.politicalforum.com
on various topics
381 K (348 K) 8 (6)
tweets 0.6M random English tweets from twitter.com.
(We count a tweet as in English if at least half
of the words are English dictionary words.
Tags and symbols were removed.)
7.1 M (5.9 M) 30 (27)
549
Computational Linguistics Volume 40, Number 3
documents that contributed to the count of each word. The intuition here is that longer
documents contain more words and hence the probability that a word in the that
document is related to the search query (i.e., the OOV word) is lower than when the
document is shorter.
In the second variant, we only consider the words that appear in the proximity of
the OOV word (i.e., within d words around the OOV word) when we compute the co-
occurrence frequency. The intuition here is that words that appear near the OOV word
are more likely to be semantically related than the words that appear far away.
In the third variant, instead of searching the entire Web, we limit the search to
social text. In the experiments described subsequently, we search for the OOV words
in tweets posted on Twitter.2 The intuition here is that searching the entire Web is likely
to return results that do not necessarily contain opinionated text?particularly because
many words have different senses. In contrast, the text written in a social context is more
likely to carry sentiment and express emotions. This helps us find better related words
that suit our task.
5.2 Word Network Extension with OOV Words
To extend the graph to include OOV words, we start with the graph G(W, E) constructed
from WordNet synsets. For each OOV word that does not exist in G, we create a new
node w. We set the part of speech of w to unspecified. Then we use the Web-based method
described in the previous section to find a set of words that are most related to w. Finally,
we create a link between each OOV word and each of its related words. To predict the
polarity of an OOV word, we use the same random walk model described earlier.
6. Experiments
We performed experiments on the gold-standard data set for positive/negative words
from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4, 206 words,
1, 915 of which are positive and 2, 291 of which are negative. Some of the ambiguous
words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005).
Some examples of positive/negative words are listed in Table 2.
We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the
word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to
generate co-occurrence statistics in the experiments that used them. We used 10-fold
cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical
significance was tested using a two-tailed paired t-test. All reported results are statisti-
cally significant at the 0.05 level. We perform experiments varying the parameters and
the network. We also look at the performance of the proposed method for different
parts of speech, and for different confidence levels. We compare our method to the
Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin
model described in Takamura, Inui, and Okumura (2005), the shortest path method
described in Kamps et al. (2004), a re-implementation of the label propagation and
Mincut methods described in Rao and Ravichandran (2009), and the bootstrapping
method described in Hu and Liu (2004).
2 http://www.twitter.com.
550
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Table 2
Examples of positive and negative words.
Positive Negative
able adjective abandon verb
acceptable adjective abuse verb
admire verb burglar noun
amazing adjective chaos noun
careful adjective contagious adjective
ease noun corruption noun
guide verb lie verb
inspire verb reluctant adjective
truthful adjective wrong adjective
6.1 Comparison with Other Methods
This method could be used in a semi-supervised setting where a set of labeled words are
used and the system learns from these labeled nodes and from other unlabeled nodes.
Under this setting, we compare our method to the spin model described in Takamura,
Inui, and Okumura (2005). Table 3 compares the performance using 10-fold cross val-
idation. The table shows that the proposed method outperforms the spin model. The
spin model approach uses word glosses, WordNet synonym, hypernym, and antonym
relations, in addition to co-occurrence statistics extracted from corpus. The proposed
method achieves better performance by only using WordNet synonym, hypernym, and
similar to relations. Adding co-occurrence statistics slightly improved performance, and
using glosses did not help at all.
We also compare our method to a re-implementation of the label propagation (LP)
method. Our method outperforms the LP method in both the 10-fold cross-validation
set-up and when only 14 seeds are used.
We also compare our method to the SO-PMI method. Turney and Littman (2002)
propose two methods for predicting the semantic orientation of words. They use
Latent Semantic Analysis (SO-LSA) and Pointwise Mutual Information (SO-PMI) for
measuring the statistical association between any given word and a set of 14 seed
words. They describe this method as unsupervised because they only use 14 seeds
as paradigm words that define the semantic orientation rather than train the model
(Turney 2002).
Table 3
Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model,
and the random walks model for 10-fold cross-validation and 14 seeds.
? CV 14 seeds
SO-PMI (1? 107) ? 61.3
SO-PMI (2? 109) ? 76.1
SO-PMI (1? 1011) ? 82.8
Spin Model 91.5 81.9
Label Propagation 88.40 74.83
Random Walks 93.1 82.1
551
Computational Linguistics Volume 40, Number 3
The SO-PMI value can be calculated as follows:
SO-PMI(w) = log
hitsw,pos ? hitsneg
hitsw,neg ? hitspos
(5)
where w is a word with unknown polarity, hitsw,pos is the number of hits returned by a
commercial search engine when the search query is the given word and the disjunction
of all positive seed words. hitspos is the number of hits when we search for the disjunction
of all positive seed words. hitsw,neg, and hitsneg are defined similarly.
After Turney (2002), we use our method to predict semantic orientation of words in
the General Inquirer lexicon (Stone et al. 1966) using only 14 seed words. The network
we used contains only WordNet relations. No glosses or co-occurrence statistics are
used. The results comparing the SO-PMI method with different data set sizes, the spin
model, and the proposed method using only 14 seeds is shown in Table 3. We observe
that the random walk method outperforms SO-PMI when SO-PMI uses data sets of
sizes 1? 107 and 2? 109 words. The performance of SO-PMI and the random walk
methods are comparable when SO-PMI uses a very large data set (1? 1011 words). The
performance of the spin model approach is also comparable to the other two methods.
The advantages of the random walk method over SO-PMI is that it is faster and it does
not need a very large corpus. Another advantage is that the random walk method can
be used along with the labeled data from the General Inquirer lexicon (Stone et al. 1966)
to get much better performance. This is costly for the SO-PMI method because that will
require the submission of almost 4,000 queries to a commercial search engine.
We also compare our method with the bootstrapping method described in Hu and
Liu (2004), and the shortest path method described in Kamps et al. (2004). We build a
network using only WordNet synonyms and hypernyms. We restrict the test set to the
set of adjectives in the General Inquirer lexicon because our method is mainly interested
in classifying adjectives.
The performance of the spin model, the bootstrapping method, the shortest path
method, the LP method, the Mincut method, and the random walk method for only
adjectives is shown in Table 4. We notice from the table that the random walk method
outperforms the spin model, the bootstrapping method, the shortest path method,
the LP method, and the Mincut method for adjectives. The reported accuracy for the
shortest path method only considers the words it could assign a non-zero orientation
value. If we consider all words, its accuracy will drop to around 61%.
6.1.1 Varying Parameters. As we mentioned in Section 3.4, we use a parameter m to put
an upper bound on the length of random walks. In this section, we explore the impact
of this parameter on our method?s performance.
Figure 1 shows the accuracy of the random walk method as a function of the
maximum number of steps m as it varies from 5 to 50. We use a network built from
WordNet synonyms and hypernyms only. The number of samples k was set to 1, 000.
Table 4
Accuracy for adjectives only for the spin model, the bootstrap method, and the random walk
model.
Method Spin Model Bootstrap Shortest Path LP Mincut Random Walks
Accuracy 83.6 72.8 68.8 84.8 73.8 88.8
552
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Figure 1
The effect of varying the maximum number of steps (m) on accuracy (k = 1,000).
We perform 10-fold cross-validation using the General Inquirer lexicon. We observe
that the maximum number of steps m has very little impact on performance until it
rises above 30. At that point, the performance drops by no more than 1%, and then it no
longer changes as m increases. An interesting observation is that the proposed method
performs quite well with a very small number of steps (around 10). We looked at the
data set to understand why increasing the number of steps beyond 30 negatively affects
performance. We found out that when the number of steps is very large compared with
the diameter of the graph, the random walk that starts at ambiguous words (which are
hard to classify) have the chance of moving until it hits a node in the opposite class.
That does not happen when the limit on the number of steps is smaller because those
walks are then terminated without hitting any labeled nodes and are hence ignored.
Next, we study the effect of the number of samples k on our method?s performance.
As explained in Section 3.4, k is the number of samples used by the Monte Carlo
algorithm to find an estimate for the hitting time. Figure 2 shows the accuracy of the
random walks method as a function of the number of samples k. We use the same
Figure 2
The effect of varying the number of samples (k) on accuracy.
553
Computational Linguistics Volume 40, Number 3
settings as in the previous experiment. The only difference is that we fix m at 15 and
vary k from 10 to 20, 000 (note the logarithmic scale). We notice that the performance
is badly affected when the value of k is very small (less than 100). We also notice that
after 1, 000, varying k has very little, if any, effect on performance. This shows that the
Monte Carlo algorithm for computing the random walks hitting time performs quite
well with values of the number of samples as small as 1, 000.
The preceding experiments suggest that the parameter m has very little impact
on the performance. This suggests that the approach is fairly robust (i.e., it is quite
insensitive to different parameter settings).
6.1.2 Other Experiments. We now measure the performance of the random walk method
when the system is allowed to abstain from classifying the words for which it has low
confidence. We regard the ratio between the hitting time to positive words and hitting
time to negative words as a confidence measure and evaluate the top words with the
highest confidence level at different values of threshold. Figure 3 shows the accuracy for
10-fold cross validation and for using only 14 seeds at different thresholds. We notice
that the accuracy improves by abstaining from classifying the difficult words. The figure
shows that the top 60% words are classified with accuracy greater than 99% for 10-fold
cross validation and 92% with 14 seed words. This may be compared with the work
described in Takamura, Inui, and Okumura (2005), where they achieve the 92% level
when they only consider the top 1,000 words (28%).
Figure 4 shows a learning curve displaying how the performance of both the pro-
posed method and the LP method is affected with varying the labeled set size (i.e., the
number of seeds). We notice that the accuracy exceeds 90% when the training set size
rises above 20%. The accuracy steadily increases as the size of labeled data increases.
We also looked at the classification accuracy for different parts of speech in Figure 5.
We notice that, in the case of 10-fold cross-validation, the performance is consistent
across parts of speech. However, when we only use 14 seeds?all of which are ad-
jectives, similar to Turney and Littman (2003)?we notice that the performance on
adjectives is much better than other parts of speech. When we use 14 seeds but replace
some of the adjectives with verbs and nouns such as love, harm, friend, enemy, the per-
formance for nouns and verbs improves considerably at the cost of a small drop in the
Figure 3
Accuracy for words with high confidence measure.
554
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Figure 4
The effect of varying the number of seeds on accuracy.
50
55
60
65
70
75
80
85
90
95
100
Adj Adv Noun Verb
CV 14 Adj Seeds 14 Seeds
Figure 5
Accuracy for different parts of speech.
performance on adjectives. Finally, we tried adding edges to the network from glosses
and co-occurrence statistics but we did not get any statistically significant improvement.
Some of the words that were very weakly linked benefited from adding new types
of links and they were correctly predicted. Others were misled by the noise and were
incorrectly classified. We had a closer look at the results to find out what are the reasons
behind incorrect predictions. We found two main reasons. First, some words have more
than one sense, possibly with different semantic orientations. Disambiguating the sense
of words given their context before trying to predict their polarity should solve this
problem. The second reason is that some words have very few connections in the
thesaurus. A possible solution to this might be to identify those words and add more
links to them from glosses of co-occurrence statistics in the corpus.
6.1.3 General Purpose Three-Way Classification. The experiments described so far all use
the General Inquirer lexicon, which contains a well-established gold standard data set
of positive and negative words. However, in realistic applications, a general purpose
555
Computational Linguistics Volume 40, Number 3
Table 5
Accuracy for three classes on a general purpose list of 2,000 words.
Class Positive Negative Neutral Overall
Accuracy 68.0 82.1 80.6 77.9
list of words will frequently have neutral words that don?t express sentiment polarity.
To evaluate the effectiveness of the random walk method in distinguishing polarized
words from neutral words, we constructed a data set of 2, 000 words randomly picked
from a standard English dictionary3 and hand labeled them with three classes: posi-
tive, negative, and neutral. Among the 2, 000 words, 494 were labeled positive,
491 negative, and 1, 015 neutral. The distribution among different parts of speech is
532 adjectives, 335 verbs, 1, 051 nouns, and 82 others.
We used the semi-supervised setting with the General Inquirer lexicon polarized
word list as the training set. Because the 2, 000 test set has some portion of polarized
words overlapping with the training set, we excluded the words that appear in the test
set from the training set. We performed Algorithm 2 in Section 3.4 with parameters
? = 0.8, m = 15, k = 1, 000. The overall accuracy as well as the precision for each class is
shown in Table 5. We can see that the accuracy of the positive class is much lower than
the negative class, due to the many positive words classified as neutral. This means
that the average confidence of negative words is higher than positive words. One factor
that could have caused this is the bias originating from the training set. Because there
are more negative seeds than positive ones, the constructed graph has an overall bias
towards the negative class.
6.2 Foreign Words
In addition to the English data we described earlier, we constructed a labeled set of 300
Arabic and 300 Hindi words for evaluation. For every language, we asked two native
speakers to examine a large amount of text and identify a set of positive and negative
words. We also used an Arabic?English and a Hindi?English dictionary to generate
Foreign?English links.
We compare our results with two baselines. The first is the SO-PMI method de-
scribed in Turney and Littman (2003). We used the same seven positive and seven
negative seeds as Turney and Littman (2003).
The second baseline constructs a network of only foreign words as described earlier.
It uses mean hitting time to find the semantic association of any given word. We used
10-fold cross-validation for this experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use the hitting time as before to predict
semantic orientation. We used the English words from Stone et al. (1966) as seeds and
the labeled foreign words for evaluation. We will refer to this system as HT-FR-EN.
Figure 6 compares the accuracy of the three methods for Arabic and Hindi. We
notice that the SO-PMI and the hitting time?based methods perform poorly on both
Arabic and Hindi. This is clearly evident when we consider that the accuracy of the two
systems on English was 83%, and 93%, respectively (Turney and Littman 2003; Hassan
3 Very infrequent words were filtered out by setting a threshold on the inverse document frequency of the
words in a corpus.
556
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
0
10
20
30
40
50
60
70
80
90
100
Arabic Hindi
SO-PMI HT - FR HT - FR+ EN
Figure 6
Accuracy of foreign word polarity identification.
and Radev 2010). This supports our hypothesis that state-of-the-art methods, designed
for English, perform poorly on foreign languages due to the limited amount of resources
in them. The figure also shows that the proposed method, which combines resources
from both English and foreign languages, performs significantly better. Finally, we
studied how much improvement is achieved by including links between foreign words
from global WordNets. We found out that it improves the performance by 2.5% and 4%
for Arabic and Hindi, respectively.
6.3 OOV Words
We created a labeled set of 300 positive and negative OOV words. We asked a native
English speaker to examine a large number of threads posted on several on-line forums
and identify OOV words and label them with their polarities. Some examples of posi-
tive/negative OOV words are listed in Table 6.
The baseline we use for OOV words is the SO-PMI method with the same 14 seeds
as in Turney and Littman (2003). The calculation of SO-PMI is given in Equation (5).
We used the approach described in Section 5 to automatically label the words. We
used the words of the General Inquirer lexicon as labeled seeds. We set the maximum
number of steps m to 15 and the number of samples k to 1, 000. We experimented with
Table 6
Examples of positive and negative OOV words.
Positive Negative
Word Meaning Word Meaning
beautimous beautiful and fabulous disastrophy a catastrophy and a disaster
gr8 great banjaxed ruined
buffting attractive ijit idiot
557
Computational Linguistics Volume 40, Number 3
Figure 7
Accuracy of different methods in predicting OOV words polarity.
the three variants we proposed for extracting the related words as described in Section 5.
We give the experimental set-up for each variant here:
1. Search the entire Web (WS): We used Yahoo search4 to execute the search
queries. For each OOV word, we retrieve the top 500 results and use them
to extract the related words.
2. Search the entire Web and limit the extraction of related words to the
proximity of the OOV word (WSP): We fix the proximity of a given
OOV word to 15 words before and 15 words after the OOV word (we
experimented with different ranges but no significant changes were
observed).
3. Limit the search to social content (SOC): We limit the search for OOV
words to tweets posted on Twitter. We use the Twitter search API
to submit the search queries. For each OOV word, we retrieve
10,000 tweets. Each tweet is maximum of 140 characters long.
Figure 7 shows the results of the three methods compared with the baseline SO-PMI.
The results show that extracting related words from tweets gives the best accuracy. This
corroborated our intuition that using social content is more likely to provide sentiment-
related words. The baseline SO-PMI and WS obtain very similar accuracy. This agrees
with the comparable performance of the two methods in the earlier experiment on the
General Inquirer lexicon.
The three variant methods for obtaining related words have a tunable parameter
R, the number of related words extracted for each OOV word. We observe that R
has a non-negligible effect on the prediction accuracy. The results shown in Figure 8
correspond to R = 90. To better understand the impact of varying this parameter, we ran
the experiment that uses Twitter to extract related words several times using different
values for R. Figure 8 shows how the accuracy of polarity prediction changes as R
changes.
4 http://www.yahoo.com.
558
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
40%
45%
50%
55%
60%
65%
70%
0 20 40 60 80 100 120 140 160 180
Accu
racy
 
Number of related words  
Figure 8
The effect of varying the number of extracted related words on accuracy.
7. Conclusions
Predicting the semantic orientation of words is a very interesting task in natural lan-
guage processing and it has a wide variety of applications. We proposed a method for
automatically predicting the semantic orientation of words using random walks and
hitting time. The proposed method is based on the observation that a random walk
starting at a given word is more likely to hit another word with the same semantic
orientation before hitting a word with a different semantic orientation. The proposed
method can be used in a semi-supervised setting, where a training set of labeled words
is used, and in a weakly supervised setting, where only a handful of seeds is used to
define the two polarity classes. We predict semantic orientation with high accuracy.
The proposed method is fast, simple to implement, and does not need any corpus. We
also extended the proposed method to cover the problem of predicting the semantic
orientation of foreign words. All previous work on this task has almost exclusively
focused on English. Applying off-the-shelf methods developed for English to other
languages does not work well because of the limited amount of resources available
in foreign languages compared with English. We show that the proposed method can
predict the semantic orientation of foreign words with high accuracy and outperforms
state-of-the-art methods limited to using language specific resources. Finally, we further
extended the method to cover out-of-vocabulary words. These words do not exist in
WordNet and are not defined in the standard dictionaries of the language. We proposed
using a Web-based approach to add the OOV words to our words network based on
co-occurrence statistics, then use the same random walk model to predict the polar-
ity. We showed that this method can predict the polarity of OOV words with good
accuracy.
Acknowledgments
This research was funded by the Office of the
Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects
Activity (IARPA), through the U.S. Army
Research Lab. All statements of fact, opinion,
or conclusions contained herein are those of
the authors and should not be construed as
representing the official views or policies of
IARPA, the ODNI, or the U.S. Government.
559
Computational Linguistics Volume 40, Number 3
References
Agirre, Eneko, Enrique Alfonseca, Keith
Hall, Jana Kravalova, Marius Pas?ca, and
Aitor Soroa. 2009. A study on similarity
and relatedness using distributional and
wordnet-based approaches. In Proceedings
of Human Language Technologies: The 2009
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, NAACL ?09, pages 19?27,
Stroudsburg, PA.
Andreevskaia, Alina and Sabine Bergler.
2006. Mining WordNet for fuzzy
sentiment: Sentiment tag extraction
from WordNet glosses. In EACL?06,
pages 209?216.
Banea, Carmen, Rada Mihalcea, and
Janyce Wiebe. 2008. A bootstrapping
method for building subjectivity lexicons
for languages with scarce resources.
In LREC?08, pages 2,764?2,767.
Black, W., S. Elkateb, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006. Introducing the
Arabic WordNet project. In Third
International WordNet Conference,
pages 295?299.
Blair-Goldensohn, Sasha, Tyler Neylon,
Kerry Hannan, George A. Reis, Ryan
McDonald, and Jeff Reynar. 2008. Building
a sentiment summarizer for local service
reviews. In NLP in the Information
Explosion Era.
Brody, Samuel and Noemie Elhadad. 2010.
An unsupervised aspect-sentiment model
for online reviews. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 804?812, Los Angeles, CA.
Elkateb, S., W. Black, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006a. Building a WordNet
for Arabic. In Fifth International Conference
on Language Resources and Evaluation,
pages 29?34.
Elkateb, S., W. Black, P. Vossen, D. Farwell,
H. Rodriguez, A. Pease, and M. Alkhalifa.
2006b. Arabic WordNet and the challenges
of Arabic. In Arabic NLP/MT Conference,
pages 15?24.
Esuli, Andrea and Fabrizio Sebastiani. 2005.
Determining the semantic orientation
of terms through gloss classification.
In CIKM?05, pages 617?624.
Esuli, Andrea and Fabrizio Sebastiani. 2006.
Sentiwordnet: A publicly available lexical
resource for opinion mining. In LREC?06,
pages 417?422.
Etzioni, Oren, Kobi Reiter, Stephen Soderl,
and Marcus Sammer. 2007. Lexical
translation with application to image
search on the Web. In Proceedings of
Machine Translation Summit XI.
Hassan, Ahmed and Dragomir R. Radev.
2010. Identifying text polarity using
random walks. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 395?403,
Uppsala.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In EACL?97,
pages 174?181.
Hatzivassiloglou, Vasileios and Janyce
Wiebe. 2000. Effects of adjective
orientation and gradability on sentence
subjectivity. In COLING, pages 299?305.
Hu, Minqing and Bing Liu. 2004. Mining
and summarizing customer reviews.
In KDD?04, pages 168?177.
Jha, S., D. Narayan, P. Pande, and
P. Bhattacharyya. 2001. A WordNet for
Hindi. In International Workshop on Lexical
Resources in Natural Language Processing.
Jijkoun, Valentin and Katja Hofmann. 2009.
Generating a non-English subjectivity
lexicon: Relations that matter. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009),
pages 398?405, Athens.
Kamps, Jaap, Maarten Marx, Robert J.
Mokken, and Maarten De Rijke. 2004.
Using WordNet to measure semantic
orientations of adjectives. In Proceedings
of the 4th International Conference on
Language Resources and Evaluation
(LREC 2004), pages 1115?1118.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis.
In EMNLP?06, pages 355?363.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In COLING, pages 1,367?1,373.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good
time. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 145?153,
Los Angeles, CA.
Lewis, D. D., Y. Yang, T. Rose, and F. Li.
2004. Rcv1: A new benchmark collection
for text categorization research. Journal of
Machine Learning Research, 5:361?397.
Mihalcea, Rada and Carmen Banea. 2007.
Learning multilingual subjective language
560
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
via cross-lingual projections. In Proceedings
of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 976?983.
Miller, George A. 1995. Wordnet: A lexical
database for English. Communications of
ACM, 38(11):39?41.
Mohammad, Saif, Cody Dunne, and Bonnie
Dorr. 2009. Generating high-coverage
semantic orientation lexicons from overtly
marked words and a thesaurus. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing: Volume 2, EMNLP ?09,
pages 599?608, Stroudsburg, PA.
Morinaga, Satoshi, Kenji Yamanishi, Kenji
Tateishi, and Toshikazu Fukushima. 2002.
Mining product reputations on the Web.
In KDD?02, pages 341?349.
Narayan, Dipak, Debasri Chakrabarti,
Prabhakar Pande, and P. Bhattacharyya.
2002. An experience in building the Indo
WordNet?a WordNet for Hindi. In First
International Conference on Global WordNet.
Nasukawa, Tetsuya and Jeonghee Yi. 2003.
Sentiment analysis: Capturing favorability
using natural language processing.
In K-CAP ?03: Proceedings of the 2nd
International Conference on Knowledge
Capture, pages 70?77.
Norris, J. 1997. Markov Chains. Cambridge
University Press.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics, ACL ?04,
Stroudsburg, PA.
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
from reviews. In HLT-EMNLP?05,
pages 339?346.
Rao, Delip and Deepak Ravichandran.
2009. Semi-supervised polarity lexicon
induction. In Proceedings of the 12th
Conference of the European Chapter of the
ACL (EACL 2009), pages 675?682, Athens.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for
subjective expressions. In EMNLP?03,
pages 105?112.
Stone, Philip, Dexter Dunphy, Marchall
Smith, and Daniel Ogilvie. 1966. The
General Inquirer: A Computer Approach
to Content Analysis. The MIT Press.
Su, Fangzhong and Katja Markert. 2009.
Subjectivity recognition on word
senses via semi-supervised mincuts.
In Proceedings of Human Language
Technologies: The 2009 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics, NAACL ?09,
pages 1?9, Stroudsburg, PA.
Szummer, Martin and Tommi Jaakkola.
2002. Partially labeled classification with
Markov random walks. In NIPS?02,
pages 945?952.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Tong, Richard M. 2001. An operational
system for detecting and tracking opinions
in on-line discussion. Workshop note,
SIGIR 2001 Workshop on Operational Text
Classification.
Turney, Peter and Michael Littman. 2003.
Measuring praise and criticism: Inference
of semantic orientation from association.
ACM Transactions on Information Systems,
21:315?346.
Turney, Peter D. 2002. Thumbs up or thumbs
down?: Semantic orientation applied to
unsupervised classification of reviews.
In ACL?02, pages 417?424.
Velikovich, Leonid, Sasha Blair-Goldensohn,
Kerry Hannan, and Ryan McDonald. 2010.
The viability of Web-derived polarity
lexicons. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 777?785,
Los Angeles, CA.
Vossen, P. 1997. Eurowordnet: A multilingual
database for information retrieval. In
DELOS Workshop on Cross-Language
Information Retrieval, pages 5?7.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In Proceedings of
the Seventeenth National Conference on
Artificial Intelligence and the Twelfth
Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Wiebe, Janyce, Rebecca Bruce, Matthew Bell,
Melanie Martin, and Theresa Wilson.
2001. A corpus study of evaluative and
speculative language. In Proceedings of the
Second SIGdial Workshop on Discourse and
Dialogue, pages 1?10.
Wiebe, Janyce and Rada Mihalcea.
2006a. Word sense and subjectivity.
In Proceedings of the 21st International
Conference on Computational Linguistics
and the 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1,065?1,072, Sydney.
Wiebe, Janyce and Rada Mihalcea. 2006b.
Word sense and subjectivity. In Proceedings
561
Computational Linguistics Volume 40, Number 3
of the 21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics, ACL-44,
pages 1,065?1,072, Stroudsburg, PA.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2-3):165?210.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In EMNLP?03, pages 129?136.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In ICML?03,
pages 912?919.
562
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 33?36,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
AttitudeMiner: Mining Attitude from Online Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Ahmed Hassan
Microsoft Research
Redmond, WA, USA
hassanam@microsoft.com
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
This demonstration presents AttitudeMiner, a
system for mining attitude from online dis-
cussions. AttitudeMiner uses linguistic tech-
niques to analyze the text exchanged between
participants of online discussion threads at dif-
ferent levels of granularity: the word level, the
sentence level, the post level, and the thread
level. The goal of this analysis is to iden-
tify the polarity of the attitude the discussants
carry towards one another. Attitude predic-
tions are used to construct a signed network
representation of the discussion thread. In this
network, each discussant is represented by a
node. An edge connects two discussants if
they exchanged posts. The sign (positive or
negative) of the edge is set based on the po-
larity of the attitude identified in the text asso-
ciated with the edge. The system can be used
in different applications such as: word polar-
ity identification, identifying attitudinal sen-
tences and their signs, signed social network
extraction from text, subgroup detect in dis-
cussion. The system is publicly available for
download and has an online demonstration at
http://clair.eecs.umich.edu/AttitudeMiner/.
1 Introduction
The rapid growth of social media has encouraged
people to interact with each other and get involved
in discussions more than anytime before. The most
common form of interaction on the web uses text
as the main communication medium. When people
discuss a topic, especially when it is a controversial
one, it is normal to see situations of both agreement
and disagreement among the discussants. It is even
not uncommon that the big group of discussants split
into two or more smaller subgroups. The members
of each subgroup mostly agree and show positive
attitude toward each other, while they mostly dis-
agree with the members of opposing subgroups and
possibly show negative attitude toward them. These
forms of sentiment are expressed in text by using
certain language constructs (e.g. use insult or nega-
tive slang to express negative attitude).
In this demonstration, we present a system that
applies linguistic analysis techniques to the text of
online discussions to predict the polarity of relations
that develop between discussants. This analysis is
done on words to identify their polarities, then on
sentences to identify attitudinal sentences and the
sign of attitude, then on the post level to identify the
sign of an interaction, and finally on the entire thread
level to identify the overall polarity of the relation.
Once the polarity of the pairwise relations that de-
velop between interacting discussants is identified,
this information is then used to construct a signed
network representation of the discussion thread.
The system also implements two signed network
partitioning techniques that can be used to detect
how the discussants split into subgroups regarding
the discussion topic.
The functionality of the system is based on
our previous research on word polarity identifica-
tion (Hassan and Radev, 2010) and attitude identifi-
cation (Hassan et al, 2010). The system is publicly
available for download and has a web interface to try
online1.
This work is related to previous work in the areas
of sentiment analysis and online discussion mining.
Many previous systems studied the problem of iden-
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003). Opinionfinder (Wilson et al, 2005a) is a sys-
tem for mining opinions from text. Another research
line focused on analyzing online discussions. For
example, Lin et al (2009) proposed a sparse coding-
based model that simultaneously models the seman-
tics and the structure of threaded discussions and
Shen et al (2006) proposed a method for exploit-
ing the temporal information in discussion streams
to identify the reply structure of the dialog. Many
systems addressed the problem of extracting social
networks from data (Elson et al, 2010; McCallum
et al, 2007), but none of them considered both pos-
itive and negative relations.
In the rest of the paper, we describe the system
architecture, implementation, usage, and its perfor-
1http://clair.eecs.umich.edu/AttitudeMiner/
33
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Text Polarity 
Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Attitude Identification 
? Identify Attitudinal 
Sentences 
? Predict the sign on 
attitude 
 
 
Post sign identification 
? Aggregate the signs of 
attitudinal sentences to 
assign a sign to the 
post. 
 
Relation Sign 
? Aggregate the signs of all 
the posts exchanged by 
interacting participants 
to assign a sign for their 
relation. 
Signed Network 
 
 
 
 
 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text 
? Split posts into sentences 
 
+ _ 
Figure 1: Overview of the system processing pipeline
mance evaluation.
2 System Overview
Figure 1 shows a block diagram of the system com-
ponents and the processing pipeline. The first com-
ponent in the system is the thread parsing com-
ponent which takes as input a discussion thread
and parses it to identify the posts, the participants,
and the reply structure of the thread. This compo-
nent uses a module from CLAIRLib (Abu-Jbara and
Radev, 2011) to tokenize the posts and split them
into sentences.
The second component in the pipeline processes
the text of the posts to identify polarized words and
tag them with their polarity. This component uses
the publicly available tool, opinionfinder (Wilson et
al., 2005a), as a framework for polarity identifica-
tion. This component uses an extended polarity lex-
icon created by applying a random walk model to
WordNet (Miller, 1995) and a set of seed polarized
words. This approach is described in detail in our
previous work (Hassan and Radev, 2010). The con-
text of words is taken into consideration by running
a contextual word classifier that determines whether
the word is used in a polarized sense given the con-
text (Wilson et al, 2005b). For example, a positive
word appearing in a negated scope is used in a neg-
ative, rather than a positive sense.
The next component is the attitude identification
component. Given a sentence, our model predicts
whether it carries an attitude from the text writer to-
ward the text recipient or not. As we are only in-
terested in attitudes between participants, we limit
our analysis to sentences that use mentions of a dis-
cussion participants (i.e. names or second person
pronouns). We also discard all sentences that do
not contain polarized expressions as detected by the
previous component. We extract several patterns at
different levels of generalization representing any
given sentence. We use words, part-of-speech tags,
and dependency relations. We use those patterns to
build two Markov models for every kind of patterns.
The first model characterizes the relation between
different tokens for all patterns that correspond to
sentences that have an attitude. The second model
is similar to the first one, but it uses all patterns that
correspond to sentences that do not have an attitude.
Given a new sentence, we extract the corresponding
patterns and estimate the likelihood of every pattern
being generated from the two corresponding mod-
els. We then compute the likelihood ratio of the sen-
tence under every pair of models. Notice that we
have a pair of models corresponding to every type of
patterns. The likelihood ratios are combined using a
linear model, the parameters of which are estimated
using a development dataset. Please refer to (Hassan
et al, 2010) for more details about this component.
The next component works on the post level. It
assigns a sign to each post based on the signs of the
sentences it contains. A post is classified as negative
if it has at leastNs negative sentences, otherwise it is
classified as positive. The value ofNs can be chosen
by the user or set to default which was estimated
using a small labeled development set. The default
value forNs is 1 (i.e. if the post contains at least one
negative sentence, the whole post is considered to be
negative).
The next component in the pipeline uses the atti-
tude predictions from posts to construct a signed net-
work representation of the discussion thread. Each
participant is represented by a node. An edge is
created between two participants if they interacted
with each other. A sign (positive or negative) is as-
signed to an edge based on the signs of the posts
the two participants connected by the edge have ex-
changed. This is done by comparing the number of
positive and negative posts. A negative sign is given
if the two participants exchanged at least Np nega-
tive posts. The value of Np can be set using a devel-
opment set. The default value is 1.
The last component is the subgroup identifica-
34
Figure 2: The web interface for detecting subgroups in discussions
tion component. This component provides imple-
mentations for two signed network partitioning algo-
rithms. The first one is a greedy optimization algo-
rithm that is based on the principals of the structural
balance theory. The algorithm uses a criterion func-
tion for a local optimization partitioning such that
positive links are dense within groups and negative
links are dense between groups. The algorithm is de-
scribed in detail in (Doreian and Mrvar, 1996). The
second algorithm is FEC (Yang et al, 2007). FEC
is based on an agent-based random walk model. It
starts by finding a sink community, and then extract-
ing it from the entire network based on a graph cut
criteria that Yang et al (2007) proposed. The same
process is then applied recursively to the extracted
community and the rest of the network.
3 Implementation Details
The system is implemented in Perl. Some of the
components in the processing pipeline use external
tools that are implemented in either Perl, Java, or
Python. All the external tools come bundled with the
system. The system is compatible with all the ma-
jor platforms including windows, Mac OS, and all
Linux distributions. The installation process is very
straightforward. There is a single installation script
that will install the system, install all the dependen-
cies, and do all the required configurations. The in-
stallation requires that Java JRE, Perl, and Python be
installed on the machine.
The system has a command-line interface that
provides full access to the system functionality. The
command-line interface can be used to run the whole
pipeline or any portion of it. It can also be used to ac-
cess any component directly. Each component has a
corresponding script that can be run separately. The
input and output specifications of each component
are described in the accompanying documentation.
All the parameters that control the performance of
the system can also be passed through the command-
line interface.
The system can process any discussion thread that
is input to it in a specific XML format. The fi-
nal output of the system is also in XML format.
The XML schema of the input/output is described
in the documentation. It is the user responsibil-
ity to write a parser that converts an online discus-
sion thread to the expected XML format. The sys-
tem package comes with three such parsers for three
different discussion sites: www.politicalforum.com,
groups.google.com, and www.createdebate.com.
The distribution also comes with three datasets
(from three different sources) comprising a total of
300 discussion threads. The datasets are annotated
with the subgroup labels of discussants. Included in
the distribution as well, a script for generating a vi-
sualization of the extracted signed network and the
identified subgroups.
AttitudeMiner also has a web interface that
demonstrates most of its functionality. The web in-
35
Figure 3: The web interface for identifying attitudinal
sentences and their polarity
terface is intended for demonstration purposes only.
No webservice is provided. Figure 2 and Figrue 3
show two screenshots for the web interface.
4 System Performance
In this section, we give a brief summary of the sys-
tem performance. The method that generates the
extended polarity lexicon that is used for word po-
larity identification achieves 88.8% accuracy as re-
ported in (Hassan and Radev, 2010). The attitude
identification component distinguishes between at-
titudinal and non-attitudinal sentences with 80.3%
accuracy, and predicts the signs of attitudinal sen-
tences with 97% accuracy as reported in (Hassan et
al., 2010). Our evaluation for the signed network
extraction component on a large annotated dataset
showed that it achieves 83.5% accuracy. Finally, our
experiments on an annotated discussion showed that
the system can detect subgroups with 77.8% purity.
The system was evaluated using a dataset with thou-
sands of posts labeled by human annotators.
5 Conclusion
We presented of a demonstration of a social me-
dia mining system that used linguistic analysis tech-
niques to understand the relations that develop be-
tween users in online communities. The system is
capable of analyzing the text exchanged during dis-
cussions and identifying positive and negative atti-
tudes. Positive attitude reflects a friendly relation
while negative attitude is a sign of an antagonistic
relation. The system can also use the attitude infor-
mation to identify subgroups with a homogeneous
and common focus among the discussants. The sys-
tem predicts attitudes and identifies subgroups with
high accuracy.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib:
A toolkit for natural language processing, information
retrieval, and network analysis. In ACL-HLT 2011-
Demo, June.
Patrick Doreian and Andrej Mrvar. 1996. A partitioning
approach to structural balance. Social Networks.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In ACL 2010, pages 138?147, Uppsala, Sweden, July.
Ahmed Hassan and Dragomir R. Radev. 2010. Identify-
ing text polarity using random walks. In ACL 2010.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In HLT/EMNLP - Demo.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng.
36
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395?403,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Identifying Text Polarity Using Random Walks
Ahmed Hassan
University of Michigan Ann Arbor
Ann Arbor, Michigan, USA
hassanam@umich.edu
Dragomir Radev
University of Michigan Ann Arbor
Ann Arbor, Michigan, USA
radev@umich.edu
Abstract
Automatically identifying the polarity of
words is a very important task in Natural
Language Processing. It has applications
in text classification, text filtering, analysis
of product review, analysis of responses
to surveys, and mining online discussions.
We propose a method for identifying the
polarity of words. We apply a Markov ran-
dom walk model to a large word related-
ness graph, producing a polarity estimate
for any given word. A key advantage of
the model is its ability to accurately and
quickly assign a polarity sign and mag-
nitude to any word. The method could
be used both in a semi-supervised setting
where a training set of labeled words is
used, and in an unsupervised setting where
a handful of seeds is used to define the
two polarity classes. The method is exper-
imentally tested using a manually labeled
set of positive and negative words. It out-
performs the state of the art methods in the
semi-supervised setting. The results in the
unsupervised setting is comparable to the
best reported values. However, the pro-
posed method is faster and does not need a
large corpus.
1 Introduction
Identifying emotions and attitudes from unstruc-
tured text is a very important task in Natural Lan-
guage Processing. This problem has a variety of
possible applications. For example, there has been
a great body of work for mining product reputation
on the Web (Morinaga et al, 2002; Turney, 2002).
Knowing the reputation of a product is very impor-
tant for marketing and customer relation manage-
ment (Morinaga et al, 2002). Manually handling
reviews to identify reputation is a very costly, and
time consuming process given the overwhelming
amount of reviews on the Web. A list of words
with positive/negative polarity is a very valuable
resource for such an application.
Another interesting application is mining online
discussions. A threaded discussion is an electronic
discussion in which software tools are used to help
individuals post messages and respond to other
messages. Threaded discussions include e-mails,
e-mail lists, bulletin boards, newsgroups, or Inter-
net forums. Threaded discussions act as a very im-
portant tool for communication and collaboration
in the Web. An enormous number of discussion
groups exists on the Web. Millions of users post
content to these groups covering pretty much ev-
ery possible topic. Tracking participant attitude
towards different topics and towards other partici-
pants is a very interesting task. For example,Tong
(2001) presented the concept of sentiment time-
lines. His system classifies discussion posts about
movies as either positive or negative. This is used
to produce a plot of the number of positive and
negative sentiment messages over time. All those
applications could benefit much from an automatic
way of identifying semantic orientation of words.
In this paper, we study the problem of automati-
cally identifying semantic orientation of any word
by analyzing its relations to other words. Auto-
matically classifying words as either positive or
negative enables us to automatically identify the
polarity of larger pieces of text. This could be
a very useful building block for mining surveys,
product reviews and online discussions. We ap-
ply a Markov random walk model to a large se-
mantic word graph, producing a polarity estimate
for any given word. Previous work on identifying
the semantic orientation of words has addressed
the problem as both a semi-supervised (Takamura
et al, 2005) and an unsupervised (Turney and
Littman, 2003) learning problem. In the semi-
supervised setting, a training set of labeled words
395
is used to train the model. In the unsupervised
setting, only a handful of seeds is used to define
the two polarity classes. The proposed method
could be used both in a semi-supervised and in
an unsupervised setting. Empirical experiments
on a labeled set of words show that the proposed
method outperforms the state of the art methods in
the semi-supervised setting. The results in the un-
supervised setting are comparable to the best re-
ported values. The proposed method has the ad-
vantages that it is faster and it does not need a large
training corpus.
The rest of the paper is structured as follows.
In Section 2, we discuss related work. Section 3
presents our method for identifying word polarity.
Section 4 describes our experimental setup. We
conclude in Section 5.
2 Related Work
Hatzivassiloglou and McKeown (1997) proposed
a method for identifying word polarity of adjec-
tives. They extract all conjunctions of adjectives
from a given corpus and then they classify each
conjunctive expression as either the same orien-
tation such as ?simple and well-received? or dif-
ferent orientation such as ?simplistic but well-
received?. The result is a graph that they cluster
into two subsets of adjectives. They classify the
cluster with the higher average frequency as posi-
tive. They created and labeled their own dataset
for experiments. Their approach will probably
works only with adjectives because there is noth-
ing wrong with conjunctions of nouns or verbs
with opposite polarities (e.g., ?war and peace?,
?rise and fall?, ..etc).
Turney and Littman (2003) identify word po-
larity by looking at its statistical association with
a set of positive/negative seed words. They use
two statistical measures for estimating association:
Pointwise Mutual Information (PMI) and Latent
Semantic Analysis (LSA). To get co-occurrence
statistics, they submit several queries to a search
engine. Each query consists of the given word and
one of the seed words. They use the search engine
near operator to look for instances where the given
word is physically close to the seed word in the re-
turned document. They present their method as an
unsupervised method where a very small amount
of seed words are used to define semantic orienta-
tion rather than train the model. One of the lim-
itations of their method is that it requires a large
corpus of text to achieve good performance. They
use several corpora, the size of the best performing
dataset is roughly one hundred billion words (Tur-
ney and Littman, 2003).
Takamura et al (2005) proposed using spin
models for extracting semantic orientation of
words. They construct a network of words us-
ing gloss definitions, thesaurus, and co-occurrence
statistics. They regard each word as an electron.
Each electron has a spin and each spin has a direc-
tion taking one of two values: up or down. Two
neighboring spins tend to have the same orienta-
tion from an energetic point of view. Their hy-
pothesis is that as neighboring electrons tend to
have the same spin direction, neighboring words
tend to have similar polarity. They pose the prob-
lem as an optimization problem and use the mean
field method to find the best solution. The anal-
ogy with electrons leads them to assume that each
word should be either positive or negative. This
assumption is not accurate because most of the
words in the language do not have any semantic
orientation. They report that their method could
get misled by noise in the gloss definition and their
computations sometimes get trapped in a local op-
timum because of its greedy optimization flavor.
Kamps et al (2004) construct a network
based on WordNet synonyms and then use the
shortest paths between any given word and the
words ?good? and ?bad? to determine word polar-
ity. They report that using shortest paths could be
very noisy. For example. ?good? and ?bad? them-
selves are closely related in WordNet with a 5-
long sequence ?good, sound, heavy, big, bad?. A
given word w may be more connected to one set
of words (e.g., positive words), yet have a shorter
path connecting it to one word in the other set. Re-
stricting seed words to only two words affects their
accuracy. Adding more seed words could help but
it will make their method extremely costly from
the computation point of view. They evaluate their
method only using adjectives.
Hu and Liu (2004) use WordNet synonyms and
antonyms to predict the polarity of words. For
any word, whose polarity is unknown, they search
WordNet and a list of seed labeled words to pre-
dict its polarity. They check if any of the syn-
onyms of the given word has known polarity. If
so, they label it with the label of its synonym. Oth-
erwise, they check if any of the antonyms of the
given word has known polarity. If so, they label it
396
with the opposite label of the antonym. They con-
tinue in a bootstrapping manner till they label all
possible word. This method is quite similar to the
shortest-path method proposed in (Kamps et al,
2004).
There are some other methods that try to build
lexicons of polarized words. Esuli and Sebas-
tiani (2005; 2006) use a textual representation of
words by collating all the glosses of the word as
found in some dictionary. Then, a binary text clas-
sifier is trained using the textual representation and
applied to new words. Kim and Hovy (2004) start
with two lists of positive and negative seed words.
WordNet is used to expand these lists. Synonyms
of positive words and antonyms of negative words
are considered positive, while synonyms of neg-
ative words and antonyms of positive words are
considered negative. A similar method is pre-
sented in (Andreevskaia and Bergler, 2006) where
WordNet synonyms, antonyms, and glosses are
used to iteratively expand a list of seeds. The senti-
ment classes are treated as fuzzy categories where
some words are very central to one category, while
others may be interpreted differently. Kanayama
and Nasukawa (2006) use syntactic features and
context coherency, the tendency for same polari-
ties to appear successively , to acquire polar atoms.
Other related work is concerned with subjec-
tivity analysis. Subjectivity analysis is the task
of identifying text that present opinions as op-
posed to objective text that present factual in-
formation (Wiebe, 2000). Text could be either
words, phrases, sentences, or any other chunks.
There are two main categories of work on sub-
jectivity analysis. In the first category, subjective
words and phrases are identified without consider-
ing their context (Wiebe, 2000; Hatzivassiloglou
and Wiebe, 2000; Banea et al, 2008). In the sec-
ond category, the context of subjective text is used
(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou,
2003; Nasukawa and Yi, 2003; Popescu and Et-
zioni, 2005) Wiebe et al (2001) lists a lot of appli-
cations of subjectivity analysis such as classifying
emails and mining reviews. Subjectivity analysis
is related to the proposed method because identi-
fying the polarity of text is the natural next step
that should follow identifying subjective text.
3 Word Polarity
We use a Markov random walk model to identify
polarity of words. Assume that we have a network
of words, some of which are labeled as either pos-
itive or negative. In this network, two words are
connecting if they are related. Different sources of
information could be used to decide whether two
words are related or not. For example, the syn-
onyms of any word are semantically related to it.
The intuition behind that connecting semantically
related words is that those words tend to have simi-
lar polarity. Now imagine a random surfer walking
along the network starting from an unlabeled word
w. The random walk continues until the surfer
hits a labeled word. If the word w is positive then
the probability that the random walk hits a positive
word is higher and if w is negative then the prob-
ability that the random walk hits a negative word
is higher. Similarly, if the word w is positive then
the average time it takes a random walk starting
at w to hit a positive node is less than the average
time it takes a random walk starting at w to hit a
negative node.
In the rest of this section, we will describe how
we can construct a word relatedness graph in Sec-
tion 3.1. The random walk model is described in
Section 3.2. Hitting time is defined in Section?3.3.
Finally, an algorithm for computing a sign and
magnitude for the polarity of any given word is
described in Section 3.4.
3.1 Network Construction
We construct a network where two nodes are
linked if they are semantically related. Several
sources of information could be used as indicators
of the relatedness of words. One such important
source is WordNet (Miller, 1995). WordNet is a
large lexical database of English. Nouns, verbs,
adjectives and adverbs are grouped into sets of
cognitive synonyms (synsets), each expressing a
distinct concept (Miller, 1995). Synsets are inter-
linked by means of conceptual-semantic and lexi-
cal relations.
The simplest approach is to connect words that
occur in the same WordNet synset. We can col-
lect all words in WordNet, and add links between
any two words that occurr in the same synset. The
resulting graph is a graph G(W,E) where W is a
set of word / part-of-speech pairs for all the words
in WordNet. E is the set of edges connecting
each pair of synonymous words. Nodes represent
word/pos pairs rather than words because the part
of speech tags are helpful in disambiguating the
different senses for a given word. For example,
397
the word ?fine? has two different meanings when
used as an adjective and as a noun.
Several other methods could be used to link
words. For example, we can use other WordNet
relations: hypernyms, similar to,...etc. Another
source of links between words is co-occurrence
statistics from corpus. Following the method pre-
sented in (Hatzivassiloglou and McKeown, 1997),
we can connect words if they appear in a conjunc-
tive form in the corpus. This method is only appli-
cable to adjectives. If two adjectives are connected
by ?and? in conjunctive form, it is highly likely
that they have the same semantic orientation. In
all our experiments, we restricted the network to
only WordNet relations. We study the effect of us-
ing co-occurrence statistics to connect words later
at the end of our experiments. If more than one re-
lation exists between any two words, the strength
of the corresponding edge is adjusted accordingly.
3.2 Random Walk Model
Imagine a random surfer walking along the word
relatedness graph G. Starting from a word with
unknown polarity i , it moves to a node j with
probability Pij after the first step. The walk con-
tinues until the surfer hits a word with a known
polarity. Seed words with known polarity act as
an absorbing boundary for the random walk. If
we repeat the number of random walks N times,
the percentage of time at which the walk ends at
a positive/negative word could be used as an in-
dicator of its positive/negative polarity. The aver-
age time a random walk starting at w takes to hit
the set of positive/negative nodes is also an indi-
cator of its polarity. This view is closely related
to the partially labeled classification with random
walks approach in (Szummer and Jaakkola, 2002)
and the semi-supervised learning using harmonic
functions approach in (Zhu et al, 2003).
Let W be the set of words in our lexicon. We
construct a graph whose nodes V are all words
in W The edges E correspond to relatedness be-
tween words We define transition probabilities
Pt+1|t(j|i) from i to j by normalizing the weights
of the edges out of node i, so:
Pt+1|t(j|i) = Wij/
?
k
Wik (1)
where k represents all nodes in the neighborhood
of i. Pt2|t1(j|i) denotes the transition probability
from node i at step t1 to node j at time step t2.
We note that the weights Wij are symmetric and
the transition probabilities Pt+1|t(j|i) are not nec-
essarily symmetric because of the node out degree
normalization.
3.3 First-Passage Time
The mean first-passage (hitting) time h(i|k) is de-
fined as the average number of steps a random
walker, starting in state i 6= k, will take to en-
ter state k for the first time (Norris, 1997). Let
G = (V,E) be a graph with a set of vertices V ,
and a set of edges E. Consider a subset of vertices
S ? V , Consider a random walk on G starting at
node i 6? S. Let Nt denote the position of the ran-
dom surfer at time t. Let h(i|S) be the the average
number of steps a random walker, starting in state
i 6? S, will take to enter a state k ? S for the first
time. Let TS be the first-passage for any vertex in
S.
P (TS = t|N0 = i) =
?
j?V
pij ? P (TS = t? 1|N0 = j) (2)
h(i|S) is the expectation of TS . Hence:
h(i|S) = E(TS |N0 = i)
=
??
t=1
t? P (TS = t|N0 = i)
=
??
t=1
t
?
j?V
pijP (TS = t? 1|N0 = j)
=
?
j?V
??
t=1
(t? 1)pijP (TS = t? 1|N0 = j)
+
?
j?V
??
t=1
pijP (TS = t? 1|N0 = j)
=
?
j?V
pij
??
t=1
tP (TS = t|N0 = j) + 1
=
?
j?V
pij ? h(j|S) + 1 (3)
Hence the first-passage (hitting) time can be for-
mally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h(j|S) + 1 otherwise
(4)
3.4 Word Polarity Calculation
Based on the description of the random walk
model and the first-passage (hitting) time above,
398
we now propose our word polarity identification
algorithm. We begin by constructing a word relat-
edness graph and defining a random walk on that
graph as described above. Let S+ and S? be two
sets of vertices representing seed words that are
already labeled as either positive or negative re-
spectively. For any given word w, we compute the
hitting time h(w|S+), and h(w|S?) for the two
sets iteratively as described earlier. if h(w|S+)
is greater than h(w|S?), the word is classified as
negative, otherwise it is classified as positive. The
ratio between the two hitting times could be used
as an indication of how positive/negative the given
word is. This is useful in case we need to pro-
vide a confidence measure for the prediction. This
could be used to allow the model to abstain from
classifying words with when the confidence level
is low.
Computing hitting time as described earlier may
be time consuming especially if the graph is large.
To overcome this problem, we propose a Monte
Carlo based algorithm for estimating it. The algo-
rithm is shown in Algorithm 1.
Algorithm 1 Word Polarity using Random Walks
Require: A word relatedness graph G
1: Given a word w in V
2: Define a randomwalk on the graph. the transi-
tion probability between any two nodes i, and
j is defined as: Pt+1|t(j|i) = Wij/
?
k Wik
3: Start k independent random walks from w
with a maximum number of steps m
4: Stop when a positive word is reached
5: Let h?(w|S+) be the estimated value for
h(w|S+)
6: Repeat for negative words computing
h?(w|S?)
7: if h?(w|S+) ? h?(w|S?) then
8: Classify w as positive
9: else
10: Classify w as negative
11: end if
4 Experiments
We performed experiments on the General In-
quirer lexicon (Stone et al, 1966). We used it
as a gold standard data set for positive/negative
words. The dataset contains 4206 words, 1915 of
which are positive and 2291 are negative. Some of
the ambiguous words were removed like (Turney,
2002; Takamura et al, 2005).
We use WordNet (Miller, 1995) as a source
of synonyms and hypernyms for the word relat-
edness graph. We used 10-fold cross validation
for all tests. We evaluate our results in terms of
accuracy. Statistical significance was tested us-
ing a 2-tailed paired t-test. All reported results
are statistically significant at the 0.05 level. We
perform experiments varying the parameters and
the network. We also look at the performance of
the proposed method for different parts of speech,
and for different confidence levels We compare
our method to the Semantic Orientation from PMI
(SO-PMI) method described in (Turney, 2002),
the Spin model (Spin) described in (Takamura et
al., 2005), the shortest path (short-path) described
in (Kamps et al, 2004), and the bootstrapping
(bootstrap) method described in (Hu and Liu,
2004).
4.1 Comparisons with other methods
This method could be used in a semi-supervised
setting where a set of labeled words are used and
the system learns from these labeled nodes and
from other unlabeled nodes. Under this setting, we
compare our method to the spin model described
in (Takamura et al, 2005). Table 2 compares the
performance using 10-fold cross validation. The
table shows that the proposed method outperforms
the spin model. The spin model approach uses
word glosses, WordNet synonym, hypernym, and
antonym relations, in addition to co-occurrence
statistics extracted from corpus. The proposed
method achieves better performance by only using
WordNet synonym, hypernym and similar to rela-
tions. Adding co-occurrence statistics slightly im-
proved performance, while using glosses did not
help at all.
We also compare our method to the SO-PMI
method presented in (Turney, 2002). They de-
scribe this setting as unsupervised (Turney, 2002)
because they only use 14 seeds as paradigm words
that define the semantic orientation rather than
train the model. After (Turney, 2002), we use our
method to predict semantic orientation of words in
the General Inquirer lexicon (Stone et al, 1966)
using only 14 seed words. The network we used
contains only WordNet relations. No glosses or
co-occurrence statistics are used. The results com-
paring the SO-PMI method with different dataset
sizes, the spin model, and the proposed method
using only 14 seeds is shown in Table 2. We no-
399
Table 1: Accuracy for adjectives only for the spin
model, the bootstrap method, and the random
walk model.
spin-model bootstrap short-path rand-walks
83.6 72.8 68.8 88.8
tice that the randomwalk method outperforms SO-
PMI when SO-PMI uses datasets of sizes 1? 107
and 2 ? 109 words. The performance of SO-PMI
and the random walk methods are comparable
when SO-PMI uses a very large dataset (1 ? 1011
words). The performance of the spin model ap-
proach is also comparable to the other 2 meth-
ods. The advantages of the random walk method
over SO-PMI is that it is faster and it does not
need a very large corpus like the one used by SO-
PMI. Another advantage is that the random walk
method can be used along with the labeled data
from the General Inquirer lexicon (Stone et al,
1966) to get much better performance. This is
costly for the SO-PMI method because that will
require the submission of almost 4000 queries to a
commercial search engine.
We also compare our method to the bootstrap-
ping method described in (Hu and Liu, 2004), and
the shortest path method described in (Kamps et
al., 2004). We build a network using only Word-
Net synonyms and hypernyms. We restrict the test
set to the set of adjectives in the General Inquirer
lexicon (Stone et al, 1966) because this method
is mainly interested in classifying adjectives. The
performance of the spin model method, the boot-
strapping method, the shortest path method, and
the random walk method for only adjectives is
shown in Table 1. We notice from the table that the
random walk method outperforms both the spin
model, the bootstrapping method, and the short-
est path method for adjectives. The reported ac-
curacy for the shortest path method only considers
the words it could assign a non-zero orientation
value. If we consider all words, the accuracy will
drop to around 61%.
4.1.1 Varying Parameters
As we mentioned in Section 3.4, we use a param-
eter m to put an upper bound on the length of ran-
dom walks. In this section, we explore the impact
Table 2: Accuracy for SO-PMI with different
dataset sizes, the spin model, and the random
walks model for 10-fold cross validation and 14
seeds.
- CV 14 seeds
SO-PMI (1? 107) - 61.3
SO-PMI (2? 109) - 76.1
SO-PMI (1? 1011) - 82.8
Spin Model 91.5 81.9
Random Walks 93.1 82.1
of this parameter on our method?s performance.
Figure 1 shows the accuracy of the randomwalk
method as a function of the maximum number of
steps m. m varies from 5 to 50. We use a net-
work built from WordNet synonyms and hyper-
nyms only. The number of samples k was set to
1000. We perform 10-fold cross validation using
the General Inquirer lexicon. We notice that the
maximum number of steps m has very little im-
pact on performance until it rises above 30. When
it does, the performance drops by no more than
1%, and then it does not change anymore as m
increases. An interesting observation is that the
proposed method performs quite well with a very
small number of steps (around 10). We looked at
the dataset to understand why increasing the num-
ber of steps beyond 30 negatively affects perfor-
mance. We found out that when the number of
steps is very large, compared to the diameter of the
graph, the random walk that starts at ambiguous
words, that are hard to classify, have the chance
of moving till it hits a node in the opposite class.
That does not happen when the limit on the num-
ber of steps is smaller because those walks are then
terminated without hitting any labeled nodes and
hence ignored.
Next, we study the effect of the random of sam-
ples k on our method?s performance. As explained
in Section 3.4, k is the number of samples used
by the Monte Carlo algorithm to find an estimate
for the hitting time. Figure 2 shows the accuracy
of the random walks method as a function of the
number of samples k. We use the same settings as
in the previous experiment. the only difference is
that we fix m at 15 and vary k from 10 to 20000
(note the logarithmic scale). We notice that the
performance is badly affected, when the value of
k is very small (less than 100). We also notice that
400
after 1000, varying k has very little, if any, effect
on performance. This shows that the Monte Carlo
algorithm for computing the random walks hitting
time performs quite well with values of the num-
ber of samples as small as 1000.
The preceding experiments suggest that the pa-
rameter have very little impact on performance.
This suggests that the approach is fairly robust
(i.e., it is quite insensitive to different parameter
settings).
Figure 1: The effect of varying the maximum
number of steps (m) on accuracy.
Figure 2: The effect of varying the number of sam-
ples (k) on accuracy.
4.1.2 Other Experiments
We now measure the performance of the proposed
method when the system is allowed to abstain
from classifying the words for which it have low
confidence. We regard the ratio between the hit-
ting time to positive words and hitting time to neg-
ative words as a confidence measure and evaluate
the top words with the highest confidence level at
different values of threshold. Figure 4 shows the
accuracy for 10-fold cross validation and for us-
ing only 14 seeds at different thresholds. We no-
tice that the accuracy improves by abstaining from
classifying the difficult words. The figure shows
that the top 60% words are classified with an ac-
curacy greater than 99% for 10-fold cross valida-
tion and 92% with 14 seed words. This may be
compared to the work descibed in (Takamura et
al., 2005) where they achieve the 92% level when
they only consider the top 1000 words (28%).
Figure 3 shows a learning curve displaying how
the performance of the proposed method is af-
fected with varying the labeled set size (i.e., the
number of seeds). We notice that the accuracy ex-
ceeds 90% when the training set size rises above
20%. The accuracy steadily increases as the la-
beled data increases.
We also looked at the classification accuracy for
different parts of speech in Figure 5. we notice
that, in the case of 10-fold cross validation, the
performance is consistent across parts of speech.
However, when we only use 14 seeds all of which
are adjectives, similar to (Turney and Littman,
2003), we notice that the performance on adjec-
tives is much better than other parts of speech.
When we use 14 seeds but replace some of the
adjectives with verbs and nouns like (love, harm,
friend, enemy), the performance for nouns and
verbs improves considerably at the cost of losing a
little bit of the performance on adjectives. We had
a closer look at the results to find out what are the
reasons behind incorrect predictions. We found
two main reasons. First, some words are ambigu-
ous and has more than one sense, possible with
different orientations. Disambiguating the sense
of words given their context before trying to pre-
dict their polarity should solve this problem. The
second reason is that some words have very few
connection in thesaurus. A possible solution to
this might be identifying those words and adding
more links to them from glosses of co-occurrence
statistics in corpus.
Figure 3: The effect of varying the number of
seeds on accuracy.
401
Figure 4: Accuracy for words with high confi-
dence measure.
Figure 5: Accuracy for different parts of speech.
5 Conclusions
Predicting the semantic orientation of words is
a very interesting task in Natural Language Pro-
cessing and it has a wide variety of applications.
We proposed a method for automatically predict-
ing the semantic orientation of words using ran-
dom walks and hitting time. The proposed method
is based on the observation that a random walk
starting at a given word is more likely to hit an-
other word with the same semantic orientation be-
fore hitting a word with a different semantic ori-
entation. The proposed method can be used in a
semi-supervised setting where a training set of la-
beled words is used, and in an unsupervised setting
where only a handful of seeds is used to define the
two polarity classes. We predict semantic orienta-
tion with high accuracy. The proposed method is
fast, simple to implement, and does not need any
corpus.
Acknowledgments
This research was funded by the Office of the
Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab.
All statements of fact, opinion or conclusions con-
tained herein are those of the authors and should
not be construed as representing the official views
or policies of IARPA, the ODNI or the U.S. Gov-
ernment.
References
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing wordnet for fuzzy sentiment: Sentiment tag ex-
traction from wordnet glosses. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics (EACL
2006).
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08).
Andrea Esuli and Fabrizio Sebastiani. 2005. Deter-
mining the semantic orientation of terms through
gloss classification. In Proceedings of the 14th Con-
ference on Information and Knowledge Manage-
ment (CIKM 2005), pages 617?624.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation (LREC
2006), pages 417?422.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on
European chapter of the Association for Computa-
tional Linguistics, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD ?04: Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to mea-
sure semantic orientations of adjectives. In National
Institute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2006), pages 355?
363.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics (COLING 2004), pages 1367?1373.
402
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 341?349.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: capturing favorability using natural
language processing. In K-CAP ?03: Proceedings
of the 2nd international conference on Knowledge
capture, pages 70?77.
J. Norris. 1997. Markov chains. Cambridge Univer-
sity Press.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pages 105?112.
Philip Stone, Dexter Dunphy, Marchall Smith, and
Daniel Ogilvie. 1966. The general inquirer: A com-
puter approach to content analysis. The MIT Press.
Martin Szummer and Tommi Jaakkola. 2002. Partially
labeled classification with markov random walks.
In Advances in Neural Information Processing Sys-
tems, pages 945?952.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 133?140.
Richard M. Tong. 2001. An operational system for de-
tecting and tracking opinions in on-line discussion.
Workshop note, SIGIR 2001 Workshop on Opera-
tional Text Classification.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 417?424.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study
of evaluative and speculative language. In Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue, pages 1?10.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the 2003 conference on
Empirical methods in natural language processing,
pages 129?136.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In In ICML, pages
912?919.
403
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 592?597,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying the Semantic Orientation of Foreign Words
Ahmed Hassan
EECS Department
University of Michigan
Ann Arbor, MI
hassanam@umich.edu
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI
amjbara@umich.edu
Rahul Jha
EECS Department
University of Michigan
Ann Arbor, MI
rahuljha@umich.edu
Dragomir Radev
EECS Department and School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We present a method for identifying the pos-
itive or negative semantic orientation of for-
eign words. Identifying the semantic orienta-
tion of words has numerous applications in the
areas of text classification, analysis of prod-
uct review, analysis of responses to surveys,
and mining online discussions. Identifying
the semantic orientation of English words has
been extensively studied in literature. Most of
this work assumes the existence of resources
(e.g. Wordnet, seeds, etc) that do not exist
in foreign languages. In this work, we de-
scribe a method based on constructing a mul-
tilingual network connecting English and for-
eign words. We use this network to iden-
tify the semantic orientation of foreign words
based on connection between words in the
same language as well as multilingual connec-
tions. The method is experimentally tested us-
ing a manually labeled set of positive and neg-
ative words and has shown very promising re-
sults.
1 Introduction
A great body of research work has focused on iden-
tifying the semantic orientation of words. Word po-
larity is a very important feature that has been used
in several applications. For example, the problem
of mining product reputation from Web reviews has
been extensively studied (Turney, 2002; Morinaga
et al, 2002; Nasukawa and Yi, 2003; Popescu and
Etzioni, 2005; Banea et al, 2008). This is a very
important task given the huge amount of product re-
views written on the Web and the difficulty of man-
ually handling them. Another interesting applica-
tion is mining attitude in discussions (Hassan et al,
2010), where the attitude of participants in a discus-
sion is inferred using the text they exchange.
Due to its importance, several researchers have
addressed the problem of identifying the semantic
orientation of individual words. This work has al-
most exclusively focused on English. Most of this
work used several language dependent resources.
For example Turney and Littman (2003) use the en-
tire English Web corpus by submitting queries con-
sisting of the given word and a set of seeds to a
search engine. In addition, several other methods
have used Wordnet (Miller, 1995) for connecting se-
mantically related words (Kamps et al, 2004; Taka-
mura et al, 2005; Hassan and Radev, 2010).
When we try to apply those methods to other lan-
guages, we run into the problem of the lack of re-
sources in other languages when compared to En-
glish. For example, the General Inquirer lexicon
(Stone et al, 1966) has thousands of English words
labeled with semantic orientation. Most of the lit-
erature has used it as a source of labeled seeds or
for evaluation. Such lexicons are not readily avail-
able in other languages. Another source that has
been widely used for this task is Wordnet (Miller,
1995). Even though other Wordnets have been built
for other languages, their coverage is very limited
when compared to the English Wordnet.
In this work, we present a method for predicting
the semantic orientation of foreign words. The pro-
592
Figure 1: Sparse Foreign Networks are connected to
Dense English Networks. Dashed nodes represent la-
beled positive and negative seeds.
posed method is based on creating a multilingual
network of words that represents both English and
foreign words. The network has English-English
connections, as well as foreign-foreign connections
and English-foreign connections. This allows us to
benefit from the richness of the resources built for
the English language and in the meantime utilize
resources specific to foreign languages. Figure 1
shows a multilingual network where a sparse foreign
network and a dense English network are connected.
We then define a random walk model over the multi-
lingual network and predict the semantic orientation
of any given word by comparing the mean hitting
time of a random walk starting from it to a positive
and a negative set of seed English words.
We use both Arabic and Hindi for experiments.
We compare the performance of several methods us-
ing the foreign language resources only and the mul-
tilingual network that has both English and foreign
words. We show that bootstrapping from languages
with dense resources such as English is useful for
improving the performance on other languages with
limited resources.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work.
We define our problem and explain our approach in
Section 3. Results and discussion are presented in
Section 4. We conclude in Section 5.
2 Related Work
The problem of identifying the polarity of individual
words is a well-studied problem that attracted sev-
eral research efforts in the past few years. In this
section, we survey several methods that addressed
this problem.
The work of Hatzivassiloglou and McKeown
(1997) is among the earliest efforts that addressed
this problem. They proposed a method for identify-
ing the polarity of adjectives. Their method is based
on extracting all conjunctions of adjectives from a
given corpus and then they classify each conjunc-
tive expression as either the same orientation such
as ?simple and well-received? or different orienta-
tion such as ?simplistic but well-received?. Words
are clustered into two sets and the cluster with the
higher average word frequency is classified as posi-
tive.
Turney and Littman (2003) identify word polar-
ity by looking at its statistical association with a set
of positive/negative seed words. They use two sta-
tistical measures for estimating association: Point-
wise Mutual Information (PMI) and Latent Seman-
tic Analysis (LSA). Co-occurrence statistics are col-
lected by submitting queries to a search engine. The
number of hits for positive seeds, negative seeds,
positives seeds near the given word, and negative
seeds near the given word are used to estimate the
association of the given word to the positive/negative
seeds.
Wordnet (Miller, 1995), thesaurus and co-
occurrence statistics have been widely used to mea-
sure word relatedness by several semantic orienta-
tion prediction methods. Kamps et al (2004) use the
length of the shortest-path in Wordnet connecting
any given word to positive/negative seeds to iden-
tify word polarity. Hu and Liu (2004) use Word-
net synonyms and antonyms to bootstrap from words
with known polarity to words with unknown polar-
ity. They assign any given word the label of its syn-
onyms or the opposite label of its antonyms if any of
them are known.
Kanayama and Nasukawa (2006) used syntactic
features and context coherency, defined as the ten-
dency for same polarities to appear successively,
to acquire polar atoms. Takamura et al (2005)
proposed using spin models for extracting seman-
tic orientation of words. They construct a network
of words using gloss definitions, thesaurus and co-
occurrence statistics. They regard each word as an
electron. Each electron has a spin and each spin has
a direction taking one of two values: up or down.
593
Two neighboring spins tend to have the same orien-
tation from an energetic point of view. Their hypoth-
esis is that as neighboring electrons tend to have the
same spin direction, neighboring words tend to have
similar polarity. Hassan and Radev (2010) use a ran-
dom walk model defined over a word relatedness
graph to classify words as either positive or negative.
Words are connected based on Wordnet relations as
well as co-occurrence statistics. They measure the
random walk mean hitting time of the given word to
the positive set and the negative set. They show that
their method outperforms other related methods and
that it is more immune to noisy word connections.
Identifying the semantic orientation of individ-
ual words is closely related to subjectivity analy-
sis. Subjectivity analysis focused on identifying
text that presents opinion as opposed to objective
text that presents factual information (Wiebe, 2000).
Some approaches to subjectivity analysis disregard
the context phrases and words appear in (Wiebe,
2000; Hatzivassiloglou and Wiebe, 2000; Banea
et al, 2008), while others take it into considera-
tion (Riloff and Wiebe, 2003; Yu and Hatzivas-
siloglou, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005).
3 Approach
The general goal of this work is to mine the seman-
tic orientation of foreign words. We do this by cre-
ating a multilingual network of words. In this net-
work two words are connected if we believe that they
are semantically related. The network has English-
English, English-Foreign and Foreign-Foreign con-
nections. Some of the English words will be used as
seeds for which we know the semantic orientation.
Given such a network, we will measure the mean
hitting time in a random walk starting at any given
word to the positive set of seeds and the negative set
of seeds. Positive words will be more likely to hit the
positive set faster than hitting the negative set and
vice versa. In the rest of this section, we define how
the multilingual word network is built and describe
an algorithm for predicting the semantic orientation
of any given word.
3.1 Multilingual Word Network
We build a network G(V,E) where V = Ven ? Vfr
is the union of a set of English and foreign words.
E is a set of edges connecting nodes in V . There
are three types of connections: English-English con-
nections, Foreign-Foreign connections and English-
Foreign connections.
For the English-English connections, we use
Wordnet (Miller, 1995). Wordnet is a large lexical
database of English. Words are grouped in synsets
to express distinct concepts. We add a link between
two words if they occur in the same Wordnet synset.
We also add a link between two words if they have a
hypernym or a similar-to relation.
Foreign-Foreign connections are created in a sim-
ilar way to the English connections. Some other lan-
guages have lexical resources based on the design of
the Princeton English Wordnet. For example: Euro
Wordnet (EWN) (Vossen, 1997), Arabic Wordnet
(AWN) (Elkateb, 2006; Black and Fellbaum, 2006;
Elkateb and Fellbaum, 2006) and the Hindi Word-
net (Narayan et al, 2002; S. Jha, 2001). We also use
co-occurrence statistics similar to the work of Hatzi-
vassiloglou and McKeown (1997).
Finally, to connect foreign words to English
words, we use a foreign to English dictionary. For
every word in a list of foreign words, we look up
its meaning in a dictionary and add an edge between
the foreign word and every other English word that
appeared as a possible meaning for it.
3.2 Semantic Orientation Prediction
We use the multilingual network we described above
to predict the semantic orientation of words based
on the mean hitting time to two sets of positive and
negative seeds. Given the graph G(V,E), we de-
scribed in the previous section, we define the transi-
tion probability from node i to node j by normaliz-
ing the weights of the edges out from i:
P (j|i) = Wij/
?
k
Wik (1)
The mean hitting time h(i|j) is the average num-
ber of steps a random walker, starting at i, will take
to enter state j for the first time (Norris, 1997). Let
the average number of steps that a random walker
starting at some node i will need to enter a state
594
k ? S be h(i|S). It can be formally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h(j|S) + 1 otherwise
(2)
where pij is the transition probability between
node i and node j.
Given two lists of seed English words with known
polarity, we define two sets of nodes S+ and S?
representing those seeds. For any given word w, we
calculate the mean hitting time between w and the
two seed sets h(w|S+) and h(w|S?). If h(w|S+)
is greater than h(w|S?), the word is classified as
negative, otherwise it is classified as positive. We
used the list of labeled seeds from (Hatzivassiloglou
and McKeown, 1997) and (Stone et al, 1966). Sev-
eral other similarity measures may be used to predict
whether a given word is closer to the positive seeds
list or the negative seeds list (e.g. average shortest
path length (Kamps et al, 2004)). However hit-
ting time has been shown to be more efficient and
more accurate (Hassan and Radev, 2010) because it
measures connectivity rather than distance. For ex-
ample, the length of the shortest path between the
words ?good? and ?bad? is only 5 (Kamps et al,
2004).
4 Experiments
4.1 Data
We used Wordnet (Miller, 1995) as a source of syn-
onyms and hypernyms for linking English words in
the word relatedness graph. We used two foreign
languages for our experiments Arabic and Hindi.
Both languages have a Wordnet that was constructed
based on the design the Princeton English Wordnet.
Arabic Wordnet (AWN) (Elkateb, 2006; Black and
Fellbaum, 2006; Elkateb and Fellbaum, 2006) has
17561 unique words and 7822 synsets. The Hindi
Wordnet (Narayan et al, 2002; S. Jha, 2001) has
56,928 unique words and 26,208 synsets.
In addition, we used three lexicons with words la-
beled as either positive or negative. For English, we
used the General Inquirer lexicon (Stone et al, 1966)
as a source of seed labeled words. The lexicon con-
tains 4206 words, 1915 of which are positive and
2291 are negative. For Arabic and Hindi we con-
structed a labeled set of 300 words for each language
0
10
20
30
40
50
60
70
80
90
100
Arabic Hindi
SO-PMI HT-FR HT-FR+EN
Figure 2: Accuracy of the proposed method and baselines
for both Arabic and Hindi.
for use in evaluation. Those sets were labeled by two
native speakers of each language. We also used an
Arabic-English and a Hindi-English dictionaries to
generate Foreign-English links.
4.2 Results and Discussion
We performed experiments on the data described in
the previous section. We compare our results to
two baselines. The first is the SO-PMI method de-
scribed in (Turney and Littman, 2003). This method
is based on finding the semantic association of any
given word to a set of positive and a set of negative
words. It can be calculated as follows:
SO-PMI(w) = log
hitsw,pos ? hitsneg
hitsw,neg ? hitspos
(3)
where w is a word with unknown polarity,
hitsw,pos is the number of hits returned by a com-
mercial search engine when the search query is the
given word and the disjunction of all positive seed
words. hitspos is the number of hits when we
search for the disjunction of all positive seed words.
hitsw,neg and hitsneg are defined similarly. We used
7 positive and 7 negative seeds as described in (Tur-
ney and Littman, 2003).
The second baseline constructs a network of for-
eign words only as described earlier. It uses mean
hitting time to find the semantic association of any
given word. We used 10 fold cross validation for this
experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use
the hitting time as before to predict semantic orien-
595
tation. We used the English words from (Stone et
al., 1966) as seeds and the labeled foreign words
for evaluation. We will refer to this system as
HT-FR + EN.
Figure 2 compares the accuracy of the three meth-
ods for Arabic and Hindi. We notice that the
SO-PMI and the hitting time based methods per-
form poorly on both Arabic and Hindi. This is
clearly evident when we consider that the accuracy
of the two systems on English was 83% and 93% re-
spectively (Turney and Littman, 2003; Hassan and
Radev, 2010). This supports our hypothesis that
state of the art methods, designed for English, per-
form poorly on foreign languages due to the limited
amount of resources available in foreign languages
compared to English. The figure also shows that the
proposed method, which combines resources from
both English and foreign languages, performs sig-
nificantly better. Finally, we studied how much im-
provement is achieved by including links between
foreign words from global Wordnets. We found out
that it improves the performance by 2.5% and 4%
for Arabic and Hindi respectively.
5 Conclusions
We addressed the problem of predicting the seman-
tic orientation of foreign words. All previous work
on this task has almost exclusively focused on En-
glish. Applying off-the-shelf methods developed for
English to other languages does not work well be-
cause of the limited amount of resources available
in foreign languages compared to English. We pro-
posed a method based on the construction of a multi-
lingual network that uses both language specific re-
sources as well as the rich semantic relations avail-
able in English. We then use a model that computes
the mean hitting time to a set of positive and neg-
ative seed words to predict whether a given word
has a positive or a negative semantic orientation.
We showed that the proposed method can predict
semantic orientation with high accuracy. We also
showed that it outperforms state of the art methods
limited to using language specific resources.
Acknowledgments
This research was funded in part by the Office
of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the ofcial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Elkateb S. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Black, W. and C. Fellbaum. 2006. Introducing the
arabic wordnet project. In Third International Word-
Net Conference.
Black. W. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Elkateb, S. and C. Fellbaum. 2006. Building a word-
net for arabic. In Fifth International Conference on
Language Resources and Evaluation.
Black W. Vossen P. Farwell D. Rodrguez H. Pease A.
Alkhalifa M. Elkateb, S. 2006. Arabic wordnet and
the challenges of arabic. In Arabic NLP/MT Confer-
ence.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD?04, pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP?06, pages 355?363.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the web. In KDD?02, pages 341?349.
596
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and P. Bhattacharyya. 2002. An experience in build-
ing the indo wordnet - a wordnet for hindi. In First
International Conference on Global WordNet.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP?05, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
P. Pande P. Bhattacharyya S. Jha, D. Narayan. 2001. A
wordnet for hindi. In International Workshop on Lexi-
cal Resources in Natural Language Processing.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL?02, pages 417?424.
P. Vossen. 1997. Eurowordnet: a multilingual database
for information retrieval. In DELOS workshop on
Cross-language Information Retrieval.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
597
Proceedings of the TextGraphs-7 Workshop at ACL, pages 6?14,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Extracting Signed Social Networks From Text
Ahmed Hassan
Microsoft Research
Redmond, WA, USA
hassanam@microsoft.com
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
Most of the research on social networks has al-
most exclusively focused on positive links be-
tween entities. There are much more insights
that we may gain by generalizing social net-
works to the signed case where both positive
and negative edges are considered. One of the
reasons why signed social networks have re-
ceived less attention that networks based on
positive links only is the lack of an explicit
notion of negative relations in most social net-
work applications. However, most such appli-
cations have text embedded in the social net-
work. Applying linguistic analysis techniques
to this text enables us to identify both positive
and negative interactions. In this work, we
propose a new method to automatically con-
struct a signed social network from text. The
resulting networks have a polarity associated
with every edge. Edge polarity is a means for
indicating a positive or negative affinity be-
tween two individuals. We apply the proposed
method to a larger amount of online discus-
sion posts. Experiments show that the pro-
posed method is capable of constructing net-
works from text with high accuracy. We also
connect out analysis to social psychology the-
ories of signed network, namely the structural
balance theory.
1 Introduction
A great body of research work has focused on so-
cial network analysis. Social network analysis plays
a huge role in understanding and improving so-
cial computing applications. Most of this research
has almost exclusively focused on positive links be-
tween individuals (e.g. friends, fans, followers,
etc.). However, if we carefully examine the relation-
ships between individuals in online communities,
we will find out that limiting links to positive inter-
actions is a very simplistic assumption. It is true that
people show positive attitude by labeling others as
friends, and showing agreement, but they also show
disagreement, and antagonism toward other mem-
bers of the online community. Discussion forums
are one example that makes it clear that considering
both positive and negative interactions is essential
for understanding the rich relationships that develop
between individuals in online communities.
If considering both negative and positive interac-
tions will provide much more insight toward under-
standing the social network, why did most of pre-
vious work only focus on positive interactions? We
think that one of the main reasons behind this is the
lack of a notion for explicitly labeling negative re-
lations. For example, most social web applications
allow people to mark others as friends, like them,
follow them, etc. However, they do not allow people
to explicitly label negative relations with others.
Previous work has built networks from discus-
sions by linking people who reply to one another.
Even though, the mere fact that X replied to Y ?s
post does show an interaction, it does not tell us any-
thing about the type of that interaction. In this case,
the type of interaction is not readily available; how-
ever it may be mined from the text that underlies
the social network. Hence, if we examine the text
exchanged between individuals, we may be able to
come up with conclusions about, not only the exis-
tence of an interaction, but also its type.
In this work, we apply Natural Language Pro-
cessing techniques to text correspondences ex-
changed between individuals to identify the under-
6
lying signed social structure in online communities.
We present and compare several algorithms for iden-
tifying user attitude and for automatically construct-
ing a signed social network representation. We ap-
ply the proposed methods to a large set of discussion
posts. We evaluate the performance using a manu-
ally labeled dataset.
The input to our algorithm is a set of text corre-
spondences exchanged between users (e.g. posts or
comments). The output is a signed network where
edges signify the existence of an interaction between
two users. The resulting network has polarity asso-
ciated with every edge. Edge polarity is a means for
indicating a positive or negative affinity between two
individuals.
The proposed method was applied to a very large
dataset of online discussions. To evaluate our auto-
mated procedure, we asked human annotators to ex-
amine text correspondences exchanged between in-
dividuals and judge whether their interaction is pos-
itive or negative. We compared the edge signs that
had been automatically identified to edges manually
created by human annotators.
We also connected our analysis to social psychol-
ogy theories, namely the Structural Balance The-
ory (Heider, 1946). The balance theory has been
shown to hold both theoretically (Heider, 1946) and
empirically (Leskovec et al, 2010b) for a variety
of social community settings. Showing that it also
holds for our automatically constructed network fur-
ther validates our results.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work
on mining sentiment from text, mining online dis-
cussions, extracting social networks from text, and
analyzing signed social networks. We define our
problem and explain our approach in Section 3. Sec-
tion 4 describes our dataset. Results and discussion
are presented in Section 5. We present a possible
application for the proposed approach in Section 6.
We conclude in Section 7.
2 Related Work
In this section, we survey several lines of research
that are related to our work.
2.1 Mining Sentiment from Text
Our general goal of mining attitude from one in-
dividual toward another makes our work related to
a huge body of work on sentiment analysis. One
such line of research is the well-studied problem
of identifying the of individual words. In previ-
ous work, Hatzivassiloglou and McKeown (1997)
proposed a method to identify the polarity of ad-
jectives based on conjunctions linking them in a
large corpus. Turney and Littman (2003) used sta-
tistical measures to find the association between a
given word and a set of positive/negative seed words.
Takamura et al (2005) used the spin model to ex-
tract word semantic orientation. Finally, Hassan and
Radev (2010) use a random walk model defined over
a word relatedness graph to classify words as either
positive or negative.
Subjectivity analysis is yet another research line
that is closely related to our general goal of mining
attitude. The objective of subjectivity analysis is to
identify text that presents opinion as opposed to ob-
jective text that presents factual information (Wiebe,
2000). Prior work on subjectivity analysis mainly
consists of two main categories: subjectivity of a
phrase or word is analyzed regardless of the context
(Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;
Banea et al, 2008), or within its context (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-
sukawa and Yi, 2003; Popescu and Etzioni, 2005).
Hassan et al (2010) presents a method for identify-
ing sentences that display an attitude from the text
writer toward the text recipient. Our work is dif-
ferent from subjectivity analysis because we are not
only interested in discriminating between opinions
and facts. Rather, we are interested in identifying
the polarity of interactions between individuals. Our
method is not restricted to phrases or words, rather it
generalizes this to identifying the polarity of an in-
teraction between two individuals based on several
posts they exchange.
2.2 Mining Online Discussions
Our use of discussion threads as a source of data
connects us to some previous work on mining
online discussions. Lin et al (2009) proposed
a sparse coding-based model that simultaneously
models semantics and structure of threaded discus-
7
sions. Huang et al (2007) learn SVM classifiers
from data to extract (thread-title, reply) pairs. Their
objective was to build a chatbot for a certain do-
main using knowledge from online discussion fo-
rums. Shen et al (2006) proposed three clustering
methods for exploiting the temporal information in
discussion streams, as well as an algorithm based on
linguistic features to analyze discourse structure in-
formation.
2.3 Extracting Social Networks from Text
Little work has been done on the front of extracting
social relations between individuals from text. El-
son et al (2010) present a method for extracting so-
cial networks from nineteenth-century British nov-
els and serials. They link two characters based on
whether they are in conversation or not. McCal-
lum et al (2007) explored the use of structured data
such as email headers for social network construc-
tion. Gruzd and Hyrthonthwaite (2008) explored the
use of post text in discussions to study interaction
patterns in e-learning communities.
Our work is related to this line of research because
we employ natural language processing techniques
to reveal embedded social structures. Despite sim-
ilarities, our work is uniquely characterized by the
fact that we extract signed social networks from text.
2.4 Signed Social Networks
Most of the work on social networks analysis has
only focused on positive interactions. A few recent
papers have taken the signs of edges into account.
Brzozowski et al (2008) study the positive and
negative relationships between users of Essembly.
Essembly is an ideological social network that dis-
tinguishes between ideological allies and nemeses.
Kunegis et al (2009) analyze user relationships in
the Slashdot technology news site. Slashdot allows
users of the website to tag other users as friends or
foes, providing positive and negative endorsements.
Leskovec et al (2010c) study signed social networks
generated from Slashdot, Epinions, and Wikipedia.
They also connect their analysis to theories of signed
networks from social psychology. A similar study
used the same datasets for predicting positive and
negative links given their context (Leskovec et al,
2010a). Other work addressed the problem of clus-
tering signed networks by taking both positive and
negative edges into consideration (Yang et al, 2007;
Doreian and Mrvar, 2009).
All this work has been limited to analyzing a
handful of datasets for which an explicit notion of
both positive and negative relations exists. Our work
goes beyond this limitation by leveraging the power
of natural language processing to automate the dis-
covery of signed social networks using the text em-
bedded in the network.
3 Approach
The general goal of this work is to mine attitude be-
tween individuals engaged in an online discussion.
We use that to extract a signed social network rep-
resenting the interactions between different partici-
pants. Our approach consists of several steps. In
this section, we will explain how we identify senti-
ment at the word level (i.e. polarity), at the sentence
level (i.e. attitude), and finally generalize over this
to find positive/negative interactions between indi-
viduals based on their text correspondences.
The first step toward identifying attitude is to
identify polarized words. Polarized words are very
good indicators of subjective sentences and hence
we their existence will be highly correlated with the
existence of attitude. The method we use for identi-
fying word polarity is a Random Walk based method
over a word relatedness graph (Hassan and Radev,
2010).
The following step is to move to the sentence level
by examining different sentences to find out which
sentences display an attitude from the text writer to
the recipient. We train a classifier based on several
sources of information to make this prediction (Has-
san et al, 2010). We use lexical items, polarity tags,
part-of-speech tags, and dependency parse trees to
train a classifier that identifies sentences with atti-
tude.
Finally, we build a network connecting partici-
pants based on their interactions. We use the predic-
tions we made both at the word and sentence levels
to associate a sign to every edge.
3.1 Identified Positive/Negative Words
The first step toward identifying attitude is to iden-
tify words with positive/negative semantic orienta-
tion. The semantic orientation or polarity of a word
8
indicates the direction the word deviates from the
norm (Lehrer, 1974). Past work has demonstrated
that polarized words are very good indicators of
subjective sentences (Hatzivassiloglou and Wiebe,
2000; Wiebe et al, 2001). We use a Random Walk
based method to identify the semantic orientation
of words (Hassan and Radev, 2010). We construct
a graph where each node represents a word/part-
of-speech pair. We connect nodes based on syn-
onyms, hypernyms, and similar-to relations from
WordNet (Miller, 1995). For words that do not
appear in WordNet, we use distributional similar-
ity (Lee, 1999) as a proxy for word relatedness.
We use a list of words with known polarity (Stone
et al, 1966) to label some of the nodes in the graph.
We then define a random walk model where the set
of nodes correspond to the state space, and transi-
tion probabilities are estimated by normalizing edge
weights. We assume that a random surfer walks
along the word relatedness graph starting from a
word with unknown polarity. The walk continues
until the surfer hits a word with a known polarity.
Seed words with known polarity act as an absorb-
ing boundary for the random walk. We calculate the
mean hitting time (Norris, 1997) from any word with
unknown polarity to the set of positive seeds and the
set of negative seeds. If the absolute difference of
the two mean hitting times is below a certain thresh-
old, the word is classified as neutral. Otherwise, it
is labeled with the class that has the smallest mean
hitting time.
3.2 Identifying Attitude from Text
The first step toward identifying attitude is to iden-
tify words with positive/negative semantic orienta-
tion. The semantic orientation or polarity of a word
indicates the direction the word deviates from the
norm (Lehrer, 1974). We use OpinionFinder (Wil-
son et al, 2005a) to identify words with positive
or negative semantic orientation. The polarity of a
word is also affected by the context where the word
appears. For example, a positive word that appears
in a negated context should have a negative polarity.
Other polarized words sometimes appear as neutral
words in some contexts. Hence, we use the method
described in (Wilson et al, 2005b) to identify the
contextual polarity of words given their isolated po-
larity. A large set of features is used for that purpose
including words, sentences, structure, and other fea-
tures.
Our overall objective is to find the direct attitude
between participants. Hence after identifying the se-
mantic orientation of individual words, we move on
to predicting which polarized expressions target the
addressee and which are not.
Sentences that show an attitude are different from
subjective sentences. Subjective sentences are sen-
tences used to express opinions, evaluations, and
speculations (Riloff and Wiebe, 2003). While ev-
ery sentence that shows an attitude is a subjective
sentence, not every subjective sentence shows an at-
titude toward the recipient. A discussion sentence
may display an opinion about any topic yet no atti-
tude.
We address the problem of identifying sentences
with attitude as a relation detection problem in a su-
pervised learning setting (Hassan et al, 2010). We
study sentences that use second person pronouns and
polarized expressions. We predict whether the sec-
ond person pronoun is related to the polarized ex-
pression or not. We regard the second person pro-
noun and the polarized expression as two entities
and try to learn a classifier that predicts whether the
two entities are related or not. The text connecting
the two entities offers a very condensed represen-
tation of the information needed to assess whether
they are related or not. For example the two sen-
tences ?you are completely unqualified? and ?you
know what, he is unqualified ...? show two differ-
ent ways the words ?you?, and ?unqualified? could
appear in a sentence. In the first case the polarized
word unqualified refers to the word you. In the sec-
ond case, the two words are not related. The se-
quence of words connecting the two entities is a
very good predictor for whether they are related or
not. However, these paths are completely lexicalized
and consequently their performance will be limited
by data sparseness. To alleviate this problem, we
use higher levels of generalization to represent the
path connecting the two tokens. These representa-
tions are the part-of-speech tags, and the shortest
path in a dependency graph connecting the two to-
kens. We represent every sentence with several rep-
resentations at different levels of generalization. For
example, the sentence your ideas are very inspiring
will be represented using lexical, polarity, part-of-
9
speech, and dependency information as follows:
LEX: ?YOUR ideas are very POS?
POS: ?YOUR NNS VBP RB JJ POS?
DEP: ?YOUR poss nsubj POS?
3.2.1 A Text Classification Approach
In this method, we treat the problem as a topic
classification problem with two topics: having pos-
itive attitude and having negative attitude. As we
are only interested in attitude between participants
rather than sentiment in general, we restrict the text
we analyze to sentences that contain mentions of the
addressee (e.g. name or second person pronouns).
A similar approach for sentiment classification has
been presented in (Pang et al, ).
We represent text using the popular bag-of-words
approach. Every piece of text is represented using
a high dimensional feature space. Every word is
considered a feature. The tf-idf weighting schema
is used to calculate feature weights. tf, or term fre-
quency, is the number of time a term t occurred in
a document d. idf, or inverse document frequency,
is a measure of the general importance of the term.
It is obtained by dividing the total number of doc-
uments by the number of documents containing the
term. The logarithm of this value is often used in-
stead of the original value.
We used Support Vector Machines (SVMs) for
classification. SVM has been shown to be highly
effective for traditional text classification. We used
the SVM Light implementation with default param-
eters (Joachims, 1999). All stop words were re-
moved and all documents were length normalized
before training.
The set of features we use are the set of unigrams,
and bigrams representing the words, part-of-speech
tags, and dependency relations connecting the two
entities. For example the following features will be
set for the previous example:
YOUR ideas, YOUR NNS, YOUR poss,
poss nsubj, ...., etc.
We use Support Vector Machines (SVM) as a
learning system because it is good with handling
high dimensional feature spaces.
3.3 Extracting the Signed Network
In this subsection, we describe the procedure we
used to build the signed network given the compo-
nents we described in the previous subsections. This
procedure consists of two main steps. The first is
building the network without signs, and the second
is assigning signs to different edges.
To build the network, we parse our data to identify
different threads, posts and senders. Every sender is
represented with a node in the network. An edge
connects two nodes if there exists an interaction be-
tween the corresponding participants. We add a di-
rected edge A? B, if A replies to B?s posts at least
n times in m different threads. We set m, and n to
2 in most of our experiments. The interaction infor-
mation (i.e. who replies to whom) can be extracted
directly from the thread structure.
Once we build the network, we move to the more
challenging task in which we associate a sign with
every edge. We have shown in the previous section
how sentences with positive and negative attitude
can be extracted from text. Unfortunately the sign
of an interaction cannot be trivially inferred from the
polarity of sentences. For example, a single negative
sentence written by A and directed to B does not
mean that the interaction between A and B is neg-
ative. One way to solve this problem would be to
compare the number of negative sentences to posi-
tive sentences in all posts between A and B and clas-
sify the interaction according to the plurality value.
We will show later, in our experiment section, that
such a simplistic method does not perform well in
predicting the sign of an interaction.
As a result, we decided to pose the problem
as a classical supervised learning problem. We
came up with a set of features that we think are
good predictors of the interaction sign, and we
train a classifier using those features on a labeled
dataset. Our features include numbers and percent-
ages of positive/negative sentences per post, posts
per thread, and so on. A sentence is labeled as posi-
tive/negative if a relation has been detected in this
sentence between a second person pronoun and a
positive/negative expression. A post is considered
positive/negative based on the majority of relations
detected in it. We use two sets of features. The first
set is related to A only or B only. The second set
10
Participant Features
Number of posts per month for A (B)
Percentage of positive posts per month for A (B)
Percentage of negative posts per month for A (B)
gender
Interaction Features
Percentage/number of positive (negative) sentences per post
Percentage/number of positive (negative) posts per thread
Discussion Topic
Table 1: Features used by the Interaction Sign Classifier.
is related to the interactions between A and B. The
features are outlined in Table 1.
4 Data
Our data consists of a large amount of discussion
threads collected from online discussion forums. We
collected around 41, 000 threads and 1.2M posts
from the period between the end of 2008 and the end
of 2010. All threads were in English and had 5 posts
or more. They covered a wide range of topics in-
cluding: politics, religion, science, etc. The data was
tokenized, sentence-split, and part-of-speech tagged
with the OpenNLP toolkit. It was parsed with the
Stanford parser (Klein and Manning, 2003).
We randomly selected 5300 posts (having approx-
imately 1000 interactions), and asked human anno-
tators to label them. Our annotators were instructed
to read all the posts exchanged between two partic-
ipants and decide whether the interaction between
them is positive or negative. We used Amazon Me-
chanical Turk for annotations. Following previous
work (Callison-Burch, 2009; Akkaya et al, 2010),
we took several precautions to maintain data in-
tegrity. We restricted annotators to those based in
the US to maintain an acceptable level of English
fluency. We also restricted annotators to those who
have more than 95% approval rate for all previous
work. Moreover, we asked three different annota-
tors to label every interaction. The label was com-
puted by taking the majority vote among the three
annotators. We refer to this data as the Interactions
Dataset.
The kappa measure between the three groups of
annotations was 0.62. To better assess the quality
of the annotations, we asked a trained annotator to
label 10% of the data. We measured the agreement
between the expert annotator and the majority label
from the Mechanical Turk. The kappa measure was
Class Pos. Neg. Weigh. Avg.
TP Rate 0.847 0.809 0.835
FP Rate 0.191 0.153 0.179
Precision 0.906 0.71 0.844
Recall 0.847 0.809 0.835
F-Measure 0.875 0.756 0.838
Accuracy - - 0.835
Table 2: Interaction sign classifier evaluation.
0.69.
We trained the classifier that detects sentences
with attitude (Section 3.1) on a set of 4000 manu-
ally annotated sentences. None of this data overlaps
with the dataset described earlier. A similar annota-
tion procedure was used to label this data. We refer
to this data as the Sentences Dataset.
5 Results and Discussion
We performed experiments on the data described
in the previous section. We trained and tested the
sentence with attitude detection classifiers described
in Section 3.1 using the Sentences Dataset. We
also trained and tested the interaction sign classi-
fier described in Section 3.3 using the Interactions
Dataset. We build one unsigned network from ev-
ery topic in the data set. This results in a signed
social network for every topic (e.g. politics, eco-
nomics,etc.). We decided to build a network for ev-
ery topic as opposed to one single network because
the relation between any two individuals may vary
across topics. In the rest of this section, we will de-
scribe the experiments we did to assess the perfor-
mance of the sentences with attitude detection and
interaction sign prediction steps.
In addition to classical evaluation, we evaluate our
results using the structural balance theory which has
been shown to hold both theoretically (Heider, 1946)
and empirically (Leskovec et al, 2010b). We val-
idate our results by showing that the automatically
extracted networks mostly agree with the theory.
5.1 Identifying Sentences with Attitude
We tested this component using the Sentences
Dataset described in Section 4. In a 10-fold cross
validation mode, the classifier achieves 80.3% accu-
racy, 81.0% precision, %79.4 recall, and 80.2% F1.
11
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 Balanced Triangles Balanced Triangles (Random)
Figure 1: Percentage of balanced triangles in extracted
network vs. random network.
5.2 Interaction Sign Classifier
We used the relation detection classifier described in
Section 3.2 to find sentences with positive and neg-
ative attitude. The output of this classifier was used
to compute the the features described in Section 3.3,
which were used to train a classifier that predicts the
sign of an interaction between any two individuals.
We used Support Vector Machines (SVM) to train
the sign interaction classifier. We report several per-
formance metrics for them in Table 2. All results
were computed using 10 fold cross validation on the
labeled data. To better assess the performance of
the proposed classifier, we compare it to a baseline
that labels the relation as negative if the percentage
of negative sentences exceeds a particular threshold,
otherwise it is labeled as positive. The thresholds
was empirically evaluated using a separate develop-
ment set. The accuracy of this baseline is only 71%.
We evaluated the importance of the features listed
in Table 1 by measuring the chi-squared statistic for
every feature with respect to the class. We found
out that the features describing the interaction be-
tween the two participants are more informative than
the ones describing individuals characteristics. The
later features are still helpful though and they im-
prove the performance by a statistically significant
amount. We also noticed that all features based on
percentages are more informative than those based
on count. The most informative features are: per-
centage of negative posts per tread, percentage of
negative sentences per post, percentage of positive
posts per thread, number of negative posts, and dis-
cussion topic.
5.3 Structural Balance Theory
The structural balance theory is a psychological the-
ory that tries to explain the dynamics of signed so-
cial interactions. It has been shown to hold both the-
oretically (Heider, 1946) and empirically (Leskovec
et al, 2010b). In this section, we study the agree-
ment between the theory and the automatically ex-
tracted networks. The theory has its origins in the
work of Heider (1946). It was then formalized in a
graph theoretic form in (Cartwright and Harary, ).
The theory is based on the principles that ?the friend
of my friend is my friend?, ?the enemy of my friend
is my enemy?, ?the friend of my enemy is my en-
emy?, and variations on these.
There are several possible ways in which trian-
gles representing the relation of three people can be
signed. The structural balance theory states that tri-
angles that have an odd number of positive signs (+
+ + and + - -) are balanced, while triangles that have
an even number of positive signs (- - - and + + -) are
not.
Even though the structural balance theory posits
some triangles as unbalanced, that does not elimi-
nate the chance of their existence. Actually, for most
observed signed structures for social groups, exact
structural balance does not hold (Doreian and Mr-
var, 1996). Davis (1967) developed the theory fur-
ther into the weak structural balance theory. In this
theory, he extended the structural balance theory to
cases where there can be more than two such mu-
tually antagonistic subgroups. Hence, he suggested
that only triangles with exactly two positive edges
are implausible in real networks, and that all other
kinds of triangles should be permissible.
In this section, we connect our analysis to the
structural balance theory. We compare the predic-
tions of edge signs made by our system to the struc-
tural balance theory by counting the frequencies of
different types of triangles in the predicted network.
Showing that our automatically constructed network
agrees with the structural balance theory further val-
idates our results.
We compute the frequency of every type of trian-
gle for ten different topics. We compare these fre-
quencies to the frequencies of triangles in a set of
random networks. We shuffle signs for all edges on
every network keeping the fractions of positive and
12
0.1
0.15
0.2
0.25
0.3
0.35
Figure 2: Percentage of negative edges across topics.
negative edges constant.
We repeat shuffling for 1000 times. Every time,
we compute the frequencies of different types of tri-
angles. We find that the all-positive triangle (+++)
is overrepresented in the generated network com-
pared to chance across all topics. We also see that
the triangle with two positive edges (++?), and the
all-negative triangle (? ? ?) are underrepresented
compared to chance across all topics. The trian-
gle with a single positive edge is slightly overrep-
resented in most but not all of the topics compared
to chance. This shows that the predicted networks
mostly agree with the structural balance theory. In
general, the percentage of balanced triangles in the
predicted networks is higher than in the shuffled net-
works, and hence the balanced triangles are signif-
icantly overrepresented compared to chance. Fig-
ure 1 compares the percentage of balanced triangles
in the predicted networks and the shuffled networks.
This proves that our automatically constructed net-
work is similar to explicit signed networks in that
they both mostly agree with the balance theory.
6 Application: Dispute Level Prediction
There are many applications that could benefit from
the signed network representation of discussions
such as community finding, stance recognition, rec-
ommendation systems, and disputed topics identifi-
cation. In this section, we will describe one such
application.
Discussion forums usually respond quickly to
new topics and events. Some of those topics usu-
ally receive more attention and more dispute than
others. We can identify such topics and in general
measure the amount of dispute every topic receives
using the extracted signed network. We computed
the percentage of negative edges to all edges for ev-
ery topic. We believe that this would act as a mea-
sure for how disputed a particular topic is. We see,
from Figure 2, that ?environment?, ?science?, and
?technology? topics are among the least disputed
topics, whereas ?terrorism?, ?abortion? and ?eco-
nomics? are among the most disputed topics. These
findings are another way of validating our predic-
tions. They also suggest another application for this
work that focuses on measuring the amount of dis-
pute different topics receive. This can be done for
more specific topics, rather than high level topics as
shown here, to identify hot topics that receive a lot
of dispute.
7 Conclusions
In this paper, we have shown that natural language
processing techniques can be reliably used to extract
signed social networks from text correspondences.
We believe that this work brings us closer to un-
derstanding the relation between language use and
social interactions and opens the door to further re-
search efforts that go beyond standard social net-
work analysis by studying the interplay of positive
and negative connections. We rigorously evaluated
the proposed methods on labeled data and connected
our analysis to social psychology theories to show
that our predictions mostly agree with them. Finally,
we presented potential applications that benefit from
the automatically extracted signed network.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In CSLDAMT
?10.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.
2008. Friends and foes: ideological social network-
ing. In SIGCHI.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In EMNLP ?9, EMNLP ?09.
Dorwin Cartwright and Frank Harary. Structure balance:
A generalization of heiders theory. Psych. Rev.
J. A. Davis. 1967. Clustering and structural balance in
graphs. Human Relations.
Patrick Doreian and Andrej Mrvar. 1996. A partitioning
approach to structural balance. Social Networks.
13
Patrick Doreian and Andrej Mrvar. 2009. Partitioning
signed social networks. Social Networks.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In ACL 2010, Uppsala, Sweden.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social networks
from threaded discussions. In (INSNA).
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In EACL?97.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING.
Fritz Heider. 1946. Attitudes and cognitive organization.
Journal of Psychology.
J. Huang, M. Zhou, and D. Yang. 2007. Extracting chat-
bot knowledge from online discussion forums. In IJ-
CAI?07.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a social
network with negative edges. In WWW ?09.
Lillian Lee. 1999. Measures of distributional similarity.
In ACL-1999.
A. Lehrer. 1974. Semantic fields and lezical structure.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010a. Predicting positive and negative links in online
social networks. In WWW ?10.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010b. Signed networks in social media. In CHI 2010.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010c. Signed networks in social media. In Proceed-
ings of the 28th international conference on Human
factors in computing systems, pages 1361?1370, New
York, NY, USA.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
Thumbs up?: sentiment classification using machine
learning techniques. In EMNLP. Association for Com-
putational Linguistics.
A. Popescu and O. Etzioni. 2005. Extracting product fea-
tures and opinions from reviews. In HLT-EMNLP?05.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In EMNLP?03.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05.
P. Turney and M. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from asso-
ciation. Transactions on Information Systems.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
evaluative and speculative language. In Proceedings
of the Second SIGdial Workshop on Discourse and Di-
alogue, pages 1?10.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In AAAI-IAAI.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In HLT/EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng., 19(10).
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03.
14
