Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 68?76,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Reengineering a domain-independent framework
for Spoken Dialogue Systems
Filipe M. Martins, Ana Mendes, Ma?rcio Viveiros, Joana Paulo Pardal,
Pedro Arez, Nuno J. Mamede and Joa?o Paulo Neto
Spoken Language Systems Laboratory, L2F ? INESC-ID
Department of Computer Science and Engineering,
Instituto Superior Te?cnico, Technical University of Lisbon
R. Alves Redol, 9 - 2? ? 1000-029 Lisboa, Portugal
{fmfm,acbm,mviveiros,joana,pedro,njm,jpn}@l2f.inesc-id.pt
http://www.l2f.inesc-id.pt
Abstract
Our work in this area started as a re-
search project but when L2F joined TecnoVoz,
a Portuguese national consortium including
Academia and Industry partners, our focus
shifted to real-time professional solutions.
The integration of our domain-independent
Spoken Dialogue System (SDS) framework
into commercial products led to a major
reengineering process.
This paper describes the changes that the
framework went through and that deeply af-
fected its entire architecture. The communi-
cation core was enhanced, the modules inter-
faces were redefined for an easier integration,
the SDS deployment process was optimized
and the framework robustness was improved.
The work was done according to software en-
gineering guidelines and making use of design
patterns.
1 Introduction
Our SDS framework was created back in
2000 (Moura?o et al, 2004), as the result of
three graduation theses (Cassaca and Maia, 2002;
Moura?o et al, 2002; Viveiros, 2004), one of which
evolved into a masters thesis (Moura?o, 2005).
The framework is highly inspired on the TRIPS
architecture (Allen et al, 2000): it is a frame-based
domain-independent framework that can be used
to build domain-specific dialogue systems. Every
domain is described by a frame, composed by
domain slots that are filled with user requests.
When a set of domain slots is filled, a service is
executed. In order to do so, the dialogue system
interacts with the user until enough information is
provided.
From the initial version of the framework two
systems were created for two different domains: a
bus ticket vending system, which provides an in-
terface to access bus timetables; and a digital vir-
tual butler named Ambro?sio that controls home de-
vices, such as TVs (volume and channel), acclima-
tization systems, and lights (switch on/off and in-
tensity) through the X10 electrical protocol and the
IrDA (Infrared Data Association) standard. Since
2003, Ambro?sio is publicly available in the ?House
of the Future?1, on the Portuguese Telecommunica-
tions Museum2.
As proof of concept, we have also built a pro-
totype system that helps the user while performing
some task. This was tested for the cooking domain
and the automobile reparation domain.
After the successful deployment of the mentioned
systems, we began developing two new automatic
telephone-based systems: a home banking system
and a personal assistant. These are part of a project
of the TecnoVoz3 consortium technology migration
to enterprises. To answer to the challenges that the
creation of those new systems brought to light, the
focus of the framework shifted from academic issues
to interactive use, real-time response and real users.
Since our goal was to integrate our SDS framework
into enterprise products, we started the development
of a commercial solution. Nevertheless, despite this
1http://www.casadofuturo.org/
2http://www.fpc.pt/
3http://www.tecnovoz.pt/
68
new focus, we wanted to maintain the research fea-
tures of the framework. This situation led to deep
changes in the framework development process: as
more robust techniques needed to be used to ensure
that new systems could easily be created to respond
to client requests. From this point of view, the goal
of the reengineering process was to create a frame-
work that provides means of rapid prototyping simi-
lar to those of Nuance4, Loquendo5 or Artificial So-
lutions6.
Also, the new systems we wanted to built carried
a significant change on the paradigm of the frame-
work: while in the first systems the effects of users?
actions were visible (as they could watch the lights
turning on and off, for instance) and a virtual agent
face provided feedback, in the new scenarios com-
munication is established only through a phone and,
being so, voice is the only feedback.
The new paradigm was the trigger to this pro-
cess and whenever a new issue needed to be solved
the best practices in similar successful systems were
studied. Not all can be mentioned. The most rele-
vant are described in what follows.
As it was previously mentioned, TRIPS was
the main inspiration for this framework. It is a
well known and stable architecture that has proven
its merits in accommodating a range of different
tasks (Allen et al, 2007; Jung et al, 2007). The
main modules of the system interact through a
Facilitator (Ferguson et al, 1996), similar to the
Galaxy HUB7 (Polifroni and Seneff, 2000) with
KQML (Labrou and Finin, 1997) messages. How-
ever, in TRIPS, the routing task is decentralized
since the sender modules decide where to send its
messages. At the same time, any module can sub-
scribe to selected messages through the Facilitator
according to the sender, the type of message or its
contents. This mechanism makes it easier to inte-
grate new modules that subscribe the relevant mes-
sages without the senders? acknowledgment.
Like our framework, the CMU Olympus is a clas-
sical pipeline dialog system architecture (Bohus et
4http://www.nuance.com/
5http://www.loquendo.com/
6http://www.artificial-solutions.com/
7The Galaxy Hub maintains connections to modules (parser,
speech recognizer, back-end, etc.), and routes messages among
them. See http://communicator.sourceforge.net/
al., 2007) where the modules are connected via a
Galaxy HUB that uses a central hub and a set of
rules for relaying messages from one component to
the other. It has the three usual main blocks: Lan-
guage Understanding, through Phoenix parser and
Helios confidence-based annotation module, Dia-
logue Management, through RavenClaw (Raux et
al., 2005; Bohus, 2004), and Language Generation,
through Rosetta. Recognition is made with Sphinx
and synthesis with Theta. The back-end applications
are directly connected to the HUB through an in-
cluded stub.
Some of our recent developments are also inspired
in Voice XML8, in an effort to simplify the frame-
work parameterization and development, required in
the enterprise context. Voice XML provides stan-
dard means of declarative configuration of new sys-
tems reducing the need of coding to the related de-
vices implementation (Nyberg et al, 2002).
Our reengineering work aimed at: i) making the
framework more robust and flexible, enhancing the
creation of new systems for different domains; ii)
simplifying the system?s development, debug and
deployment processes through common techniques
from software engineering areas, such as design pat-
terns (Gamma et al, 1994; Freeman et al, 2004).
By doing this, we are trying to promote the de-
velopment and deployment of new dialogue systems
with our framework.
This paper is organized as follows: Section 2
presents the initial version of the framework; Sec-
tion 3 describes its problems and limitations, as well
as the techniques we adopted to solve them; Sec-
tion 4 describes a brief empirical evaluation of the
reengineering work; finally, Section 5 closes the pa-
per with conclusions and some remarks about future
work directions.
2 Framework description
This section briefly presents our architecture, at its
initial stage, before the reengineering process. We
also introduce some problems of the initial architec-
ture, as they will be later explained in the next sec-
tion.
8http://www.w3.org/Voice/
69
2.1 Domain Model
The domain model that characterizes our framework
is composed by the following entities:
Domain, which includes a frame realization and
generalizes the information about several de-
vices;
Frame, which states the subset of slots to fill for a
given domain;
Device, which represents a real device with several
states and services. Only one active state exists,
at each time, for each device;
State, which includes a subset of services that are
active when the state is active;
Service, which instantiates a defined frame and
specifies a set of slots type of data and restric-
tions for that service.
When developing a new domain all these entities
have to be defined and instantiated.
2.2 Framework architecture
Our initial framework came into existence as the re-
sult of the integration of three main modules:
Input/Output Manager, that controls an Automatic
Speech Recognition (ASR) module (Meinedo,
2008), a Text-To-Speech (TTS) module (Paulo
et al, 2008) and provides a virtual agent
face (Viveiros, 2004);
Dialogue Manager, that interprets the user inten-
tions and generates output messages (Moura?o
et al, 2002; Moura?o, 2005);
Service Manager, that provides a dialogue man-
ager interface to execute the requested services,
and an external application interface through
the device concept (Cassaca and Maia, 2002).
2.3 Input/Output Manager
The Input/Output Manager (IOManager) controls an
ASR module and a TTS module. It also integrates
a virtual agent face, providing a more realistic in-
teraction with the user. The synchronization be-
tween the TTS output and the animated face is done
by an audio?face synchronization manager, which
generates the visemes9 for the corresponding TTS
phonemes information. The provided virtual agent
face is based on a state machine that informs, among
others, when the system is ?thinking? or when what
the user said was not understood.
Besides, a Graphical User Interface (GUI) exists
for text interactions between the user and the system.
Although this input interface is usually only used for
test and debug proposes (as it skips the ASR mod-
ule), it could be used in combination with speech,
if requested by any specific multi-modal system im-
plementation.
The IOManager provides an interface to the Di-
alogue Manager that only includes text input and
output functions. However, the Dialogue Manager
needs to rely on other information, such as the in-
stant the user starts to speak or the moment a syn-
thesized sentence ends. These events are useful, for
instance, to set and trigger for user input timeouts.
2.4 Dialogue Manager
The architecture of the Dialogue Manager (Figure 1)
has seven main modules: a Parser, an Interpretation
Manager, a Task Manager, a Behavior Agent, a Gen-
eration Manager, a Surface Generation and a Dis-
course Context.
HUB
Surface
Generation
[16, 19]
Generation
Manager
[13, 15]
Discourse
Context
[4, 14]
Input/
Output
Manager
[1, 20]
Service
Manager
[7,10,18]
Behavior
Agent
[12]
Parser
[2]
Interpretation 
Manager
[3, 5, 8, 11]
Task
Manager
[6, 9, 17]
External
Applications
Figure 1: Dialogue Manager architecture through the
central HUB. Numbers show the execution sequence.
9A viseme is the visual representation of a phoneme and is
usually associated with muscles positioned near the region of
the mouth (Neto et al, 2006).
70
These modules have specific code from the im-
plementations of the two first systems (the bus ticket
vending system and the butler). When building a
generic dialogue framework, this situation turns out
to be a problem since domain-dependent code was
being used that was not appropriate in new systems.
Also, the modules have many code for HUBmessag-
ing, which makes debug and development harder.
2.5 Service Manager
The Service Manager (Figure 2) was initially devel-
oped to handle all domain specific information. It
has the following components:
Service Manager Galaxy Server, that works like a
HUB stub, managing the interface with the de-
vices and the Dialogue Manager;
Device Manager, that stores information related to
all devices. This information is used by the Di-
alogue Manager to find the service that should
be executed after an interaction;
Access Manager, that controls the user access to
some devices registered in the system;
Domain Manager, that stores all the information
about the domains. This information is used to
build interpretations and for the language gen-
eration process;
Object Recognition Manager, that recognizes the
discourse objects associated with a device;
Device Proxy, abstracts all communication with
the Device Core and device specific informa-
tion protocol. This is done through the Virtual
Proxy design pattern
Device Core, that implements the other part of the
communication protocol with the Service Man-
ager and the Dialogue Manager.
Since the Service Manager interface is shared by
the Dialogue Manager and all devices, a device can
execute a service that belongs to another device or
even access to internal Dialogue Manager informa-
tion.
External
Application
HUB
Database
Service
Manager
Galaxy
Server
Domain
Manager
Device
Manager
Access
Manager
Object
Recognition
Manager
Device
Proxy
Device Core
Device
specific
Implementation
Figure 2: Service Manager architecture.
3 Reengineering a framework
When the challenge of building two new SDSs on
our framework appeared, some of the mentioned ar-
chitectural problems were highlighted. A reengi-
neering process was critical. A starting point for the
reengineering process was needed, even though that
decision was not clear.
By observing the framework?s data and control
flow, we noticed that part of the code in the different
modules was related with HUB messaging, namely
the creation of messages to send, and the conversion
of received messages into internal structures (mar-
shalling). A considerable amount of time was spent
in this task that was repeated across the framework.
Based on that, we decided that the first step should
be the analysis of the Galaxy HUB communication
flow and the XML structures used to encode those
messages, replacing them with more appropriate and
efficient protocols.
3.1 Galaxy HUB and XML
The Galaxy HUB protocol is based in generic XML
messages. That allows new modules to be easily
plugged into the framework, written in any program-
ming language, without modifying any line of code.
However, we needed to improve the development
and debugging processes of the existing modules,
71
and having a time consuming task that was repeated
whenever two modules needed to communicate was
a serious drawback.
Considering this, we decided to remove the
Galaxy HUB. This decision was enforced by the
fact that all the framework modules were written
in the Java programming language, which already
provides direct invocations and objects serialization
through Java Remote Method Invocation (RMI).
The major advantage associated with the use of
this protocol, was the possibility of removing all the
XML-based messaging that repeatedly forced the
creation and interpretation of generic messages in
execution time. With the use of RMI, these struc-
tures were replaced by Java objects that are inter-
changed between modules transparently. Not only
RMI is native to Java.
This was not a simple task, as the team that was
responsible for this process was not the team who
originally developed the framework. Because of
this, the new team lacked familiarity with the overall
code structure. In order to reduce the complexity of
the process, it was necessary to create a proper in-
terface for each module removing the several entry
points that each one had. To better understand the
real flow and to minimize the introduction of new
bugs while refactoring the code we made the infor-
mation flow temporarily synchronous.
The internal structure of each module was re-
designed and every block of code with unknown
functionality was commented out.
This substitution improved the code quality and
both the development and the debugging processes.
We believe that it also improved the runtime effi-
ciency of the system, even though no evaluation of
the performance was made. Empirically, we can say
that in the new version of the system less time is
needed to complete a task since no explicit conver-
sion of the objects into generic messages is made.
3.2 Domain dependent code
The code of the Parser, the Interpretation Manager
and the Surface Generation modules had domain de-
pendent code and it was necessary to clean it out.
Since we were modifying the Galaxy HUB code,
we took the opportunity and redesigned that code in
the aforementioned modules to make it more generic
(and, consequently less domain dependent). Being
so, the code cleaning process took place while the
Galaxy HUB was being replaced.
We were unable to redesign the domain dependent
code. Cases like hard-coded word replacement, used
both to provide a richer interpretation of the user ut-
terances and to allow giving a natural response to the
user. In such cases, we either isolated the domain
specific portions of the code or deleted them, even if
the interpretation or generation processes were de-
graded. It can be recovered in the future by includ-
ing the domain specific knowledge in the dynamic
configuration of the Interpretation and Generation
managers as suggested by Paulo Pardal (2007)
An example of this process is the split-
ting of the parser specific code into several
parsers: some domain-dependent, some domain-
independent, while creating a mechanism to com-
bine them in a configurable chain (through a pipes
and filters architecture). This allows the building
of smaller data-type specific parsers that the Inter-
pretation Manager selects to achieve the best pars-
ing result, according to the expectations of the sys-
tem (Martins et al, 2008). These expectations are
created according to the assumption that the user
will follow the mixed-initiative dialogue flow that
the system ?suggests? during its turn in the interac-
tion. The strategy also handles those cases were the
user does not keep up with those expectations.
3.3 Dialogue Manager Interface
The enhancements introduced at the IOManager
level augmented the amount of the information in-
terchanged between this module and the Dialogue
Manager, as it could deal with more data coming
from the ASR, TTS and the virtual agent face.
However, the Dialogue Manager Interface was
continuously evolving and changing. This lack of
stability made it harder to maintain the successive
versions completely functional during the process.
Following the software engineering practices, and
using the Template Method design pattern, we
started with the definition of modules interfaces and
only after that the implementation code of the meth-
ods was written. This allows the simultaneous de-
velopment of different modules that interact. Only
when some conflict is reported, the parallel develop-
ment processes need to be synchronized resulting in
the possible revision of the interfaces. Even when
72
an interface was not fully supported by the Dialogue
Manager, it was useful since it lead the IOManager
continuous improvements and allowed simultaneous
developments in the Dialogue Manager.
In order to ease the creation of this interface,
an Input/Output adapter was created. This adapter
makes the conversion of the information sent by the
IOManager to the Dialogue Manager specific for-
mat. Having this, when the information exchanged
with the Dialogue Manager changes, the Dialogue
Manager Interface does not need any transforma-
tion. In addition, the Dialogue Manager is able to
interact with other Input/Output platforms without
the need of internal changes.
This solution for the interfaces follows the Facade
design pattern, which provides an unique interface
for several internal modules.
3.4 File system reorganization
When the different dialogue systems were fully im-
plemented in the new version of the framework, we
wanted to keep providing simultaneous access to the
several available domains during the same execution
of the system.
In fact, in our initial framework it was already
possible to have several different domains running in
parallel. When an interaction is domain ambiguous,
the system tries to solve the ambiguity by asking the
user which domain is being referred.
User: Ligar
System: O que deseja fazer:
ligar um electrodome?stico
ou fazer um telefonema?
Figure 3: Example of a domain ambiguous interaction
while running with two different running domains. In
Portuguese ?ligar? means ?switch on? and ?call?
Consider the example on Figure 3: an user inter-
action with two different running domains, the but-
ler and the personal digital assistant. In Portuguese,
the verb ?ligar? means ?to switch something on? or
?to make a phone call?. Since there are two running
domains, and the user utterance is domain ambigu-
ous, the systems requests for a disambiguation in its
next turn (O que deseja fazer), by asking if the user
wants to switch on a home device (ligar um elec-
trodome?stico) or make a phone call (fazer um tele-
fonema).
While using this feature, it came to our attention
that it was necessary to reorganize the file system:
the system folder held the code of all domains, and
every time we needed to change a specific domain
property, we had hundreds of properties files to look
at. This situation was even harder for novice frame-
work developers, since it was difficult to find ex-
actly which files needed to be modified in that dense
file system structure. Moreover, the ASR, TTS and
virtual agent configurations were shared by all do-
mains.
To solve this problem we applied the concept
of system?instance. A system?instance has one or
more domains. When the system starts, it receives
a parameter that specifies which instance we want
to run. The configuration of the existing instances
is split across different folders. A library folder
was created and organized in external libraries (li-
braries from an external source), internal libraries
(library developed internally at our laboratory) and
instance specific libraries (specific libraries of a
system?instance).
With this organization we improved the version-
ing management and updates. The conflicting con-
figuration was removed since each system?instance
has now its own configuration. The configuration
files are organized and whenever we need to deliver
a new version of a system?instance, we simply need
to select the files related with it.
3.5 Service Manager redesign
The Service Manager code had too many dependen-
cies with different modules. The Service Manager
design was based on the Virtual Proxy design pat-
tern. However, it was not possible to develop new
devices without creating dependencies on all of the
Service Manager code, as the Device Core code re-
lied heavily on some classes of the Service Manager.
This situation created difficulties in the SDSs de-
velopment process and affected new developments
since the Service Manager code needed to be copied
whenever a Device Core was running in another
computer or in a web container. This is a known
bad practice in software engineering, since the code
is scattered, making it harder to maintain updated
code in all the relevant locations.
It was necessary to split the Service Manager code
73
for the communication protocol between communi-
cation itself and the device specific code.
Also, the Service Manager class10 interface was
shared by the DialogueManager and all devices. Be-
ing so, it was possible that a device requested the
execution of a service in other device, as well as to
access the internal information exchanged between
the Service Manager and the Dialogue Manager.
Example Device
Device Core
Device
specific
Implementation
Dialogue
Manager
Service Manager
Dialogue
Manager
Interface
Devices
Interface
Service
Manager
Class
Access
Manager
Device
Manager
Figure 4: Service Manager architecture.
Like we did with the Dialogue Manager, we spec-
ified a coherent interface for the different Service
Manager modules, removing the unwanted entry
points. The Service Manager class interface was
split and the Device Manager is now the interface
between the Service Manager and the devices (Fig-
ure 4). Also, the Service Manager class interface is
only accessed by the Dialogue Manager. The classes
between the Service Manager and the Device imple-
mentation were organized in a small library, contain-
ing the classes and the Device Core code. This li-
brary is all what is needed to create a new device
and to connect it to both the Service Manager and
the Dialogue Manager.
Finally, we changed the Access Manager to con-
trol not only user access to registered devices, but
also the registry of devices in the system. This
prevents a device which is running on a specific
system?instance to be registered in some other run-
ning system?instance. This module changed its po-
sition in the framework architecture: now it is be-
10The Service Manager Galaxy Server was renamed to Ser-
vice Manager. However, we decided to call it here by Service
Manager class so it will not be mistaken with the Service Man-
ager module.
tween the Service Manager class and the Device
Manager.
3.6 Event Manager
In the initial stage, when the Galaxy HUB was
removed, all the communication was made syn-
chronous. After that, to enhance the framework and
allowmixed initiative interactions, a mechanism that
provides asynchronous communication was needed.
Also, it was necessary to propagate information be-
tween the ASR, TTS, GUI and the Dialogue System,
crucial for the error handling and recovery tasks.
We came to the conclusion that most of the frame-
works deal with these problems by using event man-
agement dedicated modules. Although TRIPS, the
framework that initially inspired ours, has an Event
Manager, that was not available in ours. The ASR
and TTS modules provided already an event-based
information propagation, and we needed to imple-
ment a dedicated module to make the access to this
sort of information simpler. This decision was en-
forced by the existence of a requirement on han-
dling events originated by an external Private Branch
eXchange (PBX) system, like incoming call and
closed call events. The PBX system was integrated
with the personal assistant that is available through
a phone connection. SDS.
We decided to create an Event Manager in the
IOManager. The Dialogue Manager implements an
event handler that receives events from the Event
Manager and knows where to deliver them. Quickly
we understood that the event handler needed to be
dependent of the system?instance since the events
and their handling are different across systems (like
a telephone system and kiosk system). With this in
mind, we implemented the event handler module,
following the Simple Factory design pattern, by del-
egating the events handling to the specific system-
instance handler. If this specific system?instance
event handler is not specified, the system will use
a default event handler with ?generic? behavior.
This developments were responsible for the con-
tinuous developments in the IOManager, referred in
section 3.3, and occurred at the same time.
With this approach, we can propagate and handle
all the ASR events, the TTS events, GUI events and
external applications events.
The Event Manager has evolved to a decentral-
74
ized HUB. Through this, the sender can set identi-
fiers in some events. These identifiers are used by
other modules to identify messages relevant to them.
In TRIPS a similar service is provided by the Facil-
itator, that routes messages according to the recipi-
ents specified by the sender, and following the sub-
scriptions that modules can do by informing the Fa-
cilitator. This approach eases the integration of new
modules without changing the existing ones, just by
subscribing the relevant type of messages.
3.7 Dialogue Manager distribution
Currently, there are some clients interested in our
framework to create their own SDS. However, since
the code is completely written in Java, distributions
are made available through jar files that can be eas-
ily decoded, giving access to the source of our code.
To avoid this we need to obfuscate the code.
Even though obfuscation is an interesting solu-
tion, our code used Java?s reflexion in several points.
This technique enables dynamic retrieval of classes
and data structures by name. By doing so, it needs to
know the specific name of the classes being reflected
so that the Java class loader knows where to find
them. Obfuscation, among other things, changes
class names and locations, preventing the Java class
loader from finding them.
To cope with this additional challenge, the code
that makes use of reflexion was replaced using the
Simple Factory design pattern. This change allows
the translation of the hard-coded names to the new
obfuscated names in obfuscation time. After that,
when some class needs to instantiate one of those
classes that used reflection, that instance can be cre-
ated through the proper factory.
4 Evaluation
Although a SDS was successfully deployed in our
initial framework, which is publicly available at a
Museum since 2003, no formal evaluation was made
at that initial time. Due to this, effective or numeric
comparison between the framework as it was before
the reengineering work and as it is now, is not possi-
ble. Previous performance parameters are not avail-
able. However, some empirical evaluation is pos-
sible, based on generic principles of Software (re)
Engineering.
In the baseline framework, each improvement,
like modifications in the dialogue flow or at the
parser level, was a process that took more than two
weeks of work, of two software engineers. With the
new version, similar changes are done in less than
one week, by the same team. This includes internal
improvements, and external developments made by
entities using the system. The system is more stable
and reliable now: in the beginning, the system had
an incorrect behavior after some hours of running
time; currently with a similar load, it runs for more
than one month without needing to be restarted.
This is one great step for the adoption of our
framework. This stability, reliability and develop-
ment speed convinced our partners to create their
Spoken Dialogue Systems with our framework.
5 Conclusions and Future Work
Currently, our efforts are concentrated on interpreta-
tion improvement and on error handling and recov-
ery (Harris et al, 2004).
Currently, we are working on representing emo-
tions within the SDS framework. We want to test
the integration, and how people will react to a sys-
tem with desires and moods.
The next big step will be the inclusion of an ef-
ficient morpho-syntactic parser which generates and
provides more information (based on speech acts) to
the Interpretation Manager.
Another step we have in mind is to investigate
how the events and probabilistic information that the
ASR module injects in the system can be used to re-
cover recognition errors.
The integration of a Question-Answering (QA)
system (Mendes et al, 2007) in this framework is
also in our horizon. This might require architectural
changes in order to bring together the interpretation
and disambiguation features from the SDS with the
Information Retrieval (IR) features of QA systems.
This would provide information-providing systems
through voice interaction (Mendes, 2008).
Another ongoing work is the study of whether
ontologies can enrich a SDS. Namely, if they can
be used to abstract knowledge sources allowing the
system to focus only on dialogue phenomena rather
than architecture adaptation, when including new
domains (Paulo Pardal, 2007).
75
Acknowledgments
This work was partially funded by TECNOVOZ,
PRIME National Project number 03/165.
It was also partially funded by DIGA, project
POSI/PLP/14319/2001 of Fundac?a?o para a
Cie?ncia e Tecnologia (FCT).
Joana Paulo Pardal is supported by a PhD fellow-
ship from FCT (SFRH/BD/30791/2006).
References
James Allen, Donna Byron, Myroslava Dzikovska,
George Ferguson, Lucian Galescu, and Amanda Stent.
2000. An architecture for a generic dialogue shell.
Natural Language Engineering, Cambridge Univer-
sity Press, 6.
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Taysom. 2007. Plow: A collaborative task
learning agent. In Proc. 22th AAAI Conf. AAAI Press.
Dan Bohus, Antoine Raux, Thomas Harris, Maxine Es-
kenazi, and Alexander Rudnicky. 2007. Olympus:
an open-source framework for conversational spoken
language interface research. In Workshop on Bridging
the Gap: Academic and Industrial Research in Dialog
Technology, HLT-NAACL.
Dan Bohus. 2004. Building spoken dialog systems with
the RavenClaw/Communicator architecture. Presenta-
tion at Sphinx Lunch Talk, CMU, Fall.
Renato Cassaca and Rui Maia. 2002. Assistente electro?-
nica. Instituto Superior Te?cnico (IST), Universidade
Te?cnica de Lisboa (UTL), Graduation Thesis.
George Ferguson, James Allen, Brad Miller, and Eric
Ringger. 1996. The design and implementation of
the TRAINS-96 system: A prototype mixed-initiative
planning assistant. Technical Report TN96-5.
Elisabeth Freeman, Eric Freeman, Bert Bates, and Kathy
Sierra. 2004. Head First Design Patterns. O?Reilly.
Erich Gamma, Richard Helm, Ralph Johnson, and John
Vlissides. 1994. Design Patterns: Elements of
Reusable Object-Oriented Software. Addison-Wesley
Professional Computing Series.
Thomas Harris, Satanjeev Banerjee, Alexander Rud-
nicky, June Sison, Kerry Bodine, and Alan Black.
2004. A research platform for multi-agent dialogue
dynamics. In 13th IEEE Intl. Workshop on Robot and
Human Interactive Communication (ROMAN).
Hyuckchul Jung, James Allen, Nathanael Chambers, Lu-
cian Galescu, Mary Swift, and William Taysom. 2007.
Utilizing natural language for one-shot task learning.
Journal of Logic and Computation.
Yannis Labrou and Tim Finin. 1997. A proposal for a
new KQML specification. Technical Report CS-97-
03, Computer Science and Electrical Engineering De-
partment, Univ. of Maryland Baltimore County.
Filipe M. Martins, Ana Mendes, Joana Paulo Pardal,
Nuno J. Mamede, and Joa?o Paulo Neto. 2008. Us-
ing system expectations to manage user interactions.
In Proc. PROPOR 2008 (to appear), LNCS. Springer.
Hugo Meinedo. 2008. Audio Pre-processing and Speech
Recognition for Broadcast News. Ph.D. thesis, IST,
UTL.
Ana Mendes, Lu??sa Coheur, Nuno J. Mamede, Lu??s
Roma?o, Joa?o Loureiro, Ricardo Daniel Ribeiro, Fer-
nando Batista, and David Martins de Matos. 2007.
QA@L2F@QA@CLEF. In Cross Language Evalua-
tion Forum: Working Notes - CLEF 2007 Workshop.
Ana Mendes. 2008. Introducing dialogue in a QA sys-
tem. In Doctoral Symposium of 13th Intl. Conf. Apps.
Nat. Lang. to Information Systems, NLDB (to appear).
Ma?rcio Moura?o, Pedro Madeira, and Miguel Rodrigues.
2002. Dialog manager. IST, UTL, Graduation Thesis.
Ma?rcio Moura?o, Renato Cassaca, and Nuno J. Mamede.
2004. An independent domain dialogue system
through a service manager. In EsTAL, volume 3230
of LNCS. Springer.
Ma?rcio Moura?o. 2005. Gesta?o e representac?a?o de do-
m??nios em sistemas de dia?logo. Master?s thesis, IST,
UTL.
Joa?o Paulo Neto, Renato Cassaca, Ma?rcio Viveiros, and
Ma?rcio Moura?o. 2006. Design of a Multimodal Input
Interface for a Dialogue System. In Proc. PROPOR
2006, volume 3960 of LNCS. Springer.
Eric Nyberg, Teruko Mitamura, and Nobuo Hataoka.
2002. DialogXML: extending Voice XML for dy-
namic dialog management. In Proc. 2th Int. Conf.
on Human Language Technology Research. Morgan
Kaufmann Publishers Inc.
Se?rgio Paulo, Lu??s C. Oliveira, Carlos Mendes, Lu??s
Figueira, Renato Cassaca, Ce?u Viana, and Helena Mo-
niz. 2008. DIXI - A Generic Text-to-Speech System
for European Portuguese. In Proc. PROPOR 2008 (to
appear), LNCS. Springer.
Joana Paulo Pardal. 2007. Dynamic use of ontologies in
dialogue systems. In NAACL-HLT Doctoral Consor-
tium.
Joseph Polifroni and Stephanie Seneff. 2000. GALAXY-
II as an architecture for spoken dialogue evaluation. In
Proc. 2nd Int. Conf. Language Resources and Evalua-
tion (LREC).
Antoine Raux, Brian Langner, Dan Bohus, Alan Black,
and Maxine Eskenazi. 2005. Let?s go public! tak-
ing a spoken dialog system to the real world. In Proc.
INTERSPEECH.
Ma?rcio Viveiros. 2004. Cara falante ? uma interface vi-
sual para um sistema de dia?logo falado. IST, UTL,
Graduation Thesis.
76
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 33?38,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Exploring linguistically-rich patterns for question generation
Se?rgio Curto
L2F/INESC-ID Lisbon
sslc@l2f.inesc-id.pt
Ana Cristina Mendes
L2F/INESC-ID Lisbon
IST, Tech. Univ. Lisbon
acbm@l2f.inesc-id.pt
Lu??sa Coheur
L2F/INESC-ID Lisbon
IST, Tech. Univ. Lisbon
lcoheur@inesc-id.pt
Abstract
Linguistic patterns reflect the regularities of
Natural Language and their applicability is
acknowledged in several Natural Language
Processing tasks. Particularly, in the task of
Question Generation, many systems depend
on patterns to generate questions from text.
The approach we follow relies on patterns
that convey lexical, syntactic and semantic in-
formation, automatically learned from large-
scale corpora.
In this paper we discuss the impact of varying
several parameters during pattern learning and
matching in the Question Generation task. In
particular, we introduce semantics (by means
of named entities) in our lexico-syntactic pat-
terns. We evaluate and compare the number
and quality of the learned patterns and the
matched text segments. Also, we detail the
influence of the patterns in the generation of
natural language questions.
1 Introduction
Natural Language (NL) is known for its variability
and expressiveness. There are hundreds of ways to
express an idea, to describe a fact. But language also
comprises several regularities, or patterns, that de-
note the presence of certain information. For exam-
ple, Paris is located in France is a common way to
say that Paris is in France, indicated by the words
located in.
The use of patterns is a widely accepted as an ef-
fective approach in the field of Natural Language
Processing (NLP), in tasks like Question-Answering
(QA) (Soubbotin, 2001; Ravichandran and Hovy,
2002) or Question Generation (QG) (Wyse and Pi-
wek, 2009; Mendes et al, 2011).
Particularly, QG aims at generating questions
from text and has became a vibrant line of re-
search. Generating questions (and answers), on one
hand, allows QA or Dialogue Systems to be easily
ported to different domains, by quickly providing
new questions to train the systems. On the other
hand, it is useful for knowledge assessment-related
tasks, by reducing the amount of time allocated for
the creation of tests by teachers (a time consuming
and tedious task if done manually), or by allowing
the self evaluation of knowledge acquired by learn-
ers.
Most systems dedicated to QG are based on hand-
crafted rules and rely on pattern matching to gener-
ate questions. For example, in (Chen et al, 2009),
after the identification of key points, a situation
model is built and question templates are used to
generate questions. The Ceist system (Wyse and Pi-
wek, 2009) uses syntactic patterns and the Tregex
tool (Levy and Andrew, 2006) that receives a set
of hand-crafted rules and matches the rules against
parsed text, generating, in this way, questions (and
answers). Kalady et al(2010) bases the QG task
in Up-keys (significant phrases in documents), parse
tree manipulation and named entity recognition.
Our approach to QG also relies on linguistic pat-
terns, defined as a sequence of symbols that convey
lexical, syntactic and semantic information, reflect-
ing and expressing a regularity of the language. The
patterns associate a question to its answer and are
automatically learned from a set of seeds, based on
large-scale information corpora, shallow parsing and
named entities recognition. The generation of ques-
tions uses the learned patterns, as questions are cre-
ated from text segments found in free text after being
matched against the patterns.
33
This paper studies the impact on QG of vary-
ing linguistic parameters during pattern learning and
matching. It is organized as follows: in Sec. 2
we introduce our pattern-based approach to QG; in
Sec. 3 we show the experiments and discuss results;
in Sec. 4 we conclude and point to future work.
2 Linguistically-Rich Patterns for
Question Generation
The generation of questions involves two phases: a
first offline phase ? pattern learning ? where pat-
terns are learned from a set of seeds; and a sec-
ond online phase ? pattern matching and question
generation ? where the learned patterns are matched
against a target document and the questions are gen-
erated. Next we describe these phases.
Pattern Learning Our approach to pattern learn-
ing is inspired by the work of Ravichandran and
Hovy (2002), who propose a method to learn pat-
terns based on a two-step technique: the first ac-
quires patterns from the Web given a set of seeds and
the second validates the patterns. Despite the sim-
ilarities, ours and Ravichandran and Hovy?s work
have some differences: our patterns also contain
syntactic and semantic information and are not vali-
dated. Moreover, our seeds are well formulated NL
questions and their respective correct answers (in-
stead of two entities), which allows to directly take
advantage of the test sets already built and made
available in evaluation campaigns for QA systems
(like Text REtrieval Conference (TREC) or Cross
Language Evaluation Forum (CLEF)).
We use a set of seeds, each composed by a NL
question and its correct answer. We start by classi-
fying each seed question into a semantic category,
in order to discover the type of information these
are seeking after: for example, the question ?Who
painted the Birth of Venus ?? asks for a person?s
name. Afterwards, we extract the phrase nodes of
each seed question (excluding the Wh-phrase), en-
close each in double quotes and submit them as a
query to a search engine. For instance, given the
seed ?Who painted the Birth of Venus ??/Botticelli
and the syntactic structure of its question [WHNP
Who] [VBD painted] [NP the Birth of Venus]
1, we
1The Penn Treebank II Tags (Bies et al, 1995) are used.
build the query: "painted" "the Birth of
Venus" "Botticelli".
We build patterns that associate the entities
in the question to the answer from the top re-
trieved documents. From the sentence The Birth
of Venus was painted around 1486 by Botti-
celli, retrieved as result to the above query, we
learn the pattern ?NP VBD[was] VBN PP[around
1486]:[Date] IN:[by] NP{ANSWER}?2. The
syntactic labels without lexical information are re-
lated with the constituents of the question, while
those with ?{ANSWER}? mark the answer.
By creating queries with the inflected forms of the
main verb of the question, we learn patterns where
the surface form of the verb is different to that of
the verb in the seed question (e.g., ?NP{ANSWER}
VBD[began] VBG NP? is learned from the sen-
tence Botticelli began painting the Birth of Venus).
The patterns generated by verb inflection are IN-
FLECTED; the others are STRONG patterns.
Our patterns convey linguistic information ex-
tracted from the sentences in the documents where
all the constituents of the query exist. The pat-
tern is built with the words, their syntactic and se-
mantic classes, that constitute the segments where
those constituents are found. For that, we per-
form syntactic analysis and named entity recog-
nition in each sentence. In this paper, we ad-
dress the impact of adding semantic information
to the patterns, that is, the difference in hav-
ing a pattern ?NP VBD[was] VBN PP[around
1486]:[Date] IN:[by] NP{ANSWER}? with or
without the named entity of type DATE, for instance.
Pattern Matching and Question Generation
The match of the patterns against a given free text
is done at the lexical, syntactic and semantic lev-
els. We have implemented a (recursive) algorithm
that explores the parsed tree of the text sentences in
a top-down, left-to-right, depth-first search, unifying
the text with the linguistic information in the pattern.
Also, we discard all matched segments in which
the answer does not agree with the semantic cate-
gory expected by the question.
The generation of questions from the matched text
2The patterns are more complex than the ones presented:
they are linked to the seed question by indexes, mapping the po-
sition of each of its components into the question constituents.
34
segments is straightforward, since we keep track of
the syntactic structure of the questions and the sen-
tences on the origin of the patterns. There is a di-
rect unification of all components of the text seg-
ment with the constituents of the pattern. In the
INFLECTED patterns, the verb is inflected with the
tense and person of the seed question and the auxil-
iary verb is also used.
3 Experiments
3.1 Experimental Setup
We used the 6 seeds shown in Table 1, chosen be-
cause the questions contain regular verbs and they
focus on known entities ? being so, it is probable
that there will be several texts in the Web referring to
them. However, understanding the characteristics of
a pair that makes it a good seed is an important and
pertinent question and a direction for future work.
GId: 1
Syntactic Structure: WHNP VBD NP
Semantic Category: HUMAN:INDIVIDUAL
?Who wrote Hamlet??/Shakespeare
?Who painted Guernica??/Picasso
?Who painted The Starry Night??/Van Gogh
GId: 2
Syntactic Structure: WHADVP VBD NP VBN
Semantic Category: NUMERIC:DATE
?When was Hamlet written??/1601
?When was Guernica painted??/1937
?When was The Starry Night painted??/1889
Table 1: Seeds used in the experiments.
The syntactic analysis of the questions was done
by the Berkeley Parser (Petrov and Klein, 2007)
trained on the QuestionBank (Judge et al, 2006).
For question classification, we used Li and Roth
(2002) taxonomy and a machine learning-based
classifier fed with features derived from a rule-based
classifier (Silva et al, 2011).
For the learning of patterns we used the top
64 documents retrieved by Google and to recog-
nize the named entities in the pattern we apply
several strategies, namely: 1) the Stanford?s Con-
ditional Random-Field-based named entity recog-
nizer (Finkel et al, 2005) to detect entities of type
HUMAN; 2) regular expressions to detect NUMERIC
and DATE type entities; 3) gazetteers to detect enti-
ties of type LOCATION.
For the generation of questions we used the top 16
documents retrieved by the Google for 9 personali-
ties from several domains, like literature (e.g., Jane
Austen) and politics (e.g., Adolf Hitler). We do not
have influence on the content of the retrieved doc-
uments, nor perform any pre-processing (like text
simplification or anaphora resolution). The Berkeley
Parser (Petrov and Klein, 2007) was used to parse
the sentences, trained with the Wall Street Journal.
3.2 Pattern Learning Results
A total of 272 patterns was learned, from which 212
are INFLECTED and the remaining are STRONG. On
average, each seed led to 46 patterns.
Table 2 shows the number of learned patterns of
types INFLECTED and STRONG according to each
group of seed questions. It indicates the number of
patterns in which at least one named entity was rec-
ognized (W) and the number of patterns which do not
contain any named entity (WO). Three main results
of the pattern learning phase are shown: 1) the num-
ber of learned INFLECTED patterns is much higher
than the number of learned STRONG patterns: nearly
80% of the patterns are INFLECTED; 2) most of the
patterns do not have named entities; and 3) the num-
ber of patterns learned from the questions of group
1 are nearly 70% of the total number of patterns.
INFLECTED STRONG
GId WO W WO W TOTAL
1 127 19 36 8
146 44 190
2 40 26 10 6
66 16 82
All 167 45 46 14
212 60 272
Table 2: Number of learned patterns.
The following are examples of patterns and the
actual sentences from where they were learned:
? ?NP{ANSWER} VBZ NP?: an INFLECTED pattern
learned from group 1, from the sentence 1601
William Shakespeare writes Hamlet in London.,
without named entities;
? ?NP VBD VBN IN[in] NP{ANSWER}?: a
35
STRONG pattern learned from group 2, from the
sentence (Guernica was painted in 1937.), without
named entities;
? ?NNP VBZ[is] NP[a tragedy] ,[,]
VBN[believed] VBN IN[between]
NP[1599]:[NUMERIC COUNT,NUMERIC DATE]
CC[and] NP{ANSWER}?: an INFLECTED pattern
learned from group 2, from the sentence William
Shakespeare?s Hamlet is a tragedy , believed written
between 1599 and 1601, with 1599 being recog-
nized as named entity of type NUMERIC COUNT
and NUMERIC DATE.
3.3 Pattern Matching and Question
Generation Results
Regarding the number of text segments matched in
the texts retrieved for the 9 personalities, Table 3
shows that, from the 272 learned patterns, only 30
(11%) were in fact effective (an effective pattern
matches at least one text segment). The most effec-
tive patterns were those from group 2, as 12 from 82
(14.6%) matched at least one instance in the text.
GId INFLECTED STRONG TOTAL
1 13 5 18
2 9 3 (2 W) 12
All 22 8 30
Table 3: Matched patterns.
Regarding the patterns with named entities, only
those from group 2 matched instances in the texts.
The pattern that matched the most instances was
?NP{ANSWER} VBD NP?, learned from group 1.
In the evaluation of the questions, we use the
guidelines of Chen et al (2009), who classify ques-
tions as plausible ? if they are grammatically correct
and if they make sense regarding the text from where
they were extracted ? and implausible (otherwise).
However, we split plausible questions in three cat-
egories: 1) Pa for plausible, anaphoric questions,
e.g., When was she awarded the Nobel Peace Prize?;
2) Pc for plausible questions that need a context to
be answered, e.g., When was the manuscript pub-
lished?; and 3) Pp, a plausible perfect question. If
a question can be marked both as PLa and PLc, we
mark it as PLa. Also, we split implausible questions
in: 1) IPi: for implausible questions due to incom-
pleteness, e.g., When was Bob Marley invited?; and
2) IP: for questions that make no sense, e.g., When
was December 1926 Agatha identified?.
A total of 447 questions was generated: 31 by
STRONG patterns, 269 by INFLECTED patterns and
147 by both STRONG and INFLECTED patterns. We
manually evaluated 100 questions, randomly se-
lected. Results are in Table 4, shown according
to the type (INFLECTED/STRONG) and presence of
named entities (W/WO) in the pattern that generated
them.
Pa Pc Pp IPi IP Total
INFLECTED 57
WO 2 0 27 23 5
STRONG 13
W 1 0 1 0 1
WO 1 2 3 3 1
INFL/STR 30
WO 0 0 9 18 3
All 4 2 40 44 10 100
Table 4: Evaluation of the generated questions.
46 of the evaluated questions were considered
plausible and, from these, 40 can be used without
modifications. From the 54 implausible questions,
44 were due to lack of information in the question.
69% (9 in 13) of the questions originated in STRONG
patterns were plausible. This value is smaller for
questions generated by INFLECTED patterns: 50.8%
(29 in 57). Questions that had in their origin both a
STRONG and a INFLECTED pattern were mostly im-
plausible, only 9 in 30 were plausible (30%). The
presence of named entities led to an increase of
questions of only 3 (2 plausible and 1 implausible).
3.4 Discussion
The results concerning the transition from lexico-
syntactic to lexico-syntactic-semantic patterns were
not conclusive. There were 59 patterns with named
entities, but only 2 matched new text segments.
Only 3 questions were generated from patterns
with semantics. We think that this happened due to
two reasons: 1) not all of the named entities in the
patterns were detected; and 2) the patterns contained
lexical information that did not allow a match with
the text (e.g., ?NP{ANSWER} VBD[responded]
36
PP[in 1937]:textit[Date] WHADVP[when]
NP[he] VBD NP? requires the words responded,
when and he.)
From a small set of seeds, our approach learned
patterns that were later used to generate 447 ques-
tions from previously unseen text. In a sample of
100 questions, 46% were judged as plausible. Two
plausible questions are: ?Who had no real interest
in the former German African colonies??, ?When
was The Road to Resurgence published?? and ?Who
launched a massive naval and land campaign de-
signed to seize New York??.
The presence of syntactic information (a differ-
ence between ours and Ravichandran and Hovy?s
work) allows to relax the patterns and to gener-
ate questions of various topics: e.g., the questions
?Who invented the telegraph?? and ?Who di-
rected the Titanic?? can be generated from match-
ing the pattern ?NP VBD[was] VBN IN:[by]
NP{ANSWER}? with the sentences The telegraph was
invented by Samuel Morse and The Titanic was di-
rected by James Cameron, respectively.
4 Conclusions and Future Work
We presented an approach to generating questions
based on linguistic patterns, automatically learned
from the Web from a set of seeds. We addressed the
impact of adding semantics to patterns in matching
text segments and generating new NL questions.
We did not detect any improvement when adding
semantics to the patterns, mostly because the pat-
terns with named entities did not match too many
text segments. Nevertheless, from a small set of 6
seeds, we generated 447 NL questions. From these,
we evaluated 100 and 46% were considered correct
at the lexical, syntactic and semantic levels.
In the future, we intend to pre-process the texts
against which the patterns are matched and from
which the questions are generated. Also, we are
experimenting this approach in another language.
We aim at using more complex questions as seeds,
studying its influence on the generation of questions.
Acknowledgements
This work was supported by FCT (INESC-ID mul-
tiannual funding) through the PIDDAC Program
funds, and also through the project FALACOMIGO
(ProjectoVII em co-promoc?a?o, QREN n 13449).
Ana Cristina Mendes is supported by a PhD fel-
lowship from Fundac?a?o para a Cie?ncia e a Tecnolo-
gia (SFRH/BD/43487/2008).
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
intyre. 1995. Bracketing Guidelines for Treebank II
Style Penn Treebank Project.
Wei Chen, Gregory Aist, , and Jack Mostow. 2009. Gen-
erating questions automatically from informational
text. In The 2nd Workshop on Question Generation.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proc. 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 363?
370. ACL.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In ACL-44: Proc. 21st Int. Conf. on Com-
putational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, pages
497?504. ACL.
Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.
2010. Natural language question generation using
syntax and keywords. In The 3rd Workshop on Ques-
tion Generation.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In LREC 2006.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proc. 19th Int. Conf. on Computational Lin-
guistics, pages 1?7. ACL.
Ana Cristina Mendes, Se?rgio Curto, and Lu??sa Coheur.
2011. Bootstrapping multiple-choice tests with the-
mentor. In CICLing, 12th International Conference
on Intelligent Text Processing and Computational Lin-
guistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proc. Main Conference, pages 404?411. ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering sys-
tem. In ACL ?02: Proc. 40th Annual Meeting on As-
sociation for Computational Linguistics, pages 41?47.
ACL.
Joa?o Silva, Lu??sa Coheur, Ana Mendes, and Andreas
Wichert. 2011. From symbolic to sub-symbolic in-
37
formation in question classification. Artificial Intelli-
gence Review, 35:137?154.
M. M. Soubbotin. 2001. Patterns of potential answer
expressions as clues to the right answers. In Proc. 10th
Text REtrieval Conference (TREC), pages 293?302.
Brendan Wyse and Paul Piwek. 2009. Generating ques-
tions from openlearn study units. In The 2nd Workshop
on Question Generation.
38
