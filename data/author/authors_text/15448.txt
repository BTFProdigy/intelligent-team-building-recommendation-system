Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 814?824,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Information Extraction with Distributional Prior Knowledge
Cane Wing-ki Leung1, Jing Jiang1, Kian Ming A. Chai2, Hai Leong Chieu2, Loo-Nin Teow2
1School of Information Systems, Singapore Management University, Singapore
2DSO National Laboratories, Singapore
{caneleung,jingjiang}@smu.edu.sg, {ckianmin,chaileon,tloonin}@dso.org.sg
Abstract
We address the task of automatic discovery of
information extraction template from a given
text collection. Our approach clusters candi-
date slot fillers to identify meaningful tem-
plate slots. We propose a generative model
that incorporates distributional prior knowl-
edge to help distribute candidates in a docu-
ment into appropriate slots. Empirical results
suggest that the proposed prior can bring sub-
stantial improvements to our task as compared
to a K-means baseline and a Gaussian mixture
model baseline. Specifically, the proposed
prior has shown to be effective when coupled
with discriminative features of the candidates.
1 Introduction
Information extraction (IE) is the task of extract-
ing information from natural language texts to fill a
database record following a structure called a tem-
plate. Such templates are usually defined based
on the domain of interest. For example, the do-
main in the Sixth Message Understanding Confer-
ence (MUC-6, 1995) is management succession, and
the pre-defined template consists of the slots posi-
tion, the person leaving, the person joining, and the
organization.
Previous research on IE often requires the pre-
definition of templates. Template construction is
usually done manually by domain experts, and an-
notated documents are often created to facilitate su-
pervised learning approaches to IE. However, both
manual template construction and data annotation
are labor-intensive. More importantly, templates and
annotated data usually cannot be re-used in new do-
mains due to domain dependency. It is therefore nat-
ural to consider the problem of unsupervised tem-
plate induction and information extraction. This is
the topic of this paper.
There have been a few previous attempts to ad-
dress the unsupervised IE problem (Shinyama and
Sekine, 2006; Sekine, 2006; Rosenfeld and Feld-
man, 2006; Filatova et al, 2006). These approaches
have a commonality: they try to cluster candidate
slot fillers, which are often nouns and noun phrases,
into slots of the template to be constructed. How-
ever, most of them have neglected the following im-
portant observation: a single document or text seg-
ment tends to cover different slots rather than re-
dundantly fill the same slot. In other words, during
clustering, candidates within the same text segment
should be more likely to be distributed into different
clusters.
In this paper, we propose a generative model that
incorporates this distributional prior knowledge. We
define a prior distribution over the possible label
assignments in a document or a text segment such
that a more diversified label assignment is preferred.
This prior is based on the Poisson distribution. We
also compare a number of generative models for
generating slot fillers and find that the Gaussian mix-
ture model is the best. We then combine the Poisson-
based label assignment prior with the Gaussian mix-
ture model to perform slot clustering. We find that
compared with a K-means baseline and a Gaussian
mixture model baseline, our combined model with
the proposed label assignment prior substantially
performs better on two of the three data sets we use
for evaluation. We further analyze the results on the
third data set and find that the proposed prior will
have little effect if there are no good discriminative
features to begin with. In summary, we find that
814
our Poisson-based label assignment prior is effective
when coupled with good discriminative features.
2 Related Work
One common approach to unsupervised IE is based
on automatic IE pattern acquisition on a cluster of
similar documents. For instance, Sudo et al (2003)
and Sekine (2006) proposed different methods for
automatic IE pattern acquisition for a given domain
based on frequent subtree discovery in dependency
parse trees. These methods leveraged heavily on the
entity types of candidates when assigning them to
template slots. As a consequence, potentially dif-
ferent semantic roles of candidates having the same
entity type could become indistinguishable (Sudo et
al., 2003; Sekine, 2006). This problem is alleviated
in our work by exploiting distributional prior knowl-
edge about template slots, which is shown effective
when coupled with discriminative features of can-
didates. Filatova et al (2006) also considered fre-
quent subtrees in dependency parse trees, but their
goal was to build templates around verbs that are
statistically important in a given domain. Our work,
in contrast, is not constrained to verb-centric tem-
plates. We aim to identify salient slots in the given
domain by clustering.
Marx et al (2002) proposed the cross-component
clustering algorithm for unsupervised IE. Their al-
gorithm assigned a candidate from a document to
a cluster based on the candidate?s feature similarity
with candidates from other documents only. In other
words, the algorithm did not consider a candidate?s
relationships with other candidates in the same doc-
ument. Our work is based on a different perspec-
tive: we model label assignments for all candidates
in the same document with a distributional prior that
prefers a document to cover more distinct slots. We
show empirically that this prior improves slot clus-
tering results greatly in some cases.
Also related to our work is open domain IE, which
aims to perform unsupervised relation extraction.
TEXTRUNNER (Banko et al, 2007), for example,
automatically extracts all possible relations between
pairs of noun phrases from a given corpus. The main
difference between open domain IE and our work
is that open domain IE does not aim to induce do-
main templates, whereas we focus on a single do-
main with the goal of inducing a template that de-
scribes salient information structure of that domain.
Furthermore, TEXTRUNNER and related studies on
unsupervised relation extraction often rely on highly
redundant information on the Web or in large cor-
pus (Hasegawa et al, 2004; Rosenfeld and Feldman,
2006; Yan et al, 2009), which is not assumed in our
study.
We propose a generative model with a distribu-
tional prior for the unsupervised IE task, where
slot fillers correspond to observations in the model,
and their labels correspond to hidden variables we
want to learn. In the machine learning literature,
researchers have explored the use of similar prior
knowledge in the form of constraints through model
expectation. For example, Grac?a et al (2007) pro-
posed to place constraints on the posterior proba-
bilities of hidden variables in a generative model,
while Druck et al (2008) studied a similar problem
in a discriminative, semi-supervised setting. These
studies model constraints as features, and enforce
the constraints through expected feature values. In
contrast, we place constraints on label assignments
through a probabilistic prior on the distribution of
slots. The proposed prior is simple and easy to inter-
pret in a generative model. Nevertheless, it will be
interesting to explore how the proposed prior can be
implemented within the posterior constraint frame-
work.
3 Problem Overview
In this section, we first formally define our unsuper-
vised IE problem. We then provide an overview of
our solution, which is based on a generative model.
3.1 Problem Definition
We assume a collection of documents or short text
segments from a certain domain. These documents
or text segments describe different events or enti-
ties, but they are about the same topic or aspect of
the domain. Examples of such collections include
a collection of sentences describing the educational
background of famous scientists and a collection of
aviation incident reports. Our task is to automati-
cally discover an IE template from this collection.
The discovered template should contain a set of slots
that play different semantic roles in the domain.
815
Input text:
Topic: Graduate Student Seminar Lunch
Dates: 13-Apr-95
Time: 12:00 PM - 1:30 PM
PostedBy: Edmund J. Delaney on 5-Apr-95 at 16:24 from andrew.cmu.edu
Abstract:
The last Graduate Student Seminar Series lunch will be held on Thursday, April 13 from noon-1:30 p.m. in room
207, Student Activities Center. Professor Sara Kiesler of SDS will speak on Carving A Successful Research Niche.
Output:
Slot Slot Filler(s)
Slot 1 (start time) 12:00PM
Slot 2 (end time) 1:30PM, 1:30 p.m.
Slot 3 (location) room 207, Student Activities Center
Slot 4 (speaker) Professor Sara Kiesler
Slot 4 (irrelevant information) Edmund J. Delaney
Figure 1: An input text from a seminar announcement collection and the discovered IE template. Note that the slots
are automatically discovered and the slot names are manually assigned.
To construct such a template, we start with identi-
fying candidate slot fillers, hereafter referred to as
candidates, from the input text. Then we cluster
these candidates with the aim that each cluster will
represent a semantically meaningful slot. Figure 1
gives an example of an input text from a collection
of seminar announcements and the resulting tem-
plate discovered from the collection. As we can see,
the template contains some semantically meaningful
slots such as the start time, end time, location and
speaker of a seminar. Moreover, it also contains a
slot that covers an irrelevant candidate. We call such
slots covering irrelevant candidates garbage slots.
We can make two observations on the mapping
from candidates to template slots from real data,
such as the text in Figure 1. Firstly, a template
slot may be filled by more than one candidate from
a single document, although this number has been
observed to be small. For example, the template
slot end time in Figure 1 has two slot fillers: ?1:30
PM? from the semi-structured header and ?1:30
p.m.? from within the abstract. Secondly, a docu-
ment tends to contain candidates that cover different
template slots. We believe that this observation is a
consequence of the fact that a document will tend to
convey as much information as possible. We further
exploit these observations in Section 4.
3.2 A General Solution
Recall that our general solution to the unsupervised
IE problem is to cluster candidate slot fillers in order
to identify meaningful slots. We leave the details of
how to extract the candidates to Section 7.1. In this
section, we assume that we have a set of candidates
x = {xi,j}, where xi,j is the j-th candidate from
the i-th document in the collection. We cluster these
candidates intoK groups for a givenK.
Let yi,j ? {1, . . . ,K} denote the cluster label for
xi,j and y denote the set of all the yi,j?s. Let xi and
yi denote the sets of all the xi,j?s and the yi,j?s in the
i-th document respectively. We assume a generative
model for x and y as follows. For the i-th document
in our collection, we assume that the number of can-
didates is known and we draw a label assignment yi
according to some distribution parameterized by ?.
Then for the j-th candidate, we generate xi,j from
yi,j according to a generative model parameterized
by ?. Since the labels y are hidden, the observed
log-likelihood of the parameters given the observa-
tions x is
L(?,?) = log p(x; ?,?)
=
?
i
log
?
yi
p(xi,yi; ?,?)
=
?
i
log
?
yi
p(yi; ?)
?
j
p(xi,j |yi,j ; ?). (1)
816
D
ni
K
(a) A multinomial prior.
D
ni
KK-1K-1
(b) The proposed Poisson-based
prior.
Figure 2: Generative models with different label assign-
ment priors. D denotes the number of documents in the
given collection, ni denotes the number of candidates in
the i-th document, andK is the number of slots (clusters).
For a given functional form of p(yi; ?) and
p(xi,j |yi,j ; ?), the best model parameters can be es-
timated by maximizing Eq. (1). In the next section,
we detail two designs of the prior p(yi; ?), followed
by different generative models for the distribution
p(xi,j |yi,j ; ?) in Section 5. Then we describe the
estimation of model parameters in Section 6.
4 Label Assignment Prior
The label assignment prior, p(yi; ?), models the
generation of labels for candidates in a document.
In this section, we first describe a commonly used
multinomial prior, and then introduce the proposed
Poisson-based prior for the unsupervised IE task.
4.1 A Multinomial Prior
Usually, one would assume that the labels for
the different candidates in the same document
are generated independently, that is, p(yi; ?) =?
j p(yi,j ; ?). Under this model, we assume that
each yi,j is generated from a multinomial distribu-
tion parameterized by?, where ?y denotes the prob-
ability of generating label y. Our objective function
in Eq. (1) then becomes:
L(?,?) = log p(x; ?,?)
=
?
i,j
log
?
y
?yp(xi,j |y; ?). (2)
Figure 2(a) depicts a generative model with this
multinomial prior in plate notation. Note that the in-
dependence assumption on label assignment in this
model does not capture our observation that candi-
dates in a document are likely to cover different se-
mantic roles.
4.2 The Proposed Poisson-based Prior
We propose a prior distribution that favors more
diverse label assignments. Our proposal takes
into consideration the following three observations.
Firstly, candidates in the same document are likely
to cover different semantic roles. The proposed prior
distribution should therefore assign higher probabil-
ity to a label assignment that covers more distinct
slots. Secondly, the same piece of information is not
likely to be repeated many times in a document. Our
design thus allows a slot to generate multiple fillers
in a document, up to a limited number of times.
Thirdly, there may exist candidates that do not be-
long to slots in the extracted template. Therefore, we
introduce a dummy slot or garbage slot to the label
set to collect such candidates. Yet, we shall not as-
sume any prior/domain knowledge about candidates
generated by the garbage slot as they are essentially
irrelevant in the given domain.
We now detail the prior that exploits the above
observations. First, we fix the K-th slot (or cluster)
in the label set to be the garbage slot. For each of
the non-garbage slot k = 1, . . . ,K ? 1, we also fix
the maximum number of fillers that can be gener-
ated, which we denote by ?k. There is no ?K for the
garbage slot because the number of fillers is not con-
strained for this slot. This allows all candidates in a
document to be generated by the garbage slot. Let
ni be the number of candidates in the i-th document.
Given K, {?k}K?1k=1 and ni, the set of possible labelassignments for the i-th document can be generated.
We illustrate this with an example. Let K = 2 and
?1 = 1. The label set is {1, 2}, where 2 represents
the garbage slot. Let the number of candidates be
ni = 2. The possible label assignments within this
setting are (1, 2), (2, 1) and (2, 2).
The set of possible label assignments for the i-
th document is the sample space on which we place
the prior distribution p(yi; ?). We need a prior that
gives a higher probability to a more diverse label
assignment. For a given yi for the i-th document,
let ni,k be the number of candidates in the docu-
ment that have been assigned to slot k. That is,
ni,k def=
?ni
j=1 1(yi,j = k), where 1(?) is the indica-
tor variable. We propose the following distribution
based on the Poisson distribution:
817
p(yi; ?) def= Z?1i
K?1?
k=1
Poisson(ni,k; ?k), (3)
where Zi is the normalizing constant, and ?k is the
mean parameter of the k-th Poisson distribution,
k = 1, . . .K ? 1. The absence of a factor that
depends on ni,K reflects the lack of prior knowl-
edge on the number of garbage slot fillers. Fig-
ure 2(b) depicts the proposed generative model with
the Poisson-based prior in plate notation.
5 Generating Slot Fillers
Different existing generative models can be used to
model the generation of a slot filler given a label, that
is, p(x|y; ?). We explore four of them for our task,
namely, the naive Bayes model, the Bernoulli mix-
ture model, the Gaussian mixture model, and a lo-
cally normalized logistic regression model proposed
by Berg-Kirkpatrick et al (2010).
5.1 Multinomial Naive Bayes
In the multinomial naive Bayes model, features of
an observation x are assumed to be independent and
each generated from a multinomial distribution. We
first introduce some notations. Let f denote a fea-
ture (e.g. entity type) and Vf denote the set of possi-
ble values for f . Let xf ? Vf be the value of feature
f in x (e.g. person). For a given label y, feature f
follows a multinomial distribution parameterized by
?y,f , where ?y,f,v denotes the probability of feature
f taking the value v ? Vf given label y. The func-
tional form of the conditional probability of x given
a label y is then
p(x|y; ?) =
?
f
p(xf |y; ?) =
?
f
?y,f,xf . (4)
5.2 Bernoulli Mixture Model
In the naive Bayes model our features are defined
to be categorical. For the Bernoulli mixture model,
as well as the Gaussian mixture model and the lo-
cally normalized logistic regression model in the
next subsections, we first convert each observation
x into a binary feature vector x ? {0, 1}F where F
is the number of binary features. An example of a
binary features is ?the entity type is person?.
We assume that, for a given label y, observations
are generated from a multivariate Bernoulli distribu-
tion parameterized by?y,f , where?y,f,v denotes the
probability of feature f taking the value v ? {0, 1}
given label y. The conditional probability of x given
y can then be written as
p(x|y; ?) =
?
f
p(xf = 1|y; ?)xf ? p(xf = 0|y; ?)1?xf
=
?
f
?y,f,xf . (5)
5.3 Gaussian Mixture Model
In the Gaussian mixture model, we assume that a
given label y generates observations with a mul-
tivariate Gaussian distribution N (?y,?y), where
?y ? RF is the mean and ?y ? RF?F is the co-
variance matrix of the Gaussian. If we assume that
the different feature dimensions are independent and
have the same variance, that is, ?y = ?2yI , where I
is the identity matrix, then the conditional density of
x given y is
p(x|y; ?) = 1(2??2y)F/2
exp
(
??x? ?y?
2
2?2y
)
. (6)
5.4 Locally Normalized Logistic Regression
Berg-Kirkpatrick et al (2010) proposed a method
for incorporating features into generative models for
unsupervised learning. Their method models the
generation of x given y as a logistic function param-
eterized by a weight vector wy, defined as follows:
p(x|y; ?) = exp?x,wy??
x? exp?x?,wy?
. (7)
?x,w? denotes the inner product between x and w.
The denominator considers all data points x? in the
data set, thus Eq. (7) gives a probability distribution
over data points for a given y.
818
6 Parameter Estimation
We can apply the Expectation-Maximization (EM)
algorithm (Dempster et al, 1977) to maximize
the log-likelihood functions under both multinomial
prior in Eq. (2) and the proposed Poisson-based prior
in Eq. (1). For the multinomial prior, there are stan-
dard closed form solutions for the naive Bayes, the
Bernoulli mixture and the Gaussian mixture models.
For locally normalized logistic regression, model
parameters can also be learned via EM, but with
a gradient-based M-step (Berg-Kirkpatrick et al,
2010). We leave out the details here and focus on pa-
rameter estimation in the proposed generative model
with the Poisson-based prior.
We assume that in the Poisson-based prior, the
parameters {?k}K?1k=1 and {?k}K?1k=1 are fixed ratherthan learned in this work. For the distribution
p(x|y; ?), let ?(t?1) and ?(t) denote parameter es-
timates from two consecutive EM iterations. At the
t-th iteration, the E-step updates the responsibilities
of each label assignment yi for each document:
?i,yi = p(yi|xi; ?,?(t?1))
= p(yi; ?)p(xi|yi; ?
(t?1))?
y?i p(y
?
i; ?)p(xi|y?i; ?(t?1))
, (8)
where ?i is a distribution over all possible label as-
signments yi?s for the i-th document. The M-step
updates the estimates of ?(t) based on the current
values of ?i?s and ?(t?1). This is done by maximiz-
ing the following objective function:
?
i
?
yi
?i,yi log
(
p(yi; ?)
?
j
p(xi,j |yi,j ; ?(t?1))
)
. (9)
The exact formulas used in the M-step for
updating ? depend on the functional form of
p(xi,j |yi,j ; ?). As an example, we give the formulas
for the Gaussian mixture model, in which? contains
the set of means {?(t)k }Kk=1 and variances {?(t)k }Kk=1.Taking the derivatives of Eq. (9) with respect to ?k
and to ?k, and then setting the derivations to zero,
we can solve for ?k and for ?k to get:
?(t)k =
?
i
?
yi ?i,yi
?
j 1(yi,j = k)xi,j?
i
?
yi ?i,yi
?
j 1(yi,j = k)
, (10)
?(t)k =
?
i
?
yi ?i,yi
?
j 1(yi,j = k)||xi,j ? ?
(t)
k ||2
F?i
?
yi ?i,yi
?
j 1(yi,j = k)
, (11)
where 1(?) is the indicator variable. We skip the
derivations here due to space limit.
Closed form solutions also exist for the naive
Bayes and the Bernoulli mixture models. For lo-
cally normalized logistic regression, parameters can
be learned with a gradient-based M-step as in the
multinomial prior setting. Existing optimization al-
gorithms, such as L-BFGS, can be used for optimiz-
ing model parameters in the M-step as discussed in
(Berg-Kirkpatrick et al, 2010).
7 Experiments
In this section, we first describe the data sets we used
in our experiments, detailing the target slots and can-
didates in each data set, as well as features we ex-
tract for the candidates. We then describe our evalu-
ation metrics, followed by experimental results.
7.1 Data Sets
We use three data sets for evaluating our unsuper-
vised IE task. Note that to speed up computation,
we only include documents or text segments con-
taining no more than 10 candidates in our experi-
ments. The first data set contains a set of seminar an-
nouncements (Freitag and McCallum, 1999), anno-
tated with four slot labels, namely stime (start time),
etime (end time), speaker and location. We used as
candidates all strings labeled in the annotated data
as well as all named entities found by the Stanford
NER tagger for CoNLL (Finkel et al, 2005). There
are 309 seminar announcements with 2262 candi-
dates in this data set.
The second data set is a collection of para-
graphs describing aviation incidents, taken from the
Wikipedia article on ?List of accidents and incidents
involving commercial aircraft? (Wikipedia, 2009).
Each paragraph in the article contains one to a few
sentences describing an incident. In this domain, we
take each paragraph as a separate document, and all
hyperlinked phrases in the original Wikipedia arti-
cle as candidates. For evaluation, we manually an-
notated the paragraphs of incidents from 2006 to
2009 with five slot labels: the flight number (FN),
the airline (AL), the aircraft model (AC), the exact
819
location (LO) of the incident (e.g. airport name),
and the country (CO) where the incident occurred.
The entire data set consists of 564 paragraphs with
2783 candidates. The annotated portion consists of
74 paragraphs with 395 candidates.
The third data set comes from the management
succession domain used in the Sixth Message Un-
derstanding Conference (MUC-6, 1995). We extract
from the original data set al sentences that were
tagged with a management succession event, and use
as candidates all tagged strings in those sentences.
This domain has four target slots, namely PersonIn
(the person moving into a new position), PersonOut
(the person leaving a position), Org (the corpora-
tion?s name) and Post (the position title). Sentences
containing candidates with multiple labels (candi-
dates annotated as both PersonIn and PersonOut) are
discarded. The extracted data set consists of 757
sentences with 2288 candidates.
7.2 Features
To extract features for candidates, we first normal-
ize each word to its lower-case, with digits replaced
by the token digit. We extract the following fea-
tures for every candidate: the candidate phrase it-
self, its head word, the unigram and bigram be-
fore and after the candidate in the sentence where
it appeared, its entity type (person, location, or-
ganization, and date/time), as well as features de-
rived from dependency parse trees. Specifically, we
first apply the Stanford lexical parser to our data
(de Marneffe et al, 2006). Then for each candi-
date, we follow its dependencies in the correspond-
ing dependency parse tree until we find a relation
r ? {nsubj, csubj, dobj, iobj, pobj} in which the
candidate is the dependent. We then construct a fea-
ture (r, v) where v is governor of the relation.
7.3 Evaluation Baseline and Method
We use the standard K-means algorithm (Macqueen,
1967) as a non-generative baseline, since K-means is
commonly used for clustering. To evaluate cluster-
ing results, we match each slot in the labeled data to
the cluster that gives the best F1-measure when eval-
uated for the slot. We report the precision (P), re-
call (R) and F1-measure for individual slot labels, as
well as the macro- and micro- average results across
all labels for each experiment. We conduct 10 trials
of experiment on each model and each data set with
different random initializations. We report the trials
that give the smallest within-cluster sum-of-squares
(WCSS) distance for K-means, and those that give
the highest log-likelihood of data for all other mod-
els. Experimental trials are run until the change in
WCSS/log-likelihood between two EM iterations is
smaller than 1 ? 10?6. All trials converged within
30 minutes.
All models we evaluate involve a parameter K,
which is the number of values that y can take on.
The value of K is manually fixed in this study. As
noted, we use a garbage slot to capture irrelevant
candidates, thus the value of K is set to the number
of target slots plus 1 for each data set. We empir-
ically set the adjustable parameters in the proposed
prior, and the weight of the regularization term in the
locally normalized logistic regression model (Berg-
Kirkpatrick et al, 2010), denoted by ?. Exact set-
tings are given in the next subsection. Note that the
focus of our experiments is on evaluating the effec-
tiveness of the proposed prior. We leave the task of
learning the various parameter values to future work.
7.4 Results
Evaluation on existing generative models
We first evaluate the existing generative models
described in Section 5 with the multinomial prior.
Table 1 summarizes the performance of Naive Bayes
(NB), the Bernoulli mixture model (BMM), the
Gaussian mixture model (GMM), the locally nor-
malized logistic regression (LNLR) model, and K-
means. We only show the F1 measures in the table
due to space limit.
We first observe that NB does not perform well
for our task. LNLR, which is an interesting contri-
bution in its own right, does not seem to be suitable
for our task as well. While NB and LNLR are infe-
rior to K-means for all three data sets, BMM shows
mixed results. Specifically, BMM outperforms K-
means for aviation incidents, but performs poorly
for seminar announcements. GMM and K-means
achieve similar results, which is not surprising be-
cause K-means can be viewed as a special case of
the spherical GMM we used (Duda et al, 2001).
Overall speaking, results show that GMM is the
best among the four generative models for the distri-
820
(a) Results on seminar announcements. No macro- and micro-average result is reported
for NB and BMM as they merged the etime cluster with the stime cluster. Numbers in
brackets are the respective measures of the stime cluster when evaluated for etime.
Model stime etime speaker location Macro-avg Micro-avg Parameter
NB 0.558 (0.342) 0.276 0.172 ? ? Nil
BMM 0.822 (0.440) 0.412 0.402 ? ? Nil
GMM 0.450 0.530 0.417 0.426 0.557 0.455 Nil
LNLR 0.386 0.239 0.200 0.208 0.264 0.266 ? = .0005
K-means 0.560 0.574 0.335 0.426 0.538 0.452 Nil
(b) Results on aviation incidents. Target slots are airline (AL) , flight number (FN), aircraft
model (AC), location (LO) and country (CO).
Model AL FN AC LO CO Macro-avg Micro-avg Parameter
NB 0.896 0.473 0.676 0.504 0.533 0.618 0.628 Nil
BMM 0.862 0.794 0.656 0.695 0.614 0.741 0.724 Nil
GMM 0.859 0.914 0.635 0.576 0.538 0.730 0.692 Nil
LNLR 0.597 0.352 0.314 0.286 0.291 0.379 0.396 ? = .0005
K-means 0.859 0.936 0.661 0.576 0.538 0.729 0.701 Nil
(c) Results on management succession events. Target slots are person joining (PersonIn),
person leaving (PersonOut), organization (Org), and position (Post).
Model PersonIn PersonOut Org Post Macro-avg Micro-avg Parameter
NB 0.545 0.257 0.473 0.455 0.459 0.437 Nil
BMM 0.550 0.437 0.800 0.767 0.650 0.648 Nil
GMM 0.583 0.432 0.813 0.803 0.679 0.676 Nil
LNLR 0.419 0.245 0.319 0.399 0.351 0.346 ? = .0002
K-means 0.372 0.565 0.835 0.814 0.645 0.665 Nil
Table 1: Performance summary of the different generative models and K-means in terms of F1.
Data set Parameter Value
Seminar announcements {?k}4k=1 {2}4k=1
{?k}4k=1 {1}4k=1
Aviation incidents {?k}5k=1 {1}5k=1
{?k}5k=1 {1}5k=1
Management succession {?k}4k=1 {1,2,2,2}
{?k}4k=1 {1,2,2,2}
Table 2: Parameter settings for p(yi; ?).
bution p(x|y; ?). We proceed with incorporating the
proposed prior into GMM for further explorations.
Effectiveness of the proposed prior
We evaluate the effectiveness of the proposed
prior by combining it with GMM. Specifically, the
combined model follows Eq. (1), with p(yi; ?) com-
puted using the Poisson-based formula in Eq. (3) and
p(xi,j |yi,j ; ?) following Eq. (6) as in GMM.
We empirically determine the parameters used in
p(yi; ?) to maximize data?s log-likelihood as noted.
Table 2 reports the values of {?k}K?1k=1 and {?k}K?1k=1for different data sets. Recall that ?k specifies the
maximum number of candidates that the k-th slot can
generate, and its value is observed to be small in real
data. ?k specifies the expected number of candidates
that the k-th slot will generate.
Table 3 reports the performance of the combined
model (?GMM with prior?) on the three data sets,
along with results of GMM and K-means for easy
comparison. The combined model improves over
both GMM and K-means for seminar announce-
ments and aviation incidents, as can be seen from the
models? macro- and micro-average performance.
The advantages brought by the proposed prior are
mainly reflected in slots that are difficult to clus-
ter under GMM and K-means. Taking seminar an-
nouncements as an example, GMM and K-means
achieve high precision but low recall for stime, and
low precision but high recall for etime. When exam-
ining the clusters produced by these two models, we
found one small cluster that contains mostly stime
fillers (thus high precision but low recall), and an-
other much larger cluster that contains mostly etime
fillers together with most of the remaining stime
fillers (thus low precision but high recall for etime).
821
(a) Results on seminar announcements.
Model Metric stime etime speaker location Macro-avg Micro-avg
GMM with Prior P 0.964 0.983 0.232 0.253 0.608 0.416
R 0.680 0.932 0.952 0.481 0.761 0.738
F1 0.798 0.957 0.374 0.331 0.676 0.532
GMM P 1.000 0.362 0.300 0.436 0.524 0.407
R 0.291 0.984 0.686 0.416 0.594 0.518
F1 0.450 0.530 0.417 0.426 0.557 0.455
K-means P 0.890 0.434 0.222 0.436 0.496 0.389
R 0.408 0.847 0.679 0.416 0.588 0.541
F1 0.560 0.574 0.335 0.426 0.538 0.452
(b) Results on aviation incidents.
Model Metric AL FN AC LO CO Macro-avg Micro-avg
GMM with Prior P 1.000 1.000 1.000 0.741 0.833 0.915 0.908
R 0.753 0.877 0.465 0.588 0.727 0.682 0.673
F1 0.859 0.935 0.635 0.656 0.777 0.782 0.773
GMM P 1.000 1.000 1.000 0.563 0.433 0.799 0.724
R 0.753 0.842 0.465 0.588 0.709 0.672 0.664
F1 0.859 0.914 0.635 0.576 0.538 0.730 0.692
K-means P 1.000 0.981 0.830 0.563 0.433 0.761 0.711
R 0.753 0.895 0.549 0.588 0.709 0.699 0.691
F1 0.859 0.936 0.661 0.576 0.538 0.729 0.701
(c) Results on management succession events.
Model Metric PersonIn PersonOut Org Post Macro-avg Micro-avg
GMM with Prior P 0.458 0.610 0.720 0.774 0.640 0.642
R 0.784 0.352 0.969 0.846 0.738 0.731
F1 0.578 0.447 0.826 0.809 0.686 0.683
GMM P 0.464 0.605 0.725 0.792 0.647 0.648
R 0.782 0.336 0.925 0.815 0.715 0.707
F1 0.583 0.432 0.813 0.803 0.679 0.676
K-means P 0.382 0.515 0.733 0.839 0.607 0.639
R 0.363 0.625 0.969 0.791 0.687 0.693
F1 0.372 0.565 0.835 0.814 0.645 0.665
Table 3: Comparison between the combined model (GMM with the proposed prior), GMM and K-means.
This shows that GMM, when used with the multi-
nomial prior, and K-means have difficulties sepa-
rating candidates from these two slots. In contrast,
the combined model improves the recall of stime to
68%, as compared to 29.1% achieved by GMM with
the multinomial prior and 40.8% by K-means, with-
out sacrificing precision. It also improves the preci-
sion of etime from 36.2% to 98.3%.
For aviation incidents, the advantage of the pro-
posed prior is reflected in the location (LO) and
country (CO) slots, which may confuse the various
models as they both belong to the entity type loca-
tion. The proposed prior improves the precision of
these two slots greatly by trying to distribute them
into appropriate slots in the clustering process.
The three models achieve very similar perfor-
mance on management succession events as Ta-
ble 3(c) shows. Surprisingly, incorporating the
Poisson-based prior into GMM does not seem useful
in separating PersonIn and PersonOut slot fillers. To
investigate the possible reasons for this, we exam-
ine feature values in the centriods of the two clusters
learned by the three models.
Tables 4 and 5 respectively list the top-10 features
in the PersonIn cluster and the PersonOut cluster
learned by the combined model1, and their corre-
sponding values in the centriods of the two clusters.
The two clusters share 3 of the top-5 features, some
1We made similar observations from centriods learned in
GMM and K-Means, which are therefore not reported here.
822
Values in the centriod of:
Top-10 features PersonIn PersonOut
type:?person? 0.9985 1
unigram after:, 0.7251 0.3404
unigram before:?s? 0.2705 0
bigram after:, ?digits? 0.2105 0.1879
bigram after:, who 0.1404 0.0567
unigram before:, 0.1067 0.0035
dobj:succeeds 0.0906 0
unigarm before:succeeds 0.0892 0
nsubj:resigned 0.0746 0.0284
unigram before:said 0.0673 0
Table 4: Top-10 features in the PersonIn cluster, as
learned by GMM with the proposed prior.
of them being general context features that might not
help characterizing candidates from different slots
(e.g. the unigram after the candidate is a comma).
Both lists also contain features from dependency
parse trees. Note that the ?dobj:succeeds? feature
in the PersonIn cluster is in fact contributed by Per-
sonOut slot fillers, while the ?nsubj:succeeds? fea-
ture in the PersonOut cluster is contributed by Per-
sonIn slot fillers. Although listed among the top-
10, these features have relatively low values in the
learned centriods (about 0.1). These observations
may suggest that the management succession data
set lacks strong, discriminative features for all mod-
els to effectively distinguish between PersonIn and
PersonOut candidates in an unsupervised manner.
To conclude, the proposed prior is effective in as-
signing different but confusing candidate slot fillers
into appropriate slots, when there exist reasonable
features that can be exploited in the label assign-
ment process. This is evident by the improvements
the proposed prior brings to GMM in the seminar
announcement and aviation incident data sets.
8 Conclusions
We propose a generative model that incorporates
distributional prior knowledge about template slots
in a document for the unsupervised IE task. Specifi-
cally, we propose a Poisson-based prior that prefers
label assignments to cover more distinct slots in the
same document. The proposed prior also allows a
slot to generate multiple fillers in a document, up to
a certain number of times depending on the domain
of interest.
We experimented with four existing generative
Values in the centriod of:
Top-10 features PersonOut PersonIn
type:?person? 1 0.9985
unigram before:mr. 0.9894 0
bigram before:?s? mr. 0.5213 0
unigram after:, 0.3404 0.7251
bigram after:, ?digits? 0.1879 0.2105
unigram after:was 0.1667 0.0556
nsubj:president 0.1667 0.0117
nsubj:succeeds 0.1028 0.0102
bigram before:, mr. 0.0957 0
unigram after:?s 0.0745 0.0073
Table 5: Top-10 features in the PersonOut cluster, as
learned by GMM with the proposed prior.
models for the task of clustering slot fillers with
a multinomial prior, which assumes that labels are
generated independently in a document. We then
evaluate the effectiveness of the proposed prior by
incorporating it into the Gaussian mixture model
(GMM), which is shown to be the best among the
four existing models in our experiments. By incor-
porating the proposed prior into GMM, we can ob-
tain significantly better clustering results on two out
of three data sets.
Further improvements to this work are possible.
Firstly, we assume that some adjustable parameters
in the proposed prior can be manually fixed, such as
the number of template slots in the output and the
maximum numbers of fillers that can be generated
by different slots. We are looking into methods for
automatically learning such parameters. This will
help improve the applicability of our work to differ-
ent domains as an unsupervised model. Secondly,
we currently consider in the prior a probability dis-
tribution over all possible label assignments for ev-
ery document. This can be computationally expen-
sive if input documents are long, or when we aim
to discover large templates with large values of K.
An alternative is to consider an approximate solution
that evaluates, for instance, only the top few label as-
signments that are likely to maximize the likelihood
of our observations. This remains as an interesting
future work of this study.
Acknowledgments
This work is supported by DSO National Laborato-
ries. We thank the anonymous reviewers for their
helpful comments.
823
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In International Joint
Conference on Artificial Intelligence, pages 2670?
2676.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proceedings of Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 582?590.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595?602.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. Wiley-Interscience, 2nd
edition.
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain
templates. In Proceedings of the COLING/ACL on
Main conference poster sessions, COLING-ACL ?06,
pages 207?214, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 363?
370.
Dayne Freitag and Andrew Kachites McCallum. 1999.
Information extraction with HMMs and shrinkage. In
Proceedings of the AAAI-99 Workshop on Machine
Learning for Information Extraction.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Proceedings of the Twenty-First Annual Conference
on Neural Information Processing Systems.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 415, Morristown, NJ, USA. Association
for Computational Linguistics.
J. B. Macqueen. 1967. Some methods for classification
and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical
Statistics and Probability, Volume 1, pages 281?297.
Zvika Marx, Ido Dagan, and Eli Shamir. 2002. Cross-
component clustering for template induction. In Pro-
ceedings of the 2002 ICML Workshop on Text Learn-
ing.
MUC-6. 1995. Proceedings of the Sixth Message Under-
standing Conference. Morgan Kaufmann, San Fran-
cisco, CA.
Benjamin Rosenfeld and Ronen Feldman. 2006. URES
: An unsupervised Web relation extraction system.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 667?674.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of the COLING/ACL Main con-
ference poster sessions, pages 731?738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, pages
304?311.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 224?231, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Wikipedia. 2009. List of accidents and
incidents involving commercial aircraft.
http://en.wikipedia.org/wiki/List of accidents and
incidents involving commercial aircraft.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised re-
lation extraction by mining Wikipedia texts using in-
formation from the web. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of the
AFNLP.
824
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 744?753, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Domain Adaptation for Coreference Resolution: An Adaptive Ensemble
Approach
Jian Bo Yang???, Qi Mao?, Qiao Liang Xiang?, Ivor W. Tsang?,
Kian Ming A. Chai?, Hai Leong Chieu?
? School of Computer Engineering, Nanyang Technological University, Singapore
? Electrical and Computer Engineering Department, Duke University, USA
? DSO National Laboratories, Singapore
jianbo.yang@duke.edu, {qmao1,qlxiang,ivortsang}@ntu.edu.sg,
{ckianmin,chaileon}@dso.org.sg
Abstract
We propose an adaptive ensemble method to
adapt coreference resolution across domains.
This method has three features: (1) it can op-
timize for any user-specified objective mea-
sure; (2) it can make document-specific pre-
diction rather than rely on a fixed base model
or a fixed set of base models; (3) it can auto-
matically adjust the active ensemble members
during prediction. With simplification, this
method can be used in the traditional within-
domain case, while still retaining the above
features. To the best of our knowledge, this
work is the first to both (i) develop a domain
adaptation algorithm for the coreference reso-
lution problem and (ii) have the above features
as an ensemble method. Empirically, we show
the benefits of (i) on the six domains of the
ACE 2005 data set in domain adaptation set-
ting, and of (ii) on both the MUC-6 and the
ACE 2005 data sets in within-domain setting.
1 Introduction
Coreference resolution is a fundamental component
of natural language processing (NLP) and has been
widely applied in other NLP tasks (Stoyanov et al
2010). It gathers together noun phrases (mentions)
that refer to the same real-world entity (Ng and
Cardie, 2002). In the past decade, several corefer-
ence resolution systems have been proposed, e.g.,
(Ng and Cardie, 2002), (Denis and Baldridge, 2007)
and (Stoyanov et al2010). All of these focus on
the within-domain case ? to use the labeled doc-
uments from a domain to predict on the unlabeled
?The work is done during postdoc in NTU, Singapore.
documents in the same domain. However, in prac-
tice, there is usually limited labeled data in a specific
domain of interest, while there may be plenty of la-
beled data in other related domains. Effective use of
data from the other domains for predicting in the do-
main of interest is therefore an important strategy in
NLP. This is called domain adaptation, and, in this
context, the former domains is called the source do-
mains, while the latter domain is called the target
domain (Blitzer et al2006; Jiang and Zhai, 2007).
Based on the type of the knowledge to be trans-
ferred to the target domain, domain adaptation learn-
ing can be categorized as instance-based method,
feature-based method, parameter-based method or
relational-knowledge-based method (Pan and Yang,
2010). Previously, domain adaptation learning has
been successfully used in other NLP tasks such as
relation extraction (Jiang, 2009) and POS tagging
(Jiang and Zhai, 2007), semantic detection (Tan et
al., 2008), name entity recognition (Guo et al2009)
and entity type classification (Jiang and Zhai, 2007).
However, to the best of our knowledge, it has yet to
be explored for coreference resolution.
In this paper, we propose an adaptive ensemble
method to adapt coreference resolution across do-
mains. This proposed method can be categorized
as both feature-based and parameter-based domain
adaptation learning methods. It has three main steps:
ensemble creation, cross-domain knowledge learn-
ing and decision inference. The first step creates
the ensemble by collecting a set of base models,
which can be any individual methods with various
features/instances/parameters settings. The second
step analyzes the collected base models from vari-
744
ous domains and learns the cross-domain knowledge
between each target domain and the source domain.
The third step infers the final decision in the target
domain based on all ensemble results.
In addition to domain adaptation, the proposed
adaptive ensemble method has the following fea-
tures that are absent in the other ensemble methods.
First, it can optimize any user-specified objective
measure without using a separate development set.
Second, it can provide document-specific prediction
instead of relying on a fixed base model or a fixed
set of base models for all documents. Third, it can
automatically adjust the active ensemble members
in decision inference so that underperforming base
models are filtered out. The proposed method can
also be used in the traditional within-domain prob-
lem with some simplifications.
We conduct experiments for coreference resolu-
tion under both the within-domain setting and the
domain-adaptation setting. In the within-domain
setting, we compare the proposed adaptive ensemble
method with the mention-pair methods and other en-
semble methods on the MUC-6 and ACE 2005 cor-
pora. The results show that the proposed adaptive
ensemble method consistently outperforms these
baselines. In the domain adaptation setting, we use
the ACE 2005 corpora to create six domain adap-
tation tasks to evaluate the effectiveness of our do-
main adaptation learning. The results show that our
method outperforms baselines that do not use do-
main adaptation.
The paper is organized as follows. Section 2 re-
views some existing ensemble methods for coref-
erence resolution. Section 3 presents the proposed
adaptive ensemble method for domain adaptation
problems. Section 4 presents a special case of
the proposed method for the within-domain setting.
Section 5 presents the experiments under both the
within-domain and the domain adaptation settings.
We conclude and discuss future work in Section 6.
2 Existing Ensemble Methods
Many ensemble methods have been proposed in the
machine learning literature, e.g., bagging (Breiman,
1996), boosting (Freund and Schapire, 1996), ran-
dom forest (Breiman, 2001) and mixture models
(Bishop, 2007). Some of them have been success-
fully used in coreference resolution (Pang and Fan,
2009; Munson et al2005; Rahman and Ng, 2011a).
However, these methods only focus on the within-
domain setting.
All these methods comprise of two steps: ensem-
ble creation and decision inference. Ng and Cardie
(2003) and Vemulapalli et al2009) applied the
bagging and boosting techniques on the documents
to create the ensemble. Recently, Rahman and Ng
(2011a) further enriched the ensemble by consider-
ing various feature sets and learning models. Specif-
ically, three types of feature sets (conventional, lex-
ical and combined) and three learning algorithms
(mention-pair model, mention-ranking model and
the clustering-ranking model) are employed. In de-
cision inference, these methods used voting or av-
eraging to get the final prediction. Rahman and Ng
(2011a) proposed four voting strategies for predic-
tion: applying best Per-NP-Type model, antecedent-
based voting, cluster-based voting and weighted
clustering-based voting. Although their approaches
achieved promising results in their end-to-end sys-
tems, these do not consider the user-specific perfor-
mance measure during the ensemble learning.
Another branch of ensemble methods uses model
selection (Munson et al2005; Ng, 2005), simi-
lar to the conventional model selection method for
generic parameter-tuning. The method of (Munson
et al2005) first collects a large family of base mod-
els. Then, a separate tuning set with ground truth
is used to evaluate each base model?s performance.
Finally, an iterative approach is used to select the
best performed base models to form the ensemble.
Like other methods, this method uses the average
strategy in decision inference. Similarly, the method
of (Ng, 2005) ranks base models according to their
performance on separate tuning set, and then uses
the highest-ranked base model for predicting on test
documents. These methods require a separate set of
labeled documents to assess the generalization per-
formance.
3 Adaptive Ensemble Method
In this section, we give our adaptive ensemble
method for domain adaptation for coreference res-
olution. We first introduce some notations.
For a corpus of N documents, document Di
745
is the ith document, and it contains ni men-
tions mi = (m1i , . . . ,m
ni
i ) with the ordering of
each mention as they appear in the document.
The index set of all mention pairs in Di is
Ei = {(a, b) | 1 ? a < b ? ni}. The transpose of
vector x is x?. The performance measure function
for document D is ?(g(D); f(D)), where g(D) and
f(D) represent the coreference ground-truth and
prediction by model f on document D respectively.
In coreference resolution, typical performance mea-
sure functions include MUC (Vilain et al1995),
Rand index (Rand, 1971), B-CUBED (Bagga and
Baldwin, 1998) and CEAF (Luo, 2005). In this pa-
per, ? can either be used as part of an objective func-
tion in learning or as an evaluation measure for as-
sessing the performance of a coreference system.
We consider the typical domain adaptation prob-
lem, which has one target domain t and p (p ? 1)
source domains s1, . . . , sp. The target domain
contains N (t) labeled documents and M unla-
beled documents, while source domains contain
N (s1), . . . , N (sp) labeled documents. Unlabeled
data in the source domains are not used. We use
D(v)i for the ith document in domain v.
3.1 Ensemble Creation
Mention-pair methods have been widely-used for
coreference resolution due to their efficiency and
effectiveness, and they have often been taken as
base models in ensemble learning (Rahman and Ng,
2011a; Munson et al2005). We adopt a similar ap-
proach by using the standard mention-pair method
(Soon et al2001; Ng and Cardie, 2002) with var-
ious parameters to form the ensemble, though our
framework can incorporate other coreference meth-
ods in the ensemble. Mention-pair methods usu-
ally comprise of two steps. The first step classifies
every mention pair into either coreference or non-
coreference with a confidence between 0 and 1. The
second step partitions the set of mentions into clus-
ters based on the confidence values, where mentions
in each cluster are presumed to be the same under-
lying entity.
Classification We use Soon?s approach (Soon et
al., 2001) to select a portion of mention pairs to train
a binary classifier because this has better generaliza-
tion (Soon et al2001). The positive mention pairs
are the anaphoric mentionmbi (b = 2, . . . , ni) paired
with its closest antecedent mention mai (a < b),
while the negative mention pairs are the mention
mbi paired with each of the intervening mentions
ma+1i ,m
a+2
i , . . . ,m
b?1
i . Following (Rahman and
Ng, 2011a), our binary classifier is SVM with the
regularization parameter C. The classifier is trained
with the software Liblinear (Fan et al2008), which
is also used to give probabilistic binary predictions.
Clustering We adopt closest-first clustering (Soon
et al2001) and best-first clustering (Ng and Cardie,
2002) to determine whether a mention pair is coref-
erent. For each mention, the closest-first method
(or best-first method) links it to the the closest (or
the best) preceding mention if the confidence value
(obtained from the first step) of this mention pair is
above a specified threshold t.
Features For each mention pair, we use the
d = 39 features proposed by Rahman and Ng
(2011b) to represent it. These features can be ex-
tracted using the Reconcile software (Stoyanov et
al., 2010). We use ??a,b ? Rd to represent the fea-
tures of a mention pair (ma,mb). With this feature
set, we found that the linear kernel is insufficient to
fit the training data. However, using an rbf kernel
would be too computationally expensive. Hence, we
augment ??a,b with a d?-dimensional feature vector
[?1 ? ? ? ?d?] to give a new feature vector
?a,b = [??a,b ?1 ? ? ? ?d?], (1)
where the d? augmented features [?1 ? ? ? ?d?] are de-
termined by
?j = exp(?
???a,b ? cj?2
d
),?j = 1, . . . , d?. (2)
Herein, c1, . . . , cd? are the d? centroids of the
randomly-selected subset C from all labeled men-
tion pairs {??a,b | (a, b) ? E1, . . . , EN}. In our ex-
periments, we use the k-means algorithm to obtain
the centroids of C.
Ensemble For domain v, we create a domain-
specified ensemble F (v) = {f1, . . . , f ?} of ? base
models by including the closest-first and best-first
mention-pair methods with the differentC and t val-
ues. If multiple domains are provided, we gather all
746
the domain-specific ensembles into a grand ensem-
ble F = F (s1) ? ? ? ? F (sp) ? F (t).
3.2 Cross-domain Knowledge Learning
Generally, the feature distributions are different in
different domains. Therefore, effective domain
adaptation requires using some knowledge of cross-
domain similarity. We now propose an approach
to learn the parametric-distances between the doc-
uments in source and target domains to characterize
this cross-domain knowledge.
Distances between documents A document Di is
represented by the sum of its new mention-pair fea-
tures (Yu and Joachims, 2009; Finley and Joachims,
2005):
?(Di) =
?
(a,b)?Ei
?a,b. (3)
The distance between a source labeled document
D(su)i in domain su and a target labeled document
D(t)j is parameterized as
Dist(D(su)i ,D
(t)
j ;?) = ?
??(D(su)i ,D
(t)
j ), (4)
where vector ? ? Rd+d? is to be learned, and vec-
tor function?(D(su)i ,D
(t)
j ) ? Rd+d? is the Euclidean
distance vector between two documents given by
?(D(su)i ,D
(t)
j ) = (?(D
(su)
i )? ?(D
(t)
j ))
? (?(D(su)i )? ?(D
(t)
j )). (5)
The operator ? is the element-wise product. Dis-
tance (4) is actually the Mahanalobis distance (Yang
and Jin, 2006) with the scaling of features:
(?(D(su)i )? ?(D
(t)
j ))
?W (?(D(su)i )? ?(D
(t)
j )),
where W is a diagonal matrix with diagonal entries
?. MatrixW is diagonal to reduce computation cost
and to increase statistical confidence in estimation
when there is limited target labeled data (as is typi-
cally the case in domain adaptation).
That ? is the vector of diagonal entries in W re-
quires that each entry in ? is non-negative. If the lth
entry of ? is non-zero, then the lth feature in ?a,b
contribute towards (4). To ensure that at least B fea-
tures are used, we also constrain that each entry in ?
is not more than unity and that 1?? ? B.
Matching best base models For each labeled doc-
ument D(v)j in domain v, we identify the best per-
forming base model f (v)
?
j in F (v) with
f (v)
?
j = arg maxf?F(v)
?(g(D(v)j ); f(D
(v)
j )), (6)
where ?(? ; ?) is the the performance objective func-
tion to be instantiated in Section 3.3.
Then, for each source domain su and document
D(t)j in the target domain, we find the set I(D
(t)
j ; su)
of the documents in domain su that have the same
best performing base model as that for D(t)j :
I(D(t)j ; su) = {D
(su)
i | f
(su)?
i = f
(t)?
j ,
i = 1, . . . , N (su)}. (7)
The key idea in I(D(t)j ; su) is to select documents
in a source domain su that are similar to document
D(t)j in the sense that they have the same best per-
forming base model under a specific ?. This ensures
that optimization step (to be described next) is tar-
geted towards ? and not confounded by document
pairs that should be disimilar anyway.
Optimization We determine the vector? by mini-
mizing the parametric distance (4) between all target
labeled documents and their corresponding source
labeled document identified in the previous step.
That is,
min
?
??
N(t)
?
j=1
?
D(su)i ?I(D
(t)
j ;su)
?(D(su)i ,D
(t)
j ). (8)
The solution ? to this linear programming problem
can be regarded as the cross-domain knowledge be-
tween source domain su and the target domain t. Re-
peating for every source domain su, u = 1, . . . , p,
gives the cross-domain knowledge between every
source domain and the target domain.
The above three-steps procedure selects the effec-
tive features for each pair of source and target do-
mains. Generally, the results of feature selection
vary for different pairs of source and target domains,
due to the diversities of the feature distributions in
different domains.
747
3.3 Decision Inference
After ensemble creation and cross-domain knowl-
edge learning, we need to provide the coreference
result on an unseen document in the target domain
based on the results of all the members in F . Un-
like the previous methods using the voting/average
or their variants (Pang and Fan, 2009; Munson et
al., 2005; Rahman and Ng, 2011a), we propose the
following nearest neighbor based approach.
Given the grand ensemble F and all labeled doc-
uments, the task is to predict on the target unlabeled
document D(t)j , j = 1, . . . ,M . The idea of the pro-
posed method is to first find the k most similar docu-
ments N (D(t)j ) from all labeled documents for doc-
ument D(t)j . Then, we choose the base model that
performs best on the documents in N (D(t)j ) as the
method f (t)
?
j for document D
(t)
j .
Firstly, we employ the parametric-distance (4) to
measure the similarity between any labeled docu-
mentD(v)i ,?v, i, from all source and target domains,
and the target unlabeled document D(t)j . Here, the
cross-domain knowledge ? in (4) has already been
determined by the optimization (8) in Section 3.2.
Secondly, based on the computed distance values,
we select k nearest neighbor documents for the tar-
get unlabeled document D(t)j from all labeled doc-
uments D(v)i ,?v, i. These k nearest neighbor docu-
ments for document D(t)j make up the set N (D
(t)
j ).
Thirdly, the optimal base model for the unlabeled
document D(t)j prediction is chosen by
f (t)
?
j = arg max
Dp?N (D(t)j ), f?F
?(g(Dp); f(Dp)). (9)
We can instantiate the performance objective func-
tion ?(g(?); f(?)) in expressions (6) and (9) to be
any coreference resolution measures, such as MUC,
Rand index, B-CUBED and CEAF. We have not
known of other (ensemble) coreference resolution
methods that optimize for these measures. This ab-
sence is possibly due to their complex discrete and
non-convex properties.
3.4 Discussion
The above proposed adaptive ensemble approach in-
corporates the domain adaptation knowledge during
(a) the identification of similar documents between
different domains and (b) the determination of ac-
tive ensemble members. Beside these, it has the fol-
lowing features over other (ensemble) coreference
methods: (i) It can optimize any user-specified ob-
jective measure via (6) and (9). An intuitive rec-
ommendation is to directly optimize for an objective
function that matches the evaluation measure. (ii)
It can make document-specific decisions, as expres-
sions (4) and (9) deal with each testing document
separately. (iii) The prediction on the testing docu-
ment D(t)j is not based on all members in F but only
on the active ensemble members N (D(t)j ). This can
filter out some potentially unsuitable base models
for document D(t)j . Moreover, the active ensemble
members N (D(t)j ) is dynamically adjusted for each
test document.
For computational cost, the majority is by ensem-
ble creation, since a large number of base models
are usually used. This is common among all ensem-
ble methods. In contrast, the costs in (4) and (9)
are trivial as both are at the document level. The
cost of generating centroids in (2) can also be high
if the size of C is more than ten thousand, but this
is still negligible compared to the cost of ensemble
creation.
4 Special Case: Within-domain Setting
The adaptive ensemble method presented in Sec-
tion 3 is for the domain adaptation setting. How-
ever, it is possible to simplify it for the special case
of within-domain setting. In the within-domain set-
ting, the adaptive ensemble method only has ensem-
ble creation and decision inference steps.
In the ensemble creation step, we still use the
closest-first and best-first mention-pair methods
with various parameters to create the ensemble. Un-
like the domain adaptation setting, here we can only
use the labeled documents in the target domain to
create the ensemble F (t). Therefore, the size of en-
semble here is reduced by p times compared to the
domain adaptation setting.
In the decision inference step, we directly use the
Euclidean distance ?(D(t)i ,D
(t)
j ) in (5) for the la-
beled documentD(t)i , i = 1, . . . , N (t) and unlabeled
document D(t)j , j = 1, . . . ,M . Based on these dis-
748
tance values, we similarly select k nearest neighbor
documentsN (D(t)j ) for documentD
(t)
j , and then de-
termine the final method f (t)
?
j for document D
(t)
j by
(9) but with F replaced by F (t).
5 Experiments
We test the proposed adaptive method and sev-
eral baselines under both the within-domain and
the domain adaptation settings on the MUC-6 and
ACE 2005 corpora. MUC-6 contains 60 docu-
ments. ACE 2005 contains 599 documents from
six different domains: Newswire (NW), Broadcast
News (BN), Broadcast Conversations (BC), Web-
blog (WL), Usenet (UN), and Conversational Tele-
phone Speech (CTS). In all our experiments, we use
two popular performance measures, B-CUBED F-
measure (Bagga and Baldwin, 1998) and CEAF F-
measure (Luo, 2005) 1, to evaluate the coreference
resolution result. Since the focus of the paper is to
investigate the effectiveness of coreference resolu-
tion methods, we use the gold standard mentions in
all experiments.
For the proposed method, the ensemble F (v) in
every domain v has 208 members totally. They
are created by the closest-first and the best-first
mention-pair methods using SVM trained with pa-
rameter C taking values
C ? [0.001, 0.01, 0.1, 1, 10, 100, 1000, 1000] (10)
and using clustering with the threshold parameters t
taking values
t ? [0.2, 0.25, 0.3, 0.34, 0.38, 0.4, 0.42, 0.44,
0.46, 0.48, 0.5, 0.6, 0.7].
(11)
The size of the selected subset C is fixed to 2000,
and the number of centroids is determined by
the validation procedure from four possible values
[10, 20, 30, 40]. We use k-means algorithm to com-
pute the centroids. Due to the randomness of sub-
set C and k-means algorithm, we run the proposed
method 5 times and report the average results. For
the number of nearest neighbor k, we report three
results, each for k ? {1, 3, 5}.
1More exactly, we use the widely used ?3-CEAF F-measure.
Table 1: The settings in the experiments under within-
domain setting on MUC-6 and ACE 2005 corpora. N (t)
and M (t) and Total are the numbers of training, testing
and all documents respectively.
Domain N (t) M (t) Total
MUC-6 30 30 60
BC 48 12 60
BN 181 45 226
CTS 31 8 39
NW 85 21 106
UN 39 10 49
WL 95 24 119
5.1 Within-domain Setting
We conduct the experiment under the within-domain
setting on seven tasks, with the per-domain setting
shown in Table 1. The validation set is created by
further splitting training data into validation train-
ing and validation testing sets with the ratio of N(t)M(t) ,
where N (t) and M (t) are given in Table 1. In this
experiment, we attempt to study the following three
things. First, we investigate whether the proposed
ensemble method is better than the tuned mention-
pair methods and other ensemble methods. Second,
we investigate the optimal number of active ensem-
ble members. Third, we investigate the impact to the
performance of the coreference system, when differ-
ent objective measures are used with different eval-
uation measures.
For the proposed ensemble method, we experi-
mented with nearest neighbor set of sizes k = 1, 3, 5
paired with objective function ? in (9) set to Rand
Index, CEAF or B-CUBED. For baselines, the fol-
lowing four are used:
? Two mention-pair baselines. Two baselines are
the closest-first and the best-first mention-pair
methods with the tuned parameters C and t. In
the tuning process, the ranges of C and t are
specified in (10) and (11) respectively. These
two mention-pair methods are named as Sc and
Sb for short.
? Two existing ensemble baselines. The other
two baselines are the ensemble methods us-
ing the voting procedure in decision inference.
749
Table 2: B-CUBED F-measure results by all methods under within-domain setting on MUC-6 and ACE 2005 corpora.
Baselines ? = Rand ? = CEAF ? = B-CUBED
Sc Sb Em Ec k=1 3 5 k=1 3 5 k=1 3 5
MUC-6 66.1 66.1 61.9 57.1 67.6 67.3 68.5 65.2 64.1 65.5 68.7 66.7 67.5
BC 64.1 65.1 34.2 24.8 65.5 65.4 65.7 65.9 65.5 62.9 66.5 66.1 66.0
BN 75.9 74.8 57.7 48.0 75.7 75.1 74.9 76.3 75.9 75.3 76.4 76.3 76.7
CTS 71.0 65.1 39.6 31.5 70.6 69.3 68.3 71.3 69.9 70.4 71.7 70.6 69.1
NW 74.6 74.4 45.6 34.1 74.3 74.8 72.9 73.2 71.4 70.1 75.0 74.6 73.7
UN 69.5 70.2 44.1 27.4 70.4 69.9 69.3 69.6 67.6 66.0 70.3 71.4 70.3
WL 73.8 75.4 69.8 58.5 75.5 74.6 73.9 75.5 73.0 73.4 76.2 75.5 75.6
Average 70.7 70.2 50.4 40.2 71.4 70.9 70.5 71.0 69.6 69.1 72.1 71.6 71.3
These two baselines use the same ensemble as
the proposed method for fair comparison. In
decision inference, these two baselines use the
mention-based voting and cluster-based voting
respectively, as proposed in (Rahman and Ng,
2011a). In these two baselines, all members
in the ensemble participate the voting process.
These two ensemble baselines are named as Em
and Ec for short.
Tables 2 and 3 show the experiment results using
B-CUBED and CEAF as the evaluation measures
respectively. The best result for each of the seven
tasks is highlighted in bold. The last rows of the ta-
bles show the average performance value among all
seven tasks.
From the results, we observe that the proposed en-
semble method with objective function matching the
evaluation measure and with k = 1 generally per-
forms best among all methods and all tasks. Surpris-
ingly, the common ensemble method with mention-
based voting Em and cluster-based voting Ec strate-
gies do not perform well. The plausible reason is
the current ensemble may incorporate some bad base
models due to inappropriate C and t values, which
would undermine the voting result. Nevertheless, it
is difficult to judge the quality of the ensemble mem-
bers in advance. Therefore, this validates the impor-
tance of choosing an active set of ensemble members
in decision inference. The better performance of the
proposed method over the mention-pair baselines Sc
and Sb is probably because of the document-specific
decision. This is reasonable, as different base mod-
els in the ensemble would be good at predicting
the different documents. For the proposed ensem-
ble method with various configurations, we observe
using an objective function that matches the evalu-
ation measures is generally better. An exception is
the MUC-6 and BN tasks in CEAF F-measure. We
also observe that the ensemble method with k = 1
is generally better than that with the larger k, except
the BN and UN tasks in B-CUBED F-measure. This
suggests that the fewer the active ensemble members
the better the generalization performance. Follow-
ing (Rahman and Ng, 2011a), we also conduct the
Student?s t-test, and the results show that the pro-
posed method with the objective function matching
the evaluation measure and with k = 1 is signifi-
cantly better than the best baseline. In contrast, the
two baseline ensemble methods that use voting are
significantly worse than the best baseline. The sig-
nificance level 0.05.
5.2 Domain-adaptation Setting
We employ ACE 2005 corpora to simulate the do-
main adaptation settings in experiments. Specifi-
cally, we create six domain adaptation tasks, BC,
BN, CTS, NW, UN, WL in total. Each task has one
target domain and five source domains. For exam-
ple, in the task UN, the target domain is UN while
the other five source domains are BC, BN, CTS, NW
and WL. The number of labeled documents in each
domain is as the same as in Table 1, except when
that domain is the target domain, in which case we
use only five labeled documents. The number of test
750
Table 3: CEAF F-measure results by all methods under within-domain setting on MUC-6 and ACE 2005 corpora.
Baselines ? = Rand ? = B-CUBED ? = CEAF
Sc Sb Em Ec k=1 3 5 k=1 3 5 k=1 3 5
MUC-6 62.6 62.5 62.7 57.5 62.0 60.6 61.0 64.5 62.7 63.8 63.1 58.7 59.2
BC 58.8 56.5 36.6 26.6 56.7 57.1 57.0 58.3 58.8 57.2 59.3 59.2 58.4
BN 67.9 66.5 55.1 44.7 69.4 69.4 69.9 69.8 70.2 69.6 69.5 69.0 68.7
CTS 61.0 60.7 38.6 31.5 67.1 66.9 63.6 68.1 68.4 68.2 68.5 67.6 67.7
NW 66.9 66.4 41.1 31.2 68.4 68.0 64.6 69.2 68.4 66.4 69.3 66.1 66.7
UN 62.5 63.5 46.2 28.9 62.9 61.8 60.9 62.2 63.7 62.9 63.9 61.5 60.4
WL 69.7 70.3 63.5 54.3 70.7 70.2 72.5 71.5 71.4 72.3 72.4 69.4 70.0
Average 64.2 63.8 49.1 39.2 65.3 64.9 64.2 66.2 66.2 65.8 66.6 64.5 64.5
(or unlabeled) documents in the target document is
also the same as in Table 1. The validation set is
created similarly as in the experiment under within-
domain setting.
For the proposed ensemble method, we heuristi-
cally determine the parameter B in ? to be the num-
ber of non-zero elements in ?, where
? =
N(t)
?
j=1
?
D(su)i ?I(D
(t)
j ;su)
?(D(su)i ,D
(t)
j ).
Making use of the conclusion in the experiments
for the within-domain setting, we fix the optimized
measure to be the final performance measure in (9).
We compare with the following five baselines.
? Two mention-pair baselines in within-domain
setting. Two baselines are same as Sc and Sb in
the experiments under within-domain settings,
except that the labeled training documents are
reduced to 5.
? Three proposed adaptive ensemble methods
without cross-domain knowledge learning.
These three baselines uses neighborhood sizes
k = 1, 3, 5 with the grand ensemble F rather
than the target domain ensemble F (t). In an-
other words, these three baselines are the same
as the proposed method, but with ? = 1.
Tables 4 and 5 show the experimental results in
the domain adaptation settings using B-CUBED and
CEAF as the final performance measures respec-
tively. From the results, we can see that the pro-
posed method with cross-domain knowledge gener-
ally outperforms all the five baselines. Among them,
the best proposed domain adaptation method on av-
erage outperforms the best of Sc, Sb by 7.2% for B-
CUBED F-measure and 3% for CEAF F-measure.
The grand-ensemble baselines are also significantly
better than the within-domain baselines. These re-
sults clearly illustrate the usefulness of making use
of the labeled documents in the source domains. For
the comparison between the proposed method with
and without cross-domain knowledge learning, all
tasks, except UN task in CEAF F-measure, show
the superiority of the proposed method with cross-
domain knowledge learning. Among them, except
tasks BN and CTS in B-CUBED F-measure, the per-
formance gains are among 1%?3% for all tasks in
both measures. These results verify the necessity
of cross-domain knowledge learning. For the com-
parison of the proposed method with different k,
unlike the results in the within-domain setting, the
results here show that choosing optimal k is task-
dependent. The reason of this observation is not
clear yet. It is plausible due to the increased uncer-
tainties from multiple domains.
6 Conclusions and Future Work
In this paper, we proposed an adaptive ensem-
ble method for coreference resolution under both
within-domain and domain adaptation settings. The
key advantage of the proposed method is incor-
751
Table 4: B-CUBED F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with ?
set to B-CUBED. The within-domain and grand ensemble methods are the baselines.
Within-domain Grand ensemble Domain-adaptation
Sc Sb k=1 3 5 k=1 3 5
BC 58.0 65.1 65.0 67.1 67.0 67.5 68.2 67.7
BN 72.7 73.8 75.0 75.3 75.0 75.3 75.4 74.3
CTS 63.2 62.1 65.7 64.8 64.0 64.1 65.8 65.8
NW 54.9 54.6 73.6 73.1 74.2 73.0 74.4 74.7
UN 66.5 42.7 67.2 68.2 68.9 69.7 68.7 68.2
WL 68.6 73.2 73.0 72.6 73.4 74.8 74.5 73.6
Average 64.0 61.9 69.9 70.2 70.4 70.7 71.2 70.7
Table 5: CEAF F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with ? set
to CEAF. The within-domain and grand ensemble methods are the baselines.
Within-domain Grand ensemble Domain-adaptation
Sc Sb k=1 3 5 k=1 3 5
BC 55.7 43.7 56.9 57.6 57.3 58.5 58.8 57.2
BN 65.8 67.2 65.9 64.1 65.8 63.9 62.7 67.2
CTS 56.0 51.0 56.6 54.6 53.7 58.6 57.4 55.3
NW 52.7 55.0 66.4 64.1 63.8 69.4 66.7 66.8
UN 64.0 39.1 63.6 63.7 64.4 64.3 62.9 62.7
WL 70.3 64.2 68.1 67.8 70.2 67.3 69.6 72.0
Average 60.7 53.4 62.9 62.0 62.5 63.7 63.0 63.5
porating the cross-domain knowledge to aid coref-
erence resolution learning. This is useful when
the labeled coreference labels are scarce. We also
demonstrate that the proposed adaptive ensemble
method can be readily applied to conventional coref-
erence tasks without cross-domain knowledge learn-
ing. Compared with existing ensemble methods, the
proposed method is simultaneously endowed with
the following three distinctive features: optimizing
any user-specified performance measure, making the
document-specific prediction and automatically ad-
justing the active ensemble members. In the exper-
iments under both within-domain settings and do-
main adaptation settings, the results evidence the
effectiveness of the proposed cross-domain knowl-
edge learning method, and also demonstrate the su-
periority of the proposed adaptive ensemble method
over other baselines.
Currently, the proposed method relies on some
limited target annotations. It would be interesting
to consider the pure unsupervised tasks that have no
any target annotations. Besides, to develop some
better ways for document-level representation, e.g.,
incorporating the domain knowledge, also deserves
our attentions. Similarly, to extend the diagonal Ma-
halanobis matrix to the general covariance matrix is
also desirable. Last but not least, to find a more sys-
tematical way to determine the optimal k in the pro-
posed method is also our possible future work.
Acknowledgments
This work is supported by DSO grant
DSOCL10021.
References
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
752
model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics - Volume 1, ACL?98, pages 79?85.
Christopher M. Bishop. 2007. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer, 1st ed. 2006. corr. 2nd printing edition,
October.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP, pages
120?128.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140, August.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32, October.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proc HLT, pages 236?
243, Rochester, New York, April.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Thomas Finley and Thorsten Joachims. 2005. Super-
vised clustering with support vector machines. In
Proc. ICML.
Yoav Freund and Robert E. Schapire. 1996. Experiments
with a New Boosting Algorithm. In Proc. ICML,
pages 148?156.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. NAACL ?09, pages 281?289.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proc. ACL,
pages 264?271, Prague, Czech Republic, June.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1012?1020, Suntec, Singapore, August. Association
for Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 25?
32.
Art Munson, Claire Cardie, and Rich Caruana. 2005.
Optimizing to arbitrary NLP metrics using ensemble
selection. In Proc HLT and EMNLP, pages 539?546.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proc. ACL, pages 104?111.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proc. HLT-NAACL.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of the ACL, pages 157?164.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 22(10):1345?1359, October.
Wenbo Pang and Xiaozhong Fan. 2009. Chinese coref-
erence resolution with ensemble learning. In Proc.
PACIIA, pages 236?243.
Altaf Rahman and Vincent Ng. 2011a. Ensemble-
based coreference resolution. In Proceedings of IJ-
CAI, pages 1884?1889.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. JAIR, 1:469?52.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical Association, 66(336):pp. 846?850.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics,, pages 521?
544.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010. Coref-
erence resolution with reconcile. In Proc. ACL, pages
156?161.
Songbo Tan, Yuefen Wang, Gaowei Wu, and Xueqi
Cheng. 2008. Using unlabeled data to handle domain-
transfer problem of semantic detection. In Proceed-
ings of the 2008 ACM symposium on Applied comput-
ing, SAC ?08, pages 896?903.
S. Vemulapalli, X. Luo, J.F.Pitrelli, and I. Zitouni. 2009.
classifier combination applied to coreference resolu-
tion. In NAACL HLT Student Rsearch Workshop.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th conference on Message understanding,
MUC6 ?95, pages 45?52.
Liu Yang and Rong Jin. 2006. Distance Metric Learning:
A Comprehensive Survey. Technical report, Depart-
ment of Computer Science and Engineering, Michigan
State University.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proc. ICML, pages 1169?1176, New York, NY, USA.
753
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 807?817,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Robust Domain Adaptation for Relation Extraction via Clustering
Consistency
Minh Luan Nguyen
??
, Ivor W. Tsang
?
, Kian Ming A. Chai
?
, and Hai Leong Chieu
?
?
Institute for Infocomm Research, Singapore
?
Centre for Quantum Computation & Intelligent Systems, University of Technology, Sydney, Australia
?
DSO National Laboratories, Singapore
mlnguyen@i2r.a-star.edu.sg, ivor.tsang@gmail.com
{ckianmin,chaileon}@dso.org.sg
Abstract
We propose a two-phase framework to
adapt existing relation extraction classi-
fiers to extract relations for new target do-
mains. We address two challenges: neg-
ative transfer when knowledge in source
domains is used without considering the
differences in relation distributions; and
lack of adequate labeled samples for rarer
relations in the new domain, due to a
small labeled data set and imbalance rela-
tion distributions. Our framework lever-
ages on both labeled and unlabeled data
in the target domain. First, we determine
the relevance of each source domain to
the target domain for each relation type,
using the consistency between the clus-
tering given by the target domain labels
and the clustering given by the predic-
tors trained for the source domain. To
overcome the lack of labeled samples for
rarer relations, these clusterings operate
on both the labeled and unlabeled data in
the target domain. Second, we trade-off
between using relevance-weighted source-
domain predictors and the labeled target
data. Again, to overcome the imbalance
distribution, the source-domain predictors
operate on the unlabeled target data. Our
method outperforms numerous baselines
and a weakly-supervised relation extrac-
tion method on ACE 2004 and YAGO.
1 Introduction
The World Wide Web contains information on
real-world entities, such as persons, locations and
?
The work is done while Nguyen was a research staff in
Nanyang Technological University, Singapore.
organizations, which are interconnected by vari-
ous semantic relations. Detecting these relations
between two entities is important for many tasks
on the Web, such as information retrieval (Salton
and McGill, 1986) and information extraction for
question answering (Etzioni et al, 2008). Recent
work on relation extraction has demonstrated that
supervised machine learning coupled with intelli-
gent feature engineering can provide state-of-the-
art performance (Jiang and Zhai, 2007b). How-
ever, most supervised learning algorithms require
adequate labeled data for every relation type to be
extracted. Due to the large number of relations
among entities, it may be costly to annotate a large
enough set of training data to cover each relation
type adequately in every new domain of interest.
Instead, it can be more cost-effective to adapt an
existing relation extraction system to the new do-
main using a small set of labeled data. This paper
considers relation adaptation, where a relation ex-
traction system trained on many source domains is
adapted to a new target domain.
There are at least three challenges when adapt-
ing a relation extraction system to a new domain.
First, the same semantic relation between two en-
tities can be expressed using different lexical or
syntactic patterns. For example, the acquisition of
company A by company B can be expressed with
?B bought over by A?, ?A buys B? and ?A pur-
chases B?. To extract a relation, we need to cap-
ture the different ways in which it can be expressed
across different open domains on the Web.
Second, the emphasis or interest on the different
relation types varies from domain to domain. For
example, in the organization domain, we may be
more interested in extracting relations such as lo-
catedIn (between a company and a location) and
founderOf (between a company and a person),
807
whereas in the person domain we may be more in-
terested in extracting relations such as liveIn (be-
tween a person and a location) and workAt (be-
tween a person and a company). Therefore, al-
though the two domains may have the same set
of relations, they probably have different marginal
distributions on the relations. This can produce a
negative transfer phenomenon (Rosenstein et al,
2005), where using knowledge from other do-
mains degrades the performance on the target do-
main. Hence, when transferring knowledge from
multiple domains, it is overly optimistic to believe
that all source domains will contribute positively.
We call a source domain irrelevant when it has no
or negative contribution to the performance of the
target domain. One example is named entities ex-
traction adaptation, where na??ve transfer of infor-
mation from a mixed-case domain with capitaliza-
tion information (e.g., news-wire) to a single-case
domain (e.g., conversational speech transcripts)
will miss most names in the single-case domain
due to the absence of case information, which is
typically important in the mixed-case domain.
Third, the annotated instances for the target do-
main are typically much fewer than those for the
source domains. This is primarily due to the lack
of resources such as raw target domain documents,
time, and people with the expertise. Together with
imbalanced relation distributions inherent in the
domain, this can cause some rarer relations to con-
stitute only a very small proportion of the labeled
data set. This makes learning a relation classifier
for the target domain challenging.
To tackle these challenges, we propose a two-
phase Robust Domain Adaptation (RDA) frame-
work. In the first phase, Supervised Voting is used
to determine the relevance of each source domain
to each region in the target domain, using both la-
beled and unlabeled data in the target domain. By
using also unlabeled data, we alleviate the lack of
labeled samples for rarer relations due to imbal-
anced distributions in relation types.
The second phase uses the relevances deter-
mined the first phase to produce a reference pre-
dictor by weighing the source-domain predictors
for each target domain sample separately. The in-
tention is to alleviate the effect of mismatched dis-
tributions. The final predictor in the target domain
is trained on the labeled target domain data while
taking reference from the reference predictions on
the unlabeled target domain data. This ensures
reasonable predictive performance even when all
the source domains are irrelevant and augments
the rarer classes with examples in the unlabeled
data. We compare the proposed two-phase frame-
work with state-of-the-art domain adaptation base-
lines for the relation extraction task, and we find
that our method outperforms the baselines.
2 Related Work
Relation extraction is usually considered a classifi-
cation problem: determine if two given entities in
a sentence have a given relation. Kernel-based su-
pervised methods such as dependency tree kernels
(Culotta and Sorensen, 2004), subsequence ker-
nels (Bunescu and Mooney, 2006) and convolution
tree kernels (Qian et al, 2008) have been rather
successful in learning this task. However, purely
supervised relation extraction methods assume the
availability of sufficient labeled data, which may
be costly to obtain for new domains. We address
this by augmenting a small labeled data set with
other information in the domain adaptation setting.
Bootstrapping methods (Zhu et al, 2009;
Agichtein and Gravano, 2000; Xu et al, 2010;
Pasca et al, 2006; Riloff and Jones, 1999) to re-
lation extraction are attractive because they re-
quire fewer training instances than supervised ap-
proaches. Bootstrapping methods are either ini-
tialized with a few instances (often designated as
seeds) of the target relation (Zhu et al, 2009;
Agichtein and Gravano, 2000) or a few extraction
patterns (Xu et al, 2010). In subsequent itera-
tions, new extraction patterns are discovered, and
these are used to extract new instances. The qual-
ity of the extracted relations depends heavily on
the seeds (Kozareva and Hovy, 2010). Different
from bootstrapping, we not only use labeled tar-
get domain data as seeds, but also leverage on ex-
isting source-domain predictors to obtain a robust
relation extractor for the target domain.
Open Information Extraction (Open IE) (Et-
zioni et al, 2008; Banko et al, 2008; Mesquita
et al, 2013) is a domain-independent informa-
tion extraction paradigm to extract relation tu-
ples from collected corpus (Shinyama and Sekine,
2006) and Web (Etzioni et al, 2008; Banko et al,
2008). Open IE systems are initialized with a few
domain-independent extraction patterns. To cre-
ate labeled data, the texts are dependency-parsed,
and the domain-independent patterns on the parses
form the basis for extractions. Recently, to reduce
808
labeling effort for relation extraction, distant su-
pervision (Mintz et al, 2009; Takamatsu et al,
2012; Min et al, 2013; Xu et al, 2013) has been
proposed. This is an unsupervised approach that
exploits textual features in large unlabeled cor-
pora. In contrast to Open IE, we tune the relation
patterns for a domain of interest, using labeled re-
lation instances in source and target domains and
unlabeled instances in the target domain.
Our work is also different from the multi-
schema matching in database integration (Doan
et al, 2003). Multi-schema matching finds rela-
tions between columns of schemas, which have
the same semantic. In addition, current weighted
schema matching methods do not address negative
transfer and imbalance class distribution.
Domain adaptation methods can be classi-
fied broadly into weakly-supervised adaptation
(Daume and Marcu, 2007; Blitzer et al, 2006;
Jiang and Zhai, 2007a; Jiang, 2009), and unsuper-
vised adaptation (Pan et al, 2010; Blitzer et al,
2006; Plank and Moschitti, 2013). In the weakly-
supervised approach, we have plenty of labeled
data for the source domain and a few labeled in-
stances in the target domain; in the unsupervised
approach, the data for the target domain are not la-
beled. Among these studies, Plank and Moschitti?s
is the closest to ours because it adapts relation
extraction systems to new domains. Most other
works focused on adapting from old to new re-
lation types. Typical relation adaptation methods
first identify a set of common features in source
and target domains and then use those features as
pivots to map source domain features to the target
domain. These methods usually assume that each
source domain is relevant to the task on the target
domain. In addition, these methods do not handle
the imbalanced distribution of relation data explic-
itly. In this work, we study how to learn the target
prediction using only a few seed instances, while
dealing with negative transfer and imbalanced re-
lation distribution explicitly. These issues are sel-
dom explored in relation adaptation.
3 Problem Statement
This section defines the domain adaptation prob-
lem and describes our feature extraction scheme.
3.1 Relation Extraction Domain Adaptation
Given two entities A and B in a sentence S, rela-
tion extraction is the task of selecting the relation
y between A and B from a fixed set of c relation
types, which includes the not-a-relation type. We
introduce a feature extraction ? that maps the triple
(A,B,S) to its feature vector x. Learning relation
extraction can then be abstracted to finding a func-
tion p such that p(?(A,B,S)) = p(x) = y.
For adaptation, we have k source domains and
a target domain. We shall assume that all domains
have the same set of relation types. The target do-
main has a few labeled data D
l
= {(x
i
,y
i
)}
n
l
i=1
and
plenty of unlabeled data D
u
= {(x
i
)}
n
l
+n
u
n
l
+1
, where
n
l
and n
u
are the number of labeled and unla-
beled samples respectively, x
i
is the feature vec-
tor, y
i
is the corresponding label (if available). Let
n = n
l
+ n
u
. For the sth source domain, we have
an adequate labeled data set D
s
. We define domain
adaptation as the problem of learning a classifier
p for relation extraction in the target domain using
the data sets D
l
, D
u
and D
s
, s = 1, . . . ,k.
3.2 Relational Feature Representation
We consider relation extraction as a classifica-
tion problem, where each pair of entities A and
B within a sentence S is a candidate relation in-
stance. The contexts in which entities A and B co-
occur provide useful features to the relations be-
tween them. We use the term context to refer a
window of text in which two entities co-occur. A
context might not necessarily be a complete sen-
tence S. Retrieving contexts in which two entities
co-occur has been studied in previous works in-
vestigating the relations between entities.
Given a pair of entities (A,B) in S, the first step
is to express the relation between A and B with
some feature representation using a feature ex-
traction scheme ?. Lexical or syntactic patterns
have been successfully used in numerous natu-
ral language processing tasks, including relation
extraction. Jiang and Zhai (2007b) have shown
that selected lexical and syntactic patterns can give
good performance for relation extraction. Follow-
ing their work
1
, we also use lexical and syntactic
patterns extracted from the contexts to represent
the relations between entities. We extract features
from a sequence representation and a parse tree
representation of each relation instance. The de-
tails are as follows.
Entity Features Entity types and entity mention
types are very useful for relation extraction. We
1
The source code for extracting entity features is provided
by the authors (Jiang and Zhai, 2007b).
809
use a subgraph in the relation instance graph
(Jiang and Zhai, 2007b) that contains only the
node presenting the head word of the entity A, la-
beled with the entity type or entity mention types,
to describe a single entity attribute.
Sequence Features The sequence representation
preserves the order of the tokens as they occur in
the original sentence. Each node in the graph is a
token augmented with its relevant attributes.
Syntactic Features The syntactic parse tree of
the relation instance sentence can be augmented to
represent the relation instance. Each node is aug-
mented with relevant part-of-speech (POS) using
the Python Natural Language Processing Tool Kit.
Each node in the sequence or the parse tree
is augmented by an argument tag that indicates
whether the node corresponds to entity A, B, both,
or neither. The nodes that represent the argument
are also labeled with the entity type, subtype and
mention type. We trim the parse tree of a relation
instance so that it contains only the most essential
tree components based on constituent dependen-
cies (Qian et al, 2008). We also use unigram fea-
tures and bigram features from a relation instance
graph.
4 Robust Domain Adaptation
In this section, we describe our two-phase ap-
proach, which comprises of a Supervised Voting
scheme and a combined classifier learning phase.
4.1 Phase 1: Clustering Consistency via
Supervised Voting
In this section, we use the concept of clustering
consistency to determine the relevance of a source
domain to particular regions in the target domain.
Figure 1 illustrates this. There, both enclosing cir-
cles in the left and right figures denote the same
input space of the target domain. There are four
disjoint regions within the input space, located at
the left, right, top and bottom of the space. There
are four classes of labels: plus (+), cross (?), cir-
cle (?) and asterisk (?). The labels in the left fig-
ure are given by a preliminary predictor in the tar-
get domain data, while the labels in the right fig-
ure are given by a predictor trained on the source
domain data. Comparing the figures, we see the
preliminary predictor and source domain predic-
tor are consistent for the bottom and right regions,
Target domain input
space with transduc-
tive learning using la-
beled and unlabeled
target domain data.
Target domain input
space with labels from
the predictor trained
on the source domain
data set.
Figure 1: Clustering consistency is used to deter-
mine the relevance of a source domain to a region
in the target domain data. The bottom and right
regions are more relevant than the top and left re-
gions. See text for explanation.
but are inconsistent for the top and left regions.
This suggests that the source domain is very rele-
vant for the bottom and right regions of the target
input space, but less so for the top and left regions.
To apply this idea to relation classification, we
have to (i) partition the target domain input space
into regions and (ii) assign preliminary labels for
all the examples. We approximate the target do-
main input space with all the samples from D
l
and
D
u
. With data from both the labeled and unlabeled
data sets, we apply transductive inference or semi-
supervised learning (Zhou et al, 2003) to achieve
both (i) and (ii). By augmenting with unlabeled
data D
u
, we aim to alleviate the effect of imbal-
anced relation distribution, which causes a lack
of labeled samples for rarer classes in a small set
of labeled data. Briefly, the known labels in D
l
are propagated to the entire target input space by
encouraging label smoothness in neighborhoods.
The next three paragraphs give more details.
At present, we assume a similarity matrix W ,
where W
i j
is the similarity between the ith and the
jth input samples in D
l
?D
u
. Matrix W then de-
termines the neighborhoods. Let ? be a diagonal
matrix where the (i, i)th entry is the sum of the
ith row of W . Let us also encode the the labeled
data D
l
in an n-by-c matrix H, such that H
i j
= 1
if sample i is labeled with relation class j in D
l
,
and H
i j
= 0 otherwise. Our objective is the c-
dimensional relation-class indicator vector F
i
for
the ith sample, for every sample. This is achieved
810
via a regularization framework (Zhou et al, 2003):
min
{F
i
}
n
i=1
(
n
?
i, j=1
W
i j
?
?
?
F
i
?
?
ii
?
F
j
?
?
j j
?
?
?
2
+?
n
?
i=1
?F
i
?H
i
?
2
)
.
This trades off two criteria: the first term encour-
ages nearby samples (under distance metric W ) to
have the same labels, while the second encourages
samples to take their labels from the labeled data.
The closed-form solution is
F
?
= (I? (1+?)
?1
L)
?1
H, (1)
where L = ?
?1/2
W?
?1/2
; and the n-by-c matrix
F
?
is the concatenation of the F
i
s.
Using vector F
?
i
, we now assign preliminary la-
bels to the samples. For a sample i, we transform
F
?
i
into probabilities p
1
i
, p
2
i
, . . . , p
c
i
using softmax.
Our propagated label `
i
for sample i is then
`
i
=
{
not-a-relation if (max
j
p
j
i
)< ?,
argmax
j
p
j
i
otherwise.
(2)
The second clause is self-evident, but the first
needs further explanation. Because not-a-relation
is a background or default relation type in the re-
lation classification task, and because it has rather
high variation when manifested in natural lan-
guage, we have found it difficult to obtain a dis-
tance metric W that allows the not-a-relation sam-
ples to form clusters naturally using transductive
inference. Therefore, we introduce the first clause
to assign the not-a-relation label to a sample when
there is no strong evidence for any of the positive
relation types. The amount of evidence needed is
quantified by the parameter ?> 1/c. In addition,
the second clause will also assign not-a-relation to
a sample if that probability is the highest.
Next, we partition the data in D
l
?D
u
into c re-
gions, R
1
,R
2
, . . . ,R
c
, corresponding to the c rela-
tion types. The intuition is to use the true label in
D
l
when available, or otherwise resort to using the
propagated label. That is,
x
i
?
{
R
y
i
if x
i
? D
l
,
R
`
i
if x
i
? D
u
.
We now have the necessary ingredients to quan-
tify the clustering consistency between a source
Phy Per Emp Agt Aff GPE Dis N/A
BC
BN
NW
CTS
WL
Figure 2: Heat map of the relevance scores w
s, j
between the target domain Usenet (UN) with the
other domains on ACE 2004 data set. A lighter
shade means a higher score, or more relevant. N/A
refers to not-a-relation; for the other abbrevia-
tions, see the second paragraph in section 5.
domain and a region in the target domain. Intu-
itively, this is the agreement between the source-
domain predictor and the preliminary predictor
within the target domain. We use supervised vot-
ing in the following manner. For every source do-
main, say domain s, we first train a relation-type
predictor p
s
based on its training data D
s
. Then,
for every region R
j
, we compute the relevance
score w
s, j
=
?
x
i
?R
j
Jp
s
(x
i
) = `
i
K/|R
j
|, where J?K is
the Iverson bracket.
Figure 2 shows the heat map of the relevance
scores w
s, j
between the target domain Usenet
(UN) with the other domains in the ACE 2004 cor-
pus. We observe, for example, that the Broad-
cast News (BN) domain is more relevant in the
Personal-Social region of the target domain than
the Broadcast Conversation (BC) domain. These
relevance scores will be used in the next phase
of the framework to weigh the contributions of
source-domain predictors to the eventual target-
domain relation classifier.
4.2 Phase 2: Target Classifier Learning
The second phase uses both the weighted predic-
tions from all sources and the target labeled data
D
l
to learn a relation classifier. This ensures that
even when most of the source domains are irrele-
vant, the performance of our method is no worse
than using the target-domain labeled data alone.
The previous phase has computed the relevance
w
s, j
for source domain s in region R
j
. We trans-
late this to the relevance weight u
s,i
for an ex-
ample x
i
: if x
i
? R
j
, then u
s,i
= w
s, j
. At our dis-
posal from the previous phase are also k source-
domain predictors p
s
that have been trained on
D
s
. Combining and weighing the predictions from
multiple sources, we obtain the reference predic-
811
tion r?
ji
=
?
k
s=1
u
s,i
(2Jp
s
(x
i
) = jK?1) for example
x
i
belonging to relation j, using the ?1 encoding.
The relation classifier consists of c functions
f
1
, . . . , f
c
using the one-versus-rest decoding for
multi-class classification.
2
Inspired by the Do-
main Adaptive Machine (Duan et al, 2009), we
combine the reference predictions and the labeled
data of the target domain to learn these functions:
min
{ f
j
}
c
j=1
c
?
j=1
{
1
n
l
n
l
?
i=1
( f
j
(x
i
)? r
ji
)
2
+ ?? f
j
?
2
H
+
?
2
n
?
i=n
l
+1
? f
j
(x
i
)? r?
ji
?
2
}
, (3)
where r
ji
= 2Jy
i
= jK?1 is the ?1 binary encod-
ing for the i labeled sample belonging to relation j.
Here, we have multiple objectives: the first term
controls the training error; the second regularizes
the complexity of the functions f
j
s in the Repro-
ducing Kernel Hilbert Space (RKHS) H ; and the
third prefers the predicted labels of the unlabeled
data D
l
to be close to the reference predictions.
The third term provides additional pseudo-training
samples for the rarer relation classes, if these are
available in D
u
. Parameters ? and ? govern the
trade-offs between these objectives.
Let K(?, ?) be the reproducing kernel for H . By
the Representer Theorem (Smola and Scholkopf,
1998), the solution for Eq. 3 is linear in K(x
i
, ?):
f
j
(x) =
?
n
i=1
?
ji
K(x
i
,x). Putting this into Eq. 3,
parameter vectors ?
j
are (Belkin et al, 2006):
?
?
j
= (JK + ?(n
l
+?n
u
)I)
?1
JR
j
. (4)
Here, R
j
is an (n
l
+ n
u
)-vector, where R
ji
= r
ji
if
sample i belongs to the labeled set, and R
ji
= r?
i j
if
it belongs to the unlabeled set; and J is an (n
l
+
n
u
)-by-(n
l
+n
u
) diagonal matrix where the first n
l
diagonal entries are ones and the rest are ?s.
5 Experiments
We evaluate our algorithm on two corpora: Au-
tomatic Content Extraction (ACE) 2004 and
YAGO
3
. Table 1 provides some statistics on them.
ACE 2004 consists of six domains: Broad-
cast Conversations (BC), Broadcast News
(BN), Conversational Telephone Speech (CTS),
Newswire (NW), Usenet (UN) and Weblog
(WL). There are seven positive relation types:
2
For two-classes, though, only one function is needed.
3
http://www.mpi-inf.mpg.de/yago-naga/yago/
Table 1: Statistics on ACE 2004 and YAGO
Properties ACE 2004 YAGO
# relation types 7 20
# candidate relations 48,625 68,822
# gold relations 4,296 2,000
# mentions per entity pair 6 11
% mentions with +ve relations 8.8% 21%
Physical (Phy), Personal/Social (Per), Employ-
ment/Membership/Subsidiary (Emp), Agent-
Artifact (Agt), PER/ORG Affiliation (Aff), GPE
Affiliation (GPE) and Discourse (Dis).
YAGO is an open information extraction data
set. The relation types of YAGO are built from
Wikipedia and WordNet, while the labeled text for
YAGO is from Bollegala et al (2011). It consists
of twenty relation types such as ceo company,
bornIn and isMarriedTo, and each of them is con-
sidered as a domain in this work. YAGO is dif-
ferent from ACE 2004 in two aspects: there is
less overlapping of topics, entity types and rela-
tion types between domains; and it has more rela-
tion mentions with 11 mentions per pair of entities
on the average.
We used Collins parser (Collins, 1999) to parse
the sentences. The constituent parse trees were
then transformed into dependency parse trees, us-
ing the head of each constituent (Jiang and Zhai,
2007b). The candidate relation instances were
generated by considering all pairs of entities that
occur in the same sentence. For the similarity ma-
trix W in section 4.1 and the kernel K(?, ?) in sec-
tion 4.2, we used the composite kernel function
(Zhang et al, 2006), which is based on structured
features and entity-related features.
F
1
is used to measure the performance of the al-
gorithms. This is the harmonic mean of precision
TP/(TP + FP) and recall TP/(TP + FN), where
TP, FP and FN are the numbers of correct, missing
and wrongly recognized relations.
5.1 Experimental Settings
For ACE 2004, we used each of the six domains
as the target domain and the remaining domains
as source domains. For YAGO, each relation type
in YAGO was considered as a domain. For each
domain in YAGO, we have a binary classifica-
tion task: whether the instance has the relation
corresponding to the domain. Five-fold cross-
validation was used to evaluate the performance.
812
For every target domain, we divided all data into
5 subsets, and we used each subset for testing and
the other four subsets for training. In the training
set, we randomly removed most of the positive in-
stances of the target domain from the training set
except for 10% of the labeled data. This gave us
the weakly-supervised setting. This was repeated
five times with different training and test sets. We
report the average performance over the five runs.
In our experiments, we set ? = 0.8 in Eq. 1;
?= 0.18 in Eq. 2; and ?= 0.1 and ?= 0.3 in Eq. 3.
For each target domain, we used k ? {1,3,5} dif-
ferent source domains chosen randomly from the
remaining domains. Thus, the relevance of the
source domains to the target domain varies from
experiment to experiment.
5.2 Baselines
We compare our framework with several other
methods, including state-of-the-art machine learn-
ing, relation extraction and common domain adap-
tation methods. These are described below.
In-domain multiclass classifier This is Support-
vector-machine (Fan et al, 2008, SVM) using
the one-versus-rest decoding without removing
positive labeled data (Jiang and Zhai, 2007b)
from the target domain. Its performance can be
regarded as an upper bound on the performance
of the cross-domain methods.
No-transfer classifier (NT) We only use the few
labeled instances of the target relation type to-
gether with the negative relation instances to
train a binary classifier.
Alternate no-transfer classifier (NT-U) We use
the union of the k source-domain labeled data
sets D
s
s and the small set of target-domain la-
beled data D
l
to train a binary classifier. It is
then applied directly to predict on the unlabeled
target-domain data D
u
without any adaptation.
Laplacian SVM (L-SVM) This is a semi-
supervised learning method based on label
propagation (Melacci and Belkin, 2011).
Multi-task transfer (MTL) This is a learning
method for weakly-supervised relation extrac-
tion (Jiang, 2009).
Adaptive domain bootstrapping (DAB) This is
an instance-based domain adaptation method
for relation extraction (Xu et al, 2010).
Structural correspondence learning (SCL) We
use the feature-based domain adaptation ap-
proach by Blitzer et al (2007). We apply SCL
on the D
s
s and D
l
to train a model. The learned
model then makes predictions on D
u
.
Domain Adaptation Machine (DAM) We use
the framework of Duan et al (2009), which is a
multiple-sources domain adaptation method.
For the kernel-based methods above, we use the
same composite kernel used in our method. The
source codes of L-SVM, MTL, SCL and DAM
were obtained from the authors. The others were
re-implemented.
5.3 Experimental Results
Tables 2, 3 and 4 present the results on ACE 2004
(corresponding to k = 1,3,5), and Tables 5 present
those on YAGO (corresponding to k = 5).
From Table 3 and Table 5, we see that the
proposed method has the best F
1
among all the
other methods, except for the supervised upper
bound (In-domain). We first notice that NT-U
generally does not perform well, and sometimes
it performs worse than NT. The reason is that
NT-U aims to obtain a consensus among the do-
mains, and this will give a worse label than NT
when there are enough irrelevant sources to influ-
ence the classification decision wrongly. In fact,
one can roughly deduce that a target domain has
few relevant source domains by simply comparing
columns NT with columns NT-U in the tables: a
decrease in F
1
from NT to NT-U suggests that the
source domains are mainly irrelevant. For exam-
ple, for domain BC in ACE 2004, we find that its
F
1
decreases from NT to NT-U consistently in Ta-
bles 2, 3 and 4, which suggests that BN, NW, CTS,
UN and WL are generally irrelevant to it; and sim-
ilarly for domain CTS. We investigate this further
by examining the relevance scores w
s, j
s, and we
find that the decreases in F
1
from NT to NT-U hap-
pen when there are more regions in the target do-
main to which source-domains are irrelevant.
We find that MTL, DAB and SCL are better than
NT-U when the majority of source domains are
relevant. This shows that MTL, DAB and SCL are
able to make more effective use of relevant sources
than NT-U. Howevever, we find that their perfo-
mances are not stable: for example, MTL for tar-
get UN in Table 2. In contrast, we find the per-
formance of L-SVM and DAM to be more sta-
ble. The reason is their reduced vulnerability to
813
Table 2: The F
1
of different methods on ACE 2004 with k = 1 source domain. The best performance for
each target domain is in bold.
Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDA
BC 55.74 30.00 20.31 32.42 32.74 32.12 30.41 33.07 35.43
BN 67.24 33.43 38.31 35.40 44.81 27.32 45.27 43.26 47.28
NW 68.32 41.48 39.35 41.50 42.28 43.27 44.16 41.69 45.41
CTS 72.92 36.60 29.90 36.15 45.06 37.50 44.68 39.40 44.27
UN 45.16 21.67 17.55 25.10 18.69 18.78 28.77 26.57 31.07
WL 46.46 28.53 23.84 29.90 26.13 24.78 23.71 27.01 30.80
Average 57.58 31.95 28.21 33.41 35.02 30.46 29.57 33.50 39.00
Table 3: The F
1
of different methods on ACE 2004 with k = 3 source domains.
Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDA
BC 55.74 30.00 24.55 32.42 35.26 34.12 37.83 36.08 39.43
BN 67.24 33.43 38.31 35.40 49.76 32.15 49.25 45.89 51.28
NW 68.32 41.48 43.35 42.50 43.28 43.71 44.16 44.01 46.41
CTS 72.92 36.60 30.25 36.15 45.06 37.50 44.68 42.51 49.27
UN 45.16 21.67 27.55 25.10 19.72 35.78 31.77 33.29 35.07
WL 46.46 28.53 30.72 30.90 33.21 32.81 26.37 32.46 35.11
Average 57.58 31.95 32.46 34.20 37.72 36.01 39.01 39.10 42.76
Table 4: The F
1
of different methods on ACE 2004 with k = 5 source domains.
Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDA
BC 55.74 30.00 27.32 33.07 37.76 35.08 40.38 38.70 42.90
BN 67.24 33.43 40.83 36.42 52.69 42.76. 50.47 48.23 53.40
NW 68.32 41.48 44.35 43.69 47.80 44.09 45.50 46.06 49.13
CTS 72.92 36.60 34.60 38.90 45.06 38.71 47.35 45.69 52.63
UN 45.16 21.67 29.34 26.34 35.47 35.44 33.21 34.13 36.02
WL 46.46 28.53 32.41 31.56 34.72 32.81 36.89 32.29 37.90
Average 57.58 31.95 34.80 35.0 42.25 38.15 42.30 40.84 45.33
negative transfer from irrelevant sources by rely-
ing on similarity of feature vectors between source
and target domains based on labeled and unlabeled
data. Further improvements can still be made, as
shown by the better performance of RDA over L-
SVM and DAM. This is achieved by further ad-
justing the relevances between source and target
domains according to regions in the target-domain
input space.
We analyzed histogram of the relation types to
order the domains according to the imbalance of
the class distributions. Using this, we observe
that MTL, DAB and SCL perform relatively badly
when the target-domain distribution is more im-
balanced. In constrast, L-SVM, DAM and RDA
are more robust.
Comparing with the baselines, RDA achieves
the best performance on almost all the experi-
ments. Using the two-phase framework, RDA
can successfully transfer useful knowledge even in
the pressence of irrelevant sources and imbalanced
distributions. For ACE 2004, the improvement in
F
1
over the best baseline can be up to 4.0% and
is on average 3.6%. Similarly for YAGO, the im-
provement in F
1
over the best baseline can be up
to 5.5% and is on average 4.3%.
Impact of Number of Source Domains Tables
2, 3, 4 and 6 also demonstrate that RDA improves
monotonically as the number of source domains
increases for both ACE 2004 and YAGO.
814
Table 5: The F
1
of different methods on YAGO with k = 5 source domains.
Target In-domain NT NT-U L-SVM MTL DAB SCL DAM RDA
acquirer acquiree 58.74 32.12 33.19 43.16 45.28 39.08 44.19 45.07 51.15
actedIn 77.36 40.73 44.32 50.45 57.18 49.61 58.23 56.37 63.40
bornIn 68.32 42.39 40.35 44.38 49.80 48.36 50.67 48.12 56.93
ceo company 82.92 47.60 51.27 55.27 61.06 58.33 57.41 59.08 66.71
company headquarters 75.16 48.92 52.15 50.13 59.47 61.23 58.36 56.65 64.36
created 74.26 46.37 43.58 60.45 60.74 55.08 59.42 57.34 65.28
diedIn 81.45 42.78 47.37 57.37 62.69 57.16 65.28 60.44 71.15
directed 70.11 44.42 48.29 50.57 54.29 49.09 52.31 50.30 57.71
discovered 68.13 37.34 42.51 48.77 53.04 49.82 53.73 51.21 59.12
graduatedFrom 69.37 39.28 45.74 51.56 58.22 54.38 56.32 51.17 60.37
hasChild 74.56 49.14 50.98 56.07 64.82 53.41 62.38 61.12 66.83
hasWonPrize 69.41 38.75 45.72 53.47 57.38 52.76 58.29 54.03 63.13
isLeaderOf 79.18 46.31 52.66 58.88 63.49 60.27 63.75 61.51 70.27
isMarriedTo 73.33 47.85 48.16 52.31 56.39 50.73 55.35 52.10 62.58
livesIn 66.93 36.16 35.15 40.28 50.27 41.72 43.59 48.11 56.91
participatedIn 85.38 46.22 48.33 62.48 67.51 61.08 65.38 61.12 71.72
person birthplace 77.62 43.43 45.27 49.66 58.47 59.32 57.55 52.14 65.80
person field 68.32 36.25 37.93 47.69 54.22 50.46 50.47 48.89 59.47
politicianOf 79.10 39.17 42.25 53.38 64.56 62.11 60.74 58.82 68.12
worksAt 84.29 45.78 49.78 59.34 65.33 65.44 66.53 63.24 73.31
Average 74.20 42.55 45.25 52.28 58.21 53.97 56.80 54.84 63.72
Performance Gap From Tables 2 to 4, we ob-
serve that the smallest performance gap between
RDA and the in-domain settings is still high (about
12% with k = 5) on ACE 2004. This is because we
have used a lot less labeled instances in the target
domains: only 10% are used. However, the gaps
reduces when the number of source domains in-
creases. Comparing with the in-domain results in
Table 5 (which is constant with k), Table 6 also
shows a similar trend on YAGO. By exploiting the
labeled data in ten source domains in YAGO, our
RDA algorithm can reduce the gap between the
cross-domain and in-domain settings to 9%.
6 Conclusion and Future Work
In this paper, we have proposed a robust domain
adaptation (RDA) approach for the relation extrac-
tion problem where labeled data is scarce. Ex-
isting domain adaptation approaches suffer from
negative transfer and under imbalanced distribu-
tions. To overcome these, we have proposed a
two-phase approach to transfer only relevant in-
formation from multiple source domains, and thus
derive accurate and robust predictions on the un-
labeled target-domain data. Experimental results
Table 6: Average F
1
of RDA on YAGO
# source domains F
1
k = 1 53.81
k = 3 59.43
k = 5 63.72
k = 10 65.55
on ACE 2004 and YAGO have shown that the our
domain adaptation method achieves the best per-
formance on F
1
measure compared with the other
baselines when only few labeled target instances
are used. Because of the practical importance of
domain adaptation for relation extraction due to
lack of labeled data in new domains, we hope our
study and findings will lead to further investiga-
tions into this problem.
Acknowledgments
This work is supported by DSO grant
DSOCL10021. We thank Jiang for provid-
ing the source code for feature extraction and
Bollegala for sharing his YAGO dataset.
815
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the fifth ACM conference
on Digital libraries, pages 85?94. ACM.
Michele Banko, Oren Etzioni, and Turing Center.
2008. The tradeoffs between open and traditional
relation extraction. Proceedings of ACL-08: HLT,
pages 28?36.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled exam-
ples. The Journal of Machine Learning Research,
7:2399?2434.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, pages 120?128. ACL.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2011. Relation adaptation: learning to
extract novel relations with minimum supervision.
In Proceedings of the Twenty-Second international
joint conference on Artificial Intelligence-Volume
Volume Three, pages 2205?2210. AAAI Press.
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. Advances in
neural information processing systems, 18:171?178.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 423?429. ACL.
Hal Daume and D Marcu. 2007. Frustratingly easy
domain adaptation. In Annual meeting-association
for computational linguistics, pages 256?263.
Anhai Doan, Pedro Domingos, and Alon Halevy.
2003. Learning to match the schemas of data
sources: A multistrategy approach. Machine Learn-
ing, 50(3):279?301.
Lixin Duan, Ivor W Tsang, Dong Xu, and Tat-Seng
Chua. 2009. Domain adaptation from multiple
sources via auxiliary classifiers. In Proceedings of
the 26th Annual ICML, pages 289?296. ACM.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM,
51(12):68?74.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Jing Jiang and ChengXiang Zhai. 2007a. Instance
weighting for domain adaptation in nlp. In An-
nual Meeting-Association For Computational Lin-
guistics, pages 264?271.
Jing Jiang and ChengXiang Zhai. 2007b. A systematic
exploration of the feature space for relation extrac-
tion. In HLT-NAACL, pages 113?120.
Jing Jiang. 2009. Multi-task transfer learning for
weakly-supervised relation extraction. In Proceed-
ings of the 47th Annual Meeting of the ACL: Volume
2-Volume 2, pages 1012?1020.
Zornitsa Kozareva and Eduard Hovy. 2010. Not all
seeds are equal: Measuring the quality of text min-
ing seeds. In HLT: The 2010 Annual Conference of
the North American Chapter of the ACL, pages 618?
626. ACL.
Stefano Melacci and Mikhail Belkin. 2011. Laplacian
support vector machines trained in the primal. Jour-
nal of Machine Learning Research, 12:1149?1184.
Filipe Mesquita, Jordan Schmidek, and Denilson Bar-
bosa. 2013. Effectiveness and efficiency of open
relation extraction. In Proceedings of EMNLP-13,
volume 500, pages 447?457.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of NAACL-HLT, pages 777?
782.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proceedings of the 19th international conference
on World wide web, pages 751?760. ACM.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006. Organizing and
searching the world wide web of facts - step one:
The one-million fact extraction challenge. In Pro-
ceedings of the 21st National Conference on Artifi-
cial Intelligence - Volume 2, AAAI?06, pages 1400?
1405. AAAI Press.
816
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 1498?1507.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaom-
ing Zhu, and Peide Qian. 2008. Exploiting con-
stituent dependencies for tree kernel-based semantic
relation extraction. In Proceedings of the 22nd Con-
ference on Computational Linguistics, pages 697?
704. ACL.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In Proceedings of the National Con-
ference on AI, pages 474?479.
Michael T Rosenstein, Zvika Marx, Leslie Pack Kael-
bling, and Thomas G Dietterich. 2005. To transfer
or not to transfer. In NIPS 2005 Workshop on Trans-
fer Learning, volume 898, pages ?.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the HLT Confer-
ence of the North American Chapter of the ACL,
pages 304?311.
Alex J Smola and Bernhard Scholkopf. 1998. Learn-
ing with kernels. Citeseer.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge unifying wordnet and wikipedia. In Proceed-
ings of the 16th international conference on World
Wide Web, pages 697?706. ACM.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
721?729. Association for Computational Linguis-
tics.
Feiyu Xu, Hans Uszkoreit, Sebastian Krause, and Hong
Li. 2010. Boosting relation extraction with lim-
ited closed-world knowledge. In Proceedings of
the 23rd Conference on Computational Linguistics,
pages 1354?1362. ACL.
Wei Xu, Raphael Hoffmann Le Zhao, and Ralph Grish-
man. 2013. Filling knowledge base gaps for distant
supervision of relation extraction. In Proceedings of
EMNLP-13, pages 665?670.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832. Association for Computa-
tional Linguistics.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Sch?olkopf. 2003.
Learning with local and global consistency. In
NIPS.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and
Ji-Rong Wen. 2009. Statsnowball: a statistical ap-
proach to extracting entity relationships. In Pro-
ceedings of the 18th international conference on
World wide web, pages 101?110. ACM.
817
