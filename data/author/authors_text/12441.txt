Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1021?1029,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Relation Extraction by Mining Wikipedia Texts Using
Information from the Web
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang and Mitsuru Ishizuka
The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
yulan@mi.ci.i.u-tokyo.ac.jp
okazaki@is.s.u-tokyo.ac.jp
matsuo@biz-model.t.utokyo.ac.jp
yangzl@tkl.iis.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Abstract
This paper presents an unsupervised rela-
tion extraction method for discovering and
enhancing relations in which a specified
concept in Wikipedia participates. Using
respective characteristics of Wikipedia ar-
ticles and Web corpus, we develop a clus-
tering approach based on combinations of
patterns: dependency patterns from depen-
dency analysis of texts in Wikipedia, and
surface patterns generated from highly re-
dundant information related to the Web.
Evaluations of the proposed approach on
two different domains demonstrate the su-
periority of the pattern combination over
existing approaches. Fundamentally, our
method demonstrates how deep linguistic
patterns contribute complementarily with
Web surface patterns to the generation of
various relations.
1 Introduction
Machine learning approaches for relation extrac-
tion tasks require substantial human effort, partic-
ularly when applied to the broad range of docu-
ments, entities, and relations existing on the Web.
Even with semi-supervised approaches, which use
a large unlabeled corpus, manual construction of a
small set of seeds known as true instances of the
target entity or relation is susceptible to arbitrary
human decisions. Consequently, a need exists for
development of semantic information-retrieval al-
gorithms that can operate in a manner that is as
unsupervised as possible.
Currently, the leading methods in unsupervised
information extraction collect redundancy infor-
mation from a local corpus or use the Web as a
corpus (Pantel and Pennacchiotti, 2006); (Banko
et al, 2007); (Bollegala et al, 2007): (Fan et
al., 2008); (Davidov and Rappoport, 2008). The
standard process is to scan or search the cor-
pus to collect co-occurrences of word pairs with
strings between them, and then to calculate term
co-occurrence or generate surface patterns. The
method is used widely. However, even when pat-
terns are generated from well-written texts, fre-
quent pattern mining is non-trivial because the
number of unique patterns is loose, but many pat-
terns are non-discriminative and correlated. A
salient challenge and research interest for frequent
pattern mining is abstraction away from different
surface realizations of semantic relations to dis-
cover discriminative patterns efficiently.
Linguistic analysis is another effective tech-
nology for semantic relation extraction, as de-
scribed in many reports such as (Kambhatla,
2004); (Bunescu and Mooney, 2005); (Harabagiu
et al, 2005); (Nguyen et al, 2007). Currently, lin-
guistic approaches for semantic relation extraction
are mostly supervised, relying on pre-specification
of the desired relation or initial seed words or pat-
terns from hand-coding. The common process is
to generate linguistic features based on analyses of
the syntactic features, dependency, or shallow se-
mantic structure of text. Then the system is trained
to identify entity pairs that assume a relation and
to classify them into pre-defined relations. The ad-
vantage of these methods is that they use linguistic
technologies to learn semantic information from
different surface expressions.
As described herein, we consider integrating
linguistic analysis with Web frequency informa-
tion to improve the performance of unsupervised
relation extraction. As (Banko et al, 2007)
reported, ?deep? linguistic technology presents
problems when applied to heterogeneous text on
the Web. Therefore, we do not parse informa-
tion from the Web corpus, but from well written
texts. Particularly, we specifically examine unsu-
pervised relation extraction from existing texts of
Wikipedia articles. Wikipedia resources of a fun-
1021
damental type are of concepts (e.g., represented
by Wikipedia articles as a special case) and their
mutual relations. We propose our method, which
groups concept pairs into several clusters based on
the similarity of their contexts. Contexts are col-
lected as patterns of two kinds: dependency pat-
terns from dependency analysis of sentences in
Wikipedia, and surface patterns generated from
highly redundant information from the Web.
The main contributions of this paper are as fol-
lows:
? Using characteristics of Wikipedia articles
and the Web corpus respectively, our study
yields an example of bridging the gap sep-
arating ?deep? linguistic technology and re-
dundant Web information for Information
Extraction tasks.
? Our experimental results reveal that relations
are extractable with good precision using
linguistic patterns, whereas surface patterns
from Web frequency information contribute
greatly to the coverage of relation extraction.
? The combination of these patterns produces
a clustering method to achieve high pre-
cision for different Information Extraction
applications, especially for bootstrapping a
high-recall semi-supervised relation extrac-
tion system.
2 Related Work
(Hasegawa et al, 2004) introduced a method for
discovering a relation by clustering pairs of co-
occurring entities represented as vectors of con-
text features. They used a simple representation
of contexts; the features were words in sentences
between the entities of the candidate pairs.
(Turney, 2006) presented an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. Given a word pair, the
output list of lexicon-syntactic patterns was ranked
by pertinence, which showed how well each pat-
tern expresses the relations between word pairs.
(Davidov et al, 2007) proposed a method for
unsupervised discovery of concept specific rela-
tions, requiring initial word seeds. That method
used pattern clusters to define general relations,
specific to a given concept. (Davidov and Rap-
poport, 2008) presented an approach to discover
and represent general relations present in an arbi-
trary corpus. That approach incorporated a fully
unsupervised algorithm for pattern cluster discov-
ery, which searches, clusters, and merges high-
frequency patterns around randomly selected con-
cepts.
The field of Unsupervised Relation Identifica-
tion (URI)?the task of automatically discover-
ing interesting relations between entities in large
text corpora?was introduced by (Hasegawa et
al., 2004). Relations are discovered by cluster-
ing pairs of co-occurring entities represented as
vectors of context features. (Rosenfeld and Feld-
man, 2006) showed that the clusters discovered by
URI are useful for seeding a semi-supervised rela-
tion extraction system. To compare different clus-
tering algorithms, feature extraction and selection
method, (Rosenfeld and Feldman, 2007) presented
a URI system that used surface patterns of two
kinds: patterns that test two entities together and
patterns that test either of two entities.
In this paper, we propose an unsupervised rela-
tion extraction method that combines patterns of
two types: surface patterns and dependency pat-
terns. Surface patterns are generated from the Web
corpus to provide redundancy information for re-
lation extraction. In addition, to obtain seman-
tic information for concept pairs, we generate de-
pendency patterns to abstract away from different
surface realizations of semantic relations. Depen-
dency patterns are expected to be more accurate
and less spam-prone than surface patterns from the
Web corpus. Surface patterns from redundancy
Web information are expected to address the data
sparseness problem. Wikipedia is currently widely
used information extraction as a local corpus; the
Web is used as a global corpus.
3 Characteristics of Wikipedia articles
Wikipedia, unlike the whole Web corpus, has
several characteristics that markedly facilitate in-
formation extraction. First, as an earlier report
(Giles, 2005) explained, Wikipedia articles are
much cleaner than typical Web pages. Because
the quality is not so different from standard writ-
ten English, we can use ?deep? linguistic tech-
nologies, such as syntactic or dependency parsing.
Secondly, Wikipedia articles are heavily cross-
linked, in a manner resembling cross-linking of
the Web pages. (Gabrilovich and Markovitch,
2006) assumed that these links encode numerous
interesting relations among concepts, and that they
provide an important source of information in ad-
1022
dition to the article texts.
To establish the background for this paper, we
start by defining the problem under consideration:
relation extraction from Wikipedia. We use the en-
cyclopedic nature of the corpus by specifically ex-
amining the relation extraction between the enti-
tled concept (ec) and a related concept (rc), which
are described in anchor text in this article. A com-
mon assumption is that, when investigating the se-
mantics in articles such as those in Wikipedia (e.g.
semantic Wikipedia (Volkel et al, 2006)), key in-
formation related to a concept described on a page
p lies within the set of links l(p) on that page; par-
ticularly, it is likely that a salient semantic relation
r exists between p and a related page p? ? l(p).
Given the scenario we described along with
earlier related works, the challenges we face are
these: 1) enumerating all potential relation types
of interest for extraction is highly problematic for
corpora as large and varied as Wikipedia; 2) train-
ing data or seed data are difficult to label. Consid-
ering (Davidov and Rappoport, 2008), which de-
scribes work to get the target word and relation
cluster given a single (?hook?) word, their method
depends mainly on frequency information from
the Web to obtain a target and clusters. Attempt-
ing to improve the performance, our solution for
these challenges is to combine frequency informa-
tion from the Web and the ?high quality? charac-
teristic of Wikipedia text.
4 Pattern Combination Method for
Relation Extraction
With the scene and challenges stated, we propose a
solution in the following way. The intuitive idea is
that we integrate linguistic technologies on high-
quality text in Wikipedia and Web mining tech-
nologies on a large-scale Web corpus. In this sec-
tion, we first provide an overview of our method
along with the function of the main modules. Sub-
sequently, we explain each module in the method
in detail.
4.1 Overview of the Method
Given a set of Wikipedia articles as input, our
method outputs a list of concept pairs for each ar-
ticle with a relation label assigned to each concept
pair. Briefly, the proposed approach has four main
modules, as depicted in Fig. 1.
? Text Preprocessor and Concept Pair Col-
lector preprocesses Wikipedia articles to
Wikipedia articles
Preprocessor
Concept pair collection
Sentence filtering
Web context collector
Web Context
T
i
= t1, t2?tn
P
i
=  p1,p2?pn
Dependency 
pattern Extractor
n1i,?n1j
?
ni2i, ..n2j
ni,?nj
?
surface clustering
depend clustering
Relation list
Output: 
relations for each article
input:
Eric Emerson Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
...
...
?
?
?
?
...
...
?
?
?
?
...
...
?
?
?
?
Tyco becoming
joined
comp:
CEO
obj: cc:
joined
obj:subj:
joined
obj: cc:
Clustering approach
Figure 1: Framework of the proposed approach
split text and filter sentences. It outputs con-
cept pairs, each of which has an accompany-
ing sentence.
? Web Context Collector collects context in-
formation from the Web and generates ranked
relational terms and surface patterns for each
concept pair.
? Dependency Pattern Extractor generates
dependency patterns for each concept pair
from corresponding sentences in Wikipedia
articles.
? Clustering Algorithm clusters concept pairs
based on their context. It consists of the two
sub-modules described below.
? Depend Clustering, which merges con-
cept pairs using dependency patterns
alone, aiming at obtaining clusters of
concept pairs with good precision;
? Surface Clustering, which clusters
concept pairs using surface patterns
based on the resultant clusters of depend
clustering. The aim is to merge more
concept pairs into existing clusters with
surface patterns to improve the coverage
of clusters.
1023
4.2 Text Preprocessor and Concept Pair
Collector
This module pre-processes Wikipedia article texts
to collect concept pairs and corresponding sen-
tences. Given a concept described in a Wikipedia
article, our idea of preprocessing executes initial
consideration of all anchor-text concepts linking
to other Wikipedia articles in the article as related
concepts that might share a semantic relation with
the entitled concept. The link structure, more par-
ticularly, the structure of outgoing links, provides
a simple mechanism for identifying relevant arti-
cles. We split text into sentences and select sen-
tences containing one reference of an entitled con-
cept and one of the linked texts for the dependency
pattern extractor module.
4.3 Web Context Collector
Querying a concept pair using a search engine
(Google), we characterize the semantic relation
between the pair by leveraging the vast size of the
Web. Our hypothesis is that there exist some key
terms and patterns that provide clues to the rela-
tions between pairs. From the snippets retrieved
by the search engine, we extract relational infor-
mation of two kinds: ranked relational terms as
keywords and surface patterns. Here surface pat-
terns are generated with support of ranked rela-
tional terms.
4.3.1 Relational Term Ranking
To collect relational terms as indicators for each
concept pair, we look for verbs and nouns from
qualified sentences in the snippets instead of sim-
ply finding verbs. Using only verbs as relational
terms might engender the loss of various important
relations, e.g. noun relations ?CEO?, ?founder?
between a person and a company. Therefore, for
each concept pair, a list of relational terms is col-
lected. Then all the collected terms of all concept
pairs are combined and ranked using an entropy-
based algorithm which is described in (Chen et al,
2005). With their algorithm, the importance of
terms can be assessed using the entropy criterion,
which is based on the assumption that a term is ir-
relevant if its presence obscures the separability of
the dataset. After the ranking, we obtain a global
ranked list of relational terms Tall for the whole
dataset (all the concept pairs). For each concept
pair, a local list of relational terms Tcp is sorted ac-
cording to the terms? order in Tall. Then from the
relational term list Tcp, a keyword tcp is selected
Table 1: Surface patterns for a concept pair
Pattern Pattern
ec ceo rc rc found ec
ceo rc found ec rc succeed as ceo of ec
rc be ceo of ec ec ceo of rc
ec assign rc as ceo ec found by ceo rc
ceo of ec rc ec found in by rc
for each concept pair cp as the first term appearing
in the term list Tcp. Keyword tcp will be used to
initialize the clustering algorithm in Section 4.5.1.
4.3.2 Surface Pattern Generation
Because simply taking the entire string between
two concept words captures an excess of extra-
neous and incoherent information, we use Tcp of
each concept pair as a key for surface pattern gen-
eration. We classified words into Content Words
(CWs) and Functional Words (FWs). From each
snippet sentence, the entitled concept, related con-
cept, or the keyword kcp is considered to be a Con-
tent Word (CW). Our idea of obtaining FWs is to
look for verbs, nouns, prepositions, and coordinat-
ing conjunctions that can help make explicit the
hidden relations between the target nouns.
Surface patterns have the following general
form.
[CW1] Infix1 [CW2] Infix2 [CW3] (1)
Therein, Infix1 and Infix2 respectively con-
tain only and any number of FWs. A pattern ex-
ample is ?ec assign rc as ceo (keyword)?. All gen-
erated patterns are sorted by their frequency, and
all occurrences of the entitled concept and related
concept are replaced with ?ec? and ?rc?, respec-
tively for pattern matching of different concept
pairs.
Table 1 presents examples of surface patterns
for a sample concept pair. Pattern windows are
bounded by CWs to obtain patterns more precisely
because 1) if we use only the string between two
concepts, it may not contain some important re-
lational information, such as ?ceo ec resign rc?
in Table 1; 2) if we generate patterns by setting
a windows surrounding two concepts, the number
of unique patterns is often exponential.
4.4 Dependency Pattern Extractor
In this section, we describe how to obtain depen-
dency patterns for relation clustering. After pre-
processing, selected sentences that contain at least
1024
one mention of an entitled concept or related con-
cept are parsed into dependency structures. We de-
fine dependency patterns as sub-paths of the short-
est dependency path between a concept pair for
two reasons. One is that the shortest path de-
pendency kernels outperform dependency tree ker-
nels by offering a highly condensed representation
of the information needed to assess their relation
(Bunescu and Mooney, 2005). The other reason
is that embedded structures of the linguistic repre-
sentation are important for obtaining good cover-
age of the pattern acquisition, as explained in (Cu-
lotta and Sorensen, 2005); (Zhang et al, 2006).
The process of inducing dependency patterns has
two steps.
1. Shortest dependency path inducement. From
the original dependency tree structure by parsing
the selected sentence for each concept pair, we
first induce the shortest dependency path with the
entitled concept and related concept.
2. Dependency pattern generation. We use
a frequent tree-mining algorithm (Zaki, 2002) to
generate sub-paths as dependency patterns from
the shortest dependency path for relation cluster-
ing.
4.5 Clustering Algorithm for Relation
Extraction
In this subsection, we present a clustering algo-
rithm that merges concept pairs based on depen-
dency patterns and surface patterns. The algorithm
is based on k-means clustering for relation cluster-
ing.
The dependency pattern has the properties of
being more accurate, but the Web context has the
advantage of containing much more redundant in-
formation than Wikipedia. Our idea of concept
pair clustering is a two-step clustering process:
first it clusters concept pairs into clusters with
good precision using dependency patterns; then it
improves the coverage of the clusters using surface
patterns.
4.5.1 Initial Centroid Selection and Distance
Function Definition
The standard k-means algorithm is affected by
the choice of seeds and the number of clusters
k. However, as we claimed in the Introduc-
tion section, because we aim to extract relations
from Wikipedia articles in an unsupervised man-
ner, cluster number k is unknown and no good
centroids can be predicted. As described in this
paper, we select centroids based on the keyword
tcp of each concept pair.
First of all, all concept pairs are grouped by
their keywords tcp. Let G = {G1, G2, ...Gn}
be the resultant groups, where each Gi =
{cpi1, cpi2, ...} identify a group of concept pairs
sharing the same keyword tcp (such as ?CEO?).
We rank all the groups by their number of concept
pairs and then choose the top k groups. Then a
centroid ci is selected for each group Gi by Eq. 2.
ci = argmaxcp?Gi |{cpij |(dis1(cpij , cp)+
? ? dis2(cpij , cp)) <= Dz, 1 ? j ? |Gi|}| (2)
We assume a centroid for each group to be the
concept pair which has the most other concept
pairs in the same group that have distance less
than Dz with it. Also, Dz is a threshold to avoid
noisy concept pairs: we assign it 1/3. To balance
the contribution between dependency patterns and
surface patterns, ? is used. The distance function
to calculate the distance between dependency pat-
tern sets DPi, DPj of two concept pairs cpi and
cpj is dis1. The distance is decided by the number
of overlapped dependency patterns with Eq. 3.
dis1(cpi, cpj) = 1? |DPi ?DPj |?(|DPi| ? |DPj |)
(3)
Actually, dis2 is the distance function to calcu-
late distance between two surface pattern sets of
two concept pairs. To compute the distance over
surface patterns, we implement the distance func-
tion dis2(cpi, cpj) in Fig. 2.
Algorithm 1: distance function dis2(cpi, cpj)
Input: SP1 = {sp11, ..., sp1m}(surface patterns of
cpi)
SP2 = {sp21, ..., sp2n} (surface patterns of cpj)
Output: dis (distance between SP1 and SP2)
define a m? n distance matrix A:
{Aij = LD(sp1i,sp2j)Max(|sp1i|,|sp2j |) , 1?i?m; 1?j?n};
dis ? 0
for min(m,n) times do
(x, y) ? argmin0<i<m;0<j<nAij ;
dis ? dis + Axy/min(m,n);
Ax? ? 1; A?y ? 1;
return dis
Figure 2: Distance function over surface patterns
As shown in Fig. 2, the distance algorithm per-
forms as: firstly it defines a m?n distance matrix
A, then repeatedly selects two nearest sequences
and sums up their distances. While computing
1025
dis2, we use the Levenshtein distance LD to mea-
sure the difference of two surface patterns. The
Levenshtein distance is a metric for measuring the
amount of difference between two sequences (i.e.,
the so-called edit distance). Each generated sur-
face pattern is a sequence of words. The distance
of two surface patterns is defined as the fraction of
the LD value to the length of the longer sequence.
For estimating the number of clusters k, we ap-
ply the stability-based criteria from (Chen et al,
2005) to decide the number of optimal clusters k
automatically.
4.5.2 Concept Pair Clustering with
Dependency Patterns
Given the initial seed concept pairs and cluster
number k, this stage merges concept pairs over de-
pendency patterns into k clusters. Each concept
pair cpi has a set of dependency patterns DPi. We
calculate distances between two pairs cpi and cpj
using above the function dis1(cpi, cpj). The clus-
tering algorithm is portrayed in Fig. 3. The pro-
cess of depend clustering is to assign each concept
pair to the cluster with the closest centroid and
then recomputing each centroid based on the cur-
rent members of its cluster. As shown in Figure 3,
this is done iteratively by repeating both two steps
until a stopping criterion is met. We apply the ter-
mination condition as: centroids do not change be-
tween iterations.
Algorithm 2: Depend Clustering
Input: I = {cp1, ..., cpn}(all concept pairs)
C = {c1, ..., ck} (k initial centroids)
Output: Md : I ? C (cluster membership)
Ir (rest of concept pairs not clustered)
Cd = {c1, ..., ck} (recomputed centroids)
while stopping criterion has not been met do
for each cpi ? I do
if mins?1..k dis1(cpi, cs) <= Dl then
Md(cpi) ? argmins?1..k dis1(cpi, cs)else
Md(cpi) ? 0
for each j ? {1..k} do
recompute cj as the centroid of
{cpi|mloc(cpi) = j}
Ir ? C0
return C and Cd
Figure 3: Clustering with dependency patterns
Because many concept pairs are scattered and
do not belong to any of the top k clusters, we
filter concept pairs with distance larger than Dl
with the seed concept pairs. Such concept pairs
ST1
ST3 ST4
ST2
Text3: RC was hired as EC?s CEO Text4: EC assign RC as CEO
Text1: the CEO of EC is RC Text2: RC is the CEO of EC
Figure 4: Example showing why surface cluster-
ing is needed
are stored in C0. We named the cluster of concept
pairs Ir which are left to be clustered in the next
step of clustering. After this step, concept pairs
with similar dependency patterns are merged into
same clusters, see Fig. 4 (ST1, ST2).
4.5.3 Concept Pair Clustering with Surface
Patterns
A salient difficulty posed by dependency pattern
clustering is that concept pairs of the same se-
mantic relation cannot be merged if they are ex-
pressed in different dependency structures. Fig-
ure 4 presents an example demonstrating why we
perform surface pattern clustering. As depicted
in Fig. 4, ST1, ST2, ST3, and ST4 are depen-
dency structures for four concept pairs that should
be classified as the same relation ?CEO?. However
ST3 and ST4 can not be merged with ST1 and
ST2 using the dependency patterns because their
dependency structures are too diverse to share suf-
ficient dependency patterns.
In this step, we use surface patterns to merge
more concept pairs for each cluster to improve the
coverage. Figure 5 portrays the algorithm. We
assume that each concept pair has a set of sur-
face patterns from the Web context collector mod-
ule. As shown in Figure 5, surface clustering is
done iteratively by repeating two steps until a stop-
ping criterion is met: using the distance function
dis2 explained in the preceding section, assign
each concept pair to the cluster with the closest
centroid and recomputing each centroid based on
the current members of its cluster. We apply the
same termination condition as depend clustering.
1026
Additionally, we filter concept pairs with distance
greater than Dg with the centroid concept pairs.
Algorithm 3: Surface Clustering
Input: Ir (rest of concept pairs)
Cd = {c1, ..., ck} (initial centroids)
Output: Ms : Ir ? C (cluster membership)
Cs = {c1, ..., ck} (final centroids)
while stopping criterion has not been met do
for each cpi ? Ir do
if mins?1..k dis2(cpi, cs) <= Dg then
Ms(cpi) ? argmins?1..k dis2(cpi, cs)else
Ms(cpi) ? 0
for each j ? 1..k do
recompute cj as the centroid of cluster
{cpi|Md(cpi) = j ?Ms(cpi) = j}
return clusters C
Figure 5: Clustering with surface patterns
Finally we have k clusters of concept pairs, each
of which has a centroid concept pair. To attach
a single relation label to each cluster, we use the
centroid concept pair.
5 Experiments
We apply our algorithm to two categories in
Wikipedia: ?American chief executives? and
?Companies?. Both categories are well defined
and closed. We conduct experiments for extract-
ing various relations and for measuring the quality
of these relations in terms of precision and cover-
age. We use coverage as an evaluation instead of
using recall as a measure. The coverage is used to
evaluate all correctly extracted concept pairs. It is
defined as the fraction of all the correctly extracted
concept pairs to the whole set of concept pairs. To
balance between precision and coverage of clus-
tering, we integrate two parameters: Dl, Dg.
We downloaded the Wikipedia dump as of De-
cember 3, 2008. The performance of the pro-
posed method is evaluated using different pattern
types: dependency patterns, surface patterns, and
their combination. We compare our method with
(Rosenfeld and Feldman, 2007)?s URI method.
Their algorithm outperformed that presented in the
earlier work using surface features of two kinds for
unsupervised relation extraction: features that test
two entities together and features that test only one
entity each. For comparison, we use a k-means
clustering algorithm using the same cluster num-
ber k.
Table 2: Results for the category: ?American chief
executives?
method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
chairman 434 63.52 547 68.37
(x be chairman of y)
ceo 396 73.74 423 77.54
(x be ceo of y)
bear 138 83.33 276 86.96
(x be bear in y)
attend 225 67.11 313 70.28
(x attend y)
member 14 85.71 175 91.43
(x be member of y)
receive 97 67.97 117 73.53
(x receive y)
graduate 18 83.33 92 88.04
(x graduate from y)
degree 5 80.00 78 82.05
(x obtain y degree)
marry 55 41.67 74 61.25
(x marry y)
earn 23 86.96 51 88.24
(x earn y)
award 23 43.47 46 84.78
(x won y award)
hold 5 80.00 37 72.97
(x hold y degree)
become 35 74.29 37 81.08
(x become y)
director 24 67.35 29 79.31
(x be director of y)
die 18 77.78 19 84.21
(x die in y)
all 1510 68.27 2314 75.63
5.1 Wikipedia Category: ?American chief
executives?
We choose appropriate Dl(concept pair filter in
depend clustering) and Dg(concept pair filter in
surface clustering) in a development set. To bal-
ance precision and coverage, we set 1/3 for both
Dl and Dg.
The 526 articles in this category are used for
evaluation. We obtain 7310 concept pairs from
the articles as our dataset. The top 18 groups are
chosen to obtain the centroid concept pairs. Of
these, 15 binary relations are the clearly identifi-
able relations shown in Table 2, where # Ins. rep-
resents the number of concept pairs clustered us-
ing each method, and pre denotes the precision of
each cluster.
The proposed approach shows higher precision
and better coverage than URI in Table 2. This
result demonstrates that adding dependency pat-
terns from linguistic analysis contributes more to
the precision and coverage of the clustering task
than the sole use of surface patterns.
1027
Table 3: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 1127 84.29 13.00%
surface 1510 68.27 14.10%
Combined 2314 75.63 23.94%
Table 4: Results for the category: ?Companies?
Method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
found 82 75.61 163 84.05
(found x in y)
base 82 76.83 122 82.79
(x be base in y)
headquarter 23 86.97 120 89.34
(x be headquarter in y)
service 37 51.35 108 69.44
(x offer y service)
store 113 77.88 88 72.72
(x open store in y)
acquire 59 62.71 70 64.28
(x acquire y)
list 51 64.71 67 70.15
(x list on y)
product 25 76.00 57 77.19
(x produce y)
CEO 37 64.86 39 66.67
(ceo x found y)
buy 53 62.26 37 56.76
(x buy y)
establish 35 82.86 26 80.77
(x be establish in y)
locate 14 50.00 24 75.00
(x be locate in y)
all 685 71.03 1039 76.87
To examine the contribution of dependency pat-
terns, we compare results obtained with patterns
of different kinds. Table 3 shows the precision and
coverage scores. The best precision is achieved by
dependency patterns. The precision is markedly
better than that of surface patterns. However, the
coverage is worse than that by surface patterns. As
we reported, many concept pairs are scattered and
do not belong to any of the top k clusters, the cov-
erage is low.
5.2 Wikipedia Category: ?Companies?
We also evaluate the performance for the ?Com-
panies? category. Instead of using all the arti-
cles, we randomly select 434 articles for evalua-
tion and 4073 concept pairs from the articles form
our dataset for this category. We also set Dl and
Dg to 1/3. Then 28 groups are chosen. For each
group, a centroid concept pair is obtained. Finally,
of 28 clusters, 25 binary relations are clearly iden-
tifiable relations. Table 4 presents some relations.
Table 5: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 551 82.58 11.17%
surface 685 71.03 11.95%
Combined 1039 76.87 19.61%
Our clustering algorithms use two filters Dl and
Dg to filter scattering concept pairs. In Table 4, we
present that concept pairs are clustered with good
precision. As in the first experiments, the combi-
nation of dependency patterns and surface patterns
contribute greatly to the precision and coverage.
Table 5 shows that, using dependency patterns,
the precision is the highest (82.58%), although the
coverage is the lowest.
All experimental results support our idea
mainly in two aspects: 1) Dependency analysis
can abstract away from different surface realiza-
tions of text. In addition, embedded structures of
the dependency representation are important for
obtaining a good coverage of the pattern acqui-
sition. Furthermore, the precision is better than
that of the string surface patterns from Web pages
of various kinds. 2) Surface patterns are used to
merge concept pairs with relations represented in
different dependency structures with redundancy
information from the vast size of Web pages. Us-
ing surface patterns, more concept pairs are clus-
tered, and the coverage is improved.
6 Conclusions
To discover a range of semantic relations from
a large corpus, we present an unsupervised rela-
tion extraction method using deep linguistic in-
formation to alleviate surface and noisy surface
patterns generated from a large corpus, and use
Web frequency information to ease the sparse-
ness of linguistic information. We specifically ex-
amine texts from Wikipedia articles. Relations
are gathered in an unsupervised way over pat-
terns of two types: dependency patterns by parsing
sentences in Wikipedia articles using a linguistic
parser, and surface patterns from redundancy in-
formation from the Web corpus using a search en-
gine. We report our experimental results in com-
parison to those of previous works. The results
show that the best performance arises from a com-
bination of dependency patterns and surface pat-
terns.
1028
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead and Oren Etzioni. 2007.
Open information extraction from the Web. In Pro-
ceedings of IJCAI-2007.
Danushka Bollegala, Yutaka Matsuo and Mitsuru
Ishizuka. 2007. Measuring Semantic Similarity be-
tween Words Using Web Search Engines. In Pro-
ceedings of WWW-2007.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT/EMLNP-2005.
Jinxiu Chen, Donghong Ji, Chew Lim Tan and
Zhengyu Niu. 2005. Unsupervised Feature Se-
lection for Relation Extraction. In Proceedings of
IJCNLP-2005.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL-2004.
Dmitry Davidov, Ari Rappoport and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by Web mining. In Proceed-
ings of ACL-2007.
Dmitry Davidov and Ari Rappoport. 2008. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. In Proceedings of ACL-
2008.
Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng
Yan, Jiawei Han, Philip S. Yu and Olivier Ver-
scheure. 2008. Direct Mining of Discriminative and
Essential Frequent Patterns via Model-based Search
Tree. In Proceedings of KDD-2008.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proceedings of
AAAI-2006.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature 438:900C901.
Sanda Harabagiu, Cosmin Adrian Bejan and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In Proceedings of IJCAI-2005.
Takaaki Hasegawa, Satoshi Sekine and Ralph Grish-
man. 2004. Discovering Relations among Named
Entities from Large Corpora. In Proceedings of
ACL-2004.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els. In Proceedings of ACL-2004.
Dat P.T. Nguyen, Yutaka Matsuo and Mitsuru Ishizuka.
2007. Relation extraction from Wikipedia using sub-
tree mining. In Proceedings of AAAI-2007.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceedings
of ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2006.
URES: an Unsupervised Web Relation Extraction
System. In Proceedings of COLING/ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In
Proceedings of CIKM-2007.
Peter D. Turney. 2006. Expressing implicit seman-
tic relations without supervision. In Proceedings of
ACL-2006.
Max Volkel, Markus Krotzsch, Denny Vrandecic,
Heiko Haller and Rudi Studer. 2006. Semantic
wikipedia. In Proceedings of WWW-2006.
Mohammed J. Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of SIGKDD-2002.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of ACL-2006.
1029
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 825?835,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relation Acquisition using Word Classes and Partial Patterns
Stijn De Saeger?? Kentaro Torisawa? Masaaki Tsuchida? Jun?ichi Kazama?
Chikara Hashimoto? Ichiro Yamada? Jong Hoon Oh? Istva?n Varga? Yulan Yan?
? Information Analysis Laboratory, National Institute of
Information and Communications Technology, 619-0289 Kyoto, Japan
{stijn,torisawa,kazama,ch,rovellia,istvan,yulan}@nict.go.jp
? Information and Media Processing Laboratories, NEC Corporation, 630-0101 Nara, Japan
m-tsuchida@cq.jp.nec.com
? Human & Information Science Research Division,
NHK Science & Technology Research Laboratories, 157-8510 Tokyo, Japan
yamada.i-hy@nhk.or.jp
Abstract
This paper proposes a semi-supervised rela-
tion acquisition method that does not rely on
extraction patterns (e.g. ?X causes Y? for
causal relations) but instead learns a combi-
nation of indirect evidence for the target re-
lation ? semantic word classes and partial
patterns. This method can extract long tail
instances of semantic relations like causality
from rare and complex expressions in a large
JapaneseWeb corpus? in extreme cases, pat-
terns that occur only once in the entire cor-
pus. Such patterns are beyond the reach of cur-
rent pattern based methods. We show that our
method performs on par with state-of-the-art
pattern based methods, and maintains a rea-
sonable level of accuracy even for instances
acquired from infrequent patterns. This abil-
ity to acquire long tail instances is crucial for
risk management and innovation, where an ex-
haustive database of high-level semantic rela-
tions like causation is of vital importance.
1 Introduction
Pattern based relation acquisition methods rely on
lexico-syntactic patterns (Hearst, 1992) for extract-
ing relation instances. These are templates of natu-
ral language expressions such as ?X causes Y ? that
signal an instance of some semantic relation (i.e.,
causality). Pattern based methods (Agichtein and
Gravano, 2000; Pantel and Pennacchiotti, 2006b;
Pas?ca et al, 2006; De Saeger et al, 2009) learn many
? This work was done when all authors were at the National
Institute of Information and Communications Technology.
such patterns to extract new instances (word pairs)
from the corpus.
However, since extraction patterns are learned us-
ing statistical methods that require a certain fre-
quency of observations, pattern based methods fail
to capture relations from complex expressions in
which the pattern connecting the two words is rarely
observed. Consider the following sentence:
?Curing hypertension alleviates the deteriora-
tion speed of the renal function, thereby lower-
ing the risk of causing intracranial bleeding?
Humans can infer that this sentence expresses a
causal relation between the underlined noun phrases.
But the actual pattern connecting them, i.e., ?Cur-
ing X alleviates the deterioration speed of the re-
nal function, thereby lowering the risk of causing
Y ?, is rarely observed more than once even in a 108
page Web corpus. In the sense that the term pat-
tern implies a recurring event, this expression con-
tains no pattern for detecting the causal relation be-
tween hypertension and intracranial bleeding. This
is what we mean by ?long tail instances? ? words
that co-occur infrequently, and only in sparse extrac-
tion contexts.
Yet an important application of relation extraction
is mining the Web for so-called unknown unknowns
? in the words of D. Rumsfeld, ?things we don?t
know we don?t know? (Torisawa et al, 2010). In
knowledge discovery applications like risk manage-
ment and innovation, the usefulness of relation ex-
traction lies in its ability to find many unexpected
remedies for diseases, causes of social problems,
and so on. To give an example, our relation extrac-
825
tion system found a blog post mentioning Japanese
automaker Toyota as a hidden cause of Japan?s de-
flation. Several months later the same connection
was made in an article published in an authoritative
economic magazine.
We propose a semi-supervised relation extraction
method that does not rely on direct pattern evidence
connecting the two words in a sentence. We argue
that the role of binary patterns can be replaced by a
combination of two types of indirect evidence: se-
mantic class information about the target relation
and partial patterns, which are fragments or sub-
patterns of binary patterns. The intuition is this: if
a sentence like the example sentence above contains
some wordX belonging to the class of medical con-
ditions and another word Y from the class of trau-
mas, and X matches the partial pattern ?. . . causing
X?, there is a decent chance that this sentence ex-
presses a causal relation between X and Y . We
show that just using this combination of indirect
evidence we can pick up semantic relations with
roughly 50% precision, regardless of the complexity
or frequency of the expression in which the words
co-occur. Furthermore, by combining this idea with
a straightforward machine learning approach, the
overall performance of our method is on par with
state-of-the-art pattern based methods. However,
our method manages to extract a large number of
instances from sentences that contain no pattern that
can be learned by pattern induction methods.
Our method is a two-stage system. Figure 1
presents an overview. In Stage 1 we apply a state-
of-the-art pattern based relation extractor to a Web
corpus to obtain an initial batch of relation instances.
In Stage 2 a supervised classifier is built from vari-
ous components obtained from the output of Stage
1. Given the output of Stage 1 and access to a
Web corpus, the Stage 2 extractor is completely
self-sufficient, and the whole method requires no
supervision other than a handful of seed patterns
to start the first stage extractor. The whole proce-
dure is therefore minimally supervised. Semantic
word classes and partial patterns play a crucial role
throughout all steps of the process.
We evaluate our method on three relation acqui-
sition tasks (causation, prevention and material re-
lations) using a 600 million Japanese Web page cor-
Figure 1: Proposed method: data flow.
pus (Shinzato et al, 2008) and show that our sys-
tem can successfully acquire relations from both
frequent and infrequent patterns. Our system ex-
tracted 100,000 causal relations with 84.6% preci-
sion, 50,000 prevention relations with 58.4% preci-
sion and 25,000 material relations with 76.1% preci-
sion. In the extreme case, we acquired several thou-
sand word pairs co-occurring only in patterns that
appear once in the entire corpus. We call such pat-
terns single occurrence (SO) patterns. Word pairs
that co-occur only with SO patterns represent the
theoretical limiting case of relations that cannot be
acquired using existing pattern based methods. In
this sense our method can be seen as complemen-
tary with pattern based approaches, and merging our
method?s output with that of a pattern based method
may be beneficial.
2 Stage 1 Extractor
This section introduces our Stage 1 extractor: the
pattern based method from (De Saeger et al, 2009),
which we call CDP for ?class dependent patterns?.
We give a brief overview below, and refer the reader
to the original paper for a more comprehensive ex-
planation.
CDP takes a set of seed patterns as input, and au-
tomatically learns new class dependent patterns as
paraphrases of the seed patterns. Class dependent
patterns are semantic class restricted versions of or-
dinary lexico-syntactic patterns. Existing methods
use class independent patterns such as ?X causes
Y ? to learn causal relations betweenX and Y . Class
dependent patterns however place semantic class re-
826
strictions on the noun pairs they may extract, like
?Yaccidents causes Xincidents?. The accidents and
incidents subscripts specify the semantic class of the
X and Y slot fillers.
These class restrictions make it possible to distin-
guish between multiple senses of highly ambiguous
patterns (so-called ?generic? patterns). For instance,
given the generic pattern ?Y by X?, if we restrict
Y and X in to the semantic classes of injuries and
accidents (as in ?death by drowning?), the class de-
pendent pattern ?Yinjuries by Xaccidents? becomes a
valid paraphrase of ?X causes Y ? and can safely be
used to extract causal relations, whereas other class
dependent versions of the same generic pattern (e.g.,
?Yproducts byXcompanies?, as in ?iPhone by Apple?)
may not.
CDP ranks each noun pair in the corpus accord-
ing to a score that reflects its likelihood of being
a proper instance of the target relation, by calcu-
lating the semantic similarity of a set of seed pat-
terns to the class dependent patterns this noun pair
co-occurs with. The output of CDP is a list of noun
pairs ranked by score, together with the highest scor-
ing class dependent pattern each noun pair co-occurs
with. This list becomes the input to Stage 2 of our
method, as shown in Figure 1. We adopted CDP as
Stage 1 extractor because, besides having generally
good performance, the class dependent patterns pro-
vide the two fundamental ingredients for Stage 2 of
our method ? the target semantic word classes for a
given relation (in the form of the semantic class re-
strictions attached to patterns), and partial patterns.
To obtain fine-grained semantic word classes we
used the large scale word clustering algorithm from
(Kazama and Torisawa, 2008), which uses the EM
algorithm to compute the probability that a word w
belongs to class c, i.e., P (c|w). Probabilistic cluster-
ing defines no discrete boundary between members
and non-members of a semantic class, so we simply
assume w belongs to c whenever P (c|w) ? 0.2. For
this work we clustered 106 nouns into 500 classes.
Finally, we adopt the structural representation of
patterns introduced in (Lin and Pantel, 2001). All
sentences in our corpus are dependency parsed, and
patterns consist of words on the path of dependency
relations connecting two nouns.
3 Stage 2 Extractor
We use CDP as our Stage 1 extractor, and the top
N noun pairs along with the class dependent pat-
terns that extract them are given as input to Stage 2,
which represents the main contribution of this work.
As shown in Figure 1, Stage 2 consists of three mod-
ules: a candidate generator, a training data gener-
ator and a supervised classifier. The training data
generator builds training data for the classifier from
the top N output of CDP and sentences retrieved
from the Web corpus. This classifier then scores and
ranks the candidate relations generated by the can-
didate relation generator. We introduce each module
below.
Candidate Generator This module generates
sentences containing candidate word pairs for the
target relation from the corpus. It does so using the
semantic class restrictions and partial patterns ob-
tained from the output of CDP. The set of all seman-
tic class pairs obtained from the class dependent pat-
terns that extracted the topN results become the tar-
get semantic class pairs from which new candidate
instances are generated. We extract all sentences
containing a word pair belonging to one of the target
class pairs from the corpus.
From these sentences we keep only those that con-
tain a trace of evidence for the target semantic re-
lation. For this we decompose the class dependent
patterns from the Stage 1 extractor into partial pat-
terns. As mentioned previously, patterns consist of
words on the path of dependency relations connect-
ing the two target words in a syntactic tree. To obtain
partial patterns we split this dependency path into its
two constituent branches, each one leading from the
leaf word (i.e. variable) to the syntactic head of the
pattern. For example, ?X subj?? causes obj?? Y ? is
split into two partial patterns ?X subj?? causes? and
?causes obj?? Y ?. These partial patterns capture the
predicate structures in binary patterns.1 We discard
partial patterns with syntactic heads other than verbs
or adjectives.
The candidate genarator retrieves all sentences
from the corpus in which two nouns belonging to
one of the target semantic classes co-occur and
1 In Japanese, case information is encoded in post-positions
attached to the noun.
827
where at least one of the nouns matches a partial pat-
tern. As shown in Figure 1, these sentences and the
candidate noun pairs they contain (called (noun pair,
sentence) triples hereafter) are submitted to the clas-
sifier for scoring. Restricting candidate noun pairs
by this combination of semantic word classes and
partial pattern matching proved to be quite powerful.
For instance, in the case of causal relations we found
that close to 60% of the (noun pair, sentence) triples
produced by the candidate generator were correct
(Figure 6).
Training Data Generator As shown in Figure 1,
the (noun pair, sentence) triples used as training data
for the SVM classifier were generated from the top
results of the Stage 1 extractor and the corpus. We
consider the noun pairs in the top N output of the
Stage 1 extractor as true instances of the target re-
lation (even though they may contain erroneous ex-
tractions), and retrieve from the corpus all sentences
in which these noun pairs co-occur and that match
one of the partial patterns mentioned above. In our
experiments we set N to 25, 000. We randomly se-
lect positive training samples from this set of (noun
pair, sentence) triples.
Negative training samples are also selected ran-
domly, as follows. If one member of the target noun
pair in the positive samples above matches a partial
pattern but the other does not, we randomly replace
the latter by another noun found in the same sen-
tence, and generate this new (noun pair, sentence)
triple as a negative training sample. In the causal
relation experiments this approach had about 5%
chance of generating false negatives ? noun pairs
contained in the top N results of the Stage 1 extrac-
tor. Such samples were discarded. Our experimen-
tal results show that this scheme works quite well in
practice. We randomly sample M positive and neg-
ative samples from the autogenerated training data
to train the SVM. M was empirically set to 50,000
in our experiments.
SVM Classifier We used a straightforward fea-
ture set for training the SVM classifier. Because
our classifier will be faced with sentences contain-
ing long and infrequent patterns where the distance
between the two target nouns may be quite large,
we did not try to represent lexico-syntactic patterns
as features but deliberately restricted the feature set
to local context features of the candidate noun pair
in the target sentence. Concretely, we looked at bi-
grams and unigrams surrounding both nouns of the
candidate relation, as the local context around the
target words may contain many telling expressions
like ?increase in X? or ?X deficiency? which are use-
ful clues for causal relations. Also, in Japanese case
information is encoded in post-positions attached to
the noun, which is captured by the unigram features.
In addition to these base features, we include the
semantic classes to which the candidate noun pair
belongs, the partial patterns they match in this sen-
tence, and the infix words inbetween the target noun
pair. Note that this feature set is not intended to
be optimal beyond the actual claims of this paper,
and we have deliberately avoided exhaustive fea-
ture engineering so as not to obscure the contribu-
tion of semantic classes and partial pattern to our
approach. Clearly an optimal classifier will incorpo-
rate many more advanced features (see GuoDong et
al. (2005) for a comprehensive overview), but even
without sophisticated feature engineering our clas-
sifier achieved sufficient performance levels to sup-
port our claims. An overview of the feature set is
given in Table 1. The relative contribution of each
type of features is discussed in section 4. In prelim-
inary experiments we found a polynomial kernel of
degree 3 gave the best results, which suggests the ef-
fectiveness of combining different types of indirect
evidence.
The SVM classifier outputs (noun pair, sentence)
triples, ranked by SVM score. To obtain the final
output of our method we assign each unique noun
pair the maximum score from all (noun pair, sen-
tence) triples it occurs in, and discard all other sen-
tences for this noun pair. In section 4 below we eval-
uate the acquired noun pairs in the context of the
sentence that maximizes their score.
4 Evaluation
We demonstrate the effectiveness of semantic word
classes and partial pattern matching for relation ex-
traction by showing that the method proposed in this
paper performs at the level of other state-of-the-art
relation acquisition methods. In addition we demon-
strate that our method can successfully extract re-
lation instances from infrequent patterns, and we
828
Feature type Description Number of features
Morpheme features Unigram and bigram morphemes surrounding both target words. 554,395
POS features Coarse- and fine-grained POS tags of the noun pair and morpheme features. 2,411
Semantic features Semantic word classes of the target noun pair. 1000 (500 classes ?2)
Infix word features Morphemes found inbetween the target noun pair. 94,448
Partial patterns Partial patterns matching the target noun pair. 86
Table 1: Feature set used in the Stage 2 classifier, and their number for the causal relation experiments.
explore several criteria for what constitutes an in-
frequent pattern ? including the theoretical limit-
ing case of patterns observed only once in the en-
tire corpus. These instances are impossible to ac-
quire by pattern based methods. The ability to ac-
quire relations from extremely infrequent expres-
sions with decent accuracy demonstrates the utility
of combining semantic word classes with partial pat-
tern matching.
4.1 Experimental Setting
We evaluate our method on three semantic relation
acquisition tasks: causality, prevention and mate-
rial. Two concepts stand in a causal relation when
the source concept (the ?cause?) is directly or indi-
rectly responsible for the subsequent occurrence of
the target concept (its ?effect?). In a prevention rela-
tion the source concept directly or indirectly acts to
avoid the occurrence of the target concept, and in a
material relation the source concept is a material or
ingredient of the target concept.
For our experiments we used the latest version
of the TSUBAKI corpus (Shinzato et al, 2008),
a collection of 600 million Japanese Web pages
dependency parsed by the Japanese dependency
parser KNP2. In our implementation of CDP, lexico-
syntactic patterns consist of words on the path con-
necting two nouns in a dependency parse tree. We
discard patterns from dependency paths longer than
8 constituent nodes. Furthermore, we estimated pat-
tern frequencies in a subset of the corpus (50 million
pages, or 1/12th of the entire corpus) and discarded
patterns that co-occur with less than 10 unique noun
pairs in this smaller corpus. These restrictions do
not apply to the proposed method, which can extract
noun pairs connected by patterns of arbitrary length,
even if found only once in the corpus. For our pur-
2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
pose we treat dependency paths whose observed fre-
quency is below this threshold as insufficiently fre-
quent to be considered as ?patterns?. This threshold
is of course arbitrary, but in section 4 we show that
our results are not affected by these implementation
details.
We asked three human judges to evaluate ran-
dom (noun pair, sentence) triples, i.e. candidate
noun pairs in the context of some corpus sentence
in which they co-occur. If the judges find the sen-
tence contains sufficient evidence that the target re-
lation holds between the candidate nouns, they mark
the noun pair correct. To evaluate the performance
of each method we use two evaluation criteria: strict
(all judges must agree the candidate relation is cor-
rect) and lenient (decided by the judges? majority
vote). Over all experiments the interrater agreement
(Kappa) ranged between 0.57 and 0.82 with an aver-
age of 0.72, indicating substantial agreement (Lan-
dis and Koch, 1977).
4.1.1 Methods Compared
We compare our results to two pattern based
methods: CDP (the Stage 1 extractor) and Espresso
(Pantel and Pennacchiotti, 2006a).
Espresso is a popular bootstrapping based method
that uses a set of seed instances to induce extraction
patterns for the target relation and then acquire new
instances in an iterative bootstrapping process. In
each iteration Espresso performs pattern induction,
pattern ranking and selection using previously ac-
quired instances, and uses the newly acquired pat-
terns to extraction new instances. Espresso com-
putes a reliability score for both instances and pat-
terns based on their pointwise mutual information
(PMI) with the top-scoring patterns and instances
from the previous iteration.3 We refer to (Pantel and
3 In our implementation of Espresso we found that, despite
the many parameters for controlling the bootstrapping process,
829
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 2: Precision of acquired relations (causality). L
and S denote lenient and strict evaluation.
Pennacchiotti, 2006a) for a more detailed descrip-
tion.
For all methods compared we rank the acquired
noun pairs by their score and evaluated 500 random
samples from the top 100,000 results. For noun pairs
acquired by CDP and Espresso we select the pattern
that extracted this noun pair (in the case of Espresso,
the pattern with the highest PMI for this noun pair),
and randomly select a sentence in which the noun
pair co-occurs with that pattern from our corpus. For
the proposed method we evaluate noun pairs in the
context of the (noun pair, sentence) triple with the
highest SVM score.
4.2 Results and Discussion
The performance of each method on the causality,
prevention and material relations are shown in Fig-
ures 2, 3 and 4 respectively. In the causality exper-
iments (Figure 2) the proposed method performs on
par with CDP for the top 25,000 results, both achiev-
ing close to 90% precision. But whereas CDP?s per-
it remains very difficult to prevent semantic drift (Komachi et
al., 2008) from occurring. One small adjustment to the al-
gorithm stabilized the bootstrapping process considerably and
gave overall better results. In the pattern induction step (sec-
tion 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso com-
putes a reliability score for each candidate pattern based on the
weighted PMI of the pattern with all instances extracted so far.
As the number of extracted instances increases this dispropor-
tionally favours high frequency (i.e. generic) patterns, so in-
stead of using all instances for computing pattern reliability we
only use the m most reliable instances from the previous iter-
ation, which were used to extract the candidate patterns of the
current iteration (m = 200, like the original).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 3: Precision of acquired relations (prevention). L
and S denote lenient and strict evaluation.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 10K 20K 30K 40K 50K 60K 70K 80K 90K 100K
Proposed (L)
Proposed (S)
Prop. w/o CDP (L)
Prop. w/o CDP (S)
Prop. w/o pattern (L)
Prop. w/o pattern (S)
Espresso (L)
Espresso (S)CDP (L)CDP (S)
Figure 4: Precision of acquired relations (material). L
and S denote lenient and strict evaluation.
formance drops from there our method maintains
the same high precision throughout (84.6%, lenient).
Both our method and CDP outperform Espresso by
a large margin.
For the prevention relation (Figure 3), precision
is considerably lower for all methods except the top
10,000 of CDP (82% precision, lenient). The pro-
posed method peaks at around 20,000 results (67%
precision, lenient) and performance remains more or
less constant from there on. The proposed method
overtakes CDP?s performance around the top 45,000
mark, which suggests that combining the results of
both methods may be beneficial.
In the material relations the proposed method
slightly outperforms both pattern based methods
in the top 10,000 results (92% precision, lenient).
830
However for this relation our method produced only
35,409 instances. The reason is that the top 25,000
results of CDP were all extracted by just 12 patterns,
and these contained many patterns whose syntactic
head is not a verb or adjective (like ?Y rich in X? or
?Y containing X?). Only 12 partial patterns were ob-
tained, which greatly reduced the output of the pro-
posed method. Taking into account the high perfor-
mance of CDP for material relations, this suggests
that for some relations our method?s N and M pa-
rameters could use some tuning. In conclusion, in
all three relations our method performs at a level
comparable to state-of-the-art pattern based meth-
ods, which is remarkable given that it only uses in-
direct evidence.
Dealing with Difficult Extractions How does our
method handle noun pairs that are difficult to ac-
quire by pattern based methods? The graphs marked
?Prop. w/o CDP? (Proposed without CDP) in Fig-
ures 2 , 3 and 4 show the number and precision of
evaluated samples from the proposed method that do
not co-occur in our corpus with any of the patterns
that extracted the top N results of the first stage ex-
tractor. These graphs show that our method is not
simply regenerating CDP?s top results but actually
extracts many noun pairs that do not co-occur in pat-
terns that are easily learned. Figure 2 shows that
roughly two thirds of the evaluated samples are in
this category, and that their performance is not sig-
nificantly worse than the overall result. The same
conclusion holds for the prevention results (Figure
3), where over 80% of the proposed method?s sam-
ples are noun pairs that do not co-occur with eas-
ily learnable patterns. Their precision is about 5%
worse than all samples from the proposed method.
For material relations (Figure 4) about half of all
evaluated samples are in this category, but their pre-
cision is markedly worse compared to all results.
For genuinely infrequent patterns, the graphs
marked ?Prop. w/o pattern? (Proposed without pat-
tern) in Figures 2 , 3 and 4 show the number and
precision of noun pairs evaluated for the proposed
method that were acquired from sentences without
any discernible pattern. As explained in section 4
above, these constitute noun pairs co-occurring in a
sentence in which the path of dependency relations
connecting them is either longer than 8 nodes or can
 0
 5
 10
 15
 20
1 2 32 1024 32768 1.05x106 3.36x107
% o
f al
l sa
mp
les
# of noun pairs co-occurring with patterns
Pattern frequency, CDPPattern frequency, ProposedPattern frequency, Espresso
Figure 5: Frequencies of patterns in the evaluation data
(causation).
extract fewer than 10 noun pairs in 50 million Web
pages. Note that in theory it is possible that these
noun pairs could not be acquired by pattern based
methods due to this threshold ? patterns must be
able to extract more than 10 different noun pairs in
a subset of our corpus, while the proposed method
does not have this constraint. So at least in the-
ory, pattern based methods might be able to acquire
all noun pairs obtained by our method by lowering
this threshold. To see that this is unlikely to be the
case, consider Figure 5, which shows the pattern fre-
quency of the patterns induced by CDP and Espresso
for the causality experiment. The x-axis represents
pattern frequency in terms of the number of unique
noun pairs a pattern co-occurs with in our corpus (on
a log scale), and the y-axis shows the percentage of
samples that was extracted by patterns of a given fre-
quency.4 Figure 5 shows that for the pattern based
methods, the large majority of noun pairs was ex-
tracted by patterns that co-occur with several thou-
sand different noun pairs. Extrapolating the original
frequency threshold of 10 nounpairs to the size of
our entire corpus roughly corresponds to about 120
distinct noun pairs (10 times in 1/12th of the entire
corpus). In Figure 5, the histograms for the pattern
based methods CDP and Espresso start around 1000
noun pairs, which is far above this new lowerbound.
4 In the case of CDP we ignore semantic class restrictions
on the patterns when comparing frequencies. For Espresso, the
most frequent pattern (?Y by X? at the 24,889,329 data point
on the x-axis) extracted up to 53.8% of the results, but the graph
was cut at 20% for readability.
831
Cau
sali
ty
??????? ??????????????????????????????????????????[????]??????
Because ?catecholamine? causes a rapid increase of heart rate, the change of circulation inside the blood vessels leads to blood vessel
disorders and promotes [thrombus generation].
????? ??????????????????????????????????? [????]?????????????
When we injected Xylocaine during a ?tachycardia seizure?, the patient suddenly lost consciousness and fell into a fit of [convulsions].
???????? ????????? ????????? [???]???????????????
(. . . ) The reason is that by taking a lot of ?animal proteins? the causative agents of [tragomaschalia] increase.
*???????????? ?????? ?????????????????????? [???]?
* [Radon] heightens the (body?s) antioxidative function and is effective for eliminating activated oxygen, which is a cause of aging and
?lifestyle-related? diseases.
Pre
ven
tion
???????????????????? ???? ??????????????[???]?????????????
Because the fatty meat of tuna contains DHA and ?EPA? in abundance, it is effective for preventing [neuralgia].
??????? ????? ??????? [????]?????????
If you use ?nitrogen gas? instead of air you may prevent [dust explosions].
??????????? ??????? ???????????????????????? [???]???????????
In ancient Europe ?orthosiphon aristatus? tea was called a ?diet tea?, and supposedly it helps preventing triglycerides and [adult diseases].
* ?? ???????????????????????? [????]????????
* (It) is something that prevents [scratches] on the screen if the ?calash? gets stuck between the screens during storage.
Table 2: Causality and Prevention relations acquired from Single Occurrence (SO) patterns. ?X? and [Y] indicate the
relation instance?s source and target words, and ?*? indicates erroneous extractions.
Thus, pattern based methods naturally tend to induce
patterns that are much more frequent than the range
of patterns our method can capture, and it is unlikely
that this is a result of implementation details like pat-
tern frequency threshold.
The precision of noun pairs in the category ?Prop.
w/o pattern? is clearly lower than the overall re-
sults, but the graphs demonstrate that our method
still handles these difficult cases reasonably well.
The 500 samples evaluated contained 155 such in-
stances for causality, 403 for prevention and 276 for
material. For prevention, the high ratio of these noun
pairs helps explain why the overall performance was
lower than for the other relations.
Finally, the theoretical limiting case for pattern
based algorithms consists of patterns that only co-
occur with a single noun pair in the entire corpus
(single occurrence or SO patterns). Pattern based
methods learn new patterns that share many noun
pairs with a set of reliable patterns in order to extract
new relation instances. If a noun pair that co-occurs
with a SO pattern also co-occurs with more reliable
patterns there is no need to learn the SO pattern. If
that same noun pair does not co-occur with any other
reliable pattern, the SO pattern is beyond the reach
of any pattern induction method. Thus, SO patterns
are effectively useless for pattern based methods.
For the 500 samples evaluated from the causality
and prevention relations acquired by our method we
found 7 causal noun pairs that co-occur only in SO
patterns and 29 such noun pairs for prevention. The
precision of these instances was 42.9% and 51.7%
respectively. In total we found 8,716 causal noun
pairs and 7,369 prevention noun pairs that co-occur
only with SO patterns. Table 2 shows some example
relations from our causality and prevention experi-
ments that were extracted from SO patterns. To con-
clude, our method is able to acquire correct relations
even from the most extreme infrequent expressions.
Semantic Classes, Partial Patterns or Both? In
the remainder of this section we look at how the
combination of semantic word classes and partial
patterns benefits our method. For each relation we
evaluated 1000 random (noun pair, sentence) triples
satisfying the two conditions from section 3 ?
matching semantic class pairs and partial patterns.
Surprisingly, the precision of these samples was
59% for causality, 40% for prevention and 50.4%
for material, showing just how compelling these two
types of indirect evidence are in combination.
To estimate the relative contribution of each
heuristic we compared our candidate generation
method against two baselines. The first baseline
evaluates the precision of random noun pairs from
832
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 6: Contribution of feature sets (causality).
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 7: Contribution of feature sets (prevention).
the target semantic classes that co-occur in a sen-
tence. The second baseline does the same for the
second heuristic, selecting random sentences con-
taining a noun pair that matches some partial pat-
tern. Evaluating 100 samples for causality and pre-
vention, we found the precision of the semantic class
baseline was 16% for causality and 5% for preven-
tion. The pattern fragment baseline gave 9% for
causality and 22% for prevention. This is consid-
erably lower than the precision of random samples
that satisfy both the semantic class and partial pat-
tern conditions, showing that the combination of se-
mantic classes and partial patterns is more effective
than either one individually.
Finally, we investigated the effect of the various
feature sets used in the classifier. Figures 6, 7 and
8 show the results for the respective semantic re-
lations. The ?Base features? graph shows the per-
 30
 40
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
pre
cis
ion
 (%
)
(noun pair, sentence) triples ranked by score
Base features onlyAll minus semantic classesAll minus infix wordsAll minus partial patternsAll features
Figure 8: Contribution of feature sets (material).
formance the unigram, bigram and part-of-speech
features. ?All features? uses all features in Table
1. The other graphs show the effect of removing
one type of features. These graphs suggest that the
contribution of the individual feature types (seman-
tic class information, partial patterns or infix words)
to the classification performance is relatively minor,
but in combination they do give a marked improve-
ment over the base features, at least for some rela-
tions like causation and material. In other words,
the main contribution of semantic word classes and
partial patterns to our method?s performance lies not
in the final classification step but seems to occur at
earlier stages of the process, in the candidate and
training data generation steps.
5 Related Work
Using lexico-syntactic patterns to extract semantic
relations was first explored by Hearst (Hearst, 1992),
and has inspired a large body of work on semi-
supervised relation acquisition methods (Berland
and Charniak, 1999; Agichtein and Gravano, 2000;
Etzioni et al, 2004; Pantel and Pennacchiotti,
2006b; Pas?ca et al, 2006; De Saeger et al, 2009),
two of which were used in this work.
Some researchers have addressed the sparse-
ness problems inherent in pattern based methods.
Downey et al (2007) starts from the output of
the unsupervised information extraction system Tex-
tRunner (Banko and Etzioni, 2008), and uses lan-
guage modeling techniques to estimate the reliabil-
ity of sparse extractions. Pas?ca et al (2006) alle-
833
viates pattern sparseness by using infix patterns that
are generalized using classes of distributionally sim-
ilar words. In addition, their method employs clus-
tering based semantic similarities to filter newly ex-
tracted instances in each iteration of the bootstrap-
ping process. A comparison with our method would
have been instructive, but we were unable to imple-
ment their method because the original paper con-
tains insufficient detail to allow replication.
There is a large body of research in the super-
vised tradition that does not use explicit pattern rep-
resentations ? kernel based methods (Zelenko et
al., 2003; Culotta, 2004; Bunescu and Mooney,
2005) and CRF based methods (Culotta et al, 2006).
These approaches are all fully supervised, whereas
in our work the automatic generation of candi-
dates and training data is an integral part of the
method. An interesting alternative is distant super-
vision (Mintz et al, 2009), which trains a classi-
fier using an existing database (Freebase) containing
thousands of semantic relations, with millions of in-
stances. We believe our method is more general, as
depending on external resources like a database of
semantic relations limits both the range of seman-
tic relations (i.e., Freebase contains only relations
between named entities, and none of the relations
in this work) and languages (i.e., no resource com-
parable to Freebase exists for Japanese) to which
the technology can be applied. Furthermore, it is
unclear whether distant supervision can deal with
noisy input such as automatically acquired relation
instances.
Finally, inference based methods (Carlson et al,
2010; Schoenmackers et al, 2010; Tsuchida et al,
2010) are another attempt at relation acquisition that
goes beyond pattern matching. Carlson et al (2010)
proposed a method based on inductive logic pro-
gramming (Quinlan, 1990). Schoenmackers et al
(2010) takes relation instances produced by Tex-
tRunner (Banko and Etzioni, 2008) as input and in-
duces first-order Horn clauses, and new instances are
infered using a Markov Logic Network (Richardson
and Domingo, 2006; Huynh and Mooney, 2008).
Tsuchida et al (2010) generated new relation hy-
potheses by substituting words in seed instances
with distributionally similar words. The difference
between these works and ours lies in the treatment
of evidence. While the above methods learn infer-
ence rules to acquire new relation instances from in-
dependent information sources scattered across dif-
ferent Web pages, our method takes the other option
of working with all the clues and indirect evidence a
single sentence can provide. In the future, a combi-
nation of both approaches may prove beneficial.
6 Conclusion
We have proposed a relation acquisition method that
is able to acquire semantic relations from infrequent
expressions by focusing on the evidence provided by
semantic word classes and partial pattern matching
instead of direct extraction patterns. We experimen-
tally demonstrated the effectiveness of this approach
on three relation acquisition tasks, causality, preven-
tion and material relations. In addition we showed
our method could acquire a significant number of
relation instances that are found in extremely infre-
quent expressions, the most extreme case of which
are single occurrence patterns, which are beyond
the reach of existing pattern based methods. We be-
lieve this ability is of crucial importance for acquir-
ing valuable long tail instances. In future work we
will investigate whether the current framework can
be extended to acquire inter-sentential relations.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proc. of the fifth ACM conference on Digital li-
braries, pages 85?94.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proc. of the 46th ACL-08:HLT, pages 28?36.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 57?64, College Park, Mary-
land, USA, June.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing (HLT ?05), pages 724?731.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for neverend-
ing language learning. In Proc of the 24th AAAI, pages
1306?1313.
834
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HLT/NAACL), pages 296?303.
Aron Culotta. 2004. Dependency tree kernels for rela-
tion extraction. In In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-04, pages 423?429.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large Scale
Relation Acquisition Using Class Dependent Patterns.
In Proc. of the 9th International Conference on Data
Mining (ICDM), pages 764?769.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL2007).
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen Soder-
land, Daniel Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll. In Proc. of
the 13th international conference on World Wide Web
(WWW04), pages 100?110.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extrac-
tion. In Proc. of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?05, pages
419?444.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING?92), pages 539?545.
Tuyen N. Huynh and Raymond J. Mooney. 2008.
Discriminative structure and parameter learning for
markov logic networks. In Proc. of the 25th ICML,
pages 416?423.
Jun?ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL-08: HLT), pages 407?415.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in espresso-like bootstrapping algorithms.
In Proc. of EMNLP?08. Honolulu, USA, pages 1011?
1020.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In Proc. of the ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 323?328.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proc. of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and Similarities on
the Web: Fact Extraction in the Fast Lane. In Proc. of
the COLING-ACL06, pages 809?816.
Patrick Pantel and Marco Pennacchiotti. 2006a.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proc. of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-06,
pages 113?120.
Patrick Pantel and Pennacchiotti Pennacchiotti, Marco.
2006b. Espresso: Leveraging generic patterns for au-
tomatically harvesting semantic relations. In Proc. of
the COLING-ACL06, pages 113?120.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5(3):239?266.
Matthew Richardson and Pedro Domingo. 2006.
Markov logic networks. Machine Learning, 26:107?
136.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proc. of EMNLP2010, pages
1088?1098.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
TSUBAKI: An open search engine infrastructure for
developing new information access. In Proc. of IJC-
NLP, pages 189?196.
Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,
Masaaki Murata, Kow Kuroda, and Ichiro Yamada.
2010. Organizing the web?s information explosion to
discover unknown unknowns. New Generation Com-
puting, 28(3):217?236.
Masaaki Tsuchida, Stijn De Saeger, Kentaro Torisawa,
Masaki Murata, Jun?ichi Kazama, Kow Kuroda, and
Hayato Ohwada. 2010. Large scale similarity-based
relation expansion. In Proc of the 4th IUCS, pages
140?147.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
pages 1083?1106.
835
Proceedings of NAACL-HLT 2013, pages 63?73,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Minimally Supervised Method for Multilingual Paraphrase Extraction
from Definition Sentences on the Web
Yulan Yan? Chikara Hashimoto? Kentaro Torisawa?
Takao Kawai? Jun?ichi Kazama? Stijn De Saeger??
? ? ? ?? ?? Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{? yulan, ? ch, ? torisawa, ??stijn}@nict.go.jp
Abstract
We propose a minimally supervised method
for multilingual paraphrase extraction from
definition sentences on the Web. Hashimoto
et al (2011) extracted paraphrases from
Japanese definition sentences on the Web, as-
suming that definition sentences defining the
same concept tend to contain paraphrases.
However, their method requires manually an-
notated data and is language dependent. We
extend their framework and develop a mini-
mally supervised method applicable to multi-
ple languages. Our experiments show that our
method is comparable to Hashimoto et al?s
for Japanese and outperforms previous unsu-
pervised methods for English, Japanese, and
Chinese, and that our method extracts 10,000
paraphrases with 92% precision for English,
82.5% precision for Japanese, and 82% preci-
sion for Chinese.
1 Introduction
Automatic paraphrasing has been recognized as an
important component for NLP systems, and many
methods have been proposed to acquire paraphrase
knowledge (Lin and Pantel, 2001; Barzilay and
McKeown, 2001; Shinyama et al, 2002; Barzilay
and Lee, 2003; Dolan et al, 2004; Callison-Burch,
2008; Hashimoto et al, 2011; Fujita et al, 2012).
We propose a minimally supervised method for
multilingual paraphrase extraction. Hashimoto et al
(2011) developed a method to extract paraphrases
from definition sentences on the Web, based on
their observation that definition sentences defining
the same concept tend to contain many paraphrases.
Their method consists of two steps; they extract def-
inition sentences from the Web, and extract phrasal
(1) a. Paraphrasing is the use of your own words to express the au-
thor?s ideas without changing the meaning.
b. Paraphrasing is defined as a process of transforming an expres-
sion into another while keeping its meaning intact.
(2) a. ?????????????????????????
???????????????? (Paraphrasing refers to
the replacement of an expression into another without changing
the semantic content.)
b. ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
???????????????????????
????????? (Paraphrasing is a process of trans-
forming an expression into another of the same language while
preserving the meaning and content as much as possible.)
(3) a. ????????????????????????
??????? (Paraphrasing refers to the transformation
of sentence structure by the translator without changing the
meaning of original text.)
b. ?????????????????????????
(Paraphrasing is a translation method of keeping the content of
original text but not keeping the expression.)
Figure 1: Multilingual definition pairs on ?paraphrasing.?
paraphrases from the definition sentences. Both
steps require supervised classifiers trained by manu-
ally annotated data, and heavily depend on their tar-
get language. However, the basic idea is actually
language-independent. Figure 1 gives examples of
definition sentences on the Web that define the same
concept in English, Japanese, and Chinese (with En-
glish translation). As indicated by underlines, each
definition pair has a phrasal paraphrase.
We aim at extending Hashimoto et al?s method
to a minimally supervised method, thereby enabling
acquisition of phrasal paraphrases within one lan-
guage, but in different languages without manually
annotated data. The first contribution of our work
is to develop a minimally supervised method for
multilingual definition extraction that uses a clas-
sifier distinguishing definition from non-definition.
The classifier is learnt from the first sentences in
63
Defini?n	 ?sentences	 Defini?n	 ?pairs	 ? Paraphrase	 ?candidates	 ?
Ranked	 ?paraphrase	 ?candidates	 ?Classifier	Web	
Defini?on	 ?Extrac?on	 ?(Sec?on	 ?2.1)	 Paraphrase	 ?Extrac?on	 ?(Sec?on	 ?2.2)	
Ranking	 ?by	 ?Score	Automa?cally	 ?constructed	 ?training	 ?data	
Web	 Wikipedia	
Figure 2: Overall picture of our method.
Wikipedia articles, which can be regarded as the def-
inition of the title of Wikipedia article (Kazama and
Torisawa, 2007) and hence can be used as positive
examples. Our method relies on a POS tagger, a de-
pendency parser, a NER tool, noun phrase chunking
rules, and frequency thresholds for each language,
in addition to Wikipedia articles, which can be seen
as a manually annotated knowledge base. How-
ever, our method needs no additional manual anno-
tation particularly for this task and thus we catego-
rize our method as a minimally supervised method.
On the other hand, Hashimoto et al?s method heav-
ily depends on the properties of Japanese like the
assumption that characteristic expressions of defini-
tion sentences tend to appear at the end of sentence
in Japanese. We show that our method is applica-
ble to English, Japanese, and Chinese, and that its
performance is comparable to state-of-the-art super-
vised methods (Navigli and Velardi, 2010). Since
the three languages are very different we believe that
our definition extraction method is applicable to any
language as long as Wikipedia articles of the lan-
guage exist.
The second contribution of our work is to de-
velop a minimally supervised method for multi-
lingual paraphrase extraction from definition sen-
tences. Again, Hashimoto et al?s method utilizes
a supervised classifier trained with annotated data
particularly prepared for this task. We eliminate the
need for annotation and instead introduce a method
that uses a novel similarity measure considering
the occurrence of phrase fragments in global con-
texts. Our paraphrase extraction method is mostly
language-independent and, through experiments for
the three languages, we show that it outperforms
unsupervised methods (Pas?ca and Dienes, 2005;
Koehn et al, 2007) and is comparable to Hashimoto
et al?s supervised method for Japanese.
Previous methods for paraphrase (and entailment)
extraction can be classified into a distributional sim-
ilarity based approach (Lin and Pantel, 2001; Gef-
fet and Dagan, 2005; Bhagat et al, 2007; Szpek-
tor and Dagan, 2008; Hashimoto et al, 2009) and a
parallel corpus based approach (Barzilay and McK-
eown, 2001; Shinyama et al, 2002; Barzilay and
Lee, 2003; Dolan et al, 2004; Callison-Burch,
2008). The former can exploit large scale monolin-
gual corpora, but is known to be unable to distin-
guish paraphrase pairs from antonymous pairs (Lin
et al, 2003). The latter rarely mistakes antonymous
pairs for paraphrases, but preparing parallel corpora
is expensive. As with Hashimoto et al (2011), our
method is a kind of parallel corpus approach in that it
uses definition pairs as a parallel corpus. However,
our method does not suffer from a high labor cost
of preparing parallel corpora, since it can automati-
cally collect definition pairs from the Web on a large
scale. The difference between ours and Hashimoto
et al?s is that our method requires no manual label-
ing of data and is mostly language-independent.
2 Proposed Method
Our method first extracts definition sentences from
the Web, and then extracts paraphrases from the def-
inition sentences, as illustrated in Figure 2.
2.1 Definition Extraction
2.1.1 Automatic Construction of Training Data
Our method learns a classifier that classifies sen-
tences into definition and non-definition using auto-
matically constructed training data, TrDat. TrDat?s
positive examples, Pos, are the first sentences of
Wikipedia articles and the negative examples, Neg,
are randomly sampled Web sentences. The former
can be seen as definition, while the chance that the
sentences in the latter are definition is quite small.
Our definition extraction not only distinguishes
definition from non-definition but also identities the
defined term of a definition sentence, and in the
paraphrase extraction step our method couples two
definition sentences if their defined terms are identi-
cal. For example, the defined terms of (1a) and (1b)
in Figure 1 are both ?Paraphrasing? and thus the two
definition sentences are coupled. For Pos, we mark
up the title of Wikipedia article as the defined term.
For Neg, we randomly select a noun phrase in a sen-
64
(A)
N-gram definition pattern N-gram non-definition pattern
?[term] is the [term] may be
[term] is a type of [term] is not
(B)
Subsequence definition pattern Subsequence non-definition pattern
[term] is * which is located you may * [term]
[term] is a * in the was [term] * , who is
(C)
Subtree definition pattern Subtree non-definition pattern
[term] is defined as the NP [term] will not be
Table 1: Examples of English patterns.
tence and mark it up as a (false) defined term. Any
marked term is uniformly replaced with [term].
2.1.2 Feature Extraction and Learning
As features, we use patterns that are characteristic
of definition (definition patterns) and those that are
unlikely to be a part of definition (non-definition pat-
terns). Patterns are either N-grams, subsequences, or
dependency subtrees, and are mined automatically
from TrDat. Table 1 shows examples of patterns
mined by our method. In (A) of Table 1, ??? is
a symbol representing the beginning of a sentence.
In (B), ?*? represents a wildcard that matches any
number of arbitrary words. Patterns are represented
by either their words? surface form, base form, or
POS. (Chinese words do not inflect and thus we do
not use the base form for Chinese.)
We assume that definition patterns are fre-
quent in Pos but are infrequent in Neg, and
non-definition patterns are frequent in Neg but
are infrequent in Pos. To see if a given pat-
tern ? is likely to be a definition pattern, we
measure ??s probability rate Rate(?). If the
probability rate of ? is large, ? tends to be a
definition pattern. The probability rate of ? is:
Rate(?) =
freq(?,Pos)/|Pos|
freq(?,Neg)/|Neg|
, iffreq(?,Neg) 6= 0.
Here, freq(?,Pos) = |{s ? Pos : ? ? s}| and
freq(?,Neg) = |{s ? Neg : ? ? s}|. We write ? ? s
if sentence s contains ?. If freq(?,Neg) = 0,
Rate(?) is set to the largest value of all the patterns?
Rate values. Only patterns whose Rate is more
than or equal to a Rate threshold ?pos and whose
freq(?,Pos) is more than or equal to a frequency
threshold are regarded as definition patterns. Simi-
larly, we check if ? is likely to be a non-definition
pattern. Only patterns whose Rate is less or equal
English Japanese Chinese
Type Representation Pos Neg Pos Neg Pos Neg
N-gram
Surface 120 400 30 100 20 100
Base 120 400 30 100 ? ?
POS 2,000 4,000 500 500 100 400
Subsequence
Surface 120 400 30 100 20 40
Base 120 400 30 100 ? ?
POS 2,000 2,000 500 500 200 400
Subtree
Surface 5 10 5 10 5 5
Base 5 10 5 10 ? ?
POS 25 50 25 50 25 50
Table 2: Values of frequency threshold.
to a Rate threshold ?neg and whose freq(?,Neg)
is more than or equal to a frequency threshold are
regarded as non-definition patterns. The probability
rate is based on the growth rate (Dong and Li,
1999).
?pos and ?neg are set to 2 and 0.5, while the fre-
quency threshold is set differently according to lan-
guages, pattern types (N-gram, subsequence, and
subtree), representation (surface, base, and POS),
and data (Pos and Neg), as in Table 2. The thresholds
in Table 2 were determined manually, but not really
arbitrarily. Basically they were determined accord-
ing to the frequency of each pattern in our data (e.g.
how frequently the surface N-gram of English ap-
pears in English positive training samples (Pos)).
Below, we detail how patterns are acquired. First,
we acquire N-gram patterns. Then, subsequence
patterns are acquired using the N-gram patterns as
input. Finally, subtree patterns are acquired using
the subsequence patterns as input.
N-gram patterns We collect N-gram patterns
from TrDat with N ranging from 2 to 6. We filter
out N-grams using thresholds on the Rate and fre-
quency, and regard those that are kept as definition
or non-definition N-grams.
Subsequence patterns We generate subsequence
patterns as ordered combinations of N-grams with
the wild card ?*? inserted between them (we use
two or three N-grams for a subsequence). Then, we
check each of the generated subsequences and keep
it if there exists a sentence in TrDat that contains the
subsequence and whose root node is contained in the
subsequence. For example, subsequence ?[term]
is a * in the? is kept if a term-marked sentence like
?[term] is a baseball player in the Dominican Re-
public.? exists in TrDat. Then, patterns are filtered
65
out using thresholds on the Rate and frequency as
we did for N-grams.
Subtree patterns For each definition and non-
definition subsequence, we retrieve all the term-
marked sentences that contain the subsequence from
TrDat, and extract a minimal dependency subtree
that covers all the words of the subsequence from
each retrieved sentence. For example, assume that
we retrieve a term-marked sentence ?[term] is
usually defined as the way of life of a group of peo-
ple.? for subsequence ?[term] is * defined as the?.
Then we extract from the sentence the minimal de-
pendency subtree in the left side of (C) of Table 1.
Note that all the words of the subsequence are con-
tained in the subtree, and that in the subtree a node
(?way?) that is not a part of the subsequence is re-
placed with its dependency label (?NP?) assigned by
the dependency parser. The patterns are filtered out
using thresholds on the Rate and frequency.
We train a SVM classifier1 with a linear kernel,
using binary features that indicate the occurrence of
the patterns described above in a target sentence.
In theory, we could feed all the features to the
SVM classifier and let the classifier pick informa-
tive features. But we restricted the feature set for
practical reasons: the number of features would be-
come tremendously large. There are two reasons for
this. First, the number of sentences in our automati-
cally acquired training data is huge (2,439,257 posi-
tive sentences plus 5,000,000 negative sentences for
English, 703,208 positive sentences plus 1,400,000
negative sentences for Japanese and 310,072 posi-
tive sentences plus 600,000 negative sentences for
Chinese). Second, since each subsequence pattern
is generated as a combination of two or three N-
gram patterns and one subsequence pattern can gen-
erate one or more subtree patterns, using all possi-
ble features leads to a combinatorial explosion of
features. Moreover, since the feature vector will be
highly sparse with a huge number of infrequent fea-
tures, SVM learning becomes very time consuming.
In preliminary experiments we observed that when
using all possible features the learning process took
more than one week for each language. We there-
fore introduced the current feature selection method,
in which the learning process finished in one day but
1http://svmlight.joachims.org.
Original Web sentence: Albert Pujols is a baseball player.
Term-marked sentence 1: [term] is a baseball player.
Term-marked sentence 2: Albert Pujols is a [term].
Figure 3: Term-marked sentences from a Web sentence.
still obtains good results.
2.1.3 Definition Extraction from the Web
We extract a large amount of definition sen-
tences by applying this classifier to sentences in our
Web archive. Because our classifier requires term-
marked sentences (sentences in which the term be-
ing defined is marked) as input, we first have to iden-
tify all such defined term candidates for each sen-
tence. For example, Figure 3 shows a case where a
Web sentence has two NPs (two candidates of de-
fined term). Basically we pick up NPs in a sen-
tence by simple heuristic rules. For English, NPs are
identified using TreeTagger (Schmid, 1995) and two
NPs are merged into one when they are connected by
?for? or ?of?. After applying this procedure recur-
sively, the longest NPs are regarded as candidates of
defined terms and term-marked sentences are gener-
ated. For Japanese, we first identify nouns that are
optionally modified by adjectives as NPs, and allow
two NPs connected by ??? (of ), if any, to form
a larger NP. For Chinese, nouns that are optionally
modified by adjectives are considered as NPs.
Then, each term-marked sentence is given a fea-
ture vector and classified by the classifier. The term-
marked sentence whose SVM score (the distance
from the hyperplane) is the largest among those from
the same original Web sentence is chosen as the final
classification result for the original Web sentence.
2.2 Paraphrase Extraction
We use all the Web sentences classified as defini-
tion and all the sentences in Pos for paraphrase ex-
traction. First, we couple two definition sentences
whose defined term is the same. We filter out defini-
tion sentence pairs whose cosine similarity of con-
tent word vectors is less than or equal to threshold
C, which is set to 0.1. Then, we extract phrases
from each definition sentence, and generate all pos-
sible phrase pairs from the coupled sentences. In
this study, phrases are restricted to predicate phrases
that consist of at least one dependency relation and
in which all the constituents are consecutive in a
66
f1
The ratio of the number of words shared between two can-
didate phrases to the number of all of the words in the two
phrases. Words are represented by either their surface form
(f1,1), base form (f1,2) or POS (f1,3).
f2
The identity of the leftmost word (surface form (f2,1), base
form (f2,2) or POS (f2,3)) between two candidate phrases.
f3
The same as f2 except that we use the rightmost word.
There are three corresponding subfunctions (f3,1 to f3,3).
f4
The ratio of the number of words that appear in a candidate
phrase segment of a definition sentence s1 and in a segment
that is NOT a part of the candidate phrase of another def-
inition sentence s2 to the number of all the words of s1?s
candidate phrase. Words are in their base form (f4,1).
f5 The reversed (s1 ? s2) version of f4,1 (f5,1).
f6
The ratio of the number of words (the surface form) of a
shorter candidate phrase to that of a longer one (f6,1).
f7
Cosine similarity between two definition sentences from
which two candidate phrases are extracted. Only content
words in the base form are used (f7,1).
f8
The ratio of the number of parent dependency subtrees that
are shared by two candidate phrases to the number of all the
parent dependency subtrees. The parent dependency sub-
trees are adjacent to the candidate phrases and represented
by their surface form (f8,1), base form (f8,2), or POS (f8,3).
f9
The same as f8 except that we use child dependency sub-
trees. There are 3 subfunctions (f9,1 to f9,3) of f9 type.
f10
The ratio of the number of context N-grams that are shared
by two candidate phrases to the number of all the context N-
grams of both candidate phrases. The context N-grams are
adjacent to the candidate phrases and represented by either
the surface form, the base form, or POS. The N ranges from
1 to 3, and the context is either left-side or right-side. Thus,
there are 18 subfunctions (3? 3? 2).
Table 3: Local similarity subfunctions, f1,1 to f10,18.
sentence. Accordingly, if two definition sentences
that are coupled have three such predicate phrases
respectively, we get nine phrase pairs, for instance.
A phrase pair extracted from a definition pair is a
paraphrase candidate and is given a score that indi-
cates the likelihood of being a paraphrase, Score. It
consists of two similarity measures, local similarity
and global similarity, which are detailed below.
Local similarity Following Hashimoto et al, we
assume that two candidate phrases (p1, p2) tend to
be a paraphrase if they are similar enough and/or
their surrounding contexts are sufficiently similar.
Then, we calculate the local similarity (localSim) of
(p1, p2) as the weighted sum of 37 similarity sub-
functions that are grouped into 10 types (Table 3.)
For example, the f1 type consists of three subfunc-
tions, f1,1, f1,2, and f1,3. The 37 subfunctions are
inspired by Hashimoto et al?s features. Then, local-
Sim is defined as:
localSim(p1, p2) = max
(dl,dm)?DP (p1,p2)
ls(p1, p2, dl, dm).
Here, ls(p1, p2, dl, dm) =
?10
i=1
?ki
j=1
wi,j?fi,j(p1,p2,dl,dm)
ki .
DP (p1, p2) is the set of all definition sentence pairs
that contain (p1, p2). (dl, dm) is a definition sen-
tence pair containing (p1, p2). ki is the number
of subfunctions of fi type. wi,j is the weight for
fi,j . wi,j is uniformly set to 1 except for f4,1
and f5,1, whose weight is set to ?1 since they
indicate the unlikelihood of (p1, p2)?s being a
paraphrase. As the formula indicates, if there is
more than one definition sentence pair that contains
(p1, p2), localSim is calculated from the definition
sentence pair that gives the maximum value of
ls(p1, p2, dl, dm). localSim is local in the sense that
it is calculated based on only one definition pair
from which (p1, p2) are extracted.
Global similarity The global similarity (global-
Sim) is our novel similarity function. We decompose
a candidate phrase pair (p1, p2) into Comm, the com-
mon part between p1 and p2, and Diff , the difference
between the two. For example, Comm and Diff of
(?keep the meaning intact?, ?preserve the meaning?)
is (?the meaning?) and (?keep, intact?, ?preserve?).
globalSim measures the semantic similarity of
the Diff of a phrase pair. It is proposed based on
the following intuition: phrase pair (p1, p2) tend
to be a paraphrase if their surface difference (i.e.
Diff ) have the same meaning. For example, if
?keep, intact? and ?preserve? mean the same, then
(?keep the meaning intact?, ?preserve the meaning?)
is a paraphrase.
globalSim considers the occurrence of Diff in
global contexts (i.e., all the paraphrase candidates
from all the definition pairs). The globalSim of a
given phrase pair (p1, p2) is measured by basically
counting how many times the Diff of (p1, p2) ap-
pears in all the candidate phrase pairs from all the
definition pairs. The assumption is that Diff tends to
share the same meaning if it appears repeatedly in
paraphrase candidates from all definition sentence
pairs, i.e., our parallel corpus. Each occurrence of
Diff is weighted by the localSim of the phrase pair
in which Diff occurs. Precisely, globalSim is defined
as:
67
Threshold The frequency threshold of Table 2 (Section 2.1.2).
NP rule Rules for identifying NPs in sentences (Section 2.1.3).
POS list The list of content words? POS (Section 2.2).
Tagger/parser POS taggers, dependency parsers and NER tools.
Table 4: Language-dependent components.
globalSim(p1, p2) =
?
(pi,pj)?PP (p1,p2)
localSim(pi, pj)
M
.
PP (p1, p2) is the set of candidate phrase pairs
whose Diff is the same as (p1, p2).2 M is the num-
ber of similarity subfunction types whose weight is
1, i.e. M = 8 (all the subfunction types except f4
and f5). It is used to normalize the value of each
occurrence of Diff to [0, 1].3 globalSim is global
in the sense that it considers all the definition pairs
that have a phrase pair with the same Diff as a target
candidate phrase pair (p1, p2).
The final score for a candidate phrase pair is:
Score(p1, p2) = localSim(p1, p2) + ln globalSim(p1, p2).
The way of combining the two similarity functions
has been determined empirically after testing several
other ways of combining them. This ranks all the
candidate phrase pairs.
Finally, we summarize language-dependent com-
ponents that we fix manually in Table 4.
3 Experiments
3.1 Experiments of Definition Extraction
We show that our unsupervised definition extrac-
tion method is competitive with state-of-the-art su-
pervised methods for English (Navigli and Velardi,
2010), and that it extracts a large number of defini-
tions reasonably accurately for English (3,216,121
definitions with 70% precision), Japanese (651,293
definitions with 62.5% precision), and Chinese
(682,661 definitions with 67% precision).
2If there are more than one (pi, pj) in a definition pair, we
use only one of them that has the largest localSim value.
3Although we claim that our idea of using globalSim is ef-
fective, we do not claim that the above formula for calculating
is the optimal way to implement the idea. Currently we are in-
vestigating a more mathematically well-motivated model.
3.1.1 Preparing Corpora
First we describe Pos, Neg, and the Web corpus
from which definition sentences are extracted. As
the source of Pos, we used the English Wikipedia
of April 2011 (3,620,149 articles), the Japanese
Wikipedia of October 2011 (830,417 articles), and
the Chinese Wikipedia of August 2011 (365,545 ar-
ticles). We removed category articles, template ar-
ticles, list articles and so on from them. Then the
number of sentences of Pos was 2,439,257 for En-
glish, 703,208 for Japanese, and 310,072 for Chi-
nese. We verified our assumption that Wikipedia
first sentences can mostly be seen as definition by
manually checking 200 random samples from Pos.
96.5% of English Pos, 100% of Japanese Pos, and
99.5% of Chinese Pos were definitions.
As the source of Neg, we used 600 million
Japanese Web pages (Akamine et al, 2010) and
the ClueWeb09 corpus for English (about 504 mil-
lion pages) and Chinese (about 177 million pages).4
From each Web corpus, we collected the sentences
satisfying following conditions: 1) they contain 5
to 50 words and at least one verb, 2) less than half
of their words are numbers, and 3) they end with a
period. Then we randomly sampled sentences from
the collected sentences as Neg so that |Neg| was
about twice as large as |Pos|: 5,000,000 for English,
1,400,000 for Japanese, and 600,000 for Chinese.
In Section 3.1.3, we use 10% of the Web corpus as
the input to the definition classifier. The number of
sentences are 294,844,141 for English, 245,537,860
for Japanese, and 68,653,130 for Chinese.
All the sentences were POS-tagged and parsed.
We used TreeTagger and MSTParser (McDonald
et al, 2006) for English, JUMAN (Kurohashi and
Kawahara, 2009a) and KNP (Kurohashi and Kawa-
hara, 2009b) for Japanese, MMA (Kruengkrai et al,
2009) and CNP (Chen et al, 2009) for Chinese.
3.1.2 Comparison with Previous Methods
We compared our method with the state-of-the-
art supervised methods proposed by Navigli and Ve-
lardi (2010), using their WCL datasets v1.0 (http:
//lcl.uniroma1.it/wcl/), definition and non-
definition datasets for English (Navigli et al, 2010).
Specifically, we used its training data (TrDatwcl,
hereafter), which consisted of 1,908 definition and
4http://lemurproject.org/clueweb09.php/
68
Method Precision Recall F1 Accuracy
Proposeddef 86.79 86.97 86.88 89.18
WCL-1 99.88 42.09 59.22 76.06
WCL-3 98.81 60.74 75.23 83.48
Table 5: Definition classification results on TrDatwcl.
2,711 non-definition sentences, and compared the
following three methods. WCL-1 and WCL-3 are
methods proposed by Navigli and Velardi (2010).
They were trained and tested with 10 fold cross vali-
dation using TrDatwcl. Proposeddef is our method,
which used TrDat for acquiring patterns (Section
2.1.2) and training. We tested Proposeddef on each
of TrDatwcl?s 10 folds and averaged the results.
Note that, for Proposeddef , we removed sentences
in TrDatwcl from TrDat in advance for fairness.
Table 5 shows the results. The numbers for WCL-
1 and WCL-3 are taken from Navigli and Velardi
(2010). Proposeddef outperformed both methods in
terms of recall, F1, and accuracy. Thus, we conclude
that Proposeddef is comparable to WCL-1/WCL-3.
We conducted ablation tests of our method to in-
vestigate the effectiveness of each type of pattern.
When using only N-grams, F1 was 85.41. When
using N-grams and subsequences, F1 was 86.61.
When using N-grams and subtrees, F1 was 86.85.
When using all the features, F1 was 86.88. The re-
sults show that each type of patterns contribute to the
performance, but the contributions of subsequence
patterns and subtree patterns do not seem very sig-
nificant.
3.1.3 Experiments of Definition Extraction
We extracted definitions from 10% of the Web
corpus. We applied Proposeddef to the cor-
pus of each language, and the state-of-the-art su-
pervised method for Japanese (Hashimoto et al,
2011) (Hashidef , hereafter) to the Japanese corpus.
Hashidef was trained on their training data that con-
sisted of 2,911 sentences, 61.1% of which were def-
initions. Note that we removed sentences in TrDat
from 10% of the Web corpus in advance, while we
did not remove Hashimoto et al?s training data from
the corpus. This means that, for Hashidef , the train-
ing data is included in the test data.
For each method, we filtered out its positive out-
puts whose defined term appeared more than 1,000
times in 10% of the Web corpus, since those terms
tend to be too vague to be a defined term or re-
fer to an entity outside the definition sentence. For
example, if ?the college? appears more than 1,000
times in 10% of the corpus, we filter out sen-
tences like ?The college is one of three colleges
in the Coast Community College District and was
founded in 1947.? For Proposeddef , the number of
remaining positive outputs is 3,216,121 for English,
651,293 for Japanese, and 682,661 for Chinese. For
Hashidef , the number of positive outputs is 523,882.
For Proposeddef of each language, we randomly
sampled 200 sentences from the remaining positive
outputs. For Hashidef , we first sorted its output by
the SVM score in descending order and then ran-
domly sampled 200 from the top 651,293, i.e., the
same number as the remaining positive outputs of
Proposeddef of Japanese, out of all the remaining
sentences of Hashidef .
For each language, after shuffling all the samples,
two human annotators evaluated each sample. The
annotators for English and Japanese were not the au-
thors, while one of the Chinese annotators was one
of the authors. We regarded a sample as a defini-
tion if it was regarded as a definition by both an-
notators. Cohen?s kappa (Cohen, 1960) was 0.55
for English (moderate agreement (Landis and Koch,
1977)), 0.73 for Japanese (substantial agreement),
and 0.69 for Chinese (substantial agreement).
For English, Proposeddef achieved 70% precision
for the 200 samples. For Japanese, Proposeddef
achieved 62.5% precision for the 200 samples, while
Hashidef achieved 70% precision for the 200 sam-
ples. For Chinese, Proposeddef achieved 67% pre-
cision for the 200 samples. From these results, we
conclude that Proposeddef can extract a large num-
ber of definition sentences from the Web moderately
well for the three languages.
Although the precision is not very high, our ex-
periments in the next section show that we can still
extract a large number of paraphrases with high pre-
cision from these definition sentences, due mainly to
our similarity measures, localSim and globalSim.
3.2 Experiments of Paraphrase Extraction
We show (1) that our paraphrase extraction method
outperforms unsupervised methods for the three lan-
guages, (2) that globalSim is effective, and (3) that
our method is comparable to the state-of-the-art su-
69
ProposedScore: Our method. Outputs are ranked by Score.
Proposedlocal: This is the same as ProposedScore except that it ranks
outputs by localSim. The performance drop from ProposedScore
shows globalSim?s effectiveness.
Hashisup: Hashimoto et al?s supervised method. Training data is the
same as Hashimoto et al Outputs are ranked by the SVM score
(the distance from the hyperplane). This is for Japanese only.
Hashiuns: The unsupervised version of Hashisup. Outputs are
ranked by the sum of feature values. Japanese only.
SMT: The phrase table construction method of Moses (Koehn et al,
2007). We assume that Moses should extract a set of two phrases
that are paraphrases of each other, if we input monolingual par-
allel sentence pairs like our definition pairs. We used default
values for all the parameters. Outputs are ranked by the product
of two phrase translation probabilities of both directions.
P&D: The distributional similarity based method by Pas?ca and Di-
enes (2005) (their ?N-gram-Only? method). Outputs are ranked
by the number of contexts two phrases share. Following Pas?ca
and Dienes (2005), we used the parameters LC = 3 and
MaxP = 4, while MinP , which was 1 in Pas?ca and Dienes
(2005), was set to 2 since our target was phrasal paraphrases.
Table 6: Evaluated paraphrase extraction methods.
pervised method for Japanese.
3.2.1 Experimental Setting
We extracted paraphrases from definition sen-
tences in Pos and those extracted by Proposeddef in
Section 3.1.3. First we coupled two definition sen-
tences whose defined term was the same. The num-
ber of definition pairs was 3,208,086 for English,
742,306 for Japanese, and 457,233 for Chinese.
Then we evaluated six methods in Table 6.5 All
the methods except P&D took the same definition
pairs as input, while P&D?s input was 10% of the
Web corpus. The input can be seen as the same for
all the methods, since the definition pairs were de-
rived from that 10% of the Web corpus. In our ex-
periments Exp1 and Exp2 below, all evaluation sam-
ples were shuffled so that human annotators could
not know which sample was from which method.
Annotators were the same as those who conducted
the evaluation in Section 3.1.3. Cohen?s kappa (Co-
hen, 1960) was 0.83 for English, 0.88 for Japanese,
5We filtered out phrase pairs in which one phrase contained a
named entity but the other did not contain the named entity from
the output of ProposedScore, Proposedlocal, SMT , and P&D,
since most of them were not paraphrases. We used Stanford
NER (Finkel et al, 2005) for English named entity recognition
(NER), KNP for Japanese NER, and BaseNER (Zhao and Kit,
2008) for Chinese NER. Hashisup and Hashiuns did the named
entity filtering of the same kind (footnote 3 of Hashimoto et al
(2011)), and thus we did not apply the filter to them any further.
and 0.85 for Chinese, all of which indicated reason-
ably good (Landis and Koch, 1977). We regarded a
candidate phrase pair as a paraphrase if both annota-
tors regarded it as a paraphrase.
Exp1 We compared the methods that take def-
inition pairs as input, i.e. ProposedScore, Pro-
posedlocal, Hashisup, Hashiuns, and SMT . We ran-
domly sampled 200 phrase pairs from the top 10,000
for each method for evaluation. The evaluation of
each candidate phrase pair (p1, p2) was based on
bidirectional checking of entailment relation, p1 ?
p2 and p2 ? p1, with p1 and p2 embedded in con-
texts, as Hashimoto et al (2011) did. Entailment
relation of both directions hold if (p1, p2) is a para-
phrase. We used definition pairs from which candi-
date phrase pairs were extracted as contexts.
Exp2 We compared ProposedScore and P&D.
Since P&D restricted its output to phrase pairs in
which each phrase consists of two to four words,
we restricted the output of ProposedScore to 2-to-4-
words phrase pairs, too. We randomly sampled 200
from the top 3,000 phrase pairs from each method
for evaluation, and the annotators checked entail-
ment relation of both directions between two phrases
using Web sentence pairs that contained the two
phrases as contexts.
3.2.2 Results
From Exp1, we obtained precision curves in the
upper half of Figure 4. The curves were drawn from
the 200 samples that were sorted in descending order
by their score, and we plotted a dot for every 5 sam-
ples. ProposedScore outperformed Proposedlocal for
the three languages, and thus globalSim was effec-
tive. ProposedScore outperformed Hashisup. How-
ever, we observed that ProposedScore acquired many
candidate phrase pairs (p1, p2) for which p1 and p2
consisted of the same content words like ?send a
postcard to the author? and ?send the author a post-
card,? while the other methods tended to acquire
more content word variations like ?have a chance?
and ?have an opportunity.? Then we evaluated all
the methods in terms of how many paraphrases with
content word variations were extracted. We ex-
tracted from the evaluation samples only candidate
phrase pairs whose Diff contained a content word
(content word variation pairs), to see how many
70
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(A) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(B) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(C) Top N (#Samples)
?Proposed_score?
?Proposed_local?
?SMT?
?Hashi_sup?
?Hashi_uns?
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(a) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(b) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(c) Top N (#Samples)
?Proposed_score_cwv?
?Proposed_local_cwv?
?SMT_cwv?
?Hashi_sup_cwv?
?Hashi_uns_cwv?
Figure 4: Precision curves of Exp1: English (A)(a), Chinese (B)(b), and Japanese (C)(c).
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200
Prec
ision
(A) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(B) Top N (#Samples)  0
 0.2
 0.4
 0.6
 0.8
 1
 0  50  100  150  200(C) Top N (#Samples)
?Proposed_score?
?Proposed_score_cwv?
?Pasca?
?Pasca_cwv?
Figure 5: Precision curves of Exp2: English (A), Chinese (B), and Japanese (C).
of them were paraphrases. The lower half of Fig-
ure 4 shows the results (curves labeled with cwv).
The number of samples for ProposedScore reduced
drastically compared to the others for English and
Japanese, though precision was kept at a high level.
It is due mainly to the globalSim; the Diff of the
non-content word variation pairs appears frequently
in paraphrase candidates, and thus their globalSim
scores are high.
From Exp2, precision curves in Figure 5 were
obtained. P&D acquired more content word varia-
tion pairs as the curves labeled by cwv indicates.
However, ProposedScore?s precision outperformed
P&D?s by a large margin for the three languages.
From all of these results, we conclude (1) that our
paraphrase extraction method outperforms unsuper-
vised methods for the three languages, (2) that glob-
alSim is effective, and (3) that our method is com-
parable to the state-of-the-art supervised method for
Japanese, though our method tends to extract fewer
content word variation pairs than the others.
Table 7 shows examples of English paraphrases
extracted by ProposedScore.
is based in Halifax = is headquartered in Halifax
used for treating HIV = used to treat HIV
is a rare form = is an uncommon type
is a set = is an unordered collection
has an important role = plays a key role
Table 7: Examples of extracted English paraphrases.
4 Conclusion
We proposed a minimally supervised method for
multilingual paraphrase extraction. Our experiments
showed that our paraphrase extraction method out-
performs unsupervised methods (Pas?ca and Dienes,
2005; Koehn et al, 2007; Hashimoto et al, 2011)
for English, Japanese, and Chinese, and is compara-
ble to the state-of-the-art language dependent super-
vised method for Japanese (Hashimoto et al, 2011).
71
References
Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,
Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya
Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka
Kidawara. 2010. Organizing information on the web
to support user judgments on information credibil-
ity. In Proceedings of 2010 4th International Uni-
versal Communication Symposium Proceedings (IUCS
2010), pages 122?129.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT-NAACL
2003, pages 16?23.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting of the ACL joint
with the 10th Meeting of the European Chapter of the
ACL (ACL/EACL 2001), pages 50?57.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning direc-
tionality of inference rules. In Proceedings of Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP2007), pages 161?170.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 196?205.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?09,
pages 570?579, Singapore. Association for Computa-
tional Linguistics.
Jacob Cohen. 1960. Coefficient of agreement for nom-
inal scales. In Educational and Psychological Mea-
surement, pages 37?46.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING 2004), pages 350?
356, Geneva, Switzerland, Aug 23?Aug 27.
Guozhu Dong and Jinyan Li. 1999. Efficient mining of
emerging patterns: discovering trends and differences.
In Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?99, pages 43?52, San Diego, California, United
States.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005),
pages 363?370.
Atsushi Fujita, Pierre Isabelle, and Roland Kuhn. 2012.
Enlarging paraphrase collections through generaliza-
tion and instantiation. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2012), pages 631?
642.
Maayan Geffet and Ido Dagan. 2005. The distributional
inclusion hypotheses and lexical entailment. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), pages
107?114.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun?ichi Kazama.
2009. Large-scale verb entailment acquisition from
the web. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2009), pages 1172?1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger,
Jun?ichi Kazama, and Sadao Kurohashi. 2011. Ex-
tracting paraphrases from definition sentences on the
web. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1087?1097, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 698?707, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
72
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 513?521, Suntec, Singapore,
August. Association for Computational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2009a.
Japanese morphological analyzer system ju-
man version 6.0 (in japanese). Kyoto University,
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara. 2009b.
Japanese syntax and case analyzer knp version 3.0
(in japanese). Kyoto University, http://nlp.ist.i.kyoto-
u.ac.jp/EN/index.php?KNP.
J. Richard Landis and Gary G. Koch. 1977. Measure-
ment of observer agreement for categorical data. Bio-
metrics, 33(1):159?174.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.
2003. Identifying synonyms among distributionally
similar words. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220, New
York City, New York.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1318?1327, Uppsala, Sweden, July. Association for
Computational Linguistics.
Roberto Navigli, Paola Velardi, and Juana Mar??a Ruiz-
Mart??nez. 2010. An annotated dataset for extracting
definitions and hypernyms from the web. In Proceed-
ings of LREC 2010, pages 3716?3722.
Marius Pas?ca and Pe?ter Dienes. 2005. Aligning needles
in a haystack: paraphrase acquisition across the web.
In Proceedings of the Second international joint con-
ference on Natural Language Processing, IJCNLP?05,
pages 119?130, Jeju Island, Korea.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to german. In Proceedings
of the ACL SIGDAT-Workshop, pages 47?50.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proceedings of the 2nd international Con-
ference on Human Language Technology Research
(HLT2002), pages 313?318.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary template. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING2008), pages 849?856.
Hai Zhao and Chunyu Kit. 2008. Unsupervised seg-
mentation helps supervised learning of character tag-
ging for word segmentation and named entity recog-
nition. In Proceedings of the Sixth SIGHAN Workshop
on Chinese Language Processing, pages 106?111, Hy-
derabad, India.
73
