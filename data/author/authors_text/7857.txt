An NP-Cluster Based Approach to Coreference Resolution
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Traditionally, coreference resolution is done
by mining the reference relationships be-
tween NP pairs. However, an individual NP
usually lacks adequate description informa-
tion of its referred entity. In this paper,
we propose a supervised learning-based ap-
proach which does coreference resolution by
exploring the relationships between NPs and
coreferential clusters. Compared with indi-
vidual NPs, coreferential clusters could pro-
vide richer information of the entities for bet-
ter rules learning and reference determina-
tion. The evaluation done on MEDLINE
data set shows that our approach outper-
forms the baseline NP-NP based approach
in both recall and precision.
1 Introduction
Coreference resolution is the process of linking
as a cluster1 multiple expressions which refer
to the same entities in a document. In recent
years, supervised machine learning approaches
have been applied to this problem and achieved
considerable success (e.g. Aone and Bennett
(1995); McCarthy and Lehnert (1995); Soon et
al. (2001); Ng and Cardie (2002b)). The main
idea of most supervised learning approaches is to
recast this task as a binary classification prob-
lem. Specifically, a classifier is learned and then
used to determine whether or not two NPs in a
document are co-referring. Clusters are formed
by linking coreferential NP pairs according to a
certain selection strategy. In this way, the identi-
fication of coreferential clusters in text is reduced
to the identification of coreferential NP pairs.
One problem of such reduction, however,
is that the individual NP usually lacks ade-
quate descriptive information of its referred en-
tity. Consequently, it is often difficult to judge
whether or not two NPs are talking about the
1In this paper the term ?cluster? can be interchange-
ably used as ?chain?, while the former better emphasizes
the equivalence property of coreference relationship.
same entity simply from the properties of the
pair alone. As an example, consider the pair of a
non-pronoun and its pronominal antecedent can-
didate. The pronoun itself gives few clues for the
reference determination. Using such NP pairs
would have a negative influence for rules learn-
ing and subsequent resolution. So far, several
efforts (Harabagiu et al, 2001; Ng and Cardie,
2002a; Ng and Cardie, 2002b) have attempted to
address this problem by discarding the ?hard?
pairs and select only those confident ones from
the NP-pair pool. Nevertheless, this eliminat-
ing strategy still can not guarantee that the NPs
in ?confident? pairs bear necessary description
information of their referents.
In this paper, we present a supervised
learning-based approach to coreference resolu-
tion. Rather than attempting to mine the ref-
erence relationships between NP pairs, our ap-
proach does resolution by determining the links
of NPs to the existing coreferential clusters. In
our approach, a classifier is trained on the in-
stances formed by an NP and one of its possi-
ble antecedent clusters, and then applied dur-
ing resolution to select the proper cluster for an
encountered NP to be linked. As a coreferen-
tial cluster offers richer information to describe
an entity than a single NP in the cluster, we
could expect that such an NP-Cluster framework
would enhance the resolution capability of the
system. Our experiments were done on the the
MEDLINE data set. Compared with the base-
line approach based on NP-NP framework, our
approach yields a recall improvement by 4.6%,
with still a precision gain by 1.3%. These results
indicate that the NP-Cluster based approach is
effective for the coreference resolution task.
The remainder of this paper is organized as
follows. Section 2 introduces as the baseline the
NP-NP based approach, while Section 3 presents
in details our NP-Cluster based approach. Sec-
tion 4 reports and discusses the experimental re-
sults. Section 5 describes related research work.
Finally, conclusion is given in Section 6.
2 Baseline: the NP-NP based
approach
2.1 Framework description
We built a baseline coreference resolution sys-
tem, which adopts the common NP-NP based
learning framework as employed in (Soon et al,
2001).
Each instance in this approach takes the form
of i{NPj , NPi}, which is associated with a fea-
ture vector consisting of 18 features (f1 ? f18) as
described in Table 2. Most of the features come
from Soon et al (2001)?s system. Inspired by the
work of (Strube et al, 2002) and (Yang et al,
2004), we use two features, StrSim1 (f17) and
StrSim2 (f18), to measure the string-matching
degree of NPj and NPi. Given the following sim-
ilarity function:
Str Simlarity(Str1, Str2) = 100? |Str1 ? Str2|Str1
StrSim1 and StrSim2 are computed
using Str Similarity(SNPj , SNPi) and
Str Similarity(SNPi , SNPj ), respectively. Here
SNP is the token list of NP, which is obtained
by applying word stemming, stopword removal
and acronym expansion to the original string as
described in Yang et al (2004)?s work.
During training, for each anaphor NPj in a
given text, a positive instance is generated by
pairing NPj with its closest antecedent. A set
of negative instances is also formed by NPj and
each NP occurring between NPj and NPi.
When the training instances are ready, a clas-
sifier is learned by C5.0 algorithm (Quinlan,
1993). During resolution, each encountered noun
phrase, NPj , is paired in turn with each preced-
ing noun phrase, NPi. For each pair, a test-
ing instance is created as during training, and
then presented to the decision tree, which re-
turns a confidence value (CF)2 indicating the
likelihood that NPi is coreferential to NPj . In
our study, two antecedent selection strategies,
Most Recent First (MRF) and Best First (BF),
are tried to link NPj to its a proper antecedent
with CF above a threshold (0.5). MRF (Soon
et al, 2001) selects the candidate closest to the
anaphor, while BF (Aone and Bennett, 1995; Ng
2The confidence value is obtained by using the
smoothed ratio p+1t+2 , where p is the number of positiveinstances and t is the total number of instances contained
in the corresponding leaf node.
and Cardie, 2002b) selects the candidate with
the maximal CF.
2.2 Limitation of the approach
Nevertheless, the problem of the NP-NP based
approach is that the individual NP usually lacks
adequate description information about its re-
ferred entity. Consequently, it is often difficult
to determine whether or not two NPs refer to
the same entity simply from the properties of
the pair. See the the text segment in Table 1,
for example,
[1 A mutant of [2 KBF1/p50] ], unable to
bind to DNA but able to form homo- or [3 het-
erodimers] , has been constructed.
[4 This protein] reduces or abolishes the DNA
binding activity of wild-type proteins of [5 the
same family ([6 KBF1/p50] , c- and v-rel)].
[7 This mutant] also functions in vivo as a
transacting dominant negative regulator:. . .
Table 1: An Example from the data set
In the above text, [1 A mutant of KBF1/p50],
[4 This protein] and [7 This mutant] are anno-
tated in the same coreferential cluster. Accord-
ing to the above framework, NP7 and its closest
antecedent, NP4, will form a positive instance.
Nevertheless, such an instance is not informa-
tive in that NP4 bears little information related
to the entity and thus provides few clues to ex-
plain its coreference relationship with NP7.
In fact, this relationship would be clear if [1 A
mutant of KBF1/p50], the antecedent of NP4,
is taken into consideration. NP1 gives a de-
tailed description of the entity. By comparing
the string of NP7 with this description, it is ap-
parent that NP7 belongs to the cluster of NP1,
and thus should be coreferential to NP4. This
suggests that we use the coreferential cluster,
instead of its single element, to resolve an NP
correctly. In our study, we propose an approach
which adopts an NP-Cluster based framework to
do resolution. The details of the approach are
given in the next section.
3 The NP-Cluster based approach
Similar to the baseline approach, our approach
also recasts coreference resolution as a binary
classification problem. The difference, however,
is that our approach aims to learn a classifier
which would select the most preferred cluster,
instead of the most preferred antecedent, for an
encountered NP in text. We will give the frame-
work of the approach, including the instance rep-
Features describing the relationships between NPj and NPi
1. DefNp 1 1 if NPj is a definite NP; else 0
2. DemoNP 1 1 if NPj starts with a demonstrative; else 0
3. IndefNP 1 1 if NPj is an indefinite NP; else 0
4. Pron 1 1 if NPj is a pronoun; else 0
5. ProperNP 1 1 if NPj is a proper NP; else 0
6. DefNP 2 1 if NPi is a definite NP; else 0
7. DemoNP 2 1 if NPi starts with a demonstrative; else 0
8. IndefNP 2 1 if NPi is an indefinite NP; else 0
9. Pron 2 1 if NPi is a pronoun; else 0
10. ProperNP 2 1 if NPi is a proper NP; else 0
11. Appositive 1 if NPi and NPj are in an appositive structure; else 0
12. NameAlias 1 if NPi and NPj are in an alias of the other; else 0
13. GenderAgree 1 if NPi and NPj agree in gender; else 0
14. NumAgree 1 if NPi and NPj agree in number; else 0
15. SemanticAgree 1 if NPi and NPj agree in semantic class; else 0
16. HeadStrMatch 1 if NPi and NPj contain the same head string; else 0
17. StrSim 1 The string similarity of NPj against NPi
18. StrSim 2 The string similarity of NPi against NPj
Features describing the relationships between NPj and cluster Ck
19. Cluster NumAgree 1 if Ck and NPj agree in number; else 0
20. Cluster GenAgree 1 if Ck and NPj agree in gender; else 0
21. Cluster SemAgree 1 if Ck and NPj agree in semantic class; else 0
22. Cluster Length The number of elements contained in Ck
23. Cluster StrSim The string similarity of NPj against Ck
24. Cluster StrLNPSim The string similarity of NPj against the longest NP in Ck
Table 2: The features in our coreference resolution system (Features 1 ? 18 are also used in the
baseline system using NP-NP based approach)
resentation, the training and the resolution pro-
cedures, in the following subsections.
3.1 Instance representation
An instance in our approach is composed of three
elements like below:
i{NPj , Ck, NPi}
where NPj , like the definition in the baseline,
is the noun phrase under consideration, while Ck
is an existing coreferential cluster. Each cluster
could be referred by a reference noun phrase NPi,
a certain element of the cluster. A cluster would
probably contain more than one reference NPs
and thus may have multiple associated instances.
For a training instance, the label is positive if
NPj is annotated as belonging to Ck, or negative
if otherwise.
In our system, each instance is represented as
a set of 24 features as shown in Table 2. The
features are supposed to capture the properties
of NPj and Ck as well as their relationships. In
the table we divide the features into two groups,
one describing NPj and NPi and the other de-
scribing NPj and Ck. For the former group, we
just use the same features set as in the baseline
system, while for the latter, we introduce 6 more
features:
Cluster NumAgree, Cluster GenAgree
and Cluster SemAgree: These three fea-
tures mark the compatibility of NPj and Ck
in number, gender and semantic agreement,
respectively. If NPj mismatches the agreement
with any element in Ck, the corresponding
feature is set to 0.
Cluster Length: The number of NPs in the
cluster Ck. This feature reflects the global
salience of an entity in the sense that the more
frequently an entity is mentioned, the more im-
portant it would probably be in text.
Cluster StrSim: This feature marks the string
similarity between NPj and Ck. Suppose
SNPj is the token set of NPj , we compute
the feature value using the similarity function
Str Similarity(SNPj , SCk), where
SCk =
?
NPi?Ck
SNPi
Cluster StrLNPSim: It marks the string
matching degree of NPj and the noun phrase
in Ck with the most number of tokens. The
intuition here is that the NP with the longest
string would probably bear richer description in-
formation of the referent than other elements in
the cluster. The feature is calculated using the
similarity function Str Similarity(SNPj , SNPk),
where
NPk = arg maxNPi?Ck |SNPi |
3.2 Training procedure
Given an annotated training document, we pro-
cess the noun phrases from beginning to end.
For each anaphoric noun phrase NPj , we consider
its preceding coreferential clusters from right to
left3. For each cluster, we create only one in-
stance by taking the last NP in the cluster as
the reference NP. The process will not terminate
until the cluster to which NPj belongs is found.
To make it clear, consider the example in Ta-
ble 1 again. For the noun phrase [7 This mu-
tant], the annotated preceding coreferential clus-
ters are:
C1: { . . . , NP2, NP6 }
C2: { . . . , NP5 }
C3: { NP1, NP4 }
C4: { . . . , NP3 }
Thus three training instances are generated:
i{ NP7, C1, NP6 }
i{ NP7, C2, NP5 }
i{ NP7, C3, NP4 }
Among them, the first two instances are la-
belled as negative while the last one is positive.
After the training instances are ready, we use
C5.0 learning algorithm to learn a decision tree
classifier as in the baseline approach.
3.3 Resolution procedure
The resolution procedure is the counterpart of
the training procedure. Given a testing docu-
ment, for each encountered noun phrase, NPj ,
we create a set of instances by pairing NPj with
each cluster found previously. The instances are
presented to the learned decision tree to judge
the likelihood that NPj is linked to a cluster.
The resolution algorithm is given in Figure 1.
As described in the algorithm, for each clus-
ter under consideration, we create multiple in-
stances by using every NP in the cluster as the
reference NP. The confidence value of the cluster
3We define the position of a cluster as the position of
the last NP in the cluster.
algorithm RESOLVE (a testing document d)
ClusterSet = ?;
//suppose d has N markable NPs;
for j = 1 to N
foreach cluster in ClusterSet
CFcluster = maxNPi?clusterCFi(NPj ,cluster,NPi)
select a proper cluster, BestCluster, according
to a ceterin cluster selection strategy;
if BestCluster != NULL
BestCluster = BestCluster ? {NPj};
else
//create a new cluster
NewCluster = { NPj };
ClusterSet = ClusterSet ? {NewCluster};
Figure 1: The clusters identification algorithm
is the maximal confidence value of its instances.
Similar to the baseline system, two cluster selec-
tion strategies, i.e. MRF and BF, could be ap-
plied to link NPj to a proper cluster. For MRF
strategy, NPj is linked to the closest cluster with
confidence value above 0.5, while for BF, it is
linked to the cluster with the maximal confidence
value (above 0.5).
3.4 Comparison of NP-NP and
NP-Cluster based approaches
As noted above, the idea of the NP-Cluster based
approach is different from the NP-NP based ap-
proach. However, due to the fact that in our
approach a cluster is processed based on its refer-
ence NPs, the framework of our approach could
be reduced to the NP-NP based framework if
the cluster-related features were removed. From
this point of view, this approach could be con-
sidered as an extension of the baseline approach
by applying additional cluster features as the
properties of NPi. These features provide richer
description information of the entity, and thus
make the coreference relationship between two
NPs more apparent. In this way, both rules
learning and coreference determination capabili-
ties of the original approach could be enhanced.
4 Evaluation
4.1 Data collection
Our coreference resolution system is a compo-
nent of our information extraction system in
biomedical domain. For this purpose, an anno-
tated coreference corpus have been built 4, which
4The annotation scheme and samples are avail-
able in http://nlp.i2r.a-star.edu.sg/resources/GENIA-
coreference
MRF BF
Experiments R P F R P F
Baseline 80.2 77.4 78.8 80.3 77.5 78.9
AllAnte 84.4 70.2 76.6 85.7 71.4 77.9
Our Approach 84.4 78.2 81.2 84.9 78.8 81.7
Table 3: The performance of different coreference resolution systems
consists of totally 228 MEDLINE abstracts se-
lected from the GENIA data set. The aver-
age length of the documents in collection is 244
words. One characteristic of the bio-literature
is that pronouns only occupy about 3% among
all the NPs. This ratio is quite low compared
to that in newswire domain (e.g. above 10% for
MUC data set).
A pipeline of NLP components is applied to
pre-process an input raw text. Among them,
NE recognition, part-of-speech tagging and text
chunking adopt the same HMM based engine
with error-driven learning capability (Zhou and
Su, 2002). The NE recognition component
trained on GENIA (Shen et al, 2003) can
recognize up to 23 common biomedical entity
types with an overall performance of 66.1 F-
measure (P=66.5% R=65.7%). In addition, to
remove the apparent non-anaphors (e.g., em-
bedded proper nouns) in advance, a heuristic-
based non-anaphoricity identification module is
applied, which successfully removes 50.0% non-
anaphors with a precision of 83.5% for our data
set.
4.2 Experiments and discussions
Our experiments were done on first 100 docu-
ments from the annotated corpus, among them
70 for training and the other 30 for testing.
Throughout these experiments, default learning
parameters were applied in the C5.0 algorithm.
The recall and precision were calculated auto-
matically according to the scoring scheme pro-
posed by Vilain et al (1995).
In Table 3 we compared the performance of
different coreference resolution systems. The
first line summarizes the results of the baseline
system using traditional NP-NP based approach
as described in Section 2. Using BF strategy,
Baseline obtains 80.3% recall and 77.5% preci-
sion. These results are better than the work by
Castano et al (2002) and Yang et al (2004),
which were also tested on the MEDLINE data
set and reported a F-measure of about 74% and
69%, respectively.
In the experiments, we evaluated another NP-
NP based system, AllAnte. It adopts a similar
learning framework as Baseline except that dur-
ing training it generates the positive instances by
paring an NP with all its antecedents instead of
only the closest one. The system attempts to use
such an instance selection strategy to incorpo-
rate the information from coreferential clusters.
But the results are nevertheless disappointing:
although this strategy boosts the recall by 5.4%,
the precision drops considerably by above 6% at
the same time. The overall F-measure is even
lower than the baseline systems.
The last line of Table 3 demonstrates the re-
sults of our NP-Cluster based approach. For BF
strategy, the system achieves 84.9% recall and
78.8% precision. As opposed to the baseline sys-
tem, the recall rises by 4.6% while the precision
still gains slightly by 1.3%. Overall, we observe
the increase of F-measure by 2.8%.
The results in Table 3 also indicate that the
BF strategy is superior to the MRF strategy.
A similar finding was also reported by Ng and
Cardie (2002b) in the MUC data set.
To gain insight into the difference in the per-
formance between our NP-Cluster based system
and the NP-NP based system, we compared the
decision trees generated in the two systems in
Figure 2. In both trees, the string-similarity
features occur on the top portion, which sup-
ports the arguments by (Strube et al, 2002)
and (Yang et al, 2004) that string-matching is a
crucial factor for NP coreference resolution. As
shown in the figure, the feature StrSim 1 in left
tree is completely replaced by the Cluster StrSim
and Cluster StrLNPSim in the right tree, which
means that matching the tokens with a cluster
is more reliable than with a single NP. More-
over, the cluster length will also be checked when
the NP under consideration has low similarity
against a cluster. These evidences prove that
the information from clusters is quite important
for the coreference resolution on the data set.
The decision tree visualizes the importance of
the features for a data set. However, the tree is
learned from the documents where coreferential
clusters are correctly annotated. During resolu-
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
: NameAlias = 0:
: :...Appositive = 0: 0 (13095/265)
: Appositive = 1: 1 (15/4)
HeadMatch = 1:
:...StrSim_1 > 71:
:...DemoNP_1 = 0: 1 (615/29)
: DemoNP_1 = 1:
: :...NumAgree = 0: 0 (5)
: NumAgree = 1: 1 (26)
StrSim_1 <= 71:
:...DemoNP_2 = 1: 1 (12/2)
DemoNP_2 = 0:
:...StrSim_2 <= 77: 0 (144/17)
StrSim_2 > 77:
:...StrSim_1 <= 33: 0 (42/11)
StrSim_1 > 33: 1 (38/11)
HeadMatch = 1:
:...Cluster_StrSim > 66: 1 (663/36)
: Cluster_StrSim <= 66:
: :...StrSim_2 <= 85: 0 (140/14)
: StrSim_2 > 85:
: :...Cluster_StrLNPSim > 50: 1 (16/1)
: Cluster_StrLNPSim <= 50:
: :...Cluster_Length <= 5: 0 (59/17)
: Cluster_Length > 5: 1 (4)
HeadMatch = 0:
:...NameAlias = 1: 1 (22/1)
NameAlias = 0:
:...Appositive = 1: 1 (15/4)
Appositive = 0:
:...StrSim_2 <= 54:
:..
StrSim_2 > 54:
:..
Figure 2: The resulting decision trees for the NP-NP and NP-Cluster based approaches
Features R P F
f1?21 80.3 77.5 78.9
f1?21, f22 84.1 74.4 79.0
f1?21, f23 84.7 78.8 81.6
f1?21, f24 84.3 78.0 81.0
f1?21, f23, f22 84.9 78.6 81.6
f1?21, f23, f24 84.9 78.9 81.8
f1?21, f23, f24, f22 84.9 78.8 81.7
Table 4: Performance using combined features
(fi refers to the i(th) feature listed in Table 2)
tion, unfortunately, the found clusters are usu-
ally not completely correct, and as a result the
features important in training data may not be
also helpful for testing data. Therefore, in the
experiments we were concerned about which fea-
tures really matter for the real coreference res-
olution. For this purpose, we tested our system
using different features and evaluated their per-
formance in Table 4. Here we just considered fea-
ture Cluster Length (f22), Cluster StrSim (f23)
and Cluster StrLNPSim (f24), as Figure 2 has
indicated that among the cluster-related features
only these three are possibly effective for resolu-
tion. Throughout the experiment, the Best-First
strategy was applied.
As illustrated in the table, we could observe
that:
1. Without the three features, the system is
equivalent to the baseline system in terms
of the same recall and precision.
2. Cluster StrSim (f23) is the most effective
as it contributes most to the system per-
formance. Simply using this feature boosts
the F-measure by 2.7%.
3. Cluster StrLNPSim (f24) is also effective by
improving the F-measure by 2.1% alone.
When combined with f23, it leads to the
best F-measure.
4. Cluster Length (f22) only brings 0.1% F-
measure improvement. It could barely
increase, or even worse, reduces the F-
measure when used together with the the
other two features.
5 Related work
To our knowledge, our work is the first
supervised-learning based attempt to do coref-
erence resolution by exploring the relationship
between an NP and coreferential clusters. In the
heuristic salience-based algorithm for pronoun
resolution, Lappin and Leass (1994) introduce
a procedure for identifying anaphorically linked
NP as a cluster for which a global salience value
is computed as the sum of the salience values of
its elements. Cardie and Wagstaff (1999) have
proposed an unsupervised approach which also
incorporates cluster information into considera-
tion. Their approach uses hard constraints to
preclude the link of an NP to a cluster mismatch-
ing the number, gender or semantic agreements,
while our approach takes these agreements to-
gether with other features (e.g. cluster-length,
string-matching degree,etc) as preference factors
for cluster selection. Besides, the idea of cluster-
ing can be seen in the research of cross-document
coreference, where NPs with high context simi-
larity would be chained together based on certain
clustering methods (Bagga and Biermann, 1998;
Gooi and Allan, 2004).
6 Conclusion
In this paper we have proposed a supervised
learning-based approach to coreference resolu-
tion. Rather than mining the coreferential re-
lationship between NP pairs as in conventional
approaches, our approach does resolution by ex-
ploring the relationships between an NP and the
coreferential clusters. Compared to individual
NPs, coreferential clusters provide more infor-
mation for rules learning and reference determi-
nation. In the paper, we first introduced the con-
ventional NP-NP based approach and analyzed
its limitation. Then we described in details the
framework of our NP-Cluster based approach,
including the instance representation, training
and resolution procedures. We evaluated our ap-
proach in the biomedical domain, and the experi-
mental results showed that our approach outper-
forms the NP-NP based approach in both recall
(4.6%) and precision (1.3%).
While our approach achieves better perfor-
mance, there is still room for further improve-
ment. For example, the approach just resolves
an NP using the cluster information available so
far. Nevertheless, the text after the NP would
probably give important supplementary infor-
mation of the clusters. The ignorance of such
information may affect the correct resolution of
the NP. In the future work, we plan to work out
more robust clustering algorithm to link an NP
to a globally best cluster.
References
C. Aone and S. W. Bennett. 1995. Evaluating
automated and manual acquistion of anaphora
resolution strategies. In Proceedings of the
33rd Annual Meeting of the Association for
Compuational Linguistics, pages 122?129.
A. Bagga and A. Biermann. 1998. Entity-based
cross document coreferencing using the vector
space model. In Proceedings of the 36th An-
nual Meeting of the Association for Computa-
tional Linguisticsthe 17th International Con-
ference on Computational Linguistics, pages
79?85.
C. Cardie and K. Wagstaff. 1999. Noun phrase
coreference as clustering. In Proceedings of
the Joint Conference on Empirical Methods in
NLP and Very Large Corpora.
J. Castano, J. Zhang, and J. Pustejovsky. 2002.
Anaphora resolution in biomedical literature.
In International Symposium on Reference Res-
olution, Alicante, Spain.
C. Gooi and J. Allan. 2004. Cross-document
coreference on a large scale corpus. In Pro-
ceedings of 2004 Human Language Technology
conference / North American chapter of the
Association for Computational Linguistics an-
nual meeting.
S. Harabagiu, R. Bunescu, and S. Maiorano.
2001. Text knowledge mining for coreference
resolution. In Proceedings of the 2nd An-
nual Meeting of the North America Chapter of
the Association for Compuational Linguistics,
pages 55?62.
S. Lappin and H. Leass. 1994. An algorithm
for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):525?561.
J. McCarthy and Q. Lehnert. 1995. Using de-
cision trees for coreference resolution. In Pro-
ceedings of the 14th International Conference
on Artificial Intelligences, pages 1050?1055.
V. Ng and C. Cardie. 2002a. Combining sam-
ple selection and error-driven pruning for ma-
chine learning of coreference rules. In Proceed-
ings of the conference on Empirical Methods
in Natural Language Processing, pages 55?62,
Philadelphia.
V. Ng and C. Cardie. 2002b. Improving ma-
chine learning approaches to coreference res-
olution. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics, pages 104?111, Philadelphia.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publishers,
San Francisco, CA.
D. Shen, J. Zhang, G. Zhou, J. Su, and
C. Tan. 2003. Effective adaptation of hid-
den markov model-based named-entity recog-
nizer for biomedical domain. In Proceedings of
ACL03 Workshop on Natural Language Pro-
cessing in Biomedicine, Japan.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference resolu-
tion of noun phrases. Computational Linguis-
tics, 27(4):521?544.
M. Strube, S. Rapp, and C. Muller. 2002. The
influence of minimum edit distance on refer-
ence resolution. In Proceedings of the Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 312?319, Philadel-
phia.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan. 2004. Im-
proving noun phrase coreference resolution by
matching strings. In Proceedings of the 1st In-
ternational Joint Conference on Natural Lan-
guage Processing, Hainan.
G. Zhou and J. Su. 2002. Named Entity recog-
nition using a HMM-based chunk tagger. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, Philadelphia.
A High-Performance Coreference Resolution System  
using a Constraint-based Multi-Agent Strategy 
 
ZHOU GuoDong            SU Jian  
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
Email: zhougd@i2r.a-star.edu.sg 
 
 
Abstract 
This paper presents a constraint-based multi-
agent strategy to coreference resolution of 
general noun phrases in unrestricted English 
text. For a given anaphor and all the preceding 
referring expressions as the antecedent 
candidates, a common constraint agent is first 
presented to filter out invalid antecedent 
candidates using various kinds of general 
knowledge.  Then, according to the type of the 
anaphor, a special constraint agent is proposed to 
filter out more invalid antecedent candidates 
using constraints which are derived from various 
kinds of special knowledge. Finally, a simple 
preference agent is used to choose an antecedent 
for the anaphor form the remaining antecedent 
candidates, based on the proximity principle. 
One interesting observation is that the most 
recent antecedent of an anaphor in the 
coreferential chain is sometimes indirectly 
linked to the anaphor via some other antecedents 
in the chain.  In this case, we find that the most 
recent antecedent always contains little 
information to directly determine the coreference 
relationship with the anaphor. Therefore, for a 
given anaphor, the corresponding special 
constraint agent can always safely filter out these 
less informative antecedent candidates. In this 
way, rather than finding the most recent 
antecedent for an anaphor, our system tries to 
find the most direct and informative antecedent. 
Evaluation shows that our system achieves 
Precision / Recall / F-measures of 84.7% / 
65.8% / 73.9 and 82.8% / 55.7% / 66.5 on MUC-
6 and MUC-7 English coreference tasks 
respectively. This means that our system 
achieves significantly better precision rates by 
about 8 percent over the best-reported systems 
while keeping recall rates.   
1 Introduction 
Coreference accounts for cohesion in texts. 
Especially, a coreference denotes an identity of 
reference and holds between two expressions, 
which can be named entities, definite noun 
phrases, pronouns and so on. Coreference 
resolution is the process of determining whether 
two referring expressions refer to the same entity 
in the world. The ability to link referring 
expressions both within and across the sentence is 
critical to discourse and language understanding in 
general. For example, coreference resolution is a 
key task in natural language interfaces, machine 
translation, text summarization, information 
extraction and question answering. In particular, 
information extraction systems like those built in 
the DARPA Message Understanding Conferences 
(MUC) have revealed that coreference resolution is 
such a crucial component of an information 
extraction system that a separate coreference task 
has been defined and evaluated in MUC-6 (1995) 
and MUC-7 (1998).  
There is a long tradition of work on 
coreference resolution within computational 
linguistics. Many of the earlier works in 
coreference resolution heavily exploited domain 
and linguistic knowledge (Carter 1987; Rich and 
LuperFoy 1988; Carbonell and Brown 1988). 
However, the pressing need for the development of 
robust and inexpensive solutions encouraged the 
drive toward knowledge-poor strategies (Dagan 
and Itai 1990; Lappin and Leass 1994; Mitkov 
1998; Soon, Ng and Lim 2001; Ng and Cardie 
2002), which was further motivated by the 
emergence of cheaper and more reliable corpus-
based NLP tools such as part-of-speech taggers 
and shallow parsers alongside the increasing 
availability of corpora and other resources (e.g. 
ontology).  
Approaches to coreference resolution usually 
rely on a set of factors which include gender and 
number agreements, c-command constraints, 
semantic consistency, syntactic parallelism, 
semantic parallelism, salience, proximity, etc. 
These factors can be either ?constraints? which 
discard invalid ones from the set of possible 
candidates (such as gender and number 
agreements, c-command constraints, semantic 
consistency), or ?preferences? which gives more 
preference to certain candidates and less to others 
(such as syntactic parallelism, semantic 
parallelism, salience, proximity). While a number 
of approaches use a similar set of factors, the 
computational strategies (the way antecedents are 
determined, i.e. the algorithm and formula for 
assigning antecedents) may differ, i.e. from simple 
co-occurrence rules (Dagan and Itai 1990) to 
decision trees (Soon, Ng and Lim 2001; Ng and 
Cardie 2002) to pattern induced rules (Ng and 
Cardie 2002) to centering algorithms (Grosz and 
Sidner 1986; Brennan, Friedman and Pollard 1987; 
Strube 1998; Tetreault 2001). 
This paper proposes a simple constraint-based 
multi-agent system to coreference resolution of 
general noun phrases in unrestricted English text. 
For a given anaphor and all the preceding referring 
expressions as the antecedent candidates, a 
common constraint agent is first presented to filter 
out invalid antecedent candidates using various 
kinds of general knowledge.  Then, according to 
the type of the anaphor, a special constraint agent 
is proposed to filter out more invalid antecedent 
candidates using constraints which are derived 
from various kinds of special knowledge. Finally, a 
simple preference agent is used to choose an 
antecedent for the anaphor form the remaining 
antecedent candidates, based on the proximity 
principle. One interesting observation is that the 
most recent antecedent of an anaphor in the 
coreferential chain is sometimes indirectly linked 
to the anaphor via some other antecedents in the 
chain.  In this case, we find that the most recent 
antecedent always contains little information to 
directly determine the coreference relationship 
with the anaphor. Therefore, for a given anaphor, 
the corresponding special constraint agent can 
always safely filter out these less informative 
antecedent candidates. In this way, rather than 
finding the most recent antecedent for an anaphor, 
our system tries to find the most direct and 
informative antecedent. 
In this paper, we focus on the task of 
determining coreference relations as defined in 
MUC-6 (1995) and MUC-7 (1998). In order to 
evaluate the performance of our approach on 
coreference resolution, we utilize the annotated 
corpus and the scoring programs from MUC-6 and 
MUC-7. For MUC-6, 30 dry-run documents 
annotated with coreference information are used as 
the training data. There are also 30 annotated 
training documents from MUC-7. The total size of 
30 training documents is close 12,400 words for 
MUC-6 and 19,000 for MUC-7. For testing, we 
utilize the 30 standard test documents from MUC-
6 and the 20 standard test documents from MUC-7. 
The layout of this paper is as follows: in 
Section 2, we briefly describe the preprocessing: 
determination of referring expressions. In Section 
3, we differentiate coreference types and discuss 
how to restrict possible types of direct and 
informative antecedent candidates according to 
anaphor types. In Section 4, we describe the 
constraint-based multi-agent system. In Section 5, 
we evaluate the multi-agent algorithm. Finally, we 
present our conclusions. 
2 Preprocessing: Determination of 
Referring Expressions 
The prerequisite for automatic coreference 
resolution is to obtain possible referring 
expressions in an input document. In our system, 
the possible referring expressions are determined 
by a pipeline of NLP components: 
? Tokenization and sentence segmentation 
? Named entity recognition 
? Part-of-speech tagging 
? Noun phrase chunking 
Among them, named entity recognition, part-
of-speech tagging and noun phrase chunking apply 
the same Hidden Markov Model (HMM) based 
engine with error-driven learning capability (Zhou 
and Su 2000). The named entity recognition 
component (Zhou and Su 2002) recognizes various 
types of MUC-style named entities, that is, 
organization, location, person, date, time, money 
and percentage. The HMM-based noun phrase 
chunking component (Zhou and Su 2000) 
determines various noun phrases based on the 
results of named entity recognition and part-of-
speech tagging. 
3 Coreference Types 
Since coreference is a symmetrical and transitive 
relation, it leads to a simple partitioning of a set of 
referring expressions and each partition forms a 
coreference chain. Although any two referring 
expressions in the coreference chain is 
coreferential, some of conference pairs may be 
direct while others may be indirect since they only 
become conferential via other referring expressions 
in the same coreference chain. This indicates that 
the most recent antecedent of an anaphor in the 
coreferential chain is sometimes indirectly linked 
to the anaphor via some other antecedents in the 
chain. In these indirect cases, we find that the most 
recent antecedent always contains little 
information to directly determine the coreference 
relationship with the anaphor. Generally, direct and 
informative coreference pairs are much easier to 
resolve than indirect and less informative ones. In 
the following example1,  
Microsoft Corp. (i) announced its (i) new CEO 
yesterday. Microsoft (i) said ? 
                                                     
1 The italic markables with the same identification 
symbol are coreferential. 
?Microsoft Corp.?, ?its? and ?Microsoft? form a 
coreference chain. Among the three coreference 
pairs in the chain,  
1) The coreference pair between ?Microsoft 
Corp.? and ?Microsoft? is direct.  
2) The coreference pair between ?Microsoft 
Corp.? and ?its? is direct. 
3) The coreference pair between ?its? and 
?Microsoft? is indirect. This coreference pair 
only becomes coreferential via another 
referring expression ?Microsoft Corp.? Direct 
resolution of this coreference pair is error-
prone and not necessary since it can be 
indirectly linked by the other two coreference 
pairs in the coreference chain.  
Therefore, for a given anaphor, we can always 
safely filter out these less informative antecedent 
candidates. In this way, rather than finding the 
most recent antecedent for an anaphor, our system 
tries to find the most direct and informative 
antecedent. This also suggests that we can classify 
coreference types according to the types of 
anaphors and restrict the possible types of 
antecedent candidates for a given anaphor type as 
follows: 
? Name alias coreference 
This is the most widespread type of coreference 
which is realised by the name alias phenomenon. 
The success of name alias coreference resolution is 
largely conditional on success at determining when 
one referring expression is a name alias of another 
referring expression. Here, the direct antecedent 
candidate of a named entity anaphor can only be 
the type of named entity. For example, 
Microsoft Corp. (i) announced its new CEO 
yesterday. Microsoft (i) said ? 
? Apposition coreference 
This is the easiest type of coreference. A typical 
use of an appositional noun phrase is to provide an 
alternative description for a named entity. For 
example 
Julius Caesar (i), the well-known emperor (i), 
was born in 100 BC. 
? Predicate nominal coreference 
Predicate nominal is typically coreferential with 
the subject. For example, 
George W. Bush (i) is the president of the 
United States (i). 
? Pronominal coreference 
This is the second widespread type of coreference 
which is realised by pronouns. Pronominal 
coreference has been widely studied in literature of 
traditional anaphora resolution. The direct 
antecedent candidate of a pronoun anaphor can be 
any type of referring expressions. For example, 
Computational linguistics (i) from different 
countries attended the tutorial. They (i) took 
extensive note. 
? Definite noun phrase coreference 
This is the third widespread type of coreference 
which is realised by definite noun phrases. It has 
also been widely studied in the literature of 
traditional anaphora resolution. A typical case of 
definite noun phrase coreference is when the 
antecedent is referred by a definite noun phrase 
anaphor representing either same concept 
(repetition) or semantically close concept (e.g. 
synonyms, super-ordinates). The direct antecedent 
candidate of a definite noun phrase anaphor can be 
any type of referring expressions except pronouns. 
For example, 
Computational linguistics (i) from different 
countries attended the tutorial. The 
participants (i) took extensive note. 
? Demonstrative noun phrase coreference 
This type of coreference is not widespread. Similar 
to that of definite noun phrase coreference, the 
direct antecedent candidate of a demonstrative 
noun phrase anaphor can be any type of referring 
expressions except pronouns. For example, 
Boorda wants to limit the total number of 
sailors on the arsenal ship (i) to between 50 
and 60. Currently, this ship (i) has about 90 
sailors. 
? Bare noun phrase coreference 
The direct antecedent candidate of a bare noun 
phrase anaphor can be any type of referring 
expressions except pronouns. For example, 
The price of aluminium (i) siding has steadily 
increased, as the market for aluminium (i) 
reacts to the strike in Chile. 
4 Constraint-based Multi-Agent System 
for Coreference Resolution 
In accordance with the above differentiation of 
coreference types according to the anaphor types, a 
constraint-based multi-agent system is developed.  
4.1 Common Constraint Agent 
For all coreference types described in Section 3, a 
common constraint agent is applied first using 
following constraints:  
Morphological agreements 
These constraints require that an anaphor and its 
antecedent candidate should agree in gender and 
number. These kinds of morphological agreements 
has been widely used in the literature of anaphora 
resolution 
Semantic consistency 
This constraint stipulates that the anaphor and its 
antecedent candidate must be consistent in 
semantics. For example, the anaphor and its 
antecedent candidate should contain the same 
sense or the anaphor contains a sense which is 
parental to the antecedent candidate. In this paper, 
WordNet (Miller 1990) is used for semantic 
consistency check. 
For example, 
IBM (i) announced its new CEO yesterday. 
The company (i) said ? 
4.2 Special Constraint Agents 
For each coreference type described in Section 3, a 
special constraint agent is applied next using some 
heuristic rules mainly based on the accessibility 
space, which is learnt from the training data as 
follows:  
For a given coreference type and a given valid 
antecedent type, all the anaphors of the given 
coreference type are identified first from left to 
right as they appear in the sentences. For each 
anaphor, its antecedent is then determined using 
the principle of proximity. If the most recent 
antecedent candidate has the given antecedent 
type, meet the morphological agreements and 
semantic consistency and is in the same 
coreference chain as the anaphor, this coreference 
pair is counted as a correct instance for the given 
conference type and the given antecedent type. 
Otherwise, it is counted as an error instance. In this 
way, the precision rates of the coreference type 
over different valid antecedent types and different 
accessibility spaces are computed as the percentage 
of the correct instances among all the correct and 
error instances. Finally, the accessibility space for 
a given coreference type and a given antecedent 
type is decided using a precision rate threshold 
(e.g. 95%).  
? Agent for name alias coreference 
A named entity is co-referred with another named 
entity when the formal is a name alias of the latter. 
This type of coreference has an accessibility space 
of the whole document. In this paper, it is tackled 
by a named entity recognition component, as in 
Zhou and Su (2002), using the following name 
alias algorithm in the ascending order of 
complexity: 
1) The simplest case is to recognize full identity 
of strings. This applies to all types of entity 
names. 
2) The next simplest case is to recognize the 
various forms of location names. Normally, 
various acronyms are applied, e.g. ?NY? vs. 
?New York? and ?N.Y.? vs. ?New York?. 
Sometime, partial mention is also applied, e.g. 
?Washington? vs. ?Washington D.C.?. 
3) The third case is to recognize the various 
forms of personal proper names. Thus an 
article on Microsoft may include ?Bill Gates?, 
?Bill? and ?Mr. Gates?. Normally, the full 
personal name is mentioned first in a document 
and later mention of the same person is 
replaced by various short forms such as 
acronym, the last name and, to a less extent, 
the first name, of the full person name. 
4) The most difficult case is to recognize the 
various forms of organizational names. For 
various forms of company names, consider a) 
?International Business Machines Corp.?, 
?International Business Machines? and ?IBM?; 
b) ?Atlantic Richfield Company? and 
?ARCO?. Normally, various abbreviation 
forms (e.g. contractions and acronym) and 
dropping of company suffix are applied. For 
various forms of other organizational names, 
consider a) ?National University of 
Singapore?, ?National Univ. of Singapore? and 
?NUS?; b) ?Ministry of Education? and 
?MOE?. Normally, acronyms and 
abbreviations are applied. 
? Agent for apposition coreference 
If the anaphor is in apposition to the antecedent 
candidate, they are coreferential. The MUC-6 and 
MUC-7 coreference task definitions are slightly 
different. In MUC-6, the appositive should be a 
definite noun phrase while both indefinite and 
definite noun phrases are acceptable in MUC-7.  
? Agent for predicate nominal coreference 
If the anaphor is the predicate nominal and the 
antecedent candidate is the subject, they are 
coreferential. This agent is still under construction.  
? Agent for pronominal coreference 
This agent is applied to the most widely studied 
coreference: pronominal coreference. 6 heuristic 
rules are learnt and applied depending on the 
accessibility space and the types of the antecedent 
candidates: 
1) If the anaphor is a person pronoun and the 
antecedent candidate is a person named entity, 
they are coreferential over the whole 
document. 
2) If the anaphor is a neuter pronoun and the 
antecedent candidate is an organization named 
entity, they are coreferential when they are in 
the same sentence. 
3) If the anaphor is a neuter plural pronoun and 
the antecedent candidate is a plural noun 
phrase, they are coreferential over the whole 
document. 
4) If both the anaphor and the antecedent 
candidate are third person pronouns, they are 
coreferential over the whole document. 
5) If both the anaphor and the antecedent 
candidate are first or second person pronouns, 
they are coreferential when they are in the 
same paragraph. 
6) If both the anaphor and the antecedent 
candidate are neuter pronouns, they are 
coreferential when they are in the same 
paragraph or the antecedent candidate is in the 
previous paragraph of the anaphor. 
? Agent for definite noun phrase coreference 
The agent for definite noun phrase coreference is 
mainly based on the accessibility space. This agent 
is based on the following 3 heuristic rules: 
1) The definite noun phrase will be coreferential 
with a named entity if they are in same 
paragraph or the entity name is in the previous 
paragraph of the definite noun phrase. 
2) The definite noun phrase will be coreferential 
with a named entity if the head word of the 
definite noun phrase is only modified by the 
determiner ?the?. That is, the definite noun 
phrase is of type ?the HEADWORD?, e.g. ?the 
company?. 
3) The definite noun phrase will be coreferential 
with a definite/demonstrative/indefinite noun 
phrase if they string-match2. 
? Agent for demonstrative noun phrase 
coreference 
The agent for demonstrative noun phrase 
coreference is similar to the agent for definite noun 
phrase coreference except that the anaphor is a 
demonstrative noun phrase. 
? Agent for base noun phrase coreference 
This is the most complicated and confusing 
coreference in MUC coreference task definitions. 
Although this type of coreference occupies a large 
portion, it is hard to find heuristic rules to deal 
with it. In our system, only one heuristic rule is 
applied: If the anaphor and the antecedent 
candidate string-match and include at least two 
words except the determiner, they are coreferential 
over the whole document.  
                                                     
2 The determiners, e.g. ?a?, ?an? and ?the?, are removed 
from the strings before comparison. Therefore, ?the 
company? string-matches ?a company?. 
4.3 Common Preference Agent 
For a given anaphor, invalid antecedents are first 
filtered out using the above common constraint 
agent and the special constraint agent. Then, the 
strategy has to choose which of the remaining 
candidates, if any, is the most likely antecedent 
candidate. In our strategy, this is done through a 
common preference agent based on the principle of 
proximity. That is, our common preference agent 
takes advantages of the relative locations of the 
remaining antecedent candidates in the text. 
Among the antecedent candidates: 
1) First it looks for those occurring earlier in the 
current sentence, preferring the one that occurs 
earliest in the natural left-to-right order. 
2) If there are no antecedent candidates occurring 
earlier in the current sentence, look to those 
occurring in the immediately preceding 
sentence of the same paragraph, again 
preferring the one that occurs earliest in that 
sentence in left-to-right order. 
3) If nothing comes up, look back at those 
occurring in the earlier sentences of the same 
paragraph, moving back a sentence at a time, 
but now, within a given sentence preferring the 
most rightward candidate that occurs later in 
the sentence. 
4) Finally, if the scope extends back beyond a 
paragraph boundary, it looks to those that 
occur in the sentences of the preceding 
paragraph, again preferring later to earlier 
occurrences. 
4.4 Multi-Agent Algorithm 
The coreference resolution algorithm is 
implemented based on the previous multi-agents. 
First, all the anaphors are identified from left to 
right as they appear in the sentences. Then, for a 
given anaphor,  
1) All the referring expressions occurred before 
the anaphor are identified as antecedent 
candidates. 
2) The common constraint agent is applied to 
filter out the invalid antecedent candidates 
using various general constraints, such as 
morphological agreements and semantic 
consistency constraints. 
3) The corresponding special constraint agent (if 
exists) is recalled to first filter out indirect and 
less informative antecedent candidates and 
then check the validity of the remaining 
antecedent candidates by using some heuristic 
rules. In this way, more invalid antecedent 
candidates are discarded using various special 
constraints, such as the accessibility space. 
4) The antecedent is chosen from the remaining 
antecedent candidates, if any, using the 
common preference agent based on the 
principle of proximity. 
5 Experimentation 
Table 1 shows the performance of our constraint-
based multi-agent system on MUC-6 and MUC-7 
standard test data using the standard MUC 
evaluation programs while Table 2 gives the 
comparisons of our system with others using the 
same MUC test data and the same MUC evaluation 
programs. Here, the precision (P) measures the 
number of correct coreference pairs in the answer 
file over the total number of coreference pairs in 
the answer file and the recall (R) measures the 
number of correct coreference pairs in the answer 
file over the total number of coreference pairs in 
the key file while F-measure is the weighted 
harmonic mean of precision and recall: 
PR
RPF +
+= 2
2 )1(
?
?
 with =1.  2?
Table 1: Results of our baseline multi-agent coreference resolution system on MUC-6 and MUC-7 
MUC-6 MUC-7 Performance 
R P F R P F 
Overall 65.8 84.7 73.9 55.7 82.8 66.5 
? Agent for name alias coreference 32.7 (35) 92.3 - 33.6 (36) 89.0 - 
? Agent for apposition coreference  4.3 (5) 95.5 -   2.6 (3) 84.6 - 
? Agent for predicate nominal coreference3 - (2) - - - (3) - - 
? Agent for pronominal coreference 18.6 (22) 77.5 - 10.8 (16) 72.3 - 
? Agent for definite noun phrase coreference  9.4 (15) 80.0 -   7.0 (20) 85.0 - 
? Agent for demonstrative noun phrase coreference  0.1 (2) 50.0 -   0.2 (2) 66.7 - 
? Agent for bare noun phrase coreference  1.9 (19) 63.0    1.7 (20) 61.1 - 
 
Table 2: Comparison of our system with the best-reported systems on MUC-6 and MUC-7 
MUC-6 MUC-7 Performance Comparison 
R P F R P F 
Ours 65.8 84.7 73.9 55.7 82.8 66.5 
Ng and Cardie 2002 (C4.5) 64.1 74.9 69.1 57.4 70.8 63.4 
Ng and Cardie 2002 (RIPPER) 64.2 78.0 70.4 55.7  72.8 63.1 
 
 
Table 1 shows that our system achieves F-
measures of 73.9 and 66.5 on MUC-6 and MUC-7 
standard test data, respectively. The figures outside 
the parentheses show the contributions of various 
agents to the overall recall while the figures inside 
the parentheses show the frequency distribution of 
various coreference types in the answer file. It 
shows that the performance difference between 
MUC-6 and MUC-7 mainly comes from the 
significant distribution variation of pronominal 
coreference. It also shows that there are much 
room for improvement, especially for the types of 
pronominal coreference and definite noun pronoun 
resolution. Table 2 shows that our system achieves 
significantly better F-measures by 3.1~4.8 percent 
over the best-reported systems (Ng and Cardie 
2002). Most of the contributions come form 
precision gains. Our system achieves significantly 
better precision rates by 6.7~10.0 percent over the 
best-reported systems (Ng and Cardie 2002) while 
keeping recall rates. One reason behind such high 
performance is the restriction of indirect and less 
informative antecedent candidates according to the 
type of the anaphor. Another reason is 
differentiation of various types of coreference and 
the use of multi-agents. In this way, various types 
of coreference are dealt with effectively by 
different agents according to their characteristics. 
The recall difference between our system and the 
RIPPER system in (Ng and Cardie 2002) maybe 
come from the predicate nominal coreference, 
which can be easily resolved using a machine 
learning algorithm, e.g. (Cohen 1995). Completion 
of the agent for predicate nominal coreference can 
easily fill the difference. 
6 Conclusions 
This paper presents a constraint-based multi-agent 
strategy to coreference resolution of general noun 
phrases in unrestricted English text. 
The first contribution of this paper comes from 
the high performance of our system and its easy 
                                                     
3 The agent for predicate nominal coreference is still under construction. 
implementation. The second contribution is to 
filter out indirect and less informative antecedent 
candidates according to the anaphor type. The third 
contribution is the differentiation of various 
coreference types according to the anaphor types 
and the use of multi-agents.  
Future work includes: 
? The exploration of new constraints to improve 
the precision and new coreference types to 
increase the recall. 
? The problem of type coercion or metonymy 
which is a general problem and accounts for 
much of the overall missing recall. 
? The problem of cataphora, which is not 
handled in the current mechanism. 
References 
Brennan S. E. Friedman M. W. and Pollard C. J. 
1987. A centering approach to pronouns. 
Proceedings of the 25th Annual Meeting of the 
Association for Computational Linguistics 
(ACL?1987), pages 155-162. 
Carbonell J. and Brown R. 1988. Anaphora 
resolution: a multi-strategy approach. 
Proceedings of the 12th International Conference 
on Computational Linguistics (COLING?1988), 
pages 96-101, Budapest, Hungary. 
Carter D. M. 1987. Interpreting Anaphors in 
Natural Language Texts. Ellis Horwood, 
Chichester, UK. 
Cohen W. 1995. Fast effective rule induction. 
Proceedings of the Twelfth International 
Conference on Machine Learning (ICML?1995). 
pages 115-123. Tahoe City, CA.   
Dagan I. and Itai A. 1990. Automatic processing of 
large corpora for the resolution of anaphora 
references. Proceedings of the 13th International 
Conference on Computational Linguistics 
(COLING?1990), pages 1-3, Helsinki, Finland. 
Grosz B. J. and Sidner C. L. 1986. Attention, 
intentions and the structure of discourse. 
Computational Linguistics, 12(3):175-204. 
Lappin S. and Leass H. 1994. An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics. 20(4):535-561. 
Miller G.A. 1990. WordNet: An online lexical 
database. International Journal of Lexicography. 
3(4):235-312. 
Mitkov R. 1998. Robust pronoun resolution with 
limited knowledge. Proceedings of the 36th 
Annual Meeting for Computational Linguistics 
and the 17th International Conference on 
Computational Linguistics 
(COLING/ACL?1998), pages 869-875, 
Montreal, Canada. 
MUC-6. 1995. Proceedings of the 6th Message 
Understanding Conference (MUC-6). Morgan 
Kaufmann, San Francisco, CA. 
MUC-7. 1998. Proceedings of the 7th Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Mateo, CA. 
Ng V. and Cardie C. 2002. Improving machine 
learning approaches to coreference resolution. 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL?2002), pages 104-111, Philadelphia, Penn. 
Rich E. and LuperFoy S. 1988. An architecture for 
anaphora resolution. Proceedings of the 2nd 
Conference on Applied Natural Language 
Processing (ANLP?1988), pages 18-24, Austin, 
TX. 
Soon W. M.., Ng H. T. and Lim C. Y. 2001. A 
machine learning approach to coreference 
resolution of noun phrases. Computational 
Linguistics, 27(4):521-544. 
Strube M. 1998. Never look back: An alternative to 
centering. Proceedings of the 36th Annual 
Meeting of the Association for Computational 
Linguistics and the 17th International Conference 
on Computational Linguistics, pages 1251-1257. 
Tetreault J. R. 2001. A corpus-based evaluation of 
centering and pronoun resolution. Computation 
Linguistics, 27(4):507-520.  
Zhou G. D. and Su Jian, 2000. Error-driven HMM-
based chunk tagger with context-dependent 
lexicon. Proceedings of the Joint Conference on 
Empirical Methods on Natural Language 
Processing and Very Large Corpus (EMNLP/ 
VLC'2000). Hong Kong.  
Zhou G. D. and Su Jian. 2002. Named Entity 
Recognition Using a HMM-based Chunk 
Tagger, Proceedings of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?2002). Philadelphia.  
Direct Orthographical Mapping for Machine Transliteration  
ZHANG Min          LI Haizhou        SU Jian 
 
Institute for Infocomm Research  
21 Heng Mui Keng Terrace, Singapore 11961 
{mzhang, hli, sujian}@i2r.a-star.edu.sg 
 
Abstract 
Machine transliteration/back-transliteration plays 
an important role in many multilingual speech and 
language applications. In this paper, a novel 
framework for machine transliteration/back-
transliteration that allows us to carry out direct 
orthographical mapping (DOM) between two 
different languages is presented. Under this 
framework, a joint source-channel transliteration 
model, also called n-gram transliteration model (n-
gram TM), is further proposed to model the 
transliteration process. We evaluate the proposed 
methods through several transliteration/back-
transliteration experiments for English/Chinese and 
English/Japanese language pairs. Our study reveals 
that the proposed method not only reduces an 
extensive system development effort but also 
improves the transliteration accuracy significantly.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another language 
with approximate phonetic equivalents. The 
phonetic translation from the native language to 
foreign language is defined as transliteration; 
conversely, the process of recalling a word in 
native language from a transliteration is defined as 
back-transliteration. For example, English name 
?Smith? and ????  (pinyin 1 : Shi-Mi-Si)? in 
Chinese form a pair of transliteration and back-
transliteration. In many natural language 
processing tasks, such as multilingual named entity 
and term processing, machine translation, corpus 
alignment, cross lingual information retrieval and 
automatic bilingual dictionary compilation, 
automatic name transliteration has become an 
indispensable component. 
Recent efforts are reported for several language 
pairs, such as English/Chinese (Meng et al, 2001; 
Virga et al, 2003; Lee et al, 2003; Gao et al, 
2004; Guo et al, 2004), English/Japanese (Knight 
et al, 1998; Brill et al, 2001; Bilac et al, 2004), 
                                                     
1 Pinyin is the standard Romanization of Chinese. 
English/Korean (Oh et al, 2002; Sung et al, 
2000), and English/Arabic (Yaser et al, 2002). 
Most of the reported works utilize a phonetic clue 
to resolve the transliteration through a multiple 
step phonemic mapping where algorithms, such as 
dictionary lookup, rule-based and machine 
learning-based approaches, have been well 
explored. 
In this paper, we will discuss the limitation of 
the previous works and present a novel framework 
for machine transliteration. The new framework 
carries out the transliteration by direct 
orthographical mapping (DOM) without any 
intermediate phonemic mapping. Under this 
framework, we further propose a joint source-
channel transliteration mode (n-gram TM) as an 
alternative machine learning-based approach to 
model the source-target word orthographic 
association. Without the loss of generality, we 
evaluate the performance of the proposed method 
for English/Chinese and English/Japanese pairs. 
An experiment that compares the proposed method 
with several state-of-art approaches is also 
presented. The results reveal that our method 
outperforms other previous methods significantly. 
The reminder of the paper is organized as 
follows. Section 2 reviews the previous work. In 
section 3, the DOM framework and n-gram TM 
model are formulated. Section 4 describes the 
evaluation results and compares our method with 
other reported work. Finally, we conclude the 
study with some discussions. 
2 Previous Work 
The topic of machine transliteration has been 
studied extensively for several different language 
pairs, and many techniques have been proposed. 
To better understand the nature of the problem, we 
review the previous work from two different 
viewpoints: the transliteration framework and the 
transliteration model. The transliteration model is 
built to capture the knowledge of bilingual 
phonetic association and subsequently is applied to 
the transliteration process. 
2.1 Transliteration Framework 
The phoneme-based approach has received 
remarkable attention in the previous works (Meng 
et al, 2001; Virga et al, 2003; Knight et al, 1998; 
Oh et al, 2002; Sung et al, 2000; Yaser et al, 
2002; Lee et al, 2003). In general, this approach 
includes the following three intermediate 
phonemic/orthographical mapping steps: 
1) Conversion of a source language word into 
its phonemic representation (grapheme-to-
phoneme conversion, or G2P); 
2) Transformation of the source language 
phonemic representation to the target 
language phonemic representation; 
3) Generation of target language orthography 
from its phonemic representation (phoneme-
to-grapheme conversion, or P2G). 
To achieve phonetic equivalent transliteration, 
phoneme-based approach has become the most 
popular approach. However, the success of 
phoneme-based approach is limited by the 
following constraints: 
1) Grapheme-to-phoneme conversion, 
originated from text-to-speech (TTS) 
research, is far from perfect (The 
Onomastica Consortium, 1995), especially 
for the name of different language origins. 
2) Cross-lingual phonemic mapping presents a 
great challenge due to phonemic divergence 
between some language pairs, such as 
Chinese/English, Japanese/English (Wan 
and Verspoor, 1998; Meng et al, 2001). 
3) The conversion of phoneme-to-grapheme 
introduces yet another level of imprecision, 
esp. for the ideographic language, such as 
Chinese.  Virga and Khudanpur (2003) 
reported 8.3% absolute accuracy drops when 
converting from Pinyin to Chinese character. 
The three error-prone steps as stated above lead 
to an inferior overall system performance. The 
complication of multiple steps and introduction of 
intermediate phonemes also incur high cost in 
system development when moving from one 
language pair to another, because we have to work 
on language specific ad-hoc phonic rules. 
2.2 Transliteration Model 
Transliteration model is a knowledge base to 
support the execution of transliteration strategy. To 
build the knowledge base, machine learning or 
rule-based algorithms are adopted in phoneme-
based approach. For instance, noisy-channel model 
(NCM) (Virga et al, 2003; Lee et al, 2003), 
HMM (Sung et al, 2000), decision tree (Kang et 
al., 2000), transformation-based learning (Meng et 
al., 2001), statistical machine transliteration model 
(Lee et al, 2003), finite state transducers (Knight 
et al, 1998) and rule-based approach (Wan et al, 
1998; Oh et al, 2002). It is observed that the 
reported transliteration models share a common 
strategy, that is:  
1) To model the transformation rules; 
2) To model the target language; 
3) To model the above both; 
However, the modeling of different knowledge 
is always done independently. For example, NCM 
and HMM (Virga et al, 2003; Lee et al, 2003; 
Sung et al, 2000) model the transformation 
mapping rules and the target language separately; 
decision tree (Kang et al, 2000), transformation-
based learning (Meng et al, 2001), finite state 
transducers (Knight et al, 1998) and statistical 
machine transliteration model (Lee et al, 2003) 
only model the transformation rules.  
3 Direct Orthographical Mapping 
To overcome the limitation of phoneme-based 
approach, we propose a unified framework for 
machine transliteration, direct orthographical 
mapping (DOM). The DOM framework tries to 
model phonetic equivalent association by fully 
exploring the orthographical contextual 
information and the orthographical mapping. 
Under the DOM framework, we propose a joint 
source-channel transliteration model (n-gram TM) 
to capture the source-target word orthographical 
mapping relation and the contextual information. 
Unlike the noisy-channel model, the joint source-
channel model does not try to capture how the 
source names can be mapped to the target names, 
but rather how both source and target names can be 
generated simultaneously. 
The proposed framework is applicable to all 
language pairs. For simplicity, in this section, we 
take English/Chinese pair as example in the 
formulation, where E2C refers to English to 
Chinese transliteration and C2E  refers to Chinese 
to English back-transliteration. 
3.1 Transliteration Pair and Alignment 
Suppose that we have an English name 
1... ...i mx x x? =  and a Chinese transliteration 
1... ...i ny y y? = where ix are English letters and 
jy are Chinese characters. The English name ?  
and its Chinese Transliteration ?  can be 
segmented into a series of substrings: 
1 2... Ke e e? =  and 1 2... Kc c c? =  ( min( , )k m n< ). 
We call the substring as transliteration unit and 
each English transliteration unit ie  is aligned with 
a corresponding Chinese transliteration unit ic  to 
form a transliteration pair. An alignment between 
? and ?  is defined as ?  with 
1 1 1, ,e c e c< > =< >  
2 2 2, ,e c e c< > =< >  ?  
and , ,K K Ke c e c< > =< > . A transliteration pair 
ice >< ,  represents a two-way mapping between 
ie  and  ic . A unit could be a Chinese character or 
a monograph, a digraph or a trigraph and so on for 
English. For example, ??|a ?|b ?|ru ?|zzo? is 
one alignment of Chinese-English word pair  ??
???? and ?abruzzo?. 
3.2 DOM Transliteration Framework 
By the definition of ? , ?  and ? , the E2C 
transliteration can be formulated as 
),,(maxarg
)),,(maxarg(maxarg
),,(maxarg
),(maxarg
,
???
???
???
???
??
??
??
?
P
P
P
P
=
?
=
=
?
         (1)  
Similarly the C2E back-transliteration as 
,
arg max ( , , )P
? ?
? ? ? ??                      (2) 
To reduce the computational complexity, in eqn. 
(1), common practice is to replace the summation 
with maximization. 
The eqn. (1) and (2) formulate the DOM 
transliteration framework. ),,( ???P is the joint 
probability of ? , ?  and ? , whose definition 
depends on the transliteration model which will be 
discussed in the next two subsections. Unlike the 
phoneme-based approach, DOM does not need to 
explicitly model any phonetic information of either 
source or target language. Assuming sufficient 
training corpus, DOM transliteration framework is 
to capture the phonetic equivalents through 
orthographic mapping or transliteration 
pair ice >< , . By eliminating the potential 
imprecision introduced through a multiple-step 
phonetic mapping in the phoneme-based approach, 
DOM is expected to outperform. In contrast to 
phoneme-based approach, DOM is purely data-
driven, therefore can be extended across different 
language pairs easily. 
3.3 n-gram TM under DOM 
Given ? and ? , the joint probability of 
),,( ???P  is the probability of alignment ? , 
which can be formulated as follows: 
?
=
?><><=
=
K
k
k
k ceceP
PPP
1
1
1 ),|,(
)(*)|,(),,( ???????
 (3) 
In eqn. (3), the transliteration pair is used as the 
token to derive n-gram statistics, so we call the 
model as n-gram TM transliteration model.  
 
 
Figure 1. System structure of DOM 
The above block diagram illustrates typical 
system structure of DOM. The training of n-gram 
TM model is discussed in section 3.5. Given a 
language pair, the bidirectional transliterations can 
be achieved with the same n-gram TM and using 
the same decoder.  
3.4 DOM: n-gram TM vs. NCM 
Noisy-channel model (NCM) has been well 
studied in the phoneme-based approach. Let?s take 
E2C as an example to look into a bigram case to 
see what n-gram TM and NCM present to us under 
DOM. We have 
?
=
??
=
K
k
kkkk ccPceP
PPP
1
1 )|(*)|(
)(*)|,(),,( ???????
        (4) 
?
=
?><><?
=
K
k
kk ceceP
PPP
1
1 ),|,(
)(*)|,(),,( ???????
         (5) 
where eqn. (4) and (5) are the bigram version of 
NCM and n-gram TM under DOM, respectively. 
The formulation of eqn. (4) could be interpreted as 
a HMM that has Chinese units as its hidden states 
and English transliteration units as the observations 
(Rabiner, 1989). Indeed, NCM consists of two 
models; one is the channel model or transliteration 
model, ?
=
K
k
kk ceP
1
)|( , which tries to estimate the 
mapping probability between the two units; 
DOM Framework 
Name in 
Language A 
Bi-directional 
Decoder
Name in 
Language B 
 
n-gram 
TM
another is the source model or language model, 
?
=
?
K
k
kk ccP
1
1 )|( , which tries to estimate the 
generative probability of the Chinese name, given 
the sequence of Chinese transliteration units. 
Unlike NCM, n-gram TM model does not try to 
capture how source names can be mapped into 
target names, but rather how source and target 
names can be generated simultaneously. 
We can also study the two models from the 
contextual information usage viewpoint. One finds 
that eqn. (4) can be approximated by eqn. (5). 
)|(*)|(
),|(*),,|(
),|,(
1
11
1
?
??
?
?
><><=
><><
kkkk
kkkkk
kk
ccPceP
cecPceceP
ceceP
  (6) 
Eqn. (6) shows us that the context information 
1, ?>< kce and 1?ke  are absent in the channel 
model and source model of NCM, respectively. In 
this way, one could argue that n-gram TM model 
captures more context information than traditional 
NCM model. With adequate and sufficient training 
data, n-gram TM is expected to outperform NCM 
in the decoding.  
3.5 Transliteration Alignment Training 
For the n-gram TM model training, the bilingual 
name corpus needs to be aligned firstly at the 
transliteration unit level. The maximum likelihood 
approach, through EM algorithm (Dempster et al, 
1977) is employed to infer such an alignment. 
The aligning process is different from that of 
transliteration given in eqn. (1) or (2), here we 
have a fixed bilingual entries, ? and ? . The 
aligning process is just to find the alignment 
segmentation ? between the two strings that 
maximizes the joint probability: 
),,(maxarg ????
?
P=   (7) 
Kneser-Ney smoothing algorithm (Chen et al, 
1998) is applied to smooth the probability 
distribution. NCM model training is carried out in 
the similar way to n-gram TM. The difference 
between the two models lies in eqn (4) and (5). 
3.6 Decoding Issue 
The decoder searches for the most probabilistic 
path of transliteration pairs, given the word in 
source language, by resolving different 
combinations of alignments. Rather than Viterbi 
algorithm, we use stack decoder (Schwartz et al, 
1990) to get N-best results for further processing or 
as output for other applications. 
4 The Experiments  
4.1 Testing Environments 
We evaluate our method through several 
experiments for two language pairs: 
English/Chinese and English/Japanese.  
For English/Chinese language pair, we use a 
database from the bilingual dictionary ?Chinese 
Transliteration of Foreign Personal Names? 
(Xinhua, 1992). The database includes a collection 
of 37,694 unique English entries and their official 
Chinese transliteration. The listing includes 
personal names of English, French, and many other 
origins. The following results for this language pair 
are estimated by 13-fold cross validation for more 
accurate. We report two types of error rates: word 
error rate and character error rate. In word error 
rate, a word is considered correct only if an exact 
match happens between transliteration and the 
reference. The character error rate is the sum of 
deletion, insertion and substitution errors. Only the 
top choice in N-best results is used for character 
error rate reporting. 
For English/Japanese language pair, we use the 
same database as that in the literature (Bilac et al, 
2004) 2 . The database includes 7,021 Japanese 
words in katakana together with their English 
translation extracted from the EDICT dictionary3. 
714 tokens of these entries are withheld for 
evaluation. Only word error rate is reported for this 
language pair. 
4.2 Modeling 
The alignment is done fully automatically along 
with the n-gram TM training process. 
 
# close set bilingual entries (full data)  37,694 
# unique Chinese transliteration (close) 28,632 
# training entries for open test 34,777 
# test entries for open test 2,896 
# unique transliteration pairs  T 5,640 
# total transliteration pairs TW  119,364
# unique English units E 3,683 
# unique Chinese units C 374 
# bigram TM ),|,( 1?><>< kk ceceP  38,655 
# NCM Chinese bigram )|( 1?kk ccP  12,742 
Table 1. Modeling statistics (E-C) 
Table 1 reports statistics in the model training 
for English/Chinese pair, and table 2 is for 
English/Japanese pair. 
                                                     
2 We thank Mr. Slaven Bilac for letting us use his 
testing setup as a reference.  
3 ftp://ftp.cc.monash.edu.au/pub/nihongo/. 
# close set bilingual entries (full data)  7,021 
# training entries for open test 6,307 
# test entries for open test 714 
# unique transliteration pairs  T 2,173 
# total transliteration pairs TW  28,366
# unique English units E 1,216 
# unique Japanese units J 276 
# bigram TM 1( , | , )k kP e j e j ?< > < >  9,754 
Table 2. Modeling statistics (E-J) 
4.3 E2C Transliteration 
In this experiment, we conduct both open and 
closed tests for n-gram TM and NCM models 
under DOM paradigm. Results are reported in 
Table 3 and Table 4. 
 
 open 
(word) 
open 
(char) 
Closed 
(word) 
closed 
(char) 
1-gram 45.6% 21.1% 44.8% 20.4% 
2-gram 31.6% 13.6% 10.8% 4.7% 
3-gram 29.9% 10.8% 1.6% 0.8% 
Table 3. E2C error rates for n-gram TM tests.  
 open 
(word) 
open 
(char) 
closed 
(word) 
closed 
(char) 
1-gram 47.3% 23.9% 46.9% 22.1% 
2-gram 39.6% 20.0% 16.4% 10.9% 
3-gram 39.0% 18.8% 7.8% 1.9% 
Table 4. E2C error rates for NCM tests 
Not surprisingly, the result shows that n-gram 
TM, which benefits from the joint source-channel 
model coupling both source and target contextual 
information into the model, is superior to NCM in 
all the test cases. 
4.4 C2E Back-Transliteration 
The C2E back-transliteration is more 
challenging than E2C transliteration. Experiment 
results are reported in Table 5. As expected, C2E 
error rate is much higher than that of E2C.  
 
 open 
(word) 
Open 
(letter) 
closed 
(word) 
closed 
(letter) 
1 gram 82.3% 28.2% 81% 27.7% 
2 gram 63.8% 20.1% 40.4% 12.3% 
3 gram 62.1% 19.6% 14.7% 5.0% 
Table 5. C2E error rate for 3-gram TM tests 
Table 6 reports the N-best word error rates for 
both E2C and C2E which implies the potential of 
error reduction by using secondary knowledge 
source, such as table looking-up. The N-best error 
rates are also reduced greatly at 10-best level. 
 
 E2C 
open 
E2C 
closed 
C2E 
open 
C2E 
Closed 
1-best 29.9% 1.6% 62.1% 14.7% 
5-best 8.2% 0.94% 43.3% 5.2% 
10-best 5.4% 0.90% 24.6% 4.8% 
Table 6. N-best word error rates for 3-gram TM  
4.5 Discussions of DOM 
Due to lack of standard data sets, the DOM 
framework is unable to make a straightforward 
comparison with other approaches. Nevertheless, 
we list some reported studies on other databases of 
E2C tasks in Table 7 and those of C2E tasks in 
Table 8 for reference purpose. In Table 7, the 
reference data are extracted from Table 1 and 3 of 
(Virga et al, 2003), where only character and 
Pinyin error rates are reported. The first 4 setups 
by Virga et al all adopted the phoneme-based 
approach. In table 8, the reference data are 
extracted from Table 2 and Figure 4 of (Guo et al, 
2004), where word error rates are reported. 
 
System Trainin
g size 
Test 
size 
Pinyin 
errors 
Char 
errors 
Meng et 
al. 
2,233 1,541 52.5% N/A 
Small MT 2,233 1,541 50.8% 57.4% 
Big MT 3,625 250 49.1% 57.4% 
Huge MT 
(Big MT) 
309,019 3,122 42.5% N/A 
3-gram 
TM/DOM 
34,777 2,896 <10.8% 10.8% 
3-gram 
NCM/DOM 
34,777 2,896 <18.8% 18.8% 
Table 7. Performance Comparison of E2C 
Since we have obtained results in character 
already and the character to Pinyin mapping is one-
to-one in the 374 legitimate Chinese characters for 
transliteration in our implementation, we expect 
less Pinyin error than character error in Table 7.  
 
 Training  
size 
Test 
size 
1-best 10-best 
Guo et al 424,788 500 >82.0% >50.0% 
3-gram 
TM/DOM 
34,777 2,896 62.1% 24.6% 
Table 8. Performance Comparison of C2E 
For E2C, Table 7 shows that even with an 8 
times larger database than ours, Huge MT (Big 
MT) test case who reports the best performance 
still generates 3 times Pinyin error rate than ours. 
For C2E, Table 8 shows that even with only 9 
percent training set, our approach can still make 20 
percent absolute word error rate reduction.  Thus, 
although the experiment are done in different 
environments, to some extend, Table 7 and Table 8 
reveal that the n-gram TM/DOM outperforms other 
techniques for the case of English/Chinese 
transliteration/back-transliteration significantly.  
4.6 English/Japanese Transliteration 
In this experiment, we conduct both open and 
closed tests for n-gram TM on English/Japanese 
transliteration and back-transliteration. We use the 
same training and testing setups as those in (Bilac 
et al, 2004). 
Table 9 reports the results from three different 
transliteration mechanisms. Case 1 is the 3-gram 
TM under DOM; Case 2 is Case 1 integrated with 
a dictionary lookup validation process during 
decoding; Case 3 is extracted from (Bilac et al, 
2004). Similar to English/Chinese transliteration, 
one can find that J2E back-transliteration is more 
challenging than E2J transliteration in both open 
and closed cases. It is also found that word error 
rates are reduced greatly at 10-best level.  
(Bilac et al, 2004) proposed a hybrid-method of 
grapheme-based and phoneme-based for J2E back-
transliteration, where the whole EDICT dictionary, 
including the test set, is used to train a LM. A LM 
unit is a word itself. In this way, the dictionary is 
used as a lookup table in the decoding process to 
help identify a valid choice among candidates.  To 
establish comparison, we also integrate the 
dictionary lookup processing with the decoder, 
which is referred as Case 2 in Table 9. It is found 
that Case 2 presents a error reduction of 
43.8%=(14.6-8.2)/14.6% for word over to those 
reported in (Bilac et al, 2004). Furthermore, the n-
gram TM/DOM approach is rather straightforward 
in implementation where direct orthographical 
mapping could potentially handle Japanese 
transliteration of names of different language 
origins, while the issues with non-English terms 
are reported in (Bilac et al, 2004). 
 
The DOM framework shows us a great 
improvement in performance with n-gram TM 
being the most successful implementation. 
Nevertheless, NCM presents another successful 
implementation of DOM framework. The n-gram 
TM and NCM under direct orthographic mapping 
(DOM) paradigm simplify the process and reduce 
the chances of conversion errors. The experiments 
also show that even with much less training data, 
DOM are still much more superior performance 
than the state of art solutions. 
5 Conclusions 
In this paper, we propose a new framework, 
direct orthographical mapping (DOM) for machine 
transliteration and back-transliteration. Under the 
DOM framework, we further propose a joint 
source-channel transliteration model, also called n-
gram TM. We also implement the NCM model 
under DOM for reference. We use EM algorithm 
as an unsupervised training approach to train the n-
gram TM and NCM. The proposed methods are 
tested on an English-Chinese name corpus and 
English-Japanese katakana word pair extracted 
from EDICT dictionary. The data-driven and one-
step mapping strategies greatly reduce the 
development efforts of machine transliteration 
systems and improve accuracy significantly over 
earlier reported results. We also find the back-
transliteration is more challenging than the 
transliteration. 
The DOM framework demonstrates several 
unique edges over phoneme-based approach:  
   
English-Japanese 
Transliteration 
Japanese-English 
Back-transliteration 
 
open test closed 
test 
open test closed 
test 
1-best 40.5% 13.5% 62.8% 17.9% Case 1: 3-gram 
TM/DOM 10-best 13.2% 0.8% 17.9% 2.1% 
1-best 5.4% 0.7% 8.2% 1.2% Case 2: 3-gram 
TM/DOM with 
dictionary lookup 
10-best 0.7% 0% 1.7% 0.3% 
1-best N/A N/A 14.6% N/A Case 3: Bilac et al, 
2004 10-best N/A N/A 2.2% N/A 
Table 9. Experiment results of English-Japanese Transliteration
1) By skipping the intermediate phonemic 
interpretation, the transliteration error rate is 
reduced significantly; 
2) Transliteration models under DOM are data-
driven. Assuming sufficient training corpus, 
the modeling approach applies to different 
language pairs; 
3) DOM presents a paradigm shift for machine 
transliteration, that provides a platform for 
implementation of many other transliteration 
models; 
The n-gram TM is a successful implementation 
of DOM framework due to the following aspects: 
1) N-gram TM captures contextual information 
in both source and target languages jointly; 
unlike the phoneme-based approach, the 
modeling of transformation rules and target 
language is tightly coupled in n-gram TM 
model. 
2) As n-gram TM uses transliteration pair as 
modeling unit, the same model applies to bi-
directional transliteration; 
3) The bilingual aligning process is integrated 
into the decoding process in n-gram TM, 
which allows us to achieve a joint 
optimization of alignment and transliteration 
automatically. Hence manual pre-alignment 
is unnecessary. 
Named entities are sometimes translated in 
combination of transliteration and meanings. As 
the proposed framework allows direct 
orthographical mapping, we are extending our 
approach to handle such name translation. We also 
extending our method to handle the disorder and 
fertility issues in named entity translation. 
References  
Chun-Jen Lee and Jason S. Chang,  2003. Acquisition of 
English-Chinese Transliteration Word Pairs from 
Parallel-Aligned Texts using a Statistical Machine 
Translation Model, Proceedings of HLT-NAACL 
Workshop: Building and Using parallel Texts Data 
Driven Machine Translation and Beyond, 2003, 
Edmonton, pp. 96-103 
Dempster, A.P., N.M. Laird and D.B.Rubin, 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm, J. Roy. Stat. Soc., Ser. B. Vol. 39 
Eric Brill, Garry Kacmarcik and Chris Brockrtt, 2001. 
Automatically Harvesting Katakana-English Term 
Pairs from Search Engine Query Logs. Proceeding of 
NLPRS?01 
Helen M. Meng, Wai-Kit Lo, Berlin Chen and Karen 
Tang. 2001. Generate Phonetic Cognates to Handle 
Name Entities in English-Chinese cross-language 
spoken document retrieval, Proceedings of ASRU 
2001 
Jong-Hoon Oh and Key-Sun Choi, 2002. An English-
Korean Transliteration Model Using Pronunciation 
and Contextual Rules, Proceedings of COLING 2000 
Kang B.J. and Key-Sun Choi, 2000. Automatic 
Transliteration and Back-transliteration by Decision 
Tree Learning, Proceedings of the 2nd International 
Conference on Language Resources and Evaluation, 
Athens, Greece 
K. Knight and J. Graehl. 1998. Machine Transliteration, 
Computational Linguistics, Vol 24, No. 4 
Paola Virga, Sanjeev Khudanpur, 2003. Transliteration 
of Proper Names in Cross-lingual Information 
Retrieval. Proceedings of ACL 2003 workshop 
MLNER, 2003 
Rabiner, Lawrence R. 1989, A tutorial on hidden 
Markov models and selected applications in speech 
recognition, Proceedings of the IEEE 77(2) 
Schwartz, R. and Chow Y. L., 1990. The N-best 
algorithm: An efficient and Exact procedure for 
finding the N most likely sentence hypothesis, 
Proceedings of ICASSP 1990, Albuquerque, pp. 81-
84 
Slaven Bilac and Hozumi Tanaka, 2004. Improving 
Back-Transliteration by Combining Information 
Sources. Proceedings of IJCNLP-04, Haian, pp. 542-
547  
Stephen Wan and Cornelia Maria Verspoor, 1998. 
Automatic English-Chinese name transliteration for 
development of multilingual resources. Proceedings 
of COLING-ACL?98  
Stanley F. Chen and Joshua Goodman, 1998. An 
Empirical Study of Smoothing Techniques for 
Language Modeling, TR-10-98, Computer Science 
Group, Harvard Universituy. 1998 
Sung Young Jung, Sung Lim Hong and Eunok Paek, 
2000. An English to Korean Transliteration Model of 
Extended Markov Window, Proceedings of COLING 
2000 
The Onomastica Consortium, 1995. The Onomastica 
interlanguage pronunciation lexicon, Proceedings of 
EuroSpeech, Madrid, Spain, pp829-832 
Wei Gao, Kam-Fai Wong and Wai Lam, 2004. 
Phoneme-based Transliteration of Foreign Names 
for OOV Problems. Proceedings of IJCLNP-04, 
Hainan, pp 374-381  
Xinhua News Agency, 1992. Chinese transliteration of 
foreign personal names, The Commercial Press 
Yaser Al-Onaizan and Kevin Knight, 2002. Translating 
named entities using monolingual and bilingual 
resources. Proceedings of the 40th ACL, 
Philadelphia, 2002, pp. 400-408 
Yuqing Guo and  Haifeng Wang, 2004. Chinese-to-
English Backward Machine Transliteration. 
Companion Volume to the Proceedings of IJCNLP-
04, Hainan, pp 17-20 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 121?128
Manchester, August 2008
 
 
Other-Anaphora Resolution in Biomedical Texts with Automatically 
Mined Patterns 
 
Chen Bin#, Yang Xiaofeng$, Su Jian^ and Tan Chew Lim* 
#*School of Computing, National University of Singapore  
$^Institute for Infocomm Research, A-STAR, Singapore 
{#chenbin, *tancl}@comp.nus.edu.sg 
{$xiaofengy, ^sujian}@i2r.a-star.edu.sg 
? Abstract 
This paper proposes an other-anaphora 
resolution approach in bio-medical texts. 
It utilizes automatically mined patterns to 
discover the semantic relation between an 
anaphor and a candidate antecedent. The 
knowledge from lexical patterns is incor-
porated in a machine learning framework 
to perform anaphora resolution. The ex-
periments show that machine learning 
approach combined with the auto-mined 
knowledge is effective for other-
anaphora resolution in the biomedical 
domain. Our system with auto-mined pat-
terns gives an accuracy of 56.5%., yield-
ing 16.2% improvement against the base-
line system without pattern features, and 
9% improvement against the system us-
ing manually designed patterns.  
1 Introduction 
The last decade has seen an explosive growth in 
the amount of textual information in biomedi-
cine. There is a need for an effective and effi-
cient text-mining system to gather and utilize the 
knowledge encoded in the biomedical literature. 
For a correct discourse analysis, a text-mining 
system should have the capability of understand-
ing the reference relations among different ex-
pressions in texts. Hence, anaphor resolution, the 
task of resolving a given text expression to its 
referred expression in prior texts, is important for 
an intelligent text processing system. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
In linguistics, an expression that points back 
to a previously mentioned expression is called an 
anaphor, and the expression being referred to by 
the anaphor is called its antecedent. Most pre-
vious work on anaphora resolution aims at identi-
ty-anaphora in which both an anaphor and its 
antecedent are mentions of the same entity. 
In this paper, we focus on a special type of 
anaphora resolution, namely, other-anaphora 
resolution, in which an anaphor to be resolved 
has a prefix modifier ?other? or ?another?. The 
antecedent of an other-anaphor is a complement 
expression to the anaphor in a super set. In other 
words, an other-anaphor is a set of elements ex-
cluding the element(s) specified by the antece-
dent. If the modifier ?other? or ?another? is re-
moved, an anaphor becomes the super set includ-
ing the antecedent. Thus, other-anaphora in fact 
represents a ?part-whole? relation. Consider the 
following text  
 ?IL-10 inhibits nuclear stimulation of nuclear 
factor kappa B (NF kappa B).  
Several other transcription factors including NF- 
IL-6, AP-1, AP-2, GR, CREB, Oct-1, and Sp-1 
are not affected by IL-10.?  
Here, the expression ?other transcription fac-
tors? is an other-anaphor, while the ?NF kappa 
B? is its antecedent. The anaphor refers to any 
transcription factors except the antecedent.  By 
removing the lexical modifier ?other?, we can 
get a supper set ?transcription factors? that in-
cludes the antecedent. The anaphor and antece-
dent thus have a ?part-whole? relation1.  
Other-anaphora resolution is an important 
sub-task in information extraction for biomedical 
                                                 
1 Other-anaphora could be also held between ex-
pressions that have subset-set or member-collection 
relations. In this paper, we treat them in a uniform 
way by using the patterned-based method. 
121
  
domain. It also contributes to biomedical ontolo-
gy building as it targeted at a ?part-whole? rela-
tion which is in the same hierarchical orders as in 
ontology. Furthermore, other-anaphora resolu-
tion is a first-step exploration in the resolution of 
bridging anaphora. Furthermore, other-anaphora 
resolution is a first-step exploration in the resolu-
tion of bridging, a special anaphora phenomenon 
in which the semantic relation between an ana-
phor and its antecedent is more complex (e.g. 
part-whole) than co-reference. 
Previous work on other-anaphora resolution 
relies on knowledge resources, for example, on-
tology like WordNet to determine the ?part-
whole? relation. However, in the biomedical do-
main, a document is full of technical terms which 
are usually missing in a general-purpose ontolo-
gy. To deal with this problem, pattern-based ap-
proaches have been widely employed, in which a 
pattern that represents the ?part-whole? relation 
is designed. Two expressions are connected with 
the specific pattern and form a query. The query 
is searched in a large corpus for the occurrence 
frequency which would indicate how likely the 
two given expressions have the part-whole rela-
tion. The solution can avoid the efforts of con-
structing the ontology knowledge for the "part-
whole" relation. However, the pattern is designed 
in an ad-hoc method, usually from linguistic in-
tuition and its effectiveness for other-anaphora 
resolution is not guaranteed. 
In this paper, we propose a method to auto-
matically mine effective patterns for other-
anaphora resolution in biomedical texts. Our me-
thod runs on a small collection of seed word 
pairs. It searches a large corpus (e.g., PubMed 
abstracts as in our system) for the texts where the 
seed pairs co-occur, and collects the surrounding 
words as the surface patterns. The automatically 
found patterns will be used in a machine learning 
framework for other-anaphora resolution. To our 
knowledge, our work is the first effort of apply-
ing the pattern-base technique to other-anaphora 
resolution in biomedical texts. 
The rest of this paper is organized as follows. 
Section 2 introduces previous related work. Sec-
tion 3 describes the machine learning framework 
for other-anaphora resolution. Section 4 presents 
in detail our method for automatically pattern 
mining. Section 5 gives experiment results and 
has some discussions. Finally, Section 6 con-
cludes the paper and shows some future work. 
2 Related Work 
Previous work on other-anaphora resolution 
commonly depends on human engineered know-
ledge and/or deep semantic knowledge for the 
?part-whole? relation, and mostly works only in 
the news domain. 
Markert et al, (2003) presented a pattern-
based algorithm for other-anaphor resolution. 
They used a manually designed pattern ?ANTE-
CEDENT and/or other ANAPHOR ?. Given two 
expression to be resolved, a query is formed by 
instantiating the pattern with the two given ex-
pressions. The query is searched in the Web. The 
higher the hit number returned, the more likely 
that the anaphor and the antecedent candidate 
have the ?part-whole? relation. The anaphor is 
resolved to the candidate with the highest hit 
number. Their work was tested on 120 other-
anaphora cases extracted from Wall Street Jour-
nal. The final accuracy was 52.5%. 
Modjeska et al, (2003) also presented a simi-
lar pattern-based method for other-anaphora res-
olution, using the same pattern ?ANTECEDENT 
and/or other ANAPHOR?. The hit number re-
turned from the Web is used as a feature for a 
Na?ve Bayesian Classifier to resolve other-
anaphors. Other features include surface words, 
substring matching, distance, gender/number 
agreement, and semantic tag of the NP. They 
evaluated their method with 500 other-anaphor 
cases extracted from Wall Street Journal, and 
reported a result of 60.8% precision and 53.4% 
recall. 
Markert and Nissim (2005) compared three 
systems for other-anaphora resolution, using the 
same data set as in (Modjeska et al, 2003). 
The first system consults WordNet for the 
part-whole relation. The WordNet provides in-
formation on meronym/holonym (part-of rela-
tion) and hypernym/ hyponym (type-of relation). 
Their system achieves a performance of 56.8% 
for precision and 37.0% for recall. 
The second and third systems employ the pat-
tern based approach, employing the same manual 
pattern ?ANTECEDENT and/or other ANA-
PHOR?. The second system did search in British 
Nation Corpus, giving 62.6% precision and 
26.2% recall. The third system did search in the 
Web as in (Markert et al, 2003), giving 53.8% 
precision and 51.7% recall. 
122
  
3 Anaphora Resolution System 
3.1 Corpus 
In our study, we used the GENIA corpus2 for our 
other-anaphora resolution in biomedical texts. 
The corpus consists of 2000 MEDLINE abstracts 
(around 440,000 words). From the GENIA cor-
pus, we extracted 598 other-anaphora cases. The 
598 cases do not contain compound prepositions 
or idiomatic uses of ?other?, like ?on the other 
hand? and ?other than?. And all these anaphors 
have their antecedents found in the current and 
previous two sentences of the other-anaphor. On 
average, there are 15.33 candidate antecedents 
for each anaphor to be resolved. 
To conduct other-anaphora resolution, an in-
put document is preprocessed through a pipeline 
of NLP components, including tokenization, sen-
tence boundary detection, part-of-speech (POS) 
tagging, noun phrase (NP) chunking, and named-
entity recognition (NER). These preprocessing 
modules are aimed to determine the boundaries 
of each NP in a text, and to provide necessary 
information of an NP for subsequent processing. 
In our system, we employed the tool-kits built by 
our group for these components. The POS tagger 
was trained and tested on the GENIA corpus 
(version 2.1) and achieved an accuracy of 97.4%. 
The NP-chunking module, evaluated on UPEN 
WSJ TreeBank, produced 94% F-measure. The 
NER module, trained on GENIA corpus (version 
3.0), achieved 71.2% F-measure covering 22 ent-
ity types (e.g., Virus, Protein, Cell, DNA, etc). 
3.2 Learning Framework 
Our other-anaphora resolution system adopts the 
common learning-based model for identity-
anaphora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002). 
In the learning framework, a training or test-
ing instance has the form of ?? ?????? ,???  
where ??????  is the ?
th candidates of the antece-
dent of anaphor ???. An instance is labelled as 
positive if ??????  is the antecedent of  ??? , or 
negative if ??????  is not the antecedent of  ???. 
An instance is associated with a feature vector 
which records different properties and relations 
between ???  and ?????? . The features used in 
our system will be discussed later in the paper. 
During training, for each other-anaphor, we 
consider as the candidate antecedents the preced-
ing NPs in its current and previous two sentences. 
                                                 
2 http://www-tsujii.is.s.u-tokyo.ac.jp/~genia/topics/Corpus/ 
A positive instance is formed by pairing the ana-
phor and the correct antecedent. And a set of 
negative instances is formed by pairing the ana-
phor and each of the other candidates.  
Based on these generated training instances, 
we can train a binary classifier using any dis-
criminative learning algorithm. In our work, we 
employed support vector machine (SVM) due to 
its good performance in high dimensional feature 
vector spaces. 
During the resolution process, for each other-
anaphor encountered, all of the preceding NPs in 
a three-sentence window are considered. A test 
instance is created for each of the candidate ante-
cedents. The feature vector is presented to the 
trained classifier to determine the other-
anaphoric relation. The candidate with highest 
SVM outcome value is selected as the antecedent.  
3.3 Baseline Features 
Knowledge is usually represented as features for 
machine learning. In our system, we used the 
following groups of features for other-anaphora 
resolution 
 
? Word Distance Indicator 
This feature measures the word distance between 
an anaphor and a candidate antecedent, with the 
assumption that the candidate closer to the ana-
phor has a higher preference to be the antecedent. 
? Same Sentence Indicator 
This feature is either 0 or 1 indicating whether an 
anaphor and a candidate antecedent are in the 
same sentence. Here, the assumption is that the 
candidate in the same sentence as the anaphor is 
preferred for the antecedent. 
? Semantic Group Indicators 
A named-entity can be classified to a semantic 
category such as ?DNA?, ?RNA?, ?Protein? and 
so on3. Thus we use a set of features to record the 
category pair of an anaphor and a candidate ante-
cedent. For example, ?DNA-DNA? is generated 
for the case when both anaphor and candidate are 
DNAs. And ?DNA-Protein? is generated if an 
anaphor is a DNA and a candidate is a protein. 
These features indicate whether a semantic group 
can refer to another.  
Note that an anaphor and its antecedent may 
possibly belong to different semantic categories. 
For example, in the GENIA corpus we found that 
                                                 
3 In our study, we followed the semantic categories defined 
in the annotation scheme of the GENIA corpus.  
123
  
in some cases an expression of a protein name 
actually denotes the gene that encodes the pro-
tein. Thus for a given anaphor and a candidate 
under consideration, it is necessary to record the 
pair-wise semantic groups, instead of using a 
single feature indicating whether two expressions 
are of the same group. 
The semantic group for a named entity is giv-
en by our preprocessing NER. For the common 
NPs produced from the NP chunker, we classify 
the semantic group by looking for the words in-
side NPs. For example, an NP ending with 
?cells? is classified to ?Cell? group while an NP 
ending with ?gene? or ?allele? is classified to 
?DNA? group. 
? Lexical Pattern Indicators 
In some cases, the surrounding words of an ana-
phor and a candidate antecedent strongly indicate 
the ?part-whole? relation. For example, in 
?...asthma and other hypereosinophilic diseas-
es?, the reference between ?other hypereosino-
philic diseases? and ?asthma? is clear if the in-
between words ?and other? are taken into con-
sideration. Another example of such a hint pat-
tern is ?one? the other ?? The feature is 1 if the 
specific patterns are present for the current ana-
phor and candidate pair. A candidate with such a 
feature is preferred to be the antecedent. 
? Hierarchical Name Indicator  
This feature indicates whether an antecedent 
candidate is a substring of an anaphor or vice 
versa. This feature is used to capture cases like 
?Jun? and ?JunB? (?Jun? is a family of protein 
while ?JunB? is a member of this family). In 
many cases, an expression that is a super set 
comes with certain postfix words, for example, 
?family members? in  
?Fludarabine caused a specific depletion of 
STAT1 protein (and mRNA) but not of other 
STAT family members.?  
This kind of phenomenon is more common in 
bio-medical texts than in news articles. 
3.4 SVM Training and Classification 
In our system, we utilized the open-source soft-
ware SVM-Light4 for the classifier training and 
testing.  SVM is a robust statistical model which 
has been applied to many NLP tasks. SVM tries 
to learn a separating line to separate the positive 
instances from negative instances. Kernel trans-
formations are applied for non-linear separable 
                                                 
4 http://svmlight.joachims.org/ 
cases (Vapnik, 1995). In our study, we just used 
the default learning parameters provided by 
SVM-Light with the linear kernel. A more so-
phisticated kernel may further improve the per-
formance. 
4 Using Auto-mined Pattern Features 
The baseline features listed in Section 3.3 only 
rely on shallow lexical, position and semantic 
information about an anaphor and a candidate 
antecedent. It could not, nevertheless, disclose 
the ?part-whole? relation between two given ex-
pressions. In section 2, we have shown some ex-
isting pattern-based solutions that mine the ?part-
whole? relation in a large corpus with some pat-
terns that can represent the relation. However, 
these manually designed patterns are usually se-
lected by heuristics, which may not necessarily 
lead to a high coverage with a good accuracy in 
different domains. To overcome this shortcom-
ing, we would like to use an automatic method to 
mine effective patterns from a large data set. 
First, we create a set of seed pairs of the ?part-
whole? relation. And then, we use the seed pairs 
to discover the patterns that encode the ?part-
whole? relation from a large data set (PubMed as 
in our system). Such a solution is supposed to 
improve the coverage of lexical patterns, while 
still retain the desired ?part-whole? relation for 
other-anaphora resolution. 
The overview of our system with the automat-
ic mined patterns is illustrated in figure 1. 
Seed Pairs 
Generation
Pattern Mining
SVM
GENIA 
Corpus
Seed 
Pairs
Lexical 
Patterns
GENIA
T st 
Cas s
PubMED 
Corpus
 
Figure 1: System Overview 
There are three major parts in our system, 
namely, seed-pairs generation, pattern mining 
and SVM learning and classification. In the sub-
sequent subsections, we will discuss each of the 
three parts in details. 
124
  
4.1 Seed Pairs Preparation 
A seed pair is a pair of phrases/words following 
?part-whole? order, for example,  
?integrin alpha? - ?adhesion molecules? 
where ?integrin alpha? is a kind of ?adhesion 
molecules?.  
We extracted the seed pairs automatically 
from the GENIA corpus. The auto-extracting 
procedure makes uses of some lexical clues like 
?A, such as B, C and D?, ?A (e.g. B and C)?, ?A 
including B? and etc. The capital letter A, B, C 
and D refer to a noun phrase such as ?integrin 
alpha? and ?adhesion molecules?. For each oc-
currence of ?A such as B, C and D?, the program 
will generate seed pairs ?B-A?, ?C-A? and ?D-
A?. 
Consider the following example, 
?Mouse thymoma line EL-4 cells produce cyto-
kines such as interleukin (IL) -2, IL-3, IL-4, IL-
10, and granulocyte-macrophage colony-
stimulating factor in response to phorbol 12-
myristate 13-acetate (PMA).? 
We can extract the following seed pairs, 
?interleukin (IL) -2? ? ?cytokines? 
?IL -3? ? ?cytokines? 
?IL -4? ? ?cytokines? 
?IL -10? ? ?cytokines? 
?granulocyte-macrophage colony-stimulating 
factor? ? ?cytokines?  
A similar action is taken for other lexical 
clues. Totally, we got 909 distinct seed pairs ex-
tracted from the GENIA corpus. 
After the seed pairs have been extracted, an 
automatic verification of the seed pairs is per-
formed. The first purpose of the verification is to 
correct chunking errors. For example, ?HLA 
Class II Gene? may likely be wrongly split into 
?HLA Class? and ?II Gene?. This kind of errors 
is repaired by several simple syntactic rules. The 
second purpose of the verification is to remove 
the inappropriate seed pairs. In our system, we 
abandoned the seed pairs containing pronouns 
like ?those?, ?they?, or nouns like ?element?, 
?member? and ?agent?. Such seed pairs may ei-
ther find no patterns, or lead to meaningless pat-
terns because ?those? or ?elements? have no spe-
cific semantics and could refer to anything. 
4.2 Pattern Mining 
Having obtained the set of seed pairs, we will use 
them to mine patterns for the ?part-whole? rela-
tion. For each seed pair ?antecedent - anaphor? 
(anaphor represents the NP for the ?whole?, 
while antecedent represents the NP for the 
?part?), our system will search in a large data set 
for two queries: ?antecedent * anaphor? and 
?anaphor * antecedent? where the ?*? denotes 
any sequence of words or symbols. For a re-
turned search results, the text in between ?ante-
cedent? and ?anaphora? is extracted as a pattern. 
In our study, we used PubMed 2007 data set 
for the pattern mining. The data set contains 
about 52,000 abstracts with around 9,400,000 
words, and is an ideal large-scale resource for 
pattern mining. 
Consider, as an example, a seed pair ?NK 
kappa B ? ? ?transcription factor?. Suppose that 
a returned sentence for the query ?NK kappa B * 
transcription factor? is  
?...NK kappa B family transcription factors...? 
And a returned sentence for the query ?transcrip-
tion factor * NK kappa B? is 
?...transcription factors, including NF kappa 
B...? 
We can extract a pattern, 
?ANTECEDENT family ANAPHOR? from the 
first sentence and a pattern 
?ANAPHOR, including ANTECEDENT? from 
the second sentence.  
We restrict the patterns so that no pattern span 
across two or more sentences. In other words, the 
pattern shall not contain the symbol ?.?. The vi-
olated patterns will be removed. 
The count that a pattern occurs in the PubMed 
for a seed pair is recorded. As a pattern could be 
reduced by different seed pairs, we define the 
occurrence frequency of a pattern as the sum of 
the counts of the pattern for all the seed pairs, 
using following formula: 
???? ? =  ???(???? , ?? )
????
                           ??(1)   
where ???? ?  is the frequency of pattern ???? ; ??  is 
a seed pair; ?  is the set of all seed pairs. 
???(???? , ?? ) is the count of the pattern ????  for 
?? . 
All the mined patterns are sorted according to 
its frequency as defined in ??(1). 
4.3 Pattern Application 
For classifier training and testing, the patterns 
with high frequency are used as features. In our 
system, we used the top 40 patterns, while we 
also examined the influence the number of the 
patterns on the performance. (See Section 5.2) 
Given an instance ??(?????? , ???) and a pat-
tern feature ????  , a query is constructed by in-
125
  
stantiating with ??????  and ??? . For example, 
for an instance ??("?? ????? ?", "???????-  
?????? ???????")  and a pattern feature ?ANA-
PHOR, including ANTECEDENT?, we can get 
a query ?transcription factors, including NF 
kappa B?. The query is searched in the PubMed 
data set. The count of the query is recorded. The 
value of the pattern feature of a candidate is cal-
culated by normalizing the occurrence frequency 
among all the candidates of the anaphor. 
For demonstration, suppose we have an ana-
phor ?other transcription factors? with two ante-
cedent candidates ?IL-10? and ?NF kappa B?. 
Given a pattern feature ?ANAPHOR, including 
ANTECEDENT?, the count of the query ?tran-
scription factors, including IL-10? is 100 while 
that for ?transcription factors, including NF-
Kappa B? is 300. Then the values of the pattern 
feature for ?IL-10? and ?NF kappa B? are 0.25 
(
100
100+300
) and 0.75 (
300
100+300
), respectively. 
The value of a pattern feature can be inter-
preted as a degree of belief that an anaphor and a 
candidate antecedent have the ?part-whole? rela-
tion, with regard to the specific pattern. Since the 
value of a pattern feature is normalized among 
all the candidates, it could indicate the preference 
of a candidate against other competing candi-
dates. 
5 Experiment Results 
5.1 Experiments Setup 
In our experiments, we conducted a 3-fold cross 
validation to evaluate the performances. The total 
598 other-anaphora cases were divided into 3 
sets of size 200, 199 and 199 respectively. For 
each experiment, two sets were used for training 
while the other set was used for testing.  
For evaluation, we used the accuracy as the 
performance metric, which is defined as the cor-
rectly resolved other-anaphors divided by all the 
testing other-anaphors, that is, 
 
???????? =
# of correctly resolved anaphors
 # of total anaphors
 
5.2 Experiments Results 
Table 1 shows the performance of different 
other-anaphora resolution systems. The first line 
is for the baseline system with only the normal 
features as described in Section 3.3. From the 
table, we can find that the baseline system only 
achieves around 40% accuracy. A performance is 
lower than a similar system in news domain by 
Modjeska et al, (2003) where they reported  
51.6 % precision with 40.6% recall. This differ-
ence is probably because they utilized more se-
mantic knowledge such as hypernymy and mero-
nymy acquired from WordNet. Such knowledge, 
nevertheless, is not easily available in the bio-
medical domain. 
 
Sys Fold-1 Fold-2 Fold-3 Overall 
Baseline 
No Pattern 
42.0 % 
84/200 
38.2 % 
76/199 
40.7 % 
81/199 
40.3 % 
241/598 
Manual 
Pattern 
49.0 % 
98/200 
45.7 % 
91/199 
47.7 % 
95/199 
47.5 % 
284/598 
Auto-
mined 
Pattern 
59.0 % 
118/200 
53.8 % 
107/199 
56.8 % 
113/199 
56.5 % 
338/598 
Table 1: Performance Comparisons 
In our experiments, we tested the system with 
manually designed pattern features. We tried 10 
patterns that can represent the ?part-whole? rela-
tion. Table 2 summaries the patterns used in the 
system. Among them, the pattern ?Anaphor such 
as Antecedent? and ?Antecedent and other Ana-
phor? are commonly used in previous pattern 
based approaches (Markert et al, 2003; Mod-
jeska et al, 2003). 
 
Pattern 
ANTECEDENT is a kind of ANAPHOR 
ANTECEDENT is a type of ANAPHOR 
ANTECEDENT is a member of ANAPHOR 
ANTECEDENT is a part of ANAPHOR 
ANAPHOR such as ANTECEDENT 
ANTECEDENT and other ANAPHOR 
ANTECEDENT within ANAPHOR 
ANTECEDENT is a component of ANAPHOR 
ANTECEDENT is a sort of ANAPHOR 
ANTECEDENT belongs to ANAPHOR 
Table 2: Manually Selected Patterns 
 
The second line of Table 1 shows the results 
of the system with the manual pattern features. 
We can find that adding these pattern features 
produces an overall accuracy of 47%, yielding an 
increase of 7% accuracy against the baseline sys-
tem without the pattern features.  
The improvement in accuracy is consistent 
with previous work using the pattern-based ap-
proaches in the news domain (Modjeska et al, 
2003). However, we found the performance in 
the biomedical domain is worse than that in the 
news domain. For example, Modjeska et al 
(2003) reported a precision around 53%. This 
difference of performance suggests that the ma-
126
  
nually designed patterns may not necessarily 
work equally well in different domains.  
The last system we examined in the experi-
ment is the one with the automatically mined 
pattern features. Table 3 summarizes the top 
mined patterns ranked based on their occurrence 
frequency. Some of the patterns are intuitively 
good representation of the ?part-whole? relation. 
For example, ?ANAPHOR, including ANTE-
CEDENT?. ?ANAPHOR, such as ANTECE-
DENT? and ?ANAPHOR and other ANTECE-
DENT? which are in the manually designed pat-
tern list, are generated.  
The last line of Table 1 lists the result of the 
system with automatically mined pattern fea-
tures. It outperforms the baseline system (up to 
16% accuracy), and the system with manually 
selected patterns (9% accuracy). These results 
prove that our pattern features are effective for 
the other-anaphora resolution.  
 
Pattern Freq 
ANAPHOR, including ANTECEDENT 1213 
ANAPHOR including ANTECEDENT 726 
ANTECEDENT family ANAPHOR 583 
ANAPHOR such as ANTECEDENT 542 
ANTECEDENT transcription ANAPHOR 439 
ANAPHOR, such as ANTECEDENT 295 
ANTECEDENT and other ANAPHOR 270 
ANAPHOR and ANTECEDENT 250 
ANTECEDENT, dendritic ANAPHOR 246 
ANTECEDENT and ANAPHOR 238 
ANTECEDENT human ANAPHOR 223 
ANAPHOR (e.g., ANTECEDENT  213 
ANTECEDENT/rel ANAPHOR 188 
ANTECEDENT-like ANAPHOR 188 
ANAPHOR against ANTECEDENT  163 
Table 3: Auto-Mined Patterns 
To further compare the manually designed 
patterns and the automatically discovered pat-
terns. We examined the coverage rate of the two 
pattern sets. The coverage rate measures the ca-
pability that a set of patterns could lead to posi-
tive anaphor-antecedent pairs. An other-anaphor 
is said to be covered by a pattern set, if the ana-
phor and its antecedent could be hit (i.e., the cor-
responding query has a non-zero hit number) by 
at least one pattern in the list. Thus the coverage 
rate could be defined as 
????????(?)  
=   
#anaphors covered by the pattern set P
# total anaphors
 
The coverage rates of the two pattern sets are 
tabulated in table 4. It is apparent that the auto-
mined patterns have a significantly higher cover-
age (more than twice) than the manually de-
signed patterns. 
 
Patterns Coverage Rate 
Manually Designed 36.0 % 
Auto-Mined 92.1 % 
Table 4: Coverage Comparison 
In our experiments we were also concerned 
about the usefulness of each individual pattern. 
For this purpose, we examined the loss of the 
accuracy when withdrawing a pattern feature 
from the feature list. The top 10 patterns with the 
largest accuracy loss are summarized in table 5. 
 
Pattern 
Acc 
Loss 
ANAPHOR, including ANTECEDENT 4.18% 
ANAPHOR including ANTECEDENT 3.18% 
ANAPHOR such as ANTECEDENT 2.84% 
ANTECEDENT transcription ANAPHOR 2.17% 
ANTECEDENT and other ANAPHOR 2.01% 
ANAPHOR, such as ANTECEDENT 1.84% 
ANTECEDENT family ANAPHOR 1.84% 
ANAPHOR (e.g., ANTECEDENT 1.51% 
ANTECEDENT-like ANAPHOR 1.17% 
ANTECEDENT/rel ANAPHOR 1.17% 
Table 5: Usefulness of Each Pattern 
The process of automatic pattern mining 
would generate numerous surface patterns. It is 
not reasonable to use all the patterns as features. 
As mentioned in section 4.3, we rank the pattern 
based on their occurrence frequency and select 
the top ones as the features. It would be interest-
ing to see how the number of patterns influences 
the performance of anaphora resolution. In figure 
2, we plot the accuracy under different number 
top pattern features. We can find by using more 
patterns, the coverage keeps increasing. The ac-
curacy also increases, but it reaches the peak 
with around 40 patterns. With more patterns, the 
accuracy remains at the same level. This is be-
cause the low frequency patterns usually are not 
that indicative of the ?part-whole? relation. In-
cluding these pattern features would bring noises 
but not help the performance. The flat curve after 
the peak point suggests that the machine learning 
algorithm can effectively identify the importance 
of the pattern features for the resolution decision, 
and therefore including non-indicative patterns 
would not damage the performance. 
In our experiment, we also interested to com-
pare the utility of PubMed with other general 
data sets. Thus, we tested pattern mining by us-
127
  
ing the Google-5-grams corpus5 which lists the 
hit number of all the queries of five words or less 
in the Web. Unfortunately, we found that the per-
formance is worse than using PubMed. The pat-
terns mined from the Web corpus only gives an 
accuracy of around 41%, almost the same as the 
baseline system without using any pattern fea-
tures. The bad performance is due to the fact that 
most of bio-medical names are quite long (2~4 
words) and occur infrequently in the non-
technique data set. Consequently, a query formed 
by a biomedical seed pair usually cannot be 
found in the Web corpus (We found the coverage 
of the auto-mined patterns mined from the corpus 
is only about 20%). 
 
Figure 2: Performance of Various No. of Patterns 
6 Conclusion & Future Works 
In this paper, we have presented how to automat-
ically mined pattern features for learning-based 
other-anaphora resolution in bio-medical texts. 
The patterns that represent the ?part-whole? rela-
tions are automatically mined from a large data 
set. They are used as features for a SVM-based 
classifier learning and testing. The results of our 
experiments show a reasonably good perfor-
mance with 56.5% accuracy). It outperforms 
(16% in accuracy) the baseline system without 
the pattern features, and also beats (9%) the sys-
tem with manually designed pattern features. 
There are several directions for future work. 
We would like to employ a pattern pruning 
process to remove those less indicative patterns 
such as ?ANAPHOR, ANTECEDENT?. And we 
also plan to perform pattern normalization which 
integrates two similar or literally identical pat-
                                                 
5 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?  
   catalogId=LDC2006T13 
terns into a single one. By doing so, the useful 
patterns may come to the top of the pattern list. 
Also we would like to explore ontology re-
sources like MESH and Genes Ontology, which 
can provide enriched hierarchies of bio-medical 
terms and thus would benefit other-anaphora res-
olution. 
Acknowledgements  
This study on co-reference resolution is partially supported 
by a Specific Targeted Research Project (STREP) of the 
European Union's 6th Framework Programme within IST 
call 4, Bootstrapping of Ontologies and Terminologies 
STrategic REsearch Project (BOOTStrep). 
References 
Castano J, Zhang J and Pustejovsky J. Anaphora Resolution 
in Biomedical Literature. Submitted to International Sym-
posium on Reference Resolution 2002, Alicante, Spain 
Clark H. Bridging. In Thinking. Readings in Cognitive 
Science. Johnson-Laird and Wason edition. Cambridge. 
Cambridge University Press; 1977.411?420 
Gasperin C and Vieira R. Using Word Similarity Lists for 
Resolving Indirect Anaphora. In Proceedings of ACL 
Workshop on Reference Resolution and Its Application. 
30 June 2004; Barcelona. 2004.40-46 
Girju R, Badulescu A and Moldovan D. Automatic Discov-
ery of Part-Whole Relations. Computational Linguistics, 
2006, 32(2):83-135 
Bernauer J.. Analysis of Part-Whole Relation and Subsump-
tion in Medical Domain. Data Knowledge Enginnering 
1996, 20:405-415 
Markert K. and Nissim M. Comparing Knowledge Sources 
for Nominal Anaphora Resolution. Computational Lin-
guistics, 2005, 31(3):367-402 
Markert K, Modjeska N and Nissim M. Using the Web for 
Nominal Anaphora Resolution. In Proceedings of EACL 
Workshop on the Computational Treatment of Anaphora. 
14 April 2003; Budapest. 2003.39-46 
Mitokov R. Anaphor Resolution. The State of The Art. 
Working Paper, University of Wolverhampton, UK, 1999 
Modjeska N, Markert K and Nissim M. Using the Web in 
Machine Learning for Other-anaphor Resolution. In Pro-
ceedings of the 2003 Conference on Empirical Methods in 
Natural Language Processing. July2003,Sapporo.176-183 
Soon WM, Ng HT and Lim CY. A Machine Learning Ap-
proach to Coreference Resolution of Noun Phrases. Com-
putational Linguistics, 2001, 27(4).521-544 
Vapnik, V. Chapter 5 Methods of Pattern Recognition. In 
The Nature of Statistical Learning Theory. New York. 
Springer-Verlag, 1995.123-167 
Varzi C.  Parts, Wholes, and Part-whole Relation. The Pros-
pects of the Mereotopology. Data & Knowledge Engi-
neering, 1996, 20.259-286 
Vieira R, Bick E, Coelho J, Muller V, Collovini S, Souza J 
and Rino L. Semantic Tagging for Resolution of Indirect 
Anaphora. In Proceedings of 7th SIGdial Workshop on 
Discourse and Dialogue. July 2006; Sydney.76-79 
Burges C. A Tutorial on Supporting Vector Machines for 
Pattern Recognition. Data Mining and Knowledge Dis-
covery 1998, 2:121-167 
Ng V. and Cardie C. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of An-
nual Conference for Association of Computational Lin-
guistics 2002, Philadelphia.104-111 
128
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 600 ? 611, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Phrase-Based Context-Dependent Joint Probability 
Model for Named Entity Translation 
Min Zhang1, Haizhou Li1, Jian Su1, and Hendra Setiawan1,2 
1
 Institute for Infocomm Research,  
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, hli, sujian, stuhs}@i2r.a-star.edu.sg 
2
 Department of Computer Science,  
National University of Singapore, Singapore, 117543 
hendrase@comp.nus.edu.sg 
Abstract. We propose a phrase-based context-dependent joint probability 
model for Named Entity (NE) translation. Our proposed model consists of a 
lexical mapping model and a permutation model. Target phrases are generated 
by the context-dependent lexical mapping model, and word reordering is per-
formed by the permutation model at the phrase level. We also present a two-
step search to decode the best result from the models. Our proposed model is 
evaluated on the LDC Chinese-English NE translation corpus. The experiment 
results show that our proposed model is high effective for NE translation.  
1   Introduction 
A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is 
an indispensable component of cross-lingual applications such as machine translation 
and cross-lingual information retrieval and extraction.  
NE is translated by a combination of meaning translation and/or phoneme trans-
literation [1]. NE transliteration has been given much attention in the literature. 
Many attempts, including phoneme and grapheme-based methods, various machine 
learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) 
[4], have been made recently to tackle the issue of NE transliteration. However, 
only a few works have been reported in NE translation. Chen et al [1] proposed a 
frequency-based approach to learn formulation and transformation rules for multi-
lingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the transla-
tion of Arabic NEs to English using monolingual and bilingual resources. Huang et 
al. [6] described an approach to translate rarely occurring NEs by combining pho-
netic and semantic similarities. In this paper, we pay special attention to the issue of 
NE translation.  
Although NE translation is less sophisticated than machine translation (MT) in gen-
eral, to some extent, the issues in NE translation are similar to those in MT. Its chal-
lenges lie in not only the ambiguity in lexical mapping such as <?(Fu),Deputy> and 
<?(Fu),Vice> in Fig.1 in the next page, but also the position permutation and fertility 
of words. Fig.1 illustrates two excerpts of NE translation from the LDC corpus [7]: 
 A Phrase-Based Context-Dependent Joint Probability Model 601 
(a) Regional office of science and technology for Africa 
 
??(FeiZhou) ??(DiQu) ??(KeJi) ???(BanShiChu) 
 
 
(b) Deputy chief of staff to office of the vice president 
 
   ?(Fu) ??(ZongTong) ???(BanGongShi)?(Fu)??(ZhuRen) 
Fig. 1. Example bitexts with alignment 
where the italic word is the Chinese pinyin transcription. 
Inspired by the JSCM model for NE transliteration [4] and the success of statistical 
phrase-based MT research [8-12], in this paper we propose a phrase-based context-
dependent joint probability model for NE translation. It decomposes the NE transla-
tion problem into two cascaded steps: 
1)  Lexical mapping step, using the phrase-based context-dependent joint prob-
ability model, where the appropriate lexical item in the target language is 
chosen for each lexical item in the source language;  
2)  Reordering step, using the phrase-based n-gram permutation model, where 
the chosen lexical items are re-arranged in a meaningful and grammatical 
order of target language.  
A two-step decoding algorithm is also presented to allow for effective search of the 
best result in each of the steps. 
The layout of the paper is as follows. Section 2 introduces the proposed model. In 
Section 3 and 4, the training and decoding algorithms are discussed. Section 5 reports 
the experimental results. In Section 6, we compare our model with the other relevant 
existing models. Finally, we conclude the study in Section 7. 
2   The Proposed Model 
We present our method by starting with a definition of translation unit in Section 2.1, 
followed by the formulation of the lexical mapping model and the permutation model 
in Section 2.2. 
2.1   Defining Translation Unit 
Phrase level translation models in statistical MT have demonstrated significant im-
provement in translation quality by addressing the problem of local re-ordering across 
language boundaries [8-12]. Thus we also adopt the same concept of phrase used in 
statistical phrase-based MT [9,11,12] as the basic NE translation unit to address the 
problems of word fertility and local re-ordering within phrase.  
Suppose that we have Chinese as the source language 1 1... ...=
J
j Jc c c c and Eng-
lish as the target language 1 1... ...
I
i Ie e e e=  in an NE translation 1 1( , )J Ic e , where 
602 M. Zhang et al 
1
J
jc c?  and 1
I
ie e?  are Chinese and English words respectively. Given a directed 
word alignment A :{ 1 1?J Ic e , 1 1?I Je c }, the set of the bilingual phrase pairs ?  is 
defined as follows: 
2 2
1 11 1
1 2 1 2
( , , )={ ( , ) :
                        { ... }, { ... }:
                          }              
j iJ I
j ic e c e
j j j i i i j i
vice versa
?
? ? ? ? ? ?
?
A
A
                   (1)  
The above definition means that two phrases are considered to be translations of 
each other, if the words are aligned exclusively within the phrase pair, and not to the 
words outside [9,11,12]. The phrases have to be contiguous and a null phrase is not 
allowed. 
Suppose that the NE pair 1 1( , )J Ic e  is segmented into X phrase pairs ( 1Xc% , 1Xe% ) ac-
cording to the phrase pair set ? , where 1
Xe% is reordered so that the phrase alignment 
is in monotone order, i.e., xc% is aligned ?% %x xc e For simplicity, we denote by 
,? =< >% %x x xc e  the xth phrase pair in ( 1Xc% , 1Xe% ) = 1... ...x X? ? ? , ? ? ?x . 
2.2   Lexical Mapping Model and Permutation Model 
Given the phrase pair set ? , an NE pair ( 1Jc , 1Ie ) can be rewritten as ( 1Xc% , 1Xe% ) = 
1... ...x X? ? ? = 1X? . Let us describe a Chinese to English (C2E) bilingual training 
corpus as the output of a generative stochastic process: 
 
 
(1) Initialize queue Qc and  Qe as empty sequences; 
(2) Select a phrase pair ,x x xc e? =< >% %  according to the probability distribu-
tion 11( | )xxp ?? ? , remove x?  from ? ; 
(3) Append the phrase xc%  to Qc and append the phrase xe%  to Qe; 
(4) Repeat steps 2) and 3) until ? is empty; 
(5) Reorder all phrases in Qe according to the probability distribution of the 
permutation model; 
(6) Output Qe and Qc . 
 
As 11( | )xxp ?? ?  is typically obtained from a source-ordered aligned bilingual 
corpus, reordering is needed only for the target language. According to this generative 
story, the joint probability of the NE pair ( 1Jc , 1Ie ) can then be obtained by summing 
the probabilities over all possible ways of generating various sets of ? and all possi-
ble permutations that can arrive at ( 1
J
c , 1
I
e ).  This joint probability can be formulated 
 A Phrase-Based Context-Dependent Joint Probability Model 603 
in Eq.(2). Here we assume that the generation of the set ? and the reordering process 
are modeled by n-order Markov models, and the reordering process is independent of 
the source word position. 
1
1 1 1 1 1
1
1
1
( , )= { ( ) * ( | )}
       {( ( | ))* ( | )}
?
?
?
?
=
?
? ??
?
? ?
%
% %X
J I X I X
X
kx X
x x n k
x
p c e p p e e
p p e e
                     (2) 
1
1 1
1
( | ) ( | )xX
x x n
X
kk X
k k k
x
p e e p e e ?
?
=
? ?% % % %                                                      (3) 
where 
1
% Xkke  stands for one of the permutational sequences of 1% Xe  that can yield 1Ie  
by linearly joining all phrases, i.e., 
11
= % XkI ke e ().  The generative process, as formu-
lated above, does not try to capture how the source NE is mapped into the target NE, 
but rather how the source and target translation units can be generated simultaneously 
in the source order and how the target NE can be constructed by reordering the target 
phrases, 1% Xe .  
In essence, our proposed model consists of two sub-models: a lexical mapping 
model (LMM), characterized by 1( | )xx x np ??? ? , that models the monotonic genera-
tive process of phrase pairs; and a permutation model (PM), characterized by 
1( | )x
x x n
k
k kp e e ?
?
% % , that models the permutation process for reordering of the target 
language. The LMM in this paper is among the first attempts to introduce context-
dependent lexical mapping into statistical MT (Och et al, 2003). The PM here is also 
different from the widely used position-based distortion model in that it models 
phrase connectivity instead of position distortion. Although PM functions as an n-
gram language model, it only models the ordering connectivity between target lan-
guage phrases, i.e., it is not in charge of target word selection. 
Since the proposed model is phrase-based and we use conditional joint probability 
in LMM and use context-dependent n-gram in PM, we call the proposed model a 
phrase-based context-dependent joint probability model. 
3   Training 
Following the modeling strategy discussed above, the training process consists of 
three steps: phrase alignment, reordering of corpus, and learning statistical parameters 
for lexical mapping and permutation models. 
3.1   Acquiring Phrase Pairs 
To reduce vocabulary size and avoid sparseness, we constrain the phrase length to up 
to three words and the lower-frequency phrase pairs are pruned out for accurate 
604 M. Zhang et al 
phrase-alignment1. Given a word alignment corpus which can be obtained by means 
of the publicly available GIZA++ toolkit [15], it is very straightforward to construct 
the phrase-alignment corpus by incrementally traversing the word-aligned NE from 
left to right2. The set of resulting phrase pairs forms a lexical mapping table.  
3.2   Reordering Corpus 
The context-dependent lexical mapping model assumes monotonic alignment in the 
bilingual training corpus. Thus, the phrase aligned corpus needs to be reordered so 
that it is in either source-ordered or target-ordered alignment. We choose to reorder 
the target phrases to follow the source order. Only in this way can we use the lexical 
mapping model to describe the monotonic generative process and leave the reordering 
of target translation units to the permutation model.  
3.3   Training LMM and PM  
According to Eq. (2), the lexical mapping model (LMM) and the permutation 
model (PM) can be interpreted as a kind of n-gram Markov model. The phrase pair is 
the basic token of LMM and the target phrase is the basic token of PM. A bilingual 
corpus aligned in the source language order is used to train LMM, and a target lan-
guage corpus with phrase segmentation in their original word order is used to train 
PM. Given the two corpora, we use the SRILM Toolkit [13] to train the two n-gram 
models. 
4   Decoding 
The proposed modeling framework allows LMM and PM decoding to cascade as in 
Fig.2.  
 
Fig. 2. A cascaded decoding strategy 
The two-step operation is formulated by Eq.(4) and Eq.(5). Here, the probability 
summation as in Eq.(2) is replaced with maximization to reduce the computational 
complexity: 
1
1
1
? arg max{ ( | )}
X
X x
x x n
x
e p ?
?
?
=
= ? ??%                                                 (4) 
                                                          
1
  Koehn et. al. [12] found that that in MT learning phrases longer than three words and learning 
phrases from high-accuracy word-alignment does not have strong impact on performance. 
2
  For the details of the algorithm to acquire phrase alignment from word alignment, please refer 
to the section 2.2 & 3.2 in [9] and the section 3.1 in [12].  
%1?Xe
LMM 
Decoder
PM 
Decoder
1
Jc 1Ie
 A Phrase-Based Context-Dependent Joint Probability Model 605 
1
1
1
? arg max{ ( | )}x
x x n
X
kI
k k
x
e p e e ?
??
=
= ? % %                                                 (5) 
LMM decoding: Given the input 1
Jc , the LMM decoder searches for the most prob-
able phrase pair set ? in the source order using Eq.(4). Since this is a monotone 
search problem, we use a stack decoder [14,18] to arrive at the n-best results. 
PM decoding: Given the translation phrase sequence 1?
Xe% from the LMM decoder, 
the PM decoder searches for the best phrase order that gives the highest n-gram score 
by using Eq.(5) in the search space ? , which is all the !X  permutations of the all 
phrases in 1?
Xe% . This is a non-monotone search problem. 
The PM decoder conducts a time-synchronized search from left to right, where time 
clocking is synchronized over the number of phrases covered by the current partial 
path. To reduce the search space, we prune the partial paths along the way.  Two par-
tial paths are considered identical if they satisfy the following both conditions: 
1) They cover the same set of phrases regardless of the phrase order; 
2) The last n-1 phrases and their ordering are identical, where n is the order 
of the n-gram permutation model. 
For any two identical partial paths, only the path with higher n-gram score is retained. 
According to Eq. (5), the above pruning strategy is risk-free because the two partial 
paths cover the exact same portion of input phrases and the n-gram histories for the 
next input phrases in the two partial paths are also identical. 
It is also noteworthy that the decoder only needs to perform / 2X  expansions as 
after / 2X  expansions, all combinations of / 2X  phrases would have been explored 
already. Therefore, after / 2X  expansions, we only need to combine the correspond-
ing two partial paths to make up the entire input phrases, then select the path with 
highest n-gram score as the best translation output. 
Let us examine the number of paths that the PM decoder has to traverse. The prun-
ing reduces the search space by a factor of !Z , from !
( )!
Z
XP
X
X Z
=
?
 
to
!
! ( )!
Z
XC
X
Z X Z
=
? ?
, where Z is the number of phrases in a partial path. 
Since X ZX
Z
XC C
?
= , the maximum number of paths that we have to traverse is / 2XXC . 
For instance, when 10X = , the permutation decoder traverses 510 252C =  paths 
instead of the 510 30, 240P = in an exhausted search. 
By cascading the translation and permutation steps, we greatly reduce the search 
space. In LMM decoding, the traditional stack decoder for monotone search is very 
fast. In PM decoding, since most of NE is less than 10 phrases, the permutation de-
coder only needs to explore at most 510 252C =  living paths due to our risk-free prun-
ing strategy. 
606 M. Zhang et al 
5   Experiments 
5.1   Experimental Setting and Modeling 
All the experiments are conducted on the LDC Chinese-English NE translation corpus 
[7]. The LDC corpus consists of a large number of Chinese-Latin language NE en-
tries. Table 1 reports the statistics of the entire corpus. Because person and place 
names in this corpus are translated via transliteration, we only extract the categories 
of organization, industry, press, international organization, and others to form a cor-
pus subset for our NE translation experiment, as indicated in bold in Table 1. As the 
corpus is in its beta release, there are still many undesired entries in it. We performed 
a quick proofreading to correct some errors and remove the following types of entries:  
1) The duplicate entry; 
2) The entry of single Chinese or English word;  
3) The entries whose English translation contains two or more non-English words. 
We also segment the Chinese translation into a word sequence. Finally, we obtain a 
corpus of 74,606 unique bilingual entries, which are randomly partitioned into 10 
equal parts for 10-fold cross validation.  
Table 1.  Statistics of the LDC Corpus 
# of Entries  
Category C2E E2C 
Person 486,212 572,213 
Place 276,382 298,993 
Who-is-Who 30,028 36,881 
Organization 30,800 37,145 
Industry 54,747 58,468 
Press 29,757 32,922 
Int?l Org 7,040 7,040 
Others 13,007 14,066 
As indicated in Section 1, although MT is more difficult than NE translation, they 
both have many properties in common, such as lexical mapping ambiguity and permu-
tation/distortion. Therefore, to establish a comparison, we use the publicly available 
statistical MT training and decoding tools, which can represent the state-of-the-art of 
statistical phrase-based MT research, to carry out the same NE translation experiments 
as reference cases. All the experiments conducted in this paper are listed as follow: 
1) IBM method C: word-based IBM Model 4 trained by GIZA++3 [15] and ISI 
Decoder4 [14,16]; 
                                                          
3
 http://www.fjoch.com/ 
4
 http://www.isi.edu/natural-language/software/decoder/manual.html 
 A Phrase-Based Context-Dependent Joint Probability Model 607 
2) IBM method D:   phrase-based IBM Model 4 trained by GIZA++ on phrase-
aligned corpus and ISI Decoder working on phrase-segmented testing corpus. 
3) Koehn method: Koehn et al?s phrase-based model [12] and PHARAOH5 de-
coder6; 
4) Our method: phrase-based bi-gram LMM and bi-gram PM, and our two-step 
decoder. 
To make an accurate comparison, all the above three phrase-based models are 
trained on the same phrase-segmented and aligned corpus, and tested on the same 
phrase-segmented corpus. ISI Decoder carries out a greedy search, and PHARAOH is 
a beam-search stack decoder. To optimize their performances, the two decoders are 
allowed to do unlimited reordering without penalty. We train trigram language mod-
els in the first three experiments and bi-gram models in the forth experiment. 
5.2   NE Translation 
Table 2 and Table 3 report the performance of the four methods on the LDC NE 
translation corpus. The results are interpreted in different scoring measures, which 
allow us to compare the performances from different viewpoints.   
? ACC reports the accuracy of the exact;  
? WER reports the word error rate;  
? PER is the position-independent, or ?bag-of-words? word error rate;  
? BLEU score measures n-gram precision [19] 
? NIST score [20] is a weighted n-gram precision.  
Please note that WER and PER are error rates, the lower numbers represent better 
results. For others, the higher numbers represents the better results. 
Table 2.  E2C NE translation performance (%) 
 IBM 
  method C 
IBM 
  method D 
Koehn  
method 
Our  
method 
ACC 24.5 36.3 47.1 51.5 
WER 51.0 38.5 32.5 26.6 
PER 48.5 36.2 26.8 16.3 
BLEU 29.9 41.8 51.2 56.1 O
p e
n
 
te
s t
 
NIST 7.2 8.6 9.3 10.2 
ACC 51.1 78.9 88.2 90.9 
WER 34.1 12.8 6.3 4.3 
PER 31.5 9.5 4.1 2.7 
BLEU 54.7 80.9 89.1 91.9 
E2
C 
Cl
o s
e d
 
te
s t
 
NIST 11.1 14.2 14.7 14.8 
                                                          
5
 http://www.isi.edu/licensed-sw/pharaoh/ 
6
 http://www.isi.edu/licensed-sw/pharaoh/manual-v1.2.ps 
608 M. Zhang et al 
Table 3.  C2E NE translation performance (%) 
 IBM 
  method C 
IBM 
  method D 
Koehn  
method 
Our  
method 
ACC 13.4 21.8 31.2 36.1 
WER 60.8 45.8 41.3 38.9 
PER 49.6 38.2 32.6 26.6 
BLEU 25.1 49.8 52.9 54.1 o
p e
n  
te
s t
 
NIST 5.94 8.21 8.91 9.25 
ACC 34.3 69.5 79.2 81.3 
WER 48.2 23.6 11.3 9.2 
PER 35.7 14.7 8.7 6.2 
BLEU 42.5 76.2 85.7 88.0 
C2
E 
c l
o s
e d
 te
s t 
NIST 8.7 12.7 13.8 14.4 
Table 2 & 3 show that our method outperforms the other three methods consis-
tently in all cases and by all scores. IBM method D gives better performance than 
IBM method C, simply because it uses phrase as the translation unit instead of single 
word. Koehn et al?s phrase-based model [12] and IBM phrase-based Model 4 used in 
IBM method D are very similar in modeling. They both use context-independent 
lexical mapping model, distortion model and trigram target language model. The 
reason why Koehn method outperforms IBM method D may be due to the different 
decoding strategy. However, we still need further investigation to understand why 
Koehn method outperforms IBM method D significantly. It may also be due to the 
different LM training toolkits used in the two experiments. 
Our method tops the performance among the four experiments. The significant po-
sition-independent word error rate (PER) reduction shows that our context-dependent 
joint probability lexical mapping model is quite effective in target word selection 
compared with the other context-free conditional probability lexical model together 
with target word n-gram language model. 
Table 4. Step by step top-1 performance (%) 
 
 
LMM decoder  
 
LMM+PM decoder  
 
E2C 
 
59.9 
 
51.5 
C2E 40.5 36.1 
Table 4 studies the performance of the decoder by steps. The LMM decoder col-
umn reports the top-1 ?bag-of-words? accuracy of the LMM decoder regardless of 
word order. This is the upper bound of accuracy that the PM decoder can achieve. The 
LMM+PM decoder column shows the combined performance of two steps, where we 
 A Phrase-Based Context-Dependent Joint Probability Model 609 
measure the top-1 LMM+PM accuracy by taking top-1 LMM decoding results as 
input. It is found that the PM decoder is surprisingly effective in that it perfectly reor-
ders 85.9% (51.5/59.9) and 89.1% (36.1 /40.5) target languages in E2C and C2E 
translation respectively. 
All the experiments above recommend that our method is an effective solution for 
NE translation. 
6   Related Work 
Since our method has benefited from the JSCM of Li et al [4] and statistical MT 
research [8-12], let us compare our study with the previous related work. 
The n-gram JSCM was proposed for machine transliteration by Li et al [4]. It cou-
ples the source and channel constraints into a generative model to directly estimate 
the joint probability of source and target algnment using n-gram statistics. It was 
shown that JSCM captures rich contextual information that is present in a bilingual 
corpus to model the monotonic generative process of sequential data. In this point, our 
LMM model is the same as JSCM. The only difference is that in machine translitera-
tion Li et al [4] use phoneme unit as the basic modeling unit and our LMM is phrase-
based.  
In our study, we enhance the LMM with the PM to account for the word reorder-
ing issue in NE translation, so our model is capable of modeling the non-monotone 
problem. In contrast, JSCM only models the monotone problem. 
Both rule-based [1] and statistical model-based [5,6] methods have been proposed 
to address the NE translation problem. The model-based methods mostly are based on 
conditional probability under the noisy-channel framework [8]. Now let?s review the 
different modeling methods: 
1) As far as lexical choice issue is concerned, the noisy-channel model, repre-
sented by IBM Model 1-5 [8], models lexical dependency using a context-free 
conditional probability. Marcu and Wong [10] proposed a phrase-based con-
text-free joint probability model for lexical mapping. In contrast, our LMM 
models lexical dependency using n-order bilingual contextual information.  
2) Another characteristic of our method lies in its modeling and search strat-
egy.  NE translation and MT are usually viewed as a non-monotone search 
problem and it is well-known that a non-monotone search is exponentially 
more complex than a monotone search. Thus, we propose the two separated 
models and the two-step search, so that the lexical mapping issue can be re-
solved by monotone search. This results in a large improvement on transla-
tion selection. 
3) In addition, instead of the position-based distortion model [8-12], we use the 
n-gram permutation model to account for word reordering. A risk-free de-
coder is also proposed for the permutation model.  
One may argue that our proposed model bears a strong resemblance to IBM Model 
1: a position-independent translation model and a language model on target sentence 
without explicit distortion modeling. Let us discuss the major differences between 
them: 
610 M. Zhang et al 
1) Our LMM models the lexical mapping and target word selection using a con-
text-dependent joint probability while IBM Model 1 using a context-
independent conditional probability and a target n-gram language model. 
2) Our LMM carries out the target word selection and our PM only models the 
target word connectivity while the language model in IBM Model 1 performs 
the function of target word selection. 
Alternatively, finite-state automata (FSA) for statistical MT were previous sug-
gested for decoding using contextual information [21,22]. Bangalore and Riccardi 
[21] proposed a phrase-based variable length n-gram model followed by a reordering 
scheme for spoken language translation. However, their re-ordering scheme was not 
evaluated by empirical experiments.  
7   Conclusions 
In this paper, we propose a new model for NE translation. We present the training and 
decoding methods for the proposed model. We also compare the proposed method 
with related work. Empirical experiments show that our method outperforms the pre-
vious methods significantly in all test cases. We conclude that our method works 
more effectively and efficiently in NE translation than previous work does.  
Our method does well in NE translation, which is relatively less sophisticated in 
terms of word distortion. We expect to improve its permutation model by integrating 
a distortion model to account for larger sentence structure and apply to machine trans-
lation study. 
Acknowledgments 
We would like to thank the anonymous reviews for their invaluable suggestions on 
our original manuscript. 
References 
1. Hsin-Hsi Chen, Changhua Yang and Ying Lin. 2003. Learning Formulation and Trans-
formation Rules for Multilingual NEs. Proceedings of the ACL 2003 Workshop on 
MMLNER 
2. K. Knight and J. Graehl. 1998. Machine Transliteration. Computational Linguistics, 24(4) 
3. Jong-Hoon Oh and Key-Sun Choi, 2002. An English-Korean Transliteration Model Using 
Pronunciation and Contextual Rules. Proceedings of COLING 2002 
4. Haizhou Li, Ming Zhang and Jian Su. 2004. A Joint Source-Channel Model for Machine 
Transliteration. Proceedings of the 42th ACL, Barcelona,  160-167 
5. Y. Al-Onaizan and K. Knight, 2002. Translating named entities using monolingual and bi-
lingual resources. Proceedings of the 40th ACL, Philadelphia,  400-408 
6. Fei Huang, S. Vogel and A. Waibel, 2004. Improving NE Translation Combining Phonetic 
and Semantic Similarities. Proceedings of HLT-NAACL-2004 
7. LDC2003E01, 2003. http://www.ldc.upenn.edu/ 
 A Phrase-Based Context-Dependent Joint Probability Model 611 
8. P.F. Brown, S.A.D. Pietra, V.J.D. Pietra and R.L. Mercer.1993. The mathematics of statis-
tical machine translation. Computational Linguistics,19(2):263-313 
9. Richard Zens and Hermann Ney. 2004. Improvements in Phrase-Based Statistical Machine 
Translation. Proceedings of HLT-NAACL-2004 
10. D. Marcu and W. Wong. 2002. A Phrase-based, Joint Probability Model for Statistical 
Machine Translation. Proceedings of EMNLP-2002 
11. Franz Joseh Och, C. Tillmann and H. Ney. 1999. Improved Alignment Models for Statisti-
cal Machine Translation. Proceedings of Joint Workshop on EMNLP and Very Large Cor-
pus: 20-28 
12. P. Koehn, F. J. Och and D. Marcu. 2003. Statistical Phrase-based Translation. Proceedings 
of HLT-2003 
13. A. Stolcke. 2002. SRILM -- An Extensible Language Modeling Toolkit. Proceedings of 
ICSLP-2002, vol. 2, 901-904, Denver. 
14. U. Germann, M. Jahr, K. Knight, D. Marcu and K. Yamada. 2001. Fast Decoding and Op-
timal Decoding for Machine Translation. Proceedings of ACL-2001 
15. Franz Joseh Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical 
Alignment Models. Computational Linguistics, 29(1):19-51 
16. U. Germann. 2003. Greedy Decoding for Statistical Machine Translation in Almost Linear 
Time. Proceedings of HLT-NAACL-2003 
17. Christoph Tillmann and Hermann Ney. 2003. Word Reordering and a Dynamic Program-
ming Beam Search Algorithm for Statistical Machine Translation. Computational Linguis-
tics, 29(1):97-133 
18. R. Schwartz and Y. L. Chow. 1990. The N-best algorithm: An efficient and Exact procedure 
for finding the N most likely sentence hypothesis, Proceedings of ICASSP 1990, 81-84 
19. K. Papineni, S. Roukos, T. Ward and W. J. Zhu. 2001. BLEU: a method for automatic 
evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Re-
search Report. 
20. G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram 
co-occurrence statistics. Proceedings of ARPA Workshop on HLT 
21. S. Bangalore and G. Riccardi, 2000, Stochastic Finite State Models for Spoken Language 
Machine Translation, Workshop on Embedded MT System 
22. Stephan Kanthak and Hermann Hey, 2004. FSA: An Efficient and Flexiable C++ Tookkit 
for Finite State Automata Using On-Demand Computation, Proceedings of ACL-2004 
A Twin-Candidate Model of Coreference
Resolution with Non-Anaphor
Identification Capability
Xiaofeng Yang1,2, Jian Su1, and Chew Lim Tan2
1 Institute for Infocomm Research,
21, Heng Mui Keng Terrace, Singapore, 119613
{xiaofengy, sujian}@i2r.a-star.edu.sg
2 Department of Computer Science,
National University of Singapore, Singapore, 117543
{yangxiao, tancl}@comp.nus.edu.sg
Abstract. Although effective for antecedent determination, the tradi-
tional twin-candidate model can not prevent the invalid resolution of
non-anaphors without additional measures. In this paper we propose a
modified learning framework for the twin-candidate model. In the new
framework, we make use of non-anaphors to create a special class of
training instances, which leads to a classifier capable of identifying the
cases of non-anaphors during resolution. In this way, the twin-candidate
model itself could avoid the resolution of non-anaphors, and thus could
be directly deployed to coreference resolution. The evaluation done on
newswire domain shows that the twin-candidate based system with our
modified framework achieves better and more reliable performance than
those with other solutions.
1 Introduction
In recent years supervised learning approaches have been widely used in corefer-
ence resolution task and achieved considerable success [1,2,3,4,5]. Most of these
approaches adopt the single-candidate learning model, in which coreference rela-
tion is determined between a possible anaphor and one individual candidate at a
time [1,3,4]. However, it has been claimed that the reference between an anaphor
and its candidate is often subject to the other competing candidates [5]. Such
information is nevertheless difficult to be captured in the single-candidate model.
As an alternative, several researchers proposed a twin-candidate model [2,5,6].
Instead of directly determining coreference relations, this model would judge the
preference between candidates and then select the most preferred one as the an-
tecedent. The previous work has reported that such a model can effectively help
antecedent determination for anaphors [5,6].
However, one problem exits with the twin-candidate model. For every encoun-
tered NP during resolution, the model would always pick out a ?best? candidate
as the antecedent, even if the current NP is not an anaphor. The twin-candidate
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 719?730, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
720 X. Yang, J. Su, and C.L. Tan
model itself could not identify and block such invalid resolution of non-anaphors.
Therefore, to apply such a model to coreference resolution, some additional ef-
forts have to be required, e.g., using an anaphoricity determination module to
eliminate non-anaphors in advance [5], or using threshold to prevent the selection
of a candidate if the confidence it wins other competitors is low [6].
In this paper, we explore how to effectively apply the twin-candidate model
to the coreference resolution task. We propose a modified learning framework
with the capability of processing non-anaphors. In the framework, we make use of
non-anaphors to create training instances. This special class of instances would
enable the learned classifier to identify the test instances formed by non-anaphors
during resolution. Thus, the resulting model could avoid resolving a non-anaphor
to a non-existent antecedent by itself, without specifying a threshold or using an
additional anaphoricity determination module. Our experiments on MUC data
set systematically evaluated effectiveness of our modified learning framework.
We found that with this new framework, the twin-candidate based system could
not only outperform the single-candidate based one, but also achieve better and
more reliable results than those twin-candidate based systems using the two
mentioned solutions.
The rest of the paper is organized as follows. Section 2 describes the original
framework of the twin-candidate model. Section 3 presents in details the modified
framework, including the training and resolution procedures. Section 4 reports
and discusses the experimental results and finally Section 6 gives the conclusions.
2 The Original Framework of the Twin-Candidate Model
The basic idea of the twin-candidate model is to learn a binary classifier which
could judge the preference between candidates of an anaphor. In this section we
will describe a general framework of such a model.
2.1 Instance Representation
In the twin-candidate model, an instance takes a form like i{C1, C2, M }, where
M is a possible anaphor and C1 and C2 are two of its antecedent candidates.
We stipulate that C2 should be closer to M than C1 in distance. An instance is
labelled as ?10? if C1 is preferred to C2 to be the antecedent, or ?01? if otherwise.
A feature vector would be specified for an instance. The features may describe
the lexical, syntactic, semantic and positional relationships between M and each
one of the candidates, C1 or C2. In addition, inter-candidate features could
be used to represent the relationships between the pair of candidates, e.g. the
distance between C1 and C2 in position.
2.2 Training Procedure
For each anaphor Mana in a given training text, its closet antecedent, Cante,
would be selected as the anchor candidate to compare with other candidates.
A Twin-Candidate Model of Coreference Resolution 721
A set of ?10? instances, i{Cante, Cp, Mana}, is generated by pairing Mana and
Cante, as well as each of the interning candidates Cp. Also a set of ?01? instances,
i{Ca, Cante, Mana}, is created by pairing Cante and each non-antecedental can-
didate Ca before Cante.
Table 1. An example text
[1 Globalstar] still needs to raise [2 $600 million], and [3
Schwartz] said [4 that company] would try to raise [5 the money]
in [6 the debt market] .
Consider the example in Table 1. In the text segment, [4 that company] and
[5 the money] are two anaphors with [1 Globalstar] and [2 $600 million] being
their antecedents respectively. Thus the training instances to be created for this
text would be:
i{[1 Globalstar], [2 $600 million], [4 that company]} : 10
i{[1 Globalstar], [3 Schwartz], [4 that company]} : 10
i{[1 Globalstar], [2 $600 million], [5 the money]} : 01
i{[2 $600 million], [3 Schwartz], [5 the money]} : 10
i{[2 $600 million], [4 that company], [5 the money]} : 10
Based on the training instances, a classifier is trained using a certain machine
learning algorithm. Given the feature vector of a test instance, the classifier
would return ?10? or ?01? indicating which one of the two candidates under
consideration is preferred.
2.3 Resolution
After the classifier is ready, it could be employed to select the antecedent for
an encountered anaphor. The resolution algorithm is shown in Figure 1. In the
algorithm, a round-robin model is employed, in which each candidate is compared
with every other candidate and the final winner is determined by the won-lost
records. The round-robin model would be fair for each competitor and the result
is reliable to represent the rank of the candidates.
As described in the algorithm, after each match between two candidates,
the record of the winning candidate (i.e., the one judged as preferred by the
classifier) will increase and that of the loser will decrease. The algorithm simply
uses a unit of one as the increment and decrement. Therefore, the final record of
a candidate is its won-lost difference in the round-robin matches. Alternatively,
we can use the confidence value returned by the classifier as the in(de)crement,
while we found no much performance difference between these two recording
strategies in experiments.
722 X. Yang, J. Su, and C.L. Tan
algorithm ANTE-SEL
input:
M : the anaphor to be resolved
candidate set: the set of antecedent candidates of M,
{C1, C2, . . . , Ck}
for i = 1 to K
Score[ i ] = 0;
for j = K downto 2
for i = j - 1 downto 1
/*CR returns the classification result*/
if CR(i{Ci, Cj, M}) ) = = 10 then
Score[ i ]++;
Score[ j ]??;
if CR(i{Ci, Cj, M}) ) = = 01 then
Score[ i ]??;
Score[ j ]++;
SelectedIdx = arg
i
max
Ci?candidate set
Score[i];
return CSelectedIdx
Fig. 1. The original antecedent selection algorithm
3 Modified Framework for Coreference Resolution Task
3.1 Non-anaphor Processing
In the task of coreference resolution, it is often that an encountered NP is non-
anaphoric, that is, no antecedent exists among its possible candidates. However,
the resolution algorithm described in the previous section would always try to
pick out a ?best? candidate as the antecedent for each given NP, and thus could
not be applied for coreference resolution directly.
One natural solution to this is to use an anaphoricity determination (AD)
module to identify the non-anaphoric NPs in advance (e.g. [5]). If an NP is judged
as anaphoric, then we deploy the resolution algorithm to find its antecedent.
Otherwise we just leave the NP unresolved. This solution, however, would heavily
rely on the performance of the AD module. Unfortunately, the accuracy that
most state-of-the-art AD systems could provide is still not high enough (around
80% as reported in [7]) for our coreference resolution task.
Another possible solution is to set a threshold to avoid selecting a candidate
that wins with low confidence (e.g. [6]). Specifically, for two candidates in a
match, we update their match records only if the confidence returned from the
classifier is above the specified threshold. If no candidate has a positive record in
the end, we deem the NP in question as non-anaphoric and leave it unresolved.
In other words, a NP would be resolved to a candidate only if the candidate won
at least one competitor with confidence above the threshold.
The assumption under this solution is that the classifier would return low con-
fidence for the test instances formed by non-anaphors. Although it may be true,
A Twin-Candidate Model of Coreference Resolution 723
there exist other cases for which the classifier would also assign low confidence
values, for example, when the two candidates of an anaphoric NP both have
strong or weak preference. The solution of using threshold could not discrimi-
nate these different cases and thus may not be reliable for coreference resolution.
In fact, the above problem could be addressed if we could teach the classi-
fier to explicitly identify the cases of non-anaphors, instead of using threshold
implicitly. To do this, we need to provide a special set of instances formed by
the non-anaphors to train the classifier. Given a test instance formed by a non-
anaphor, the newly learned classifier is supposed to give a class label different
from the instances formed by anaphors. This special label would indicate that
the current NP is a non-anaphor, and no preference relationship is held be-
tween the two candidates under consideration. In this way, the twin-candidate
model could do the anaphoricity determination by itself, without any additional
pre-possessing module. We will describe the modified training and resolution
procedures in the subsequent subsections.
3.2 Training
In the modified learning framework, an instance also takes a form like i{C1,
C2, M }. During training, for an encountered anaphor, we create ?01? or ?10?
training instances in the same way as in the original learning framework, while
for a non-anaphor Mnon ana, we
? From the candidate set, randomly select a candidate Crand as the anchor
candidate.
? Create an instance by pairing Mnon ana, Crand, and each of the candidates
other than Crand.
The above instances formed by non-anaphors would be labelled as ?00?. Note that
an instance may have a form like i{Ca, Crand, Mnon ana} if candidate Ca is pre-
ceding Crand, or like i{Crand, Cp, Mnon ana} if candidate Cp is following Crand.
Consider the text in Table 1 again. For the non-anaphors [3 Schwartz] and
[6 the debt market], supposing the selected anchor candidates are [1 Globalstar]
and [2 $600 million], respectively. The ?00? instances generated for the text are:
i{[1 Globalstar], [2 $600 million], [3 Schwartz]} : 00
i{[1 Globalstar], [2 $600 million], [6 the debt market]} : 00
i{[2 $600 million], [3 Schwartz], [6 the debt market]} : 00
i{[2 $600 million], [4 that company], [6 the debt market]} : 00
i{[2 $600 million], [5 the money], [6 the debt market]} : 00
3.3 Resolution
The ?00? training instances are used together with the ?01? and ?10? ones to
train a classifier. The resolution procedure is described in Figure 2. Like in the
original algorithm, each candidate is compared with every other candidate. The
724 X. Yang, J. Su, and C.L. Tan
difference is that, if two candidates are judged as ?00? in a match, both candi-
dates would receive a penalty of ?1 in their respective record; If no candidate
has a positive final score, then the NP would be deemed as non-anaphoric and
left unresolved. Otherwise, it would be resolved to the candidate with highest
score as usual. In the case when an NP has only one antecedent candidate, a
pseudo-instance is created by paring the candidate with itself. The NP would be
resolved to the candidate if the return label is not ?00?.
Note that in the algorithm a threshold could still be used, for example, to
update the match record only if the classification confidence is high enough.
algorithm ANTE-SEL
input:
M : the new NP to be resolved
candidate set: the candidates set of M, {C1, C2, . . . , Ck}
for i = 1 to K
Score[ i ] = 0;
for j = K downto 2
for i = j - 1 downto 1
if CR(i{Ci, Cj, M}) ) = = 10 then
Score[ i ]++;
Score[ j ]??;
if CR(i{Ci, Cj, M}) ) = = 01 then
Score[ i ]??;
Score[ j ]++;
if CR(i{Ci, Cj, M}) ) = = 00 then
Score[ i ]??;
Score[ j ]??;
SelectedIdx = arg
i
max
Ci?candidate set
Score[i];
if (Score[SelectedIdx] <= 0)
return nil;
return CSelectedIdx;
Fig. 2. The new antecedent selection algorithm
4 Evaluation and Discussion
4.1 Experiment Setup
The experiments were done on the newswire domain, using MUC coreference
data set (Wall Street Journal articles). For MUC-6 [8] and MUC-7 [9], 30 ?dry-
run? documents were used for training as well as 20-30 documents for testing.
In addition, another 100 annotated documents from MUC-6 corpus were also
prepared for the purpose of deeper system analysis. Throughout the experiments,
C5 was used as the learning algorithm [10]. The recall and precision rates of
the coreference resolution systems were calculated based on the scoring scheme
proposed by Vilain et al [11].
A Twin-Candidate Model of Coreference Resolution 725
Table 2. Features for coreference resolution using the twin-candidate model
Features describing the new markable M :
1. M DefNP 1 if M is a definite NP; else 0
2. M IndefNP 1 if M is an indefinite NP; else 0
3. M ProperNP 1 if M is a proper noun; else 0
4. M Pronoun 1 if M is a pronoun; else 0
Features describing the candidate, C1 or C2, of M
5. candi DefNp 1(2) 1 if C1 (C2) is a definite NP; else 0
6. candi IndefNp 1(2) 1 if C1 (C2) is an indefinite NP; else 0
7. candi ProperNp 1(2) 1 if C1 (C2) is a proper noun; else 0
8. candi Pronoun 1(2) 1 if C1 (C2) is a pronoun; else 0
Features describing the relationships between C1(C2) and M :
9. Appositive 1(2) 1 if C1 (C2) and M are in an appositive structure; else 0
10. NameAlias 1(2) 1 if C1 (C2) and M are in an alias of the other; else 0
11. GenderAgree 1(2) 1 if C1 (C2) and M agree in gender; else 0 if disagree; -1
if unknown
12. NumAgree 1(2) 1 if C1 (C2) and M agree in number; else 0 if disagree;
-1 if unknown
13. SentDist 1(2) Distance between C1 (C2) in sentences
14. HeadStrMatch 1(2) 1 if C1 (C2) and M match in head string; else 0
15. NPStrMatch 1(2) 1 if C1 (C2) and M match in full strings; else 0
16. StrSim 1(2) The ratio of the common strings between C1 (C2) and
M , over the strings of C1 (C2)
17. SemSim 1(2) The semantic agreement of C1 (C2) against M in Word-
Net
Features describing the relationships between C1 and C2
18. inter SentDist Distance between C1 and C2 in sentences
19. inter StrSim 0, 1, 2 if StrSim 1(C1, M) is equal to, larger or less than
StrSim 1(C2, M)
20. inter SemSim 0, 1, 2 if SemSim 1(C1, M) is equal to, larger or less than
SemSim 1(C2, M)
The candidates of a markable to be resolved were selected as follows. During
training, for each encountered markable, the preceding markables in the current
and previous four sentences were taken as the candidates. During resolution, for
a non-pronoun, all the preceding markables were included into the candidate
set, while for a pronoun, only the markables in the previous four sentences were
used, as the antecedent of a pronoun usually occurs in a short distance.
For MUC-6 and MUC-7, our modified framework generated 207k training
training instances, three times larger than the single-candidate based system
by Soon et al[3]. Among them, the ratio of ?00?,?01? and ?10? instances was
around 8:2:1. The distribution of the class labels was more balanced than in Soon
et al?s system, where only 5% training instances were positive while others were
all negative.
In our study we only considered domain-independent features that could be
obtained with low computational cost but with high reliability. Table 2 summa-
rizes the features with their respective possible values. Features f1-f17 record
726 X. Yang, J. Su, and C.L. Tan
the properties of a new markable and its two candidates, as well as their relation-
ships. Most of these features could be found in previous systems on coreference
resolution (e.g. [3], [4]). In addition, three inter-candidate features, f18-f20,
mark the relationship between the two candidates. The first one, inter SentDist,
records the distance between the two candidates in sentences, while the latter
two, inter StrSim and inter SemSim compare the similarity scores of the two
candidates, in string-matching and semantics respectively.
To provide necessary information of feature computation, an input raw text
was preprocessed automatically by a pipeline of NLP components. Among them,
the chunking component was trained and tested for the shared task for CoNLL-
2000 and achieved 92% F-score. The HMM based NE recognition component
was capable of recognizing the MUC-style NEs with F-scores of 96.9% (MUC-6)
and 94.3% (MUC-7).
4.2 Results and Discussion
In the experiment we compared four systems:
SC. The system based on the single-candidate model. It was a duplicate of the
system by Soon et al [3]. The feature set used in the baseline system was
similar to those listed in Table 2, except that no inter-candidate feature
would be used and only one set of features related to the single candidate
was required.
TC AD. The system based on the twin-candidate mode with the original learn-
ing framework, in which non-anaphors were eliminated by an anaphoricity
determination module in advance. We built a supervised learning based AD
module similar to the system proposed by Ng and Cardie [7]. We trained
the AD classifier on the additional 100 MUC-6 documents. By adjusting the
misclassification cost parameter of C5, we obtained a set of classifiers capable
of identifying ?positive? anaphors with variant recall and precision rates.
TC THRESH. The system based on the twin-candidate mode with the origi-
nal learning framework, using threshold to discard the low-confidenced com-
parison results between candidates.
TC NEW. The system based on the twin-candidate mode, with our modified
learning framework.
The results of the four systems on MUC-6 and MUC-7 are summarized in
Table 3. In these experiments, five-fold cross-evaluation was performed on the
training data to select the resolution parameters, for example, the threshold for
systems TC THRESH and TC NEW, and final AD classifier for TC AD.
As shown in the table, the baseline system SC achieves 66.1% and 65.9%
F-measure for MUC-6 and MUC-7 data sets. This performance is better than
that reported by Soon et al [3], and is comparable to that of the state-of-the-art
systems on the same data sets.
From the table we could find system TC AD achieves a comparatively high pre-
cision but a low recall, resulting in a F-measure worse than that of SC. The analysis
A Twin-Candidate Model of Coreference Resolution 727
Table 3. The performance of different coreference resolution systems
30 Docs 100 Docs
MUC-6 MUC-7 MUC-6 MUC-7
Experiments R P F R P F R P F R P F
SC 70.4 62.4 66.1 69.8 62.5 65.9 67.9 62.1 64.9 69.8 62.5 65.9
TC AD 62.6 66.4 64.4 60.8 64.7 62.7 61.6 65.4 63.4 60.8 64.6 62.7
TC THRESH 70.7 59.1 64.4 70.0 61.7 65.6 71.0 60.7 65.4 70.6 60.9 65.4
TC NEW 64.8 70.1 67.3 66.0 68.6 67.2 67.0 70.2 68.5 67.0 69.2 68.1
of the AD classifier reveals that it successfully identifies 79.3% anaphors (79.48%
precision) for MUC-6, and 70.9% anaphors (76.3% precision) for MUC-6. That
means, although the pre-processing AD module could partly avoid the wrong res-
olution of a non-anaphor, it eliminates many anaphors at the same, which leads to
the low recall for coreference resolution. Although in resolution different AD clas-
sifiers could be applied, we only observe the tradeoff between recall and precision,
with no effective resolution improvement in F-measure.
In contrast to TC AD, system TC THRESH yields large gains in recall. The
recall, up to above 70%, is higher than all the other three systems. However, the
precision at the same time is unfortunately the lowest. Such a pattern of high
recall and low precision indicates that using threshold could reduce, to some
degree, the risk of eliminating true anaphors, but it would be too lenient to
effectively block the resolution of non-anaphors.
Compared with TC AD and TC THRESH, TC NEW produces large gains in
the precision rates, which rank the highest among all the four systems. Although
the recall also drops at the same time, the increase in the precision could compen-
sate it well; we observe a F-measure of 67.3% for MUC-6 and 67.2% for MUC-7,
significantly better (p ? 0.05, by a sign test) than the other twin-candidate
based systems. These results suggest that with our modified framework, the
twin-candidate model could effectively identify non-anaphors and block their in-
valid resolution, without affecting the accuracy of the antecedent determination
for anaphors.
In our experiment we were interested to evaluate the resolution performance
of TC NEW under different sizes of training data. For this purpose, we used the
additional 100 annotated documents for training, and plotted the learning curve
in Figure 3. The curve indicates that the system could perform well with a small
number of training data, while the performance would get further improved with
more training data (the best performance is obtained on 90 documents).
In Table 3, we also summarized the results of different systems trained on 100
documents. In contrast to TC NEW, we find for system SC, there is no much
performance difference between using 30 and 100 training documents. This is
consistent with the report by Soon et al [3] that the single-candidate model
would achieve the peak performance with a moderate size of data. In the table
we could also find that the performance improvement of TC NEW against the
other three systems is apparently larger on 100 training documents than on
30 documents.
728 X. Yang, J. Su, and C.L. Tan
0 10 20 30 40 50 60 70 80 90 100
60
61
62
63
64
65
66
67
68
69
70
Number of Documents Trained On
F?
m
ea
su
re
Fig. 3. Learning curve of system TC NEW on MUC-7
40 50 60 70 80 90 100
30
35
40
45
50
55
60
65
70
75
80
Recall
Pr
ec
is
io
n
TC_THRESH
TC_AD
TC_NEW
Fig. 4. Recall and precision results for the twin-candidate based systems
In Figure 4, we plotted the variant recall and precision scores that the three
twin-candidate based systems were capable of producing when trained on 100
documents. (Here we only showed the results for MUC-7. Similar results could
be obtained for MUC-6). In line with the results in Table 3, system TC AD
tends to obtain a high precision but low recall, while system TC THRESH
tends to obtain a high recall but low precision. Comparatively, system TC NEW
produces even recall and precision. For the range of recall within which the
three systems coincide, TC NEW yields higher precision than the other two
systems. This figure further proves the effectiveness of our modified
learning framework.
As mentioned, in systems TC THRESH and TC NEW the threshold pa-
rameter could be adjusted. It would be interesting to evaluate the influence of
different thresholds on the resolution performance. In Figure 5 we compared the
recall and precision of two systems, with thresholds ranging from 65 to 100.
In TC THRESH, when the threshold is low, the recall is almost 100% while
the precision is quite low. In such a case, all the markables, regardless anaphors or
A Twin-Candidate Model of Coreference Resolution 729
65 70 75 80 85 90 95 100
30
40
50
60
70
80
90
100
Threshold
R
es
ul
ts
TC_THRESH
Recall
Precision
65 70 75 80 85 90 95 100
0
10
20
30
40
50
60
70
80
90
Threshold
R
es
ul
ts
TC_NEW
Recall
Precision
Fig. 5. Performance of TC THRESH and TC NEW under different thresholds
non-anaphors, will be resolved. As a consequence, all the occurring markables in a
document tends to be linked together. In fact, the effective range of the threshold
that leads to an acceptable performance is quite short. The threshold would only
work when it is considerably high (above 95). Before that, the precision remains
very low (less than 40%) while the recall keeps going down with the increase of
the threshold.
By contrast, in TC NEW, both the recall and precision vary little unless the
threshold is extremely high. That means, the threshold would not impose much
influence on the resolution performance of TC NEW. This should be because in
the modified framework, the cases of non-anaphors are determined by the special
class label ?00?, instead of the threshold as in TC THRESH. The purpose of
using threshold in TC NEW is not to identify the non-anaphors, but to improve
the accuracy of class labelling. Indeed, we could obtain a good result without
using any threshold in TC NEW. These further confirm our claims that the
modified learning framework could perform more reliably than the solution of
using threshold.
5 Conclusions
In this paper we aimed to find an effective way to apply the twin-candidate model
into coreference resolution task. We proposed a modified learning framework in
which non-anaphors were utilized to create a special class of training instances.
With such instances, the resulting classifier could avoid the invalid resolution of
non-anaphors, which enables the twin-candidate model to be directly deployed to
coreference resolution, without using an additional anaphoricity determination
module or using a pre-defined threshold.
In the paper we evaluated the effectiveness of our modified framework on
the MUC data set. The results show that the system with the new framework
outperforms the single-candidate based system, as well as the twin-candidate
730 X. Yang, J. Su, and C.L. Tan
based systems using other solutions. Especially, the analysis of the results indi-
cates that our modified framework could lead to more reliable performance than
the solution of using threshold. All these suggest that the twin-candidate model
with the new framework is effective for coreference resolution.
References
1. McCarthy, J., Lehnert, Q.: Using decision trees for coreference resolution. In:
Proceedings of the 14th International Conference on Artificial Intelligences. (1995)
1050?1055
2. Connolly, D., Burger, J., Day, D. New Methods in Language Processing. In: A
machine learning approach to anaphoric reference. (1997) 133?144
3. Soon, W., Ng, H., Lim, D.: A machine learning approach to coreference resolution
of noun phrases. Computational Linguistics 27 (2001) 521?544
4. Ng, V., Cardie, C.: Improving machine learning approaches to coreference resolu-
tion. In: Proceedings of the 40th Annual Meeting of the Association for Compu-
tational Linguistics, Philadelphia (2002) 104?111
5. Yang, X., Zhou, G., Su, J., Tan, C.: Coreference resolution using competition
learning approach. In: Proceedings of the 41st Annual Meeting of the Association
for Computational Linguistics, Japan (2003)
6. Iida, R., Inui, K., Takamura, H., Matsumoto, Y.: Incorporating contextual cues in
trainable models for coreference resolution. In: Proceedings of the 10th Conference
of EACL, Workshop ?The Computational Treatment of Anaphora?. (2003)
7. Ng, V., Cardie, C.: Identifying anaphoric and non-anaphoric noun phrases to im-
prove coreference resolution. In: Proceedings of the 19th International Conference
on Computational Linguistics (COLING02). (2002)
8. MUC-6: Proceedings of the Sixth Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA (1995)
9. MUC-7: Proceedings of the Seventh Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA (1998)
10. Quinlan, J.R.: C4.5: Programs for machine learning. Morgan Kaufmann Publishers,
San Francisco, CA (1993)
11. Vilain, M., Burger, J., Aberdeen, J., Connolly, D., Hirschman, L.: A model-
theoretic coreference scoring scheme. In: Proceedings of the Sixth Message under-
standing Conference (MUC-6), San Francisco, CA, Morgan Kaufmann Publishers
(1995) 45?52
An effective method of using Web based information for Relation Extraction
Yong Wai Keong, Stanley
Institute for Inforcomm Research
21 Heng Mui Keng Terrace,
Singapore 119613
wkyong@i2r.a-star.edu.sg
Su Jian
Institute for Inforcomm Research
21 Heng Mui Keng Terrace,
Singapore 119613
sujian@i2r.a-star.edu.sg
Abstract
We propose a method that incorporates
paraphrase information from the Web to
boost the performance of a supervised re-
lation extraction system. Contextual infor-
mation is extracted from the Web using a
semi-supervised process, and summarized
by skip-bigram overlap measures over the
entire extract. This allows the capture of lo-
cal contextual information as well as more
distant associations. We observe a statisti-
cally significant boost in relation extraction
performance.
We investigate two extensions, thematic
clustering and hypernym expansion. In tan-
dem with thematic clustering to reduce noise
in our paraphrase extraction, we attempt to
increase the coverage of our search for para-
phrases using hypernym expansion.
Evaluation of our method on the ACE 2004
corpus shows that it out-performs the base-
line SVM-based supervised learning algo-
rithm across almost all major ACE relation
types, by a margin of up to 31%.
1 Introduction and motivation
In this paper, we shall be primarily dealing with
the sort of relations defined in the NIST?s Auto-
matic Content Extraction program, specifically for
the Relation Detection and Characterization (RDC)
task (Doddington et al, 2004). These are links be-
tween two entities mentioned in the same sentence,
and further restrict our consideration to those rela-
tionships clearly supported by evidence in the scope
of the same document.
The ACE?s annotators mark all mentions of re-
lations where there is a direct syntactic connection
between the entities, i.e. when one entity mention
modifies another one, or when two entity mentions
are arguments of the same event. Relations between
entities that are implied in the text but which do not
satisfy either requirement are considered to be im-
plicit, and are marked only once.
Our work sits squarely in the realm of work on
regular IE done by (Zelenko et al, 2003; Zhou et
al., 2005; Chen et al, 2006). Here, the corpus of
interest is a well defined set of texts, such as news
articles, and we have to detect and classify all ap-
pearances of relations from a set of given relation
types in the documents. In line with assumptions
in the related work, we assert that the differences in
the markup for implicit and explicit relations does
not significantly affect our performance.
Supervised learning methods have proved to be
some of the most effective for regular IE. They do
however, need large volumes of expensively anno-
tated examples to perform robustly. As a result, even
the large ACE compilation has deficiencies in the
number of instances available for some of the rela-
tion types. (Zhang et al, 2005) reports an F-score
of 50% for categories of intermediate size and only
30% for the least common relation types. It appears
that there are hard limits on the performance of rela-
tion extraction systems as long as they have to rely
solely on information in the training set.
We were thus inspired to explore how one could
350
exploit theWeb, the largest raw text collection freely
available, for regular IE. In this paper, we detail
the ways one can fruitfully employ relation spe-
cific sentences retrieved from the Web with a semi-
supervised labeling approach. Most importantly we
show how the output from such an approach can be
combined with existing knowledge gleaned from su-
pervised learning to improve the performance of re-
lation extraction significantly.
2 Related Work and Differences
To our knowledge, there is no previous work that ex-
ploits the information from a large raw text corpus
like the Web to improve supervised relation extrac-
tion. In the spirit of the work done by (Shinyama
and Sekine, 2003; Bunescu and Mooney, 2007), we
are trying to collect clusters of paraphrases for given
relation mentions. Briefly, since the same relation
can be expressed in many ways, the information we
may learn about that relation in any single sentence
is very limited. The idea then is to alleviate this bias
by collecting many paraphrases of the same relation
instance into clusters when we train our system.
Shinyama generalizes the expressions using part-
of-speech information and dependency tree similar-
ity into generic templates. Bunescu?s work uses a
relation kernel on subsquences of words developed
in (Bunescu and Mooney, 2005). We observed that
both approaches suffer from low recall despite the
attempts to generalize the subsequences and tem-
plates probably because they rely on local context
only.
Based on our observation, we looked for a way to
use our clusters without losing non-local informa-
tion about the sentences. Bag-of-words or unigram
representations of our paraphrase clusters are easy
to compute, but information about word ordering is
lost. Hence, we settled on the use of a skip-bigram
representation of our relation clusters instead.
Skip-bigrams (Lin and Och, 2004) are pairs of
words in sentence order allowing for gaps in be-
tween. Longer gaps capture far-flung associations
between words, while short gaps of between 1 and 4
capture local structure. In using them, we do not re-
strict ourselves to context centered around the entity
mentions. Another advantage of using skip-bigrams
is that we can capture some extra-sentential infor-
mation since we are no longer restricted by the abil-
ity to generate dependency structures within a single
sentence as Shinyama is.
Using skip-bigrams, we can assess the similarity
of a particular new relation mention instance against
the relation clusters we collect in training. We can
then compute a likelihood that we combine with the
predictions of the supervised learning algorithm for
final classification.
Two possible extensions to the basic method
stated above were examined.
A central problem with the paraphrase collection
approach when applied to an open corpus is noise.
As pointed out by (Bunescu and Mooney, 2007),
even though the same entities co-occur in multiple
sentences, they are not necessarily linked by the
same relationship in all of them. The problem is
exacerbated when the open corpus we look at con-
tains documents from heterogenous domains. In-
deed, we cannot even assume that the predominant
relation that holds between two entities in the set of
sentences is the relation of interest to us.
One means of combating this is suggested by
(Bunescu and Mooney, 2007). They re-weight the
importance of word features in their model to re-
duce topic drift. We try a different solution based on
the thematic clustering of sentences. Sentences ex-
tracted from the raw corpus are mapped to a vector
space and partitioned into different clusters using the
Partitioning Around Medoids algorithm (Kaufmann
and Rousseeuw, 1987). Sentences in the clusters
closest to the original relation mention instance are
more likely to embody the same relationship. Hence
we retain such clusters, while discarding the rest. As
the relation we wish to recover may not be the pre-
dominant one, the cluster that is retained is also of-
ten not the largest one.
Another problem identified by Shinyama is that
the same entity may itself be referred to in differ-
ent ways. If the form used in the original relation
mention is uncommon, then few paraphrases will be
found. For instance, ??President Bush?? may be re-
ferred to as ??Dubya?? by a writer. Searching for
sentences online with the word ??Dubya?? and the
other entity participating in the relation is likely to
result in a collection heavily biased towards the orig-
inator of the nickname. Shinyama?s solution is to
use a limited form of co-reference resolution to re-
351
place these forms with a more general noun phrase.
As co-reference resolution is itself an unreliable pro-
cess, we suggest the use of hypernym substitution
instead.
In subsequent sections, we will outline the struc-
ture of our system, examine the experimental evi-
dence of its viability using the ACE program data,
and finish with a discussion of the extensions.
3 Overall structure
Our system is organized very naturally into two
main phases, a learning or training phase, followed
by a usage or testing phase. The learning phase is
subdivided into two parallel paths, reflecting the hy-
brid nature of our algorithm.
Fully supervised learning based on annotations
takes place in tandem with a semi-supervised algo-
rithm that captures the paraphrase clusters using en-
tity mention instances. We will combine the mod-
els from both the supervised learning and the semi-
supervised algorithm using a meta-classifier trained
a different subset of the data.
3.1 Learning Procedure
Our goal is to acquire as much contextual informa-
tion from the available annotations as possible via
our supervised learner and expand on that usingWeb
based information found by our semi-supervised al-
gorithm.
We constructed our fully supervised learner ac-
cording to the specifications for the system devel-
oped by (Zhou et al, 2005). It utilizes a feature
based framework with a Support Vector Machine
(SVM) classifier (Cortes and Vapnik, 1995). We
support the same set of features as Zhou, namely: lo-
cal word position features, entity type, base phrase
chunking features, dependency and parse tree fea-
tures as well as semantic information like coun-
try names and a list of trigger words. In our cur-
rent work, we use Michael Collins? (Collins, 2003)
parser for syntactic information.
Sentence boundary detection, chunking, and pars-
ing are done as preprocessing steps before we begin
our learning.
Given a sentence with the relation mention in-
stance, the semi-supervised method goes through the
following five stages:
1. From the list of entitites marked in the sen-
tence, generate all possible pairings as candi-
dates. We pick one of these candidates and pro-
ceed to the next step.
2. Gather hypernyms for each entity mention us-
ing Wordnet synsets and generate all possible
combinations of entity pairs from the two sets.
3. Find sentences from the Web that mention both
entities.
4. Cluster the sentences found using k-medoids,
filter out noise and retain only the cluster that
the original relation mention would be assigned
to.
5. Collate all clusters by relation type and gener-
ate a skip-bigram index for each relation type.
We will spend the rest of this section on the details
of each stage.
3.1.1 Extracting entity mentions and gathering
hypernyms
The process starts when we receive a relation
mention and the sentence it was found in, for in-
stance in the following sentence from the ACE 2004
corpus: ?As president of Sotheby?s, she often con-
ducted the biggest, the highest profile auctions.?
From the annotation, we find that the two
entities of interest are president and Sotheby?s
in the EMP-ORG Executive relation, which
we could represent as the predicate EMP ?
ORG Executive(em1, em2). Since our aim is to
find as many instances of semantically similar sen-
tences as possible, we want to lessen the negative
impact of quirky spelling or naming in a given sen-
tence. Hence we do a hypernym search in Wordnet
to find more general terms for our entities and create
list of similarly related entities (em1?, em2?) etc. If
the mention is a named entity, we do without the hy-
pernym expansion and use coreference information
(when available) to find the most common substring
amongst the mentions for the same entity.
In this example, it might result in the four pairs
show in Table 1.
352
Table 1: Examples of entity pairs after hypernym
expansion
President Sotheby?s
Chief Executive Sotheby?s
Decision maker Sotheby?s
leader Sotheby?s
Table 2: Examples of extracted text from Google
The 40 year old former president
travels incognitio to Sotheby?s
Brooks was named president of
Sotheby?s Inc.
Subsidiary President at Sotheby?s...
Bill Ruprecht, chief executive of Sotheby?s,
agreed that September 11 had been...
The Duke will also remain leader
of Sotheby?s Germany...
3.1.2 Web search
(Geleijnse and Korst, 2006) use Google as their
search engine for extracting surface patterns from
Web documents. We use the same procedure here
to find our paraphrases. For each pair of arguments,
we create a boolean query string em1 ? em2, and
submit it to Google. The query will find documents
where mentions of em1 are separated from em2 by
any number of words. We restrict the language to
English.
Google returns a two line extract from the docu-
ments that match our boolean query. The extracts
are generally those lines where the key query terms
are most densely collocated. These are parsed
and obviously nonsensical sentences are discarded
based on the occurrence of words from a list of stop
words. If a group of sentences are very similar,
we choose a single representative and discard the
rest. For every remaining sentence, we normalize
them by removing extraneous HTML tags. Some
examples of extracts found are listed in Table 2.
3.1.3 Cluster, filter and collate
In general, the collection of sentence extracts we
have at the end of the previous stage are likely to be
about a diverse range of topics. As we are only inter-
ested in the subset that is most compatible with the
thematic content of our original relation mention, we
will have to filter away unrelated extracts. For exam-
ple, the EMP-ORG Executive relation does not hold
in the sentence: ?The 40 year old former president
travels incognito to Sotheby?s?.
We make use of the K-medoids method by (Kauf-
mann and Rousseeuw, 1987). The terms from all
sentences are extracted and their frequency in each
sentence is computed. Each sentence is nowmapped
as a vector of frequencies in the space of the terms
that we observed. The resulting vectors are stored
as a large matrix. Picking a random partition of the
sentences as our starting point, we assign some sen-
tences to be the cluster centers, and iteratively refine
the clusters based on a distance minimization heuris-
tic.
Through some preliminary experiments, we find
that K-medoids based clustering with 5 classes pro-
duced the most consistent results. From a list of
excerpts, our algorithm culls the sentences that be-
longed to the 4 irrelevant clusters and produces
the excerpts which capture the original relationship
best.Since the quality of the partitions produced by
the algorithm is sensitive to the initial random start,
we do this process twice with different configura-
tions and take the union of the two clusters as our
final result.
The best excerpts are stored with accompanying
meta-data about the originating training relation in-
stance in what we call pseudo-documents. We group
the pseudo-documents in our database by the rela-
tion label that the instance pairs were given. Thus
we end up with several bags of pseudo-documents,
where each bag corresponds to a single relation type
of interest.
For computational efficiency, we generate an in-
verted hash-index for the pseudo-documents. Our
skip-bigrams act as the keys and the records are lists
of meta-data nodes. Each node records the sentence
that the bigram is observed in, the relation type of
that sentence, and the position of the bigram.
All we need now is a means of measuring the sim-
ilarity of a new relation mention instance with the
bags of pseudo-documents to assign a relation label
to it.
353
3.1.4 Skip-bigrams
As discussed in the introduction, instead of gen-
eralizing our bags of documents into patterns or re-
lation extraction kernels, we create skip-bigram in-
dices. There are several advantages in doing so.
Skip-bigrams are easy to compute relative to
the dependency trees or subsequence kernels used
in (Shinyama and Sekine, 2003) or (Bunescu and
Mooney, 2005). Moreover, we can tune the num-
ber of gaps allowed to capture long-distance word
dependency information, which is not done by the
other approaches because it is relatively more expen-
sive for them to do so, due to the combinatorial ex-
plosion. In addition, as compared to Bunescu?s sub-
sequence approach which needs 4 words to match,
bigrams are far less likely to be sparse in the docu-
ment space.
Since we relied upon skip-bigrams in our queries
to Google, it is only natural that we use it again in
assessing the similarity of two pseudo-documents.
Each pseudo-document is really an extractive sum-
mary of online articles about the same topic, with
the same entities. The degree of overlap between
two pseudo-documents is a good measure of their
thematic overlap.
Now that we have a metric, that still leaves the
question of our matching heuristic open. Do we au-
tomatically assign a test instance the relation label
of the sentence with the highest skip-bigram over-
lap? This naive approach is problematic. In gen-
eral, longer sentences will have more bigrams and
hence higher probability of overlapping with other
sentences. We could normalize the bigram overlap
score by the length of the sentences, but here we
leave the optimization to a machine learner.
Another possible heuristic is to pick the relation
label whose bag has the highest number of match-
ing bigrams with our test instance. Again, this will
be biased, but now towards bags with larger num-
bers of pseudo-documents. A last possibility is to
look at the total number of sentences that have bi-
gram matches, and weight the overlap score higher
for those with more sentence matches.
Therefore, instead of designing the heuristic ex-
plicitly, we use a validation set to observe the statis-
tical correlations of each of the three possible heuris-
tics we discussed above. We train an additional
model, using an SVM to choose the weights for each
heuristic automatically.
Accordingly, we do the following for each valida-
tion instance, V, and its pseudo-document P.
For each extracted sentence j in pseudo-
document P, we look up the database of pseudo-
documents from our training set, and compute the
skip-bigram similarity with every single sentence.
We have a skip-bigram similarity score for every sin-
gle sentence in the database with respect to V. The
scores are collated according to the relation classes.
For each relation class we generate three numbers,
TopS, Matching docs, and Total. Using the nota-
tion by (Lin and Och, 2004), we denote the skip-
bigram overlap between two sentences X and Y as
Skip2(X,Y ). For the ith relation, Ci is the set of all
pseudo-documents in our training set of that relation
type.
TopS = max
Y ?Ci,j?P
Skip2(Pj , Y ) (1)
Matching =
?
Y ?Ci
?
j?P
I[Skip2(Pj , Y ) > 0] (2)
Total =
?
Y ?Ci
?
j?P
Skip2(Pj , Y ) (3)
The three figures provide a summary of the best
sentence level match, and the overall relation level
overlap in terms of the number of sentences and
number of overlaps. As an illustration, we consider
the case where we have two sentences in our pseudo-
document P = P1, P2 and a relation RX . We com-
pute Skip2(P1, Y ) and Skip2(P2, Y ) by looking up
the skip-bigrams in the database for RX and aggre-
gating over sentences. Let?s assume that |RX| = 3
and only Skip2(P1, Y1) = 2, Skip2(P1, Y3) = 5,
Skip2(P2, Y3) = 4 are non-zero. Then TopS for in-
stance V is 5. Matching will be 2 since only Y1 and
Y3 have overlaps with elements of P . The Total is
simply 2 + 5 + 4 = 11.
3.2 Combining supervised with
semi-supervised models
After the preceding steps, we have a trained SVM
model based, and our skip-bigram index from the
semi-supervised procedure. In this section, we will
describe a method of combining these into a better
classifier.
354
The validation data we left aside earlier is sent
through our system with the relation labels removed.
Each entity pair in this validation set has a corre-
sponding pseudo-document and a file with numeri-
cal features for the SVM model.
An instance V is scored by Zhou?s SVM clas-
sifier, which assigns a relation tag, VST to it.
In parallel, the skip-bigram assessment results in
{TopS,Matching, Total} scores for each of the
relation classes. We treat the tag and numbers as fea-
tures for training another SVM, which we shall refer
to as SVMC . This is our final meta-classifier for
relation extraction on the tenth of the original data
set aside for testing. The meta-classifier may also be
used for completely new data.
4 Experimentation
We use the ACE corpus provided by the LDC from
2004 to train and evaluate our system. There are
674 annotated text documents and 9683 relation in-
stances in the set.
Starting with a single training set provided by
the user, we split that into three parts: the major-
ity (80%) is used for the learning phase, one tenth
is used for the validation during construction of the
combined model, and the remaining tenth is used for
testing. We ran a series of experiments, using five-
fold cross-validation.
Unlike typical cross-validation, the fifth that we
set aside is further sub-divided into two parts as we
stated before. Half is used when we construct the hy-
brid model merging supervised and semi-supervised
paths, and the remainder is used for the actual testing
and evaluation.
We used the 6 main relation types defined in the
annotation for the ACE dataset: ART, EMP-ORG,
GPE-AFF, OTHER-AFF, PER-SOC, and PHYS.We
computed three measures of the effectiveness, the
recall (R), precision (P) and F-1 score (F-1).
4.1 Comparison against the baseline
The first set of experiments we shall discuss com-
pares the overall system against our baseline. The
baseline system is implemented as a feature extrac-
tion module on top of a set of binary SVM classi-
fiers. A primary classifier is used to separate the
candidates without any relation of interest from the
rest. Secondary classifiers for each relation type are
then used to partition the positive candidates by vot-
ing. The performance of our baseline classifier when
tested on the ACE2003 dataset is statistically indis-
tinguishable from that reported by Zhou et al in
(Zhou et al, 2005).
Drilling down to the level of individual relation
classes as shown below, we note that the meta-
classifier performs better than the baseline on all but
one of the relations. This might be due to the inher-
ent ambiguity of the OTHER-AFF class.
Relation Ratio System R P F-1
ART
5.6 Hybrid 0.48 0.73 0.59
baseline 0.29 0.43 0.34
EMP-ORG
40.0 Hybrid 0.78 0.75 0.76
baseline 0.67 0.83 0.73
GPE-AFF
11.7 Hybrid 0.49 0.59 0.53
baseline 0.36 0.56 0.45
OTHER-AFF
3.4 Hybrid 0.14 0.18 0.16
baseline 0.18 0.59 0.28
PER-SOC
9.7 Hybrid 0.63 0.80 0.70
baseline 0.32 0.5 0.39
PHYS
29.4 Hybrid 0.75 0.59 0.66
baseline 0.40 0.64 0.49
The hybrid system has slightly lower precision
on the two largest relation classes, EMP-ORG and
PHYS, but higher recall, resulting in better F-scores
on both types. Finally, note that on the three inter-
mediate sized classes, ART, PER-SOC, and GPE-
AFF, the recall and precision were both higher. The
results suggest that the Web information does im-
prove recall substantially, but affects precision in
cases where there already is a substantial amount of
training data. It confirms our original assertion that
the hybrid approach works well for mid-sized rela-
tion classes, where the amount of training data is not
enough for the supervised system to perform opti-
mally.
We use the T-test to see if our improvements over
the baseline were significant at a 0.05 level. To
summarize, recall for the PHYS, PER-SOC, GPE-
AFF and EMP-ORG relations was improved signif-
icantly. The difference in precision is significant for
the PER-SOC, and OTHER-AFF classes, while F-1
score differences for the PER-SOC and PHYS rela-
tion classes at 0.00085, 0.032 respectively were both
significant.
355
4.2 Testing hypernym expansion and clustering
Subsequent experiments were aimed at quantifying
the contribution of hypernym expansion and the-
matic clustering to our hybrid system. We ran a
2 factorial experiment with four of our five folds,
where we take the hypernym expansion and cluster-
ing as treatments. Since we have more than one fea-
ture being tested, and we wish to observe the relative
contribution of each factor, we used an ANOVA test
instead of T-tests (Montgomery, 2005).
Our intuitive justification for hypernym expansion
was that recall would be boosted for relation types
where the name entities tend to be overly specific
in the training corpus. Place names, personal names
and the object names are obvious targets. Indeed, we
noted that the recall did increase in absolute terms
on average for the ART, PER-SOC and PHYS rela-
tion types (about 3% for each), but declined slightly
(about 0.5%) for the rest. Overall however, the size
of the effects was too small to be statistically sig-
nificant. This suggests that other methods of term
generalization may be needed to achieve a larger ef-
fect.
Next, we looked at the contribution of clustering.
Our initial experiments showed that k-medoids with
5 clusters was able to produce very precise clusters.
However, it would be at the expense of some of the
potential gain in recall form Web extracts.
Our experiments shows that clustering does in-
deed lower the potential recall. However, the hoped
for improvement in precision was observed only in
the PER-SOC (6%) and GPE-AFF (0.7%) relations.
This suggests that the effect of name entities having
multiple relations is concentrated in the classes of
named entities related to Persons and GPEs. Again,
the size of the effects was not statistically significant.
A more thorough investigation of clustering tech-
niques with different settings for k and different
algorithms will be needed before we can make
stronger statements.
5 Discussion and Conclusion
We have presented a hybrid approach to relation
extraction that incorporates Web based information
successfully to boost the performance of state-of-
the-art supervised feature based systems. Evaluation
on the ACE corpus shows that our skip-bigram based
relevance measures for finding the right paraphrase
in our Web extract database are very effective.
While our analysis shows that the addition of clus-
tering and hypernym expansion to the skip-bigram
based process is not statistically significant, we have
indications that the effect on recall and precision is
positive for certain relation classes.
In future work, we will examine improvements to
the clustering algorithm to reduce the impact on re-
call. We will look at alternative ways of attacking
the problem of name entity generalization and assess
the impact of methods like co-reference resolution in
the same ANOVA framework.
Acknowledgment
This research is supported by a Specific Tar-
geted Research Project (STREP) of the European
Union?s 6th Framework Programme within IST call
4, Bootstrapping Of Ontologies and Terminologies
STtragegic REsearch Project (BOOTStrep).
References
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In NIPS.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
576?583, Prague, Czech Republic, June. Association
for Computational Linguistics.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zheng-
Yu Niu. 2006. Relation extraction using label prop-
agation based semi-supervised learning. In ACL. The
Association for Computer Linguistics.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. volume 20, pages 273?297.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program. tasks, data and evaluation.
G. Geleijnse and J. Korst. 2006. Learning ef-
fective surface text patterns for information ex-
traction. In Proceedings of the EACL 2006
workshop on Adaptive Text Extraction and Min-
ing (ATEM 2006), pages 1 ? 8, Trento, Italy,
356
April. The Association for Computational Linguistics.
http://acl.ldc.upenn.edu/eacl2006/ws06 atem.pdf.
L. Kaufmann and P. J. Rousseeuw. 1987. Clustering
by means of medoids. In Y. Dodge, editor, Statistical
Data Analysis based on the L1 Norm, pages 405?416,
Amsterdam. Elsevier/North Holland.
Chin Y. Lin and Franz J. Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of Col-
ing 2004, pages 501?507, Geneva, Switzerland, Aug
FebruaryMarch?Aug FebruaryJuly. COLING.
Douglas C. Montgomery. 2005. Design and analysis of
experiments. John Wiley And Sons.
Yusuke Shinyama and Satoshi Sekine. 2003. Paraphrase
acquisition for information extraction. In Kentaro Inui
and Ulf Hermjakob, editors, Proceedings of the Sec-
ond International Workshop on Paraphrasing, pages
65?71.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. Journal of Machine Learning Research,
3:1083?1106.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In IJCNLP, pages 378?
389.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation extrac-
tion. In ACL. The Association for Computer Linguis-
tics.
357
A Twin-Candidate Model for Learning-Based
Anaphora Resolution
Xiaofeng Yang?
Institute for Infocomm Research
Jian Su??
Institute for Infocomm Research
Chew Lim Tan?
School of Computing,
National University of Singapore
The traditional single-candidate learning model for anaphora resolution considers the antecedent
candidates of an anaphor in isolation, and thus cannot effectively capture the preference relation-
ships between competing candidates for its learning and resolution. To deal with this problem,
we propose a twin-candidate model for anaphora resolution. The main idea behind the model
is to recast anaphora resolution as a preference classification problem. Specifically, the model
learns a classifier that determines the preference between competing candidates, and, during
resolution, chooses the antecedent of a given anaphor based on the ranking of the candidates. We
present in detail the framework of the twin-candidate model for anaphora resolution. Further,
we explore how to deploy the model in the more complicated coreference resolution task. We
evaluate the twin-candidate model in different domains using the Automatic Content Extraction
data sets. The experimental results indicate that our twin-candidate model is superior to the
single-candidate model for the task of pronominal anaphora resolution. For the task of coreference
resolution, it also performs equally well, or better.
1. Introduction
Anaphora is reference to an entity that has been previously introduced into the dis-
course (Jurafsky and Martin 2000). The referring expression used is called the anaphor
and the expression being referred to is its antecedent. The anaphor is usually used
to refer to the same entity as the antecedent; hence, they are coreferential with each
other. The process of determining the antecedent of an anaphor is called anaphora
resolution. As a key problem in discourse and language understanding, anaphora
resolution is crucial inmany natural language applications, such asmachine translation,
text summarization, question answering, information extraction, and so on. In recent
? 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: xiaofengy@i2r.a-star.edu.sg.
?? 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: sujian@i2r.a-star.edu.sg.
? 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg.
Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication:
5 May 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
years, supervised learning approaches have been widely applied to anaphora resolu-
tion, and they have achieved considerable success (Aone and Bennett 1995; McCarthy
and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak
1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube andMueller 2003; Luo et al
2004; Ng et al 2005).
The strength of learning-based anaphora resolution is that resolution regularities
can be automatically learned from annotated data. Traditionally, learning-based ap-
proaches to anaphora resolution adopt the single-candidate model, in which the po-
tential antecedents (i.e., antecedent candidates) are considered in isolation for both
learning and resolution. In such a model, the purpose of classification is to determine if
a candidate is the antecedent of a given anaphor. A training or testing instance is formed
by an anaphor and each of its candidates, with features describing the properties of the
anaphor and the individual candidate. During resolution, the antecedent of an anaphor
is selected based on the classification results for each candidate.
One assumption behind the single-candidate model is that whether a candidate
is the antecedent of an anaphor is completely independent of the other competing
candidates. However, anaphora resolution can be more accurately represented as a
ranking problem in which candidates are ordered based on their preference and the best
one is the antecedent of the anaphor (Jurafsky and Martin 2000). The single-candidate
model, which only considers the candidates of an anaphor in isolation, is incapable
of effectively capturing the preference relationship between candidates for its training.
Consequently, the learned classifier cannot produce reliable results for preference deter-
mination during resolution.
To deal with this problem, we propose a twin-candidate learning model for
anaphora resolution. Themain idea behind themodel is to recast anaphora resolution as
a preference classification problem. The purpose of the classification is to determine the
preference between two competing candidates for the antecedent of a given anaphor. In
the model, an instance is formed by an anaphor and two of its antecedent candidates,
with features used to describe their properties and relationships. The antecedent is
selected based on the judged preference among the candidates.
In the article we focus on two issues about the twin-candidate model. In the first
part, we will introduce the framework of the twin-candidate model for anaphora reso-
lution, including detailed training procedures and resolution schemes. In the second
part, we will further explore how to deploy the twin-candidate model in the more
complicated task of coreference resolution. We will present an empirical evaluation of
the twin-candidate model in different domains, using the Automatic Content Extraction
(ACE) data sets. The experimental results indicate that the twin-candidate model is
superior to the single-candidate model for the task of pronominal anaphora resolution.
For the coreference resolution task, it also performs equally well, or better.
2. Related Work
To our knowledge, the first work on the twin-candidate model for anaphora resolution
was proposed by Connolly, Burger, and Day (1997). Their work relied on a set of features
that included lexical type, grammatical role, recency, and number/gender/semantic
agreement, and employed a simple linear search scheme to choose the most preferred
candidate. Their system produced a relatively low accuracy rate for pronoun reso-
lution (55.3%) and definite NP resolution (37.4%) on a set of selected news articles.
Iida et al (2003) used the twin-candidate model (called the tournament model in their
work) to perform Japanese zero-anaphora resolution. They utilized the same linear
328
Yang, Su, and Tan A Twin-Candidate Model for AR
scheme to search for antecedents. Compared with Connolly, Burger, and Day (1997),
they adopted richer features in which centering information was incorporated to cap-
ture contextual knowledge. Their system achieved an accuracy of around 70% on a
data set drawn from a corpus of newspaper articles. Both of these studies were carried
out on uncommon data sets, which makes it difficult to compare their results with
other baseline systems. In contrast to the previous work, we will explore the twin-
candidate model comprehensively by describing the model in more detail, trying more
effective resolution schemes, deploying the model in the more complicated coreference
resolution task, performing more extensive experiments, and evaluating the model in
more depth.
Denis and Baldridge (2007) proposed a pronoun resolution system that directly
used a ranking learning algorithm (based on Maximal Entropy) to train a preference
classifier for antecedent selection. They reported an accuracy of around 72?76% for
the different domains in the ACE data set. In our study, we will also investigate the
solution of using a general ranking learner (e.g., Ranking-SVM). By comparison, the
twin-candidate model is applicable to any discriminative learning algorithm, no matter
whether it is capable of ranking learning or not. Moreover, as the model is trained and
tested on pairwise candidates, it can effectively capture various relationships between
candidates for better preference learning and determination.
Ng (2005) presented a ranking model for coreference resolution. The model focused
on the preference between the potential partitions of NPs, instead of the potential
antecedents of an NP as in our work. Given an input document, the model first em-
ployed n pre-selected coreference resolution systems to generate n candidate partitions
of NPs. The model learned a preference classifier (trained using Ranking-SVM) that
could distinguish good and bad partitions during testing. The best rank partition would
be selected as the resolution output of the current text. The author evaluated the model
on the ACE data set and reported an F-measure of 55?69% for the different domains.
Although ranking-based, Ng?s model is quite different from ours as it operates at the
cluster-level whereas ours operates at the mention-level. In fact, the result of our twin-
candidate system can be used as an input to his model.
3. The Twin-Candidate Model for Anaphora Resolution
3.1 The Single-Candidate Model
Learning-based anaphora resolution uses a machine learning method to obtain p(ante
(Ck)|ana,C1,C2, . . . ,Cn), the probability that a candidate Ck is the antecedent of the
anaphor ana in the context of its antecedent candidates, C1,C2, . . . ,Cn. The single-
candidate model assumes that the probability that Ck is the antecedent is only de-
pendent on the anaphor ana and Ck, and independent of all the other candidates.
That is:
p (ante(Ck) | ana,C1,C2, . . . ,Cn) = p (ante(Ck) | ana,Ck) (1)
Thus, the probability of a candidate Ck being the antecedent can be approximated using
the classification result on the instance describing the anaphor and Ck alone.
The single-candidate model is widely used in most anaphora resolution sys-
tems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube and
Mueller 2003; Kehler et al 2004; Ng et al 2005). In our study, we also build as the
329
Computational Linguistics Volume 34, Number 3
Table 1
A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]
proposed to [3 legislators] in [4 September]. If [5 the government] can
stick with [6 them], [7 it] will be able to halve this year?s 120 billion
ruble (US $193 billion) deficit.
Table 2
Training instances generated under the single-candidate model for anaphora resolution.
Anaphor Training Instance Label
i{[6 them] , [1 Those figures]} 1
[6 them] i{[6 them] , [2 the government]} 0
i{[6 them] , [3 legislators]} 0
i{[6 them] , [4 September]} 0
i{[6 them] , [5 the government]} 0
i{[7 it] , [1 Those figures]} 0
i{[7 it] , [3 legislators]} 0
[7 it] i{[7 it] , [4 September]} 0
i{[7 it] , [5 the government]} 1
i{[7 it] , [6 them]} 0
baseline a system for pronominal anaphora resolution based on the single-candidate
model.
In the single-candidate model, an instance has the form of i{ana, candi}, where ana
is an anaphor and candi is an antecedent candidate.1 For training, instances are created
for each anaphor occurring in an annotated text. Specifically, given an anaphor ana and
its antecedent candidates, a set of negative instances (labeled ?0?) is formed by pairing
ana and each of the candidates that is not coreferential with ana. In addition, a single
positive instance (labeled ?1?) is formed by pairing ana and the closest antecedent, that
is, the closest candidate that is coreferential with ana.2 Note that it is possible that an
anaphor has two or more antecedents, but we only create one positive instance for the
closest antecedent as its reference relationship with the anaphor is usually the most
direct and thus the most confident.
As an example, consider the text in Table 1.
Here, [6 them] and [7 it] are two anaphors. [1 Those figures] and [5 the government] are
their closest antecedents, respectively. Supposing that the antecedent candidates of the
two anaphors are just all their preceding NPs in the current text, the training instances
to be created for the text segment are listed in Table 2.
1 In our study, we only consider anaphors whose antecedents are noun phrases. Typically, all the NPs
preceding an anaphor can be taken as the initial antecedent candidates. For better learning and
resolution, however, candidates can be filtered so that only those ?confident? NPs, which occur in the
specified search scope and meet constraints such as number/gender agreement, are considered. The
details of candidate selection in our system will be discussed later in the section on experiments.
2 We assume that at least one antecedent exists in the candidate set of an anaphor. However, for real
resolution, if none of the antecedents of an anaphor occur in the candidate set, we simply discard the
anaphor and do not create any training instance for it.
330
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 3
Feature set for pronominal anaphora resolution.
ana Reflexive whether the anaphor is a reflexive pronoun
ana PronType type of the anaphor if it is a pronoun (he, she, it or they?)
candi Def whether the candidate is a definite description
candi Indef whether the candidate is an indefinite NP
candi Name whether the candidate is a named entity
candi Pron whether the candidate is a pronoun
candi FirstNP whether the candidate is the first mentioned NP in the sentence
candi Subject whether the candidate is the subject of a sentence, the subject of a clause, or not.
candi Oject whether the candidate is the object of a verb, the object of a preposition,
or not
candi ParallelStruct whether the candidate has an identical collocation pattern with the anaphor
candi SentDist the sentence distance between the candidate and the anaphor
candi NearestNP whether the candidate is the candidate closest to the anaphor in position
Note that for [7 it], we do not use [2 the government] to create a positive training
instance as it is not the closest candidate that is coreferential with the anaphor.
A vector of features is specified for each training instance. The featuresmay describe
the characteristics of the anaphor and the candidate, as well as their relationships from
lexical, syntactic, semantic, and positional aspects. Table 3 lists the features used in our
study. All these features can be computed with high reliability, and have been proven
effective for pronoun resolution in previous work.
Based on the generated feature vectors, a classifier is trained using a certain learning
algorithm. During resolution, given a newly encountered anaphor, a test instance is
formed for each of the antecedent candidates. The instance is passed to the classifier,
which then returns a confidence value indicating the likelihood that the candidate is the
antecedent of the anaphor. The candidate with the highest confidence is selected as the
antecedent. For example, suppose [7 it] is an anaphor to be resolved. Six test instances
will be created for its six antecedent candidates, as listed in Table 4. The learned classifier
is supposed to give the highest confidence to i{[7 it] , [5 the government]}, indicating the
candidate [5 the government] is the antecedent of [7 it].
3.2 A Problem with the Single-Candidate Model
As described, the assumption behind the single-candidate model is that the probability
of a candidate being the antecedent of a given anaphor is completely independent of
Table 4
Test instances generated under the single-candidate model for anaphora resolution.
Anaphor Test Instance
i{[7 it] , [1 Those figures]}
i{[7 it] , [2 the government]}
i{[7 it] , [3 legislators]}
[7 it] i{[7 it] , [4 September]}
i{[7 it] , [5 the government]}
i{[7 it] , [6 them]}
331
Computational Linguistics Volume 34, Number 3
the other competing candidates. However, for an anaphor, the determination of the
antecedent is often subject to preference among the candidates (Jurafsky and Martin
2000). Whether a candidate is the antecedent depends on whether it is the ?best? among
the candidate set, that is, whether there exists no other candidate that is preferred over
it. Hence, simply considering one candidate individually is an indirect and unreliable
way to select the correct antecedent.
The idea of preference is common in linguistic theories on anaphora. Garnham
(2001) summarizes different factors that influence the interpretation of anaphoric
expressions. Some factors such as morphology (gender, number, animacy, and case)
or syntax (e.g., the role of binding and commanding relations [Chomsky 1981]) are
?eliminating,? forbidding certain NPs from being antecedents. However, many others
are ?preferential,? giving more preference to certain candidates over others; examples
include:
 Sentence-based factors: Pronouns in one clause prefer to refer to the
NP that is the subject of the previous clause (Crawley, Stevenson, and
Kleinman 1990). Also, the NP that is the first-mentioned expression is
preferred regardless of the syntactic and semantic role played by the
referring expression (Gernsbacher and Hargreaves 1988).
 Stylistic factors: Pronouns preferentially take parallel antecedents that play
the same role as the anaphor in their respective clauses (Grober, Beardsley,
and Caramazza 1978; Stevenson, Nelson, and Stenning 1995).
 Discourse-based factors: Items currently in focus are the prime candidates
for providing antecedents for anaphoric expressions. According to
centering theory (Grosz, Joshi, and Weinstein 1995), each utterance has a
set of forward-looking centers that have higher preference to be referred to
in later utterances. The forward-looking centers can be ranked based
on grammatical roles or other factors.
 Distance-based factors: Pronouns prefer candidates in the previous
sentence compared with those two or more sentences back (Clark
and Sengul 1979).
As a matter of fact, ?eliminating? factors could also be considered ?preferential? if
we think of the act of eliminating candidates as giving them low preference.
Preference-based strategies are also widely seen in earlier manual approaches to
pronominal anaphora resolution. For example, the SHRDLU system byWinograd (1972)
prefers antecedent candidates in the subject position over those in the object position.
The system by Wilks (1973) prefers candidates that satisfy selectional restrictions with
the anaphor. Hobbs?s algorithm (Hobbs 1978) prefers candidates that are closer to the
anaphor in the syntax tree, and the RAP algorithm (Lappin and Leass 1994) prefers
candidates that have a high salience value computed by aggregating the weights of
different factors.
During resolution, the single-candidate model does select an antecedent based on
preference by using classification confidence for candidates; that is, the higher con-
fidence value the classifier returns, the more likely the candidate is preferred as the
antecedent. Nevertheless, as the model considers only one candidate at a time during
training, it cannot effectively capture the preference between candidates for classifier
learning. For example, consider an anaphor and a candidate Ci. If there are no ?better?
332
Yang, Su, and Tan A Twin-Candidate Model for AR
candidates in the candidate set, Ci is the antecedent and forms a positive instance.
Otherwise, Ci is not selected as the antecedent and thus forms a negative instance.
Simply looking at a candidate alone cannot explain this, and may possibly result in
inconsistent training instances (i.e., the same feature vector but different class labels).
Consequently, the confidence values returned by the learned classifier cannot reliably
reflect the preference relationship between candidates.
3.3 The Twin-Candidate Model
To address the problem with the single-candidate model, we propose a twin-candidate
model to handle anaphora resolution. As opposed to the single-candidate model, the
model explicitly learns a preference classifier to determine the preference relationship
between candidates. Formally, the model considers the probability that a candidate
is the antecedent as the probability that the candidate is preferred over all the other
competing candidates. That is:
p (ante(Ck) | ana,C1,C2, . . . ,Cn)
= p (Ck  {C1, . . . ,Ck?1,Ck+1, . . .Cn} | ana,C1,C2, . . . ,Cn) (2)
= p(Ck  C1, . . . ,Ck  Ck?1,Ck  Ck+1, . . . ,Ck  Cn | ana,C1,C2, . . . ,Cn)
Assuming that the preference between Ck and Ci is independent of the preference
between Ck and the candidates other than Ci, we have:
p(Ck  C1, . . . ,Ck  Ck?1,Ck  Ck+1, . . . ,Ck  Cn | ana,C1,C2, . . . ,Cn)
=
?
1<i<n,i =k
p(Ck  Ci | ana,Ck,Ci) (3)
Thus:
ln p (ante(Ck) | ana,C1,C2, . . . ,Cn)
=
?
1<i<n,i =k
ln p(Ck  Ci | ana,Ck,Ci) (4)
This suggests that the probability that a candidate Ck is the antecedent can be esti-
mated using the classification results on the set of instances describing Ck and each of
the other competing candidates. To do this, we learn a classifier that, given any two can-
didates of a given anaphor, can determine which one is preferred to be the antecedent
of the anaphor. The final antecedent is identified based on the classified preference
relationships among the candidates. This is the main idea of the twin-candidate model.
In such a model, each instance consists of three elements: i{ana, Ci, Cj}, where ana
is an anaphor, and Ci and Cj are two of its antecedent candidates. The class label of
an instance represents the preference between the two candidates for the antecedent,
for example, ?01? indicating Cj is preferred over Ci and ?10? indicating Ci is preferred.
Being trained with instances built based on this principle, the classifier is capable of de-
termining the preference between any two candidates of a given anaphor by returning
333
Computational Linguistics Volume 34, Number 3
Table 5
A sample text for anaphora resolution.
[1 Those figures] are almost exactly what [2 the government]
proposed to [3 legislators] in [4 September]. If [5 the government]
can stick with [6 them], [7 it] will be able to halve this year?s
120 billion ruble (US $193 billion) deficit.
a class label, either ?01? or ?10?, accordingly. In the next section, we will introduce in
detail a system based on the twin-candidate model for anaphora resolution.
3.4 Framework of the Twin-Candidate Model
3.4.1 Instance Representation. In the twin-candidate model, an instance takes the form
i{ana,Ci,Cj}, where ana is an anaphor andCi andCj are two of its antecedent candidates.
We stipulate that Cj should be closer to ana than Ci in position (i.e., i < j). An instance is
labeled ?10? if Ci is preferred over Cj as the antecedent, or ?01? if otherwise.
A feature vector is associated with an instance, and it describes different properties
and relationships between ana and each of the candidates, Ci or Cj. In our study, the
system with the twin-candidate model adopts the same feature set as the baseline
system with the single-candidate model (shown in Table 3). The difference is that a
feature for the single candidate, candi X, has to be replaced by a pair of features for
the twin candidates, candi1 X and candi2 X. For example, feature candi Pron, which
describes whether a candidate is a pronoun, will be replaced by two features candi1 Pron
and candi2 Pron, which describe whether Ci and Cj are pronouns, respectively.
3.4.2 Training Instances Creation. To learn a preference classifier, a training instance for an
anaphor should be composed of two candidates with an explicit preference relationship,
for example, one being an antecedent and the other being a non-antecedent. A pair
of candidates that are both antecedents or both non-antecedents are not suitable for
instance creation because their preference cannot be explicitly represented for training,
although it does exist.
Based on this idea, during training, for an encountered anaphor ana, we take the
closest antecedent, Cante, as the anchor candidate.
3 Cante is paired with each of the
candidates Cnc that is not coreferential with ana. If Cante is closer to ana than Cnc, an
instance i{ana, Cnc, Cante} is created and labeled ?01?. Otherwise, if Cnc is closer, an
instance i{ana, Cante, Cnc} is created and labeled ?10? instead.
Consider again the sample text given in Table 1, which is repeated in Table 5. For the
anaphor [7 it], the closest antecedent, [5 the government] (denoted as NP5), is chosen as
the anchor candidate. It is paired with the four non-coreferential candidates (i.e., NP1,
NP3,NP4, andNP6) to create four training instances. Among them, the instances formed
withNP1,NP3 orNP4 are labeled ?01? and the one withNP6 is labeled ?10?. Table 6 lists
all the training instances to be generated for the text.
3.4.3 Classifier Generation. Based on the feature vectors for the generated training in-
stances, a classifier can be trained using a discriminative learning algorithm. Given a
test instance i{ana, Ci, Cj} (i < j), the classifier is supposed to return a class label of ?10?,
3 If no antecedent is found in the candidate set, we do not generate any training instance for the anaphor.
334
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 6
Training instances generated under the twin-candidate model for anaphora resolution.
Anaphor Training Instance Label
i{[6 them], [1 Those figures], [2 the government]} 10
[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10
i{[6 them], [1 Those figures], [4 September]} 10
i{[6 them], [1 Those figures], [5 the government]} 10
i{[7 it], [1 Those figures], [5 the government]} 01
i{[7 it], [3 legislators], [5 the government]} 01
[7 it] i{[7 it], [4 September], [5 the government]} 01
i{[7 it], [5 the government], [6 them]} 10
indicating that Ci is preferred over Cj for the antecedent of ana, or ?01?, indicating that
Cj is preferred.
3.4.4 Antecedent Identification. After training, the preference classifier can be used to
resolve anaphors. The process of determining the antecedent of a given anaphor, called
antecedent identification, could be thought of as a tournament, a competition in which
many participants play against each other in individual matches. The candidates are like
players in a tournament. A series of matches between candidates is held to determine
the champion of the tournament, that is, the final antecedent of the anaphor under con-
sideration. Here, the preference classifier is like the referee who judges which candidate
wins or loses in a match.
If an anaphor has only one antecedent candidate, it is resolved to the candidate
directly. For anaphors that have more than one candidate, two possible schemes can be
employed to find the antecedent.
Tournament Elimination Tournament Elimination is a type of tournament where
the loser in a match is immediately eliminated. Such a scheme is also applicable to
antecedent identification. In the scheme, candidates are compared linearly from the
beginning to the end. Specifically, the first candidate is compared with the second one,
forming a test instance, which is then passed to the classifier to determine the prefer-
ence. The ?losing? candidate that is judged less preferred by the classifier is eliminated
and never considered. The winner, that is, the preferred candidate, is compared with
the third candidate. The process continues until all the candidates are compared, and
the candidate that wins in the last comparison is selected as the antecedent.
For demonstration, we use the text in Table 5 as a test example. Suppose we have a
?perfect? classifier that can correctly determine the preference between candidates. That
is, the candidates that are coreferential with the anaphor will be classified as preferred
over those that are not. (If the two candidates are both coreferential or both non-
coreferential with the anaphor, the one closer to the anaphor in position is preferred.)
To resolve the anaphor [7 it], the candidate NP1 is first compared with NP2. The formed
instance is classified as ?01?, indicating NP2 is preferred. Thus, NP1 is eliminated and
NP2 continues to compete with NP3 and NP4 until it fails in the comparison with NP5.
Finally, NP5 beats NP6 in the last match and is selected as the antecedent. All the test
instances to be generated in sequence for the resolution of [6 them] and [7 it] are listed in
Table 7.
The Tournament Elimination scheme has a computational complexity of O(N),
where N is the number of the candidates. Thus, it enables a relatively large number
335
Computational Linguistics Volume 34, Number 3
Table 7
Test instances generated under the twin-candidate model with the Tournament Elimination
scheme.
Anaphor Test Instance Result
i{[6 them], [1 Those figures], [2 the government]} 10
[6 them] i{[6 them], [1 Those figures], [3 legislators]} 10
i{[6 them], [1 Those figures], [4 September]} 10
i{[6 them], [1 Those figures], [5 the government]} 10
i{[7 it ], [1 Those figures], [2 the government]} 01
i{[7 it ], [2 the government], [3 legislators]} 10
[7 it ] i{[7 it ], [2 the government], [4 September]} 10
i{[7 it ], [2 the government], [5 the government]} 01
i{[7 it ], [5 the government], [6 them]} 10
of candidates to be processed. However, as our twin-candidate model imposes no
constraints that enforce transitivity of the preference relation, the preference classifier
would likely output C1  C2, C2  C3, and C3  C1. Hence, it is unreliable to eliminate
a candidate once it happens to lose in one comparison, without considering all of its
winning/losing results against the other candidates.
Round Robin In Section 3.3, we have shown that the probability that a candidate is
the antecedent can be calculated using the preference classification results between the
candidate and its opponents. The candidate with the highest preference is selected as
the antecedent, that is:
Antecedent(ana) = argimax p (ante(Ci) | ana,C1,C2, . . . ,Cn)
? argimax
?
j =i
CF(i{ana,Ci,Cj},Ci) (5)
where CF(i{ana, Ci, Cj}, Ci) is the confidence with which the classifier determines Ci to
be preferred over Cj as the antecedent of ana. If we define the score of Ci as:
Score(Ci) =
?
j =i
CF(i{ana,Ci,Cj},Ci) (6)
Then, the most preferred candidate is the candidate that has the maximum score. If we
simply use 1 to denote the result that Ci is classified as preferred over Cj, and ?1 if Cj is
preferred otherwise, then:
Score(Ci) = |{Cj|Ci  Cj}| ? |{Cj|Cj  Ci}| (7)
That is, the score of a candidate is the number of the opponents to which it is preferred,
less the number of the opponents to which it is less preferred. To obtain the scores, the
antecedent candidates are comparedwith each other. For each candidate, its comparison
336
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 8
Test instances generated under the twin-candidate model with the Round Robin scheme.
Anaphor Test Instance Result
i{[7 it], [1 Those figures], [2 the government]} 01
i{[7 it], [1 Those figures], [3 legislators]} 01
i{[7 it], [1 Those figures], [4 September]} 01
i{[7 it], [1 Those figures], [5 the government]} 01
i{[7 it], [1 Those figures], [6 them]} 01
i{[7 it], [2 the government], [3 legislators]} 10
i{[7 it], [2 the government], [4 September]} 10
[7 it] i{[7 it], [2 the government], [5 the government]} 01
i{[7 it], [2 the government], [6 them]} 10
i{[7 it], [3 legislators], [4 September]} 01
i{[7 it], [3 legislators], [5 the government]} 01
i{[7 it], [3 legislators], [6 them]} 01
i{[7 it], [4 September], [5 the government]} 01
i{[7 it], [4 September], [6 them]} 01
i{[7 it], [5 the government], [6 them]} 10
result against every other candidate is recorded. Its score increases by one if it wins a
match, or decreases by one if it loses. The candidate with the highest score is selected as
the antecedent.
Antecedent identification carried out in such a way corresponds to a type of tourna-
ment called Round Robin in which each participant plays every other participant once,
and the final champion is selected based on the winning?losing records of the players.
In contrast to the Elimination scheme, the Round Robin scheme is more reliable in
that the preference of a candidate is determined by overall comparisons with the other
competing candidates. The computational complexity of the scheme is O(N2), where N
is the number of the candidates.
To illustrate this, consider the example in Table 5 again. The test instances to be
generated for resolving the anaphor [7 it] are listed in Table 8. As shown, each of
the candidates is compared with every other competing candidate. The scores of the
candidates are summarized in Table 9. Here, the candidate NP5 beats all the opponents
in the comparisons and obtains the maximum score of five. Thus it will be selected as
the antecedent.
An extension of the above Round Robin scheme is called the Weighted Round
Robin scheme. In the weighted version, the confidence values returned by the classifier,
Table 9
Scores for the candidates under the Round Robin scheme.
NP1 NP2 NP3 NP4 NP5 NP6 Score
NP1 ?1 ?1 ?1 ?1 ?1 ?5
NP2 +1 +1 +1 ?1 +1 +3
NP3 +1 ?1 ?1 ?1 ?1 ?3
NP4 +1 ?1 +1 ?1 ?1 ?1
NP5 +1 +1 +1 +1 +1 +5
NP6 +1 ?1 +1 +1 ?1 +1
337
Computational Linguistics Volume 34, Number 3
Table 10
Statistics for the training and testing data sets.
NWire NPaper BNews
# Tokens 85k 72k 67kTrain
# Files 130 76 216
# Tokens 20k 18k 18kTest
# Files 29 17 51
instead of the simple 0 and 1, are employed to calculate the score of a candidate based
on the formula
Score(Ci) =
?
CiCj
CF(Ci  Cj)?
?
CkCi
CF(Ck  Ci) (8)
Here, CF is the confidence value that the classifier returns for the corresponding
instance.
3.5 Evaluation
3.5.1 Experimental Setup.We used the ACE (Automatic Content Extraction)4 coreference
data set for evaluation. All the experiments were done on the ACE-2 V1.0 corpus. It
contains two data sets, training and devtest, which were used for training and testing,
respectively. Each of these sets is further divided into three domains: newswire (NWire),
newspaper (NPaper), and broadcast news (BNews). Statistics for the data sets are sum-
marized in Table 10.
For both training and resolution, a raw input document was processed by a
pipeline of NLP modules including a Tokenizer, Part-of-Speech tagger, NP chunker,
Named-Entity (NE) Recognizer, and so on. These preprocessing modules were meant to
determine the boundary of each NP in a text, and to provide the necessary information
about an NP for subsequent processing. Trained and tested on the UPENWSJ TreeBank,
the POS tagger (Zhou and Su 2000) could obtain an accuracy of 97% and the NP
chunker (Zhou and Su 2000) could produce an F-measure above 94%. Evaluated for
the MUC-6 and MUC-7 Named-Entity task, the NER module (Zhou and Su 2002) could
provide an F-measure of 96.6% (MUC-6) and 94.1% (MUC-7).
In our experiments, we focused on the resolution of the third-personal pronominal
anaphors, including she, he, it, they as well as their morphologic variants (such as her, his,
him, its, itself, them, etc.). For both training and testing, we considered all the pronouns
that had at least one preceding NP in their respective annotated coreferential chains. We
used the accuracy rate as the evaluation metric, and defined it as follows:
Accuracy =
number of anaphors being correctly resolved
total number of anaphors to be resolved
(9)
Here, an anaphor is deemed ?correctly resolved? if the found antecedent is in the co-
referential chain of the anaphor.
4 See http://www.itl.nist.gov/iad/894.01/tests/ace for a detailed description of the ACE program.
338
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 11
Statistics of the training instances generated for the pronominal anaphora resolution task.
NWire NPaper BNews
0 instances 8,200 11,648 6,037
Single-Candidate 1 instances 1,241 1,466 1,291
01 instances 6,899 9,861 5,004
Twin-Candidate 10 instances 1,301 1,787 1,033
For pronoun resolution, the distance between the closest antecedent and the
anaphor is usually short, predominantly (98% for the current data set) limited to only
one or two sentences (McEnery, Tanaka, and Botley 1997). For this reason, given an
anaphor, we only took the NPs occurring within the current and previous two sentences
as initial antecedent candidates. The candidates with mismatched number and gender
agreement were filtered automatically from the candidate set. Also, pronouns or NEs
that disagreed in person with the anaphor were removed in advance. For training, there
were 1,241 (NWire), 1,466 (NPaper), and 1,291 (BNews) anaphors found with at least
one antecedent in the candidate set. For testing, the numbers were 313 (NWire), 399
(NPaper), and 271 (BNews). On average, an anaphor had nine antecedent candidates.
Table 11 summarizes the statistics of the training instances as well as the class
distribution. Note that for the single-candidate model, the number of ?1? instances
was identical to the number of anaphors in the training data, because we only used
the closest antecedents of anaphors to create the positive instances. The number of ?0?
instances was equal to the total number of ?01? and ?10? training instances for the twin-
candidate model.
We examined three different learning algorithms: C5 (Quinlan 1993), Maximum
Entropy (Berger, Della Pietra, and Della Pietra 1996), and SVM (linear kernel) (Vapnik
1995),5 using the software See5,6 OpenNlp.MaxEnt,7 and SVM-light,8 respectively. All
the classifiers were learned with the default learning parameters set in the respective
learning software.
3.5.2 Results and Discussions. Table 12 lists the performance of the different anaphora
resolution systems with the single-candidate (SC) and the twin-candidate (TC) models.
For the TC model, two antecedent identification schemes, Tournament Elimination and
Round Robin, were compared.
From the table, we can see that our baseline systemwith the single-candidate model
can obtain accuracy of up to 72.9% (NWire), 77.1% (NPaper), and 74.9% (BNews).
5 As MaxEnt learns a probability model, we used the returned probability as the confidence of a candidate
being the antecedent. For C5, the confidence value of a candidate was estimated based on the following
smoothed ratio:
CF =
p+ 1
t+ 2
where cwas the number of positive instances and twas the total number of instances stored in the
corresponding leaf node. For SVM, the returned value was used as the confidence value: the lower
(maybe negative) the less confident.
6 http://www.rulequest.com/see5-info.html
7 http://MaxEnt.sourceforge.net/
8 http://svmlight.joachims.org/
339
Computational Linguistics Volume 34, Number 3
Table 12
Accuracy in percent for the pronominal anaphora resolution.
NWire NPaper BNews Average
C5 SC 71.6 75.6 69.5 72.7
TC
- Elimination 71.6 81.3 74.5 76.4
- Round Robin 72.9 81.3 74.9 76.9
- Weighted Round Robin 72.9 80.5 75.6 76.7
MaxEnt SC 72.9 77.1 74.9 75.2
TC
- Elimination 75.1 79.1 77.5 77.4
- Round Robin 75.1 79.1 77.5 77.4
- Weighted Round Robin 75.7 78.6 77.1 77.3
SVM SC 72.9 77.3 74.2 75.1
TC
- Elimination 73.5 82.0 78.9 78.5
- Round Robin 74.4 82.0 78.9 78.7
- Weighted Round Robin 74.6 79.3 78.2 77.5
Rank SVM 73.5 79.3 76.4 76.7
The average accuracy is comparable to that reported by Kehler et al (2004) (around
75%), who also used the single-candidate model to do pronoun resolution with similar
features (using MaxEnt) on the ACE data sets. By contrast, the systems with the twin-
candidate model are able to achieve accuracy of up to 75.7% (NWire), 82.0% (NPaper),
and 78.9% (BNews). The average accuracy is 76.9% for C5, 77.4% for MaxEnt, and 78.7%
for SVM,which is statistically significantly9 better than the results of the baselines (4.2%,
2.2%, and 3.6% in accuracy). These results confirm our claim that the twin-candidate
model is more effective than the single-candidate model for the task of pronominal
anaphora resolution.
We see no significant difference between the accuracy rates (less than 1.0% accuracy)
produced by the two antecedent identification schemes, Tournament Elimination and
Round Robin. This is in contrast to our belief that the Round Robin scheme, which is
more reliable than the Tournament Elimination, should lead to much better results. One
possible reason could be that the classifier in our systems can make a correct preference
judgement (with accuracy above 92% as in our test) in the cases where one candidate is
the antecedent and the other is not. As a consequence, the simple linear search can find
the final antecedent as well as the Round Robin method. These results suggest that we
can use the Elimination scheme in a practical system to make antecedent identification
more efficient. (Recall that the Elimination scheme requires complexity ofO(N), instead
of O(N2) as in Round Robin.)
Ranking-SVM In our experiments, we were particularly interested in comparing
the results using the twin-candidate model and those directly using a preference learn-
ing algorithm. For this purpose, we built a system based on Ranking-SVM (Joachims
2002), an extension of SVM capable of preference learning.
9 Throughout our experiments, the significance was examined by using the paired t-test, with p < 0.05.
340
Yang, Su, and Tan A Twin-Candidate Model for AR
The system uses a similar framework to the single-candidate-based system. For
training, given an anaphor, a set of instances is created for each of the antecedent candi-
dates. To learn the preference between competing candidates, a ?query-ID? is specified
for each training instance in such a way that the instances formed by the candidates of
the same anaphor bear the same query-ID. The label of an instance represents the rank of
the candidate in the candidate set; here, ?1? for the instances formed by the candidates
that are the antecedents, and ?0? for the instances formed by the others. The training
instances are associated with features as defined in Table 3, to which the Ranking-
SVM algorithm is then applied to generate a preference classifier. During resolution, for
each candidate of a given anaphor, a test instance is formed and passed to the learned
classifier, which in turn returns a value to represent the rank of the candidate among all
the candidates. The anaphor is resolved to the one with the highest value.
In fact, if we look into the learning mechanism of Ranking-SVM, we can find
that the algorithm will, in the background, pair any two instances that have the same
query-ID but different rank labels. This is quite similar to the twin-candidate model,
which creates an instance by putting together two candidates with different preferences.
However, one advantage of the twin-candidate model is that it can explicitly record
various relationships between two competing candidates, for example, ?which one
of the two candidates is closer to the anaphor in position/syntax/semantics??10 Such
inter-candidate information can make the preference between candidates clearer, and
thus facilitate both preference learning and determination. In contrast, Ranking-SVM,
which constructs instances in the single-candidate form, cannot effectively capture this
kind of information.
The last line of Table 12 shows the results from such a system based on Ranking-
SVM. We can see that the system achieves an average accuracy of 76.7%, statistically
significantly better than the baseline system with the single-candidate model by 1.6%
(0.4% for NWire, 2.0% for NPaper, and 2.2% for BNews). The results lend support to our
claim that the preference relationships between candidates, if taken into consideration
for classifier training, can lead to better resolution performance. Still, we observe that
our twin-candidate model beats Ranking-SVM in average accuracy by 1.8% (Elimina-
tion scheme) and 2.0% (Round Robin).
Decision Tree One advantage of the C5 learning algorithm is that the generated
classifier can be easily interpreted by humans, and the importance of the features
can be visually illustrated. In Figures 1 and 2, we show the decision trees (top four
levels) output by C5 for the NWire domain, based on the single-candidate and the
twin-candidate models, respectively. As the twin-candidate model uses a larger pool
of features, the tree for the twin-candidate model is more complicated (180 nodes) than
the one for the single-candidate model (36 nodes).
From the two trees, we can see that bothmodels rely on similar features such as lexi-
cal, positional, and grammatical properties for pronoun resolution. However, we can see
that the preferential factors (e.g., subject preference, parallelism preference, and distance
preference as discussed in Section 3.2) are more clearly presented in the twin-candidate-
based tree. For example, if two candidates are both pronouns, the twin-candidate-based
tree will suggest that the one closer to the anaphor has a higher preference to be the
antecedent. By contrast, such a preference relationship has to be implicitly represented
10 In the current work, we only consider the positional relationship between candidates by stipulating
that i < j for an instance i{ana, Ci, Cj}. In our future work, we will explore more inter-candidate
relationships that are helpful for preference determination.
341
Computational Linguistics Volume 34, Number 3
Figure 1
Decision tree generated for pronoun resolution under the single-candidate model. For feature
ana Type, the values PRON SHE,PRON SHE,PRON SHE, and PRON THEY represent whether
the anaphor is a pronoun such as she, he, it, and they, respectively. For candi Subject, the values
SUBJ MAIN, SUBJ CLAUSE and NO represent whether the candidate is the subject of a main
sentence, or the subject of a clause, or not. For candi Object, the values OBJ VERB, OBJ PREP, and
NO represent whether the candidate is the object of a verb, a preposition, or not, respectively.
For other features, 0 and 1 represent yes/no.
in the single-candidate-based tree, with different confidence values being assigned to
the candidates in different sentences.
Learning Curve In our experiments, we were also concerned about how training
data size might influence anaphora resolution performance. For this purpose, we di-
vided the anaphors in the training documents into 10 batches, and then performed
resolution using the classifiers trained with 1, 2, . . . , 10 batches of anaphors. Figure 3
plots the learning curves of the systems with the single-candidate model and the twin-
candidate model (Round Robin scheme) for the NPaper domain. Each accuracy rate
shown in the figure is the average of the results from three trials trained on different
anaphors.
From the figure we can see that both the single-candidate model and the twin-
candidate model reach their peak performance with around six batches (around
880 anaphors). As shown, the twin-candidate model is not apparently superior to
the single-candidate model when the size of the training data is small (below two
batches, 290 anaphors). This is due to the fact that the number of features in the twin-
candidate model is nearly double that in the single-candidate model. As a result, the
twin-candidate model requires more training data than the single-candidate model to
avoid the data sparseness problem. Nevertheless, it does not need too much training
data to beat the latter; it can produce the accuracy rates consistently higher than the
342
Yang, Su, and Tan A Twin-Candidate Model for AR
Figure 2
Decision tree generated for pronoun resolution under the twin-candidate model.
Figure 3
Learning curves of different models for pronominal anaphora resolution in the NPaper Domain
(120 anaphors per batch).
343
Computational Linguistics Volume 34, Number 3
Table 13
A sample text for coreference resolution.
[1 Globalstar] still needs to raise [2 $600 million], and
[3 Schwartz] said [4 that company] would try to raise [5 the
money] in [6 the debt market].
single-candidate model when trained with more than two batches of anaphors. This
figure further demonstrates that the twin-candidate model is reliable and effective for
the pronominal anaphora resolution task.
4. Deploying the Twin-Candidate Model to Coreference Resolution
One task that is closely related to anaphora resolution is coreference resolution, the
process of identifying all the coreferential expressions in texts.11 Coreference resolution
is different from anaphor resolution. The latter focuses on how an anaphor can be suc-
cessfully resolved, and the resolution is done on given anaphors. The former, in contrast,
focuses on how the NPs that are coreferential with each other can be found correctly
and completely, and the resolution is done on all possible NPs. In a text, many NPs,
especially the non-pronouns, are non-anaphors that have no antecedent to be found
in the previous text. Hence, the task of coreference resolution is a more complicated
challenge than anaphora resolution, as a solution should not only be able to resolve
an anaphor to the correct antecedent, but should also refrain from resolving a non-
anaphor. In this section, we will explore how to deploy the learning models for anaphor
resolution in the coreference resolution task. As pronouns are usually anaphors, we will
focus mainly on the resolution of non-pronouns.
4.1 Coreference Resolution Based on the Single-Candidate Model
In practice, the single-candidate model can be applied to coreference resolution directly,
using the similar training and testing procedures to those used in anaphora resolution
(described in Section 2).
For training, we create ?0? and ?1? training instances for each encountered anaphor,
that is, the NP that is coreferential with at least one preceding NP. Specifically, given an
anaphor and its antecedent candidates, a positive instance is generated for the closest
antecedent and a set of negative instances is generated for each of the candidates that is
not coreferential with the anaphor.12
Consider the text in Table 13 as an example. In the text, [4 that company] and [5 the
money] are two anaphors, with [1 Globalstar] and [2 $600 million] being their antecedents,
respectively. Table 14 lists the training instances to be created for this text.
11 In our study, we only consider within-document noun phrase coreference resolution.
12 In some coreference resolution systems (Soon, Ng, and Lim 2001; Ng and Cardie 2002b), only the
non-coreferential candidates occurring between the closest antecedent and the anaphor are used to create
negative instances. In the experiments, we found that these sampling strategies for negative instances
led to a trade-off between recall and precision, but no significant difference in the overall F-measure.
344
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 14
Training instances generated under the single-candidate model for coreference resolution.
Anaphor Training Instance Label
i{[4 that company] , [1 Globalstar]} 1
[4 that company] i{[4 that company] , [2 $600 million]} 0
i{[4 that company] , [3 Schwartz]} 0
i{[5 the money] , [1 Globalstar]} 0
[5 the money] i{[5 the money] , [2 $600 million]} 1
i{[5 the money] , [3 Schwartz]} 0
i{[5 the money] , [4 that company]} 0
Table 15
Feature set for coreference resolution.
ana Def whether the possible anaphor is a definite description
ana Indef whether the possible anaphor is an indefinite NP
ana Name whether the possible anaphor is a named entity
candi Def whether the candidate is a definite description
candi Indef whether the candidate is an indefinite description
candi Name whether the candidate is a named-entity
candi SentDist the sentence distance between the possible anaphor and the candidate
candi NameAlias whether the candidate and the candidate are aliases for each other
candi Appositive whether the possible anaphor and the candidate are in an appositive
structure
candi NumberAgree whether the possible anaphor and the candidate agree in number
candi GenderAgree whether the possible anaphor and the candidate agree in gender
candi HeadStrMatch whether the possible anaphor and the candidate have the same head
string
candi FullStrMatch whether the possible anaphor and the candidate contain the same
strings (excluding the determiners)
candi SemAgree whether the possible anaphor and the candidate belong to the same
semantic category in WordNet
In Table 15, we list the features used in our study for coreference resolution, which
are similar to those proposed in Soon, Ng, and Lim?s (2001) system.13 All these features
are domain independent and the values can be computed with low cost but high
reliability.
After training, the learned classifier can be directly used for coreference resolution.
Given an NP to be resolved, a test instance is generated for each of its antecedent
candidates. The classifier, being given the instance, will determine the likelihood that
the candidate is the antecedent of the possible anaphor. If the confidence is below
a pre-specified threshold, the candidate is discarded. In the case where none of the
candidates have a confidence higher than the threshold, the current NP is deemed a
13 As we focus on coreference resolution for non-pronouns, we do not use the feature that describes
whether or not the NP to be resolved is a pronoun. Also, we do not use the feature that describes
whether or not a candidate is a pronoun, because, as will be discussed together with the experiments,
a pronoun is not taken as an antecedent candidate for a non-pronoun to be resolved.
345
Computational Linguistics Volume 34, Number 3
non-anaphor and left unresolved. Otherwise, it is resolved to the candidate with the
highest confidence.14
4.2 Coreference Resolution Based on the Twin-Candidate Model
The twin-candidate model presented in the previous section focuses on the preference
between candidates. The model will always select a ?best? candidate as the antecedent,
even if the current NP is a non-anaphor. To deal with this problem, we will teach the
preference classifier how to identify non-anaphors, by incorporating non-anaphors to
create a special class of training instances. For resolution, if the newly learned classifier
returns the special class label, wewill know that the current NP is a non-anaphor, and no
preference relationship holds between the two candidates under consideration. In this
way, the twin-candidate model is capable of carrying out both antecedent identification
and anaphoricity determination by itself, and thus can be deployed for coreference
resolution directly. In this section, we will describe the modified training and resolution
procedures of the twin-candidate model.
4.2.1 Training. As with anaphora resolution, an instance of the twin-candidate model
for coreference resolution takes the form i{ana, Ci, Cj}, where ana is a possible anaphor,
and Cj and Cj are two of its antecedent candidates (i < j). The feature set is similar to
that for the single-candidate model as defined in Table 15, except that a candi X feature
is replaced by a pair of features, cand1 x and candi2 x, for the two competing candi-
dates, respectively.
During training, if an encountered NP is an anaphor, we create ?01? or ?10? training
instances in the same way as in the original learning framework. If the NP is a non-
anaphor, we do the following:
 From the antecedent candidates,15 randomly select one as the anchor
candidate.
 Create a set of instances by pairing the anchor candidate and each of the
other non-coreferential candidates.
The instances formed by the non-anaphors are labeled ?00.?
Consider the sample text in Table 13. For the two anaphors [4 that company] and
[5 the money], we create the ?01? and ?10? instances as usual. For the non-anaphors
[3 Schwartz] and [6 the debt market], we generate two sets of ?00? instances. Table 16 lists
all the training instances for the text (supposing [1 Globalstar] and [2 $600 million] are the
anchor candidates for [3 Schwartz] and [6 the debt market], respectively).
The ?00? training instances are used together with the ?01? and ?10? ones to train
a classifier. Given a test instance i{ana, Ci, Cj} (i < j), the newly learned classifier is
supposed to return ?01? (or ?10?), indicating ana is an anaphor and Ci (or Cj) is preferred
as its antecedent, or return ?00?, indicating ana is a non-anaphor and no preference
exists between Ci and Cj.
14 Other clustering strategies are also available, for example, ?closest-first? where a possible anaphor is
resolved to the closest candidate with the confidence above the specified threshold, if any (Soon, Ng,
and Lim 2001).
15 For a non-anaphor, we also take the preceding NPs as its antecedent candidates. We will discuss this
issue later together with the experimental setup.
346
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 16
Training instances generated under the twin-candidate model for coreference resolution.
Possible Anaphor Training Instance Label
i{[4 that company], [1 Globalstar], [2 $600 million]} 10
[4 that company] i{[4 that company], [1 Globalstar], [3 Schwartz]} 10
i{[5 the money], [1 Globalstar], [2 $600 million]} 01
[5 the money] i{[5 the money], [2 $600 million], [3 Schwartz]} 10
i{[5 the money], [2 $600 million], [4 that company]} 10
[3 Schwartz] i{[3 Schwartz], [1 Globalstar], [2 $600 million]} 00
i{[6 the debt market], [1 Globalstar], [2 $600 million]} 00
i{[6 the debt market], [2 $600 million], [3 Schwartz]} 00
[6 the debt market] i{[6 the debt market], [2 $600 million], [4 that company]} 00
i{[6 the debt market], [2 $600 million], [5 the money]} 00
4.2.2 Antecedent Identification. Accordingly, we make a modification to the original Tour-
nament Elimination and the Round Robin schemes:
Tournament Elimination Scheme As with anaphora resolution, given an NP to be
resolved, candidates are compared linearly from the beginning to the end. If an instance
for two competing candidates is classified as ?01? or ?10?, the preferred candidate will
be compared with subsequent competitors while the loser is eliminated immediately.
If the instance is classified as ?00?, both the two candidates are discarded and the
comparison restarts with the next two candidates.16 The process continues until all the
candidates have been compared. If both of the candidates in the last match are judged to
be ?00?, the current NP is left unresolved. Otherwise, the NPwill be resolved to the final
winner, on the condition that the highest confidence that the winner has ever obtained
is above a pre-specified threshold.
Round Robin Scheme In the Round Robin scheme, each candidate is compared
with every other candidate. If two candidates are labeled ?00? in a match, both candi-
dates receive a penalty of ?1 in their respective scores. If no candidate has a positive
final score, then the NP is considered non-anaphoric and left unresolved. Otherwise,
it is resolved to the candidate with the highest score as usual. Here, we can also use a
threshold. That is, we will update the scores of the two candidates in a match if and
only if the preference confidence returned by the classifier is higher than a pre-specified
threshold.
In rare cases where an NP to be resolved has only one antecedent candidate, a
pseudo-instance is created by pairing the candidate with itself. The NP will be resolved
to the candidate unless the instance is labeled ?00?.
4.3 Evaluation
4.3.1 Experimental Setup. We used the same ACE data sets for coreference resolution
evaluation, as described in the previous section for anaphora resolution. A raw input
document was processed in advance by the same pipeline of NLP modules including
16 If only one candidate remains, it will be compared with the candidate eliminated last.
347
Computational Linguistics Volume 34, Number 3
Table 17
Statistics of the training instances generated for coreference resolution (non-pronoun).
NWire NPaper BNews
0 instances 78,191 105,152 33,748Single-Candidate 1 instances 3,197 3,792 2,094
00 instances 296,000 331,957 159,752
Twin-Candidate 01 instances 50,499 70,433 21,170
10 instances 27,692 34,719 12,578
POS-tagger, NP chunker, NE recognizer, and so on, to obtain all possible NPs and
related information (see Section 3.5.1).
For evaluation, we adopted Vilain et al?s (1995) scoring algorithm in which recall
and precision17 were computed by comparing the key chains (i.e., the annotated ?stan-
dard? coreferential chains) and the response chains (i.e., the chains generated by the
coreference resolution system).
As already mentioned, the twin-candidate model described in this section is mainly
meant for non-pronouns that are often not anaphoric. To better examine the utility
of the model in our experiments, we first focused on coreference resolution for non-
pronominal NPs. The recall and precision to be reported were computed based on the
response chains and the key chains from which all the pronouns are removed. We will
later show the results of overall coreference resolution for whole NPs by combining the
resolution of pronouns and non-pronouns.
In non-pronoun resolution, an anaphor and its antecedent do not often occur a short
distance apart as they do in pronoun resolution. For this reason, during training, we
took as antecedent candidates all the preceding non-pronominal NPs18 in the current
and previous four sentences; while during testing, we used all the preceding non-
pronouns, regardless of distance, as candidates.19 The statistics of the training instances
for each data set are summarized in Table 17.
Again, we examined the three learning algorithms: C5, MaxEnt, and SVM.20 As
both the single-candidate and the twin-candidate models used a threshold to block low-
confidence coreferential pairs, we performed three-fold cross-evaluation on the training
data to determine the thresholds for the coreference resolution systems.
4.3.2 Results and Discussions. Table 18 lists the results for the different systems on the
non-pronominal NP coreference resolution. We used as the baseline the system with the
single-candidate model described in Section 4.1. As mentioned, the system was trained
17 The overall F-measure was defined as
2 ? Recall ? Precision
Recall+ Precision
18 As suggested in Ng and Cardie (2002b), we did not include pronouns in the candidate set of a
non-pronoun, because a pronoun is usually anaphoric and cannot give much information about
the entity to which it refers.
19 Unlike in the case of pronoun resolution, we did not filter candidates that had mismatched
number/gender agreement as these constraints are not reliable for non-pronoun resolution (e.g.,
in our data set, around 15% of coreferential pairs do not agree in number). Instead, we took these
factors as features (see Table 15) and let the learning algorithm make the preference decision.
20 For SVM, we employed the one-against-all aggregation method for the 3-class learning and testing.
348
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 18
Recall (R), Precision (P), and F-measure (F) in percent for coreference resolution (non-pronoun).
NWire NPaper BNews
R P F R P F R P F
C5 SC
- baseline 63.3 48.1 54.7 63.8 42.2 50.8 63.5 53.7 58.2
- with non-anaphors 40.9 81.5 54.4 39.8 81.4 53.4 35.1 76.8 48.2
TC
- Elimination 50.8 63.0 56.2 56.6 60.1 58.3 44.6 71.2 54.9
- Round Robin 58.7 57.9 58.3 56.5 60.5 58.4 49.0 70.1 57.7
MaxEnt SC
- baseline 62.1 52.3 56.8 56.4 58.8 57.6 61.8 54.1 57.7
- with non-anaphors 59.6 54.0 56.7 54.2 62.6 58.1 53.8 58.4 56.0
TC
- Elimination 59.1 55.4 57.2 52.2 69.0 59.5 53.5 61.9 57.4
- Round Robin 58.7 55.9 57.2 53.4 65.9 59.0 54.3 62.8 58.3
SVM SC
- baseline 64.1 49.0 55.5 65.5 42.1 51.3 63.5 53.7 58.2
- with non-anaphors 42.3 70.0 52.7 40.0 76.6 52.5 35.7 77.0 48.8
TC
- Elimination 57.8 53.2 55.4 51.7 56.5 54.0 63.3 53.8 58.2
- Round Robin 54.3 56.9 55.6 56.1 58.1 57.1 63.7 53.8 58.3
on the instances formed by anaphors. For better comparison with the twin-candidate
model, we built another single-candidate-based system inwhich the non-anaphors were
also incorporated for training. Specifically, for each encountered non-anaphor during
training, we created a set of ?0? instances by pairing the non-anaphor with each of the
candidates. These instances were added to the original instances formed by anaphors
to learn a classifier,21 which was then applied for the resolution as usual.
The results for the two single-candidate based systems are listed in Table 18. When
trained with the instances formed only by anaphors, the system could achieve recall
above 60% and precision of around 50% for the three domains. When trained with the
instances formed by both anaphors and non-anaphors, the system yielded a significant
improvement in precision. In the case of using C5 and SVM, the system is capable of
producing precision rates of up to 80%. The increase in precision is reasonable since the
classifier tends to be stricter in blocking non-anaphors. Unfortunately, however, at the
same time recall drops significantly, and no apparent improvement can be observed in
the resulting overall F-measure.
When trained with non-anaphors incorporated, the systems with the twin-
candidate model, described in Section 4.2, are capable of yielding higher precision
against the baseline. Although recall also drops at the same time, the increase in
precision can compensate it well: We observe that in most cases, the system with the
twin-candidate model can achieve a better F-measure than the baseline system with
the single-candidate model. Also, the improvement is statistically significant (t-test,
p < 0.05) in the NWire domain when C5 is used (3.6%), and in the NPaper domain
21 The statistics of the ?0? instances shown in Table 17 become 392,646, 455,167, and 207,667 for NWire,
NPaper, and BNews, respectively.
349
Computational Linguistics Volume 34, Number 3
when any of the three learning algorithms, C5 (5.0%), MaxEnt (1.4%), and SVM (4.6%),
is used. These results suggest that our twin-candidate model can effectively identify
non-anaphors and block their invalid resolution, without affecting the accuracy of
determining antecedents for anaphors.
Compared with the pronoun resolution described in the previous section, here
we find that for non-pronoun resolution the superiority of the twin-candidate model
against the single-candidate model is not apparent. In some domains such as BNews,
the difference between the two models is not statistically significant. One possible
explanation is that for non-pronoun resolution, the features that really matter are quite
limited, that is, NameAlias, String-Matching, and Appositive (we will later show this
in the decision trees). A candidate that has any one of these features is most likely the
antecedent, regardless of the other competing candidates. In this situation, the single-
candidate model, which considers candidates in isolation, does as well as the twin-
candidate model. Still, the results suggest that the twin-candidate model is suitable for
both resolution tasks, no matter whether the features involved are strongly indicative
(as with non-pronoun resolution) or not (as with pronoun resolution).
As with anaphora resolution, we do not observe any apparent performance differ-
ence between the two twin-candidate identification schemes, Tournament Elimination
and Round Robin. The Round Robin scheme performs better than Elimination when
trained using C5 and SVM, by up to 2.8% and 2.9% in F-measure, respectively. However,
the Elimination scheme, when trained using MaxEnt, is capable of performing equally
well or slightly better (0.5% F-measure) than the Round Robin scheme.
Recall vs. Precision As discussed, the results in Table 18 show different recall and
precision patterns for different systems. The baseline system with the single-candidate
model tends to yield higher recall while the system with the twin-candidate model
tends to produce higher precision. Thus, a fairer comparison of the two systems is to
examine the precision rates that these systems achieve under the same recall rates. For
this purpose, in Figure 4, we plot the variant recall and precision rates that the two
systems are capable of obtaining (tested using MaxEnt, Round Robin scheme, for the
NPaper domain), focusing on precision rates above 50% and recall rates above 40%.
From the figure, we find that the systemwith the twin-candidate model achieves higher
precision for recall rates ranging from 40% and 55%, and performs equally well for recall
rates above 55%, which further proves the reliability of our twin-candidate model for
coreference resolution.
Decision Trees In Figures 5 and 6, we show the two decision trees (NWire domain)
generated by the systems with the single-candidate model and the twin-candidate
model, respectively. The tree from the single-candidate model contains only 13 nodes,
considerably smaller than that from the twin-candidate model, which contains around
1.2k nodes. From the figure, we can see that both models heavily rely on string-
matching, name-alias, and appositive features to perform non-pronoun resolution, in
contrast to pronoun resolution where lexical and positional features seem more impor-
tant (as shown in Figures 1 and 2).
Learning Curves In our experiments, we were also interested in evaluating the
resolution performance of the two learning models on different quantities of training
data. Figure 7 plots the learning curves for the systems using the single-candidate model
and the system using the twin-candidate model (NPaper domain). The F-measure is
averaged over three random trials trained on 5, 10, 15, . . . documents. Consistent with
the curves for the anaphora resolution task as depicted in Figure 3, the system with
the twin-candidate model outperforms the one with the single-candidate model on a
small amount of training data (less than five documents). When more data is available,
350
Yang, Su, and Tan A Twin-Candidate Model for AR
Figure 4
Various recall (%) and precision (%) of different models for non-pronoun resolution.
Figure 5
Decision tree generated for non-pronoun resolution under the single-candidate model.
the twin-candidate model also yields a consistently better F-measure than the single-
candidate model.
Overall Coreference Resolution Having demonstrated the performance of the
twin-candidate model on coreference resolution for non-pronouns, we now further
examine overall coreference resolution for whole NPs, combining both pronoun
resolution and non-pronoun resolution. Specifically, given an input test document,
we check each encountered NP from beginning to end. If it is a pronoun,22 we use
22 We identify the pleonastic use of it in advance (79.2% accuracy) using a set of predefined pattern rules
based on regular expressions. The first-person and second-person pronouns are heuristically resolved
to the closest pronoun of the same type or a speaker nearby, if any, with an average 61.8% recall and
79.5% precision.
351
Computational Linguistics Volume 34, Number 3
Figure 6
Decision tree generated for non-pronoun resolution under the twin-candidate model.
Figure 7
Learning curves of different models for non-pronoun resolution.
the pronominal anaphora resolution systems, as described in the previous section, to
resolve it to an antecedent. Otherwise, we use the non-pronoun coreference resolution
systems described in this section to resolve the NP to an antecedent, if any is found. All
the coreferential pairs are put together in a coreferential chain. The recall and precision
rates are computed by comparing the standard key chains and generated response
chains using Vilain et al?s (1995) algorithm.
352
Yang, Su, and Tan A Twin-Candidate Model for AR
Table 19
Recall (R), Precision (P), and F-Measure (F) in percent for coreference resolution.
NWire NPaper BNews
R P F R P F R P F
C5 SC 62.2 52.6 57.0 64.9 50.6 56.9 62.9 58.5 60.6
TC
- Elimination 53.8 65.9 59.2 61.2 64.4 62.8 53.1 70.9 60.7
- Round Robin 59.0 61.2 60.1 62.0 64.3 63.1 56.0 69.9 62.2
MaxEnt SC 60.7 56.0 58.3 60.8 62.2 61.5 63.8 60.6 62.2
TC
- Elimination 59.5 59.2 59.3 58.6 67.8 62.9 59.3 66.4 62.7
- Round Robin 60.6 57.9 59.2 59.4 69.2 63.3 61.7 64.5 63.0
SVM SC 62.3 53.3 57.5 66.2 50.5 57.3 64.7 60.1 62.3
TC
- Elimination 57.6 57.0 57.3 58.5 62.6 60.5 65.0 60.6 62.7
- Round Robin 56.0 60.6 58.2 60.4 63.6 62.0 65.4 60.7 63.0
Table 19 lists the coreference resolution results of the systemswith different learning
models. We observe that the results for overall coreference resolution are better than
those of non-pronoun coreference resolution as shown in Table 18, which is due to the
comparatively high accuracy of the resolution of pronouns.
In line with the previous results for pronoun resolution and non-pronoun resolu-
tion, the twin-candidate model outperforms the single-candidate model in coreference
resolution for whole NPs. Consider the system trained with MaxEnt as an example.
The single-candidate-based system obtains F-measures of 58.3%, 61.5%, and 62.2% for
the NWire, NPaper, and BNews domains.23 By comparison, the twin-candidate-based
system (Round Robin scheme) can achieve F-measures of 59.2%, 63.3%, and 63.0% for
the three domains. The improvement over the single-candidate model in F-measure
(0.9%, 1.8%, and 0.8%) is larger than that for non-pronoun resolution (0.4%, 1.4%,
and 0.6% as shown in Table 18), owing to the higher gains obtained from pronoun
resolution. For the systems trained using C5 and SVM, similar patterns of performance
improvement may be observed.
5. Conclusion
In this article, we have presented a twin-candidate model for learning-based anaphora
resolution. The traditional single-candidate model considers candidates in isolation,
and thus cannot accurately capture the preference relationships between competing
candidates to provide reliable resolution. To deal with this problem, our proposed twin-
candidate model recasts anaphora resolution as a preference classification problem.
It learns a classifier that can explicitly determine the preference between competing
candidates, and then during resolution, choose the antecedent of an anaphor based on
the ranking of the candidates.
23 The results are comparable to the baseline system by Ng (2005), which also uses the single-candidate
model and is capable of F-measures of 50.1%, 62.1%, and 57.5% for the three domains, respectively.
353
Computational Linguistics Volume 34, Number 3
We have introduced in detail the framework of the twin-candidate model for
anaphora resolution, including instance representation, training procedure, and the an-
tecedent identification scheme. The efficacy of the twin-candidate model for pronominal
anaphora resolution has been evaluated in different domains, using ACE data sets. The
experimental results show that the model yields statistically significantly higher accu-
racy rates than the traditional single-candidate model (up to 4.2% in average accuracy
rate), suggesting that the twin-candidate model is superior to the latter for pronominal
anaphora resolution.
We have further investigated the deployment of the twin-candidate model in the
more complicated coreference resolution task, where not all the encountered NPs are
anaphoric. We have modified the model to make it directly applicable for coreference
resolution. The experimental results for non-pronoun resolution indicate that the twin-
candidate-based system performs equally well, and, in some domains, statistically
significantly better than the single-candidate based systems. When combined with
the results for pronoun resolution, the twin-candidate based system achieves further
improvement against the single-candidate-based systems in all the domains.
A number of further contributions can be made by extending this work in new
directions. Currently, we only adopt simple domain-independent features for learning.
Our recent work (Yang, Su, and Tan 2005) suggests that more complicated features, such
as statistics-based semantic compatibility, can be effectively incorporated in the twin-
candidate model for pronoun resolution. In future work, we intend to provide a more
in-depth investigation into the various kinds of knowledge that are suitable for the twin-
candidate model. Furthermore, in our current work for coreference resolution, all the
NPs preceding an anaphor are used as antecedent candidates, and all encountered non-
anaphors in texts are incorporated without filtering into training instance creation. For
more balanced training data and better classifier learning, we intend to explore some
instance-sampling techniques, such as those proposed by Ng and Cardie (2002a), to
remove in advance low-confidence candidates and the less informative non-anaphors.
We hope that these efforts can further improve the performance of the twin-candidate
model in both anaphora resolution and coreference resolution.
Acknowledgments
We would like to thank Guodong Zhou,
Alexia Leong, Stanley Wai Keong Yong,
and three anonymous reviewers for their
helpful comments and suggestions.
References
Aone, Ghinatsu and Scott W. Bennett.
1995. Evaluating automated and
manual acquisition of anaphora
resolution strategies. In Proceedings of
the 33rd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 122?129, Cambridge, Massachusetts.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39?71.
Chomsky, Noam. 1981. Lectures on
Government and Binding. Foris,
Dordrecht, The Netherlands.
Clark, Herber H. and C. J. Sengul. 1979.
In search of referents for noun phrases
and pronouns.Memory and Cognition,
7:35?41.
Connolly, Dennis, John D. Burger, and
David S. Day, 1997. A machine learning
approach to anaphoric reference. In
New Methods in Language Processing,
pages 133?144, Taylor and Francis,
Bristol, Pennsylvania.
Crawley, Rosalind A., Rosemary J.
Stevenson, and David Kleinman. 1990.
The use of heuristic strategies in the
interpretation of pronouns. Journal of
Psycholinguistic Research, 19:245?264.
Denis, Pascal and Jason Baldridge. 2007. A
ranking approach to pronoun resolution.
In Proceedings of the 20th International Joint
354
Yang, Su, and Tan A Twin-Candidate Model for AR
Conference on Artificial Intelligence (IJCAI),
pages 1588?1593, Hyderabad, India.
Garnham, Alan, 2001.Mental Models and the
Interpretation of Anaphora. Psychology
Press Ltd., Hove, East Sussex, UK.
Ge, Niyu, John Hale, and Eugene Charniak.
1998. A statistical approach to anaphora
resolution. In Proceedings of the 6th
Workshop on Very Large Corpora,
pages 161?171, Montreal, Quebec, Canada.
Gernsbacher, Morton A. and David
Hargreaves. 1988. Accessing sentence
participants: The advantage of first
mention. Journal of Memory and Language,
27:699?717.
Grober, Ellen H., William Beardsley, and
Alfonso Caramazza. 1978. Parallel
function in pronoun assignment.
Cognition, 6:117?133.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Hobbs, Jerry. 1978. Resolving pronoun
references. Lingua, 44:339?352.
Iida, Ryu, Kentaro Inui, Hiroya Takamura,
and Yuji Matsumoto. 2003. Incorporating
contextual cues in trainable models for
coreference resolution. In Proceedings of the
10th Conference of EACL, Workshop ?The
Computational Treatment of Anaphora,?
pages 23?30, Budapest, Hungary.
Joachims, Thorsten. 2002. Optimizing search
engines using clickthrough data. In
Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining
(KDD), pages 133?142, Edmonton,
Alberta, Canada.
Jurafsky, Daniel and James H. Martin.
2000. Speech and Language Processing:
An Introduction to Natural Language
Processing, Computational Linguistics,
and Speech Recognition. Prentice Hall,
Upper Saddle River, New Jersey.
Kehler, Andrew. 1997. Probabilistic
coreference in information extraction.
In Proceedings of the 2nd Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 163?173,
Providence, Rhode Island.
Kehler, Andrew, Douglas Appelt,
Lara Taylor, and Aleksandr Simma. 2004.
The (non)utility of predicate-argument
frequencies for pronoun interpretation.
In Proceedings of the North American
Chapter of the Association for Computational
Linguistics annual meeting (NAACL),
pages 289?296, Boston, MA.
Lappin, Shalom and Herbert J. Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):525?561.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan
Jing, Nanda Kambhatla, and Salim
Roukos. 2004. A mention-synchronous
coreference resolution algorithm
based on the bell tree. In Proceedings
of the 42nd Annual Meeting of the
Association for Computational
Linguistics (ACL), pages 135?142,
Barcelona, Spain.
McCarthy, Joseph F. and Wendy G. Lehnert.
1995. Using decision trees for coreference
resolution. In Proceedings of the 14th
International Conference on Artificial
Intelligences (IJCAI), pages 1050?1055,
Montreal, Quebec, Canada.
McEnery, A., I. Tanaka, and S. Botley.
1997. Corpus annotation and reference
resolution. In Proceedings of the ACL
Workshop on Operational Factors in
Practical Robust Anaphora Resolution
for Unrestricted Texts, pages 67?74,
Madrid, Spain.
Ng, Hwee Tou, Yu Zhou, Robert Dale,
and Mary Gardiner. 2005. Machine
learning approach to identification
and resolution of one-anaphora. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1105?1110, Edinburgh,
Scotland.
Ng, Vincent. 2005. Machine learning for
coreference resolution: From local
classification to global ranking. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 157?164,
Ann Arbor, Michigan.
Ng, Vincent and Claire Cardie. 2002a.
Combining sample selection and
error-driven pruning for machine
learning of coreference rules. In
Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 55?62,
Philadelphia, PA.
Ng, Vincent and Claire Cardie. 2002b.
Improving machine learning approaches
to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 104?111, Philadelphia, PA.
Preiss, Judita. 2001. Machine learning for
anaphora resolution. Technical Report
CS-01-10, University of Sheffield,
Sheffield, England.
355
Computational Linguistics Volume 34, Number 3
Quinlan, J. Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann
Publishers, San Francisco, CA.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine
learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521?544.
Stevenson, Rosemary J., Alexander W. R.
Nelson, and Keith Stenning. 1995. The role
of parallelism in strategies of pronoun
comprehension. Language and Speech,
29:393?418.
Strube, Michael and Christoph Mueller.
2003. A machine learning approach to
pronoun resolution in spoken dialogue.
In Proceedings of the 41st Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 168?175,
Sapporo, Japan.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York, NY.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6),
pages 45?52, San Francisco, CA.
Wilks, Yorick. 1973. Preference Semantics.
Stanford AI Laboratory Memo AIM-206.
Stanford University.
Winograd, Terry. 1972. Understanding Natural
Language. Academic Press, New York.
Yang, Xiaofeng, Jian Su, and Chew Lim Tan.
2005. Improving pronoun resolution using
statistics-based semantic compatibility
information. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 165?172, Ann Arbor, MI.
Zhou, Guodong and Jian Su. 2000.
Error-driven HMM-based chunk tagger
with context-dependent lexicon. In
Proceedings of the Joint Conference on
Empirical Methods in Natural Language
Processing and Very Large Corpora,
pages 71?79, Hong Kong.
Zhou, Guodong and Jian Su. 2002. Named
Entity recognition using a HMM-based
chunk tagger. In Proceedings of the 40th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 473?480, Philadelphia, PA.
356
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 288?295,
New York, June 2006. c?2006 Association for Computational Linguistics
Exploring Syntactic Features for Relation Extraction using 
a Convolution Tree Kernel 
 
 
 
Min ZHANG         Jie ZHANG        Jian SU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, zhangjie, sujian}@i2r.a-star.edu.sg 
 
 
Abstract 
This paper proposes to use a convolution 
kernel over parse trees to model syntactic 
structure information for relation extrac-
tion. Our study reveals that the syntactic 
structure features embedded in a parse 
tree are very effective for relation extrac-
tion and these features can be well cap-
tured by the convolution tree kernel. 
Evaluation on the ACE 2003 corpus 
shows that the convolution kernel over 
parse trees can achieve comparable per-
formance with the previous best-reported 
feature-based methods on the 24 ACE re-
lation subtypes. It also shows that our 
method significantly outperforms the pre-
vious two dependency tree kernels on the 
5 ACE relation major types. 
1 Introduction 
Relation extraction is a subtask of information ex-
traction that finds various predefined semantic re-
lations, such as location, affiliation, rival, etc., 
between pairs of entities in text. For example, the 
sentence ?George Bush is the president of the 
United States.? conveys the semantic relation 
?President? between the entities ?George Bush? 
(PER) and ?the United States? (GPE: a Geo-Political 
Entity --- an entity with land and a government (ACE, 2004)). 
Prior feature-based methods for this task 
(Kambhatla 2004; Zhou et al, 2005) employed a 
large amount of diverse linguistic features, varying 
from lexical knowledge, entity mention informa-
tion to syntactic parse trees, dependency trees and 
semantic features. Since a parse tree contains rich 
syntactic structure information, in principle, the 
features extracted from a parse tree should contrib-
ute much more to performance improvement for 
relation extraction. However it is reported (Zhou et 
al., 2005; Kambhatla, 2004) that hierarchical struc-
tured syntactic features contributes less to per-
formance improvement. This may be mainly due to 
the fact that the syntactic structure information in a 
parse tree is hard to explicitly describe by a vector 
of linear features. As an alternative, kernel meth-
ods (Collins and Duffy, 2001) provide an elegant 
solution to implicitly explore tree structure features 
by directly computing the similarity between two 
trees. But to our surprise, the sole two-reported 
dependency tree kernels for relation extraction on 
the ACE corpus (Bunescu and Mooney, 2005; Cu-
lotta and Sorensen, 2004) showed much lower per-
formance than the feature-based methods. One 
may ask: are the syntactic tree features very useful 
for relation extraction? Can tree kernel methods 
effectively capture the syntactic tree features and 
other various features that have been proven useful 
in the feature-based methods? 
In this paper, we demonstrate the effectiveness 
of the syntactic tree features for relation extraction 
and study how to capture such features via a con-
volution tree kernel. We also study how to select 
the optimal feature space (e.g. the set of sub-trees 
to represent relation instances) to optimize the sys-
tem performance. The experimental results show 
that the convolution tree kernel plus entity features 
achieves slightly better performance than the pre-
vious best-reported feature-based methods. It also 
shows that our method significantly outperforms 
the two dependency tree kernels (Bunescu and 
Mooney, 2005; Culotta and Sorensen, 2004) on the 
5 ACE relation types. 
The rest of the paper is organized as follows. In 
Section 2, we review the previous work. Section 3 
discusses our tree kernel based learning algorithm. 
288
Section 4 shows the experimental results and com-
pares our work with the related work. We conclude 
our work in Section 5.   
2 Related Work 
The task of relation extraction was introduced as a 
part of the Template Element task in MUC6 and 
formulated as the Template Relation task in MUC7 
(MUC, 1987-1998). 
Miller et al (2000) address the task of relation 
extraction from the statistical parsing viewpoint. 
They integrate various tasks such as POS tagging, 
NE tagging, template extraction and relation ex-
traction into a generative model. Their results es-
sentially depend on the entire full parse tree. 
 Kambhatla (2004) employs Maximum Entropy 
models to combine diverse lexical, syntactic and 
semantic features derived from the text for relation 
extraction. Zhou et al (2005) explore various fea-
tures in relation extraction using SVM. They con-
duct exhaustive experiments to investigate the 
incorporation and the individual contribution of 
diverse features. They report that chunking infor-
mation contributes to most of the performance im-
provement from the syntactic aspect.  
The features used in Kambhatla (2004) and 
Zhou et al (2005) have to be selected and carefully 
calibrated manually. Kambhatla (2004) use the 
path of non-terminals connecting two mentions in 
a parse tree as the parse tree features. Besides, 
Zhou et al (2005) introduce additional chunking 
features to enhance the parse tree features. How-
ever, the hierarchical structured information in the 
parse trees is not well preserved in their parse tree-
related features.  
As an alternative to the feature-based methods, 
kernel methods (Haussler, 1999) have been pro-
posed to implicitly explore features in a high di-
mensional space by employing a kernel function to 
calculate the similarity between two objects di-
rectly. In particular, the kernel methods could be 
very effective at reducing the burden of feature 
engineering for structured objects in NLP research 
(Culotta and Sorensen, 2004). This is because a 
kernel can measure the similarity between two dis-
crete structured objects directly using the original 
representation of the objects instead of explicitly 
enumerating their features. 
Zelenko et al (2003) develop a tree kernel for 
relation extraction. Their tree kernel is recursively 
defined in a top-down manner, matching nodes 
from roots to leaf nodes. For each pair of matching 
nodes, a subsequence kernel on their child nodes is 
invoked, which matches either contiguous or 
sparse subsequences of node. Culotta and Sorensen 
(2004) generalize this kernel to estimate similarity 
between dependency trees. One may note that their 
tree kernel requires the matchable nodes must be at 
the same depth counting from the root node. This 
is a strong constraint on the matching of syntax so 
it is not surprising that the model has good preci-
sion but very low recall on the ACE corpus (Zhao 
and Grishman, 2005). In addition, according to the 
top-down node matching mechanism of the kernel, 
once a node is not matchable with any node in the 
same layer in another tree, all the sub-trees below 
this node are discarded even if some of them are 
matchable to their counterparts in another tree. 
Bunescu and Mooney (2005) propose a shortest 
path dependency kernel for relation extraction. 
They argue that the information to model a rela-
tionship between entities is typically captured by 
the shortest path between the two entities in the 
dependency graph. Their kernel is very straight-
forward. It just sums up the number of common 
word classes at each position in the two paths. We 
notice that one issue of this kernel is that they limit 
the two paths must have the same length, otherwise 
the kernel similarity score is zero. Therefore, al-
though this kernel shows non-trivial performance 
improvement than that of Culotta and Sorensen 
(2004), the constraint makes the two dependency 
kernels share the similar behavior: good precision 
but much lower recall on the ACE corpus. 
Zhao and Grishman (2005) define a feature-
based composite kernel to integrate diverse fea-
tures. Their kernel displays very good performance 
on the 2004 version of ACE corpus. Since this is a 
feature-based kernel, all the features used in the 
kernel have to be explicitly enumerated. Similar 
with the feature-based method, they also represent 
the tree feature as a link path between two entities. 
Therefore, we wonder whether their performance 
improvement is mainly due to the explicitly incor-
poration of diverse linguistic features instead of the 
kernel method itself. 
The above discussion suggests that the syntactic 
features in a parse tree may not be fully utilized in 
the previous work, whether feature-based or ker-
nel-based. We believe that the syntactic tree fea-
tures could play a more important role than that 
289
reported in the previous work. Since convolution 
kernels aim to capture structural information in 
terms of sub-structures, which providing a viable 
alternative to flat features, in this paper, we pro-
pose to use a convolution tree kernel to explore 
syntactic features for relation extraction. To our 
knowledge, convolution kernels have not been ex-
plored for relation extraction1.  
3 Tree Kernels for Relation Extraction  
In this section, we discuss the convolution tree 
kernel associated with different relation feature 
spaces. In Subsection 3.1, we define seven differ-
ent relation feature spaces over parse trees. In Sub-
section 3.2, we introduce a convolution tree kernel 
for relation extraction. Finally we compare our 
method with the previous work in Subsection 3.3. 
3.1 Relation Feature Spaces 
In order to study which relation feature spaces (i.e., 
which portion of parse trees) are optimal for rela-
tion extraction, we define seven different relation 
feature spaces as follows (as shown in Figure 1): 
 
(1) Minimum Complete Tree (MCT):  
It is the complete sub-tree rooted by the node of 
the nearest common ancestor of the two entities 
under consideration.  
 
(2) Path-enclosed Tree (PT): 
It is the smallest common sub-tree including the 
two entities. In other words, the sub-tree is en-
closed by the shortest path linking the two entities 
in the parse tree (this path is also typically used as 
the path tree features in the feature-based meth-
ods). 
 
(3) Chunking Tree (CT): 
It is the base phrase list extracted from the PT. 
We prune out all the internal structures of the PT 
and only keep the root node and the base phrase 
list for generating the chunking tree. 
                                                          
1 Convolution kernels were proposed as a concept of kernels 
for a discrete structure by Haussler (1999) in machine learning 
study. This framework defines a kernel between input objects 
by applying convolution ?sub-kernels? that are the kernels for 
the decompositions (parts) of the objects. Convolution kernels 
are abstract concepts, and the instances of them are deter-
mined by the definition of ?sub-kernels?. The Tree Kernel 
(Collins and Duffy, 2001), String Subsequence Kernel (SSK) 
(Lodhi et al, 2002) and Graph Kernel (HDAG Kernel) (Su-
zuki et al, 2003) are examples of convolution kernels in-
stances in the NLP field.  
(4) Context-Sensitive Path Tree (CPT): 
It is the PT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the 
node of entity 2. If the sibling is unavailable, then 
we move to the parent of current node and repeat 
the same process until the sibling is available or 
the root is reached. 
(5) Context-Sensitive Chunking Tree (CCT): 
It is the CT extending with the 1st left sibling of 
the node of entity 1 and the 1st right sibling of the 
node of entity 2. If the sibling is unavailable, the 
same process as generating the CPT is applied. 
Then we do a further pruning process to guarantee 
that the context structures of the CCT is still a list 
of base phrases.  
(6) Flattened  PT (FPT): 
We define two criteria to flatten the PT in order 
to generate the Flattened Parse tree: if the in and 
out arcs of a non-terminal node (except POS node) 
are both single, the node is to be removed; if a 
node has the same phrase type with its father node, 
the node is also to be removed. 
(7) Flattened CPT (FCPT): 
We use the above two criteria to flatten the CPT 
tree to generate the Flattened CPT.  
Figure 1 in the next page illustrates the different 
sub-tree structures for a relation instance in sen-
tence ?Akyetsu testified he was powerless to stop 
the merger of an estimated 2000 ethnic Tutsi's in 
the district of Tawba.?. The relation instance is an 
example excerpted from the ACE corpus, where an 
ACE-defined relation ?AT.LOCATED? exists be-
tween the entities ?Tutsi's? (PER) and ?district? 
(GPE).  
We use Charniak?s parser (Charniak, 2001) to 
parse the example sentence. Due to space limita-
tion, we do not show the whole parse tree of the 
entire sentence here. Tree T1 in Figure 1 is the 
MCT of the relation instance example, where the 
sub-structure circled by a dashed line is the PT. 
For clarity, we re-draw the PT as in T2. The only 
difference between the MCT and the PT lies in 
that the MCT does not allow the partial production 
rules. For instance, the most-left two-layer sub-tree 
[NP [DT ? E1-O-PER]] in T1 is broken apart in 
T2. By comparing the performance of T1 and T2, we 
can test whether the sub-structures with partial 
production rules as in T2 will decrease perform-
ance. T3 is the CT. By comparing the performance 
of T2 and T3, we want to study whether the chunk-
ing information or the parse tree is more effective 
290
for relation extraction. T4 is the CPT, where the 
two structures circled by dashed lines are the so-
called context structures. T5 is the CCT, where the 
additional context structures are also circled by 
dashed lines. We want to study if the limited con-
text information in the CPT and the CCT can help 
boost performance. Moreover, we illustrate the 
other two flattened trees in T6 and T7. The two cir-
cled nodes in T2 are removed in the flattened trees. 
We want to study if the eliminated small structures 
are noisy features for relation extraction.  
3.2 The Convolution Tree Kernel 
Given the relation instances defined in the previous 
section, we use the same convolution tree kernel as 
the parse tree kernel (Collins and Duffy, 2001) and 
the semantic kernel (Moschitti, 2004). Generally, 
we can represent a parse tree T by a vector of inte-
ger counts of each sub-tree type (regardless of its 
ancestors): 
 
( )T? = (# of sub-trees of type 1, ?, # of sub-
trees of type i, ?, # of sub-trees of type n) 
 
This results in a very high dimensionality since the 
number of different sub-trees is exponential in its 
size. Thus it is computational infeasible to directly 
use the feature vector ( )T? . To solve the compu-
 
T1): MCT 
T2): PT
T3): CT T4):CPT 
T5):CCT 
T6):FPT 
T7):FCPT
Figure 1. Relation Feature Spaces of the Example Sentence ??? to stop the merger of an estimated 
2000 ethnic Tutsi's in the district of Tawba.?, where the phrase type ?E1-O-PER? denotes 
that the current phrase is the 1st entity, its entity type is ?PERSON? and its mention level is 
?NOMIAL?, and likewise for the other two phrase types ?E2-O-GPE? and ?E-N-GPE?. 
291
tational issue, we introduce the tree kernel function 
which is able to calculate the dot product between 
the above high dimensional vectors efficiently. The 
kernel function is defined as follows: 
 
1 1 2 2
1 2 1 2 1 2
1 2
( , ) ( ), ( ) ( )[ ], ( )[ ]
( ) ( )
i
i in N n N i
K T T T T T i T i
I n I n
? ? ? ?
? ?
=< >=
= ?
?
? ? ?
 
where N1 and N2 are the sets of all nodes in trees T1 
and T2, respectively, and Ii(n) is the indicator func-
tion that is 1 iff a sub-tree of type i occurs with 
root at node n and zero otherwise. Collins and 
Duffy (2002) show that 1 2( , )K T T  is an instance of 
convolution kernels over tree structures, and which 
can be computed in 1 2(| | | |)O N N?  by the follow-
ing recursive definitions (Let 1 2( , )n n? =  
1 2( ) ( )i ii I n I n?? ):  
(1) if 1n  and 2n  do not have the same syntactic tag 
or their children are different then 1 2( , ) 0n n? = ; 
(2) else if their children are leaves (POS tags), then 
1 2( , ) 1n n ?? = ? ; 
(3) else 
1( )
1 2 1 2
1
( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j?
=
? = +?? , 
where 1( )nc n is the number of the children of 1n , 
( , )ch n j  is the jth child of node n  and 
? ( 0 1?< < ) is the decay factor in order to make 
the kernel value less variable with respect to the 
tree sizes. 
3.3 Comparison with Previous Work 
It would be interesting to review the differences 
between our method and the feature-based meth-
ods. The basic difference between them lies in the 
relation instance representation and the similarity 
calculation mechanism. A relation instance in our 
method is represented as a parse tree while it is 
represented as a vector of features in the feature-
based methods. Our method estimates the similar-
ity between two relation instances by only count-
ing the number of sub-structures that are in 
common while the feature methods calculate the 
dot-product between the feature vectors directly. 
The main difference between them is the different 
feature spaces. By the kernel method, we implicitly 
represent a parse tree by a vector of integer counts 
of each sub-structure type. That is to say, we con-
sider the entire sub-structure types and their occur-
ring frequencies. In this way, on the one hand, the 
parse tree-related features in the flat feature set2 
are embedded in the feature space of our method: 
?Base Phrase Chunking? and ?Parse Tree? fea-
tures explicitly appear as substructures of a parse 
tree. A few of entity-related features in the flat fea-
ture set are also captured by our feature space: ?en-
tity type? and ?mention level? explicitly appear as 
phrase types in a parse tree. On the other hand, the 
other features in the flat feature set, such as ?word 
features?, ?bigram word features?, ?overlap? and 
?dependency tree? are not contained in our feature 
space. From the syntactic viewpoint, the tree repre-
sentation in our feature space is more robust than 
?Parse Tree Path? feature in the flat feature set 
since the path feature is very sensitive to the small 
changes of parse trees (Moschitti, 2004) and it also 
does not maintain the hierarchical information of a 
parse tree. Due to the extensive exploration of syn-
tactic features by kernel, our method is expected to 
show better performance than the previous feature-
based methods. 
It is also worth comparing our method with the 
previous relation kernels. Since our method only 
counts the occurrence of each sub-tree without 
considering its ancestors, our method is not limited 
by the constraints in Culotta and Sorensen (2004) 
and that in Bunescu and Mooney (2005) as dis-
cussed in Section 2. Compared with Zhao and 
Grishman?s kernel, our method directly uses the 
original representation of a parse tree while they 
flatten a parse tree into a link and a path. Given the 
above improvements, our method is expected to 
outperform the previous relation kernels.  
4 Experiments 
The aim of our experiment is to verify the effec-
tiveness of using richer syntactic structures and the 
convolution tree kernel for relation extraction. 
4.1 Experimental Setting 
Corpus: we use the official ACE corpus for 2003 
evaluation from LDC as our test corpus. The ACE 
corpus is gathered from various newspaper, news-
wire and broadcasts. The same as previous work 
                                                          
2 For the convenience of discussion, without losing generality, 
we call the features used in Zhou et al (2005) and Kambhatla 
(2004) flat feature set. 
292
(Zhou et al, 2005), our experiments are carried out 
on explicit relations due to the poor inter-annotator 
agreement in annotation of implicit relations and 
their limited numbers. The training set consists of 
674 annotated text documents and 9683 relation 
instances. The test set consists of 97 documents 
and 1386 relation instances. The 2003 evaluation 
defined 5 types of entities: Persons, Organizations, 
Locations, Facilities and GPE. Each mention of an 
entity is associated with a mention type: proper 
name, nominal or pronoun. They further defined 5 
major relation types and 24 subtypes: AT (Base-In, 
Located?), NEAR (Relative-Location), PART 
(Part-of, Subsidiary ?), ROLE (Member, Owner 
?) and SOCIAL (Associate, Parent?). As previ-
ous work, we explicitly model the argument order 
of the two mentions involved. We thus model rela-
tion extraction as a multi-class classification prob-
lem with 10 classes on the major types (2 for each 
relation major type and a ?NONE? class for non-
relation (except 1 symmetric type)) and 43 classes 
on the subtypes (2 for each relation subtype and a 
?NONE? class for non-relation (except 6 symmet-
ric subtypes)). In this paper, we only measure the 
performance of relation extraction models on 
?true? mentions with ?true? chaining of corefer-
ence (i.e. as annotated by LDC annotators).  
  
Classifier: we select SVM as the classifier used in 
this paper since SVM can naturally work with ker-
nel methods and it also represents the state-of-the-
art machine learning algorithm. We adopt the one 
vs. others strategy and select the one with largest 
margin as the final answer. The training parameters 
are chosen using cross-validation (C=2.4 (SVM); 
? =0.4(tree kernel)). In our implementation, we 
use the binary SVMLight developed by Joachims 
(1998) and Tree Kernel Toolkits developed by 
Moschitti (2004). 
 
Kernel Normalization: since the size of a parse 
tree is not constant, we normalize 1 2( , )K T T by divid-
ing it by 1 1 2 2( , ) ( , )K T T K T T? .  
 
Evaluation Method: we parse the sentence using 
Charniak parser and iterate over all pair of men-
tions occurring in the same sentence to generate 
potential instances. We find the negative samples 
are 10 times more than the positive samples. Thus 
data imbalance and sparseness are potential prob-
lems. Recall (R), Precision (P) and F-measure (F) 
are adopted as the performance measure. 
4.2 Experimental Results  
In order to study the impact of the sole syntactic 
structure information embedded in parse trees on 
relation extraction, we remove the entity informa-
tion from parse trees by replacing the entity-related 
phrase type (?E1-O-PER?, etc., in Figure 1) with 
?NP?. Then we carry out a couple of preliminary 
experiments on the test set using parse trees re-
gardless of entity information.  
 
Feature Spaces P R F 
Minimum Complete Tree 77.45 38.39 51.34 
Path-enclosed Tree (PT) 72.77 53.80 61.87 
Chunking Tree (CT) 75.18 44.75 56.11 
Context-Sensitive PT(CPT) 77.87 42.80 55.23 
Context-Sensitive CT 78.33 40.84 53.69 
Flattened PT 76.86 45.69 57.31 
Flattened CPT 80.60 41.20 54.53 
 
Table 1. Performance of seven relation feature 
spaces over the 5 ACE major types using parse 
tree information only 
 
Table 1 reports the performance of our defined 
seven relation feature spaces over the 5 ACE major 
types using parse tree information regardless of 
any entity information. This preliminary experi-
ments show that:  
 
 
? Overall the tree kernel over different relation 
feature spaces is effective for relation extraction 
since we use the parse tree information only. We 
will report the detailed performance comparison 
results between our method and previous work 
later in this section. 
? Using the PTs achieves the best performance. 
This means the portion of a parse tree enclosed 
by the shortest path between entities can model 
relations better than other sub-trees. 
? Using the MCTs get the worst performance. 
This is because the MCTs introduce too much 
left and right context information, which may be 
noisy features, as shown in Figure 1. It suggests 
that only allowing complete (not partial) produc-
tion rules in the MCTs does harm performance. 
? The performance of using CTs drops by 5 in F-
measure compared with that of using the PTs. 
This suggests that the middle and high-level 
structures beyond chunking is also very useful 
for relation extraction. 
293
? The context-sensitive trees show lower perform-
ance than the corresponding original PTs and 
CTs. In some cases (e.g. in sentence ?the merge 
of company A and company B?.?, ?merge? is 
the context word), the context information is 
helpful. However the effective scope of context 
is hard to determine. 
? The two flattened trees perform worse than the 
original trees, but better than the corresponding 
context-sensitive trees. This suggests that the 
removed structures by the flattened trees con-
tribute non-trivial performance improvement.  
 
In the above experiments, the path-enclosed tree 
displays the best performance among the seven 
feature spaces when using the parse tree structural 
information only. In the following incremental ex-
periments, we incorporate more features into the 
path-enclosed parse trees and it shows significant 
performance improvement. 
 
Path-enclosed Tree (PT) P R F 
Parse tree structure in-
formation only 
72.77 53.80 61.87 
+Entity information  76.14 62.85 68.86 
+Semantic features 76.32 62.99 69.02 
 
Table 2. Performance of Path-enclosed Trees 
with different setups over the 5 ACE major types 
 
Table 2 reports the performance over the 5 ACE 
major types using Path-enclosed trees enhanced 
with more features in nodes. The 1st row is the 
baseline performance using structural information 
only. We then integrate entity information, includ-
ing Entity type and Mention level features, into the 
corresponding nodes as shown in Figure 1. The 2nd 
row in Table 2 reports the performance of this 
setup. Besides the entity information, we further 
incorporate the semantic features used in Zhou et 
al. (2005) into the corresponding leaf nodes. The 
3rd row in Table 2 reports the performance of this 
setup. Please note that in the 2nd and 3rd setups, we 
still use the same tree kernel function with slight 
modification on the rule (2) in calculating 
1 2( , )n n?  (see subsection 3.2) to make it consider 
more features associated with each individual 
node: 1 2( , )  n n feature weight ?? = ? . From Table 
2, we can see that the basic feature of entity infor-
mation is quite useful, which largely boosts per-
formance by 7 in F-measure. The final 
performance of our tree kernel method for relation 
extraction is 76.32/62.99/69.02 in preci-
sion/recall/F-measure over the 5 ACE major types.   
 
Methods P R F 
Ours: convolution kernel 
over parse trees 
76.32 
(64.6) 
62.99 
(50.76)
69.02 
(56.83)
Kambhatla (2004):  
feature-based ME 
- 
(63.5) 
- 
(45.2) 
- 
(52.8) 
Zhou et al (2005):  
feature-based SVM 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5)
Culotta and Sorensen 
(2004): dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
Bunescu and Mooney 
(2005): shortest path de-
pendency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
 
Table 3. Performance comparison, the numbers in 
parentheses report the performance over the 24 
ACE subtypes while the numbers outside paren-
theses is for the 5 ACE major types 
 
Table 3 compares the performance of different 
methods on the ACE corpus3. It shows that our 
method achieves the best-reported performance on 
both the 24 ACE subtypes and the 5 ACE major 
types. It also shows that our tree kernel method 
significantly outperform the previous two depend-
ency kernel algorithms by 16 in F-measure on the 
5 ACE relation types4. This may be due to two rea-
sons: one reason is that the dependency tree lacks 
the hierarchical syntactic information, and another 
reason is due to the two constraints of the two de-
pendency kernels as discussed in Section 2 and 
Subsection 3.3. The performance improvement by 
our method suggests that the convolution tree ker-
nel can explore the syntactic features (e.g. parse 
tree structures and entity information) very effec-
tively and the syntactic features are also particu-
                                                          
3 Zhao and Grishman (2005) also evaluated their algorithm on 
the ACE corpus and got good performance. But their experi-
mental data is for 2004 evaluation, which defined 7 entity 
types with 44 entity subtypes, and 7 relation major types with 
27 subtypes, so we are not ready to compare with each other. 
4 Bunescu and Mooney (2005) used the ACE 2002 corpus, 
including 422 documents, which is known to have many in-
consistencies than the 2003 version. Culotta and Sorensen 
(2004) used an ACE corpus including about 800 documents, 
and they did not specify the corpus version. Since the testing 
corpora are in different sizes and versions, strictly speaking, it 
is not ready to compare these methods exactly and fairly. Thus 
Table 3 is only for reference purpose. We just hope that we 
can get a few clues from this table. 
294
larly effective for the task of relation extraction. In 
addition, we observe from Table 1 that the feature 
space selection (the effective portion of a parse 
tree) is also critical to relation extraction. 
  
Error Type # of error instance 
False Negative 414 
False Positive 173 
Cross Type 97 
 
Table 4. Error Distribution 
 
Finally, Table 4 reports the error distribution in 
the case of the 3rd experiment in Table 2. It shows 
that 85.9% (587/684) of the errors result from rela-
tion detection and only 14.1% (97/684) of the er-
rors result from relation characterization. This is 
mainly due to the imbalance of the posi-
tive/negative instances and the sparseness of some 
relation types on the ACE corpus. 
5 Conclusion and Future Work 
In this paper, we explore the syntactic features us-
ing convolution tree kernels for relation extraction. 
We conclude that: 1) the relations between entities 
can be well represented by parse trees with care-
fully calibrating effective portions of parse trees; 
2) the syntactic features embedded in a parse tree 
are particularly effective for relation extraction; 3) 
the convolution tree kernel can effectively capture 
the syntactic features for relation extraction. 
The most immediate extension of our work is to 
improve the accuracy of relation detection. We 
may adopt a two-step method (Culotta and Soren-
sen, 2004) to separately model the relation detec-
tion and characterization issues. We may integrate 
more features (such as head words or WordNet 
semantics) into nodes of parse trees. We can also 
benefit from the learning algorithm to study how to 
solve the data imbalance and sparseness issues 
from the learning algorithm viewpoint. In the fu-
ture, we would like to test our algorithm on the 
other version of the ACE corpus and to develop 
fast algorithm (Vishwanathan and Smola, 2002) to 
speed up the training and testing process of convo-
lution kernels.  
 
Acknowledgements: We would like to thank Dr. 
Alessandro Moschitti for his great help in using his 
Tree Kernel Toolkits and fine-tuning the system. 
We also would like to thank the three anonymous 
reviewers for their invaluable suggestions. 
References  
ACE. 2004. The Automatic Content Extraction (ACE) 
Projects. http://www.ldc.upenn.edu/Projects/ACE/ 
Bunescu R. C. and Mooney R. J. 2005. A Shortest Path 
Dependency Kernel for Relation Extraction. 
EMNLP-2005 
Charniak E. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001 
Collins M. and Duffy N. 2001. Convolution Kernels for 
Natural Language. NIPS-2001 
Culotta A. and Sorensen J. 2004. Dependency Tree Ker-
nel for Relation Extraction. ACL-2004 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
Joachims T. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kambhatla Nanda. 2004. Combining lexical, syntactic 
and semantic features with Maximum Entropy mod-
els for extracting relations. ACL-2004 (poster) 
Lodhi H., Saunders C., Shawe-Taylor J., Cristianini N. 
and Watkins C. 2002. Text classification using string 
kernel. Journal of Machine Learning Research, 
2002(2):419-444 
Miller S., Fox H., Ramshaw L. and Weischedel R. 2000. 
A novel use of statistical parsing to extract informa-
tion from text. NAACL-2000 
Moschitti Alessandro. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. ACL-2004 
MUC. 1987-1998. The nist MUC website: http: 
//www.itl.nist.gov/iaui/894.02/related_projects/muc/ 
Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003. Hi-
erarchical Directed Acyclic Graph Kernel: Methods 
for Structured Natural Language Data. ACL-2003 
Vishwanathan S.V.N. and Smola A.J. 2002. Fast ker-
nels for String and Tree Matching. NIPS-2002 
Zelenko D., Aone C. and Richardella A. 2003. Kernel 
Methods for Relation Extraction. Journal of Machine 
Learning Research. 2003(2):1083-1106 
Zhao Shubin and Grishman Ralph. 2005. Extracting 
Relations with Integrated Information Using Kernel 
Methods. ACL-2005 
Zhou Guodong, Su Jian, Zhang Jie and Zhang Min. 
2005. Exploring Various Knowledge in Relation Ex-
traction. ACL-2005 
295
Coreference Resolution Using Competition Learning Approach 
Xiaofeng Yang*+ Guodong Zhou*  Jian Su* Chew Lim Tan + 
*Institute for Infocomm Research, 
21 Heng Mui Keng Terrace, 
Singapore 119613 
+Department of Computer Science, 
National University of Singapore,  
Singapore 117543  
*{xiaofengy,zhougd,sujian}@ 
i2r.a-star.edu.sg 
+(yangxiao,tancl)@comp.nus.edu.sg
  
Abstract 
In this paper we propose a competition 
learning approach to coreference resolu-
tion. Traditionally, supervised machine 
learning approaches adopt the single-
candidate model. Nevertheless the prefer-
ence relationship between the antecedent 
candidates cannot be determined accu-
rately in this model. By contrast, our ap-
proach adopts a twin-candidate learning 
model. Such a model can present the 
competition criterion for antecedent can-
didates reliably, and ensure that the most 
preferred candidate is selected. Further-
more, our approach applies a candidate 
filter to reduce the computational cost and 
data noises during training and resolution. 
The experimental results on MUC-6 and 
MUC-7 data set show that our approach 
can outperform those based on the single-
candidate model.  
1 Introduction 
Coreference resolution is the process of linking 
together multiple expressions of a given entity. The 
key to solve this problem is to determine the ante-
cedent for each referring expression in a document.  
In coreference resolution, it is common that two 
or more candidates compete to be the antecedent of 
an anaphor (Mitkov, 1999). Whether a candidate is 
coreferential to an anaphor is often determined by 
the competition among all the candidates. So far, 
various algorithms have been proposed to deter-
mine the preference relationship between two can-
didates. Mitkov?s knowledge-poor pronoun 
resolution method (Mitkov, 1998), for example, 
uses the scores from a set of antecedent indicators 
to rank the candidates. And centering algorithms 
(Brennan et al, 1987; Strube, 1998; Tetreault, 
2001), sort the antecedent candidates based on the 
ranking of the forward-looking or backward-
looking centers. 
In recent years, supervised machine learning 
approaches have been widely used in coreference 
resolution (Aone and Bennett, 1995; McCarthy, 
1996; Soon et al, 2001; Ng and Cardie, 2002a), 
and have achieved significant success. Normally, 
these approaches adopt a single-candidate model in 
which the classifier judges whether an antecedent 
candidate is coreferential to an anaphor with a con-
fidence value. The confidence values are generally 
used as the competition criterion for the antecedent 
candidates. For example, the ?Best-First? selection 
algorithms (Aone and Bennett, 1995; Ng and 
Cardie, 2002a) link the anaphor to the candidate 
with the maximal confidence value (above 0.5). 
One problem of the single-candidate model, 
however, is that it only takes into account the rela-
tionships between an anaphor and one individual 
candidate at a time, and overlooks the preference 
relationship between candidates. Consequently, the 
confidence values cannot accurately represent the 
true competition criterion for the candidates. 
In this paper, we present a competition learning 
approach to coreference resolution. Motivated by 
the research work by Connolly et al (1997), our 
approach adopts a twin-candidate model to directly 
learn the competition criterion for the antecedent 
candidates. In such a model, a classifier is trained 
based on the instances formed by an anaphor and a 
pair of its antecedent candidates. The classifier is 
then used to determine the preference between any 
two candidates of an anaphor encountered in a new 
document. The candidate that wins the most com-
parisons is selected as the antecedent. In order to 
reduce the computational cost and data noises, our 
approach also employs a candidate filter to elimi-
nate the invalid or irrelevant candidates.  
The layout of this paper is as follows. Section 2 
briefly describes the single-candidate model and 
analyzes its limitation. Section 3 proposes in de-
tails the twin-candidate model and Section 4 pre-
sents our coreference resolution approach based on 
this model. Section 5 reports and discusses the ex-
perimental results. Section 6 describes related re-
search work. Finally, conclusion is given in 
Section 7. 
2 The Single-Candidate Model 
The main idea of the single-candidate model for 
coreference resolution is to recast the resolution as 
a binary classification problem. 
During training, a set of training instances is 
generated for each anaphor in an annotated text. 
An instance is formed by the anaphor and one of 
its antecedent candidates. It is labeled as positive 
or negative based on whether or not the candidate 
is tagged in the same coreferential chain of the 
anaphor. 
After training, a classifier is ready to resolve the 
NPs1 encountered in a new document. For each NP 
under consideration, every one of its antecedent 
candidates is paired with it to form a test instance. 
The classifier returns a number between 0 and 1 
that indicates the likelihood that the candidate is 
coreferential to the NP. 
The returned confidence value is commonly 
used as the competition criterion to rank the candi-
date. Normally, the candidates with confidences 
less than a selection threshold (e.g. 0.5) are dis-
carded. Then some algorithms are applied to 
choose one of the remaining candidates, if any, as 
the antecedent. For example, ?Closest-First? (Soon 
et al, 2001) selects the candidate closest to the 
anaphor, while ?Best-First? (Aone and Bennett, 
1995; Ng and Cardie, 2002a) selects the candidate 
with the maximal confidence value.  
One limitation of this model, however, is that it 
only considers the relationships between a NP en-
countered and one of its candidates at a time dur-
ing its training and testing procedures. The 
confidence value reflects the probability that the 
candidate is coreferential to the NP in the overall 
                                                          
1 In this paper a NP corresponds to a Markable in MUC 
coreference resolution tasks. 
distribution 2 , but not the conditional probability 
when the candidate is concurrent with other com-
petitors. Consequently, the confidence values are 
unreliable to represent the true competition crite-
rion for the candidates.  
To illustrate this problem, just suppose a data 
set where an instance could be described with four 
exclusive features: F1, F2, F3 and F4. The ranking 
of candidates obeys the following rule: 
CSF1 >> CSF2 >> CSF3 >> CSF4 
Here CSFi ( 41 ?? i ) is the set of antecedent can-
didates with the feature Fi on. The mark of ?>>? 
denotes the preference relationship, that is, the 
candidates in CSF1 is preferred to those in CSF2, and 
to those in CSF3 and CSF4.  
Let CF2 and CF3 denote the class value of a leaf 
node ?F2 = 1? and ?F3 = 1?, respectively. It is pos-
sible that CF2 < CF3, if the anaphors whose candi-
dates all belong to CSF3 or CSF4 take the majority in 
the training data set.  In this case, a candidate in 
CSF3 would be assigned a larger confidence value 
than a candidate in CSF2. This nevertheless contra-
dicts the ranking rules. If during resolution, the 
candidates of an anaphor all come from CSF2 or 
CSF3, the anaphor may be wrongly linked to a can-
didate in CSF3 rather than in CSF2. 
3 The Twin-Candidate Model 
Different from the single-candidate model, the 
twin-candidate model aims to learn the competition 
criterion for candidates. In this section, we will 
introduce the structure of the model in details. 
3.1 Training Instances Creation 
Consider an anaphor ana and its candidate set can-
didate_set, {C1, C2, ?, Ck}, where Cj is closer to 
ana than Ci if j > i. Suppose positive_set is the set 
of candidates that occur in the coreferential chain 
of ana, and negative_set is the set of candidates not 
in the chain, that is, negative_set = candidate_set  
- positive_set. The set of training instances based 
on ana, inst_set, is defined as follows:  
                                                          
2 Suppose we use C4.5 algorithm and the class value takes the 
smoothed ration, 
2
1
+
+
t
p , where p is the number of positive 
instances and t is the total number of instances contained in 
the corresponding leaf node. 
} _  C  , _Cj,i |{
  } _  C  ,_ C j,i |{
_
ji),,(
ji),,(
setpositvesetnegativeinst
setnegativesetpositveinst
setinst
anaCjCi
anaCjCi
??>
??>
=
U
 
From the above definition, an instance is 
formed by an anaphor, one positive candidate and 
one negative candidate. For each instance, 
)ana,cj,ci(inst , the candidate at the first position, Ci, 
is closer to the anaphor than the candidate at the 
second position, Cj.  
A training instance )ana,cj,ci(inst is labeled as 
positive if Ci ?  positive-set and Cj ?  negative-set; 
or negative if Ci ?  negative-set and Cj ?  positive-
set.  
See the following example:  
 
Any design to link China's accession to the WTO 
with the missile tests1 was doomed to failure.  
 ?If some countries2 try to block China TO acces-
sion, that will not be popular and will fail to win the 
support of other countries3? she said.  
Although no governments4 have suggested formal 
sanctions5 on China over the missile tests6, the United 
States has called them7 ?provocative and reckless? and 
other countries said they could threaten Asian stability.  
 
In the above text segment, the antecedent can-
didate set of the pronoun ?them7? consists of six 
candidates highlighted in Italics. Among the can-
didates, Candidate 1 and 6 are in the coreferential 
chain of ?them7?, while Candidate 2, 3, 4, 5 are not. 
Thus, eight instances are formed for ?them7?:  
 
(2,1,7)  (3,1,7)  (4,1,7)  (5,1,7) 
(6,5,7)  (6,4,7)  (6,3,7)  (6,2,7) 
 
Here the instances in the first line are negative, 
while those in the second line are all positive.  
3.2 Features Definition 
A feature vector is specified for each training or 
testing instance. Similar to those in the single-
candidate model, the features may describe the 
lexical, syntactic, semantic and positional relation-
ships of an anaphor and any one of its candidates. 
Besides, the feature set may also contain inter-
candidate features characterizing the relationships 
between the pair of candidates, e.g. the distance 
between the candidates in the number distances or 
paragraphs. 
3.3 Classifier Generation 
Based on the feature vectors generated for each 
anaphor encountered in the training data set, a 
classifier can be trained using a certain machine 
learning algorithm, such as C4.5, RIPPER, etc. 
Given the feature vector of a test instance 
)ana,cj,ci(inst  (i > j), the classifier returns the posi-
tive class indicating that Ci is preferred to Cj as the 
antecedent of ana; or negative indicating that Cj is 
preferred.  
3.4 Antecedent Identification 
Let CR( )ana,cj,ci(inst ) denote the classification re-
sult for an instance )ana,cj,ci(inst . The antecedent of 
an anaphor is identified using the algorithm shown 
in Figure 1.  
 
Algorithm ANTE-SEL 
Input: ana: the anaphor under consideration  
candidate_set: the set of antecedent can-
didates of ana, {C1, C2,?,Ck} 
 
for i = 1 to K do 
   Score[ i ] = 0; 
for  i = K downto 2 do 
for j = i ? 1 downto 1 do 
  if  CR( )ana,cj,ci(inst ) = = positive then  
Score[ i ]++; 
else  
Score[ j ] ++; 
  endif 
SelectedIdx= ][maxarg
_
iScore
setcandidateCii ?
 
return CselectedIdx; 
Figure 1:The antecedent identification algorithm
 
Algorithm ANTE-SEL takes as input an ana-
phor and its candidate set candidate_set, and re-
turns one candidate as its antecedent. In the 
algorithm, each candidate is compared against any 
other candidate. The classifier acts as a judge dur-
ing each comparison. The score of each candidate 
increases by one every time when it wins. In this 
way, the final score of a candidate records the total 
times it wins. The candidate with the maximal 
score is singled out as the antecedent.  
If two or more candidates have the same maxi-
mal score, the one closest to the anaphor would be 
selected. 
3.5 Single-Candidate Model: A Special Case 
of Twin-Candidate Model? 
While the realization and the structure of the twin-
candidate model are significantly different from 
the single-candidate model, the single-candidate 
model in fact can be regarded as a special case of 
the twin-candidate model.  
To illustrate this, just consider a virtual ?blank? 
candidate C0 such that we could convert an in-
stance )ana,ci(inst in the single-candidate model to 
an instance )ana,c,ci( 0inst in the twin-candidate 
model. Let )ana,c,ci( 0inst have the same class label 
as )ana,ci(inst , that is, )ana,c,ci( 0inst is positive if Ci is 
the antecedent of ana; or negative if not.  
Apparently, the classifier trained on the in-
stance set { )ana,ci(inst }, T1, is equivalent to that 
trained on { )ana,c,ci( 0inst }, T2.  T1 and T2 would 
assign the same class label for the test instances 
)ana,ci(inst  and )ana,c,ci( 0inst , respectively. That is to 
say, determining whether Ci is coreferential to ana 
by T1 in the single-candidate model equals to 
determining whether Ci is better than C0 w.r.t ana 
by T2 in the twin-candidate model. Here we could 
take C0 as a ?standard candidate?. 
While the classification in the single-candidate 
model can find its interpretation in the twin-
candidate model, it is not true vice versa. Conse-
quently, we can safely draw the conclusion that the 
twin-candidate model is more powerful than the 
single-candidate model in characterizing the rela-
tionships among an anaphor and its candidates. 
4 The Competition Learning Approach 
Our competition learning approach adopts the 
twin-candidate model introduced in the Section 3. 
The main process of the approach is as follows: 
1. The raw input documents are preprocessed to 
obtain most, if not all, of the possible NPs.  
2. During training, for each anaphoric NP, we 
create a set of candidates, and then generate 
the training instances as described in Section 3.  
3. Based on the training instances, we make use 
of the C5.0 learning algorithm (Quinlan, 1993) 
to train a classifier. 
4. During resolution, for each NP encountered, 
we also construct a candidate set. If the set is 
empty, we left this NP unresolved; otherwise 
we apply the antecedent identification algo-
rithm to choose the antecedent and then link 
the NP to it.  
4.1 Preprocessing 
To determine the boundary of the noun phrases, a 
pipeline of Nature Language Processing compo-
nents are applied to an input raw text: 
z Tokenization and sentence segmentation 
z Named entity recognition 
z Part-of-speech tagging 
z Noun phrase chunking 
Among them, named entity recognition, part-of-
speech tagging and text chunking apply the same 
Hidden Markov Model (HMM) based engine with 
error-driven learning capability (Zhou and Su, 
2000 & 2002). The named entity recognition 
component recognizes various types of MUC-style 
named entities, i.e., organization, location, person, 
date, time, money and percentage.  
4.2 Features Selection 
For our study, in this paper we only select those 
features that can be obtained with low annotation 
cost and high reliability. All features are listed in 
Table 1 together with their respective possible val-
ues.  
4.3 Candidates Filtering 
For a NP under consideration, all of its preceding 
NPs could be the antecedent candidates. Neverthe-
less, since in the twin-candidate model the number 
of instances for a given anaphor is about the square 
of the number of its antecedent candidates, the 
computational cost would be prohibitively large if 
we include all the NPs in the candidate set. More-
over, many of the preceding NPs are irrelevant or 
even invalid with regard to the anaphor. These data 
noises may hamper the training of a good-
performanced classifier, and also damage the accu-
racy of the antecedent selection: too many com-
parisons are made between incorrect candidates. 
Therefore, in order to reduce the computational 
cost and data noises, an effective candidate filter-
ing strategy must be applied in our approach. 
During training, we create the candidate set for 
each anaphor with the following filtering algorithm: 
1. If the anaphor is a pronoun,  
(a) Add to the initial candidate set al the pre-
ceding NPs in the current and the previous 
two sentences. 
(b) Remove from the candidate set those that 
disagree in number, gender, and person. 
(c) If the candidate set is empty, add the NPs in 
an earlier sentence and go to 1(b). 
2. If the anaphor is a non-pronoun, 
(a) Add all the non-pronominal antecedents to 
the initial candidate set. 
(b) For each candidate added in 2(a), add the 
non-pronouns in the current, the previous 
and the next sentences into the candidate set. 
During resolution, we filter the candidates for 
each encountered pronoun in the same way as dur-
ing training. That is, we only consider the NPs in 
the current and the preceding 2 sentences. Such a 
context window is reasonable as the distance be-
tween a pronominal anaphor and its antecedent is 
generally short. In the MUC-6 data set, for exam-
ple, the immediate antecedents of 95% pronominal 
anaphors can be found within the above distance. 
Comparatively, candidate filtering for non-
pronouns during resolution is complicated. A po-
tential problem is that for each non-pronoun under 
consideration, the twin-candidate model always 
chooses a candidate as the antecedent, even though 
all of the candidates are ?low-qualified?, that is, 
unlikely to be coreferential to the non-pronoun un-
der consideration.  
In fact, the twin-candidate model in itself can 
identify the qualification of a candidate. We can 
compare every candidate with a virtual ?standard 
candidate?, C0. Only those better than C0 are 
deemed qualified and allowed to enter the ?round 
robin?, whereas the losers are eliminated. As we 
have discussed in Section 3.5, the classifier on the 
pairs of a candidate and C0 is just a single-
candidate classifier. Thus, we can safely adopt the 
single-candidate classifier as our candidate filter.  
The candidate filtering algorithm during resolu-
tion is as follows:  
Features describing the candidate: 
1. 
2. 
3. 
4. 
5. 
6. 
7. 
8. 
9. 
10 
ante_DefNp_1(2) 
ante_IndefNP_1(2) 
ante_Pron_1(2) 
ante_ProperNP_1(2) 
ante_M_ProperNP_1(2) 
ante_ProperNP_APPOS_1(2) 
ante_Appositive_1(2) 
ante_NearestNP_1(2) 
ante_Embeded_1(2) 
ante_Title_1(2) 
1 if Ci (Cj) is a definite NP; else 0 
1 if Ci (Cj) is an indefinite NP; else 0 
1 if Ci (Cj) is a pronoun; else 0 
1 if Ci (Cj) is a proper NP; else 0  
1 if Ci (Cj) is a mentioned proper NP; else 0 
1 if Ci (Cj) is a proper NP modified by an appositive; else 0 
1 if Ci (Cj) is in a apposition structure; else 0 
1 if Ci (Cj) is the nearest candidate to the anaphor; else 0 
1 if Ci (Cj) is in an embedded NP; else 0 
1 if Ci (Cj) is in a title; else 0 
Features describing the anaphor: 
11. 
12. 
13. 
14. 
15. 
 
16. 
ana_DefNP 
ana_IndefNP 
ana_Pron 
ana_ProperNP 
ana_PronType 
 
ana_FlexiblePron 
1 if ana is a definite NP; else 0 
1 if ana is an indefinite NP; else 0 
1 if ana is a pronoun; else 0 
1 if ana is a proper NP; else 0 
1 if ana is a third person pronoun; 2 if a single neuter pro-
noun; 3 if a plural neuter pronoun; 4 if other types 
1 if ana is a flexible pronoun; else 0 
Features describing the candidate and the anaphor: 
17. 
18. 
 
18. 
 
20. 
21. 
ante_ana_StringMatch_1(2) 
ante_ana_GenderAgree_1(2) 
 
ante_ana_NumAgree_1(2) 
 
ante_ana_Appositive_1(2) 
ante_ana_Alias_1(2) 
1 if Ci (Cj) and ana match in string; else 0 
1 if Ci (Cj) and ana agree in gender; else 0 if disagree; -1 if 
unknown 
1 if Ci (Cj) and ana agree in number; 0 if disagree; -1 if un-
known 
1 if Ci (Cj) and ana are in an appositive structure; else 0 
1 if Ci (Cj) and ana are in an alias of the other; else 0 
Features describing the two candidates 
22. 
23. 
inter_SDistance 
inter_Pdistance 
Distance between Ci and Cj in sentences 
Distance between Ci and Cj in paragraphs 
Table 1:  Feature set for coreference resolution (Feature 22, 23 and features involving Cj are not 
used in the single-candidate model) 
1. If the current NP is a pronoun, construct the 
candidate set in the same way as during training.  
2. If the current NP is a non-pronoun,  
(a) Add all the preceding non-pronouns to the ini-
tial candidate set. 
(b) Calculate the confidence value for each candi-
date using the single-candidate classifier. 
(c) Remove the candidates with confidence value 
less than 0.5. 
5 Evaluation and Discussion 
Our coreference resolution approach is evaluated 
on the standard MUC-6 (1995) and MUC-7 (1998) 
data set. For MUC-6, 30 ?dry-run? documents an-
notated with coreference information could be used 
as training data. There are also 30 annotated train-
ing documents from MUC-7. For testing, we util-
ize the 30 standard test documents from MUC-6 
and the 20 standard test documents from MUC-7. 
5.1 Baseline Systems 
In the experiment we compared our approach with 
the following research works:  
1. Strube?s S-list algorithm for pronoun resolu-
tion (Stube, 1998).  
2. Ng and Cardie?s machine learning approach to 
coreference resolution (Ng and Cardie, 2002a).  
3. Connolly et al?s machine learning approach to 
anaphora resolution (Connolly et al, 1997).  
Among them, S-List, a version of centering 
algorithm, uses well-defined heuristic rules to rank 
the antecedent candidates; Ng and Cardie?s ap-
proach employs the standard single-candidate 
model and ?Best-First? rule to select the antece-
dent; Connolly et al?s approach also adopts the 
twin-candidate model, but their approach lacks of 
candidate filtering strategy and uses greedy linear 
search to select the antecedent (See ?Related 
work? for details). 
We constructed three baseline systems based on 
the above three approaches, respectively. For com-
parison, in the baseline system 2 and 3, we used 
the similar feature set as in our system (see table 1).  
5.2 Results and Discussion 
Table 2 and 3 show the performance of different 
approaches in the pronoun and non-pronoun reso-
lution, respectively. In these tables we focus on the 
abilities of different approaches in resolving an 
anaphor to its antecedent correctly. The recall 
measures the number of correctly resolved ana-
phors over the total anaphors in the MUC test data 
set, and the precision measures the number of cor-
rect anaphors over the total resolved anaphors. The 
F-measure F=2*RP/(R+P) is the harmonic mean of 
precision and recall. 
The experimental result demonstrates that our 
competition learning approach achieves a better 
performance than the baseline approaches in re-
solving pronominal anaphors. As shown in Table 2, 
our approach outperforms Ng and Cardie?s single-
candidate based approach by 3.7 and 5.4 in F-
measure for MUC-6 and MUC-7, respectively. 
Besides, compared with Strube?s S-list algorithm, 
our approach also achieves gains in the F-measure 
by 3.2 (MUC-6), and 1.6 (MUC-7). In particular, 
our approach obtains significant improvement 
(21.1 for MUC-6, and 13.1 for MUC-7) over Con-
nolly et al?s twin-candidate based approach. 
 
MUC-6 MUC-7  
 R P F R P F 
Strube (1998)  76.1 74.3 75.1 62.9 60.3 61.6 
Ng and Cardie (2002a) 75.4 73.8 74.6 58.9 56.8 57.8 
Connolly et al (1997) 57.2 57.2 57.2 50.1 50.1 50.1 
Our approach 79.3 77.5 78.3 64.4 62.1 63.2 
Table 2:  Results for the pronoun resolution  
 
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 51.0 89.9 65.0 39.1 86.4 53.8 
Connolly et al (1997) 52.2 52.2 52.2 43.7 43.7 43.7 
Our approach  51.3 90.4 65.4 39.7 87.6 54.6 
Table 3:  Results for the non-pronoun resolution  
MUC-6 MUC-7  
R P F R P F 
Ng and Cardie (2002a) 62.2 78.8 69.4 48.4 74.6 58.7 
Our approach 64.0 80.5 71.3 50.1 75.4 60.2 
Table 4: Results for the coreference resolution  
 
Compared with the gains in pronoun resolution, 
the improvement in non-pronoun resolution is 
slight. As shown in Table 3, our approach resolves 
non-pronominal anaphors with the recall of 51.3 
(39.7) and the precision of 90.4 (87.6) for MUC-6 
(MUC-7). In contrast to Ng and Cardie?s approach, 
the performance of our approach improves only 0.3 
(0.6) in recall and 0.5 (1.2) in precision. The rea-
son may be that in non-pronoun resolution, the 
coreference of an anaphor and its candidate is usu-
ally determined only by some strongly indicative 
features such as alias, apposition, string-matching, 
etc (this explains why we obtain a high precision 
but a low recall in non-pronoun resolution). There-
fore, most of the positive candidates are coreferen-
tial to the anaphors even though they are not the 
?best?. As a result, we can only see comparatively 
slight difference between the performances of the 
two approaches.  
Although Connolly et al?s approach also adopts 
the twin-candidate model, it achieves a poor per-
formance for both pronoun resolution and non-
pronoun resolution. The main reason is the absence 
of candidate filtering strategy in their approach 
(this is why the recall equals to the precision in the 
tables). Without candidate filtering, the recall may 
rise as the correct antecedents would not be elimi-
nated wrongly. Nevertheless, the precision drops 
largely due to the numerous invalid NPs in the 
candidate set. As a result, a significantly low F-
measure is obtained in their approach. 
Table 4 summarizes the overall performance of 
different approaches to coreference resolution. Dif-
ferent from Table 2 and 3, here we focus on 
whether a coreferential chain could be correctly 
identified. For this purpose, we obtain the recall, 
the precision and the F-measure using the standard 
MUC scoring program (Vilain et al 1995) for the 
coreference resolution task. Here the recall means 
the correct resolved chains over the whole 
coreferential chains in the data set, and precision 
means the correct resolved chains over the whole 
resolved chains.  
In line with the previous experiments, we see 
reasonable improvement in the performance of the 
coreference resolution: compared with the baseline 
approach based on the single-candidate model, the 
F-measure of approach increases from 69.4 to 71.3 
for MUC-6, and from 58.7 to 60.2 for MUC-7.  
6 Related Work 
A similar twin-candidate model was adopted in the 
anaphoric resolution system by Connolly et al 
(1997). The differences between our approach and 
theirs are: 
(1) In Connolly et al?s approach, all the preceding 
NPs of an anaphor are taken as the antecedent 
candidates, whereas in our approach we use 
candidate filters to eliminate invalid or irrele-
vant candidates.  
(2) The antecedent identification in Connolly et 
al.?s approach is to apply the classifier to 
successive pairs of candidates, each time 
retaining the better candidate. However, due to 
the lack of strong assumption of transitivity, 
the selection procedure is in fact a greedy 
search. By contrast, our approach evaluates a 
candidate according to the times it wins over 
the other competitors. Comparatively this 
algorithm could lead to a better solution. 
(3) Our approach makes use of more indicative 
features, such as Appositive, Name Alias, 
String-matching, etc. These features are effec-
tive especially for non-pronoun resolution. 
7 Conclusion 
In this paper we have proposed a competition 
learning approach to coreference resolution. We 
started with the introduction of the single-
candidate model adopted by most supervised ma-
chine learning approaches. We argued that the con-
fidence values returned by the single-candidate 
classifier are not reliable to be used as ranking cri-
terion for antecedent candidates. Alternatively, we 
presented a twin-candidate model that learns the 
competition criterion for antecedent candidates 
directly. We introduced how to adopt the twin-
candidate model in our competition learning ap-
proach to resolve the coreference problem. Particu-
larly, we proposed a candidate filtering algorithm 
that can effectively reduce the computational cost 
and data noises.  
The experimental results have proved the effec-
tiveness of our approach. Compared with the base-
line approach using the single-candidate model, the 
F-measure increases by 1.9 and 1.5 for MUC-6 and 
MUC-7 data set, respectively. The gains in the 
pronoun resolution contribute most to the overall 
improvement of coreference resolution. 
Currently, we employ the single-candidate clas-
sifier to filter the candidate set during resolution. 
While the filter guarantees the qualification of the 
candidates, it removes too many positive candi-
dates, and thus the recall suffers. In our future 
work, we intend to adopt a looser filter together 
with an anaphoricity determination module (Bean 
and Riloff, 1999; Ng and Cardie, 2002b). Only if 
an encountered NP is determined as an anaphor, 
we will select an antecedent from the candidate set 
generated by the looser filter. Furthermore, we 
would like to incorporate more syntactic features 
into our feature set, such as grammatical role or 
syntactic parallelism. These features may be help-
ful to improve the performance of pronoun resolu-
tion.  
References 
Chinatsu Aone and Scott W.Bennett. 1995. Evaluating 
automated and manual acquisition of anaphora reso-
lution strategies. In Proceedings of the 33rd Annual 
Meeting of the Association for Computational Lin-
guistics, Pages 122-129. 
D.Bean and E.Riloff. 1999. Corpus-Based identification 
of non-anaphoric noun phrases. In Proceedings of the 
37th Annual Meeting of the Association for Computa-
tional Linguistics, Pages 373-380. 
Brennan, S, E., M. W. Friedman and C. J. Pollard. 1987. 
A Centering approach to pronouns. In Proceedings of 
the 25th Annual Meeting of The Association for Com-
putational Linguistics, Page 155-162. 
Dennis Connolly, John D. Burger and David S. Day. 
1997. A machine learning approach to anaphoric ref-
erence. New Methods in Language Processing, Page 
133-144.  
Joseph F. McCarthy. 1996. A trainable approach to 
coreference resolution for Information Extraction. 
Ph.D. thesis. University of Massachusetts. 
Ruslan Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. In Proceedings of the 17th Int. 
Conference on Computational Linguistics (COLING-
ACL'98), Page 869-875. 
Ruslan Mitkov. 1999. Anaphora resolution: The state of 
the art. Technical report. University of Wolverhamp-
ton, Wolverhampton. 
MUC-6. 1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). Morgan Kauf-
mann, San Francisco, CA. 
MUC-7. 1998. Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan Kauf-
mann, San Francisco, CA. 
Vincent Ng and Claire Cardie. 2002a. Improving ma-
chine learning approaches to coreference resolution. 
In Proceedings of the 40rd Annual Meeting of the As-
sociation for Computational Linguistics, Pages 104-
111. 
Vincent Ng and Claire Cardie. 2002b. Identifying ana-
phoric and non-anaphoric noun phrases to improve 
coreference resolution. In Proceedings of 19th Inter-
national Conference on Computational Linguistics 
(COLING-2002). 
J R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Mateo, CA. 
Wee Meng Soon, Hwee Tou Ng and Daniel Chung 
Yong Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4), Page 521-544. 
Michael Strube. Never look back: An alternative to 
Centering. 1998. In Proceedings of the 17th Int. Con-
ference on Computational Linguistics and 36th An-
nual Meeting of ACL, Page 1251-1257 
Joel R. Tetreault. 2001. A Corpus-Based evaluation of 
Centering and pronoun resolution. Computational 
Linguistics, 27(4), Page 507-520. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and 
L.Hirschman. 1995. A model-theoretic coreference 
scoring scheme. In Proceedings of the Sixth Message 
understanding Conference (MUC-6), Pages 42-52. 
GD Zhou and J. Su, 2000. Error-driven HMM-based 
chunk tagger with context-dependent lexicon. In 
Proceedings of the Joint Conference on Empirical 
Methods on Natural Language Processing and Very 
Large Corpus (EMNLP/ VLC'2000).  
GD Zhou and J. Su. 2002. Named Entity recognition 
using a HMM-based chunk tagger. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics, P473-478. 
Improving Pronoun Resolution by Incorporating Coreferential
Information of Candidates
Xiaofeng Yang?? Jian Su? Guodong Zhou? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian,zhougd}
@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
Coreferential information of a candidate, such
as the properties of its antecedents, is important
for pronoun resolution because it reflects the
salience of the candidate in the local discourse.
Such information, however, is usually ignored in
previous learning-based systems. In this paper
we present a trainable model which incorporates
coreferential information of candidates into pro-
noun resolution. Preliminary experiments show
that our model will boost the resolution perfor-
mance given the right antecedents of the can-
didates. We further discuss how to apply our
model in real resolution where the antecedents
of the candidate are found by a separate noun
phrase resolution module. The experimental re-
sults show that our model still achieves better
performance than the baseline.
1 Introduction
In recent years, supervised machine learning ap-
proaches have been widely explored in refer-
ence resolution and achieved considerable suc-
cess (Ge et al, 1998; Soon et al, 2001; Ng and
Cardie, 2002; Strube and Muller, 2003; Yang et
al., 2003). Most learning-based pronoun res-
olution systems determine the reference rela-
tionship between an anaphor and its antecedent
candidate only from the properties of the pair.
The knowledge about the context of anaphor
and antecedent is nevertheless ignored. How-
ever, research in centering theory (Sidner, 1981;
Grosz et al, 1983; Grosz et al, 1995; Tetreault,
2001) has revealed that the local focusing (or
centering) also has a great effect on the pro-
cessing of pronominal expressions. The choices
of the antecedents of pronouns usually depend
on the center of attention throughout the local
discourse segment (Mitkov, 1999).
To determine the salience of a candidate
in the local context, we may need to check
the coreferential information of the candidate,
such as the existence and properties of its an-
tecedents. In fact, such information has been
used for pronoun resolution in many heuristic-
based systems. The S-List model (Strube,
1998), for example, assumes that a co-referring
candidate is a hearer-old discourse entity and
is preferred to other hearer-new candidates.
In the algorithms based on the centering the-
ory (Brennan et al, 1987; Grosz et al, 1995), if
a candidate and its antecedent are the backward-
looking centers of two subsequent utterances re-
spectively, the candidate would be the most pre-
ferred since the CONTINUE transition is al-
ways ranked higher than SHIFT or RETAIN.
In this paper, we present a supervised
learning-based pronoun resolution system which
incorporates coreferential information of candi-
dates in a trainable model. For each candi-
date, we take into consideration the properties
of its antecedents in terms of features (hence-
forth backward features), and use the supervised
learning method to explore their influences on
pronoun resolution. In the study, we start our
exploration on the capability of the model by
applying it in an ideal environment where the
antecedents of the candidates are correctly iden-
tified and the backward features are optimally
set. The experiments on MUC-6 (1995) and
MUC-7 (1998) corpora show that incorporating
coreferential information of candidates boosts
the system performance significantly. Further,
we apply our model in the real resolution where
the antecedents of the candidates are provided
by separate noun phrase resolution modules.
The experimental results show that our model
still outperforms the baseline, even with the low
recall of the non-pronoun resolution module.
The remaining of this paper is organized as
follows. Section 2 discusses the importance of
the coreferential information for candidate eval-
uation. Section 3 introduces the baseline learn-
ing framework. Section 4 presents and evaluates
the learning model which uses backward fea-
tures to capture coreferential information, while
Section 5 proposes how to apply the model in
real resolution. Section 6 describes related re-
search work. Finally, conclusion is given in Sec-
tion 7.
2 The Impact of Coreferential
Information on Pronoun
Resolution
In pronoun resolution, the center of attention
throughout the discourse segment is a very im-
portant factor for antecedent selection (Mitkov,
1999). If a candidate is the focus (or center)
of the local discourse, it would be selected as
the antecedent with a high possibility. See the
following example,
<s> Gitano1 has pulled off a clever illusion2
with its3 advertising4. <s>
<s> The campaign5 gives its6 clothes a
youthful and trendy image to lure consumers
into the store. <s>
Table 1: A text segment from MUC-6 data set
In the above text, the pronoun ?its6? has
several antecedent candidates, i.e., ?Gitano1?,
?a clever illusion2?, ?its3?, ?its advertising4?
and ?The campaign5?. Without looking back,
?The campaign5? would be probably selected
because of its syntactic role (Subject) and its
distance to the anaphor. However, given the
knowledge that the company Gitano is the fo-
cus of the local context and ?its3? refers to
?Gitano1?, it would be clear that the pronoun
?its6? should be resolved to ?its3? and thus
?Gitano1?, rather than other competitors.
To determine whether a candidate is the ?fo-
cus? entity, we should check how the status (e.g.
grammatical functions) of the entity alternates
in the local context. Therefore, it is necessary
to track the NPs in the coreferential chain of
the candidate. For example, the syntactic roles
(i.e., subject) of the antecedents of ?its3? would
indicate that ?its3? refers to the most salient
entity in the discourse segment.
In our study, we keep the properties of the an-
tecedents as features of the candidates, and use
the supervised learning method to explore their
influence on pronoun resolution. Actually, to
determine the local focus, we only need to check
the entities in a short discourse segment. That
is, for a candidate, the number of its adjacent
antecedents to be checked is limited. Therefore,
we could evaluate the salience of a candidate
by looking back only its closest antecedent in-
stead of each element in its coreferential chain,
with the assumption that the closest antecedent
is able to provide sufficient information for the
evaluation.
3 The Baseline Learning Framework
Our baseline system adopts the common
learning-based framework employed in the sys-
tem by Soon et al (2001).
In the learning framework, each training or
testing instance takes the form of i{ana, candi},
where ana is the possible anaphor and candi is
its antecedent candidate1. An instance is associ-
ated with a feature vector to describe their rela-
tionships. As listed in Table 2, we only consider
those knowledge-poor and domain-independent
features which, although superficial, have been
proved efficient for pronoun resolution in many
previous systems.
During training, for each anaphor in a given
text, a positive instance is created by paring
the anaphor and its closest antecedent. Also a
set of negative instances is formed by paring the
anaphor and each of the intervening candidates.
Based on the training instances, a binary classi-
fier is generated using C5.0 learning algorithm
(Quinlan, 1993). During resolution, each possi-
ble anaphor ana, is paired in turn with each pre-
ceding antecedent candidate, candi, from right
to left to form a testing instance. This instance
is presented to the classifier, which will then
return a positive or negative result indicating
whether or not they are co-referent. The pro-
cess terminates once an instance i{ana, candi}
is labelled as positive, and ana will be resolved
to candi in that case.
4 The Learning Model Incorporating
Coreferential Information
The learning procedure in our model is similar
to the above baseline method, except that for
each candidate, we take into consideration its
closest antecedent, if possible.
4.1 Instance Structure
During both training and testing, we adopt the
same instance selection strategy as in the base-
line model. The only difference, however, is the
structure of the training or testing instances.
Specifically, each instance in our model is com-
posed of three elements like below:
1In our study candidates are filtered by checking the
gender, number and animacy agreements in advance.
Features describing the candidate (candi)
1. candi DefNp 1 if candi is a definite NP; else 0
2. candi DemoNP 1 if candi is an indefinite NP; else 0
3. candi Pron 1 if candi is a pronoun; else 0
4. candi ProperNP 1 if candi is a proper name; else 0
5. candi NE Type 1 if candi is an ?organization? named-entity; 2 if ?person?, 3 if
other types, 0 if not a NE
6. candi Human the likelihood (0-100) that candi is a human entity (obtained
from WordNet)
7. candi FirstNPInSent 1 if candi is the first NP in the sentence where it occurs
8. candi Nearest 1 if candi is the candidate nearest to the anaphor; else 0
9. candi SubjNP 1 if candi is the subject of the sentence it occurs; else 0
Features describing the anaphor (ana):
10. ana Reflexive 1 if ana is a reflexive pronoun; else 0
11. ana Type 1 if ana is a third-person pronoun (he, she,. . . ); 2 if a single
neuter pronoun (it,. . . ); 3 if a plural neuter pronoun (they,. . . );
4 if other types
Features describing the relationships between candi and ana:
12. SentDist Distance between candi and ana in sentences
13. ParaDist Distance between candi and ana in paragraphs
14. CollPattern 1 if candi has an identical collocation pattern with ana; else 0
Table 2: Feature set for the baseline pronoun resolution system
i{ana, candi, ante-of-candi}
where ana and candi, similar to the defini-
tion in the baseline model, are the anaphor and
one of its candidates, respectively. The new
added element in the instance definition, ante-
of-candi, is the possible closest antecedent of
candi in its coreferential chain. The ante-of-
candi is set to NIL in the case when candi has
no antecedent.
Consider the example in Table 1 again. For
the pronoun ?it6?, three training instances will
be generated, namely, i{its6, The compaign5,
NIL}, i{its6, its advertising4, NIL}, and
i{its6, its3, Gitano1}.
4.2 Backward Features
In addition to the features adopted in the base-
line system, we introduce a set of backward fea-
tures to describe the element ante-of-candi. The
ten features (15-24) are listed in Table 3 with
their respective possible values.
Like feature 1-9, features 15-22 describe the
lexical, grammatical and semantic properties of
ante-of-candi. The inclusion of the two features
Apposition (23) and candi NoAntecedent (24) is
inspired by the work of Strube (1998). The
feature Apposition marks whether or not candi
and ante-of-candi occur in the same appositive
structure. The underlying purpose of this fea-
ture is to capture the pattern that proper names
are accompanied by an appositive. The entity
with such a pattern may often be related to the
hearers? knowledge and has low preference. The
feature candi NoAntecedent marks whether or
not a candidate has a valid antecedent in the
preceding text. As stipulated in Strube?s work,
co-referring expressions belong to hearer-old en-
tities and therefore have higher preference than
other candidates. When the feature is assigned
value 1, all the other backward features (15-23)
are set to 0.
4.3 Results and Discussions
In our study we used the standard MUC-
6 and MUC-7 coreference corpora. In each
data set, 30 ?dry-run? documents were anno-
tated for training as well as 20-30 documents
for testing. The raw documents were prepro-
cessed by a pipeline of automatic NLP com-
ponents (e.g. NP chunker, part-of-speech tag-
ger, named-entity recognizer) to determine the
boundary of the NPs, and to provide necessary
information for feature calculation.
In an attempt to investigate the capability of
our model, we evaluated the model in an opti-
mal environment where the closest antecedent
of each candidate is correctly identified. MUC-
6 and MUC-7 can serve this purpose quite well;
the annotated coreference information in the
data sets enables us to obtain the correct closest
Features describing the antecedent of the candidate (ante-of-candi):
15. ante-candi DefNp 1 if ante-of-candi is a definite NP; else 0
16. ante-candi IndefNp 1 if ante-of-candi is an indefinite NP; else 0
17. ante-candi Pron 1 if ante-of-candi is a pronoun; else 0
18. ante-candi Proper 1 if ante-of-candi is a proper name; else 0
19. ante-candi NE Type 1 if ante-of-candi is an ?organization? named-entity; 2 if ?per-
son?, 3 if other types, 0 if not a NE
20. ante-candi Human the likelihood (0-100) that ante-of-candi is a human entity
21. ante-candi FirstNPInSent 1 if ante-of-candi is the first NP in the sentence where it occurs
22. ante-candi SubjNP 1 if ante-of-candi is the subject of the sentence where it occurs
Features describing the relationships between the candidate (candi) and ante-of-candi :
23. Apposition 1 if ante-of-candi and candi are in an appositive structure
Features describing the candidate (candi):
24. candi NoAntecedent 1 if candi has no antecedent available; else 0
Table 3: Backward features used to capture the coreferential information of a candidate
antecedent for each candidate and accordingly
generate the training and testing instances. In
the next section we will further discuss how to
apply our model into the real resolution.
Table 4 shows the performance of different
systems for resolving the pronominal anaphors 2
in MUC-6 and MUC-7. Default learning param-
eters for C5.0 were used throughout the exper-
iments. In this table we evaluated the perfor-
mance based on two kinds of measurements:
? ?Recall-and-Precision?:
Recall = #positive instances classified correctly#positive instances
Precision = #positive instances classified correctly#instances classified as positive
The above metrics evaluate the capability
of the learned classifier in identifying posi-
tive instances3. F-measure is the harmonic
mean of the two measurements.
? ?Success?:
Success = #anaphors resolved correctly#total anaphors
The metric4 directly reflects the pronoun
resolution capability.
The first and second lines of Table 4 compare
the performance of the baseline system (Base-
2The first and second person pronouns are discarded
in our study.
3The testing instances are collected in the same ways
as the training instances.
4In the experiments, an anaphor is considered cor-
rectly resolved only if the found antecedent is in the same
coreferential chain of the anaphor.
ante-candi_SubjNP = 1: 1 (49/5)
ante-candi_SubjNP = 0:
:..candi_SubjNP = 1:
:..SentDist = 2: 0 (3)
: SentDist = 0:
: :..candi_Human > 0: 1 (39/2)
: : candi_Human <= 0:
: : :..candi_NoAntecedent = 0: 1 (8/3)
: : candi_NoAntecedent = 1: 0 (3)
: SentDist = 1:
: :..ante-candi_Human <= 50 : 0 (4)
: ante-candi_Human > 50 : 1 (10/2)
:
candi_SubjNP = 0:
:..candi_Pron = 1: 1 (32/7)
candi_Pron = 0:
:..candi_NoAntecedent = 1:
:..candi_FirstNPInSent = 1: 1 (6/2)
: candi_FirstNPInSent = 0: ...
candi_NoAntecedent = 0: ...
Figure 1: Top portion of the decision tree
learned on MUC-6 with the backward features
line) and our system (Optimal), where DTpron
and DTpron?opt are the classifiers learned in
the two systems, respectively. The results in-
dicate that our system outperforms the base-
line system significantly. Compared with Base-
line, Optimal achieves gains in both recall (6.4%
for MUC-6 and 4.1% for MUC-7) and precision
(1.3% for MUC-6 and 9.0% for MUC-7). For
Success, we also observe an apparent improve-
ment by 4.7% (MUC-6) and 3.5% (MUC-7).
Figure 1 shows the portion of the pruned deci-
sion tree learned for MUC-6 data set. It visual-
izes the importance of the backward features for
the pronoun resolution on the data set. From
Testing Backward feature MUC-6 MUC-7
Experiments classifier assigner* R P F S R P F S
Baseline DTpron NIL 77.2 83.4 80.2 70.0 71.9 68.6 70.2 59.0
Optimal DTpron?opt (Annotated) 83.6 84.7 84.1 74.7 76.0 77.6 76.8 62.5
RealResolve-1 DTpron?opt DTpron?opt 75.8 83.8 79.5 73.1 62.3 77.7 69.1 53.8
RealResolve-2 DTpron?opt DTpron 75.8 83.8 79.5 73.1 63.0 77.9 69.7 54.9
RealResolve-3 DT?pron DTpron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
RealResolve-4 DT?pron DT
?
pron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8
Table 4: Results of different systems for pronoun resolution on MUC-6 and MUC-7
(*Here we only list backward feature assigner for pronominal candidates. In RealResolve-1 to
RealResolve-4, the backward features for non-pronominal candidates are all found by DTnon?pron.)
the tree we could find that:
1.) Feature ante-candi SubjNP is of the most
importance as the root feature of the tree.
The decision tree would first examine the
syntactic role of a candidate?s antecedent,
followed by that of the candidate. This
nicely proves our assumption that the prop-
erties of the antecedents of the candidates
provide very important information for the
candidate evaluation.
2.) Both features ante-candi SubjNP and
candi SubjNP rank top in the decision tree.
That is, for the reference determination,
the subject roles of the candidate?s referent
within a discourse segment will be checked
in the first place. This finding supports well
the suggestion in centering theory that the
grammatical relations should be used as the
key criteria to rank forward-looking centers
in the process of focus tracking (Brennan
et al, 1987; Grosz et al, 1995).
3.) candi Pron and candi NoAntecedent are
to be examined in the cases when the
subject-role checking fails, which confirms
the hypothesis in the S-List model by
Strube (1998) that co-refereing candidates
would have higher preference than other
candidates in the pronoun resolution.
5 Applying the Model in Real
Resolution
In Section 4 we explored the effectiveness of
the backward feature for pronoun resolution. In
those experiments our model was tested in an
ideal environment where the closest antecedent
of a candidate can be identified correctly when
generating the feature vector. However, during
real resolution such coreferential information is
not available, and thus a separate module has
algorithm PRON-RESOLVE
input:
DTnon?pron: classifier for resolving non-pronouns
DTpron: classifier for resolving pronouns
begin:
M1..n:= the valid markables in the given docu-
ment
Ante[1..n] := 0
for i = 1 to N
for j = i - 1 downto 0
if (Mi is a non-pron and
DTnon?pron(i{Mi,Mj}) == + )
or
(Mi is a pron and
DTpron(i{Mi,Mj , Ante[j]}) == +)
then
Ante[i] := Mj
break
return Ante
Figure 2: The pronoun resolution algorithm by
incorporating coreferential information of can-
didates
to be employed to obtain the closest antecedent
for a candidate. We describe the algorithm in
Figure 2.
The algorithm takes as input two classifiers,
one for the non-pronoun resolution and the
other for pronoun resolution. Given a testing
document, the antecedent of each NP is identi-
fied using one of these two classifiers, depending
on the type of NP. Although a separate non-
pronoun resolution module is required for the
pronoun resolution task, this is usually not a
big problem as these two modules are often in-
tegrated in coreference resolution systems. We
just use the results of the one module to improve
the performance of the other.
5.1 New Training and Testing
Procedures
For a pronominal candidate, its antecedent can
be obtained by simply using DTpron?opt. For
Training Procedure:
T1. Train a non-pronoun resolution clas-
sifier DTnon?pron and a pronoun resolution
classifier DTpron, using the baseline learning
framework (without backward features).
T2. Apply DTnon?pron and DTpron to iden-
tify the antecedent of each non-pronominal
and pronominal markable, respectively, in a
given document.
T3. Go through the document again. Gen-
erate instances with backward features as-
signed using the antecedent information ob-
tained in T2.
T4. Train a new pronoun resolution classifier
DT?pron on the instances generated in T3.
Testing Procedure:
R1. For each given document, do T2?T3.
R2. Resolve pronouns by applying DT?pron.
Table 5: New training and testing procedures
a non-pronominal candidate, we built a non-
pronoun resolution module to identify its an-
tecedent. The module is a duplicate of the
NP coreference resolution system by Soon et
al. (2001)5 , which uses the similar learn-
ing framework as described in Section 3. In
this way, we could do pronoun resolution
just by running PRON-RESOLVE(DTnon?pron,
DTpron?opt), where DTnon?pron is the classifier
of the non-pronoun resolution module.
One problem, however, is that DTpron?opt is
trained on the instances whose backward fea-
tures are correctly assigned. During real resolu-
tion, the antecedent of a candidate is found by
DTnon?pron or DTpron?opt, and the backward
feature values are not always correct. Indeed,
for most noun phrase resolution systems, the
recall is not very high. The antecedent some-
times can not be found, or is not the closest
one in the preceding coreferential chain. Con-
sequently, the classifier trained on the ?perfect?
feature vectors would probably fail to output
anticipated results on the noisy data during real
resolution.
Thus we modify the training and testing pro-
cedures of the system. For both training and
testing instances, we assign the backward fea-
ture values based on the results from separate
NP resolution modules. The detailed proce-
dures are described in Table 5.
5Details of the features can be found in Soon et al
(2001)
algorithm REFINE-CLASSIFIER
begin:
DT1pron := DT
?
pron
for i = 1 to ?
Use DTipron to update the antecedents of
pronominal candidates and the correspond-
ing backward features;
Train DTi+1pron based on the updated training
instances;
if DTi+1pron is not better than DTipron then
break;
return DTipron
Figure 3: The classifier refining algorithm
The idea behind our approach is to train
and test the pronoun resolution classifier on
instances with feature values set in a consis-
tent way. Here the purpose of DTpron and
DTnon?pron is to provide backward feature val-
ues for training and testing instances. From this
point of view, the two modules could be thought
of as a preprocessing component of our pronoun
resolution system.
5.2 Classifier Refining
If the classifier DT?pron outperforms DTpron
as expected, we can employ DT?pron in place
of DTpron to generate backward features for
pronominal candidates, and then train a clas-
sifier DT??pron based on the updated training in-
stances. Since DT?pron produces more correct
feature values than DTpron, we could expect
that DT??pron will not be worse, if not better,
than DT?pron. Such a process could be repeated
to refine the pronoun resolution classifier. The
algorithm is described in Figure 3.
In algorithm REFINE-CLASSIFIER, the it-
eration terminates when the new trained clas-
sifier DTi+1pron provides no further improvement
than DTipron. In this case, we can replace
DTi+1pron by DTipron during the i+1(th) testing
procedure. That means, by simply running
PRON-RESOLVE(DTnon?pron,DTipron), we can
use for both backward feature computation and
instance classification tasks, rather than apply-
ing DTpron and DT?pron subsequently.
5.3 Results and Discussions
In the experiments we evaluated the perfor-
mance of our model in real pronoun resolution.
The performance of our model depends on the
performance of the non-pronoun resolution clas-
sifier, DTnon?pron. Hence we first examined the
coreference resolution capability of DTnon?pron
based on the standard scoring scheme by Vi-
lain et al (1995). For MUC-6, the module ob-
tains 62.2% recall and 78.8% precision, while for
MUC-7, it obtains 50.1% recall and 75.4% pre-
cision. The poor recall and comparatively high
precision reflect the capability of the state-of-
the-art learning-based NP resolution systems.
The third block of Table 4 summarizes the
performance of the classifier DTpron?opt in real
resolution. In the systems RealResolve-1 and
RealResolve-2, the antecedents of pronominal
candidates are found by DTpron?opt and DTpron
respectively, while in both systems the an-
tecedents of non-pronominal candidates are by
DTnon?pron. As shown in the table, compared
with the Optimal where the backward features
of testing instances are optimally assigned, the
recall rates of two systems drop largely by 7.8%
for MUC-6 and by about 14% for MUC-7. The
scores of recall are even lower than those of
Baseline. As a result, in comparison with Op-
timal, we see the degrade of the F-measure and
the success rate, which confirms our hypothesis
that the classifier learned on perfect training in-
stances would probably not perform well on the
noisy testing instances.
The system RealResolve-3 listed in the fifth
line of the table uses the classifier trained
and tested on instances whose backward fea-
tures are assigned according to the results from
DTnon?pron and DTpron. From the table we can
find that: (1) Compared with Baseline, the sys-
tem produces gains in recall (2.1% for MUC-6
and 2.8% for MUC-7) with no significant loss
in precision. Overall, we observe the increase in
F-measure for both data sets. If measured by
Success, the improvement is more apparent by
4.7% (MUC-6) and 1.8% (MUC-7). (2) Com-
pared with RealResolve-1(2), the performance
decrease of RealResolve-3 against Optimal is
not so large. Especially for MUC-6, the system
obtains a success rate as high as Optimal.
The above results show that our model can
be successfully applied in the real pronoun res-
olution task, even given the low recall of the
current non-pronoun resolution module. This
should be owed to the fact that for a candidate,
its adjacent antecedents, even not the closest
one, could give clues to reflect its salience in
the local discourse. That is, the model prefers a
high precision to a high recall, which copes well
with the capability of the existing non-pronoun
resolution module.
In our experiments we also tested the clas-
sifier refining algorithm described in Figure 3.
We found that for both MUC-6 and MUC-7
data set, the algorithm terminated in the second
round. The comparison of DT2pron and DT1pron
(i.e. DT?pron) showed that these two trees were
exactly the same. The algorithm converges fast
probably because in the data set, most of the
antecedent candidates are non-pronouns (89.1%
for MUC-6 and 83.7% for MUC-7). Conse-
quently, the ratio of the training instances with
backward features changed may be not substan-
tial enough to affect the classifier generation.
Although the algorithm provided no further
refinement for DT?pron, we can use DT
?
pron, as
suggested in Section 5.2, to calculate back-
ward features and classify instances by running
PRON-RESOLVE(DTnon?pron, DT?pron). The
results of such a system, RealResolve-4, are
listed in the last line of Table 4. For both MUC-
6 and MUC-7, RealResolve-4 obtains exactly
the same performance as RealResolve-3.
6 Related Work
To our knowledge, our work is the first ef-
fort that systematically explores the influence of
coreferential information of candidates on pro-
noun resolution in learning-based ways. Iida et
al. (2003) also take into consideration the con-
textual clues in their coreference resolution sys-
tem, by using two features to reflect the ranking
order of a candidate in Salience Reference List
(SRL). However, similar to common centering
models, in their system the ranking of entities
in SRL is also heuristic-based.
The coreferential chain length of a candidate,
or its variants such as occurrence frequency and
TFIDF, has been used as a salience factor in
some learning-based reference resolution sys-
tems (Iida et al, 2003; Mitkov, 1998; Paul et
al., 1999; Strube and Muller, 2003). However,
for an entity, the coreferential length only re-
flects its global salience in the whole text(s), in-
stead of the local salience in a discourse segment
which is nevertheless more informative for pro-
noun resolution. Moreover, during resolution,
the found coreferential length of an entity is of-
ten incomplete, and thus the obtained length
value is usually inaccurate for the salience eval-
uation.
7 Conclusion and Future Work
In this paper we have proposed a model which
incorporates coreferential information of candi-
dates to improve pronoun resolution. When
evaluating a candidate, the model considers its
adjacent antecedent by describing its properties
in terms of backward features. We first exam-
ined the effectiveness of the model by applying
it in an optimal environment where the clos-
est antecedent of a candidate is obtained cor-
rectly. The experiments show that it boosts
the success rate of the baseline system for both
MUC-6 (4.7%) and MUC-7 (3.5%). Then we
proposed how to apply our model in the real res-
olution where the antecedent of a non-pronoun
is found by an additional non-pronoun resolu-
tion module. Our model can still produce Suc-
cess improvement (4.7% for MUC-6 and 1.8%
for MUC-7) against the baseline system, de-
spite the low recall of the non-pronoun resolu-
tion module.
In the current work we restrict our study only
to pronoun resolution. In fact, the coreferential
information of candidates is expected to be also
helpful for non-pronoun resolution. We would
like to investigate the influence of the coreferen-
tial factors on general NP reference resolution in
our future work.
References
S. Brennan, M. Friedman, and C. Pollard.
1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting of
the Association for Compuational Linguis-
tics, pages 155?162.
N. Ge, J. Hale, and E. Charniak. 1998. A
statistical approach to anaphora resolution.
In Proceedings of the 6th Workshop on Very
Large Corpora.
B. Grosz, A. Joshi, and S. Weinstein. 1983.
Providing a unified account of definite noun
phrases in discourse. In Proceedings of the
21st Annual meeting of the Association for
Computational Linguistics, pages 44?50.
B. Grosz, A. Joshi, and S. Weinstein. 1995.
Centering: a framework for modeling the
local coherence of discourse. Computational
Linguistics, 21(2):203?225.
R. Iida, K. Inui, H. Takamura, and Y. Mat-
sumoto. 2003. Incorporating contextual cues
in trainable models for coreference resolu-
tion. In Proceedings of the 10th Confer-
ence of EACL, Workshop ?The Computa-
tional Treatment of Anaphora?.
R. Mitkov. 1998. Robust pronoun resolution
with limited knowledge. In Proceedings of the
17th Int. Conference on Computational Lin-
guistics, pages 869?875.
R. Mitkov. 1999. Anaphora resolution: The
state of the art. Technical report, University
of Wolverhampton.
MUC-6. 1995. Proceedings of the Sixth Message
Understanding Conference. Morgan Kauf-
mann Publishers, San Francisco, CA.
MUC-7. 1998. Proceedings of the Seventh
Message Understanding Conference. Morgan
Kaufmann Publishers, San Francisco, CA.
V. Ng and C. Cardie. 2002. Improving machine
learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics, pages 104?111, Philadelphia.
M. Paul, K. Yamamoto, and E. Sumita. 1999.
Corpus-based anaphora resolution towards
antecedent preference. In Proceedings of
the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, Work-
shop ?Coreference and It?s Applications?,
pages 47?52.
J. R. Quinlan. 1993. C4.5: Programs for ma-
chine learning. Morgan Kaufmann Publish-
ers, San Francisco, CA.
C. Sidner. 1981. Focusing for interpretation
of pronouns. American Journal of Computa-
tional Linguistics, 7(4):217?231.
W. Soon, H. Ng, and D. Lim. 2001. A ma-
chine learning approach to coreference reso-
lution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
M. Strube and C. Muller. 2003. A machine
learning approach to pronoun resolution in
spoken dialogue. In Proceedings of the 41st
Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Japan.
M. Strube. 1998. Never look back: An alterna-
tive to centering. In Proceedings of the 17th
Int. Conference on Computational Linguis-
tics and 36th Annual Meeting of ACL, pages
1251?1257.
J. R. Tetreault. 2001. A corpus-based eval-
uation of centering and pronoun resolution.
Computational Linguistics, 27(4):507?520.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly,
and L. Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proceedings of
the Sixth Message understanding Conference
(MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
X. Yang, G. Zhou, J. Su, and C. Tan.
2003. Coreference resolution using competi-
tion learning approach. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, Japan.
A Joint Source-Channel Model for Machine Transliteration 
Li Haizhou, Zhang Min, Su Jian 
 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{hli,sujian,mzhang}@i2r.a-star.edu.sg 
 
Abstract 
Most foreign names are transliterated into 
Chinese, Japanese or Korean with 
approximate phonetic equivalents. The 
transliteration is usually achieved through 
intermediate phonemic mapping. This 
paper presents a new framework that 
allows direct orthographical mapping 
(DOM) between two different languages, 
through a joint source-channel model, also 
called n-gram transliteration model (TM). 
With the n-gram TM model, we automate 
the orthographic alignment process to 
derive the aligned transliteration units from 
a bilingual dictionary. The n-gram TM 
under the DOM framework greatly reduces 
system development effort and provides a 
quantum leap in improvement in 
transliteration accuracy over that of other 
state-of-the-art machine learning 
algorithms. The modeling framework is 
validated through several experiments for 
English-Chinese language pair.  
1 Introduction 
In applications such as cross-lingual information 
retrieval (CLIR) and machine translation, there is 
an increasing need to translate out-of-vocabulary 
words from one language to another, especially 
from alphabet language to Chinese, Japanese or 
Korean.  Proper names of English, French, 
German, Russian, Spanish and Arabic origins 
constitute a good portion of out-of-vocabulary 
words. They are translated through transliteration, 
the method of translating into another language by 
preserving how words sound in their original 
languages. For writing foreign names in Chinese, 
transliteration always follows  the original 
romanization. Therefore, any foreign name will 
have only one Pinyin (romanization of Chinese) 
and thus in Chinese characters. 
In this paper, we focus on automatic Chinese 
transliteration of foreign alphabet names. Because 
some alphabet writing systems use various 
diacritical marks, we find it more practical to write 
names containing such diacriticals as they are 
rendered in English. Therefore, we refer all 
foreign-Chinese transliteration to English-Chinese 
transliteration, or E2C.  
Transliterating English names into Chinese is 
not straightforward. However, recalling the 
original from Chinese transliteration is even more 
challenging as the E2C transliteration may have 
lost some original phonemic evidences. The 
Chinese-English backward transliteration process 
is also called back-transliteration, or C2E (Knight 
& Graehl, 1998).  
In machine transliteration, the noisy channel 
model (NCM), based on a phoneme-based 
approach, has recently received considerable 
attention (Meng et al 2001; Jung et al 2000; Virga 
& Khudanpur, 2003; Knight & Graehl, 1998). In 
this paper we discuss the limitations of such an 
approach and address its problems by firstly 
proposing a paradigm that allows direct 
orthographic mapping (DOM), secondly further 
proposing a joint source-channel model as a 
realization of DOM. Two other machine learning 
techniques, NCM and ID3 (Quinlan, 1993) 
decision tree, also are implemented under DOM as 
reference to compare with the proposed n-gram 
TM. 
This paper is organized as follows: In section 2, 
we present the transliteration problems. In section 
3, a joint source-channel model is formulated. In 
section 4, several experiments are carried out to 
study different aspects of proposed algorithm. In 
section 5, we relate our algorithms to other 
reported work. Finally, we conclude the study with 
some discussions. 
2 Problems in transliteration 
Transliteration is a process that takes a character 
string in source language as input and generates a 
character string in the target language as output. 
The process can be seen conceptually as two levels 
of decoding: segmentation of the source string into 
transliteration units; and relating the source 
language transliteration units with units in the 
target language, by resolving different 
combinations of alignments and unit mappings. A 
unit could be a Chinese character or a monograph, 
a digraph or a trigraph and so on for English. 
2.1 Phoneme-based approach 
The problems of English-Chinese transliteration 
have been studied extensively in the paradigm of 
noisy channel model (NCM). For a given English 
name E as the observed channel output, one seeks 
a posteriori the most likely Chinese transliteration 
C that maximizes P(C|E). Applying Bayes rule, it 
means to find C to maximize 
 
P(E,C) = P(E | C)*P(C)                       (1) 
 
with equivalent effect. To do so, we are left with 
modeling two probability distributions: P(E|C), the 
probability of transliterating C to E through a noisy 
channel, which is also called transformation rules, 
and P(C), the probability distribution of source, 
which reflects what is considered good Chinese 
transliteration in general. Likewise, in C2E back-
transliteration, we would find E that maximizes 
 
P(E,C) = P(C | E)*P(E)                       (2) 
 
for a given Chinese name.  
In eqn (1) and (2), P(C) and P(E) are usually 
estimated using n-gram language models (Jelinek, 
1991). Inspired by research results of grapheme-to-
phoneme research in speech synthesis literature, 
many have suggested phoneme-based approaches 
to resolving P(E|C) and P(C|E), which 
approximates the probability distribution by 
introducing a phonemic representation. In this way, 
we convert the names in the source language, say 
E, into an intermediate phonemic representation P, 
and then convert the phonemic representation into 
the target language, say Chinese C. In E2C 
transliteration, the phoneme-based approach can be 
formulated as P(C|E) = P(C|P)P(P|E) and 
conversely we have P(E|C) = P(E|P)P(P|C) for 
C2E back-transliteration.  
Several phoneme-based techniques have been 
proposed in the recent past for machine 
transliteration using transformation-based learning 
algorithm (Meng et al 2001; Jung et al 2000; 
Virga & Khudanpur, 2003) and using finite state 
transducer that implements transformation rules 
(Knight & Graehl, 1998), where both handcrafted 
and data-driven transformation rules have been 
studied.  
However, the phoneme-based approaches are 
limited by two major constraints, which could 
compromise transliterating precision, especially in 
English-Chinese transliteration: 
1) Latin-alphabet foreign names are of different 
origins. For instance, French has different phonic 
rules from those of English.  The phoneme-based 
approach requires derivation of proper phonemic 
representation for names of different origins. One 
may need to prepare multiple language-dependent 
grapheme-to-phoneme (G2P) conversion systems 
accordingly, and that is not easy to achieve (The 
Onomastica Consortium, 1995). For example, 
/Lafontant/ is transliterated into ???(La-Feng-
Tang) while /Constant/ becomes ????(Kang-
Si-Tan-Te)? where syllable /-tant/ in the two 
names are transliterated differently depending on 
the names? language of origin.  
2) Suppose that language dependent grapheme-
to-phoneme systems are attainable, obtaining 
Chinese orthography will need two further steps: a) 
conversion from generic phonemic representation 
to Chinese Pinyin; b) conversion from Pinyin to 
Chinese characters. Each step introduces a level of 
imprecision. Virga and Khudanpur (2003) reported 
8.3% absolute accuracy drops when converting 
from Pinyin to Chinese characters, due to 
homophone confusion. Unlike Japanese katakana 
or Korean alphabet, Chinese characters are more 
ideographic than phonetic. To arrive at an 
appropriate Chinese transliteration, one cannot rely 
solely on the intermediate phonemic representation.  
2.2 Useful orthographic context  
To illustrate the importance of contextual 
information in transliteration, let?s take name 
/Minahan/ as an example, the correct segmentation 
should be /Mi-na-han/, to be transliterated as ?-
?-? (Pinyin: Mi-Na-Han).  
 
English /mi- -na- -han/ 
Chinese ? ? ? 
Pinyin Mi Nan Han 
 
However, a possible segmentation /Min-ah-an/ 
could lead to an undesirable syllabication of ?-
?-? (Pinyin: Min-A-An).  
 
English /min- -ah- -an/ 
Chinese ? ? ? 
Pinyin Min A An 
 
According to the transliteration guidelines, a 
wise segmentation can be reached only after 
exploring the combination of the left and right 
context of transliteration units. From the 
computational point of view, this strongly suggests 
using a contextual n-gram as the knowledge base 
for the alignment decision.  
Another example will show us how one-to-many 
mappings could be resolved by context. Let?s take 
another name /Smith/ as an example. Although we 
can arrive at an obvious segmentation /s-mi-th/, 
there are three Chinese characters for each of /s-/, 
/-mi-/ and /-th/. Furthermore, /s-/ and /-th/ 
correspond to overlapping characters as well, as 
shown next. 
 
English /s- -mi- -th/ 
Chinese 1 ? ? ? 
Chinese 2 ? ? ? 
Chinese 3 ? ? ? 
 
A human translator will use transliteration rules 
between English syllable sequence  and Chinese 
character sequence to obtain the best mapping ?-
?-?, as indicated in italic in the table above.  
To address the issues in transliteration, we 
propose a direct orthographic mapping (DOM) 
framework through a joint source-channel model 
by fully exploring orthographic contextual 
information, aiming at alleviating the imprecision 
introduced by the multiple-step phoneme-based 
approach.  
3 Joint source-channel model 
In view of the close coupling of the source and 
target transliteration units, we propose to estimate 
P(E,C) by a joint source-channel model, or n-gram 
transliteration model (TM). For K aligned 
transliteration units, we have 
 
)...,,...,(),( 2121 KK ccceeePCEP =  
   ),...,,,( 21 KcececeP ><><><= (3) 
   ?
=
?><><=
K
k
k
k ceceP
1
1
1 ),|,(       
 
which provides an alternative to the phoneme-
based approach for resolving eqn. (1) and (2) by 
eliminating the intermediate phonemic 
representation. 
Unlike the noisy-channel model, the joint 
source-channel model does not try to capture how 
source names can be mapped to target names, but 
rather how source and target names can be 
generated simultaneously. In other words, we 
estimate a joint probability model that can be 
easily marginalized in order to yield conditional 
probability models for both transliteration and 
back-transliteration. 
Suppose that we have an English name 
mxxx ...21=?  and a Chinese transliteration 
nyyy ...21=? where ix are letters and jy are 
Chinese characters. Oftentimes, the number of 
letters is different from the number of Chinese 
characters. A Chinese character may correspond to 
a letter substring in English or vice versa.  
 
mii xxxxxxx ...... 21321 ++  
 
 
 
nj yyyy ......21  
 
where there exists an alignment  ?  with 
 
>=<>< 111 ,, yxce  
>=<>< 2322 ,, yxxce  ? 
 
and >=<>< nmK yxce ,, . A transliteration unit 
correspondence >< ce,  is called a transliteration 
pair. Then, the E2C transliteration can be 
formulated as 
 
),,(maxarg
,
????
??
P=  (4) 
 
and similarly the C2E back-transliteration as 
 
),,(maxarg
,
????
??
P=  (5) 
 
An n-gram transliteration model is defined as the 
conditional probability, or transliteration 
probability, of a transliteration pair kce >< ,  
depending on its immediate n predecessor pairs: 
 
      ),,(),( ???PCEP =  
?
=
?
+?><><=
K
k
k
nkk ceceP
1
1
1),|,(        (6) 
 
3.1 Transliteration alignment 
A bilingual dictionary contains entries mapping 
English names to their respective Chinese 
transliterations. Like many other solutions in 
computational linguistics, it is possible to 
automatically analyze the bilingual dictionary to 
acquire knowledge in order to map new English 
names to Chinese and vice versa. Based on the 
transliteration formulation above, a transliteration 
model can be built with transliteration unit?s n-
gram statistics. To obtain the statistics, the 
bilingual dictionary needs to be aligned. The 
maximum likelihood approach, through EM 
algorithm (Dempster, 1977), allows us to infer 
such an alignment easily as described in the table 
below. 
 
 
 
 
 
 
 
 
 
 
 
 
The aligning process is different from that of 
transliteration given in eqn. (4) or (5) in that, here 
we have fixed bilingual entries, ? and ? . The 
aligning process is just to find the alignment 
segmentation ? between the two strings that 
maximizes the joint probability: 
),,(maxarg ????
?
P=   (7) 
A set of transliteration pairs that is derived from 
the aligning process forms a transliteration table, 
which is in turn used in the transliteration 
decoding.  As the decoder is bounded by this table, 
it is important to make sure that the training 
database covers as much as possible the potential 
transliteration patterns. Here are some examples of 
resulting alignment pairs. 
 
?|s  ?|l ?|t ?|d 
?|k ?|b ?|g ?|r  
?|ll ?|c  ?|ro  ?|ri  
?|man  ?|m  ?|p  ?|de  
?|ra  ?|le  ?|a  ?|ber  
?|la  ?|son  ?|ton  ?|tt  
?|re  ?|co  ?|o  ?|e  
?|ma  ?|ley  ?|li  ?|mer 
 
Knowing that the training data set will never be 
sufficient for every n-gram unit, different 
smoothing approaches are applied, for example, by 
using backoff or class-based models, which can be 
found in statistical language modeling literatures 
(Jelinek, 1991). 
3.2 DOM: n-gram TM vs. NCM 
Although in the literature, most noisy channel 
models (NCM) are studied under phoneme-based 
paradigm for machine transliteration, NCM can 
also be realized under direct orthographic mapping 
(DOM). Next, let?s look into a bigram case to see 
what n-gram TM and NCM present to us. For E2C 
conversion, re-writing eqn (1) and eqn (6) , we 
have 
?
=
??
K
k
kkkk ccPcePP
1
1)|()|(),,( ???       (8) 
),,( ???P ),|,( 1
1
?
=
><><?? kkK
k
ceceP   (9)      
                
The formulation of eqn. (8) could be interpreted 
as a hidden Markov model with Chinese characters 
as its hidden states and English transliteration units 
as the observations (Rabiner, 1989). The number 
of parameters in the bigram TM is potentially 2T , 
while in the noisy channel model (NCM) it?s 
2CT + , where T  is the number of transliteration 
pairs and C is the number of Chinese 
transliteration units. In eqn. (9), the current 
transliteration depends on both Chinese and 
English transliteration history while in eqn. (8), it 
depends only on the previous Chinese unit. 
As 22 CTT +>> , an n-gram TM gives a finer 
description than that of NCM. The actual size of 
models largely depends on the availability of 
training data. In Table 1, one can get an idea of 
how they unfold in a real scenario. With 
adequately sufficient training data, n-gram TM is 
expected to outperform NCM in the decoding. A 
perplexity study in section 4.1 will look at the 
model from another perspective. 
4 The experiments1 
We use a database from the bilingual dictionary 
?Chinese Transliteration of Foreign Personal 
Names? which was edited by Xinhua News 
Agency and was considered the de facto standard 
of personal name transliteration in today?s Chinese 
press. The database includes a collection of 37,694 
unique English entries and their official Chinese 
transliteration. The listing includes personal names 
of English, French, Spanish, German, Arabic, 
Russian and many other origins. 
The database is initially randomly distributed 
into 13 subsets. In the open test, one subset is 
withheld for testing while the remaining 12 subsets 
are used as the training materials. This process is 
repeated 13 times to yield an average result, which 
is called the 13-fold open test. After experiments, 
we found that each of the 13-fold open tests gave 
consistent error rates with less than 1% deviation. 
Therefore, for simplicity, we randomly select one 
of the 13 subsets, which consists of 2896 entries, 
as the standard open test set to report results. In the 
close test, all data entries are used for training and 
testing.  
                                                     
1 demo at http://nlp.i2r.a-star.edu.sg/demo.htm 
The Expectation-Maximization algorithm 
1. Bootstrap initial random alignment 
2. Expectation: Update n-gram statistics to 
estimate probability distribution 
3. Maximization: Apply the n-gram TM to 
obtain new alignment 
4. Go to step 2 until the alignment converges 
5. Derive a list transliteration units from final 
       alignment as transliteration table 
4.1 Modeling 
The alignment of transliteration units is done 
fully automatically along with the n-gram TM 
training process. To model the boundary effects, 
we introduce two extra units <s> and </s> for start 
and end of each name in both languages. The EM 
iteration converges at 8th round when no further 
alignment changes are reported. Next are some 
statistics as a result of the model training: 
 
# close set bilingual entries (full data)  37,694 
# unique Chinese transliteration (close) 28,632 
# training entries for open test 34,777 
# test entries for open test 2,896 
# unique transliteration pairs  T 5,640 
# total transliteration pairs TW  119,364
# unique English units E 3,683 
# unique Chinese units C 374 
# bigram TM ),|,( 1?><>< kk ceceP  38,655 
# NCM Chinese bigram )|( 1?kk ccP  12,742 
Table 1. Modeling statistics 
The most common metric for evaluating an n-
gram model is the probability that the model 
assigns to test data, or perplexity (Jelinek, 1991). 
For a test set W composed of V names, where each 
name has been aligned into a sequence of 
transliteration pair tokens, we can calculate the 
probability of test set 
?
=
=
V
v
vvvPWp
1
),,()( ??? by applying the n-gram 
models to the token sequence. The cross-entropy 
)(WH p  of a model on data W is defined as 
)(log1)( 2 WpW
WH
T
p ?=  where TW is the total 
number of aligned transliteration pair tokens in the 
data W. The perplexity )(WPPp of a model is the 
reciprocal of the average probability assigned by 
the model to each aligned pair in the test set W 
as )(2)( WHp pWPP = . 
Clearly, lower perplexity means that the model 
describes better the data. It is easy to understand 
that closed test always gives lower perplexity than 
open test.  
 
 
 
 
 
 
 TM 
open  
NCM 
open 
TM 
closed 
NCM 
closed 
1-gram 670 729 655 716 
2-gram 324 512 151 210 
3-gram 306 487 68 127 
Table 2. Perplexity study of bilingual database 
We have the perplexity reported in Table 2 on 
the aligned bilingual dictionary, a database of 
119,364 aligned tokens. The NCM perplexity is 
computed using n-gram equivalents of eqn. (8) for 
E2C transliteration, while TM perplexity is based 
on those of eqn (9) which applies to both E2C and 
C2E. It is shown that TM consistently gives lower 
perplexity than NCM in open and closed tests. We 
have good reason to expect TM to provide better 
transliteration results which we expect to be 
confirmed later in the experiments. 
The Viterbi algorithm produces the best 
sequence by maximizing the overall probability, 
),,( ???P . In CLIR or multilingual corpus 
alignment (Virga and Khudanpur, 2003), N-best 
results will be very helpful to increase chances of 
correct hits. In this paper, we adopted an N-best 
stack decoder (Schwartz and Chow, 1990) in both 
TM and NCM experiments to search for N-best 
results. The algorithm also allows us to apply 
higher order n-gram such as trigram in the search. 
4.2 E2C transliteration 
In this experiment, we conduct both open and 
closed tests for TM and NCM models under DOM 
paradigm. Results are reported in Table 3 and 
Table 4.  
 open 
(word) 
open 
(char) 
closed 
(word) 
closed 
(char) 
1-gram 45.6% 21.1% 44.8% 20.4% 
2-gram 31.6% 13.6% 10.8% 4.7% 
3-gram 29.9% 10.8% 1.6% 0.8% 
Table 3. E2C error rates for n-gram TM tests.  
 open 
(word)
open 
(char) 
closed 
(word) 
closed 
(char) 
1-gram 47.3% 23.9% 46.9% 22.1% 
2-gram 39.6% 20.0% 16.4% 10.9% 
3-gram 39.0% 18.8% 7.8% 1.9% 
Table 4. E2C error rates for n-gram NCM tests 
In word error report, a word is considered 
correct only if an exact match happens between 
transliteration and the reference. The character 
error rate is the sum of deletion, insertion and 
substitution errors. Only the top choice in N-best 
results is used for error rate reporting. Not 
surprisingly, one can see that n-gram TM, which 
benefits from the joint source-channel model 
coupling both source and target contextual 
information into the model, is superior to NCM in 
all the test cases.  
4.3 C2E back-transliteration 
The C2E back-transliteration is more 
challenging than E2C transliteration. Not many 
studies have been reported in this area. It is 
common that multiple English names are mapped 
into the same Chinese transliteration. In Table 1, 
we see only 28,632 unique Chinese transliterations 
exist for 37,694 English entries, meaning that some 
phonemic evidence is lost in the process of 
transliteration. To better understand the task, let?s 
compare the complexity of the two languages 
presented in the bilingual dictionary.  
Table 1 also shows that the 5,640 transliteration 
pairs are cross mappings between 3,683 English 
and 374 Chinese units. In order words, on average, 
for each English unit, we have 1.53 = 5,640/3,683 
Chinese correspondences. In contrast, for each 
Chinese unit, we have 15.1 = 5,640/374 English 
back-transliteration units! Confusion is increased 
tenfold going backward.  
The difficulty of back-transliteration is also 
reflected by the perplexity of the languages as in 
Table 5. Based on the same alignment 
tokenization, we estimate the monolingual 
language perplexity for Chinese and English 
independently using the n-gram language models 
)|( 1 1
?
+?
k
nkk ccP  and )|(
1
1
?
+?
k
nkk eeP . Without 
surprise, Chinese names have much lower 
perplexity than English names thanks to fewer 
Chinese units. This contributes to the success of 
E2C but presents a great challenge to C2E back-
transliteration. 
 
 1-gram 2-gram 3-gram 
Chinese 207/206 97/86 79/45 
English 710/706 265/152 234/67 
Table 5 language perplexity comparison 
(open/closed test) 
 open 
(word) 
open 
(letter) 
closed 
(word) 
closed 
(letter) 
1 gram 82.3% 28.2% 81% 27.7% 
2 gram 63.8% 20.1% 40.4% 12.3% 
3 gram 62.1% 19.6% 14.7% 5.0% 
Table 6. C2E error rate for n-gram TM tests 
 E2C 
open 
E2C 
closed 
C2E 
open 
C2E 
closed 
1-best 29.9% 1.6% 62.1% 14.7% 
5-best 8.2% 0.94% 43.3% 5.2% 
10-best 5.4% 0.90% 24.6% 4.8% 
Table 7. N-best word error rates for 3-gram TM 
tests 
A back-transliteration is considered correct if it 
falls within the multiple valid orthographically 
correct options. Experiment results are reported in 
Table 6. As expected, C2E error rate is much 
higher than that of E2C. 
In this paper, the n-gram TM model serves as the 
sole knowledge source for transliteration. 
However, if secondary knowledge, such as a 
lookup table of valid target transliterations, is 
available, it can help reduce error rate by 
discarding invalid transliterations top-down the N 
choices. In Table 7, the word error rates for both 
E2C and C2E are reported which imply potential 
error reduction by secondary knowledge source. 
The N-best error rates are reduced significantly at 
10-best level as reported in Table 7. 
5 Discussions 
It would be interesting to relate n-gram TM to 
other related framework. 
5.1 DOM: n-gram TM vs. ID3 
In section 4, one observes that contextual 
information in both source and target languages is 
essential. To capture them in the modeling, one 
could think of decision tree, another popular 
machine learning approach. Under the DOM 
framework, here is the first attempt to apply 
decision tree in E2C and C2E transliteration. 
With the decision tree, given a fixed size 
learning vector, we used top-down induction trees 
to predict the corresponding output. Here we 
implement ID3 (Quinlan, 1993) algorithm to 
construct the decision tree which contains 
questions and return values at terminal nodes. 
Similar to n-gram TM, for unseen names in open 
test, ID3 has backoff smoothing, which lies on the 
default case which returns the most probable value 
as its best guess for a partial tree path according to 
the learning set.  
In the case of E2C transliteration, we form a 
learning vector of 6 attributes by combining 2 left 
and 2 right letters around the letter of focus ke  and 
1 previous Chinese unit 1?kc . The process is 
illustrated in Table 8, where both English and 
Chinese contexts are used to infer a Chinese 
character. Similarly, 4 attributes combining 1 left, 
1 centre and 1 right Chinese character and 1 
previous English unit are used for the learning 
vector in C2E test. An aligned bilingual dictionary 
is needed to build the decision tree.  
To minimize the effects from alignment 
variation, we use the same alignment results from 
section 4. Two trees are built for two directions, 
E2C and C2E. The results are compared with those 
3-gram TM  in Table 9. 
 
2?ke  1?ke  ke  1+ke  2+ke  1?kc   kc
_ _ N I C _ > ?
_ N I C E ? > _ 
N I C E _ _ > ?
I C E _ _ ? > _ 
Table 8. E2C transliteration using ID3 decision 
tree  for transliterating Nice to               
?? (?|NI ?|CE)  
 open  closed  
ID3 E2C  39.1% 9.7% 
3-gram TM E2C 29.9% 1.6% 
ID3 C2E 63.3% 38.4% 
3-gram TM C2E 62.1% 14.7% 
Table 9. Word error rate ID3 vs. 3-gram TM 
One observes that n-gram TM consistently 
outperforms ID3 decision tree in all tests. Three 
factors could have contributed:  
 
1) English transliteration unit size ranges from 1 
letter to 7 letters. The fixed size windows in ID3 
obviously find difficult to capture the dynamics of 
various ranges.  n-gram TM seems to have better 
captured the dynamics of transliteration units;  
2) The backoff smoothing of n-gram TM is more 
effective than that of ID3;  
3) Unlike n-gram TM, ID3 requires a separate 
aligning process for bilingual dictionary. The 
resulting alignment may not be optimal for tree 
construction.  Nevertheless, ID3 presents another 
successful implementation of DOM framework.  
 
5.2 DOM vs. phoneme-based approach 
Due to lack of standard data sets, it is difficult to 
compare the performance of the n-gram TM to that 
of other approaches. For reference purpose, we list 
some reported studies on other databases of E2C 
transliteration tasks in Table 10. As in the 
references, only character and Pinyin error rates 
are reported, we only include our character and 
Pinyin error rates for easy reference. The reference 
data are extracted from Table 1 and 3 of (Virga and 
Khudanpur 2003). As we have not found any C2E 
result in the literature, only E2C results are 
compared here. 
The first 4 setups by Virga et alall adopted the 
phoneme-based approach in the following steps:  
 
1) English name to English phonemes; 
2) English phonemes to Chinese Pinyin;  
3) Chinese Pinyin to Chinese characters. 
 
It is obvious that the n-gram TM compares 
favorably to other techniques. n-gram TM presents 
an error reduction of 74.6%=(42.5-10.8)/42.5% for 
Pinyin over the best reported result, Huge MT (Big 
MT) test case, which is noteworthy.  
The DOM framework shows a quantum leap in 
performance with n-gram TM being the most 
successful implementation. The n-gram TM and 
ID3 under direct orthographic mapping (DOM) 
paradigm simplify the process and reduce the 
chances of conversion errors. As a result, n-gram 
TM and ID3 do not generate Chinese Pinyin as 
intermediate results. It is noted that in the 374 
legitimate Chinese characters for transliteration, 
character to Pinyin mapping is unique while Pinyin 
to character mapping could be one to many. Since 
we have obtained results in character already, we 
expect less Pinyin error than character error should 
a character-to-Pinyin mapping be needed. 
 
System Trainin
g size 
Test 
size 
Pinyin 
errors 
Char 
errors 
Meng et al2,233 1,541 52.5% N/A 
Small MT 2,233 1,541 50.8% 57.4% 
Big MT 3,625 250 49.1% 57.4% 
Huge MT 
(Big MT) 
309,01
9 
3,122 42.5% N/A 
3-gram 
TM/DOM 
34,777 2,896 < 10.8% 10.8% 
ID3/DOM 34,777 2,896 < 15.6% 15.6% 
Table 10. Performance reference in recent 
studies 
6 Conclusions 
In this paper, we propose a new framework 
(DOM) for transliteration. n-gram TM is a 
successful realization of DOM paradigm. It 
generates probabilistic orthographic transformation 
rules using a data driven approach. By skipping the 
intermediate phonemic interpretation, the 
transliteration error rate is reduced significantly. 
Furthermore, the bilingual aligning process is 
integrated into the decoding process in n-gram TM, 
which allows us to achieve a joint optimization of 
alignment and transliteration automatically. Unlike 
other related work where pre-alignment is needed, 
the new framework greatly reduces the 
development efforts of machine transliteration 
systems. Although the framework is implemented 
on an English-Chinese personal name data set, 
without loss of generality, it well applies to 
transliteration of other language pairs such as 
English/Korean and English/Japanese. 
It is noted that place and company names are 
sometimes translated in combination of 
transliteration and meanings, for example, 
/Victoria-Fall/ becomes ? ? ? ? ? ? 
(Pinyin:Wei Duo Li Ya Pu Bu). As the proposed 
framework allows direct orthographical mapping, 
it can also be easily extended to handle such name 
translation. We expect to see the proposed model 
to be further explored in other related areas. 
References  
Dempster, A.P., N.M. Laird and D.B.Rubin, 1977. 
Maximum likelihood from incomplete data via 
the EM algorithm, J. Roy. Stat. Soc., Ser. B. Vol. 
39, pp138 
Helen M. Meng, Wai-Kit Lo, Berlin Chen and 
Karen Tang. 2001. Generate Phonetic Cognates 
to Handle Name Entities in English-Chinese 
cross-language spoken document retrieval, 
ASRU 2001 
Jelinek, F. 1991, Self-organized language 
modeling for speech recognition, In Waibel, A. 
and Lee K.F. (eds), Readings in Speech 
Recognition, Morgan Kaufmann., San Mateo, 
CA 
K. Knight and J. Graehl. 1998. Machine 
Transliteration, Computational Linguistics 24(4) 
Paola Virga, Sanjeev Khudanpur, 2003. 
Transliteration of Proper Names in Cross-
lingual Information Retrieval. ACL 2003 
workshop MLNER 
Quinlan J. R. 1993, C4.5 Programs for machine 
learning, Morgan Kaufmann , San Mateo, CA  
Rabiner, Lawrence R. 1989, A tutorial on hidden 
Markov models and selected applications in 
speech recognition, Proceedings of the IEEE 
77(2) 
Schwartz, R. and Chow Y. L., 1990, The N-best 
algorithm: An efficient and Exact procedure for 
finding the N most likely sentence hypothesis, 
Proceedings of ICASSP 1990, Albuquerque, pp 
81-84 
Sung Young Jung, Sung Lim Hong and Eunok 
Paek, 2000, An English to Korean 
Transliteration Model of Extended Markov 
Window, Proceedings of COLING 
The Onomastica Consortium, 1995. The 
Onomastica interlanguage pronunciation 
lexicon, Proceedings of EuroSpeech, Madrid, 
Spain, Vol. 1, pp829-832 
Xinhua News Agency, 1992, Chinese 
transliteration of foreign personal names, The 
Commercial Press 
 
Multi-Criteria-based Active Learning for Named Entity Recognition 
Dan Shen??1 Jie Zhang?? Jian Su? Guodong Zhou? Chew-Lim Tan? 
? Institute for Infocomm Technology 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg 
{shendan,zhangjie,tancl}@comp.nus.edu.sg 
                                                                 
1 Current address of the first author: Universit?t des Saarlandes, Computational Linguistics Dept., 66041 Saarbr?cken, Germany 
dshen@coli.uni-sb.de 
 
 
 
 
Abstract 
In this paper, we propose a multi-criteria -
based active learning approach and effec-
tively apply it to named entity recognition. 
Active learning targets to minimize the 
human annotation efforts by selecting ex-
amples for labeling.  To maximize the con-
tribution of the selected examples, we 
consider the multiple criteria: informative-
ness, representativeness and diversity  and 
propose measures to quantify them.  More 
comprehensively, we incorporate all the 
criteria using two selection strategies, both 
of which result in less labeling cost than 
single-criterion-based method.  The results 
of the named entity recognition in both 
MUC-6 and GENIA show that the labeling 
cost can be reduced by at least 80% with-
out degrading the performance. 
1 Introduction 
In the machine learning approaches of natural lan-
guage processing (NLP), models are generally 
trained on large annotated corpus.  However, anno-
tating such corpus is expensive and time-
consuming, which makes it difficult to adapt an 
existing model to a new domain.  In order to over-
come this difficulty, active learning (sample selec-
tion) has been studied in more and more NLP 
applications such as POS tagging (Engelson and 
Dagan 1999), information extraction (Thompson et 
al. 1999), text classif ication (Lewis and Catlett 
1994; McCallum and Nigam 1998; Schohn and 
Cohn 2000; Tong and Koller 2000; Brinker 2003), 
statistical parsing (Thompson et al 1999; Tang et 
al. 2002; Steedman et al 2003), noun phrase 
chunking (Ngai and Yarowsky 2000), etc. 
Active learning is based on the assumption that 
a small number of annotated examples and a large 
number of unannotated examples are available.  
This assumption is valid in most NLP tasks.  Dif-
ferent from supervised learning in which the entire 
corpus are labeled manually, active learning is to 
select the most useful example for labeling and add 
the labeled example  to training set to retrain model.  
This procedure is repeated until the model achieves 
a certain level of performance.  Practically, a batch 
of examples are selected at a time, called batched-
based sample selection (Lewis and Catlett 1994) 
since it is time consuming to retrain the model if 
only one new example is added to the training set.  
Many existing work in the area focus on two ap-
proaches: certainty-based methods (Thompson et 
al. 1999; Tang et al 2002; Schohn and Cohn 2000; 
Tong and Koller 2000; Brinker 2003) and commit-
tee-based methods (McCallum and Nigam 1998; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000) to select the most informative examples for 
which the current model are most uncertain. 
Being the first piece of work on active learning 
for name entity recognition (NER) task, we target 
to minimize the human annotation efforts yet still 
reaching the same level of performance as a super-
vised learning approach.  For this purpose, we 
make a more comprehensive consideration on the 
contribution of individual examples, and more im-
portantly maximizing the contribution of a batch 
based on three criteria : informativeness, represen-
tativeness and diversity. 
First, we propose three scoring functions to 
quantify the informativeness of an example , which 
can be used to select the most uncertain examples.  
Second, the representativeness measure is further 
proposed to choose the examples representing the 
majority.  Third, we propose two diversity consid-
erations (global and local) to avoid repetition 
among the examples of a batch.  Finally, two com-
bination strategies with the above three criteria are 
proposed to reach the maximum effectiveness on 
active learning for NER. 
We build our NER model using Support Vec-
tor Machines (SVM).  The experiment shows that 
our active learning methods achieve a promising 
result in this NER task.  The results in both MUC-
6 and GENIA show that the amount of the labeled 
training data can be reduced by at least 80% with-
out degrading the quality of the named entity rec-
ognizer.  The contributions not only come from the 
above measures, but also the two sample selection 
strategies which effectively incorporate informa-
tiveness, representativeness and diversity criteria.  
To our knowledge, it is the first work on consider-
ing the three criteria all together for active learning.  
Furthermore, such measures and strategies can be 
easily adapted to other active learning tasks as well.  
 
2 Multi-criteria for NER Active Learning 
Support Vector Machines (SVM) is a powerful 
machine learning method, which has been applied 
successfully in NER tasks, such as (Kazama et al 
2002; Lee et al 2003).  In this paper, we apply ac-
tive learning methods to a simple  and effective 
SVM model to recognize one class of names at a 
time, such as protein names, person names, etc.  In 
NER, SVM is to classify a word into positive class 
?1? indicating that the word is a part of an entity, 
or negative class ?-1? indicating that the word is 
not a part of an entity.  Each word in SVM is rep-
resented as a high-dimensional feature vector in-
cluding surface word information, orthographic 
features, POS feature and semantic trigger features 
(Shen et al 2003).  The semantic trigger features 
consist of some special head nouns for an entity 
class which is supplied by users.  Furthermore, a 
window (size = 7), which represents the local con-
text of the target word w, is also used to classify w.   
However, for active learning in NER, it is not 
reasonable to select a single word without context 
for human to label.  Even if we require human to 
label a single word, he has to make an addition 
effort to refer to the context of the word.  In our 
active learning process, we select a word sequence 
which consists of a machine-annotated named en-
tity and its context rather than a single word.  
Therefore, all of the measures we propose for ac-
tive learning should be applied to the machine-
annotated named entities and we have to further 
study how to extend the measures for words to 
named entities.  Thus, the active learning in SVM-
based NER will be more complex than that in sim-
ple classification tasks, such as text classif ication 
on which most SVM active learning works are 
conducted (Schohn and Cohn 2000; Tong and 
Koller 2000; Brinker 2003).  In the next part, we 
will introduce informativeness, representativeness 
and diversity measures for the SVM-based NER. 
2.1 Informativeness 
The basic idea of informativeness criterion is simi-
lar to certainty-based sample selection methods, 
which have been used in many previous works.  In 
our task, we use a distance-based measure to 
evaluate the informativeness of a word and extend 
it to the measure of an entity using three scoring 
functions.  We prefer the examples with high in-
formative degree for which the current model are 
most uncertain. 
2.1.1 Informativeness Measure for Word 
In the simplest linear form, training SVM is to find 
a hyperplane that can separate the posit ive and 
negative examples in training set with maximum 
margin.  The margin is defined by the distance of 
the hyperplane to the nearest of the positive and 
negative examples.  The training examples which 
are closest to the hyperplane are called support 
vectors.  In SVM, only the support vectors are use-
ful for the classification, which is different from 
statistical models.  SVM training is to get these 
support vectors and their weights from training set 
by solving quadratic programming problem.  The 
support vectors can later be used to classify the test 
data. 
Intuitively, we consider the informativeness of 
an example  as how it can make effect on the sup-
port vectors by adding it to training set.  An exam-
ple may be informative for the learner if the 
distance of its feature vector to the hyperplane is 
less than that of the support vectors to the hyper-
plane (equal to 1).  This intuition is also justified 
by (Schohn and Cohn 2000; Tong and Koller 2000) 
based on a version space analysis.  They state that 
labeling an example that lies on or close to the hy-
perplane is guaranteed to have an effect on the so-
lution.  In our task, we use the distance to measure 
the informativeness of an example. 
The distance of a word?s feature vector to the 
hyperplane is computed as follows: 
1
( ) ( , )
N
i i i
i
Dist y k ba
=
= +?w s w  
where w is the feature vector of the word, ai, yi, si 
corresponds to the weight, the class and the feature 
vector of the ith support vector respectively.  N is 
the number of the support vectors in current model. 
We select the example with minimal Dist, 
which indicates that it comes closest to the hyper-
plane in feature space.  This example is considered 
most informative for current model. 
2.1.2 Informativeness Measure for Named 
Entity 
Based on the above informativeness measure for a 
word, we compute the overall informativeness de-
gree of a named entity NE.  In this paper, we pro-
pose three scoring functions as follows. Let NE = 
w1?wN in which wi is the feature vector of the ith 
word of NE. 
? Info_Avg: The informativeness of NE is 
scored by the average distance of the words in 
NE to the hyperplane.  
 ( ) 1 ( )
i
i
N E
Info NE Dist
?
= - ?
w
w  
 where, wi is the feature vector of the ith word in 
NE. 
? Info_Min: The informativeness of NE is 
scored by the minimal distance of the words in 
NE. 
 ( ) 1 { ( )}
i
iNE
Info NE Min Dist
?
= -
w
w  
? Info_S/N: If the distance of a word to the hy-
perplane is less than a threshold a (= 1 in our 
task), the word is considered with short dis-
tance.  Then, we compute the proportion of the 
number of words with short distance to the to-
tal number of words in the named entity and 
use this proportion to quantify the informa-
tiveness of the named entity.  
 
( ( ) )
( ) i
i
N E
NUM Dist
Info NE
N
a
?
<
= w
w
 
In Section 4.3, we will evaluate the effective-
ness of these scoring functions. 
2.2 Representativeness 
In addition to the most informative example, we 
also prefer the most representative example.  The 
representativeness of an example can be evaluated 
based on how many examples there are similar or 
near to it.  So, the examples with high representa-
tive degree are less likely to be an outlier.  Adding 
them to the training set will have effect on a large 
number of unlabeled examples.  There are only a 
few works considering this selection criterion 
(McCallum and Nigam 1998; Tang et al 2002) and 
both of them are specific to their tasks, viz. text 
classification and statistical parsing.  In this section, 
we compute the simila rity between words using a 
general vector-based measure, extend this measure 
to named entity level using dynamic time warping 
algorithm and quantify the representativeness of a 
named entity by its density. 
2.2.1 Similarity Measure  between Words 
In general vector space model, the similarity be-
tween two vectors may be measured by computing 
the cosine value of the angle between them.  The 
smaller the angle is, the more similar between the 
vectors are.  This measure, called cosine-similarity 
measure, has been widely used in information re-
trieval tasks (Baeza-Yates and Ribeiro-Neto 1999).    
In our task, we also use it to quantify the similarity 
between two words.  Particularly, the calculation in 
SVM need be projected to a higher dimensional 
space by using a certain kernel function ( , )i jK w w .  
Therefore, we adapt the cosine-similarity measure 
to SVM as follows: 
( , )
( , )
( , ) ( , )
i j
i j
i i j j
k
Sim
k k
=
w w
w w
w w w w
 
where, wi and wj are the feature vectors of the 
words i and j.  This calculation is also supported by 
(Brinker 2003)?s work.  Furthermore, if we use the 
linear kernel ( , )i j i jk = ?w w w w , the measure is 
the same as the traditional cosine similarity meas-
ure cos i j
i j
q
?
=
?
w w
w w
 and may be regarded as a 
general vector-based similarity measure. 
2.2.2 Similarity Meas ure between Named En-
tities 
In this part, we compute the similarity between two 
machine-annotated named entities given the simi-
larities between words.  Regarding an entity as a 
word sequence, this work is analogous to the 
alignment of two sequences.  We employ the dy-
namic time warping (DTW) algorithm (Rabiner et 
al. 1978) to find an optimal alignment between the 
words in the sequences which maximize the accu-
mulated similarity degree between the sequences.  
Here, we adapt it to our task.  A sketch of the 
modified algorithm is as follows. 
Let NE1 = w11w12?w1n?w1N, (n = 1,?, N) and 
NE2 = w21w22?w2m?w2M, (m = 1,?, M) denote two 
word sequences to be matched.  NE1 and NE2 con-
sist of M and N words respectively.  NE1(n) = w1n 
and NE2(m) = w2m.  A similarity value Sim(w1n ,w2m) 
has been known for every pair of words (w1n,w2m) 
within NE1 and NE2.  The goal of DTW is to find a 
path, m = map(n), which map n onto the corre-
sponding m such that the accumulated similarity 
Sim* along the path is maximized. 
1 2
{ ( )} 1
* { ( ( ), ( ( ))}
N
m a p n n
Sim M a x Sim N E n N E m a p n
=
= ?  
A dynamic programming method is used to deter-
mine the optimum path map(n).  The accumulated 
similarity SimA to any grid point (n, m) can be re-
cursively calculated as 
1 2( , ) ( , ) ( 1, )A n m Aq mSim n m Sim w w M a x S i m n q?= + -
Finally, * ( , )ASim Sim N M=  
Certainly, the overall similarity measure Sim* 
has to be normalized as longer sequences normally 
give higher similarity value.  So, the similarity be-
tween two sequences NE1 and NE2 is calculated as 
1 2
*( , )
( , )
SimSim NE NE
Max N M
=  
2.2.3 Representativeness Measure for Named 
Entity 
Given a set of machine-annotated named entities 
NESet = {NE1, ? , NEN}, the representativeness of 
a named entity NEi in NESet is quantified by its 
density.  The density of NEi is defined as the aver-
age similarity between NEi and all the other enti-
ties NEj in NESet as follows. 
( , )
( )
1
i j
j i
i
Sim NE NE
Density N E
N
?=
-
?
 
If NEi has the largest density among all the entities 
in NESet, it can be regarded as the centroid of NE-
Set and also the most representative examples in 
NESet. 
2.3 Diversity 
Diversity criterion is to maximize the training util-
ity of a batch.  We prefer the batch in which the 
examples have high variance to each other.  For 
example, given the batch size 5, we try not to se-
lect five repetitious examples at a time.  To our 
knowledge, there is only one work (Brinker 2003) 
exploring this criterion.  In our task, we propose 
two methods: local and global, to make the exam-
ples diverse enough in a batch.   
2.3.1 Global Consideration 
For a global consideration, we cluster all named 
entities in NESet based on the similarity measure 
proposed in Section 2.2.2.  The named entities in 
the same cluster may be considered similar to each 
other, so we will select the named entities from 
different clusters at one time.  We employ a K-
means clustering algorithm (Jelinek 1997), which 
is shown in Figure 1. 
Given: 
NESet = {NE1, ? , NEN} 
Suppose: 
The number of clusters is K 
Initialization: 
Randomly equally partition {NE1, ? , NEN} into K 
initial clusters Cj (j = 1, ? , K). 
Loop until the number of changes for the centroids of 
all clusters is less than a threshold 
? Find the centroid of each cluster Cj (j = 1, ? , K). 
 arg ( ( , ))
j i j
j i
NE C NE C
NECent max Sim NE NE
? ?
= ?  
? Repartition {NE1, ? , NEN} into K clusters.  NEi 
will be assigned to Cluster Cj if 
 
( , ) ( , ),i j i wSim NE NECent Sim NE NECent w j? ?  
Figure 1: Global Consideration for Diversity: K-
Means Clustering algorithm 
In each round, we need to compute the pair-
wise similarities within each cluster to get the cen-
troid of the cluster.  And then, we need to compute 
the similarities between each example and all cen-
troids to repartition the examples.  So, the algo-
rithm is time-consuming.  Based on the assumption 
that N examples are uniformly distributed between 
the K clusters, the time complexity of the algo-
rithm is about O(N2/K+NK) (Tang et al 2002).  In 
one of our experiments, the size of the NESet (N) is 
around 17000 and K is equal to 50, so the time 
complexity is about O(106).  For efficiency, we 
may filter the entities in NESet before clustering 
them, which will be further discussed in Section 3.  
2.3.2 Local Consideration 
When selecting a machine-annotated named entity, 
we compare it with all previously selected named 
entities in the current batch.  If the similarity be-
tween them is above a threshold ?, this example 
cannot be allowed to add into the batch.  The order 
of selecting examples is based on some measure, 
such as informativeness measure, representative-
ness measure or their combination.  This local se-
lection method is shown in Figure 2.  In this way, 
we avoid selecting too similar examples (similarity 
value ?  ?) in a batch.  The threshold ? may be the 
average similarity between the examples in NESet. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = empty 
Loop until BatchSet is full 
? Select NEi based on some measure from NESet. 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 2: Local Consideration for Diversity 
 
This consideration only requires O(NK+K2) 
computational time.  In one of our experiments (N 
 ?17000 and K = 50), the time complexity is about 
O(105).  It is more efficient than clustering algo-
rithm described in Section 2.3.1.  
 
3 Sample Selection strategies 
In this section, we will study how to combine and 
strike a proper balance between these criteria, viz. 
informativeness, representativeness and diversity, 
to reach the maximum effectiveness on NER active 
learning.  We build two strategies to combine the 
measures proposed above.  These strategies are 
based on the varying priorities of the criteria and 
the varying degrees to satisfy the criteria. 
? Strategy 1: We first consider the informative-
ness criterion.  We choose m examples with the 
most informativeness score from NESet to an in-
termediate set called INTERSet.  By this pre-
selecting, we make the selection process faster in 
the later steps since the size of INTERSet is much 
smaller than that of NESet.  Then we cluster the 
examples in INTERSet and choose the centroid of 
each cluster into a batch called BatchSet.  The cen-
troid of a cluster is the most representative exam-
ple in that cluster since it has the largest density.  
Furthermore, the examples in different clusters 
may be considered diverse to each other.  By this 
means, we consider representativeness and diver-
sity criteria at the same time.  This strategy is 
shown in Figure 3.  One limitation of this strategy 
is that clustering result may not reflect the distribu-
tion of whole sample space since we only cluster 
on INTERSet for efficiency.  The other is that since 
the representativeness of an example is only evalu-
ated on a cluster.  If the cluster size is too small, 
the most representative example in this cluster may 
not be representative in the whole sample space. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
INTERSet with the maximal size M 
Steps :  
? BatchSet  = ?  
? INTERSet = ?  
? Select M entities with most Info score from NESet 
to INTERSet. 
? Cluster the entities in INTERSet into K clusters 
? Add the centroid entity of each cluster to BatchSet 
Figure 3: Sample Selection Strategy 1 
 
? Strategy 2: (Figure 4) We combine the infor-
mativeness and representativeness criteria  using 
the functio ( ) (1 ) ( )i iInfo NE Density NEl l+ - , in 
which the Info and Density  value of NEi are nor-
malized first.  The individual importance of each 
criterion in this function is adjusted by the trade-
off parameter l ( 0 1l? ? ) (set to 0.6 in our 
experiment).  First, we select a candidate example 
NEi with the maximum value of this function from 
NESet.  Second, we consider diversity criterion 
using the local method in Section 3.3.2.  We add 
the candidate example NEi to a batch only if NEi is 
different enough from any previously selected ex-
ample in the batch.  The threshold ? is set to the 
average pair-wise similarity of the entities in NE-
Set. 
 
Given: 
NESet = {NE1, ? , NEN} 
BatchSet with the maximal size K. 
Initialization:  
BatchSet  = ?  
Loop until BatchSet is full 
? Select NEi which have the maximum value for the 
combination function between Info score and Den-
sity socre from NESet. 
arg ( ( ) (1 ) ( ))
i
i i i
N E NESet
N E Max Info NE Density NEl l
?
= + -
 
? RepeatFlag = false; 
? Loop from j = 1 to CurrentSize(BatchSet)  
 If ( , )i jSim NE NE b? Then 
 RepeatFlag = true; 
 Stop the Loop; 
? If RepeatFlag == false Then 
add NEi into BatchSet 
? remove NEi from NESet 
Figure 4: Sample Selection Strategy 2 
 
4 Experimental Results and Analysis 
4.1 Experiment Settings  
In order to evaluate the effectiveness of our selec-
tion strategies, we apply them to recognize protein 
(PRT) names in biomedical domain using GENIA 
corpus V1.1 (Ohta et al 2002) and person (PER), 
location (LOC), organization (ORG) names in 
newswire domain using MUC-6 corpus.  First, we 
randomly split the whole corpus into three parts: an 
initial training set to build an in itial model, a test 
set to evaluate the performance of the model and 
an unlabeled set to select examples.  The size of 
each data set is shown in Table 1.  Then, iteratively, 
we select a batch of examples following the selec-
tion strategies proposed, require human experts to 
label them and add them into the training set.  The 
batch size K = 50 in GENIA and 10 in MUC-6.  
Each example is defined as a machine-recognized 
named entity and its context words (previous 3 
words and next 3 words). 
Domain Class Corpus Initial Training Set Test Set Unlabeled Set 
Biomedical PRT GENIA1.1 10 sent. (277 words) 900 sent. (26K words) 8004 sent. (223K words) 
PER 5 sent. (131 words) 7809 sent. (157K words) 
LOC 5 sent. (130 words) 7809 sent. (157K words) 
 
Newswire 
ORG 
 
MUC-6 
 5 sent. (113 words) 
 
602 sent. (14K words) 
 7809 sent. (157K words) 
Table 1: Experiment settings for active learning using GENIA1.1(PRT) and MUC-6(PER,LOC,ORG) 
The goal of our work is to minimize the human 
annotation effort to learn a named entity recognizer 
with the same performance level as supervised 
learning.  The performance of our model is evalu-
ated using ?precision/recall/F-measure?. 
4.2 Overall Result in GENIA and MUC-6 
In this section, we evaluate our selection strategies 
by comparing them with a random selection 
method, in which a batch of examples is randomly 
selected iteratively, on GENIA and MUC-6 corpus.  
Table 2 shows the amount of training data needed 
to achieve the performance of supervised learning 
using various selection methods, viz. Random, 
Strategy1 and Strategy2.  In GENIA, we find: 
? The model achieves 63.3 F-measure using 223K  
words in the supervised learning. 
? The best performer is Strategy2 (31K words), 
requiring less than 40% of the training data that 
Random (83K words) does and 14% of the train-
ing data that the supervised learning does. 
? Strategy1 (40K words) performs slightly worse 
than Strategy2, requiring 9K more words.  It is 
probably because Strategy1 cannot avoid select-
ing outliers if a cluster is too small. 
? Random (83K words) requires about 37% of the 
training data that the supervised learning does.  It 
indicates that only the words in and around a 
named entity are useful for classification and the 
words far from the named entity may not be 
helpful. 
 
Class Supervised Random Strategy1 Strategy2 
PRT 223K (F=63.3) 83K 40K 31K 
PER 157K (F=90.4) 11.5K 4.2K 3.5K 
LOC 157K (F=73.5) 13.6K 3.5K 2.1K 
ORG 157K (F=86.0) 20.2K 9.5K 7.8K 
Table 2: Overall Result in GENIA and MUC-6 
Furthermore, when we apply our model to news-
wire domain (MUC-6) to recognize person, loca-
tion and organization names, Strategy1 and 
Strategy2 show a more promising result by com-
paring with the supervised learning and Random, 
as shown in Table 2.  On average, about 95% of 
the data can be reduced to achieve the same per-
formance with the supervised learning in MUC-6.  
It is probably because NER in the newswire do-
main is much simpler than that in the biomedical 
domain (Shen et al 2003) and named entities are 
less and distributed much sparser in the newswire 
texts than in the biomedical texts. 
 
4.3 Effectiveness of Informativeness-based 
Selection Method 
In this section, we investigate the effectiveness of 
informativeness criterion in NER task.  Figure 5 
shows a plot of training data size versus F-measure 
achieved by the informativeness-based measures in 
Section 3.1.2: Info_Avg, Info_Min  and Info_S/N as 
well as Random.  We make the comparisons in 
GENIA corpus.  In Figure 5, the horizontal line is 
the performance level (63.3 F-measure) achieved 
by supervised learning (223K words).  We find 
that the three informativeness-based measures per-
form similarly and each of them outperforms Ran-
dom.  Table 3 highlights the various data sizes to 
achieve the peak performance using these selection 
methods.  We find that Random (83K words) on 
average requires over 1.5 times as much as data to 
achieve the same performance as the informative-
ness-based selection methods (52K words). 
 
0.5
0.55
0.6
0.65
0 20 40 60 80K words
F
Supervised
Random
Info_Min
Info_S/N
Info_Avg
 
Figure 5: Active learning curves: effectiveness of the three in-
formativeness-criterion-based selections comparing with the 
Random selection. 
Supervised Random Info_Avg Info_Min Info_ S/N 
223K 83K 52.0K 51.9K 52.3K 
Table 3: Training data sizes for various selection methods to 
achieve the same performance level as the supervised learning 
 
4.4 Effectiveness of Two Sample Selection 
Strategies 
In addition to the informativeness criterion, we 
further incorporate representativeness and diversity 
criteria into active learning using two strategies 
described in Section 3.  Comparing the two strate-
gies with the best result of the single-criterion-
based selection methods Info_Min , we are to jus-
tify that representativeness and diversity are also 
important factors for active learning.  Figure 6 
shows the learning curves for the various methods: 
Strategy1, Strategy2 and Info_Min.  In the begin-
ning iterations (F-measure < 60), the three methods 
performed similarly.  But with the larger training 
set, the efficiencies of Stratety1 and Strategy2 be-
gin to be evident.  Table 4 highlights the final re-
sult of the three methods.  In order to reach the 
performance of supervised learning, Strategy1 
(40K words) and Strategyy2 (31K words) require 
about 80% and 60% of the data that Info_Min 
(51.9K) does.  So we believe the effective combi-
nations of informativeness, representativeness and 
diversity will help to learn the NER model more 
quickly and cost less in annotation. 
0.5
0.55
0.6
0.65
0 20 40 60 K words
F
Supervised
Info_Min
Strategy1
Strategy2
 
Figure 6: Active learning curves: effectiveness of the two 
multi-criteria-based selection strategies comparing with the 
informativeness-criterion-based selection (Info_Min). 
Info_Min Strategy1 Strategy2 
51.9K 40K 31K 
Table 4: Comparisons of training data sizes for the multi-
criteria-based selection strategies and the informativeness-
criterion-based selection (Info_Min) to achieve the same per-
formance level as the supervised learning. 
 
5 Related Work 
Since there is no study on active learning for NER 
task previously, we only introduce general active 
learning methods here.  Many existing active learn-
ing methods are to select the most uncertain exam-
ples using various measures (Thompson et al 1999; 
Schohn and Cohn 2000; Tong and Koller 2000; 
Engelson and Dagan 1999; Ngai and Yarowsky 
2000).  Our informativeness-based measure is 
similar to these works.  However these works just 
follow a single criterion.  (McCallum and Nigam 
1998; Tang et al 2002) are the only two works 
considering the representativeness criterion in ac-
tive learning.  (Tang et al 2002) use the density 
information to weight the selected examples while 
we use it to select examples.  Moreover, the repre-
sentativeness measure we use is relatively general 
and easy to adapt to other tasks, in which the ex-
ample selected is a sequence of words, such as text 
chunking, POS tagging, etc.  On the other hand, 
(Brinker 2003) first incorporate diversity in active 
learning for text classification.  Their work is simi-
lar to our local consideration in Section 2.3.2.  
However, he didn?t further explore how to avoid 
selecting outliers to a batch.  So far, we haven?t 
found any previous work integrating the informa-
tiveness, representativeness and diversity all to-
gether. 
 
6 Conclusion and Future Work 
In this paper, we study the active learning in a 
more complex NLP task, named entity recognition.  
We propose a multi-criteria -based approach to se-
lect examples based on their informativeness, rep-
resentativeness and diversity, which are 
incorporated all together by two strategies (local 
and global).  Experiments show that, in both MUC-
6 and GENIA, both of the two strategies combin-
ing the three criteria outperform the single criterion 
(informativeness).  The labeling cost can be sig-
nificantly reduced by at least 80% comparing with 
the supervised learning.  To our best knowledge, 
this is not only the first work to report the empiri-
cal results of active learning for NER, but also the 
first work to incorporate the three criteria all to-
gether for selecting examples. 
Although the current experiment results are 
very promising, some parameters in our experi-
ment, such as the batch size K and the l in the 
function of strategy 2, are decided by our experi-
ence in the domain.  In practical application, the 
optimal value of these parameters should be de-
cided automatically based on the training process.  
Furthermore, we will study how to overcome the 
limitation of the strategy 1 discussed in Section 3 
by using more effective clustering algorithm.  An-
other interesting work is to study when to stop ac-
tive learning.  
 
References 
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Mod-
ern Information Retrieval. ISBN 0-201-39829-X. 
K. Brinker. 2003. Incorporating Diversity in Ac-
tive Learning with Support Vector Machines. In 
Proceedings of ICML, 2003. 
S. A. Engelson and I. Dagan. 1999. Committee-
Based Sample Selection for Probabilistic Classi-
fiers. Journal of Artifical Intelligence Research. 
F. Jelinek. 1997. Statistical Methods for Speech 
Recognition. MIT Press. 
J. Kazama, T. Makino, Y. Ohta and J. Tsujii. 2002. 
Tuning Support Vector Machines for Biomedi-
cal Named Entity Recognition. In Proceedings 
of the ACL2002 Workshop on NLP in Biomedi-
cine. 
K. J. Lee, Y. S. Hwang and H. C. Rim. 2003. Two-
Phase Biomedical NE Recognition based on 
SVMs.  In Proceedings of the ACL2003 Work-
shop on NLP in Biomedicine. 
D. D. Lewis and J. Catlett. 1994. Heterogeneous 
Uncertainty Sampling for Supervised Learning. 
In Proceedings of ICML, 1994. 
A. McCallum and K. Nigam. 1998. Employing EM 
in Pool-Based Active Learning for Text Classi-
fication. In Proceedings of ICML, 1998. 
G. Ngai and D. Yarowsky. 2000. Rule Writing or 
Annotation: Cost-efficient Resource Usage for 
Base Noun Phrase Chunking. In Proceedings of 
ACL, 2000. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima and J. Tsujii. 
2002. The GENIA corpus: An annotated re-
search abstract corpus in molecular biology do-
main. In Proceedings of HLT 2002. 
L. R. Rabiner, A. E. Rosenberg and S. E. Levinson. 
1978. Considerations in Dynamic Time Warping 
Algorithms for Discrete Word Recognition.  In 
Proceedings of IEEE Transactions on acoustics, 
speech and signal processing. Vol. ASSP-26, 
NO.6. 
D. Schohn and D. Cohn. 2000. Less is More: Ac-
tive Learning with Support Vector Machines. In 
Proceedings of the 17th International Confer-
ence on Machine Learning. 
D. Shen, J. Zhang, G. D. Zhou, J. Su and C. L. Tan. 
2003. Effective Adaptation of a Hidden Markov 
Model-based Named Entity Recognizer for Bio-
medical Domain. In Proceedings of the 
ACL2003 Workshop on NLP in Biomedicine. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. 
Sarkar, J. Hockenmaier, P. Ruhlen, S. Baker and 
J. Crim. 2003. Example Selection for Bootstrap-
ping Statistical Parsers. In Proceedings of HLT-
NAACL, 2003. 
M. Tang, X. Luo and S. Roukos. 2002. Active 
Learning for Statistical Natural Language Pars-
ing. In Proceedings of the ACL 2002. 
C. A. Thompson, M. E. Califf and R. J. Mooney. 
1999. Active Learning for Natural Language 
Parsing and Information Extraction. In Proceed-
ings of ICML 1999. 
S. Tong and D. Koller. 2000. Support Vector Ma-
chine Active Learning with Applications to Text 
Classification. Journal of Machine Learning Re-
search. 
V. Vapnik. 1998. Statistical learning theory. 
N.Y.:John Wiley. 
 
Proceedings of the 43rd Annual Meeting of the ACL, pages 165?172,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Improving Pronoun Resolution Using Statistics-Based
Semantic Compatibility Information
Xiaofeng Yang?? Jian Su? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
{yangxiao,tancl}@comp.nus.edu.sg
Abstract
In this paper we focus on how to improve
pronoun resolution using the statistics-
based semantic compatibility information.
We investigate two unexplored issues that
influence the effectiveness of such in-
formation: statistics source and learning
framework. Specifically, we for the first
time propose to utilize the web and the
twin-candidate model, in addition to the
previous combination of the corpus and
the single-candidate model, to compute
and apply the semantic information. Our
study shows that the semantic compatibil-
ity obtained from the web can be effec-
tively incorporated in the twin-candidate
learning model and significantly improve
the resolution of neutral pronouns.
1 Introduction
Semantic compatibility is an important factor for
pronoun resolution. Since pronouns, especially neu-
tral pronouns, carry little semantics of their own,
the compatibility between an anaphor and its an-
tecedent candidate is commonly evaluated by ex-
amining the relationships between the candidate and
the anaphor?s context, based on the statistics that the
corresponding predicate-argument tuples occur in a
particular large corpus. Consider the example given
in the work of Dagan and Itai (1990):
(1) They know full well that companies held tax
money aside for collection later on the basis
that the government said it1 was going to col-
lect it2.
For anaphor it1, the candidate government should
have higher semantic compatibility than money be-
cause government collect is supposed to occur more
frequently than money collect in a large corpus. A
similar pattern could also be observed for it2.
So far, the corpus-based semantic knowledge has
been successfully employed in several anaphora res-
olution systems. Dagan and Itai (1990) proposed
a heuristics-based approach to pronoun resolu-
tion. It determined the preference of candidates
based on predicate-argument frequencies. Recently,
Bean and Riloff (2004) presented an unsupervised
approach to coreference resolution, which mined
the co-referring NP pairs with similar predicate-
arguments from a large corpus using a bootstrapping
method.
However, the utility of the corpus-based se-
mantics for pronoun resolution is often argued.
Kehler et al (2004), for example, explored the
usage of the corpus-based statistics in supervised
learning based systems, and found that such infor-
mation did not produce apparent improvement for
the overall pronoun resolution. Indeed, existing
learning-based approaches to anaphor resolution
have performed reasonably well using limited
and shallow knowledge (e.g., Mitkov (1998),
Soon et al (2001), Strube and Muller (2003)).
Could the relatively noisy semantic knowledge give
us further system improvement?
In this paper we focus on improving pronominal
anaphora resolution using automatically computed
semantic compatibility information. We propose to
enhance the utility of the statistics-based knowledge
from two aspects:
Statistics source. Corpus-based knowledge usu-
ally suffers from data sparseness problem. That is,
many predicate-argument tuples would be unseen
even in a large corpus. A possible solution is the
165
web. It is believed that the size of the web is thou-
sands of times larger than normal large corpora, and
the counts obtained from the web are highly corre-
lated with the counts from large balanced corpora
for predicate-argument bi-grams (Keller and Lapata,
2003). So far the web has been utilized in nominal
anaphora resolution (Modjeska et al, 2003; Poesio
et al, 2004) to determine the semantic relation be-
tween an anaphor and candidate pair. However, to
our knowledge, using the web to help pronoun reso-
lution still remains unexplored.
Learning framework. Commonly, the predicate-
argument statistics is incorporated into anaphora res-
olution systems as a feature. What kind of learn-
ing framework is suitable for this feature? Previous
approaches to anaphora resolution adopt the single-
candidate model, in which the resolution is done on
an anaphor and one candidate at a time (Soon et al,
2001; Ng and Cardie, 2002). However, as the pur-
pose of the predicate-argument statistics is to eval-
uate the preference of the candidates in semantics,
it is possible that the statistics-based semantic fea-
ture could be more effectively applied in the twin-
candidate (Yang et al, 2003) that focusses on the
preference relationships among candidates.
In our work we explore the acquisition of the se-
mantic compatibility information from the corpus
and the web, and the incorporation of such semantic
information in the single-candidate model and the
twin-candidate model. We systematically evaluate
the combinations of different statistics sources and
learning frameworks in terms of their effectiveness
in helping the resolution. Results on the MUC data
set show that for neutral pronoun resolution in which
an anaphor has no specific semantic category, the
web-based semantic information would be the most
effective when applied in the twin-candidate model:
Not only could such a system significantly improve
the baseline without the semantic feature, it also out-
performs the system with the combination of the cor-
pus and the single-candidate model (by 11.5% suc-
cess).
The rest of this paper is organized as follows. Sec-
tion 2 describes the acquisition of the semantic com-
patibility information from the corpus and the web.
Section 3 discusses the application of the statistics
in the single-candidate and twin-candidate learning
models. Section 4 gives the experimental results,
and finally, Section 5 gives the conclusion.
2 Computing the Statistics-based Semantic
Compatibility
In this section, we introduce in detail how to com-
pute the semantic compatibility, using the predicate-
argument statistics obtained from the corpus or the
web.
2.1 Corpus-Based Semantic Compatibility
Three relationships, possessive-noun, subject-verb
and verb-object, are considered in our work. Be-
fore resolution a large corpus is prepared. Doc-
uments in the corpus are processed by a shallow
parser that could generate predicate-argument tuples
of the above three relationships1.
To reduce data sparseness, the following steps are
applied in each resulting tuple, automatically:
? Only the nominal or verbal heads are retained.
? Each Named-Entity (NE) is replaced by a com-
mon noun which corresponds to the seman-
tic category of the NE (e.g. ?IBM? ? ?com-
pany?) 2.
? All words are changed to their base morpho-
logic forms (e.g. ?companies ? company?).
During resolution, for an encountered anaphor,
each of its antecedent candidates is substituted with
the anaphor . According to the role and type of the
anaphor in its context, a predicate-argument tuple is
extracted and the above three steps for data-sparse
reduction are applied. Consider the sentence (1),
for example. The anaphors ?it1? and ?it2? indicate
a subject verb and verb object relationship, respec-
tively. Thus, the predicate-argument tuples for the
two candidates ?government? and ?money? would
be (collect (subject government)) and (collect (sub-
ject money)) for ?it1?, and (collect (object govern-
ment)) and (collect (object money)) for ?it2?.
Each extracted tuple is searched in the prepared
tuples set of the corpus, and the times the tuple oc-
curs are calculated. For each candidate, its semantic
1The possessive-noun relationship involves the forms like
?NP2 of NP1? and ?NP1?s NP2?.
2In our study, the semantic category of a NE is identified
automatically by the pre-processing NE recognition component.
166
compatibility with the anaphor could be represented
simply in terms of frequency
StatSem(candi, ana) = count(candi, ana) (1)
where count(candi, ana) is the count of the tuple
formed by candi and ana, or alternatively, in terms
of conditional probability (P (candi, ana|candi)),
where the count of the tuple is divided by the count
of the single candidate in the corpus. That is
StatSem(candi, ana) = count(candi, ana)count(candi) (2)
In this way, the statistics would not bias candidates
that occur frequently in isolation.
2.2 Web-Based Semantic Compatibility
Unlike documents in normal corpora, web pages
could not be preprocessed to generate the predicate-
argument reserve. Instead, the predicate-argument
statistics has to be obtained via a web search engine
like Google and Altavista. For the three types of
predicate-argument relationships, queries are con-
structed in the forms of ?NPcandi VP? (for subject-
verb), ?VP NPcandi? (for verb-object), and ?NPcandi
?s NP? or ?NP of NPcandi? (for possessive-noun).
Consider the following sentence:
(2) Several experts suggested that IBM?s account-
ing grew much more liberal since the mid 1980s
as its business turned sour.
For the pronoun ?its? and the candidate ?IBM?, the
two generated queries are ?business of IBM? and
?IBM?s business?.
To reduce data sparseness, in an initial query only
the nominal or verbal heads are retained. Also, each
NE is replaced by the corresponding common noun.
(e.g, ?IBM?s business? ? ?company?s business? and
?business of IBM? ? ?business of company?).
A set of inflected queries is generated by ex-
panding a term into all its possible morphologi-
cal forms. For example, in Sentence (1), ?collect
money? becomes ?collected|collecting|... money?,
and in (2) ?business of company? becomes ?business
of company|companies?). Besides, determiners are
inserted for every noun. If the noun is the candidate
under consideration, only the definite article the is
inserted. For other nouns, instead, a/an, the and the
empty determiners (for bare plurals) would be added
(e.g., ?the|a business of the company|companies?).
Queries are submitted to a particular web search
engine (Google in our study). All queries are per-
formed as exact matching. Similar to the corpus-
based statistics, the compatibility for each candidate
and anaphor pair could be represented using either
frequency (Eq. 1) or probability (Eq. 2) metric. In
such a situation, count(candi, ana) is the hit num-
ber of the inflected queries returned by the search
engine, while count(candi) is the hit number of the
query formed with only the head of the candidate
(i.e.,?the + candi?).
3 Applying the Semantic Compatibility
In this section, we discuss how to incorporate the
statistics-based semantic compatibility for pronoun
resolution, in a machine learning framework.
3.1 The Single-Candidate Model
One way to utilize the semantic compatibility is to
take it as a feature under the single-candidate learn-
ing model as employed by Ng and Cardie (2002).
In such a learning model, each training or testing
instance takes the form of i{C, ana}, where ana is
the possible anaphor and C is its antecedent candi-
date. An instance is associated with a feature vector
to describe their relationships.
During training, for each anaphor in a given text,
a positive instance is created by pairing the anaphor
and its closest antecedent. Also a set of negative in-
stances is formed by pairing the anaphor and each
of the intervening candidates. Based on the train-
ing instances, a binary classifier is generated using a
certain learning algorithm, like C5 (Quinlan, 1993)
in our work.
During resolution, given a new anaphor, a test in-
stance is created for each candidate. This instance is
presented to the classifier, which then returns a pos-
itive or negative result with a confidence value indi-
cating the likelihood that they are co-referent. The
candidate with the highest confidence value would
be selected as the antecedent.
3.2 Features
In our study we only consider those domain-
independent features that could be obtained with low
167
Feature Description
DefNp 1 if the candidate is a definite NP; else 0
Pron 1 if the candidate is a pronoun; else 0
NE 1 if the candidate is a named entity; else 0
SameSent 1 if the candidate and the anaphor is in the same sentence; else 0
NearestNP 1 if the candidate is nearest to the anaphor; else 0
ParalStuct 1 if the candidate has an parallel structure with ana; else 0
FirstNP 1 if the candidate is the first NP in a sentence; else 0
Reflexive 1 if the anaphor is a reflexive pronoun; else 0
Type Type of the anaphor (0: Single neuter pronoun; 1: Plural neuter pronoun; 2:
Male personal pronoun; 3: Female personal pronoun)
StatSem? the statistics-base semantic compatibility of the candidate
SemMag?? the semantic compatibility difference between two competing candidates
Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model
while **ed feature is only for the twin-candidate mode)
computational cost but with high reliability. Table 1
summarizes the features with their respective possi-
ble values. The first three features represent the lex-
ical properties of a candidate. The POS properties
could indicate whether a candidate refers to a hearer-
old entity that would have a higher preference to be
selected as the antecedent (Strube, 1998). SameSent
and NearestNP mark the distance relationships be-
tween an anaphor and the candidate, which would
significantly affect the candidate selection (Hobbs,
1978). FirstNP aims to capture the salience of the
candidate in the local discourse segment. ParalStuct
marks whether a candidate and an anaphor have sim-
ilar surrounding words, which is also a salience fac-
tor for the candidate evaluation (Mitkov, 1998).
Feature StatSem records the statistics-based se-
mantic compatibility computed, from the corpus or
the web, by either frequency or probability metric,
as described in the previous section. If a candidate
is a pronoun, this feature value would be set to that
of its closest nominal antecedent.
As described, the semantic compatibility of a can-
didate is computed under the context of the cur-
rent anaphor. Consider two occurrences of anaphors
?. . . it1 collected . . . ? and ?. . . it2 said . . . ?. As ?NP
collected? should occur less frequently than ?NP
said?, the candidates of it1 would generally have
predicate-argument statistics lower than those of it2.
That is, a positive instance for it1 might bear a lower
semantic feature value than a negative instance for
it2. The consequence is that the learning algorithm
would think such a feature is not that ?indicative?
and reduce its salience in the resulting classifier.
One way to tackle this problem is to normalize the
feature by the frequencies of the anaphor?s context,
e.g., ?count(collected)? and ?count(said)?. This,
however, would require extra calculation. In fact,
as candidates of a specific anaphor share the same
anaphor context, we can just normalize the semantic
feature of a candidate by that of its competitor:
StatSemN (C, ana) = StatSem(C, ana)max
ci?candi set(ana)
StatSem(ci, ana)
The value (0 ? 1) represents the rank of the
semantic compatibility of the candidate C among
candi set(ana), the current candidates of ana.
3.3 The Twin-Candidate Model
Yang et al (2003) proposed an alternative twin-
candidate model for anaphora resolution task. The
strength of such a model is that unlike the single-
candidate model, it could capture the preference re-
lationships between competing candidates. In the
model, candidates for an anaphor are paired and
features from two competing candidates are put to-
gether for consideration. This property could nicely
deal with the above mentioned training problem of
different anaphor contexts, because the semantic
feature would be considered under the current can-
didate set only. In fact, as semantic compatibility is
168
a preference-based factor for anaphor resolution, it
would be incorporated in the twin-candidate model
more naturally.
In the twin-candidate model, an instance takes a
form like i{C1, C2, ana}, where C1 and C2 are two
candidates. We stipulate that C2 should be closer to
ana than C1 in distance. The instance is labelled as
?10? if C1 the antecedent, or ?01? if C2 is.
During training, for each anaphor, we find its
closest antecedent, Cante. A set of ?10? instances,
i{Cante, C, ana}, is generated by pairing Cante and
each of the interning candidates C. Also a set of ?01?
instances, i{C, Cante, ana}, is created by pairing
Cante with each candidate before Cante until another
antecedent, if any, is reached.
The resulting pairwise classifier would return
?10? or ?01? indicating which candidate is preferred
to the other. During resolution, candidates are paired
one by one. The score of a candidate is the total
number of the competitors that the candidate wins
over. The candidate with the highest score would be
selected as the antecedent.
Features The features for the twin-candidate
model are similar to those for the single-candidate
model except that a duplicate set of features has to
be prepared for the additional candidate. Besides,
a new feature, SemMag, is used in place of Stat-
Sem to represent the difference magnitude between
the semantic compatibility of two candidates. Let
mag = StatSem(C1, ana)/StatSem(C2, ana), feature
SemMag is defined as follows,
SemMag(C1, C2, ana) =
{
mag ? 1 : mag >= 1
1?mag?1 : mag < 1
The positive or negative value marks the times that
the statistics of C1 is larger or smaller than C2.
4 Evaluation and Discussion
4.1 Experiment Setup
In our study we were only concerned about the third-
person pronoun resolution. With an attempt to ex-
amine the effectiveness of the semantic feature on
different types of pronouns, the whole resolution
was divided into neutral pronoun (it & they) reso-
lution and personal pronoun (he & she) resolution.
The experiments were done on the newswire do-
main, using MUC corpus (Wall Street Journal ar-
ticles). The training was done on 150 documents
from MUC-6 coreference data set, while the testing
was on the 50 formal-test documents of MUC-6 (30)
and MUC-7 (20). Throughout the experiments, de-
fault learning parameters were applied to the C5 al-
gorithm. The performance was evaluated based on
success, the ratio of the number of correctly resolved
anaphors over the total number of anaphors.
An input raw text was preprocessed automati-
cally by a pipeline of NLP components. The noun
phrase identification and the predicate-argument ex-
traction were done based on the results of a chunk
tagger, which was trained for the shared task of
CoNLL-2000 and achieved 92% accuracy (Zhou et
al., 2000). The recognition of NEs as well as their
semantic categories was done by a HMM based
NER, which was trained for the MUC NE task
and obtained high F-scores of 96.9% (MUC-6) and
94.3% (MUC-7) (Zhou and Su, 2002).
For each anaphor, the markables occurring within
the current and previous two sentences were taken
as the initial candidates. Those with mismatched
number and gender agreements were filtered from
the candidate set. Also, pronouns or NEs that dis-
agreed in person with the anaphor were removed in
advance. For the training set, there are totally 645
neutral pronouns and 385 personal pronouns with
non-empty candidate set, while for the testing set,
the number is 245 and 197.
4.2 The Corpus and the Web
The corpus for the predicate-argument statistics
computation was from the TIPSTER?s Text Re-
search Collection (v1994). Consisting of 173,252
Wall Street Journal articles from the year 1988 to
1992, the data set contained about 76 million words.
The documents were preprocessed using the same
POS tagging and NE-recognition components as in
the pronoun resolution task. Cass (Abney, 1996), a
robust chunker parser was then applied to generate
the shallow parse trees, which resulted in 353,085
possessive-noun tuples, 759,997 verb-object tuples
and 1,090,121 subject-verb tuples.
We examined the capacity of the web and the
corpus in terms of zero-count ratio and count num-
ber. On average, among the predicate-argument tu-
ples that have non-zero corpus-counts, above 93%
have also non-zero web-counts. But the ratio is only
around 40% contrariwise. And for the predicate-
169
Neutral Pron Personal Pron Overall
Learning Model System Corpus Web Corpus Web Corpus Web
baseline 65.7 86.8 75.1
+frequency 67.3 69.9 86.8 86.8 76.0 76.9
Single-Candidate +normalized frequency 66.9 67.8 86.8 86.8 75.8 76.2
+probability 65.7 65.7 86.8 86.8 75.1 75.1
+normalized probability 67.7 70.6 86.8 86.8 76.2 77.8
baseline 73.9 91.9 81.9
Twin-Candidate +frequency 76.7 79.2 91.4 91.9 83.3 84.8
+probability 75.9 78.0 91.4 92.4 82.8 84.4
Table 2: The performance of different resolution systems
Relationship N-Pron P-Pron
Possessive-Noun 0.508 0.517
Verb-Object 0.503 0.526
Subject-Verb 0.619 0.676
Table 3: Correlation between web and corpus counts
on the seen predicate-argument tuples
argument tuples that could be seen in both data
sources, the count from the web is above 2000 times
larger than that from the corpus.
Although much less sparse, the web counts are
significantly noisier than the corpus count since no
tagging, chunking and parsing could be carried out
on the web pages. However, previous study (Keller
and Lapata, 2003) reveals that the large amount of
data available for the web counts could outweigh the
noisy problems. In our study we also carried out a
correlation analysis3 to examine whether the counts
from the web and the corpus are linearly related,
on the predicate-argument tuples that can be seen
in both data sources. From the results listed in Ta-
ble 3, we observe moderately high correlation, with
coefficients ranging from 0.5 to 0.7 around, between
the counts from the web and the corpus, for both
neutral pronoun (N-Pron) and personal pronoun (P-
Pron) resolution tasks.
4.3 System Evaluation
Table 2 summarizes the performance of the systems
with different combinations of statistics sources and
learning frameworks. The systems without the se-
3All the counts were log-transformed and the correlation co-
efficients were evaluated based on Pearsons? r.
mantic feature were used as the baseline. Under the
single-candidate (SC) model, the baseline system
obtains a success of 65.7% and 86.8% for neutral
pronoun and personal pronoun resolution, respec-
tively. By contrast, the twin-candidate (TC) model
achieves a significantly (p ? 0.05, by two-tailed t-
test) higher success of 73.9% and 91.9%, respec-
tively. Overall, for the whole pronoun resolution,
the baseline system under the TC model yields a
success 81.9%, 6.8% higher than SC does4. The
performance is comparable to most state-of-the-art
pronoun resolution systems on the same data set.
Web-based feature vs. Corpus-based feature
The third column of the table lists the results us-
ing the web-based compatibility feature for neutral
pronouns. Under both SC and TC models, incorpo-
ration of the web-based feature significantly boosts
the performance of the baseline: For the best sys-
tem in the SC model and the TC model, the success
rate is improved significantly by around 4.9% and
5.3%, respectively. A similar pattern of improve-
ment could be seen for the corpus-based semantic
feature. However, the increase is not as large as
using the web-based feature: Under the two learn-
ing models, the success rate of the best system with
the corpus-based feature rises by up to 2.0% and
2.8% respectively, about 2.9% and 2.5% less than
that of the counterpart systems with the web-based
feature. The larger size and the better counts of the
web against the corpus, as reported in Section 4.2,
4The improvement against SC is higher than that reported
in (Yang et al, 2003). It should be because we now used 150
training documents rather than 30 ones as in the previous work.
The TC model would benefit from larger training data set as it
uses more features (more than double) than SC.
170
should contribute to the better performance.
Single-candidate model vs. Twin-Candidate
model The difference between the SC and the TC
model is obvious from the table. For the N-Pron
and P-Pron resolution, the systems under TC could
outperform the counterpart systems under SC by
above 5% and 8% success, respectively. In addition,
the utility of the statistics-based semantic feature is
more salient under TC than under SC for N-Pron res-
olution: the best gains using the corpus-based and
the web-based semantic features under TC are 2.9%
and 5.3% respectively, higher than those under the
SC model using either un-normalized semantic fea-
tures (1.6% and 3.3%), or normalized semantic fea-
tures (2.0% and 4.9%). Although under SC, the nor-
malized semantic feature could result in a gain close
to under TC, its utility is not stable: with metric fre-
quency, using the normalized feature performs even
worse than using the un-normalized one. These re-
sults not only affirm the claim by Yang et al (2003)
that the TC model is superior to the SC model for
pronoun resolution, but also indicate that TC is more
reliable than SC in applying the statistics-based se-
mantic feature, for N-Pron resolution.
Web+TC vs. Other combinations The above
analysis has exhibited the superiority of the web
over the corpus, and the TC model over the
SC model. The experimental results also re-
veal that using the the web-based semantic fea-
ture together with the TC model is able to further
boost the resolution performance for neutral pro-
nouns. The system with such a Web+TC combi-
nation could achieve a high success of 79.2%, de-
feating all the other possible combinations. Es-
pecially, it considerably outperforms (up to 11.5%
success) the system with the Corpus+SC combina-
tion, which is commonly adopted in previous work
(e.g., Kehler et al (2004)).
Personal pronoun resolution vs. Neutral pro-
noun resolution Interestingly, the statistics-based
semantic feature has no effect on the resolution of
personal pronouns, as shown in the table 2. We
found in the learned decision trees such a feature
did not occur (SC) or only occurred in bottom nodes
(TC). This should be because personal pronouns
have strong restriction on the semantic category (i.e.,
human) of the candidates. A non-human candidate,
even with a high predicate-argument statistics, could
Feature Group Isolated Combined
SemMag (Web-based) 61.2 61.2
Type+Reflexive 53.1 61.2
ParaStruct 53.1 61.2
Pron+DefNP+InDefNP+NE 57.1 67.8
NearestNP+SameSent 53.1 70.2
FirstNP 65.3 79.2
Table 4: Results of different feature groups under
the TC model for N-pron resolution
SameSent_1 = 0:
:..SemMag > 0:
: :..Pron_2 = 0: 10 (200/23)
: : Pron_2 = 1: ...
: SemMag <= 0:
: :..Pron_2 = 1: 01 (75/1)
: Pron_2 = 0:
: :..SemMag <= -28: 01 (110/19)
: SemMag > -28: ...
SameSent_1 = 1:
:..SameSent_2 = 0: 01 (1655/49)
SameSent_2 = 1:
:..FirstNP_2 = 1: 01 (104/1)
FirstNP_2 = 0:
:..ParaStruct_2 = 1: 01 (3)
ParaStruct_2 = 0:
:..SemMag <= -151: 01 (27/2)
SemMag > -151:...
Figure 1: Top portion of the decision tree learned
under TC model for N-pron resolution (features ended
with ? 1? are for the first candidate C1 and those with ? 2? are
for C2.)
not be used as the antecedent (e.g. company said in
the sentence ?. . . the company . . . he said . . . ?). In
fact, our analysis of the current data set reveals that
most P-Prons refer back to a P-Pron or NE candidate
whose semantic category (human) has been deter-
mined. That is, simply using features NE and Pron
is sufficient to guarantee a high success, and thus the
relatively weak semantic feature would not be taken
in the learned decision tree for resolution.
4.4 Feature Analysis
In our experiment we were also concerned about the
importance of the web-based compatibility feature
(using frequency metric) among the feature set. For
this purpose, we divided the features into groups,
and then trained and tested on one group at a time.
Table 4 lists the feature groups and their respective
results for N-Pron resolution under the TC model.
171
The second column is for the systems with only the
current feature group, while the third column is with
the features combined with the existing feature set.
We see that used in isolation, the semantic compati-
bility feature is able to achieve a success up to 61%
around, just 4% lower than the best indicative fea-
ture FirstNP. In combination with other features, the
performance could be improved by as large as 18%
as opposed to being used alone.
Figure 1 shows the top portion of the pruned deci-
sion tree for N-Pron resolution under the TC model.
We could find that: (i) When comparing two can-
didates which occur in the same sentence as the
anaphor, the web-based semantic feature would be
examined in the first place, followed by the lexi-
cal property of the candidates. (ii) When two non-
pronominal candidates are both in previous sen-
tences before the anaphor, the web-based semantic
feature is still required to be examined after FirstNP
and ParaStruct. The decision tree further indicates
that the web-based feature plays an important role in
N-Pron resolution.
5 Conclusion
Our research focussed on improving pronoun reso-
lution using the statistics-based semantic compati-
bility information. We explored two issues that af-
fect the utility of the semantic information: statis-
tics source and learning framework. Specifically, we
proposed to utilize the web and the twin-candidate
model, in addition to the common combination of
the corpus and single-candidate model, to compute
and apply the semantic information.
Our experiments systematically evaluated differ-
ent combinations of statistics sources and learn-
ing models. The results on the newswire domain
showed that the web-based semantic compatibility
could be the most effectively incorporated in the
twin-candidate model for the neutral pronoun res-
olution. While the utility is not obvious for per-
sonal pronoun resolution, we can still see the im-
provement on the overall performance. We believe
that the semantic information under such a config-
uration would be even more effective on technical
domains where neutral pronouns take the majority
in the pronominal anaphors. Our future work would
have a deep exploration on such domains.
References
S. Abney. 1996. Partial parsing via finite-state cascades. In
Workshop on Robust Parsing, 8th European Summer School
in Logic, Language and Information, pages 8?15.
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proceed-
ings of 2004 North American chapter of the Association for
Computational Linguistics annual meeting.
I. Dagan and A. Itai. 1990. Automatic processing of large cor-
pora for the resolution of anahora references. In Proceedings
of the 13th International Conference on Computational Lin-
guistics, pages 330?332.
J. Hobbs. 1978. Resolving pronoun references. Lingua,
44:339?352.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004. The
(non)utility of predicate-argument frequencies for pronoun
interpretation. In Proceedings of 2004 North American
chapter of the Association for Computational Linguistics an-
nual meeting.
F. Keller and M. Lapata. 2003. Using the web to obtain
freqencies for unseen bigrams. Computational Linguistics,
29(3):459?484.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proceedings of the 17th Int. Conference on
Computational Linguistics, pages 869?875.
N. Modjeska, K. Markert, and M. Nissim. 2003. Using the web
in machine learning for other-anaphora resolution. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing, pages 176?183.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 104?111, Philadelphia.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceedings of
42th Annual Meeting of the Association for Computational
Linguistics.
J. R. Quinlan. 1993. C4.5: Programs for machine learning.
Morgan Kaufmann Publishers, San Francisco, CA.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521?544.
M. Strube and C. Muller. 2003. A machine learning approach
to pronoun resolution in spoken dialogue. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 168?175, Japan.
M. Strube. 1998. Never look back: An alternative to centering.
In Proceedings of the 17th Int. Conference on Computational
Linguistics and 36th Annual Meeting of ACL, pages 1251?
1257.
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference reso-
lution using competition learning approach. In Proceedings
of the 41st Annual Meeting of the Association for Computa-
tional Linguistics, Japan.
G. Zhou and J. Su. 2002. Named Entity recognition using a
HMM-based chunk tagger. In Proceedings of the 40th An-
nual Meeting of the Association for Computational Linguis-
tics, Philadelphia.
G. Zhou, J. Su, and T. Tey. 2000. Hybrid text chunking. In
Proceedings of the 4th Conference on Computational Natu-
ral Language Learning, pages 163?166, Lisbon, Portugal.
172
Proceedings of the 43rd Annual Meeting of the ACL, pages 427?434,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Exploring Various Knowledge in Relation Extraction 
 
ZHOU GuoDong   SU Jian  ZHANG Jie  ZHANG Min  
Institute for Infocomm research  
21 Heng Mui Keng Terrace, Singapore 119613  
Email: {zhougd, sujian, zhangjie, mzhang}@i2r.a-star.edu.sg  
  
Abstract 
Extracting semantic relationships between en-
tities is challenging. This paper investigates 
the incorporation of diverse lexical, syntactic 
and semantic knowledge in feature-based rela-
tion extraction using SVM. Our study illus-
trates that the base phrase chunking 
information is very effective for relation ex-
traction and contributes to most of the per-
formance improvement from syntactic aspect 
while additional information from full parsing 
gives limited further enhancement. This sug-
gests that most of useful information in full 
parse trees for relation extraction is shallow 
and can be captured by chunking. We also 
demonstrate how semantic information such as 
WordNet and Name List, can be used in fea-
ture-based relation extraction to further im-
prove the performance. Evaluation on the 
ACE corpus shows that effective incorporation 
of diverse features enables our system outper-
form previously best-reported systems on the 
24 ACE relation subtypes and significantly 
outperforms tree kernel-based systems by over 
20 in F-measure on the 5 ACE relation types. 
1 Introduction 
With the dramatic increase in the amount of textual 
information available in digital archives and the 
WWW, there has been growing interest in tech-
niques for automatically extracting information 
from text. Information Extraction (IE) systems are 
expected to identify relevant information (usually 
of pre-defined types) from text documents in a cer-
tain domain and put them in a structured format.  
According to the scope of the NIST Automatic 
Content Extraction (ACE) program, current 
research in IE has three main objectives: Entity 
Detection and Tracking (EDT), Relation Detection 
and Characterization (RDC), and Event Detection 
and Characterization (EDC). The EDT task entails 
the detection of entity mentions and chaining them 
together by identifying their coreference. In ACE 
vocabulary, entities are objects, mentions are 
references to them, and relations are semantic 
relationships between entities. Entities can be of 
five types: persons, organizations, locations, 
facilities and geo-political entities (GPE: 
geographically defined regions that indicate a 
political boundary, e.g. countries, states, cities, 
etc.). Mentions have three levels: names, nomial 
expressions or pronouns. The RDC task detects 
and classifies implicit and explicit relations1 
between entities identified by the EDT task. For 
example, we want to determine whether a person is 
at a location, based on the evidence in the context. 
Extraction of semantic relationships between 
entities can be very useful for applications such as 
question answering, e.g. to answer the query ?Who 
is the president of the United States??.  
This paper focuses on the ACE RDC task and 
employs diverse lexical, syntactic and semantic 
knowledge in feature-based relation extraction 
using Support Vector Machines (SVMs). Our 
study illustrates that the base phrase chunking 
information contributes to most of the performance 
inprovement from syntactic aspect while additional 
full parsing information does not contribute much, 
largely due to the fact that most of relations 
defined in ACE corpus are within a very short 
distance. We also demonstrate how semantic in-
formation such as WordNet (Miller 1990) and 
Name List can be used in the feature-based frame-
work. Evaluation shows that the incorporation of 
diverse features enables our system achieve best 
reported performance. It also shows that our fea-
                                                          
1 In ACE (http://www.ldc.upenn.edu/Projects/ACE), 
explicit relations occur in text with explicit evidence 
suggesting the relationships. Implicit relations need not 
have explicit supporting evidence in text, though they 
should be evident from a reading of the document.  
427
ture-based approach outperforms tree kernel-based 
approaches by 11 F-measure in relation detection 
and more than 20 F-measure in relation detection 
and classification on the 5 ACE relation types.  
The rest of this paper is organized as follows. 
Section 2 presents related work. Section 3 and 
Section 4 describe our approach and various 
features employed respectively. Finally, we present 
experimental setting and  results in Section 5 and 
conclude with some general observations in 
relation extraction in Section 6. 
2 Related Work 
The relation extraction task was formulated at the 
7th Message Understanding Conference (MUC-7 
1998) and is starting to be addressed more and 
more within the natural language processing and 
machine learning communities.  
Miller et al(2000) augmented syntactic full 
parse trees with semantic information correspond-
ing to entities and relations, and built generative 
models for the augmented trees. Zelenko et al
(2003) proposed extracting relations by computing 
kernel functions between parse trees. Culotta et al
(2004) extended this work to estimate kernel func-
tions between augmented dependency trees and 
achieved 63.2 F-measure in relation detection and 
45.8 F-measure in relation detection and classifica-
tion on the 5 ACE relation types. Kambhatla 
(2004) employed Maximum Entropy models for 
relation extraction with features derived from 
word, entity type, mention level, overlap, depend-
ency tree and parse tree. It achieves 52.8 F-
measure on the 24 ACE relation subtypes. Zhang 
(2004) approached relation classification by com-
bining various lexical and syntactic features with 
bootstrapping on top of Support Vector Machines. 
Tree kernel-based approaches proposed by Ze-
lenko et al(2003) and Culotta et al(2004) are able 
to explore the implicit feature space without much 
feature engineering. Yet further research work is 
still expected to make it effective with complicated 
relation extraction tasks such as the one defined in 
ACE. Complicated relation extraction tasks may 
also impose a big challenge to the modeling ap-
proach used by Miller et al(2000) which integrates 
various tasks such as part-of-speech tagging, 
named entity recognition, template element extrac-
tion and relation extraction, in a single model.   
This paper will further explore the feature-based 
approach with a systematic study on the extensive 
incorporation of diverse lexical, syntactic and se-
mantic information. Compared with Kambhatla 
(2004), we separately incorporate the base phrase 
chunking information, which contributes to most 
of the performance improvement from syntactic 
aspect. We also show how semantic information 
like WordNet and Name List can be equipped to 
further improve the performance. Evaluation on 
the ACE corpus shows that our system outper-
forms Kambhatla (2004) by about 3 F-measure on 
extracting 24 ACE relation subtypes. It also shows 
that our system outperforms tree kernel-based sys-
tems (Culotta et al2004) by over 20 F-measure on 
extracting 5 ACE relation types. 
3 Support Vector Machines 
Support Vector Machines (SVMs) are a supervised 
machine learning technique motivated by the sta-
tistical learning theory (Vapnik 1998). Based on 
the structural risk minimization of the statistical 
learning theory, SVMs seek an optimal separating 
hyper-plane to divide the training examples into 
two classes and make decisions based on support 
vectors which are selected as the only effective 
instances in the training set. 
Basically, SVMs are binary classifiers. 
Therefore, we must extend SVMs to multi-class 
(e.g. K) such as the ACE RDC task. For efficiency, 
we apply the one vs. others strategy, which builds 
K classifiers so as to separate one class from all 
others, instead of the pairwise strategy, which 
builds K*(K-1)/2 classifiers considering all pairs of 
classes. The final decision of an instance in the 
multiple binary classification is determined by the 
class which has the maximal SVM output. 
Moreover, we only apply the simple linear kernel, 
although other kernels can peform better.  
The reason why we choose SVMs for this 
purpose is that SVMs represent the state-of?the-art 
in  the machine learning research community, and 
there are good implementations of the algorithm 
available. In this paper, we use the binary-class 
SVMLight2 deleveloped by Joachims (1998). 
                                                          
2 Joachims has just released a new version of SVMLight 
for multi-class classification. However, this paper only 
uses the binary-class version. For details about 
SVMLight, please see http://svmlight.joachims.org/ 
428
4 Features 
The semantic relation is determined between two 
mentions. In addition, we distinguish the argument 
order of the two mentions (M1 for the first mention 
and M2 for the second mention), e.g. M1-Parent-
Of-M2 vs. M2-Parent-Of-M1. For each pair of 
mentions3, we compute various lexical, syntactic 
and semantic features. 
4.1 Words 
According to their positions, four categories of 
words are considered: 1) the words of both the 
mentions, 2) the words between the two mentions, 
3) the words before M1, and 4) the words after M2. 
For the words of both the mentions, we also differ-
entiate the head word4 of a mention from other 
words since the head word is generally much more 
important. The words between the two mentions 
are classified into three bins: the first word in be-
tween, the last word in between and other words in 
between. Both the words before M1 and after M2 
are classified into two bins: the first word next to 
the mention and the second word next to the men-
tion. Since a pronominal mention (especially neu-
tral pronoun such as ?it? and ?its?) contains little 
information about the sense of the mention, the co-
reference chain is used to decide its sense. This is 
done by replacing the pronominal mention with the 
most recent non-pronominal antecedent when de-
termining the word features, which include: 
? WM1: bag-of-words in M1 
? HM1: head word of M1 
                                                          
3 In ACE, each mention has a head annotation and an 
extent annotation. In all our experimentation, we only 
consider the word string between the beginning point of 
the extent annotation and the end point of the head an-
notation. This has an effect of choosing the base phrase 
contained in the extent annotation. In addition, this also 
can reduce noises without losing much of information in 
the mention. For example, in the case where the noun 
phrase ?the former CEO of McDonald? has the head 
annotation of ?CEO? and the extent annotation of ?the 
former CEO of McDonald?, we only consider ?the for-
mer CEO? in this paper. 
4 In this paper, the head word of a mention is normally 
set as the last word of the mention. However, when a 
preposition exists in the mention, its head word is set as 
the last word before the preposition. For example, the 
head word of the name mention ?University of Michi-
gan? is ?University?. 
? WM2: bag-of-words in M2 
? HM2: head word of M2 
? HM12: combination of HM1 and HM2 
? WBNULL: when no word in between 
? WBFL: the only word in between when only 
one word in between 
? WBF: first word in between when at least two 
words in between 
? WBL: last word in between when at least two 
words in between 
? WBO: other words in between except first and 
last words when at least three words in between 
? BM1F: first word before M1 
? BM1L: second word before M1 
? AM2F: first word after M2 
? AM2L: second word after M2 
4.2 Entity Type 
This feature concerns about the entity type of both 
the mentions, which can be PERSON, 
ORGANIZATION, FACILITY, LOCATION and 
Geo-Political Entity or GPE: 
? ET12: combination of mention entity types 
4.3 Mention Level 
This feature considers the entity level of both the 
mentions, which can be NAME, NOMIAL and 
PRONOUN: 
? ML12: combination of mention levels 
4.4 Overlap 
This category of features includes: 
? #MB: number of other mentions in between 
? #WB: number of words in between 
? M1>M2 or M1<M2: flag indicating whether 
M2/M1is included in M1/M2.  
Normally, the above overlap features are too 
general to be effective alone. Therefore, they are 
also combined with other features: 1) 
ET12+M1>M2; 2) ET12+M1<M2; 3) 
HM12+M1>M2; 4) HM12+M1<M2. 
4.5 Base Phrase Chunking 
It is well known that chunking plays a critical role 
in the Template Relation task of the 7th Message 
Understanding Conference (MUC-7 1998). The 
related work mentioned in Section 2 extended to 
explore the information embedded in the full parse 
trees. In this paper, we separate the features of base 
429
phrase chunking from those of full parsing. In this 
way, we can separately evaluate the contributions 
of base phrase chunking and full parsing. Here, the 
base phrase chunks are derived from full parse 
trees using the Perl script5 written by Sabine 
Buchholz from Tilburg University and the Collins? 
parser (Collins 1999) is employed for full parsing. 
Most of the chunking features concern about the 
head words of the phrases between the two men-
tions. Similar to word features, three categories of 
phrase heads are considered: 1) the phrase heads in 
between are also classified into three bins: the first 
phrase head in between, the last phrase head in 
between and other phrase heads in between; 2) the 
phrase heads before M1 are classified into two 
bins: the first phrase head before and the second 
phrase head before; 3) the phrase heads after M2 
are classified into two bins: the first phrase head 
after and the second phrase head after. Moreover, 
we also consider the phrase path in between. 
? CPHBNULL when no phrase in between 
? CPHBFL: the only phrase head when only one 
phrase in between 
? CPHBF: first phrase head in between when at 
least two phrases in between 
? CPHBL: last phrase head in between when at 
least two phrase heads in between 
? CPHBO: other phrase heads in between except 
first and last phrase heads when at least three 
phrases in between 
? CPHBM1F: first phrase head before M1 
? CPHBM1L: second phrase head before M1 
? CPHAM2F: first phrase head after M2 
? CPHAM2F: second phrase head after M2 
? CPP: path of phrase labels connecting the two 
mentions in the chunking  
? CPPH: path of phrase labels connecting the two 
mentions in the chunking augmented with head 
words, if at most two phrases in between 
4.6 Dependency Tree 
This category of features includes information 
about the words, part-of-speeches and phrase la-
bels of the words on which the mentions are de-
pendent in the dependency tree derived from the 
syntactic full parse tree. The dependency tree is 
built by using the phrase head information returned 
by the Collins? parser and linking all the other 
                                                          
5 http://ilk.kub.nl/~sabine/chunklink/ 
fragments in a phrase to its head. It also includes 
flags indicating whether the two mentions are in 
the same NP/PP/VP. 
? ET1DW1: combination of the entity type and 
the dependent word for M1 
? H1DW1: combination of the head word and the 
dependent word for M1 
? ET2DW2: combination of the entity type and 
the dependent word for M2 
? H2DW2: combination of the head word and the 
dependent word for M2 
? ET12SameNP: combination of ET12 and 
whether M1 and M2 included in the same NP 
? ET12SamePP: combination of ET12 and 
whether M1 and M2 exist in the same PP 
? ET12SameVP: combination of ET12 and 
whether M1 and M2 included in the same VP 
4.7 Parse Tree 
This category of features concerns about the in-
formation inherent only in the full parse tree.  
? PTP: path of phrase labels (removing dupli-
cates) connecting M1 and M2 in the parse tree  
? PTPH: path of phrase labels (removing dupli-
cates) connecting M1 and M2 in the parse tree 
augmented with the head word of the top phrase 
in the path.  
4.8 Semantic Resources 
Semantic information from various resources, such 
as WordNet, is used to classify important words 
into different semantic lists according to their indi-
cating relationships. 
Country Name List 
This is to differentiate the relation subtype 
?ROLE.Citizen-Of?, which defines the relationship 
between a person and the country of the person?s 
citizenship, from other subtypes, especially 
?ROLE.Residence?, where defines the relationship 
between a person and the location in which the 
person lives. Two features are defined to include 
this information: 
? ET1Country: the entity type of M1 when M2 is 
a country name 
? CountryET2: the entity type of M2 when M1 is 
a country name 
 
 
430
Personal Relative Trigger Word List 
This is used to differentiate the six personal social 
relation subtypes in ACE: Parent, Grandparent, 
Spouse, Sibling, Other-Relative and Other-
Personal. This trigger word list is first gathered 
from WordNet by checking whether a word has the 
semantic class ?person|?|relative?. Then, all the 
trigger words are semi-automatically6 classified 
into different categories according to their related 
personal social relation subtypes. We also extend 
the list by collecting the trigger words from the 
head words of the mentions in the training data 
according to their indicating relationships. Two 
features are defined to include this information: 
? ET1SC2: combination of the entity type of M1 
and the semantic class of M2 when M2 triggers 
a personal social subtype. 
? SC1ET2: combination of the entity type of M2 
and the semantic class of M1 when the first 
mention triggers a personal social subtype. 
5 Experimentation 
This paper uses the ACE corpus provided by LDC 
to train and evaluate our feature-based relation ex-
traction system. The ACE corpus is gathered from 
various newspapers, newswire and broadcasts. In 
this paper, we only model explicit relations be-
cause of poor inter-annotator agreement in the an-
notation of implicit relations and their limited 
number. 
5.1 Experimental Setting 
We use the official ACE corpus from LDC. The 
training set consists of 674 annotated text docu-
ments (~300k words) and 9683 instances of rela-
tions. During development, 155 of 674 documents 
in the training set are set aside for fine-tuning the 
system. The testing set is held out only for final 
evaluation. It consists of 97 documents (~50k 
words) and 1386 instances of relations. Table 1 
lists the types and subtypes of relations for the 
ACE Relation Detection and Characterization 
(RDC) task, along with their frequency of occur-
rence in the ACE training set. It shows that the 
                                                          
6 Those words that have the semantic classes ?Parent?, 
?GrandParent?, ?Spouse? and ?Sibling? are automati-
cally set with the same classes without change. How-
ever, The remaining words that do not have above four 
classes are manually classified. 
ACE corpus suffers from a small amount of anno-
tated data for a few subtypes such as the subtype 
?Founder? under the type ?ROLE?. It also shows 
that the ACE RDC task defines some difficult sub-
types such as the subtypes ?Based-In?, ?Located? 
and ?Residence? under the type ?AT?, which are 
difficult even for human experts to differentiate.  
Type Subtype Freq 
AT(2781) Based-In 347 
 Located 2126 
 Residence 308 
NEAR(201) Relative-Location 201 
PART(1298) Part-Of 947 
 Subsidiary 355 
 Other 6 
ROLE(4756) Affiliate-Partner 204 
 Citizen-Of 328 
 Client 144 
 Founder 26 
 General-Staff 1331 
 Management 1242 
 Member 1091 
 Owner 232 
 Other 158 
SOCIAL(827) Associate 91 
 Grandparent 12 
 Other-Personal 85 
 Other-Professional 339 
 Other-Relative 78 
 Parent 127 
 Sibling 18 
 Spouse 77 
Table 1: Relation types and subtypes in the ACE 
training data 
In this paper, we explicitly model the argument 
order of the two mentions involved. For example, 
when comparing mentions m1 and m2, we distin-
guish between m1-ROLE.Citizen-Of-m2 and m2-
ROLE.Citizen-Of-m1. Note that only 6 of these 24 
relation subtypes are symmetric: ?Relative-
Location?, ?Associate?, ?Other-Relative?, ?Other-
Professional?, ?Sibling?, and ?Spouse?. In this 
way, we model relation extraction as a multi-class 
classification problem with 43 classes, two for 
each relation subtype (except the above 6 symmet-
ric subtypes) and a ?NONE? class for the case 
where the two mentions are not related. 
5.2 Experimental Results 
In this paper, we only measure the performance of 
relation extraction on ?true? mentions with ?true? 
chaining of coreference (i.e. as annotated by the 
corpus annotators) in the ACE corpus. Table 2 
measures the performance of our relation extrac-
431
tion system over the 43 ACE relation subtypes on 
the testing set. It shows that our system achieves 
best performance of 63.1%/49.5%/ 55.5 in preci-
sion/recall/F-measure when combining diverse 
lexical, syntactic and semantic features. Table 2 
also measures the contributions of different fea-
tures by gradually increasing the feature set. It 
shows that: 
Features P R F 
Words 69.2 23.7 35.3 
+Entity Type 67.1 32.1 43.4 
+Mention Level 67.1 33.0 44.2 
+Overlap 57.4 40.9 47.8 
+Chunking 61.5 46.5 53.0 
+Dependency Tree 62.1 47.2 53.6 
+Parse Tree 62.3 47.6 54.0 
+Semantic Resources 63.1 49.5 55.5 
Table 2: Contribution of different features over 43 
relation subtypes in the test data 
? Using word features only achieves the perform-
ance of 69.2%/23.7%/35.3 in precision/recall/F-
measure.  
? Entity type features are very useful and improve 
the F-measure by 8.1 largely due to the recall 
increase. 
? The usefulness of mention level features is quite 
limited. It only improves the F-measure by 0.8 
due to the recall increase. 
? Incorporating the overlap features gives some 
balance between precision and recall. It in-
creases the F-measure by 3.6 with a big preci-
sion decrease and a big recall increase. 
? Chunking features are very useful. It increases 
the precision/recall/F-measure by 4.1%/5.6%/ 
5.2 respectively. 
? To our surprise, incorporating the dependency 
tree and parse tree features only improve the F-
measure by 0.6 and 0.4 respectively. This may 
be due to the fact that most of relations in the 
ACE corpus are quite local. Table 3 shows that 
about 70% of relations exist where two men-
tions are embedded in each other or separated 
by at most one word. While short-distance rela-
tions dominate and can be resolved by above 
simple features, the dependency tree and parse 
tree features can only take effect in the remain-
ing much less long-distance relations. However, 
full parsing is always prone to long distance er-
rors although the Collins? parser used in our 
system represents the state-of-the-art in full 
parsing. 
? Incorporating semantic resources such as the 
country name list and the personal relative trig-
ger word list further increases the F-measure by 
1.5 largely due to the differentiation of the rela-
tion subtype ?ROLE.Citizen-Of? from ?ROLE. 
Residence? by distinguishing country GPEs 
from other GPEs. The effect of personal relative 
trigger words is very limited due to the limited 
number of testing instances over personal social 
relation subtypes. 
Table 4 separately measures the performance of 
different relation types and major subtypes. It also 
indicates the number of testing instances, the num-
ber of correctly classified instances and the number 
of wrongly classified instances for each type or 
subtype. It is not surprising that the performance 
on the relation type ?NEAR? is low because it oc-
curs rarely in both the training and testing data. 
Others like ?PART.Subsidary? and ?SOCIAL. 
Other-Professional? also suffer from their low oc-
currences. It also shows that our system performs 
best on the subtype ?SOCIAL.Parent? and ?ROLE. 
Citizen-Of?. This is largely due to incorporation of 
two semantic resources, i.e. the country name list 
and the personal relative trigger word list. Table 4 
also indicates the low performance on the relation 
type ?AT? although it frequently occurs in both the 
training and testing data. This suggests the diffi-
culty of detecting and classifying the relation type 
?AT? and its subtypes. 
Table 5 separates the performance of relation 
detection from overall performance on the testing 
set. It shows that our system achieves the perform-
ance of 84.8%/66.7%/74.7 in precision/recall/F-
measure on relation detection. It also shows that 
our system achieves overall performance of 
77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in preci-
sion/recall/F-measure on the 5 ACE relation types 
and the best-reported systems on the ACE corpus. 
It shows that our system achieves better perform-
ance by ~3 F-measure largely due to its gain in 
recall. It also shows that feature-based methods 
dramatically outperform kernel methods. This sug-
gests that feature-based methods can effectively 
combine different features from a variety of 
sources (e.g. WordNet and gazetteers) that can be 
brought to bear on relation extraction. The tree 
kernels developed in Culotta et al(2004) are yet to 
be effective on the ACE RDC task. 
Finally, Table 6 shows the distributions of er-
rors. It shows that 73% (627/864) of errors results 
432
from relation detection and 27% (237/864) of er-
rors results from relation characterization, among 
which 17.8% (154/864) of errors are from misclas-
sification across relation types and 9.6% (83/864) 
of errors are from misclassification of relation sub-
types inside the same relation types. This suggests 
that relation detection is critical for relation extrac-
tion. 
# of other mentions in between # of relations 
0 1 2 3 >=4 Overall 
0 3991 161 11 0 0 4163 
1 2350 315 26 2 0 2693 
2 465 95 7 2 0 569 
3 311 234 14 0 0 559 
4 204 225 29 2 3 463 
5 111 113 38 2 1 265 
>=6 262 297 277 148 134 1118 
#  
of  
the words 
 in  
between 
Overall 7694 1440 402 156 138 9830 
Table 3: Distribution of relations over #words and #other mentions in between in the training data 
Type Subtype #Testing Instances #Correct #Error P R F 
AT  392 224 105 68.1 57.1 62.1 
 Based-In 85 39 10 79.6 45.9 58.2 
 Located 241 132 120 52.4 54.8 53.5 
 Residence 66 19 9 67.9 28.8 40.4 
NEAR  35 8 1 88.9 22.9 36.4 
 Relative-Location 35 8 1 88.9 22.9 36.4 
PART  164 106 39 73.1 64.6 68.6 
 Part-Of 136 76 32 70.4 55.9 62.3 
 Subsidiary 27 14 23 37.8 51.9 43.8 
ROLE  699 443 82 84.4 63.4 72.4 
 Citizen-Of 36 25 8 75.8 69.4 72.6 
 General-Staff 201 108 46 71.1 53.7 62.3 
 Management 165 106 72 59.6 64.2 61.8 
 Member 224 104 36 74.3 46.4 57.1 
SOCIAL  95 60 21 74.1 63.2 68.5 
 Other-Professional 29 16 32 33.3 55.2 41.6 
 Parent 25 17 0 100 68.0 81.0 
Table 4: Performance of different relation types and major subtypes in the test data 
Relation Detection RDC on Types RDC on Subtypes System 
P R F P R F P R F 
Ours: feature-based 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5 
Kambhatla (2004):feature-based - - - - - - 63.5 45.2 52.8 
Culotta et al(2004):tree kernel 81.2 51.8 63.2 67.1 35.0 45.8 - - - 
Table 5: Comparison of our system with other best-reported systems on the ACE corpus 
Error Type #Errors 
False Negative 462 Detection Error 
False Positive 165 
Cross Type Error 154 Characterization  
Error Inside Type Error 83 
Table 6: Distribution of errors 
6 Discussion and Conclusion 
In this paper, we have presented a feature-based 
approach for relation extraction where diverse 
lexical, syntactic and semantic knowledge are em-
ployed. Instead of exploring the full parse tree in-
formation directly as previous related work, we 
incorporate the base phrase chunking information 
first. Evaluation on the ACE corpus shows that 
base phrase chunking contributes to most of the 
performance improvement from syntactic aspect 
while further incorporation of the parse tree and 
dependence tree information only slightly im-
proves the performance. This may be due to three 
reasons: First, most of relations defined in ACE 
have two mentions being close to each other. 
While short-distance relations dominate and can be 
resolved by simple features such as word and 
chunking features, the further dependency tree and 
parse tree features can only take effect in the re-
maining much less and more difficult long-distance 
relations. Second, it is well known that full parsing 
433
is always prone to long-distance parsing errors al-
though the Collins? parser used in our system 
achieves the state-of-the-art performance. There-
fore, the state-of-art full parsing still needs to be 
further enhanced to provide accurate enough in-
formation, especially PP (Preposition Phrase) at-
tachment. Last, effective ways need to be explored 
to incorporate information embedded in the full 
parse trees. Besides, we also demonstrate how se-
mantic information such as WordNet and Name 
List, can be used in feature-based relation extrac-
tion to further improve the performance. 
The effective incorporation of diverse features 
enables our system outperform previously best-
reported systems on the ACE corpus. Although 
tree kernel-based approaches facilitate the explora-
tion of the implicit feature space with the parse tree 
structure, yet the current technologies are expected 
to be further advanced to be effective for relatively 
complicated relation extraction tasks such as the 
one defined in ACE where 5 types and 24 subtypes 
need to be extracted. Evaluation on the ACE RDC 
task shows that our approach of combining various 
kinds of evidence can scale better to problems, 
where we have a lot of relation types with a rela-
tively small amount of annotated data. The ex-
periment result also shows that our feature-based 
approach outperforms the tree kernel-based ap-
proaches by more than 20 F-measure on the extrac-
tion of 5 ACE relation types.  
In the future work, we will focus on exploring 
more semantic knowledge in relation extraction, 
which has not been covered by current research. 
Moreover, our current work is done when the En-
tity Detection and Tracking (EDT) has been per-
fectly done. Therefore, it would be interesting to 
see how imperfect EDT affects the performance in 
relation extraction. 
References  
Agichtein E. and Gravano L. (2000). Snowball: Extract-
ing relations from large plain text collections. In Pro-
ceedings of 5th ACM International Conference on 
Digital Libraries. 4-7 June 2000. San Antonio, TX. 
Brin S. (1998). Extracting patterns and relations from 
the World Wide Web. In Proceedings of WebDB 
workshop at 6th International Conference on Extend-
ing DataBase Technology (EDBT?1998).23-27 
March 1998, Valencia, Spain 
Collins M. (1999).  Head-driven statistical models for 
natural language parsing. Ph.D. Dissertation, Univer-
sity of Pennsylvania. 
Collins M. and Duffy N. (2002). Covolution kernels for 
natural language. In Dietterich T.G., Becker S. and 
Ghahramani Z. editors. Advances in Neural Informa-
tion Processing Systems 14. Cambridge, MA.  
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. In Proceedings of 42th 
Annual Meeting of the Association for Computational 
Linguistics. 21-26 July 2004. Barcelona, Spain 
Cumby C.M. and Roth D. (2003). On kernel methods 
for relation learning. In Fawcett T. and Mishra N. 
editors. In Proceedings of 20th International Confer-
ence on Machine Learning (ICML?2003). 21-24 Aug 
2003. Washington D.C. USA. AAAI Press. 
Haussler D. (1999). Covention kernels on discrete struc-
tures. Technical Report UCS-CRL-99-10. University 
of California, Santa Cruz. 
Joachims T. (1998). Text categorization with Support 
Vector Machines: Learning with many relevant fea-
tures. In Proceedings of European Conference on 
Machine Learning(ECML?1998).  21-23 April 1998. 
Chemnitz, Germany 
Miller G.A. (1990). WordNet: An online lexical data-
base. International Journal of Lexicography. 
3(4):235-312. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to extract 
information from text. In Proceedings of 6th Applied 
Natural Language Processing Conference. 29 April  
- 4 May 2000, Seattle, USA 
MUC-7. (1998). Proceedings of the 7th Message Under-
standing Conference (MUC-7). Morgan Kaufmann, 
San Mateo, CA. 
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. In Proceedings of 42th Annual 
Meeting of the Association for Computational Lin-
guistics. 21-26 July 2004. Barcelona, Spain. 
Roth D. and Yih W.T. (2002). Probabilistic reasoning 
for entities and relation recognition. In Proceedings 
of 19th International Conference on Computational 
Linguistics(CoLING?2002). Taiwan. 
Vapnik V. (1998). Statistical Learning Theory. Whiley, 
Chichester, GB. 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. pp1083-1106. 
Zhang Z. (2004). Weekly-supervised relation classifica-
tion for Information Extraction. In Proceedings of 
ACM 13th Conference on Information and Knowl-
edge Management (CIKM?2004). 8-13 Nov 2004. 
Washington D.C., USA. 
434
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Kernel-Based Pronoun Resolution with Structured Syntactic Knowledge
Xiaofeng Yang? Jian Su? Chew Lim Tan?
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
? Department of Computer Science
National University of Singapore,
Singapore, 117543
tancl@comp.nus.edu.sg
Abstract
Syntactic knowledge is important for pro-
noun resolution. Traditionally, the syntac-
tic information for pronoun resolution is
represented in terms of features that have
to be selected and defined heuristically.
In the paper, we propose a kernel-based
method that can automatically mine the
syntactic information from the parse trees
for pronoun resolution. Specifically, we
utilize the parse trees directly as a struc-
tured feature and apply kernel functions to
this feature, as well as other normal fea-
tures, to learn the resolution classifier. In
this way, our approach avoids the efforts
of decoding the parse trees into the set of
flat syntactic features. The experimental
results show that our approach can bring
significant performance improvement and
is reliably effective for the pronoun reso-
lution task.
1 Introduction
Pronoun resolution is the task of finding the cor-
rect antecedent for a given pronominal anaphor
in a document. Prior studies have suggested that
syntactic knowledge plays an important role in
pronoun resolution. For a practical pronoun res-
olution system, the syntactic knowledge usually
comes from the parse trees of the text. The is-
sue that arises is how to effectively incorporate the
syntactic information embedded in the parse trees
to help resolution. One common solution seen in
previous work is to define a set of features that rep-
resent particular syntactic knowledge, such as the
grammatical role of the antecedent candidates, the
governing relations between the candidate and the
pronoun, and so on. These features are calculated
by mining the parse trees, and then could be used
for resolution by using manually designed rules
(Lappin and Leass, 1994; Kennedy and Boguraev,
1996; Mitkov, 1998), or using machine-learning
methods (Aone and Bennett, 1995; Yang et al,
2004; Luo and Zitouni, 2005).
However, such a solution has its limitation. The
syntactic features have to be selected and defined
manually, usually by linguistic intuition. Unfor-
tunately, what kinds of syntactic information are
effective for pronoun resolution still remains an
open question in this research community. The
heuristically selected feature set may be insuffi-
cient to represent all the information necessary for
pronoun resolution contained in the parse trees.
In this paper we will explore how to utilize the
syntactic parse trees to help learning-based pro-
noun resolution. Specifically, we directly utilize
the parse trees as a structured feature, and then use
a kernel-based method to automatically mine the
knowledge embedded in the parse trees. The struc-
tured syntactic feature, together with other nor-
mal features, is incorporated in a trainable model
based on Support Vector Machine (SVM) (Vapnik,
1995) to learn the decision classifier for resolution.
Indeed, using kernel methods to mine structural
knowledge has shown success in some NLP ap-
plications like parsing (Collins and Duffy, 2002;
Moschitti, 2004) and relation extraction (Zelenko
et al, 2003; Zhao and Grishman, 2005). However,
to our knowledge, the application of such a tech-
nique to the pronoun resolution task still remains
unexplored.
Compared with previous work, our approach
has several advantages: (1) The approach uti-
lizes the parse trees as a structured feature, which
avoids the efforts of decoding the parse trees into
a set of syntactic features in a heuristic manner.
(2) The approach is able to put together the struc-
tured feature and the normal flat features in a
trainable model, which allows different types of
41
information to be considered in combination for
both learning and resolution. (3) The approach
is applicable for practical pronoun resolution as
the syntactic information can be automatically ob-
tained from machine-generated parse trees. And
our study shows that the approach works well un-
der the commonly available parsers.
We evaluate our approach on the ACE data set.
The experimental results over the different do-
mains indicate that the structured syntactic fea-
ture incorporated with kernels can significantly
improve the resolution performance (by 5%?8%
in the success rates), and is reliably effective for
the pronoun resolution task.
The remainder of the paper is organized as fol-
lows. Section 2 gives some related work that uti-
lizes the structured syntactic knowledge to do pro-
noun resolution. Section 3 introduces the frame-
work for the pronoun resolution, as well as the
baseline feature space and the SVM classifier.
Section 4 presents in detail the structured feature
and the kernel functions to incorporate such a fea-
ture in the resolution. Section 5 shows the exper-
imental results and has some discussion. Finally,
Section 6 concludes the paper.
2 Related Work
One of the early work on pronoun resolution rely-
ing on parse trees was proposed by Hobbs (1978).
For a pronoun to be resolved, Hobbs? algorithm
works by searching the parse trees of the current
text. Specifically, the algorithm processes one sen-
tence at a time, using a left-to-right breadth-first
searching strategy. It first checks the current sen-
tence where the pronoun occurs. The first NP
that satisfies constraints, like number and gender
agreements, would be selected as the antecedent.
If the antecedent is not found in the current sen-
tence, the algorithm would traverse the trees of
previous sentences in the text. As the searching
processing is completely done on the parse trees,
the performance of the algorithm would rely heav-
ily on the accuracy of the parsing results.
Lappin and Leass (1994) reported a pronoun
resolution algorithm which uses the syntactic rep-
resentation output by McCord?s Slot Grammar
parser. A set of salience measures (e.g. Sub-
ject, Object or Accusative emphasis) is derived
from the syntactic structure. The candidate with
the highest salience score would be selected as
the antecedent. In their algorithm, the weights of
Category: whether the candidate is a definite noun phrase,
indefinite noun phrase, pronoun, named-entity or others.
Reflexiveness: whether the pronominal anaphor is a reflex-
ive pronoun.
Type: whether the pronominal anaphor is a male-person
pronoun (like he), female-person pronoun (like she), sin-
gle gender-neuter pronoun (like it), or plural gender-neuter
pronoun (like they)
Subject: whether the candidate is a subject of a sentence, a
subject of a clause, or not.
Object: whether the candidate is an object of a verb, an
object of a preposition, or not.
Distance: the sentence distance between the candidate and
the pronominal anaphor.
Closeness: whether the candidate is the candidate closest
to the pronominal anaphor.
FirstNP: whether the candidate is the first noun phrase in
the current sentence.
Parallelism: whether the candidate has an identical collo-
cation pattern with the pronominal anaphor.
Table 1: Feature set for the baseline pronoun res-
olution system
salience measures have to be assigned manually.
Luo and Zitouni (2005) proposed a coreference
resolution approach which also explores the infor-
mation from the syntactic parse trees. Different
from Lappin and Leass (1994)?s algorithm, they
employed a maximum entropy based model to au-
tomatically compute the importance (in terms of
weights) of the features extracted from the trees.
In their work, the selection of their features is
mainly inspired by the government and binding
theory, aiming to capture the c-command relation-
ships between the pronoun and its antecedent can-
didate. By contrast, our approach simply utilizes
the parse trees as a structured feature, and lets the
learning algorithm discover all possible embedded
information that is necessary for pronoun resolu-
tion.
3 The Resolution Framework
Our pronoun resolution system adopts the com-
mon learning-based framework similar to those
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance is formed by a pronoun and one of its
antecedent candidate. During training, for each
pronominal anaphor encountered, a positive in-
stance is created by paring the anaphor and its
closest antecedent. Also a set of negative instances
is formed by paring the anaphor with each of the
42
non-coreferential candidates. Based on the train-
ing instances, a binary classifier is generated using
a particular learning algorithm. During resolution,
a pronominal anaphor to be resolved is paired in
turn with each preceding antecedent candidate to
form a testing instance. This instance is presented
to the classifier which then returns a class label
with a confidence value indicating the likelihood
that the candidate is the antecedent. The candidate
with the highest confidence value will be selected
as the antecedent of the pronominal anaphor.
3.1 Feature Space
As with many other learning-based approaches,
the knowledge for the reference determination is
represented as a set of features associated with
the training or test instances. In our baseline sys-
tem, the features adopted include lexical property,
morphologic type, distance, salience, parallelism,
grammatical role and so on. Listed in Table 1, all
these features have been proved effective for pro-
noun resolution in previous work.
3.2 Support Vector Machine
In theory, any discriminative learning algorithm is
applicable to learn the classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels to
incorporate the structured feature.
Suppose the training set S consists of labelled
vectors {(xi, yi)}, where xi is the feature vector
of a training instance and yi is its class label. The
classifier learned by SVM is
f(x) = sgn(
?
i=1
yiaix ? xi + b) (1)
where ai is the learned parameter for a support
vector xi. An instance x is classified as positive
(negative) if f(x) > 0 (f(x) < 0)1.
One advantage of SVM is that we can use ker-
nel methods to map a feature space to a particu-
lar high-dimension space, in case that the current
problem could not be separated in a linear way.
Thus the dot-product x1 ? x2 is replaced by a ker-
nel function (or kernel) between two vectors, that
is K(x1, x2). For the learning with the normal
features listed in Table 1, we can just employ the
well-known polynomial or radial basis kernels that
can be computed efficiently. In the next section we
1For our task, the result of f(x) is used as the confidence
value of the candidate to be the antecedent of the pronoun
described by x.
will discuss how to use kernels to incorporate the
more complex structured feature.
4 Incorporating Structured Syntactic
Information
4.1 Main Idea
A parse tree that covers a pronoun and its an-
tecedent candidate could provide us much syntac-
tic information related to the pair. The commonly
used syntactic knowledge for pronoun resolution,
such as grammatical roles or the governing rela-
tions, can be directly described by the tree struc-
ture. Other syntactic knowledge that may be help-
ful for resolution could also be implicitly repre-
sented in the tree. Therefore, by comparing the
common substructures between two trees we can
find out to what degree two trees contain similar
syntactic information, which can be done using a
convolution tree kernel.
The value returned from the tree kernel reflects
the similarity between two instances in syntax.
Such syntactic similarity can be further combined
with other knowledge to compute the overall simi-
larity between two instances, through a composite
kernel. And thus a SVM classifier can be learned
and then used for resolution. This is just the main
idea of our approach.
4.2 Structured Syntactic Feature
Normally, parsing is done on the sentence level.
However, in many cases a pronoun and an an-
tecedent candidate do not occur in the same sen-
tence. To present their syntactic properties and
relations in a single tree structure, we construct a
syntax tree for an entire text, by attaching the parse
trees of all its sentences to an upper node.
Having obtained the parse tree of a text, we shall
consider how to select the appropriate portion of
the tree as the structured feature for a given in-
stance. As each instance is related to a pronoun
and a candidate, the structured feature at least
should be able to cover both of these two expres-
sions. Generally, the more substructure of the tree
is included, the more syntactic information would
be provided, but at the same time the more noisy
information that comes from parsing errors would
likely be introduced. In our study, we examine
three possible structured features that contain dif-
ferent substructures of the parse tree:
Min-Expansion This feature records the mini-
mal structure covering both the pronoun and
43
Min-Expansion Simple-Expansion Full-Expansion
Figure 1: structured-features for the instance i{?him?, ?the man?}
the candidate in the parse tree. It only in-
cludes the nodes occurring in the shortest
path connecting the pronoun and the candi-
date, via the nearest commonly commanding
node. For example, considering the sentence
?The man in the room saw him.?, the struc-
tured feature for the instance i{?him?,?the
man?} is circled with dash lines as shown in
the leftmost picture of Figure 1.
Simple-Expansion Min-Expansion could, to
some degree, describe the syntactic relation-
ships between the candidate and pronoun.
However, it is incapable of capturing the
syntactic properties of the candidate or
the pronoun, because the tree structure
surrounding the expression is not taken into
consideration. To incorporate such infor-
mation, feature Simple-Expansion not only
contains all the nodes in Min-Expansion, but
also includes the first-level children of these
nodes2. The middle of Figure 1 shows such a
feature for i{?him?, ?the man?}. We can see
that the nodes ?PP? (for ?in the room?) and
?VB? (for ?saw?) are included in the feature,
which provides clues that the candidate is
modified by a prepositional phrase and the
pronoun is the object of a verb.
Full-Expansion This feature focusses on the
whole tree structure between the candidate
and pronoun. It not only includes all the
nodes in Simple-Expansion, but also the
nodes (beneath the nearest commanding par-
ent) that cover the words between the candi-
date and the pronoun3. Such a feature keeps
the most information related to the pronoun
2If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences
before the candidate or after the pronoun.
3We will not expand the nodes denoting the sentences
other than where the pronoun and the candidate occur.
and candidate pair. The rightmost picture of
Figure 1 shows the structure for feature Full-
Expansion of i{?him?, ?the man?}. As illus-
trated, different from in Simple-Expansion,
the subtree of ?PP? (for ?in the room?) is
fully expanded and all its children nodes are
included in Full-Expansion.
Note that to distinguish from other words, we
explicitly mark up in the structured feature the
pronoun and the antecedent candidate under con-
sideration, by appending a string tag ?ANA? and
?CANDI? in their respective nodes (e.g.,?NN-
CANDI? for ?man? and ?PRP-ANA? for ?him? as
shown in Figure 1).
4.3 Structural Kernel and Composite Kernel
To calculate the similarity between two structured
features, we use the convolution tree kernel that is
defined by Collins and Duffy (2002) and Moschitti
(2004). Given two trees, the kernel will enumerate
all their subtrees and use the number of common
subtrees as the measure of the similarity between
the trees. As has been proved, the convolution
kernel can be efficiently computed in polynomial
time.
The above tree kernel only aims for the struc-
tured feature. We also need a composite kernel
to combine together the structured feature and the
normal features described in Section 3.1. In our
study we define the composite kernel as follows:
Kc(x1, x2) = Kn(x1, x2)|Kn(x1, x2)| ?
Kt(x1, x2)
|Kt(x1, x2)|(2)
where Kt is the convolution tree kernel defined
for the structured feature, and Kn is the kernel
applied on the normal features. Both kernels are
divided by their respective length4 for normaliza-
tion. The new composite kernel Kc, defined as the
4The length of a kernel K is defined as |K(x1, x2)| =?
K(x1, x1) ?K(x2, x2)
44
multiplier of normalized Kt and Kn, will return a
value close to 1 only if both the structured features
and the normal features from the two vectors have
high similarity under their respective kernels.
5 Experiments and Discussions
5.1 Experimental Setup
In our study we focussed on the third-person
pronominal anaphora resolution. All the exper-
iments were done on the ACE-2 V1.0 corpus
(NIST, 2003), which contain two data sets, train-
ing and devtest, used for training and testing re-
spectively. Each of these sets is further divided
into three domains: newswire (NWire), newspa-
per (NPaper), and broadcast news (BNews).
An input raw text was preprocessed automati-
cally by a pipeline of NLP components, including
sentence boundary detection, POS-tagging, Text
Chunking and Named-Entity Recognition. The
texts were parsed using the maximum-entropy-
based Charniak parser (Charniak, 2000), based on
which the structured features were computed au-
tomatically. For learning, the SVM-Light soft-
ware (Joachims, 1999) was employed with the
convolution tree kernel implemented by Moschitti
(2004). All classifiers were trained with default
learning parameters.
The performance was evaluated based on the
metric success, the ratio of the number of cor-
rectly resolved5 anaphor over the number of all
anaphors. For each anaphor, the NPs occurring
within the current and previous two sentences
were taken as the initial antecedent candidates.
Those with mismatched number and gender agree-
ments were filtered from the candidate set. Also,
pronouns or NEs that disagreed in person with the
anaphor were removed in advance. For training,
there were 1207, 1440, and 1260 pronouns with
non-empty candidate set found pronouns in the
three domains respectively, while for testing, the
number was 313, 399 and 271. On average, a
pronoun anaphor had 6?9 antecedent candidates
ahead. Totally, we got around 10k, 13k and 8k
training instances for the three domains.
5.2 Baseline Systems
Table 2 lists the performance of different systems.
We first tested Hobbs? algorithm (Hobbs, 1978).
5An anaphor was deemed correctly resolved if the found
antecedent is in the same coreference chain of the anaphor.
NWire NPaper BNews
Hobbs (1978) 66.1 66.4 72.7
NORM 74.4 77.4 74.2
NORM MaxEnt 72.8 77.9 75.3
NORM C5 71.9 75.9 71.6
S Min 76.4 81.0 76.8
S Simple 73.2 82.7 82.3
S Full 73.2 80.5 79.0
NORM+S Min 77.6 82.5 82.3
NORM+S Simple 79.2 82.7 82.3
NORM+S Full 81.5 83.2 81.5
Table 2: Results of the syntactic structured fea-
tures
Described in Section 2, the algorithm uses heuris-
tic rules to search the parse tree for the antecedent,
and will act as a good baseline to compare with the
learned-based approach with the structured fea-
ture. As shown in the first line of Table 2, Hobbs?
algorithm obtains 66%?72% success rates on the
three domains.
The second block of Table 2 shows the baseline
system (NORM) that uses only the normal features
listed in Table 1. Throughout our experiments, we
applied the polynomial kernel on the normal fea-
tures to learn the SVM classifiers. In the table we
also compared the SVM-based results with those
using other learning algorithms, i.e., Maximum
Entropy (Maxent) and C5 decision tree, which are
more commonly used in the anaphora resolution
task.
As shown in the table, the system with normal
features (NORM) obtains 74%?77% success rates
for the three domains. The performance is simi-
lar to other published results like those by Keller
and Lapata (2003), who adopted a similar fea-
ture set and reported around 75% success rates
on the ACE data set. The comparison between
different learning algorithms indicates that SVM
can work as well as or even better than Maxent
(NORM MaxEnt) or C5 (NORM C5).
5.3 Systems with Structured Features
The last two blocks of Table 2 summarize the re-
sults using the three syntactic structured features,
i.e, Min Expansion (S MIN), Simple Expansion
(S SIMPLE) and Full Expansion (S FULL). Be-
tween them, the third block is for the systems us-
ing the individual structured feature alone. We
can see that all the three structured features per-
45
NWire NPaper BNews
Sentence Distance 0 1 2 0 1 2 0 1 2
(Number of Prons) (192) (102) (19) (237) (147) (15) (175) (82) (14)
NORM 80.2 72.5 26.3 81.4 75.5 33.3 80.0 65.9 50.0
S Simple 79.7 70.6 21.1 87.3 81.0 26.7 89.7 70.7 57.1
NORM+S Simple 85.4 76.5 31.6 87.3 79.6 40.0 88.6 74.4 50.0
Table 3: The resolution results for pronouns with antecedent in different sentences apart
NWire NPaper BNews
Type person neuter person neuter person neuter
(Number of Prons) (171) (142) (250) (149) (153) (118)
NORM 81.9 65.5 80.0 73.2 74.5 73.7
S Simple 81.9 62.7 83.2 81.9 82.4 82.2
NORM+S Simple 87.1 69.7 83.6 81.2 86.9 76.3
Table 4: The resolution results for different types of pronouns
form better than the normal features for NPaper
(up to 5.3% success) and BNews (up to 8.1% suc-
cess), or equally well (?1 ? 2% in success) for
NWire. When used together with the normal fea-
tures, as shown in the last block, the three struc-
tured features all outperform the baselines. Es-
pecially, the combinations of NORM+S SIMPLE
and NORM+S FULL can achieve significantly6
better results than NORM, with the success rate
increasing by (4.8%, 5.3% and 8.1%) and (7.1%,
5.8%, 7.2%) respectively. All these results prove
that the structured syntactic feature is effective for
pronoun resolution.
We further compare the performance of the
three different structured features. As shown in
Table 2, when used together with the normal
features, Full Expansion gives the highest suc-
cess rates in NWire and NPaper, but neverthe-
less the lowest in BNews. This should be be-
cause feature Full-Expansion captures a larger
portion of the parse trees, and thus can provide
more syntactic information than Min Expansion
or Simple Expansion. However, if the texts are
less-formally structured as those in BNews, Full-
Expansion would inevitably involve more noises
and thus adversely affect the resolution perfor-
mance. By contrast, feature Simple Expansion
would achieve balance between the information
and the noises to be introduced: from Table 2 we
can find that compared with the other two features,
Simple Expansion is capable of producing aver-
age results for all the three domains. And for this
6p < 0.05 by a 2-tailed t test.
reason, our subsequent reports will focus on Sim-
ple Expansion, unless otherwise specified.
As described, to compute the structured fea-
ture, parse trees for different sentences are con-
nected to form a large tree for the text. It would
be interesting to find how the structured feature
works for pronouns whose antecedents reside in
different sentences. For this purpose we tested
the success rates for the pronouns with the clos-
est antecedent occurring in the same sentence,
one-sentence apart, and two-sentence apart. Ta-
ble 3 compares the learning systems with/without
the structured feature present. From the table,
for all the systems, the success rates drop with
the increase of the distances between the pro-
noun and the antecedent. However, in most cases,
adding the structured feature would bring consis-
tent improvement against the baselines regardless
of the number of sentence distance. This observa-
tion suggests that the structured syntactic informa-
tion is helpful for both intra-sentential and inter-
sentential pronoun resolution.
We were also concerned about how the struc-
tured feature works for different types of pro-
nouns. Table 4 lists the resolution results for two
types of pronouns: person pronouns (i.e., ?he?,
?she?) and neuter-gender pronouns (i.e., ?it? and
?they?). As shown, with the structured feature in-
corporated, the system NORM+S Simple can sig-
nificantly boost the performance of the baseline
(NORM), for both personal pronoun and neuter-
gender pronoun resolution.
46
1 2 3 4 5 6 7 8 9 100.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple 2 4 6 8 10 120.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple 1 2 3 4 5 6 7 80.65
0.7
0.75
0.8
Number of Training Documents
Succe
ss
NORMS_SimpleNORM+S_Simple
NWire NPaper BNews
Figure 2: Learning curves of systems with different features
5.4 Learning Curves
Figure 2 plots the learning curves for the sys-
tems with three feature sets, i.e, normal features
(NORM), structured feature alone (S Simple),
and combined features (NORM+S Simple). We
trained each system with different number of in-
stances from 1k, 2k, 3k, . . . , till the full size. Each
point in the figures was the average over two trails
with instances selected forwards and backwards
respectively. From the figures we can find that
(1) Used in combination (NORM+S Simple), the
structured feature shows superiority over NORM,
achieving results consistently better than the nor-
mal features (NORM) do in all the three domains.
(2) With training instances above 3k, the struc-
tured feature, used either in isolation (S Simple)
or in combination (NORM+S Simple), leads to
steady increase in the success rates and exhibit
smoother learning curves than the normal features
(NORM). These observations further prove the re-
liability of the structured feature in pronoun reso-
lution.
5.5 Feature Analysis
In our experiment we were also interested to com-
pare the structured feature with the normal flat
features extracted from the parse tree, like fea-
ture Subject and Object. For this purpose we
took out these two grammatical features from the
normal feature set, and then trained the systems
again. As shown in Table 5, the two grammatical-
role features are important for the pronoun resolu-
tion: removing these features results in up to 5.7%
(NWire) decrease in success. However, when the
structured feature is included, the loss in success
reduces to 1.9% and 1.1% for NWire and BNews,
and a slight improvement can even be achieved for
NPaper. This indicates that the structured feature
can effectively provide the syntactic information
NWire NPaper BNews
NORM 74.4 77.4 74.2
NORM - subj/obj 68.7 76.2 72.7
NORM + S Simple 79.2 82.7 82.3
NORM + S Simple - subj/obj 77.3 83.0 81.2
NORM + Luo05 75.7 77.9 74.9
Table 5: Comparison of the structured feature and
the flat features extracted from parse trees
Feature Parser NWire NPaper BNews
Charniak00 73.2 82.7 82.3
S Simple Collins99 75.1 83.2 80.4
NORM+ Charniak00 79.2 82.7 82.3
S Simple Collins99 80.8 81.5 82.3
Table 6: Results using different parsers
important for pronoun resolution.
We also tested the flat syntactic feature set pro-
posed in Luo and Zitouni (2005)?s work. As de-
scribed in Section 2, the feature set is inspired
the binding theory, including those features like
whether the candidate is c commanding the pro-
noun, and the counts of ?NP?, ?VP?, ?S? nodes
in the commanding path. The last line of Table 5
shows the results by adding these features into the
normal feature set. In line with the reports in (Luo
and Zitouni, 2005) we do observe the performance
improvement against the baseline (NORM) for all
the domains. However, the increase in the success
rates (up to 1.3%) is not so large as by adding the
structured feature (NORM+S Simple) instead.
5.6 Comparison with Different Parsers
As mentioned, the above reported results were
based on Charniak (2000)?s parser. It would be
interesting to examine the influence of different
parsers on the resolution performance. For this
purpose, we also tried the parser by Collins (1999)
47
(Mode II)7, and the results are shown in Table 6.
We can see that Charniak (2000)?s parser leads to
higher success rates for NPaper and BNews, while
Collins (1999)?s achieves better results for NWire.
However, the difference between the results of the
two parsers is not significant (less than 2% suc-
cess) for the three domains, no matter whether the
structured feature is used alone or in combination.
6 Conclusion
The purpose of this paper is to explore how to
make use of the structured syntactic knowledge to
do pronoun resolution. Traditionally, syntactic in-
formation from parse trees is represented as a set
of flat features. However, the features are usu-
ally selected and defined by heuristics and may
not necessarily capture all the syntactic informa-
tion provided by the parse trees. In the paper, we
propose a kernel-based method to incorporate the
information from parse trees. Specifically, we di-
rectly utilize the syntactic parse tree as a struc-
tured feature, and then apply kernels to such a fea-
ture, together with other normal features, to learn
the decision classifier and do the resolution. Our
experimental results on ACE data set show that
the system with the structured feature included
can achieve significant increase in the success rate
by around 5%?8%, for all the different domains.
The deeper analysis on various factors like training
size, feature set or parsers further proves that the
structured feature incorporated with our kernel-
based method is reliably effective for the pronoun
resolution task.
References
C. Aone and S. W. Bennett. 1995. Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. In Proceedings of the 33rd Annual
Meeting of the Association for Compuational Lin-
guistics, pages 122?129.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of North American chapter
of the Association for Computational Linguistics an-
nual meeting, pages 132?139.
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: kernels over discrete
structures and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association
7As in their pulic reports on Section 23 of WSJ TreeBank,
Charniak (2000)?s parser achieves 89.6% recall and 89.5%
precision with 0.88 crossing brackets (words ? 100), against
Collins (1999)?s 88.1% recall and 88.3% precision with 1.06
crossing brackets.
for Computational Linguistics (ACL?02), pages 263?
270.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. Hobbs. 1978. Resolving pronoun references. Lin-
gua, 44:339?352.
T. Joachims. 1999. Making large-scale svm learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
F. Keller and M. Lapata. 2003. Using the web to ob-
tain freqencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
C. Kennedy and B. Boguraev. 1996. Anaphora
for everyone: pronominal anaphra resolution with-
out a parser. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 113?118, Copenhagen, Denmark.
S. Lappin and H. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):525?561.
X. Luo and I. Zitouni. 2005. Milti-lingual coreference
resolution with syntactic features. In Proceedings of
Human Language Techonology conference and Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 660?667.
R. Mitkov. 1998. Robust pronoun resolution with lim-
ited knowledge. In Proceedings of the 17th Int. Con-
ference on Computational Linguistics, pages 869?
875.
A. Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL?04), pages 335?342.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 104?111,
Philadelphia.
W. Soon, H. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?
544.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improv-
ing pronoun resolution by incorporating coreferen-
tial information of candidates. In Proceedings of
42th Annual Meeting of the Association for Compu-
tational Linguistics, pages 127?134, Barcelona.
D. Zelenko, C. Aone, and A. Richardella. 2003. Ker-
nel methods for relation extraction. Journal of Ma-
chine Learning Research, 3(6):1083 ? 1106.
S. Zhao and R. Grishman. 2005. Extracting rela-
tions with integrated information using kernel meth-
ods. In Proceedings of 43rd Annual Meeting of the
Association for Computational Linguistics (ACL05),
pages 419?426.
48
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 121?128,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Modeling Commonality among Related Classes in Relation Extraction 
 
Zhou GuoDong      Su Jian      Zhang Min 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
Email: {zhougd, sujian, mzhang}@i2r.a-star.edu.sg 
 
Abstract 
This paper proposes a novel hierarchical learn-
ing strategy to deal with the data sparseness 
problem in relation extraction by modeling the 
commonality among related classes. For each 
class in the hierarchy either manually prede-
fined or automatically clustered, a linear dis-
criminative function is determined in a top-
down way using a perceptron algorithm with 
the lower-level weight vector derived from the 
upper-level weight vector. As the upper-level 
class normally has much more positive train-
ing examples than the lower-level class, the 
corresponding linear discriminative function 
can be determined more reliably. The upper-
level discriminative function then can effec-
tively guide the discriminative function learn-
ing in the lower-level, which otherwise might 
suffer from limited training data. Evaluation 
on the ACE RDC 2003 corpus shows that the 
hierarchical strategy much improves the per-
formance by 5.6 and 5.1 in F-measure on 
least- and medium- frequent relations respec-
tively. It also shows that our system outper-
forms the previous best-reported system by 2.7 
in F-measure on the 24 subtypes using the 
same feature set. 
1 Introduction 
With the dramatic increase in the amount of tex-
tual information available in digital archives and 
the WWW, there has been growing interest in 
techniques for automatically extracting informa-
tion from text. Information Extraction (IE) is 
such a technology that IE systems are expected 
to identify relevant information (usually of pre-
defined types) from text documents in a certain 
domain and put them in a structured format. 
According to the scope of the ACE program 
(ACE 2000-2005), current research in IE has 
three main objectives: Entity Detection and 
Tracking (EDT), Relation Detection and 
Characterization (RDC), and Event Detection 
and Characterization (EDC). This paper will 
focus on the ACE RDC task, which detects and 
classifies various semantic relations between two 
entities. For example, we want to determine 
whether a person is at a location, based on the 
evidence in the context. Extraction of semantic 
relationships between entities can be very useful 
for applications such as question answering, e.g. 
to answer the query ?Who is the president of the 
United States??.  
One major challenge in relation extraction is 
due to the data sparseness problem (Zhou et al
2005). As the largest annotated corpus in relation 
extraction, the ACE RDC 2003 corpus shows 
that different subtypes/types of relations are 
much unevenly distributed and a few relation 
subtypes, such as the subtype ?Founder? under 
the type ?ROLE?, suffers from a small amount of 
annotated data. Further experimentation in this 
paper (please see Figure 2) shows that most rela-
tion subtypes suffer from the lack of the training 
data and fail to achieve steady performance given 
the current corpus size. Given the relative large 
size of this corpus, it will be time-consuming and 
very expensive to further expand the corpus with 
a reasonable gain in performance. Even if we can 
somehow expend the corpus and achieve steady 
performance on major relation subtypes, it will 
be still far beyond practice for those minor sub-
types given the much unevenly distribution 
among different relation subtypes. While various 
machine learning approaches, such as generative 
modeling (Miller et al2000), maximum entropy 
(Kambhatla 2004) and support vector machines 
(Zhao and Grisman 2005; Zhou et al2005), have 
been applied in the relation extraction task, no 
explicit learning strategy is proposed to deal with 
the inherent data sparseness problem caused by 
the much uneven distribution among different 
relations.  
This paper proposes a novel hierarchical 
learning strategy to deal with the data sparseness 
problem by modeling the commonality among 
related classes. Through organizing various 
classes hierarchically, a linear discriminative 
function is determined for each class in a top-
down way using a perceptron algorithm with the 
lower-level weight vector derived from the up-
per-level weight vector. Evaluation on the ACE 
RDC 2003 corpus shows that the hierarchical 
121
strategy achieves much better performance than 
the flat strategy on least- and medium-frequent 
relations. It also shows that our system based on 
the hierarchical strategy outperforms the previ-
ous best-reported system. 
The rest of this paper is organized as follows. 
Section 2 presents related work. Section 3 
describes the hierarchical learning strategy using 
the perceptron algorithm. Finally, we present 
experimentation in Section 4 and conclude this 
paper in Section 5.  
2 Related Work 
The relation extraction task was formulated at 
MUC-7(1998). With the increasing popularity of 
ACE, this task is starting to attract more and 
more researchers within the natural language 
processing and machine learning communities. 
Typical works include Miller et al(2000), Ze-
lenko et al(2003), Culotta and Sorensen (2004), 
Bunescu and Mooney (2005a), Bunescu and 
Mooney  (2005b), Zhang et al(2005), Roth and 
Yih (2002), Kambhatla (2004), Zhao and Grisman  
(2005) and Zhou et al(2005). 
Miller et al(2000) augmented syntactic full 
parse trees with semantic information of entities 
and relations, and built generative models to in-
tegrate various tasks such as POS tagging, named 
entity recognition, template element extraction 
and relation extraction. The problem is that such 
integration may impose big challenges, e.g. the 
need of a large annotated corpus. To overcome 
the data sparseness problem, generative models 
typically applied some smoothing techniques to 
integrate different scales of contexts in parameter 
estimation, e.g. the back-off approach in Miller 
et al(2000).  
Zelenko et al(2003) proposed extracting re-
lations by computing kernel functions between 
parse trees. Culotta and Sorensen (2004) extended 
this work to estimate kernel functions between 
augmented dependency trees and achieved F-
measure of 45.8 on the 5 relation types in the 
ACE RDC 2003 corpus1. Bunescu and Mooney 
(2005a) proposed a shortest path dependency 
kernel. They argued that the information to 
model a relationship between two entities can be 
typically captured by the shortest path between 
them in the dependency graph. It achieved the F-
measure of 52.5 on the 5 relation types in the 
ACE RDC 2003 corpus. Bunescu and Mooney 
(2005b) proposed a subsequence kernel and ap-
                                                          
1 The ACE RDC 2003 corpus defines 5/24 relation 
types/subtypes between 4 entity types. 
plied it in protein interaction and ACE relation 
extraction tasks. Zhang et al(2005) adopted clus-
tering algorithms in unsupervised relation extrac-
tion using tree kernels. To overcome the data 
sparseness problem, various scales of sub-trees 
are applied in the tree kernel computation. Al-
though tree kernel-based approaches are able to 
explore the huge implicit feature space without 
much feature engineering, further research work 
is necessary to make them effective and efficient. 
Comparably, feature-based approaches 
achieved much success recently. Roth and Yih 
(2002) used the SNoW classifier to incorporate 
various features such as word, part-of-speech and 
semantic information from WordNet, and pro-
posed a probabilistic reasoning approach to inte-
grate named entity recognition and relation 
extraction. Kambhatla (2004) employed maxi-
mum entropy models with features derived from 
word, entity type, mention level, overlap, de-
pendency tree, parse tree and achieved F-
measure of 52.8 on the 24 relation subtypes in 
the ACE RDC 2003 corpus. Zhao and Grisman 
(2005) 2  combined various kinds of knowledge 
from tokenization, sentence parsing and deep 
dependency analysis through support vector ma-
chines and achieved F-measure of 70.1 on the 7 
relation types of the ACE RDC 2004 corpus3. 
Zhou et al(2005) further systematically explored 
diverse lexical, syntactic and semantic features 
through support vector machines and achieved F-
measure of 68.1 and 55.5 on the 5 relation types 
and the 24 relation subtypes in the ACE RDC 
2003 corpus respectively. To overcome the data 
sparseness problem, feature-based approaches 
normally incorporate various scales of contexts 
into the feature vector extensively. These ap-
proaches then depend on adopted learning algo-
rithms to weight and combine each feature 
effectively. For example, an exponential model 
and a linear model are applied in the maximum 
entropy models and support vector machines re-
spectively to combine each feature via the 
learned weight vector. 
In summary, although various approaches 
have been employed in relation extraction, they 
implicitly attack the data sparseness problem by 
using features of different contexts in feature-
based approaches or including different sub-
                                                          
2 Here, we classify this paper into feature-based ap-
proaches since the feature space in the kernels of 
Zhao and Grisman (2005) can be easily represented 
by an explicit feature vector. 
3 The ACE RDC 2004 corpus defines 7/27 relation 
types/subtypes between 7 entity types. 
122
structures in kernel-based approaches. Until now, 
there are no explicit ways to capture the hierar-
chical topology in relation extraction. Currently, 
all the current approaches apply the flat learning 
strategy which equally treats training examples 
in different relations independently and ignore 
the commonality among different relations. This 
paper proposes a novel hierarchical learning 
strategy to resolve this problem by considering 
the relatedness among different relations and 
capturing the commonality among related rela-
tions. By doing so, the data sparseness problem 
can be well dealt with and much better perform-
ance can be achieved, especially for those rela-
tions with small amounts of annotated examples.  
3 Hierarchical Learning Strategy 
Traditional classifier learning approaches apply 
the flat learning strategy. That is, they equally 
treat training examples in different classes 
independently and ignore the commonality 
among related classes. The flat strategy will not 
cause any problem when there are a large amount 
of training examples for each class, since, in this 
case, a classifier learning approach can always 
learn a nearly optimal discriminative function for 
each class against the remaining classes. How-
ever, such flat strategy may cause big problems 
when there is only a small amount of training 
examples for some of the classes. In this case, a 
classifier learning approach may fail to learn a 
reliable (or nearly optimal) discriminative func-
tion for a class with a small amount of training 
examples, and, as a result, may significantly af-
fect the performance of the class or even the 
overall performance. 
To overcome the inherent problems in the 
flat strategy, this paper proposes a hierarchical 
learning strategy which explores the inherent 
commonality among related classes through a 
class hierarchy. In this way, the training exam-
ples of related classes can help in learning a reli-
able discriminative function for a class with only 
a small amount of training examples. To reduce 
computation time and memory requirements, we 
will only consider linear classifiers and apply the 
simple and widely-used perceptron algorithm for 
this purpose with more options open for future 
research. In the following, we will first introduce 
the perceptron algorithm in linear classifier 
learning, followed by the hierarchical learning 
strategy using the perceptron algorithm. Finally, 
we will consider several ways in building the 
class hierarchy. 
3.1 Perceptron Algorithm 
_______________________________________ 
Input:  the initial weight vector w , the training 
example sequence 
TtYXyx tt ...,2,1,),( =?? and the number of 
the maximal iterations N (e.g. 10 in this 
paper) of the training sequence4  
Output: the weight vector w  for the linear 
discriminative function  xwf ?=  
BEGIN 
    ww =1  
    REPEAT for t=1,2,?,T*N 
1. Receive the instance nt Rx ?  
2. Compute the output ttt xwo ?=  
3. Give the prediction )( tt osigny =
?
 
4. Receive the desired label }1,1{ +??ty  
5. Update the hypothesis according to   
   ttttt xyww ?+=+1            (1) 
                where 0=t? if the margin of tw  at the 
given example ),( tt yx  0>? ttt xwy  
and 1=t?  otherwise 
    END REPEAT 
    Return 5/
4
1*?
?=
+=
N
Ni
iTww  
END BEGIN 
_______________________________________ 
Figure 1: the perceptron algorithm 
This section first deals with binary classification 
using linear classifiers. Assume an instance space 
nRX =  and a binary label space }1,1{ +?=Y . 
With any weight vector nRw?  and a given 
instance nRx? , we associate a linear classifier 
wh  with a linear discriminative function
5 
xwxf ?=)(  by )()( xwsignxhw ?=  , where 
1)( ?=? xwsign  if 0<? xw  and 1)( +=? xwsign  
otherwise. Here, the margin of w  at ),( tt yx  is 
defined as tt xwy ? . Then if the margin is positive, 
we have a correct prediction with tw yxh =)( , and 
if the margin is negative, we have an error with 
tw yxh ?)( . Therefore, given a sequence of 
training examples TtYXyx tt ...,2,1,),( =?? , 
linear classifier learning attemps to find a weight 
vector w  that achieves a positive margin on as 
many examples as possible. 
                                                          
4 The training example sequence is feed N times for 
better performance. Moreover, this number can con-
trol the maximal affect a training example can pose. 
This is similar to the regulation parameter C in 
SVM, which affects the trade-off between complex-
ity and proportion of non-separable examples. As a 
result, it can be used to control over-fitting and 
robustness. 
5 )( xw ?  denotes the dot product of the weight vector 
nRw?  and a given instance nRx? . 
123
The well-known perceptron algorithm, as 
shown in Figure 1, belongs to online learning of 
linear classifiers, where the learning algorithm 
represents its t -th hyposthesis by a weight vector 
n
t Rw ? . At trial t , an online algorithm receives 
an instance nt Rx ? , makes its prediction 
)( ttt xwsigny ?=
?
 and receives the desired label 
}1,1{ +??ty . What distinguishes different online 
algorithms is how they update tw  into 1+tw  based 
on the example ),( tt yx  received at trial t . In 
particular, the perceptron algorithm updates the 
hypothesis by adding a scalar multiple of the 
instance, as shown in Equation 1 of Figure 1, 
when there is an error. Normally, the tradictional 
perceptron algorithm initializes the hypothesis as 
the zero vector 01 =w . This is usually the most 
natural choice, lacking any other preference. 
Smoothing 
In order to further improve the performance, we 
iteratively feed the training examples for a possi-
ble better discriminative function. In this paper, 
we have set the maximal iteration number to 10 
for both efficiency and stable performance and 
the final weight vector in the discriminative func-
tion is averaged over those of the discriminative 
functions in the last few iterations (e.g. 5 in this 
paper).  
Bagging 
One more problem with any online classifier 
learning algorithm, including the perceptron al-
gorithm, is that the learned discriminative func-
tion somewhat depends on the feeding order of 
the training examples. In order to eliminate such 
dependence and further improve the perform-
ance, an ensemble technique, called bagging 
(Breiman 1996), is applied in this paper. In bag-
ging, the bootstrap technique is first used to build 
M (e.g. 10 in this paper) replicate sample sets by 
randomly re-sampling with replacement from the 
given training set repeatedly. Then, each training 
sample set is used to train a certain discrimina-
tive function. Finally, the final weight vector in 
the discriminative function is averaged over 
those of the M discriminative functions in the 
ensemble. 
Multi-Class Classification 
Basically, the perceptron algorithm is only for 
binary classification. Therefore, we must extend 
the perceptron algorithms to multi-class 
classification, such as the ACE RDC task. For 
efficiency, we apply the one vs. others strategy, 
which builds K classifiers so as to separate one 
class from all others. However, the outputs for 
the perceptron algorithms of different classes 
may be not directly comparable since any 
positive scalar multiple of the weight vector will 
not affect the actual prediction of a perceptron 
algorithm. For comparability, we map the 
perceptron algorithm output into the probability 
by using an additional sigmoid model: 
)exp(1
1)|1(
BAf
fyp ++==          (2) 
where xwf ?=  is the output of a perceptron 
algorithm and the coefficients A & B are to be 
trained using the model trust alorithm as 
described in Platt (1999). The final decision of an 
instance in multi-class classification is 
determined by the class which has the maximal 
probability from the corresponding perceptron 
algorithm.  
3.2 Hierarchical Learning Strategy using the 
Perceptron Algorithm 
Assume we have a class hierarchy for a task, e.g. 
the one in the ACE RDC 2003 corpus as shown 
in Table 1 of Section 4.1. The hierarchical learn-
ing strategy explores the inherent commonality 
among related classes in a top-down way. For 
each class in the hierarchy, a linear discrimina-
tive function is determined in a top-down way 
with the lower-level weight vector derived from 
the upper-level weight vector iteratively. This is 
done by initializing the weight vector in training 
the linear discriminative function for the lower-
level class as that of the upper-level class. That 
is, the lower-level discriminative function has the 
preference toward the discriminative function of 
its upper-level class. For an example, let?s look 
at the training of the ?Located? relation subtype 
in the class hierarchy as shown in Table 1: 
1) Train the weight vector of the linear 
discriminative function for the ?YES? 
relation vs. the ?NON? relation with the 
weight vector initialized as the zero vector. 
2) Train the weight vector of the linear 
discriminative function for the ?AT? relation 
type vs. all the remaining relation types 
(including the ?NON? relation) with the 
weight vector initialized as the weight vector 
of the linear discriminative function for the 
?YES? relation vs. the ?NON? relation. 
3) Train the weight vector of the linear 
discriminative function for the ?Located? 
relation subtype vs. all the remaining relation 
subtypes under all the relation types 
(including the ?NON? relation) with the 
124
weight vector initialized as the weight vector 
of the linear discriminative function for the 
?AT? relation type vs. all the remaining 
relation types. 
4) Return the above trained weight vector as the 
discriminatie function for the ?Located? 
relation subtype. 
In this way, the training examples in differ-
ent classes are not treated independently any 
more, and the commonality among related 
classes can be captured via the hierarchical learn-
ing strategy. The intuition behind this strategy is 
that the upper-level class normally has more 
positive training examples than the lower-level 
class so that the corresponding linear discrimina-
tive function can be determined more reliably. In 
this way, the training examples of related classes 
can help in learning a reliable discriminative 
function for a class with only a small amount of 
training examples in a top-down way and thus 
alleviate its data sparseness problem. 
3.3 Building the Class Hierarchy  
We have just described the hierarchical learning 
strategy using a given class hierarchy. Normally, 
a rough class hierarchy can be given manually 
according to human intuition, such as the one in 
the ACE RDC 2003 corpus. In order to explore 
more commonality among sibling classes, we 
make use of binary hierarchical clustering for 
sibling classes at both lowest and all levels. This 
can be done by first using the flat learning strat-
egy to learn the discriminative functions for indi-
vidual classes and then iteratively combining the 
two most related classes using the cosine similar-
ity function between their weight vectors in a 
bottom-up way. The intuition is that related 
classes should have similar hyper-planes to sepa-
rate from other classes and thus have similar 
weight vectors. 
? Lowest-level hybrid: Binary hierarchical 
clustering is only done at the lowest level 
while keeping the upper-level class hierar-
chy. That is, only sibling classes at the low-
est level are hierarchically clustered. 
? All-level hybrid: Binary hierarchical cluster-
ing is done at all levels in a bottom-up way. 
That is, sibling classes at the lowest level are 
hierarchically clustered first and then sibling 
classes at the upper-level. In this way, the bi-
nary class hierarchy can be built iteratively 
in a bottom-up way. 
 
 
4 Experimentation 
This paper uses the ACE RDC 2003 corpus pro-
vided by LDC to train and evaluate the hierarchi-
cal learning strategy. Same as Zhou et al(2005), 
we only model explicit relations and explicitly 
model the argument order of the two mentions 
involved.  
4.1 Experimental Setting 
Type Subtype Freq Bin Type 
AT Based-In 347 Medium 
 Located 2126 Large 
 Residence 308 Medium 
NEAR Relative-Location 201 Medium 
PART Part-Of 947 Large 
 Subsidiary 355 Medium 
 Other 6 Small 
ROLE Affiliate-Partner 204 Medium 
 Citizen-Of 328 Medium 
 Client 144 Small 
 Founder 26 Small 
 General-Staff 1331 Large 
 Management 1242 Large 
 Member 1091 Large 
 Owner 232 Medium 
 Other 158 Small 
SOCIAL Associate 91 Small 
 Grandparent 12 Small 
 Other-Personal 85 Small 
 Other-Professional 339 Medium 
 Other-Relative 78 Small 
 Parent 127 Small 
 Sibling 18 Small 
 Spouse 77 Small 
Table 1: Statistics of relation types and subtypes 
in the training data of the ACE RDC 2003 corpus 
(Note: According to frequency, all the subtypes 
are divided into three bins: large/ middle/ small, 
with 400 as the lower threshold for the large bin 
and 200 as the upper threshold for the small bin). 
The training data consists of 674 documents 
(~300k words) with 9683 relation examples 
while the held-out testing data consists of 97 
documents (~50k words) with 1386 relation ex-
amples. All the experiments are done five times 
on the 24 relation subtypes in the ACE corpus, 
except otherwise specified, with the final per-
formance averaged using the same re-sampling 
with replacement strategy as the one in the bag-
ging technique. Table 1 lists various types and 
subtypes of relations for the ACE RDC 2003 
corpus, along with their occurrence frequency in 
the training data. It shows that this corpus suffers 
from a small amount of annotated data for a few 
subtypes such as the subtype ?Founder? under 
the type ?ROLE?. 
For comparison, we also adopt the same fea-
ture set as Zhou et al(2005): word, entity type, 
125
mention level, overlap, base phrase chunking, 
dependency tree, parse tree and semantic infor-
mation. 
4.2 Experimental Results 
Table 2 shows the performance of the hierarchi-
cal learning strategy using the existing class hier-
archy in the given ACE corpus and its 
comparison with the flat learning strategy, using 
the perceptron algorithm. It shows that the pure 
hierarchical strategy outperforms the pure flat 
strategy by 1.5 (56.9 vs. 55.4) in F-measure. It 
also shows that further smoothing and bagging 
improve the performance of the hierarchical and 
flat strategies by 0.6 and 0.9 in F-measure re-
spectively. As a result, the final hierarchical 
strategy achieves F-measure of 57.8 and outper-
forms the final flat strategy by 1.8 in F-measure. 
Strategies  P R F 
Flat 58.2 52.8 55.4 
Flat+Smoothing 58.9 53.1 55.9 
Flat+Bagging 59.0 53.1 55.9 
Flat+Both 59.1 53.2 56.0 
Hierarchical 61.9 52.6 56.9 
Hierarchical+Smoothing 62.7 53.1 57.5 
Hierarchical+Bagging 62.9 53.1 57.6 
Hierarchical+Both 63.0 53.4 57.8 
Table 2: Performance of the hierarchical learning 
strategy using the existing class hierarchy and its 
comparison with the flat learning strategy 
Class Hierarchies P R F 
Existing 63.0 53.4 57.8 
Entirely Automatic 63.4 53.1 57.8 
Lowest-level Hybrid 63.6 53.5 58.1 
All-level Hybrid 63.6 53.6 58.2 
Table 3: Performance of the hierarchical learning 
strategy using different class hierarchies 
Table 3 compares the performance of the hi-
erarchical learning strategy using different class 
hierarchies. It shows that, the lowest-level hybrid 
approach, which only automatically updates the 
existing class hierarchy at the lowest level, im-
proves the performance by 0.3 in F-measure 
while further updating the class hierarchy at up-
per levels in the all-level hybrid approach only 
has very slight effect. This is largely due to the 
fact that the major data sparseness problem oc-
curs at the lowest level, i.e. the relation subtype 
level in the ACE corpus. As a result, the final 
hierarchical learning strategy using the class hi-
erarchy built with the all-level hybrid approach 
achieves F-measure of 58.2 in F-measure, which 
outperforms the final flat strategy by 2.2 in F-
measure. In order to justify the usefulness of our 
hierarchical learning strategy when a rough class 
hierarchy is not available and difficult to deter-
mine manually, we also experiment using en-
tirely automatically built class hierarchy (using 
the traditional binary hierarchical clustering algo-
rithm and the cosine similarity measurement) 
without considering the existing class hierarchy. 
Table 3 shows that using automatically built 
class hierarchy performs comparably with using 
only the existing one. 
With the major goal of resolving the data 
sparseness problem for the classes with a small 
amount of training examples, Table 4 compares 
the best-performed hierarchical and flat learning 
strategies on the relation subtypes of different   
training data sizes. Here, we divide various rela-
tion subtypes into three bins: large/middle/small, 
according to their available training data sizes. 
For the ACE RDC 2003 corpus, we use 400 as 
the lower threshold for the large bin6 and 200 as 
the upper threshold for the small bin7. As a re-
sult, the large/medium/small bin includes 5/8/11 
relation subtypes, respectively. Please see Table 
1 for details. Table 4 shows that the hierarchical 
strategy outperforms the flat strategy by 
1.0/5.1/5.6 in F-measure on the 
large/middle/small bin respectively. This indi-
cates that the hierarchical strategy performs 
much better than the flat strategy for those 
classes with a small or medium amount of anno-
tated examples although the hierarchical strategy 
only performs slightly better by 1.0 and 2.2 in F-
measure than the flat strategy on those classes 
with a large size of annotated corpus and on all 
classes as a whole respectively. This suggests 
that the proposed hierarchical strategy can well 
deal with the data sparseness problem in the 
ACE RDC 2003 corpus.  
An interesting question is about the similar-
ity between the linear discriminative functions 
learned using the hierarchical and flat learning 
strategies.  Table 4 compares the cosine similari-
ties between the weight vectors of the linear dis-
criminative functions using the two strategies for 
different bins, weighted by the training data sizes 
                                                          
6 The reason to choose this threshold is that no rela-
tion subtype in the ACE RC 2003 corpus has train-
ing examples in between 400 and 900. 
7 A few minor relation subtypes only have very few 
examples in the testing set. The reason to choose 
this threshold is to guarantee a reasonable number of 
testing examples in the small bin. For the ACE RC 
2003 corpus, using 200 as the upper threshold will 
fill the small bin with about 100 testing examples 
while using 100 will include too few testing exam-
ples for reasonable performance evaluation. 
126
of different relation subtypes. It shows that the 
linear discriminative functions learned using the 
two strategies are very similar (with the cosine 
similarity 0.98) for the relation subtypes belong-
ing to the large bin while the linear discrimina-
tive functions learned using the two strategies are 
not for the relation subtypes belonging to the 
medium/small bin with the cosine similarity 
0.92/0.81 respectively. This means that the use of 
the hierarchical strategy over the flat strategy 
only has very slight change on the linear dis-
criminative functions for those classes with a 
large amount of annotated examples while its 
effect on those with a small amount of annotated 
examples is obvious. This contributes to and ex-
plains (the degree of) the performance difference 
between the two strategies on the different train-
ing data sizes as shown in Table 4. 
Due to the difficulty of building a large an-
notated corpus, another interesting question is 
about the learning curve of the hierarchical learn-
ing strategy and its comparison with the flat 
learning strategy. Figure 2 shows the effect of 
different training data sizes for some major rela-
tion subtypes while keeping all the training ex-
amples of remaining relation subtypes. It shows 
that the hierarchical strategy performs much bet-
ter than the flat strategy when only a small 
amount of training examples is available. It also 
shows that the hierarchical strategy can achieve 
stable performance much faster than the flat 
strategy. Finally, it shows that the ACE RDC 
2003 task suffers from the lack of training exam-
ples. Among the three major relation subtypes, 
only the subtype ?Located? achieves steady per-
formance. 
Finally, we also compare our system with the 
previous best-reported systems, such as Kamb-
hatla  (2004) and Zhou et al(2005). Table 5 
shows that our system outperforms the previous 
best-reported system by 2.7 (58.2 vs. 55.5) in F-
measure, largely due to the gain in recall. It indi-
cates that, although support vector machines and 
maximum entropy models always perform better 
than the simple perceptron algorithm in most (if 
not all) applications, the hierarchical learning 
strategy using the perceptron algorithm can eas-
ily overcome the difference and outperforms the 
flat learning strategy using the overwhelming 
support vector machines and maximum entropy 
models in relation extraction, at least on the ACE 
RDC 2003 corpus. 
Large Bin (0.98) Middle Bin (0.92) Small Bin (0.81) Bin Type(cosine similarity) 
P R F P R F P R F 
Flat Strategy 62.3 61.9 62.1 60.8 38.7 47.3 33.0 21.7 26.2 
Hierarchical Strategy 66.4 60.2 63.1 67.6 42.7 52.4 40.2 26.3 31.8 
Table 4: Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ-
ent training data sizes. Notes: the figures in the parentheses indicate the cosine similarities between 
the weight vectors of the linear discriminative functions learned using the two strategies. 
10
20
30
40
50
60
70
20
0
40
0
60
0
80
0
10
00
12
00
14
00
16
00
18
00
20
00
Training Data Size
F
-m
ea
su
re
HS: General-Staff
FS: General-Staff
HS: Part-Of
FS: Part-Of
HS: Located
FS: Located
Figure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for some 
major relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy) 
Performance System 
P R F 
Our: Perceptron Algorithm + Hierarchical Strategy 63.6 53.6 58.2 
Zhou et al(2005): SVM + Flat Strategy 63.1 49.5 55.5 
Kambhatla (2004): Maximum Entropy + Flat Strategy 63.5 45.2 52.8 
Table 5: Comparison of our system with other best-reported systems 
127
5 Conclusion 
This paper proposes a novel hierarchical learning 
strategy to deal with the data sparseness problem 
in relation extraction by modeling the common-
ality among related classes. For each class in a 
class hierarchy, a linear discriminative function 
is determined in a top-down way using the per-
ceptron algorithm with the lower-level weight 
vector derived from the upper-level weight vec-
tor. In this way, the upper-level discriminative 
function can effectively guide the lower-level 
discriminative function learning. Evaluation on 
the ACE RDC 2003 corpus shows that the hier-
archical strategy performs much better than the 
flat strategy in resolving the critical data sparse-
ness problem in relation extraction. 
In the future work, we will explore the hier-
archical learning strategy using other machine 
learning approaches besides online classifier 
learning approaches such as the simple percep-
tron algorithm applied in this paper. Moreover, 
just as indicated in Figure 2, most relation sub-
types in the ACE RDC 2003 corpus (arguably 
the largest annotated corpus in relation extrac-
tion) suffer from the lack of training examples. 
Therefore, a critical research in relation extrac-
tion is how to rely on semi-supervised learning 
approaches (e.g. bootstrap) to alleviate its de-
pendency on a large amount of annotated training 
examples and achieve better and steadier per-
formance. Finally, our current work is done when 
NER has been perfectly done. Therefore, it 
would be interesting to see how imperfect NER 
affects the performance in relation extraction. 
This will be done by integrating the relation ex-
traction system with our previously developed 
NER system as described in Zhou and Su (2002). 
References  
ACE. (2000-2005). Automatic Content Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/  
Bunescu R. & Mooney R.J. (2005a). A shortest 
path dependency kernel for relation extraction. 
HLT/EMNLP?2005: 724-731. 6-8 Oct 2005. 
Vancover, B.C. 
Bunescu R. & Mooney R.J. (2005b). Subsequence 
Kernels for Relation Extraction  NIPS?2005. 
Vancouver, BC, December 2005  
Breiman L. (1996) Bagging Predictors. Machine 
Learning, 24(2): 123-140. 
Collins M. (1999).  Head-driven statistical models 
for natural language parsing. Ph.D. Dissertation, 
University of Pennsylvania. 
Culotta A. and Sorensen J. (2004). Dependency 
tree kernels for relation extraction. ACL?2004. 
423-429. 21-26 July 2004. Barcelona, Spain. 
Kambhatla N. (2004). Combining lexical, syntactic 
and semantic features with Maximum Entropy 
models for extracting relations. 
ACL?2004(Poster). 178-181. 21-26 July 2004. 
Barcelona, Spain. 
Miller G.A. (1990). WordNet: An online lexical 
database. International Journal of Lexicography. 
3(4):235-312. 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
(2000). A novel use of statistical parsing to ex-
tract information from text. ANLP?2000. 226-
233. 29 April  - 4 May 2000, Seattle, USA 
MUC-7. (1998). Proceedings of the 7th Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Mateo, CA. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and Comparisions to regular-
ized Likelihood Methods. In Advances in Large 
Margin Classifiers. Edited by Smola .J., Bartlett 
P., Scholkopf B. and Schuurmans D. MIT Press. 
Roth D. and Yih W.T. (2002). Probabilistic reason-
ing for entities and relation recognition. CoL-
ING?2002. 835-841.26-30 Aug 2002. Taiwan. 
Zelenko D., Aone C. and Richardella. (2003). Ker-
nel methods for relation extraction. Journal of 
Machine Learning Research. 3(Feb):1083-1106. 
Zhang M., Su J., Wang D.M., Zhou G.D. and Tan 
C.L. (2005). Discovering Relations from a Large 
Raw Corpus Using Tree Similarity-based Clus-
tering, IJCNLP?2005, Lecture Notes in 
Computer Science (LNCS 3651). 378-389. 11-16 
Oct 2005. Jeju Island, South Korea. 
Zhao S.B. and Grisman R. 2005. Extracting rela-
tions with integrated information using kernel 
methods. ACL?2005: 419-426. Univ of Michi-
gan-Ann Arbor? USA? 25-30 June 2005. 
Zhou G.D. and Su Jian. Named Entity Recogni-
tion Using a HMM-based Chunk Tagger, 
ACL?2002. pp473-480. Philadelphia. July 
2002.  
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). 
Exploring various knowledge in relation extrac-
tion. ACL?2005. 427-434. 25-30 June, Ann Ar-
bor, Michgan, USA. 
128
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 825?832,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Composite Kernel to Extract Relations between Entities with 
both Flat and Structured Features 
Min Zhang         Jie Zhang       Jian Su      Guodong Zhou 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, zhangjie, sujian, zhougd}@i2r.a-star.edu.sg 
 
Abstract 
This paper proposes a novel composite ker-
nel for relation extraction. The composite 
kernel consists of two individual kernels: an 
entity kernel that allows for entity-related 
features and a convolution parse tree kernel 
that models syntactic information of relation 
examples. The motivation of our method is 
to fully utilize the nice properties of kernel 
methods to explore diverse knowledge for 
relation extraction. Our study illustrates that 
the composite kernel can effectively capture 
both flat and structured features without the 
need for extensive feature engineering, and 
can also easily scale to include more fea-
tures. Evaluation on the ACE corpus shows 
that our method outperforms the previous 
best-reported methods and significantly out-
performs previous two dependency tree ker-
nels for relation extraction. 
1 Introduction 
The goal of relation extraction is to find various 
predefined semantic relations between pairs of 
entities in text. The research on relation extrac-
tion has been promoted by the Message Under-
standing Conferences (MUCs) (MUC, 1987-
1998) and Automatic Content Extraction (ACE) 
program (ACE, 2002-2005). According to the 
ACE Program, an entity is an object or set of ob-
jects in the world and a relation is an explicitly 
or implicitly stated relationship among entities. 
For example, the sentence ?Bill Gates is chair-
man and chief software architect of Microsoft 
Corporation.? conveys the ACE-style relation 
?EMPLOYMENT.exec? between the entities 
?Bill Gates? (PERSON.Name) and ?Microsoft 
Corporation? (ORGANIZATION. Commercial).  
In this paper, we address the problem of rela-
tion extraction using kernel methods (Sch?lkopf 
and Smola, 2001). Many feature-based learning 
algorithms involve only the dot-product between 
feature vectors. Kernel methods can be regarded 
as a generalization of the feature-based methods 
by replacing the dot-product with a kernel func-
tion between two vectors, or even between two 
objects. A kernel function is a similarity function 
satisfying the properties of being symmetric and 
positive-definite. Recently, kernel methods are 
attracting more interests in the NLP study due to 
their ability of implicitly exploring huge amounts 
of structured features using the original represen-
tation of objects. For example, the kernels for 
structured natural language data, such as parse 
tree kernel (Collins and Duffy, 2001), string ker-
nel (Lodhi et al, 2002) and graph kernel (Suzuki 
et al, 2003) are example instances of the well-
known convolution kernels1 in NLP. In relation 
extraction, typical work on kernel methods in-
cludes: Zelenko et al (2003), Culotta and Soren-
sen (2004) and Bunescu and Mooney (2005). 
This paper presents a novel composite kernel 
to explore diverse knowledge for relation extrac-
tion. The composite kernel consists of an entity 
kernel and a convolution parse tree kernel. Our 
study demonstrates that the composite kernel is 
very effective for relation extraction. It also 
shows without the need for extensive feature en-
gineering the composite kernel can not only cap-
ture most of the flat features used in the previous 
work but also exploit the useful syntactic struc-
ture features effectively. An advantage of our 
method is that the composite kernel can easily 
cover more knowledge by introducing more ker-
nels. Evaluation on the ACE corpus shows that 
our method outperforms the previous best-
reported methods and significantly outperforms 
the previous kernel methods due to its effective 
exploration of various syntactic features. 
The rest of the paper is organized as follows. 
In Section 2, we review the previous work. Sec-
tion 3 discusses our composite kernel. Section 4 
reports the experimental results and our observa-
tions. Section 5 compares our method with the 
                                                 
1 Convolution kernels were proposed for a discrete structure 
by Haussler (1999) in the machine learning field. This 
framework defines a kernel between input objects by apply-
ing convolution ?sub-kernels? that are the kernels for the 
decompositions (parts) of the objects.  
825
previous work from the viewpoint of feature ex-
ploration. We conclude our work and indicate the 
future work in Section 6. 
2 Related Work 
Many techniques on relation extraction, such as 
rule-based (MUC, 1987-1998; Miller et al, 
2000), feature-based (Kambhatla 2004; Zhou et 
al., 2005) and kernel-based (Zelenko et al, 2003; 
Culotta and Sorensen, 2004; Bunescu and 
Mooney, 2005), have been proposed in the litera-
ture. 
Rule-based methods for this task employ a 
number of linguistic rules to capture various rela-
tion patterns. Miller et al (2000) addressed the 
task from the syntactic parsing viewpoint and 
integrated various tasks such as POS tagging, NE 
tagging, syntactic parsing, template extraction 
and relation extraction using a generative model. 
Feature-based methods (Kambhatla, 2004; 
Zhou et al, 2005; Zhao and Grishman, 20052) 
for this task employ a large amount of diverse 
linguistic features, such as lexical, syntactic and 
semantic features. These methods are very effec-
tive for relation extraction and show the best-
reported performance on the ACE corpus. How-
ever, the problems are that these diverse features 
have to be manually calibrated and the hierarchi-
cal structured information in a parse tree is not 
well preserved in their parse tree-related features, 
which only represent simple flat path informa-
tion connecting two entities in the parse tree 
through a path of non-terminals and a list of base 
phrase chunks. 
Prior kernel-based methods for this task focus 
on using individual tree kernels to exploit tree 
structure-related features. Zelenko et al (2003) 
developed a kernel over parse trees for relation 
extraction. The kernel matches nodes from roots 
to leaf nodes recursively layer by layer in a top-
down manner. Culotta and Sorensen (2004) gen-
eralized it to estimate similarity between depend-
ency trees. Their tree kernels require the match-
able nodes to be at the same layer counting from 
the root and to have an identical path of ascend-
ing nodes from the roots to the current nodes. 
The two constraints make their kernel high preci-
sion but very low recall on the ACE 2003 corpus. 
Bunescu and Mooney (2005) proposed another 
dependency tree kernel for relation extraction. 
                                                 
2 We classify the feature-based kernel defined in (Zhao and 
Grishman, 2005) into the feature-based methods since their 
kernels can be easily represented by the dot-products be-
tween explicit feature vectors. 
Their kernel simply counts the number of com-
mon word classes at each position in the shortest 
paths between two entities in dependency trees. 
The kernel requires the two paths to have the 
same length; otherwise the kernel value is zero. 
Therefore, although this kernel shows perform-
ance improvement over the previous one (Culotta 
and Sorensen, 2004), the constraint makes the 
two dependency kernels share the similar behav-
ior: good precision but much lower recall on the 
ACE corpus. 
The above discussion shows that, although 
kernel methods can explore the huge amounts of 
implicit (structured) features, until now the fea-
ture-based methods enjoy more success. One 
may ask: how can we make full use of the nice 
properties of kernel methods and define an effec-
tive kernel for relation extraction? 
In this paper, we study how relation extraction 
can benefit from the elegant properties of kernel 
methods: 1) implicitly exploring (structured) fea-
tures in a high dimensional space; and 2) the nice 
mathematical properties, for example, the sum, 
product, normalization and polynomial expan-
sion of existing kernels is a valid kernel 
(Sch?lkopf and Smola, 2001). We also demon-
strate how our composite kernel effectively cap-
tures the diverse knowledge for relation extrac-
tion.  
3 Composite Kernel for Relation Ex-
traction  
In this section, we define the composite kernel 
and study the effective representation of a rela-
tion instance. 
3.1 Composite Kernel 
Our composite kernel consists of an entity kernel 
and a convolution parse tree kernel. To our 
knowledge, convolution kernels have not been 
explored for relation extraction. 
 
(1) Entity Kernel: The ACE 2003 data defines 
four entity features: entity headword, entity type 
and subtype (only for GPE), and mention type 
while the ACE 2004 data makes some modifica-
tions and introduces a new feature ?LDC men-
tion type?. Our statistics on the ACE data reveals 
that the entity features impose a strong constraint 
on relation types. Therefore, we design a linear 
kernel to explicitly capture such features: 
1 2 1 21,2
( , ) ( . , . )L E i iiK R R K R E R E== ?  (1) 
where 1R and 2R stands for two relation instances, 
Ei means the ith entity of a relation instance, and 
826
( , )EK ? ?  is a simple kernel function over the fea-
tures of entities: 
1 2 1 2( , ) ( . , . )E i iiK E E C E f E f=? (2) 
where if represents the i
th entity feature, and the 
function ( , )C ? ?  returns 1 if the two feature val-
ues are identical and 0 otherwise. ( , )EK ? ?  re-
turns the number of feature values in common of 
two entities. 
 
(2) Convolution Parse Tree Kernel: A convo-
lution kernel aims to capture structured informa-
tion in terms of substructures. Here we use the 
same convolution parse tree kernel as described 
in Collins and Duffy (2001) for syntactic parsing 
and Moschitti (2004) for semantic role labeling. 
Generally, we can represent a parse tree T  by a 
vector of integer counts of each sub-tree type 
(regardless of its ancestors):  
 
( )T? = (# subtree1(T), ?, # subtreei(T), ?,  # 
subtreen(T) ) 
 
where # subtreei(T) is the occurrence number of 
the ith sub-tree type (subtreei) in T. Since the 
number of different sub-trees is exponential with 
the parse tree size, it is computationally infeasi-
ble to directly use the feature vector ( )T? . To 
solve this computational issue, Collins and Duffy 
(2001) proposed the following parse tree kernel 
to calculate the dot product between the above 
high dimensional vectors implicitly. 
 
 
1 1 2 2
1 1 2 2
1 2 1 2
1 2
1 2
1 2
( , ) ( ), ( )
   # ( ) # ( )
   ( ) ( )
   ( , )
( ) ( )
i i
i ii
subtree subtreei n N n N
n N n N
K T T T T
subtree T subtree T
I n I n
n n
? ?
? ?
? ?
=< >
=
=
= ?
?
?
?
? ? ?
? ?
(3) 
 
where N1 and N2 are the sets of nodes in trees T1 
and T2, respectively, and ( )
isubtree
I n  is a function 
that is 1 iff the subtreei occurs with root at node n 
and zero otherwise, and 1 2( , )n n?  is the number of 
the common subtrees rooted at n1 and n2, i.e. 
 
1 2 1 2( , ) ( ) ( )i isubtree subtreein n I n I n? = ??  
 
1 2( , )n n? can be computed by the following recur-
sive rules:  
(1) if the productions (CFP rules) at 1n  and 2n  
are different, 1 2( , ) 0n n? = ; 
(2) else if both 1n  and 2n  are pre-terminals (POS 
tags), 1 2( , ) 1n n ?? = ? ; 
(3) else, 1( )1 2 1 21( , ) (1 ( ( , ), ( , )))
nc n
j
n n ch n j ch n j? =? = +?? ,  
where 1( )nc n is the child number of 1n , ch(n,j) is 
the jth child of node n  and? (0<? <1) is the de-
cay factor in order to make the kernel value less 
variable with respect to the subtree sizes. In ad-
dition, the recursive rule (3) holds because given 
two nodes with the same children, one can con-
struct common sub-trees using these children and 
common sub-trees of further offspring.  
The parse tree kernel counts the number of 
common sub-trees as the syntactic similarity 
measure between two relation instances. The 
time complexity for computing this kernel 
is 1 2(| | | |)O N N? . 
In this paper, two composite kernels are de-
fined by combing the above two individual ker-
nels in the following ways: 
 
1) Linear combination: 
 
1 1 2 1 2 1 2
? ?( , ) ( , ) (1 ) ( , )LK R R K R R K T T? ?? ?= + ? (4) 
 
Here, ? ( , )K ? ?  is the normalized3 ( , )K ? ? and ?  
is the coefficient. Evaluation on the development 
set shows that this composite kernel yields the 
best performance when ? is set to 0.4. 
 
2) Polynomial expansion: 
 
2 1 2 1 2 1 2
? ?( , ) ( , ) (1 ) ( , )PLK R R K R R K T T? ?? ?= + ? (5) 
 
Here, ? ( , )K ? ?  is the normalized ( , )K ? ? , ( , )pK ? ?  
is the polynomial expansion of ( , )K ? ?  with de-
gree d=2, i.e. 2( , ) ( ( , ) 1)pK K? ? ? ?= + , and ?  is the 
coefficient. Evaluation on the development set 
shows that this composite kernel yields the best 
performance when ? is set to 0.23. 
The polynomial expansion aims to explore the 
entity bi-gram features, esp. the combined fea-
tures from the first and second entities, respec-
tively. In addition, due to the different scales of 
the values of the two individual kernels, they are 
normalized before combination. This can avoid 
one kernel value being overwhelmed by that of 
another one.  
The entity kernel formulated by eqn. (1) is a 
proper kernel since it simply calculates the dot 
product of the entity feature vectors. The tree 
kernel formulated by eqn. (3) is proven to be a 
proper kernel (Collins and Duffy, 2001). Since 
kernel function set is closed under normalization, 
polynomial expansion and linear combination 
(Sch?lkopf and Smola, 2001), the two composite 
kernels are also proper kernels. 
                                                 
3  A kernel ( , )K x y  can be normalized by dividing it by 
( , ) ( , )K x x K y y? .  
827
3.2 Relation Instance Spaces 
A relation instance is encapsulated by a parse 
tree. Thus, it is critical to understand which por-
tion of a parse tree is important in the kernel cal-
culation. We study five cases as shown in Fig.1. 
 
(1) Minimum Complete Tree (MCT): the com-
plete sub-tree rooted by the nearest common an-
cestor of the two entities under consideration. 
 
(2) Path-enclosed Tree (PT): the smallest com-
mon sub-tree including the two entities. In other 
words, the sub-tree is enclosed by the shortest 
path linking the two entities in the parse tree (this 
path is also commonly-used as the path tree fea-
ture in the feature-based methods). 
 
(3) Context-Sensitive Path Tree (CPT): the PT 
extended with the 1st left word of entity 1 and the 
1st right word of entity 2. 
 
(4) Flattened Path-enclosed Tree (FPT): the 
PT with the single in and out arcs of non-
terminal nodes (except POS nodes) removed. 
 
(5) Flattened CPT (FCPT): the CPT with the 
single in and out arcs of non-terminal nodes (ex-
cept POS nodes) removed.  
 
Fig. 1 illustrates different representations of an 
example relation instance. T1 is MCT for the 
relation instance, where the sub-tree circled by a 
dashed line is PT, which is also shown in T2 for 
clarity. The only difference between MCT and 
PT lies in that MCT does not allow partial pro-
duction rules (for example, NP?PP is a partial 
production rule while NP?NP+PP is an entire 
production rule in the top of T2). For instance, 
only the most-right child in the most-left sub-tree 
[NP [CD 200] [JJ domestic] [E1-PER ?]] of T1 
is kept in T2. By comparing the performance of 
T1 and T2, we can evaluate the effect of sub-trees 
with partial production rules as shown in T2 and 
the necessity of keeping the whole left and right 
context sub-trees as shown in T1 in relation ex-
traction. T3 is CPT, where the two sub-trees cir-
cled by dashed lines are included as the context 
to T2 and make T3 context-sensitive. This is to 
evaluate whether the limited context information 
in CPT can boost performance. FPT in T4 is 
formed by removing the two circled nodes in T2. 
This is to study whether and how the elimination 
of single non-terminal nodes affects the perform-
ance of relation extraction.  
T1): MCT T2): PT 
T3):CPT T4): FPT 
Figure 1. Different representations of a relation instance in the example sentence ??provide bene-
fits to 200 domestic partners of their own workers in New York?, where the phrase type 
?E1-PER? denotes that the current node is the 1st entity with type ?PERSON?, and like-
wise for the others. The relation instance is excerpted from the ACE 2003 corpus, where 
a relation ?SOCIAL.Other-Personal? exists between entities ?partners? (PER) and 
?workers? (PER). We use Charniak?s parser (Charniak, 2001) to parse the example sen-
tence. To save space, the FCPT is not shown here. 828
4 Experiments 
4.1 Experimental Setting 
Data: We use the English portion of both the 
ACE 2003 and 2004 corpora from LDC in our 
experiments. In the ACE 2003 data, the training 
set consists of 674 documents and 9683 relation 
instances while the test set consists of 97 docu-
ments and 1386 relation instances. The ACE 
2003 data defines 5 entity types, 5 major relation 
types and 24 relation subtypes. The ACE 2004 
data contains 451 documents and 5702 relation 
instances. It redefines 7 entity types, 7 major re-
lation types and 23 subtypes. Since Zhao and 
Grishman (2005) use a 5-fold cross-validation on 
a subset of the 2004 data (newswire and broad-
cast news domains, containing 348 documents 
and 4400 relation instances), for comparison, we 
use the same setting (5-fold cross-validation on 
the same subset of the 2004 data, but the 5 parti-
tions may not be the same) for the ACE 2004 
data. Both corpora are parsed using Charniak?s 
parser (Charniak, 2001). We iterate over all pairs 
of entity mentions occurring in the same sen-
tence to generate potential relation instances. In 
this paper, we only measure the performance of 
relation extraction models on ?true? mentions 
with ?true? chaining of coreference (i.e. as anno-
tated by LDC annotators). 
 
Implementation: We formalize relation extrac-
tion as a multi-class classification problem. SVM 
is selected as our classifier. We adopt the one vs. 
others strategy and select the one with the largest 
margin as the final answer. The training parame-
ters are chosen using cross-validation (C=2.4 
(SVM); ? =0.4(tree kernel)). In our implementa-
tion, we use the binary SVMLight (Joachims, 
1998) and Tree Kernel Tools (Moschitti, 2004). 
Precision (P), Recall (R) and F-measure (F) are 
adopted to measure the performance. 
4.2 Experimental Results 
In this subsection, we report the experiments of 
different kernel setups for different purposes. 
 
(1) Tree Kernel only over Different Relation 
Instance Spaces: In order to better study the im-
pact of the syntactic structure information in a 
parse tree on relation extraction, we remove the 
entity-related information from parse trees by 
replacing the entity-related phrase types (?E1-
PER? and so on as shown in Fig. 1) with ?NP?. 
Table 1 compares the performance of 5 tree ker-
nel setups on the ACE 2003 data using the tree 
structure information only. It shows that:  
 
? Overall the five different relation instance 
spaces are all somewhat effective for relation 
extraction. This suggests that structured syntactic 
information has good predication power for rela-
tion extraction and the structured syntactic in-
formation can be well captured by the tree kernel.  
? MCT performs much worse than the others. 
The reasons may be that MCT includes too 
much left and right context information, which 
may introduce many noisy features and cause 
over-fitting (high precision and very low recall 
as shown in Table 1). This suggests that only 
keeping the complete (not partial) production 
rules in MCT does harm performance. 
? PT achieves the best performance. This means 
that only keeping the portion of a parse tree en-
closed by the shortest path between entities can 
model relations better than all others. This may 
be due to that most significant information is 
with PT and including context information may 
introduce too much noise. Although context 
may include some useful information, it is still a 
problem to correctly utilize such useful informa-
tion in the tree kernel for relation extraction. 
? CPT performs a bit worse than PT. In some 
cases (e.g. in sentence ?the merge of company A 
and company B?.?, ?merge? is a critical con-
text word), the context information is helpful. 
However, the effective scope of context is hard 
to determine given the complexity and variabil-
ity of natural languages. 
? The two flattened trees perform worse than the 
original trees. This suggests that the single non-
terminal nodes are useful for relation extraction.  
Evaluation on the ACE 2004 data also shows 
that PT achieves the best performance (72.5/56.7 
/63.6 in P/R/F). More evaluations with the entity 
type and order information incorporated into tree 
nodes (?E1-PER?, ?E2-PER? and ?E-GPE? as 
shown in Fig. 1) also show that PT performs best 
with 76.1/62.6/68.7 in P/R/F on the 2003 data 
and 74.1/62.4/67.7 in P/R/F on the 2004 data. 
 
Instance Spaces P(%) R(%) F 
Minimum Complete Tree 
(MCT) 77.5 38.4 51.3 
Path-enclosed Tree (PT) 72.8 53.8 61.9 
Context-Sensitive PT(CPT) 75.9 48.6 59.2 
Flattened PT 72.7 51.7 60.4 
Flattened CPT 76.1 47.2 58.2 
 
Table 1. five different tree kernel setups on the 
ACE 2003 five major types using the parse 
tree structure information only (regardless of 
any entity-related information) 
829
PTs (with Tree Struc-
ture Information only) 
P(%) R(%) F 
Entity kernel only 75.1 
(79.5) 
42.7 
(34.6)
54.4 
(48.2) 
Tree kernel only 72.5 
(72.8) 
56.7 
(53.8)
63.6 
(61.9) 
Composite kernel 1 
(linear combination) 
73.5 
(76.3) 
67.0 
(63.0)
70.1 
(69.1) 
Composite kernel 2 
(polynomial expansion)
76.1 
(77.3) 
68.4 
(65.6)
72.1 
(70.9) 
 
Table 2. Performance comparison of different 
kernel setups over the ACE major types of 
both the 2003 data (the numbers in parenthe-
ses) and the 2004 data (the numbers outside 
parentheses) 
 
(2) Composite Kernels: Table 2 compares the 
performance of different kernel setups on the 
ACE major types. It clearly shows that:  
? The composite kernels achieve significant per-
formance improvement over the two individual 
kernels. This indicates that the flat and the struc-
tured features are complementary and the com-
posite kernels can well integrate them: 1) the 
flat entity information captured by the entity 
kernel; 2) the structured syntactic connection 
information between the two entities captured 
by the tree kernel. 
 
? The composite kernel via the polynomial ex-
pansion outperforms the one via the linear com-
bination by ~2 in F-measure. It suggests that the 
bi-gram entity features are very useful.  
 
? The entity features are quite useful, which can 
achieve F-measures of 54.4/48.2 alone and can 
boost the performance largely by ~7 (70.1-
63.2/69.1-61.9) in F-measure when combining 
with the tree kernel.  
 
? It is interesting that the ACE 2004 data shows 
consistent better performance on all setups than 
the 2003 data although the ACE 2003 data is 
two times larger than the ACE 2004 data. This 
may be due to two reasons: 1) The ACE 2004 
data defines two new entity types and re-defines 
the relation types and subtypes in order to re-
duce the inconsistency between LDC annota-
tors. 2) More importantly, the ACE 2004 data 
defines 43 entity subtypes while there are only 3 
subtypes in the 2003 data. The detailed classifi-
cation in the 2004 data leads to significant per-
formance improvement of 6.2 (54.4-48.2) in F-
measure over that on the 2003 data. 
Our composite kernel can achieve 
77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over 
the ACE 2003/2004 major types, respectively. 
Methods (2002/2003 data) P(%) R(%) F 
Ours: composite kernel 2 
(polynomial expansion) 
77.3 
(64.9) 
65.6 
(51.2) 
70.9 
(57.2) 
Zhou et al (2005):  
feature-based SVM 
77.2 
(63.1) 
60.7 
(49.5) 
68.0 
(55.5) 
Kambhatla (2004):  
feature-based ME 
 (-) 
(63.5) 
 (-) 
(45.2) 
 (-) 
(52.8) 
Ours: tree kernel with en-
tity information at node 
76.1 
(62.4) 
62.6 
(48.5) 
68.7 
(54.6) 
Bunescu and Mooney 
(2005): shortest path de-
pendency kernel 
65.5 
(-) 
43.8 
(-) 
52.5 
(-) 
Culotta and Sorensen 
(2004): dependency kernel 
67.1 
(-) 
35.0 
(-) 
45.8 
(-) 
 
Table 3. Performance comparison on the ACE 
2003/2003 data over both 5 major types (the 
numbers outside parentheses) and 24 subtypes 
(the numbers in parentheses)  
 
Methods (2004 data) P(%) R(%) F 
Ours: composite kernel 2 
(polynomial expansion) 
76.1 
 (68.6) 
68.4 
(59.3)
72.1 
 (63.6)
Zhao and Grishman (2005): 
feature-based kernel 
69.2 
(-) 
70.5 
(-) 
70.4 
(-) 
 
Table 4. Performance comparison on the ACE 
2004 data over both 7 major types (the numbers 
outside parentheses) and 23 subtypes (the num-
bers in parentheses) 
 
(3) Performance Comparison: Tables 3 and 4 
compare our method with previous work on the 
ACE 2002/2003/2004 data, respectively. They 
show that our method outperforms the previous 
methods and significantly outperforms the previ-
ous two dependency kernels4. This may be due to 
two reasons: 1) the dependency tree (Culotta and 
Sorensen, 2004) and the shortest path (Bunescu 
and Mooney, 2005) lack the internal hierarchical 
phrase structure information, so their correspond-
ing kernels can only carry out node-matching 
directly over the nodes with word tokens; 2) the 
parse tree kernel has less constraints. That is, it is 
                                                 
4 Bunescu and Mooney (2005) used the ACE 2002 corpus, 
including 422 documents, which is known to have many 
inconsistencies than the 2003 version. Culotta and Sorensen 
(2004) used a generic ACE corpus including about 800 
documents (no corpus version is specified). Since the testing 
corpora are in different sizes and versions, strictly speaking, 
it is not ready to compare these methods exactly and fairly. 
Therefore Table 3 is only for reference purpose. We just 
hope that we can get a few clues from this table. 
830
not restricted by the two constraints of the two 
dependency kernels (identical layer and ances-
tors for the matchable nodes and identical length 
of two shortest paths, as discussed in Section 2).  
 
The above experiments verify the effective-
ness of our composite kernels for relation extrac-
tion. They suggest that the parse tree kernel can 
effectively explore the syntactic features which 
are critical for relation extraction.  
 
# of error instances Error Type 
  2004 data 2003 data 
False Negative 198  416 
False Positive 115 171 
Cross Type 62 96 
 
Table 5. Error distribution of major types on 
both the 2003 and 2004 data for the compos-
ite kernel by polynomial expansion 
 
(4) Error Analysis: Table 5 reports the error 
distribution of the polynomial composite kernel 
over the major types on the ACE data. It shows 
that 83.5%(198+115/198+115+62) / 85.8%(416 
+171/416+171+96) of the errors result from rela-
tion detection and only 16.5%/14.2% of the er-
rors result from relation characterization. This 
may be due to data imbalance and sparseness 
issues since we find that the negative samples are 
8 times more than the positive samples in the 
training set. Nevertheless, it clearly directs our 
future work. 
5 Discussion 
In this section, we compare our method with the 
previous work from the feature engineering 
viewpoint and report some other observations 
and issues in our experiments. 
5.1 Comparison with Previous Work 
This is to explain more about why our method 
performs better and significantly outperforms the 
previous two dependency tree kernels from the 
theoretical viewpoint. 
(1) Compared with Feature-based Methods: 
The basic difference lies in the relation instance 
representation (parse tree vs. feature vector) and 
the similarity calculation mechanism (kernel 
function vs. dot-product). The main difference is 
the different feature spaces. Regarding the parse 
tree features, our method implicitly represents a 
parse tree by a vector of integer counts of each 
sub-tree type, i.e., we consider the entire sub-tree 
types and their occurring frequencies. In this way, 
the parse tree-related features (the path features 
and the chunking features) used in the feature-
based methods are embedded (as a subset) in our 
feature space. Moreover, the in-between word 
features and the entity-related features used in 
the feature-based methods are also captured by 
the tree kernel and the entity kernel, respectively. 
Therefore our method has the potential of effec-
tively capturing not only most of the previous 
flat features but also the useful syntactic struc-
ture features. 
 
(2) Compared with Previous Kernels: Since 
our method only counts the occurrence of each 
sub-tree without considering the layer and the 
ancestors of the root node of the sub-tree, our 
method is not limited by the constraints (identi-
cal layer and ancestors for the matchable nodes, 
as discussed in Section 2) in Culotta and Soren-
sen (2004). Moreover, the difference between 
our method and Bunescu and Mooney (2005) is 
that their kernel is defined on the shortest path 
between two entities instead of the entire sub-
trees. However, the path does not maintain the 
tree structure information. In addition, their ker-
nel requires the two paths to have the same 
length. Such constraint is too strict. 
5.2 Other Issues 
(1) Speed Issue: The recursively-defined convo-
lution kernel is much slower compared to fea-
ture-based classifiers. In this paper, the speed 
issue is solved in three ways. First, the inclusion 
of the entity kernel makes the composite kernel 
converge fast. Furthermore, we find that the 
small portion (PT) of a full parse tree can effec-
tively represent a relation instance. This signifi-
cantly improves the speed. Finally, the parse tree 
kernel requires exact match between two sub-
trees, which normally does not occur very fre-
quently. Collins and Duffy (2001) report that in 
practice, running time for the parse tree kernel is 
more close to linear (O(|N1|+|N2|), rather than 
O(|N1|*|N2| ). As a result, using the PC with Intel 
P4 3.0G CPU and 2G RAM, our system only 
takes about 110 minutes and 30 minutes to do 
training on the ACE 2003 (~77k training in-
stances) and 2004 (~33k training instances) data, 
respectively.  
(2) Further Improvement: One of the potential 
problems in the parse tree kernel is that it carries 
out exact matches between sub-trees, so that this 
kernel fails to handle sparse phrases (i.e. ?a car? 
vs. ?a red car?) and near-synonymic grammar 
tags (for example, the variations of a verb (i.e. 
go, went, gone)). To some degree, it could possi-
bly lead to over-fitting and compromise the per-
831
formance. However, the above issues can be 
handled by allowing grammar-driven partial rule 
matching and other approximate matching 
mechanisms in the parse tree kernel calculation. 
Finally, it is worth noting that by introducing 
more individual kernels our method can easily 
scale to cover more features from a multitude of 
sources (e.g. Wordnet, gazetteers, etc) that can 
be brought to bear on the task of relation extrac-
tion. In addition, we can also easily implement 
the feature weighting scheme by adjusting the 
eqn.(2) and the rule (2) in calculating 1 2( , )n n?  
(see subsection 3.1). 
6 Conclusion and Future Work 
Kernel functions have nice properties. In this 
paper, we have designed a composite kernel for 
relation extraction. Benefiting from the nice 
properties of the kernel methods, the composite 
kernel could well explore and combine the flat 
entity features and the structured syntactic fea-
tures, and therefore outperforms previous best-
reported feature-based methods on the ACE cor-
pus. To our knowledge, this is the first research 
to demonstrate that, without the need for exten-
sive feature engineering, an individual tree ker-
nel achieves comparable performance with the 
feature-based methods. This shows that the syn-
tactic features embedded in a parse tree are par-
ticularly useful for relation extraction and which 
can be well captured by the parse tree kernel. In 
addition, we find that the relation instance repre-
sentation (selecting effective portions of parse 
trees for kernel calculations) is very important 
for relation extraction. 
The most immediate extension of our work is 
to improve the accuracy of relation detection. 
This can be done by capturing more features by 
including more individual kernels, such as the 
WordNet-based semantic kernel (Basili et al, 
2005) and other feature-based kernels. We can 
also benefit from machine learning algorithms to 
study how to solve the data imbalance and 
sparseness issues from the learning algorithm 
viewpoint. In the future work, we will design a 
more flexible tree kernel for more accurate simi-
larity measure.  
 
Acknowledgements: We would like to thank 
Dr. Alessandro Moschitti for his great help in 
using his Tree Kernel Toolkits and fine-tuning 
the system. We also would like to thank the three 
anonymous reviewers for their invaluable sug-
gestions. 
References 
ACE. 2002-2005. The Automatic Content Extraction 
Projects. http://www.ldc.upenn.edu/Projects /ACE/ 
Basili R., Cammisa M. and Moschitti A. 2005. A Se-
mantic Kernel to classify text with very few train-
ing examples. ICML-2005 
Bunescu R. C. and Mooney R. J. 2005. A Shortest 
Path Dependency Kernel for Relation Extraction. 
EMNLP-2005 
Charniak E. 2001. Immediate-head Parsing for Lan-
guage Models. ACL-2001 
Collins M. and Duffy N. 2001. Convolution Kernels 
for Natural Language. NIPS-2001 
Culotta A. and Sorensen J. 2004. Dependency Tree 
Kernel for Relation Extraction. ACL-2004 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz. 
Joachims T. 1998. Text Categorization with Support 
Vecor Machine: learning with many relevant fea-
tures. ECML-1998 
Kambhatla N. 2004. Combining lexical, syntactic and 
semantic features with Maximum Entropy models 
for extracting relations. ACL-2004 (poster) 
Lodhi H., Saunders C., Shawe-Taylor J., Cristianini 
N. and Watkins C. 2002. Text classification using 
string kernel. Journal of Machine Learning Re-
search, 2002(2):419-444 
Miller S., Fox H., Ramshaw L. and Weischedel R. 
2000. A novel use of statistical parsing to extract 
information from text. NAACL-2000 
Moschitti A. 2004. A Study on Convolution Kernels 
for Shallow Semantic Parsing. ACL-2004 
MUC. 1987-1998. http://www.itl.nist.gov/iaui/894.02/ 
related_projects/muc/ 
Sch?lkopf B. and Smola A. J. 2001. Learning with 
Kernels: SVM, Regularization, Optimization and 
Beyond. MIT Press, Cambridge, MA 407-423 
Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003. 
Hierarchical Directed Acyclic Graph Kernel: 
Methods for Structured Natural Language Data. 
ACL-2003 
Zelenko D., Aone C. and Richardella A. 2003. Kernel 
Methods for Relation Extraction. Journal of Ma-
chine Learning Research. 2003(2):1083-1106 
Zhao S.B. and Grishman R. 2005. Extracting Rela-
tions with Integrated Information Using Kernel 
Methods. ACL-2005 
Zhou G.D., Su J, Zhang J. and Zhang M. 2005. Ex-
ploring Various Knowledge in Relation Extraction. 
ACL-2005 
832
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 33?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Phrase-based Statistical Model for SMS Text Normalization 
AiTi Aw, Min Zhang, Juan Xiao, Jian Su 
Institute of Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
{aaiti,mzhang,stuxj,sujian}@i2r.a-star.edu.sg 
Abstract 
Short Messaging Service (SMS) texts be-
have quite differently from normal written 
texts and have some very special phenom-
ena. To translate SMS texts, traditional 
approaches model such irregularities di-
rectly in Machine Translation (MT). How-
ever, such approaches suffer from 
customization problem as tremendous ef-
fort is required to adapt the language 
model of the existing translation system to 
handle SMS text style. We offer an alter-
native approach to resolve such irregulari-
ties by normalizing SMS texts before MT. 
In this paper, we view the task of SMS 
normalization as a translation problem 
from the SMS language to the English 
language 1  and we propose to adapt a 
phrase-based statistical MT model for the 
task. Evaluation by 5-fold cross validation 
on a parallel SMS normalized corpus of 
5000 sentences shows that our method can 
achieve 0.80702 in BLEU score against 
the baseline BLEU score 0.6958. Another 
experiment of translating SMS texts from 
English to Chinese on a separate SMS text 
corpus shows that, using SMS normaliza-
tion as MT preprocessing can largely 
boost SMS translation performance from 
0.1926 to 0.3770 in BLEU score. 
1 Motivation 
SMS translation is a mobile Machine Translation 
(MT) application that translates a message from 
one language to another. Though there exists 
many commercial MT systems, direct use of 
such systems fails to work well due to the special 
phenomena in SMS texts, e.g. the unique relaxed 
and creative writing style and the frequent use of 
unconventional and not yet standardized short-
forms. Direct modeling of these special phenom-
ena in MT requires tremendous effort. Alterna-
tively, we can normalize SMS texts into 
                                                          
1 This paper only discusses English SMS text normalization. 
grammatical texts before MT.  In this way, the 
traditional MT is treated as a ?black-box? with 
little or minimal adaptation. One advantage of 
this pre-translation normalization is that the di-
versity in different user groups and domains can 
be modeled separately without accessing and 
adapting the language model of the MT system 
for each SMS application. Another advantage is 
that the normalization module can be easily util-
ized by other applications, such as SMS to 
voicemail and SMS-based information query. 
In this paper, we present a phrase-based statis-
tical model for SMS text normalization. The 
normalization is visualized as a translation prob-
lem where messages in the SMS language are to 
be translated to normal English using a similar 
phrase-based statistical MT method (Koehn et al, 
2003). We use IBM?s BLEU score (Papineni et 
al., 2002) to measure the performance of SMS 
text normalization. BLEU score computes the 
similarity between two sentences using n-gram 
statistics, which is widely-used in MT evalua-
tion. A set of parallel SMS messages, consisting 
of 5000 raw (un-normalized) SMS messages and 
their manually normalized references, is con-
structed for training and testing. Evaluation by 5-
fold cross validation on this corpus shows that 
our method can achieve accuracy of 0.80702 in 
BLEU score compared to the baseline system of 
0.6985. We also study the impact of our SMS 
text normalization on the task of SMS transla-
tion. The experiment of translating SMS texts 
from English to Chinese on a corpus comprising 
402 SMS texts shows that, SMS normalization as 
a preprocessing step of MT can boost the transla-
tion performance from 0.1926 to 0.3770 in 
BLEU score. 
The rest of the paper is organized as follows. 
Section 2 reviews the related work. Section 3 
summarizes the characteristics of English SMS 
texts. Section 4 discusses our method and Sec-
tion 5 reports our experiments. Section 6 con-
cludes the paper. 
2 Related Work 
There is little work reported on SMS normaliza-
tion and translation. Bangalore et al (2002) used 
33
a consensus translation technique to bootstrap 
parallel data using off-the-shelf translation sys-
tems for training a hierarchical statistical transla-
tion model for general domain instant messaging 
used in Internet chat rooms. Their method deals 
with the special phenomena of the instant mes-
saging language (rather than the SMS language) 
in each individual MT system.  Clark (2003) 
proposed to unify the process of tokenization, 
segmentation and spelling correction for nor-
malization of general noisy text (rather than SMS 
or instant messaging texts) based on a noisy 
channel model at the character level. However, 
results of the normalization are not reported. Aw 
et al (2005) gave a brief description on their in-
put pre-processing work for an English-to-
Chinese SMS translation system using a word-
group model. In addition, in most of the com-
mercial SMS translation applications 2 , SMS 
lingo (i.e., SMS short form) dictionary is pro-
vided to replace SMS short-forms with normal 
English words. Most of the systems do not han-
dle OOV (out-of-vocabulary) items and ambigu-
ous inputs. Following compares SMS text 
normalization with other similar or related appli-
cations. 
2.1 SMS Normalization versus General 
Text Normalization 
General text normalization deals with Non-
Standard Words (NSWs) and has been well-
studied in text-to-speech (Sproat et al, 2001) 
while SMS normalization deals with Non-Words 
(NSs) or lingoes and has seldom been studied 
before. NSWs, such as digit sequences, acronyms, 
mixed case words (WinNT, SunOS), abbrevia-
tions and so on, are grammatically correct in lin-
guistics. However lingoes, such as ?b4? (before) 
and ?bf? (boyfriend), which are usually self-
created and only accepted by young SMS users, 
are not yet formalized in linguistics. Therefore, 
the special phenomena in SMS texts impose a 
big challenge to SMS normalization. 
2.2 SMS Normalization versus Spelling 
Correction Problem 
Intuitively, many would regard SMS normaliza-
tion as a spelling correction problem where the 
lingoes are erroneous words or non-words to be 
replaced by English words. Researches on spell-
ing correction centralize on typographic and 
cognitive/orthographic errors (Kukich, 1992) and 
use approaches (M.D. Kernighan, Church and 
                                                          
2 http://www.etranslator.ro and http://www.transl8bit.com 
Gale, 1991) that mostly model the edit operations 
using distance measures (Damerau 1964; Leven-
shtein 1966), specific word set confusions (Gold-
ing and Roth, 1999) and pronunciation modeling 
(Brill and Moore, 2000; Toutanova and Moore, 
2002). These models are mostly character-based 
or string-based without considering the context. 
In addition, the author might not be aware of the 
errors in the word introduced during the edit op-
erations, as most errors are due to mistype of 
characters near to each other on the keyboard or 
homophones, such as ?poor? or ?pour?.  
In SMS, errors are not isolated within word 
and are usually not surrounded by clean context. 
Words are altered deliberately to reflect sender?s 
distinct creation and idiosyncrasies. A character 
can be deleted on purpose, such as ?wat? (what) 
and ?hv? (have).  It also consists of short-forms 
such as ?b4? (before), ?bf? (boyfriend). In addi-
tion, normalizing SMS text might require the 
context to be spanned over more than one lexical 
unit such as ?lemme? (let me), ?ur? (you are) etc. 
Therefore, the models used in spelling correction 
are inadequate for providing a complete solution 
for SMS normalization. 
2.3 SMS Normalization versus Text Para-
phrasing Problem 
Others may regard SMS normalization as a para-
phrasing problem. Broadly speaking, paraphrases 
capture core aspects of variability in language, 
by representing equivalencies between different 
expressions that correspond to the same meaning. 
In most of the recent works (Barzilay and 
McKeown, 2001; Shimohata, 2002), they are 
acquired (semi-) automatically from large com-
parable or parallel corpora using lexical and 
morpho-syntactic information. 
Text paraphrasing works on clean texts in 
which contextual and lexical-syntactic features 
can be extracted and used to find ?approximate 
conceptual equivalence?. In SMS normalization, 
we are dealing with non-words and ?ungram-
matically? sentences with the purpose to normal-
ize or standardize these words and form better 
sentences. The SMS normalization problem is 
thus different from text paraphrasing. On the 
other hand, it bears some similarities with MT as 
we are trying to ?convert? text from one lan-
guage to another. However, it is a simpler prob-
lem as most of the time; we can find the same 
word in both the source and target text, making 
alignment easier. 
 
34
3 Characteristics of English SMS  
Our corpus consists of 55,000 messages collected 
from two sources, a SMS chat room and corre-
spondences between university students. The 
content is mostly related to football matches, 
making friends and casual conversations on 
?how, what and where about?. We summarize 
the text behaviors into two categories as below. 
3.1 Orthographic Variation 
The most significant orthographic variant in 
SMS texts is in the use of non-standard, self-
created short-forms. Usually, sender takes advan-
tage of phonetic spellings, initial letters or num-
ber homophones to mimic spoken conversation 
or shorten words or phrases (hw vs. homework or 
how, b4 vs. before, cu vs. see you, 2u vs. to you, 
oic vs. oh I see, etc.) in the attempt to minimize 
key strokes. In addition, senders create a new 
form of written representation to express their 
oral utterances. Emotions, such as ?:(? symboliz-
ing  sad, ?:)? symbolizing smiling, ?:()? symbol-
izing shocked, are representations of body 
language. Verbal effects such as ?hehe? for 
laughter and emphatic discourse particles such as 
?lor?, ?lah?, ?meh? for colloquial English are 
prevalent in the text collection. 
The loss of ?alpha-case? information posts an-
other challenge in lexical disambiguation and 
introduces difficulty in identifying sentence 
boundaries, proper nouns, and acronyms. With 
the flexible use of punctuation or not using punc-
tuation at all, translation of SMS messages with-
out prior processing is even more difficult. 
3.2 Grammar Variation 
SMS messages are short, concise and convey 
much information within the limited space quota 
(160 letters for English), thus they tend to be im-
plicit and influenced by pragmatic and situation 
reasons. These inadequacies of language expres-
sion such as deletion of articles and subject pro-
noun, as well as problems in number agreements 
or tenses make SMS normalization more chal-
lenging. Table 1 illustrates some orthographic 
and grammar variations of SMS texts. 
3.3 Corpus Statistics  
We investigate the corpus to assess the feasibility 
of replacing the lingoes with normal English 
words and performing limited adjustment to the 
text structure. Similarly to Aw et al (2005), we 
focus on the three major cases of transformation 
as shown in the corpus: (1) replacement of OOV 
words and non-standard SMS lingoes; (2) re-
moval of slang and (3) insertion of auxiliary or 
copula verb and subject pronoun.  
 
Phenomena Messages 
1. Dropping ??? at 
the end of 
question 
btw, wat is ur view 
(By the way, what is your 
view?) 
2. Not using any 
punctuation at 
all 
Eh speak english mi malay 
not tt good  
(Eh, speak English! My Ma-
lay is not that good.) 
3. Using spell-
ing/punctuation 
for emphasis 
goooooood Sunday morning 
!!!!!!  
(Good Sunday morning!) 
4. Using phonetic 
spelling 
dat iz enuf  
(That is enough) 
5. Dropping 
vowel 
i hv cm to c my luv. 
(I have come to see my love.)
6. Introducing 
local flavor 
yar lor where u go juz now  
(yes, where did you go just 
now?) 
7.  Dropping verb 
I hv 2 go. Dinner w parents.  
(I have to go. Have dinner 
with parents.) 
 
Table 1. Examples of SMS Messages 
 
 
Transformation Percentage (%) 
Insertion 8.09 
Deletion 5.48 
Substitution 86.43 
 
Table 2. Distribution of Insertion, Deletion and 
Substitution Transformation. 
 
Substitution  Deletion Insertion 
u -> you m are 
2 ? to lah am 
n ? and t is 
r ? are ah you 
ur ?your leh to 
dun ? don?t 1 do 
man ? manches-
ter 
huh a 
no ? number one in 
intro ? introduce lor yourself 
wat ? what ahh will 
 
Table 3. Top 10 Most Common Substitu-
tion, Deletion and Insertion 
 
Table 2 shows the statistics of these transfor-
mations based on 700 messages randomly se-
lected, where 621 (88.71%) messages required 
35
If we include the word ?null? in the English 
vocabulary, the above model can fully address 
the deletion and substitution transformations, but 
inadequate to address the insertion transforma-
tion. For example, the lingoes ?duno?, ?ysnite? 
have to be normalized using an insertion trans-
formation to become ?don?t know? and ?yester-
day night?. Moreover, we also want the 
normalization to have better lexical affinity and 
linguistic equivalent, thus we extend the model 
to allow many words to many words alignment, 
allowing a sequence of SMS words to be normal-
ized to a sequence of contiguous English words. 
We call this updated model a phrase-based nor-
malization model.  
normalization with a total of 2300 transforma-
tions. Substitution accounts for almost 86% of all 
transformations. Deletion and substitution make 
up the rest. Table 3 shows the top 10 most com-
mon transformations. 
4 SMS Normalization  
We view the SMS language as a variant of Eng-
lish language with some derivations in vocabu-
lary and grammar. Therefore, we can treat SMS 
normalization as a MT problem where the SMS 
language is to be translated to normal English. 
We thus propose to adapt the statistical machine 
translation model (Brown et al, 1993; Zens and 
Ney, 2004) for SMS text normalization. In this 
section, we discuss the three components of our 
method: modeling, training and decoding for 
SMS text normalization. 
4.2 Phrase-based Model 
Given an English sentence e  and SMS sentence 
s , if we assume that e  can be decomposed into 
 phrases with a segmentation T , such that 
each phrase e  in  can be corresponded with 
one phrase s  in 
K
k
k
e
s , we have e e  
and 
1 1
N
k Ke e  ? ?=
1 1
M
k Ks s s  s= ? ? . The channel model can be 
rewritten in equation (3).  
4.1 Basic Word-based Model  
The SMS normalization model is based on the 
source channel model (Shannon, 1948). Assum-
ing that an English sentence e, of length N is 
?corrupted? by a noisy channel to produce a 
SMS message s, of length M, the English sen-
tence e, could be recovered through a posteriori 
distribution for a channel target text given the 
source text P s , and a prior distribution for 
the channel source text . 
( | )e
( )P e
 
 { }
{ }
1
1
1 1 1
1 1 1
? arg max ( | )
arg max ( | ) ( )
N
N
N N M
e
M N N
e
e P e s
P s e P e
=
= i          (1) 
 
{ }
1 1 1 1
1 1 1
1 1 1
1 1 1
( | ) ( , | )
( | ) ( | , )
( | ) ( | )
max ( | ) ( | )
M N M N
T
N M N
T
N K K
T
N K K
T
P s e P s T e
P T e P s T e
P T e P s e
P T e P s e
=
=
=
?
?
?
?
i
 i
 i
    (3) 
 
This is the basic function of the channel model 
for the phrase-based SMS normalization model, 
where we used the maximum approximation for 
the sum over all segmentations. Then we further 
decompose the probability 1 1( | )
K KP s e  using a 
phrase alignment  as done in the previous 
word-based model. 
A
 
Assuming that one SMS word is mapped ex-
actly to one English word in the channel model 
 under an alignment , we need to con-
sider only two types of probabilities: the align-
ment probabilities denoted by P m  and the 
lexicon mapping probabilities denoted by 
(Brown et al 1993). The channel 
model can be written as in the following equation 
where m is the position of a word in 
( | )P s e
( |m aP s e
A
( | )ma
)
m
s and  its 
alignment in . 
ma
e
 
 
{ }
1 1 1 1
1 1 1
1
( | ) ( , | )
( | ) ( | , )
( | ) ( | )
m
M N M N
A
N M N
A
M
m m a
A m
P s e P s A e
P A e P s A e
P m a P s e
=
=
=
? ?? ??
?
?
? ?
i
i ??
  (2) 
{ }
{ }
{ }
1
1 1 1 1
1 1 1
1
1
1
1
( | ) ( , | )
( | ) ( | , )
( | ) ( | , )
( | ) ( | )
k
k
K K K K
A
K K K
A
K
ak
k k a
kA
K
k k a
kA
P s e P s A e
P A e P s A e
P k a P s s e
P k a P s e
?
=
=
=
=
? ?= ? ?? ?
? ?? ? ?? ?
?
?
? ?
? ?







   
   i
   i
  i
 (4) 
 
We are now able to model the three transfor-
mations through the normalization pair ( , )
kk a
s e    , 
36
with the mapping probability . The fol-
lowings show the scenarios in which the three 
transformations occur. 
( | )
kk a
P s e 
kk a
s e<  
kk a
s e=  
| ) (k ka P s
?
?  i
( |kP s e 
)
)k ke 
}1
1
1
1
1 1
| )
)
) ( |
) (
M
K
k
N K
n k
P s e
P s
P s
?
=
?
= =
???
???
?
? ?
i i
i
1 )
N
 
Insertion  
Deletion 
ka
e  = null 
Substitution  
 
The statistics in our training corpus shows that 
by selecting appropriate phrase segmentation, the 
position re-ordering at the phrase level occurs 
rarely. It is not surprising since most of the Eng-
lish words or phrases in normal English text are 
replaced with lingoes in SMS messages without 
position change to make SMS text short and con-
cise and to retain the meaning. Thus we need to 
consider only monotone alignment at phrase 
level, i.e., k , as in equation (4). In addition, 
the word-level reordering within phrase is 
learned during training. Now we can further de-
rive equation (4) as follows: 
ka= 
{ }1 1
1
1
( | ) ( | )
( | )
k
K
K K
a
kA
K
k k
k
P s e P k e
P s e
=
=
? ? ?
?
? ?
?


  
 
 (5) 
?
?
The mapping probability is esti-
mated via relative frequencies as follows: 
)k
 
'
'
( ,( | )
( ,
k
k k
k k
s
N s eP s e
N s
= ?

                            (6) 
Here, denotes the frequency of the 
normalization pair . 
( , )k kN s e 
( , )k ks e 
Using a bigram language model and assuming 
Bayes decision rule, we finally obtain the follow-
ing search criterion for equation (1). 
 {
1
1
1
1 1 1
1
1
,
? arg max ( ) (
arg max ( |
      max ( | )
arg max ( | | )
N
N
N
N N N
e
N
n n
e n
N
k kT
n n k k
e T
e P e
P e e
P T e e
P e e e
=
=
?? ??
?
?
?
?
i
 
 
(7) 
????
The alignment process given in equation (8) is 
different from that of normalization given in 
equation (7) in that, here we have an aligned in-
put sentence pair, s and . The alignment 
process is just to find the alignment segmentation 
,? ,k ks e ks e? < > =<   
( , )k kP s e 
between the two sen-
tences that maximizes the joint probability. 
Therefore, in step (2) of the EM algorithm given 
at Figure 1, only the joint probabilities 
are involved and updated.  
???
 
For the above equation, we assume the seg-
mentation probability ( |P T e to be constant. 
Finally, the SMS normalization model consists of 
two sub-models: a word-based language model 
(LM), characterized by 1( | )n nP e e ?
)k
 and a phrase-
based lexical mapping model (channel model), 
characterized by ( |kP s e
)ke 
)ke 
  . 
,?
arg m
s ek k 1
ax ( ,
K
k
k
P s
=
?  ? < > 
1
M
1
Ne
1,k k K=>
4.3 Training Issues 
For the phrase-based model training, the sen-
tence-aligned SMS corpus needs to be aligned 
first at the phrase level. The maximum likelihood 
approach, through EM algorithm and Viterbi 
search (Dempster et al, 1977) is employed to 
infer such an alignment. Here, we make a rea-
sonable assumption on the alignment unit that a 
single SMS word can be mapped to a sequence 
of contiguous English words, but not vice verse. 
The EM algorithm for phrase alignment is illus-
trated in Figure 1 and is formulated by equation 
(8). 
 
 
 
The Expectation-Maximization Algorithm 
 
(1) Bootstrap initial alignment using ortho-
graphic similarities 
(2)  Expectation: Update the joint probabili-
ties  ( ,kP s
(3)  Maximization: Apply the joint probabili-
ties to get new alignment using 
Viterbi search algorithm 
( ,kP s
(4)  Repeat (2) to (3) until alignment con-
verges 
(5) Derive normalization pairs from final 
alignment 
 
Figure 1. Phrase Alignment Using EM Algorithm 
 
, 1? | , )k k
M N
s e ke s e? < > =  (8) 1
 
Since EM may fall into local optimization, in 
order to speed up convergence and find a nearly 
global optimization, a string matching technique 
is exploited at the initialization step to identify 
the most probable normalization pairs. The or-
37
thographic similarities captured by edit distance 
and a SMS lingo dictionary3  which contains the 
commonly used short-forms are first used to es-
tablish phrase mapping boundary candidates. 
Heuristics are then exploited to match tokens 
within the pairs of boundary candidates by trying 
to combine consecutive tokens within the bound-
ary candidates if the numbers of tokens do not 
agree. 
Finally, a filtering process is carried out to 
manually remove the low-frequency noisy 
alignment pairs. Table 4 shows some of the ex-
tracted normalization pairs. As can be seen from 
the table, our algorithm discovers ambiguous 
mappings automatically that are otherwise miss-
ing from most of the lingo dictionary. 
 
( , )s e   log ( | )P s e   
(2, 2) 0 
(2, to) -0.579466 
(2, too) -0.897016 
(2, null) -2.97058 
(4, 4) 0 
(4, for) -0.431364 
(4, null) -3.27161 
(w, who are) -0.477121 
(w, with) -0.764065 
(w, who) -1.83885 
(dat, that) -0.726999 
(dat, date) -0.845098 
(tmr, tomorrow) -0.341514 
 
Table 4. Examples of normalization pairs 
 
Given the phrase-aligned SMS corpus, the 
lexical mapping model, characterized by 
( | )k kP s e  , is easily to be trained using equation 
(6). Our n-gram LM 1( | )n nP e e ? is trained on 
English Gigaword provided by LDC using 
SRILM language modeling toolkit (Stolcke, 
2002). Backoff smoothing (Jelinek, 1991) is used 
to adjust and assign a non-zero probability to the 
unseen words to address data sparseness. 
4.4 Monotone Search  
Given an input , the search, characterized in 
equation (7), is to find a sentence e that maxi-
s
mizes  using the normalization 
model. In this paper, the maximization problem 
in equation (7) is solved using a monotone search, 
implemented as a Viterbi search through dy-
namic programming. 
( | ) ( )P s e P ei
5 Experiments 
The aim of our experiment is to verify the effec-
tiveness of the proposed statistical model for 
SMS normalization and the impact of SMS nor-
malization on MT. 
A set of 5000 parallel SMS messages, which 
consists of raw (un-normalized) SMS messages 
and reference messages manually prepared by 
two project members with inter-normalization 
agreement checked, was prepared for training 
and testing. For evaluation, we use IBM?s BLEU 
score (Papineni et al, 2002) to measure the per-
formance of the SMS normalization. BLEU score 
measures the similarity between two sentences 
using n-gram statistics with a penalty for too 
short sentences, which is already widely-used in 
MT evaluation.  
 
Setup BLEU score (3-gram) 
Raw SMS without 
Normalization 0.5784 
Dictionary Look-up 
plus Frequency 0.6958 
Bi-gram Language 
Model Only 0.7086 
 
Table 5. Performance of different set-
ups of the baseline experiments on the 
5000 parallel SMS messages 
5.1 Baseline Experiments: Simple SMS 
Lingo Dictionary Look-up and Using 
Language Model Only 
The baseline experiment is to moderate the texts 
using a lingo dictionary comprises 142 normali-
zation pairs, which is also used in bootstrapping 
the phrase alignment learning process.  
Table 5 compares the performance of the dif-
ferent setups of the baseline experiments. We 
first measure the complexity of the SMS nor-
malization task by directly computing the simi-
larity between the raw SMS text and the 
normalized English text. The 1st row of Table 5 
reports the similarity as 0.5784 in BLEU score, 
which implies that there are quite a number of 
English word 3-gram that are common in the raw 
and normalized messages. The 2nd experiment is 
carried out using only simple dictionary look-up. 
                                                          
3 The entries are collected from various websites such as 
http://www.handphones.info/sms-dictionary/sms-lingo.php, 
and http://www.funsms.net/sms_dictionary.htm, etc.  
38
Lexical ambiguity is addressed by selecting the 
highest-frequency normalization candidate, i.e., 
only unigram LM is used. The performance of 
the 2nd experiment is 0.6958 in BLEU score. It 
suggests that the lingo dictionary plus the uni-
gram LM is very useful for SMS normalization. 
Finally we carry out the 3rd experiment using 
dictionary look-up plus bi-gram LM. Only a 
slight improvement of 0.0128 (0.7086-0.6958) is 
obtained. This is largely because the English 
words in the lingo dictionary are mostly high-
frequency and commonly-used. Thus bi-gram 
does not show much more discriminative ability 
than unigram without the help of the phrase-
based lexical mapping model. 
Experimental result analysis reveals that the 
strength of our model is in its ability to disam-
biguate mapping as in ?2? to ?two? or ?to? and 
?w? to ?with? or ?who?. Error analysis shows 
that the challenge of the model lies in the proper 
insertion of subject pronoun and auxiliary or 
copula verb, which serves to give further seman-
tic information about the main verb, however this 
requires significant context understanding. For 
example, a message such as ?u smart? gives little 
clues on whether it should be normalized to ?Are 
you smart?? or ?You are smart.? unless the full 
conversation is studied. 
 
Takako w r u? 
Takako who are you? 
Im in ns, lik soccer, clubbin hangin w frenz! 
Wat bout u mee? 
I'm in ns, like soccer, clubbing hanging with 
friends!  What about you? 
fancy getting excited w others' boredom 
Fancy getting excited with others' boredom 
If u ask me b4 he ask me then i'll go out w u all 
lor. N u still can act so real. 
If you ask me before he asked me then I'll go 
out with you all.  And you still can act so real. 
Doing nothing, then u not having dinner w us? 
Doing nothing, then you do not having dinner 
with us? 
Aiyar sorry lor forgot 2 tell u... Mtg at 2 pm. 
Sorry forgot to tell you...  Meeting at two pm. 
tat's y I said it's bad dat all e gals know u... 
Wat u doing now? 
That's why I said it's bad that all the girls know 
you...  What you doing now? 
 
5.2 Using Phrase-based Model 
We then conducted the experiment using the pro-
posed method (Bi-gram LM plus a phrase-based 
lexical mapping model) through a five-fold cross 
validation on the 5000 parallel SMS messages. 
Table 6 shows the results. An average score of 
0.8070 is obtained. Compared with the baseline 
performance in Table 5, the improvement is very 
significant. It suggests that the phrase-based 
lexical mapping model is very useful and our 
method is effective for SMS text normalization. 
Figure 2 is the learning curve. It shows that our 
algorithm converges when training data is 
increased to 3000 SMS parallel messages. This 
suggests that our collected corpus is representa-
tive and enough for training our model. Table 7 
illustrates some examples of the normalization 
results. 
  
5-fold cross validation BLEU score (3-gram)
Setup 1 0.8023 
Setup 2 0.8236 
Setup 3 0.8071 
Setup 4 0.8113 
Setup 5 0.7908 
Ave. 0.8070 
 
Table 7. Examples of Normalization Results 
5.3 Effect on English-Chinese MT 
An experiment was also conducted to study the 
effect of normalization on MT using 402 mes-
sages randomly selected from the text corpus. 
We compare three types of SMS message: raw 
SMS messages, normalized messages using sim-
ple dictionary look-up and normalized messages 
using our method. The messages are passed to 
two different English-to-Chinese translation sys-
tems provided by Systran4 and Institute for Info-
comm Research5(I2R) separately to produce three 
sets of translation output. The translation quality 
is measured using 3-gram cumulative BLEU 
score against two reference messages. 3-gram is 
 
Table 6. Normalization results for 5-
fold cross validation test 
0.7
0.72
0.74
0.76
0.78
0.8
0.82
1000 2000 3000 4000 5000
BLEU
 Figure 2. Learning Curve  
                                                          
4 http://www.systranet.com/systran/net 
5 http://nlp.i2r.a-star.edu.sg/techtransfer.html 
39
used as most of the messages are short with aver-
age length of seven words. Table 8 shows the 
details of the BLEU scores. We obtain an aver-
age of 0.3770 BLEU score for normalized mes-
sages against 0.1926 for raw messages. The 
significant performance improvement suggests 
that preprocessing of normalizing SMS text us-
ing our method before MT is an effective way to 
adapt a general MT system to SMS domain. 
 
 I2R Systran Ave. 
Raw Message 0.2633 0.1219 0.1926 
Dict Lookup 0.3485 0.1690 0.2588 
Normalization 0.4423 0.3116 0.3770 
 
Table 8. SMS Translation BLEU score with or 
without SMS normalization 
6 Conclusion 
In this paper, we study the differences among 
SMS normalization, general text normalization, 
spelling check and text paraphrasing, and inves-
tigate the different phenomena of SMS messages. 
We propose a phrase-based statistical method to 
normalize SMS messages. The method produces 
messages that collate well with manually normal-
ized messages, achieving 0.8070 BLEU score 
against 0.6958 baseline score. It also signifi-
cantly improves SMS translation accuracy from 
0.1926 to 0.3770 in BLEU score without adjust-
ing the MT model. 
This experiment results provide us with a good 
indication on the feasibility of using this method 
in performing the normalization task. We plan to 
extend the model to incorporate mechanism to 
handle missing punctuation (which potentially 
affect MT output and are not being taken care at 
the moment),  and making use of pronunciation 
information to handle OOV caused by the use of 
phonetic spelling. A bigger data set will also be 
used to test the robustness of the system leading 
to a more accurate alignment and normalization.  
References  
A.T. Aw, M. Zhang, Z.Z. Fan, P.K. Yeo and J. Su. 
2005. Input Normalization for an English-to-
Chinese SMS Translation System. MT Summit-
2005  
S. Bangalore, V. Murdock and G. Riccardi. 2002. 
Bootstrapping Bilingual Data using Consensus 
Translation for a Multilingual Instant Messaging 
System. COLING-2002 
R. Barzilay and K. R. McKeown. 2001. Extracting 
paraphrases from a parallel corpus. ACL-2001 
E. Brill and R. C. Moore. 2000. An Improved Error 
Model for Noisy Channel Spelling Correction. 
ACL-2000 
P. F. Brown, S. D. Pietra, V. D. Pietra and R. Mercer. 
1993. The Mathematics of Statistical Machine 
Translation: Parameter Estimation. Computational 
Linguistics: 19(2) 
A. Clark. 2003. Pre-processing very noisy text. In 
Proceedings of Workshop on Shallow Processing 
of Large Corpora, Lancaster, 2003  
F. J. Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communica-
tions ACM 7, 171-176 
A.P. Dempster, N.M. Laird and D.B. Rubin. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm, Journal of the Royal Statistical So-
ciety, Series B, Vol. 39, 1-38 
A. Golding and D. Roth. 1999. A Winnow-Based Ap-
proach to Spelling Correction. Machine Learning 
34: 107-130 
F. Jelinek. 1991. Self-organized language modeling 
for speech recognition. In A. Waibel and K.F. Lee, 
editors, Readings in Speech Recognition, pages 
450-506. Morgan Kaufmann, 1991 
M. D. Kernighan, K Church and W. Gale. 1990. A 
spelling correction program based on a noisy 
channel model. COLING-1990 
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys, 
24(4):377-439  
K. A. Papineni, S. Roukos, T. Ward and W. J. Zhu. 
2002. BLEU : a Method for Automatic Evaluation 
of Machine Translation. ACL-2002 
P. Koehn, F.J. Och and D. Marcu. 2003. Statistical 
Phrase-Based Translation. HLT-NAACL-2003 
C. Shannon. 1948. A mathematical theory of commu-
nication. Bell System Technical Journal 27(3): 
379-423 
M. Shimohata and E. Sumita 2002. Automatic Para-
phrasing Based on Parallel Corpus for Normaliza-
tion. LREC-2002 
R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf 
and C. Richards. 2001. Normalization of Non-
Standard Words. Computer Speech and Language, 
15(3):287-333 
A. Stolcke. 2002. SRILM ? An extensible language 
modeling toolkit. ICSLP-2002 
K. Toutanova and R. C. Moore. 2002. Pronunciation 
Modeling for Improved Spelling Correction. ACL-
2002 
R. Zens and H. Ney. 2004. Improvements in Phrase-
Based Statistical MT. HLT-NAALL-2004 
40
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 528?535,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Coreference Resolution Using Semantic Relatedness Information from
Automatically Discovered Patterns
Xiaofeng Yang Jian Su
Institute for Infocomm Research
21 Heng Mui Keng Terrace, Singapore, 119613
{xiaofengy,sujian}@i2r.a-star.edu.sg
Abstract
Semantic relatedness is a very important fac-
tor for the coreference resolution task. To
obtain this semantic information, corpus-
based approaches commonly leverage pat-
terns that can express a specific semantic
relation. The patterns, however, are de-
signed manually and thus are not necessar-
ily the most effective ones in terms of ac-
curacy and breadth. To deal with this prob-
lem, in this paper we propose an approach
that can automatically find the effective pat-
terns for coreference resolution. We explore
how to automatically discover and evaluate
patterns, and how to exploit the patterns to
obtain the semantic relatedness information.
The evaluation on ACE data set shows that
the pattern based semantic information is
helpful for coreference resolution.
1 Introduction
Semantic relatedness is a very important factor for
coreference resolution, as noun phrases used to re-
fer to the same entity should have a certain semantic
relation. To obtain this semantic information, previ-
ous work on reference resolution usually leverages
a semantic lexicon like WordNet (Vieira and Poe-
sio, 2000; Harabagiu et al, 2001; Soon et al, 2001;
Ng and Cardie, 2002). However, the drawback of
WordNet is that many expressions (especially for
proper names), word senses and semantic relations
are not available from the database (Vieira and Poe-
sio, 2000). In recent years, increasing interest has
been seen in mining semantic relations from large
text corpora. One common solution is to utilize a
pattern that can represent a specific semantic rela-
tion (e.g., ?X such as Y? for is-a relation, and ?X
and other Y? for other-relation). Instantiated with
two given noun phrases, the pattern is searched in a
large corpus and the occurrence number is used as
a measure of their semantic relatedness (Markert et
al., 2003; Modjeska et al, 2003; Poesio et al, 2004).
However, in the previous pattern based ap-
proaches, the selection of the patterns to represent a
specific semantic relation is done in an ad hoc way,
usually by linguistic intuition. The manually se-
lected patterns, nevertheless, are not necessarily the
most effective ones for coreference resolution from
the following two concerns:
? Accuracy. Can the patterns (e.g., ?X such as
Y?) find as many NP pairs of the specific se-
mantic relation (e.g. is-a) as possible, with a
high precision?
? Breadth. Can the patterns cover a wide variety
of semantic relations, not just is-a, by which
coreference relationship is realized? For ex-
ample, in some annotation schemes like ACE,
?Beijing:China? are coreferential as the capital
and the country could be used to represent the
government. The pattern for the common ?is-
a? relation will fail to identify the NP pairs of
such a ?capital-country? relation.
To deal with this problem, in this paper we pro-
pose an approach which can automatically discover
effective patterns to represent the semantic relations
528
for coreference resolution. We explore two issues in
our study:
(1) How to automatically acquire and evaluate
the patterns? We utilize a set of coreferential NP
pairs as seeds. For each seed pair, we search a large
corpus for the texts where the two noun phrases co-
occur, and collect the surrounding words as the sur-
face patterns. We evaluate a pattern based on its
commonality or association with the positive seed
pairs.
(2) How to mine the patterns to obtain the seman-
tic relatedness information for coreference resolu-
tion? We present two strategies to exploit the pat-
terns: choosing the top best patterns as a set of pat-
tern features, or computing the reliability of seman-
tic relatedness as a single feature. In either strategy,
the obtained features are applied to do coreference
resolution in a supervised-learning way.
To our knowledge, our work is the first effort that
systematically explores these issues in the corefer-
ence resolution task. We evaluate our approach on
ACE data set. The experimental results show that
the pattern based semantic relatedness information
is helpful for the coreference resolution.
The remainder of the paper is organized as fol-
lows. Section 2 gives some related work. Section 3
introduces the framework for coreference resolution.
Section 4 presents the model to obtain the pattern-
based semantic relatedness information. Section 5
discusses the experimental results. Finally, Section
6 summarizes the conclusions.
2 Related Work
Earlier work on coreference resolution commonly
relies on semantic lexicons for semantic relatedness
knowledge. In the system by Vieira and Poesio
(2000), for example, WordNet is consulted to obtain
the synonymy, hypernymy and meronymy relations
for resolving the definite anaphora. In (Harabagiu
et al, 2001), the path patterns in WordNet are uti-
lized to compute the semantic consistency between
NPs. Recently, Ponzetto and Strube (2006) suggest
to mine semantic relatedness from Wikipedia, which
can deal with the data sparseness problem suffered
by using WordNet.
Instead of leveraging existing lexicons, many
researchers have investigated corpus-based ap-
proaches to mine semantic relations. Garera and
Yarowsky (2006) propose an unsupervised model
which extracts hypernym relation for resloving def-
inite NPs. Their model assumes that a definite NP
and its hypernym words usually co-occur in texts.
Thus, for a definite-NP anaphor, a preceding NP that
has a high co-occurrence statistics in a large corpus
is preferred for the antecedent.
Bean and Riloff (2004) present a system called
BABAR that uses contextual role knowledge to do
coreference resolution. They apply an IE component
to unannotated texts to generate a set of extraction
caseframes. Each caseframe represents a linguis-
tic expression and a syntactic position, e.g. ?mur-
der of <NP>?, ?killed <patient>?. From the case-
frames, they derive different types of contextual role
knowledge for resolution, for example, whether an
anaphor and an antecedent candidate can be filled
into co-occurring caseframes, or whether they are
substitutable for each other in their caseframes. Dif-
ferent from their system, our approach aims to find
surface patterns that can directly indicate the coref-
erence relation between two NPs.
Hearst (1998) presents a method to automate the
discovery of WordNet relations, by searching for the
corresponding patterns in large text corpora. She ex-
plores several patterns for the hyponymy relation,
including ?X such as Y? ?X and/or other Y?, ?X
including / especially Y? and so on. The use of
Hearst?s style patterns can be seen for the reference
resolution task. Modjeska et al (2003) explore the
use of the Web to do the other-anaphora resolution.
In their approach, a pattern ?X and other Y? is used.
Given an anaphor and a candidate antecedent, the
pattern is instantiated with the two NPs and forms a
query. The query is submitted to the Google search-
ing engine, and the returned hit number is utilized to
compute the semantic relatedness between the two
NPs. In their work, the semantic information is used
as a feature for the learner. Markert et al (2003) and
Poesio et al (2004) adopt a similar strategy for the
bridging anaphora resolution.
In (Hearst, 1998), the author also proposes to dis-
cover new patterns instead of using the manually
designed ones. She employs a bootstrapping algo-
rithm to learn new patterns from the word pairs with
a known relation. Based on Hearst?s work, Pan-
tel and Pennacchiotti (2006) further give a method
529
which measures the reliability of the patterns based
on the strength of association between patterns and
instances, employing the pointwise mutual informa-
tion (PMI).
3 Framework of Coreference Resolution
Our coreference resolution system adopts the
common learning-based framework as employed
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{NPi, NPj}, in which
NPj is a possible anaphor and NPi is one of its an-
tecedent candidates. An instance is associated with
a vector of features, which is used to describe the
properties of the two noun phrases as well as their
relationships. In our baseline system, we adopt the
common features for coreference resolution such as
lexical property, distance, string-matching, name-
alias, apposition, grammatical role, number/gender
agreement and so on. The same feature set is de-
scribed in (Ng and Cardie, 2002) for reference.
During training, for each encountered anaphor
NPj , one single positive training instance is created
for its closest antecedent. And a group of negative
training instances is created for every intervening
noun phrases between NPj and the antecedent.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm, like C5 in our study. For resolution, an
input document is processed from the first NP to the
last. For each encountered NPj , a test instance is
formed for each antecedent candidate, NPi1. This
instance is presented to the classifier to determine
the coreference relationship. NPj will be resolved
to the candidate that is classified as positive (if any)
and has the highest confidence value.
In our study, we augment the common framework
by incorporating non-anaphors into training. We fo-
cus on the non-anaphors that the original classifier
fails to identify. Specifically, we apply the learned
classifier to all the non-anaphors in the training doc-
uments. For each non-anaphor that is classified as
positive, a negative instance is created by pairing the
non-anaphor and its false antecedent. These neg-
1For resolution of pronouns, only the preceding NPs in cur-
rent and previous two sentences are considered as antecedent
candidates. For resolution of non-pronouns, all the preceding
non-pronouns are considered.
ative instances are added into the original training
instance set for learning, which will generate a clas-
sifier with the capability of not only antecedent iden-
tification, but also non-anaphorically identification.
The new classier is applied to the testing document
to do coreference resolution as usual.
4 Patterned Based Semantic Relatedness
4.1 Acquiring the Patterns
To derive patterns to indicate a specific semantic re-
lation, a set of seed NP pairs that have the relation of
interest is needed. As described in the previous sec-
tion, we have a set of training instances formed by
NP pairs with known coreference relationships. We
can just use this set of NP pairs as the seeds. That is,
an instance i{NPi, NPj} will become a seed pair
(Ei:Ej) in which NPi corresponds to Ei and NPj
corresponds to Ej . In creating the seed, for a com-
mon noun, only the head word is retained while for
a proper name, the whole string is kept. For ex-
ample, instance i{?Bill Clinton?, ?the former pres-
ident?} will be converted to a NP pair (?Bill Clin-
ton?:?president?).
We create the seed pair for every training instance
i{NPi, NPj}, except when (1) NPi or NPj is a
pronoun; or (2) NPi and NPj have the same head
word. We denote S+ and S- the set of seed pairs
derived from the positive and the negative training
instances, respectively. Note that a seed pair may
possibly belong to S+ can S- at the same time.
For each of the seed NP pairs (Ei:Ej), we search
in a large corpus for the strings that match the reg-
ular expression ?Ei * * * Ej? or ?Ej * * * Ei?,
where * is a wildcard for any word or symbol. The
regular expression is defined as such that all the co-
occurrences of Ei and Ej with at most three words
(or symbols) in between are retrieved.
For each retrieved string, we extract a surface pat-
tern by replacing expression Ei with a mark <#t1#>
and Ej with <#t2#>. If the string is followed by a
symbol, the symbol will be also included in the pat-
tern. This is to create patterns like ?X * * * Y [, . ?]?
where Y, with a high possibility, is the head word,
but not a modifier of another noun phrase.
As an example, consider the pair (?Bill Clin-
ton?:?president?). Suppose that two sentences in a
corpus can be matched by the regular expressions:
530
(S1) ? Bill Clinton is elected President of the
United States.?
(S2) ?The US President, Mr Bill Clinton, to-
day advised India to move towards nuclear non-
proliferation and begin a dialogue with Pakistan to
... ?.
The patterns to be extracted for (S1) and (S2), re-
spectively, are
P1: <#t1#> is elected <#t2#>
P2: <#t2#> , Mr <#t1#> ,
We record the number of strings matched by a pat-
tern p instantiated with (Ei:Ej), noted |(Ei, p, Ej)|,
for later use.
For each seed pair, we generate a list of surface
patterns in the above way. We collect all the pat-
terns derived from the positive seed pairs as a set
of reference patterns, which will be scored and used
to evaluate the semantic relatedness for any new NP
pair.
4.2 Scoring the Patterns
4.2.1 Frequency
One possible scoring scheme is to evaluate a pat-
tern based on its commonality to positive seed pairs.
The intuition here is that the more often a pattern is
seen for the positive seed pairs, the more indicative
the pattern is to find positive coreferential NP pairs.
Based on this idea, we score a pattern by calculating
the number of positive seed pairs whose pattern list
contains the pattern. Formally, supposing the pat-
tern list associated with a seed pair s is PList(s), the
frequency score of a pattern p is defined as
Freqency(p) = |{s|s ? S+, p ? PList(s)}| (1)
4.2.2 Reliability
Another possible way to evaluate a pattern is
based on its reliability, i.e., the degree that the pat-
tern is associated with the positive coreferential NPs.
In our study, we use pointwise mutual informa-
tion (Cover and Thomas, 1991) to measure associ-
ation strength, which has been proved effective in
the task of semantic relation identification (Pantel
and Pennacchiotti, 2006). Under pointwise mutual
information (PMI), the strength of association be-
tween two events x and y is defined as follows:
pmi(x, y) = log P (x, y)P (x)P (y) (2)
Thus the association between a pattern p and a
positive seed pair s:(Ei:Ej) is:
pmi(p, (Ei : Ej)) = log
|(Ei,p,Ej)|
|(?,?,?)|
|(Ei,?,Ej)|
|(?,?,?)|
|(?,p,?)|
|(?,?,?)|
(3)
where |(Ei,p,Ej)| is the count of strings matched
by pattern p instantiated with Ei and Ej . Asterisk *
represents a wildcard, that is:
|(Ei, ?, Ej)| =
?
p?PList(Ei:Ej)
|(Ei, p, Ej)| (4)
|(?, p, ?)| =
?
(Ei:Ej)?S+?S?
|(Ei, p, Ej)| (5)
|(?, ?, ?)| =
?
(Ei:Ej)?S+?S?;p?Plist(Ei:Ej)
|(Ei, p, Ej)| (6)
The reliability of pattern is the average strength of
association across each positive seed pair:
r(p) =
?
s?S+
pmi(p,s)
max pmi
|S + | (7)
Here max pmi is used for the normalization pur-
pose, which is the maximum PMI between all pat-
terns and all positive seed pairs.
4.3 Exploiting the Patterns
4.3.1 Patterns Features
One strategy is to directly use the reference pat-
terns as a set of features for classifier learning and
testing. To select the most effective patterns for
the learner, we rank the patterns according to their
scores and then choose the top patterns (first 100 in
our study) as the features.
As mentioned, the frequency score is based on the
commonality of a pattern to the positive seed pairs.
However, if a pattern also occurs frequently for the
negative seed pairs, it should be not deemed a good
feature as it may lead to many false positive pairs
during real resolution. To take this factor into ac-
count, we filter the patterns based on their accuracy,
which is defined as follows:
Accuracy(p) = |{s|s ? S+, p ? PList(s)}||{s|s ? S + ? S?, p ? PList(s)}| (8)
A pattern with an accuracy below threshold 0.5 is
eliminated from the reference pattern set. The re-
maining patterns are sorted as normal, from which
the top 100 patterns are selected as features.
531
NWire NPaper BNews
R P F R P F R P F
Normal Features 54.5 80.3 64.9 56.6 76.0 64.9 52.7 75.3 62.0
+ ?X such as Y? proper names 55.1 79.0 64.9 56.8 76.1 65.0 52.6 75.1 61.9
all types 55.1 78.3 64.7 56.8 74.7 64.4 53.0 74.4 61.9
+ ?X and other Y? proper names 54.7 79.9 64.9 56.4 75.9 64.7 52.6 74.9 61.8
all types 54.8 79.8 65.0 56.4 75.9 64.7 52.8 73.3 61.4
+ pattern features (frequency) proper names 58.7 75.8 66.2 57.5 73.9 64.7 54.0 71.1 61.4
all types 59.7 67.3 63.3 57.4 62.4 59.8 55.9 57.7 56.8
+ pattern features (filtered frequency) proper names 57.8 79.1 66.8 56.9 75.1 64.7 54.1 72.4 61.9
all types 58.1 77.4 66.4 56.8 71.2 63.2 55.0 68.1 60.9
+ pattern features (PMI reliability) proper names 58.8 76.9 66.6 58.1 73.8 65.0 54.3 72.0 61.9
all types 59.6 70.4 64.6 58.7 61.6 60.1 56.0 58.8 57.4
+ single reliability feature proper names 57.4 80.8 67.1 56.6 76.2 65.0 54.0 74.7 62.7
all types 57.7 76.4 65.7 56.7 75.9 64.9 55.1 69.5 61.5
Table 1: The results of different systems for coreference resolution
Each selected pattern p is used as a single fea-
ture, PFp. For an instance i{NPi, NPj}, a list of
patterns is generated for (Ei:Ej) in the same way as
described in Section 4.1. The value of PFp for the
instance is simply |(Ei, p, Ej)|.
The set of pattern features is used together with
the other normal features to do the learning and test-
ing. Thus, the actual importance of a pattern in
coreference resolution is automatically determined
in a supervised learning way.
4.3.2 Semantic Relatedness Feature
Another strategy is to use only one semantic fea-
ture which is able to reflect the reliability that a NP
pair is related in semantics. Intuitively, a NP pair
with strong semantic relatedness should be highly
associated with as many reliable patterns as possi-
ble. Based on this idea, we define the semantic re-
latedness feature (SRel) as follows:
SRel(i{NPi, NPj}) =
1000 ?
?
p?PList(Ei:Ej)
pmi(p, (Ei : Ej)) ? r(p) (9)
where pmi(p, (Ei:Ej)) is the pointwise mutual in-
formation between pattern p and a NP pair (Ei:Ej),
as defined in Eq. 3. r(p) is the reliability score of p
(Eq. 7). As a relatedness value is always below 1,
we multiple it by 1000 so that the feature value will
be of integer type with a range from 0 to 1000. Note
that among PList(Ei:Ej), only the reference patterns
are involved in the feature computing.
5 Experiments and Discussion
5.1 Experimental setup
In our study we did evaluation on the ACE-2 V1.0
corpus (NIST, 2003), which contains two data set,
training and devtest, used for training and testing re-
spectively. Each of these sets is further divided by
three domains: newswire (NWire), newspaper (NPa-
per), and broadcast news (BNews).
An input raw text was preprocessed automati-
cally by a pipeline of NLP components, includ-
ing sentence boundary detection, POS-tagging, Text
Chunking and Named-Entity Recognition. Two dif-
ferent classifiers were learned respectively for re-
solving pronouns and non-pronouns. As mentioned,
the pattern based semantic information was only ap-
plied to the non-pronoun resolution. For evaluation,
Vilain et al (1995)?s scoring algorithm was adopted
to compute the recall and precision of the whole
coreference resolution.
For pattern extraction and feature computing, we
used Wikipedia, a web-based free-content encyclo-
pedia, as the text corpus. We collected the English
Wikipedia database dump in November 2006 (re-
fer to http://download.wikimedia.org/). After all the
hyperlinks and other html tags were removed, the
whole pure text contains about 220 Million words.
5.2 Results and Discussion
Table 1 lists the performance of different coref-
erence resolution systems. The first line of the
table shows the baseline system that uses only
the common features proposed in (Ng and Cardie,
2002). From the table, our baseline system can
532
NO Frequency Frequency (Filtered) PMI Reliabilty
1 <#t1> <#t2> <#t2> | | <#t1> | <#t1> : <#t2>
2 <#t2> <#t1> <#t1> ) is a <#t2> <#t2> : <#t1>
3 <#t1> , <#t2> <#t1> ) is an <#t2> <#t1> . the <#t2>
4 <#t2> , <#t1> <#t2> ) is an <#t1> <#t2> ( <#t1> )
5 <#t1> . <#t2> <#t2> ) is a <#t1> <#t1> ( <#t2>
6 <#t1> and <#t2> <#t1> or the <#t2> <#t1> ( <#t2> )
7 <#t2> . <#t1> <#t1> ( the <#t2> <#t1> | | <#t2> |
8 <#t1> . the <#t2> <#t1> . during the <#t2> <#t2> | | <#t1> |
9 <#t2> and <#t1> <#t1> | <#t2> <#t2> , the <#t1>
10 <#t1> , the <#t2> <#t1> , an <#t2> <#t1> , the <#t2>
11 <#t2> . the <#t1> <#t1> ) was a <#t2> <#t2> ( <#t1>
12 <#t2> , the <#t1> <#t1> in the <#t2> - <#t1> , <#t2>
13 <#t2> <#t1> , <#t1> - <#t2> <#t1> and the <#t2>
14 <#t1> <#t2> , <#t1> ) was an <#t2> <#t1> . <#t2>
15 <#t1> : <#t2> <#t1> , many <#t2> <#t1> ) is a <#t2>
16 <#t1> <#t2> . <#t2> ) was a <#t1> <#t1> during the <#t2>
17 <#t2> <#t1> . <#t1> ( <#t2> . <#t1> <#t2> .
18 <#t1> ( <#t2> ) <#t2> | <#t1> <#t1> ) is an <#t2>
19 <#t1> and the <#t2> <#t1> , not the <#t2> <#t2> in <#t1> .
20 <#t2> ( <#t1> ) <#t2> , many <#t1> <#t2> , <#t1>
. . . . . . . . . . . .
Table 2: Top patterns chosen under different scoring schemes
achieve a good precision (above 75%-80%) with a
recall around 50%-60%. The overall F-measure for
NWire, NPaper and BNews is 64.9%, 64.9% and
62.0% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure of about 62% for the
same data set.
The rest lines of Table 1 are for the systems us-
ing the pattern based information. In all the sys-
tems, we examine the utility of the semantic infor-
mation in resolving different types of NP Pairs: (1)
NP Pairs containing proper names (i.e., Name:Name
or Name:Definites), and (2) NP Pairs of all types.
In Table 1 (Line 2-5), we also list the results of
incorporating two commonly used patterns, ?X(s)
such as Y? and ?X and other Y(s)?. We can find that
neither of the manually designed patterns has signif-
icant impact on the resolution performance. For all
the domains, the manual patterns just achieve slight
improvement in recall (below 0.6%), indicating that
coverage of the patterns is not broad enough.
5.2.1 Pattern Features
In Section 4.3.1 we propose a strategy that di-
rectly uses the patterns as features. Table 2 lists the
top patterns that are sorted based on frequency, fil-
tered frequency (by accuracy), and PMI reliability,
on the NWire domain for illustration.
From the table, evaluated only based on fre-
quency, the top patterns are those that indicate the
appositive structure like ?X, an/a/the Y?. However,
if filtered by accuracy, patterns of such a kind will
be removed. Instead, the top patterns with both high
frequency and high accuracy are those for the copula
structure, like ?X is/was/are Y?. Sorted by PMI reli-
ability, patterns for the above two structures can be
seen in the top of the list. These results are consis-
tent with the findings in (Cimiano and Staab, 2004)
that the appositive and copula structures are indica-
tive to find the is-a relation. Also, the two commonly
used patterns ?X(s) such as Y? and ?X and other
Y(s)? were found in the feature lists (not shown in
the table). Their importance for coreference resolu-
tion will be determined automatically by the learn-
ing algorithm.
An interesting pattern seen in the lists is ?X || Y |?,
which represents the cases when Y and X appear in
the same of line of a table in Wikipedia. For exam-
ple, the following text
?American || United States | Washington D.C. | . . . ?
is found in the table ?list of empires?. Thus the pair
?American:United States?, which is deemed coref-
erential in ACE, can be identified by the pattern.
The sixth till the eleventh lines of Table 1 list the
results of the system with pattern features. From the
table, adding the pattern features brings the improve-
ment of the recall against the baseline. Take the sys-
tem based on filtered frequency as an example. We
can observe that the recall increases by up to 3.3%
(for NWire). However, we see the precision drops
(up to 1.2% for NWire) at the same time. Over-
all the system achieves an F-measure better than the
baseline in NWire (1.9%), while equal (?0.2%) in
NPaper and BNews.
Among the three ranking schemes, simply using
frequency leads to the lowest precision. By contrast,
using filtered frequency yields the highest precision
with nevertheless the lowest recall. It is reasonable
since the low accuracy features prone to false posi-
533
NameAlias = 1: ...
NameAlias = 0:
:..Appositive = 1: ...
Appositive = 0:
:..P014 > 0:
:...P003 <= 4: 0 (3)
: P003 > 4: 1 (25)
P014 <= 0:
:..P004 > 0:...
P004 <= 0:
:..P027 > 0: 1 (25/7)
P027 <= 0:
:..P002 > 0: ...
P002 <= 0:
:..P005 > 0: 1 (49/22)
P005 <= 0:
:..String_Match = 1: .
String_Match = 0: .
// p002: <t1> ) is a <t2>
// P003: <t1> ) is an <t2>
// P004: <t2> ) is an <t1>
// p005: <t2> ) is a <t1>
// P014: <t1> ) was an <t2>
// p027: <t1> , ( <t2> ,
Figure 1: The decision tree (NWire domain) for the
system using pattern features (filtered frequency)
(feature String Match records whether the string of anaphor
NP j matches that of a candidate antecedent NP i)
tive NP pairs are eliminated, at the price of recall.
Using PMI Reliability can achieve the highest re-
call with a medium level of precision. However, we
do not find significant difference in the overall F-
measure for all these three schemes. This should be
due to the fact that the pattern features need to be
further chosen by the learning algorithm, and only
those patterns deemed effective by the learner will
really matter in the real resolution.
From the table, the pattern features only work
well for NP pairs containing proper names. Ap-
plied on all types of NP pairs, the pattern features
further boost the recall of the systems, but in the
meanwhile degrade the precision significantly. The
F-measure of the systems is even worse than that
of the baseline. Our error analysis shows that a
non-anaphor is often wrongly resolved to a false an-
tecedent once the two NPs happen to satisfy a pat-
tern feature, which affects precision largely (as an
evidence, the decrease of precision is less significant
when using filtered frequency than using frequency).
Still, these results suggest that we just apply the pat-
tern based semantic information in resolving proper
names which, in fact, is more compelling as the se-
mantic information of common nouns could be more
easily retrieved from WordNet.
We also notice that the patterned based semantic
information seems more effective in the NWire do-
main than the other two. Especially for NPaper, the
improvement in F-measure is less than 0.1% for all
the systems tested. The error analysis indicates it
may be because (1) there are less NP pairs in NPa-
per than in NWire that require the external seman-
tic knowledge for resolution; and (2) For many NP
pairs that require the semantic knowledge, no co-
occurrence can be found in the Wikipedia corpus.
To address this problem, we could resort to the Web
which contains a larger volume of texts and thus
could lead to more informative patterns. We would
like to explore this issue in our future work.
In Figure 1, we plot the decision tree learned
with the pattern features for non-pronoun resolution
(NWire domain, filtered frequency), which visually
illustrates which features are useful in the reference
determination. We can find the pattern features oc-
cur in the top of the decision tree, among the features
for name alias, apposition and string-matching that
are crucial for coreference resolution as reported in
previous work (Soon et al, 2001). Most of the pat-
tern features deemed important by the learner are for
the copula structure.
5.2.2 Single Semantic Relatedness Feature
Section 4.3.2 presents another strategy to exploit
the patterns, which uses a single feature to reflect the
semantic relatedness between NP pairs. The last two
lines of Table 1 list the results of such a system.
Observed from the table, the system with the sin-
gle semantic relatedness feature beats those with
other solutions. Compared with the baseline, the
system can get improvement in recall (up to 2.9%
as in NWire), with a similar or even higher preci-
sion. The overall F-measure it produces is 67.1%,
65.0% and 62.7%, better than the baseline in all the
domains. Especially in the NWire domain, we can
see the significant (t-test, p ? 0.05) improvement of
2.1% in F-measure. When applied on All-Type NP
pairs, the degrade of performance is less significant
as using pattern features. The resulting performance
is better than the baseline or equal. Compared with
the systems using the pattern features, it can still
achieve a higher precision and F-measure (with a lit-
tle loss in recall) .
There are several reasons why the single seman-
tic relatedness feature (SRel) can perform better than
the set of pattern features. Firstly, the feature value
of SRel takes into consideration the information of
all the patterns, instead of only the selected patterns.
Secondly, since the SRel feature is computed based
on all the patterns, it reduces the risk of false posi-
534
NameAlias = 1: ...
NameAlias = 0:
:..Appositive = 1: ...
Appositive = 0:
:..SRel > 28:
:..SRel > 47: ...
: SRel <= 47: ...
SRel <= 28:
:..String_Match = 1: ...
String_Match = 0: ...
Figure 2: The decision tree (Nwire) for the system
using the single semantic relatedness feature
tive when a NP pair happens to satisfy one or several
pattern features. Lastly, from the point of view of
machine learning, using only one semantic feature,
instead of hundreds of pattern features, can avoid
overfitting and thus benefit the classifier learning.
In Figure 2, we also show the decision tree learned
with the semantic relatedness feature. We observe
that the decision tree is simpler than that with pat-
tern features as depicted in Figure 1. After feature
name-alias and apposite, the classifier checks dif-
ferent ranges of the SRel value and make different
resolution decision accordingly. This figure further
illustrates the importance of the semantic feature.
6 Conclusions
In this paper we present a pattern based approach to
coreference resolution. Different from the previous
work which utilizes manually designed patterns, our
approach can automatically discover the patterns ef-
fective for the coreference resolution task. In our
study, we explore how to acquire and evaluate pat-
terns, and investigate how to exploit the patterns to
mine semantic relatedness information for corefer-
ence resolution. The evaluation on ACE data set
shows that the patterned based features, when ap-
plied on NP pairs containing proper names, can ef-
fectively help the performance of coreference res-
olution in the recall (up to 4.3%) and the overall
F-measure (up to 2.1%). The results also indicate
that using the single semantic relatedness feature has
more advantages than using a set of pattern features.
For future work, we intend to investigate our
approach in more difficult tasks like the bridging
anaphora resolution, in which the semantic relations
involved are more complicated. Also, we would like
to explore the approach in technical (e.g., biomedi-
cal) domains, where jargons are frequently seen and
the need for external knowledge is more compelling.
Acknowledgements This research is supported by a
Specific Targeted Research Project (STREP) of the European
Union?s 6th Framework Programme within IST call 4, Boot-
strapping Of Ontologies and Terminologies STrategic REsearch
Project (BOOTStrep).
References
D. Bean and E. Riloff. 2004. Unsupervised learning of contex-
tual role knowledge for coreference resolution. In Proceed-
ings of NAACL, pages 297?304.
P. Cimiano and S. Staab. 2004. Learning by googling.
SIGKDD Explorations Newsletter, 6(2):24?33.
T. Cover and J. Thomas. 1991. Elements of Information The-
ory. Hohn Wiley & Sons.
N. Garera and D. Yarowsky. 2006. Resolving and generating
definite anaphora by modeling hypernymy using unlabeled
corpora. In Proceedings of CoNLL , pages 37?44.
S. Harabagiu, R. Bunescu, and S. Maiorano. 2001. Text knowl-
edge mining for coreference resolution. In Proceedings of
NAACL, pages 55?62.
M. Hearst. 1998. Automated discovery of wordnet relations. In
Christiane Fellbaum, editor, WordNet: An Electronic Lexical
Database and Some of its Applications. MIT Press, Cam-
bridge, MA.
K. Markert, M. Nissim, and N. Modjeska. 2003. Using the
web for nominal anaphora resolution. In Proceedings of the
EACL workshop on Computational Treatment of Anaphora,
pages 39?46.
N. Modjeska, K. Markert, and M. Nissim. 2003. Using the
web in machine learning for other-anaphora resolution. In
Proceedings of EMNLP, pages 176?183.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of ACL,
pages 104?111, Philadelphia.
V. Ng. 2005. Machine learning for coreference resolution:
From local classification to global ranking. In Proceedings
of ACL, pages 157?164.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging
generic patterns for automatically harvesting semantic rela-
tions. In Proceedings of ACL, pages 113?1200.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceedings of
ACL, pages 143?150.
S. Ponzetto and M. Strube. 2006. Exploiting semantic role
labeling, wordnet and wikipedia for coreference resolution.
In Proceedings of NAACL, pages 192?199.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521?544.
R. Vieira and M. Poesio. 2000. An empirically based system
for processing definite descriptions. Computational Linguis-
tics, 27(4):539?592.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the Sixth Message understand-
ing Conference (MUC-6), pages 45?52, San Francisco, CA.
Morgan Kaufmann Publishers.
535
Proceedings of ACL-08: HLT, pages 843?851,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Entity-Mention Model for Coreference Resolution
with Inductive Logic Programming
Xiaofeng Yang1 Jian Su1 Jun Lang2
Chew Lim Tan3 Ting Liu2 Sheng Li2
1Institute for Infocomm Research
{xiaofengy,sujian}@i2r.a-star.edu.sg
2Harbin Institute of Technology
{bill lang,tliu}@ir.hit.edu.cn
lisheng@hit.edu.cn
3National University of Singapore,
tancl@comp.nus.edu.sg
Abstract
The traditional mention-pair model for coref-
erence resolution cannot capture information
beyond mention pairs for both learning and
testing. To deal with this problem, we present
an expressive entity-mention model that per-
forms coreference resolution at an entity level.
The model adopts the Inductive Logic Pro-
gramming (ILP) algorithm, which provides a
relational way to organize different knowledge
of entities and mentions. The solution can
explicitly express relations between an entity
and the contained mentions, and automatically
learn first-order rules important for corefer-
ence decision. The evaluation on the ACE data
set shows that the ILP based entity-mention
model is effective for the coreference resolu-
tion task.
1 Introduction
Coreference resolution is the process of linking mul-
tiple mentions that refer to the same entity. Most
of previous work adopts the mention-pair model,
which recasts coreference resolution to a binary
classification problem of determining whether or not
two mentions in a document are co-referring (e.g.
Aone and Bennett (1995); McCarthy and Lehnert
(1995); Soon et al (2001); Ng and Cardie (2002)).
Although having achieved reasonable success, the
mention-pair model has a limitation that informa-
tion beyond mention pairs is ignored for training and
testing. As an individual mention usually lacks ad-
equate descriptive information of the referred entity,
it is often difficult to judge whether or not two men-
tions are talking about the same entity simply from
the pair alone.
An alternative learning model that can overcome
this problem performs coreference resolution based
on entity-mention pairs (Luo et al, 2004; Yang et
al., 2004b). Compared with the traditional mention-
pair counterpart, the entity-mention model aims to
make coreference decision at an entity level. Classi-
fication is done to determine whether a mention is a
referent of a partially found entity. A mention to be
resolved (called active mention henceforth) is linked
to an appropriate entity chain (if any), based on clas-
sification results.
One problem that arises with the entity-mention
model is how to represent the knowledge related to
an entity. In a document, an entity may have more
than one mention. It is impractical to enumerate all
the mentions in an entity and record their informa-
tion in a single feature vector, as it would make the
feature space too large. Even worse, the number of
mentions in an entity is not fixed, which would re-
sult in variant-length feature vectors and make trou-
ble for normal machine learning algorithms. A solu-
tion seen in previous work (Luo et al, 2004; Culotta
et al, 2007) is to design a set of first-order features
summarizing the information of the mentions in an
entity, for example, ?whether the entity has any men-
tion that is a name alias of the active mention?? or
?whether most of the mentions in the entity have the
same head word as the active mention?? These fea-
tures, nevertheless, are designed in an ad-hoc man-
ner and lack the capability of describing each indi-
vidual mention in an entity.
In this paper, we present a more expressive entity-
843
mention model for coreference resolution. The
model employs Inductive Logic Programming (ILP)
to represent the relational knowledge of an active
mention, an entity, and the mentions in the entity. On
top of this, a set of first-order rules is automatically
learned, which can capture the information of each
individual mention in an entity, as well as the global
information of the entity, to make coreference deci-
sion. Hence, our model has a more powerful repre-
sentation capability than the traditional mention-pair
or entity-mention model. And our experimental re-
sults on the ACE data set shows the model is effec-
tive for coreference resolution.
2 Related Work
There are plenty of learning-based coreference reso-
lution systems that employ the mention-pair model.
A typical one of them is presented by Soon et al
(2001). In the system, a training or testing instance
is formed for two mentions in question, with a fea-
ture vector describing their properties and relation-
ships. At a testing time, an active mention is checked
against all its preceding mentions, and is linked with
the closest one that is classified as positive. The
work is further enhanced by Ng and Cardie (2002)
by expanding the feature set and adopting a ?best-
first? linking strategy.
Recent years have seen some work on the entity-
mention model. Luo et al (2004) propose a system
that performs coreference resolution by doing search
in a large space of entities. They train a classifier that
can determine the likelihood that an active mention
should belong to an entity. The entity-level features
are calculated with an ?Any-X? strategy: an entity-
mention pair would be assigned a feature X, if any
mention in the entity has the feature X with the ac-
tive mention.
Culotta et al (2007) present a system which uses
an online learning approach to train a classifier to
judge whether two entities are coreferential or not.
The features describing the relationships between
two entities are obtained based on the information
of every possible pair of mentions from the two en-
tities. Different from (Luo et al, 2004), the entity-
level features are computed using a ?Most-X? strat-
egy, that is, two given entities would have a feature
X, if most of the mention pairs from the two entities
have the feature X.
Yang et al (2004b) suggest an entity-based coref-
erence resolution system. The model adopted in the
system is similar to the mention-pair model, except
that the entity information (e.g., the global num-
ber/gender agreement) is considered as additional
features of a mention in the entity.
McCallum and Wellner (2003) propose several
graphical models for coreference analysis. These
models aim to overcome the limitation that pair-
wise coreference decisions are made independently
of each other. The simplest model conditions coref-
erence on mention pairs, but enforces dependency
by calculating the distance of a node to a partition
(i.e., the probability that an active mention belongs
to an entity) based on the sum of its distances to all
the nodes in the partition (i.e., the sum of the prob-
ability of the active mention co-referring with the
mentions in the entity).
Inductive Logic Programming (ILP) has been ap-
plied to some natural language processing tasks, in-
cluding parsing (Mooney, 1997), POS disambigua-
tion (Cussens, 1996), lexicon construction (Claveau
et al, 2003), WSD (Specia et al, 2007), and so on.
However, to our knowledge, our work is the first ef-
fort to adopt this technique for the coreference reso-
lution task.
3 Modelling Coreference Resolution
Suppose we have a document containing n mentions
{mj : 1 < j < n}, in which mj is the jth mention
occurring in the document. Let ei be the ith entity in
the document. We define
P (L|ei,mj), (1)
the probability that a mention belongs to an entity.
Here the random variable L takes a binary value and
is 1 if mj is a mention of ei.
By assuming that mentions occurring after mj
have no influence on the decision of linking mj to
an entity, we can approximate (1) as:
P (L|ei,mj)
? P (L|{mk ? ei, 1 ? k ? j ? 1},mj) (2)
? max
mk?ei,1?k?j?1
P (L|mk,mj) (3)
(3) further assumes that an entity-mention score
can be computed by using the maximum mention-
844
[ Microsoft Corp. ]11 announced [ [ its ]12 new CEO ]23
[ yesterday ]34. [ The company ]15 said [ he ]26 will . . .
Table 1: A sample text
pair score. Both (2) and (1) can be approximated
with a machine learning method, leading to the tra-
ditional mention-pair model and the entity-mention
model for coreference resolution, respectively.
The two models will be described in the next sub-
sections, with the sample text in Table 1 used for
demonstration. In the table, a mention m is high-
lighted as [ m ]eidmid, where mid and eid are the IDs
for the mention and the entity to which it belongs,
respectively. Three entity chains can be found in the
text, that is,
e1 : Microsoft Corp. - its - The company
e2 : its new CEO - he
e3 : yesterday
3.1 Mention-Pair Model
As a baseline, we first describe a learning framework
with the mention-pair model as adopted in the work
by Soon et al (2001) and Ng and Cardie (2002).
In the learning framework, a training or testing
instance has the form of i{mk, mj}, in which mj is
an active mention and mk is a preceding mention.
An instance is associated with a vector of features,
which is used to describe the properties of the two
mentions as well as their relationships. Table 2 sum-
marizes the features used in our study.
For training, given each encountered anaphoric
mention mj in a document, one single positive train-
ing instance is created for mj and its closest an-
tecedent. And a group of negative training in-
stances is created for every intervening mentions
between mj and the antecedent. Consider the ex-
ample text in Table 1, for the pronoun ?he?, three
instances are generated: i(?The company?,?he?),
i(?yesterday?,?he?), and i(?its new CEO?,?he?).
Among them, the first two are labelled as negative
while the last one is labelled as positive.
Based on the training instances, a binary classifier
can be generated using any discriminative learning
algorithm. During resolution, an input document is
processed from the first mention to the last. For each
encountered mention mj , a test instance is formed
for each preceding mention, mk. This instance is
presented to the classifier to determine the corefer-
ence relationship. mj is linked with the mention that
is classified as positive (if any) with the highest con-
fidence value.
3.2 Entity-Mention Model
The mention-based solution has a limitation that in-
formation beyond a mention pair cannot be captured.
As an individual mention usually lacks complete de-
scription about the referred entity, the coreference
relationship between two mentions may be not clear,
which would affect classifier learning. Consider
a document with three coreferential mentions ?Mr.
Powell?, ?he?, and ?Powell?, appearing in that or-
der. The positive training instance i(?he?, ?Powell?)
is not informative, as the pronoun ?he? itself dis-
closes nothing but the gender. However, if the whole
entity is considered instead of only one mention, we
can know that ?he? refers to a male person named
?Powell?. And consequently, the coreference rela-
tionships between the mentions would become more
obvious.
The mention-pair model would also cause errors
at a testing time. Suppose we have three mentions
?Mr. Powell?, ?Powell?, and ?she? in a document.
The model tends to link ?she? with ?Powell? be-
cause of their proximity. This error can be avoided,
if we know ?Powell? belongs to the entity starting
with ?Mr. Powell?, and therefore refers to a male
person and cannot co-refer with ?she?.
The entity-mention model based on Eq. (2) per-
forms coreference resolution at an entity-level. For
simplicity, the framework considered for the entity-
mention model adopts similar training and testing
procedures as for the mention-pair model. Specif-
ically, a training or testing instance has the form of
i{ei, mj}, in which mj is an active mention and ei
is a partial entity found before mj . During train-
ing, given each anaphoric mention mj , one single
positive training instance is created for the entity to
which mj belongs. And a group of negative train-
ing instances is created for every partial entity whose
last mention occurs between mj and the closest an-
tecedent of mj .
See the sample in Table 1 again. For the pronoun
?he?, the following three instances are generated for
845
Features describing an active mention, mj
defNP mj 1 if mj is a definite description; else 0
indefNP mj 1 if mj is an indefinite NP; else 0
nameNP mj 1 if mj is a named-entity; else 0
pron mj 1 if mj is a pronoun; else 0
bareNP mj 1 if mj is a bare NP (i.e., NP without determiners) ; else 0
Features describing a previous mention, mk
defNP mk 1 if mk is a definite description; else 0
indefNP mk 1 if mk is an indefinite NP; else 0
nameNP mk 1 if mk is a named-entity; else 0
pron mk 1 if mk is a pronoun; else 0
bareNP mk 1 if mk is a bare NP; else 0
subject mk 1 if mk is an NP in a subject position; else 0
Features describing the relationships between mk and mj
sentDist sentence distance between two mentions
numAgree 1 if two mentions match in the number agreement; else 0
genderAgree 1 if two mentions match in the gender agreement; else 0
parallelStruct 1 if two mentions have an identical collocation pattern; else 0
semAgree 1 if two mentions have the same semantic category; else 0
nameAlias 1 if two mentions are an alias of the other; else 0
apposition 1 if two mentions are in an appositive structure; else 0
predicative 1 if two mentions are in a predicative structure; else 0
strMatch Head 1 if two mentions have the same head string; else 0
strMatch Full 1 if two mentions contain the same strings, excluding the determiners; else 0
strMatch Contain 1 if the string of mj is fully contained in that of mk ; else 0
Table 2: Feature set for coreference resolution
entity e1, e3 and e2:
i({?Microsoft Corp.?, ?its?, ?The company?},?he?),
i({?yesterday?},?he?),
i({?its new CEO?},?he?).
Among them, the first two are labelled as negative,
while the last one is positive.
The resolution is done using a greedy clustering
strategy. Given a test document, the mentions are
processed one by one. For each encountered men-
tion mj , a test instance is formed for each partial en-
tity found so far, ei. This instance is presented to the
classifier. mj is appended to the entity that is classi-
fied as positive (if any) with the highest confidence
value. If no positive entity exists, the active mention
is deemed as non-anaphoric and forms a new entity.
The process continues until the last mention of the
document is reached.
One potential problem with the entity-mention
model is how to represent the entity-level knowl-
edge. As an entity may contain more than one candi-
date and the number is not fixed, it is impractical to
enumerate all the mentions in an entity and put their
properties into a single feature vector. As a base-
line, we follow the solution proposed in (Luo et al,
2004) to design a set of first-order features. The fea-
tures are similar to those for the mention-pair model
as shown in Table 2, but their values are calculated
at an entity level. Specifically, the lexical and gram-
matical features are computed by testing any men-
tion1 in the entity against the active mention, for ex-
1Linguistically, pronouns usually have the most direct coref-
ample, the feature nameAlias is assigned value 1 if
at least one mention in the entity is a name alias of
the active mention. The distance feature (i.e., sent-
Dist) is the minimum distance between the mentions
in the entity and the active mention.
The above entity-level features are designed in an
ad-hoc way. They cannot capture the detailed infor-
mation of each individual mention in an entity. In
the next section, we will present a more expressive
entity-mention model by using ILP.
4 Entity-mention Model with ILP
4.1 Motivation
The entity-mention model based on Eq. (2) re-
quires relational knowledge that involves informa-
tion of an active mention (mj), an entity (ei), and
the mentions in the entity ({mk ? ei}). How-
ever, normal machine learning algorithms work on
attribute-value vectors, which only allows the repre-
sentation of atomic proposition. To learn from rela-
tional knowledge, we need an algorithm that can ex-
press first-order logic. This requirement motivates
our use of Inductive Logic Programming (ILP), a
learning algorithm capable of inferring logic pro-
grams. The relational nature of ILP makes it pos-
sible to explicitly represent relations between an en-
tity and its mentions, and thus provides a powerful
expressiveness for the coreference resolution task.
erence relationship with antecedents in a local discourse.
Hence, if an active mention is a pronoun, we only consider the
mentions in its previous two sentences for feature computation.
846
ILP uses logic programming as a uniform repre-
sentation for examples, background knowledge and
hypotheses. Given a set of positive and negative ex-
ample E = E+ ? E?, and a set of background
knowledge K of the domain, ILP tries to induce a
set of hypotheses h that covers most of E+ with no
E?, i.e., K ? h |= E+ and K ? h 6|= E?.
In our study, we choose ALEPH2, an ILP imple-
mentation by Srinivasan (2000) that has been proven
well suited to deal with a large amount of data in
multiple domains. For its routine use, ALEPH fol-
lows a simple procedure to induce rules. It first se-
lects an example and builds the most specific clause
that entertains the example. Next, it tries to search
for a clause more general than the bottom one. The
best clause is added to the current theory and all the
examples made redundant are removed. The proce-
dure repeats until all examples are processed.
4.2 Apply ILP to coreference resolution
Given a document, we encode a mention or a par-
tial entity with a unique constant. Specifically, mj
represents the jth mention (e.g., m6 for the pronoun
?he?). ei j represents the partial entity i before the
jth mention. For example, e1 6 denotes the part of
e1 before m6, i.e., {?Microsoft Corp.?, ?its?, ?the
company?}, while e1 5 denotes the part of e1 be-
fore m5 (?The company?), i.e., {?Microsoft Corp.?,
?its?}.
Training instances are created as described in Sec-
tion 3.2 for the entity-mention model. Each instance
is recorded with a predicate link(ei j , mj), where mj
is an active mention and ei j is a partial entity. For
example, the three training instances formed by the
pronoun ?he? are represented as follows:
link(e1 6,m6).
link(e3 6,m6).
link(e2 6,m6).
The first two predicates are put into E?, while the
last one is put to E+.
The background knowledge for an instance
link(ei j , mj) is also represented with predicates,
which are divided into the following types:
1. Predicates describing the information related to
ei j and mj . The properties of mj are pre-
2http://web.comlab.ox.ac.uk/oucl/
research/areas/machlearn/Aleph/aleph toc.html
sented with predicates like f (m, v), where f
corresponds to a feature in the first part of Ta-
ble 2 (removing the suffix mj), and v is its
value. For example, the pronoun ?he? can be
described by the following predicates:
defNP(m6, 0). indefNP(m6, 0).
nameNP(m6, 0). pron(m6, 1).
bareNP(m6, 0).
The predicates for the relationships between
ei j and mj take a form of f (e, m, v). In our
study, we consider the number agreement (ent-
NumAgree) and the gender agreement (entGen-
derAgree) between ei j and mj . v is 1 if all
of the mentions in ei j have consistent num-
ber/gender agreement with mj , e.g,
entNumAgree(e1 6,m6, 1).
2. Predicates describing the belonging relations
between ei j and its mentions. A predicate
has mention(e, m) is used for each mention in
e 3. For example, the partial entity e1 6 has
three mentions, m1, m2 and m5, which can be
described as follows:
has mention(e1 6,m1).
has mention(e1 6,m2).
has mention(e1 6,m5).
3. Predicates describing the information related to
mj and each mention mk in ei j . The predi-
cates for the properties of mk correspond to the
features in the second part of Table 2 (removing
the suffix mk), while the predicates for the re-
lationships between mj and mk correspond to
the features in the third part of Table 2. For ex-
ample, given the two mentions m1 (?Microsoft
Corp.) and m6 (?he), the following predicates
can be applied:
nameNP(m1, 1).
pron(m1, 0).
. . .
nameAlias(m1,m6, 0).
sentDist(m1,m6, 1).
. . .
the last two predicates represent that m1 and
3If an active mention mj is a pronoun, only the previous
mentions in two sentences apart are recorded by has mention,
while the farther ones are ignored as they have less impact on
the resolution of the pronoun.
847
m6 are not name alias, and are one sentence
apart.
By using the three types of predicates, the dif-
ferent knowledge related to entities and mentions
are integrated. The predicate has mention acts as
a bridge connecting the entity-mention knowledge
and the mention-pair knowledge. As a result, when
evaluating the coreference relationship between an
active mention and an entity, we can make use of
the ?global? information about the entity, as well as
the ?local? information of each individual mention
in the entity.
From the training instances and the associated
background knowledge, a set of hypotheses can be
automatically learned by ILP. Each hypothesis is
output as a rule that may look like:
link(A,B):-
predi1, predi2, . . . , has mention(A,C), . . . , prediN.
which corresponds to first-order logic
?A,B(predi1 ? predi2 ? . . .?
?C(has mention(A,C) ? . . . ? prediN)
? link(A,B))
Consider an example rule produced in our system:
link(A,B) :-
has mention(A,C), numAgree(B,C,1),
strMatch Head(B,C,1), bareNP(C,1).
Here, variables A and B stand for an entity and an
active mention in question. The first-order logic is
implemented by using non-instantiated arguments C
in the predicate has mention. This rule states that a
mention B should belong to an entity A, if there ex-
ists a mention C in A such that C is a bare noun
phrase with the same head string as B, and matches
in number with B. In this way, the detailed informa-
tion of each individual mention in an entity can be
captured for resolution.
A rule is applicable to an instance link(e, m), if
the background knowledge for the instance can be
described by the predicates in the body of the rule.
Each rule is associated with a score, which is the
accuracy that the rule can produce for the training
instances.
The learned rules are applied to resolution in a
similar way as described in Section 3.2. Given an
active mention m and a partial entity e, a test in-
stance link(e, m) is formed and tested against every
rule in the rule set. The confidence that m should
Train Test
#entity #mention #entity #mention
NWire 1678 9861 411 2304
NPaper 1528 10277 365 2290
BNews 1695 8986 468 2493
Table 3: statistics of entities (length > 1) and contained
mentions
belong to e is the maximal score of the applicable
rules. An active mention is linked to the entity with
the highest confidence value (above 0.5), if any.
5 Experiments and Results
5.1 Experimental Setup
In our study, we did evaluation on the ACE-2003
corpus, which contains two data sets, training and
devtest, used for training and testing respectively.
Each of these sets is further divided into three do-
mains: newswire (NWire), newspaper (NPaper), and
broadcast news (BNews). The number of entities
with more than one mention, as well as the number
of the contained mentions, is summarized in Table 3.
For both training and resolution, an input raw
document was processed by a pipeline of NLP
modules including Tokenizer, Part-of-Speech tag-
ger, NP Chunker and Named-Entity (NE) Recog-
nizer. Trained and tested on Penn WSJ TreeBank,
the POS tagger could obtain an accuracy of 97% and
the NP chunker could produce an F-measure above
94% (Zhou and Su, 2000). Evaluated for the MUC-
6 and MUC-7 Named-Entity task, the NER mod-
ule (Zhou and Su, 2002) could provide an F-measure
of 96.6% (MUC-6) and 94.1%(MUC-7). For evalu-
ation, Vilain et al (1995)?s scoring algorithm was
adopted to compute recall and precision rates.
By default, the ALEPH algorithm only generates
rules that have 100% accuracy for the training data.
And each rule contains at most three predicates. To
accommodate for coreference resolution, we loos-
ened the restrictions to allow rules that have above
50% accuracy and contain up to ten predicates. De-
fault parameters were applied for all the other set-
tings in ALEPH as well as other learning algorithms
used in the experiments.
5.2 Results and Discussions
Table 4 lists the performance of different corefer-
ence resolution systems. For comparison, we first
848
NWire NPaper BNews
R P F R P F R P F
C4.5
- Mention-Pair 68.2 54.3 60.4 67.3 50.8 57.9 66.5 59.5 62.9
- Entity-Mention 66.8 55.0 60.3 64.2 53.4 58.3 64.6 60.6 62.5
- Mention-Pair (all mentions in entity) 66.7 49.3 56.7 65.8 48.9 56.1 66.5 47.6 55.4
ILP
- Mention-Pair 66.1 54.8 59.5 65.6 54.8 59.7 63.5 60.8 62.1
- Entity-Mention 65.0 58.9 61.8 63.4 57.1 60.1 61.7 65.4 63.5
Table 4: Results of different systems for coreference resolution
examined the C4.5 algorithm4 which is widely used
for the coreference resolution task. The first line of
the table shows the baseline system that employs the
traditional mention-pair model (MP) as described in
Section 3.1. From the table, our baseline system
achieves a recall of around 66%-68% and a preci-
sion of around 50%-60%. The overall F-measure
for NWire, NPaper and BNews is 60.4%, 57.9% and
62.9% respectively. The results are comparable to
those reported in (Ng, 2005) which uses similar fea-
tures and gets an F-measure ranging in 50-60% for
the same data set. As our system relies only on sim-
ple and knowledge-poor features, the achieved F-
measure is around 2-4% lower than the state-of-the-
art systems do, like (Ng, 2007) and (Yang and Su,
2007) which utilized sophisticated semantic or real-
world knowledge. Since ILP has a strong capability
in knowledge management, our system could be fur-
ther improved if such helpful knowledge is incorpo-
rated, which will be explored in our future work.
The second line of Table 4 is for the system
that employs the entity-mention model (EM) with
?Any-X? based entity features, as described in Sec-
tion 3.2. We can find that the EM model does not
show superiority over the baseline MP model. It
achieves a higher precision (up to 2.6%), but a lower
recall (2.9%), than MP. As a result, we only see
?0.4% difference between the F-measure. The re-
sults are consistent with the reports by Luo et al
(2004) that the entity-mention model with the ?Any-
X? first-order features performs worse than the nor-
mal mention-pair model. In our study, we also tested
the ?Most-X? strategy for the first-order features as
in (Culotta et al, 2007), but got similar results with-
out much difference (?0.5% F-measure) in perfor-
4http://www.rulequest.com/see5-info.html
mance. Besides, as with our entity-mention predi-
cates described in Section 4.2, we also tried the ?All-
X? strategy for the entity-level agreement features,
that is, whether all mentions in a partial entity agree
in number and gender with an active mention. How-
ever, we found this bring no improvement against
the ?Any-X? strategy.
As described, given an active mention mj , the MP
model only considers the mentions between mj and
its closest antecedent. By contrast, the EM model
considers not only these mentions, but also their an-
tecedents in the same entity link. We were interested
in examining what if the MP model utilizes all the
mentions in an entity as the EM model does. As
shown in the third line of Table 4, such a solution
damages the performance; while the recall is at the
same level, the precision drops significantly (up to
12%) and as a result, the F-measure is even lower
than the original MP model. This should be because
a mention does not necessarily have direct corefer-
ence relationships with all of its antecedents. As the
MP model treats each mention-pair as an indepen-
dent instance, including all the antecedents would
produce many less-confident positive instances, and
thus adversely affect training.
The second block of the table summarizes the per-
formance of the systems with ILP. We were first con-
cerned with how well ILP works for the mention-
pair model, compared with the normally used algo-
rithm C4.5. From the results shown in the fourth
line of Table 4, ILP exhibits the same capability in
the resolution; it tends to produce a slightly higher
precision but a lower recall than C4.5 does. Overall,
it performs better in F-measure (1.8%) for Npaper,
while slightly worse (<1%) for Nwire and BNews.
These results demonstrate that ILP could be used as
849
link(A,B) :-
bareNP(B,0), has mention(A,C), appositive(C,1).
link(A,B) :-
has mention(A,C), numAgree(B,C,1), strMatch Head(B,C,1), bareNP(C,1).
link(A,B) :-
nameNP(B,0), has mention(A,C), predicative(C,1).
link(A,B) :-
has mention(A,C), strMatch Contain(B,C,1), strMatch Head(B,C,1), bareNP(C,0).
link(A,B) :-
nameNP(B,0), has mention(A,C), nameAlias(C,1), bareNP(C,0).
link(A,B) :-
pron(B,1), has mention(A,C), nameNP(C,1), has mention(A,D), indefNP(D,1),
subject(D, 1).
...
Figure 1: Examples of rules produced by ILP (entity-
mention model)
a good classifier learner for the mention-pair model.
The fifth line of Table 4 is for the ILP based entity-
mention model (described in Section 4.2). We can
observe that the model leads to a better performance
than all the other models. Compared with the sys-
tem with the MP model (under ILP), the EM version
is able to achieve a higher precision (up to 4.6% for
BNews). Although the recall drops slightly (up to
1.8% for BNews), the gain in the precision could
compensate it well; it beats the MP model in the
overall F-measure for all three domains (2.3% for
Nwire, 0.4% for Npaper, 1.4% for BNews). Es-
pecially, the improvement in NWire and BNews is
statistically significant under a 2-tailed t test (p <
0.05). Compared with the EM model with the man-
ually designed first-order feature (the second line),
the ILP-based EM solution also yields better perfor-
mance in precision (with a slightly lower recall) as
well as the overall F-measure (1.0% - 1.8%).
The improvement in precision against the
mention-pair model confirms that the global infor-
mation beyond a single mention pair, when being
considered for training, can make coreference rela-
tions clearer and help classifier learning. The bet-
ter performance against the EM model with heuristi-
cally designed features also suggests that ILP is able
to learn effective first-order rules for the coreference
resolution task.
In Figure 1, we illustrate part of the rules pro-
duced by ILP for the entity-mention model (NWire
domain), which shows how the relational knowledge
of entities and mentions is represented for decision
making. An interesting finding, as shown in the last
rule of the table, is that multiple non-instantiated ar-
guments (i.e. C and D) could possibly appear in
the same rule. According to this rule, a pronominal
mention should be linked with a partial entity which
contains a named-entity and contains an indefinite
NP in a subject position. This supports the claims
in (Yang et al, 2004a) that coreferential informa-
tion is an important factor to evaluate a candidate an-
tecedent in pronoun resolution. Such complex logic
makes it possible to capture information of multi-
ple mentions in an entity at the same time, which is
difficult to implemented in the mention-pair model
and the ordinary entity-mention model with heuris-
tic first-order features.
6 Conclusions
This paper presented an expressive entity-mention
model for coreference resolution by using Inductive
Logic Programming. In contrast to the traditional
mention-pair model, our model can capture infor-
mation beyond single mention pairs for both training
and testing. The relational nature of ILP enables our
model to explicitly express the relations between an
entity and its mentions, and to automatically learn
the first-order rules effective for the coreference res-
olution task. The evaluation on ACE data set shows
that the ILP based entity-model performs better than
the mention-pair model (with up to 2.3% increase in
F-measure), and also beats the entity-mention model
with heuristically designed first-order features.
Our current work focuses on the learning model
that calculates the probability of a mention be-
longing to an entity. For simplicity, we just use a
greedy clustering strategy for resolution, that is, a
mention is linked to the current best partial entity.
In our future work, we would like to investigate
more sophisticated clustering methods that would
lead to global optimization, e.g., by keeping a large
search space (Luo et al, 2004) or using integer
programming (Denis and Baldridge, 2007).
Acknowledgements This research is supported
by a Specific Targeted Research Project (STREP)
of the European Union?s 6th Framework Programme
within IST call 4, Bootstrapping Of Ontologies and
Terminologies STrategic REsearch Project (BOOT-
Strep).
850
References
C. Aone and S. W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strate-
gies. In Proceedings of the 33rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 122?129.
V. Claveau, P. Sebillot, C. Fabre, and P. Bouillon. 2003.
Learning semantic lexicons from a part-of-speech and
semantically tagged corpus using inductive logic pro-
gramming. Journal of Machine Learning Research,
4:493?525.
A. Culotta, M. Wick, and A. McCallum. 2007. First-
order probabilistic models for coreference resolution.
In Proceedings of the Annual Meeting of the North
America Chapter of the Association for Computational
Linguistics (NAACL), pages 81?88.
J. Cussens. 1996. Part-of-speech disambiguation using
ilp. Technical report, Oxford University Computing
Laboratory.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North America Chapter of the Association for
Computational Linguistics (NAACL), pages 236?243.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
135?142.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In Proceedings of IJCAI-
03 Workshop on Information Integration on the Web,
pages 79?86.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of
the 14th International Conference on Artificial Intel-
ligences (IJCAI), pages 1050?1055.
R. Mooney. 1997. Inductive logic programming for nat-
ural language processing. In Proceedings of the sixth
International Inductive Logic Programming Work-
shop, pages 3?24.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 104?111,
Philadelphia.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
157?164.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 536?543.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
L. Specia, M. Stevenson, and M. V. Nunes. 2007. Learn-
ing expressive models for words sense disambiguation.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
41?48.
A. Srinivasan. 2000. The aleph manual. Technical re-
port, Oxford University Computing Laboratory.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth Mes-
sage understanding Conference (MUC-6), pages 45?
52, San Francisco, CA. Morgan Kaufmann Publishers.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automati-
cally discovered patterns. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 528?535.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004a. Improv-
ing pronoun resolution by incorporating coreferential
information of candidates. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 127?134, Barcelona.
X. Yang, J. Su, G. Zhou, and C. Tan. 2004b. An
NP-cluster approach to coreference resolution. In
Proceedings of the 20th International Conference on
Computational Linguistics, pages 219?225, Geneva.
G. Zhou and J. Su. 2000. Error-driven HMM-based
chunk tagger with context-dependent lexicon. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora, pages 71?79, Hong Kong.
G. Zhou and J. Su. 2002. Named Entity recognition us-
ing a HMM-based chunk tagger. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 473?480, Philadel-
phia.
851
Effective Adaptation of a Hidden Markov Model-based Named Entity 
Recognizer for Biomedical Domain 
Dan Shen?? Jie Zhang?? Guodong Zhou? Jian Su? Chew-Lim Tan?
? Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
? Department of Computer Science 
National University of Singapore 
3 Science Drive 2, Singapore 117543 
{shendan,zhangjie,zhougd,sujian}@i2r.a-star.edu.sg 
{tancl}@comp.nus.edu.sg 
 
 
Abstract 
In this paper, we explore how to adapt a 
general Hidden Markov Model-based 
named entity recognizer effectively to 
biomedical domain.  We integrate various 
features, including simple deterministic 
features, morphological features, POS 
features and semantic trigger features, to 
capture various evidences especially for 
biomedical named entity and evaluate 
their contributions.  We also present a 
simple algorithm to solve the abbreviation 
problem and a rule-based method to deal 
with the cascaded phenomena in biomedi-
cal domain.  Our experiments on GENIA 
V3.0 and GENIA V1.1 achieve the 66.1 
and 62.5 F-measure respectively, which 
outperform the previous best published 
results by 8.1 F-measure when using the 
same training and testing data.  
1 Introduction 
As the research in biomedical domain has grown 
rapidly in recent years, a huge amount of nature 
language resources have been developed and be-
come a rich knowledge base.  The technique of 
named entity (NE) recognition (NER) is strongly 
demanded to be applied in biomedical domain.  
Since in previous work, many NER systems have 
been applied successfully in newswire domain 
(Zhou and Su 2002; Bikel et al 1999; Borthwich et 
al. 1999), more and more explorations have been 
done to port existing NER system into biomedical 
domain (Kazama et al 2002; Takeuchi et al 2002; 
Nobata et al 1999 and 2000; Collier et al 2000; 
Gaizauskas et al 2000; Fukuda et al 1998; Proux 
et al 1998).  However, compared with those in 
newswire domain, these systems haven?t got high 
performance.  It is probably because of the follow-
ing factors of biomedical NE (Zhang et al 2003): 
1. Some modifiers are often before basic NEs, 
e.g. activated B cell lines, and sometimes biomedi-
cal NEs are very long, e.g. 47 kDa sterol regula-
tory element binding factor.  This kind of factor 
highlights the difficulty for identifying the bound-
ary of NE. 
2. Two or more NEs share one head noun by 
using conjunction or disjunction construction, e.g. 
91 and 84 kDa proteins.  It is hard to identify these 
NEs respectively. 
3. An entity may be found with various spelling 
forms, e.g. N-acetylcysteine, N-acetyl-cysteine, 
NAcetylCysteine, etc.  Since the use of capitaliza-
tion is casual, the capitalization information may 
not be so evidential in this domain. 
4. NE may be cascaded.  One NE may be em-
bedded in another NE, e.g. <PROTEIN><DNA> 
kappa 3</DNA> binding factor </PROTEIN>.  
More effort must be made to identify this kind of 
NE. 
5. Abbreviations are frequently used in bio-
medical domain, e.g. TCEd, IFN, TPA, etc.  Since 
abbreviations don?t have many evidences for cer-
tain NE class, it is difficult to classify them cor-
rectly. 
These factors above make NER in biomedical 
domain difficult.  Therefore, it is necessary to ex-
plore more evidential features and more effective 
methods to cope with such difficulties. 
In this paper, we will study how to adapt a gen-
eral Hidden Markov Model (HMM)-based NE rec-
ognizer (Zhou and Su 2002) to biomedical domain.  
We specially explore various evidences for bio-
medical NE and propose methods to cope with ab-
breviations and cascaded phenomena.  As a result, 
features (simple deterministic features, morpho-
logical features, part-of-speech features and head 
noun trigger features) and methods (abbreviation 
recognition algorithm and rule-based cascaded 
phenomena resolution) are integrated in our system.  
The experiment shows that system outperforms the 
best published system by 8.1 F-measure. 
In Section 2, we will introduce the HMM-
based NE recognizer briefly.  In Section 3, we will 
focus on the features that we have used.  The 
methods and the adaptations of different features 
will be discussed in detail.  In Section 5 and 6, we 
will present the solutions of abbreviation and cas-
caded phenomena. Finally, our experiment results 
will be presented and the contributions of different 
features will be analyzed in Section 7. 
2 
3 
3.1 
HMM-based Named Entity Recognizer 
Our system is adapted from a HMM-based NE 
recognizer, which has been proved very effective 
in MUC (Zhou and Su 2002). 
The purpose of HMM is to find the most likely 
tag sequence T for a given sequence 
of tokens G  that maxi-
mizes . 
n
n ttt ???= 211
n gg ?= 211
)1
nG
ng??
|( 1
nTP
In token sequence G , the token g  is defined 
as , where w is the word and is 
the feature set related with the word . 
n
1 i
i
>=< iii wfg , i if
w
In tag sequenceT , each tag consists of three 
parts: 1. Boundary category, which denotes the 
position of the current word in NE.  2. Entity cate-
gory, which indicates the NE class.  3. Feature set, 
which will be discussed in Section 3. 
n
1 it
When we incorporate a plentiful feature set in 
HMM, we will encounter data sparseness problem.  
An alternative back-off modeling approach by 
means of constraint relaxation is applied in our 
model (Zhou and Su 2002).  It enables the decod-
ing process effectively find a near optimal fre-
quently occurred pattern entry in determining the 
NE tag probability distribution of current word. 
Finally, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence 
in the state space of the possible tag distribution 
based on the state transition probabilities.  Fur-
thermore, some constraints on the boundary cate-
gory and entity category between two consecutive 
tags are applied to filter the invalid NE tags (Zhou 
and Su 2002). 
Feature Set 
Simple Deterministic Features (Fsd) 
The purpose of simple deterministic features is to 
capture the capitalization, digitalization and word 
formation information.  This kind of features have 
been widely used in both newswire NER system, 
such as (Zhou and Su 2002), and biomedical NER 
system, such as (Nobata et al 1999; Gaizauskas et 
al. 2000; Collier et al 2000; Takeuchi and Collier 
2002; Kazama et al 2002).  Based on the charac-
teristics of biomedical NEs, we designed simple 
deterministic features manually.  Table 1 shows the 
simple deterministic features with descending or-
der of priority. 
 
Fsd Name Example 
Comma , 
Dot . 
LRB ( 
RRB ) 
LSB [ 
RSB ] 
RomanDigit II 
GreekLetter Beta 
StopWord in, at 
ATCGsequence AACAAAG 
OneDigit 5 
AllDigits 60 
DigitCommaDigit 1,25 
DigitDotDigit 0.5 
OneCap T 
AllCaps CSF 
CapLowAlpha All 
CapMixAlpha IgM 
LowMixAlpha kDa 
AlphaDigitAlpha H2A 
AlphaDigit T4 
DigitAlphaDigit 6C2 
DigitAlpha 19D 
Table 1: Simple deterministic features 
From Table 1, we can find that: 
1. Features such as comma, dot, StopWord, etc. 
are designed intuitively to provide information to 
detect the boundary of NE. 
2. Features Parenthesis is often used to indicate 
the definition of abbreviation in biomedical docu-
ments. 
3. Features GreekLetter and RomanDigit are 
specially designed to capture the symbols 
frequently occurred in biomedical NE. 
4. Feature ATCG sequence identify the similar-
ity of words according to their word formations, 
e.g. AACAAAG, CTCAGGA, etc. 
5. Features dealing with mixed alphabets and 
digits such as AlphaDigitAlpha, CapMixAlpha, etc. 
are beneficial for biomedical abbreviations. 
Furthermore, we evaluate these features and 
compare with those used in MUC (Zhou and Su, 
2002).  The reported result of the simple determi-
nistic features used in MUC can achieve F-
measure of 74.1 (Zhou and Su 2002), but when 
they are used in biomedical domain, they only get 
F-measure of 24.3.  By contrast, using the simple 
deterministic features we designed for biomedical 
NER, the system achieves F-measure of 29.4.  Ac-
cording to the comparison, some findings may be 
concluded as follows: 
1) Simple deterministic features are domain de-
pendent, which suggests that it is necessary to de-
sign special features for biomedical NER. 
2) Simple deterministic features have weaker 
predictive power for NE classes in biomedical do-
main than in newswire domain. 
3.2 Morphological Feature (Fm) 
Morphological information, such as prefix/suffix, 
is considered as an important cue for terminology 
identification.  In our system, we get most frequent 
100 prefixes and suffixes from training data as 
candidates.  Then, each of these candidates is 
evaluated according to formula f1.  ( )
i
ii
i N
OUTIN
Wt
## ?=   (f1) 
in which, #INi is the number that prefix/suffix i 
occurs within NEs; #OUTi is the number that pre-
fix/suffix i occurs out of NEs; Ni is the total num-
ber of prefix/suffix i. 
The formula assumes that the particular pre-
fix/suffix, which is most likely inside NEs and 
least likely outside NEs, may be thought as a good 
evidence for distinguishing the NEs.  The candi-
dates with Wt above a certain threshold (0.7 in ex-
periment) are chosen.  Then, we calculated the 
frequency of each prefix/suffix in each NE class 
and group the prefixes/suffixes with the similar 
distribution among NE classes into one feature.  
This is because prefixes/suffixes with the similar 
distribution have the similar contribution, and it 
will avoid suffering from the data sparseness prob-
lem.  Some of morphological features were listed 
in Table 2. 
 
Fm Name Prefix/Suffix Example 
sOOC ~cin actinomycin 
 ~mide Cycloheximide 
 ~zole Sulphamethoxazole 
sLPD ~lipid Phospholipids 
 ~rogen Estrogen 
 ~vitamin dihydroxyvitamin 
sCTP ~blast erythroblast 
 ~cyte thymocyte 
 ~phil eosinophil 
sPEPT ~peptide neuropeptide 
sMA ~ma hybridoma 
sVIR ~virus cytomegalovirus 
Table 2: Examples of morphological features 
 
From Table 2, the suffixes ~cin, ~mide, ~zole 
have been grouped into one feature sOOC because 
they all have the high frequency in the NE class 
OtherOrganicCompound and relatively low fre-
quencies in the other NE classes.   In our system, 
totally 37 prefixes and suffixes were selected and 
grouped to 23 features. 
3.3 Part-of-Speech Features (Fpos) 
In the previous NER research in newswire domain, 
part-of-speech (POS) features were stated not use-
ful, as POS features may affect the use of some 
important capitalization information (Zhou and Su 
2002).  However, since more and more words with 
lower case are included in NEs, capitalization in-
formation in biomedical domain is not as eviden-
tial as it in newswire domain (Zhang et al 2003).  
Moreover, since many biomedical NEs are descrip-
tive and long, identifying NE boundary is not a 
trivial task.  POS tagging can provide the evidence 
of noun phrase region based on word syntactic in-
formation and the noun phrases are most likely to 
be NE.  Therefore, we reconsidered the POS tag-
ging.   
In previous research, (Kazama et al 2002) 
make use of POS information and conclude that it 
only slightly improves performance.  Moreover, 
(Collier et al 2000; Nobata et al 2000; Takeuchi 
and Collier. 2002) don?t incorporate POS informa-
tion in their systems.  The probable reason ex-
plained by them is that since POS tagger they used 
is trained on newswire articles, the assigned POS 
tags are often incorrect in biomedical documents.  
On the whole, it can be concluded that POS infor-
mation hasn?t been well used in previous work. 
In our experiment, a POS tagger was trained us-
ing 80% of GENIA V2.1 corpus (536 abstracts, 
123K words) and evaluated on the rest 20% (134 
abstracts, 29K words).  We use GENIA corpus to 
train the POS tagger in order to let it be adapted for 
biomedical domain.  As for comparison, we also 
trained the POS tagger on Wall Street Journal arti-
cles (2500 articles, 756K words) and tested on the 
20% of GENIA corpus.  The results are shown in 
Table 3. 
 
Training set Testing set Precision 
2500 WSJ articles 84.31 
536 GENIA abstracts 
134 GENIA 
abstracts 97.37 
Table 3: Comparison of POS tagger using dif-
ferent training data  
 
From Table 3, it can be found that POS tagger 
trained on the biomedical documents performs 
much better on the biomedical testing documents 
than that trained on WSJ articles.  This is consis-
tent with earlier explanation for why POS features 
are not so useful in biomedical NER (Nobata et al 
2000; Takeuchi and Collier 2002).   
3.4 Semantic Trigger Features 
Semantic trigger features are collected to capture 
the evidence of certain NE class based on the se-
mantic information of some key words.  Initially, 
we design two types of semantic triggers: head 
noun triggers and special verb triggers. 
3.4.1 Head Noun Triggers (Fhnt) 
Head noun means the main noun or noun phrase of 
some compound words and describes the function 
or the property, e.g. ?B cells? is the head noun for 
the NE ?activated human B cells?.  Compared with 
the other words in NE, head noun is a much more 
decisive factor for distinguishing NE classes.  For 
instance, 
<OtherName>IFN-gamma treatment</OtherName> 
<DNA>IFN-gamma activation sequence</DNA> 
In our work, we extract uni-gram and bi-grams 
of head nouns automatically from training data, 
and rank them by frequency.  According to the ex-
periment, we selected 60% top ranked head nouns 
as trigger features for each NE class.  Some exam-
ples are shown in Table 4. 
In the future application, we may also extract 
the head nouns from some public resources to en-
hance the triggers. 
 
1-gram 2-grams 
PROTEIN 
interleukin activator protein 
interferon binding protein 
kinase cell receptor 
ligand gene product 
CELL TYPE 
lymphocyte blast cell 
astrocyte blood lymphocyte 
eosinophil killer cell 
fibroblast peripheral monocyte 
DNA 
DNA X chromosome 
breakpoint alpha promoter 
cDNA binding motif 
chromosome promoter element 
Table 4: Examples of head noun triggers 
3.4.2 Special Verb Triggers (Fsvt) 
Besides collecting the triggers, such as head noun 
triggers, from the NEs themselves, we also extract 
the triggers from the local contexts of the NEs.  
Recently, some frequently occurred verbs in bio-
medical document have been proved useful for 
extracting the interaction between entities (Thomas 
et al 2000; Sekimizu et al 1998).  In biomedical 
NER, we have the intuition that particular verbs 
may also provide the evidence for boundary and 
NE class.  For instance, the verb bind is often used 
to indicate the interaction between proteins. 
In our system, we selected 20 most frequent 
verbs which occur adjacent to NE from training 
data automatically as the verb trigger features, 
which is shown in Table 5.   
 
 
Special Verb Triggers 
activate express 
bind induce 
inhibit interact 
regulate stimulate 
Table 5: Examples of special verb triggers 
4 Method for Abbreviation Recognition 
Abbreviations are widely used in biomedical do-
main.  Identifying the class of them constitutes an 
important and difficult problem (Zhang et al 2003). 
In our current system, we incorporate a method 
to classify abbreviation by mapping the abbrevia-
tion to its full form. This approach is based on the 
assumption that it is easier to classify the full form 
than abbreviation.  In most cases, this assumption 
is valid because the full form has more evidences 
than its abbreviation to capture the NE class.  
Moreover, if we can map the abbreviation to its 
full form in the current document, the recognized 
abbreviation is still helpful for classifying the same 
forthcoming abbreviations in the same document, 
as in (Zhou and Su 2002). 
In practice, abbreviation and its full form often 
occur simultaneously with parenthesis when first 
appear in biomedical documents.  There are two 
cases: 
1. full form (abbreviation) 
2. abbreviation (full form) 
Most patterns conform to the first case and if 
the content inside the parenthesis includes more 
than two words, the second case is assumed 
(Schwartz and Hearst 2003).   
In these two cases, the use of parenthesis is 
both evidential and confusing.  On one hand, it is 
evidential because it can provide the indication to 
map the abbreviation to its full form.  On the other 
hand, it is confusing because it makes the annota-
tion of NE more complicated.  Sometimes, the ab-
breviation and its full form are annotated 
separately, such as  
<CellType>human mononulear leuko-
cytes</CellType>(<CellType>hMNL</CellType>), 
and sometimes, they are all embedded in the whole 
entity, such as 
<OtherName>leukotriene B4 (LTB4) genera-
tion</OtherName>.   
Therefore, parenthesis needs to be treated specially.  
We develop an abbreviation recognition algorithm 
described in Figure 1. 
In preprocessing stage, we remove the abbre-
viations and parentheses from the sentence, when 
the abbreviation is first defined.  This measure will 
make the annotation simpler and the NE recognizer 
more effective.  The main work in this stage is to 
judge which case the current pattern belongs to and 
record the original positions of the abbreviation 
and parenthesis. 
After applying the HMM-based NE recognizer 
to the sentence, we restore the abbreviation and 
parenthesis to the original position in the sentence.  
Next, the abbreviation is classified.  There are two 
priorities of the class (from high to low): the class 
of its full form identified by the recognizer, and the 
class of the abbreviation itself identified by the 
recognizer.  At last, the same abbreviation occur-
ring in the rest sentences of the current document 
are assigned the same NE class.   
 
for each sentence Si in the document{ 
if exist parenthesis{ 
judge the case of { 
?full form (abbr.)?; 
?abbr. (full form)?; 
} 
store the abbr. A and position Pa  to a list; 
record the parenthesis position Pp; 
remove A and parenthesis from sentence; 
apply HMM-based NE recognizer to Si; 
restore A and parenthesis into Pa, Pp; 
if Pp within an identified NE E with the class CE 
parenthesis is included in E; 
else{ 
parenthesis is not included; 
   classify A to CE; 
   classify A in the rest part of document to CE; 
} 
} 
else apply HMM-based NE recognizer to Si; 
} 
Figure 1: Abbreviation recognition algorithm 
5 Solution of Cascaded Phenomena 
In (Zhang et al 2003), they state that 16.57% of 
NEs in GENIA V3.0 have cascaded annotations, 
such as  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Currently, we only consider the longest NE and 
ignore the embedded NEs.   
Based on the features described in section 3, 
our system counters some problems when dealing 
with cascaded NEs.  The probable reason is that 
the features we used are not so effective for this 
kind of NEs.   
For instance, POS is based on the assumption 
that NE is most likely to be a noun phrase.  For 
cascaded NE, this assumption may not always be 
valid because one NE may consist of two or more 
noun phrases connected by some special words, 
such as TSH receptor specific T cell lines. 
Moreover, in section 3.4.1, we have shown that 
head noun is the significant clue for distinguishing 
NE classes.  Even for cascaded NEs, head noun 
features are still effective to some extent, such as 
IL-2 mRNA.  However, cascaded NEs sometimes 
contain two or more head nouns, which belong to 
different NE classes.  For example, <DNA>IgG Fc 
receptor type IC gene</DNA>, in which receptor 
is the head noun of protein and gene is the head 
noun of DNA.  In general, the latter head noun will 
be more important.  Unfortunately, it seems that 
sometimes the shorter NE is more possible to be 
identified, such as <protein>IgG Fc recep-
tor</protein> type IC gene.   
On the whole, we have to explore an additional 
method to cope with the cascaded phenomena 
separately.  In our experiment, we attempt to solve 
this problem based on some rules. 
In GENIA corpus, we find that there are four 
basic types of cascaded NEs: 
1. < <NE> head noun >  
2. < modifier <NE> > 
3. < <NE1> <NE2> > 
4. < <NE1> word <NE2> > 
Moreover, these cascaded NEs may be generated 
iteratively.  For instance, 
5. < modifier <NE> head noun > 
6. < <NE1> <NE2> head noun > 
The rules are constructed automatically from 
the cascaded NEs in training data.  Corresponding 
to the four basic types of cascaded NEs mentioned 
before, we propose four patterns and apply them 
iteratively in each sentence: 
1. <entity1> head noun ? <entity2>  
e.g. <Protein> binding motif ? <DNA> 
2. <entity1> <entity2> ? <entity3> 
e.g. <Lipid> <Protein> ? <Protein> 
3. modifier <entity1> ? <entity2> 
e.g. anti <Protein> ? <Protein> 
4. <entity1> word <entity2> ? <entity3> 
e.g. <Virus> infected <Multicell> ? <Multicell> 
In our system, 102 rules are incorporated to 
classify the cascaded NEs. 
6 
6.1 
6.2 
Experiments 
GENIA Corpus 
GENIA corpus is the largest annotated corpus in 
molecular biology domain available to public 
(Ohta et al 2002).  In our experiment, three ver-
sions are used: 
? GENIA Version 1.1 (V1.1) -- It contains 670 
MEDLINE abstracts.  Since a lot of previous re-
lated work used this version, we use it to compare 
our result with others?. 
? GENIA Version 2.1 (V2.1) -- It contains the 
same 670 abstracts as V1.1 and POS tagging.  We 
use it to train and evaluate our POS tagger. 
? GENIA Version 3.0 (V3.0) -- It contains 2000 
abstracts, which is the superset of V1.1.  We use it 
to get the latest result and find out the effect of 
training data size. 
The annotation of NE is based on the GENIA 
ontology.  In our task, we use 23 distinct NE 
classes.  As for the conjunctive and disjunctive 
NEs, we ignore such cases and take the whole con-
struction as one entity.  In addition, for the cas-
caded annotations in V3.0, currently, we only 
consider the longest one level of the annotations. 
Experimental Results 
The system is evaluated using standard ?preci-
sion/recall/F-measure?, in which ?F-measure? is 
defined as F-measure = (2PR) / (P+R). 
We evaluate our NER system on both V3.0 and 
V1.1, each of which has been split into a training 
set and a testing set.  As for V1.1, we divide the 
corpus into 590 abstracts (136K words) as training 
set and the rest 80 abstracts (17K words) as testing 
set.  As for V3.0, we use the same testing set as 
V1.1 and the rest 1920 abstracts (447K words) as 
training set. 
 
Corpus P R F 
Our system on V3.0 66.5 65.7 66.1 
Our system on V1.1 63.8 61.3 62.5 
Kazama?s on V1.1 56.2 52.8 54.4 
Table 6: Comparison of overall performance 
 
Table 6 shows the overall performance of our 
system on V3.0 and V1.1, and the best reported 
system on V1.1 described in (Kazama et al 2002).  
On V1.1, we use the same training and testing data 
and capture the same NE classes as (Kazama et al 
2002).  Our system (62.5 F-measure) outperforms 
Kazama?s (54.4 F-measure) by 8.1 F-measure.  
This probably benefits from the various evidential 
features and the effective methods we proposed.  
Furthermore, as our expectation, the performance 
achieved on V3.0 (66.1 F-measure) is better than 
that on V1.1 (62.5 F-measure), which indicate that 
our system still has some room for improvement 
with the larger training data set. 
 
 
Figure 2: Performance of each NE class 
 
In addition, Figure 2 shows the detailed per-
formance chart of each NE class on V3.0.  In the 
figure, the numbers in the parenthesis are the num-
ber that NEs of that class occur in training/testing 
data.  It can be found that the performances vary a 
lot among the NE classes.  Some NE classes that 
have very few training data, such as Carbohydrate 
and Organism, get extremely low performance.  
In order to evaluate the contributions of differ-
ent features, we evaluate our system using different 
combinations of features (Table 7). 
From Table 7, several findings are concluded:  
1) With only Fsd, our system achieves a basic 
level F-measure of 29.4. 
2) Fm shows the positive effect with 2.4 F-
measure improvement based on the basic level.  
However, it only can slightly improve the perform-
ance (+1.2 F-measure) based on Fsd, Fpos and Fhnt.  
The probable reason is that the evidences included 
in Fm have already been captured by Fhnt.  More-
over, the evidences captured by Fhnt are more accu-
rate than that captured by Fm.  The contribution 
made by Fm may come from where there is no indi-
cation of Fhnt. 
 
Fsd Fm Fpos Fhnt Fsvt P R F 
?     42.4 22.5 29.4 
? ?    44.8 24.6 31.8 
? ? ?   58.3 50.9 54.3 
?  ? ?  62.0 61.6 61.8 
? ? ? ?  64.4 61.7 63.0 
? ? ? ? ? 60.6 59.3 60.0 
Table 7: Effects of different features on V3.0 
 
3) Fpos is proved very beneficial as it makes 
great increase on F-measure (+22.5) based on Fsd 
and Fm.   
4) Fhnt leads to an improvement of 8.7 F-
measure based on Fsd, Fm and Fpos. 
5) Out of our expectation, the use of Fsvt de-
creases both precision and recall, which may be 
explained as the present and past participles of 
some special verbs often play the adjective-like 
roles inside biomedical NEs, such as IL10-
inhibited lymphocytes.  
 
 P R F 
Fsd+Fm+Fpos+Fhnt 64.4 61.7 63.0 
+abbr. recog. algorithm 64.6 62.5 63.5 
+rule-based casc. method 66.2 65.8 66.0 
+both 66.5 65.7 66.1 
Table 8: Effects of solution for abbr. and casc. 
 
From Table 8, it can be found that the abbrevia-
tion recognition method slightly improves the per-
formance by 0.5 F-measure.  The probable reason 
is that the recognition of abbreviation relies too 
much on the recognition of its full form.  Once the 
full form is wrongly classified, the abbreviation 
and the forthcoming ones throughout the document 
are wrong altogether.  In the near future, the pre-
defined abbreviation dictionary may be incorpo-
rated to enhance the decision of NE class. 
Moreover, it can be found that the rule-based 
method effectively solves the problem of cascaded 
phenomena and shows prominent improvement 
(+3.0 F-measure) based on the performance of 
?Fsd+Fm+Fpos+Fhnt?. 
7 Conclusion 
In the paper, we describe our exploration on how 
to adapt a general HMM-based named entity rec-
ognizer to biomedical domain.  We integrate vari-
ous evidences for biomedical NER, including 
lexical, morphological, syntactic and semantic in-
formation.  Furthermore, we present a simple algo-
rithm to solve the abbreviation problem and a rule-
based method to deal with the cascaded phenom-
ena. Based on such evidences and methods, our 
system is successfully adapted to biomedical do-
main and achieves significantly better performance 
than the best published system.  In the near future, 
more effective abbreviation recognition algorithm 
and some pre-defined NE lists for some classes 
may be incorporated to enhance our system. 
Acknowledgements 
We would like to thank Mr. Tan Soon Heng for his 
support of biomedical knowledge.   
References 
M. Bikel Danie, R.Schwartz and M. Weischedel Ralph. 
1999.  An Algorithm that Learns What's in a Name. 
In Proc. of Machine Learning (Special Issue on NLP). 
A. Borthwick.  1999.  A Maximum Entropy Approach 
to Named Entity Recognition. Ph.D. Thesis. New 
York University. 
N. Collier, C. Nobata, and J. Tsujii.  2000.  Extracting 
the names of genes and gene products with a hidden 
Markov model.  In Proc. of COLING 2000, pages 
201-207. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi.  
1998.  Toward information extraction: identifying 
protein names from biological papers.  In Proc. of the 
Pacific Symposium on Biocomputing?98 (PSB?98), 
pages 707-718, January. 
R. Gaizauskas, G. Demetriou and K. Humphreys.  Term 
Recognition and Classification in Biological Science 
Journal Articles.  2000.  In Proc. of the Computional 
Terminology for Medical and Biological Applications 
Workshop of the 2nd International Conference on 
NLP, pages 37-44. 
J. Kazama, T. Makino, Y.Ohta, and J. Tsujii.  2002.  
Tuning Support Vector Machines for Biomedical 
Named Entity Recognition.  In Proc. of the Work-
shop on Natural Language Processing in the Bio-
medical Domain (at ACL?2002), pages 1-8. 
C. Nobata, N. Collier, and J. Tsujii.  1999.  Automatic 
term identification and classification in biology texts.  
In Proc. of the 5th NLPRS, pages 369-374. 
C. Nobata, N. Collier, and J. Tsujii.  2000.  Comparison 
between tagged corpora for the named entity task.  In 
Proc. of the Workshop on Comparing Corpora (at 
ACL?2000), pages 20-27. 
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.  
2002.  The GENIA corpus: An annotated research 
abstract corpus in molecular biology domain.  In 
Proc. of HLT 2002. 
D. Proux, F. Rechenmann, L. Julliard, V. Pillet and B. 
Jacq.  1998.  Detecting Gene Symbols and Names in 
Biological Texts: A First Step toward Pertinent In-
formation Extraction.  In Proc. of Genome Inform 
Ser Workshop Genome Inform, pages 72-80. 
A.S. Schwartz and M.A. Hearst.  2003.  A Simple Algo-
rithm for Identifying Abbreviation Definitions in 
Biomedical Text.  In Proc. of the Pacific Symposium 
on Biocomputing (PSB 2003) Kauai. 
T. Sekimizu, H. Park, and J. Tsujii.  1998.  Identifying 
the interaction between genes and gene products 
based on frequently seen verbs in medline abstracts.  
In Proc. of Genome Informatics, Universal Academy 
Press, Inc.  
K. Takeuchi and N. Collier.  2002.  Use of Support Vec-
tor Machines in Extended Named Entity Recognition.  
In Proc. of the Sixth Conference on Natural Lan-
guage Learning (CONLL 2002), pages 119-125. 
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and M. 
Carroll.  2000.  Automatic extraction of protein inter-
actions from scientific abstracts.  In Proc. of the Pa-
cific Symposium on Biocomputing?2000 (PSB?2000), 
pages 541-551, Hawaii, January. 
A. J. Viterbi.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding algo-
rithm.  In Proc. of IEEE Transactions on Information 
Theory, pages 260-269. 
J. Zhang, D. Shen, G. Zhou, J. Su and C. Tan. 2003.  
Exploring Various Evidences for Recognition of 
Named Entities in Biomedical Domain.  Submitted to 
EMNLP 2003. 
G. Zhou and J. Su.  2002.  Named Entity Recognition 
using an HMM-based Chunk Tagger.  In Proc. of the 
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 473-480. 
A Chinese Efficient Analyser Integrating Word Segmentation, 
Part-Of-Speech Tagging, Partial Parsing and Full  Parsing 
 
GuoDong ZHOU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
zhougd@i2r.a-star.edu.sg 
Jian SU 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore, 119613 
sujian@ i2r.a-star.edu.sg 
 
The objective of this paper is to develop an 
efficient analyser for the Chinese language, 
exploring different intermediate forms, achieving a 
target speed in the region of 1,000 wps for full 
parsing, 2,000 wps for partial parsing and 10,000 
wps for word segmentation and part-of-speech 
tagging, with state-of-art performances. 
Abstract  
This paper introduces an efficient analyser for 
the Chinese language, which efficiently and 
effectively integrates  word segmentation, 
part-of-speech tagging, partial parsing and full 
parsing. The Chinese efficient analyser is based 
on a Hidden Markov Model (HMM) and an 
HMM-based tagger. That is, all the 
components are based on the same 
HMM-based tagging engine. One advantage of 
using the same single engine is that it largely 
decreases the code size and makes the 
maintenance easy. Another advantage is that it 
is easy to optimise the code and thus improve 
the speed while speed plays a critical important 
role in many applications. Finally, the 
performances of all the components can benefit 
from the optimisation of existing algorithms 
and/or adoption of better algorithms to a single 
engine. Experiments show that all the 
components can achieve state-of-art 
performances with high efficiency for the 
Chinese language.  
The layout of this paper is as follows. Section 2 
describes the Chinese efficient analyser. Section 3 
presents the HMM and the HMM-based tagger. 
Sections 4 and 5 describe the applications of the 
HMM-based tagger in integrated word 
segmentation and part-of-speech tagging, partial 
parsing, and full parsing respectively. Section 6 
gives the experimental results. Finally, some 
conclusions are drawn with possible extensions of 
future work in section 7. 
2 Chinese Efficient Analyser 
The Chinese efficient analyser can be described by 
the example as shown in Figure 1. Here, "." in 
Figure 1 means that the current node has not been 
chunked till now. For convenience, it is regarded as 
a "special chunk" in this paper and others as 
"normal chunks". Therefore, every node in Figure 1 
can be represented as a 3tuple , where  
is the i -th chunk in the input chunk sequence and 
),( iii wpc ic
1  Introduction 
Traditionally, a text parser outputs a complete parse 
tree for each input sentence, achieving a speed in 
the order of 10 words per second (wps) (Abney 
1997). However, for many applications like text 
mining, a parse tree is not necessary and a speed of 
10 wps is unacceptable when we have to process 
millions of words in thousands of documents in a 
reasonable time (Feldman 1997). Therefore, there is 
a compromise between speed and performance in 
many applications. 
?  is the head word of c  and  is the POS 
tag of w  when 
iw i ip
i .?ic  ( c  is a normal chunk). 
In this case, we call node c  a normal 
chunk node. 
i
)(i p , ii w
Full Parsing = N levels of Partial Parsing  
(N = Parsing Depth and for Figure 1, N=3) 
 
 
 S(VB, ??) 
3rd-level 3tuple sequence  
 
 
 
 
   
   
   
   
   
   
   
   
   
  
  
   
   
   
   
   
   
   
 
?  is just the word linked with c  and  is 
the POS tag of  when  ( c  is a special 
chunk). In this case, we call node c  a 
special chunk or POS node. 
iw i
i
i
ip
)iw
iw .=ic
,( ip
Figure 1 shows that, sequentially from bottom 
to top, 
1) Given a Chinese sentence (e.g. ????????
?????), it is segmented and tagged into a 
sequence of special chunk, POS and word 
3tuples (.(ADJ, ? ? ) .(NN, ? ? ) .(ADV, 
?) .(VB, ??) .(ADJ, , ??) ) via the 
integrated word segme
In this paper, this res
called "0th-level 3tuple
2) The 0th-level 3tuple sequence is then chunked 
into 1st-level 3tuple sequence (NP(NN, ?
?) .(ADV, ?) .(VB, ??) NP(NN, ??)) via 
1st-level partial parsing, while POS nodes .(ADJ, 
??) and .(NN, ??) are chunked into a normal 
chunk node NP(NN, ? ? ), and POS 
nodes .(ADJ, ??) .(NN, ??) into NP(NN, ?
?). 
3) The 1st-level 3tuple sequence is further chunked 
into 2nd-level 3 N, ??) 
VP(VB, ?? )) parsing, 
while mixed al chunk 
nodes .(ADV, ? N, ??) are 
NP(NN, ??) VP(VB, ??) 
NP(NN, ??) .(ADV,?) .(VB,??) NP(NN, ??)) 
.(ADJ,??) .(NN,??)  .(ADV,?) .(VB,??) .(ADJ,??) .(NN,??) 
      ?           ?            ?      ?                      ?                          ?  ?                   ?    ?                    ?         ? 
   developed      country             also                 exist              man
   (English Translation: There also exists many problems in deve
Figure 1: An Example Sentence ????????????? of the Chinese Efficien
2nd-level Partial Parsing 
1st-level  PartialParsing  
egmentation  
agging
3rd-level Partial Parsing
2nd-level 3tuple sequence
1st-level 3tuple sequence
0th-level 3tuple  
sequence??) .(NN 
 
Integrated Word S
and POS Tntation and POS tagging. 
ulting 3tuple sequence is 
 sequence". 
chunked i
?). y              problem 
loped countries) nto atuple sequence (NP(N
 via 2nd-level partial  normal chunt Analyser POS and norm
) .(VB, ??) NP(Nk node VP(VB, ?
4) Finally, 2nd-level 3tuple sequence is chunked 
into 3rd-level 3tuple sequence (S(VB, ??)) via 
3rd-level partial parsing, while normal chunk 
nodes NP(NN, ?? ) and VP(VB, ?? ) are 
chunked into a normal chunk node S(VB, ??).  
5) In this way, full parsing is completed with a 
fully parsed tree after several levels (3 in the 
example of Figure 1) of cascaded partial 
parsing.  
3 HMM-based Tagger 
The Chinese efficient analyser is based on the 
HMM-based tagger described in Zhou et al2000a. 
Given a token sequence G , the goal 
of tagging is to find a stochastic optimal tag 
sequence  that maximizes       
n
n ggg L211 =
n
n tttT L211 =
)()(
),(log)(log)|(log
11
11
111 nn
nn
nnn
GPTP
GTPTPGTP ?+=  
By assuming mutual information 
independence:  
?
=
=
n
i
n
i
nn GtMIGTMI
1
111 ),(),(  or 
?
= ?=?
n
i
n
i
n
i
nn
nn
GPtP
GtP
GPTP
GTP
1 1
1
11
11
)()(
),(log
)()(
),(log   
we have: 
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
GtP
tPTPGTP
1
1
1
111
)|(log
)(log)(log)|(log
  
Both the first and second items correspond to 
the language model component of the tagger. We 
will not discuss these two items further in this paper 
since they are well studied in ngram modeling. This 
paper will focus on the third item 
, which is the main difference 
between our tagger and other HMM-based taggers. 
Ideally, it can be estimated by using the 
forward-backward algorithm (Rabiner 1989) 
recursively for the first-order (Rabiner 1989) or 
second-order HMMs (Watson et al1992). To 
simplify the complexity, several context dependent 
approximations on it will be attempted in this paper 
instead, as detailed in sections 3 and 4. 
?
=
n
i
n
i GtP
1
1 )|(log
All of this modelling would be for naught were 
it not for the existence of an efficient algorithm for 
finding the optimal state sequence, thereby 
"decoding" the original sequence of tags. The 
stochastic optimal tag sequence can be found by 
maximizing the previous equation over all the 
possible tag sequences. This is implemented via the 
well-known Viterbi algorithm (Viterbi 1967) by 
using dynamic programming and an appropriate 
merging of multiple theories when they converge 
on a particular state. Since we are interested in 
recovering the tag state sequence, we pursue 16 
theories at every given step of the algorithm. 
4 Word Segmentation and POS Tagging 
Traditionally, in Chinese Language Processing, 
word segmentation and POS tagging are 
implemented sequentially. That is, the input 
Chinese sentence is segmented into words first and 
then the segmented result (in the form of word 
lattice or N-best word sequences) is passed to POS 
tagging component. However, this processing 
strategy has following disadvantages: 
? The word lexicons used in word segmentation 
and POS tagging may be different. This 
difference is difficult to overcome and largely 
drops the system accuracy although different 
optimal algorithms may be applied to word 
segmentation and POS tagging. 
? With speed in consideration, the two-stage 
processing strategy is not efficient. 
Therefore, we apply the strategy of integrating 
word segmentation and POS tagging in a single 
stage. This can be implemented as follows:  
1) Given an input sentence, a 3tuple (special 
chunk, POS and word) lattice is generated by 
skimming the sentence from left-to-right, and 
looking up the word and POS lexicon to 
determine all the possible words and get POS 
tag probability distribution for each possible 
word. 
2) Viterbi algorithm is applied to decode the 
3tuple lattice to find the most possible POS tag 
sequence. 
3) In this way, the given sentence is segmented 
into words with POS tags. 
The rationale behind the above algorithm is the 
ability of HMM in parallel segmentation and 
classification (Rabiner 1989). 
In order to overcome the coarse n-gram models 
raised by the limited number of orignial POS tags 
used in current Chinese POS tag bank (corpus), a 
word clustering algorithm (Bai et al1998) is applied 
to classify words into classes first and then the N 
(e.g. N=500) most frequently occurred word class 
and POS pairs are added to the original POS tag set 
to achieve more accurate models. For example, 
ADJ(<?? >) represents a special POS tag ADJ 
which pairs with the word class <??>. Here, <??> 
is a word class label. For convenience and clarity, 
we use the most frequently occurred word in a word 
class as the label to represent the word class. 
5 Partial Parsing and Full Parsing 
As discussed in section 2, obviously partial parsing 
can have different levels and full parsing can be 
achieved by cascading several levels of partial 
parsing (e.g. 3 levels of cascaded partial parsing can 
achieve full parsing for the example as shown in 
Figure 1).  
In this paper, a certain level (e.g. l -th level) of 
partial parsing is implemented via a chunking 
model, built on the HMM-based tagger as described 
in section 2, with ( -th level 3tuple sequence 
as input. That is, for the l -th level partial parsing, 
the chunking model has the ( -th level 3tuple 
sequence  (Here, 3tuple 
) as input. In the meantime, chunk 
tag t  used in the chunking model is structural and 
consists of following three parts: 
)1?l
gg 21=
)1?l
ng
nG1
)iw
L
,( iii pcg =
i
? Boundary Category B: It is a set of four values 
0, 1, 2, 3, where "0" means that the current 
3tuple is a whole chunk, "1" means that the 
current 3tuple is at the beginning of a chunk,  
"2" means that the current 3tuple is in the 
middle of a chunk and "3" means that the 
current stuple is at the end of a chunk. 
? Chunk Category C:  It is used to denote the 
output chunk category of the chunking model, 
which includes normal chunks and the special 
chunk ("."). The reason to include the special 
chunk is that some of POS 3tuple in the input 
sequence may not be chunked in the current 
chunking stage. 
? POS Category POS: Because of the limited 
number in boundary category and output chunk 
category, the POS category is added into the 
structural tag to represent more accurate 
models.  
Therefore,  can be represented by 
, where b  is the boundary type of 
,  is the output chunk type of t  and  is 
the POS type of t . Obviously, there exist some 
constraints between t  and  on the boundary 
categories and output chunk categories, as briefed 
in table 1, where "valid"/"invalid" means the chunk 
tag sequence t  is valid/invalid while "validon" 
means  is valid on the condition 
it
i
it
iii poscb __
it ic
ii tt 1?
i
1?i
i
c
ipos
ic
it
i 1?
i =?1 . 
 0 1 2 3 
0 Valid Valid Invalid Invalid 
1 Invalid Invalid Valid on Validon 
2 Invalid Invalid Valid Valid 
3 Valid Valid Invalid Invalid 
Table 1: Constraints between t  and t  1?i i
(Column: b  in t ; Row:  in t ) 1?i 1?i ib i
For the example as shown in Figure 1, we can 
see that: 
? In the 1st-level partial parsing, the input 3tuple 
sequence is the 0th-level 3tuple sequence .(ADJ, 
??) .(NN, ??) .(ADV, ?) .(VB, ??) .(ADJ, ?
? ) .(NN, ?? ) and the output tag sequence 
1_NP_ADJ 3_NP_NN 0_._ADV 0_._VB 
1_NP_ADJ 3_NP_NN, from where derived is 
the 1st-level 3tuple sequence NP(NN, ?
?) .(ADV, ?) .(VB, ??) NP(NN, ??). 
? In the 2nd-level partial parsing, the input 3tuple 
sequence is the 1st-level 3tuple sequence 
NP(NN, ??) .(ADV, ?) .(VB, ??) NP(NN, ?
? ) and the output tag sequence 0_NP_NN 
1_VP_ADV 2_VP_VB 3_VP_NN, from where 
derived is the 2nd-level 3tuple sequence NP(NN, 
??) VP(VB, ??). 
? In the 3rd-level partial parsing, the input 3tuple 
sequence is the 2nd-level 3tuple sequence 
NP(NN, ??) VP(VB, ??) and the output tag 
sequence 1_S_NN 3_S_VB, from where 
derived is the 3rd-level 3tuple sequence S(VB, ?
?). In this way, a fully parsed tree is reached. 
? In the cascaded chunking procedure, necessary 
information is stored for back-tracing. 
Partially/fully parsed trees can be constructed 
by tracing from the final 3tuple sequence back 
to 0th-level 3tuple sequence. Different levels of 
partial parsing can be achieved according to the 
need of the application. 
6 Experimental Results 
The Chinese efficient analyser is implemented in 
C++, providing a rapid and easy 
code-compile-train-test development cycle. In fact, 
many NLP systems suffer from a lack of software 
and computer-science engineering effort: running 
efficiency is key to performing numerous 
experiments, which, in turn, is key to improving 
performance. A system may have excellent 
performance on a given task, but if it takes long to 
compile and/or run on test data, the rate of 
improvement of that system will be contrained 
compared to that which can run very efficiently. 
Moreover, speed plays a critical role in many 
applications such as text mining.  
All the experiments are implemented on a 
Pentium II/450MHZ PC. All the performances are 
measured in precisions, recalls and F-measures. 
Here, the precision (P) measures the number of 
correct units in the answer file over the total number 
of units in the answer file and the recall (R) 
measures the number of correct units in the answer 
file over the total number of units in the key file 
while F-measure is the weighted harmonic mean of 
precision and recall: 
PR
RP
+
+= 2
2 )1(
?
?F  with 
=1. 2?
6.1 Word Segmentation and POS Tagging 
Table 2 shows the integrated word segmentation 
and POS tagging results on the Chinese tag bank 
PFR1.0 of 3.69M Chinese characters (1.12 Chinese 
Words) developed by Institute of Computational 
Linguistics at Beijing Univ. Here, 80% of the 
corpus is used as formal training data, another 10% 
as development data and remaining 10% as formal 
test data. 
Function P R F Speed  
Word Segment. 97.5 98.2 97.8 
POS Tagging 93.5 94.1 93.8 
11,000 
wps 
Table 2: Performances of Word Segmentation  
and POS Tagging (wps: words per second) 
The word segmentation corresponds to 
bracketing of the chunking model while POS 
tagging corresponds to bracketing and labelling. 
Table 2 shows that recall (P) is higher than 
precision (P). The main reason may be the existence 
of unknown words. In the Chinese efficient 
analyser, unknown words are segmented into 
individual Chinese characters. This makes the 
number of segmented words/POS tagged words in 
the system output higher than that in the correct 
answer. 
6.2 Partial Parsing and Full Parsing 
Table 3 shows the results of 1st-level partial parsing 
and full parsing, using the PARSEVAL evaluation 
methodology (Black et al1991) on the UPENN 
Chinese Tree Bank of 100k words developed by 
Univ. of Penn. Here, 80% of the corpus is used as 
formal training data, another 10% as development 
data and remaining 10% as formal test data. 
Function P R F  Speed 
Partial Parsing 85.1 82.5 83.8 4500 wps 
Full Parsing 77.1 70.3 73.7 2100 wps 
Table 3: Performances of 1st-level Partial Parsing 
and Full Parsing (wps: words per second) 
Table 3 shows that the performances of partial 
parsing and full parsing are quite low, compared to 
those of state-of-art partial parsing and full parsing 
for the English language (Zhou et al2000a; Collins 
1997). The main reason behind is the small size of 
the training corpus used in our experiments. 
However, the Chinese PENN Tree Bank is the 
largest corpus we can find for partial parsing and 
full parsing. Therefore, developing a much larger 
Chinese tree bank (comparable to UPENN English 
Tree Bank) becomes an urgent task for the Chinese 
language processing community. Actually, the best 
individual system (Zhou et al2000b) in 
CoNLL?2000 chunking shared task for the English 
language (Tjong et al2000) used the same 
HMM-based tagging engine.  
7  Conclusion 
This paper presents an efficient analyser for the 
Chinese language, based on a HMM and a single 
engine -- HMM-based tagger. Experiments show 
that the analyser achieves state-of-art performance 
at very high speed, which can meet the requirement 
of speed-critical applications such as text mining. 
Our future work includes: 
? Syntactic analysis of the partial/full parsing 
results into a meaningful intermediate form. 
? Research and development of Chinese named 
entity recognition using the same HMM-based 
tagger and its integration to the Chinese 
efficient analyser. 
Acknowledgements 
Thanks go to Institute of Computational Linguistics 
at Beijing Univ. and LDC at Univ. of Penn. for free 
research use of their Chinese Tag Bank and Chinese 
Tree Bank. 
References  
Abney S. 1997. Part-of-Speech Tagging and Partial 
Parsing. Corpus-based Methods in Natural 
Language Processing. Edited by Steve Young 
and Gerrit Bloothooft. Kluwer Academic 
Publishers, Dordrecht.  
Bai ShuanHu, Li HaiZhou, Lin ZhiWei and Yuan 
BaoSheng. 1998. Building class-based language 
models with contextual statistics. Proceedings of 
International Conference on Acoustics, Speech 
and Signal Processing (ICASSP'1998). 
pages173-176. Seattle, Washington, USA.  
Black E. and Abney S. 1991. A Procedure for 
Quantitatively Comparing the Syntactic Coverage 
of English Grammars. Proceedings of DRAPA 
workshop on Speech and Natural Language. 
pages306-311. Pacific Grove, CA. DRAPA.  
Collins M.J. 1997. Three Generative, Lexicalised 
Models for Statistical Parsing. Proceedings of the 
Thirtieth-Five Annual Meeting of the Association 
for Computational Linguistics (ACL?97). 
pages184-191. 
Feldman R. 1997. Text Mining - Theory and 
Practice. Proceedings of the Third International 
Conference on Knowledge Discovery & Data 
Mining (KDD?1997).  
Rabiner L. 1989. A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition. IEEE 77(2), pages257-285.  
Tjong K.S. Erik and Buchholz S. 2000. Introduction 
to the CoNLL-2000 Shared Task: Chunking. 
Proceedings of the Conference on Computational 
Language Learning (CoNLL'2000). 
Pages127-132. Lisbon, Portugal. 11-14 Sept. 
Viterbi A.J. 1967. Error Bounds for Convolutional 
Codes and an Asymptotically Optimum Decoding 
Algorithm. IEEE Transactions on Information 
Theory, IT 13(2), 260-269. 
Watson B. and Tsoi A Chunk. 1992. Second order 
Hidden Markov Models for speech recognition. 
Proceeding of the Fourth Australian 
International Conference on Speech Science and 
Technology. pages146-151.  
Zhou GuoDong and Su Jian. 2000a. Error-driven 
HMM-based Chunk Tagger with 
Context-dependent Lexicon. Proceedings of the 
Joint Conference on Empirical Methods on 
Natural Language Processing and Very Large 
Corpus (EMNLP/ VLC'2000). Hong Kong, 7-8 
Oct. 
Zhou GuoDong, Su Jian and Tey TongGuan. 
2000b. Hybrid Text Chunking. Proceedings of 
the Conference on Computational Language 
Learning (CoNLL'2000). Pages163-166. Lisbon, 
Portugal, 11-14 Sept. 
 
Exploring Deep Knowledge Resources in Biomedical Name Recognition 
 
ZHOU GuoDong    SU Jian 
Institute for Infocomm Research 
21 Heng Mui Keng Terrace 
Singapore 119613 
Email: {zhougd, sujian}@i2r.a-star.edu.sg  
 
Abstract 
In this paper, we present a named entity 
recognition system in the biomedical domain. In 
order to deal with the special phenomena in the 
biomedical domain, various evidential features are 
proposed and integrated through a Hidden 
Markov Model (HMM). In addition, a Support 
Vector Machine (SVM) plus sigmoid is proposed 
to resolve the data sparseness problem in our 
system. Besides the widely used lexical-level 
features, such as word formation pattern, 
morphological pattern, out-domain POS and 
semantic trigger, we also explore the name alias 
phenomenon, the cascaded entity name 
phenomenon, the use of both a closed dictionary 
from the training corpus and an open dictionary 
from the database term list SwissProt and the alias 
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus.  
1. The Baseline System 
1.1 Hidden Markov Model 
In this paper, we use the Hidden Markov Model 
(HMM) as described in Zhou et al(2002). Given 
an output sequence O , the system finds 
the most likely state sequence S  that 
maximizes  as follows:  
n
n ooo ...211 =
)
n
n sss ...211 =
|( 11
nn OSP
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
OsP
sPSPOSP
1
1
1
111
)|(log
)(log)(log)|(log
            (1) 
From Equation (1), we can see that: 
? The first term can be computed by applying 
chain rules. In ngram modeling (Chen et al1996), 
each tag is assumed to be dependent on the N-1 
previous tags.  
? The second term is the summation of log 
probabilities of all the individual tags. 
? The third term corresponds to the ?lexical? 
component (dictionary) of the tagger.   
The idea behind the model is that it tries to 
assign each output an appropriate tag (state), which 
contains boundary and class information.  For 
example, ?TCF 1 binds stronger than NF kB to 
TCEd DNA?. The tag assigned to token ?TCF? 
should indicate that it is at the beginning of an 
entity name and it belongs to the ?Protein? class; 
and the tag assigned to token ?binds? should 
indicate that it does not belong to an entity name.  
Here, the Viterbi algorithm (Viterbi 1967) is 
implemented to find the most likely tag sequence.  
The problem with the above HMM lies in the 
data sparseness problem raised by P  in the 
third term of Equation (1). In this paper, a Support 
Vector Machine (SVM) plus sigmoid is proposed 
to resolve this problem in our system. 
)|( 1
n
i Os
1.2 Support Vector Machine plus Sigmoid 
Support Vector Machines (SVMs) are a popular 
machine learning approach first presented by 
Vapnik (1995). Based on the structural risk 
minimization of statistical learning theory, SVMs 
seek an optimal separating hyper-plane to divide 
the training examples into two classes and make 
decisions based on support vectors which are 
selected as the only effective examples in the 
training set. However, SVMs produce an un-
calibrated value that is not probability. That is, the 
unthresholded output of an SVM can be 
represented as 
?
?
+??=
SVi
iii bxxkyaxf ),()(                 (2) 
To map the SVM output into the probability, we 
train an additional sigmoid model(Platt 1999): 
)exp(1
1)|1(
BAf
fyp ++==                (3) 
Basically, SVMs are binary classifiers. 
Therefore, we must extend SVMs to multi-class 
(e.g. K) classifiers. For efficiency, we apply the 
one vs. others strategy, which builds K classifiers 
so as to separate one class from all others, instead 
of the pairwise strategy, which builds K*(K-1)/2 
classifiers considering all pairs of classes. 
Moreover, we only apply the simple linear kernel, 
although other kernels (e.g. polynomial kernel) and 
pairwise strategy can have better performance. 
96
1.3 Features 
Various widely used lexical-level features are 
explored in the baseline system. 
? Word Formation Pattern (FWFP): The purpose 
of this feature is to capture capitalization, 
digitalization and other word formation 
information. In this paper, the same feature as in 
Shen et al2003 is used. 
? Morphological Pattern (FMP): Morphological 
information, such as prefix and suffix, is 
considered as an important cue for terminology 
identification.  Same as Shen et al2003, we use a 
statistical method to get the most useful 
prefixes/suffixes from the training data.  
? Part-of-Speech (FPOS): Since many of the 
words in biomedical entity names are in lowercase, 
capitalization information in the biomedical 
domain is not as evidential as that in the newswire 
domain. Moreover, many biomedical entity names 
are descriptive and very long. Therefore, POS may 
provide useful evidence about the boundaries of 
biomedical entity names. In the baseline system, an 
out-domain POS using the PENN TreeBank is 
applied. 
? Head Noun Trigger (FHEAD): The head noun, 
which is the major noun of a noun phrase, often 
describes the function or the property of the noun 
phrase. In this paper, we automatically extract 
unigram and bigram head nouns from the training 
data, and rank them by frequency. For each entity 
class, we select 50% of top ranked head nouns as 
head noun triggers.   
2. Deep Knowledge Resources 
Besides the widely used lexical-level features as 
described above, we also explore the name alias 
phenomenon, the cascaded entity name 
phenomenon, the use of both a closed dictionary 
from the training corpus and an open dictionary 
from the database term list SwissProt and the alias 
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus. 
2.1 Name Alias Resolution 
A novel name alias feature is proposed to resolve 
the name alias phenomenon. The intuition behind 
this feature is the name alias phenomenon that 
relevant entities will be referred to in many ways 
throughout a given text and thus success of named 
entity recognition is conditional on success at 
determining when one noun phrase refers to the 
very same entity as another noun phrase.  
During decoding, the entity names already 
recognized from the previous sentences of the 
document are stored in a list. When the system 
encounters an entity name candidate (e.g. a word 
with a special word formation pattern), a name 
alias algorithm (similar to Schwartz et al2003) is 
invoked to first dynamically determine whether the 
entity name candidate might be alias for a 
previously recognized name in the recognized list. 
The name alias feature FALIAS is represented as 
ENTITYnLm (L indicates the locality of the name 
alias phenomenon). Here ENTITY indicates the 
class of the recognized entity name and n indicates 
the number of the words in the recognized entity 
name while m indicates the number of the words in 
the recognized entity name from which the name 
alias candidate is formed.  For example, when the 
decoding process encounters the word ?TCF?, the 
word ?TCF? is proposed as an entity name 
candidate and the name alias algorithm is invoked 
to check if the word ?TCF? is an alias of a 
recognized named entity. If ?T cell Factor? is a 
?Protein? name recognized earlier in the 
document, the word ?TCF? is determined as an 
alias of ?T cell Factor? with the name alias feature 
Protein3L3 by taking the three initial letters of the 
three-word ?protein? name ?T cell Factor?. 
2.2 Cascaded Entity Name Resolution 
It is found (Shen et al2003) that 16.57% of entity 
names in GENIA V3.0 have cascaded 
constructions, e.g.  
<RNA><DNA>CIITA</DNA> mRNA</RNA>.   
Therefore, it is important to resolve such 
phenomenon.  
Here, a pattern-based module is proposed to 
resolve the cascaded entity names while the above 
HMM is applied to recognize embedded entity 
names and non-cascaded entity names. In the 
GENIA corpus, we find that there are six useful 
patterns of cascaded entity name constructions: 
? <ENTITY> := <ENTITY> + head noun, e.g. 
<PROTEIN> binding motif?<DNA> 
? <ENTITY> := <ENTITY>  + <ENTITY> 
? <ENTITY> := modifier + <ENTITY>, e.g.          
anti <Protein>?<Protein> 
? <ENTITY> := <ENTITY>  + word + 
<ENTITY> 
? <ENTITY> :=  modifier + <ENTITY> + head 
noun 
? <ENTITY> := <ENTITY> +  <ENTITY>  + 
head noun 
In our experiments, all the rules of above six 
patterns are extracted from the cascaded entity 
names in the GENIA V3.0 to deal with the 
97
cascaded entity name phenomenon where the 
<ENTITY> above is restricted to the five 
categories in the shared task: Protein, DNA, RNA, 
CellLine, CellType.  
2.3 Abbreviation Resolution 
While the name alias feature is useful to detect the 
inter-sentential name alias phenomenon, it is 
unable to identify the inner-sentential name alias 
phenomenon: the inner-sentential abbreviation.  
Such abbreviations widely occur in the biomedical 
domain.   
In our system, we present an effective and 
efficient algorithm to recognize the inner-sentential 
abbreviations more accurately by mapping them to 
their full expanded forms. In the GENIA corpus, 
we observe that the expanded form and its 
abbreviation often occur together via parentheses. 
Generally, there are two patterns: ?expanded form 
(abbreviation)? and ?abbreviation (expanded 
form)?.  
Our algorithm is based on the fact that it is 
much harder to classify an abbreviation than its 
expanded form. Generally, the expanded form is 
more evidential than its abbreviation to determine 
its class.  The algorithm works as follows: Given a 
sentence with parentheses, we use a similar 
algorithm as in Schwartz et al(2003) to determine 
whether it is an abbreviation with parentheses. If 
yes, we remove the abbreviation and the 
parentheses from the sentence. After the sentence 
is processed, we restore the abbreviation with 
parentheses to its original position in the sentence.  
Then, the abbreviation is classified as the same 
class of the expanded form, if the expanded form is 
recognized as an entity name. In the meanwhile, 
we also adjust the boundaries of the expanded form 
according to the abbreviation, if necessary. Finally, 
the expanded form and its abbreviation are stored 
in the recognized list of biomedical entity names 
from the document to help the resolution of 
forthcoming occurrences of the same abbreviation 
in the document. 
2.4 Dictionary 
In our system, two different features are explored 
to capture the existence of an entity name in a 
closed dictionary and an open dictionary. Here, the 
closed dictionary is constructed by extracting all 
entity names from the training data while the open 
dictionary (~700,000 entries) is combined from the 
database term list Swissport and the alias list 
LocusLink. The closed dictionary feature is 
represented as ClosedENTITYn (Here ENTITY 
indicates the class of the entity name and n 
indicates the number of the words in the entity 
name) while the open dictionary feature is 
represented as Openn (Here n indicates the number 
of the words in the entity name. We don?t 
differentiate the class of the entity name since the 
open dictionary only contains protein/gene names 
and their aliases). 
2.5 In-domain POS 
We also examine the impact of an in-domain POS 
feature instead of an out-domain POS feature 
which is trained on PENN TreeBank. Here, the in-
domain POS is trained on the GENIA corpus 
V3.02p. 
3. Evaluation 
Table 1 shows the performance of the baseline 
system and the impact of deep knowledge 
resources while Table 2-4 show the detailed 
performance using the provided scoring algorithm. 
Table 1 shows that: 
? The baseline system achieves F-measure of 
60.3 while incorporation of deep knowledge 
resources can improve the performance by 12.2 to 
72.5 in F-measure. 
? The replacement of the out-domain POS with 
in-domain POS improves the performance by 3.8 
in F-measure. This suggests in-domain POS can 
much improve the performance. 
? The name alias feature in name alias resolution 
slightly improves the performance by 0.9 in F-
measure. 
? The cascaded entity name resolution improves 
the performance by 3.1 in F-measure. This 
suggests that the cascaded entity name resolution is 
very useful due to the fact that about 16% of entity 
names have cascaded constructions. 
? The abbreviation resolution improves the 
performance by 2.1 in F-measure. 
? The small closed dictionary improves the 
performance by 1.5 in F-measure. In the 
meanwhile, the large open dictionary improves the 
performance by 1.2 in F-measure largely due to the 
performance improvement for the protein class. It 
is interesting that the small closed dictionary 
contributes more than the large open dictionary 
does. This may be due to the high ambiguity in the 
open dictionary and that the open dictionary only 
contains protein and gene names. 
 
Table 1: Impact of Deep Knowledge Resources 
Performance F 
Baseline 60.3 
98
+In-domain POS +3.8 
+Name Alias Feature +0.9 
+Cascaded Entity Name Res. +3.1 
+Abbreviation Resolution +2.1 
+Small Closed Dictionary +1.5 
+Large Open Dictionary +1.2 
+All Deep Knowledge Resources +12.2 
Table 2: Final Detailed Performance: full correct 
answer  
(# of correct 
answers) 
P R F 
Protein (4015) 69.01 79.24 73.77 
DNA (772) 66.84 73.11 69.83 
RNA (75) 64.66 63.56 64.10 
Cell Line (329) 53.85 65.80 59.23 
Cell Type (1391) 78.06 72.41 75.13 
Overall (6582) 69.42 75.99 72.55 
Table 3: Final Detailed Performance: correct left 
boundary with correct class information  
(# of correct 
answers) 
P R F 
Protein (4239) 72.86 83.66 77.89 
DNA (798) 69.09 75.57 72.18 
RNA (76) 65.52 64.41 64.96 
Cell Line (346) 56.63 69.20 62.29 
Cell Type (1418) 79.57 73.82 76.59 
Overall (6877) 72.53 79.39 75.80 
Table 4: Final Detailed Performance: correct right 
boundary with correct class information  
(# of correct 
answers) 
P R F 
Protein (4285) 73.65 84.57 78.73 
DNA (854) 73.94 80.87 77.25 
RNA (83) 71.55 70.34 70.94 
Cell Line (383) 62.68 76.60 68.95 
Cell Type (1532) 85.97 79.75 82.74 
Overall (7137) 75.27 82.39 78.67 
4. Conclusion 
In the paper, we have explored various deep 
knowledge resources such as the name alias 
phenomenon, the cascaded entity name 
phenomenon, the use of both a closed dictionary 
from the training corpus and an open dictionary 
from the database term list SwissProt and the alias 
list LocusLink, the abbreviation resolution and in-
domain POS using the GENIA corpus.  
In the near future, we will further improve the 
performance by investigating more on conjunction 
and disjunction construction and the combination 
of coreference resolution.  
Acknowledgement 
We thank ZHANG Zhuo for providing the 
database entity name list SwissProt and the alias 
list LocusLink. 
References 
Chen and Goodman. 1996. An Empirical Study of 
Smoothing Technniques for Language 
Modeling. In Proceedings of the 34th Annual 
Meeting of the Association of Computational 
Linguistics (ACL?1996). pp310-318. Santa Cruz, 
California, USA. 
Ohta T., Tateisi Y., Kim J., Mima H., and Tsujii J.  
2002.  The GENIA corpus: An annotated 
research abstract corpus in molecular biology 
domain.  In Proc. of HLT 2002. 
Platt J. 1999. Probabilistic Outputs for Support 
Vector Machines and comparisions to 
regularized Likelihood Methods. MIT Press. 
Schwartz A.S. and Hearst M.A.  2003.  A Simple 
Algorithm for Identifying Abbreviation 
Definitions in Biomedical Text.  In Proc. of the 
Pacific Symposium on Biocomputing (PSB 
2003) Kauai. 
Shen Dan, Zhang Jie, Zhou GuoDong, Su Jian and 
Tan Chew Lim, Effective Adaptation of a 
Hidden Markov Model-based Named Entity 
Recognizer for Biomedical Domain, 
Proceedings of ACL?2003 Workshop on Natural 
Language Processing in Biomedicine, Sapporo, 
Japan, 11 July 2003. pp49-56. 
Vapnik V. 1995. The Nature of Statistical 
Learning Theory. NY, USA: Springer-Verlag. 
Viterbi A.J.  1967.  Error bounds for convolutional 
codes and an asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, 260-269. 
Zhou G.D. and Su J.  2002.  Named Entity 
Recognition using an HMM-based Chunk 
Tagger.  In Proc. of the 40th Annual Meeting of 
the Association for Computational Linguistics 
(ACL), 473-480. 
99
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 378 ? 389, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Discovering Relations Between Named Entities  
from a Large Raw Corpus Using Tree  
Similarity-Based Clustering 
Min Zhang1, Jian Su1, Danmei Wang1,2, Guodong Zhou1, and Chew Lim Tan2 
1 Institute for Infocomm Research, 
21 Heng Mui Keng Terrace, Singapore 119613 
{mzhang, sujian, stuwang, zhougd}@i2r.a-star.edu.sg 
2 Department of Computer Science,  
National University of Singapore, 
Singapore, 117543 
tancl@comp.nus.edu.sg 
Abstract. We propose a tree-similarity-based unsupervised learning method to 
extract relations between Named Entities from a large raw corpus. Our method 
regards relation extraction as a clustering problem on shallow parse trees. First, 
we modify previous tree kernels on relation extraction to estimate the similarity 
between parse trees more efficiently. Then, the similarity between parse trees is 
used in a hierarchical clustering algorithm to group entity pairs into different 
clusters. Finally, each cluster is labeled by an indicative word and unreliable 
clusters are pruned out. Evaluation on the New York Times (1995) corpus 
shows that our method outperforms the only previous work by 5 in F-measure. 
It also shows that our method performs well on both high-frequent and less-
frequent entity pairs. To the best of our knowledge, this is the first work to use a 
tree similarity metric in relation clustering. 
1   Introduction  
The relation extraction task identifies various semantic relations such as location, 
affiliation, revival and so on between entities from text. For example, the sentence 
?George Bush is the president of the United States.? conveys the semantic relation 
?President?, between the entities ?George Bush? (PERSON) and ?the United States? 
(GPE1). The task of relation extraction was first introduced as part of the Template 
Element task in MUC6 and formulated as the Template Relation task in MUC7 [1]. 
Most work at MUC [1] was rule-based, which tried to use syntactic and semantic 
patterns to capture the corresponding relations by means of manually written linguis-
tic rules. The major drawback of this method is the poor adaptability and the poor 
robustness in handling large-scale or new domain data due to two reasons. First, rules 
have to be rewritten for different tasks or when porting to different domains. Second, 
generating rules manually is quite labor- and time-consuming. 
                                                          
1
 GPE is an acronym introduced by the ACE (2004) program to represent a Geo-Political Entity 
--- an entity with land and a government. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 379 
Since then, various supervised learning approaches [2,3,4,5] have been explored ex-
tensively in relation extraction. These approaches automatically learn relation patterns 
or models from a large annotated corpus. To decrease the corpus annotation require-
ment, some researchers turned to weakly supervised learning approaches [6,7], which 
rely on a small set of initial seeds instead of a large annotated corpus. However, there is 
no systematic way in selecting initial seeds and deciding an ?optimal? number of them. 
Alternatively, Hasegawa et al [8] proposed a cosine similarity-based unsupervised 
learning approach for extracting relations from a large raw corpus. The context words 
in between the same entity pairs in different sentences are used to form word vectors, 
which are then clustered according to the cosine similarity. This approach does not 
rely on any annotated corpus and works effectively on high-frequent entity pairs [8]. 
However, there are two problems in this approach: 
? The assumption that the same entity pairs in different sentences have the same 
relation. 
? The cosine similarity measure between the flat feature vectors, which only con-
sider the words between entities. 
In this paper, we propose a tree similarity-based unsupervised learning approach 
for relation extraction. In order to resolve the above two problems in Hasegawa et al 
[8], we assume that the same entity pairs in different sentences can have different 
relation types. Moreover, rather than the cosine similarity measure, a similarity func-
tion over parse trees is proposed to capture much larger feature spaces instead of the 
simple word features. 
The rest of the paper is organized as follows. In Section 2, we discuss the proposed 
tree-similarity-based clustering algorithm. Section 3 shows the experimental result. 
Section 4 compares our work with the previous work. We conclude our work with a 
summary and an outline of the future direction in Section 5. 
2   Tree Similarity-Based Unsupervised Learning 
We use the shallow parse tree as the representation of relation instances, and regard 
relation extraction as a clustering problem on shallow parse trees. Our method con-
sists of three steps: 
1) Calculating the similarity between two parse trees using a tree similarity func-
tion; 
2) Clustering relation instances based on the similarities using a hierarchical 
clustering algorithm; 
3) Labeling each cluster using indicative words as its relation type, and pruning 
out unreliable clusters.  
In this section, we introduce the parse tree representation for a relation instance, 
define the tree similarity function, and describe the clustering algorithm. 
2.1   Parse Tree Representation for Relation Instance 
A parse tree T is a set of node {p1?pn}, which are connected hierarchically. Here, a 
node pi includes a set of features { f1,?, f4} as follows: 
380 M. Zhang et al 
? Head Word ( 1f ): for a leaf (or terminal) node, it is the word itself of the leaf 
node; for a non-terminal node, it is a ?Head Word? propagated from a leaf 
node. This feature defines the main meaning of the phrase or the sub-tree rooted 
by the current node. 
? Node Tag ( 2f ): for a leaf node, it is the part-of-speech of this node; for a non-
terminal node, it is a phrase name, such as Noun Phrase (NP), Verb Phrase (VP). 
This feature defines the linguistic category of this node. 
? Entity Type ( 3f )2:it indicates the entity type which can be PER, COM or GPE 
if the current node refers to a Named Entity.  
? Relation Order ( 4f ): it is used to differentiate asymmetric relations, e.g., ?A 
belongs to B? or ?B belongs to A?. 
These features are widely-adopted in Relation Extraction task.  In the parse tree repre-
sentation, we denote by .
i jfp  the j
th
 feature of node ip , by [ ]ip j   the j
th
 child of 
node ip , and by [ ]ip C  the set of all children of node ip , i.e., [ ] [ ]i ip j p? C .   
2.2   Tree Similarity Function  
Inspired by the special property of kernel-based methods3, we extend the tree kernels 
in Zelenko et al [3] to a novel tree similarity measure function, and apply the above 
tree similarity function to unsupervised learning for relation extraction. Mostly, in 
previous work, kernels are used in supervised learning algorithms such as SVM, Per-
ceptron and PCA (Collins and Duffy, 2001). In our approach, the hierarchical cluster-
ing algorithm is adopted, this allows us to explore more robust and powerful similar-
ity functions, other than a proper kernel function4.  
A similarity function returns a normalized, symmetric similarity score in the range 
[0, 1]. Especially, our tree similarity function 1 2( , )K T T over two trees 1T  and 2T , with 
the root nodes 1r  and 2r , is defined as follows: 
1 2 1 2 1 2 1 2
( , ) ( , ) * ( , ) ( [ ], [ ]){ }
C
K T T m s Kr r r r r r= + c c                                   (1) 
where,  
                                                          
2
  For the features of ?Entity Type?, please refer to the literature ACE [22] for details. 
3
  As an alternative to the feature-based method [5], the advantage of kernels [9] is that they can 
replace any dot product between input points in a high dimensional feature space. Compared 
with the feature-based method, the kernel method displays several unique characteristics, 
such as implicitly mapping the feature space from low-dimension to high-dimension, and ef-
fectively modeling structure data. A few kernels over structured data have been proposed in 
NLP study [10-16]. Zelenko et al [3] and Culotta et al [4] explored tree kernels with SVM 
[9] for relation extraction. We study the tree kernels from similarity measure viewpoints. 
4
  A function is a kernel function if and only if the function is symmetric and positive semi-
definite [3, 9].  
 Discovering Relations Between Named Entities from a Large Raw Corpus 381 
? , )( i jm p p is a matching function over the features of two tree nodes ip  and jp . 
In this paper, only the node tag feature ( 2f ) is considered:  
2 2  
, )  1      if . .(
 0     otherwise
 ji
i j
f fp p
m p p
?? =???
=                                                (2) 
The binary function (1) means that two nodes are matched only if they share 
the same Node Tag. 
? 1 2( , )p ps  is a similarity function between two nodes ip  and jp : 
1 1
3 3
1 1
  
 &   
if 
, else if
other features match
no match
 
 
       
. .
1         
. .
( ) 0.5       . .
0.25    
0    
i j
i j
jii j
p f p f
p f p f
p p f fp ps
? =??
=??
????
= =?????
                                 (3) 
where the values of the weights are assigned empirically according to the discrimina-
tive ability of the feature types. Function (3) measures the similarity between two 
nodes according to the weights of matched features. 
? CK  is the similarity function over the two children node sequences 1[ ]p c  
and 2[ ]p c :  
 
1 2 1 2
, , ( ) ( )
( [ ], [ ]) ( [ ], [ ])argmax { }
C l l
K p p K p p
=
=
a b a b
c c a b                              (4)  
  
1 2 21
( )
1
( [ ], [ ]) ( [ ], [ ])
l
i
K p p K p p
=
= ? i i
a
a ba b                                             (5) 
where a and b are two index sequences, i.e., a is a sequence  1 10 ... [ ]ma a p< < ? | |C  and 
l(a) is the length of sequence a, and likewise for b. The node set 
1 1 1
[ ] { [ ], ..., [ ]}p p p= 1 ma a a  is the subset of 1[ ]p c , 1 1[ ] [ ]p p?a c , 1[ ]p ai is the i
th
 
node of
1
[ ]p a , and likewise for 2p . 
We define 1 2( , )K T T in terms of the similarity function 1 2( , )r rs  between the par-
ent nodes and the similarity function CK  over the two children node sequences 1[ ]r c  
and 2[ ]r c . Formula (5) defines the similarity between two node sequences by sum-
ming up the similarity of each corresponding node pair. CK  in Formula (4) searches 
out such two children node subsequences 1[ ]p a and 2[ ]p b , which has the maximum 
node sequence similarity among all the possible combining pairs of node subse-
quences. Given the similarity scores of all children node pairs, Formula (4) can be 
382 M. Zhang et al 
easily resolved by the dynamic programming (DP) algorithm5. By traversing the two 
trees from top to down and applying the DP algorithm layer by layer, the parse tree 
similarity 1 2( , )K T T defined by Formula (1) is obtained. Due to the DP algorithm, the 
defined tree similarity function is computable in O(mn), where m and n are the num-
ber of nodes in the two trees, respectively. The matching function , )( i jm p p in For-
mula (2) can narrow down the search space during similarity calculation, since the 
sub-trees with unmatched root nodes are unnecessary to be further explored.  
 
Fig. 1.  Sub-structure with maximum similarity 
From the above discussion, we can see that our defined tree similarity function is 
trying to detect the two trees? maximum isomorphic sub-structures. The similarity 
score between the maximum isomorphic sub-structures is returned as the value of the 
similarity function. Fig. 1 illustrates the sub-structures with the maximum similarity 
between two trees. Among the all matched sub-structures, only the sub-structures 
circled by the dashed lines are the isomorphic sub-structures with the maximum simi-
larity. The similarity score between the sub-structures is obtained by summing up the 
similarity score between the corresponding matched nodes.  
Finally, since the size of the input parse tree is not constant, the similarity score is 
normalized as follows: 
1 2
1 1 2 2
1 2
( , )
( , )* ( , )
? ( , ) K T T
K T T K T T
K T T =                (6) 
The value of 1 2? ( , )K T T ranges from 0 to 1. In particular, 1 2? ( , )K T T =1 if and only 
if 1 2T T= . For example, given two parse trees A and B, and A is a subtree of B, then 
under Formula (1), K(A, B) = K(A, A). However, after the normalization through 
                                                          
5
  A well-known application of Dynamic Programming is to compute the edit distance between 
two character strings. Let us regard a node as a character and a node sequence as a character 
string. Then given the similarity  score between nodes, Formula (4) can be resolved using DP 
algorithm in the same way as that of strings. Due to space limitation, the implementation 
deatils are not discussed here. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 383 
Equation (6), we can get ? ?
  ( , ) ( , ) 1K A B K A A< = . In this way, we can differentiate such 
two cases.  
According to the Formula (1) to (5), the similarity function 1 2( , )K T T  over the two 
trees in Fig. 1 is computed as follows: 
1 2 ([NP, VP], [NP, VP])
([bought, NP], [sold, NP, yesterday])
1 bought sold (NP, NP)
= 1+0.25+0.25+ ([a, red, car]
( , ) (S,S) *{ (S,S) }
0.25 (NP, NP) (VP, VP)
0.25 0.25 (Paul, Smith) 0.25
  
( , )
c
c
c
K T T m s K
K K
K
K
K K
K
= +
= +
= + +
= + + +
+
+
, [the, flat])
1.5 a the car flat( , ) ( , )
2
K K= + +
=
 
The above similarity score is more than one. This is because we did not normalize 
the score using Formula (6). 
2.3   Tree Similarity Based Unsupervised Learning 
Our method consists of five steps: 
1) Named Entity (NE) tagging and sentence parsing: Detailed and accurate NE 
types provide more effective information for relation discovery. Here we use Sekine?s 
NE tagger [20], where 150 hierarchical types and subtypes of Named Entities are 
defined [21]. This NE tagger has also been adopted by Hasegawa et al [8]. Besides, 
Collin?s parser [18] is adopted to generate shallow parse trees.  
2) Similarity calculation: The similarity between two relation instances is defined 
between two parse trees. However, the state-of-the-art of parser is always error-prone. 
Therefore, we only use the minimum span parse tree including the NE pairs when 
calculating the similarity function [4]. Please note that the two entities may not be the 
leftmost or rightmost node in the sub-tree. 
3) NE pairs clustering: Clustering of NE pairs is based on the similarity score gener-
ated by the tree similarity function. Rather than k-means [17], we used a bottom-up 
hierarchical clustering method so that there is no need to determine the number of 
clusters in advance. This means that we are not restricted to the limited types of rela-
tions defined in MUC [1] or ACE [22]. Therefore, more substantial existing relations 
can be discovered. We adopt the group-average clustering algorithm [17] since it 
produces the best performance compared with the complete-link and single-link algo-
rithms in our study.  
 
4) Cluster labeling: In our study, we label each cluster by the most frequent ?Head 
Word? in this cluster. As indicated in subsection 2.1, the ?Head Word? of root node 
defines the main meaning of a parse tree. This way, the ?Head Word? of the root 
384 M. Zhang et al 
node of the minimum span tree naturally characterizes the relation between this NE 
pair in this tree. Thus, we simply count the frequency of the ?Head Word? of the root 
node in the cluster, and then chose the most frequent ?Head Word? as the relation 
type of the cluster.  
 
5) Cluster pruning: Unreliable clusters may be generated due to various reasons 
such as divergent relation type distributions and the fact that most of the entity pairs 
inside this cluster are totally unrelated. Therefore, pruning is necessary and done in 
our approach using two criteria. Firstly, if the most frequent ?Head Word? occurs 
less than a predefined percentage in this cluster, which means that the relation type 
defined by this ?Head Word? is not significant statistically, the cluster is pruned out. 
Secondly, we prune out the clusters whose NE pair number is below a predefined 
threshold because such clusters may not be representative enough for this relation.  
3   Experiments 
3.1   Experimental Setting 
To verify our proposed method and establish proper comparison with Hasegawa et al 
[8], we use the same corpus ?The New York Times (1995)?, and evaluate our work on 
the same two kinds of NE pairs: COMPANY-COMPANY (COM-COM) and 
PERSON-GPE (PER-GPE) as Hasegawa et al in [8]. First, we iterate over all pairs of 
Named Entities occurring in the same sentence to generate potential relation in-
stances. Then, according to the co-occurrence frequency of NE pairs, all the relation 
instances are grouped into three categories: 
1) High frequent instances with the co-occurrence frequency not less than 30. In 
this category, only the relation instances, which satisfy the all criteria of Ha-
segawa et al [8]6, are kept for final experiment. By doing so, this category 
data is the same as the entire experimental set used by Hasegawa et al [8]. 
2) Intermediate frequent instances with the co-occurrence frequency between 5 
and 30. In this category, only two distinct NE pairs are randomly picked at 
each frequency for final evaluation due to the large number of such NE pairs. 
3) Less frequent instances with the co-occurrence frequency not more than 5. In 
this category, twenty distinct NE pairs are randomly picked at each fre-
quency for final evaluation due to the similar reason as 2). 
Table 1 reports the statistics of the entire evaluation corpus7 which is manually 
tagged. Table 2 reports the percentage of the NE pairs which carry more than one 
relation types when occurring at different relation instances. The numbers inside pa-
rentheses in Table 1 and Table 2 correspond to the statistical values of the NE pair 
?PER-GPE?, while the numbers outside parentheses are related to the NE pair ?COM-
COM?. Table 2 shows that at least 9.88% of distinct NE pairs have more than one 
                                                          
6
 To discover reliable relations, Hasegawa et al [8] sets five conditions to generate relation 
instance set. NE pair co-occurrence more than 30 times is one of the five conditions. 
7
  Due to the parsing errors and NE tagging errors, the actual number of relation instances is 
less than the theory number that we should pick up. 
 Discovering Relations Between Named Entities from a Large Raw Corpus 385 
relation types in the test corpus. Thus it is reasonable and necessary to assume that 
each occurrence of NE pairs forms one individual relation instance. 
Table 1. Statistics on the manually annotated evaluation data 
Category by 
frequency 
# of instances # of distinct NE pairs    # of relation  
types 
High 8931 (13205) 65 (177) 10 (38) 
Intermediate 672  (783) 38 (41) 6 (7) 
Less 276  (215) 76 (81) 5 (8) 
Table 2. % of distinct NE pairs with more than one relation types on the evaluation data 
Category by frequency % of NE pairs have more than one relations 
    High  15.4   (12.99) 
    Intermediate 28.9   (24.4) 
    Less 11.8   (9.88) 
3.2   Evaluation Measures 
All the experiments are carried out against the manually annotated evaluation corpus. 
We adopt the same criteria as Hasegawa et al [8] to evaluate the performance of our 
method. Grouping and labeling are evaluated separately. For grouping evaluation, all 
the single NE pair clusters are labeled as non-relation while all the other clusters are 
labeled as the most frequent relation type counted in this cluster. For each individual 
relation instance, if the manually assigned relation type is the same as its cluster label, 
the grouping of this relation instance is counted as correct, otherwise, are counted as 
incorrect. Recall (R), Precision (P) and F-measure (F) are adopted as the main per-
formance measure for grouping [8]. For labeling evaluation, a cluster is labeled cor-
rectly only if the labeling relation type, represented by most frequent ?Head Word? 
of the root node of the minimal-span subtree, is the same as the cluster label gotten in 
the grouping process. 
3.3   Experimental Results 
Like other applications using clustering algorithms, the performance of the proposed 
method also depends on the threshold of the clustering similarity. Here this threshold 
is used to truncate the hierarchical tree, so that the different clusters are generated. 
When the threshold is set to 1, then each individual relation instance forms one unique 
group; when the threshold is set to 0, then the all relation instance form one big group. 
Table 3 reports the evaluation results of grouping, where the best F-measures and the 
corresponding similarity thresholds are listed. We can see that our method not only 
achieves good performance on the high-frequent data, but also performs well on the 
386 M. Zhang et al 
intermediate and less-frequent data. The higher frequency, the higher performance. 
Since the best thresholds of the two NE cases are the almost same, we just fix the 
universal threshold as the one used in ?PER-GPE? case in each category.  
Table 3.  Performance evaluation of Grouping phase, the numbers inside parentheses corre-
spond to the evaluation score of ?PER-GPE? while the numbers outside parentheses are related 
to ?COM-COM?. 
Performance Category by fre-
quency 
  F P (%) R (%) 
Threshold 
High 80 (87) 82 (90) 78 (84) 0.28 (0.29) 
Intermediate 74 (76) 87 (84) 64 (69)  0.32 (0.30) 
Less   62 (65) 75 (77) 53 (56)  0.36 (0.35) 
Table 4. Best performance comparison in the high-frequent data (F) 
 Our approach  Hasegawa et al [8] 
PER-GPE 87 82 
COM-COM 80 77 
Table 4 compares the performances of the proposed method and Hasegawa et al 
[8], where the best F-measures on the same high-frequent data are reported. Table 4 
shows that our method outperforms the previous approach by 5 and 3 F-measures in 
clustering NE pairs of ?PER-GPE? and ?COM-COM?, respectively.  
An interesting phenomenon is that the best threshold is set to be just above 0 for 
the cosine similarity in Hasegawa et al [8]. This means that each word feature vector 
of each combination of NE pairs in the same cluster shares at least one word in com-
mon --- and most of these common words were pertinent to the relations [8]. This also 
prevents them from working well on less-frequent data [8]. In contrast, for the simi-
larity function in our approach, the best threshold is much greater than 0. The differ-
ence between the two thresholds implies that the similarity function over the parse 
trees can capture more common structured features than the word feature vectors can. 
This is also the reason why our method is effective on both high and less- 
frequent data. 
It is not surprising that we do have that a few identical NE pairs, occurring in dif-
ferent relation instances, are grouped into different relation sets. For example, the NE 
pairs ?General Electric Co. and NBC?, in one sentence ?General Electric Co., which 
bought NBC in 1986, will announce a new marketing plan.?, is grouped into the rela-
tion set ?M&A?, but in another sentence ?Prime Star Partners and General Electric 
Co., parent of NBC, has signed up 430,000 subscribers.?, is grouped into another 
relation set ?parent?. Among all the NE pairs that carry more than one relation types, 
41.8% of them are grouped correctly using our tree similarity function.  
 Discovering Relations Between Named Entities from a Large Raw Corpus 387 
The performance of grouping is the upper bound of the performances of labeling 
and pruning. In the final, there are 146 PER-GPE clusters and 95 COM-COM clusters 
are generated after grouping. Out of which, only 57 PER-GPE clusters and 42 COM-
COM clusters are labeled correctly before pruning. This is because that a large por-
tion of the non-relation clusters are labeled as one kind of true relations. After prun-
ing, 117 PER-GPE clusters and 84 COM-COM clusters are labeled correctly. This is 
because lots of the non-relation clusters are labeled correctly by the pruning process, 
so we can say that pruning is a non-relation labeling process, which greatly improves 
the performance of labeling.  
The experimental results discussed above suggest that our proposed method is an 
effective solution for discovering relation from a large raw corpus. 
4   Discussions 
It would be interesting to review and summarize how the proposed method deals with 
the relation extraction issue differently from other related works. Table 5 in the next 
page summarizes the differences between our method and Hasegawa et al [8]. 
Table 5. The differences between our method and Hasegawa et al [8] 
 Our approach  Hasegawa et al [8] 
Similarity  
Measure 
tree similarity over parse 
tree structures 
cosine similarity between the 
context word feature vectors 
Assumption No Yes (The same entity pairs in 
different sentences have the 
same relation) 
Labeling the most frequent ?Head 
Word? of the root node of 
sub-tree 
the most frequent context 
word 
Pruning Yes (We present two prun-
ing criterion) 
No 
Data Frequency effective on both high and 
less-frequent data 
effective only on high-
frequent data 
In addition, since our tree similarity function has benefited from the relation tree 
kernels of Zelenko et al [3], let us compare our similarity measure function with their 
relation kernel function [3] from the viewpoint of computational efficiency. Zelenko 
et al [3] defined the first parse tree kernels for relation extraction, and then this  
relation tree kernels were extended to dependency tree kernels by Culotta et al [4].  
Their tree kernels sum up the similarity scores among all possible subsequences of 
children nodes with matching parents, and give a penalty to longer sequences. Their 
388 M. Zhang et al 
tree kernels are closely related to the convolution kernels [12]. But, by doing so, lots 
of sub-trees will be considered again and again. An extreme case occurs when two 
tree structures are identical. In that situation all the sub-trees will be considered 
exhaustedly, even if the sub-tree is a part of other bigger sub-trees. We use the maxi-
mum score in Formula (4) instead of the summation in our approach. With our ap-
proach, the entire tree is only considered once. The replacement of summation with 
maximization reduces the computational time greatly. 
5   Conclusions and Future Directions 
We modified the relation tree kernels [3] to be a tree similarity measure function by 
replacing the summation over all possible subsequences of children nodes with 
maximization, and used it in clustering for relation extraction. The experimental result 
showed much improvement over the previous best result [8] on the same test corpus. 
It also showed that our method is high effective on both high-frequent and less-
frequent data. Our work demonstrated the effectiveness of combining the tree similar-
ity measure with unsupervised learning for relation extraction. 
Although our method shows good performance, there are still other aspects of the 
proposed method worth discussing here. Without additional knowledge, relation de-
tecting and relation labeling are still not easy to be resolved, especially in less-
frequent data. We expect that using additional easily-acquired knowledge can im-
prove the performance of the proposed method. For example, we can introduce the 
WordNet [19] thesaurus information into Formula (3) to obtain more accurate node 
similarities and resolve data sparse problem. We can also use the same resource to 
improve the labeling scheme and find more abstract relation types like the definitions 
used in ACE program [22].  
References  
1. MUC. 1987-1998. The nist MUC website:  http://www.itl.nist.gov/iaui/894.02/related_ 
projects/muc/ 
2. Miller, S., Fox, H., Ramshaw, L. and Weischedel, R. 2000. A novel use of statistical pars-
ing to extract information from text. Proceedings of NAACL-00 
3. Zelenko, D., Aone, C. and Richardella, A. 2003. Kernel Methods for Relation Extraction. 
Journal of Machine Learning Research. 2003(2):1083-1106 
4. Culotta, A. and Sorensen, J. 2004. Dependency Tree Kernel for Relation Extraction. Pro-
ceeding of ACL-04 
5. Kambhatla, N. 2004. Combining Lexical, Syntactic, and Semantic Features with Maxi-
mum Entropy Models for Extracting Relations. Proceeding of ACL-04, Poster paper. 
6. Agichtein, E. and Gravano, L. 2000. Snow-ball: Extracting Relations from Large Plain-
text Collections. Proceedings of the Fifth ACM International Conference on Digital Li-
braries. 
7. Stevenson, M. 2004. An Unsupervised WordNet-based Algorithm for Relation Extraction. 
Proceedings of the 4th LREC workshop "Beyond Named Entity: Semantic Labeling for 
NLP tasks" 
 Discovering Relations Between Named Entities from a Large Raw Corpus 389 
8. Hasegawa, T., Sekine, S. and Grishman, R. 2004. Discovering Relations among Named 
Entities from Large Corpora. Proceeding of ACL-04 
9. Vapnik, V. 1998. Statistical Learning Theory. John Wiley 
10. Collins, M. and Duffy, N. 2001. Convolution Kernels for Natural Language. Proceeding of 
NIPS-01 
11. Collins, M. and Duffy, N. 2002. New Ranking Algorithm for Parsing and Tagging: Kernel 
over Discrete Structure, and the Voted Perceptron. Proceeding of ACL-02. 
12. Haussler, D. 1999. Convolution Kernels on Discrete Structures. Technical Report UCS-
CRL-99-10, University of California 
13. Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N. and Watkins, C. 2002. Text clas-
sification using string kernel. Journal of Machine Learning Research, 2002(2):419-444 
14. Suzuki, J., Hirao, T., Sasaki Y. and Maeda, E. 2003. Hierarchical Directed Acyclic Graph 
Kernel: Methods for Structured Natural Language Data. Proceedings of ACL-03 
15. Suzuki, J., Isozaki, H. and Maeda, E. 2003. Convolution Kernels with Feature Selection 
for Natural Language Processing Tasks. Proceedings of ACL-04 
16. Moschitti, A. 2004. A study on Convolution Kernels for Shallow Semantic Parsing. Pro-
ceedings of ACL-04 
17. Manning, C. and Schutze, H. 1999. Foundations of Statistical Natural Language Process-
ing. The MIT Press: 500-527 
18. Collins, M. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. 
Thesis. University of Pennsylvania 
19. Fellbaum, C. 1998. WordNet: An Electronic Lexical Database and some of its Applica-
tions. Cambridge, MA: MIT Press. 
20. Sekine, S. 2001. OAK System (English Sentence Analysis). Http://nlp.cs.nyu.edu/oak 
21. Sekine, S., Sudo, K. and Nobata, C. 2002. Extended named entity hierarchy. Proceedings 
of LREC-02 
22. ACE. 2004. The Automatic Content Extraction (ACE) Projects. http://www.ldc.upenn.edu/ 
Projects/ACE/ 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 163-165, Lisbon, Portugal, 2000. 
Hybrid Text Chunking 
GuoDong Zhou and J ian  Su  and TongGuan Tey  
Kent Ridge Digital Labs 
21 Heng Mui Keng Terrace 
Singapore 119613 
{zhougd, sujian, tongguan}@krdl, org.sg 
Abst rac t  
This paper proposes an error-driven HMM- 
based text chunk tagger with context-dependent 
lexicon. Compared with standard HMM-based 
tagger, this tagger incorporates more contextual 
information into a lexical entry. Moreover, an 
error-driven learning approach is adopted to de- 
crease the memory requirement by keeping only 
positive lexical entries and makes it possible 
to further incorporate more context-dependent 
lexical entries. Finally, memory-based learning 
is adopted to further improve the performance 
of the chunk tagger. 
1 In t roduct ion  
The idea of using statistics for chunking goes 
back to Church(1988), who used corpus frequen- 
cies to determine the boundaries of simple non- 
recursive noun phrases. Skut and Brants(1998) 
modified Church's approach in a way permitting 
efficient and reliable recognition of structures of 
limited depth and encoded the structure in such 
a way that it can be recognised by a Viterbi 
tagger. Our approach follows Skut and Brants' 
way by employing HMM-based tagging method 
to model the chunking process. 
2 HMM-based  Chunk  Tagger  w i th  
Context -dependent  Lex icon  
Given a token sequence G~ = glg2""gn , 
the goal is to find an optimal tag sequence 
T~ = tit2.. "tn which maximizes log P(T~IG~): 
. P (T~,G?)  
log P(T~IG?) = log P(T~) +log p(T~)P(G?) 
The second item in the above equation is the 
mutual information between the tag sequence 
T~ and the given token sequence G~. By as- 
suming that the mutual information between 
G~ and T~ is equal to the summation off mutual 
information between G~ and the individual tag 
ti (l<i_<n): 
l P(T~, G~) n _ P(ti, G?) 
o g p - ~ )  = E log P'(t~-P-~) 
i=1  
n 
MI(T~, G?) = ~ MI(ti, G~), 
i=1  
we have: 
n P(ti, G~) 
log P(T~IG~)  = log P(T~)+~ log P(~i)P-~) 
i=1  
n n 
= log P (T~) -  ~ log P(ti) + ~ log P(tilG?) 
i----1 i=1  
The first item of above equation can be solved 
by chain rules. Normally, each tag is assumed 
to be probabilistic dependent on the N-1 previ- 
ous tags. Here, backoff bigram(N=2) model is 
used. The second item is the summation of log 
probabilities of all the tags. Both the first item 
and second item constitute the language model 
component while the third item constitutes the 
lexicon component. Ideally the third item can 
be estimated by the forward-backward algo- 
rithm(Rabiner 1989) recursively for the first- 
order(Rabiner 1989) or second-order HMMs. 
However, several approximations on it will be 
attempted later in this paper instead. The 
stochastic optimal tag sequence can be found 
by maximizing the above equation over all the 
possible tag sequences using the Viterbi algo- 
rithm. 
The main difference between our tagger and 
the standard taggers lies in our tagger has a 
context-dependent lexicon while others use a 
context-independent lexicon. 
163 
For chunk tagger, we have gl = piwi where 
W~ = wlw2""Wn is the word sequence and 
P~ = PlP2""Pn is the part-of-speech(POS) 
sequence. Here, we use structural tags to 
representing chunking(bracketing and labeling) 
structure. The basic idea of representing 
the structural tags is similar to Skut and 
Brants(1998) and the structural tag consists of 
three parts: 
1) Structural relation. The basic idea is sim- 
ple: structures of limited depth are encoded 
using a finite number of flags. Given a se- 
quence of input tokens(here, the word and POS 
pairs), we consider the structural relation be- 
tween the previous input token and the current 
one. For the recognition of chunks, it is suffi- 
cient to distinguish the following four different 
structural relations which uniquely identify the 
sub-structures of depth l(Skut and Brants used 
seven different structural relations to identify 
the sub-structures of depth 2). 
? 00: the current input token and the previ- 
ous one have the same parent 
? 90: one ancestor of the current input token 
and the previous input token have the same 
parent 
? 09: the current input token and one an- 
cestor of the previous input token have the 
same parent 
? 99 one ancestor of the current input token 
and one ancestor of the previous input to- 
ken have the same parent 
Compared with the B-Chunk and I-Chunk 
used in Ramshaw and Marcus(1995)~, structural 
relations 99 and 90 correspond to B-Chunk 
which represents the first word of the chunk, 
and structural relations 00 and 09 correspond 
to I-Chunk which represents each other in the 
chunk while 90 also means the beginning of the 
sentence and 09 means the end of the sentence. 
2)Phrase category. This is used to identify 
the phrase categories of input tokens. 
3)Part-of-speech. Because of the limited 
number of structural relations and phrase cate- 
gories, the POS is added into the structural tag 
to represent more accurate models. 
Principally, the current chunk is dependent 
on all the context words and their POSs. How- 
ever, in order to decrease memory require- 
ment and computational complexity, our base- 
line HMM-based chunk tagger only considers 
previous POS, current POS and their word to- 
kens whose POSs are of certain kinds, such as 
preposition and determiner etc. The overall 
precision, recall and F~=i rates of our baseline 
tagger on the test data of the shared task are 
89.58%, 89.56% and 89.57%. 
3 Error-driven Learning 
After analysing the chunking results, we find 
many errors are caused by a limited number of 
words. In order to overcome such errors, we 
include such words in the chunk dependence 
context by using error-driven learning. First, 
the above HMM-based chunk tagger is used to 
chunk the training data. Secondly, the chunk 
tags determined by the chunk tagger are com- 
pared with the given chunk tags in the training 
data. For each word, its chunking error number 
is summed. Finally, those words whose chunk- 
ing error numbers are equal to or above a given 
threshold(i.e. 3) are kept. The HMM-based 
chunk tagger is re-trained with those words con- 
sidered in the chunk dependence ontext. 
The overall precision, recall and FZ=i rates 
of our error-driven HMM-based chunk tagger 
on the test data of the shared task are 91.53%, 
92.02% and 91.77 
4 Memory based Learning 
Memory-based learning has been widely used 
in NLP tasks in the last decade. Principally, it 
falls into two paradigms. First paradigm rep- 
resents examples as sets of features and car- 
ries out induction by finding the most simi- 
lar cases. Such works include Daelemans et 
a1.(1996) for POS tagging and Cardie(1993) 
for syntactic and semantic tagging. Second 
paradigm makes use of raw sequential data 
and generalises by reconstructing test examples 
from different pieces of the training data. Such 
works include Bod(1992) for parsing, Argamon 
et a1.(1998) for shallow natural anguage pat- 
terns and Daelemans et a1.(1999) for shallow 
parsing. 
The memory-based method presented here 
follows the second paradigm and makes use of 
raw sequential data. Here, generalization is per- 
formed online at recognition time by comparing 
164 
the new pattern to the ones in the training cor- 
pus. 
Given one of the N most probable chunk se- 
quences extracted by the error-driven HMM- 
based chunk tagger, we can extract a set of 
chunk patterns, each of them with the format: 
XP 1 n n+l r~+l = poroPlrn Pn+l, where is the 
structural relation between Pi and Pi+l. 
As an example, from the bracketed and la- 
beled sentence: 
\[NP He/PRP \] \[VP reckons/VSZ \] 
\[NP the/DT current/ J J  account/NN 
deficit/NN \] \[VP will/MD narrow/VB 
\] \[ PP to /TO\ ]  \[NP only/RB #/# 
1.8/CD billion/CD \] \[PP in/IN \ ] \ [NP 
September/NNP \] \[O ./. \] 
we can extract following chunk patterns: 
NP=NULL 90 PRP 99 VBZ 
VP=PRP 99 VBZ 99 DT 
NP=VBZ 99 DT JJ NN NN 99 MD 
PP=VB 99 TO 99 RB 
NP=TO 99 RB # CD CD 99 IN 
PP=CD 99 IN 99 NNP 
NP=IN 99 NNP 99 . 
O=NNP 99 . 09 NULL 
For every chunk pattern, we estimate its proba- 
bility by using memory-based learning. If the 
chunk pattern exists in the training corpus, 
its probability is computed by the probability 
of such pattern among all the chunk patterns. 
Otherwise, its probability is estimated by the 
multiply of its overlapped sub-patterns. Then 
the probability of each of the N most probable 
chunk sequences i adjusted by multiplying the 
probabilities of its extracted chunk patterns. 
Table 1 shows the performance oferror-driven 
HMM-based chunk tagger with memory-based 
learning. 
5 Conc lus ion  
It is found that the performance with the help of 
error-driven learning is improved by 2.20% and 
integration of memory-based learning further 
improves the performance by 0.35% to 92.12%. 
For future work, the experimentation  large 
scale task will be speculated in the near future. 
Finally, a closer integration of memory-based 
method with HMM-based chunk tagger will also 
be conducted. 
test data 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
precision 
76.17% 
78.25% 
46.67% 
20.00% 
00.00% 
92.19% 
96.09% 
72.36% 
83.56% 
92.77% 
recall 
70.78% 
78.52% 
77.78% 
5O.OO% 
OO.O0% 
92.59% 
96.94% 
83.96% 
79.81% 
92.85% 
all 91.99% 92.25% 
F~=i 
73.37 
78.39 
58.33 
28.57 
00.00 
92.39 
96.51 
77.73 
81.64 
92.81 
92.12 
Table 1: performance of chunking 
References  
S. Argamon, I. Dagan, and Y. Krymolowski. 1998. 
A memory-based approach to learning shallow 
natural language patterns. In COLING/ACL- 
1998, pages 67-73. Montreal, Canada. 
R. Bod. 1992. A computational model of lan- 
guage performance: Data-oriented parsing. In 
COLING-1992, pages 855-859. Nantes, France. 
C. Cardie. 1993. A case-based approach to knowl- 
edge acquisition for domain-specific sentence anal- 
ysis. In Proceeding of the I1th National Con- 
ference on Artificial Intelligence, pages 798-803. 
Menlo Park, CA, USA. AAAI Press. 
K.W. Church. 1988. A stochastic parts program and 
noun phrase parser for unrestricted text. In Pro- 
ceedings of Second Conference on Applied Natu- 
ral Language Processing, pages 136-143. Austin, 
Texas, USA. 
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 
1996. Mbt: A memory-based part-of-speech tag- 
ger generator. In Proceeding of the Fourth Work- 
shop on Large Scale Corpora, pages 14-27. ACL 
SIGDAT. 
W. Daelemans, S. Buchholz, and J. Veenstra. 1999. 
Memory-based shallow parsing. In CoNLL-1999, 
pages 53-60. Bergen, Norway. 
L.R. Rabiner. 1989. A tutorial on hidden markov 
models and selected applications in speech recog- 
nition. In Proceedings of the IEEE, volume 77, 
pages 257-286. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text chunking using transformation-based learn- 
ing. In Proceedings of the Third ACL Work- 
shop on Very Large Corpora. Cambridge, Mas- 
sachusetts, USA. 
W. Skut and T. Brants. 1998. Chunk tagger: sta- 
tistical recognition of noun phrases. In ESSLLI- 
1998 Workshop on Automated Acquisition of Syn- 
tax and Parsing. Saarbruucken, Germany. 
165 
Error-driven HMM-based Chunk Tagger 
with Context-dependent Lexicon 
GuoDong ZHOU 
Kent Ridge Digital Labs 
21 Heng Hui Keng Terrace 
Singapore 119613 
zhougd@krdl.org.sg 
Jian SU 
Kent Ridge Digital Labs 
21 Heng Hui Keng Terrace 
Singapore 119613 
sujian@krdl.org.sg 
Abstract 
This paper proposes a new error-driven HMM- 
based text chunk tagger with context-dependent 
lexicon. Compared with standard HMM-based 
tagger, this tagger uses a new Hidden Markov 
Modelling approach which incorporates more 
contextual information into a lexical entry. 
Moreover, an error-driven learning approach is 
adopted to decrease the memory requirement by 
keeping only positive lexical entries and makes 
it possible to further incorporate more context- 
dependent lexical entries. Experiments how 
that this technique achieves overall precision 
and recall rates of 93.40% and 93.95% for all 
chunk types, 93.60% and 94.64% for noun 
phrases, and 94.64% and 94.75% for verb 
phrases when trained on PENN WSJ TreeBank 
section 00-19 and tested on section 20-24, while 
25-fold validation experiments of PENN WSJ 
TreeBank show overall precision and recall 
rates of 96.40% and 96.47% for all chunk types, 
96.49% and 96.99% for noun phrases, and 
97.13% and 97.36% for verb phrases. 
Introduction 
Text chunking is to divide sentences into non- 
overlapping segments on the basis of fairly 
superficial analysis. Abney(1991) proposed this 
as a useful and relatively tractable precursor to 
full parsing, since it provides a foundation for 
further levels of analysis, while still allowing 
more complex attachment decisions to be 
postponed toa later phase. 
Text chunking typically relies on fairly 
simple and efficient processing algorithms. 
Recently, many researchers have looked at text 
chunking in two different ways: Some 
researchers have applied rule-based methods, 
combining lexical data with finite state or other 
rule constraints, while others have worked on 
inducing statistical models either directly from 
the words and/or from automatically assigned 
part-of-speech classes. On the statistics-based 
approaches, Skut and Brants(1998) proposed a
HMM-based approach to recognise the syntactic 
structures of limited length. Buchholz, Veenstra 
and Daelemans(1999), and Veenstra(1999) 
explored memory-based learning method to fred 
labelled chunks. Ratnaparkhi(1998) used 
maximum entropy to recognise arbitrary chunk 
as part of a tagging task. On the rule-based 
approaches, Bourigaut(1992) used some 
heuristics and a grammar to extract 
"terminology noun phrases" from French text. 
Voutilainen(1993) used similar method to detect 
English noun phrases. Kupiec(1993) applied. 
finite state transducer in his noun phrases 
recogniser for both English and French. 
Ramshaw and Marcus(1995) used 
transformation-based l arning, an error-driven 
learning technique introduced by Eric 
Bn11(1993), to locate chunks in the tagged 
corpus. Grefenstette(1996) applied finite state 
transducers to fred noun phrases and verb 
phrases. 
In this paper, we will focus on statistics- 
based methods. The structure of this paper is as 
follows: In section 1, we will briefly describe 
the new error-driven HMM-based chunk tagger 
with context-dependent lexicon in principle. In 
section 2, a baseline system which only includes 
the current part-of-speech in the lexicon is 
given. In section 3, several extended systems 
with different context-dependent lexicons are 
described. In section 4, an error=driven learning 
method is used to decrease memory requirement 
of the lexicon by keeping only positive lexical 
71 
entries and make it possible to further improve 
the accuracy by merging different context- 
dependent lexicons into one after automatic 
analysis of the chunking errors. Finally, the 
conclusion is given. 
The data used for all our experiments i
extracted from the PENN" WSJ Treebank 
(Marcus et al 1993) by the program provided 
by Sabine Buchholz from Tilbug University. 
We use sections 00-19 as the training data and 
20-24 as test data. Therefore, the performance is 
on large scale task instead of small scale task on 
CoNLL-2000 with the same evaluation 
program. 
For evaluation of our results, we use the 
precision and recall measures. Precision is the 
percentage of predicted chunks that are actually 
correct while the recall is the percentage of 
correct chunks that are actually found. For 
convenient comparisons of only one value, we 
also list the F~= I value(Rijsbergen 1979): 
(/32 + 1). precision, recall 
, with/3 = 1. 
/3 2. precision + recall 
1 HMM-based Chunk Tagger 
The idea of using statistics for chunking goes 
back to Church(1988), who used corpus 
frequencies to determine the boundaries of 
simple non-recursive noun phrases. Skut and 
Brants(1998) modified Church's approach in a 
way permitting efficient and reliable recognition 
of structures of limited depth and encoded the 
structure in such a way that it can be recognised 
by a Viterbi tagger. This makes the process run 
in time linear to the length of the input string. 
Our approach follows Skut and Brants' way 
by employing HMM-based tagging method to 
model the chunking process. 
Given a token sequence G~ = g~g2 ""g, ,  
the goal is to fred a stochastic optimal tag 
sequence Tin = tlt2...t n which maximizes 
log P(T~" I Of ) : 
e(:q",G?) 
log P(Ti n \[ G? ) = log P(Ti n ) + log P(Ti n )" P(G? ) 
The second item in the above equation is the 
mutual information between the tag sequence 
Tin and the given token sequence G~. By 
assuming that the mutual information between 
G~ and T1 ~ is equal to the summation of mutual 
information between G~ and the individual tag 
ti(l_<i_<n ) : 
n log P(TI"' G?) = ~ log P(t,, G~) 
e(Tln ). P(G~) i=1 P(t,). P(G? ) 
or  
n 
n MI(T~ ~ , G~ ) = ~ MI(t, ,  G? ) , 
i= l  
we have: 
log P(T~ n I G~) 
= log P(T1 n ) + ~, log P(ti' G? )_ 
P(t i). P(G?) 
rl n 
= log P(T1 ~ ) - Z log P(t, ) + ~ log P(t, \[ G? ) 
i=1  i=1 
The first item of above equation can be 
solved by using chain rules. Normally, each tag 
is assumed to be probabilistic dependent on the 
N-1 previous tags. Here, backoff bigram(N=2) 
model is used. The second item is the 
summation of log probabilities of all the tags. 
Both the first item and second item correspond 
to the language model component of the tagger 
while the third item corresponds to the lexicon 
component of the tagger. Ideally the third item 
can be estimated by using the forward-backward 
algorithm(Rabiner 1989) recursively for the 
first-order(Rabiner 1989) or second-order 
HMMs(Watson and Chunk 1992). However, 
several approximations on it will be attempted 
later in this paper instead. The stochastic 
optimal tag sequence can be found by 
maxmizing the above equation over all the 
possible tag sequences. This is implemented by
the Viterbi algorithm. 
The main difference between our tagger and 
other standard taggers lies in our tagger has a 
context-dependent lexicon while others use a 
context-independent l xicon. 
For chunk tagger, we haveg 1= piwi where 
W~ n = w~w2---w n is the word-sequence and 
P~ = PiP2 "" P~ is the part-of-speech 
72 
sequence. Here, we use structural tags to 
representing chunking(bracketing and labelling) 
structure. The basic idea of representing the 
structural tags is similar to Skut and 
Brants(1998) and the structural tag consists of 
three parts: 
1) Structural relation. The basic idea is simple: 
structures of limited depth are encoded using a 
finite number of flags. Given a sequence of 
input tokens(here, the word and part-of-speech 
pairs), we consider the structural relation 
between the previous input token and the 
current one. For the recognition of chunks, it is 
sufficient to distinguish the following four 
different structural relations which uniquely 
identify the sub-structures of depth l(Skut and 
Brants used seven different structural relations 
to identify the sub-structures of depth 2). 
00 the current input token and the previous one 
have the same parent 
90 one ancestor of the current input token and 
the previous input oken have the same parent 
09 the current input token and one ancestor of 
the previous input oken have the same parent 
99 one ancestor of the current input token and 
one ancestor of the previous input token have 
the same parent 
For example, in the following chunk tagged 
sentence(NULL represents the beginning and 
end of the sentence): 
NULL \[NP He/PRP\] \[VP reckons/VBZ\] [NP 
the/DT current/JJ account/NN deficit/NN\] \[VP 
will/MD narrow/VB\] \[PP to/TO\] \[NP only/RB 
#/# 1.8/CD billion/CD\] \[PP in/IN\] \[NP 
September/NNP\] \[O./.\] NULL 
the corresponding structural relations between 
two adjacent input okens are: 
90(NULL He/PRP) 
99(He/PRP reckons/VBZ) 
99(reckons/VBZ the/DT) 
00(the/DT current/JJ) 
00(current/JJ account/NN) 
00(account/NN deficit/NN) 
99(deficit/NN will/MD) 
00(will/MD narrow/VB) 
99(narrow/VB to/TO) 
99(to/TO only/RB) 
O0(only/RB #/#) 
00(#/# 1.8/CD) 
00(1.8/CD billion/CD) 
99(billion/CD in/IN) 
99(in/IN september/NNP) 
99(september/NNP ./.)
09(./. NULL) 
Compared with the B-Chunk and I-Chunk 
used in Ramshaw and Marcus(1995), structural 
relations 99 and 90 correspond to B-Chunk 
which represents the first word of the chunk, 
and structural relations 00 and 09 correspond to 
I-Chunk which represnts each other in the chunk 
while 90 also means the beginning of the 
sentence and 09 means the end of the sentence. 
2)Phrase category. This is used to identify the 
phrase categories of input tokens. 
3)Part-of-speech. Because of the limited 
number of structural relations and phrase 
categories, the part-of-speech is added into the 
structural tag to represent more accurate models. 
For the above chunk tagged sentence, the 
structural tags for all the corresponding input 
tokens are: 
90 PRt~NP(He/PRP) 
99_VB Z_VP(reckons/VBZ) 
99 DT NP(the/DT) 
O0 JJ NP(currentJJJ) 
00_N/'~NP(account/NN) 
00 N1NNP(deficiffNN) 
99_MDSVP(will/MD) 
00 VB_VP(narrow/VB) 
99_TO PP(to/TO) 
99_RB~,IP(only/RB) 
oo_# NP(#/#) 
00 CD_NP(1.8/CD) 
0(~CD~qP(billion/CD) 
99_IN PP(in/IN) 
99~lNP~,lP(september/NNP) 
99_._0(./.) 
2 The Baseline System 
As the baseline system, we assume 
P(t i I G?)= P(t i I pi ). That is to say, only the 
current part-of-speech is used as a lexical entry 
to determine the current structural chunk tag. 
Here, we define: 
? ? is the list of lexical entries in the 
chunking lexicon, 
73 
? \[ @ \[ is the number of lexical entries(the size 
of the chunking lexicon) 
? C is the training data. 
For the baseline system, we have : 
? @={pi,p~3C}, where Pi is a part-of- 
speech existing in the tra\]Lning data C 
? \]@ \[=48 (the number of part-of-speech tags 
in the training data). 
Table 1 gives an overview of the results of 
the chunking experiments. For convenience, 
precision, recall and F#_ 1 values are given 
seperately for the chunk types NP, VP, ADJP, 
ADVP and PP. 
Type Precision Recall Fa__~ 
Overall 87.01 89.68 88.32 
NP 90.02 90.50 90.26 
VP 89.86 93.14 91.47 
ADJP 70.94 63.84 67.20 
ADVP 57.98 80.33 I 67.35 
PP 85.95 96.62 90.97 
Table 1 : Results of chunking experiments with 
the lexical entry list : ~ = { pi, p~3C} 
3 Context-dependent Lexicons 
In the last section, we only use current part-of- 
speech as a lexical entry. In this section, we will 
attempt o add more contextual information to 
approximate P(t i/G~). This can be done by 
adding lexical entries with more contextual 
information into the lexicon ~.  In the 
following, we will discuss five context- 
dependent lexicons which consider different 
contextual information. 
3.1 Context of current part-of-speech and 
current word 
Here, we assume: 
e(t i I G~) = I P(ti I p~wi) 
\[ P(tl I Pi) 
where 
piwi ~ dp 
PiWi ~ dp 
~={piwi,piwi3C}+{pi,pi3C } and piwi is a 
part-of-speech and word pair existing in the 
training data C.  
In this case, the current part-of-speech and 
word pair is also used as a lexical entry to 
determine the current structural chunk tag and 
we have a total of about 49563 lexical 
entries(\[ ? \]=49563). Actually, the lexicon used 
here can be regarded as context-independent. 
The reason we discuss it in this section is to 
distinguish it from the context-independent 
lexicon used in the baseline system. Table 2 
give an overview of the results of the chunking 
experiments on the test data. 
Type \[Precision 
Overall 90.32 
NP 90.75 
VP 90.88 
ADJP 76.01 
ADVP 72.67 
PP 94.96 
Table 2 : Results of chunking experiments 
the lexical entry 
= {piwi ,Piwi3C} "1" {Pi" Pi 3C} 
Recall Fa~.l 
92.18 9i.24 
92.14 91.44 
92.78 91.82 
70.00 72.88 
88.33 79.74 
96.48 95.71 
with 
list : 
Table 2 shows that incorporation of current 
word information improves the overall F~=~ 
value by 2.9%(especially for the ADJP, ADVP 
and PP chunks), compared with Table 1 of the 
baseline system which only uses current part-of- 
speech information. This result suggests that 
current word information plays a very important 
role in determining the current chunk tag. 
3.2 Context of previous part-of-speech and 
current part-of-speech 
Here, we assume : 
P(t i / G~) 
I P(ti / pi-lPi ) Pi-lPi E 
= \[ P(ti I Pi) Pi-! Pi ~ ~ 
where 
= {Pi-l Pi, P~-1Pi 3C} + { Pi, pi3C} and Pi-lPi 
is a pair of previous part-of-speech and current 
part-of-speech existing in the training data C.  
In this case, the previous part-of-speech and 
current part-of-speech pair is also used as a 
lexical entry to determine the current structural 
chunk tag and we have a total of about 1411 
lexical entries(l~\]=1411). Table 3 give an 
overview of the results of the chunking 
experiments. 
74 
Type 
Overall 
Precision 
88.63 
NP 90.77 
VP 92.46 
ADJP 74.93 60.13 66.72 
ADVP 71.65 73.21 72.42 
PP 87.28 91.80 89.49 
Table 3: Results of chunking experiments with 
the lexical entry list : ? = 
{Pi-lPi, Pi-lPi 3C} + {Pi, Pi 3C} 
Recall F#= I 
89.00 88.82 
91.18 90.97 
92.98 92.72 
Compared with Table 1 of the baseline 
system, Table 3 shows that additional contextual 
information of previous part-of-speech improves 
the overall F/~_~ value by 0.5%. Especially, 
F/3_ ~ value for VP improves by 1.25%, which 
indicates that previous part-of-speech 
information has a important role in determining 
the chunk type VP. Table 3 also shows that the 
recall rate for chunk type ADJP decrease by 
3.7%. It indicates that additional previous part- 
of-speech information makes ADJP chunks 
easier to merge with neibghbouring chunks. 
3.3 Context of previous part-of-speech, 
previous word and current part-of-speech 
Here, we assume : 
P(t, / G~) 
IP(ti / pi_lwi_lpi) pi_lwi_lpl ~ dp 
I 
\[ P(ti \[ Pi ) Pi-lWi-I Pi ~ ~ 
where 
= { Pi-i wi-l Pi, Pi-l wi-I Pi3 C} + { Pi, Pi 3 C }, 
where pi_lwi_lp~ is a triple pattern existing in 
the training corpus. 
In this case, the previous part-of-speech, 
previous word and current part-of-speech triple 
is also used as a lexical entry to determine the 
current structural chunk tag and } ? 1=136164. 
Table 4 gives the results of the chunking 
experiments. Compared with Table 1 of the 
baseline system, Table 4 shows that additional 
136116 new lexical entries of format 
Pi-lw~-lPi improves the overall F#= l value by 
3.3%. Compared with Table 3 of the extended 
system 2.2 which uses previous part-of-speech 
and current part-of-speech as a lexical entry, 
Table 4 shows that additional contextual 
information of previous word improves the 
overall Fa= 1 value by 2.8%. 
Type Precision Recall F~=l 
Overall 91.23 92.03 91.63 
NP 92.89 93.85 93.37 
VP 94.10 94.23 94.16 
ADJP 79.83 69.01 74.03 
ADVP 76.91 80.53 78.68 
PP 90.41 94.77 92.53 
Table 4 : Results of chunking experiments with 
the lexical entry list : 
={p,_lw~_~ p,,p,_~ w,_ip,3C } + {Pi , p~3C} 
3.4 Context of previous part-of-speech, current 
part-of-speech and current word 
Here, we assume : 
P(t i I G~ ) 
IP(tt I Pi-i PiWi) Pi-I piwi E dp 
\[ P(ti / Pi ) Pi-I Pi Wi ~ 1I) 
where 
= {Pi-lPiWi, Pi-lP~W~ 3C} + {Pi, Pi3C}, 
where pi_lpiw~ is a triple pattern existing in 
the training and \] ? \[=131416. 
Table 5 gives the results of the chunking 
experiments. 
Type Precision Recall F/3= 1
Overall 92.67 93.43 93.05 
NP 93.35 94.10 93.73 
VP 93.05 94.30 93.67 
ADJP 80.65 72.27 76.23 
ADVP 78.92 84.48 81.60 
PP 95.30 96.67 95.98 
Table 5: Results of chunking experiments with 
the lexical entry list : 
={Pi-lPiWi, P,-iP, w,3C} + {pi , Pi 3C} 
Compared with Table 2 of the extended 
system which uses current part-of-speech and 
current word as a lexical entry, Table 5 shows 
that additional contextual information of 
previous part-of-speech improves the overall 
Fa= 1 value by 1.8%. 
3.5 Context of previous part-of-speech, 
previous word, current part-of-speech and 
current word 
Here, the context of previous part-of-speech, 
current part-of-speech and current word is used 
as a lexical entry to determine the current 
75 
structural chunk tag and qb = 
{Pi-l wi-lPiWi, Pi-lwi-~piwi 36'} + {Pi, P i3C} , 
where p~_lWi_~P~W~ is a pattern existing in the 
training corpus. Due to memory limitation, only 
lexical entries which occurs :more than 1 times 
are kept. Out of 364365 possible lexical entries 
existing in the training data, 98489 are kept( 
1~ 1=98489). 
= I P(ti/Pi-\]wi-,PiWli) 
\[ P(t, lp,) pi_lwi_lpiwi ~ 
Table 6 gives the results of the chunking 
experiments. 
Type 
Overall 
NP 
VP 
ADJP 
ADVP 
PP 
Precision 
92.28 
93.50 
92.62 
81.39 
75.09 
94.12 
Recall 
93.04 
93.53 
94.07 
72.17 
86.23 
97.12 
F~=l 
92.66 
93.52 
93.35 
76.50 
80.27 
95.59 
Table 6: Results of chunking experiments with 
the lexical entry list : ? = 
{Pi-l wi-\]PiWi, Pi-lwi-lpiwi3C} + {Pi, p~3C} 
Compared with Table 2 of the extended 
system which uses current part-of-speech and 
current word as a lexical entry, Table 6 shows 
that additional contextual information of 
previous part-of-speech improves the overall 
Ft3=l value by 1.8%. 
3.6 Conclusion 
Above experiments shows that adding more 
contextual information i to lexicon significantly 
improves the chunking accuracy. However, this 
improvement is gained at the expense of a very 
large lexicon and we fred it difficult o merge all 
the above context-dependent l xicons in a single 
lexicon to further improve the chunking 
accurracy because of memory limitation. In 
order to reduce the size of lexicon effectively, 
an error-driven learning approach is adopted to 
examine the effectiveness of lexical entries and 
make it possible to further improve the 
chunking accuracy by merging all the above 
context-dependent l xicons in a single lexicon. 
This will be discussed in the next section. 
4 Error-driven Learning 
In section 2, we implement a basefine system 
which only considers current part-of-speech as a 
lexical entry to dete, ufine the current chunk tag 
while in section 3, we implement several 
extended systems which take more contextual 
information i to consideration. 
Here, we will examine the effectiveness of
lexical entries to reduce the size of lexicon and 
make it possible to further improve the 
chunking accuracy by merging several context- 
dependent lexicons in a single lexicon. 
For a new lexical entry e i, the effectiveness 
F~ (e i) is measured by the reduction in error 
which results from adding the lexical entry to 
- -  ~ Er ro r  (e,). the lexicon : F~ (e i ) = F :  rr?r (e i ) - o+Ao 
Here, F,~ r~?r (el) is the chunking error number 
of the lexical entry e i for the old lexicon 
r~ Er ror  / x and r~,+~ te i) is the chunking error number of 
the lexical entry e i for the new lexicon 
+ AO where e~ e A~ (A~ is the list of 
new lexical entries added to the old lexicon ~ ). 
If F o (e i ) > 0, we define the lexical entry ei as 
positive for lexicon ~.  Otherwise, the lexical 
entry e i is negative for lexicon ~.  
Tables 7 and 8 give an overview of the 
effectiveness distributions for different lexicons 
applied in the extended systems, compared with 
the lexicon appfied in the baseline system, on 
the test data and the training data, respectively. 
Tables 7 and 8 show that only a minority of 
lexical entries are positive. This indicates that 
discarding non-positive lexical entries will 
largely decrease the lexicon memory 
requirement while keeping the chunking 
accurracy. 
Context Positive 
1800 
209 
Negative 
314 
136 
Total 
4083 I 155 
Table 7 : The effectiveness of lexical 
the test data ..... 
49515 
1363 
2876 229 136116 
2895 193 131368 
98441 
entries on 
76 
Context Positive i Negative 
6724l 
Type i Precision Recall Fa=l 
Overall 91.02 92.21 91.61 
NP 92.36 93.69 93.02 
VP 93.68 94.94 94.30 
ADJP 78.28 71.46 74.71 
ADVP 76.77 81.79 79.20 
PP 90.67 95.37 92.96 
Total 
vos,w, 719 49515 
eos,_,Pos, 357 196 1363 
POS,.~w,.,eos,, 13205 582 136116 
POS,_,eos,w, 14186 325 131368 
POS,.,w,_leos,,w, 15516 144 98441 
Table 8 : The effectiveness of lexical entries on 
the training data 
Tables 9-13 give the performances of the 
five error-driven systems which discard all the 
non-positive l xical enrties on the training data. 
Here, ~'  is the lexicon used in the baseline 
system, dP'={pi,pi3C } and A~=~-~' .  It 
is found that Ffl_~ values of error driven 
systems for context of current part-of-speech 
and word pak and for context of previous part- 
of-speech and current part-of-speech increase by 
1.2% and 0.6%. Although F~= 1values for other 
three cases slightly decrease by 0.02%, 0.02% 
and 0.19%, the sizes of lexicons have been 
greatly reduced by 85% to 97%. 
Type Precision Recall F#=l 
Overall 91.69 93.28 92.48 
NP 92.64 93.48 93.06 
VP 92.16 93.66 92.90 
ADJP 78.39 71.69 74.89 
ADVP 73.66 87.80 80.11 
PP 95.18 97.38 96.27 
Table 9 : Results of chunking experiments with 
error-driven lexicon : dp= 
{ p~w~, p,w,3C & F~,. (p~w i ) > O} + { p~, p~3C} 
Type Precision Recall F~=l 
Overall 88.68 90.28 89.47 
NP 90.61 91.57 91.08 
VP 91.80 94.08 92.90 
ADJP 72.20 62.72 67.13 
ADVP 70.53 78.90 74.48 
PP 86.55 96.34 91.19 
Table 10: Results of chunking experiments 
with error-driven lexicon : ? = 
{ P,-~ Pi, Pi-1 Pi ~C & F~. (p,_~ p, ) > 0} 
+ { Pi, Pi 3C} 
Table 11: Results of chunking experiments 
with error-driven lexicon : ? = 
{ pi_l Wi_lPi , pi_l wi_lpi3C & V~,(Pi_l Wi_iPi ) > O} 
+{pi ,P i~C} 
Type 
Overall 92.84 
NP 
VP 
Precision 
93.35 
93.97 
79.49 
95.19 
Recall 
93.21 
93.65 
94.67 
72.94 
Ffl=l 
93.03 
93.50 
94.32 
76.07 ADJP 
ADVP 79.47 85.91 82.57 
PP 
77 
96.29 95.74 
Table 12: Results of chunking experiments 
with error-driven lexicon : ? = 
{ Pi-I P~W~, p~_~ Piw,3C & F.. (pi_~ p,w i) > 0} 
+{pi ,P i3C} 
Type Precision Recall F~_ 1 
Overall 91.99 92.95 92.47 
NP 93.35 93.39 93.37 
VP 92.89 94.36 93.62 
ADJP 80.01 71.70 75.63 
ADVP 73.40 87.32 79.76 
PP 93.42 97.33 95.33 
Table 13: Results of chunking experiments 
with error-driven lexicon : ? = 
{Pi-l Wi-lPiWi' Pi-lWi-lpiWi3C+{pi ' Pi3C} 
& F?.(p~_~w~_~p~w~) > O} 
After discussing the five context-dependent 
lexicons separately, now we explore the 
merging of context-dependent lexicons by 
assuming : 
CI~ .~ { Pi-lWi-I PiWi, Pi-lWi-I PiwigC 
& Fa,. (pi-lwi-t piwi ) > 0} 
+ { Pi-I PiW~, Pi-l piwi ~C & Fa" (Pi-l piwi ) > O} 
+ { Pi-lWi-I Pi" Pi-lWi-1Pi 3C & F~. (pi_lWi_l Pi ) > 0} 
+ { Pi-1 Pi, Pi-I Pii ~C & F~, (Pi-l Pi )> O} 
+ { piw~, Piw~3C & F~,. (PiWi) > 0} + { Pi, p~3C} 
and P(t i /G~) is approximatl~ by the following 
order :
1. if Pi_lWi_iPiWi E fI~, 
P(ti /G~)=P( t  i / p i _ lw i _ lP iWi )  
2. if p~_lp~wi E cb, 
P(ti /G~)=P( t  i /p i _ lw i _ lP iWi )  
3. if Pi-twi-lPi E ~,  
P(t i/G~) = P(t i / pi_l wi_l: pi ) 
4. if PiWi E ~,  P(t i / G~ ) = P(t i / piwi ) 
5. if Pi-I Pi E ~,  P(t i / G~ ) = P(t i / Pi-1Pi) 
6. P(t i lG : )=P( t  i lpi_lpi) 
Table 14 gives an overview of the chunking 
experiments using the above assumption. It 
shows that the F:=i value for the merged 
context-dependent lexicon inreases to 93.68%. 
For a comparison, the F/~=i value is 93.30% 
when all the possible lexical entries are included 
in ~ (Due to memory limitation, only the top 
150000 mostly occurred lexical entries are 
included). 
Type Precision Recall F#=i 
Overall 93.40 93.95 93.68 
NP 93.60 94.64 94.12 
VP 94.64 94.75 94.70 
ADJP 77.12 74.55 75.81 
ADVP 82.39 83.80 83.09 
PP 96.61 96.63 96.62 
Table 14: Results of chunking experiments 
with the merged context-dependent l xicon 
For the relationship between the training 
corpus size and error driven learning 
performance, Table 15 shows that the 
performance of error-driven learning improves 
stably when the training corpus ize increases. 
Training Sections I ~ I Accuracy i FB 1 
0-1 
0-3 
0-5 
0-7 
0-9 
0-11 
0-13 
0-i5 
0-17 
0-19 
14384 94.78% 91.95 
24507 95.19% i 92.51 
32316 95.28%1 92.77 
38286 95.41% 93.00 
39876 95.53% i 93.12 
43372 95.65% 93.31 
46029 95.62% 93.29 
47901 95.66% 93.34 
48813 95.74% i 93.41 
49988 95.92% 93.68 
Table 15: The performance oferror-driven 
learning with different training corpus ize 
For comparison with other chunk taggers, 
we also evaluate our chunk tagger with the 
merged context-dependent lexicon by cross- 
validation on all 25 partitions of the PENN WSJ 
TreeBank. Table 16 gives an overview of such 
chunking experiments. 
Type Precision Recall Fa=l 
Overall 96.40 96.47 96.44 
NP 96.49 96.99 96.74 
VP 97.13 97.36 97.25 
ADJP 89.92 88.15 89.03 
ADVP 91.52 87.57 89.50 
97.13 97.36 PP 97.25 
Table 16: Results of 25-fold cross-validation 
chunking experiments with the merged 
context-dependent l xicon 
Tables 14 and 16 shows that our new chunk 
tagger greatly outperforms other eported chunk 
taggers on the same training data and test data 
by 2%-3%.(Buchholz S., Veenstra J. and 
Daelmans W.(1999), Ramshaw L.A. and 
Marcus M.P.(1995), Daelemans W., Buchholz 
S. and Veenstra J.(1999), and Veenstra 
J.(1999)). 
Conclusion 
This paper proposes a new error-driven HMM- 
based chunk tagger with context-dependent 
lexicon. Compared with standard HMM-based 
tagger, this new tagger uses a new Hidden 
Markov Modelling approach which incorporates 
more contextual information into a lexical entry 
n 
by assuming MI(Tqn,G~)= 2Ml ( t , ,G f ) .  
i=1 
Moreover, an error-driven learning approach is 
adopted to drease the memeory requirement and 
further improve the accuracy by including more 
context-dependent information i to lexicon. 
It is found that our new chunk tagger 
singnificantly outperforms other eported chunk 
taggers on the same training data and test data. 
For future work, we will explore the 
effectivessness of considering even more 
contextual information on approximation of 
P(T~"IG ~) by using the forward-backward 
algodthm(Rabiner 1989) while currently we 
only consider the contextual information of 
current location and previous location. 
78 
Acknowledgement 
We wish to thank Sabine Buchholz 
from Tilbug University for kindly providing us 
her program which is also used to 
extact data for Conll-2000 share task. 
References 
Abney S. "Parsing by chunks ". Principle-Based 
Parsing edited by Berwick, Abney and Tenny. 
Kluwer Academic Publishers. 
Argamon S., Dagan I. and Krymolowski Y. "A 
memory-based approach to learning shallow 
natural language patterns." COL1NG/ACL- 
1998. Pp.67-73. Montreal, Canada. 1998. 
Bod R. "A computational model of language 
performance: Data-oriented parsing." 
COLING-1992. Pp.855-859. Nantes, France. 
1992. 
Boungault D. "Surface grammatical nalysis for 
the extraction of terminological noun 
phrases". COLING-92. Pp.977-981. 1992. 
Bdll Eric. "A corpus-based approach to 
language learning". Ph.D thesis. Univ. of 
Penn. 1993 
Buchholz S., Veenstra J. and Daelmans W. 
"Cascaded grammatical relation assignment." 
Proceeding of EMNLP/VLC-99, at ACL'99. 
1999 
Cardie C. "A case-based approach to knowledge 
acquisition for domain-specific sentence 
analysis." Proceeding of the 11 'h National 
Conference on Artificial Intelligence. Pp.798- 
803. Menlo Park, CA, USA. AAAI Press. 
1993. 
Church K.W. "A stochastic parts program and 
noun phrase parser for unrestricted Text." 
Proceeding of Second Conference on Applied 
Natural Language Processing. Pp.136-143. 
Austin, Texas, USA. 1988. 
Daelemans W., Buchholz S. and Veenstra J. 
"Memory-based shallow parsing." CoNLL- 
1999. Pp.53-60. Bergen, Norway. 1999. 
Daelemans W., Zavrel J., Berck P. and Gillis S. 
"MBT: A memory-based part-of-speech 
tagger generator." Proceeding of the Fourth 
Workshop on Large Scale Corpora. Pp. 14-27. 
ACL SIGDAT. 1996. 
Grefenstette G. "Light parsing as finite-state 
filtering". Workshop on Extended Finite State 
Models of Language at ECAI'96. Budapest, 
Hungary. 1996. 
Kupiec J. " An algorithm for finding noun 
phrase correspondences in bilingual corpora". 
ACL'93. Pp17-22. 1993. 
Marcus M., Santodni B. and Marcinkiewicz 
? M.A. "Buliding a large annotated corpus of 
English: The Penn Treebank". Computational 
Linguistics. 19(2):313-330. 1993. 
Rabiner L. "A tutorial on Hidden Markov 
Models and selected applications in speech 
recognition". IEEE 77(2), pp.257-285. 1989. 
Ramshaw L.A. and Marcus M.P. 
"Transformation-based Learning". 
Proceeding of 3th ACL Workshop on Very 
Large Corpora at ACL'95. 1995. 
Rijsbergen C.J.van. Information Retrieval. 
Buttersworth, London. 1979. 
Skut W. and Brants T. "Chunk tagger: statistical 
recognition of noun phrases." ESSLLI-1998 
Workshop on Automated Acquisition of Syntax 
and Parsing. Saarbruucken, Germany. 1998. 
Veenstra J. "Memory-based text chunking". 
Workshop on machine learning in human 
language technology at A CAI'99. 1999. 
Voutilainen A. "Nptool: a detector of English 
phrases". Proceeding of the Workshop on 
Very Large Corpora. Pp48-57. ACL' 93. 
1993 
Watson B. and Chunk Tsoi A. "Second order 
Hidden Markov Models for speech 
recognition". Proceeding of 4 ~ Australian 
International Conference on Speech Science 
and Technology. Pp. 146-151.1992. 
79 
Named Entity Recognition using an HMM-based Chunk Tagger 
 
GuoDong Zhou               Jian Su  
Laboratories for Information Technology 
21 Heng Mui Keng Terrace 
Singapore 119613 
           zhougd@lit.org.sg          sujian@lit.org.sg
 
Abstract 
This paper proposes a Hidden Markov 
Model (HMM) and an HMM-based chunk 
tagger, from which a named entity (NE) 
recognition (NER) system is built to 
recognize and classify names, times and 
numerical quantities. Through the HMM, 
our system is able to apply and integrate 
four types of internal and external 
evidences: 1) simple deterministic internal 
feature of the words, such as capitalization 
and digitalization; 2) internal semantic 
feature of important triggers; 3) internal 
gazetteer feature; 4) external macro context 
feature. In this way, the NER problem can 
be resolved effectively. Evaluation of our 
system on MUC-6 and MUC-7 English NE 
tasks achieves F-measures of 96.6% and 
94.1% respectively. It shows that the 
performance is significantly better than 
reported by any other machine-learning 
system. Moreover, the performance is even 
consistently better than those based on 
handcrafted rules.  
1 Introduction 
Named Entity (NE) Recognition (NER) is to 
classify every word in a document into some 
predefined categories and "none-of-the-above". In 
the taxonomy of computational linguistics tasks, it 
falls under the domain of "information extraction", 
which extracts specific kinds of information from 
documents as opposed to the more general task of 
"document management" which seeks to extract all 
of the information found in a document. 
Since entity names form the main content of a 
document, NER is a very important step toward 
more intelligent information extraction and 
management. The atomic elements of information 
extraction -- indeed, of language as a whole -- could 
be considered as the "who", "where" and "how 
much" in a sentence. NER performs what is known 
as surface parsing, delimiting sequences of tokens 
that answer these important questions. NER can 
also be used as the first step in a chain of processors: 
a next level of processing could relate two or more 
NEs, or perhaps even give semantics to that 
relationship using a verb. In this way, further 
processing could discover the "what" and "how" of 
a sentence or body of text.  
While NER is relatively simple and it is fairly 
easy to build a system with reasonable performance, 
there are still a large number of ambiguous cases 
that make it difficult to attain human performance. 
There has been a considerable amount of work on 
NER problem, which aims to address many of these 
ambiguity, robustness and portability issues. During 
last decade, NER has drawn more and more 
attention from the NE tasks [Chinchor95a] 
[Chinchor98a] in MUCs [MUC6] [MUC7], where 
person names, location names, organization names, 
dates, times, percentages and money amounts are to 
be delimited in text using SGML mark-ups.  
Previous approaches have typically used 
manually constructed finite state patterns, which 
attempt to match against a sequence of words in 
much the same way as a general regular expression 
matcher. Typical systems are Univ. of Sheffield's 
LaSIE-II [Humphreys+98], ISOQuest's NetOwl 
[Aone+98] [Krupha+98] and Univ. of Edinburgh's 
LTG [Mikheev+98] [Mikheev+99] for English 
NER. These systems are mainly rule-based. 
However, rule-based approaches lack the ability of 
coping with the problems of robustness and 
portability. Each new source of text requires 
significant tweaking of rules to maintain optimal 
performance and the maintenance costs could be 
quite steep. 
The current trend in NER is to use the 
machine-learning approach, which is more 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 473-480.
                         Proceedings of the 40th Annual Meeting of the Association for
attractive in that it is trainable and adaptable and the 
maintenance of a machine-learning system is much 
cheaper than that of a rule-based one. The 
representative machine-learning approaches used in 
NER are HMM (BBN's IdentiFinder in [Miller+98] 
[Bikel+99] and KRDL's system [Yu+98] for 
Chinese NER.), Maximum Entropy (New York 
Univ.'s MEME in [Borthwick+98] [Borthwich99]) 
and Decision Tree (New York Univ.'s system in 
[Sekine98] and SRA's system in [Bennett+97]). 
Besides, a variant of Eric Brill's 
transformation-based rules [Brill95] has been 
applied to the problem [Aberdeen+95]. Among 
these approaches, the evaluation performance of 
HMM is higher than those of others. The main 
reason may be due to its better ability of capturing 
the locality of phenomena, which indicates names 
in text.  Moreover, HMM seems more and more 
used in NE recognition because of the efficiency of 
the Viterbi algorithm [Viterbi67] used in decoding 
the NE-class state sequence. However, the 
performance of a machine-learning system is 
always poorer than that of a rule-based one by about 
2% [Chinchor95b] [Chinchor98b]. This may be 
because current machine-learning approaches 
capture important evidence behind NER problem 
much less effectively than human experts who 
handcraft the rules, although machine-learning 
approaches always provide important statistical 
information that is not available to human experts. 
As defined in [McDonald96], there are two kinds 
of evidences that can be used in NER to solve the 
ambiguity, robustness and portability problems 
described above. The first is the internal evidence 
found within the word and/or word string itself 
while the second is the external evidence gathered 
from its context. In order to effectively apply and 
integrate internal and external evidences, we 
present a NER system using a HMM. The approach 
behind our NER system is based on the 
HMM-based chunk tagger in text chunking, which 
was ranked the best individual system [Zhou+00a] 
[Zhou+00b] in CoNLL'2000 [Tjong+00]. Here, a 
NE is regarded as a chunk, named "NE-Chunk". To 
date, our system has been successfully trained and 
applied in English NER. To our knowledge, our 
system outperforms any published 
machine-learning systems. Moreover, our system 
even outperforms any published rule-based 
systems.  
The layout of this paper is as follows. Section 2 
gives a description of the HMM and its application 
in NER: HMM-based chunk tagger. Section 3 
explains the word feature used to capture both the 
internal and external evidences. Section 4 describes 
the back-off schemes used to tackle the sparseness 
problem. Section 5 gives the experimental results of 
our system. Section 6 contains our remarks and 
possible extensions of the proposed work. 
2 HMM-based Chunk Tagger 
2.1  HMM Modeling 
Given a token sequence n
n gggG L211 = , the goal 
of NER is to find a stochastic optimal tag sequence 
n
n tttT L211 =  that maximizes                     (2-1) 
)()(
),(
log)(log)|(log
11
11
111 nn
nn
nnn
GPTP
GTP
TPGTP
?
+=  
The second item in (2-1) is the mutual 
information between nT1  and
nG1 . In order to 
simplify the computation of this item, we assume 
mutual information independence:  
?
=
=
n
i
n
i
nn GtMIGTMI
1
111 ),(),(   or              (2-2) 
?
= ?
=
?
n
i
n
i
n
i
nn
nn
GPtP
GtP
GPTP
GTP
1 1
1
11
11
)()(
),(
log
)()(
),(
log    (2-3) 
Applying it to equation (2.1), we have: 
?
?
=
=
+
?=
n
i
n
i
n
i
i
nnn
GtP
tPTPGTP
1
1
1
111
)|(log
)(log)(log)|(log
   (2-4) 
The basic premise of this model is to consider 
the raw text, encountered when decoding, as though 
it had passed through a noisy channel, where it had 
been originally marked with NE tags. The job of our 
generative model is to directly generate the original 
NE tags from the output words of the noisy channel. 
It is obvious that our generative model is reverse to 
the generative model of traditional HMM1, as used 
                                                     
1 In traditional HMM to maximise )|(log 11 nn GTP , first we 
apply Bayes' rule: 
)(
),(
)|(
1
11
11 n
nn
nn
GP
GTP
GTP =   
and have: 
in BBN's IdentiFinder, which models the original 
process that generates the NE-class annotated 
words from the original NE tags. Another 
difference is that our model assumes mutual 
information independence (2-2) while traditional 
HMM assumes conditional probability 
independence (I-1). Assumption (2-2) is much 
looser than assumption (I-1) because assumption 
(I-1) has the same effect with the sum of 
assumptions (2-2) and (I-3)2. In this way, our model 
can apply more context information to determine 
the tag of current token. 
From equation (2-4), we can see that: 
1) The first item can be computed by applying 
chain rules. In ngram modeling, each tag is 
assumed to be probabilistically dependent on the 
N-1 previous tags.  
2) The second item is the summation of log 
probabilities of all the individual tags. 
3) The third item corresponds to the "lexical" 
component of the tagger.  
We will not discuss both the first and second 
items further in this paper. This paper will focus on 
the third item?
=
n
i
n
i GtP
1
1 )|(log , which is the main 
difference between our tagger and other traditional 
HMM-based taggers, as used in BBN's IdentiFinder. 
Ideally, it can be estimated by using the 
forward-backward algorithm [Rabiner89] 
recursively for the 1st-order [Rabiner89] or 2nd 
-order HMMs [Watson+92]. However, an 
alternative back-off modeling approach is applied 
instead in this paper (more details in section 4). 
2.2 HMM-based Chunk Tagger 
                                                                                   
))(log)|((logmaxarg
)|(logmaxarg
111
11
nnn
T
nn
T
TPTGP
GTP
+=
 
Then we assume conditional probability 
independence: ?
=
=
n
i
ii
nn tgPTGP
1
11 )|()|(                 (I-1) 
and have: 
))(log)|(log(maxarg
)|(logmaxarg
1
1
11
nn
i
ii
T
nn
T
TPtgP
GTP
+= ?
=
                    (I-2) 
2 We can obtain equation (I-2) from (2.4) by assuming 
)|(log)|(log
1 ii
n
i tgPGtP =                                    (I-3) 
For NE-chunk tagging, we have 
token >=< iii wfg , , where nn wwwW L211 =  is the 
word sequence and nn fffF L211 =  is the 
word-feature sequence. In the meantime, NE-chunk 
tag it  is structural and consists of three parts: 
1) Boundary Category: BC = {0, 1, 2, 3}. Here 0 
means that current word is a whole entity and 
1/2/3 means that current word is at the 
beginning/in the middle/at the end of an entity. 
2) Entity Category: EC. This is used to denote the 
class of the entity name. 
3) Word Feature: WF. Because of the limited 
number of boundary and entity categories, the 
word feature is added into the structural tag to 
represent more accurate models. 
Obviously, there exist some constraints between 
1?it  and it  on the boundary and entity categories, as 
shown in Table 1, where "valid" / "invalid" means 
the tag sequence ii tt 1?  is valid / invalid while "valid 
on" means ii tt 1?  is valid with an additional 
condition ii ECEC =?1 . Such constraints have been 
used in Viterbi decoding algorithm to ensure valid 
NE chunking. 
 0 1 2 3 
0 Valid Valid Invalid Invalid
1 Invalid Invalid Valid on Valid on 
2 Invalid Invalid Valid Valid 
3 Valid Valid Invalid Invalid
Table 1: Constraints between 1?it  and it  (Column: 
1?iBC  in 1?it ; Row: iBC  in it ) 
3 Determining Word Feature 
As stated above, token is denoted as ordered pairs of 
word-feature and word itself: >=< iii wfg , . 
Here, the word-feature is a simple deterministic 
computation performed on the word and/or word 
string with appropriate consideration of context as 
looked up in the lexicon or added to the context. 
In our model, each word-feature consists of 
several sub-features, which can be classified into 
internal sub-features and external sub-features. The 
internal sub-features are found within the word 
and/or word string itself to capture internal 
evidence while external sub-features are derived 
within the context to capture external evidence. 
3.1 Internal Sub-Features 
Our model captures three types of internal 
sub-features: 1) 1f : simple deterministic internal 
feature of the words, such as capitalization and 
digitalization; 2) 2f : internal semantic feature of 
important triggers; 3) 3f : internal gazetteer feature. 
1) 1f  is the basic sub-feature exploited in this 
model, as shown in Table 2 with the descending 
order of priority. For example, in the case of 
non-disjoint feature classes such as 
ContainsDigitAndAlpha and 
ContainsDigitAndDash, the former will take 
precedence. The first eleven features arise from 
the need to distinguish and annotate monetary 
amounts, percentages, times and dates. The rest 
of the features distinguish types of capitalization 
and all other words such as punctuation marks. 
In particular, the FirstWord feature arises from 
the fact that if a word is capitalized and is the 
first word of the sentence, we have no good 
information as to why it is capitalized (but note 
that AllCaps and CapPeriod are computed before 
FirstWord, and take precedence.)  This 
sub-feature is language dependent. Fortunately, 
the feature computation is an extremely small 
part of the implementation. This kind of internal 
sub-feature has been widely used in 
machine-learning systems, such as BBN's 
IdendiFinder and New York Univ.'s MENE. The 
rationale behind this sub-feature is clear: a) 
capitalization gives good evidence of NEs in 
Roman languages; b) Numeric symbols can 
automatically be grouped into categories. 
2) 2f  is the semantic classification of important 
triggers, as seen in Table 3, and is unique to our 
system. It is based on the intuitions that 
important triggers are useful for NER and can be 
classified according to their semantics. This 
sub-feature applies to both single word and 
multiple words. This set of triggers is collected 
semi-automatically from the NEs and their local 
context of the training data. 
3) Sub-feature 3f , as shown in Table 4, is the 
internal gazetteer feature, gathered from the 
look-up gazetteers: lists of names of persons, 
organizations, locations and other kinds of 
named entities. This sub-feature can be 
determined by finding a match in the 
gazetteer of the corresponding NE type 
where n (in Table 4) represents the word 
number in the matched word string. In stead 
of collecting gazetteer lists from training 
data, we collect a list of 20 public holidays in 
several countries, a list of 5,000 locations 
from websites such as GeoHive3, a list of 
10,000 organization names from websites 
such as Yahoo4 and a list of 10,000 famous 
people from websites such as Scope 
Systems5. Gazetters have been widely used 
in NER systems to improve performance.  
3.2 External Sub-Features 
For external evidence, only one external macro 
context feature 4f , as shown in Table 5, is captured 
in our model. 4f  is about whether and how the 
encountered NE candidate is occurred in the list of 
NEs already recognized from the document, as 
shown in Table 5 (n is the word number in the 
matched NE from the recognized NE list and m is 
the matched word number between the word string 
and the matched NE with the corresponding NE 
type.). This sub-feature is unique to our system. The 
intuition behind this is the phenomena of name 
alias.  
During decoding, the NEs already recognized 
from the document are stored in a list. When the 
system encounters a NE candidate, a name alias 
algorithm is invoked to dynamically determine its 
relationship with the NEs in the recognized list. 
Initially, we also consider part-of-speech (POS) 
sub-feature. However, the experimental result is 
disappointing that incorporation of POS even 
decreases the performance by 2%. This may be 
because capitalization information of a word is 
submerged in the muddy of several POS tags and 
the performance of POS tagging is not satisfactory, 
especially for unknown capitalized words (since 
many of NEs include unknown capitalized words.). 
Therefore, POS is discarded. 
                                                     
3 http://www.geohive.com/ 
4 http://www.yahoo.com/ 
5 http://www.scopesys.com/ 
Sub-Feature 1f  Example  Explanation/Intuition 
OneDigitNum 9 Digital Number 
TwoDigitNum 90 Two-Digit year 
FourDigitNum 1990 Four-Digit year 
YearDecade 1990s Year Decade 
ContainsDigitAndAlpha A8956-67 Product Code 
ContainsDigitAndDash 09-99 Date 
ContainsDigitAndOneSlash 3/4 Fraction or Date 
ContainsDigitAndTwoSlashs 19/9/1999 DATE 
ContainsDigitAndComma 19,000 Money 
ContainsDigitAndPeriod 1.00 Money, Percentage 
OtherContainsDigit 123124 Other Number 
AllCaps IBM Organization 
CapPeriod M. Person Name Initial 
CapOtherPeriod St. Abbreviation 
CapPeriods N.Y. Abbreviation 
FirstWord First word of sentence No useful capitalization information 
InitialCap Microsoft Capitalized Word 
LowerCase Will Un-capitalized Word 
Other $ All other words 
Table 2: Sub-Feature 1f : the Simple Deterministic Internal Feature of the Words 
NE Type (No of Triggers) Sub-Feature 2f  Example Explanation/Intuition 
PERCENT (5) SuffixPERCENT % Percentage Suffix 
PrefixMONEY $ Money Prefix MONEY (298) 
SuffixMONEY Dollars Money Suffix 
SuffixDATE Day Date Suffix 
WeekDATE Monday Week Date 
MonthDATE July Month Date 
SeasonDATE Summer Season Date 
PeriodDATE1 Month Period Date 
PeriodDATE2 Quarter Quarter/Half of Year 
EndDATE Weekend Date End  
DATE (52) 
ModifierDATE Fiscal Modifier of Date 
SuffixTIME a.m. Time Suffix TIME (15) 
PeriodTime Morning Time Period 
PrefixPERSON1 Mr. Person Title 
PrefixPERSON2 President Person Designation  
PERSON (179) 
FirstNamePERSON Micheal Person First Name 
LOC (36) SuffixLOC River Location Suffix 
ORG (177) SuffixORG Ltd Organization Suffix 
Others (148) Cardinal, Ordinal, etc. Six,, Sixth Cardinal and Ordinal Numbers 
Table 3: Sub-Feature 2f : the Semantic Classification of Important Triggers 
NE Type (Size of Gazetteer) Sub-Feature 3f  Example 
DATE (20) DATEnGn Christmas Day: DATE2G2 
PERSON (10,000) PERSONnGn Bill Gates: PERSON2G2 
LOC (5,000) LOCnGn Beijing: LOC1G1 
ORG (10,000) ORGnGn United Nation: ORG2G2 
Table 4: Sub-Feature 3f : the Internal Gazetteer Feature (G means Global gazetteer) 
NE Type Sub-Feature Example 
PERSON PERSONnLm Gates: PERSON2L1 ("Bill Gates" already recognized as a person name) 
LOC LOCnLm N.J.: LOC2L2 ("New Jersey" already recognized as a location name) 
ORG ORGnLm UN: ORG2L2 ("United Nation" already recognized as a org name) 
Table 5: Sub-feature 4f : the External Macro Context Feature (L means Local document) 
4  Back-off Modeling 
Given the model in section 2 and word feature in 
section 3, the main problem is how to 
compute ?
=
n
i
n
i GtP
1
1 )/( . Ideally, we would have 
sufficient training data for every event whose 
conditional probability we wish to calculate. 
Unfortunately, there is rarely enough training data 
to compute accurate probabilities when decoding on 
new data, especially considering the complex word 
feature described above. In order to resolve the 
sparseness problem, two levels of back-off 
modeling are applied to approximate )/( 1
n
i GtP : 
1) First level back-off scheme is based on different 
contexts of word features and words themselves, 
and nG1  in )/( 1
n
i GtP  is approximated in the 
descending order of iiii wfff 12 ?? , 21 ++ iiii ffwf , 
iii wff 1? , 1+iii fwf , iii fwf 11 ?? , 11 ++ iii wff , 
iii fff 12 ?? , 21 ++ iii fff , ii wf , iii fff 12 ?? , 1+ii ff  
and if . 
2) The second level back-off scheme is based on 
different combinations of the four sub-features 
described in section 3, and kf  is approximated 
in the descending order of 4321 kkkk ffff , 
31
kk ff , 
41
kk ff , 
21
kk ff  and 
1
kf . 
5 Experimental Results 
In this section, we will report the experimental 
results of our system for English NER on MUC-6 
and MUC-7 NE shared tasks, as shown in Table 6, 
and then for the impact of training data size on 
performance using MUC-7 training data. For each 
experiment, we have the MUC dry-run data as the 
held-out development data and the MUC formal test 
data as the held-out test data.  
For both MUC-6 and MUC-7 NE tasks, Table 7 
shows the performance of our system using MUC 
evaluation while Figure 1 gives the comparisons of 
our system with others. Here, the precision (P) 
measures the number of correct NEs in the answer 
file over the total number of NEs in the answer file 
and the recall (R) measures the number of correct 
NEs in the answer file over the total number of NEs 
in the key file while F-measure is the weighted 
harmonic mean of precision and recall: 
PR
RPF
+
+
= 2
2 )1(
?
?
 with 2? =1. It shows that the 
performance is significantly better than reported by 
any other machine-learning system. Moreover, the 
performance is consistently better than those based 
on handcrafted rules. 
Statistics 
(KB) 
Training 
Data 
Dry Run 
Data 
Formal Test 
Data 
MUC-6 1330 121 124 
MUC-7 708 156 561 
Table 6: Statistics of Data from MUC-6  
and MUC-7 NE Tasks 
 F P R 
MUC-6 96.6 96.3 96.9 
MUC-7 94.1 93.7 94.5 
Table 7: Performance of our System on MUC-6 
and MUC-7 NE Tasks 
Composition F P R 
1ff =  77.6 81.0 74.1 
21 fff =  87.4 88.6 86.1 
321 ffff =  89.3 90.5 88.2 
421 ffff =  92.9 92.6 93.1 
4321 fffff =  94.1 93.7 94.5 
Table 8: Impact of Different Sub-Features 
With any learning technique, one important 
question is how much training data is required to 
achieve acceptable performance. More generally 
how does the performance vary as the training data 
size changes? The result is shown in Figure 2 for 
MUC-7 NE task. It shows that 200KB of training 
data would have given the performance of 90% 
while reducing to 100KB would have had a 
significant decrease in the performance. It also 
shows that our system still has some room for 
performance improvement. This may be because of 
the complex word feature and the corresponding sparseness problem existing in our system.  
Figure 1: Comparison of our system with others 
on MUC-6 and MUC-7 NE tasks
80
85
90
95
100
80 85 90 95 100Recall
Pr
ec
is
io
n Our MUC-6 System
Our MUC-7 System
Other MUC-6 Systems
Other MUC-7 Syetems
Figure 2: Impact of Various Training Data on Performance
80
85
90
95
100
100 200 300 400 500 600 700 800
Training Data Size(KB)
F-
m
ea
su
re
MUC-7
Another important question is about the effect of 
different sub-features. Table 8 answers the question 
on MUC-7 NE task: 
1) Applying only 1f  gives our system the 
performance of 77.6%. 
2) 2f  is very useful for NER and increases the 
performance further by 10% to 87.4%.   
3) 4f  is impressive too with another 5.5% 
performance improvement.  
4)  However, 3f  contributes only further 1.2% to 
the performance. This may be because 
information included in 3f  has already been 
captured by 2f  and 4f . Actually, the 
experiments show that the contribution of 3f  
comes from where there is no explicit indicator 
information in/around the NE and there is no 
reference to other NEs in the macro context of 
the document. The NEs contributed by 3f  are 
always well-known ones, e.g. Microsoft, IBM 
and Bach (a composer), which are introduced in 
texts without much helpful context. 
6  Conclusion 
This paper proposes a HMM in that a new 
generative model, based on the mutual information 
independence assumption (2-3) instead of the 
conditional probability independence assumption 
(I-1) after Bayes' rule, is applied. Moreover, it 
shows that the HMM-based chunk tagger can 
effectively apply and integrate four different kinds 
of sub-features, ranging from internal word 
information to semantic information to NE 
gazetteers to macro context of the document, to 
capture internal and external evidences for NER 
problem. It also shows that our NER system can 
reach "near human performance". To our 
knowledge, our NER system outperforms any 
published machine-learning system and any 
published rule-based system.  
While the experimental results have been 
impressive, there is still much that can be done 
potentially to improve the performance. In the near 
feature, we would like to incorporate the following 
into our system: 
? List of domain and application dependent person, 
organization and location names. 
? More effective name alias algorithm.  
? More effective strategy to the back-off modeling 
and smoothing. 
References 
[Aberdeen+95] J. Aberdeen, D. Day, L. 
Hirschman, P. Robinson and M. Vilain. MITRE: 
Description of the Alembic System Used for 
MUC-6. MUC-6. Pages141-155. Columbia, 
Maryland. 1995. 
[Aone+98] C. Aone, L. Halverson, T. Hampton, 
M. Ramos-Santacruz. SRA: Description of the IE2 
System Used for MUC-7. MUC-7. Fairfax, Virginia. 
1998. 
[Bennett+96] S.W. Bennett, C. Aone and C. 
Lovell. Learning to Tag Multilingual Texts 
Through Observation. EMNLP'1996. Pages109-116. 
Providence, Rhode Island. 1996. 
[Bikel+99] Daniel M. Bikel, Richard Schwartz 
and Ralph M. Weischedel. An Algorithm that 
Learns What's in a Name. Machine Learning 
(Special Issue on NLP). 1999. 
[Borthwick+98] A. Borthwick, J. Sterling, E. 
Agichtein, R. Grishman. NYU: Description of the 
MENE Named Entity System as Used in MUC-7. 
MUC-7. Fairfax, Virginia. 1998. 
[Borthwick99] Andrew Borthwick. A Maximum 
Entropy Approach to Named Entity Recognition. 
Ph.D. Thesis. New York University. September, 
1999. 
[Brill95] Eric Brill. Transform-based 
Error-Driven Learning and Natural Language 
Processing: A Case Study in Part-of-speech 
Tagging. Computational Linguistics 21(4). 
Pages543-565. 1995. 
[Chinchor95a] Nancy Chinchor. MUC-6 Named 
Entity Task Definition (Version 2.1). MUC-6. 
Columbia, Maryland. 1995. 
[Chinchor95b] Nancy Chinchor. Statistical 
Significance of MUC-6 Results. MUC-6. Columbia, 
Maryland. 1995. 
[Chinchor98a] Nancy Chinchor. MUC-7 Named 
Entity Task Definition (Version 3.5). MUC-7. 
Fairfax, Virginia. 1998. 
[Chinchor98b] Nancy Chinchor. Statistical 
Significance of MUC-7 Results. MUC-7. Fairfax, 
Virginia. 1998. 
[Humphreys+98] K. Humphreys, R. Gaizauskas, 
S. Azzam, C. Huyck, B. Mitchell, H. Cunningham, 
Y. Wilks. Univ. of Sheffield: Description of the 
LaSIE-II System as Used for MUC-7. MUC-7. 
Fairfax, Virginia. 1998. 
[Krupka+98]  G. R. Krupka, K. Hausman. 
IsoQuest Inc.: Description of the NetOwlTM 
Extractor System as Used for MUC-7. MUC-7. 
Fairfax, Virginia. 1998. 
[McDonald96] D. McDonald. Internal and 
External Evidence in the Identification and 
Semantic Categorization of Proper Names. In B. 
Boguraev and J. Pustejovsky editors: Corpus 
Processing for Lexical Acquisition. Pages21-39. 
MIT Press. Cambridge, MA. 1996. 
[Miller+98] S. Miller, M. Crystal, H. Fox, L. 
Ramshaw, R. Schwartz, R. Stone, R. Weischedel, 
and the Annotation Group. BBN: Description of the 
SIFT System as Used for MUC-7. MUC-7. Fairfax, 
Virginia. 1998. 
[Mikheev+98] A. Mikheev, C. Grover, M. 
Moens. Description of the LTG System Used for 
MUC-7. MUC-7. Fairfax, Virginia. 1998. 
[Mikheev+99] A. Mikheev, M. Moens, and C. 
Grover. Named entity recognition without gazeteers. 
EACL'1999. Pages1-8. Bergen, Norway. 1999.  
[MUC6] Morgan Kaufmann Publishers, Inc. 
Proceedings of the Sixth Message Understanding 
Conference (MUC-6). Columbia, Maryland. 1995. 
[MUC7] Morgan Kaufmann Publishers, Inc. 
Proceedings of the Seventh Message Understanding 
Conference (MUC-7). Fairfax, Virginia. 1998. 
[Rabiner89] L. Rabiner. A Tutorial on Hidden 
Markov Models and Selected Applications in 
Speech Recognition?. IEEE 77(2). Pages257-285. 
1989. 
[Sekine98] Satoshi Sekine. Description of the 
Japanese NE System Used for MET-2. MUC-7. 
Fairfax, Virginia. 1998. 
[Tjong+00] Erik F. Tjong Kim Sang and Sabine 
Buchholz. Introduction to the CoNLL-2000 Shared 
Task: Chunking. CoNLL'2000. Pages127-132. 
Lisbon, Portugal. 11-14 Sept 2000. 
[Viterbi67] A. J. Viterbi. Error Bounds for 
Convolutional Codes and an Asymptotically 
Optimum Decoding Algorithm. IEEE Transactions 
on Information Theory. IT(13). Pages260-269, 
April 1967. 
[Watson+92] B. Watson and Tsoi A Chunk. 
Second Order Hidden Markov Models for Speech 
Recognition?. Proceeding of 4th Australian 
International Conference on Speech Science and 
Technology. Pages146-151. 1992. 
 [Yu+98] Yu Shihong, Bai Shuanhu and Wu 
Paul. Description of the Kent Ridge Digital Labs 
System Used for MUC-7. MUC-7. Fairfax, Virginia. 
1998. 
 [Zhou+00] Zhou GuoDong, Su Jian and Tey 
TongGuan. Hybrid Text Chunking. CoNLL'2000. 
Pages163-166. Lisbon, Portugal, 11-14 Sept 2000. 
[Zhou+00b] Zhou GuoDong and Su Jian, 
Error-driven HMM-based Chunk Tagger with 
Context-dependent Lexicon. EMNLP/ VLC'2000. 
Hong Kong, 7-8 Oct 2000. 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 188?196,
Beijing, August 2010
A Twin-Candidate Based Approach for Event Pronoun Resolution us-
ing Composite Kernel  
Chen Bin1 Su Jian2 Tan Chew Lim1 
1National University of Singapore 2Institute for Inforcomm Research, A-STAR 
{chenbin,tancl}@comp.nus.edu.sg sujian@i2r.a-star.edu.sg 
 
Abstract 
Event Anaphora Resolution is an important 
task for cascaded event template extraction 
and other NLP study. In this paper, we provide 
a first systematic study of resolving pronouns 
to their event verb antecedents for general 
purpose. First, we explore various positional, 
lexical and syntactic features useful for the 
event pronoun resolution. We further explore 
tree kernel to model structural information 
embedded in syntactic parses. A composite 
kernel is then used to combine the above di-
verse information. In addition, we employed a 
twin-candidate based preferences learning 
model to capture the pair wise candidates? pre-
ference knowledge. Besides we also look into 
the incorporation of the negative training in-
stances with anaphoric pronouns whose ante-
cedents are not verbs. Although these negative 
training instances are not used in previous 
study on anaphora resolution, our study shows 
that they are very useful for the final resolu-
tion through random sampling strategy. Our 
experiments demonstrate that it?s meaningful 
to keep certain training data as development 
data to help SVM select a more accurate hyper 
plane which provides significant improvement 
over the default setting with all training data. 
1 Introduction 
Anaphora resolution, the task of resolving a giv-
en text expression to its referred expression in 
prior texts, is important for intelligent text 
processing systems. Most previous works on 
anaphora resolution mainly aims at object ana-
phora in which both the anaphor and its antece-
dent are mentions of the same real world objects 
In contrast, an event anaphora as first defined 
in (Asher, 1993) is an anaphoric reference to an 
event, fact, and proposition which is representa-
tive of eventuality and abstract entity. Consider 
the following example: 
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The anaphor [It]2 in the above example refers 
back to an event, ?all-white and all-Christian city 
of Postville is diluted by different ethnic groups.? 
Here, we take the main verb of the event, [in-
vaded]1 as the representation of this event and 
the antecedent for pronoun [It]2.  
According to (Asher, 1993), antecedents of 
event pronoun include both gerunds (e.g. de-
struction) and inflectional verbs (e.g. destroying). 
In our study, we focus on the inflectional verb 
representation, as the gerund representation is 
studied in the conventional anaphora resolution. 
For the rest of this paper, ?event pronouns? are 
pronouns whose antecedents are event verbs 
while ?non-event anaphoric pronouns? are those 
with antecedents other than event verbs. 
 Entity anaphora resolution provides critical 
links for cascaded event template extraction. It 
also provides useful information for further infe-
rence needed in other natural language 
processing tasks such as discourse relation and 
entailment. Event anaphora (both pronouns and 
noun phrases) contributes a significant propor-
tion in anaphora corpora, such as OntoNotes. 
19.97% of its total number of entity chains con-
tains event verb mentions. 
In (Asher, 1993) chapter 6, a method to re-
solve references to abstract entities using dis-
course representation theory is discussed. How-
ever, no computation system was proposed for 
entity anaphora resolution. (Byron, 2002) pro-
posed semantic filtering as a complement to sa-
lience calculations to resolve event pronoun tar-
geted by us. This knowledge deep approach only 
188
works for much focused domain like trains spo-
ken dialogue with handcraft knowledge of rele-
vant events for only limited number of verbs in-
volved.  Clearly, this approach is not suitable for 
general event pronoun resolution say in news 
articles. Besides, there?s also no specific perfor-
mance report on event pronoun resolution, thus 
it?s not clear how effective their approach is. 
(M?ller, 2007) proposed pronoun resolution sys-
tem using a set of hand-crafted constraints such 
as ?argumenthood? and ?right-frontier condition? 
together with logistic regression model based on 
corpus counts. The event pronouns are resolved 
together with object pronouns. This explorative 
work produced an 11.94% F-score for event pro-
noun resolution which demonstrated the difficul-
ty of event anaphora resolution. In (Pradhan, 
et.al, 2007), a general anaphora resolution sys-
tem is applied to OntoNotes corpus. However, 
their set of features is designed for object ana-
phora resolution. There is no specific perfor-
mance reported on event anaphora. We suspect 
the event pronouns are not correctly resolved in 
general as most of these features are irrelevant to 
event pronoun resolution.  
In this paper, we provide the first systematic 
study on pronominal references to event antece-
dents. First, we explore various positional, lexi-
cal and syntactic features useful for event pro-
noun resolution, which turns out quite different 
from conventional pronoun resolution except 
sentence distance information. These have been 
used together with syntactic structural informa-
tion using a composite kernel. Furthermore, we 
also consider candidates? preferences informa-
tion using twin-candidate model. 
Besides we further look into the incorporation 
of negative instances from non-event anaphoric 
pronoun, although these instances are not used in 
previous study on co-reference or anaphora reso-
lution as they make training instances extremely 
unbalanced. Our study shows that they can be 
very useful for the final resolution after random 
sampling strategy.  
We further demonstrate that it?s meaningful to 
keep certain training data as development data to 
help SVM select a more accurate hyper-plane 
which provide significant improvement over the 
default setting with all training data.  
The rest of this paper is organized as follows.  
Section 2 introduces the framework for event 
pronoun resolution, the considerations on train-
ing instance, the various features useful for event 
pronoun resolution and SVM classifier with ad-
justment of hyper-plane. Twin-candidate model 
is further introduced to capture the preferences 
among candidates. Section 3 presents in details 
the structural syntactic feature and the kernel 
functions to incorporate such a feature in the res-
olution. Section 4 presents the experiment results 
and some discussion. Section 5 concludes the 
paper. 
2 The Resolution Framework 
Our event-anaphora resolution system adopts the 
common learning-based model for object ana-
phora resolution, as employed by (Soon et al, 
2001) and (Ng and Cardie, 2002a). 
2.1 Training and Testing instance 
In the learning framework, training or testing 
instance of the resolution system has a form of 
               where        is the i
th candi-
date of the antecedent of anaphor    . An in-
stance is labeled as positive if        is the ante-
cedent of      , or negative if        is not the 
antecedent of     . An instance is associated 
with a feature vector which records different 
properties and relations between     and       . 
The features used in our system will be discussed 
later in this paper.  
During training, for each event pronoun, we 
consider the preceding verbs in its current and 
previous two sentences as its antecedent candi-
dates. A positive instance is formed by pairing an 
anaphor with its correct antecedent. And a set of 
negative instances is formed by pairing an ana-
phor with its candidates other than the correct 
antecedent. In addition, more negative instances 
are generated from non-event anaphoric pro-
nouns. Such an instance is created by pairing up 
a non-event anaphoric pronoun with each of the 
verbs within the pronoun?s sentence and previous 
two sentences. This set of instances from non-
event anaphoric pronouns is employed to provide 
extra power on ruling out non-event anaphoric 
pronouns during resolution. This is inspired by 
the fact that event pronouns are only 14.7% of all 
the pronouns in the OntoNotes corpus. Based on 
these generated training instances, we can train a 
binary classifier using any discriminative learn-
ing algorithm. 
189
The natural distribution of textual data is of-
ten imbalanced. Classes with fewer examples are 
under-represented and classifiers often perform 
far below satisfactory. In our study, this becomes 
a significant issue as positive class (event ana-
phoric) is the minority class in pronoun resolu-
tion task. Thus we utilize a random down sam-
pling method to reduce majority class samples to 
an equivalent level with the minority class sam-
ples which is described in (Kubat and Matwin, 
1997) and (Estabrooks et al 2004). In (Ng and 
Cardie, 2002b), they proposed a negative sample 
selection scheme which included only negative 
instances found in between an anaphor and its 
antecedent. However, in our event pronoun reso-
lution, we are distinguishing the event-anaphoric 
from non-event anaphoric which is different 
from (Ng and Cardie, 2002b). 
2.2 Feature Space 
In a conventional pronoun resolution, a set of 
syntactic and semantic knowledge has been re-
ported as in (Strube and M?ller, 2003; Yang et al 
2004;2005a;2006). These features include num-
ber agreement, gender agreement and many oth-
ers. However, most of these features are not use-
ful for our task, as our antecedents are inflection-
al verbs instead of noun phrases. Thus we have 
conducted a study on effectiveness of potential 
positional, lexical and syntactic features. The 
lexical knowledge is mainly collected from cor-
pus statistics. The syntactic features are mainly 
from intuitions. These features are purposely en-
gineered to be highly correlated with positive 
instances. Therefore such kind of features will 
contribute to a high precision classifier.  
? Sentence Distance 
This feature measures the sentence distance be-
tween an anaphor and its antecedent candidate 
under the assumptions that a candidate in the 
closer sentence to the anaphor is preferred to be 
the antecedent. 
? Word Distance  
This feature measures the word distance between 
an anaphor and its antecedent candidate. It is 
mainly to distinguish verbs from the same sen-
tence. 
? Surrounding Words and POS Tags 
The intuition behind this set of features is to find 
potential surface words that occur most frequent-
ly with the positive instances. Since most of 
verbs occurred in front of pronoun, we have built 
a frequency table from the preceding 5 words of 
the verb to succeeding 5 surface words of the 
pronoun. After the frequency table is built, we 
select those words with confidence1  > 70% as 
features. Similar to Surrounding Words, we have 
built a frequency table to select indicative sur-
rounding POS tags which occurs most frequently 
with positive instances. 
? Co-occurrences of Surrounding Words 
The intuition behind this set of features is to cap-
ture potential surface patterns such as ?It 
caused?? and ?It leads to?. These patterns are 
associated with strong indication that pronoun 
?it? is an event pronoun. The range for the co-
occurrences is from preceding 5 words to suc-
ceeding 5 words. All possible combinations of 
word positions are used for a co-occurrence 
words pattern. For example ?it leads to? will 
generate a pattern as ?S1_S2_lead_to? where S1 
and S2 mean succeeding position 1 and 2. Simi-
lar to previous surrounding words, we will con-
duct corpus statistics analysis and select co-
occurrence patterns with a confidence greater 
than 70%. Following the same process, we have 
examined co-occurrence patterns for surrounding 
POS tags.  
? Subject/Object Features 
This set of features aims to capture the relative 
position of the pronoun in a sentence. It denotes 
the preference of pronoun?s position at the clause 
level. There are 4 features in this category as 
listed below. 
Subject of Main Clause 
This feature indicates whether a pronoun is at the 
subject position of a main clause. 
Subject of Sub-clause 
This feature indicates whether a pronoun is at the 
subject position of a sub-clause. 
Object of Main Clause 
This feature indicates whether a pronoun is at the 
object position of a main clause. 
Object of Sub-clause 
This feature indicates whether a pronoun is at the 
object position of a sub-clause. 
? Verb of Main/Sub Clause 
Similar to the Subject/Object features of pro-
noun, the following two features capture the rela-
                                                 
1               
                                        
                    
 
190
tive position of a verb in a sentence. It encodes 
the preference of verb position between main 
verbs in main/sub clauses. 
Main Verb in Main Clause 
This feature indicates whether a verb is a main 
verb in a main clause. 
Main Verb in Sub-clause 
This feature indicates whether a verb is a main 
verb in a sub-clause. 
2.3 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn a classifier for pronoun res-
olution. In our study, we use Support Vector Ma-
chine (Vapnik, 1995) to allow the use of kernels 
to incorporate the structure feature. One advan-
tage of SVM is that we can use tree kernel ap-
proach to capture syntactic parse tree information 
in a particular high-dimension space. 
Suppose a training set   consists of labeled 
vectors          , where    is the feature vector 
of a training instance and    is its class label. The 
classifier learned by SVM is: 
                     
   
  
where    is the learned parameter for a support 
vector   . An instance   is classified as positive 
if       . Otherwise,   is negative. 
? Adjust Hyper-plane with Development Data 
Previous works on pronoun resolution such as 
(Yang et al 2006) used the default setting for 
hyper-plane which sets       . And an in-
stance is positive if        and negative oth-
erwise. In our study, we look into a method of 
adjusting the hyper-plane?s position using devel-
opment data to improve the classifier?s perfor-
mance.  
Considering a default model setting for SVM 
as shown in Figure 2(for illustration purpose, we 
use a 2-D example). 
 
Figure 2: 2-D SVM Illustration 
The objective of SVM learning process is to find 
a set of weight vector   which maximizes the 
margin (defined as  
   
) with constraints defined 
by support vectors. The separating hyper-plane is 
given by         as bold line in the center. 
The margin is the region between the two dotted 
lines (bounded by         and     
    ). The margin is a space without any in-
formation from training instances. The actual 
hyper-plane may fall in any place within the 
margin. It does not necessarily occur in the. 
However, the hyper-plane is used to separate 
positive and negative instances during classifica-
tion process without consideration of the margin. 
Thus if an instance falls in the margin, SVM can 
only decide class label from hyper-plane which 
may cause misclassification in the margin. 
 Based on the previous discussion, we propose 
an adjustment of the hyper-plane using develop-
ment data. For simplicity, we adjust the hyper-
plane function value instead of modeling the 
function itself. The hyper-plane function value 
will be further referred as a threshold  . The fol-
lowing is a modified version of a learned SVM 
classifier. 
        
                          
   
   
                         
   
   
  
where   is the threshold,    is the learned para-
meter for a feature    and    is its class label. A 
set of development data is used to adjust the hy-
per-plane function threshold   in order to max-
imize the accuracy of the learned SVM classifier 
on development data. The adjustment of hyper-
plane is defined as: 
                            
   
  
where        is an indicator function which out-
put 1 if       is same sign as   and 0 otherwise. 
Thereafter, the learned threshold    is applied to 
the testing set. 
3 Incorporating Structural Syntactic In-
formation 
A parse tree that covers a pronoun and its ante-
cedent candidate could provide us much syntac-
tic information related to the pair which is expli-
citly or implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to what degree 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. The value returned from tree kernel reflects 
similarity between two instances in syntax. Such 
191
syntactic similarity can be further combined with 
other knowledge to compute overall similarity 
between two instances, through a composite ker-
nel. Normally, parsing is done at sentence level. 
However, in many cases a pronoun and its ante-
cedent candidate do not occur in the same sen-
tence. To present their syntactic properties and 
relations in a single tree structure, we construct a 
syntax tree for an entire text, by attaching the 
parse trees of all its sentences to an upper node. 
Having obtained the parse tree of a text, we shall 
consider how to select the appropriate portion of 
the tree as the structured feature for a given in-
stance. As each instance is related to a pronoun 
and a candidate, the structured feature at least 
should be able to cover both of these two expres-
sions. 
3.1 Structural Syntactic Feature 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information that comes from parsing errors 
would likely be introduced. In our study, we ex-
amine three possible structured features that con-
tain different substructures of the parse tree: 
 
? Minimum Expansion Tree 
This feature records the minimal structure cover-
ing both pronoun and its candidate in parse tree. 
It only includes the nodes occurring in the short-
est path connecting the pronoun and its candidate, 
via the nearest commonly commanding node.  
When the pronoun and candidate are from differ-
ent sentences, we will find a path through pseudo 
?TOP? node which links all the parse trees. Con-
sidering the example given in section 1,  
This was an all-white, all-Christian community 
that all the sudden was taken over -- not taken 
over, that's a very bad choice of words, but [in-
vaded]1 by, perhaps different groups. 
[It]2 began when a Hasidic Jewish family bought 
one of the town's two meat-packing plants 13 
years ago. 
The minimum expansion structural feature of the 
instance {invaded, it} is annotated with bold 
lines and shaded nodes in figure 1.  
? Simple Expansion Tree 
Minimum-Expansion could, to some degree, de-
scribe the syntactic relationships between the 
candidate and pronoun. However, it is incapable 
of capturing the syntactic properties of the can-
didate or the pronoun, because the tree structure 
surrounding the expression is not taken into con-
sideration. To incorporate such information, fea-
ture Simple-Expansion not only contains all the 
nodes in Minimum-Expansion, but also includes 
the first-level children of these nodes2 except the 
punctuations. The simple-expansion structural 
feature of instance {invaded, it} is annotated in 
figure 2. In the left sentence?s tree, the node ?NP? 
for ?perhaps different groups? is terminated to 
provide a clue that we have a noun phrase at the 
object position of the candidate verb. 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 1: Minimum-Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 2: Simple Expansion Tree 
It began when a .
PRP VBD WRB DT
?...
.
NP
WHADVP
SBAR
NP
VP
S
VP
S
?...
TOP
S
.
.
groupsdifferentperhaps,invadedbutwasThis ?...
DT
NP VP
VBD NNSJJRB,VBNCC
NP
S
VP
VP PP NP
ADVP
by
IN
 
Figure 3: Full-Expansion Tree 
? Full Expansion Tree 
This feature focuses on the whole tree structure 
between the candidate and pronoun. It not only 
includes all the nodes in Simple-Expansion, but 
also the nodes (beneath the nearest commanding 
parent) that cover the words between the candi-
                                                 
2 If the pronoun and the candidate are not in the same sen-
tence, we will not include the nodes denoting the sentences 
before the candidate or after the pronoun. 
192
date and the pronoun3. Such a feature keeps the 
most information related to the pronoun and can-
didate pair. Figure 3 shows the structure for fea-
ture full-expansion for instance {invaded, it}. As 
illustrated, the ?NP? node for ?perhaps different 
groups? is further expanded to the POS level. All 
its child nodes are included in the full-expansion 
tree except the surface words. 
3.2 Convolution Parse Tree Kernel and Com-
posite Kernel 
To calculate the similarity between two struc-
tured features, we use the convolution tree kernel 
that is defined by Collins and Duffy (2002) and 
Moschitti (2004). Given two trees, the kernel 
will enumerate all their sub-trees and use the 
number of common sub-trees as the measure of 
similarity between two trees. The above tree ker-
nel only aims for the structured feature. We also 
need a composite kernel to combine the struc-
tured feature and the flat features from section 
2.2. In our study we define the composite kernel 
as follows: 
             
            
              
 
            
              
 
where       is the convolution tree kernel de-
fined for the structured feature, and       is the 
kernel applied on the flat features. Both kernels 
are divided by their respective length4 for norma-
lization. The new composite kernel      , de-
fined as the sum of normalized       and      , 
will return a value close to 1 only if both the 
structured features and the flat features have high 
similarity under their respective kernels. 
3.3 Twin-Candidate Framework using Rank-
ing SVM Model 
In a ranking SVM kernel as described in (Mo-
schitti et al 2006) for Semantic Role Labeling, 
two argument annotations (as argument trees) are 
presented to the ranking SVM model to decide 
which one is better.  In our case, we present two 
syntactic trees from two candidates to the rank-
ing SVM model. The idea is inspired by (Yang, 
et.al, 2005b;2008). The intuition behind the 
twin-candidate model is to capture the informa-
tion of how much one candidate is more pre-
                                                 
3 We will not expand the nodes denoting the sentences other 
than where the pronoun and the candidate occur. 
4  The length of a kernel   is defined as            
                   
ferred than another. The candidate wins most of 
the pair wise comparisons is selected as antece-
dent. 
The feature vector for each training instance 
has a form of                    . An in-
stance is positive if       is a better antecedent 
choice than       . Otherwise, it is a negative 
instance. For each feature vector, both tree struc-
tural features and flat features are used.  Thus 
each feature vector has a form of    
              where    and    are trees of candi-
date i and j respectively,    and    are flat feature 
vectors of candidate i and j respectively.  
In the training instances generation, we only 
generate those instances with one candidate is 
the correct antecedent. This follows the same 
strategy used in (Yang et al 2008) for object 
anaphora resolution. 
In the resolution process, a list of m candi-
dates is extracted from a three sentences window. 
A total of  
 
 
  instances are generated by pairing-
up the m candidates pair-wisely. We used a 
Round-Robin scoring scheme for antecedent se-
lection. Suppose a SVM output for an instance 
                   is 1, we will give a score 
1 for        and -1 for        and vice versa. At 
last, the candidate with the highest score is se-
lected as antecedent. In order to handle a non-
event anaphoric pronoun, we have set a threshold 
to distinguish event anaphoric from non-event 
anaphoric. A pronoun is considered as event 
anaphoric if its score is above the threshold. In 
our experiments, we kept a set of development 
data to find out the threshold in an empirical way. 
4 Experiments and Discussions 
4.1 Experimental Setup 
OntoNotes Release 2.0 English corpus as in 
(Hovy et al 2006) is used in our study, which 
contains 300k words of English newswire data 
(from the Wall Street Journal) and 200k words of 
English broadcast news data (from ABC, CNN, 
NBC, Public Radio International and Voice of 
America).  Table 1 shows the distribution of var-
ious entities. We focused on the resolution of 
502 event pronouns encountered in the corpus. 
The resolution system has to handle both the 
event pronoun identification and antecedent se-
lection tasks. To illustrate the difficulty of event 
pronoun resolution, 14.7% of all pronoun men-
tions are event anaphoric and only 31.5% of 
193
event pronoun can be resolved using ?most re-
cent verb? heuristics. Therefore a most-recent-
verb baseline will yield an f-score 4.63%. 
To conduct event pronoun resolution, an input 
raw text was preprocessed automatically by a 
pipeline of NLP components. The noun phrase 
identification and the predicate-argument extrac-
tion were done based on Stanford Parser (Klein 
and Manning, 2003a;b) with F-score of 86.32% 
on Penn Treebank corpus.  
Non-Event Anaphora:        4952   80.03% 
Event  
Anaphora: 
1235  
19.97% 
Event NP:        733   59.35% 
Event  
Pronoun: 
502   40.65% 
It:       29.0% 
This:   16.9% 
That:  54.1% 
Table 1: The distribution of various types of 6187 
anaphora in OntoNotes 2.0 
For each pronoun encountered during resolu-
tion, all the inflectional verbs within the current 
and previous two sentences are taken as candi-
dates. For the current sentence, we take only 
those verbs in front of the pronoun. On average, 
each event pronoun has 6.93 candidates. Non-
event anaphoric pronouns will generate 7.3 nega-
tive instances on average.  
4.2 Experiment Results and Discussion 
In this section, we will present our experimental 
results with discussions. The performance meas-
ures we used are precision, recall and F-score. 
All the experiments are done with a 10-folds 
cross validation. In each fold of experiments, the 
whole corpus is divided into 10 equal sized por-
tions. One of them is selected as testing corpus 
while the remaining 9 are used for training. In 
experiments with development data, 1 of the 9 
training portions is kept for development purpose. 
In case of statistical significance test for differ-
ences is needed, a two-tailed, paired-sample Stu-
dent?s t-Test is performed at 0.05 level of signi-
ficance. 
In the first set of experiments, we are aiming 
to investigate the effectiveness of each single 
knowledge source. Table 2 reports the perfor-
mance of each individual experiment. The flat 
feature set yields a baseline system with 40.6% f-
score. By using each tree structure along, we can 
only achieve a performance of 44.4% f-score 
using the minimum-expansion tree. Therefore, 
we will further investigate the different ways of 
combining flat and syntactic structure knowledge 
to improve resolution performances. 
 Precision Recall F-score 
Flat 0.406 0.406 0.406 
Min-Exp 0.355 0.596 0.444 
Simple-Exp 0.347 0.512 0.414 
Full-Exp 0.323 0.476 0.385 
Table 2: Contribution from Single Knowledge Source 
The second set of experiments is conducted to 
verify the performances of various tree structures 
combined with flat features. The performances 
are reported in table 3. Each experiment is re-
ported with two performances. The upper one is 
done with default hyper-plane setting. The lower 
one is done using the hyper-plane adjustment as 
we discussed in section 2.3. 
 Precision Recall F-score 
Min-Exp + 
Flat 
0.433 0.512 0.469 
(0.727) (0.446) (0.553) 
Simple-Exp 
+Flat 
0.423 0.534 0.472 
(0.652) (0.492) (0.561) 
Full-Exp + 
Flat 
0.416 0.526 0.465 
(0.638) (0.496) (0.558) 
Table 3: Comparison of Different Tree Structure +Flat 
As table 3 shows, minimum-expansion gives 
highest precision in both experiment settings. 
Minimum-expansion emphasizes syntactic struc-
tures linking the anaphor and antecedent. Al-
though using only the syntactic path may lose the 
contextual information, but it also prune out the 
potential noise within the contextual structures. 
In contrast, the full-expansion gives the highest 
recall. This is probably due to the widest know-
ledge coverage provides by the full-expansion 
syntactic tree. As a trade-off, the precision of 
full-expansion is the lowest in the experiments. 
One reason for this may be due to OntoNotes 
corpus is from broadcasting news domain. Its 
texts are less-formally structured. Another type 
of noise is that a narrator of news may read an 
abnormally long sentence. It should appear as 
several separate sentences in a news article. 
However, in broadcasting news, these sentences 
maybe simply joined by conjunction word ?and?. 
Thus a very nasty and noisy structure is created 
from it. Comparing the three knowledge source, 
simple-expansion achieves moderate precision 
and recall which results in the highest f-score. 
From this, we can draw a conclusion that simple-
expansion achieves a balance between the indica-
tive structural information and introduced noises. 
In the next set of experiments, we will com-
pare different setting for training instances gen-
eration. A typical setting contains no negative 
194
instances generated from non-event anaphoric 
pronoun. This is not an issue for object pronoun 
resolution as majority of pronouns in an article is 
anaphoric. However in our case, the event pro-
noun consists of only 14.7% of the total pro-
nouns in OntoNotes. Thus we incorporate the 
instances from non-event pronouns to improve 
the precision of the classifier. However, if we 
include all the negative instances from non-event 
anaphoric pronouns, the positive instances will 
be overwhelmed by the negative instances. A 
down sampling is applied to the training in-
stances to create a more balanced class distribu-
tion. Table 4 reports various training settings 
using simple-expansion tree structure.  
Simple-Exp Tree Precision Recall F-score 
Without Non-
event Negative 
0.423 0.534 0.472 
Incl. All Negative 0.733 0.410 0.526 
Balanced Negative 0.599 0.506 0.549 
Development Data 0.652 0.492 0.561 
Table 4: Comparison of Training Setup, Simple-Exp 
In table 4, the first line is experiment without 
any negative instances from non-event pronouns. 
The second line is the performance with all nega-
tive instances from non-event pronouns. Third 
line is performance using a balanced training set 
using down sampling. The last line is experiment 
using hyper-plane adjustment. The first line 
gives the highest recall measure because it has no 
discriminative knowledge on non-event anaphor-
ic pronoun. The second line yields the highest 
precision which complies with our claim that 
including negative instances from non-event 
pronouns will improve precision of the classifier 
because more discriminative power is given by 
non-event pronoun instances. The balanced train-
ing set achieves a better f-score comparing to 
models with no/all negative instances. This is 
because balanced training set provides a better 
weighted positive/negative instances which im-
plies a balanced positive/negative knowledge 
representation. As a result of that, we achieve a 
better balanced f-score. In (Ng and Cardie, 
2002b), they concluded that only the negative 
instances in between the anaphor and antecedent 
are useful in the resolution. It is same as our 
strategy without negative instances from non-
event anaphoric pronouns. However, our study 
showed an improvement by adding in negative 
instances from non-event anaphoric pronouns as 
showed in table 4. This is probably due to our 
random sampling strategy over the negative in-
stances near to the event anaphoric instances. It 
empowers the system with more discriminative 
power. The best performance is given by the hy-
per-plane adaptation model. Although the num-
ber of training instances is further reduced for 
development data, we can have an adjustment of 
the hyper-plane which is more fit to dataset.  
In the last set of experiments, we will present 
the performance from the twin-candidates based 
approach in table 5. The first line is the best per-
formance from single candidate system with hy-
per-plane adaptation. The second line is perfor-
mance using the twin-candidates approach. 
Simple-Exp Tree Precision Recall F-score 
Single Candidate 0.652 0.492 0.561 
Twin-Candidates 0.626 0.540 0.579 
Table 5: Single vs. Twin Candidates, Simple-Exp 
Comparing to the single candidate model, the 
recall is significantly improved with a small 
trade-off in precision. The difference in results is 
statistically significant using t-test at 5% level of 
significance. It reinforced our intuition that pre-
ferences between two candidates are contributive 
information sources in co-reference resolution.  
5 Conclusion and Future Work 
The purpose of this paper is to conduct a syste-
matic study of the event pronoun resolution. We 
propose a resolution system utilizing a set of flat 
positional, lexical and syntactic feature and 
structural syntactic feature. The state-of-arts 
convolution tree kernel is used to extract indica-
tive structural syntactic knowledge. A twin-
candidates preference learning based approach is 
incorporated to reinforce the resolution system 
with candidates? preferences knowledge. Last but 
not least, we also proposed a study of the various 
incorporations of negative training instances, 
specially using random sampling to handle the 
imbalanced data. Development data is also used 
to select more accurate hyper-plane in SVM for 
better determination. 
To further our research work, we plan to em-
ploy more semantic information into the system 
such as semantic role labels and verb frames.  
Acknowledgment 
We would like to thank Professor Massimo Poesio 
from University of Trento for the initial discussion of 
this work. 
195
References  
N. Asher. 1993. Reference to Abstract Objects in Dis-
course. Kluwer Academic Publisher. 1993. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer.1995. 
M. Kubat and S. Matwin, 1997. Addressing the curse 
of imbalanced data set: One sided sampling. In 
Proceedings of the Fourteenth International Con-
ference on Machine Learning,1997. pg179?186. 
T. Joachims. 1999. Making large-scale svm learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press.1999. 
W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun 
phrases. In Computational Linguistics, Vol:27(4), 
pg521? 544. 
D. Byron. 2002. Resolving Pronominal Reference to 
Abstract Entities, in Proceedings of the 40th An-
nual Meeting of the Association for Computational 
Linguistics (ACL?02). July 2002. , USA  
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?02). July 
2002. , USA 
V. Ng and C. Cardie. 2002a. Improving machine 
learning approaches to coreference resolution. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?02). 
July 2002. , USA. pg104?111. 
V. Ng, and C. Cardie. 2002b. Identifying anaphoric 
and non-anaphoric noun phrases to improve core-
ference resolution. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics (COLING02). (2002) 
M. Strube and C. M?ller. 2003. A Machine Learning 
Approach to Pronoun Resolution in Spoken Dialo-
gue. . In Proceedings of the 41st Annual Meeting of 
the Association for Computational Linguistics 
(ACL?03), 2003 
D. Klein and C. Manning. 2003a. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
D. Klein and C.Manning. 2003b. Accurate Unlexica-
lized Parsing. In Proceedings of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL?03), 2003.  pg423-430. 
X. Yang, G. Zhou, J. Su, and C.Tan. 2003. Corefe-
rence Resolution Using Competition Learning Ap-
proach. In Proceedings of the 41st Annual Meeting 
of the Association for Computational Linguistics 
(ACL?03), 2003. pg176?183. 
A. Moschitti. 2004. A study on convolution kernels 
for shallow semantic parsing. In Proceedings of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL?04), pg335?342. 
A. Estabrooks, T. Jo, and N. Japkowicz. 2004. A mul-
tiple resampling method for learning from imba-
lanced data sets. In Computational Intelligence  
Vol:20(1). pg18?36. 
X. Yang, J. Su, G. Zhou, and C. Tan. 2004. Improving 
pronoun resolution by incorporating coreferential 
information of candidates. In Proceedings of 42th 
Annual Meeting of the Association for Computa-
tional Linguistics, 2004. pg127?134. 
X. Yang, J. Su and C.Tan. 2005a. Improving Pronoun 
Resolution Using Statistics-Based Semantic Com-
patibility Information. In Proceedings of Proceed-
ings of the 43rd Annual Meeting of the Association 
for Computational Linguistics (ACL?05). June 
2005.  
X. Yang, J. Su and C.Tan. 2005b. A Twin-Candidates 
Model for Coreference Resolution with Non-
Anaphoric Identification Capability. In Proceed-
ings of IJCNLP-2005. Pp. 719-730, 2005 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. 
Weischedel. 2006. OntoNotes: The 90\% Solution. 
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, 2006 
X. Yang, J. Su and C.Tan. 2006. Kernel-Based Pro-
noun Resolution with Structured Syntactic Know-
ledge. In Proceedings of the 44th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?06). July 2006. Australia. 
A. Moschitti, Making tree kernels practical for natural 
language learning. In Proceedings EACL 2006, 
Trento, Italy, 2006. 
C. M?ller. 2007. Resolving it, this, and that in unre-
stricted multi-party dialog. In Proceedings of the 
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL?07). 2007.  Czech Re-
public. pg816?823. 
X. Yang, J. Su and C.Tan. 2008. A Twin-Candidates 
Model for Learning-Based Coreference Resolution. 
In Computational Linguistics, Vol:34(3). pg327-
356. 
S. Pradhan, L. Ramshaw, R. Weischedel, J. Mac-
Bride, and L. Micciulla. 2007. Unrestricted Corefe-
rence: Identifying Entities and Events in Onto-
Notes. In Proceedings of the IEEE International 
Conference on Semantic Computing (ICSC), Sep. 
2007. 
196
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1290?1298,
Beijing, August 2010
Entity Linking Leveraging
Automatically Generated Annotation 
Wei Zhang?    Jian Su? Chew Lim Tan?   Wen Ting Wang?
?School of Computing 
National University of Singapore 
{z-wei, tancl} 
@comp.nus.edu.sg
? Institute for Infocomm Research 
{sujian, wwang}
@i2r.a-star.edu.sg
Abstract
Entity linking refers entity mentions in a 
document to their representations in a 
knowledge base (KB). In this paper, we 
propose to use additional information 
sources from Wikipedia to find more 
name variations for entity linking task. In 
addition, as manually creating a training 
corpus for entity linking is labor-
intensive and costly, we present a novel 
method to automatically generate a large 
scale corpus annotation for ambiguous 
mentions leveraging on their unambi-
guous synonyms in the document collec-
tion. Then, a binary classifier is trained 
to filter out KB entities that are not simi-
lar to current mentions. This classifier 
not only can effectively reduce the am-
biguities to the existing entities in KB, 
but also be very useful to highlight the 
new entities to KB for the further popu-
lation. Furthermore, we also leverage on 
the Wikipedia documents to provide ad-
ditional information which is not availa-
ble in our generated corpus through a 
domain adaption approach which pro-
vides further performance improve-
ments.  The experiment results show that 
our proposed method outperforms the 
state-of-the-art approaches. 
1 Introduction 
The named entity (NE) ambiguation has raised 
serious problems in many areas, including web 
people search, knowledge base population 
(KBP), and information extraction, because an 
entity (such as Abbott Laboratories, a diversified 
pharmaceuticals health care company) can be 
referred to by multiple mentions (e.g. ?ABT? and 
?Abbott?), and a mention (e.g. ?Abbott?) can be 
shared by different entities (e.g. Abbott Texas: a 
city in United States; Bud Abbott, an American 
actor; and Abbott Laboratories, a diversified 
pharmaceutical health care company).  
Both Web People Search (WePS) task (Artiles 
et al 2007) and Global Entity Detection & Rec-
ognition task (GEDR) in Automatic Content Ex-
traction 2008 (ACE08) disambiguate entity men-
tions by clustering documents with these men-
tions. Each cluster then represents a unique enti-
ty. Recently entity linking has been proposed in 
this field. However, it is quite different from the 
previous tasks.
Given a knowledge base, a document collec-
tion, entity linking task as defined by KBP-091
(McNamee and Dang, 2009) is to determine for 
each name string and the document it appears, 
which knowledge base entity is being referred to, 
or if the entity is a new entity which is not 
present in the reference KB.  
Compared with GEDR and WePS, entity link-
ing has a given entity list (i.e. the reference KB) 
to which we disambiguate the entity mentions. 
Moreover, in document collection, there are new 
entities which are not present in KB and can be 
used for further population. In fact, new entities 
with or without the names in KB cover more 
than half of testing instances. 
1 http://apl.jhu.edu/~paulmac/kbp.html 
1290
Entity linking has been explored by several re-
searchers. Without any training data available, 
most of the previous work ranks the similarity 
between ambiguous mention and candidate enti-
ties through Vector Space Model (VSM). Since 
they always choose the entity with the highest 
rank as the answer, the ranking approaches hard-
ly detect a situation where there may be a new 
entity that is not present in KB. It is also difficult 
to combine bag of words (BOW) with other fea-
tures. For example, to capture the ?category? 
information, the method of Cucerzan (2007) in-
volves a complicated optimization issue and the 
approach has to be simplified for feasible com-
putation, which compromises the accuracy.  Be-
sides unsupervised methods, some supervised 
approaches (Agirre et al 2009, Li et al 2009 and 
McNamee et al 2009) also have been proposed 
recently for entity linking. However, the super-
vised approaches for this problem require large 
amount of training instances. But manually 
creating a corpus is labor-intensive and costly.  
In this paper, we explore how to solve the enti-
ty linking problem. We present a novel method 
that can automatically generate a large scale 
corpus for ambiguous mentions leveraging on 
their unambiguous synonyms in the document 
collection.  A binary classifier based on Support 
Vector Machine (SVM) is trained to filter out 
some candidate entities that are not similar to 
ambiguous mentions. This classifier can effec-
tively reduce the ambiguities to the existing enti-
ties in KB, and it is very useful to highlight the 
new entities to KB for the further population. 
We also leverage on the Wikipedia documents to 
provide additional information which is not 
available in our generated corpus through a do-
main adaption approach which provides further 
performance improvements. Besides, more in-
formation sources for finding more variations 
also contribute to the overall 22.9% accuracy 
improvements on KBP-09 test data over baseline. 
The remainder of the paper is organized as fol-
lows. Section 2 reviews related work for entity 
linking. In Section 3 we detail our algorithm in-
cluding name variation and entity disambigua-
tion. Section 4 describes the experimental setup 
and results. Finally, Section 5 concludes the pa-
per.
2 Related Work 
The crucial component of entity linking is the 
disambiguation process. Raphael et al (2007) 
report a disambiguation algorithm for geography. 
The algorithm ranks the candidates based on the 
manually assigned popularity scores in KB. The 
class with higher popularity will be assigned 
higher score. It causes that the rank of entities 
would never change, such as Lancaster (Califor-
nia) would always have a higher rank than Lan-
caster (UK) for any mentions. However, as the 
popularity scores for the classes change over 
time, it is difficult to accurately assign dynamic 
popularity scores. Cucerzan (2007) proposes a 
disambiguation approach based on vector space 
model for linking ambiguous mention in a doc-
ument with one entity in Wikipedia. The ap-
proach ranks the candidates and chooses the ent-
ity with maximum agreement between the con-
textual information extracted from Wikipedia 
and the context of a document, as well as the 
agreement among the category tags associated 
with the candidate entities. Nguyen and Cao 
(2008) refer the mentions in a document to KIM 
(Popov et al 2004) KB. KIM KB is populated 
with over 40,000 named entities. They represent 
a mention and candidates as vectors of their con-
textual noun phrase and co-occurring NEs, and 
then the similarity is determined by the common 
terms of the vectors and their associated weights. 
For linking mentions in news articles with a Wi-
kipedia-derived KB (KBP-09 data set), Varma et 
al. (2009) rank the entity candidates using a 
search engine. Han and Zhao (2009) rank the 
candidates based on BOW and Wikipedia se-
mantic knowledge similarity. 
All the related work above rank the candidates 
based on the similarity between ambiguous men-
tion and candidate entities. However, the ranking 
approach hardly detects the new entity which is 
not present in KB. 
Some supervised approaches also have been 
proposed. Li et al (2009) and McNamee et al 
(2009) train their models on a small manually 
created data set containing only 1,615 examples. 
But entity linking requires large training data. 
Agirre et al (2009) use Wikipedia to construct 
their training data by utilizing Inter-Wikipedia 
links and the surrounding snippets of text. How-
ever, their training data is created from a         
1291
different domain which does not work well in 
the targeted news article domain.  
3 Approach
In this section we describe our two-stage ap-
proach for entity linking: name variation and 
entity disambiguation. The first stage finds vari-
ations for every entity in the KB and generates 
an entity candidate set for a given query. The 
second stage is entity disambiguation, which 
links an entity mention with the real world entity 
it refers to. 
3.1 Name Variation 
The aim for Name Variation is to build a 
Knowledge Repository of entities that contains 
vast amount of world knowledge of entities like 
name variations, acronyms, confusable names, 
spelling variations, nick names etc. We use 
Wikipedia to build our knowledge repository 
since Wikipedia is the largest encyclopedia in 
the world and surpasses other knowledge bases 
in its coverage of concepts and up-to-date 
content. We obtain useful information from 
Wikipedia by the tool named Java Wikipedia 
Library 2  (Zesch et al 2008), which allows to 
access all information contained in Wikipedia. 
Cucerzan (2007) extracts the name variations 
of an entity by leveraging four knowledge 
sources in Wikipedia: ?entity pages?, ?disam-
biguation pages?  ?redirect pages? and ?anchor 
text?.
Entity page in Wikipedia is uniquely identified 
by its title ? a sequence of words, with the first 
word always capitalized. The title of Entity Page 
represents an unambiguous name variation for 
the entity. A redirect page in Wikipedia is an aid 
to navigation. When a page in Wikipedia is redi-
rected, it means that those set of pages are refer-
ring to the same entity. They often indicate syn-
onym terms, but also can be abbreviations, more 
scientific or more common terms, frequent 
misspellings or alternative spellings etc. Disam-
biguation pages are created only for ambiguous 
mentions which denote two or more entities in 
Wikipedia, typically followed by the word ?dis-
ambiguation? and containing a list of references 
to pages for entities that share the same name. 
This is more useful in extracting the abbrevia-
2 http://www.ukp.tu-darmstadt.de/software/JWPL 
tions of entities, other possible names for an ent-
ity etc. Besides, both outlinks and inlinks in Wi-
kipedia are associated with anchor texts that 
represent name variations for the entities.
Using these four sources above, we extracted 
name variations for every entity in KB to form 
the Knowledge Repository as Cucerzan?s (2007) 
method. For example, the variation set for entity 
E0272065 in KB is {Abbott Laboratories, Ab-
bott Nutrition, Abbott ?}. Finally, we can gen-
erate the entity candidate set for a given query 
using the Knowledge Repository. For example, 
for the query containing ?Abbott?, the entity 
candidate set retrieved is {E0272065, E0064214 
?}.
From our observation, for some queries the re-
trieved candidate set is empty. If the entity for 
the query is a new entity, not present in KB, 
empty candidate set is correct. Otherwise, we 
fail to identify the mention in the query as a var-
iation, commonly because the mention is a miss-
pelling or infrequently used name. So we pro-
pose to use two more sources ?Did You Mean? 
and ?Wikipedia Search Engine? when Cucerzan 
(2007) algorithm returns empty candidate set. 
Our experiment results show that both proposed 
knowledge sources are effective for entity link-
ing. This contributes to a performance improve-
ment on the final entity linking accuracy. 
Did You Mean: The ?did you mean? feature 
of Wikipedia can provide one suggestion for 
misspellings of entities. This feature can help to 
correct the misspellings. For example, ?Abbot 
Nutrition? can be corrected to ?Abbott Nutri-
tion?.
Wikipedia Search Engine: This key word 
based search engine can return a list of relevant 
entity pages of Wikipedia. This feature is more 
useful in extracting infrequently used name. 
Algorithm 1 below presents the approach to 
generate the entity candidate set over the created 
Knowledge Repository. Ref
E
(s) is the entity set 
indexed by mention s retrieved from Knowledge 
Repository.  In Step 8, we use the longest com-
mon subsequence algorithm to measure the simi-
larity between strings s and the title of the entity 
page with highest rank. More details about long-
est common subsequence algorithm can be 
found in Cormen et al (2001). 
1292
Algorithm 1 Candidate Set Generation 
Input: mention s;       
1: if RefE(s) is empty
2:        s??Wikipedia?did you 
           mean?Suggestion 
3:        If s? is not NULL  
4:             s ? s?
5:        else
6:            EntityPageList ? WikipediaSear
               chEngine(s) 
7:            EntityPage?FirstPage of EntityPageL 
               ist 
8:            Sim=Similarity(s,EntityPage.title)
9:            if Sim > Threshold 
10:   s? EntityPage.title
11:          end if 
12: end if 
13: end if 
Output: RefE(s);
3.2 Entity Disambiguation 
The disambiguation component is to link the 
mention in query with the entity it refers to in 
candidate set. If the entity to which the mention 
refers is a new entity which is not present in KB, 
nil will be returned. In this Section, we will de-
scribe the method for automatic data creation, 
domain adaptation from Wikipedia data, and our 
supervised learning approach as well. 
3.2.1 Automatic Data Creation  
The basic idea is to take a document with an un-
ambiguous reference to an entity E1 and replac-
ing it with a phrase which may refer to E1, E2 or 
others.
Observation: Some full names for the entities 
in the world are unambiguous. This phenomenon 
also appears in the given document collection of 
entity linking. The mention ?Abbott Laborato-
ries? appearing at multiple locations in the doc-
ument collection refers to the same entity ?a
pharmaceuticals health care company? in KB.
From this observation, our method takes into 
account the mentions in the Knowledge Reposi-
tory associated with only one entity and we treat 
these mentions as unambiguous name. Let us 
take Abbott Laboratories-{E0272065} in the 
Knowledge Repository as an example. We first 
use an index and search tool to find the docu-
ments with unambiguous mentions. Such as, the 
mention ?Abbott Laboratories? occurs in docu-
ment LDC2009T13 and LDC2007T07 in the 
document collection. The chosen text indexing 
and searching tool is the well-known Apache 
Lucene information retrieval open-source li-
brary3.
Next, to validate the consistency of NE type 
between entities in KB and in document,   we 
run the retrieved documents through a Named 
Entity Recognizer, to tag the named entities in 
the documents. Then we link the document to 
the entity in KB if the document contains a 
named entity whose name exactly matches with 
the unambiguous mention and type (i.e. Person, 
Organization and Geo-Political Entity) exactly 
matches with the type of entity in KB. In this 
example, after Named Entity Recognition, ?Ab-
bott Laboratories? in document LDC2009T13 is
tagged as an Organization which is consistent 
with the entity type of E0272065 in KB. We link 
the ?Abbott Laboratories? occurring in 
LDC2009T13 with entity E0272065.  
Finally, we replace the mention in the selected 
documents with the ambiguous synonyms. For 
example, we replace the mention ?Abbott La-
boratories? in document LDC2009T13 with
?Abbott? where Abbott-{E0064214, 
E0272065?} is an entry in Knowledge Reposi-
tory. ?Abbott? is ambiguous, because it is refer-
ring not only to E0272065, but also to E0064214 
in Knowledge Repository. Then, we can get two 
instances for the created data set as Figure 1, 
where one is positive and the other is negative.  
Figure 1: An instance of the data set 
However, from our studies, we realize some 
limitations on our training data. For example, as 
shown in Figure 1, the negative instance for 
E0272065 and the positive instance for 
3 http://lucene.apache.org 
(Abbott, LDC2009T13)  E0272065    +
(Abbott, LDC2009T13)  E0064214    -
          ? 
                         +   refer to  -  not refer to
1293
E0064214 are not in our created data set. 
However, those instances exist in the current 
document collection. We do not retrieve them 
since there is no unambiguous mention for 
E0064214 in the document collection.   
To reduce the effect of this problem, we pro-
pose to use the Wikipedia data as well, since 
Wikipedia data has training examples for all the 
entities in KB. Articles in Wikipedia often con-
tain mentions of entities that already have a cor-
responding article, and at least the first occur-
rence of the mentions of an entity in a Wikipedia 
article must be linked to its corresponding Wiki-
pedia article, if such an article exists. Therefore, 
if the mention is ambiguous, the hyperlink is 
disambiguating it. Next, we will describe how to 
incorporate Wikipedia data. 
Incorporating Wikipedia Data. The docu-
ment collection for entity linking is commonly 
from other domains, but not Wikipedia. To ben-
efit from Wikipedia data, we introduce a domain 
adaption approach (Daum? III, 2007) which is 
suitable for this work since we have enough 
?target? domain data. The approach is to aug-
ment the feature vectors of the instances. Denote 
by X the input space, and by Y the output space, 
in this case, X is the space of the real vectors 
???? for the instances in data set and Y= {+1,-1} 
is the label. Ds is the Wikipedia domain dataset 
and Dt is our automatically created data set. 
Suppose for simplicity that X=RF for some F > 0 
(RF is the space of F-dimensions). The aug-
mented input space will be defined by ??  =R3F.
Then, define mappings ?s, ?t : X ? ?? for map-
ping the Wikipedia and our created data set re-
spectively.  These are defined as follows:
????? ?? ????? ????? ? ?
????? ?? ??????? ???? ?
Where 0=<0,0,?,0> ?RF is the zero vector. We 
use the simple linear kernel in our experiments. 
However, the following kernelized version can 
help us to gain some insight into the method. K
denotes the dot product of two vectors. 
K(x,x?)=< ?  (x), ?  (x?)>. When the domain is 
the same, we get: ????? ??? ?? ????? ????? ?
?? ????? ????? ? ????? ??? . When they are 
from different domains, we get: ????? ??? ??
????? ????? ?? ???? ???. Putting this togeth-
er, we have: 
?? ? ???
??? ?????????????
???? ???????? ??????
This is an intuitively pleasing result. Loosely 
speaking, this means that data points from our 
created data set have twice as much influence as 
Wikipedia points when making predictions 
about test data from document collection. 
3.2.2 The Disambiguation Framework 
To disambiguate a mention in document collec-
tion, the ranking method is to rank the entities in 
candidate set based on the similarity score. In 
our work, we transform the ranking problem into 
a classification problem: deciding whether a 
mention refers to an entity on an SVM classifier.
If there are 2 or more than 2 candidate entities 
that are assigned positive label by the binary 
classifier, we will use the baseline system (ex-
plained in Section 4.2) to rank the candidates 
and the entity with the highest rank will be cho-
sen.
In the learning framework, the training or test-
ing instance is formed by (query, entity) pair.
For Wikipedia data, (query, entity) is positive if 
there is a hyperlink from the article containing 
the mention in query to the entity, otherwise 
(query, entity) is negative. Our automatically 
created data has been assigned labels in Section 
3.2.1.  Based on the training instances, a binary 
classifier is generated by using particular learn-
ing algorithm.  During disambiguation, (query,
entity) is presented to the classifier which then 
returns a class label.  
Each (query, entity) pair is represented by the 
feature vector using different features and simi-
larity metrics. We chose the following three 
classes of features as they represent a wide range 
of information - lexical features, word-category 
pair, NE type - that have been proved to be ef-
fective in previous works and tasks. We now 
discuss the three categories of features used in 
our framework in details. 
Lexical features. For Bag of Words feature in 
Web People Search, Artiles et al (2009) illu-
strated that noun phrase and n-grams longer than 
2 were not effective in comparison with token-
based features and using bi-grams gives the best 
1294
results only reaching recall 0.7. Thus, we use 
token-based features. The similarity metric we 
choose is cosine (using standard tf.idf weight-
ing). Furthermore, we also take into account the 
co-occurring NEs and represent it in the form of 
token-based features. Then, the single cosine 
similarity feature is based on Co-occurring NEs 
and Bag of Words. 
Word Category Pair. Bunescu (2007) dem-
onstrated that word-category pairs extracted 
from the document and Wikipedia article are a 
good signal for disambiguation. Thus we also 
consider word-category pairs as a feature class, 
i.e., all (w,c) where w is a word from Bag of 
Words of document and c is a category to which 
candidate entity belongs.  
NE Type. This feature is a single binary fea-
ture to guarantee that the type of entity in docu-
ment (i.e. Person, Geo-Political Entity and Or-
ganization) is consistent with the type of entity 
in KB. 
4 Experiments and Discussions 
4.1 Experimental Setup 
    In our study, we use KBP-09 knowledge base 
and document collection for entity linking. In the 
current setting of KBP-09 Data, the KB has been 
generated automatically from Wikipedia. The 
KB contains 818,741 different entities. The doc-
ument collection is mainly composed of news-
wire text from different press agencies. The col-
lection contains 1.3 million documents that span 
from 1994 to the end of 2008. The test data has 
3904 queries across three named entity types: 
Person, Geo-Political Entity and Organization. 
Each query contains a document with an ambi-
guous mention.    
Wikipedia data can be obtained easily from 
the website4 for free research use. It is available 
in the form of database dumps that are released 
periodically. In order to leverage various infor-
mation mentioned in Section 3.1 to derive name 
variations, make use of the links in Wikipedia to 
generate our training corpus and get word cate-
gory information for the disambiguation, we fur-
ther get Wikipedia data directly from the website. 
The version we used in our experiments was re-
leased on Sep. 02, 2009. The automatically 
4 http://download.wikipedia.org   
created corpus (around 10K) was used as the 
training data, and 30K training instances asso-
ciated with the entities in our corpus was derived 
from Wikipedia. 
For pre-processing, we perform sentence 
boundary detection and Chunking derived from 
Stanford parser (Klein and Manning, 2003), 
Named Entity Recognition using a SVM based 
system trained and tested on ACE 2005 with 
92.5(P) 84.3(R) 88.2(F), and coreference resolu-
tion using a SVM based coreference resolver 
trained and tested on ACE 2005 with 79.5%(P), 
66.7%(R) and 72.5%(F).  
We select SVM as the classifier used in this 
paper since SVM can represent the stat-of-the-
art machine learning algorithm. In our imple-
mentation, we use the binary SVMLight devel-
oped by Joachims (1999). The classifier is 
trained with default learning parameters. 
We adopt the measure used in KBP-09 to eva-
luate the performance of entity linking. This 
measure is micro-averaged accuracy: the number 
of correct link divided by the total number of 
queries.
4.2 Baseline Systems 
We build the baseline using the ranking ap-
proach which ranks the candidates based on si-
milarity between mention and candidate entities. 
The entity with the highest rank is chosen. Bag 
of words and co-occurring NEs are represented 
in the form of token-based feature vectors. Then 
tf.idf is employed to calculate similarity between 
feature vectors.  
To make the baseline system with token-
based features state-of-the-art, we conduct a se-
ries of experiments.  Table 1 lists the perfor-
mances of our token-based ranking systems. In 
our experiment, local tokens are text segments 
generated by a text window centered on the 
mention. We set the window size to 55, which is 
the value that was observed to give optimum 
performance for the disambiguation problem 
(Gooi and Allan, 2004). Full tokens and NE are 
all the tokens and named entities co-occurring in 
the text respectively. We notice that tokens of 
the full text as well as the co-occurring named 
entity produce the best baseline performance, 
which we use for the further experiment. 
1295
 Micro-averaged 
Accuracy 
local tokens 60.0 
local tokens + NE 60.6 
full tokens + NE 61.9 
Table 1: Results of the ranking methods 
4.3 Experiment and Result 
As discussed in Section 3.1, we exploit two 
more knowledge sources in Wikipedia: ?did you 
mean? (DYM) and ?Wikipedia search engine? 
(SE) for name variation step. We conduct some 
experiments to compare our name variation me-
thod using Algorithm 1 in Section 3.1 with the 
name variation method of Cucerzan (2007). Ta-
ble 2 shows the comparison results of different 
name variation methods for entity linking. The 
experiments results show that, in entity linking 
task, our name variation method outperforms the 
method of Cucerzan (2007) for both entity dis-
ambiguation methods. 
Name Variation 
Approaches 
Ranking
Method 
Our Disambig-
uation Method 
Cucerzan
(2007) 
60.9 82.2 
+DYM+SE 61.9 83.8 
Table 2: Entity Linking Result for two name 
variation approaches. Column 1 used the base-
line method for entity disambiguation step. Col-
umn 2 used our proposed entity disambiguation 
method.
Table 3 compares the performance of different 
methods for entity linking on the KBP-09 test 
data. Row 1 is the result for baseline system. 
Row 2 and Row 3 show the results training on 
Wikipedia data and our automatically data re-
spectively. Row 4 is the result training on both 
Wikipedia and our created data using the domain 
adaptation method mentioned in Section 3.2.1. It 
shows that our method trained on the automati-
cally generated data alone significantly outper-
forms baseline. Compared Row 3 with Row 2, 
our created data set serves better at training the 
classifier than Wikipedia data. This is due to the 
reason that Wikipedia is a different domain from 
newswire domain. By comparing Row 4 with 
Row 3, we find that by using the domain adapta-
tion method in Section 3.2.1, our method for 
entity linking can be further improved by 1.5%. 
Likely, this is because of the limitation of the 
auto-generated corpus as discussed in Section 
3.2.1. In another hand, Wikipedia can comple-
ment the missing information with the auto-
generated corpus. So combining Wikipedia data 
with our generated data can achieve better result. 
Compared with baseline system using Cucerzan 
(2007) name variation method in Table 2, in to-
tal our proposed method achieves a significant 
22.9% improvement.  
 Micro-averaged Accu-
racy
Baseline 61.9 
Wiki 79.9 
Created Data 82.3 
Wiki? Created Data 83.8 
Table 3: Micro-averaged Accuracy for Entity 
Linking   
     To test the effectiveness of our method to 
deal with new entities not present in KB and ex-
isting entities in KB respectively, we conduct 
some experiments to compare with Baseline.  
Table 4 shows the performances of entity linking 
systems for existing entities (non-NIL) in KB 
and new entity (NIL) which is not present in KB. 
We can see that the binary classifier not only 
effectively reduces the ambiguities to the exist-
ing entities in KB, but also is very useful to 
highlight the new entities to KB for the further 
population. Note that, in baseline system, all the 
new entities are found by the empty candidate 
set of name variation process, while the disam-
biguation component has no contribution.  How-
ever, our approach finds the new entities not on-
ly by the empty candidate set, but also leverag-
ing on disambiguation component which also 
contributes to the performance improvement.  
 non-NIL NIL 
Baseline 72.6  52.4  
Wiki? Created 
Data 
79.2 87.8  
Table 4: Entity Linking on Existing and New 
Entities
1296
Finally, we also compare our method with the 
top 5 systems in KBP-09. Among them, 
Siel_093 (Varma et al 2009) and NLPR_KBP1
(Han and Zhao 2009) use similarity ranking ap-
proach; Stanford_UBC2 (Agirre et al 2009),
QUANTA1 (Li et al 2009) and hltcoe1 (McNa-
mee et al 2009) use supervised approach. From 
the results shown in Figure 2, we observe that 
our method outperforms all the top 5 systems 
and the baseline system of KBP-09. Specifically, 
our method achieves better result than both simi-
larity ranking approaches. This is due to the li-
mitations of the ranking approach which have 
been discussed in Section 2. We also observe 
that our method gets a 5% improvement over 
Stanford_UBC2. This is because they collect 
their training data from Wikipedia which is a 
different domain from document collection of 
entity linking, news articles in this case; while 
our automatic data generation method can create 
a data set from the same domain as the docu-
ment collection. Our system also outperforms 
QUANTA1 and hltcoe1 because they train their 
model on a small manually created data set 
(1,615 examples), while our method can auto-
matically generate a much larger data set. 
Figure 2: A comparison with KBP09 systems 
5 Conclusion
 The purpose of this paper is to explore how 
to leverage the automatically generated large 
scale annotation for entity linking. Traditionally, 
without any training data available, the solution 
is to rank the candidates based on similarity. 
However, it is difficult for the ranking approach 
to detect a new entity that is not present in KB, 
and it is also difficult to combine different fea-
tures. In this paper, we create a large corpus for 
entity linking by an automatic method. A binary 
classifier is then trained to filter out KB entities 
that are not similar to current mentions. We fur-
ther leverage on the Wikipedia documents to 
provide other information which is not available 
in our generated corpus through a domain adap-
tion approach. Furthermore, new information 
sources for finding more variations also contri-
bute to the overall 22.9% accuracy improve-
ments on KBP-09 test data over baseline. 
References  
E. Agirre et al Stanford-UBC at TAC-KBP. In Pro-
ceedings of Test Analysis Conference 2009 (TAC 
09).
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The se-
meval-2007 web evaluation: Establishing a 
benchmark for the web people search task. In Pro-
ceeding of the Fourth International Work-shop on 
Semantic Evaluations (SemEval-2007).
J. Artiles, E. Amigo and J. Gonzalo. 2009. The role 
of named entities in Web People Search. In pro-
ceeding of the 47th Annual Meeting of the Associa-
tion for Computational Linguistics. 
R. Bunescu. 2007. Learning for information extrac-
tion from named entity recognition and disambig-
uation to relation extraction. Ph.D thesis, Universi-
ty of Texas at Austin, 2007. 
T. H. Cormen, et al 2001. Introduction To Algo-
rithms (Second Edition). The MIT Press, Page 
350-355. 
S. Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. Empirical 
Methods in Natural Language Processing, June 
28-30, 2007. 
H. Daum? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics . 
C. H. Gooi and J. Allan. 2004. Cross-document core-
ference on a large scale corpus. In proceedings of 
Human Language Technology Conference North 
American Association for Computational Linguis-
tics Annual Meeting, Boston, MA. 
X. Han and J. Zhao. NLPR_KBP in TAC 2009 KBP 
Track: A Two-Stage Method to Entity Linking. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).
0.838
0.8217
0.8033
0.7984
0.7884
0.7672
0.571
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
1297
T. Joachims. 1999. Making large-scale SVM learning 
practical. In Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press. 
D. Klein and C. D. Manning. 2003. Fast Exact Infe-
rence with a Factored Model for Natural Language 
Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press, pp. 3-10. 
F. LI et al THU QUANTA at TAC 2009 KBP and 
RTE Track. In Proceedings of Test Analysis Con-
ference 2009 (TAC 09).  
P. McNamee and H. T. Dang. 2009. Overview of the 
TAC 2009 Knowledge Base Population Track. In 
Proceedings of Test Analysis Conference 2009 
(TAC 09).  
P. McNamee et al HLTCOE Approaches to Know-
ledge Base Population at TAC 2009.  In Proceed-
ings of Test Analysis Conference 2009 (TAC 09).  
H. T. Nguyen and T. H. Cao. 2008. Named Entity 
Disambiguation on an Ontology Enriched by Wi-
kipedia. 2008 IEEE International Conference on 
Research, Innovation and Vision for the Future in 
Computing & Communication Technologies. 
B. Popov et al 2004. KIM - a Semantic Platform for 
Information Extraction and Retrieval. In Journal 
of Natural Language Engineering, Vol. 10, Issue 
3-4, Sep 2004, pp. 375-392, Cambridge University 
Press.
V. Raphael, K. Joachim and M. Wolfgang, 2007. 
Towards ontology-based disambiguation of geo-
graphical identifiers. In Proceeding of the 16th
WWW workshop on I3: Identity, Identifiers, Identi-
fications, 2007.  
V. Varma et al 2009. IIIT Hyderabad at TAC 2009. 
In Proceedings of Test Analysis Conference 2009 
(TAC 09).  
T. Zesch, C. Muller and I. Gurevych. 2008. Extrac-
tiong Lexical Semantic Knowledge from Wikipe-
dia and Wiktionary. In Proceedings of the Confe-
rence on Language Resources and Evaluation 
(LREC), 2008.  
1298
Coling 2010: Poster Volume, pages 1507?1514,
Beijing, August 2010
Predicting Discourse Connectives for Implicit Discourse Relation
Recognition
Zhi-Min Zhou and Yu Xu
East China Normal University
51091201052@ecnu.cn
Zheng-Yu Niu
Toshiba China R&D Center
zhengyu.niu@gmail.com
Man Lan and Jian Su
Institute for Infocomm Research
sujian@i2r.a-star.edu.sg
Chew Lim Tan
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Existing works indicate that the absence
of explicit discourse connectives makes
it difficult to recognize implicit discourse
relations. In this paper we attempt to
overcome this difficulty for implicit rela-
tion recognition by automatically insert-
ing discourse connectives between argu-
ments with the use of a language model.
Then we propose two algorithms to lever-
age the information of these predicted
connectives. One is to use these pre-
dicted implicit connectives as additional
features in a supervised model. The other
is to perform implicit relation recognition
based only on these predicted connectives.
Results on Penn Discourse Treebank 2.0
show that predicted discourse connectives
help implicit relation recognition and the
first algorithm can achieve an absolute av-
erage f-score improvement of 3% over a
state of the art baseline system.
1 Introduction
Discourse relation analysis is to automatically
identify discourse relations (e.g., explanation re-
lation) that hold between arbitrary spans of text.
This analysis may be a part of many natural lan-
guage processing systems, e.g., text summariza-
tion system, question answering system. If there
are discourse connectives between textual units
to explicitly mark their relations, the recognition
task on these texts is defined as explicit discourse
relation recognition. Otherwise it is defined as im-
plicit discourse relation recognition.
Previous study indicates that the presence of
discourse connectives between textual units can
greatly help relation recognition. In Penn Dis-
course Treebank (PDTB) corpus (Prasad et al,
2008), the most general senses, i.e., Comparison
(Comp.), Contingency (Cont.), Temporal (Temp.)
and Expansion (Exp.), can be disambiguated in
explicit relations with more than 90% f-scores
based only on the discourse connectives explicitly
used to signal the relation (Pitler and Nenkova.,
2009b).
However, for implicit relations, there are no
connectives to explicitly mark the relations, which
makes the recognition task quite difficult. Some of
existing works attempt to perform relation recog-
nition without hand-annotated corpora (Marcu
and Echihabi, 2002), (Sporleder and Lascarides,
2008) and (Blair-Goldensohn, 2007). They use
unambiguous patterns such as [Arg1, but Arg2]
to create synthetic examples of implicit relations
and then use [Arg1, Arg2] as an training example
of an implicit relation. Another research line is
to exploit various linguistically informed features
under the framework of supervised models, (Pitler
et al, 2009a) and (Lin et al, 2009), e.g., polarity
features, semantic classes, tense, production rules
of parse trees of arguments, etc.
Our study on PDTB test data shows that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we simply mapped the ground
truth implicit connective of each test instance to
its most frequent sense. It indicates the impor-
tance of connective information for implicit rela-
tion recognition. However, so far there is no previ-
ous study attempting to use such kind of connec-
tive information for implicit relation. One possi-
1507
ble reason is that implicit connectives do not ex-
ist in unannotated real texts. Another evidence
of the importance of connectives for implicit re-
lations is shown in PDTB annotation. The PDTB
annotation consists of inserting a connective ex-
pression that best conveys the inferred relation by
the readers. Connectives inserted in this way to
express inferred relations are called implicit con-
nectives, which do not exist in real texts. These
evidences inspire us to consider two interesting re-
search questions:
(1) Can we automatically predict implicit connec-
tives between arguments?
(2) How to use the predicted implicit connectives
to build an automatic discourse relation analysis
system?
In this paper we address these two questions as
follows: (1) We insert appropriate discourse con-
nectives between two textual units with the use of
a language model. Here we train the language
model on large amount of raw corpora without
the use of any hand-annotated data. (2) Then we
present two algorithms to use these predicted con-
nectives for implicit relation recognition. One is
to use these connectives as additional features in a
supervised model. The other is to perform relation
recognition based only on these connectives.
We performed evaluation of the two algorithms
and a baseline system on PDTB 2.0 corpus. Ex-
perimental results showed that using predicted
discourse connectives as additional features can
significantly improve the performance of implicit
discourse relation recognition. Specifically, the
first algorithm achieved an absolute average f-
score improvement of 3% over a state of the art
baseline system.
The rest of this paper is organized as follows.
Section 2 describes the two algorithms for implicit
discourse relation recognition. Section 3 presents
experiments and results on PDTB data. Section
4 reviews related work. Section 5 concludes this
work.
2 Our Algorithms for Implicit Discourse
Relation Recognition
2.1 Prediction of implicit connectives
Explicit discourse relations are easily identifiable
due to the presence of discourse connectives be-
tween arguments. (Pitler and Nenkova., 2009b)
showed that in PDTB corpus, the most general
senses, i.e., Comparison (Comp.), Contingency
(Cont.), Temporal (Temp.) and Expansion (Exp.),
can be disambiguated in explicit relations with
more than 90% f-scores based only on discourse
connectives.
But for implicit relations, there are no connec-
tives to explicitly mark the relations, which makes
the recognition task quite difficult. PDTB data
provides implicit connectives that are inserted be-
tween paragraph-internal adjacent sentence pairs
not marked by any of explicit connectives. The
availability of ground-truth implicit connectives
makes it possible to evaluate the contribution of
these connectives for implicit relation recognition.
Our initial study on PDTB data show that the av-
erage f-score for the most general 4 senses can
reach 91.8% when we obtained the sense of each
test example by mapping each ground truth im-
plicit connective to its most frequent sense. We
see that connective information is an important
knowledge source for implicit relation recogni-
tion. However these implicit connectives do not
exist in real texts. In this paper we overcome this
difficulty by inserting a connective between two
arguments with the use of a language model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two ar-
guments, denoted as Arg1 and Arg2. Typically,
there are two possible positions for most of im-
plicit connectives1, i.e., the position before Arg1
and the position between Arg1 and Arg2. Given a
set of possible implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as PPL(Sci,j). According
1For parallel connectives, e.g., if . . . then. . . , the two con-
nectives will take the two arguments together, so there is only
one possible combination for connectives and arguments.
1508
to the value of PPL(Sci,j) (the lower the better),
we can rank these sentences and select the con-
nectives in top N sentences as implicit connec-
tives for this argument pair. The language model
may be trained on large amount of unannotated
corpora that can be cheaply acquired, e.g., North
American News corpus.
2.2 Using predicted implicit connectives as
additional features
We predict implicit connectives on both training
set and test set. Then we can use the predicted
implicit connectives as additional features for su-
pervised implicit relation recognition. Previous
works exploited various linguistically informed
features under the framework of supervised mod-
els. In this paper, we include 9 types of features
in our system due to their superior performance
in previous studies, e.g., polarity features, seman-
tic classes of verbs, contextual sense, modality,
inquirer tags of words, first-last words of argu-
ments, cross-argument word pairs, ever used in
(Pitler et al, 2009a), production rules of parse
trees of arguments used in (Lin et al, 2009), and
intra-argument word pairs inspired by the work of
(Saito et al, 2006).
Here we provide the details of the 9 features,
shown as follows:
Verbs: Similar to the work in (Pitler et al,
2009a), the verb features consist of the number of
pairs of verbs in Arg1 and Arg2 if they are from
the same class based on their highest Levin verb
class level (Dorr, 2001). In addition, the average
length of verb phrase and the part of speech tags
of main verb are also included as verb features.
Context: If the immediately preceding (or fol-
lowing) relation is an explicit, its relation and
sense are used as features. Moreover, we use an-
other feature to indicate if Arg1 leads a paragraph.
Polarity: We use the number of positive,
negated positive, negative and neutral words in ar-
guments and their cross product as features. For
negated positives, we locate the negated words in
text span and then define the closely behind posi-
tive word as negated positive.
Modality: We look for modal words including
their various tenses or abbreviation forms in both
arguments. Then we generate a feature to indicate
the presence or absence of modal words in both
arguments and their cross product.
Inquirer Tags: Inquirer Tags extracted from
General Inquirer lexicon (Stone et al, 1966) con-
tains positive or negative classification of words.
In fact, its fine-grained categories, such as Fall
versus Rise, or Pleasure versus Pain, can indi-
cate the relation between two words, especially
for verbs. So we choose the presence or absence
of 21 pair categories with complementary relation
in Inquirer Tags as features. We also include their
cross production as features.
FirstLastFirst3: We choose the first and last
words of each argument as features, as well as the
pair of first words, the pair of last words, and the
first 3 words in each argument. In addition, we ap-
ply Porter?s Stemmer (Porter, 1980) to each word
before preparation of these features.
Production Rule: According to (Lin et al,
2009), we extract all the possible production rules
from arguments, and check whether the rules ap-
pear in Arg1, Arg2 and both arguments. We re-
move the rules occurring less than 5 times in train-
ing data.
Cross-argument Word Pairs: We perform the
Porter?s stemming (Porter, 1980), and then group
all words from Arg1 and Arg2 into two sets W1
and W2 respectively. Then we generate any possi-
ble word pair (wi, wj) (wi ? W1, wj ? W2). We
remove the word pairs with less than 5 times.
Intra-argument Word Pairs: Let
Q1 = (q1, q2, . . . , qn) be the word se-
quence of Arg1. The intra-argument word
pairs for Arg1 is defined as WP1 =
((q1, q2), (q1, q3), . . . , (q1, qn), (q2, q3), . . . ,
(qn?1, qn)). We extract all the intra-argument
word pairs from Arg1 and Arg2 and remove word
pairs appearing less than 5 times in training data.
2.3 Relation recognition based only on
predicted implicit connectives
After the prediction of implicit connectives, we
can address the implicit relation recognition task
with the methods for explicit relation recogni-
tion due to the presence of implicit connectives,
e.g., sense classification based only on connec-
tives (Pitler and Nenkova., 2009b). The work of
(Pitler and Nenkova., 2009b) showed that most
1509
of connectives are unambiguous and it is possible
to obtain high performance in prediction of dis-
course sense due to the simple mapping relation
between connectives and senses. Given two ex-
amples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey Comparison and Contingency sense
respectively. In most cases, we can easily recog-
nize the relation sense by the appearance of dis-
course connective since it can be interpreted in
only one way. That means, the ambiguity of the
mapping between sense and connective is quite
few.
We count the frequency of sense tags for each
possible connective on PDTB training data for im-
plicit relation. Then we build a sense recognition
model by simply mapping each connective to its
most frequent sense. Here we do not perform con-
nective prediction on training data. During test-
ing, we use the language model to insert implicit
connectives into each test argument pair. Then we
perform relation recognition by mapping each im-
plicit connective to its most frequent sense.
3 Experiments and Results
3.1 Experiments
3.1.1 Data sets
In this work we used the PDTB 2.0 corpus for
evaluation of our algorithms. Following the work
of (Pitler et al, 2009a), we used sections 2-20 as
training set, sections 21-22 as test set, and sec-
tions 0-1 as development set for parameter opti-
mization. For comparison with the work of (Pitler
et al, 2009a), we ran four binary classification
tasks to identify each of the main relations (Cont.,
Comp., Exp., and Temp.) from the rest. For each
relation, we used equal numbers of positive and
negative examples as training data2. The negative
examples were chosen at random from sections 2-
20. We used all the instances in sections 21 and
22 as test set, so the test set is representative of
2Here the numbers of training and test instances for Ex-
pansion relation are different from those in (Pitler et al,
2009a). The reason is that we do not include instances of
EntRel as positive examples.
the natural distribution. The numbers of positive
and negative instances for each sense in different
data sets are listed in Table 1.
Table 1: Statistics of positive and negative sam-
ples in training, development and test sets for each
relation.
Relation Train Dev Test
Pos/Neg Pos/Neg Pos/Neg
Comp. 1927/1927 191/997 146/912
Cont. 3375/3375 292/896 276/782
Exp. 6052/6052 651/537 556/502
Temp. 730/730 54/1134 67/991
In this work we used LibSVM toolkit to con-
struct four linear SVM models for a baseline sys-
tem and the system in Section 2.2.
3.1.2 A baseline system
We first built a baseline system, which used 9
types of features listed in Section 2.2.
We tuned the numbers of firstLastFirst3, cross-
argument word pair, intra-argument word pair on
development set. Finally we set the frequency
threshold at 3, 5 and 5 respectively.
3.1.3 Prediction of implicit connectives
To predict implicit connectives, we adopt the
following two steps:(1) train a language model;
(2) select top N implicit connectives.
Step 1: We used SRILM toolkit to train the lan-
guage models on three benchmark news corpora,
i.e., New York part in the BLLIP North Ameri-
can News, Xin and Ltw parts of English Gigaword
(4th Edition). We also tried different values for
n in n-gram model. The parameters were tuned
on the development set to optimize the accuracy
of prediction. In this work we chose 3-gram lan-
guage model trained on NY corpus.
Step 2: We combined each instance?s Arg1 and
Arg2 with connectives extract from PDTB2 (100
in all). There are two types of connectives, sin-
gle connective (e.g. because and but) and paral-
lel connective (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of pos-
sible implicit connectives {ci}, for single connec-
tive {ci}, we constructed two synthetic sentences,
ci+Arg1+Arg2 and Arg1+ci+Arg2. In case of
1510
parallel connective, we constructed one synthetic
sentence like ci1+Arg1+ci2+Arg2.
As a result, we can get 198 synthetic sentences
for each argument pair. Then we converted all
words to lower cases and used the language model
trained in the above step to calculate perplexity
on sentence level. The perplexity scores were
ranked from low to high. For example, we got the
perplexity (ppl) for two sentences as follows:
(1) but this is an old story, we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 652.837
(2) this is an old story, but we?re talking about
years ago before anyone heard of asbestos having
any questionable properties.
ppl= 583.514
We considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is, the
presence and absence of the specific connective.
According to the value of PPL(Sci,j) (the
lower the better), we selected the connectives in
top N sentences as implicit connectives for this
argument pair. In order to get the optimal N value,
we tried various values of N on development set
and selected the minimum value of N so that the
ground-truth connectives appeared in top N con-
nectives. The final N value is set to 60 based on
the trade-off between performance and efficiency.
3.1.4 Using predicted connectives as
additional features
This system combines the predicted implicit
connectives as additional features and the 9 types
of features in an supervised framework. The 9
types of features are listed as shown in Section 2.2
and tuned on development set.
We combined predicted connectives with the
best subset features from the development data set
with respect to f-score. In our experiment of se-
lecting best subset features, single features rather
than the combination of several features achieved
much higher scores. So we combine single fea-
tures with predicted connectives as final features.
3.1.5 Using only predicted connectives for
implicit relation recognition
We built two variants for the algorithm in Sec-
tion 2.3. One is to use the data for explicit re-
lations in PDTB sections 2-20 as training data.
The other is to use the data for implicit relations
in PDTB sections 2-20 as training data. Given
training data, we obtained the most frequent sense
for each connective appearing in the training data.
Then given test data, we recognized the sense of
each argument pair by mapping each predicted
connective to its most frequent sense. In this
work we conducted another experiment to see the
upper-bound performance of this algorithm. Here
we performed recognition based on ground-truth
implicit connectives and used the data for implicit
relations as training data.
3.2 Results
3.2.1 Result of baseline system
Table 2 summarizes the best performance
achieved by the baseline system in compari-
son with previous state-of-the-art performance
achieved in (Pitler et al, 2009a). The first two
lines in the table show their best results using sin-
gle feature and using combined feature subset. It
indicates that the performance of using combined
feature subset is higher than that using single fea-
ture alone.
From this table, we can find that our base-
line system has a comparable result on Contin-
gency and Temporal. On Comparison, our system
achieved a better performance around 9% f-score
higher than their best result. However, for Expan-
sion, they expanded both training and testing sets
by including EntRel relation as positive examples,
which makes it impossible to perform direct com-
parison. Generally, our baseline system is reason-
able and thus the consequent experiments on it are
reliable.
3.2.2 Result of algorithm 1: using predicted
connectives as additional features
Table 3 summarizes the best performance
achieved by the baseline system and the first al-
gorithm (i.e., baseline + Language Model) on test
set. The second and third column show the best
performance achieved by the baseline system and
1511
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test
set.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Using the best single feature (Pitler et al, 2009a) 21.01(52.59) 36.75(62.44) 71.29(59.23) 15.93(61.20)
Using the best feature subset (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
the first algorithm using predicted connectives as
additional features.
Table 3: Performance comparison of the algo-
rithm in Section 2.2 with the baseline system on
test set.
Rela- Features Baseline Baseline+LM
tion F1 (Acc) F1 (Acc)
Comp. Production Rule 30.72(78.26) 31.08(68.15)
Context 24.66(42.25) 27.64(53.97)
InquirerTags 23.31(73.25) 27.87(55.48)
Polarity 21.11(40.64) 23.64(52.36)
Modality 17.25(80.06) 26.17(55.20)
Verbs 25.00(53.50) 31.79(58.22)
Cont. Prodcution Rule 45.38(40.17) 47.16(48.96)
Context 37.61(44.70) 34.74(48.87)
Polarity 35.57(50.00) 43.33(33.74)
InquirerTags 38.04(41.49) 42.22(36.11)
Modality 32.18(66.54) 35.26(55.58)
Verbs 40.44(54.06) 42.04(32.23)
Exp. Context 48.34(54.54) 68.32(53.02)
FirstLastFirst3 65.95(57.94) 68.94(53.59)
InquirerTags 61.29(52.84) 68.49(53.21)
Modality 64.36(56.14) 68.9(52.55)
Polarity 49.95(50.38) 68.62(53.40)
Verbs 52.95(53.31) 70.11(54.54)
Temp. Context 13.52(64.93) 16.99(79.68)
FirstLastFirst3 15.75(66.64) 19.70(64.56)
InquirerTags 8.51(83.74) 19.20(56.24)
Modality 16.46(29.96) 19.97(54.54)
Polarity 16.29(51.42) 20.30(55.48)
Verbs 13.88(54.25) 13.53(61.34)
From this table, we found that this additional
feature obtained from language model showed
significant improvements in almost four relations.
Specifically, the top two improvements are on Ex-
pansion and Temporal relations, which improved
4.16% and 3.84% in f-score respectively. Al-
though on Comparison relation there is only a
slight improvement (+1.07%), our two best sys-
tems both got around 10% improvements of f-
score over a state-of-the-art system in (Pitler et al,
2009a). As a whole, the first algorithm achieved
3% improvement of f-score over a state of the art
baseline system. All these results indicate that
predicted implicit connectives can help improve
the performance.
3.2.3 Result of algorithm 2: using only
predicted connectives for implicit
relation recognition
Table 4 summarizes the best performance
achieved by the second algorithm in comparison
with the baseline system on test set.
The experiment showed that the baseline sys-
tem using just gold-truth implicit connectives can
achieve an f-score of 91.8% for implicit relation
recognition. It once again proved that implicit
connectives make significant contributions for im-
plicit relation recognition. This also encourages
our future work on finding the most suitable con-
nectives for implicit relation recognition.
From this table, we found that, using only pre-
dicted implicit connectives achieved an compara-
ble performance to (Pitler et al, 2009a), although
it was still a bit lower than our best baseline. But
we should bear in mind that this algorithm only
uses 4 features for implicit relation recognition
and these 4 features are easy computable and fast
run, which makes the system more practical in ap-
plication. Furthermore, compared with other al-
gorithms which require hand-annotated data for
training, the performance of this second algorithm
is acceptable if we take into account that no la-
beled data is used for model training.
3.3 Analysis
Experimental results on PDTB showed that using
the predicted implicit connectives significantly
improves the performance of implicit discourse
relation recognition. Our first algorithm achieves
an average f-score improvement of 3% over a
state of the art baseline system. Specifically, for
the relations: Comp., Cont., Exp., Temp., our
first algorithm can achieve 1.07%, 1.78%, 4.16%,
3.84% f-score improvements over a state of the
art baseline system. Since (Pitler et al, 2009a)
1512
Table 4: Performance comparison of the algorithm in Section 2.3 with the baseline system on test set.
System Comp. vs. Other Cont. vs. Other Exp. vs. Other Temp. vs. Other
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
The baseline system 30.72(78.26) 45.38(40.17) 65.95(57.94) 16.46(29.96)
Our algorithm with training data for explicit relation 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97)
Our algorithm with training data for implicit relation 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51)
Sense recognition using gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07)
used different selection of instances for Expan-
sion sense3, we cannot make a direct compari-
son. However, we achieve the best f-score around
70%, which provide 5% improvements over our
baseline system. On the other hand, the second
proposed algorithm using only predicted connec-
tives still achieves promising results for each rela-
tion. Specifically, the model for the Comparison
relation achieves an f-score of 26.02% (5% over
the previous work in (Pitler et al, 2009a)). Fur-
thermore, the models for Contingency and Tem-
poral relation achieve 35.72% and 13.76% f-score
respectively, which are comparable to the previ-
ous work in (Pitler et al, 2009a). The model for
Expansion relation obtains an f-score of 64.95%,
which is only 1% less than our baseline system
which consists of ten thousands of features.
4 Related Work
Existing works on automatic recognition of dis-
course relations can be grouped into two cat-
egories according to whether they used hand-
annotated corpora.
One research line is to perform relation recog-
nition without hand-annotated corpora.
(Marcu and Echihabi, 2002) used a pattern-
based approach to extract instances of discourse
relations such as Contrast and Elaboration from
unlabeled corpora. Then they used word-pairs be-
tween two arguments as features for building clas-
sification models and tested their model on artifi-
cial data for implicit relations.
There are other efforts that attempt to extend the
work of (Marcu and Echihabi, 2002). (Saito et al,
2006) followed the method of (Marcu and Echi-
habi, 2002) and conducted experiments with com-
bination of cross-argument word pairs and phrasal
3They expanded the Expansion data set by adding ran-
domly selected EntRel instances by 50%, which is consid-
ered to significantly change data distribution.
patterns as features to recognize implicit relations
between adjacent sentences in a Japanese corpus.
They showed that phrasal patterns extracted from
a text span pair provide useful evidence in the re-
lation classification. (Sporleder and Lascarides,
2008) discovered that Marcu and Echihabi?s mod-
els do not perform as well on implicit relations as
one might expect from the test accuracies on syn-
thetic data. (Blair-Goldensohn, 2007) extended
the work of (Marcu and Echihabi, 2002) by re-
fining the training and classification process using
parameter optimization, topic segmentation and
syntactic parsing.
(Lapata and Lascarides, 2004) dealt with tem-
poral links between main and subordinate clauses
by inferring the temporal markers linking them.
They extracted clause pairs with explicit temporal
markers from BLLIP corpus as training data.
Another research line is to use human-
annotated corpora as training data, e.g., the RST
Bank (Carlson et al, 2001) used by (Soricut and
Marcu, 2003), adhoc annotations used by (?),
(Baldridge and Lascarides, 2005), and the Graph-
Bank (Wolf et al, 2005) used by (Wellner et al,
2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2008) bene-
fits the researchers with a large discourse anno-
tated corpora, using a comprehensive scheme for
both implicit and explicit relations. (Pitler et al,
2009a) performed implicit relation classification
on the second version of the PDTB. They used
several linguistically informed features, such as
word polarity, verb classes, and word pairs, show-
ing performance increases over a random classi-
fication baseline. (Lin et al, 2009) presented an
implicit discourse relation classifier in PDTB with
the use of contextual relations, constituent Parse
Features, dependency parse features and cross-
argument word pairs.
1513
In comparison with existing works, we investi-
gated a new knowledge source, implicit connec-
tives, for implicit relation recognition. Moreover,
our two models can exploit both labeled and un-
labeled data by training a language model on un-
labeled data and then using this language model
to generate implicit connectives for recognition
models trained on labeled data.
5 Conclusions
In this paper we use a language model to auto-
matically generate implicit connectives and then
present two methods to use these connectives for
recognition of implicit relations. One method is to
use these predicted implicit connectives as addi-
tional features in a supervised model and the other
is to perform implicit relation recognition based
only on these predicted connectives. Results on
Penn Discourse Treebank 2.0 show that predicted
discourse connectives help implicit relation recog-
nition and the first algorithm achieves an absolute
average f-score improvement of 3% over a state of
the art baseline system.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, Col-
lege Park, MD,2001.
R. Girju. 2003. Automatic detection of causal rela-
tions for question answering. In ACL 2003 Work-
shops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
EMNLP.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th ACL.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th ACL.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. 1980. An algorithm for suffix stripping. In
Program, vol. 14, no. 3, pp.130-137.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. Sentence Level Discourse
Parsing using Syntactic and Lexical Information.
Proceedings of HLT/NAACL 2003.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
1514
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 872?881,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
	
				
		
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12?23,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Discourse Analysis for Article-Wide Temporal Classification
Jun-Ping Ng1, Min-Yen Kan1,2, Ziheng Lin3, Wei Feng4, Bin Chen5, Jian Su5, Chew-Lim Tan1
1School of Computing, National University of Singapore, Singapore
2Interactive and Digital Media Institute, National University of Singapore, Singapore
3Research & Innovation, SAP Asia Pte Ltd, Singapore
4Department of Computer Science, University of Toronto, Canada
5Institute for Infocomm Research, Singapore
junping@comp.nus.edu.sg
Abstract
In this paper we classify the temporal relations
between pairs of events on an article-wide ba-
sis. This is in contrast to much of the exist-
ing literature which focuses on just event pairs
which are found within the same or adjacent
sentences. To achieve this, we leverage on dis-
course analysis as we believe that it provides
more useful semantic information than typical
lexico-syntactic features. We propose the use
of several discourse analysis frameworks, in-
cluding 1) Rhetorical Structure Theory (RST),
2) PDTB-styled discourse relations, and 3)
topical text segmentation. We explain how
features derived from these frameworks can be
effectively used with support vector machines
(SVM) paired with convolution kernels. Ex-
periments show that our proposal is effective
in improving on the state-of-the-art signifi-
cantly by as much as 16% in terms of F1, even
if we only adopt less-than-perfect automatic
discourse analyzers and parsers. Making use
of more accurate discourse analysis can fur-
ther boost gains to 35%.
1 Introduction
A good amount of research had been invested in un-
derstanding temporal relationships within text. Par-
ticular areas of interest include determining the re-
lationship between an event mention and a time ex-
pression (timex), as well as determining the relation-
ship between two event mentions. The latter, which
we refer to as event-event (E-E) temporal classifica-
tion is the focus of this work.
For a given event pair which consists of two
events e1 and e2 found anywhere within an article,
we want to be able to determine if e1 happens be-
fore e2 (BEFORE), after e2 (AFTER), or within the
same time span as e2 (OVERLAP).
Consider this sentence1:
At least 19 people were killed and 114 people were
wounded in Tuesday?s southern Philippines airport blast,
officials said, but reports said the death toll could climb
to 30.
(1)
Three event mentions found within the sentence are
bolded. We say that there is an OVERLAP rela-
tionship between the ?killed ? wounded? event pair
as these two events happened together after the air-
port blast. Similarly there is a BEFORE relationship
between both the ?killed ? said?, and ?wounded ?
said? event pairs, as the death and injuries happened
before reports from the officials.
Being able to infer these temporal relationships
allows us to build up a better understanding of the
text in question, and can aid several natural lan-
guage understanding tasks such as information ex-
traction and text summarization. For example, we
can build up a temporal characterization of an article
by constructing a temporal graph denoting the rela-
tionships between all events within an article (Ver-
hagen et al, 2009). This can then be used to help
construct an event timeline which layouts sequen-
tially event mentions in the order they take place (Do
et al, 2012). The temporal graph can also be used
in text summarization, where temporal order can be
used to improve sentence ordering and thereby the
eventual generated summary (Barzilay et al, 2002).
Given the importance and value of temporal re-
lations, the community has organized shared tasks
1From article AFP ENG 20030304.0250 of the ACE 2005
corpus (ACE, 2005).
12
to spur research efforts in this area, including the
TempEval-1, -2 and -3 evaluation workshops (Ver-
hagen et al, 2009; Verhagen et al, 2010; Uzzaman
et al, 2012). Most related work in this area have
focused primarily on the task defintitions of these
evaluation workshops. In the task definitions, E-
E temporal classification involves determining the
relationship between events found within the same
sentence, or in adjacent sentences. For brevity we
will refer to this loosely as intra-sentence E-E tem-
poral classification in the rest of this paper.
This definition however is limiting and insuffi-
cient. It was adopted as a trade-off between com-
pleteness, and the need to simplify the evaluation
process (Verhagen et al, 2009). In particular, one
deficiency is that it does not allow us to construct the
complete temporal graph we seek. As illustrated in
Figure 1, being able to perform only intra-sentence
E-E temporal classification may result in a forest of
disconnected temporal graphs. A sentence s3 sepa-
rates events C and D, as such an intra-sentence E-E
classification system will not be able to determine
the temporal relationship between them. While we
can determine the relationship between A and C in
the figure with the use of temporal transitivity rules
(Setzer et al, 2003; Verhagen, 2005), we cannot re-
liably determine the relationship between say A and
D.
A
B C
D E
s1
s2
s3
s4
Figure 1: A disconnected temporal graph of events within
an article. Horizontal lines depict sentences s1 to s4, and
the circles identify events of interest.
In this work, we seek to overcome this limitation,
and study what can enable effective article-wide E-E
temporal classification. That is, we want to be able
to determine the temporal relationship between two
events located anywhere within an article.
The main contribution of our work is going
beyond the surface lexical and syntactic features
commonly adopted by existing state-of-the-art ap-
proaches. We suggest making use of semantically
motivated features derived from discourse analysis
instead, and show that these discourse features are
superior.
While we are just focusing on E-E temporal
classification, our work can complement other ap-
proaches such as the joint inference approach pro-
posed by Do et al (2012) and Yoshikawa et al
(2009) which builds on top of event-timex (E-T) and
E-E temporal classification systems. We believe that
improvements to the underlying E-T and E-E classi-
fication systems will help with global inference.
2 Related Work
Many researchers have worked on the E-E temporal
classification problem, especially as part of the Tem-
pEval series of evaluation workshops. Bethard and
Martin (2007) presented one of the earliest super-
vised machine learning systems, making use of sup-
port vector machines (SVM) with a variety of lexical
and syntactic features. Kolya et al (2010) described
a conditional random field (CRF) based learner mak-
ing use of similar features. Other researchers includ-
ing Uzzaman and Allen (2010) and Ha et al (2010)
made use of Markov Logic Networks (MLN). By
leveraging on the transitivity properties of temporal
relationships (Setzer et al, 2003), they found that
MLNs are useful in inferring new temporal relation-
ships from known ones.
Recognizing that the temporal relationships be-
tween event pairs and time expressions are related,
Yoshikawa et al (2009) proposed the use of a joint
inference model and showed that improvements in
performance are obtained. However this gain is at-
tributed to the joint inference model they had devel-
oped, making use of similar surface features.
To the best of our knowledge, the only piece
of work to have gone beyond sentence boundaries
and tackle the problem of article-wide E-E temporal
classification is by Do et al (2012). Making use of
integer linear programming (ILP), they built a joint
inference model which is capable of classifying tem-
poral relationships between any event pair within
a given document. They also showed that event
co-reference information can be useful in determin-
ing these temporal relationships. However they did
not make use of features directed specifically at de-
termining the temporal relationships of event pairs
13
across different sentences. Other than event co-
reference information, they adopted the same mix
of lexico-syntactic features.
Underlying these disparate data-driven methods
for similar temporal processing tasks, the reviewed
works all adopted a similar set of surface fea-
tures including vocabulary features, part-of-speech
tags, constituent grammar parses, governing gram-
mar nodes and verb tenses, among others. We ar-
gue that these features are not sufficiently discrimi-
native of temporal relationships because they do not
explain how sentences are combined together, and
thus are unable to properly differentiate between the
different temporal classifications. Supporting our
argument is the work of Smith (2010), where she
argued that syntax cannot fully account for the un-
derlying semantics beneath surface text. D?Souza
and Ng (2013) found out as much, and showed that
adopting richer linguistic features such as lexical re-
lations from curated dictionaries (e.g. Webster and
WordNet) as well as discourse relations help tempo-
ral classification. They had shown that the Penn Dis-
course TreeBank (PDTB) style (Prasad et al, 2008)
discourse relations are useful. We expand on their
study to assess the utility of adopting additional dis-
course frameworks as alternative and complemen-
tary views.
3 Making Use of Discourse
To highlight the deficiencies of surface features, we
quote here an example from Lascarides and Asher
(1993):
[A] Max opened the door. The room was pitch dark.
[B] Max switched off the light. The room was pitch dark.
(2)
The two lines of text A and B in Example 2 have
similar syntactic structure. Given only syntactic fea-
tures, we may be drawn to conclude that they share
similar temporal relationships. However in the first
line of text, the events temporally OVERLAP, while
in the second line they do not. Clearly, syntax alone
is not going to be useful to help us arrive at the cor-
rect temporal relations.
If existing surface features are insufficient, what is
sufficient? Given a E-E pair which crosses sentence
boundaries, how can we determine the temporal re-
lationship between them? We take our cue from the
work of Lascarides and Asher (1993). They sug-
gested instead that discourse relations hold the key
to interpreting such temporal relationships.
Building on their observations, we believe that
discourse analysis is integral to any solution for the
problem of article-wide E-E temporal classification.
We thus seek to exploit a series of different discourse
analysis studies, including 1) the Rhetorical Struc-
ture Theory (RST) discourse framework, 2) Penn
Discourse Treebank (PDTB)-styled discourse rela-
tions based on the lexicalized Tree Adjoining Gram-
mar for Discourse (D-LTAG), and 3) topical text seg-
mentation, and validate their effectiveness for tem-
poral classification.
RST Discourse Framework. RST (Mann and
Thompson, 1988) is a well-studied discourse anal-
ysis framework. In RST, a piece of text is split into a
sequence of non-overlapping text fragments known
as elementary discourse units (EDUs). Neighboring
EDUs are related to each other by a typed relation.
Most RST relations are hypotactic, where one of the
two EDUs participating in the relationship is demar-
cated as a nucleus, and the other a satellite. The nu-
cleus holds more importance, from the point of view
of the writer, while the satellite?s purpose is to pro-
vide more information to help with the understand-
ing of the nucleus. Some RST relations are however
paratactic, where the two participating EDUs are
both marked as nuclei. A discourse tree can be com-
posed by viewing each EDU as a leaf node. Nodes
in the discourse tree are linked to one another via the
discourse relations that hold between the EDUs.
RST discourse relations capture the semantic re-
lation between two EDUs, and these often offer a
clue to the temporal relationship between events in
the two EDUs too. As an example, let us refer once
again to Example 2. Recall that in the second line of
text ?switched off? happens BEFORE ?dark?. The
RST discourse structure for the second line of text
is shown on the left of Figure 2. We see that the
two sentences are related via a ?Result? discourse
relation. This fits our intuition that when there is
causation, there should be a BEFORE/AFTER rela-
tionship. The RST discourse relation in this case is
very useful in helping us determine the relationship
between the two events.
PDTB-styled Discourse Relations. Another widely
adopted discourse relation annotation is the PDTB
framework (Prasad et al, 2008). Unlike the RST
14
Max switched off the light. The room was pitch dark.
RESULT
The room was pitch dark.
CONTINGENCY :: CAUSE
arg1 arg2
Max switched off the light.
Figure 2: RST and PDTB discourse structures for the second line of text in Example 2. The structure on the left is the
RST discourse structure, while the structure on the right is for PDTB.
framework, the discourse relations in PDTB build on
the work on D-LTAG by Webber (2004), a lexicon-
grounded approach to discourse analysis. Practi-
cally, this means that instead of starting from a pre-
identified set of discourse relations, PDTB-styled
annotations are more focused on detecting possible
connectives (can be either explicit or implicit) within
the text, before identifying the text fragments which
they connect and how they are related to one another.
Applied again to the second line of text we have in
Example 2, we get a structure as shown on the right
side of Figure 2. From the figure we can see that
the two sentences are related via a ?Cause? relation-
ship. Similar to what we have explained earlier for
the case of RST, the presence of a causal effect here
strongly hints to us that events in the two sentences
share a BEFORE/AFTER relationship.
At this point we want to note the differences be-
tween the use of the RST framework and PDTB-
styled discourse relations in the context of our work.
The theoretical underpinnings behind these two dis-
course analysis are very different, and we believe
that they can be complementary to each other. First,
the RST framework breaks up text within an article
linearly into non-overlapping EDUs. Relations can
only be defined between neighboring EDUs. How-
ever this constraint is not found in PDTB-styled re-
lations, where a text fragment can participate in one
discourse relation, and a subsequence of it partic-
ipate in another. PDTB relations are also not re-
stricted only to adjacent text fragments. In this as-
pect, the flexibility of the PDTB relations can com-
plement the seemingly more rigid RST framework.
Second, with PDTB-styled relations not every
sentence needs to be in a relation with another as
the PDTB framework does not aim to build a global
discourse tree that covers all sentence pairs. This is
a problem when we need to do an article-wide anal-
ysis. The RST framework does not suffer from this
limitation however as we can build up a discourse
tree connecting all the text within a given article.
Topical Text Segmentation. A third complemen-
tary type of inter-sentential analysis is topical text
segmentation. This form of segmentation separates
a piece of text into non-overlapping segments, each
of which can span several sentences. Each segment
represents passages or topics, and provides a coarse-
grained study of the linear structure of the text (Sko-
rochod?Ko, 1972; Hearst, 1994). The transition be-
tween segments can represent possible topic shifts
which can provide useful information about tempo-
ral relationships.
Referring to Example 32, we have delimited the
different lines of text into segments with parenthe-
ses along with a subscript. Segment (1) talks about
the casualty numbers seen at a medical centre, while
Segment (2) provides background information that
informs us a bomb explosion had taken place. The
segment boundary signals to us a possible temporal
shift and can help us to infer that the bombing event
took place BEFORE the deaths and injuries had oc-
curred.
(The Davao Medical Center, a regional government hos-
pital, recorded 19 deaths with 50 wounded. Medical
evacuation workers however said the injured list was
around 114, spread out at various hospitals.)1
(A powerful bomb tore through a waiting shed at the
Davao City international airport at about 5.15 pm (0915
GMT) while another explosion hit a bus terminal at the
city.)2
(3)
4 Methodology
Having motivated the use of discourse analysis for
our problem, we now proceed to explain how we can
make use of them for temporal classification. The
different facets of discourse analysis that we are ex-
ploring in this work are structural in nature. RST
2From article AFP ENG 20030304.0250 of the ACE 2005
corpus.
15
EDU2 EDU3
r2
r1
EDU1
A
B
Figure 3: A possible RST discourse tree. The two circles
denote two events A and B which we are interested in.
t1 t2
t3
t4
r1 r2
r3
B
A
Figure 4: A possible PDTB-styled discourse annotation
where the circles represent events we are interested in.
and PDTB discourse relations are commonly repre-
sented as graphs, and we can also view the output
of text segmentation as a graph with individual text
segments forming vertices, and the transitions be-
tween them forming edges.
Considering this, we propose the use of support
vector machines (SVM), adopting a convolution ker-
nel (Collins and Duffy, 2001) for its kernel function
(Vapnik, 1999; Moschitti, 2006). The use of convo-
lution kernels allows us to do away with the exten-
sive feature engineering typically required to gener-
ate flat vectorized representations of features. This
process is time consuming and demands specialized
knowledge to achieve representations that are dis-
criminative, yet are sufficiently generalized. Con-
volution kernels had also previously been shown to
work well for the related problem of E-T temporal
classification (Ng and Kan, 2012), where the fea-
tures adopted are similarly structural in nature.
We now describe our use of the discourse analysis
frameworks to generate appropriate representations
for input to the convolution kernel.
RST Discourse Framework. Recall that the RST
framework provides us with a discourse tree for an
entire input article. In recent years several automatic
RST discourse parsers have been made available. In
our work, we first make use of the parser by Feng
and Hirst (2012) to obtain a discourse tree represen-
tation of our input. To represent the meaningful por-
tion of the resultant tree, we encode path information
between the two sentences of interest.
We illustrate this procedure using the example
discourse tree illustrated in Figure 3. EDUs includ-
ing EDU1 to EDU3 form the vertices while dis-
course relations r1 and r2 between the EDUs form
the edges. For a E-E pair, {A,B}, we can obtain a
feature structure by first locating the EDUs within
which A and B are found. A is found inside EDU1
and B is found within EDU3. We trace the short-
est path between EDU1 and EDU3, and use this
path as the feature structure for the E-E pair, i.e.
{r1 ? r2}.
PDTB-styled Discourse Relations. We make use of
the automatic PDTB discourse parser from Lin et al
(2013) to obtain the discourse relations over an input
article. Similar to how we work with the RST dis-
course framework, for a given E-E pair, we retrieve
the relevant text fragments and use the shortest path
linking the two events as a feature structure for our
convolution kernel classifier.
An example of a possible PDTB-styled discourse
annotation is shown in Figure 4. The horizontal
lines represent different sentences in an article. The
parentheses delimit text fragments, t1 to t4, which
have been identified as arguments participating in
discourse relations, r1 to r3. For a given E-E pair
{A,B}, we use the trace of the shortest path be-
tween them i.e. {r1 ? r2} as a feature structure.
We take special care to regularize the input (as,
unlike EDUs in RST, arguments to different PDTB
relations may overlap, as in r2 and r3). We model
each PDTB discourse annotation as a graph and em-
ploy Dijkstra?s shortest path algorithm. The graph
resulting from the annotation in Figure 4 is given in
Figure 5. Each text fragment ti maps to a vertex
ni in the graph. PDTB relations between text frag-
ments form edges between corresponding vertices.
As r2 relates t2 to both t3 and t4, two edges link
up n2 to the corresponding vertices n3 and n4 re-
spectively. By doing this, Dijkstra?s algorithm will
always allow us to find the desired shortest path.
n1 n2 n3 n4
r1
r2 r3
r2
Figure 5: Graph derived from discourse annotation in
Figure 4.
16
Topical Text Segmentation. Taking as input a com-
plete text article, we make use of the state-of-the-art
text segmentation system from Kazantseva and Sz-
pakowicz (2011). The output of the system is a se-
ries of non-overlapping, linear text segments, which
we can number sequentially.
In Figure 6 the horizontal lines represent sen-
tences. Parentheses with subscripts mark out the
segment boundaries. We can see two segments s1
and s2 here. Given a target E-E pair {A,B} (repre-
sented as circles inside the figure), we identify the
segment number of the corresponding segment in
which each of A and B is found. We build a fea-
ture structure with the identified segment numbers,
i.e. {s1 ? s2} to capture the segmentation.
A
B
s1
s2
Figure 6: A possible segmentation of three sentences into
two segments.
5 Results
We conduct a series of experiments to validate the
utility of our proposed features.
Data Set. We make use of the same data set built
by Do et al (2012). The data set consists of 20
newswire articles which originate from the ACE
2005 corpus (ACE, 2005). Initially, the data set
consist of 324 event mentions, and a total of 375
annotated E-E pairs. We perform the same temporal
saturation step as described in Do et al (2012), and
obtained a total of 7,994 E-E pairs3.
A breakdown of the number of instances by each
temporal classes is shown in Table 1. Unlike earlier
data sets such as that for TempEval-2 where more
than half (about 55%) of test instances belong to the
3Though we have obtained the data set from the original au-
thors, there was a discrepancy in the number of E-E pairs. The
original paper reported a total of 376 annotated E-E pairs. Be-
sides this, we also repeated the saturation steps iteratively until
no new relationship pairs are generated. We believe this to be
an enhancement as it ensures that all inferred temporal relation-
ships are generated.
OVERLAP class, OVERLAP instances make up just
10% of the data set.
This difference is due mainly to the fact that our
data set consists not only of intra-sentence E-E pairs,
but also of article-wide E-E pairs. Figure 7 shows
the number of instances for each temporal class bro-
ken down by the number of sentences (i.e. sentence
gap) that separate the events within each E-E pair.
We see that as the sentence gap increases, the pro-
portion of OVERLAP instances decreases. The in-
tuitive explanation for this is that when event men-
tions are very far apart in an article, it becomes more
unlikely that they happen within the same time span.
Class AFTER BEFORE OVERLAP
# E-E pairs 3,588 (45%) 3,589 (45%) 815 (10%)
Table 1: Number of E-E pairs in data set attributable to
each temporal class. Percentages shown in parentheses.
Figure 7: Breakdown of number of E-E pairs for each
temporal class based on sentence gap.
Experiments. The work done in Do et al (2012) is
highly related to our experiments, and so we have
reported the relevant results for local E-E classifi-
cation in Row 1 of Table 2 as a reference. While
largely comparable, note that a direct comparison is
not possible because 1) the number of E-E instances
we have is slightly different from what was reported,
and 2) we do not have access to the exact partitions
they have created for 5-fold cross-validation.
As such, we have implemented a baseline adopt-
ing similar surface lexico-syntactic features used in
previous work (Mani et al, 2006; Bethard and Mar-
tin, 2007; Ng and Kan, 2012; Do et al, 2012), in-
cluding 1) part-of-speech tags, 2) tenses, 3) depen-
dency parses, 4) relative position of events in article,
17
System Precision Recall F1
(1) DO2012 43.86 52.65 47.46
(2) BASE 59.55 38.14 46.50
(3) BASE + RST + PDTB + TOPICSEG 71.89 41.99 53.01
(4) BASE + RST + PDTB + TOPICSEG + COREF 75.23 43.58 55.19
(5) BASE + O-RST + PDTB + O-TOPICSEG + O-COREF 78.35 54.24 64.10
Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive
row is statistically significant, but a comparison is not possible between rows (1) and (2).
5) the number of sentences between the target events
and 6) VerbOcean (Chklovski and Pantel, 2004) re-
lations between events. This baseline system, and
the subsequent systems we will describe, comprises
of three separate one-vs-all classifiers for each of the
temporal classes. The result obtained by our base-
line is shown in Row 2 (i.e. BASE) in Table 2. We
note that our baseline is competitive and performs
similarly to the results obtained by Do et al (2012).
However as we do not have the raw judgements from
Do?s system, we cannot test for statistical signifi-
cance.
We also implemented our proposed features and
show the results obtained in the remaining rows of
Table 2. In Row 3, RST denotes the RST discourse
feature, PDTB denotes the PDTB-styled discourse
features, and TOPICSEG denotes the text segmen-
tation feature. Compared to our own baseline, there
is a relative increase of 14% in F1, which is statis-
tically significant when verified with the one-tailed
Student?s paired t-test (p < 0.01).
In addition, Do et al (2012) have shown the value
of event co-reference. Therefore we have also in-
cluded this feature by making use of an automatic
event co-reference system by Chen et al (2011).
The result obtained after adding this feature (de-
noted by COREF) is shown in Row 4. The relative in-
crease in F1 of about 4% from Row 3 is statistically
significant (p < 0.01) and affirms that event co-
reference is a useful feature to have, together with
our proposed features. We note that our complete
system in Row 4 gives a 16% improvement in F1,
relative to the reference system DO2012 in Row 1.
To get a better idea of the performance we can ob-
tain if oracular versions of our features are available,
we also show the results obtained if hand-annotated
RST discourse structures, text segments, as well as
event co-reference information were used. Annota-
tions for the RST discourse structures and text seg-
ments were performed by the first author (RST an-
notations were made following the annotation guide-
lines given by Carlson and Marcu (2001)). Oracular
event co-reference information was included in the
dataset that we have used.
In Row 5 the prefix O denotes oracular versions
of the features we had proposed. From the results
we see that there is a marked increase of over 15%
in F1 relative to Row 4. Compared to Do?s state-of-
the-art system, there is also a relative gain of at least
35%. These oracular results further confirm the im-
portance of non-local discourse analysis for tempo-
ral processing.
6 Discussion
Ablation tests. We performed ablation tests to as-
sess the efficacy of the discourse features used in
our earlier experiments. Starting from the full sys-
tem, we dropped each discourse feature in turn to see
the effect this has on overall system performance.
Our test is performed over the same data set, again
with 5-fold cross-validation. The results in Table 3
show a statistically significant (based on the one-
tailed Student?s paired t-test) drop in F1 in each case,
which proves that each of our proposed features is
useful and required.
From the ablation tests, we also observe that the
RST discourse feature contributes the most to over-
all system performance while the PDTB discourse
feature contributes the least. However we should not
conclude prematurely that the former is more use-
ful than the latter; as the results are obtained using
parses from automatic systems, and are not reflec-
tive of the full utility of ground truth discourse an-
notations.
Useful Relations. The ablation test results showed
us that discourse relations (in particular RST dis-
18
Figure 8: Proportion of occurence in temporal classes for every RST and PDTB relation.
Ablated Feature Change in F1 Sig
?RST -9.03 **
?TOPICSEG -2.98 **
?COREF -2.18 **
?PDTB -1.42 *
Table 3: Ablation test results. ?**? and ?*? denote statis-
tically significance against the full system with p < 0.01
and p < 0.05, respectively.
course relations) are the most important in our sys-
tem. We have also motivated our work earlier with
the intuition that certain relations such as the RST
?Result? and the PDTB ?Cause? relations provide
very useful temporal cues. We now offer an intro-
spection into the use of these discourse relations.
Figure 8 illustrates the relative proportion of tem-
poral classes in which each RST and PDTB re-
lation appear. If the relations are randomly dis-
tributed, we should expect their distribution to fol-
low that of the temporal classes as shown in Table 1.
However we see that many of the relations do not
follow this distribution. For example, we observe
that several relations such as the RST ?Condition?
and PDTB ?Cause? relations are almost exclusively
found within AFTER and BEFORE event pairs only,
while the RST ?Manner-means? and PDTB ?Syn-
chrony? relations occur in a disproportionately large
number of OVERLAP event pairs. These relations
are likely useful in disambiguating between the dif-
ferent temporal classes.
To verify this, we examine the convolution tree
fragments that lie on the support vector of our SVM
classifier. The work of Pighin and Moschitti (2010)
in linearizing kernel functions allows us to take a
look at these tree fragments. Applying the lineariza-
tion process leads to a different classifier from the
one we have used. The identified tree fragments are
therefore just an approximation to those actually em-
ployed by our classifier. However, this analysis still
offers an introspection as to what relations are most
influential for classification.
BEFORE OVERLAP
B1 (Temporal ... O1 (Manner-means ...
B2 (Temporal (Elaboration ...
B3 (Condition (Explanation ...
B4 (Condition (Attribution ...
B5 (Elaboration (Bckgrnd ...
Table 4: Subset of top RST discourse fragments on sup-
port vectors identified by linearizing kernel function.
Table 4 shows a subset of the top RST discourse
fragments identified for the BEFORE and OVER-
LAP one-vs-all classifiers. The list is in line with
what we expect from Figure 8. The former consists
of fragments containing relations such as ?Tempo-
ral? and ?Condition?, while the latter has a sole frag-
ment containing ?Manner-Means?.
To illustrate what these fragments may mean, we
show several example sentences from our data set
in Example 4. Sentence A consists of the tree frag-
ment B1, i.e. ?(Temporal...?. Its corresponding dis-
course structure is illustrated in the top half of Fig-
ure 9. This fragment indicates to us (correctly) that
the event ?wielded? happened BEFORE Milosevic
was ?swept out? of power. Sentence B is made
up of tree fragment O1, i.e. ?(Manner-means...?,
19
and its discourse structure is shown in the bottom
half of Figure 9. As with the previous example, the
fragment suggests (correctly) that there should be a
OVERLAP relationship for the ?requested ? said?
event pair.
[A] Milosevic and his wife wielded enormous power in
Yugoslavia for more than a decade before he was swept
out of power after a popular revolt in October 2000.
[B] The court order was requested by Jack Welch?s at-
torney, Daniel K. Webb, who said Welch would likely be
asked about his business dealings, his health and entries
in his personal diary.
(4)
Milosevic ? wielded? 
a decade 
before.. swept out.. 
power
after a?  October
2000.
temporal
temporal
The court? requested
by Jack .. Webb,
elaboration
who said Welch would ?
diary.
attribution
manner-means
Figure 9: RST discourse structures for sentences A (top
half) and B (bottom half) in Example 4.
Segment Numbers. From the ablation test results,
text segmentation is the next most important feature
after the RST discourse feature. This is interesting
given that the defined feature structure for topical
text segmentation is not the most intuitive. By us-
ing actual segment numbers, the structure may not
generalize well for articles of different lengths for
example, as each article may have vastly different
number of segments. The transition across segments
may also not carry the same semantic significance
for different articles.
Our experiments have however shown that this
feature design is useful in improving performance.
This is likely because:
1. The default settings of the text segmentation
system we had used are such that precision is
favoured over recall (Kazantseva and Szpakow-
icz, 2011, p. 292). As such there is just an aver-
age of between two to three identified segments
per article. This makes the feature more gener-
alizable despite making use of actual segment
numbers.
2. The style of writing in newswire articles which
we are experimenting on generally follows
common journalistic guidelines. The semantics
behind the transitions across the coarse-grained
segments that were identified are thus likely to
be of a similar nature across many different ar-
ticles.
We leave for future work an investigation into
whether more fine-grained topic segments can lead
to further performance gains. In particular, it will be
interesting to study if work on argumentative zoning
(Teufel and Kan, 2011) can be applied to newswire
articles, and whether the subsequent learnt docu-
ment structures can be used to delineate topic seg-
ments more accurately.
Error Analysis. Besides examining the features we
had used, we also want to get a better idea of the er-
rors made by our classifier. Recall that we are using
separate one-vs-all classifiers for each of the tempo-
ral classes, so each of the three classifiers generates
a column in the aggregate confusion matrix shown
in Table 5. In cases where none of the SVM clas-
sifiers return a positive confidence value, we do not
assign a temporal class (captured as column N). The
high number of event pairs which are not assigned to
any temporal class explains the lower recall scores
obtained by our system, as observed in Table 2.
Predicted
O B A N
O 119 (14.7%) 114 (14.1%) 104 (12.8%) 474 (58.5%)
B 19 (0.5%) 2067 (57.9%) 554 (15.5%) 928 (26.0%)
A 16 (0.5%) 559 (15.7%) 2046 (57.3%) 947 (26.5%)
Table 5: Confusion matrix obtained for the full system,
classifying into (O)VERLAP, (B)EFORE, (A)FTER, and
(N)o result.
Additionally, an interesting observation is the low
percentage of OVERLAP instances that our classi-
fier managed to predict correctly. About 57% of
BEFORE and AFTER instances are classified cor-
20
rectly, however only about 15% of OVERLAP in-
stances are correct.
Figure 10 offers more evidence to suggest that
our classifier works better for the BEFORE and AF-
TER classes than the OVERLAP class. We see that
as sentence gap increases, we achieve a fairly con-
sistent performance for both BEFORE and AFTER
instances. OVERLAP instances are concentrated
where the sentence gap is less than 7, with the best
accuracy figure coming in below 30%.
Although not definitive, this may be because our
data set consists of much fewer OVERLAP in-
stances than the other two classes. This bias may
have led to insufficient training data for accurate
OVERLAP classification. It will be useful to inves-
tigate if using a more balanced data set for training
can help overcome this problem.
Figure 10: Accuracy of the classifer for each temporal
class, plotted against the sentence gap of each E-E pair.
7 Conclusion
We believe that discourse features play an important
role in the temporal ordering of events in text. We
have proposed the use of different discourse anal-
ysis frameworks and shown that they are effective
for classifying the temporal relationships of article-
wide E-E pairs. Our proposed discourse-based fea-
tures are robust and work well even though auto-
matic discourse analysis is noisy. Experiments fur-
ther show that improvements to these underlying
discourse analysis systems will benefit system per-
formance.
In future work, we will like to explore how to
better exploit the various discourse analysis frame-
works for temporal classification. For instance, RST
relations are either hypotactic or paratactic. Marcu
(1997) made use of this to generate automatic sum-
maries by considering EDUs which are nuclei to be
more salient. We believe it is interesting to examine
how such information can help. We are also inter-
ested to apply discourse features in the context of a
global inferencing system (Yoshikawa et al, 2009;
Do et al, 2012), as we think such analyses will also
benefit these systems as well.
Acknowledgments
We like to express our gratitude to Quang Xuan Do,
Wei Lu, and Dan Roth for generously making avail-
able the data set they have used for their work in
EMNLP 2012. We would also like to thank the
anonymous reviewers who reviewed this paper for
their valuable feedback.
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
ACE. 2005. The ACE 2005 (ACE05) Evaluation Plan.
October.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring Strategies for Sentence Order-
ing in Multidocument News Summarization. Journal
of Artificial Intelligence Research (JAIR), 17:35?55.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal Relation Classification Using Syntactic and
Semantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 129?132, June.
Lynn Carlson and Daniel Marcu. 2001. Discourse tag-
ging manual. Technical Report ISI-TR-545, Informa-
tion Sciences Institute, University of Southern Califor-
nia, July.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.
2011. A Unified Event Coreference Resolution by In-
tegrating Multiple Resolvers. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), pages 102?110, Novem-
ber.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bOcean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33?40, July.
21
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
Inference for Event Timeline Construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP), pages
677?689, July.
Jennifer D?Souza and Vincent Ng. 2013. Classifying
Temporal Relations with Rich Linguistic Knowledge.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT), pages 918?927, June.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistics Features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL), pages 60?68, July.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and
James C. Lester. 2010. NCSU: Modeling Temporal
Relations with Markov Logic and Lexical Ontology.
In Proceedings of the 5th International Workshop on
Semantic Evaluation (SemEval), pages 341?344, July.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation
of Expository Text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 9?16, June.
Anna Kazantseva and Stan Szpakowicz. 2011. Lin-
ear Text Segmentation Using Affinity Propagation.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 284?293, July.
Anup Kumar Kolya, Asif Ekbal, and Sivaji Bandyopad-
hyay. 2010. JU CSE TEMP: A First Step Towards
Evaluating Events, Time Expressions and Temporal
Relations. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
345?350, July.
Alex Lascarides and Nicholas Asher. 1993. Temporal
Interpretation, Discourse Relations and Commonsense
Entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013. A
PDTB-styled End-to-End Discourse Parser. Natural
Language Engineering, FirstView:1?34, February.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 753?760, July.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From Discourse Structures to Text
Summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, volume 97,
pages 82?88, July.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning (ECML), September.
Jun-Ping Ng and Min-Yen Kan. 2012. Improved Tem-
poral Relation Classification using Dependency Parses
and Selective Crowdsourced Annotations. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING), pages 2109?2124, December.
Daniele Pighin and Alessandro Moschitti. 2010. On Re-
verse Feature Engineering of Syntactic Tree Kernels.
In Proceedings of the 14th Conference on Natural Lan-
guage Learning (CoNLL), August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC), May.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple.
2003. Using Semantic Inferences for Temporal An-
notation Comparison. In Proceedings of the 4th In-
ternational Workshop on Inference in Computational
Semantics (ICoS), September.
Eduard F. Skorochod?Ko. 1972. Adaptive Method of
Automatic Abstracting and Indexing. In Proceedings
of the IFIP Congress, pages 1179?1182.
Carlota S. Smith. 2010. Temporal Structures in Dis-
course. Text, Time, and Context, 87:285?302.
Simone Teufel and Min-Yen Kan. 2011. Robust Argu-
mentative Zoning for Sensemaking in Scholarly Doc-
uments. In Advanced Language Technologies for Dig-
ital Libraries, pages 154?170. Springer.
Naushad Uzzaman and James F. Allen. 2010. TRIPS and
TRIOS System for TempEval-2: Extracting Temporal
Information. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
276?283, July.
Naushad Uzzaman, Hector Llorens, James F. Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2012. TempEval-3: Evaluating Events, Time Expres-
sions, and Temporal Relations. Computing Research
Repository (CoRR), abs/1206.5333.
Vladimir N. Vapnik, 1999. The Nature of Statistical
Learning Theory, chapter 5. Springer.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval Challenge: Identifying
22
Temporal Relations in Text. Language Resources and
Evaluation, 43(2):161?179.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval), pages
57?62, July.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2-3):211?241.
Bonnie Webber. 2004. D-LTAG: Extending Lexicalized
TAG to Discourse. Cognitive Science, 28(5):751?779.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly Identifying
Temporal Relations with Markov Logic. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL) and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the Asian Federation of Natural Language Pro-
cessing (AFNLP), pages 405?413, August.
23
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 710?719,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Kernel Based Discourse Relation Recognition with Temporal  
Ordering Information 
 
 
WenTing Wang1                   Jian Su1                   Chew Lim Tan2 
1Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis 
Singapore 138632 
{wwang,sujian}@i2r.a-star.edu.sg 
2Department of Computer Science 
University of Singapore 
Singapore 117417 
tacl@comp.nus.edu.sg 
 
  
 
Abstract 
Syntactic knowledge is important for dis-
course relation recognition. Yet only heu-
ristically selected flat paths and 2-level 
production rules have been used to incor-
porate such information so far. In this 
paper we propose using tree kernel based 
approach to automatically mine the syn-
tactic information from the parse trees for 
discourse analysis, applying kernel func-
tion to the tree structures directly. These 
structural syntactic features, together 
with other normal flat features are incor-
porated into our composite kernel to cap-
ture diverse knowledge for simultaneous 
discourse identification and classification 
for both explicit and implicit relations. 
The experiment shows tree kernel ap-
proach is able to give statistical signifi-
cant improvements over flat syntactic 
path feature. We also illustrate that tree 
kernel approach covers more structure in-
formation than the production rules, 
which allows tree kernel to further incor-
porate information from a higher dimen-
sion space for possible better discrimina-
tion. Besides, we further propose to leve-
rage on temporal ordering information to 
constrain the interpretation of discourse 
relation, which also demonstrate statistic-
al significant improvements for discourse 
relation recognition on PDTB 2.0 for 
both explicit and implicit as well. 
1 Introduction 
Discourse relations capture the internal structure 
and logical relationship of coherent text, includ-
ing Temporal, Causal and Contrastive relations 
etc. The ability of recognizing such relations be-
tween text units including identifying and classi-
fying provides important information to other 
natural language processing systems, such as 
language generation, document summarization, 
and question answering. For example, Causal 
relation can be used to answer more sophisti-
cated, non-factoid ?Why? questions. 
Lee et al (2006) demonstrates that modeling 
discourse structure requires prior linguistic anal-
ysis on syntax. This shows the importance of 
syntactic knowledge to discourse analysis. How-
ever, most of previous work only deploys lexical 
and semantic features (Marcu and Echihabi, 
2002; Pettibone and PonBarry, 2003; Saito et al, 
2006; Ben and James, 2007; Lin et al, 2009; Pit-
ler et al, 2009) with only two exceptions (Ben 
and James, 2007; Lin et al, 2009). Nevertheless, 
Ben and James (2007) only uses flat syntactic 
path connecting connective and arguments in the 
parse tree. The hierarchical structured informa-
tion in the trees is not well preserved in their flat 
syntactic path features. Besides, such a syntactic 
feature selected and defined according to linguis-
tic intuition has its limitation, as it remains un-
clear what kinds of syntactic heuristics are effec-
tive for discourse analysis. 
The more recent work from Lin et al (2009) 
uses 2-level production rules to represent parse 
tree information. Yet it doesn?t cover all the oth-
er sub-trees structural information which can be 
also useful for the recognition. 
In this paper we propose using tree kernel 
based method to automatically mine the syntactic 
710
information from the parse trees for discourse 
analysis, applying kernel function to the parse 
tree structures directly. These structural syntactic 
features, together with other flat features are then 
incorporated into our composite kernel to capture 
diverse knowledge for simultaneous discourse 
identification and classification. The experiment    
shows that tree kernel is able to effectively in-
corporate syntactic structural information and 
produce statistical significant improvements over 
flat syntactic path feature for the recognition of 
both explicit and implicit relation in Penn Dis-
course Treebank (PDTB; Prasad et al, 2008). 
We also illustrate that tree kernel approach cov-
ers more structure information than the produc-
tion rules, which allows tree kernel to further 
work on a higher dimensional space for possible 
better discrimination. 
Besides, inspired by the linguistic study on 
tense and discourse anaphor (Webber, 1988), we 
further propose to incorporate temporal ordering 
information to constrain the interpretation of dis-
course relation, which also demonstrates statis-
tical significant improvements for discourse rela-
tion recognition on PDTB v2.0 for both explicit 
and implicit relations. 
The organization of the rest of the paper is as 
follows. We briefly introduce PDTB in Section 
2. Section 3 gives the related work on tree kernel 
approach in NLP and its difference with produc-
tion rules, and also linguistic study on tense and 
discourse anaphor. Section 4 introduces the 
frame work for discourse recognition, as well as 
the baseline feature space and the SVM classifi-
er. We present our kernel-based method in Sec-
tion 5, and the usage of temporal ordering feature 
in Section 6. Section 7 shows the experiments 
and discussions.  We conclude our works in Sec-
tion 8. 
2 Penn Discourse Tree Bank 
The Penn Discourse Treebank (PDTB) is the 
largest available annotated corpora of discourse 
relations (Prasad et al, 2008) over 2,312 Wall 
Street Journal articles. The PDTB models dis-
course relation in the predicate-argument view, 
where a discourse connective (e.g., but) is treated 
as a predicate taking two text spans as its argu-
ments. The argument that the discourse connec-
tive syntactically bounds to is called Arg2, and 
the other argument is called Arg1. 
The PDTB provides annotations for both ex-
plicit and implicit discourse relations. An explicit 
relation is triggered by an explicit connective. 
Example (1) shows an explicit Contrast relation 
signaled by the discourse connective ?but?. 
 
     (1). Arg1. Yesterday, the retailing and finan-
cial services giant reported a 16% drop in 
third-quarter earnings to $257.5 million, 
or 75 cents a share, from a restated $305 
million, or 80 cents a share, a year earlier. 
             Arg2. But the news was even worse for 
Sears's core U.S. retailing operation, the 
largest in the nation. 
 
In the PDTB, local implicit relations are also 
annotated. The annotators insert a connective 
expression that best conveys the inferred implicit 
relation between adjacent sentences within the 
same paragraph. In Example (2), the annotators 
select ?because? as the most appropriate connec-
tive to express the inferred Causal relation be-
tween the sentences. There is one special label 
AltLex pre-defined for cases where the insertion 
of an Implicit connective to express an inferred 
relation led to a redundancy in the expression of 
the relation. In Example (3), the Causal relation 
derived between sentences is alternatively lexi-
calized by some non-connective expression 
shown in square brackets, so no implicit connec-
tive is inserted. In our experiments, we treat Alt-
Lex Relations the same way as normal Implicit 
relations. 
 
     (2). Arg1. Some have raised their cash posi-
tions to record levels. 
            Arg2. Implicit = Because High cash po-
sitions help buffer a fund when the market 
falls. 
 
     (3). Arg1. Ms. Bartlett?s previous work, 
which earned her an international reputa-
tion in the non-horticultural art world, of-
ten took gardens as its nominal subject. 
             Arg2. [Mayhap this metaphorical con-
nection made] the BPC Fine Arts Com-
mittee think she had a literal green thumb. 
 
The PDTB also captures two non-implicit cas-
es: (a) Entity relation where the relation between 
adjacent sentences is based on entity coherence 
(Knott et al, 2001) as in Example (4); and (b) No 
relation where no discourse or entity-based cohe-
rence relation can be inferred between adjacent 
sentences. 
 
711
    (4).   But for South Garden, the grid was to be 
a 3-D network of masonry or hedge walls 
with real plants inside them. 
              In a Letter to the BPCA, kelly/varnell 
called this ?arbitrary and amateurish.? 
 
Each Explicit, Implicit and AltLex relation is 
annotated with a sense. The senses in PDTB are 
arranged in a three-level hierarchy. The top level 
has four tags representing four major semantic 
classes: Temporal, Contingency, Comparison 
and Expansion. For each class, a second level of 
types is defined to further refine the semantic of 
the class levels. For example, Contingency has 
two types Cause and Condition. A third level of 
subtype specifies the semantic contribution of 
each argument. In our experiments, we use only 
the top level of the sense annotations. 
3 Related Work 
Tree Kernel based Approach in NLP.  While 
the feature based approach may not be able to 
fully utilize the syntactic information in a parse 
tree, an alternative to the feature-based methods, 
tree kernel methods (Haussler, 1999) have been 
proposed to implicitly explore features in a high 
dimensional space by employing a kernel func-
tion to calculate the similarity between two ob-
jects directly. In particular, the kernel methods 
could be very effective at reducing the burden of 
feature engineering for structured objects in NLP 
research (Culotta and Sorensen, 2004). This is 
because a kernel can measure the similarity be-
tween two discrete structured objects by directly 
using the original representation of the objects 
instead of explicitly enumerating their features. 
Indeed, using kernel methods to mine structur-
al knowledge has shown success in some NLP 
applications like parsing (Collins and Duffy, 
2001; Moschitti, 2004) and relation extraction 
(Zelenko et al, 2003; Zhang et al, 2006). How-
ever, to our knowledge, the application of such a 
technique to discourse relation recognition still 
remains unexplored. 
Lin et al (2009) has explored the 2-level pro-
duction rules for discourse analysis. However, 
Figure 1 shows that only 2-level sub-tree struc-
tures (e.g. ?? - ?? ) are covered in production 
rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) 
are only captured in the tree kernel, which allows 
tree kernel to further leverage on information 
from higher dimension space for possible better 
discrimination. Especially, when there are 
enough training data, this is similar to the study  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
on language modeling that N-gram beyond uni-
gram and bigram further improves the perfor-
mance in large corpus. 
Tense and Temporal Ordering Information.   
Linguistic studies (Webber, 1988) show that a 
tensed clause ??  provides two pieces of semantic 
information: (a) a description of an event (or sit-
uation) ?? ; and (b) a particular configuration of 
the point of event (??), the point of reference 
(??) and the point of speech (??). Both the cha-
racteristics of ??  and the configuration of ??, ?? 
and ?? are critical to interpret the relationship of 
event ??  with other events in the discourse mod-
el. Our observation on temporal ordering infor-
mation is in line with the above, which is also 
incorporated in our discourse analyzer. 
4 The Recognition Framework 
In the learning framework, a training or testing 
instance is formed by a non-overlapping 
clause(s)/sentence(s) pair. Specifically, since im-
plicit relations in PDTB are defined to be local, 
only clauses from adjacent sentences are paired 
for implicit cases. During training, for each dis-
course relation encountered, a positive instance 
is created by pairing the two arguments. Also a 
Figure 1. Different sub-tree sets for ?1 used by 
2-level production rules and convolution tree 
kernel approaches. ?? -??  and ?1  itself are cov-
ered by tree kernel, while only ?? -??  are covered 
by production rules. 
Decomposition 
C 
E 
G 
F 
H 
A 
B 
D 
(?1) A 
B C 
(??) D 
F E 
(??) 
C 
D 
(??) E 
G 
(??) 
F 
H 
(??) 
D 
E 
G 
F 
H 
(??) (??) A 
C 
D 
B 
D 
E 
G 
F 
H 
C 
(?? ) C (??) 
D 
F E 
(??) A 
C 
D 
B 
F E 
712
set of negative instances is formed by paring 
each argument with neighboring non-argument 
clauses or sentences. Based on the training in-
stances, a binary classifier is generated for each 
type using a particular learning algorithm. Dur-
ing resolution, (a) clauses within same sentence 
and sentences within three-sentence spans are 
paired to form an explicit testing instance; and 
(b) neighboring sentences within three-sentence 
spans are paired to form an implicit testing in-
stance. The instance is presented to each explicit 
or implicit relation classifier which then returns a 
class label with a confidence value indicating the 
likelihood that the candidate pair holds a particu-
lar discourse relation. The relation with the high-
est confidence value will be assigned to the pair. 
4.1 Base Features 
In our system, the base features adopted include 
lexical pair, distance and attribution etc. as listed 
in Table 1. All these base features have been 
proved effective for discourse analysis in pre-
vious work. 
 
 
 
4.2 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn the classifier for discourse 
analysis. In our study, we use Support Vector 
Machine (Vapnik, 1995) to allow the use of ker-
nels to incorporate the structure feature. 
Suppose the training set ? consists of labeled 
vectors { ?? ,?? }, where ??  is the feature vector 
of a training instance and ??  is its class label. The 
classifier learned by SVM is: 
? ? = ???   ????? ? ?? + ?
?=1
  
where ??  is the learned parameter for a feature 
vector ?? , and ? is another parameter which can 
be derived from ??  . A testing instance ? is clas-
sified as positive if ? ? > 01. 
One advantage of SVM is that we can use tree 
kernel approach to capture syntactic parse tree 
information in a particular high-dimension space. 
In the next section, we will discuss how to use 
kernel to incorporate the more complex structure 
feature. 
5 Incorporating Structural Syntactic 
Information 
A parse tree that covers both discourse argu-
ments could provide us much syntactic informa-
tion related to the pair. Both the syntactic flat 
path connecting connective and arguments and 
the 2-level production rules in the parse tree used 
in previous study can be directly described by the 
tree structure. Other syntactic knowledge that 
may be helpful for discourse resolution could 
also be implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to which level 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. 
The value returned from the tree kernel re-
flects the similarity between two instances in 
syntax. Such syntactic similarity can be further 
combined with other flat linguistic features to 
compute the overall similarity between two in-
stances through a composite kernel. And thus an 
SVM classifier can be learned and then used for 
recognition. 
5.1 Structural Syntactic Feature 
Parsing is a sentence level processing. However, 
in many cases two discourse arguments do not 
occur in the same sentence. To present their syn-
tactic properties and relations in a single tree 
structure, we construct a syntax tree for each pa-
ragraph by attaching the parsing trees of all its 
sentences to an upper paragraph node. In this 
paper, we only consider discourse relations with-
in 3 sentences, which only occur within each pa-
                                                 
1 In our task, the result of ? ?  is used as the confidence 
value of the candidate argument pair ? to hold a particular 
discourse relation. 
Feature 
Names 
 Description 
(F1)  cue phrase 
(F2) neighboring punctuation 
(F3)  position of connective if 
presents 
(F4) extents of arguments 
(F5)  relative order of  arguments 
(F6)  distance between  arguments 
(F7)  grammatical role of  arguments 
(F8)  lexical pairs 
(F9) attribution  
Table 1. Base Feature Set 
713
ragraph, thus paragraph parse trees are sufficient. 
Our 3-sentence spans cover 95% discourse rela-
tion cases in PDTB v2.0. 
Having obtained the parse tree of a paragraph, 
we shall consider how to select the appropriate 
portion of the tree as the structured feature for a 
given instance. As each instance is related to two 
arguments, the structured feature at least should 
be able to cover both of these two arguments. 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information would likely be introduced. In our 
study, we examine three structured features that 
contain different substructures of the paragraph 
parse tree: 
Min-Expansion This feature records the mi-
nimal structure covering both arguments 
and connective word in the parse tree. It 
only includes the nodes occurring in the 
shortest path connecting Arg1, Arg2 and 
connective, via the nearest commonly 
commanding node. For example, consi-
dering Example (5), Figure 2 illustrates 
the representation of the structured feature 
for this relation instance. Note that the 
two clauses underlined with dashed lines 
are attributions which are not part of the 
relation. 
 
     (5). Arg1. Suppression of the book, Judge 
Oakes observed, would operate as a prior 
restraint and thus involve the First 
Amendment. 
              Arg2. Moreover, and here Judge Oakes 
went to the heart of the question, ?Respon-
sible biographers and historians constantly 
use primary sources, letters, diaries and 
memoranda.? 
 
Simple-Expansion Min-Expansion could, to 
some degree, describe the syntactic rela-
tionships between the connective and ar-
guments. However, the syntactic proper-
ties of the argument pair might not be 
captured, because the tree structure sur-
rounding the argument is not taken into 
consideration. To incorporate such infor-
mation, Simple-Expansion not only con-
tains all the nodes in Min-Expansion, but 
also includes the first-level children of  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
       these nodes2. Figure 3 illustrates such a 
feature for Example (5). We can see that 
the nodes ?PRN? in both sentences are in-
cluded in the feature. 
Full-Expansion This feature focuses on the 
tree structure between two arguments. It 
not only includes all the nodes in Simple-
Expansion, but also the nodes (beneath 
the nearest commanding parent) that cov-
er the words between the two arguments. 
Such a feature keeps the most information 
related to the argument pair. Figure 4 
                                                 
2 We will not expand the nodes denoting the sentences other 
than where the arguments occur. 
Figure 2. Min-Expansion tree built from gol-
den standard parse tree for the explicit dis-
course relation in Example (5). Note that to 
distinguish from other words, we explicitly 
mark up in the structured feature the arguments 
and connective, by appending a string tag 
?Arg1?, ?Arg2? and ?Connective? respective-
ly. 
Figure 3. Simple-Expansion tree for the expli-
cit discourse relation in Example (5).  
714
shows the structure for feature Full-
Expansion of Example (5). As illustrated, 
different from in Simple-Expansion, each 
sub-tree of ?PRN? in each sentence is ful-
ly expanded and all its children nodes are 
included in Full-Expansion. 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.2 Convolution Parse Tree Kernel 
Given the parse tree defined above, we use the 
same convolution tree kernel as described in 
(Collins and Duffy, 2002) and (Moschitti, 2004). 
In general, we can represent a parse tree ? by a 
vector of integer counts of each sub-tree type 
(regardless of its ancestors):  
? ? = (#?? ???????? ?? ???? 1,? , # ??  
     ???????? ?????? ?,? , # ?? ???????? ??   
     ???? ?). 
This results in a very high dimensionality 
since the number of different sub-trees is expo-
nential in its size. Thus, it is computational in-
feasible to directly use the feature vector ?(?). 
To solve the computational issue, a tree kernel 
function is introduced to calculate the dot prod-
uct between the above high dimensional vectors 
efficiently. 
Given two tree segments ?1  and ?2 , the tree 
kernel function is defined:  
   ? ?1 ,?2 = < ? ?1 ,? ?2 > 
                   =  ? ?1  ? ,? ?2 [?]?  
                   =    ?? ?1 ? ??(?2)??2??2?1??1  
where  ?1and ?2 are the sets of all nodes in trees 
?1and ?2, respectively; and ??(?) is the indicator 
function that is 1 iff a subtree of type ?  occurs 
with root at node ? or zero otherwise. (Collins 
and Duffy, 2002) shows that ?(?1 ,?2) is an in-
stance of convolution kernels over tree struc-
tures, and can be computed in ?( ?1 ,  ?2 ) by 
the following recursive definitions: 
            ? ?1 ,?2 =  ?? ?1 ? ??(?2)?                                                                                                   
(1) ? ?1 ,?2 = 0  if ?1  and ?2  do not have the 
same syntactic tag or their children are different; 
(2) else if both ?1 and  ?2 are pre-terminals (i.e. 
POS tags), ? ?1 ,?2 = 1 ? ?; 
(3)  else, ? ?1 ,?2 = 
              ? (1 + ?(??(
?? (?1)
?=1 ?1 , ?), ??(?2 , ?))),                                 
where ??(?1) is the number of the children of 
?1 , ??(?, ?)  is the ?
??  child of node ?  and ? 
(0 < ? < 1) is the decay factor in order to make 
the kernel value less variable with respect to the 
sub-tree sizes. In addition, the recursive rule (3) 
holds because given two nodes with the same 
children, one can construct common sub-trees 
using these children and common sub-trees of 
further offspring. 
    The parse tree kernel counts the number of 
common sub-trees as the syntactic similarity 
measure between two instances. The time com-
plexity for computing this kernel is ?( ?1 ?
 ?2 ). 
5.3 Composite Tree Kernel 
Besides the above convolution parse tree kernel 
? ????  ?1 , ?2 = ?(?1 ,?2) defined to capture the 
syntactic information between two instances ?1 
and ?2, we also use another kernel ? ????  to cap-
ture other flat features, such as base features (de-
scribed in Table 1) and temporal ordering infor-
mation (described in Section 6). In our study, the 
composite kernel is defined in the following 
way: 
? 1 ?1 , ?2 = ? ? ? ????  ?1 , ?2 + 
                                    1 ? ? ? ? ????  ?1 , ?2 . 
Here, ? (?,?) can be normalized by ?  ?, ? =
? ?, ?  ? ?, ? ? ? ?, ?   and ? is the coeffi-
cient. 
6 Using Temporal Ordering Informa-
tion 
In our discourse analyzer, we also add in tem-
poral information to be used as features to pre-
dict discourse relations. This is because both our 
observations and some linguistic studies (Web-
ber, 1988) show that temporal ordering informa-
tion including tense, aspectual and event orders 
between two arguments may constrain the dis-
course relation type. For example, the connective 
Figure 4. Full-Expansion tree for the explicit 
discourse relation in Example (5).  
715
word is the same in both Example (6) and (7), 
but the tense shift from progressive form in 
clause 6.a to simple past form in clause 6.b, indi-
cating that the twisting occurred during the state 
of running the marathon, usually signals a tem-
poral discourse relation; while in Example (7), 
both clauses are in past tense and it is marked as 
a Causal relation. 
 
     (6). a. Yesterday Holly was running a mara-
thon  
            b. when she twisted her ankle. 
 
      (7). a. Use of dispersants was approved 
            b. when a test on the third day showed  
some positive results. 
 
Inspired by the linguistic model from Webber 
(1988) as described in Section 3, we explore the 
temporal order of events in two adjacent sen-
tences for discourse relation interpretation. Here 
event is represented by the head of verb, and the 
temporal order refers to the logical occurrence 
(i.e. before/at/after) between events. For in-
stance, the event ordering in Example (8) can be 
interpreted as:  
     ????? ?????? ??????? ?????(????) . 
 
     8.  a.  John went to the hospital.  
          b. He had broken his ankle on a patch of 
ice. 
 
We notice that the feasible temporal order of 
events differs for different discourse relations. 
For example, in causal relations, cause event 
usually happens before effect event, i.e.           
     ????? ????? ??????? ?????(??????). 
So it is possible to infer a causal relation in 
Example (8) if and only if 8.b is taken to be the 
cause event and 8.a is taken to be the effect 
event. That is, 8.b is taken as happening prior to 
his going into hospital. 
In our experiments, we use the TARSQI3  sys-
tem to identify event, analyze tense and aspectual 
information, and label the temporal order of 
events. Then the tense and temporal ordering 
information is extracted as features for discourse 
relation recognition. 
 
                                                 
3 http://www.isi.edu/tarsqi/ 
7 Experiments and Results 
In this section we provide the results of a set of 
experiments focused on the task of simultaneous 
discourse identification and classification. 
7.1 Experimental Settings 
We experiment on PDTB v2.0 corpus. Besides 
four top-level discourse relations, we also con-
sider Entity and No relations described in Section 
2. We directly use the golden standard parse 
trees in Penn TreeBank. We employ an SVM 
coreference resolver trained and tested on ACE 
2005 with 79.5% Precision, 66.7% Recall and 
72.5% F1 to label coreference mentions of the 
same named entity in an article. For learning, we 
use the binary SVMLight developed by (Joa-
chims, 1998) and Tree Kernel Toolkits devel-
oped by (Moschitti, 2004). All classifiers are 
trained with default learning parameters. 
The performance is evaluated using Accuracy 
which is calculated as follow: 
???????? =
????????????+ ????????????
???
 
Sections 2-22 are used for training and Sec-
tions 23-24 for testing. In this paper, we only 
consider any non-overlapping clauses/sentences 
pair in 3-sentence spans. For training, there were 
14812, 12843 and 4410 instances for Explicit, 
Implicit and Entity+No relations respectively; 
while for testing, the number was 1489, 1167 and 
380. 
7.2 System with Structural Kernel 
Table 2 lists the performance of simultaneous 
identification and classification on level-1 dis-
course senses. In the first row, only base features 
described in Section 4 are used. In the second 
row, we test Ben and James (2007)?s algorithm 
which uses heuristically defined syntactic paths 
and acts as a good baseline to compare with our 
learned-based approach using the structured in-
formation. The last three rows of Table 2 reports 
the results combining base features with three 
syntactic structured features (i.e. Min-Expansion, 
Simple-Expansion and Full-Expansion) de-
scribed in Section 5. 
We can see that all our tree kernels outperform 
the manually constructed flat path feature in all 
three groups including Explicit only, Implicit 
only and All relations, with the accuracy increas-
ing by 1.8%, 6.7% and 3.1% respectively. Espe-
cially, it shows that structural syntactic informa-
tion is more helpful for Implicit cases which is 
generally much harder than Explicit cases. We  
716
 
 
 
 
conduct chi square statistical significance test on 
All relations between flat path approach and 
Simple-Expansion approach, which shows the 
performance improvements are statistical signifi-
cant (? < 0.05) through incorporating tree ker-
nel. This proves that structural syntactic informa-
tion has good predication power for discourse 
analysis in both explicit and implicit relations. 
We also observe that among the three syntactic 
structured features, Min-Expansion and Simple-
Expansion achieve similar performances which 
are better than the result for Full-Expansion. This 
may be due to that most significant information 
is with the arguments and the shortest path con-
necting connectives and arguments. However, 
Full-Expansion that includes more information 
in other branches may introduce too many details 
which are rather tangential to discourse recogni-
tion. Our subsequent reports will focus on Sim-
ple-Expansion, unless otherwise specified. 
As described in Section 5, to compute the 
structural information, parse trees for different 
sentences are connected to form a large tree for a 
paragraph. It would be interesting to find how 
the structured information works for discourse 
relations whose arguments reside in different 
sentences. For this purpose, we test the accuracy 
for discourse relations with the two arguments 
occurring in the same sentence, one-sentence 
apart, and two-sentence apart. Table 3 compares 
the learning systems with/without the structured 
feature present. From the table, for all three cas-
es, the accuracies drop with the increase of the 
distances between the two arguments. However, 
adding the structured information would bring 
consistent improvement against the baselines 
regardless of the number of sentence distance. 
This observation suggests that the structured syn-
tactic information is more helpful for inter-
sentential discourse analysis.  
We also concern about how the structured in-
formation works for identification and classifica-
tion respectively. Table 4 lists the results for the 
two sub-tasks. As shown, with the structured in-
formation incorporated, the system (Base + Tree 
Kernel) can boost the performance of the two 
baselines (Base Features in the first row andBase 
+ Manually selected paths in the second row), for 
both identification and classification respective-
ly. We also observe that the structural syntactic 
information is more helpful for classification task 
which is generally harder than identification. 
This is in line with the intuition that classifica-
tion is generally a much harder task. We find that 
due to the weak modeling of Entity relations, 
many Entity relations which are non-discourse 
relation instances are mis-identified as implicit 
Expansion relations. Nevertheless, it clearly di-
rects our future work. 
 
 
 
 
 
 
 
 
 
 
 
7.3 System with Temporal Ordering Infor-
mation 
To examine the effectiveness of our temporal 
ordering information, we perform experiments 
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Manually 
selected flat path 
features 
70.3 32 52.6 
Base + Tree kernel 
(Min-Expansion) 
71.9 38.6 55.6 
Base + Tree kernel 
(Simple-Expansion) 
72.1 38.7 55.7 
Base + Tree kernel 
(Full-Expansion) 
71.8 38.4 55.4 
Sentence Dis-
tance 
0 
(959) 
1 
(1746) 
2 
(331) 
Base Features 52 49.2 35.5 
Base + Manually 
selected flat path 
features 
56.7 52 43.8 
Base + Tree 
Kernel 
58.3 55.6 49.7 
Tasks Identifica-
tion 
Classifica-
tion 
Base Features 58.6 50.5 
Base + Manually 
selected flat path 
features 
59.7 52.6 
Base + Tree 
Kernel 
63.3 59.3 
Table 3. Results of the syntactic structured kernel 
for discourse relations recognition with argu-
ments in different sentences apart. 
Table 4. Results of the syntactic structured ker-
nel for simultaneous discourse identification and 
classification subtasks. 
Table 2. Results of the syntactic structured ker-
nels on level-1 discourse relation recognition. 
717
on simultaneous identification and classification 
of level-1 discourse relations to compare with 
using only base feature set as baseline. The re-
sults are shown in Table 5.  We observe that the 
use of temporal ordering information increases 
the accuracy by 3%, 3.6% and 3.2% for Explicit, 
Implicit and All groups respectively. We conduct 
chi square statistical significant test on All rela-
tions, which shows the performance improve-
ment is statistical significant (? < 0.05). It indi-
cates that temporal ordering information can 
constrain the discourse relation types inferred 
within a clause(s)/sentence(s) pair for both expli-
cit and implicit relations. 
 
 
 
 
We observe that although temporal ordering 
information is useful in both explicit and implicit 
relation recognition, the contributions of the spe-
cific information are quite different for the two 
cases. In our experiments, we use tense and as-
pectual information for explicit relations, while 
event ordering information is used for implicit 
relations. The reason is explicit connective itself 
provides a strong hint for explicit relation, so 
tense and aspectual analysis which yields a relia-
ble result can provide additional constraints, thus 
can help explicit relation recognition. However, 
event ordering which would inevitably involve 
more noises will adversely affect the explicit re-
lation recognition performance. On the other 
hand, for implicit relations with no explicit con-
nective words, tense and aspectual information 
alone is not enough for discourse analysis. Event 
ordering can provide more necessary information 
to further constrain the inferred relations. 
7.4 Overall Results 
We also evaluate our model which combines 
base features, tree kernel and tense/temporal or-
dering information together on Explicit, Implicit 
and All Relations respectively. The overall re-
sults are shown in Table 6. 
 
 
 
 
 
 
 
 
 
8 Conclusions and Future Works 
The purpose of this paper is to explore how to 
make use of the structural syntactic knowledge to 
do discourse relation recognition. In previous 
work, syntactic information from parse trees is 
represented as a set of heuristically selected flat 
paths or 2-level production rules. However, the 
features defined this way may not necessarily 
capture all useful syntactic information provided 
by the parse trees for discourse analysis. In the 
paper, we propose a kernel-based method to in-
corporate the structural information embedded in 
parse trees. Specifically, we directly utilize the 
syntactic parse tree as a structure feature, and 
then apply kernels to such a feature, together 
with other normal features. The experimental 
results on PDTB v2.0 show that our kernel-based 
approach is able to give statistical significant 
improvement over flat syntactic path method. In 
addition, we also propose to incorporate tempor-
al ordering information to constrain the interpre-
tation of discourse relations, which also demon-
strate statistical significant improvements for 
discourse relation recognition, both explicit and 
implicit. 
In future, we plan to model Entity relations 
which constitute 24% of Implicit+Entity+No re-
lation cases, thus to improve the accuracy of re-
lation detection. 
Reference 
Ben W. and James P. 2007. Automatically Identifying 
the Arguments of Discourse Connectives. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pages 
92-101.  
Culotta A. and Sorensen J. 2004. Dependency Tree 
Kernel for Relation Extraction. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 423-
429.  
Collins M. and Duffy N. 2001. New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Dis-
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Tem-
poral Ordering 
Information 
70.1 32.6 51.8 
Relations Accuracy 
Explicit 74.2 
Implicit 40.0 
All 57.3 
Table 5. Results of tense and temporal order 
information on level-1 discourse relations. 
Table 6. Overall results for combined model 
(Base  + Tree Kernel + Tense/Temporal). 
718
crete Structures and the Voted Perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2002), 
pages 263-270. 
Collins M. and Duffy N. 2002. Convolution Kernels 
for Natural Language. NIPS-2001. 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz. 
Joachims T.  1999. Making Large-scale SVM Learn-
ing Practical. In Advances in Kernel Methods ? 
Support Vector Learning. MIT Press. 
Knott, A., Oberlander, J., O?Donnel, M., and Mellish, 
C. 2001. Beyond elaboration: the interaction of re-
lations and focus in coherent text. In T. Sanders, J. 
Schilperoord, and W. Spooren, editors, Text Re-
presentation: Linguistic and Psycholinguistics As-
pects, pages 181-196. Benjamins, Amsterdam. 
Lee A., Prasad R., Joshi A., Dinesh N. and Webber  
B. 2006. Complexity of dependencies in discourse: 
are dependencies in discourse more complex than 
in syntax? In Proceedings of the 5th International 
Workshop on Treebanks and Linguistic Theories. 
Prague, Czech Republic, December. 
Lin Z., Kan M. and Ng H. 2009. Recognizing Implicit 
Discourse Relations in the Penn Discourse Tree-
bank. In Proceedings of the 2009 Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP 2009), Singapore, August. 
Marcu D. and Echihabi A. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of the 40th Annual Meeting of ACL, 
pages 368-375. 
Moschitti A. 2004. A Study on Convolution Kernels 
for Shallow Semantic Parsing. In Proceedings of 
the 42th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 335-
342. 
Pettibone J. and Pon-Barry H. 2003. A Maximum En-
tropy Approach to Recognizing Discourse Rela-
tions in Spoken Language. Working Paper. The 
Stanford Natural Language Processing Group, June 
6. 
Pitler E., Louis A. and Nenkova A. 2009. Automatic 
Sense Predication for Implicit Discourse Relations 
in Text. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the Association for 
Computational Linguistics and the 4th International 
Joint Conference on Natural Language Processing 
of the Asian Federation of Natural Language 
Processing (ACL-IJCNLP 2009). 
Prasad R., Dinesh N., Lee A., Miltsakaki E., Robaldo 
L., Joshi A. and Webber B. 2008. The Penn Dis-
course TreeBank 2.0. In Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC 2008). 
Saito M., Yamamoto K. and Sekine S. 2006. Using 
phrasal patterns to identify discourse relations. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics (HLT-
NAACL 2006), pages 133?136, New York, USA. 
Vapnik V.  1995. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York. 
Webber Bonnie. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14:61?73. 
Zelenko D., Aone C. and Richardella A. 2003.  Ker-
nel Methods for Relation Extraction.  Journal of 
Machine Learning Research, 3(6):1083-1106. 
Zhang M., Zhang J. and Su J. Exploring Syntactic 
Features for Relation Extraction using a Convolu-
tion Tree Kernel. In Proceedings of the Human 
Language Technology conference - North Ameri-
can chapter of the Association for Computational 
Linguistics annual meeting (HLT-NAACL 2006), 
New York, USA. 
719
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 226?229,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
ECNU: Effective Semantic Relations Classification without Complicated
Features or Multiple External Corpora
Yuan Chen
?
, Man Lan
?,?
, Jian Su
?
, Zhi Min Zhou
?
, Yu Xu
?
?
East China Normal University, Shanghai, PRC.
?
Institute for Infocomm Research, Singapore.
lanman.sg@gmail.com
Abstract
This paper describes our approach to the
automatic identification of semantic rela-
tions between nominals in English sen-
tences. The basic idea of our strategy
is to develop machine-learning classifiers
which: (1) make use of class-independent
features and classifier; (2) make use of
a simple and effective feature set without
high computational cost; (3) make no use
of external annotated or unannotated cor-
pus at all. At SemEval 2010 Task 8 our
system achieved an F-measure of 75.43%
and a accuracy of 70.22%.
1 Introduction
Knowledge extraction of semantic relations be-
tween pairs of nominals from English text is one
important application both as an end in itself and
as an intermediate step in various downstream
NLP applications, such as information extraction,
summarization, machine translation, QA etc. It is
also useful for many auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing and discourse relation processing.
In the past decade, semantic relation classifica-
tion has attracted a lot of interest from researchers
and a wide variety of relation classification
schemes exist in the literature. However, most
research work is quite different in definition of
relations and granularities of various applications.
That is, there is little agreement on relation
inventories. SemEval 2010 Task 8 (Hendrickx
et al, 2008) provides a new standard benchmark
for semantic relation classification to a wider
community, where it defines 9 relations includ-
ing CAUSE-EFFECT, COMPONENT-WHOLE,
CONTENT-CONTAINER, ENTITY-DESTINATION,
ENTITY-ORIGIN, INSTRUMENT-AGENCY,
MEMBER-COLLECTION, MESSAGE-TOPIC,
PRODUCT-PRODUCER, and a tenth pseudo-
relation OTHER (where relation is not one of the
9 annotated relations).
Unlike the previous semantic relation task in
SemEval 2007 Task 4, the current evaluation pro-
vides neither query pattern for each sentence nor
manually annotated word sense (in WordNet se-
mantic) for each nominals. Since its initiative is
to provide a more realistic real-world application
design that is practical, any classification system
must be usable without too much effort. It needs
to be easily computable. So we need to take into
account the following special considerations.
1. The extracted features for relation are ex-
pected to be easily computable. That is, the
steps in the feature extraction process are to
be simple and direct for the purpose of reduc-
ing errors possibly introduced by many NLP
tools. Furthermore, a unified (global) feature
set is set up for all relations rather than for
each relation.
2. Most previous work at SemEval 2007 Task
4 leveraged on external theauri or corpora
(whether unannotated or annotated) (Davi-
dov and Rappoport, 2008), (Costello, 2007),
(Beamer et al, 2007) and (Nakov and Hearst,
2008) that make the task adaption to different
domains and languages more difficult, since
they would not have such manually classified
or annotated corpus available. From a practi-
cal point of view, our system would make use
of less resources.
3. Most previous work at Semeval 2007 Task
4 constructed several local classifiers on dif-
ferent algorithms or different feature subsets,
one for each relation (Hendrickx et al, 2007)
and (Davidov and Rappoport, 2008). Our ap-
proach is to build a global classifier for all
relations in practical NLP settings.
226
Based on the above considerations, the idea of
our system is to make use of external resources as
less as possible. The purpose of this work is two-
fold. First, it provides an overview of our simple
and effective process for this task. Second, it com-
pares different features and classification strate-
gies for semantic relation.
Section 2 presents the system description. Sec-
tion 3 describes the results and discussions. Sec-
tion 4 concludes this work.
2 System Description
2.1 Features Extraction
For each training and test sentence, we reduce the
annotated target entities e1 and e2 to single nouns
noun1 and noun2, by keeping their last nouns only,
which we assume to be heads.
We create a global feature set for all relations.
The features extracted are of three types, i.e., lex-
ical, morpho-syntactic and semantic. The feature
set consists of the following 6 types of features.
Feature set 1: Lemma of target entities e1
and e2. The lemma of the entities annotated in
the given sentence.
Feature set 2: Stem and POS of words be-
tween e1 and e2. The stem and POS tag of the
words between two nominals. First all the words
between two nominals were extracted and then the
Porter?s stemming was performed to reduce words
to their base forms (Porter, 1980). Meanwhile,
OpenNLP postag tool was used to return part-of-
speech tagging for each word.
Feature set 3: syntactic pattern derived from
syntactic parser between e1 and e2. Typically,
the verb phrase or preposition phrase which con-
tain the nominals are important for relation clas-
sification. Therefore, OpenNLP Parser was per-
formed to do full syntactic parsing for each sen-
tence. Then for each nominal, we look for its par-
ent node in the syntactic tree until the parent node
is a verb phrase or preposition phrase. Then the
label of this phrase and the verb or preposition of
this phrase were extracted as the syntactic features.
Besides, we also extracted other 3 feature types
with the aid of WordNet.
Feature set 4: WordNet semantic class of e1
and e2. The WordNet semantic class of each an-
notated entity in the relation. If the nominal has
two and more words, then we examine the seman-
tic class of ?w1 w2? in WordNet. If no result re-
turned from WordNet, we examine the semantic
class of head in the nominal. Since the cost of
manually WSD is expensive, the system simply
used the first (most frequent) noun senses for those
words.
Feature set 5: meronym-holonym relation
between e1 and e2. The meronym-holonym
relation between nominals. These information
are quite important for COMPONENT-WHOLE and
MEMBER-COLLECTION relations. WordNet3.0
provides meronym and holonym information for
some nouns. The features are extracted in the fol-
lowing steps. First, for nominal e1, we extract its
holonym from WN and for nominal e2, we extract
its Synonyms/Hypernyms. Then, the system will
check if there is same word between e1?s holonym
and e2?s synonym & hypernym. The yes or no
result will be a binary feature. If yes, we also ex-
amine the type of this match is ?part of ? or ?mem-
ber of ? in holonym result. Then this type is also
a binary feature. After that, we exchange the posi-
tion of e1 and e2 and perform the same process-
ing. By creating these features, the system can
also take the direction of relations into account.
Feature set 6: hyponym-hypernym rela-
tion between nominal and the word of ?con-
tainer?. This feature is designed for CONTENT-
CONTAINER relation. For each nominal, WordNet
returns its hypernym set. Then the system examine
if the hypernym set contains the word ?container?.
The result leads to a binary feature.
2.2 Classifier Construction
Our system is to build up a global classifier based
on global feature set for all 9 non-Other relations.
Generally, for this multi-class task, there are two
strategies for building classifier, which both con-
struct classifier on a global feature set. The first
scheme is to treat this multi-class task as an multi-
way classification. Since each pair of nominals
corresponds to one relation, i.e., single label clas-
sification, we build up a 10-way SVM classifier for
all 10 relations. Here, we call it multi-way clas-
sification. That is, the system will construct one
single global classifier which can classify 10 rela-
tions simultaneously in a run. The second scheme
is to split this multi-class task into multiple binary
classification tasks. Thus, we build 9 binary SVM
classifiers, one for each non-Other relation. Noted
that in both strategies the classifiers are built on
global feature set for all relations. For the sec-
ond multiple binary classification, we also exper-
227
imented on different prob. thresholds, i.e., 0.25
and 0.5. Furthermore, in order to reduce errors
and boost performance, we also adopt the major-
ity voting strategy to combine different classifiers.
3 Results and Discussion
3.1 System Configurations and Results
The classifiers for all relations were optimized
independently in a number of 10-fold cross-
validation (CV) experiments on the provided train-
ing sets. The feature sets and learning algorithms
which were found to obtain the highest accuracies
for each relation were then used when applying the
classifiers to the unseen test data.
Table 1 summaries the 7 system configurations
we submitted and their performance on the test
data.
Among the above 7 system, SR5 system shows
the best macro-averaged F1 measure. Table 2 de-
scribes the statistics and performance obtained per
relation on the SR5 system.
Table 3 shows the performance of these 7 sys-
tems on the test data as a function of training set
size.
3.2 Discussion
The first three systems are based on three feature
sets, i.e.,F1-F3, with different classification strat-
egy. The next three systems are based on all six
feature sets with different classification strategy.
The last system adopts majority voting scheme on
the results of four systems, i.e., SR1, SR2, SR4
and SR5. Based on the above series of exper-
iments and results shown in the above 3 tables,
some interesting observations can be found as fol-
lows.
Obviously, although we did not perform WSD
on each nominal and only took the first noun sense
as semantic class, WordNet significantly improved
the performance. This result is consistent with
many previous work on Semeval 2007 Task 4 and
once again it shows that WordNet is important
for semantic relation classification. Specifically,
whether for multi-way classification or multiple
binary classification, the systems involved features
extracted from WordNet performed better than the
others not involved WN, for example, SR4 better
than SR1 (74.82% vs 60.08%), SR5 better than
SR2 (75.43% vs 72.59%), SR6 better than SR3
(72.19% vs 68.50%).
Generally, the performance of multiple binary
classifier is better than multi-way classifier. That
means, given a global feature set for 9 relations,
the performance of 9 binary classifiers is better
than a 10-way classifier. Specifically, when F1-F3
are involved, SR2 (72.59%) and SR3 (68.50%) are
both better than SR1 (60.08%). However, when
F1-F6 feature sets are involved, the performance
of SR4 is between that of SR5 and SR6 in terms of
macro-averaged F
1
measure. With respect to ac-
curacy measure (Acc), SR4 system performs the
best.
Moreover, for multiple binary classification, the
threshold of probability has impact on the perfor-
mance. Generally, the system with prob. threshold
0.25 is better than that with 0.5, for example, SR2
better than SR3 (72.59% vs 68.50%), SR5 better
than SR6 (75.43% vs 72.19%).
As an ensemble system, SR7 combines the re-
sults of SR1, SR2, SR4 and SR5. However, this
majority voting strategy has not shown significant
improvements. The possible reason may be that
these classifiers come from a family of SVM clas-
sifiers and thus the random errors are not signifi-
cantly different.
Besides, one interesting observation is that SR4
system achieved the top 2 performance on TD1
data amongst all participating systems. This
shows that, even with less training data, SR4 sys-
tem achieves good performance.
Acknowledgments
This work is supported by grants from Na-
tional Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D.
?
O S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Ro-
mano and S. Szpakowicz. SemEval-2010 Task 8:
Multi-Way Classification of Semantic Relations Be-
tween Pairs of Nominals. In Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, pp.94-
99, 2010, Uppsala, Sweden.
D. Davidov and A. Rappoport. Classification of
Semantic Relationships between Nominals Using
Pattern Clusters. Proceedings of ACL-08: HLT,
pp.227-235, 2008.
F. J. Costello. UCD-FC: Deducing semantic rela-
tions using WordNet senses that occur frequently
228
Run Feature Set Classifier P (%) R (%) F
1
(%) Acc (%)
SR1 F1-F3 multi-way classification 70.69 58.05 60.08 57.05
SR2 F1-F3 multiple binary (prob. threshold =0.25) 74.02 71.61 72.59 67.10
SR3 F1-F3 multiple binary (prob. threshold =0.5) 80.25 60.92 68.50 62.02
SR4 F1-F6 multi-way classification 75.72 74.16 74.82 70.52
SR5 F1-F6 multiple binary (prob. threshold =0.25) 75.88 75.29 75.43 70.22
SR6 F1-F6 multiple binary (prob. threshold =0.5) 83.08 64.72 72.19 65.81
SR7 F1-F6 majority voting based on SR1, SR2, SR4 and SR5 74.83 75.97 75.21 70.15
Table 1: Summary of 7 system configurations and performance on the test data. Precision, Recall, F1
are macro-averaged for system?s performance on 9 non-Other relations and evaluated with directionality
taken into account.
Run Total # P (%) R (%) F
1
(%) Acc (%)
Cause-Effect 328 83.33 86.89 85.07 86.89
Component-Whole 312 74.82 65.71 69.97 65.71
Content-Container 192 79.19 81.25 80.21 81.25
Entity-Destination 292 79.38 86.99 83.01 86.99
Entity-Origin 258 81.01 81.01 81.01 81.01
Instrument-Agency 156 63.19 58.33 60.67 58.33
Member-Collection 233 73.76 83.26 78.23 83.26
Message-Topic 261 75.2 73.18 74.17 73.18
Product-Producer 231 73.06 61.04 66.51 61.04
Other 454 38.56 40.09 39.31 40.09
Micro-Average 76.88 76.27 76.57 70.22
Macro-Average 75.88 75.29 75.43 70.22
Table 2: Performance obtained per relation on SR5 system. Precision, Recall, F1 are macro-averaged for
system?s performance on 9 non-Other relations and evaluated with directionality taken into account.
Run TD1 TD2 TD3 TD4
F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%) F
1
(%) Acc (%)
SR1 52.13 49.50 56.58 54.84 58.16 56.16 60.08 57.05
SR2 46.24 38.90 47.99 40.45 69.83 64.67 72.59 67.10
SR3 39.89 34.56 42.29 36.66 65.47 59.59 68.50 62.02
SR4 67.95 63.45 70.58 66.14 72.99 68.94 74.82 70.52
SR5 49.32 41.59 50.70 42.77 72.63 67.72 75.43 70.22
SR6 42.88 36.99 45.54 39.57 69.87 64.00 72.19 65.81
SR7 58.67 52.71 58.87 53.18 72.79 68.09 75.21 70.15
Table 3: Performance of these 7 systems on the test data as a function of training set size. The four
training subsets, TD1, TD2, TD3 and TD4, have 1000, 2000, 4000 and 8000 (complete) training samples
respectively. F1 is macro-averaged for system?s performance on 9 non-Other relations and evaluated
with directionality taken into account.
in a database of noun-noun compounds. ACL Se-
mEval?07 Workshop, pp.370C373, 2007.
B. Beamer, S. Bhat, B. Chee, A. Fister, A. Rozovskaya
and R.Girju. UIUC: A knowledge-rich approach
to identifying semantic relations between nominals.
ACL SemEval?07 Workshop, pp.386-389, 2007.
I. Hendrickx, R. Morante, C. Sporleder and A. Bosch.
ILK: machine learning of semantic relations with
shallow features and almost no data. ACL Se-
mEval?07 Workshop, pp.187C190, 2007.
P. Nakov and M. A. Hearst. Solving Relational Simi-
larity Problems Using the Web as a Corpus. In Pro-
ceedings of ACL, pp.452-460, 2008.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
229
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139?146,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
The Effects of Discourse Connectives Prediction on Implicit Discourse
Relation Recognition
Zhi Min Zhou?, Man Lan?,?, Zheng Yu Niu?, Yu Xu?, Jian Su?
?East China Normal University, Shanghai, PRC.
?Baidu.com Inc., Beijing, PRC.
?Institute for Infocomm Research, Singapore.
51091201052@ecnu.cn, lanman.sg@gmail.com
Abstract
Implicit discourse relation recognition is
difficult due to the absence of explicit
discourse connectives between arbitrary
spans of text. In this paper, we use lan-
guage models to predict the discourse con-
nectives between the arguments pair. We
present two methods to apply the pre-
dicted connectives to implicit discourse
relation recognition. One is to use the
sense frequency of the specific connec-
tives in a supervised framework. The
other is to directly use the presence of the
predicted connectives in an unsupervised
way. Results on PDTB2 show that using
language model to predict the connectives
can achieve comparable F-scores to the
previous state-of-art method. Our method
is quite promising in that not only it has
a very small number of features but also
once a language model based on other re-
sources is trained it can be more adaptive
to other languages and domains.
1 Introduction
Discourse relation analysis involves identifying
the discourse relations (e.g., the comparison re-
lation) between arbitrary spans of text, where
the discourse connectives (e.g., ?however?, ?be-
cause?) may or may not explicitly exist in the text.
This analysis is one important application both as
an end in itself and as an intermediate step in var-
ious downstream NLP applications, such as text
summarization, question answering etc.
As discussed in (Pitler and Nenkova., 2009b),
although explicit discourse connectives may have
two types of ambiguity, i.e., one is discourse or
non-discourse usage (?once? can be either a tem-
poral connective or a word meaning ?formerly?),
the other is discourse relation sense ambiguity
(?since? can serve as either a temporal or causal
connective), their study shows that for explicit
discourse relations in Penn Discourse Treebank
(PDTB) corpus, the most general 4 senses, i.e.,
Comparison (Comp.), Contingency (Cont.), Tem-
poral (Temp.) and Expansion (Exp.), can be eas-
ily addressed by the presence of discourse con-
nectives and a simple method only considering the
sense frequency of connectives can achieve more
than 93% accuracy. This indicates the importance
of connectives for discourse relation recognition.
However, with implicit discourse relation
recognition, there is no connective between the
textual arguments, which results in a very difficult
task. In recent years, a multitude of efforts have
been employed to solve this task. One approach
is to exploit various linguistically informed fea-
tures extracted from human-annotated corpora in
a supervised framework (Pitler et al, 2009a) and
(Lin et al, 2009). Another approach is to perform
recognition without human-annotated corpora by
creating synthetic examples of implicit relations in
an unsupervised way (Marcu and Echihabi, 2002).
Moreover, our initial study on PDTB implicit
relation data shows that the averaged F-score for
the most general 4 senses can reach 91.8% when
we obtain the sense of test examples by map-
ping each implicit connective to its most frequent
sense (i.e., sense recognition using gold-truth im-
plicit connectives). This high F-score performance
again proves that the connectives are very crucial
source for implicit relation recognition.
In this paper, we present a new method to ad-
dress the problem of recognizing implicit dis-
course relation. This method is inspired by the
above observations, especially the two gold-truth
results, which reveals that discourse connectives
are very important signals for discourse relation
recognition. Our basic idea is to recover the im-
plicit connectives (not present in real text) be-
tween two spans of text with the use of a language
139
model trained on large amount of raw data without
any human-annotation. Then we use these pre-
dicted connectives to generate feature vectors in
two ways for implicit discourse relation recogni-
tion. One is to use the sense frequency of the spe-
cific connectives in a supervised framework. The
other is to directly use the presence of the pre-
dicted connectives in an unsupervised way.
We performed evaluation on explicit and im-
plicit relation data sets in the PDTB 2 corpus. Ex-
perimental results showed that the two methods
achieved comparable F-scores to the state-of-art
methods. It indicates that the method using lan-
guage model to predict connectives is very useful
in solving this task.
The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our methods for implicit discourse relation
recognition. Section 4 presents experiments and
results. Section 5 offers some conclusions.
2 Related Work
Existing works on automatic recognition of im-
plicit discourse relations fall into two categories
according to whether the method is supervised or
unsupervised.
Some works perform relation recognition with
supervised methods on human-annotated corpora,
for example, the RST Bank (Carlson et al, 2001)
used by (Soricut and Marcu, 2003), adhoc anno-
tations used by (Girju, 2003) and (Baldridge and
Lascarides, 2005), and the GraphBank (Wolf et al,
2005) used by (Wellner et al, 2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al, 2006) has sig-
nificantly expanded the discourse-annotated cor-
pora available to researchers, using a comprehen-
sive scheme for both implicit and explicit rela-
tions. (Pitler et al, 2009a) performed implicit re-
lation classification on the second version of the
PDTB. They used several linguistically informed
features, such as word polarity, verb classes, and
word pairs, showing performance increases over a
random classification baseline. (Lin et al, 2009)
presented an implicit discourse relation classifier
in PDTB with the use of contextual relations, con-
stituent Parse Features, dependency parse features
and cross-argument word pairs. Although both of
two methods achieved the state of the art perfor-
mance for automatical recognition of implicit dis-
course relations, due to lack of human-annotated
corpora, their approaches are not very useful in the
real word.
Another line of research is to use the unsuper-
vised methods on unhuman-annotated corpus.
(Marcu and Echihabi, 2002) used several pat-
terns to extract instances of discourse relations
such as contrast and elaboration from unlabeled
corpora. Then they used word-pairs between argu-
ments as features for building classification mod-
els and tested their model on artificial data for im-
plicit relations.
Subsequently other studies attempt to ex-
tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered that
Marcu and Echihabi?s models do not perform as
well on implicit relations as one might expect
from the test accuracy on synthetic data. (Gold-
ensohn, 2007) extended the work of (Marcu and
Echihabi, 2002) by refining the training and clas-
sification process using parameter optimization,
topic segmentation and syntactic parsing. (Saito
et al, 2006) followed the method of (Marcu and
Echihabi, 2002) and conducted experiments with
a combination of cross-argument word pairs and
phrasal patterns as features to recognize implicit
relations between adjacent sentences in a Japanese
corpus.
Previous work showed that with the use of some
patterns, structures, or the pairs of words, rela-
tion classification can be performed using unsu-
pervised methods.
In contrast to existing work, we investigated a
new knowledge source, i.e., implicit connectives
predicted using a language model, for implicit re-
lation recognition. Moreover, this method can
be applied in both supervised and unsupervised
ways by generating features on labeled and unla-
beled training data and then performing implicit
discourse connectives recognition.
3 Methodology
3.1 Predicting implicit connectives via a
language model
Previous work (Pitler and Nenkova., 2009b)
showed that with the presence of discourse con-
nectives, explicit discourse relations in PDTB can
be easily identified with more than 90% F-score.
Our initial study on PDTB human-annotated im-
plicit relation data shows that the averaged F-score
for the most general 4 senses can reach 91.8%
when we simply map each implicit connective to
140
its most frequent sense. These high F-scores indi-
cate that the connectives are very crucial source of
information for both explicit and implicit relation
recognition. However, for implicit relations, there
are no explicitly discourse connectives in real text.
This built-in absence makes the implicit relation
recognition task quite difficult. In this work we
overcome this difficulty by inserting connectives
into the two arguments with the use of a language
model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two
arguments, denoted as Arg1 and Arg2. Typi-
cally, there are two possible positions for most
of implicit connectives, i.e., the position before
Arg1 and the position between Arg1 and Arg2.
Given a set of implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as Ppl(Sci,j). According to
the value of Ppl(Sci,j) (the lower the better), we
can rank these sentences and select the connec-
tives in top N sentences as implicit connectives
for this argument pair. Here the language model
may be trained on any large amount of unanno-
tated corpora that can be cheaply acquired. Typi-
cally, a large corpora with the same domain as the
test data will be used for training language model.
Therefore, we chose news corpora, such as North
American News Corpora.
After that, we use the top N predicted connec-
tives to generate different feature vectors and per-
form the classification in two ways. One is to use
the sense frequency of predicted connectives in a
supervised framework. The other is to directly use
the presence of the predicted connectives in an un-
supervised way. The two approaches are described
as follows.
3.2 Using sense frequency of predicted
discourse connectives as features
After the above procedure, we get a sorted set of
predicted discourse connectives. Due to the pres-
ence of an implicit connective, the implicit dis-
course relation recognition task can be addressed
with the methods for explicit relation recognition,
e.g., sense classification based only on connectives
(Pitler et al, 2009a). Inspired by their work, the
first approach is to use sense frequency of pre-
dicted discourse connectives as features. We take
the connective with the lowest perplexity value
(i.e., top 1 connective) as the real connective for
the arguments pair. Then we count the sense
frequency of this connective on the training set.
Figure 1 illustrates the procedure of generating
predicted discourse connective from a language
model and calculating its sense frequency from
training data. Here the calculation of sense fre-
quency of connective is based on the annotated
training data which has labeled discourse rela-
tions, thus this method is a supervised one.
Figure 1: Procedure of generating a predicted dis-
course connective and its sense frequency from the
training set and a language model.
Then we can directly use the sense frequency
to generate a 4-feature vector to perform the clas-
sification. For example, the sense frequency of
the connective but in the most general 4 senses
can be counted from training set as 691, 6, 49,
2, respectively. For a given pair of arguments,
if but is predicted as the top 1 connective based
on a language model, a 4-dimension feature vec-
tor (691, 6, 49, 2) is generated for this pair and
used for training and test procedure. Figure 2
and 3 show the training and test procedure for this
method.
Figure 2: Training procedure for the first ap-
proach.
141
Figure 3: Test procedure for the first approach.
3.3 Using presence or absence of predicted
discourse connective as features
(Pitler et al, 2008) showed that most connectives
are unambiguous and it is possible to obtain high-
accuracy in prediction of discourse senses due to
the simple mapping relation between connectives
and senses. Given two examples:
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
The two connectives, i.e., but in E1 and because
in E2, convey the Comparison and Contingency
senses respectively. In most cases, we can easily
recognize the relation sense by the appearance of
a discourse connective since it can be interpreted
in only one way. That means the ambiguity of
the mapping between sense and connective is quite
low. Therefore, the second approach is to use only
the presence of the top N predicted discourse con-
nectives to generate a feature vector for a given
pair of arguments.
4 Experiment
4.1 Data sets
We used PDTB as our data set to perform the eval-
uation of our methods. The corpus contains anno-
tations of explicit and implicit discourse relations.
The first evaluation is performed on the annotated
implicit data set. Following the work of (Pitler et
al., 2009a), we used sections 2-20 as the training
set, sections 21-22 as the test set and sections 0-
1 as the development set for parameter optimiza-
tion (e.g., N value). The second evaluation is per-
formed on the annotated explicit data set. We fol-
low the method used in (Sporleder and Lascarides,
2008) to remove the discourse connective from the
explicit instances and consider these processed in-
stances as implicit ones.
We constructed four binary classifiers to recog-
nize each main senses (i.e., Cont., Cont., Exp.,
Temp.) from the rest. For each sense we used
equal numbers of positive and negative instances
in training set. The negative instances were cho-
sen at random from the rest of training set. For
both evaluations all instances in sections 21-22
were used as test set. Table 1 lists the numbers
of positive and negative instances for each sense
in training, development and test sets of implicit
and explicit relation data sets.
4.2 Evaluation and classifier
To evaluate the performance of above systems, we
used two widely-used measures, F-score ( i.e., F1)
and accuracy. In addition, in this work we used
the LIBSVM toolkit to construct four linear SVM
classifiers for each sense.
4.3 Preprocessing
We used the SRILM toolkit to build a language
model and calculated the perplexity value for each
training and test sample. The steps are described
as follows. First, since perplexity is an intrin-
sic score to measure the similarity between train-
ing and test samples, in order to fit the restric-
tion of perplexity we chose 3 widely-used cor-
pora in the Newswire domain to train the language
model, i.e., (1) the New York part of BLLIP North
American News Text (Complete), (2) the Xin and
(3) the Ltw parts of the English Gigaword Fourth
Edition. For the BLLIP corpus with 1,796,386
automatically parsed English sentences, we con-
verted the parsed sentences into original textual
data. Some punctuation marks such as commas,
periods, minuses, right/left parentheses are con-
verted into their original form. For the Xin and
Ltw parts, we only used the Sentence Detector
toolkit in OpenNLP to split each sentence. Finally
we constructed 3-, 4- and 5-grams language mod-
els from these three corpora. Table 2 lists statis-
tics of different n-grams in the different language
models and different corpora.
Next, for each instance we combined its Arg1
and Arg2 with connectives obtained from PDTB.
There are two types of connectives, single con-
nectives (e.g. ?because? and ?but?) and paral-
142
Table 1: Statistics of positive and negative instances for each sense in training, development and test sets
of implicit and explicit relation data sets.
Implicit Explicit
Comp. Cont. Exp. Temp. Comp. Cont. Exp. Temp.
Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663
Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247
Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124
Table 2: Statistics of different n-grams in the dif-
ferent language models and different corpora.
n-gram BLLIP - Gigaword- Gigaword-
New York Xin Ltw
1-gram 1638156 2068538 2276491
2-grams 26156851 23961796 33504873
3-grams 80876435 77799100 101855639
4-grams 127142452 134410879 159791916
5-grams 146454530 168166195 183794771
lel connectives (such as ?not only . . . , but also?).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of
possible implicit connectives {ci}, for a single
connective ci, we constructed two synthetic sen-
tences, ci+Arg1+Arg2 and Arg1+ci+Arg2. In case
of parallel connectives, we constructed one syn-
thetic sentence like ci,1+Arg1+ci,2+Arg2.
As a result, we obtain 198 synthetic sentences
(|ci| ? 2 for single connective or |ci| for parallel
connective) for each pair of arguments. Then we
converted all words to lower cases and used the
language model trained in the above step to calcu-
late its perplexity (the lower the better) value on
sentence level. The sentences were ranked from
low to high according to their perplexity scores.
For example, given a sentence with arguments pair
as follows:
Arg1: it increased its loan-loss reserves by $93
million after reviewing its loan portfolio,
Arg2: before the loan-loss addition it had operat-
ing profit of $10 million for the quarter.
we got the perplexity (Ppl) values for this argu-
ments pair in combination with two connectives
(but and by comparison) in two positions as fol-
lows:
1. but + Arg1 + Arg2: Ppl= 349.622
2. Arg1 + but + Arg2: Ppl= 399.339
3. by comparison + Arg1 + Arg2: Ppl= 472.206
4. Arg1 + by comparison + Arg2: Ppl= 543.051
In our second approach described in Section
3.3, we considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is,
the presence or absence of the specific connective.
According to the value of Ppl(Sci,j), we tried var-
ious N values on development set to get the opti-
mal N value.
4.4 Results
Table 3 summarizes the best performance
achieved using gold-truth implicit connectives,
the previous state-of-art performance achieved
by (Pitler et al, 2009a) and our approaches.
The first line shows the result by mapping the
gold-truth implicit connectives directly to the
relation?s sense. The second line presents the best
result of (Pitler et al, 2009a). One thing worth
mentioning here is that for the Expansion relation,
(Pitler et al, 2009a) expanded both training and
test sets by including EntRel relation as positive
examples, which makes it impossible to perform
direct comparison. The third and fourth lines
show the best results using our first approach,
where the sense frequency is counted on explicit
and implicit training set respectively. The last line
shows the best result of our second approach only
considering the presence of top N connectives.
Table 4 summarizes the best performance using
gold-truth explicit connectives reported in (Pitler
and Nenkova., 2009b) and our two approaches.
Figure 4 shows the curves of averaged F-scores
on implicit connective classification with differ-
ent n-gram language models. From this figure we
can see that all 4-grams language models achieved
around 0.5% better averaged F-score than 3-grams
models. And except for Ltw corpus, other 5-grams
models achieved lower averaged F-score than 4-
grams models. Specially the 5-grams result of
New York corpus is much lower than its 3-grams
result.
Figure 5 shows the averaged F-scores of dif-
ferent top N on the New York corpus with 3-,
4- and 5-grams language models. The essential
143
Table 3: Best result of implicit relations compared with state-of-art methods.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using
gold-truth implicit connectives 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)
Best result in (Pitler et al, 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)
Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)
Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)
Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)
Table 4: Best result of explicit relation conversion to implicit relation compared with results using the
same method.
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using gold-truth
explicit connectives in (Pitler et al, 2009a) N/A N/A N/A N/A N/A(93.67)
Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)
Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)
0 10 20 30 40 50 60 70 80 90 100110120130140150160170180190200
30.0
30.5
31.0
31.5
32.0
32.5
33.0
33.5
34.0
34.5
 
 NY 3-gram
 NY 4-gram
 NY 5-gram
Top N value
A
v
e
r
a
g
e
d
 
F
-
S
c
o
r
e
Figure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with different
top N values.
trend of these curves cannot be summarized in
one sentence. But we can see that the best aver-
aged F-scores mostly appeared in the range from
100 ? 160. For 4-grams and 5-grams models, the
system achieved the top averaged F-scores when
N = 20 as well.
4.5 Discussion
Experimental results on PDTB showed that using
predicted connectives achieved the comparable F-
scores of the state-of-art method.
From Table 3 we can find that our results are
closely to the best performance of previous state-
of-art methods in terms of averaged F-score. On
the Comparison sense, our first approach has an
improvement of more than 4% F-score on the pre-
vious state-of-art method (Pitler et al, 2009a). As
we mentioned before, for the Expansion sense,
they included EntRel relation to expand the train-
ing set and test set, which makes it impossible to
perform a direct comparison. Since the positive in-
stances size has been increased by 50%, they may
achieve a higher F-score than our approach. For
other relations, our best performance is slightly
lower than theirs. While bearing in mind that our
approach only uses a very small amount of fea-
tures for implicit relation recognition. Compared
144
3-gram 4-gram 5-gram
31.0
31.2
31.4
31.6
31.8
32.0
32.2
32.4
32.6
 
 New York
 Xin
 Ltw
n-gram
A
v
e
r
a
g
e
d
 
F
-
s
c
o
r
e
Figure 4: Curves of averaged F-score on implicit
connective classification with n-Gram language
model.
with other approaches involving thousands of fea-
tures, our method is quite promising.
From Table 4 we observe comparable averaged
F-score (39.98% F-score) on explicit relation data
set to that on implicit relation data set. Previ-
ously, (Sporleder and Lascarides, 2008) also used
the same conversion method to perform implicit
relation recognition on different corpora and their
best result is around 33.69% F-score. Although
the two results cannot be compared directly due to
different data sets, the magnitude of performance
quantities is comparable and reliable.
By comparing with the above different systems,
we find several useful observations. First, our
method using predicted implicit connectives via a
language model can help the task of implicit dis-
course relation recognition. The results are com-
parable to the previous state-of-art studies. Sec-
ond, our method has a lot of advantages, i.e., a
very small amount of features (several or no more
than 200 vs. ten thousand), easy computation
(only based on the trained language model vs. us-
ing a lot of NLP tools to extract a large amount of
linguistically informed features) and fast running,
which makes it more practical in real world appli-
cation. Furthermore, since the language model can
be trained on many corpora whether annotated or
unannotated, this method is more adaptive to other
languages and domains.
5 Conclusions
In this paper we have presented an approach to
implicit discourse relation recognition using pre-
dicted implicit connectives via a language model.
The predicted connectives have been used for im-
plicit relation recognition in two ways, i.e., super-
vised and unsupervised framework. Results on the
Penn Discourse Treebank 2.0 show that the pre-
dicted discourse connectives can help implicit re-
lation recognition and the two algorithms achieve
comparable F-scores with the state-of-art method.
In addition, this method is quite promising due to
its simple, easy to retrieve, fast run and increased
adaptivity to other languages and domains.
Acknowledgments
We thank the reviewers for their helpful com-
ments and Jonathan Ginzburg for his mentor-
ing. This work is supported by grants from
National Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
References
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, College
Park, MD,2001.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL 2003 Workshops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics.
145
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.
Lee, A. Joshi. 2008. Easily Identifiable Dis-
course Relations. Coling 2008: Companion vol-
ume: Posters.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. Proceedings of the COLING/ACL Work-
shop on Sentiment and Subjectivity in Text.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC?08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. Proceedings of the Human Language Technol-
ogy and North American Association for Computa-
tional Linguistics Conference.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
146
