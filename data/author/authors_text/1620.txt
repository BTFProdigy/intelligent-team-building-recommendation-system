Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 109?116,
Prague, June 2007. c?2007 Association for Computational Linguistics
Cognate identification and alignment using practical orthographies
Michael Cysouw
Max Planck Institute for Evolutionary
Anthropology, Leipzig
cysouw@eva.mpg.de
Hagen Jung
Max Planck Institute for Evolutionary
Anthropology, Leipzig
jung@eva.mpg.de
Abstract
We use an iterative process of multi-gram
alignment between associated words in dif-
ferent languages in an attempt to identify
cognates. To maximise the amount of data,
we use practical orthographies instead of
consistently coded phonetic transcriptions.
First results indicate that using practical or-
thographies can be useful, the more so when
dealing with large amounts of data.
1 Introduction
The comparison of lexemes across languages is a
powerful method to investigate the historical rela-
tions between languages. A central prerequisite for
any interpretation of historical relatedness is to es-
tablish lexical cognates, i.e. lexemes in different
languages that are of shared descend (in contrast to
similarity by chance). If a pair of lexemes in two dif-
ferent languages stem from the same origin, this can
be due to the fact that both languages derive from a
common ancestor language, but it can also be caused
by influence from one language on another (or influ-
ence on both language from a third language). To
decide whether cognates are indicative of a common
ancestor language (?vertical transmission?) or due to
language influence (?horizontal transmission?) is a
difficult problem with no shortcuts. We do not think
that one kind of cognacy is more interesting that an-
other. Both loans (be it from a substrate or a super-
strate) and lexemes derived from a shared ancestor
are indicative of the history of a language, and both
should be acknowledged in the unravelling of lin-
guistic (pre)history.
In this paper, we approach the identification of
cognate lexemes on the basis of large parallel lex-
ica between languages. This approach is an explicit
attempt to reverse the ?Swadesh-style? wordlist
method. In the Swadesh-style approach, first mean-
ings are selected that are assumed to be less prone
to borrowing, then cognates are identified in those
lists, and these cognates are then interpreted as in-
dicative of shared descend. In contrast, we propose
to first identify (possible) cognates among all avail-
able information, then divide these cognates into
strata, and then interpret these strata in historical
terms. (Because of limitations of space, we will
only deal with the first step, the identification of cog-
nates, in this paper.) This is of course exactly the
route of the traditional historical-comparative ap-
proach to language comparison. However, we think
that much can be gained by applying computational
approaches to this approach.
A major problem arises when dealing with large
quantities of lexical material from many different
languages. In most cases it will be difficult (or very
costly and time consuming in the least) to use co-
herent and consistent phonetic transcriptions of all
available information. Even if we would have dictio-
naries with phonetic transcriptions for all languages
that we are interested in, this would not necessarily
help, as the details of phonetic transcription are nor-
mally not consistent across different authors. In this
paper, we will therefore attempt to deal with unpro-
cessed material in practical orthographies. This will
of course pose problems for history-ridden orthogra-
phies like in English or French. However, we beleve
that for most of the world?s languages the practical
109
orthographies are not as inconsistent as those (be-
cause they are much younger) and might very well
be useful for linguistic purposes.
In this paper, we will first discuss the data used in
this investigation. Then we will describe the algo-
rithm that we used to infer alignments between word
pairs. Finally, we will discuss a few of the results
using this algorithm on large wordlists in practical
orthography.
2 Resources
In this study we used parallel wordlists that
we extracted from the Intercontinental Dictio-
nary Series (IDS) database, currently under
development at the Max Planck Institute for
Evolutionary Anthropology in Leipzig (see
http://www.eva.mpg.de/lingua/files/ids.html for
more information). The IDS wordlists contain
more than thousand entries of basic words from
each language, and many entries contain alternative
wordforms. At this time, there are only a few
basic transcription languages (English, French
and Portuguese) and some Caucasian languages
available. We choose some of them for the purpose
of the present study and preprocessed the data.
To compare languages, we chose only word pairs
that were available and non-compound in both
languages. For all words that occurred several times
in the whole collection of a language, we accepted
only one randomly choosen wordform and left out
all others. We also deleted content in brackets or
in between other special characters. If, after these
preparation, a wordform is still longer than twelve
UTF-8 characters, we disregard these for reasons
of computational efficiency. After this, we are still
left with a large number of about 900 word pairs for
each pair of languages.
3 Alignment
An alignment of two words wa and wb is a bijective
and maintained ordered one-to-one correspondence
from all subsequences sa of the word wa with wa =
concat(sa1 , sa2 , . . . , sak) to all subsequences sb of
the word wb with wb = concat(sb1 , sb2 , . . . , sbk). It
is possible that one of the associated subsequences
is the empty word . In general one may construct
a distance measure from such a linked sequence of
two given words by assigning a cost for each single
link of the alignment. There are many such align-
ment/cost functions described in the literature, and
they are often used to calculate a distance measure
between two sequences of characters (Inkpen et al,
2005). A measurement regularly used for linguistic
sequences is the Levenshtein distance, or a modi-
fications of it. Other distance measures detect, for
example, the longest common subsequences or the
longest increasing subsequences.
It is our special interest to use multi-character
mappings for calculating a distance between two
words. Therefore, we adapt and extend the Leven-
shtein measurement. First, we allow for mapping
of any arbitrary string length (not just strings of one
character as in Levenshtein) and, second, we assign
a continuous cost between 0 and 1 for every map-
ping.
Our algorithm consist basically of two steps. In
the first step, all possible subsequence pairs between
associated words are considered, and a cost function
is extracted for every multi-gram pair from their co-
occurrences in the whole wordlist. In a second step,
this cost function is used to infer an alignment be-
tween whole words. On the basis of this alignment
a new cost function is established for all multi-gram
pairs. This second step can be iterated until the cost
function stabilizes.
3.1 Cost of an multi-gram pair
For every pair of subsequences sai and sbj we count
the number of co-occurrences. The subsequences
sai and sbj co-occur when they are found in two as-
sociated words wa and wb from a language wordlist
of two languages La and Lb. We then use a sim-
ple Dice coefficient as a cost function between all
possible subsequences. For computational reasons,
it is necessary to limit the size of the multi-grams
considered. We decided to limit the multi-gram
size to a number of maximally four UTF-8 char-
acters. Still, in the first step of our algorithm,
there is a very large set of such subsequence pairs
because all possible combinations are considered.
When an alignment is inferred in the iterative pro-
cess, only the aligned subsequences are counted as
co-occurrences, so the number of possible combi-
nations is considerably lower. Further, to prevent
low frequent co-occurrences to have a dispropor-
110
tional impact, we added an attestation threshold of
2% of the wordlist size for two subsequences to be
accepted for the alignment process.
3.2 Alignment of words
An alignment of two words is a complete ordered
linking of subsequences. We annotate it in the
following way (vertical dashes delimit the subse-
quences; note that subsequences may be empty):
( | w | ool)(wers | t~ | )
There is a huge amount of possible combinations
of aligned subsequences. On the basis of the cost
function, a distance is established for every word
pair alignment. The summation of all multi-gram
mapping costs represents the distance of the align-
ment. Because we are dealing with multi-grams of
variable length, alternative alignments of the same
word pair will consist of a different number of sub-
sequences. So, simple summation would lead to dis-
tances out of the range from 0 to 1. To counteract
this, we normalized the word distance. We weighted
each subsequence relative to the number of charac-
ters in the subsequence. For example, the mapping
of w and t~ in the example above would be multi-
plied by 310 , because w and t~ have together 3 char-
acters and the complete words have in total 10 char-
acters.
To make use of efficient divide and conquer solv-
ing strategies and to get meaningful linguistic state-
ments with the base of the calculated best align-
ments, we decided to look for a special subset of
best alignments. As (Kondrak, 2002) pointed out,
there are some situations in which the consideration
of local alignment gets the required results. If only
a part of a word aligning sequence is of high simi-
larity then sometimes a linguistic justification of the
whole word similarity is given. Those alignments
contain the lowest cost multi-gram pairs, but are not
necessarily of best similarity in total.
To illustrate the difference between local and
global alignment, consider an example that shows
different results, depending whether the total sum of
multi-gram similarities is taken or the best local one.
Look at the two words ?abc? and ????? and a part of
its multi-gram cost function in Table 1. The sum-
mation of the costs would prefer alignment A2, as
can be seen in Table 2. But we prefer A1, because
it contains the subsequence pair (ab | ??) with the
multi-gram 1 multi-gram 2 cost
ab ?? 0.1
bc ?? 0.3
a ? 0.4
c ? 0.8
.
.
.
.
.
.
.
.
.
Table 1: Costs for constructed subsequence pairs
(ordered by cost)
Index Alignment Distance
A2 (a | bc)(? | ??) 0.4 + 0.3 = 0.7
A1 (ab | c)(?? | ?) 0.1 + 0.8 = 0.9
.
.
.
.
.
.
.
.
.
Table 2: Alignments with distance
lowest cost.
With these assumptions, we composed a fast and
easy method to find the best alignment. We pre-
fer alignments where some links are very good,
but the rest might not be. We assume that words
are more related to each other, if there are such
highly rated pairs. This approach can also be found
in other string based comparing methods like, for
example, the Longest Common Increasing Subse-
quence method, which calculates the longest equal
multi-gram and neglects the rest of the word. We
first order all possible multi-gram mappings by their
costs and pick the subsequence pair with the low-
est cost. Starting from this mapping seed, we look
for mappings for the rest of the word pair, both be-
fore and after the initial mapped subsequence. For
both these suffixes and prefixes, we again search for
the subsequence with the lowest cost. This process
is re-applied until the whole words are mapped. If
there is more than one optimal linking subsequence
pair, then all possible alignments are considered. In
this way, we do not restrict, in contrast to Kondrak,
which position for the multi-gram mapping will be
preferred for the local alignment. The algorithm
runs in O(n6). It takes O(n4) time for all combina-
tions of different multi-gram pairs within O(n) steps
in O(n) iterations.
111
4 Experimental Evaluation
As mentioned above, we applied our model to some
test data from the IDS database. For later anal-
yses, we also constructed some random wordlists.
With these we are able to say something about how
significant our results are. To make these random
wordlists we remap each word wa from La to an ar-
bitrarily chosen word wb from collection Lb. This
new mapped word was adjusted to the size of the
originally associated word from Lb. The adjustment
works by stretching or shrinking the new word to the
required length by doubling the word several times
and cutting of the overlaying head or tail afterwards.
In this way, we controlled for word length and multi-
gram frequencies. This randomization process was
performed five times from La to Lb, and five the
times from Lb to La, and the results were averaged
over all these ten cases.
For the calculation process, we stored all lists in
SQL tables. We first built a preprocessed work-
ing table with the lexemes from the languages to be
compared, and afterwards we constructed the result-
ing tables that hold all the results:
? compare table: the word pairs, their alignments
and alignment goodness;
? subsequence table: the subsequence pairs
found and their co-occurrence coefficients;
? random compare table: pseudo random word
pairs like the compare table;
? random subsequence table: the subsequence
pairs found from random compare table.
Table 3 consists of the best alignments for word pairs
of English and French after 30 iterations, and Table
4 shows the best alignments for the comparison of
English and Hunzib (a Caucasian language). First
note that our algorithm works independently of the
orthography used. We do not assume that the same
UTF-8 characters in the two languages are identi-
cal. The fact that ?c? is mapped between English
clan and French clan is a result of the statistical dis-
tribution of these characters in the two languages.
This orthography-independence means that we can
apply our algorithm without modifications to cyrillic
scripts as shown with the English-Hunzib compari-
son. Second, we payed close attention to the fact that
the word similarity values are comparable among
different language comparisons. This means that it
is highly significant that the highest word similar-
ities between English and French are much higher
than those between English and Hunzib (actually,
the alignments between English and Hunzib are non-
sensical, but more about that later). Further, our al-
gorithm finds vowel-consonant multi-grams in some
cases (e.g. see Table 5). As far as we can see, there
are not linguistically meaningful and should be con-
sidered an artifact of our current approach. We hope
to fine-tune the algorithm in the future to prevent this
behavior.
Our method finds alignments, but also the subse-
quences in the alignments are of interest. The best
mapped multi-grams between English and French
are illustrated in Table 5. Strangely, the highest
ranked ones are a few vowel+consonant bigrams,
that occur not very often. Since the Dice coefficient
depends on the size of the investigated collection, we
assumed a minimum frequency of co-occurrences in
each calculation step of 2% of the collection size
(which is 20 cases in the English-French compari-
son). The high-ranked bigrams are all just above this
threshold. Therefore, we might argue that all the bi-
grams from the top of the list are a side-effect of the
collection size itself.
Following these bigrams are many one-to-
one matches of all alphabetic characters except
?j,k,q,w,x,y,z?. These mappings are found without
assuming any similarity based on the UTF-8 encod-
ing of the characters. What we actually find here is
a mapping for the orthography of the stratum of the
French loan words in English. As can be seen in the
histogram in Figure 1, the mapping between multi-
grams falls off dramatically after these links.
112
English French Alignment similarity
tribe,clan tribu,clan ( | c | | l | | an | ) ( | c | | l | | an | ) 0.955872
long long ( | l | | on | | g | ) ( | l | | on | | g | ) 0.925542
lion lion ( | l | | i | | on | ) ( | l | | i | | on | ) 0.916239
canoe canoe,pirogue ( | c | | an | | o | | e | ) ( | c | | an | | o | | e | ) 0.911236
famine famine,disette ( | f | | a | | m | | in | | e | ) ( | f | | a | | m | | in | | e | ) 0.910465
innocent innocent ( | in | | n | | o | | c | | e | | n | | t | ) ( | in | | n | | o | | c | | e | | n | | t | ) 0.908913
prison,jail prison ( | p | | r | | i | | s | | on | ) ( | p | | r | | i | | s | | on | ) 0.9089
poncho poncho ( | p | | on | | c | | h | | o | ) ( | p | | on | | c | | h | | o | ) 0.907496
sure,certain su?r,certain ( | c | | e | | r | | t | | a | | in | ) ( | c | | e | | r | | t | | a | | in | ) 0.905022
tapioca,manioc manioc ( | m | | an | | i | | o | | c | ) ( | m | | an | | i | | o | | c | ) 0.904811
.
.
.
.
.
.
.
.
.
.
.
.
Table 3: English-French best rated alignments after 30 iterations
English Hunzib Alignment similarity
jewel avg~ar,Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 1?6,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Visualization of Linguistic Patterns
and
Uncovering Language History from Multilingual Resources
Miriam Butt1 Jelena Prokic?2 Thomas Mayer2 Michael Cysouw3
1Department of Linguistics, University of Konstanz
2Research Unit Quantitative Language Comparison, LMU Munich
3Research Center Deutscher Sprachatlas, Philipp University of Marburg
1 Introduction
The LINGVIS and UNCLH (Visualization of Lin-
guistic Patterns & Uncovering Language His-
tory from Multilingual Resources) were originally
conceived of as two separate workshops. Due to
perceived similarities in content, the two work-
shops were combined and organized jointly.
The overal aim of the joint workshop was to
explore how methods developed in computational
linguistics, statistics and computer science can
help linguists in exploring various language phe-
nomena. The workshop focused particularly on
two topics: 1) visualization of linguistic patterns
(LINGVIS); 2) usage of multilingual resources in
computational historical linguistics (UNCLH).
2 LINGVIS
The overall goal of the first half of the work-
shop was to bring together researchers work-
ing within the emerging subfield of computa-
tional linguistics ? using methods established
within Computer Science in the fields of Infor-
mation Visualization (InfoVis) and Visual Ana-
lytics in conjunction with methodology and anal-
yses from theoretical and computational linguis-
tics. Despite the fact that statistical methods for
language analysis have proliferated in the last
two decades, computational linguistics has so far
only marginally availed itself of techniques from
InfoVis and Visual Analytics (e.g., Honkela et
al. (1995); Neumann et al (2007); Collins et
al. (2009); Collins (2010); Mayer et al (2010a);
Mayer et al (2010b); Rohrdantz et al (2011)).
The need to integrate methods from InfoVis and
Visual Analytics arises particularly with respect
to situations in which the amount of data to be
analyzed is huge and the interactions between rel-
evant features are complex. Both of these situ-
ations hold for much of current (computational)
linguistic analysis. The usual methods of sta-
tistical analysis do not allow for quick and easy
grasp and interpretation of the patterns discovered
through statistical processing and an integration
of innovative visualization techniques has become
imperative.
The overall aim of the first half of the workshop
was thus to draw attention to this need and to the
newly emerging type of work that is beginning to
respond to the need. The workshop succeeded in
bringing together researchers interesting in com-
bining techniques and methodology from theo-
retical and computational linguistics with InfoVis
and Visual Analytics.
Three of the papers in the workshop focused
on the investigation and visualization of lexical
semantics. Rohrdantz et al present a diachronic
study of fairly recently coined derivational suf-
fixes (-gate, -geddon, -athon) as used in newspa-
per corpora across several languages. Their anal-
ysis is able to pin-point systematic differences in
contextual use as well as some first clues as to
how and why certain new coinages spread bet-
ter than others. Heylen et al point out that me-
thods such as those used in Rohrdantz et al,
while producing interesting results, are essentially
black boxes for the researchers ? it is not clear
exactly what is being calculated. Their paper
presents some first steps towards making the black
box more transparent. In particular, they take
a close look at individual tokens and their se-
mantic use with respect to Dutch synsets. Cru-
cially, they anticipate an interactive visualization
that will allow linguistically informed lexicogra-
1
phers to work with the available data and patterns.
A slightly different take on synset relations is pre-
sented by Lohk et al, who use visualization me-
thods to help identify errors in WordNets across
different languages.
Understanding differences and relatedness be-
tween languages or types of a language is the sub-
ject of another three papers. Littauer et al use
data from the WALS (World Atlas of Language
Structures; Dryer and Haspelmath (2011)) to
model language relatedness via heat maps. They
overcome two difficulties: one is the sparseness
of the WALS data; another is that WALS does
not directly contain information about possible ef-
fects of language contact. Littauer et al attempt
to model the latter by taking geographical infor-
mation about languages into account (neighboring
languages and their structure). A different kind
of language relatedness is investigated by Yan-
nakoudakis et al, who look at learner corpora and
develop tools that allow an assessment of learner
competence with respect to various linguistic fea-
tures found in the corpora. The number of rel-
evant features is large and many of them are in-
terdependent or interact. Thus, the amount and
complexity of the data present a classic case of
complex data sets that are virtually impossible to
analyze well without the application of visualiza-
tion methods. Finally, Lyding et al take academic
texts and investigate the use of modality across
academic registers and across time in order to
identify whether the academic language used in
different subfields (or adjacent fields) of an aca-
demic field has an effect on the language use of
that field.
3 UNCLH
The second half of the workshop focused on
the usage of multilingual resources in computa-
tional historical linguistics. In the past 20 years,
the application of quantitative methods in his-
torical linguistics has received increasing atten-
tion among linguists (Dunn et al, 2005; Heg-
garty et al, 2010; McMahon and McMahon,
2006), computational linguists (Kondrak, 2001;
Hall and Klein, 2010) and evolutionary anthropol-
ogists (Gray and Atkinson, 2003). Due to the ap-
plication of these quantitative methods, the field
of historical linguistics is undergoing a renais-
sance. One of the main problems that researchers
face is the limited amount of suitable compara-
tive data, often falling back on relatively restricted
?Swadesh type? wordlists. One solution is to use
synchronic data, like dictionaries or texts, which
are available for many languages. For example,
in Kondrak (2001), vocabularies of four Algo-
nquian languages were used in the task of au-
tomatic cognate identification. Another solution
employed by Snyder et al (2010) is to apply a
non-parametric Bayesian framework to two non-
parallel texts in the task of text deciphering. Al-
though very promising, these approaches have so
far only received modest attention. Thus, many
questions and challenges in the automatization
of language resources in computational historical
linguistics remain open and ripe for investigation.
In dialectological studies, there is a long tra-
dition, starting with Se?guy (1971), in which lan-
guage varieties are grouped together on the ba-
sis of their similarity with respect to certain prop-
erties. Later work in this area has incorporated
methods of string alignment for a quantitative
comparison of individual words to obtain an aver-
age measure of the similarity of languages. This
line of research became known as dialectome-
try. Unlike traditional dialectology which is based
on the analysis of individual items, dialectometry
shifts focus on the aggregate level of differences.
Most of the work done so far in dialectometry
is based on the carefully selected wordlists and
problems with the limited amount of suitable data
(i.e. computer readable and comparable across di-
alects) are also present in this field.
This workshop brings together researchers in-
terested in computational approaches that uncover
sound correspondences and sound changes, auto-
matic identification of cognates across languages
and language comparison based both on wordlists
and parallel texts. First, Wettig et al investigate
the sound correspondences in cognate sets in a
sample of Uralic languages. Then, List?s contri-
bution to the volume introduces a novel method
for automatic cognate detection in multilingual
wordlists which combines various previous ap-
proaches for string comparison. The paper by
Mayer & Cysouw presents a first step to use par-
allel texts for a quantitative comparison of lan-
guages. The papers by Scherrer and Prokic? et
al. both are in the spirit of the dialectometric line
of research. Further, Ja?ger reports on quantify-
ing language similarity via phonetic alignment of
core vocabulary items. Finally, some of the pa-
2
pers presented in this workshop deal with further
topics in quantitative language comparison, like
the application of phylogenetic methods in cre-
ole research in the paper by Daval-Markussen &
Bakker, and the study of the evolution of the Aus-
tralian kinship terms reported on in the paper by
McConvell & Dousset.
In the next section, we give a brief introduc-
tion into the papers presented in this workshop,
ordered according to the program of the oral pre-
sentations at the workshop.
4 Papers
Christian Rohrdantz, Andreas Niekler, Annette
Hautli, Miriam Butt and Daniel A. Keim (?Lex-
ical Semantics and Distribution of Suffixes ?
A Visual Analysis) present a quantitative cross-
linguistic investigation of the lexical semantic
content expressed by three suffixes originating in
English: -gate, -geddon and -athon. Using data
from newspapers, they look at the distribution and
lexical semantic usage of these morphemes across
several languages and also across time, with a
time-depth of 20 years for English. Using tech-
niques from InfoVis and Visual Analytics is cru-
cial for the analysis as the occurrence of these suf-
fixes in the available corpora is comparatively rare
and it is only by dint of processing and visualiz-
ing huge amounts of data that a clear pattern can
begin to emerge.
Kris Heylen, Dirk Speelman and Dirk Geer-
aerts (?Looking at Word Meaning. An Interac-
tive Visualization of Semantic Vector Spaces for
Dutch synsets?) focus on the pervasive use of Se-
mantic Vector Spaces (SVS) in statistical NLP
as a standard technique for the automatic mod-
eling of lexical semantics. They take on the
fact that while the method appears to work fairly
well (though they criticize the standardly avail-
able evaluation measures via some created gold
standard), it is in fact quite unclear how it captures
word meaning. That is, the standard technology
can be seen as a black box. In order to find a way
of providing some transparency to the method,
they explore the way an SVS structures the indi-
vidual occurrences of words with respect to the
occurrences of 476 Dutch nouns. These were
grouped into 214 synsets in previous work. This
paper looks at a token-by-token similarity matrix
in conjunction with a visualization that uses the
Google Chart Tools and compares the results with
previous work, especially in light of different uses
in different versions of Dutch.
Ahti Lohk, Kadri Vare and Leo Vo?handu
(?First Steps in Checking and Comparing Prince-
ton WordNet and Estonian WordNet?) use visu-
alization methods to compare two existing Word-
Nets (English and Estonian) in order to identify
errors and semantic inconsistencies that are a re-
sult of the manual coding. Their method opens
up a potentially interesting way of automatically
checking for inconsistencies and errors not only
at a fairly basic and surface level, but by work-
ing with the lexical semantic classification of the
words in question.
Richard Littauer, Rory Turnbull and Alexis
Palmer (?Visualizing Typological Relationships:
Plotting WALS with Heat Maps?) present a novel
way of visualizing relationships between lan-
guages. The paper is based on data extracted from
the World Atlas of Language Structures (WALS),
which is the most complete set of typological and
digitized data available to date, but which presents
two challenges: 1) it actually has very low cover-
age both in terms of languages represented and
in terms of feature description for each language;
2) areal effects are not coded for. While the au-
thors find a way to overcome the first challenge,
the paper?s real contribution lies in proposing a
method for overcoming the second challenge. In
particular, the typological data is filtered by geo-
graphical proximity and then displayed by means
of heat maps, which reflect the strength of similar-
ity between languages for different linguistic fea-
tures. Thus, the data should allow one to be able
to ascertain areal typological effects via a single
integrated visualization.
Helen Yannakoudakis, Ted Briscoe and
Theodora Alexopoulou (?Automatic Second
Language Acquisition Research: Integrating
Information Visualisation and Machine Learn-
ing?) look at yet another domain of application.
They show how data-driven approaches to
learner corpora can support Second Language
Acquisition (SLA) research when integrated
with visualization tools. Learner corpora are
interesting because their analysis requires a good
understanding of a complex set of interacting
linguistic features across corpora with different
distributional patterns (since each corpus po-
tentially diverges from the standard form of the
language by a different set of features). The paper
3
presents a visual user interface which supports
the investigation of a set of linguistic features
discriminating between pass and fail exam
scripts. The system displays directed graphs to
model interactions between features and supports
exploratory search over a set of learner texts.
A very useful result for SLA is the proposal
of a new method for empirically quantifying
the linguistic abilities that characterize different
levels of language learning.
Verena Lyding, Ekaterina Lapshinova-
Koltunski, Stefania Degaetano-Ortlieb, Henrik
Dittmann and Chris Culy (?Visualizing Linguistic
Evolution in Academic Discourse?) describe
methods for visualizing diachronic language
changes in academic writing. In particular, they
look at the use of modality across different aca-
demic subfields and investigate whether adjacent
subfields affect the use of language in a given
academic subfield. Their findings potentially
provide crucial information for further NLP tasks
such as automatic text classification.
Grzegorz Kondrak?s invited contribution
(?Similarity Patterns in Words?) sketches a num-
ber of the author?s research projects on diachronic
linguistics. He first discusses computational tech-
niques for implementing several steps of the
comparative method. These techniques include
algorithms that deal with a wide range of prob-
lems: pairwise and multiple string alignment,
calculation of phonetic similarity between two
strings, automatic extraction of recurrent sound
correspondences, quantification of semantic
similarity between two words, identification of
sets of cognates and building of phylogenetic
trees. In the second part, Kondrak sketches
several NLP projects that directly benefitted
from his research on diachronic linguistics:
statistical machine translation, word align-
ment, identification of confusable drug names,
transliteration, grapheme-to-phoneme conver-
sion, letter-phoneme alignment and mapping of
annotations.
Thomas Mayer and Michael Cysouw (?Lan-
guage Comparison through Sparse Multilingual
Word Alignment?) present a novel approach on
how to calculate similarities among languages
with the help of massively parallel texts. In-
stead of comparing languages pairwise they sug-
gest a simultaneous analysis of languages with re-
spect to their co-occurrence statistics for individ-
ual words on the sentence level. These statistics
are then used to group words into clusters which
are considered to be partial (or ?sparse?) align-
ments. These alignments then serve as the basis
for the similarity count where languages are taken
to be more similar the more words they share in
the various alignments, regardless of the actual
form of the words. In order to cope with the
computationally demanding multilingual analysis
they introduce a sparse matrix representation of
the co-occurrence statistics.
Yves Scherrer (?Recovering Dialect Geogra-
phy from an Unaligned Comparable Corpus?) pro-
poses a simple metric of dialect distance, based
on the ratio between identical word pairs and cog-
nate word pairs occurring in two texts. Scherrer
proceeds from a multidialectal corpus and applies
techniques from machine translation in order to
extract identical words and cognate words. The
dialect distance is defined as as function of the
number of cognate word pairs and identical word
pairs. Different variations of this metric are tested
on a corpus containing comparable texts from dif-
ferent Swiss German dialects and evaluated on the
basis of spatial autocorrelation measures.
Jelena Prokic?, C?ag?r? Co?ltekin and John Ner-
bonne (?Detecting Shibboleths?) propose a gen-
eralization of the well-known precision and re-
call scores to deal with the case of detecting dis-
tinctive, characteristic variants in dialect groups,
in case the analysis is based on numerical differ-
ence scores. This method starts from the data that
has already been divided into groups using clus-
ter analyses, correspondence analysis or any other
technique that can identify groups of language va-
rieties based on linguistic or extra-linguistic fac-
tors (e.g. geography or social properties). The
method seeks items that differ minimally within a
group but differ a great deal with respect to ele-
ments outside it. They demonstrate the effective-
ness of their approach using Dutch and German
dialect data, identifying those words that show
low variation within a given dialect area, and high
variation outside a given area.
Gerhard Ja?ger (?Estimating and Visualizing
Language Similarities Using Weighted Align-
ment and Force-Directed Graph Layout?) reports
several studies to quantify language similarity
via phonetic alignment of core vocabulary items
(taken from the Automated Similarity Judgement
Program data base). Ja?ger compares several string
4
comparison measures based on Levenshtein dis-
tance and based on Needleman-Wunsch similar-
ity score. He also tests two normalization func-
tions, one based on the average score and the
other based on the informatic theoretic similar-
ity measure. The pairwise similarity between all
languages are analyzed and visualized using the
CLANS software, a force directed graph layout
that does not assume an underlying tree structure
of the data.
Aymeric Daval-Markussen and Peter Bakker
(?Explorations in Creole Research with Phyloge-
netic Tools?) employ phylogenetic tools to inves-
tigate and visualize the relationship of creole lan-
guages to other (non-)creole languages on the ba-
sis of structural features. Using the morphosyn-
tactic features described in the monograph on
Comparative Creole Syntax (Holm and Patrick,
2007), they create phylogenetic trees and net-
works for the languages in the sample, which
show the similarity between the various languages
with respect to the grammatical features inves-
tigated. Their results lend support to the uni-
versalist approach which assumes that creoles
show creole-specific characteristics, possibly due
to restructuring universals. They also apply their
methodology to the comparison of creole lan-
guages to other languages, on the basis of typo-
logical features from the World Atlas of Language
Structures. Their findings confirm the hypothe-
sis that creole languages form a synchronically
distinguishable subgroup among the world?s lan-
guages.
Patrick McConvell and Laurent Dousset
(?Tracking the Dynamics of Kinship and So-
cial Category Terms with AustKin II?) give an
overview of their ongoing work on kinship and
social category terms in Australian languages.
They describe the AustKin I database which
allows for the reconstruction of older kinship
systems as well as the visualization of patterns
and changes. In particular, their method recon-
structs so-called ?Kariera? kinship systems for the
proto-languages in Australia. This supports ear-
lier hypotheses about the primordial world social
organization from which Dravidian-Kariera sys-
tems are considered to have evolved. They also
report on more recent work within the AustKin II
project which is devoted to the co-evoluation of
marriage and social category systems.
Hannes Wettig, Kirill Reshetnikov and Roman
Yangarber (?Using Context and Phonetic Fea-
tures in Models of Etymological Sound Change?)
present a novel method for a context-sensitive
alignment of cognate words, which relies on the
information theoretic concept of Minimum De-
scription Length to decide on the most compact
representation of the data given the model. Start-
ing with an initial random alignment for each
word pair, their algorithm iteratively rebuilds de-
cision trees for each feature and realigns the cor-
pus while monotonically decreasing the cost func-
tion until convergence. They also introduce a
novel test for the quality of the models where one
word pair is omitted from the training phase. The
rules that have been learned are then used to guess
one word from the other in the pair. The Lev-
enshtein distance of the correct and the guessed
word is then computed to give an idea of how
good the model actually learned the regularities
in the sound correspondences.
Johann-Mattis List (?LexStat: Automatic De-
tection of Cognates in Multilingual Wordlists?)
presents a new method for automatic cognate
detection in multilingual wordlists. He com-
bines different approaches to sequence compari-
son in historical linguistics and evolutionary bi-
ology into a new framework which closely mod-
els central aspects of the comparative method.
The input sequences, i.e. words, are converted to
sound classes and their sonority profiles are deter-
mined. In step 2, a permutation method is used to
create language specific scoring schemes. In step
3, the pairwise distances between all word pairs,
based on the language-specific scoring schemes,
are computed. In step 4, the sequences are clus-
tered into cognate sets whose average distance is
beyond a certain threshold. The method is tested
on 9 multilingual wordlists.
5 Final remarks
The breadth and depth of the research collected
in this workshop more than testify to the scope
and possibilities for applying new methods that
combine quantitative methods with not only a so-
phisticated linguistic understanding of language
phenomena, but also with visualization methods
coming out of the Computer Science fields of In-
foVis and Visual Analytics. The papers in the
workshop addressed how the emerging new body
of work can provide advances and new insights
for questions pertaining to theoretical linguistics
5
(lexical semantics, derivational morphology, his-
torical linguistics, dialectology and typology) and
applied linguistic fields such as second language
acquisition and statistical NLP.
6 Acknowledgments
We are indebted to the members of the pro-
gram committee of the workshop for their ef-
fort in thoroughly reviewing the papers: Quentin
Atkinson, Christopher Collins, Chris Culy, Dan
Dediu, Michael Dunn, Sheila Embleton, Simon
Greenhill, Harald Hammarstro?m, Annette Hautli,
Wilbert Heeringa, Gerhard Heyer, Eric Hol-
man, Gerhard Ja?ger, Daniel Keim, Tibor Kiss,
Jonas Kuhn, Anke Lu?deling, Steven Moran, John
Nerbonne, Gerald Penn, Don Ringe, Christian
Rohrdantz, Tandy Warnow, S?ren Wichmann.
We also thank the organizers of the EACL 2012
conference for their help in setting up the joint
workshop.
References
Christopher Collins, Sheelagh Carpendale, and Ger-
ald Penn. 2009. Docuburst: Visualizing document
content using language structure. Computer Graph-
ics Forum (Proceedings of Eurographics/IEEE-
VGTC Symposium on Visualization (EuroVis ?09)),
28(3):1039?1046.
Christopher Collins. 2010. Interactive Visualizations
of Natural Language. Ph.D. thesis, University of
Toronto.
Matthew S. Dryer and Martin Haspelmath, editors.
2011. The World Atlas of Language Structures On-
line. Max Planck Digital Library, Munich, 2011
edition.
Michael Dunn, Angela Terrill, Ger Resnik, Robert A.
Foley, and Stephen C. Levinson. 2005. Structural
phylogenetics and the reconstruction of ancient lan-
guage history. Science, 309(5743):2072?2075.
Russell Gray and Quentin Atkinson. 2003. Language-
tree divergence times support the Anatolian theory
of Indo-European origins. Nature, 426:435?439.
David LW Hall and Dan Klein. 2010. Finding cognate
groups using phylogenies. In Proceedings of the
Association for Computational Linguistics.
Paul Heggarty, Warren Maguire, and April McMahon.
2010. Splits or waves? trees or webs? how diver-
gence measures and network analysis can unravel
language histories. In Philosophical Transactions
of the Royal Society (B), volume 365, pages 3829?
3843.
John Holm and Peter L. Patrick, editors. 2007. Com-
parative Creole Syntax. London: Battlebridge.
Timo Honkela, Ville Pulkki, and Teuvo Kohonen.
1995. Contextual relations of words in grimm tales,
analyzed by self-organizing map. In Proceedings of
International Conference on Artificial Neural Net-
works (ICANN-95), pages 3?7.
Grzegorz Kondrak. 2001. Identifying cognates by
phonetic and semantic similarity. In Proceedings
of the North American Chapter of the Association
of Computational Linguistics.
Thomas Mayer, Christian Rohrdantz, Miriam Butt,
Frans Plank, and Daniel A. Keim. 2010a. Visualiz-
ing vowel harmony. Linguistic Issues in Language
Technology (LiLT), 2(4).
Thomas Mayer, Christian Rohrdantz, Frans Plank,
Peter Bak, Miriam Butt, and Daniel A. Keim.
2010b. Consonant co-occurrence in stems across
languages: Automatic analysis and visualization of
a phonotactic constraint. In Proceedings of the
2010 Workshop on NLP and Linguistics: Finding
the Common Ground, ACL 2010, pages 70?78.
April McMahon and Robert McMahon. 2006. Lan-
guage Classification by Numbers. OUP.
Petra Neumann, Annie Tat, Torre Zuk, and Shee-
lagh Carpendale. 2007. Keystrokes: Personaliz-
ing typed text with visualization. In Proceedings
of Eurographics IEEE VGTC Symposium on Visu-
alization.
Christian Rohrdantz, Annette Hautli, Thomas Mayer,
Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual
analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics (Short Papers), pages 305?310. Portland, Ore-
gon.
Jean Se?guy. 1971. La relation entre la distance spa-
tiale et la distance lexicale. Revue de Linguistique
Romane, 35(138):335?357.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language deci-
pherment. In Proceedings of the Association for
Computational Linguistics.
6
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 54?62,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Language comparison through sparse multilingual word alignment
Thomas Mayer
Research Unit
Quantitative Language Comparison
LMU Munich
thommy.mayer@googlemail.com
Michael Cysouw
Research Center
Deutscher Sprachatlas
Philipp University of Marburg
cysouw@uni-marburg.de
Abstract
In this paper, we propose a novel approach
to compare languages on the basis of par-
allel texts. Instead of using word lists or
abstract grammatical characteristics to infer
(phylogenetic) relationships, we use mul-
tilingual alignments of words in sentences
to establish measures of language similar-
ity. To this end, we introduce a new method
to quickly infer a multilingual alignment of
words, using the co-occurrence of words in
a massively parallel text (MPT) to simulta-
neously align a large number of languages.
The idea is that a simultaneous multilin-
gual alignment yields a more adequate clus-
tering of words across different languages
than the successive analysis of bilingual
alignments. Since the method is computa-
tionally demanding for a larger number of
languages, we reformulate the problem us-
ing sparse matrix calculations. The useful-
ness of the approach is tested on an MPT
that has been extracted from pamphlets of
the Jehova?s Witnesses. Our preliminary
experiments show that this approach can
supplement both the historical and the ty-
pological comparison of languages.
1 Introduction
The application of quantitative methods in histor-
ical linguistics has attracted a lot of attention in
recent years (cf. Steiner et al (2011) for a sur-
vey). Many ideas have been adapted from evolu-
tionary biology and bioinformatics, where similar
problems occur with respect to the genealogical
grouping of species and the multiple alignment
of strings/sequences. One of the main differences
between those areas and attempts to uncover lan-
guage history is the limited amount of suitable
data that can serve as the basis for language com-
parison. A widely used resource are Swadesh lists
or similar collections of translational equivalents
in the form of word lists. Likewise, phylogenetic
methods have been applied using structural char-
acteristics (e.g., Dunn et al (2005)). In this paper,
we propose yet another data source, namely par-
allel texts.
Many analogies have been drawn between the
evolution of species and languages (see, for in-
stance, Pagel (2009) for such a comparison). One
of the central problems is to establish what is the
equivalent of the gene in the reproduction of lan-
guages. Like in evolutionary biology, where gene
sequences in organisms are compared to infer
phylogenetic trees, a comparison of the ?genes?
of language would be most appropriate for a quan-
titative analysis of languages. Yet, Swadesh-
like wordlists or structural characteristics do not
neatly fit into this scheme as they are most likely
not the basis on which languages are replicated.
After all, language is passed on as the expression
of propositions, i.e. sentences, which usually con-
sists of more than single words. Hence, follow-
ing Croft (2000), we assume that the basic unit of
replication is a linguistic structure embodied in a
concrete utterance.
According to this view, strings of DNA in bio-
logical evolution correspond to utterances in lan-
guage evolution. Accordingly, genes (i.e., the
functional elements of a string of DNA) corre-
spond to linguistic structures occurring in those
utterances. Linguistic replicators (the ?genes? of
language) are thus structures in the context of an
utterance. Such replicators are not only the words
as parts of the sentence but also constructions to
express a complex semantic structure, or phonetic
54
realizations of a phoneme, to give just a few ex-
amples.
In this paper, we want to propose an approach
that we consider to be a first step in the direc-
tion of using the structure of utterances as the
basic unit for the comparison of languages. For
this purpose, a multilingual alignment of words in
parallel sentences (as the equivalent of utterances
in parallel texts) is computed, similar to multi-
species alignments of DNA sequences.1 These
alignments are clusters of words from different
languages in the parallel translations of the same
sentence.2
The remainder of the paper is organized as fol-
lows. First, we quickly review the position of our
approach in relation to the large body of work on
parallel text analysis (Section 2). Then we de-
scribe the method for the multilingual alignment
of words (Section 3). Since the number of lan-
guages and sentences that have to be analyzed re-
quire a lot of computationally expensive calcula-
tions of co-occurrence counts, the whole analysis
is reformulated into manipulations of sparse ma-
trices. The various steps are presented in detail
to give a better overview of the calculations that
are needed to infer the similarities. Subsequently,
we give a short description of the material that we
used in order to test our method (Section 4). In
Section 5 we report on some of the experiments
that we carried out, followed by a discussion of
the results and their implications. Finally, we con-
clude with directions for future work in this area.
2 Word Alignment
Alignment of words using parallel texts has been
widely applied in the field of statistical ma-
chine translation (cf. Koehn (2010)). Alignment
methods have largely been employed for bitexts,
i.e., parallel texts of two languages (Tiedemann,
2011). In a multilingual context, the same meth-
ods could in principle be used for each pair of lan-
guages in the sample. One of the goals of this pa-
1The choice of translational equivalents in the form of
sentences rather than words accounts for the fact that some
words cannot be translated accurately between some lan-
guages whereas most sentences can.
2In practice, we simply use wordforms as separated by
spaces or punctuation instead of any more linguistically sen-
sible notion of ?word?. For better performance, more detailed
language-specific analysis is necessary, like morpheme sep-
aration, or the recognition of multi-word expressions and
phrase structures.
per, however, is to investigate what can be gained
when including additional languages in the align-
ment process at the same time and not iteratively
looking for correspondences in pairs of languages
(see Simard (1999), Simard (2000) for a similar
approach).
There are basically two approaches to comput-
ing word alignments as discussed in the literature
(cf. Och and Ney (2003)): (i) statistical alignment
models and (ii) heuristic models. The former have
traditionally been used for the training of parame-
ters in statistical machine translation and are char-
acterized by their high complexity, which makes
them difficult to implement and tune. The latter
are considerably simpler and thus easier to im-
plement as they only require a function for the
association of words, which is computed from
their co-occurrence counts. A wide variety of co-
occurrence measures have been employed in the
literature. We decided to use a heuristic method
for the first steps reported on here, but plan to inte-
grate statistical alignment models for future work.
Using a global co-occurrence measure, we pur-
sue an approach in which the words are compared
for each sentence individually, but for all lan-
guages at the same time. That is, a co-occurrence
matrix is created for each sentence, containing all
the words of all languages that occur in the cor-
responding translational equivalents for that sen-
tence. This matrix then serves as the input for
a partitioning algorithm whose results are inter-
preted as a partial alignment of the sentence. In
most cases, the resulting alignments do not in-
clude words from all languages. Only those words
that are close translational equivalents occur in
alignments. This behavior, while not optimal
for machine translation, is highly useful for lan-
guage comparison because differences between
languages are implicitly marked as such by split-
ting different structures into separate alignments.
The languages are then compared on the basis
of having words in the same clusters with other
languages. The more word forms they share in the
same clusters, the more similar the languages are
considered to be.3 The form of the words them-
selves is thereby of no importance. What counts
3A related approach is discussed in Wa?lchli (2011). The
biggest difference to the present approach is that Wa?lchli
only compares languages pairwise. In addition, he makes use
of a global glossing method and not an alignment of words
within the same parallel sentence.
55
is their frequency of co-occurrence in alignments
across languages. This is in stark contrast to
methods which focus on the form of words with
similar meanings (e.g., using Swadesh lists) in or-
der to compute some kind of language similar-
ity. One major disadvantage of the present ap-
proach for a comparison of languages from a his-
torical perspective is the fact that such similarities
also could be a consequence of language contact.
This is a side effect that is shared by the word
list approach, in which loanwords have a simi-
lar effect on the results. It has to be seen how
strongly this influences the final results in order
to assess whether our current approach is useful
for the quantitative analysis of genealogical relat-
edness.
3 Method
We start from a massively parallel text, which we
consider as an n?m matrix consisting of n differ-
ent parallel sentences S = {S1, S2, S3, ..., Sn} in
m different languages L = {L1, L2, L3, ..., Lm}.
This data-matrix is called SL (?sentences ? lan-
guages?). We assume here that the parallel sen-
tences are short enough so that most words occur
only once per sentence. Because of this assump-
tion we can ignore the problem of decoding the
correct alignment of multiple occurring words, a
problem we leave to be tackled in future research.
We also ignore the complications of language-
specific chunking and simply take spaces and
punctuation marks to provide a word-based sep-
aration of the sentences into parts. In future re-
search we are planning to include the (language-
specific) recognition of bound morphemes, multi-
word expressions and phrase structures to allow
for more precise cross-language alignment.
Based on these assumptions, we decompose the
SL matrix into two sparse matrices WS (?words
? sentences?) and WL (?words ? languages?)
based on all words w that occur across all lan-
guages in the parallel texts. We define them as
follows. First, WSij = 1 when word wi oc-
curs in sentence Sj , and is 0 elsewhere. Second,
WLij = 1 when word wi is a word of language
Lj , and is 0 elsewhere. The product WST ?WL
then results in a matrix of the same size as SL,
listing in each cell the number of different words
in each sentence. Instead of the current approach
of using WS only for marking the occurrence of
a word in a sentence (i.e., a ?bag of words? ap-
proach), it is also possible to include the order of
words in the sentences by defining WSij = k
when word wi occurs in position k in sentence
Sj . We will not use this extension in this paper.
The matrix WS will be used to compute co-
occurrence statistics of all pairs of words, both
within and across languages. Basically, we define
O (?observed co-occurrences?) and E (?expected
co-occurrences?) as:
O = WS ?WST
E = WS ?
1SS
n
?WST
Eij thereby gives the expected number of sen-
tences where wi and wj occur in the correspond-
ing translational equivalents, on the assumption
that words from different languages are statisti-
cally independent of each other and occur at ran-
dom in the translational equivalents. Note that
the symbol ?1ab? in our matrix multiplications
refers to a matrix of size a ? b consisting of
only 1?s. Widespread co-occurrence measures are
pointwise mutual information, which under these
definitions simply is logE? logO, or the cosine
similarity, which would be O?
n?E
. However, we
assume that the co-occurrence of words follow
a poisson process (Quasthoff and Wolff, 2002),
which leads us to define the co-occurrence matrix
WW (?words ? words?) using a poisson distri-
bution as:
WW = ? log[
EO exp(?E)
O!
]
= E+ logO!?O logE
This WW matrix represents a similarity ma-
trix of words based on their co-occurrence in
translational equivalents for the respective lan-
guage pair. Using the alignment clustering that
is based on the WW matrices for each sentence,
we then decompose the words-by-sentences ma-
trix WS into two sparse matrices WA (?words?
alignments?) and AS (?alignments ? sentences?)
such that WS = WA ?AS. This decomposition
is the basic innovation of the current paper.
The idea is to compute concrete alignments
from the statistical alignments in WW for each
sentence separately, but for all languages at the
same time. For each sentence Si we take the
subset of the similarity matrix WW only includ-
ing those words that occur in the column WSi,
56
i.e., only those words that occur in sentence Si.
We then perform a partitioning on this subset of
the similarity matrix WW. In this paper we use
the affinity propagation clustering approach from
Frey and Dueck (2007) to identify the clusters, but
this is mainly a practical choice and other meth-
ods could be used here as well. The reason for
this choice is that this clustering does not require
a pre-defined number of clusters, but establishes
the optimal number of clusters together with the
clustering itself.4 In addition, it yields an exem-
plar for each cluster, which is the most typical
member of the cluster. This enables an inspec-
tion of intermediate results of what the clusters
actually contain. The resulting clustering for each
sentence identifies groups of words that are sim-
ilar to each other, which represent words that are
to be aligned across languages. Note that we do
not force such clusters to include words from all
languages, nor do we force any restrictions on the
number of words per language in each cluster.5
In practice, most alignments only include words
from a small number of the languages included.
To give a concrete example for the clustering
results, consider the English sentence given below
(no. 93 in our corpus, see next section) together
with its translational equivalents in German, Bul-
garian, Spanish, Maltese and Ewe (without punc-
tuation and capitalization).
i. who will rule with jesus (English, en)
ii. wer wird mit jesus regieren (German, de)
iii. ko$i we upravlva s isus (Bulgarian, bl)
iv. quie?nes gobernara?n con jesu?s (Spanish, es)
v. min se jah?kem ma g?esu` (Maltese, mt)
vi. amekawoe a?u fia kple yesu (Ewe, ew)
These six languages are only a subset of the
50 languages that served as input for the matrix
WW where all words that occur in the respective
sentence for all 50 languages are listed together
with their co-occurrence significance. When re-
stricting the output of the clustering to those
words that occur in the six languages given above,
4Instead of a prespecified number of clusters, affinity
propagation in fact takes a real number as input for each data
point where data points with larger values are more likely to
be chosen as exemplars. If no input preference is given for
each data point, as we did in our experiments, exemplar pref-
erences are initialized as the median of non infinity values in
the input matrix.
5Again, this takes into account that some words cannot
be translated accurately between some languages.
however, the following clustering result is ob-
tained:
1. isusbl jesusen fiaew yesuew g?esu`mt jesu?ses
jesusde
2. ko$ibl whoen minmt werde
3. regierende
4. upravlvabl a?uew jah?kemmt gobernara?nes
5. amekawoeew quie?neses
6. webl willen semtwirdde
7. sbl withen cones mitde
8. kpleew
9. mamt
10. ruleen
First note that the algorithm does not require
all languages to be given in the same script. Bul-
garian isus is grouped together with its transla-
tional equivalents in cluster 1 even though it does
not share any grapheme with them. Rather, words
from different languages end up in the same clus-
ter if they behave similarly across languages in
terms of their co-occurrence frequency. Further,
note that the ?question word? clusters 2 and 5 dif-
fer in their behavior as will be discussed in more
detail in Section 5.2. Also note that the English
?rule? and German ?regieren? are not included in
the cluster 4 with similar translations in the other
languages. This turns out to be a side effect of the
very low frequency of these words in the current
corpus.
In the following, we will refer to these clusters
of words as alignments (many-to-many mappings
between words) within the same sentence across
languages. For instance, sentences i., iii. and v.
above would have the following alignment, where
indices mark those words that are aligned by the
alignment clusters (1.-10.) above:
who2 will6 rule10 with7 jesus1
min2 se6 jah?kem4 ma7 g?esu`1
ko$i2 we6 upravlva4 s7 isus1
All alignment-clusters from all sentences are
summarized as columns in the sparse matrixWA,
defined as WAij = 1 when word wi is part of
alignment Aj , and is 0 elsewhere.6 We also estab-
lish the ?book-keeping? matrix AS to keep track
6For instance, the alignment in 2. above contains the four
words {ko$i, who, min, wer}, which are thus marked with 1
whereas all other words have 0 in this column of the WA
matrix.
57
of which alignment belongs to which sentence,
defined as ASij = 1 when the alignment Ai oc-
curs in sentence Sj , and as 0 elsewhere. The
alignment matrix WA is the basic information
to be used for language comparison. For exam-
ple, the product WA ?WAT represents a sparse
version of the words ? words similarity matrix
WW.
A more interesting usage of WA is to derive
a similarity between the alignments AA. We de-
fine both a sparse version of AA, based on the
number of words that co-occur in a pair of align-
ments, and a statistical version of AA, based on
the average similarity between the words in the
two alignments:
AAsparse = WAT ?WA
AAstatistical =
WAT ?WW ?WA
WAT ? 1WW ?WA
The AA matrices will be used to select suit-
able alignments from the parallel texts to be used
for language comparison. Basically, the statistical
AA will be used to identify similar alignments
within a single sentence and the sparse AA will
be used to identify similar alignments across dif-
ferent sentences. Using a suitable selection of
alignments (we here use the notation A? for a se-
lection of alignments7), a similarity between lan-
guages LL can be defined as:
LL = LA? ? LA?T
by defining LA? (?languages ? alignments?) as
the number of words per language that occur in
each selected alignment:
LA? = WLT ?WA?
The similarity between two languages LL is then
basically defined as the number of times words
are attested in the selected alignments for both
languages. It thus gives an overview of how
structurally similar two languages are, where lan-
guages are considered to have a more similar
structure the more words they share in the align-
ment clusters.
7Note that the prime in this case does not stand for the
transpose of a matrix, as it is sometimes used.
4 Data
Parallel corpora have received a lot of attention
since the advent of statistical machine translation
(Brown et al, 1988) where they serve as training
material for the underlying alignment models. For
this reason, the last two decades have seen an in-
creasing interest in the collection of parallel cor-
pora for a number of language pairs (Hansard8),
also including text corpora which contain texts
in three or more languages (OPUS9, Europarl10,
Multext-East11). Yet there are only few resources
which comprise texts for which translations are
available into many different languages. Such
texts are here referred to as ?massively parallel
texts? (MPT; cf. Cysouw and Wa?lchli (2007)).
The most well-known MPT is the Bible, which
has a long tradition in being used as the basis
for language comparison. Apart from that, other
religious texts are also available online and can
be used as MPTs. One of them is a collection
of pamphlets of the Jehova?s Witnesses, some of
which are available for over 250 languages.
In order to test our methods on a variety of
languages, we collected a number of pamphlets
from the Watchtower website http://www.
watchtower.org) together with their trans-
lational equivalents for 146 languages in total.
The texts needed some preprocessing to remove
HTML markup, and they were aligned with re-
spect to the paragraphs according to the HTML
markup. We extracted all paragraphs which con-
sisted of only one sentence in the English ver-
sion and contained exactly one English question
word (how, who, where, what, why, whom, whose,
when, which) and a question mark at the end.
From these we manually excluded all sentences
where the ?question word? is used with a differ-
ent function (e.g., where who is a relative pronoun
rather than a question word). In the end we were
left with 252 questions in the English version and
the corresponding sentences in the 145 other lan-
guages. Note that an English interrogative sen-
tence is not necessarily translated as a question
in each other language (e.g., the English question
what is the truth about God? is simply translated
into German as die Wahrheit u?ber Gott ?the truth
8http://www.isi.edu/natural-language/
download/hansard/
9http://opus.lingfil.uu.se
10http://www.statmt.org/europarl/
11http://nl.ijs.si/ME/
58
about God?). However, such translations appear
to be exceptions.
5 Experiments
5.1 Global comparison of Indo-European
As a first step to show that our method yields
promising results we ran the method for the 27
Indo-European languages in our sample in order
to see what kind of global language similarity
arises when using the present approach. In our
procedure, each sentence is separated into various
multilingual alignments. Because the structures
of languages are different, not each alignment will
span across all languages. Most alignments will
be ?sparse?, i.e., they will only include words from
a subset of all languages included. In total, we
obtained 6, 660 alignments (i.e., 26.4 alignments
per sentence on average), with each alignment in-
cluding on average 9.36 words. The number of
alignments per sentence turns out to be linearly
related to the average number of words per sen-
tence, as shown in Fig. 1. A linear interpolation
results in a slope of 2.85, i.e., there are about three
times as many alignments per sentence as the av-
erage number of words. We expect that this slope
depends on the number of languages that are in-
cluded in the analysis: the more languages, the
steeper the slope.
5 10 15
10
20
30
40
50
average sentence length in words
numbe
r of ali
gnmen
ts per 
senten
ce
Figure 1: Linear relation between the average number
of words per sentence and number of alignments per
sentence
We use the LL matrix as the similarity matrix
for languages including all 6, 660 alignments. For
each language pair this matrix contains the num-
ber of times words from both languages are at-
tested in the same alignment. This similarity ma-
trix is converted into a distance matrix by sub-
tracting the similarity value from the highest value
that occurs in the matrix:
LLdist = max(LL)? LL
This distance matrixLLdist is transformed into
a NeighborNet visualization for an inspection of
the structures that are latent in the distance ma-
trix. The NeighborNet in Fig. 2 reveals an ap-
proximate grouping of languages according to the
major language families, the Germanic family on
the right, the Romance family on the top and the
Slavic family at the bottom. Note that the sole
Celtic language in our sample, Welsh, is included
inside the Germanic languages, closest to English.
This might be caused by horizontal influence from
English on Welsh. Further, the only Baltic lan-
guage in our sample, Lithuanian, is grouped with
the Slavic languages (which is phylogenetically
expected behavior in line with Gray and Atkin-
son (2003)), though note that it is grouped par-
ticularly close to Russian and Polish, which sug-
gests more recent horizontal transfer. Interest-
ingly, the separate languages Albanian and Greek
roughly group together with two languages from
the other families: Romanian (Romance) and Bul-
garian (Slavic). This result is not in line with their
phylogenetic relatedness but rather reflects a con-
tact situation in which all four languages are part
of the Balkan Sprachbund.
Although the NeighborNet visualization ex-
hibits certain outcomes that do not correspond to
the attested genealogical relationship of the lan-
guages, the method still fares pretty well based
on a visual inspection of the resulting Neighbor-
Net. In the divergent cases, the groupings can be
explained by the fact that the languages are in-
fluenced by the surrounding languages (as is most
clear for the Balkan languages) through direct lan-
guage contact. As mentioned before, a similar
problem also exists when using word lists to in-
fer phylogenetic trees when loanwords introduce
noise into the calculations and thus lead to a closer
relationship of languages than is genealogically
tenable. However, in the case of our alignments
59
Afrikaans
English
Welsh
German
Icelandic
Lithuanian
Polish
Russian
UkrainianCzechSlovak
SlovenianCroatian
Serbian
Albanian
Greek
Bulgarian
Romanian
PortugueseSpanishCatalan
FrenchItalian Danish
NorwegianSwedish
Dutch
1000000.0
Figure 2: NeighborNet (created with SplitsTree, Huson and Bryant (2006)) of all Indo-European languages in
the sample
the influence of language contact is not related to
loanwords but to the borrowing of similar con-
structions or structural features. In the Balkan
case, linguists have noted over one hundred such
shared structural features, among them the loss
of the infinitive, syncretism of dative and geni-
tive case and postposed articles (cf. Joseph (1992)
and references therein). These features are partic-
ularly prone to lead to a higher similarity in our
approach where the alignment of words within
sentences is sensitive to the fact that certain word
forms are identical or different even though the
exact form of the word is not relevant.
5.2 Typology of PERSON interrogatives
A second experiment we conducted involved a
closer study of just a few questions in the data at
hand to obtain a better impression of the results
of the alignment procedure. For this experiment,
we took the same 252 questions for a worldwide
sample of 50 languages. After running the whole
procedure, we selected just the six sentences in
the sample that were formulated in English with a
who interrogative, i.e., questions as to the person
who did something. The English sentences are the
following:
I Who will be resurrected?
II Who will rule with Jesus?
III Who created all living things?
IV Who are god?s true worshipers on earth to-
day?
V Who is Jesus Christ?
VI Who is Michael the Archangel?
We expected to be able to find all translations
of English who in the alignments. Interestingly,
this is not what happened. The six alignments that
comprised the English who only included words
in 23 to 30 other languages in the sample, so we
are clearly not finding all translations of who. By
using a clustering on AAstatistical we were able
to find seven more alignments that appear to be
highly similar to the six alignments including En-
glish who. Together, these 13 alignments included
words for almost all languages in the six sentences
(on average 47.7 words for each sentence). We
computed a language similarity LL only on the
basis of these 13 alignments, which represents a
typology of the structure of PERSON interrog-
atives. This typology clearly separates into two
60
clusters of languages, two ?types? so to speak, as
can be seen in Fig. 3.
Investigating the reason for these two types, it
turns out that the languages in the right cluster of
Fig. 3 consistently separate the six sentences into
two groups. The first, second, and fourth sen-
tence are differently marked than the third, fifth
and sixth sentence. For example, Finnish uses
ketka? vs. kuka and Spanish quie?nes vs. quie?n.
These are both oppositions in number, suggesting
that all languages in the right cluster of Fig. 3 dis-
tinguish between a singular and a plural form of
who. Interpreting the meaning of the English sen-
tences quoted above, this distinction makes com-
plete sense. The Ewe form amekawoe in example
vi. (see Section 3) contains the plural marker -wo,
which distinguishes it from the singular form and
indeed correctly clusters together with quie?nes in
the alignment cluster 5.
This example shows that it is possible to use
parallel texts to derive a typology of languages for
a highly specific characteristic.
6 Conclusion and Future Work
One major problem with using our approach for
phylogentic reconstruction is the influence of lan-
guage contact. Traits of the languages which are
not inherited from a common proto-language but
are transmitted through contact situations lead to
noise in the similarity matrix which does not re-
flect a genealogical signal. However, other meth-
ods also suffer from the shortcoming that lan-
guage contact cannot be automatically subtracted
from the comparison of languages without man-
ual input (such as manually created cognate lists).
With translational equivalents, a further problem
for the present approach is the influence of trans-
lationese on the results. If one version in a lan-
guage is a direct translation of another language,
the structural similarity might get a higher score
due to the fact that constructions will be literally
translated which otherwise would be expressed
differently in that language.
The experiments that have been presented in
this paper are only a first step. However, we firmly
believe that a multilingual alignment of words is
more appropriate for a large-scale comparison of
languages than an iterative bilingual alignment.
Yet so far we do not have the appropriate evalu-
ation method to prove this. We therefore plan to
include a validation scheme in order to test how
much can be gained from the simultaneous analy-
sis of more than two languages. Apart from this,
we intend to improve the alignment method itself
by integrating techniques from statistical align-
ment models, like adding morpheme separation or
phrase structures into the analysis.
Another central problem for the further devel-
opment of this method is the selection of align-
ments for the language comparison. As our sec-
ond experiment showed, just starting from a se-
lection of English words will not automatically
generate the corresponding words in the other lan-
guages. It is possible to use the AA matrices to
search for further similar alignments, but this pro-
cedure is not yet formalized enough to automati-
cally produce language classification for selected
linguistic domains (like for the PERSON interrog-
atives in our experiment). When this step is better
understood, we will be able to automatically gen-
erate typological parameters for a large number
of the world?s languages, and thus easily produce
more data on which to base future language com-
parison.
Acknowledgements
This work has been funded by the DFG project
?Algorithmic corpus-based approaches to typo-
logical comparison?. We are grateful to four
anonymous reviewers for their valuable com-
ments and suggestions.
References
Peter F. Brown, John Cocke, Stephen A. Della-Pietra,
Vincent J. Della-Pietra, Frederick Jelinek, Robert L.
Mercer, and Paul S. Roossin. 1988. A statistical
approach to language translation. In Proceedings
of the 12th International Conference on Computa-
tional Linguistics (COLING-88), pages 71?76.
William Croft. 2000. Explaining Language Change:
An Evolutionary Approach. Harlow: Longman.
Michael Cysouw and Bernhard Wa?lchli. 2007. Paral-
lel texts: using translational equivalents in linguis-
tic typology. Sprachtypologie und Universalien-
forschung STUF, 60(2):95?99.
Michael Dunn, Angela Terrill, Ger Reesink, R. A. Fo-
ley, and Steve C. Levinson. 2005. Structural phylo-
genetics and the reconstruction of ancient language
history. Science, 309(5743):2072?5, 9.
Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:972?976.
61
Alb
an
ian
Ra
ro
to
ng
an
Ma
lte
se
Ma
lag
as
y
Lit
hu
an
ian
Ilo
ko
Cr
oa
tia
n
Ch
ich
ew
a
Bu
lga
ria
n
Ge
rm
an
Po
na
pe
an
Pa
pi
am
en
to
 (A
ru
ba
)
Pa
pi
am
en
to
 (C
ur
a?
ao
)
Du
tch
Ni
ue
an
M
isk
ito
In
do
ne
sia
n
Ita
lia
n
Kir
iba
ti
Fr
en
ch
En
gli
sh
Da
nis
h
Ha
itia
n 
Cr
eo
le
Ca
tal
an
Af
rik
aa
ns
At
es
o
Fij
ian
Tu
va
lua
n
Sw
ed
ish
Gu
na
Hu
ng
ar
ian
Qu
ec
hu
a (
An
ca
sh
)
Kw
an
ya
ma
Tu
mb
uk
a
Ch
in 
(H
ak
ha
)
Ts
wa
na
Sp
an
ish
Nd
on
ga
Ny
an
ek
a
Gr
ee
k
Fin
nis
h
Ew
e
Da
ng
me
Ch
ito
ng
a
Sh
on
a
Bi
co
l
Xit
sh
wa
Ac
ho
li
Lu
ga
nd
a
Se
pe
di
10
15
20
25
30
Cluster Dendrogram
hclust (*, "complete")
as.dist(max(LL) - LL)
He
igh
t
Figure 3: Hierarchical cluster using Ward?s minimum variance method (created with R, R Development Core
Team (2010)) depicting a typology of languages according to the structure of their PERSON interrogatives
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435?439.
Daniel H. Huson and David Bryant. 2006. Applica-
tion of phylogenetic networks in evolutionary stud-
ies. Molecular Biology and Evolution, 23(2):254?
267.
Brian D. Joseph. 1992. The Balkan languages. In
William Bright, editor, International Encyclopedia
of Linguistics, pages 153?155. Oxford: Oxford Uni-
versity Press.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Mark Pagel. 2009. Human language as a culturally
transmitted replicator. Nature Reviews Genetics,
10:405?415.
Uwe Quasthoff and Christian Wolff. 2002. The
poisson collocation measure and its applications.
In Proceedings of the 2nd International Workshop
on Computational Approaches to Collocations, Vi-
enna, Austria.
R Development Core Team, 2010. R: A language
and environment for statistical computing. Wien:
R Foundation for Statistical Computing.
Michel Simard. 1999. Text-translation alignment:
Three languages are better than two. In Proceed-
ings of EMNLP/VLC-99, pages 2?11.
Michel Simard. 2000. Text-translation alignment:
Aligning three or more versions of a text. In Jean
Ve?ronis, editor, Parallel Text Processing: Align-
ment and Use of Translation Corpora, pages 49?67.
Dordrecht: Kluwer Academic Publishers.
Lydia Steiner, Peter F. Stadler, and Michael Cysouw.
2011. A pipeline for computational historical
linguistics. Language Dynamics and Change,
1(1):89?127.
Jo?rg Tiedemann. 2011. Bitext Alignment. Morgan &
Claypool Publishers.
Bernhard Wa?lchli. 2011. Quantifying inner form: A
study in morphosemantics. Arbeitspapiere. Bern:
Institut fu?r Sprachwissenschaft.
62
