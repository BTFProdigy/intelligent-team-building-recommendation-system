PartslD: A Dialogue-Based System for Identifying Parts for Medical 
Systems 
Amit BAGGA, Tomek STRZALKOWSKI, and G. Bowden WISE 
Information Technology Laboratory 
GE Corporate Research and Development 
1 Research Circle 
Niskayuna, USA, NY 12309 
{ bagga, strzalkowski, wisegb } @crd.ge.com 
Abstract 
This paper describes a system that 
provides customer service by allowing 
users to retrieve identification umbers of 
parts for medical systems using spoken 
natural language dialogue. The paper also 
presents an evaluation of the system 
which shows that the system successfully 
retrieves the identification numbers of 
approximately 80% of the parts. 
Introduction 
Currently people deal with customer service 
centers either over the phone or on the world 
wide web on a regular basis. These service 
centers upport a wide variety of tasks including 
checking the balance of a bank or a credit card 
account, transferring money from one account o 
another, buying airline tickets, and filing one's 
income tax returns. Most of these customer 
service centers use interactive voice response 
(IVR) systems on the front-end for determining 
the user's need by providing a list of options that 
the user can choose from, and then routing the 
call appropriately. The IVRs also gather 
essential information like the user's bank 
account number, social security number, etc. 
For back-end support, the customer service 
centers use either specialized computer systems 
(example: a system that retrieves the account 
balance from a database), or, as in most cases, 
human operators. 
However, the IVR systems are unwieldy 
to use. Often a user's needs are not covered by 
the options provided by the system forcing the 
user to hit 0 to transfer to a human operator. In 
addition, frequent users often memorize the 
sequence of options that will get them the 
desired information. Therefore, any change in 
the options greatly inconveniences these users. 
Moreover, there are users that always hit 0 to 
speak to a live operator because they prefer to 
deal with a human instead of a machine. 
Finally, as customer service providers continue 
to rapidly add functionality to their IVR 
systems, the size and complexity of these 
systems continues to grow proportionally. In 
some popular systems like the IVR system that 
provides customer service for the Internal 
Revenue Service (IRS), the user is initially 
bombarded with 10 different options with each 
option leading to sub-menus offering a further 3- 
5 options, and so on. The total number of nodes 
in the tree corresponding to the IRS' IVR system 
is quite large (approximately 100) making it 
extremely complex to use. 
Some customer service providers have 
started to take advantage of the recent advances 
in speech recognition technology. Therefore, 
some of the IVR systems now allow users to say 
the option number (1, 2, 3 . . . . .  etc.) instead of 
pressing the corresponding button. In addition, 
some providers have taken this a step further by 
allowing users to say a keyword or a phrase 
from a list of keywords and/or phrases. For 
example, AT&T, the long distance company, 
provides their users the following options: 
"Please say information for information on 
placing a call, credit for requesting credit, or 
operator to speak to an operator." 
However, given the improved speech 
recognition technology, and the research done in 
natural anguage dialogue over the last decade, 
there exists tremendous potential in enhancing 
29 
these customer service centers by allowing users 
to conduct a more natural human-like dialogue 
with an automated system to provide a 
customer-friendly s stem. In this paper we 
describe a system that uses natural language 
dialogue to provide customer service for a 
medical domain. The system allows field 
engineers to call and obtain identification 
numbers of parts for medical systems using 
natural language dialogue. We first describe 
some work done previously in using natural 
language dialogue for customer service 
applications. Next, we present he architecture 
of our system along with a description of each of 
the key components. Finally, we conclude by 
providing results from an evaluation of the 
system. 
1. Previous Work 
As mentioned earlier, some customer service 
centers now allow users to say either the option 
number or a keyword from a list of 
options/descriptions. However, the only known 
work which automates part of a customer service 
center using natural language dialogue is the one 
by Chu-Carroll and Carpenter (1999). The 
system described here is used as the front-end of 
a bank's customer service center. It routes calls 
by extracting key phrases from a user utterance 
and then by statistically comparing these phrases 
to phrases extracted from utterances in a training 
corpus consisting of pre-recorded calls where 
the routing was done by a human. The call is 
routed to the destination of the utterance from 
the training corpus that is most "similar" to the 
current utterance. On occasion, the system will 
interact with the user to clarify the user's request 
by asking a question. For example, if the user 
wishes to reach the loan department, the system 
will ask if the loan is for an automobile, or a 
home. Other related work is (Georgila et al, 
1998). 
While we are aware of the work being 
done by speech recognition companies like 
Nuance (www.nuance.com) and Speechworks 
(www.speechworks.com) in the area of 
providing more natural anguage dialogue-based 
customer service, we are not aware of any 
conference or journal publications from them. 
Some magazine articles which mention their 
work are (Rosen 1999; Rossheim 1999; 
Greenemeier 1999 ; Meisel 1999). In addition, 
when we tried out a demo of Nuance's ystems, 
we found that their systems had a very IVRish 
feel to them. For example, if one wanted to 
transfer $50 from one account o another, the 
system would first ask the account that the 
money was coming from, then the account hat 
the money was going to, and finally, the amount 
to be transferred. Therefore, a user could not 
say "I want to transfer $50 from my savings 
account o my checking account" and have the 
system conduct that transaction. 
In addition to the works mentioned above, 
there have been several classic projects in the 
area of natural language dialogue like 
TRAINS/TRIPS project at Rochester (Allen et 
al., 1989, 1995, 1996), Duke's Circuit-Fixit- 
Shoppe and Pascal Tutoring System (Biermann 
et al, 1997; 1995), etc. While the Circuit-Fixit- 
Shoppe system helps users fix a circuit through a
dialogue with the system, the TRIPS and the 
TRAINS projects allow users to plan their 
itineraries through dialogue. Duke's Pascal 
tutoring system helps students in an introductory 
programming class debug their programs by 
allowing them to analyze their syntax errors, get 
additional information on the error, and learn the 
correct syntax. Although these systems have 
been quite successful, they use detailed models 
of the domain and therefore cannot be used for 
diverse applications uch as the ones required 
for customer service centers. Other related work 
on dialogue include (Carberry, 1990; Grosz and 
Sidner, 1986; Reichman, 1981). 
2. PartslD: A System for Identification 
of Parts for Medical Systems 
Initially, we were approached by the medical 
systems business of our company for help in 
reducing the number of calls handled by human 
operators at their call center. An analysis of the 
types of customer service provided by their call 
center showed that a large volume of calls 
handled by their operators were placed by field 
engineers requesting identification umbers of 
parts for various medical systems. The ID 
numbers were most often used for ordering the 
corresponding parts using an automated IVR 
system. Therefore, the system we have built 
30 
Figure 1. PartslD System Architecture 
W 
I Parser l 
~ User 
Dia logue  Manager  
F . , .  
pros entetion 
helps automate some percentage of these calls 
by allowing the engineer to describe a part using 
natural language. The rest of this section 
describes our system in detail. 
2.1 Data 
The database we used for our system was the 
same as the one used by the operators at the call 
center. This database consists of the most 
common parts and was built by the operators 
themselves. However, the data contained in the 
database is not clean and there are several types 
of errors including mis-spellings, use of non- 
standard abbreviations, use of several different 
abbreviations for the same word, etc. 
The database consists of approximately 
7000 different parts. For each part, the database 
contains its identification umber, a description, 
and the product (machine type) that it is used in. 
The descriptions consist of approximately 
60,000 unique words of which approximately 
3,000 are words which either are non-standard 
abbreviations or are unique to the medical 
domain (example: collimator). 
Due to the large size of the database, we 
did not attempt to clean the data. However, we 
did build several data structures based on the 
database which were used by the system. The 
primary data structures built were two inverted 
hash tables corresponding to the product, and the 
part description fields in the database. The 
inverted hash tables were built as follows: 
1) Each product and part description field 
was split into words. 
2) Stop-words (words containing no 
information like: a, the, an, etc.) were 
filtered. 
3) Each remaining word was inserted as the 
index of the appropriate hash table with 
the identification number of the part 
being the value corresponding to the 
index. 
Therefore, for each non-stop-word word used in 
describing a part, the hash table contains a list of 
all the parts whose descriptions contained that 
word. Similarly, the products hash table 
contains a list of all parts corresponding to each 
product word. 
2.2 System Architecture 
The architecture of the system is shown in 
Figure 1. The system was designed in a manner 
such that it could be easily ported from one 
application to another with minimal effort other 
than providing the domain-specific knowledge 
regarding the new application. Therefore, we 
decided to abstract away the domain-specific 
information into self-contained modules while 
keeping the other modules completely 
independent. The domain-specific modules are 
shown in the dark shaded boxes in Figure I. 
The remainder of this section discusses each of 
the modules hown in the system architecture. 
2.2.1 The Speech Recognition System (ASR) 
Since customer service centers are meant o be 
used by a variety of users, we needed a user- 
independent speech recognition system. In 
31 
addition, since the system could not restrict he 
manner in which a user asked for service, the 
speech recognition system could not be 
grammar-based. Therefore, we used a general 
purpose dictation engine for the system. The 
dictation system used was Lernout & Hauspie's 
VoiceXPress ystem (www.lhs.com). Although 
the system was general purpose, we did provide 
to it the set of keywords and phrases that are 
commonly used in the domain thereby enabling 
it to better recognize these domain-specific 
keywords and phrases. The keywords and 
phrases used were simply the list of descriptions 
and product names corresponding to each part in 
the database. It should be noted that the set of 
domain-specific keywords and phrases was 
provided to the speech recognition system as a 
text document. In other words, the training was 
not done by a human speaking the keywords and 
phrases into the speech recognition system. In 
addition, the speech recognition system is far 
from perfect. The recognition rates hover 
around 50%, and the system has additional 
difficulty in identifying product names which 
are most often words not found in a dictionary 
(examples: 3MlaserCam, 8000BUCKY, etc.). 
2.2.2 Parser and the Lexicon 
The parser is domain-driven i the sense that it 
uses domain-dependent information produced by 
the lexicon to look for information, in a user 
utterance, that is useful in the current domain. 
However, it does not attempt to understand fully 
each user utterance. It is robust enough to 
handle ungrammatical sentences, hort phrases, 
and sentences that contain mis-recognized text. 
The lexicon, in addition to providing 
domain-dependent keywords and phrases to the 
parser, also provides the semantic knowledge 
associated with each keyword and phrase. 
Therefore, for each content word in the inverted 
hash tables, the lexicon contains entries which 
help the system determine whether the word was 
used in a part description, or a product name. In 
addition, the lexicon also provides the semantic 
knowledge associated with the pre-specified 
actions which can be taken by the user like 
"operator" which allows the user to transfer to 
an operator, and "stop," or "quit" which allow 
the user to quit the system. Some sample ntries 
are: 
collimator => (description_word, collimator) 
camera => (product_word, camera) 
operator => (user action, operator) 
etc. 
The parser scans a user utterance and 
returns, as output, a list of semantic tuples 
associated with each keyword/phrase contained 
in the utterance. It is mainly interested in "key 
words" (words that are contained in product and 
part descriptions, user action words, etc.) and it 
ignores all the other words in the user utterance. 
The parser also returns a special tuple containing 
the entire input string which may be used later 
by the context-based parser for sub-string 
matching specially in cases when the DM has 
asked a specific question to the user and is 
expecting a particular kind of response. 
2.2.3 The Filler and Template Modules 
The filler takes as input the set of tuples 
generated by the parser and attempts to check 
off templates contained in the templates module 
using these tuples, The set of templates in the 
templates module contains most of remaining 
domain-specific knowledge required by the 
system. Each template is an internal 
representation of a part in the database. It 
contains for each part, its ID, its description, and 
the product which contains it. In addition, there 
are several additional templates corresponding to
pre-specified user actions like "operator," and 
"quit." A sample template follows: 
tl__I = ( 
'product' = > 'SFD', 
'product__ids' = > 2229005" 
'product_descriptions' => 'IR RECEIVER PC 
BOARD CI104 BISTABLE MEMORY') 
For each tuple input from the parser, the 
filler checks off the fields which correspond to 
the tuple. For example, if the filler gets as input 
(description_word, collimator), it checks off the 
description fields of those templates containing 
collimator as a word in the field. A template is 
checked off iff one or more of its fields is 
checked off. In addition, the filler also 
maintains a list of all description and product 
words passed through the tuples (i.e. these words 
32
have been uttered by the user). These two lists 
are subsequently passed to the dialogue 
manager. 
Although the filler does not appear to be 
very helpful for the current application domain, 
it is an important part of the architecture for 
other application domains. For example, the 
current PartslD system is a descendant from an 
earlier system which allowed users to process 
financial transactions where the filler was 
instrumental in helping the dialogue manager 
determine the type of transaction being carried 
out by the user (Bagga et al, 2000). 
2.2.4 The Dialogue Manager (DM) 
The DM receives as input from the filler the set 
of templates which are checked off. In addition, 
it also receives two lists containing the list of 
description words, and product word uttered by 
the user. The DM proceeds using the following 
algorithm: 
1) It first checks the set of checked off 
templates input from the filler. If there is 
exactly one template in this set, the DM asks 
the user to confirm the part that the template 
corresponds to. Upon receipt of the 
confirmation from the user, it returns the 
identification number of the part to the user. 
2) Otherwise, for each description word uttered 
by the user, the DM looks up the set of parts 
(or templates) containing the word from the 
descriptions inverted hash table. It then 
computes the intersection of these sets. If 
the intersection is empty, the DM computes 
the union of these sets and proceeds treating 
the union as the intersection. 
3) If the intersection obtained from (2) above 
contains exactly one template, the DM asks 
the user to confirm the part corresponding to
the template as in (1) above. 
4) Otherwise, the DM looks at the set of 
product words uttered by the user. If this set 
is empty, the DM queries the user for the 
product name. Since the DM is expecting a
product name here, the input provided by the 
user is handled by the context-based parser. 
Since most product names consist of non- 
standard words consisting of alpha-numeric 
characters (examples: AMX3, 
8000BUCKY, etc.), the recognition quality 
is quite poor. Therefore, the context-based 
parser anks the input received from the user 
using a sub-string matching algorithm that 
uses character-based unigram and bigram 
counts (details are provided in the next 
section). The sub-string matching algorithm 
greatly enhances the performance of the 
system (as shown in the sample dialogue 
below). 
5) If the set of product words is non-empty, or 
if the DM has successfully queried the user 
for a product name, it extracts the set of 
parts (templates) containing each product 
word from the product words inverted hash 
table. It then computes an intersection of 
these sets with the intersection set of 
description words obtained from (2) above. 
The resulting intersection is the joint product 
and description i tersection. 
6) If the joint intersection has exactly one 
template, the DM proceeds as in (1) above. 
Alternatively, if the number of templates in 
the joint intersection is less than 4, the DM 
lists the parts corresponding toeach of these 
and asks the user to confirm the correct one. 
7) If there are more than 4 templates in the 
joint intersection, the DM ranks the 
templates based upon word overlap with the 
description words uttered by the user. If the 
number of resulting top-ranked templates i
less than 4, the DM proceeds as in the 
second half of (6) above. 
8) If the joint intersection is empty, or in the 
highly unlikely case of there being more 
than 4 top-ranked templates in (7), the DM 
asks the user to enter additional 
disambiguating information. 
The goal of the DM is to hone in on the part 
(template) desired by the user, and it has to 
determine this from the set of templates input to 
it by the filler. It has to be robust enough to deal 
with poor recognition quality, inadequate 
information input by the user, and ambiguous 
data. Therefore, the DM is designed to handle 
these issues. For example, description words 
that are mis-recognized as other description 
words usually cause the intersection of the sets 
of parts corresponding to these words to be 
empty. The DM, in this case, takes a union of 
the sets of parts corresponding to the description 
333333
words thereby ensuring that the template 
corresponding tothe desired part is in the union. 
The DM navigates the space of possibilities 
by first analyzing the intersection of the sets of 
parts corresponding to the description words 
uttered by the user. If no unique part emerges, 
the DM then checks to see if the user has 
provided any information about the product hat 
the part is going to be used in. If no product was 
mentioned by the user, the DM queries the user 
for the product name. Once this is obtained, the 
DM then checks to see if a unique part 
corresponds to the product name and the part 
description provided by the user. If no unique 
part emerges, then the DM backs off and asks 
the user to re-enter the part description. 
Alternatively, if more than one part corresponds 
to the specified product and part description, 
then the DM ranks the parts based upon the 
number of words uttered by the user. 
Obviously, since the DM in this case uses a 
heuristic, it asks the user to confirm the part that 
ranks the highest. If more than one (although 
less than 4) parts have the same rank, then the 
DM explicitly lists these parts and asks the user 
to specify the desired part. It should be noted 
that the DM has to ensure that the information it
receives is actually what the user meant. This is 
especially true when the DM uses heuristics, and 
sub-string matches (as in the case of product 
names). Therefore, the DM occasionally asks 
the user to confirm input it has received. 
2.2.5 The Sub-String Matching Algorithm 
When the dialogue manager is expecting a 
certain type of input (examples : product names, 
yes/no responses) from the user, the user 
response is processed by the context-based 
parser. Since the type of input is known, the 
context-based parser uses a sub-string matching 
algorithm that uses character-based unigram and 
bigram counts to match the user input with the 
expectation of the dialogue manager. Therefore, 
the sub-string matching module takes as input a 
user utterance string along with a list of 
expected responses, and it ranks the list of 
expected responses based upon the user 
response. Listed below are the details of the 
algorithm : 
1) The algorithm first concatenates the words 
of the user utterance into one long string. 
This is needed because the speech 
recognition system often breaks up the 
utterance into words even though a single 
word is being said. For example, the 
product name AMXl l0  is often broken up 
into the string 'Amex 110'. 
2) Next, the algorithm goes through the string 
formed in (1) and compares this character by 
character with the list of expected responses. 
It assigns one point for every common 
character. Therefore, the expected response 
'AMX3' gets three points for the utterance 
'Amex110'. 
3) The algorithm then compares the user 
utterance with the list of expected responses 
using 2 characters (bigrams) at a time. It 
assigns 2 points for each bigram match. For 
the example shown in (2), there are two 
bigram matches: the first is that the 
utterance starts with an 'A' (the previous 
character is this case is the null character), 
and the second is the bigram 'AM'. 
4) The algorithm now compares the length of 
the user utterance string and the expected 
response. If the length of the two strings is 
the same, then it assigns 2 points to the 
expected response. 
5) Finally, the algorithm calculates the number 
of unique characters in the expected 
response, and the user utterance string. If 
these characters are the same, then it assigns 
4 points to the expected response. 
The expected response which has the highest 
number of points is the most likely one. If two 
or more expected responses have the same 
number of points, then the system asks the user 
to confh'm the correct one. 
While we have not evaluated this sub- 
string matching algorithm independently, a brief 
evaluation in the context of the system resulted 
in about 90% accuracy. 
2.2.6 The Presentation Module 
The presentation module works in one of two 
possible modes: over the phone, and over the 
web. This module takes as input a string 
generated by the question-generation module 
and presents this string to the user in the 
appropriate mode of communication. If the 
speech option for the system is turned on, the 
speech-based output is generated using Lernout 
"~ld. 34
and Hauspie's RealSpeak text-to-speech system. 
Although the system currently cannot use both 
modes of communication simultaneously, we 
plan to incorporate this feature sometime in the 
future. 
2.2. 7 Robustness of  the System 
As with any dialogue system, it is extremely 
important for the system to be robust. Our 
system has the following two features which 
make it extremely robust: 
1) The user can, at any given moment, 
say operator to transfer to an 
operator, quit~stop to exit, and 
back~restart to start afresh. 
2) When expecting a response from the 
user, if the system does not receive an 
expected input, it repeats the question 
at most twice before transferring 
control to an operator. 
3. Sample Dialogues 
This section gives examples of two sample 
dialogues that occur during the testing of the 
system. The system's response is indicated by 
"S>", the user's response by "U>", and the 
recognition of the user's response by "V>". 
S> Welcome to the PartslD Center. Please tell 
me the part that you want the ID for. 
U> I need the ID for  PCB Control. 
V> I need the I the for PCB home thrown. 
S> What product do you need this for? 
U> COMPAX 40E 
V> home fax for a O0 E 
S> Is COMPAX 40E the product name that you 
said? 
U> yes 
V> gas 
S> Do you want the ID for PCB Display SID or 
the PCB Control? 
U> The second one. 
V> The second home. 
S> Do you need the ID for PCB Control used in 
product COMPAX 40E? 
U> yes 
V> yes 
S> The ID for PCB Control used in product 
COMPAX 40E is 2168983. Do you wan to exit 
or f ind the ID for another part ? 
U> look up another part. 
S> Welcome to the PartslD Center. Please tell 
me the part that you want the ID for. 
U> I need cable data for cradle 2M long. 
V> I needed cable date them for cradle to be 
loan. 
S> Do you need this part for a ANGIXM2004, 
or a AFM ? 
U> AFM 
V> a fan 
S> Is AFM the product name that you said? 
U> yes 
V> yes 
S> Do you want the ID for Cable Data for 
Cradle 2M long ? 
U> yes 
V> yes 
S> The ID for  Cable Data for Cradle 2M long 
used in product AFM is 45438628. Do you wan 
to exit or f ind the ID for another part? 
U> look up another part. 
4. Evaluation and Results 
The goal of our evaluation was to ensure that the 
system helped a user successfully identify parts 
irrespective of the performance of the speech 
recognition engine for the user. In other words, 
we wanted to see if the system was robust 
enough to conduct transactions with a diverse 
mix of users. We tested the system with 4 
different users two of whom had foreign accents. 
For each user, we randomly selected 20 parts 
from the database. The results are summarized 
in Table 1. 
These results show that the system was 
quite successful in handling requests from users 
with a variety of accents achieving varying 
recognition rates. Out of the 80 parts tested, 
only twice did the user feel that he/she had to 
transfer to an operator. The system successfully 
retrieved the identification umbers of 79% of 
the parts while transferring 19% of the cases to a 
human operator because of extremely bad 
:$5 
User Parts 
successfully 
identified 
15 
Calls system 
transfers to 
operator 
3 
Calls user 
transfers to 
operator 
2 
System 
prompts per 
call 
3.7 
Relevant words 
recognized per 
part 
2.5 
18 2 0 3 2.35 
13 7 0 2.5 1.65 
17 3 0 2.9 2.7 
Table 1: Summary of Results 
recognition. We are planning on conducting a
more elaborate test which a larger set of users. 
Conclusions 
In this paper we have described a robust system 
that provides customer service for a medical 
parts application. The preliminary results are 
extremely encouraging with the system being 
able to successfully process approximately 80% 
of the requests from users with diverse accents. 
Acknowledgements 
We wish to thank the GE Medical Systems team 
of Todd Reinke, Jim Tierney, and Lisa 
Naughton for providing support and funding for 
this project. In addition, we also wish to thank 
Dong Hsu of Lernout and Hauspie for his help 
on the ASR and the text-to-speech systems. 
Finally, we wish to thank the Information 
Technology Laboratory of GE CRD for 
providing additional funding for this project. 
References 
Allen, J. F. et al (1995) The TRAINS Project: A 
case study in building a conversational p anning 
agent. Journal of Experimental nd Theoretical AI, 
(7) 7-48. 
Allen, J. F., Miller, B. W.; Ringer, E. K.; and 
Sikorski, T. (1996) A Robust System for Natural 
Spoken Dialogue. 34th Annual Meeting of the 
ACL, Santa Cruz, 62-70. 
Bagga, A., Stein G. C., and Strzalkowski, T. (2000) 
FidelityXPress: A Multi-Modal System for 
Financial Transactions. Proceedings of the 6 a~ 
Conference on Content-Based Multimedia 
Information Access (RIAO'00). 
Biermann, A.W.; Rodman, R.; Rubin, D.; and 
Heidlage, J.R. (1985) Natural language with 
discrete speech as a mode for human to machine 
communication. Communication of the ACM 
18(6): 628-636. 
Biermann, Alan W.; Guinn, Curry I.; Fulkerson, M.: 
Keim, G.A.; Liang, Z.; Melamed, D.M.; and 
Rajagopalan, K. (1997) Goal-orientedMultimedia 
Dialogue with Variable Initiative. Lecture Notes in 
Artificial Intelligence 1325; Springer-Verlag, New 
York; pp. 1-16. 
Carberry, S. (1990) Plan Recognition in Natural 
Language Dialogue. Cambridge, Mass.: The MIT 
Press. 
Chu-Carroll, J, and R. Carpenter. (1999) Vector- 
Based Natural Language Call Routing. Journal of 
Computational Linguistics, 25(30), pp. 361-388. 
Georgila, K., A.Tsopanoglou, N.Fakotakis and 
G.Kokkinakis. (1998) An Integrated Dialogue 
System for the Automation of Call Centre Services. 
ICLSP'98, 5th International Conference on Spoken 
Language Processing, Sydney, Australia. 
Grosz, B.J. and Sidner, C.L. (1986) Attentions, 
intentions, and the structure of discourse. 
Computational Linguistics 12(3): 175-204. 
Greenemeier, L. (1999) Voice-Recognition 
Technology Builds a Following. Information 
Week, December 13. 
Meisel, W. (1999) Can Speech Recognition Give 
Telephones a New Face? Business 
Communications Review, November 1. 
Reichman, R.. (1981) Plain-speaking: A theory and 
grammar of spontaneous discourse. PhD thesis, 
Department of Computer Science, Harvard 
University, Cambridge, Massachusetts. 
Rosen, C. (1999) Speech Has Industry Talking. 
Business Travel News, November. 
Rossheim, J. (1999) Giving Voice to Customer 
Service. Datamation, November 1. 
36 
HITIQA: Towards Analytical Question Answering 
Sharon Small1, Tomek Strzalkowski1, Ting Liu1, Sean Ryan1, Robert Salkin1,  
Nobuyuki Shimizu1, Paul Kantor2, Diane Kelly2, Robert Rittman2, Nina Wacholder2 
 
1The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tl7612,seanryan, 
rs6021,ns3202}@albany.edu 
2Rutgers University 
4 Huntington Street 
New Brunswick, NJ 08904 
{kantor,diane,hitiqa, 
wacholder}@scils.rutgers.edu
 
Abstract 
In this paper we describe the analytic 
question answering system HITIQA (High-
Quality Interactive Question Answering) 
which has been developed over the last 2 years 
as an advanced research tool for information 
analysts. HITIQA is an interactive open-
domain question answering technology 
designed to allow analysts to pose complex 
exploratory questions in natural language and 
obtain relevant information units to prepare 
their briefing reports. The system uses novel 
data-driven semantics to conduct a 
clarification dialogue with the user that 
explores the scope and the context of the 
desired answer space. The system has 
undergone extensive hands-on evaluations by 
a group of intelligence analysts. This 
evaluation validated the overall approach in 
HITIQA but also exposed limitations of the 
current prototype.  
1 Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as 
?What has been Russia?s reaction to the U.S. 
bombing of Kosovo?? The distinguishing property 
of such questions is that one cannot generally 
anticipate what might constitute the answer. While 
certain types of things may be expected (e.g., 
diplomatic statements), the answer is heavily 
conditioned by what information is in fact 
available on the topic. From a practical viewpoint, 
analytical questions are often underspecified, thus 
casting a broad net on a space of possible answers. 
Questions posed by professional analysts are 
aimed to probe the available data along certain 
dimensions. The results of these probes determine 
follow up questions, if necessary. Furthermore, at 
any stage clarifications may be needed to adjust 
the scope and intent of each question. Figure 1 
shows a fragment of an analytical session with 
HITIQA; note that these questions are not aimed at 
factoids, despite their simple form. 
User: What is the history of the nuclear arms 
program linking Iraq and other countries in the 
region? 
HITIQA: [responses and clarifications] 
User: Who financed the nuclear arms program 
in Iraq? 
HITIQA:? 
User: Has Iraq been able to import uranium? 
HITIQA:? 
User: What type of debt does exist between Iraq 
and her trading partners in the region? 
FIGURE 1: A fragment of an analyst?s session 
with HITIQA 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in 
the state of the art of automated question 
answering.  In this paper we focus on three aspects 
of our work: 
1. Question Semantics: how the system 
?understands? user requests 
2. Human-Computer Dialogue: how the user and 
the system negotiate this understanding 
3. User Evaluations and Results 
2 Factoid vs. Analytical QA 
There are significant differences between 
factoid, or fact-finding, and analytical question 
answering.  A factoid question is normally 
understood to seek a piece of information that 
would make a corresponding statement true (i.e., it 
becomes a fact): ?How many states are in the 
U.S.?? / ?There are X states in the U.S.? In this 
sense, a factoid question usually has just one 
correct answer that can generally be judged for its 
truthfulness with respect to some information 
source.  
As noted by Harabagiu et al (1999), factoid 
questions display a distinctive ?answer type?, 
which is the type of the information item needed 
for the answer, e.g., ?person? or ?country?, etc. 
Most existing factoid QA systems deduct this 
expected answer type from the form of the 
question using a finite list of possible answer 
types. For example, ?Who was the first man in 
space? expects a ?person? as the answer type. This 
is generally a very good strategy that has been 
exploited successfully in a number of automated 
QA systems, especially in the context of TREC 
QA1 evaluations. Given the excellent results posted 
by the best systems and an adequate performance 
attained even by some entry-level system, we 
believe that the process of factoid question 
answering is now fairly well understood 
(Harabagiu et al, 2002; Hovy et al, 2000; Prager 
at al., 2001, Wu et al, 2003). 
   In contrast to a factoid question, an analytical 
question has a virtually unlimited variety of 
syntactic forms with only a loose connection 
between their syntax and the expected answer. 
Given the many possible forms of analytical 
questions, it would be counter-productive to 
restrict them to a predefined number of 
question/answer types. Therefore, the formation of 
an answer in analytical QA should instead be 
guided by the user?s intended interest expressed in 
the question, as well as through any follow up 
dialogue with the system. This clearly involves 
user's intentions (the speech acts) and how they 
evolve with respect to the overall information 
strategy they are pursuing. 
In this paper we argue that the semantics 
(though not necessarily the intent) of an analytical 
question is more likely to be deduced from the 
information that is considered relevant to the 
question than through a detailed analysis of its 
particular form. We noted that the questions 
analysts ask, while clearly part of a strategy, are 
generally quite flexible and ?forgiving?, in the 
sense that there is always a strong possibility that 
the answer may not arrive in the expected form, 
and thus a change of strategy, and even the initial 
expectations, may be warranted. This suggests 
strongly that a solution to analytic QA must 
involve a dialogue that combines information 
seeking and problem solving strategies. 
3 Document Retrieval 
HITIQA works with unstructured text data, 
which means that a document retrieval step is 
required to detect any information that may be 
relevant to the user question. It has to be noted that 
determining ?relevant? information is not the same 
as finding an answer; indeed we can use relatively 
simple information retrieval methods (keyword 
matching, etc.) to obtain perhaps 200 ?relevant? 
                                                     
1 TREC QA is the annual Question Answering evaluation 
sponsored by the U.S. National Institute of Standards and Technology 
www.trec.nist.gov 
documents from a database. This gives us an initial 
information space to work on in order to determine 
the scope and complexity of the answer, but we are 
nowhere near the answer yet. The current version 
of HITIQA uses the INQUERY system (Callan et 
al., 1992), although we have also used SMART 
(Buckley, 1985) and other IR systems (such as 
Google).   
4 Text Framing 
In HITIQA we use a text framing technique to 
delineate the gap between the possible meaning of 
the user?s question and the system ?understanding? 
of this question. We can approximate the meaning 
of the question by extracting references to known 
concepts in it, including named entities. The 
information retrieved from the database may well 
lead to other interpretations of the question, and we 
need to determine which of these are ?correct?.  
The framing process imposes a partial structure 
on the text passages that allows the system to 
systematically compare different passages against 
each other and against the question. Framing is not 
attempting to capture the entire meaning of the 
passage; it needs to be just sufficient enough to 
communicate with the user about the differences in 
their question and the returned text. In particular, 
the framing process may uncover topics or aspects 
within the answer space which the user has not 
explicitly asked for, and thus may be unaware of 
their existence. If these topics or aspects align 
closely with the user?s question, (i.e., matching 
many of the salient attributes) we may want to 
make the user aware of them and let him/her 
decide if they should be included in the answer.   
Frames are built from the retrieved data, after 
clustering it into several topical groups. Passages 
are clustered using a combination of hierarchical 
clustering and n-bin classification (Hardy et al, 
2002a). Each cluster represents a topic theme 
within the retrieved set: usually an alternative or 
complimentary interpretation of the user?s 
question. Since clusters are built out of small text 
passages, we initially associate a frame with each 
passage that serves as a seed of a cluster. We 
subsequently merge passages and their associated 
frames to arrive at one or more combined frames 
for the cluster. 
HITIQA starts text framing by building a 
general frame on the seed passages of the clusters 
and any of the top N (currently N=10) scored 
passages that are not already in a cluster. The 
general frame represents an event or a relation 
involving any number of entities, which make up 
the frame?s attributes, such as LOCATION, PERSON, 
ORGANIZATION, DATE, etc. Attributes are extracted 
from text passages by BBN?s Identifinder, which 
tags 24 types of named entities. The event/relation 
itself could be pretty much anything, e.g., accident, 
pollution, trade, etc. and it is captured into the 
TOPIC attribute from the central verb or noun 
phrase of the passage. In the general frame, 
attributes have no assigned roles; they are loosely 
grouped around the TOPIC (Figure 2).  
We have also defined three slightly more 
specialized typed frames by assigning roles to 
selected attributes in the general frame. These 
three ?specialized? frames are: (1) a Transfer 
frame with three roles including FROM, TO and 
OBJECT; (2) a two-role Relation frame with AGENT 
and OBJECT roles; and (3) an one-role Property 
frame. These typed frames represent certain 
generic events/relationships, which then map into 
more specific event types in each domain. Other 
frame types may be defined if needed, but we do 
not anticipate there will be more than a handful all 
together.2 For example, another 3-role frame may 
be State-Change frame with AGENT, OBJECT and 
INSTRUMENT roles, etc.3  
FRAME TYPE: General 
TOPIC: imported 
LOCATION: Iraq, France, Israel 
ORGANIZATION: IAEA [missed: Nukem] 
PERSON: Leonard Spector 
WEAPON: uranium, nuclear bomb 
DATES: 1981, 30 November 1990, .. 
FIGURE 2: A general frame obtained from the 
text passage in Figure 3 (not all attributes shown). 
 
Where the general frame is little more than just 
a ?bag of attributes?, the typed frames capture 
some internal structure of an event, but only to the 
extent required to enable an efficient dialogue with 
the user. Typed frames are ?triggered? by 
appearance of specific words in text, for example 
the word export may trigger a Transfer frame. A 
single text passage may invoke one or more typed 
frames, or none at all. When no typed frame is 
invoked, the general frame is used as default. If a 
typed frame is invoked, HITIQA will attempt to 
identify the roles, e.g. FROM, TO, OBJECT, etc. This 
is done by mapping general frame attributes 
selected from text onto the typed attributes in the 
frames. In any given domain, e.g., weapon non-
proliferation, both the trigger words and the role 
identification rules can be specialized from a 
                                                     
2 Scalability is certainly an outstanding issue here, and we are 
working on effective frame acquisition methods, which is outside of 
the scope of this paper. While classifications such as (Levin, 1993) or 
FrameNet (Fillmore, 2001) are relevant, we are currently aiming at a 
less detailed system. 
3 A more detailed discussion of possible frame types is beyond the 
scope of the current paper. 
training corpus of typical documents and 
questions. For example, the role-id rules rely both 
on syntactic cues and the expected entity types, 
which are domain adaptable.  
Domain adaptation is desirable for obtaining 
more focused dialogue, but it is not necessary for 
HITIQA to work. We used both setups under 
different conditions: the generic frames were used 
with TREC document collection to measure impact 
of IR precision on QA accuracy (Small et al, 
2004). The domain-adapted frames were used for 
sessions with intelligence analysts working with 
the WMD Domain (see below). Currently, the 
adaptation process includes manual tuning 
followed by corpus bootstrapping using an 
unsupervised learning method (Strzalkowski & 
Wang, 1996). We generally rely on BBN?s 
Identifinder for extraction of basic entities, and use 
bootstrapping to define additional entity types as 
well as to assign roles to attributes. 
The version of HITIQA reported here and used 
by analysts during the evaluation has been adapted 
to the Weapons of Mass Destruction Non-
Proliferation domain (WMD domain, henceforth).  
Figure 3 contains an example passage from this 
data set. In the WMD domain, the typed frames 
were mapped onto WMDTransfer 3-role frame, 
and two 2-role frames WMDTreaty  and 
WMDDevelop. Adapting the frames to the WMD 
domain required very minimal modification, such 
as adding the WEAPON entity to augment the 
Identifinder entity set, generating a list of 
international weapon control treaties, etc. 
The Bush Administration claimed that Iraq was 
within one year of producing a nuclear bomb. On 
30 November 1990... Leonard Spector said that 
Iraq possesses 200 tons of natural uranium 
imported and smuggled from several countries. 
Iraq possesses a few working centrifuges and the 
blueprints to build them. Iraq imported centrifuge 
materials from Nukem of the FRG and from other 
sources. One decade ago, Iraq imported 27 pounds 
of weapons-grade uranium from France, for Osirak 
nuclear research center. In 1981, Israel destroyed 
the Osirak nuclear reactor. In November 1990, the 
IAEA inspected Iraq and found all material 
accounted for....  
FIGURE 3: A text passage from the WMD 
domain data    
 
HITIQA frames define top-down constraints on 
how to interpret a given text passage, which is 
quite different from MUC4 template filling task 
                                                     
4 MUC, the Message Understanding Conference, funded by 
DARPA, involved the evaluation of information extraction systems 
applied to a common task. 
(Humphreys et al, 1998). What we?re trying to do 
here is to ?fit? a frame over a text passage. This 
also means that multiple frames can be associated 
with a text passage, or to be exact, with a cluster of 
passages. Since most of the passages that undergo 
the framing process are part of some cluster of 
very similar passages, the added redundancy helps 
to reinforce the most salient features for extraction. 
This makes the framing process potentially less 
error-prone than MUC-style template filling. 
A very similar framing process is applied to the 
user?s question, resulting in one or more Goal 
frames, which are subsequently compared to the 
data frames obtained from retrieved text passages. 
A Goal frame can be a general frame or any of the 
typed frames. Goal frames generated from the 
question, ?Has Iraq been able to import 
uranium?? are shown in Figures 4 and 5. 
FRAME TYPE: General 
TOPIC: import 
WEAPON:  uranium 
LOCATION: Iraq 
FIGURE 4: A general goal frame from the Iraq 
question 
The frame in Figure 4 is simply a General 
frame which is invoked first. HITIQA then 
discovers that TOPIC=import denotes a Transfer-
event in the WMD domain, so it creates a 
WMDTransfer frame that replaces the general 
frame. This new frame, shown in Figure 5, has 
three role attributes TRF_TO, TRF_FROM and 
TRF_OBJECT, plus the relation type (TRF_TYPE). 
Each role attribute is defined over an underlying 
general frame attribute (given in parentheses), 
which are used to compare frames of different 
types.  The role-id rules rely both on syntactic cues 
and the expected entity types, which are domain 
adaptable. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): import 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION, ORGANIZATION): ? 
TRF_OBJECT (WEAPON): uranium 
FIGURE 5: A typed goal frame from the Iraq 
question 
HITIQA automatically judges a particular data 
frame as relevant, and subsequently the 
corresponding segment of text as relevant, by 
comparison to the Goal frame. The data frames are 
scored based on the number of conflicts found with 
the Goal frame. The conflicts are mismatches on 
values of corresponding attributes, specifically 
when the data frame attribute list does not contain 
any of the entities in the corresponding Goal 
Frame attribute list.  If a data frame is found to 
have no conflicts, it is given the highest relevance 
rank, and a conflict score of zero.   
All other data frames are scored with an 
increasing value based on the number of conflicts, 
score 1 for frames with one conflict with the Goal 
frame, score 2 for two conflicts etc. Frames that 
conflict with all information found in the query are 
given the score 99 indicating the lowest rank. 
Currently, frames with a conflict score 99 are 
excluded from further processing as outliers. The 
frame in Figure 6 is scored as relevant to the user?s 
query and included in the answer space. 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): imported 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION): France 
TRF_OBJECT (WEAPON): uranium 
CONFLICT SCORE: 0 
FIGURE 6: A typed frame obtained from the 
text passage in Figure 3, in response to the Iraq 
question 
5 Enabling Dialogue with the User 
Framed information allows HITIQA to 
automatically judge text passages as fully or 
partially relevant and to conduct a meaningful 
dialogue with the user about their content. The 
purpose of the dialogue is to help the user navigate 
the answer space and to negotiate more precisely 
what information he or she is seeking. The main 
principle here is that the dialogue is primarily 
content oriented. Thus, it is okay to ask the user 
whether information about the AIDS conference in 
Cape Town should be included in the answer to a 
question about combating AIDS in Africa. 
However, the user should never be asked if a 
particular keyword is useful or not, or if a 
document is relevant or not.  
Our approach to dialogue in HITIQA is 
modeled to some degree upon the mixed-initiative 
dialogue management adopted in the AMITIES 
project (Hardy et al, 2002b). The main advantage 
of the AMITIES model is its reliance on data-
driven semantics which allows for spontaneous 
and mixed initiative dialogue to occur. By contrast, 
the major approaches to implementation of 
dialogue systems to date rely on systems of 
functional transitions that make the resulting 
system much less flexible. In the grammar-based 
approach, which is prevalent in commercial 
systems, such as in various telephony products, as 
well as in practically oriented research prototypes 
(e.g., DARPA Communicator; Seneff and Polifoni, 
2000; Ferguson and Allen, 1998), a complete 
dialogue transition graph is designed to guide the 
conversation and predict user responses, which is 
suitable for closed domains only. In the statistical 
variation of this approach, a transition graph is 
derived from a large body of annotated 
conversations (e.g., Walker, 2000; Litman and Pan, 
2002). This latter approach is facilitated through a 
dialogue annotation process, e.g., using Dialogue 
Act Markup in Several Layers (DAMSL) (Allen 
and Core, 1997), which is a system of functional 
dialogue acts.  
Nonetheless, an efficient, spontaneous dialogue 
cannot be designed on a purely functional layer. 
Therefore, here we are primarily interested in the 
semantic layer, that is, the information exchange 
and information building effects of a conversation. 
In order to properly understand a dialogue, both 
semantic and functional layers need to be 
considered. In this paper we are concentrating 
exclusively on the semantic layer. 
6 Clarification Dialogue 
The clarification dialogue is when the user and 
the system negotiate the information task that 
needs to be performed. Data frames with a conflict 
score of 0 form the initial kernel answer space and 
HITIQA proceeds by generating an answer from 
this space. Depending upon the presence of other 
frames outside of this set, the system may initiate a 
dialogue with the user. When the Goal frame is a 
general frame HITIQA first initiates a clarification 
dialogue on existing general data frames that have 
one conflict. All of these 1-conflict general frames 
are first grouped on their common conflict 
attribute. HITIQA begins asking the user questions 
on these near-miss frame groups, with the largest 
group first. The groups must be at least groups of 
size N, where N is a user controlled setting.  This 
setting restricts of all HITIQA?s generated 
dialogue. HITIQA then check for the existence of 
any data frames that are one of the three typed 
frames. Clarification dialogue will be initiated on 
these, when all of their general attributes agree 
with the general attributes of the Goal frame 
respectively. Alternatively, if the Goal frame is one 
of the three type specific frames, a clarification 
dialogue is first initiated on groups of one conflict 
data frames that are the same type as the Goal 
frame. The clarification dialogue will then 
continue to the remaining two type specific frames 
if any exist, and finally on to any General data 
frames. 
A 1-conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a 
mismatch on any of the general frame attributes, 
for example, LOCATION, ORGANIZATION, TIME, 
etc., or in one of the role-assigned attributes, TO, 
FROM, OBJECT, etc.  A special case arises when the 
conflict occurs on the TOPIC attribute, which 
indicated the event type. Since all other attributes 
match, we may be looking at potentially different 
events (though similar type events) involving the 
same entities, occurring at the same location or 
time. The purpose of the clarification dialogue in 
this case is to probe which of these additional 
events may be of interest to the user.  
Another special case arises when the Goal 
frame has a different value of the TYPE attribute 
than a data frame. The purpose of the clarification 
dialogue here is to see if the user wishes to expand 
the answer space to include events of a different 
type. This situation is illustrated in the exchange 
shown in Figure 7.  
The clarification question asked by HITIQA in 
Figure 7 was obtained by comparing the Goal 
frame (Figure 5) to a partly matching frame 
(Figure 8) generated from another text passage. 
We note first that the Goal frame for this example 
is WMDTransfer type, while the data frame in 
Figure 8 is WMDDevelop type. Nonetheless, both 
frames match on their general-frame attributes 
WEAPON and LOCATION. Therefore, HITIQA asks 
the user if it should expand the answer space to 
include development of uranium in Iraq as well. 
User: ?Has Iraq been able to import uranium?? 
HITIQA: IRAQ REPORTED TO HAVE 
IMPORTED MISSILES AND URANIUM  
Supporting passage:  
...that Iraq possesses 200 tons of natural 
uranium imported and smuggled from several 
countries. Iraq possesses a few working 
centrifuges and the blueprints to build them. Iraq 
imported centrifuge materials from Nukem of the 
FRG and from other sources. One decade ago, 
Iraq imported 27 pounds of weapons-grade 
uranium from France, for Osirak nuclear research 
center... 
HITIQA: ?Are you also interested in 
background information on the uranium 
development program in Iraq?? 
User: ? 
 
FIGURE 7:  The clarification dialogue detail 
During the dialogue, as new information is 
obtained from the user, the Goal frame is updated 
and the scores of all the data frames are 
reevaluated.  If the user responds the equivalent of 
?yes? to the system clarification question in the 
dialogue in Figure 7, a corresponding 
WMDDevelop frame will be added to the set of 
active Goal frames and all WMDDevelop frames 
obtained from text passages will be re-scored for 
possible inclusion in the answer. 
FRAME TYPE: WMDDevelop    
DEV_TYPE (TOPIC): development, produced 
DEV_OBJ (WEAPON): nuc. weapons, uranium 
DEV_AGENT (LOCATION): Iraq, Tuwaitha 
CONFLICT SCORE: 2 
Conflicts with FRAME_TYPE and TOPIC  
FIGURE 8: A 2-conflict frame against the 
Iraq/uranium question that generated the dialogue 
in Figure 7. 
The user may end the dialogue at any point using 
the generated answer given the current state of the 
frames. Currently, the answer is simply composed 
of text passages from the zero conflict frames. In 
addition, HITIQA will generate a ?headline? for 
the text passages in the answer space.  This is done 
using a combination of text templates and simple 
grammar rules applied to the attributes of the 
passage frame. Figure 7 shows a portion of the 
answer generated by HITIQA for the Iraq query. 
7 HITIQA Preliminary Evaluations 
We have evaluated HITIQA in a series of 
workshops with professional analysts in order to 
obtain an in-depth and comprehensive assessment 
of the system usability and performance. In 
addition to evaluating our research progress, the 
purpose of these workshops was to test several 
evaluation instruments to see if they can be 
meaningfully applied to a complex information 
system such as HITIQA. 
     For the participating analysts, the primary 
activity at these workshops involved preparation of 
reports in response to ?scenarios? ? complex 
questions that often encompass multiple sub-
questions, aspects and hypotheses. For example, in 
one scenario, analysts were asked ti locate 
information about the al Qaeda terorist group: its 
membership, sources of funding and activities. In 
another scenario, the analysts were requested to 
find information on the chemical weapon Sarin. 
Figure 9 shows one of the analytical scenarios used 
in these workshops. We prepared a database of 
over 1GByte of text documents; it included articles 
from the Center for Non-proliferation (CNS) data 
collected for the AQUAINT program and similar 
data retrieved from the web using Google. The 
analysts? task was to prepare a report ?as much like 
what you would do in your normal work 
environment as possible.? Over the six days of the 
workshops, each analyst prepared five such reports 
in sessions of one to three hours. Each session 
involved multiple questions posed to the system, as 
well as clarification dialogue, visual browsing and 
report construction. Figure 10 shows an abridged 
transcript from another analytical session with 
HITIQA.  
 Figure 9: A scenario level analytic task  
One of our primary concerns was to design 
tasks that were similar in scope and difficulty to 
those that the analysts are used to performing at 
work and to ensure that they felt comfortable using 
the system. 5 questions in the scenario evaluation 
dealt with this issue; for example, one question 
asked how the scenarios compared in difficulty 
with the tasks the analysts normally perform at 
work. The mean score for these five questions was 
3.75 on a 5 point scale (five is the best score). The 
lowest score (M=2.88) was received on the 
question ?How did the scenario compare in 
difficulty to tasks that you normally perform at 
work??; this slightly above average rating of 
difficulty of the tasks was quite satisfactory for our 
purposes.  
    In the final evaluation, analysts were asked to 
rate their agreement with statements such as 
?Having HITIQA helps me find important 
information? (score 4.50), ?Having Hitiqa at work 
would help me find information faster than I can 
currently find it? (score 4.33), and ?Hitiqa would 
be a useful addition to the tools that I already have 
at work? (score 4.25). The mean normalized score 
for the combined final evaluation of Workshop I 
was 3.75 on the 5 point scale; this means that the 
system received many more ratings of 4 and 5 than 
of 1 and 2. Comments made by the analysts in the 
group discussion and in the individual interviews 
confirmed that analysts liked the interactive 
dialogue and were very pleased with the results. 
For example, one analyst said ?I learned more 
about Sarin gas in 30 minutes than I probably 
would have at work in a half a day.? As desired, 
the analysts also made many suggestions for 
improving the interface and the interoperation of 
The department chief has requested a report by the 
close of business today on the nuclear arms program in 
Iraq and how it was influenced by the neighboring 
countries. List the extent of the nuclear program in each 
involved country including funding, capabilities, quantity, 
etc. Your report should also include key figures in Iraq 
nuclear program as well as in other countries in the region, 
and,any travels that these key figures have made to other 
countries in regards to a nuclear program, any weapons 
that have been used in the past by either country, any 
purchases or trades that have been made relevant to 
weapons of mass destruction (possibly oil trade, etc.), any 
ingredients and chemicals that have been used, any 
potential weapons that could be under development, 
countries that are involved or have close ties to Iraq or her 
trade partners, possible locations of development sites, and 
possible companies or organizations that these countries 
work with for their nuclear arms program. Add any other 
information relating to the Iraqi Nuclear Arms Programs.  
the visual and text display. For a research system 
undergoing its first rigorous evaluation, these 
results are very satisfactory ? they support the 
value of the design of the HITIQA system, 
including the interactive mode and the visual 
display and encourage us to move forward with 
this approach. 
 FIGURE 10: Fragment of an analytical session 
8 Future work 
The AQUAINT Program has entered its second 
phase in May 2004. Over the next 2 years our 
focus will be on augmenting HITIQA to provide 
more advanced dialogue capabilities, including 
problem solving dialogue related to hypothesis 
formation and verification. This implies building 
up system?s knowledge acquisition capabilities by 
exploiting diverse data sources, including 
structured databases and the internet. 
9 Acknowledgements 
This paper is based on work supported in part by 
the Advanced Research and Development Activity 
(ARDA)?s Advanced Question Answering for 
Intelligence (AQUAINT) Program. Special thanks 
to Heather McCallum-Bayliss and John Rogers for 
helping to arrange the analyst workshops. 
Additional thanks for Google for extending their 
license for this experiment, to Ralph Weischedel of 
BBN/Verizon for the use of IdentiFinder, to Chuck 
Messenger and Peter LaMonica for assistance in 
development of the analytical scenarios, and to 
Bruce Croft at University of Massachusetts for the 
use of INQUERY system. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  
Dialog Act Markup in Several Layers. 
www.cs.rochester.edu/research/cisd.  
Buckley, C. 1985. Implementation of the Smart 
information retrieval system. TR85-686, 
Computer Science, Cornell University. 
Ferguson, G. and J. Allen. 1998. TRIPS: An 
Intelligent Integrated Problem-Solving Assistant. 
AAAI-98 Conf., pp. 567-573. 
Fillmore, C. & C. F. Baker. 2001. Frame semantics 
for text understanding. WordNet Workshop at 
NAACL. 
Hardy, H., et al 2002a. Cross-Document 
Summarization by Concept Classification. 
Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., et al 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual 
Customer Service. ISLE Workshop, UK. 
Harabagiu, S., et. al. 2002. Answering Complex, 
List and Context questions with LCC?s Question 
Answering Server.   TREC-10. 
Hovy, E., et al 2000. Question Answering in 
Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference TREC-9. 
Humphreys, R. et al 1998. Description of the 
LaSIE-II System as Used for MUC-7. Proc. of  
7th Message Under. Conf. (MUC-7.). 
Levin, B. 1993. English Verb Class and 
Alternations: A Preliminary Investigation. 
Chicago: University of Chicago Press. 
Litman, Diane J. and Shimei Pan. 2002. Designing 
and Evaluating an Adaptive Spoken Dialogue 
System. User Modeling and User-Adapted 
Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Prager, J. et al 2003. In Question-Answering Two 
Heads are Better Than One. Proceedings of 
HLT-NAACL 2003, pp 24-31.  
Seneff, S. and J. Polifroni. 2000. Dialogue 
Management in the MERCURY Flight 
Reservation System. ANLP-NAACL 2000. 
Small et al 2004. A Data Driven Approach to 
Interactive Question Answering. In M. Maybury 
(ed). Future Directions in Automated Question 
Answering. MIT Press (to appear). 
Strzalkowski, T and J. Wang. 1996. A self-learning 
Universal Concept Spotter. Proceedings of 
COLING-96, pp. 931-936. 
Walker, M. A. 2002. An Application of 
Reinforcement Learning to Dialogue Strategy 
Selection in a Spoken Dialogue System for 
Email. Journal of AI Research, vol 12., pp. 387-
416. 
Wu, M. et al 2003. Question Answering by 
Pattern Matching, Web-Proofing, Semantic 
Form Proofing. TREC-12.Notebook. 
 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 60 minutes  
Copying 24 passages to report 
 Visual Panel Browsing: 5 minutes 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
 Clarification Dialogue: 1 minute 
 Studying Answer Panel: 26 minutes 
 Copying 6 passages to report 
 Visual Panel browsing: 1 minute 
 Adding 1 passage to report 
User: How was South Africa's CBW program 
financed?  
 Clarification Dialogue: 40 seconds 
 Studying Answer Panel: 11 minutes 
 Copying 3 passages to report 
Automatically Predicting Information Quality in News Documents 
Rong Tang 
School of Information 
Science and Policy 
University at Albany 
135 Western Avenue 
Albany, NY 12222 
tangr@albany.edu 
Kwong Bor Ng 
Graduate School of 
Library and Information 
Studies, Queens 
College, CUNY.  
New York, NY 11367 
kbng@qc.edu 
Tomek Strzalkowski 
ILS Institute 
Univerity at Albany 
1400 Washington Ave 
Albany, NY 12222 
tomek@albany.edu
Paul B. Kantor 
School of Communication 
Information and Library 
Studies 
Rutgers University 
New Brunswick, NJ 08901 
kan-
tor@scils.rutgers.e
du 
 
 
 
 
 
 
Abstract 
We report here empirical results of a series of 
studies aimed at automatically predicting in-
formation quality in news documents. Multiple 
research methods and data analysis techniques 
enabled a good level of machine prediction of 
information quality. Procedures regarding user 
experiments and statistical analysis are de-
scribed.  
1 
2 
Introduction 
As a part of a large-scale multi-institutional project 
HITIQA (High-quality Interactive Question Answer-
ing), we worked on developing an extended model for 
classifying information by quality, in addition to, and as 
an extension of the traditional notion of relevance. The 
project involves Computer and Information Science 
researchers from University at Albany and Rutgers Uni-
versity. Our serving clientele are intelligent analysts, 
and the documents that we targeted were news articles.    
Research Approach 
The term ?Quality? is defined by International Organi-
zation of Standards (1986) as ?the totality of 
characteristics of an entity that bear on its ability to 
satisfy stated and implied need? (Standard 8402, 3.1). 
Among numerous study on classification of information 
quality, Wang and Strong (1996) proposed four dimen-
sions of qualities as detailed in Table 1: intrinsic, 
contextual, representational, and accessibility.  
 
 
Categories Elements 
Intrinsic IQ Accuracy, Objectivity, Believ-
ability, Reputation 
Accessibility IQ Accessibility, Security 
Contextual IQ Relevancy, Value-added, 
Timeliness, Completeness, 
Amount of Information 
Representa-
tional IQ 
Interpretability, Ease of Under-
standing, Concise Representa-
tion, Consistent Representation 
Table 1. Information Quality Dimensions (Source: 
Strong, Lee, Wang, 1997, p.39) 
 
Empirical attempts to assess quality have primarily 
focused on counting hyperlinks in a networked envi-
ronment. Representative studies include the work by 
Amento and his colleagues (Amento, Terveen, & Hills, 
2000), Price and Hersh (1999), and Zhu and Gauch 
(2000). However, as a whole, previous studies were 
only able to produce algorithmic measures for Web 
documents based on link counts and with a limited 
number of quality aspects such as popularity. Our ap-
proach is to record actual users? quality assessments of 
news articles and conduct advanced statistical models of 
association between users? quality scoring and occur-
rence and prevalence of certain textual features.  
3 Methodology and Results 
Multiple research methods were used. Firstly, we con-
ducted focus-group sessions to elicit key quality aspects 
from news analysts. Secondly, we performed experts 
and students quality judgment experimental sessions. 
Thirdly, we identified a set of textual features, ran pro-
grams to generate counts of the features, and performed 
statistical analysis to establish the correlation between 
features and users? quality ratings. 
Two focus group sessions were conducted during 
March and April of 2002. Participants included journal-
ism faculty members, professional editors, and a num-
ber of journalists from a local newspaper Albany Times 
Union. Nine information quality criteria were consid-
ered to be salient to the context of news analysis: Accu-
racy, Source reliability, Objectivity, Depth, Author 
credibility, Readability, Conciseness, Grammatically 
Correctness, and Multiple Viewpoints. 
A computerized quality judgment system that incor-
porated the nine quality aspects was developed. One 
thousand medium-sized (100 to 2500 words) news arti-
cles were selected from the TREC collection (Voorhees, 
2001) with 25 relevant documents each from five TREC 
Q&A topics.  
We recruited expert and student participants for 
judgment experiments. Expert sessions were performed 
first and ten documents judged by experts were selected 
and used as the training and testing material for the stu-
dent participants. The entire judgment experiment pe-
riod ran from May to August of 2002. As a result, each 
of the 1,000 documents was rated twice, by two differ-
ent judges, one at Albany, and one at Rutgers. 
There were high inter-judge agreements between Al-
bany and Rutgers. Figure 1 is the normality plot of the 
difference between scores assigned by Rutgers? judges 
and Albany?s judges on the variable of ?accuracy,? with 
a mean almost equals to zero (with range from ? 9 to + 
9). The curves of the other eight quality variables are 
similar to the one below, indicating a very insignificant 
disagreement in judgments. 
 
Observed Value
86420-2-4-6-8
E
xp
e
ct
e
d
 N
o
rm
a
l V
a
lu
e
8
6
4
2
0
-2
-4
-6
-8
 
Figure 1. Normality Plot of differences in quality 
judgments on the aspect of ?Accuracy? 
 
Principle component analysis (PCA) revealed the 
same two components from Albany data as from Rut-
gers data. As shown in Figure 2, one component (the 
lower one) consists of ?credibility?, ?source reliability?, 
?accuracy?, ?multi-view?, ?depth?, and ?objectivity.? 
The second component (the upper one) consists of 
?grammar?, ?readability?, and ?verbose and concise-
ness?. Together they explain 58% of the variance.  
 
Component 1
1.0.50.0-.5-1.0
C
o
m
p
o
n
e
n
t 
2
1.0
.5
0.0
-.5
-1.0
a_multiv
a_gramma
a_verbosa_readab
a_credit
a_depth
a_object
a_source
a_accura
 
Figure 2. PCA of Judgment data, in rotated space. 
Rotation method: Oblimin with Kaiser Normaliza-
tion. Rotation converged in 5 iterations.  
 
We recoded users? scores 1 to 5 as low and scores 6 
to 10 as high. We split the 1,000 documents into two 
halves by random selection. In our training round the 
first half was used to estimate the parameters that would 
give best discriminant and logistic regression functions. 
In our testing round, we applied the functions to the 
other half to predict the quality criteria of the docu-
ments.  
 
 Discriminant 
Analysis Cor-
rect-Rate 
Logistic 
Regression 
Correct-Rate 
Accuracy 75.8% 75.9% 
Source Reliability 67.8% 68.5% 
Objectivity 70.6% 73.8% 
Depth 77.4% 77.9% 
Author Credibility 69.3% 71.7% 
Readability 81.3% 83.0% 
Conciseness 70.5% 70.9% 
Grammar 74.9% 75.1% 
Multi-view 82.1% 82.2% 
Table 2. Performance of prediction (based on split- 
half training and testing) by two methods 
 
We then employed stepwise discriminant analysis to 
select the dominant predictive variables from a range of 
104 textual features. These features included elements 
of punctuations, special symbols, length of document 
segments, upper case, quotations, key terms, POS, and 
entities. Our further analysis suggested that certain text 
features are highly correlated with each of the nine as-
pects. 
Quality As-
pects 
Textual Feature Pearson 
correlation  
(2 tails) 
Accuracy Personal Pronoun 0.0002 
Source  Distinct organization 0.0048 
Objectivity Pronoun 0.0001 
Depth Document length 0.0000 
Author 
Credibility 
Date unit, e.g. day, 
week 
0.0000 
Readability Closing parenthesis 0.0099 
Conciseness Subordinating prepo-
sition or conjunction 
0.0003 
Multi-view Past tense verb 0.0000 
Grammatical 
correctness 
Average length of 
paragraph in words 
0.0016 
Table 3. Highly correlated textual features and 
quality aspects 
 
At this point, we are able to produce good prediction 
of several aspects of information quality, including 
Depth, Objectivity, Multi-view, and Readability. The 
prediction testing and training for the remaining quality 
aspects are currently in progress. Tables 4 and 5 illus-
trate the results of training versus testing classification 
for the criteria of  ?objectivity? and ?depth,? with rat-
ings grouped into high and low categories.  
 
Predicted Group 
Membership 
 Objectivity 
 
   Low High 
Low 58.7% 41.3%Training 
Cases   Original High 12.7% 87.3%
Low 45.5% 54.5%Testing 
Cases   
Original 
  High 23.5% 76.5%
Table 4. Classification result of ?objectivity.? 
75.5% of training cases correctly classified, 63.5% 
of testing cases correctly classified 
 
Predicted Group 
Membership 
Depth  
 
 Low High 
Low 64.5% 35.5% Training 
Cases 
Original 
  High 11.9% 88.1% 
Low 51.0% 49.0% Testing 
Cases  
Original 
  High 22.6% 75.4% 
Table 5. Classification result of ?depth.? 74.5% of 
training cases correctly classified, 61.6% of testing 
cases correctly classified 
 
4 Summary 
In this study, we were able to identify important quality 
criteria relevant to intelligent analysts? work and we 
were also able to generate automatic quality metrics of 
news documents using users? quality judgments. Our 
next step is to apply our machine prediction method to 
produce measures of a new set of documents and have 
users to verify and modify machines? scoring. We hope 
that through this, we can collect new data to test our 
quality metrics and to further improve its? performance.   
 
Acknowledgement 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Ad-
vanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 2002-
H790400-000. 
References 
Amendo, B., Terveen, L., & Hill, W. (2000). Does ?au-
thority? mean quality? Predicting expert quality rat-
ings of Web documents. Proceedings of the Twenty-
Third Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval, 296-303. 
Price, S. L., & Hersh, W. R. (1999). Filtering Web 
pages for quality indicators: An empirical approach 
to finding high quality consumer health information 
on the World Wide Web. Proceedings of the AMIA 
1999 Annual Symposium. 911-915. 
Voorhees, E. (2001). Overview of TREC 2001. In E. 
Voorhees (ed.) NIST Special Publication 500-250: 
The Tenth Text REtrieval Conference, pp. 1 ? 15. 
Washington, D.C. 
Strong, D., Lee, Y., & Wang, R. Y. (1997). 10 potholes 
in the road to information quality. IEEE Computer, 
30(8), 38-46. 
Wang, R. Y., & Strong, D. M. (1996). Beyond accu-
racy: What data quality means to data consumers. 
Journal of Management Information Systems, 12(4), 
5-34. 
Zhu, X., & Gauch, S. (2000). Incorporating quality met-
rics in centralized/distributed information retrieval on 
the World Wide Web. Proceedings of the Twenty-
Third Annual International ACM SIGIR Conference 
on Research and Development in Information Re-
trieval, 288-295. 
HITIQA: A Data Driven Approach to Interactive Analytical Question 
Answering 
Sharon Small and Tomek Strzalkowski 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek}@cs.albany.edu 
 
Abstract 
In this paper we describe the analytic question 
answering system HITIQA (High-Quality In-
teractive Question Answering) which has been 
developed over the last 2 years as an advanced 
research tool for information analysts. 
HITIQA is an interactive open-domain ques-
tion answering technology designed to allow 
analysts to pose complex exploratory ques-
tions in natural language and obtain relevant 
information units to prepare their briefing re-
ports. The system uses novel data-driven se-
mantics to conduct a clarification dialogue 
with the user that explores the scope and the 
context of the desired answer space. The sys-
tem has undergone extensive hands-on evalua-
tions by a group of intelligence analysts repre-
senting various foreign intelligence services.  
This evaluation validated the overall approach 
in HITIQA but also exposed limitations of the 
current prototype.  
1   Introduction 
Our objective in HITIQA is to allow the user to submit 
exploratory, analytical questions, such as ?What has 
been Russia?s reaction to U.S. bombing of Kosovo?? 
The distinguishing property of such questions is that one 
cannot generally anticipate what might constitute the 
answer. While certain types of things may be expected 
(e.g., diplomatic statements), the answer is heavily con-
ditioned by what information is in fact available on the 
topic. From a practical viewpoint, analytical questions 
are often underspecified, thus casting a broad net on a 
space of possible answers. Questions posed by profes-
sional analysts are aimed to probe the available data 
along certain dimensions. The results of these probes 
determine follow up questions, if necessary. Further-
more, at any stage clarifications may be needed to adjust 
the scope and intent of each question. Figure 1a shows a 
fragment of an analytical session with HITIQA; please 
note that these questions are not aimed at factoids, de-
spite appearances.  HITIQA project is part of the ARDA 
AQUAINT program that aims to make significant ad-
vances in state of the art of automated question answer-
ing.   
User: What is the history of the nuclear arms program be-
tween Russia and Iraq? 
HITIQA: [responses and clarifications] 
User: Who financed the nuclear arms program in Iraq? 
HITIQA:? 
User: Has Iraq been able to import uranium? 
HITIQA:? 
User: What type of debt does exist between Iraq and Russia? 
FIGURE 1a: A fragment of analytic session    
2   Factoid vs. Analytical QA 
The process of automated question answering is now 
fairly well understood for most types of factoid ques-
tions. Factoid questions display a fairly distinctive ?an-
swer type?, which is the type of the information item 
needed for the answer, e.g., ?person? or ?country?, etc. 
Most existing factoid QA systems deduct this expected 
answer type from the form of the question using a finite 
list of possible answer types. For example, ?How long 
was the Titanic?? expects some length measure as an 
answer, probably in yards and feet, or meters. This is 
generally a very good strategy that has been exploited 
successfully in a number of automated QA systems that 
appeared in recent years, especially in the context of 
TREC QA1  evaluations (Harabagiu et al, 2002; Hovy 
et al, 2000; Prager at al., 2001).     
This answer-typing process is not easily applied to 
analytical questions because the type of an answer for 
analytical questions cannot always be anticipated due to 
their inherently exploratory character.  In contrast to a 
factoid question, an analytical question has an unlimited 
variety of syntactic forms with only a loose connection 
between their syntax and the expected answer.  Given 
the unlimited potential of the formation of analytical 
questions, it would be counter-productive to restrict 
them to a limited number of question/answer types.  
Therefore, the formation of an answer in analytical QA 
should instead be guided by the user?s interest as ex-
pressed in the question, as well as through an interactive 
dialogue with the system.  
In this paper we argue that the semantics of an ana-
lytical question is more likely to be deduced from the 
                                                 
1 TREC QA is the annual Question Answering evaluation sponsored 
by the U.S. National Institute of Standards and Technology 
www.trec.nist.gov 
 information that is considered relevant to the question 
than through a detailed analysis of its particular form. 
Determining ?relevant? information is not the same as 
finding an answer; indeed we can use relatively simple 
information retrieval methods (keyword matching, etc.) 
to obtain perhaps 200 ?relevant? documents from a da-
tabase. This gives us an initial answer space to work 
from in order to determine the scope and complexity of 
the answer, but we are nowhere near the answer yet. In 
our project, we use structured templates, which we call 
frames, to map out the content of pre-retrieved docu-
ments, and subsequently to delineate the possible mean-
ing of the question before we can attempt to formulate 
an answer. 
3   Text Framing 
In HITIQA we use a text framing technique to delineate 
the gap between the meaning of the user?s question and 
the system?s ?understanding? of this question. The 
framing process does not attempt to capture the entire 
meaning of the passages; instead it imposes a partial 
structure on the text passages that would allow the sys-
tem to systematically compare different passages 
against each other and against the question.  Framing is 
just sufficient enough to communicate with the user 
about the differences in their question and the returned 
text. In particular, the framing process may uncover 
topics or aspects within the answer space which the user 
has not explicitly asked for, and thus may be unaware of 
their existence. If these topics or aspects align closely 
with the user?s question, we may want to make the user 
aware of them and let him/her decide if they should be 
included in the answer.   
Frames are built from the retrieved data, after clus-
tering it into several topical groups. Retrieved docu-
ments are first broken down into passages, mostly ex-
ploiting the naturally occurring paragraph structure of 
the original sources, filtering out duplicates.  The re-
maining passages are clustered using a combination of 
hierarchical clustering and n-bin classification (Hardy et 
al., 2002).  Typically three to six clusters are generated.  
Each cluster represents a topic theme within the re-
trieved set: usually an alternative or complimentary in-
terpretation of the user?s question. Since clusters are 
built out of small text passages, we associate a frame 
with each passage that serves as a seed of a cluster. We 
subsequently merge passages, and their associated 
frames whenever anaphoric and other cohesive links are 
detected.   
HITIQA starts by building a general frame on the 
seed passages of the clusters and any of the top N (cur-
rently N=10) scored passages that are not already in a 
cluster. The general frame represents an event or a rela-
tion involving any number of entities, which make up 
the frame?s attributes, such as LOCATION, PERSON, 
COUNTRY, ORGANIZATION, etc. Attributes are extracted 
from text passages by BBN?s Identifinder, which tags 
24 types of named entities. The event/relation itself 
could be pretty much anything, e.g., accident, pollution, 
trade, etc. and it is captured into the TOPIC attribute 
from the central verb or noun phrase of the passage. In 
general frames, attributes have no assigned roles; they 
are loosely grouped around the TOPIC.  
We have also defined three slightly more specialized 
typed frames by assigning roles to selected attributes in 
the general frame. These three ?specialized? frames are: 
(1) a Transfer frame with three roles including FROM, TO 
and OBJECT; (2) a two-role Relation frame with AGENT 
and OBJECT roles; and (3) a one-role Property frame. 
These typed frames represent certain generic 
events/relationships, which then map into more specific 
event types in each domain. Other frame types may be 
defined if needed, but we do not anticipate there will be 
more than a handful all together.2 Where the general 
frame is little more than just a ?bag of attributes?, the 
typed frames capture some internal structure of an 
event, but only to the extent required to enable an effi-
cient dialogue with the user. Typed frames are ?trig-
gered? by appearance of specific words in text, for ex-
ample the word export may trigger a Transfer frame. A 
single text passage may invoke one or more typed 
frames, or none at all. When no typed frame is invoked, 
the general frame is used as default. If a typed frame is 
invoked, HITIQA will attempt to identify the roles, e.g. 
FROM, TO, OBJECT, etc. This is done by mapping general 
frame attributes selected from text onto the typed attrib-
utes in the frames. In any given domain, e.g., weapon 
non-proliferation, both the trigger words and the role 
identification rules can be specialized from a training 
corpus of typical documents and questions. For exam-
ple, the role-ID rules rely both on syntactic cues and the 
expected entity types, which are domain adaptable.  
Domain adaptation is desirable for obtaining more 
focused dialogue, but it is not necessary for HITIQA to 
work. We used both setups under different conditions: 
the generic frames were used with TREC document 
collection to measure impact of IR precision on QA 
accuracy (Small et al, 2004). The domain-adapted 
frames were used for sessions with intelligence analysts 
working with the WMD Domain (see below). Currently, 
the adaptation process includes manual tuning followed 
by corpus bootstrapping using an unsupervised learning 
method (Strzalkowski & Wang, 1996). We generally 
rely on BBN?s Identifinder for extraction of basic enti-
ties, and use bootstrapping to define additional entity 
types as well as to assign roles to attributes. 
The version of HITIQA reported here and used by 
analysts during the evaluation has been adapted to the 
                                                 
2 Scalability is certainly an outstanding issue here, and we are work-
ing on effective frame acquisition methods, which is outside of the 
scope of this paper. 
 Weapons of Mass Destruction Non-Proliferation do-
main (WMD domain, henceforth).  Figure 1b contains 
an example passage from this data set. In the WMD 
domain, the typed frames were mapped onto 
WMDTransfer 3-role frame, and two 2-role frames 
WMDTreaty  and WMDDevelop. Adapting the frames to 
WMD domain required only minimal modification, 
such as adding WEAPON entity to augment Identifinder 
entity set, specializing OBJECT attribute in WMDTrans-
fer to WEAPON, generating a list of international weapon 
control treaties, etc. 
HITIQA frames define top-down constraints on how 
to interpret a given text passage, which is quite different 
from MUC3 template filling task (Humphreys et al, 
1998). What we?re trying to do here is to ?fit? a frame 
over a text passage. This means also that multiple 
frames can be associated with a text passage, or to be 
exact, with a cluster of passages. Since most of the pas-
sages that undergo the framing process are part of some 
cluster of very similar passages, the added redundancy 
helps to reinforce the most salient features for extrac-
tion. This makes the framing process potentially less 
error-prone than MUC-style template filling4.  
The Bush Administration claimed that Iraq was within one 
year of producing a nuclear bomb. On 30 November 1990... 
Leonard Spector said that Iraq possesses 200 tons of natural 
uranium imported and smuggled from several countries. Iraq 
possesses a few working centrifuges and the blueprints to 
build them. Iraq imported centrifuge materials from Nukem of 
the FRG and from other sources. One decade ago, Iraq im-
ported 27 pounds of weapons-grade uranium from France, ... 
FIGURE 1b: A text passage from the WMD domain data    
A very similar framing process is applied to the 
user?s question, resulting in one or more Goal frames, 
which are subsequently compared to the data frames 
obtained from retrieved text passages. A Goal frame can 
be a general frame or any of the typed frames. The Goal 
frame generated from the question, ?Has Iraq been able 
to import uranium?? is shown in Figure 2. This frame is 
of WMDTransfer type, with 3 role attributes TRF_TO, 
TRF_FROM and TRF_OBJECT, plus the relation type 
(TRF_TYPE). Each role attribute is defined over an un-
derlying general frame attribute (given in parentheses), 
which is used to compare frames of different types.  
HITIQA automatically judges a particular data 
frame as relevant, and subsequently the corresponding 
segment of text as relevant, by comparison to the Goal 
frame. The data frames are scored based on the number 
of conflicts found with the Goal frame. The conflicts are 
mismatches on values of corresponding attributes. If a 
                                                 
3 MUC, the Message Understanding Conference, funded by DARPA, 
involved the evaluation of information extraction systems applied to a 
common task. 
4 We do not have enough data to make a definite comparison at this 
time. 
data frame is found to have no conflicts, it is given the 
highest relevance rank, and a conflict score of zero.   
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): import 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION, ORGANIZATION): ? 
TRF_OBJECT (WEAPON): uranium 
FIGURE 2: A domain Goal frame from the Iraq question 
FRAME TYPE: WMDTransfer 
TRF_TYPE (TOPIC): imported 
TRF_TO (LOCATION): Iraq 
TRF_FROM (LOCATION): France [missed: Nukem of FRG] 
TRF_OBJECT (WEAPON): uranium 
CONFLICT SCORE: 0 
FIGURE 3: A frame obtained from the text passage in 
Figure 1b, in response to the Iraq question 
All other data frames are scored with an increasing 
value based on the number of conflicts, score 1 for 
frames with one conflict with the Goal frame, score 2 
for two conflicts etc. Frames that conflict with all in-
formation found in the query are given the score 99 in-
dicating the lowest rank. Currently, frames with a con-
flict score 99 are excluded from further processing as 
outliers. The frame in Figure 3 is scored as relevant to 
the user?s query and included in the answer space. 
4   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space and HITIQA proceeds by 
generating an answer from this space. Depending upon 
the presence of other frames outside of this set, the sys-
tem may initiate a dialogue with the user. HITIQA be-
gins asking the user questions on these near-miss frame 
groups, groups with one or more conflicts, with the 
largest group first. In order to keep the dialogue from 
getting too winded, we set thresholds on number of con-
flicts and group size that are considered by the dialogue 
manager.  
A 1-conflict frame has only a single attribute mis-
match with the Goal frame. This could be a mismatch 
on any of the general frame attributes, for example, LO-
CATION, ORGANIZATION, TIME, etc., or in one of the 
role-assigned attributes, TO, FROM, OBJECT, etc.  A spe-
cial case arises when the conflict occurs on the TOPIC 
attribute, which indicates the event type.  Since all other 
attributes match, we may be looking at potentially dif-
ferent events of the same kind involving the same enti-
ties, possibly occurring at the same location or time. 
The purpose of the clarification dialogue in this case is 
to probe which of these topics may be of interest to the 
user. Another special case arises when the Goal frame is 
of a different type than a data frame.  The purpose of the 
clarification dialogue in this case is to see if the user 
wishes to expand the answer space to include events of 
a different type. This situation is illustrated in the ex-
 change shown in Figure 4. Note that the user can exam-
ine a partial answer prior to answering clarification 
questions.   
User: ?Has Iraq been able to import uranium?? 
[a partial answer displayed in an answer window] 
HITIQA: ?Are you also interested in background information 
on the uranium development program in Iraq?? 
FIGURE 4:  Clarification question generated for the 
Iraq/uranium question 
The clarification question in Figure 4 is generated by 
comparing the Goal frame in Figure 2 to a partly match-
ing frame (Figure 5) generated from some other text 
passage. We note first that the Goal frame for this ex-
ample is of WMDTransfer type, while the data frame in 
Figure 5 is of the type WMDDevelop. Nonetheless, both 
frames match on their general-frame attributes WEAPON 
and LOCATION. Therefore, HITIQA asks the user if it 
should expand the answer space to include development 
of uranium in Iraq as well. 
During the dialogue, as new information is obtained 
from the user, the Goal frame is updated and the scores 
of all the data frames are reevaluated.  If the user re-
sponds the equivalent of ?yes? to the system clarifica-
tion question in the dialogue in Figure 4, a correspond-
ing WMDDevelop frame will be added to the set of ac-
tive Goal frames and all WMDDevelop frames obtained 
from text passages will be re-scored for possible inclu-
sion in the answer.  
FRAME TYPE: WMDDevelop    
DEV_TYPE (TOPIC): development, produced 
DEV_OBJECT (WEAPON): nuclear weapons, uranium 
DEV_AGENT (LOCATION): Iraq, Tuwaitha 
CONFLICT SCORE: 2 
Conflicts with FRAME_TYPE and TOPIC  
FIGURE 5: A 2-conflict frame against the Iraq/uranium ques-
tion that generated the dialogue in Figure 4. 
The user may end the dialogue at any point using the 
generated answer given the current state of the frames. 
Currently, the answer is simply composed of text pas-
sages from the zero conflict frames. In addition, 
HITIQA will generate a ?headline? for the text passages 
in the answer space.  This is done using a combination 
of text templates and simple grammar rules applied to 
the attributes of the passage frame.   
5   HITIQA Qualitative Evaluations 
In order to assess our progress thus far, and to also 
develop metrics to guide future evaluation, we invited a 
group of analysts employed by the US government to 
participate in two three-day workshops, held in Septem-
ber and October 2003.  
The two basic objectives of the workshops were: 
1. To perform a realistic assessment of the useful-
ness and usability of HITIQA as an end-to-end system, 
from the information seeker's initial questions to com-
pletion of a draft report.  
2. To develop metrics to compare the answers ob-
tained by different analysts and evaluate the quality of 
the support that HITIQA provides.     
The analysts' primary task was preparation of reports 
in response to scenarios - complex questions that usu-
ally encompassed multiple sub-questions. The scenarios 
were developed in conjunction with several U.S. gov-
ernment offices. These scenarios, detailing information 
required for the final report, were not normally used 
directly as questions to HITIQA, instead, they were 
treated as a basis to issues possibly leading to a series of 
questions, as shown in Figure 1a. 
The results of these evaluations strongly validated 
our approach to analytical QA. At the same time, we 
learned a great deal about how analysts work, and about 
how to improve the interface.  
Analysts completed several questionnaires de-
signed to assess their overall experience with the sys-
tem.  Many of the questions required the analysts to 
compare HITIQA to other tools they were currently 
using in their work. HITIQA scores were quite high, 
with mean score 3.73 out of 5.  We scored particularly 
high in comparison to current analytic tools. We have 
also asked the analysts to cross-evaluate their product 
reports obtained from interacting with HITIQA. Again, 
the results were quite good with a mean answer quality 
score of 3.92 out of 5. While this evaluation was only 
preliminary, it nonetheless gave us confidence that our 
design is ?correct? in a broad sense.5 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Advanced 
Question Answering for Intelligence (AQUAINT) Program 
under contract number 2002-H790400-000. 
References  
Hardy, H., et al 2002. Cross-Document Summarization by Concept 
Classification. Proceedings of SIGIR, Tampere, Finland. 
Harabagiu, S., et. al. 2002. Answering Complex, List and Context 
questions with LCC?s Question Answering Server.   In Proceed-
ings of Text Retrieval Conference (TREC-10). 
Hovy, E., et al 2000. Question Answering in Webclopedia. Notebook. 
Proceedings of Text Retrieval Conference (TREC-9). 
Humphreys, R. et al 1998. Description of the LaSIE-II System as 
Used for MUC-7. Proc. of  7th Message Under. Conf. (MUC-7.). 
Prager, J. et al 2003. In Question-Answering Two Heads are Better 
Than One. Proceedings of HLT-NAACL 2003, pp 24-31.  
Strzalkowski, T and J. Wang. 1996. A self-learning Universal Concept 
Spotter. Proceedings of COLING-86, pp. 931-936. 
Small S., Strzalkowski T., et al 2004. A Data Driven Approach to 
Interactive Question Answering. In M. Maybury (ed). Future Di-
rections in Automated Question Answering. MIT Press (to appear) 
                                                 
5 Space limitations do not allow for more complete discussion of the 
analysts workshops and the results of the evaluations. 
Data-Driven Strategies for an Automated Dialogue System 
Hilda HARDY, Tomek 
STRZALKOWSKI, Min WU 
ILS Institute 
University at Albany, SUNY 
1400 Washington Ave., SS262 
Albany, NY  12222   USA 
hhardy|tomek|minwu@ 
cs.albany.edu  
Cristian URSU, Nick WEBB 
Department of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St. 
Sheffield  S1 4DP   UK 
c.ursu@sheffield.ac.uk, 
n.webb@dcs.shef.ac.uk 
Alan BIERMANN, R. Bryce 
INOUYE, Ashley MCKENZIE 
Department of Computer Science 
Duke University 
P.O. Box 90129, Levine Science 
Research Center, D101  
Durham, NC  27708   USA 
awb|rbi|armckenz@cs.duke.edu 
 
Abstract 
We present a prototype natural-language 
problem-solving application for a financial 
services call center, developed as part of the 
Amiti?s multilingual human-computer 
dialogue project. Our automated dialogue 
system, based on empirical evidence from real 
call-center conversations, features a data-
driven approach that allows for mixed 
system/customer initiative and spontaneous 
conversation. Preliminary evaluation results 
indicate efficient dialogues and high user 
satisfaction, with performance comparable to 
or better than that of current conversational 
travel information systems. 
1 Introduction 
Recently there has been a great deal of interest in 
improving natural-language human-computer 
conversation. Automatic speech recognition 
continues to improve, and dialogue management 
techniques have progressed beyond menu-driven 
prompts and restricted customer responses. Yet 
few researchers have made use of a large body of 
human-human telephone calls, on which to form 
the basis of a data-driven automated system.  
The Amiti?s project seeks to develop novel 
technologies for building empirically induced 
dialogue processors to support multilingual 
human-computer interaction, and to integrate these 
technologies into systems for accessing 
information and services (http://www.dcs.shef.ac. 
uk/nlp/amities). Sponsored jointly by the European 
Commission and the US Defense Advanced 
Research Projects Agency, the Amiti?s Consortium 
includes partners in both the EU and the US, as 
well as financial call centers in the UK and France. 
A large corpus of recorded, transcribed 
telephone conversations between real agents and 
customers gives us a unique opportunity to analyze 
and incorporate features of human-human 
dialogues into our automated system. (Generic 
names and numbers were substituted for all 
personal details in the transcriptions.) This corpus 
spans two different application areas: software 
support and (a much smaller size) customer 
banking. The banking corpus of several hundred 
calls has been collected first and it forms the basis 
of our initial multilingual triaging application, 
implemented for English, French and German 
(Hardy et al, 2003a); as well as our prototype 
automatic financial services system, presented in 
this paper, which completes a variety of tasks in 
English. The much larger software support corpus 
(10,000 calls in English and French) is still being 
collected and processed and will be used to 
develop the next Amiti?s prototype. 
We observe that for interactions with structured 
data ? whether these data consist of flight 
information, spare parts, or customer account 
information ? domain knowledge need not be built 
ahead of time. Rather, methods for handling the 
data can arise from the way the data are organized. 
Once we know the basic data structures, the 
transactions, and the protocol to be followed (e.g., 
establish caller?s identity before exchanging 
sensitive information); we need only build 
dialogue models for handling various 
conversational situations, in order to implement a 
dialogue system. For our corpus, we have used a 
modified DAMSL tag set (Allen and Core, 1997) 
to capture the functional layer of the dialogues, and 
a frame-based semantic scheme to record the 
semantic layer (Hardy et al, 2003b). The ?frames? 
or transactions in our domain are common 
customer-service tasks: VerifyId, ChangeAddress, 
InquireBalance, Lost/StolenCard and Make 
Payment. (In this context ?task? and ?transaction? 
are synonymous.) Each frame is associated with 
attributes or slots that must be filled with values in 
no particular order during the course of the 
dialogue; for example, account number, name, 
payment amount, etc. 
2 Related Work 
Relevant human-computer dialogue research 
efforts include the TRAINS project and the 
DARPA Communicator program. 
The classic TRAINS natural-language dialogue 
project (Allen et al, 1995) is a plan-based system 
which requires a detailed model of the domain and 
therefore cannot be used for a wide-ranging 
application such as financial services. 
The US DARPA Communicator program has 
been instrumental in bringing about practical 
implementations of spoken dialogue systems. 
Systems developed under this program include 
CMU?s script-based dialogue manager, in which 
the travel itinerary is a hierarchical composition of 
frames (Xu and Rudnicky, 2000). The AT&T 
mixed-initiative system uses a sequential decision 
process model, based on concepts of dialog state 
and dialog actions (Levin et al, 2000). MIT?s 
Mercury flight reservation system uses a dialogue 
control strategy based on a set of ordered rules as a 
mechanism to manage complex interactions 
(Seneff and Polifroni, 2000). CU?s dialogue 
manager is event-driven, using a set of hierarchical 
forms with prompts associated with fields in the 
forms. Decisions are based not on scripts but on 
current context (Ward and Pellom, 1999). 
Our data-driven strategy is similar in spirit to 
that of CU. We take a statistical approach, in 
which a large body of transcribed, annotated 
conversations forms the basis for task 
identification, dialogue act recognition, and form 
filling for task completion.  
3 System Architecture and Components 
The Amiti?s system uses the Galaxy 
Communicator Software Infrastructure (Seneff et 
al., 1998). Galaxy is a distributed, message-based, 
hub-and-spoke infrastructure, optimized for spoken 
dialogue systems. 
  
 
Figure 1. Amiti?s System Architecture 
 
Components in the Amiti?s system (Figure 1) 
include a telephony server, automatic speech 
recognizer, natural language understanding unit, 
dialogue manager, database interface server, 
response generator, and text-to-speech conversion. 
3.1 Audio Components 
Audio components for the Amiti?s system are 
provided by LIMSI. Because acoustic models have 
not yet been trained, the current demonstrator 
system uses a Nuance ASR engine and TTS 
Vocalizer.  
To enhance ASR performance, we integrated 
static GSL (Grammar Specification Language) 
grammar classes provided by Nuance for 
recognizing several high-frequency items: 
numbers, dates, money amounts, names and yes-no 
statements. 
Training data for the recognizer were collected 
both from our corpus of human-human dialogues 
and from dialogues gathered using a text-based 
version of the human-computer system. Using this 
version we collected around 100 dialogues and 
annotated important domain-specific information, 
as in this example: ?Hi my name is [fname ; 
David] [lname ; Oconnor] and my account number 
is [account ; 278 one nine five].? 
Next we replaced these annotated entities with 
grammar classes. We also utilized utterances from 
the Amiti?s banking corpus (Hardy et al, 2002) in 
which the customer specifies his/her desired task, 
as well as utterances which constitute common, 
domain-independent speech acts such as 
acceptances, rejections, and indications of non-
understanding. These were also used for training 
the task identifier and the dialogue act classifier 
(Section 3.3.2). The training corpus for the 
recognizer consists of 1744 utterances totaling 
around 10,000 words. 
Using tools supplied by Nuance for building 
recognition packages, we created two speech 
recognition components: a British model in the UK 
and an American model at two US sites. 
For the text to speech synthesizer we used 
Nuance?s Vocalizer 3.0, which supports multiple 
languages and accents. We integrated the 
Vocalizer and the ASR using Nuance?s speech and 
telephony API into a Galaxy-compliant server 
accessible over a telephone line. 
3.2 Natural Language Understanding 
The goal of the language understanding 
component is to take the word string output of the 
ASR module, and identify key semantic concepts 
relating to the target domain. This is a specialized 
kind of information extraction application, and as 
such, we have adapted existing IE technology to 
this task.  
Hub 
Speech 
Recognition 
Dialogue 
Manager Database 
Server 
Nat?l Language 
Understanding 
Telephony 
Server 
Response      
Generation 
Customer 
Database 
Text-to-speech
Conversion 
We have used a modified version of the ANNIE 
engine (A Nearly-New IE system; Cunningham et 
al., 2002; Maynard, 2003). ANNIE is distributed as 
the default built-in IE component of the GATE 
framework (Cunningham et al, 2002). GATE is a 
pure Java-based architecture developed over the 
past eight years in the University of Sheffield 
Natural Language Processing group. ANNIE has 
been used for many language processing 
applications, in a number of languages both 
European and non-European. This versatility 
makes it an attractive proposition for use in a 
multilingual speech processing project. 
ANNIE includes customizable components 
necessary to complete the IE task ? tokenizer, 
gazetteer, sentence splitter, part of speech tagger 
and a named entity recognizer based on a powerful 
engine named JAPE (Java Annotation Pattern 
Engine; Cunningham et al, 2000). 
Given an utterance from the user, the NLU unit 
produces both a list of tokens for detecting 
dialogue acts, an important research goal inside 
this project, and a frame with the possible named 
entities specified by our application. We are 
interested particularly in account numbers, credit 
card numbers, person names, dates, amounts of 
money, locations, addresses and telephone 
numbers.  
In order to recognize these, we have updated the 
gazetteer, which works by explicit look-up tables 
of potential candidates, and modified the rules of 
the transducer engine, which attempts to match 
new instances of named entities based on local 
grammatical context. There are some significant 
differences between the kind of prose text more 
typically associated with information extraction, 
and the kind of text we are expecting to encounter. 
Current models of IE rely heavily on punctuation 
as well as certain orthographic information, such as 
capitalized words indicating the presence of a 
name, company or location. We have access to 
neither of these in the output of the ASR engine, 
and so had to retune our processors to data which 
reflected that. 
In addition, we created new processing 
resources, such as those required to spot number 
units and translate them into textual representations 
of numerical values; for example, to take ?twenty 
thousand one hundred and fourteen pounds?, and 
produce ??20,114?. The ability to do this is of 
course vital for the performance of the system. 
If none of the main entities can be identified 
from the token string, we create a list of possible 
fallback entities, in the hope that partial matching 
would help narrow the search space. 
For instance, if a six-digit account number is not 
identified, then the incomplete number recognized 
in the utterance is used as a fallback entity and sent 
to the database server for partial matching. 
Our robust IE techniques have proved 
invaluable to the efficiency and spontaneity of our 
data-driven dialogue system. In a single utterance 
the user is free to supply several values for 
attributes, prompted or unprompted, allowing tasks 
to be completed with fewer dialogue turns. 
3.3 Dialogue Manager 
The dialogue manager identifies the goals of the 
conversation and performs interactions to achieve 
those goals. Several ?Frame Agents?, implemented 
within the dialogue manager, handle tasks such as 
verifying the customer?s identity, identifying the 
customer?s desired transaction, and executing those 
transactions. These range from a simple balance 
inquiry to the more complex change of address and 
debit-card payment. The structure of the dialogue 
manager is illustrated in Figure 2. 
Rather than depending on a script for the 
progression of the dialogue, the dialogue manager 
takes a data-driven approach, allowing the caller to 
take the initiative. Completing a task depends on 
identifying that task and filling values in frames, 
but this may be done in a variety of ways: one at a 
time, or several at once, and in any order. 
For example, if the customer identifies himself 
or herself before stating the transaction, or even if 
he or she provides several pieces of information in 
one utterance?transaction, name, account number, 
payment amount?the dialogue manager is flexible 
enough to move ahead after these variations. 
Prompts for attributes, if needed, are not restricted 
to one at a time, but they are usually combined in 
the way human agents request them; for example, 
city and county, expiration date and issue number, 
birthdate and telephone number. 
 
 
 
Figure 2. Amiti?s Dialogue Manager 
If the system fails to obtain the necessary values 
from the user, reprompts are used, but no more 
than once for any single attribute. For the customer 
verification task, different attributes may be 
 
 
 
 
 
 
 
 
 Response Decision 
Input:  
from NLU via 
Hub (token string, 
language id, 
named entities) 
Task infoExternal files, 
domain-specific
Dialogue Act 
Classifier 
Frame Agent 
Task ID 
Frame Agent 
Verify-Caller 
Frame Agent 
DB Server 
Customer 
Database
 
 
 
 
 
 
Task Execution 
Frame Agents via Hub 
Dialogue History 
requested. If the system fails even after reprompts, 
it will gracefully give up with an explanation such 
as, ?I?m sorry, we have not been able to obtain the 
information necessary to update your address in 
our records. Please hold while I transfer you to a 
customer service representative.? 
3.3.1 Task ID Frame Agent 
For task identification, the Amiti?s team has 
made use of the data collected in over 500 
conversations from a British call center, recorded, 
transcribed, and annotated. Adapting a vector-
based approach reported by Chu-Carroll and 
Carpenter (1999), the Task ID Frame Agent is 
domain-independent and automatically trained. 
Tasks are represented as vectors of terms, built 
from the utterances requesting them. Some 
examples of labeled utterances are: ?Erm I'd like to 
cancel the account cover premium that's on my, 
appeared on my statement? [CancelInsurance] and 
?Erm just to report a lost card please? 
[Lost/StolenCard].   
The training process proceeds as follows: 
1. Begin with corpus of transcribed, annotated 
calls. 
2. Document creation: For each transaction, collect 
raw text of callers? queries. Yield: one 
?document? for each transaction (about 14 of 
these in our corpus). 
3. Text processing: Remove stopwords, stem 
content words, weight terms by frequency. 
Yield: one ?document vector? for each task. 
4. Compare queries and documents: Create ?query 
vectors.? Obtain a cosine similarity score for 
each query/document pair. Yield: cosine 
scores/routing values for each query/document 
pair. 
5. Obtain coefficients for scoring: Use binary 
logistic regression. Yield: a set of coefficients 
for each task. 
Next, the Task ID Frame Agent is tested on 
unseen utterances or queries: 
1. Begin with one or more user queries. 
2. Text processing: Remove stopwords, stem 
content words, weight terms (constant weights). 
Yield: ?query vectors?. 
3. Compare each query with each document. 
Yield: cosine similarity scores. 
4. Compute confidence scores (use training 
coefficients). Yield: confidence scores, 
representing the system?s confidence that the 
queries indicate the user?s choice of a particular 
transaction. 
Tests performed over the entire corpus, 80% of 
which was used for training and 20% for testing, 
resulted in a classification accuracy rate of 85% 
(correct task is one of the system?s top 2 choices). 
The accuracy rate rises to 93% when we eliminate 
confusing or lengthy utterances, such as requests 
for information about payments, statements, and 
general questions about a customer?s account. 
These can be difficult even for human annotators 
to classify. 
3.3.2 Dialogue Act Classifier 
The purpose of the DA Classifier Frame Agent 
is to identify a caller?s utterance as one or more 
domain-independent dialogue acts. These include 
Accept, Reject, Non-understanding, Opening, 
Closing, Backchannel, and Expression. Clearly, it 
is useful for a dialogue system to be able to 
identify accurately the various ways a person may 
say ?yes?, ?no?, or ?what did you say?? As with 
the task identifier, we have trained the DA 
classifier on our corpus of transcribed, labeled 
human-human calls, and we have used vector-
based classification techniques. Two differences 
from the task identifier are 1) an utterance may 
have multiple correct classifications, and 2) a 
different stoplist is necessary. Here we can filter 
out the usual stops, including speech dysfluencies, 
proper names, number words, and words with 
digits; but we need to include words such as yeah, 
uh-huh, hi, ok, thanks, pardon and sorry.  
Some examples of DA classification results are 
shown in Figure 3. For sure, ok, the classifier 
returns the categories Backchannel, Expression and 
Accept. If the dialogue manager is looking for 
either Accept or Reject, it can ignore Backchannel 
and Expression in order to detect the correct 
classification. In the case of certainly not, the first 
word has a strong tendency toward Accept, though 
both together constitute a Reject act.  
 
Text: ?sure, okay? Text: ?certainly not?
Categories returned: Backchannel, 
Expression, Accept 
Categories returned:
Reject, Accept 
Expression
Closing
Accept
Back.
0
0.2
0.4
0.6
0.8
1
Top four cosine scores
Expression
Accept Closing
Back.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Reject
Reject-part
Accept Expression
0
0.1
0.2
0.3
0.4
0.5
0.6
Top four cosine scores
Reject
Accept Expression
Reject-part
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Figure 3. DA Classification examples 
 
Our classifier performs well if the utterance is 
short and falls into one of the selected categories 
(86% accuracy on the British data); and it has the 
advantages of automatic training, domain 
independence, and the ability to capture a great 
variety of expressions. However, it can be 
inaccurate when applied to longer utterances, and it 
is not yet equipped to handle domain-specific 
assertions, questions, or queries about a 
transaction. 
3.4 Database Manager 
Our system identifies users by matching 
information provided by the caller against a 
database of user information. It assumes that the 
speech recognizer will make errors when the caller 
attempts to identify himself. Therefore perfect 
matches with the database entries will be rare. 
Consequently, for each record in the database, we 
attach a measure of the probability that the record 
is the target record. Initially, these measures are 
estimates of the probability that this individual will 
call. When additional identifying information 
arrives, the system updates these probabilities 
using Bayes? rule. 
Thus, the system might begin with a uniform 
probability estimate across all database records. If 
the user identifies herself with a name recognized 
by the machine as ?Smith?, the system will 
appropriately increment the probabilities of all 
entries with the name ?Smith? and all entries that 
are known to be confused with ?Smith? in 
proportion to their observed rate of substitution. Of 
course, all records not observed to be so 
confusable would similarly have their probabilities 
decreased by Bayes? rule. When enough 
information has come in to raise the probability for 
some record above a threshold (in our system 0.99 
probability), the system assumes that the caller has 
been correctly identified. The designer may choose 
to include a verification dialog, but our decision 
was to minimize such interactions to shorten the 
calls.  
Our error-correcting database system receives 
tokens with an identification of what field each 
token should represent. The system processes the 
tokens serially. Each represents an observation 
made by the speech recognizer. To process a token, 
the system examines each record in the database 
and updates the probability that the record is the 
target record using Bayes? rule: 
 
 
  
where rec is the event where the record under 
consideration is the target record.  
As is common in Bayes? rule calculations, the 
denominator P(obs) is treated as a scaling factor, 
and is not calculated explicitly. All probabilities 
are renormalized at the end of the update of all of 
the records. P(rec) is the previous estimate of the 
probability that the record is the target record. 
P(obs|rec) is the probability that the recognizer 
returned the observation that it did given that the 
target record is the current record under 
examination. For some of the fields, such as the 
account number and telephone number, the user 
responses consist of digits. We collected data on 
the probability that the speech recognition system 
we are using mistook one digit for another and 
calculated the values for P(obs|rec) from the data. 
For fields involving place names and personal 
names, the probabilities were estimated.  
Once a record has been selected (by virtue of its 
probability being greater than the threshold) the 
system compares the individual fields of the record 
with values obtained by the speech recognizer. If 
the values differ greatly, as measured by their 
Levenshtein distance, the system returns the field 
name to the dialogue manager as a candidate for 
additional verification. If no record meets the 
threshold probability criterion, the system returns 
the most probable record to the dialogue manager, 
along with the fields which have the greatest 
Levenshtein distance between the recognized and 
actual values, as candidates for reprompting.  
Our database contains 100 entries for the system 
tests described in this paper. We describe the 
system in a more demanding environment with one 
million records in Inouye et al (2004). In that 
project, we required all information to be entered 
by spelling the items out so that the vocabulary 
was limited to the alphabet plus the ten digits. In 
the current project, with fewer names to deal with, 
we allowed the complete vocabulary of the 
domain: names, streets, counties, and so forth.  
3.5 Response Generator 
Our current English-only system preserves the 
language-independent features of our original tri-
lingual generator, storing all language- and 
domain-specific information in separate text files. 
It is a template-based system, easily modified and 
extended. The generator constructs utterances 
according to the dialogue manager?s specification 
of one or more speech acts (prompt, request, 
confirm, respond, inform, backchannel, accept, 
reject), repetition numbers, and optional lists of 
attributes, values, and/or the person?s name. As far 
as possible, we modeled utterances after the 
human-human dialogues. 
For a more natural-sounding system, we 
collected variations of the utterances, which the 
generator selects at random. Requests, for 
example, may take one of twelve possible forms: 
Request, part 1 of 2: 
Can you just confirm | Can I have | Can I take | 
What is | What?s | May I have 
)(
)()|()|(
obsP
recPrecobsPobsrecP ?=
Request, part 2 of 2: 
[list of attributes], [person name]? | [list of 
attributes], please? 
Offers to close or continue the dialogue are 
similarly varied: 
Closing offer, part 1 of 2: 
Is there anything else | Anything else | Is there 
anything else at all 
Closing offer, part 2 of 2: 
I can do for you today? | I can help you with 
today? | I can do for you? | I can help you with? | 
you need today? | you need? 
4 Preliminary Evaluation 
Ten native speakers of English, 6 female and 4 
male, were asked to participate in a preliminary in-
lab system evaluation (half in the UK and half in 
the US). The Amiti?s system developers were not 
among these volunteers. Each made 9 phone calls 
to the system from behind a closed door, according 
to scenarios designed to test various customer 
identities as well as single or multiple tasks. After 
each call, participants filled out a questionnaire to 
register their degree of satisfaction with aspects of 
the interaction. 
Overall call success was 70%, with 98% 
successful completions for the VerifyId and 96% 
for the CheckBalance subtasks (Figure 4). 
?Failures? were not system crashes but simulated 
transfers to a human agent. There were 5 user 
terminations. 
Average word error rates were 17% for calls that 
were successfully completed, and 22% for failed 
calls. Word error rate by user ranged from 11% to 
26%. 
 
0.70
0.98 0.96
0.88 0.90
0.57
0.85
0.00
0.20
0.40
0.60
0.80
1.00
1.20
Ca
ll S
uc
ce
ss
Ve
rify
Id
Ch
ec
kB
ala
nc
e
Lo
stC
ar
d
Ma
ke
Pa
ym
en
t
Ch
an
ge
Ad
dr
es
s
Fin
ish
Di
alo
gu
e
 
Figure 4. Task Completion Rates 
Call duration was found to reflect the 
complexity of each scenario, where complexity is 
defined as the number of ?concepts? needed to 
complete each task. The following items are 
judged to be concepts: task identification; values 
such as first name, last name, house number, street 
and phone number; and positive or negative 
responses such as whether a new card is desired. 
Figures 5 and 6 illustrate the relationship between 
length of call and task complexity. It should be 
noted that customer verification, a task performed 
in every dialogue, requires a minimum of 3 
personal details to be verified against a database 
record, but may require more in the case of 
recognition errors. 
The overall average number of turns per 
dialogue was 18.28. The user spoke an average of 
6.89 words per turn and the system 11.42. 
User satisfaction for each call was assessed by 
way of a questionnaire containing five statements. 
These covered the clarity of the instructions, ease 
of doing the task, how well the system understands 
the caller, how well the system works, and the 
caller?s enjoyment of the system. Participants rated 
each on a five-point Likert scale. Summed results 
showed an average score of 20.45 over all users 
(range 5?25; higher = stronger agreement). 
 
0
2
4
6
8
10
12
1 2 3 4 5 6 7 8 9
Scenario
Nu
m
be
r o
f c
on
ce
pt
s
 
85.0
135.8
119.3
88.3
106.7 114.0
48.4
109.3
127.1
0.0
20.0
40.0
60.0
80.0
100.0
120.0
140.0
160.0
1 2 3 4 5 6 7 8 9
Figures 5 and 6. Scenario Complexity (top) and 
Average Call Duration by Scenario (in seconds) 
Although user satisfaction was high, we were 
more interested in identifying the major problems 
for the callers. Users were often frustrated by 
recognition failures and/or unsuccessful attempts 
to capture values such as a new street address, 
county, or phone number. Sometimes the system 
failed to determine that the user was finished. 
Because the system is designed to be non-
restrictive in accepting users? input, 
misidentifications were common. One user spoke 
for many by saying, ?There were times when the 
system would ask me to repeat information I had 
not yet given to it.? We plan to revise our strategy 
so that we are a little more cautious in our 
reprompts. Occasionally, the system misidentified 
the user?s desired task and had difficulty backing 
off gracefully and starting the correct task. We are 
working on improving our recovery strategies for 
these cases. 
 The following transcription of an interaction 
with our system illustrates the efficiency made 
possible when the data supplied by the caller drives 
the conversation: 
AMITI?S:  Hello, this is Am-ih-tee-eh! What can 
I do for you? 
USER:  yes this is Stella Lang. My card number 
is three one six seven one four. Can you tell me the 
balance on my account please? 
AMITI?S:  Can you just confirm the city and 
county? 
USER:  uh yes it?s London, Middlesex 
AMITI?S:  I?m sorry, I didn?t understand what 
you said.  Can you repeat, please? 
USER:  London, Middlesex 
AMITI?S:  All right, your balance is 1753 
pounds and 20 pence. Anything else I can do for 
you? 
USER:  yes I'd like to make a payment of fifty 
three pounds with my debit card please 
AMITI?S:  Can I have the debit card number, 
Miss Lang? 
USER:  sure, it's two eight seven four six six four 
nine five eight nine five three eight nine one, and it 
expires October two thousand six 
AMITI?S:  What's the issue number, please? 
USER:  one 
AMITI?S:  Ok, Miss Lang, I'll send in the 
payment amount of 53 pounds. Did I get that right? 
USER:  yes, what's my new balance please? 
AMITI?S:  Ok, that's gone through. Your 
payment will be credited overnight and will reflect 
on your next statement. Your balance is 1700 
pounds and 20 pence. Anything else I can do for 
you today? 
USER:  no, that's it thanks 
AMITI?S:  Thanks very much for your call, 
goodbye. 
USER:  bye 
5 Discussion, Future Work 
The preliminary evaluation reported here 
indicates promise for an automated dialogue 
system such as ours, which incorporates robust 
techniques for information extraction, record 
matching, task identification, dialogue act 
classification, and an overall data-driven strategy. 
Task duration and number of turns per dialogue 
both appear to indicate greater efficiency and 
corresponding user satisfaction than many other 
similar systems. In the DARPA Communicator 
evaluation, for example, between 60 and 79 calls 
were made to each of 8 participating sites (Walker, 
et al, 2001, 2002). A sample scenario for a 
domestic round-trip flight contained 8 concepts 
(airline, departure city, state, date, etc.). The 
average duration for such a call was over 300 
seconds; whereas our overall average was 104 
seconds. ASR accuracy rates in 2001 were about 
60% and 75%, for airline itineraries not completed 
and completed; and task completion rates were 
56%. Our average number of user words per turn, 
6.89, is also higher than that reported for 
Communicator systems. This number seems to 
reflect lengthier responses to open prompts, 
responses to system requests for multiple 
attributes, and greater user initiative. 
We plan to port the system to a new domain: 
from telephone banking to information-technology 
support. As part of this effort we are again 
collecting data from real human-human calls. For 
advanced speech recognition, we hope to train our 
ASR on new acoustic data. We also plan to expand 
our dialogue act classification so that the system 
can recognize more types of acts, and to improve 
our classification reliability.  
6 Acknowledgements 
This paper is based on work supported in part by 
the European Commission under the 5th 
Framework IST/HLT Programme, and by the US 
Defense Advanced Research Projects Agency. 
References 
J. Allen and M. Core. 1997. Draft of DAMSL: 
Dialog Act Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/resour
ces/damsl/. 
J. Allen, L. K. Schubert, G. Ferguson, P. Heeman, 
Ch. L. Hwang, T. Kato, M. Light, N. G. Martin, 
B. W. Miller, M. Poesio, and D. R. Traum. 
1995. The TRAINS Project: A Case Study in 
Building a Conversational Planning Agent. 
Journal of Experimental and Theoretical AI, 7 
(1995), 7?48. 
Amiti?s, http://www.dcs.shef.ac.uk/nlp/amities.  
J. Chu-Carroll and B. Carpenter. 1999. Vector-
Based Natural Language Call Routing. 
Computational Linguistics, 25 (3): 361?388. 
H. Cunningham, D. Maynard, K. Bontcheva, V. 
Tablan. 2002. GATE: A Framework and 
Graphical Development Environment for Robust 
NLP Tools and Applications. Proceedings of the 
40th Anniversary Meeting of the Association for 
Computational Linguistics (ACL'02), 
Philadelphia, Pennsylvania. 
H. Cunningham and D. Maynard and V. Tablan. 
2000. JAPE: a Java Annotation Patterns Engine 
(Second Edition). Technical report CS--00--10, 
University of Sheffield, Department of 
Computer Science.  
DARPA, 
http://www.darpa.mil/iao/Communicator.htm. 
H. Hardy, K. Baker, L. Devillers, L. Lamel, S. 
Rosset, T. Strzalkowski, C. Ursu and N. Webb. 
2002. Multi-Layer Dialogue Annotation for 
Automated Multilingual Customer Service. 
Proceedings of the ISLE Workshop on Dialogue 
Tagging for Multi-Modal Human Computer 
Interaction, Edinburgh, Scotland. 
H. Hardy, T. Strzalkowski and M. Wu. 2003a. 
Dialogue Management for an Automated 
Multilingual Call Center. Research Directions in 
Dialogue Processing, Proceedings of the HLT-
NAACL 2003 Workshop, Edmonton, Alberta, 
Canada. 
H. Hardy, K. Baker, H. Bonneau-Maynard, L. 
Devillers, S. Rosset and T. Strzalkowski. 2003b. 
Semantic and Dialogic Annotation for 
Automated Multilingual Customer Service. 
Eurospeech 2003, Geneva, Switzerland. 
R. B. Inouye, A. Biermann and A. Mckenzie. 
2004. Caller Identification from Spelled-Out 
Personal Data Using a Database for Error 
Correction. Duke University Internal Report. 
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, 
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. 
Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and 
M. Walker. 2000. The AT&T-DARPA 
Communicator Mixed-Initiative Spoken Dialog 
System. ICSLP 2000. 
D. Maynard. 2003. Multi-Source and Multilingual 
Information Extraction. Expert Update. 
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, 
and V. Zue. 1998. Galaxy-II: A Reference 
Architecture for Conversational System 
Development. ICSLP 98, Sydney, Australia. 
S. Seneff and J. Polifroni. 2000. Dialogue 
Management in the Mercury Flight Reservation 
System. Satellite Dialogue Workshop, ANLP-
NAACL, Seattle, Washington. 
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. 
Garofolo, L. Hirschman, A. Le, S. Lee, S. 
Narayanan, K. Papineni, B. Pellom, J. Polifroni, 
A. Potamianos, P. Prabhu, A. Rudnicky, G. 
Sanders, S. Seneff, D. Stallard and S. Whittaker. 
2001. DARPA Communicator Dialog Travel 
Planning Systems: The June 2000 Data 
Collection. Eurospeech 2001. 
M. Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J. 
Garofolo, H. Hastie, A. Le, B. Pellom, A. 
Potamianos, R. Passonneau, R. Prasad, S. 
Roukos, G. Sanders, S. Seneff and D. Stallard. 
2002. DARPA Communicator Evaluation: 
Progress from 2000 to 2001. ICSLP 2002. 
W. Ward and B. Pellom. 1999. The CU 
Communicator System. IEEE ASRU, pp. 341?
344. 
W. Xu and A. Rudnicky. 2000. Task-based Dialog 
Management Using an Agenda. ANLP/NAACL 
Workshop on Conversational Systems, pp. 42?
47. 
 
HITIQA:  An Interactive Question Answering System  
A Preliminary Report 
 
Sharon Small, Ting Liu, Nobuyuki Shimizu, and Tomek Strzalkowski  
 
ILS Institute 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tl7612,ns3203,tomek}@albany.edu 
 
 
Abstract
HITIQA is an interactive question answering 
technology designed to allow intelligence analysts 
and other users of information systems to pose 
questions in natural language and obtain relevant 
answers, or the assistance they require in order to 
perform their tasks. Our objective in HITIQA is to 
allow the user to submit exploratory, analytical, 
non-factual questions, such as ?What has been 
Russia?s reaction to U.S. bombing of Kosovo?? 
The distinguishing property of such questions is 
that one cannot generally anticipate what might 
constitute the answer. While certain types of things 
may be expected (e.g., diplomatic statements), the 
answer is heavily conditioned by what information 
is in fact available on the topic. From a practical 
viewpoint, analytical questions are often under-
specified, thus casting a broad net on a space of 
possible answers. Therefore, clarification dialogue 
is often needed to negotiate with the user the exact 
scope and intent of the question. 
 
1   Introduction 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in 
the state of the art of automated question answer-
ing.  In this paper we focus on two aspects of our 
work: 
1. Question Semantics: how the system ?un-
derstands? user requests. 
2. Human-Computer Dialogue: how the user 
and the system negotiate this understand-
ing. 
     We will also discuss very preliminary evalua-
tion results from a series of pilot tests of the system 
conducted by intelligence analysts via a remote 
internet link.  
   
2   Factual vs. Analytical 
The objective in HITIQA is to allow the user to 
submit and obtain answers to exploratory, analyti-
cal, non-factual questions.  There are very signifi-
cant differences between factual, or fact-finding, 
and analytical question answering. A factual ques-
tion seeks pieces of information that would make a 
corresponding statement true (i.e., they become 
facts): ?How many states are in the U.S.?? / ?There 
are X states in the U.S.? In this sense, a factual 
question usually has just one correct answer that 
can generally, be judged for its truthfulness. By 
contrast, an analytical question is when the ?truth? 
of the answer is more a matter of opinion and may 
depend upon the context in which the question is 
asked. Answers to analytical questions are rarely 
unilateral, indeed, a mere ?correct? answer may 
have limited value, and in some cases may not 
even be determinate (?Which college is the best??, 
?How do I stop my baby?s crying??). Instead, an-
swers to analytical questions are often judged as 
helpful, or useful, or satisfactory, etc. ?Technically 
correct? answers (e.g., ?feed the baby milk?) may 
be considered as irrelevant or at best unresponsive.   
     The distinction between factual and analytical 
questions depends primarily on the intention of the 
person who is asking, however, the form of a ques-
tion is often indicative of which of the two classes 
it is more likely to belong to.  Factual questions 
can be classified into a number of syntactic formats 
(?question typology?) that aids in automatic proc-
essing. 
     Factual questions display a fairly distinctive 
?answer type?, which is the type of the information 
piece needed to fulfill the statement.  Recent auto-
mated systems for answering factual questions  
deduct  this expected answer type from the form of 
the question and a finite list of possible answer 
 types. For example, ?Who was the first man in 
space? expects a ?person? as the answer, while 
?How long was the Titanic?? expects some length 
measure as an answer, probably in yards and feet, 
or meters.  This is generally a very good strategy, 
that has been exploited successfully in a number of 
automated QA systems that appeared in recent 
years, especially in the context of TREC QA1 
evaluations (Harabagiu et al, 2000; Hovy et al, 
2000; Prager at al., 2001).     
     This process is not easily applied to analytical 
questions. This is because the type of an answer for 
analytical questions cannot always be anticipated 
due to their inherently exploratory character.  In 
contrast to a factual question, an analytical ques-
tion has an unlimited variety of syntactic forms 
with only a loose connection between their syntax 
and the expected answer.  Given the unlimited po-
tential of the formation of analytical questions, it 
would be counter-productive to restrict them to a 
limited number of question/answer types. Even 
finding a non-strictly factual answer to an other-
wise simple question about Titanic length (e.g., 
?two football fields?) would push the limits of the 
answer-typing approach. Therefore, the formation 
of an answer should instead be guided by the top-
ics the user is interested in, as recognized in the 
query and/or through the interactive dialogue, 
rather than by a single type as inferred from the 
query in a factual system.   
     This paper argues that the semantics of an ana-
lytical question is more likely to be deduced from 
the information that is considered relevant to the 
question than through a detailed analysis of their 
particular form. While this may sound circular, it 
needs not be. Determining ?relevant? information 
is not the same as finding an answer; indeed we 
can use relatively simple information retrieval 
methods (keyword matching, etc.) to obtain per-
haps 50 or 100 ?relevant? documents from a data-
base. This gives us an initial answer space to work 
on in order to determine the scope and complexity 
of the answer. In our project, we use structured 
templates, which we call frames to map out the 
content of pre-retrieved documents, and subse-
quently to delineate the possible meaning of the 
question (Section 6). 
                                                 
1 TREC QA is the annual Question Answering evalua-
tion sponsored by the U.S. National Institute of Stan-
dards and Technology www.trec.nist.gov. 
 
3   Document Retrieval 
When the user poses a question to a system sitting 
atop a huge database of unstructured data (text 
files), the first order of business is to reduce that 
pile to perhaps a handful of documents where the 
answer is likely to be found. This means, most of-
ten, document retrieval, using fast but non-exact 
selection methods.  Questions are tokenized and 
sent to a document retrieval engine, such as Smart 
(Buckley, 1985) or InQuery (Callan et al, 1992).  
Noun phrases and verb phrases are extracted from 
the question to give us a list of potential topics that 
the user may be interested in.   
    In the experiments with the HITIQA prototype, 
see Figure 1, we are retrieving the top fifty docu-
ments from three gigabytes of newswire 
(AQUAINT corpus plus web-harvested docu-
ments).  
 
Document 
Retrieval
Document 
Retrieval
Build
Frames
Build
Frames
Process
Frames
Process
Frames
Dialogue
Manager
Dialogue
anager
Segment/
Filter
Segment/
Filter
Cluster
Paragraphs
Cluster
Paragraphs
Answer
Generator
Answer
Generator
answer
Tokenized 
question
top 50 
documents
distinct 
paragraphs
clusters
framed text 
segments
candidate 
answer topics
relevant text 
segments
system 
clarification 
question/
user response
DB
Gate
Wordnet
Figure 1: HITIQA preliminary architecture 
 
4   Data Driven Semantics of Questions 
The set of documents and text passages returned 
from the initial search is not just a random subset 
of the database. Depending upon the quality (recall 
and precision) of the text retrieval system avail-
 able, this set can be considered as a first stab at 
understanding the user?s question by the machine.  
Again, given the available resources, this is the 
best the system can do under the circumstances. 
Therefore, we may as well consider this collection 
of retrieved texts (the Retrieved Set) as the mean-
ing of the question as understood by the system. 
This is a fair assessment: the better our search ca-
pabilities, the closer this set would be to what the 
user may accept as an answer to the question.  
     We can do better, however. We can perform 
automatic analysis of the retrieved set, attempting 
to uncover if it is a fairly homogenous bunch (i.e., 
all texts have very similar content), or whether 
there are a number of diverse topics represented 
there, somehow tied together by a common thread. 
In the former case, we may be reasonably confi-
dent that we have the answer, modulo the retriev-
able information. In the latter case, we know that 
the question is more complex than the user may 
have intended, and a negotiation process is needed. 
     We can do better still. We can measure how 
well each of the topical groups within the retrieved 
set is ?matching up? against the question. This is 
accomplished through a framing process described 
later in this paper. The outcome of the framing 
process is twofold: firstly, the alternative interpre-
tations of the question are ranked within 3 broad 
categories: on-target, near-misses and outliers. 
Secondly, salient concepts and attributes for each 
topical group are extracted into topic frames. This 
enables the system to conduct a meaningful dia-
logue with the user, a dialogue which is wholly 
content oriented, and thus entirely data driven.  
ON-TARGET
OUTLIERS
NEAR-MISSES
 
Figure 2: Answer Space Topology.  The goal of interac-
tive QA it to optimize the ON-TARGET middle zone. 
 
5   Clustering 
We use n-gram-based clustering of text passages 
and concept extraction  to uncover the main topics, 
themes and entities in this set.  
     Retrieved documents are first broken into natu-
rally occurring paragraphs.  Duplicate paragraphs 
are filtered out and the remaining passages are 
clustered using a combination of hierarchical clus-
tering and n-bin classification (details of the clus-
tering algorithm can be found in Hardy et al, 
2002a).  Typically three to six clusters are gener-
ated out of the top 50 documents, which may yield 
as many as 1000 passages.  Each cluster represents 
a topic theme within the retrieved set: usually an 
alternative or complimentary interpretation of the 
user?s question. 
    A list of topic labels is assigned to each cluster. 
A topic label may come from one of two places:  
First, the texts in the cluster are compared against 
the list of key phrases extracted from the user?s 
query.  For each match found, the matching phrase 
is used as a topic label for the cluster. If a match 
with the key phrases from the question cannot be 
obtained, Wordnet is consulted to see if a common 
ancestor can be found. For example, ?rifle? and 
?machine gun? are kinds of ?weaponry? in Word-
net, which allows an indirect match between a 
question about weapon inspectors and a text re-
porting a discovery by the authorities of a cache of 
?rifles? and ?machine guns?.  
 
6   Framing 
In HITIQA we use a text framing technique to de-
lineate the gap between the meaning of the user?s 
question and the system ?understanding? of this 
question. The framing is an attempt to impose a 
partial structure on the text that would allow the 
system to systematically compare different text 
pieces against each other and against the question, 
and also to communicate with the user about this. 
In particular, the framing process may uncover 
topics and themes within the retrieved set which 
the user has not explicitly asked for, and thus may 
be unaware of their existence. Nonetheless these 
may carry important information ? the NEAR-
MISSES in Figure 2. 
     In the current version of the system, frames are 
fairly generic templates, consisting of a small 
number of attributes, such as LOCATION, PERSON, 
COUNTRY, ORGANIZATION, etc.  Future versions of 
HITIQA will add domain specialized frames, for 
example, we are currently constructing frames for 
the Weapons Non-proliferation Domain. Most of 
the frame attributes are defined in advance, how-
 ever, dynamic frame expansion is also possible. 
Each of the attributes in a frame is equipped with 
an extractor function which specializes in locating 
and extracting instances of this attribute in the run-
ning text. The extractors are implemented using 
information extraction utilities which form the ker-
nel of Sheffield?s GATE2 system.  We have modi-
fied GATE to separate organizations into compa-
nies and other organizations, and we have also ex-
panded by adding new concepts such as industries.  
Therefore, the framing process resembles strongly 
the template filling task in information extraction 
(cf. MUC3 evaluations), with one significant ex-
ception: while the MUC task was to fill in a tem-
plate using potentially any amount of source text 
(Humphreys et al, 1998), the framing is essentially 
an inverse process. In framing, potentially multiple 
frames can be associated with a small chunk of text 
(a passage or a short paragraph). Furthermore, this 
chunk of text is part of a cluster of very similar text 
chunks that further reinforce some of the most sali-
ent features of these texts. This makes the frame 
filling a significantly less error-prone task ? our 
experience has been far more positive than the 
MUC evaluation results may indicate. This is be-
cause, rather than trying to find the most appropri-
ate values for attributes from among many poten-
tial candidates, we in essence fit the frames over 
small passages4.  
Therefore, data frames are built from the re-
trieved data, after clustering it into several topical 
groups. Since clusters are built out of small text 
passages, we associate a frame with each passage 
that serves as a seed of a cluster. We subsequently 
merge passages, and their associated frames when-
ever anaphoric and other cohesive links are de-
tected.   
     A very similar process is applied to the user?s 
question, resulting in a Goal Frame which can be 
subsequently compared to the data frames obtained 
from retrieved data. For example, the Goal Frame 
generated from the question, ?How has pollution in 
the Black Sea affected the fishing industry, and 
                                                 
2 GATE is Generalized Architecture for Text Engineering, an 
information extraction system developed at the University of 
Sheffield (Cunningham, 2000). 
3 MUC, the Message Understanding Conference, funded by 
ARPA, involved the evaluation of information extraction sys-
tems applied to a common task. 
4 We should note that selecting the right frame type for a pas-
sage is an important pre-condition to ?understanding?. 
what are the sources of this pollution?? is shown 
in Figure 3 below. 
 
TOPIC:[pollution, industry, sources] 
LOCATION: [Black Sea] 
INDUSTRY:[fishing] 
Figure 3: HITIQA generated Goal Frame 
 
            
TOPIC: pollution 
SUB-TOPIC: [sources] 
LOCATION: [Black Sea] 
INDUSTRY :[fisheries, tourism] 
TEXT: [In a period of only three decades (1960's-1980's), 
the Black Sea has suffered the catastrophic degradation 
of a major part of its natural resources. Particularly acute 
problems have arisen as a result of pollution (notably 
from nutrients, fecal material, solid waste and oil), a 
catastrophic decline in commercial fish stocks, a severe 
decrease in tourism and an uncoordinated approach to-
wards coastal zone management. Increased loads of nutri-
ents from rivers and coastal sources caused an overpro-
duction of phytoplankton leading to extensive eutrophica-
tion and often extremely low dissolved oxygen concentra-
tions. The entire ecosystem began to collapse. This prob-
lem, coupled with pollution and irrational exploitation of 
fish stocks, started a sharp decline in fisheries resources.] 
RELEVANCE: Matches on all elements found in goalframe  
Figure 4: A HITIQA generated data frame.  Words in 
bold were used to fill the Frame. 
 
     The data frames are then compared to the Goal 
Frame. We pay particular attention to matching the 
topic attributes, before any other attributes are con-
sidered. If there is an exact match between a Goal 
Frame topic and the text being used to build the 
data frame, then this becomes the data frame?s 
topic as well.  If more than one match is found, the 
subsequent matches become the sub-topics of the 
data frame. On the other hand, if no match is pos-
sible against the Goal Frame topic, we choose the 
topic from the list of the Wordnet generated hy-
pernyms. An example data frame generated from 
the text retrieved in response to the query about the 
Black Sea is shown in Figure 4. After the initial 
framing is done, frames judged to be related to the 
same concept or event, are merged together and 
values of their attributes are combined. 
 
7   Judging Frame Relevance 
We judge a particular data frame as relevant, and 
subsequently the corresponding segment of text as 
relevant, by comparison to the Goal Frame. The 
 data frames are scored based on the number of 
conflicts found between them and the Goal Frame. 
The conflicts are mismatches on values of corre-
sponding attributes. If a data frame is found to 
have no conflicts, it is given the highest relevance 
rank, and a conflict score of zero.  All other data 
frames are scored with an incrementing conflict 
value, one for frames with one conflict with the 
Goal Frame, two for two conflicts etc.  Frames that 
conflict with all information found in the query are 
given a score of 99 indicating the lowest relevancy 
rank.  Currently, frames with a conflict score of 99  
are excluded from further processing. The frame in 
Figure 4 is scored as fully relevant to the question 
(0 conflicts). 
 
8   Enabling Dialogue with the User 
Framed information allows HITIQA to automati-
cally judge some text as relevant and to conduct a 
meaningful dialogue with the user as needed on 
other text. The purpose of the dialogue is to help 
the user to navigate the answer space and to solicit 
from the user more details as to what information 
he or she is seeking. The main principle here is that 
the dialogue is at the information semantic level, 
not at the information organization level. Thus, it is 
okay to ask the user whether information about the 
AIDS conference in Cape Town should be in-
cluded in the answer to a question about combating 
AIDS in Africa. However, the user should never be 
asked if a particular keyword is useful or not, or if 
a document is relevant or not. We have developed 
a 3-pronged strategy: 
1. Narrowing dialogue: ask questions that 
would allow the system to reduce the size 
of the answer set.  
2. Expanding dialogue: ask questions that 
would allow the system to decide if the an-
swer set needs to be expanded by informa-
tion just outside of it (near-misses). 
3. Fact seeking dialogue: allow the user to 
ask questions seeking additional facts and 
specific examples, or similar situations. 
Of the above, we have thus far implemented the 
first two options as part of the preliminary clarifi-
cation dialogue. The clarification dialogue is when 
the user and the system negotiate the task that 
needs to be performed. We can call this a ?triaging 
stage?, as opposed to the actual problem solving 
stage (point 3 above). In practice, these two stages 
are not necessarily separated and may be overlap-
ping throughout the entire interaction. Nonetheless, 
these two have decidedly distinct character and 
require different dialogue strategies on the part of 
the system. 
     Our approach to dialogue in HITIQA is mod-
eled to some degree upon the mixed-initiative dia-
logue management adopted in the AMITIES pro-
ject (Hardy et al, 2002b). The main advantage of 
the AMITIES model is its reliance on data-driven 
semantics which allows for spontaneous and mixed 
initiative dialogue to occur.  
     By contrast, the major approaches to implemen-
tation of dialogue systems to date rely on systems 
of functional transitions that make the resulting 
system much less flexible. In the grammar-based 
approach, which is prevalent in commercial sys-
tems, such as in various telephony products, as 
well as in practically oriented research prototypes5, 
(e.g., DARPA, 2002; Seneff and Polifoni, 2000; 
Ferguson and Allen, 1998) a complete dialogue 
transition graph is designed to guide the conversa-
tion and predict user responses, which is suitable 
for closed domains only. In the statistical variation 
of this approach, a transition graph is derived from 
a large body of annotated conversations (e.g., 
Walker, 2000; Litman and Pan, 2002). This latter 
approach is facilitated through a dialogue annota-
tion process, e.g., using Dialogue Act Markup in 
Several Layers (DAMSL) (Allen and Core, 1997), 
which is a system of functional dialogue acts.  
     Nonetheless, an efficient, spontaneous dialogue 
cannot be designed on a purely functional layer. 
Therefore, here we are primarily interested in the 
semantic layer, that is, the information exchange 
and information building effects of a conversation. 
In order to properly understand a dialogue, both 
semantic and functional layers need to be consid-
ered. In this paper we are concentrating exclusively 
on the semantic layer. 
 
9   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space. Depending upon the 
size of this set and the presence of other frames 
outside of it, the system either proceeds to generate 
the answer or initiates a dialogue with the user. For 
                                                 
5 A notable exception is CU Communicator developed 
at University of Colorado (Ward and Pellom, 1999) 
 example, if the answer space appears too large or 
varied, e.g. consists of many different topics, the 
system may ask the user how to narrow it. Alterna-
tively, the presence of large groups of texts frames 
with near-miss frames assigned to them (i.e., 
frames with 1 or 2 conflicts with the Goal Frame) 
may indicate that the answer space is actually lar-
ger, and the user will be consulted about a  possi-
ble broadening of the question. Currently, we only 
initiate a clarification dialogue for 1-conflict 
frames.  
     A 1-conflict frame has only a single attribute 
mismatch with the Goal Frame. This could be a 
mismatch on any attribute, for example, LOCA-
TION, or ORGANIZATION, or TIME, etc. A special 
case arises when the conflict occurs on the TOPIC 
attribute.  Since all other attributes match, we may 
be looking at potentially different events or situa-
tions involving the same entities, or occurring at 
the same location or time. The purpose of the clari-
fication dialogue in this case is to probe which of 
these topics may be of interest to the user.  This is 
illustrated in the exchange below recorded during 
an evaluation session with an intelligence analyst: 
 
User: ?Who is Elizardo Sanchez?? 
HITIQA: ?Are you interested in seeing information 
about civil rights as it is related to Elizardo Sanchez? 
 
ONE-CONFLICT FRAME 
TOPIC: civil_rights  
SUB-TOPIC: []  
LOCATION: [Cuba, Cuba, Cuba]  
COMPANIES: []  
PEOPLE: [Sanchez, Sanchez]  
DOCUMENT DATE: [2000, 1, 11]  
SOURCE: HAVANA (AP) ?  
TEXT: [``I consider that the situation for civil and political 
rights in Cuba has worsened over the past year... owing to 
that Cuba continues to be the only closed society in this hemi-
sphere,'' Sanchez said. ``There have been no significant re-
lease of prisoners, the number of people sanctioned or proc-
essed for political motives increased. Sanchez, who himself 
spent many years in Cuban prisons, is among the communist 
island's best known opposition activists. The commission he 
heads issues a report on civil rights every six months, along 
with a list of people it considers to be imprisoned for political 
motives. ] 
Figure 5: One of the Frames that were used in generat-
ing Sanchez  dialogue.  Words in bold were used to fill 
the Frame. 
 
    In order to understand what happened here, we 
need to note first that the Goal Frame for the user 
question does not have any specific value assigned 
to its TOPIC attribute. This of course is as we would 
expect it: the question does not give us a hint as to 
what information we need to look for or may be 
hoping to find about Sanchez. This also means that 
all the text frames obtained from the retrieved set 
for this question will have at least one conflict, 
near-misses. One such text frame is shown in Fig-
ure 5: its topic is ?civil rights? and it about San-
chez. HITIQA thus asks if ?civil rights? is a topic 
of interest to the user. If the user responds posi-
tively, this topic will be added to the answer space.    
     The above dialogue strategy is applicable to 
other attribute mismatch cases, and produces intel-
ligent-sounding responses from the system. During 
the dialogue, as new information is obtained from 
the user, the Goal Frame is updated and the scores 
of all the data frames are reevaluated. The system 
may interpret the new information as a positive or 
negative. Positives are added to the Goal Frame. 
Negatives are stored in a Negative-Goal Frame and 
will also be used in the re-scoring of the data 
frames, possibly causing conflict scores to in-
crease. The Negative-Goal Frame is created when 
HITIQA receives a negative response from the 
user. The Negative-Goal Frame includes informa-
tion that HITIQA has identified as being of no in-
terest to the user.  If the user responds the equiva-
lent of ?yes? to the system clarification question  in 
the Sanchez dialogue, civil_rights will be added to 
the topic list in the Goal Frame and all one-conflict 
frames with a civil_rights topic will be re-scored to 
Zero conflicts, two-conflict frames with 
civil_rights as a topic will be rescored to one, etc.  
If the user responds ?no?, the Negative-Goal 
Frame will be generated and all frames with 
civil_rights as a topic will be rescored to 99 in or-
der to remove them from further processing. 
     The clarification dialogue will continue on the 
topic level until all the significant sets of NEAR-
MISS frames are either included in the answer 
space (through user broadening the scope of the 
question that removes the initial conflicts) or dis-
missed as not relevant. When HITIQA reaches this 
point it will re-evaluate the data frames in its an-
swer space.  If there are too many answer frames 
now (more than a pre-determined upper threshold), 
the dialogue manager will offer to the user to nar-
row the question using another frame attribute. If 
the size of the new answer space is still too small 
(i.e., there are many unresolved near-miss  frames), 
 the dialogue manager will suggest to the user ways 
of further broadening the question, thus making 
more data frames relevant, or possibly retrieving 
new documents by adding terms acquired through  
the clarification dialogue.  When the number of 
frames is within the acceptable range, HITIQA will 
generate the answer using the text from the frames 
in the current answer space.  The user may end the 
dialogue at any point and have an answer gener-
ated given the current state of the frames. 
 
9.1   Narrowing Dialogue 
HITIQA attempts to reduce the number of frames 
judged to be relevant through a Narrowing Dia-
logue. This is done when the answer space con-
tains too many elements to form a succinct answer. 
This typically happens when the initial question 
turns out to be too vague or unspecific, with re-
spect to the available data. 
 
9.2   Broadening Dialogue 
As explained before, the system may attempt to 
increase the number of frames judged relevant 
through a Broadening Dialogue (BD), whenever 
the answer space appears too narrow, i.e., contains 
too few zero-conflict frames.  We are conducting 
further experiments to define this situation more 
precisely. Currently, the BD will only occur if 
there are one-conflict frames, or near misses. 
Broadening questions can be asked about any of 
the attributes which have values in the Goal Frame. 
 
10   Answer Generation 
Currently, the answer is simply composed of text 
passages from the zero conflict frames. The text of 
these frames are ordered by date and outputted to 
the user.  Typically the answer to these analytical 
type questions will require many pages of informa-
tion.  Example 1 below shows the first portion of 
the answer generated by HITIQA for the Black Sea 
query. Current work is focusing on answer genera-
tion. 
 
2002:  
The Black Sea is widely recognized as one of the re-
gional seas most damaged by human activity. Almost 
one third of the entire land area of continental Europe 
drains into this sea? major European rivers, the Da-
nube, Dnieper and Don, discharge into this sea while its 
only connection to the world's oceans is the narrow 
Bosphorus Strait. The Bosphorus is as little as 70 me-
ters deep and 700 meters wide but the depth of the 
Black Sea itself exceeds two kilometers in places. Con-
taminants and nutrients enter the Black Sea via river 
run-off mainly and by direct discharge from land-based 
sources. The management of the Black Sea itself is the 
shared responsibility of the six coastal countries: Bul-
garia, Georgia, Romania, Russian Federation, Turkey, 
and Ukraine? 
Example 1: Partial answer generated by HITIQA to the 
Black Sea query. 
 
11   Evaluations 
We have just completed the first round of a pilot 
evaluation for testing the interactive dialogue com-
ponent of HITIQA. The purpose of this first stage 
of evaluation is to determine what kind of dialogue 
is acceptable/tolerable to the user and whether an 
efficient navigation though the answer space is 
possible.  HITIQA was blindly tested by two dif-
ferent analysts on eleven different topics.  Five 
different groups participated, but no analyst tested 
more than one system, as system comparison was 
not a goal.  The analysts were given complete free-
dom in forming their queries and responses to 
HITIQA?s questions.  They were only provided 
with descriptions of the eleven topics the systems 
would be tested on.  The analysts were given 15 
minutes for each topic to arrive at what they be-
lieved to be an acceptable answer. During testing a 
Wizard (human) was allowed to intervene if 
HITIQA generated a dialogue question/response 
that was felt inappropriate. The Wizard was able to 
override the system and send a Wizard generated 
question/response to the analyst.  The HITIQA 
Wizard intervened an average of 13% of the time. 
     These results are for information purposes only 
as it was not a formal evaluation.  HITIQA earned 
an average score of 5.8 from both Analysts for dia-
logue, where 1 was ?extremely dissatisfied? and 7 
was ?completely satisfied?.  The highest score pos-
sible was a 7 for each dialogue.  The Analysts were 
asked to grade each scenario for success or failure.  
We divide the failures from both analysts into three 
categories: 
1) the user gives up on the system for the 
given scenario(9%) 
2) the 15 minute time limit was up(13%) 
3) the data was not in the database(9%) 
HITIQA had a 63% success rate for Analyst 1 and 
a 73% success rate for Analyst 2. It is unclear how 
 these results should be interpreted, if at all, as the 
evaluation was a mere pilot, mostly to test the me-
chanics of the setup. We know only that a human 
Wizard equipped with all necessary information 
can easily achieve 100% success in this test. What 
is still needed is a baseline performance, perhaps 
based on using an ordinary keyword-based search 
engine.  
12   Future Work 
This paper describes a work in progress. We ex-
pect that the initial specification of content frame 
will evolve as we subject the initial system to more 
demanding evaluations. Currently, the frames are 
not topically specialized, and this appears the most 
logical next refinement, i.e., develop several (10-
30) types of frames covering different classes of 
events, from politics to medicine to science to in-
ternational economics, etc. This is expected to in-
crease the accuracy of the dialogue as is the inter-
active visualization which is also under develop-
ment. Answer generation will involve fusion of 
information on the frame level, and is currently in 
an initial phase of implementation. 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Ad-
vanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 2002-
H790400-000. 
References  
J. Allen.  and Core. 1997. Draft of DAMSL:  Dialog Act 
Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/ resources/damsl/    
Bagga, A., T. Strzalkowski, and G.B. Wise. 2000. PartsID: A 
Dialog-Based System for Identifying Parts for Medical 
Systems. Proc. of the ANLP-NAACL-2.  
Chris Buckley. May 1985. Implementation of the Smart in-
formation retrieval system. Technical Report TR85-686, 
Department of Computer Science, Cornell University, 
Ithaca, NY.  
James P. Callan, W. Bruce Croft, Stephen M. Harding 1992. 
The INQUERY Retrieval System.  Proc. of DEXA-92, 3rd 
International Conference on Database and Expert Systems 
Applications. 78-83.  
Cunningham, H., D. Maynard, K. Bontcheva, V. Tablan and 
Y. Wilks. 2000 Experience of using GATE for NLP R&D. 
In Coling 2000 Workshop on Using Toolsets and Architec-
tures To Build NLP Systems.  
DARPA Communicator Program. 2002. 
http://www.darpa.mil/iao/communicator   
 Grinstein, G.G., Levkowitz, H., Pickett, R.M., Smith, S. 1993. 
?Visualization alternatives: non-pixel based images,? 
Proc. of IS&T 46th Annual Conf. 132-133.  
George Ferguson and James Allen. 1998. "TRIPS: An Intelli-
gent Integrated Problem-Solving Assistant," in Proc. of 
the Fifteenth National Conference on Artificial Intelli-
gence (AAAI-98), Madison, WI. 567-573.  
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, B. Wise and 
X. Zhang 2002a. Cross-Document Summarization by 
Concept Classification. Proceedings of SIGIR-2002, Tam-
pere, Finland.  
H. Hardy, K. Baker, L. Devillers, L. Lamel, S. Rosset, T. 
Strzalkowski, C. Ursu and N. Webb.  2002b.  Multi-layer 
Dialogue Annotation for Automated Multilingual Cus-
tomer Service. ISLE Workshop, Edinburgh, Scotland.  
Harabagiu, S., M. Pasca and S. Maiorano. 2000. Experiments 
with Open-Domain Textual Question Answering. In Proc. 
of COLING-2000. 292-298.  
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, 
H. Cunningham, Y. Wilks. 1998. Description of the 
LaSIE-II System as Used for MUC-7. In Proceedings of 
the Seventh Message Understanding Conference (MUC-
7.)  
Judith Hochberg, Nanda Kambhatla and Salim Roukos. 2002. 
A Flexible Framework for Developing Mixed-Initiative 
Dialog Systems. Proc. of 3rd SIGDIAL Workshop on Dis-
course and Dialogue, Philadelphia.  
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, C-Y. Lin. 2000. 
Question Answering in Webclopedia. Notebook Proceed-
ings of Text Retrieval Conference (TREC-9).  
Johnston, M., Ehlen, P., Bangalore, S., Walker., M., Stent, A., 
Maloor, P., and Whittaker, S. 2002. MATCH: An Archi-
tecture for Multimodal Dialogue Systems. In Meeting of 
the Association for Computational Linguistics , 2002.  
Diane J. Litman and Shimei Pan. Designing and Evaluating an 
Adaptive Spoken Dialogue System. 2002. User Modeling 
and User-Adapted Interaction. 12(2/3):111-137.  
Miller, G.A. 1995. WordNet: A Lexical Database. Comm. of 
the ACM, 38(11):39-41. 
John Prager, Dragomir R. Radev, and Krzysztof Czuba. An-
swering what-is questions by virtual annotation. In Human 
Language Technology Conference, Demonstrations Sec-
tion, San Diego, CA, 2001.  
S. Seneff and J. Polifroni, ``Dialogue Management in the 
MERCURY Flight Reservation System,'' Proc. ANLP-
NAACL 2000, Satellite Workshop, 1-6, Seattle, WA, 2000.   
Marilyn A. Walker. An Application of Reinforcement Learn-
ing to Dialogue Strategy Selection in a Spoken Dialogue 
System for Email . Journal of Artificial Intelligence Re-
search.12:387-416.  
W. Ward and B. Pellom.  1999.  The CU Communicator Sys-
tem.  IEEE ASRU. 341-344.  
HITIQA: Scenario Based Question Answering 
Sharon Small, Tomek Strzalkowski, Tracy Janack, Ting Liu,  
Sean Ryan, Robert Salkin, Nobuyuki Shimizu 
The State University of New York at Albany 
1400 Washington Avenue 
Albany, NY 12222 
{small,tomek,tj5550,tl7612,seanryan,rs6021,ns3203}@albany.edu 
 
Paul Kantor, Diane Kelly, Robert Rittman, Nina Wacholder 
Rutgers University 
New Brunswick, New Jersey 08903 
{kantor, nina, diane, rritt}@scils.rutgers.edu 
 
Boris Yamrom 
Lehman College of the City University of New York 
Bronx, New York 10468 
byamrom@lehman.cuny.edy 
 
 
 
Abstract 
In this paper we describe some preliminary 
results of qualitative evaluation of the answer-
ing system HITIQA (High-Quality Interactive 
Question Answering) which has been devel-
oped over the last 2 years as an advanced re-
search tool for information analysts. HITIQA 
is an interactive open-domain question an-
swering technology designed to allow analysts 
to pose complex exploratory questions in natu-
ral language and obtain relevant information 
units to prepare their briefing reports in order 
to satisfy a given scenario. The system uses 
novel data-driven semantics to conduct a clari-
fication dialogue with the user that explores 
the scope and the context of the desired answer 
space. The system has undergone extensive 
hands-on evaluations by a group of intelli-
gence analysts representing various foreign in-
telligence services. This evaluation validated 
the overall approach in HITIQA but also ex-
posed limitations of the current prototype.  
1   Introduction 
Our objective in HITIQA is to allow the user to 
submit exploratory, analytical questions, such as ?What 
has been Russia?s reaction to U.S. bombing of Kos-
ovo?? The distinguishing property of such questions is 
that one cannot generally anticipate what might consti-
tute the answer. While certain types of things may be 
expected (e.g., diplomatic statements), the answer is 
heavily conditioned by what information is in fact avail-
able on the topic, background knowledge of the user, 
context in the scenario, intended audience, etc. From a 
practical viewpoint, analytical questions are often un-
derspecified, thus casting a broad net on a space of pos-
sible answers. Therefore, clarification dialogue is often 
needed to negotiate with the user the exact scope and 
intent of the question, and clarify whether similar topics 
found might also be of interest to the user in order to 
complete their scenario report. This paper will present 
results from a series of evaluations conducted in a series 
of workshops with the intended end users of HITIQA 
(professional intelligence analysts) using the system to 
solve realistic analytic problems. 
HITIQA project is part of the ARDA AQUAINT 
program that aims to make significant advances in the 
state of the art of automated question answering.  In this 
paper we focus on our approach to analytical question 
answering in order to produce a report in response to a 
given scenario.  We also report on the user evaluations 
we conducted and their results with respect to our 
unique approach. 
2   Analytical QA Scenarios 
Analytical scenarios are information task directives 
assigned to analysts to support a larger foreign policy 
process. Scenarios thus contain the information need 
specifications at various levels of detail,  the type, for-
mat and timing of the response required (an intelligence 
report) as well as the primary recipient of the report 
(e.g., the Secretary of State). A hypothetical, but realis-
tic scenario is shown in Figure 1 below. This scenario, 
along with several others like it, was used in evaluating 
 HITIQA performance and fitness for supporting the 
analytical process.  
As can be readily assessed from the directives in 
Figure 1, scenarios are not merely tough questions; they 
are far too complex to be considered as a single question 
at all. It is equally clear that no simple answer can be 
expected and that preparing a report would mean find-
ing answers to a series of interlocking questions or vari-
ous granularities.  
 
Scenario: The al-Qaida Terrorist Group 
 
As an employee of the Central Intelligence Agency, your pro-
fession entails knowledge of the al-Qaida terrorist group.  
Your division chief has ordered a detailed report on the al-
Qaida Terrorist Group due in three weeks. Provide as much 
information as possible on this militant organization. Eventu-
ally, this report should present information regarding the most 
essential concerns, including who are the key figures involved 
with al-Qaida along with other organizations, countries, and 
members that are affiliated, any trades that al-Qaida has made 
with organizations or countries, what facilities they possess, 
where they receive their financial support, what capabilities 
they have (CBW program, other weapons, etc.) and how have 
they acquired them, what is their possible future activity, how 
their training program operates, who their new members are. 
Also, include any other relevant information to your report as 
you see fit.  
 FIGURE 1: Scenario used during user evaluations 
  
We have organized a series of usability evaluations 
with active duty intelligence analysts to find out how 
they approach the problem of solving a scenario. The 
prerequisites for this were are follows: 
1. A robust, broadly functional analytical QA sys-
tem capable of sustaining realistic analytic 
tasks. 
2. A realistic corpus of ?raw intelligence? in form 
of varying quality and verity new-like reports. 
3. A set of realistic, average complexity analytic 
tasks or scenarios to be used. 
HITIQA has been developed over the past two years as 
an open-ended highly flexible interactive QA system to 
allow just this type of evaluation. The system supports a 
variety of information gathering functions without 
straight jacketing the user into any particular mode or 
interaction style. The system does not produce cut and 
dry ?answers?; instead it allows the analysts to build the 
answers the way they want them. While this open-
endedness may seem like unfinished business, we be-
lieve that further development must take into account 
the needs of analysts if they were ever to adopt this 
technology in their work. 
Our main hypothesis is that analysts employ a range 
of strategies to find the required information and that 
these strategies depend significantly upon the nature of 
the task and the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. Our experience with interactive systems also indi-
cated that real users are unlikely to follow any single 
information exploration strategy, but instead would use 
multiple, parallel, even overlapping approaches in order 
to maximize the returns and their confidence in the re-
sults. As a corollary we may expect that the scenario 
tasks are unlikely to be systematically decomposed into 
a series of smaller tasks ahead of actual search. In other 
words, the analytical process is a dialogue, not a se-
quence of commands. Moreover, questions actually 
submitted to the system during the analytical process 
seldom seek just the exact answer, instead they are often 
considered as ?light beams? through the data: focusing 
on the answer but also illuminating adjacent, related 
information which may prove just as valuable.  
AFRL, NIST, CNS and ARDA collaborated in the 
development of scenarios used in our evaluation ses-
sions.  
3   Data Driven Semantics of Questions 
When the user poses a question to a system having 
access to a huge database of unstructured data (text 
files), we need to first reduce the big pile to perhaps a 
handful of documents where the answer is likely to be 
found. The easiest way to do it is to convert the question 
into a search query (by removing stopwords and stem-
ming and tokenizing other words) and submitting this 
query to a fast but non-exact document retrieval system, 
e.g.,   Smart (Buckley, 1985) or InQuery (Callan et al, 
1992), or if you are on the web, Google, etc.   
In the current prototype of HITIQA, we use a com-
bination of Google and InQuery to retrieve the top 50 to 
200 documents from a large document database, con-
sisting of several smaller collections such as newspaper 
stories, documents from the Center of Nonproliferation 
Studies, as well as web mined files.  The retrieved 
documents are then broken down into passages, mostly 
exploiting the naturally occurring paragraph structure of 
the original sources. 
The set of text passages returned from the initial 
search is the first (very crude) approximation of the An-
swer Space for the user?s first question. In order to de-
termine what this answer space consists of we perform 
automatic analysis (a combination of hierarchical clus-
tering and classification) to uncover if what we got is a 
fairly homogenous collection (i.e., all texts have very 
similar content), or whether there are a number of di-
verse topics or aspects represented in there, somehow 
tied together by a common thread. In the former case, 
we may be reasonably confident that we have the an-
swer, modulo the retrievable information. In the latter 
case, we know that the question is more complex than 
the user may have intended, and a negotiation process is 
needed to clarify topics of interest for the scenario re-
port. 
 The next step is to measure how well each of the as-
pects within the answer space is ?matching up? against 
the original question. This is accomplished through the 
framing process described later in this paper. The out-
come of the framing process is twofold: first, the alter-
native interpretations of the question are ranked within 3 
broad categories: on-target, near-misses and outliers. 
Second, salient concepts and attributes for each topi-
cal/aspectual group are extracted into topic frames. This 
enables the system to conduct a meaningful dialogue 
with the user, a dialogue which is wholly content ori-
ented, and entirely data driven.  
4   Partial structuring of text data 
In HITIQA we use a text framing technique to de-
lineate the gap between the meaning of the user?s ques-
tion and the system ?understanding? of this question. 
The framing is an attempt to impose a partial structure 
on the text that would allow the system to systemati-
cally compare different text pieces against each other 
and against the question, and also to communicate with 
the user about this. In particular, the framing process 
may uncover topics or aspects within the answer space 
which the user has not explicitly asked for, and thus 
may be unaware of their existence.  This approach is 
particularly beneficial to the needs of the scenario prob-
lem, where these similar aspects frequently are needed 
in completely ?answering? the scenario, with the sce-
nario report.   
In the current version of HITIQA, frames are pre-
defined structures representing various event types. We 
started with the General frame, which can represent any 
event or relation involving any number of entities such 
as people, locations, organizations, time, and so forth.  
In a specialized domain, or if the user interests are 
known to be limited to a particular set of topics, we de-
fine domain-specific frames. Current HITIQA prototype 
has three broad domain-specific frames, related to the 
Weapon of Mass Destruction proliferation domain 
(which was one of the domains of interest to our users). 
These frames are: WMDTransfer, WMDDevelop, 
WMDTreaty, and of course we keep the General frame.  
Obviously, these three frames do not cover the domain 
represented by our data set; they merely capture the 
most commonly occurring types of events. All frames 
contain a small number of core attributes, such as LO-
CATION, PERSON, COUNTRY, ORGANIZATION, ETC., which 
are extracted using BBN?s Identifinder software, which 
extracts 24 types of entities.  Domain-specific frames 
add event specific attributes, which may require extract-
ing additional items from text, or assigning roles to ex-
isting attributes, or both.  For example, WMDTransfer?s 
attributes TRANSFER_TO and TRANSFER_FROM define 
roles of some COUNTRY or ORGANIZATION, while the 
TRANSFER_TYPE attribute scans the text for keywords 
that may indicate the type of transfer, e.g., export, sale, 
etc.  
HITIQA creates a Goal frame for the user?s ques-
tion, which can be subsequently compared to the data 
frames obtained from retrieved data. A Goal frame can 
be a General frame or any of the domain specific frames 
available in HITIQA.  For example, the Goal frame 
generated from the question, ?Where does al-Qaida 
have training facilities?? is a General frame as shown in 
Figure 2.  This was the first question generated by one 
of our analysts during the first evaluation while working 
on the al-Qaida scenario shown in Figure 1. 
 
FRAME TYPE: General 
TOPIC: training facilities 
ORGANIZATION: al-Qaida 
FIGURE 2: HITIQA generated General-type Goal frame from 
the al-Qaida training facilities question 
 
FRAME TYPE: General 
CONFLICT SCORE: 1 
TRANSFER TYPE: provided 
TRANSFER TO: al-Qaida 
TRANSFER FROM: Iraq 
TOPIC: provided 
SUB-TOPIC: imported 
LOCATION: Iraq 
PEOPLE: Abu Musab al-Zarqawi, Bush, George 
Tenet, Saddam Hussein 
ORGANIZATION:CIA, Administration, al-Qaida 
DOCUMENT: web_283330  
PARAGRAPHS:  ["CIA chief George Tenet seems to 
have gone a long way to back the Bush Administrations dec-
larations that the long split between Islamic fundamentalist 
terrorist organizations like Al-Qaida and secular Iraqi ruler 
Saddam Hussein is healed.   
He has testified that the CIA has evidence of Iraqi provid-
ing Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members 
of a terrorist network led by Abu Musab al-Zarqawi, a close 
Al Qaida associate. "]  
RELEVANCE:  Conflict: [Topic] 
FIGURE 3: A HITIQA generated data frame and the un-
derlying text passage. Words in bold were used to fill the 
Frame.   
 
HTIQA automatically judges a particular data frame 
as relevant, and subsequently the corresponding seg-
ment of text as relevant, by comparison to the Goal 
frame. The data frames are scored based on the number 
of conflicts found between them and the Goal frame. 
The conflicts are mismatches on values of correspond-
ing attributes. If a data frame is found to have no con-
flicts, it is given the highest relevance rank, and a con-
flict score of zero.  All other data frames are scored with 
 a decreasing value based on the number of conflicts, 
negative one for frames with one conflict with the Goal 
frame, negative two for two conflicts etc.  Frames that 
conflict with all information found in the question are 
given a score of -99 indicating the lowest relevancy 
rank.  Currently, frames with a conflict score of -99 are 
excluded from further processing as outliers. The frame 
in Figure 2 is scored as a near miss and will generate 
dialogue, where the user will decide whether or not it 
should be included in the answer space. 
5   Clarification Dialogue 
Data frames with a conflict score of zero form the 
initial kernel answer space. Depending upon the pres-
ence of other frames outside of this set, the system ei-
ther proceeds to generate the answer or initiates a dia-
logue with the user.  HITIQA begins asking the user 
questions on these near-miss frame groups, with the 
largest group first.  The groups must be at least groups 
of size N, where N is a user controlled setting.  This 
setting restricts all of HITIQA?s generated dialogue.   
A one conflict frame has only a single attribute 
mismatch with the Goal frame. This could be a mis-
match on any of the General attributes, for example, 
LOCATION, or ORGANIZATION, or TIME, etc., or in one of 
the domain specific attributes, TRANSFER_TO, or TRANS-
FER_TYPE, etc.  A special case arises when the conflict 
occurs on the TOPIC attribute.  Since all other attributes 
match, we may be looking at potentially different events 
or situations involving the same entities, or occurring at 
the same location or time. The purpose of the clarifica-
tion dialogue in this case is to probe which of these top-
ics may be of interest to the user.  Another special case 
arises when the Goal frame is of a different type than a 
data frame.  The purpose of the clarification dialogue in 
this case is to expand the user?s answer space into a 
different but possibly related event.  A combination of 
both of these cases is illustrated in the exchange in Fig-
ure 4 below.   
User: ?Where does al-Qaida have training facili-
ties?? 
HITIQA: ?Do you want to see material on the trans-
fer of weapons and intelligence to al-Qaida?? 
FIGURE 4: Dialogue generated by HITIQA for the al-Qaida 
training facilities question 
 
In order to understand what happened here, we need 
to note first that the Goal frame for this example is a 
General Frame, from Figure 2.  One of the data frames 
that caused this dialogue to be generated is shown in 
Figure 3 above.  While this frame is of a different frame 
type than the Goal frame, namely WMD Transfer, it 
matches on all of the General attributes except TOPIC, so 
HITIQA asks the user if they would like to expand their 
answer space to this other domain, namely to include 
the transfer of weapons involving this organization as 
well.   
 
ANSWER REPORT:  
 
The New York Times said the Mindanao had become the 
training center for the Jemaah Islamiah network, believed by 
many Western governments to be affiliated to the al-Qaida 
movement of Osama bin Laden 
DocName: A-web_283305 ParaId: 2  
 
? 
IRAQ REPORTED TO HAVE PROVIDED MATERIALS 
TO AL QAIDA  
2003  
[CIA chief George Tenet seems to have gone a long way to 
back the Bush Administrations declarations that the long split 
between Islamic fundamentalist terrorist organizations like Al 
Qiada and secular Iraqi ruler Saddam Hussein is healed. 
DocName: A-web_283330 ParaId: 6  
He has testified that the CIA has evidence of Iraqi providing 
Al Qaida with training in forgery and bomb making and of 
providing two, Al Qaida associates with training in gas and 
poisons. He said also that Iraq is harboring senior members of 
a terrorist network led by Abu Musab al-Zarqawi, a close Al 
Qaida associate. The Bush Administration and the press has 
carelessly shorthanded this to mean, a senior Al Qaida mem-
ber, ignoring the real ambiguities that surround the true nature 
of that association, and whether Zarqawi shares Al Qaidas 
ends, or is receiving anything more than lodging inside Iraq. ] 
DocName: A-web_283330 ParaId: 7  
FIGURE 5: Partial answer generated by HITIQA to the al-
Qaida training facilities question 
 
During the dialogue, as new information is obtained 
from the user, the Goal frame is updated and the scores 
of all the data frames are reevaluated. The system may 
interpret the new information as a positive or negative. 
Positives are added to the Goal frame. Negatives are 
stored in a Negative-Goal frame and will also be used in 
the re-scoring of the data frames, possibly causing con-
flict scores to increase. If the user responds the equiva-
lent of ?yes? to the system clarification question in Fig-
ure 4, a corresponding WMD Transfer frame would be 
added to the Goal frame and all WMD Transfer frames 
will be re-scored.  If the user responds ?no?, the Nega-
tive-Goal frame will be generated and all WMD Trans-
fer frames will be rescored to 99 in order to remove 
them from further processing.  The user may end the 
dialogue, at any point and have an answer generated 
given the current state of the frames.   
Currently, the answer is simply composed of text 
passages from the zero conflict frames. In addition, 
HITIQA will generate a ?headline? for the text passages 
in all the Frames in the answer space.  This is done us-
ing grammar rules and the attributes of a frame.  Figure 
 5 shows a portion of the answer generated by HITIQA 
for the al-Qaida training facilities question. 
 
6   HITIQA Interface 
There are two distinct ways for the user to interact 
with HITIQA to explore their answer space.  The An-
swer Panel displays the user?s current answer at any 
given time during the interaction for a single question.  
Through this panel the user can read the paragraphs that 
are currently in their answer.  There are links on this 
panel so the user is able to view the full original source 
document from which the passage(s) were extracted. 
 The Visual panel offers the user an alternative to 
reading text by providing a tool for visually browsing 
the entire answer space.  Figure 6 shows a typical view 
of the visualization panel. The spheres are representa-
tive of single frames and groups of frames.  The user?s 
attention may be drawn to particular frames by the color 
coding or the attribute spikes.  The colors represent the 
frame?s score, so the user can quickly see what is in 
their answer, blue, and what is not, all other colors.  The 
attribute spikes may also be used as a navigation tool.  
The active attribute is chosen by the user through radio 
buttons. The current active attribute in Figure 6, is Lo-
cation.  This displays all instances of locations men-
tioned in the corresponding text. 
 
 
        Figure 6: Frame Level Display 
 
The underlying text that was used to build the frame 
may be displayed in the lower right hand window.  In 
this text display window there is a hyperlink that takes 
the user directly to the full source document. The user is 
able to interact with this panel by adding and removing 
information from their generated answer. Moving from 
the visualization to the textual dialogue, the generated 
answer, and back is seamless in a sense that any 
changes to the frame scores in one modality are imme-
diately accessible to the user in another modality. Users 
can add and remove frames from the answer space and 
HITIQA will always seamlessly pickup a new dialogue 
or generate a new answer.  
 
7   HITIQA Qualitative Evaluations 
In order to assess our progress thus far, and to also 
develop metrics to guide future evaluation, we invited a 
group of analysts employed by the US government to 
participate in two three-day workshops held in Septem-
ber and October 2003.  
The two basic objectives of the workshops were: 
1. To perform a realistic assessment of the useful-
ness and usability of HITIQA as an end-to-end system, 
from the information seeker's initial questions to com-
pletion of a draft report.  
2. To develop metrics to compare the answers ob-
tained by different analysts and evaluate the quality of 
the support that HITIQA provides.     
Each of these objectives entails a particular chal-
lenge. Performing a realistic assessment of HITIQA is 
difficult because many of the resources that the analysts 
use, as well as the reports they produce, are classified 
and therefore inaccessible to researchers.  
Assessing the quality of the support that the system 
provides is not easy because analytical questions rarely 
have a single right answer. It is not obvious how to de-
fine, for example, the precision of the system. We there-
fore conducted an 'information unit' exercise, whose 
purpose was to determine whether the analysts could 
identify information building blocks in their reports, so 
that we could compare and contrast different reports.  
To obtain an adequate supply of appropriate text 
data to support extensive question answering sessions 
(1, 2, 3 and 4 hours long), we prepared a new corpus of 
approximately 1.2 Gbytes. This new corpus consists of 
the reports from the Center for Non-Proliferation Stud-
ies (CNS) collected for the AQUAINT Program, aug-
mented with a much larger collection of texts on similar 
subject matter mined from the web using Google1. The 
final corpus proved to be sufficient to support about 
three hours of use of HITIQA to ?solve? each of the 
scenarios. 
The first day of the first workshop was devoted to 
training, including a two-part proficiency test. HITIQA 
is a fairly complex system, that includes multiple layers 
of data processing and user interaction, and it was criti-
cal that the users are sufficiently ?fluent? if we were to 
measure their productivity. The analysts' primary task 
on the second day was preparation of reports in re-
sponse to the scenarios. 
                                                 
1 Google has kindly agreed to temporarily extend our 
usage license so we could collect the data over a short 
time. 
  The third day was devoted to quantitative and quali-
tative evaluation, discussed later. In addition, we asked 
the analysts to score each others reports, as well as to 
identify key information units in them. These informa-
tion units could be later compared across different re-
ports in order to determine their completeness.  
8   Workshop Results 
The results of the quantitative evaluations strongly vali-
date the approach that we have taken. These conclusions 
are confirmed by analysts comments gleaned both from 
the formal qualitative assessment and from informal 
discussion. As one analyst said, ?the system as it stands 
now, in my mind, gave me enough information to try to 
put together a 80% solution but ?I don't think you're 
ever gonna reach that 100% state.? At the same time, we 
learned a great deal about how analysts work. 
It is important to determine the realism of the sce-
narios used during the workshop relative to the analysts? 
current work tasks in order for any results to be mean-
ingful. Each analyst was asked a series of five questions 
such as, ?How realistic was the scenario?  In other 
words did it resemble tasks you could imagine perform-
ing at work?? These 5 questions were all relative to the 
realism and difficulty of the scenario tasks.  Analysts 
used a scale of 1 to 5 based on their agreement with the 
statements, where 5 was complete agreement.  Our 
mean score was 3.84, indicating our scenarios were real-
istic and of about average difficulty when compared to 
the work they normally perform.   
We have classified the type of passages that an ana-
lyst copied to their report into two categories, answer 
passages and additional information passages, see Fig-
ure 7 below.  The answer passages either exactly an-
swered the user?s initial question or supplied supporting 
information.  The additional passages do not answer the 
original question posed, but may have been added to the 
answer through dialogue, or through the user?s explora-
tion of document links offered.  This could be a piece of 
information needed to satisfy some other aspect of the 
scenario that they had not asked about yet, or possibly a 
topic the user had not even considered but found rele-
vant when it was presented to them. As can be seen 
there was a very large amount of ?additional? informa-
tion that the user copied to their report.  The amounts 
reported here are the averages for all of the analysts for 
both workshops.  This supports our hypothesis that ana-
lysts seldom seek just the exact answer, but they are 
also looking at adjacent, related information, much of 
which they retain for their report.  Note that there were a 
small number of passages that contained a combination 
of answer and additional information; these were added 
to answer.   
 
Average Number of Passages Copied to Report
2.83
13.63
1.54
5.06
0.00
2.00
4.00
6.00
8.00
10.00
12.00
14.00
16.00
answ er additional
Passage Type
N
um
be
r o
f P
as
sa
ge
s
copied f rom link
copied f rom answ er
 Figure 7: Average Number of Passages Copied 
 
Total Passages Copied and Viewed: Analyst 2
37
8 16
28 27
230
50
242
352
152
16 11 4 7 4
49
27 26 34
44
0
50
100
150
200
250
300
350
400
1 2 3 4 5
Scenario
N
um
be
r o
f P
as
sa
ge
s
passages copied from links
passages viewed from links
passages copied from answer
passages viewed on answer
 
 
Figure 8: Number of Passages Copied Vs. Those Viewed 
 
We should now establish the number of passages 
copied versus those viewed, relative to links and the 
answer.  Figure 8 above shows the total number of pas-
sages copied versus the total number of passages 
viewed.  It is seen that many more passages need to be 
viewed through full document links before a useful pas-
sage is found.  In comparison a much smaller number of 
answer passages need to be viewed from the Answer 
panel in order to find useful passages.   
All of the analysts? sessions were recorded using 
Camtasia.  Figure 9 shows an annotation created for a 
typical session.  Analysts were observed to utilize a 
range of varying strategies as they worked different 
scenarios and even while working different queries of 
the same scenario.  Figure 10 shows the statistics for 
each Analyst?s use of HITIQA while working on the 
scenarios during the two workshops (note that Analyst-4 
was only able to attend the first workshop and Analyst-1 
did not create a report for Scenario 2).  Some of the 
variations in strategies among the analysts while work-
ing the same scenario are quite striking.  For example, 
Scenario 4 was  worked quite  differently  by  Analyst-1  
 versus Analyst-2.  While Analyst-1 spent almost all of 
his/her  time in  the Visual Panel, Analyst-2 spent virtu-
ally all of his/her time in the Answer panel.  Analyst-1 
produced his/her report copying 52 paragraphs while 
Analyst 2 copied only 35.  There are also large varia-
tions in the number of questions asked for the same sce-
nario.  Examine scenario 5, where Analyst-3 asked a 
total of 11 questions and Analyst-2 only needed to ask 2 
questions.  Relative to this, Analyst-3, who asked a 
much larger number of questions, copied only 28 pas-
sages, whereas Analyst-2 copied 31.  These variations, 
as stated earlier in the paper, could be due to the nature 
of the task, the progress the analyst is making on the 
task, in addition to individual differences between ana-
lysts. For example, the difference in the number of 
questions asked between Analyst-2 and Analyst-3 for 
scenario 5 may be due to difference in search strategies 
employed, but may also reflect the amount of back-
ground knowledge of the topic.   
 
      
        FIGURE 9: Fragment of an analytical session 
 
Variation of Strategies: Analyst 1
8
0
5 5 5
18
0
20
52
4248
0
41.5
86
60.5
12
0 1
6.67
26
1
10
100
1 2 3 4 5
Scenario
Variation of Strategies: Analyst 2
5
4
3 3
2
53
19 20
35 3135 29
47
5
17
61
20
33
100
72
1
10
100
1 2 3 4 5
Scenario
 
101 115
Variation of Strategies: Analyst 3
12
3
4
6
11
58
17
25 24
28
21
7
19
70
54
65
1
26
1
10
100
1 2 3 4 5
Scenario
 
Variation of Strategies: Analyst 4
2
3
0 0 0
35
0 0 0
37
0 0 0 0
36 40
0 0 0
34
1
10
100
1 2 3 4 5
Scenario
# questions asked
# passages copied
time in visual
time in answer
 
Figure 10: Varying Strategies Employed 
User: What is the status of South Africa's chemical, 
biological, and nuclear programs?  
          Clarification Dialogue: 1 minute 
? 6 questions generated by HITIQA 
? replied ?Yes? to 5 and ?No? to 1 
? 5+ passages added to answer 
           Studying Answer Panel: 60 minutes  
? Copying 24 passages to report 
? 10 from Answer 
? 14 from Links to Full Document 
? Visual Panel Browsing: 5 minutes 
? Nothing copied 
User: Has South Africa provided CBW material or 
assistance to any other countries?  
          Clarification Dialogue: 1 minute 
? 5 questions generated by HITIQA 
? replied ?Yes? to 2 and ?No? to 3 
? 2+ passages added to answer 
           Studying Answer Panel: 26 minutes 
? Copying 6 passages to report 
? 6 from Links to Full Document 
            Visual Panel browsing: 1 minute 
? Copying 1 passage to report 
? 1 from Links to Full Document 
User: How was South Africa's CBW program fi-
nanced?  
         Clarification Dialogue: 40 seconds 
? 7 questions generated by HITIQA 
? replied ?Yes? to 3 and ?No? to 4 
? 3+ passages added to answer 
            Studying Answer Panel: 11 minutes 
? Copying 3 passages to report 
? 1 from Answer 
      2 from Links to full Document 
  
There is, however, some consistency across the ana-
lysts in the amount of information retained per scenario. 
The charts are drawn in logarithmic scale, but it should 
be visible that scenarios 2 and 3 produced less interac-
tion and required less information to fulfill than scenar-
ios 4 and 5. It is also visible that scenario 1 required 
more questions to be asked and more exploration to be 
done in visual panel than other scenarios. 
Finally, it is important to provide some metric re-
garding the user?s overall satisfaction with their use of 
HITIQA.  At the end of each workshop Analysts were 
given a series of 17 questions, such as ?HITIQA helps 
me find important information?, shown in Figure 11, to 
assess their overall experience with the system.  Many 
of these questions were designed for the user to com-
pare HITIQA to the current tools they are using for this 
type of task.   Analysts again used a scale of 1 to 5 
based on their agreement with the statements.  The re-
sults were then converted, where 5 would always denote 
the best, and are shown in Figure 11 below.  It is impor-
tant to note that we scored highly overall, but addition-
ally we scored highly in the majority of questions rela-
tive to comparison of their current tools.  For example, 
for Question 14: ?Having HITIQA at work would help 
me find information faster than I can currently find it?, 
our mean score was 3.83.  
 
3.7215702092Total
3.00311117
4.141616
2.8614215
3.8314114
3.1632113
4.1424112
4.0024111
4.00710
3.71529
3.29258
3.143317
3.711516
4.43345
4.293314
4.14163
3.7114112
3.71611
ScoreScore5Score4Score3Score2Score1Question
MeanFrequency of Analyst's Scores of Overall Workshop I & II
1                  2                 3                 4    5 score
frequency
 
      FIGURE 11: Final Evaluation Results, Workshop 1 & 2 
 
In summary, the results from these two evaluations 
indicate that HITIQA, in its current state, is already 
competitive with the tools that the analysts are currently 
using in their work, supporting our overall approach to 
Analytical Question Answering.  HTIQA provides the 
user with a tool to find the passages needed to complete 
a report for a given scenario.  While working on a sce-
nario HITIQA has been shown to provide information 
which exactly answers the user?s question, and addi-
tionally HITIQA?s method brings to light other related 
information that the analyst retains in order to complete 
their report. 
 
Acknowledgements 
This paper is based on work supported by the Advanced 
Research and Development Activity (ARDA)?s Advanced 
Question Answering for Intelligence (AQUAINT) Program 
under contract number 2002-H790400-000. 
References  
Allen, J. and M. Core. 1997. Draft of DAMSL:  Dialog Act Markup in 
Several Layers. www.cs.rochester.edu/research/cisd/   
Baeza-Yates and Ribeiro-Neto. 1999. Modern Information Retrieval. 
Addison Wesley. 
Chris Buckley. 1985. Implementation of the Smart information re-
trieval system. Technical Report TR85-686, Department of Com-
puter Science, Cornell University, Ithaca, NY. 
Ferguson, George and James Allen. 1998. TRIPS: An Intelligent Inte-
grated Problem-Solving Assistant, in Proceedings of the 15th 
AAAI Conference (AAAI-98), Madison, WI, pp. 567-573. 
Hardy, H., N. Shimizu, T. Strzalkowski, L. Ting, B. Wise and X. 
Zhang. 2002a. Cross-Document Summarization by Concept Clas-
sification. Proceedings of SIGIR, Tampere, Finland. 
Hardy, H., K. Baker, L. Devillers, L. Lamel, S. Rosset, T. 
Strzalkowski, C. Ursu and N. Webb. 2002b.  Multi-layer Dialogue 
Annotation for Automated Multilingual Customer Service. ISLE 
Workshop, Edinburgh, Scotland. 
Harabagiu, S., et. al. 2002. Answering Complex, List and Context 
questions with LCC?s Question Answering Server.   In Proceedings 
of Text Retrieval Conference (TREC-10). 
Hovy, E., L. Gerber, U. Hermjakob, M. Junk, C-Y. Lin. 2000. Ques-
tion Answering in Webclopedia. Notebook. Proceedings of Text 
Retrieval Conference (TREC-9). 
Humphreys, R. Gaizauskas, S. Azzam, C. Huyck, B. Mitchell, H. 
Cunningham, Y. Wilks. 1998. Description of the LaSIE-II System 
as Used for MUC-7. In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7.) 
Litman, Diane J. and Shimei Pan. 2002. Designing and Evaluating an 
Adaptive Spoken Dialogue System. User Modeling and User-
Adapted Interaction. Vol. 12, No. 2/3, pp. 111-137. 
Seneff, S. and J. Polifroni. 2000. Dialogue Management in the MER-
CURY Flight Reservation System. Proc. ANLP-NAACL 2000, 
Satellite Workshop, pp. 1-6, Seattle, WA. 
Small, Sharon, Nobuyuki Shimizu, Tomek Strzalkowski and Liu Ting 
(2003). HITIQA: A Data Driven Approach to Interactive Question 
Answering: A Preliminary Report. AAAI Spring Symposium on 
New Directions in Question Answering, Stanford University, 
March 24-26, 2003. pp. 94?104. 
Tang, Rong, K.B. Ng, Tomek Strzalkowski and Paul Kantor (2003). 
Automatic Prediction of Information Quality in News Documents. 
Proceedings of HLT-NAACL 2003, Edmonton, May 27-June 1 
Walker, Marilyn A. 2002. An Application of Reinforcement Learning 
to Dialogue Strategy Selection in a Spoken Dialogue System for 
Email . Journal of AI Research, vol 12., pp. 387-416. 
Dialogue Management for an Automated Multilingual Call Center 
Hilda Hardy, Tomek Strzalkowski and Min Wu 
Institute for Informatics, Logics and Security Studies 
University at Albany, Albany, NY  12222 
and  
The AMITIES Consortium1 
hardyh,tomek,minwu@cs.albany.edu 
 
                                                          
1 
2 
1 The AMITIES consortium members include University of Sheffield, CNRS-LIMSI, Duke University, SUNY Albany, 
VESCYS, and Viel et Cie. 
 
Introduction 
The AMITI?S project (Automated Multilingual Interac-
tion with Information and Services) has been estab-
lished under joint funding from the European 
Commission?s 5th Framework Program and the U.S. 
DARPA to develop the next generation of empirically-
induced human-computer interaction capabilities in 
spoken language. One of the central goals of this project 
is to create a dialogue management system capable of 
engaging the user in human-like conversation within a 
specific domain. The domain we selected is telephone-
based customer service where the system has access to 
an appropriate information database to support callers? 
information needs. Our objective is to automate at least 
some of the more mundane human functions in cus-
tomer service call centers, but do so in a manner that is 
maximally responsive to the customer. This practically 
eliminates all prompt or menu based voice response 
systems used at commercial call centers today. 
Exploiting the corpus of hundreds (and soon to be 
thousands) of annotated dialogues, recorded at Euro-
pean financial call centers, we have developed a call 
triaging prototype for financial services domain. This 
demonstrator system handles the initial portion of a cus-
tomer call: identifying the customer (based on a sample 
customer database) and determining the reason the cus-
tomer is calling (based on a subset of transactions han-
dled at the call center). Our approach to dialogue act 
semantics allows for mixed system/customer initiative 
and spontaneous conversation to occur. We are cur-
rently extending this prototype beyond its triage role to 
negotiate and execute the transactions requested by the 
customers, ranging from simple address changes to 
more complex account payment transactions. 
The aim of AMITIES project is to build a large-
scale, empirical system using data-driven design, de-
rived from actual and purposeful (i.e., not acted or con-
trived) human-to-human dialogues. This proves to be a 
lengthy and complicated process due to a variety of le-
gal constraints we need to overcome to obtain real data 
in sufficient quantities. We have devoted a considerable 
effort to this issue, which only now is beginning to 
bring results. The prototype described here has not been 
empirically validated yet. 
Dialogue with Information and Services 
The key concept underlying AMITIES dialogue man-
ager is the notion of dialogue with data. The prevalent 
type of dialogue in a call center environment is informa-
tion seeking/information access, which displays specific 
characteristics that can be exploited in the design of an 
automated system. In a human-operated call center, an 
operator mediates between the caller and a variety of 
data sources: information about customers, products, 
regulations, etc. Much of this data is in a structured 
form, usually a relational database (accounts informa-
tion), while some may remain in an unstructured form 
(e.g., text memos, flyers, regulations manuals.) The ob-
jective of an automated call center is to obtain a natu-
rally interactive mediation, between the caller and the 
information which is as close to a human-human dia-
logue as possible. 
This automated call center scenario applies to many 
customer service situations, including the following: 
? Financial services (AMITIES primary domain) 
? Product support 
? Travel reservations 
where the objective is to locate, insert or update a single 
(or several) data object in a structured data base. At a 
more abstract level, the call center of the type described 
here can be characterized as an Interaction with Struc-
tured Data (ISD). ISD consists of the following compo-
nents: 
1. Data structure, which defines the set of basic enti-
ties (accounts, spare parts, flights) and their attrib-
utes (account number, part size, destination city, 
etc.) as well as methods for identifying references 
to these attributes in user statements. 
2. List of basic transactions supported by the service 
(account payment, address change, locating a 
flight) along with methods to detect references to 
these transactions. 
3. Dialogue models for handling various conversa-
tional situations in human-like fashion (e.g., re-
sponding to requests, emotions, indecision) and 
consistent with the character of the service (polite, 
helpful, caring). 
4. Optional dialogue meta-strategy as required to ad-
dress privacy and security concerns (e.g., positive 
caller identification must precede exchange of any 
sensitive information.) 
The components 1, 2 and 4 can be built using limited 
amount of static data about the service and are to a large 
degree domain-independent or domain-adaptable. These 
components are sufficient to design basic mixed-
initiative dialogue capabilities, as explained further in 
the following section. Although the dialogue may not 
feel very ?natural? it will be quite efficient, giving the 
user a broad initiative to conduct it as they wish. Dia-
logue models (component #3) are required to create an 
illusion of naturalness and these can only be derived 
from large corpora of actual call center conversations. 
Large corpora of real conversations are also needed to 
develop speech and prosody models. 
We have built a prototype caller triaging dialogue 
management which has been incorporated in the first 
AMITIES demonstrator. The system is based on Galaxy 
Communicator architecture (Seneff et al, 1998) in a 
standard configuration shown in Figure 1. The DM can 
handle dialogues in 3 European languages, and can ad-
ditionally switch from one language to another in mid-
conversation. 
 
Figure 1.  AMITI?S System Architecture 
3 Dialogue Manager/Frame Router 
In this section we explain some key principles of de-
signing an interactive dialogue with Structured Data 
(ISD). The overall strategy is to locate an item or items 
in the database that meet a number of specific condi-
tions, for example, the most convenient flight, the 
caller?s bank account, etc. This overall objective is bro-
ken down into a set of sub-goals some of which may 
need to be satisfied to achieve the objective. The role of 
ISD dialogue is to chart a path through the sub-goals in 
such as way that: 
1. the objective is achieved 
2. any partial constraints on the order or selection of 
the sub-goals are met, and 
3. the most efficient route is chosen. 
The dialogue manager identifies the goal of the con-
versation and performs interactions to achieve that goal. 
The overall mechanism works by filling attribute values 
in frames representing transactions and the sub-goals. 
Spontaneous conversation works in this environment, 
because values may be filled in any order, or several 
values may be supplied in one turn. As attribute values 
in the frames are filled, the need for dialogue decreases. 
The system sets key milestones or goals to be 
reached by gathering sufficient information from the 
customer, but these milestones may be approached by a 
variety of different paths. If the customer?s last name is 
misrecognized, for example, or if there are multiple 
database records returned, the system will ask for a dif-
ferent attribute, such as the address or postal code. Re-
prompts are used when necessary, but no more than 
once for any single attribute. The process continues un-
til a unique (e.g., bank account) or best (e.g., a flight) 
record is identified. Thus the dialogue system has flexi-
bility to deal with user input arriving in any order or 
form and the input that is not completely captured, 
without getting stuck on any single attribute. The paths 
to the key milestones, and even the order of the mile-
stones, may be seen as a series of hidden transitions. 
This means exact progression of the dialogue is never 
pre-set or can be known in advance ? a major advance 
over system-driven prompts.   
In order to keep the dialogue manager language- and 
domain-independent, mechanisms were created to store 
the language-specific and task-specific information in 
separate modules, to be loaded as needed. These are 
illustrated in Figure 2. 
 
 
Figure 2.  Dialogue Manager Structure 
Hub 
Speech  
Recognition 
Dialogue 
Manager 
Database 
Server 
Text-to-speech 
Conversion 
Nat?l Language 
Understanding 
French 
German 
Telephony 
Server 
Response       
Generation 
French 
German 
Customer 
Database
Engl. 
  (English)
  Keyword profile 
  Prompt sequence
  Task2 ? 
 
 
 
 Response Decision 
User ID 
Frame Router
Input:  
from NLU via Hub 
Task Properties File 
User Properties File 
 Task ID 
Frame Router
Task1 
account_number,
fname, lname, 
post_code, ? 
In Fig. 2, the transaction identification module 
(TaskID Frame Router) matches the incoming user ut-
terances to identify which transaction is being invoked. 
If multiple transactions are matched, their representa-
tions (frames) are ranked in the likelihood order. Each 
frame consists of a keyword profile (a list of salient 
terms derived from human-human dialogues) and a 
prompt sequence, which is a list of attributes whose 
values must be collected in order to complete the trans-
action. This prompt sequence is passed to the Dialogue 
Manager which then handles it in a manner consistent 
with the adopted dialogue model. Task identification 
succeeds even with high speech recognition error rates, 
provided the keywords are detected accurately.2 Ties (or 
near ties) are resolved through clarification dialogue by 
determining the value of some contrasting attribute, 
which is dynamically selected for each situation. The 
User ID Frame Router operates analogously, with each 
frame representing a different user (customer).  
Two simple dialogues with AMITIES are below: 
 
AMITI?S:  Amiti?s, how can I help, je vous ecoute, was kann ich 
f?r Sie tun? 
USER: Bonjour, je viens de quitter la r?gion parisienne, je voulais 
vous donner l'adresse de ma nouvelle r?sidence. 
AMITI?S:  Pouvez-vous me donner votre nom et votre ancien 
code postal, s'il vous pla?t? 
USER: madame Cecile Brun, j'habite ? Paris dans le ll?me code 
postal 38200 
AMITI?S:  Merci, Mme Brun.  Alors je vais avoir quelques 
renseignements ? vous demander pour mettre ? jour 
votre adresse. 
 
AMITI?S:  Amiti?s, how can I help, je vous ecoute, was kann ich 
f?r Sie tun? 
USER: Hi, I'm Margaret Roberts, post code CO4 3SQ 
AMITI?S:  How can I help, Mrs. Roberts? 
USER: Can I get my card replaced?  My old one was stolen. 
AMITI?S:  Thank you, Mrs. Roberts.  I'll just ask you some ques-
tions and have a new card sent to you. 
4 
                                                          
Dialogue Annotations 
Working with the real call center dialogues required 
us to develop a new method for dialogue annotation. 
The DAMSL functional annotation scheme had to be 
modified and further augmented with semantic annota-
tion. To do so, we have created dialogue act taxonomy 
appropriate for ISD dialogues. To capture the semantics, 
we used a domain-independent framework populated 
with domain-specific lists. Furthermore, to facilitate 
speedy annotation, we have designed a new flexible, 
annotation tool, XDMLTool, and annotated several 
hundred French and English dialogues using it. 
In order to annotate semantic information with 
XDMLTool, the user makes entries for a particular turn 
or turn segment in a semantic table on the user interface. 
Transactions such as MAKEPYMNT or CHANGEADDR are 
selected and their attributes appear in combo-boxes on 
the GUI. If necessary, the user may type in new labels. 
To fill a value for an attribute, text from the displayed 
dialogue may be copied into a table cell. 
2 While different combinations of keywords may invoke a 
transaction frame, this process is robust because the selection 
of transactions is limited to those known to the system.  
For example, the following exchange, part of a 
VERIFYID transaction, would be labeled with the attrib-
utes Name and PostCode. The values John Smith and 
AB1 1CD would be tagged for the answer. 
A: Your full name and postcode please? 
C: Yes it's err John Smith AB1 1CD 
The new annotation scheme reflects our approach to 
dialogue design ? we hope it will help us to automati-
cally derive appropriate dialogue strategies for novel 
ISD situations, and beyond.3                                                                          
Acknowledgments 
This paper is based on work supported in part by the 
European Commission under the 5th Framework 
IST/HLT Programme, and by the U.S. Defense Ad-
vanced Research Projects Agency. 
References 
J. Allen and M. Core.  1997.  Draft of DAMSL: Dialog 
Act Markup in Several Layers.  http://www.cs. roches-
ter. edu/research/cisd/resources/damsl/. 
J. Allen, et al 1995.  The TRAINS Project:  A Case 
Study in Building a Conversational Planning Agent.  
Journal of Experimental and Theoretical AI, 7, 7?48. 
AMITI?S, http://www.dcs.shef.ac.uk/nlp/amities/. 
A. Bagga, T. Strzalkowski and G. B. Wise.  2000.  Parts 
ID : A Dialogue-Based System for Finding Parts for 
Medical Systems.  In Proc. of ANLP-2000. 
J. Chu-Carroll and B. Carpenter.  1999.  Vector-Based 
Natural Language Call Routing.  Computational 
Linguistics, 25 (3): 361?388. 
DARPA, http://www.darpa.mil/iao/Communicator.htm.  
L. Devillers, S. Rosset, H. Maynard and L. Lamel.  May 
2002.  Annotations for Dynamic Diagnosis of the 
Dialog State.  In Proc. of LREC, Las Palmas. 
R. Gaizauskas et al 1996.  GATE :  An Environment to 
Support Research and Development in Natural 
Language Engineering.  In Proc. Of 8th IEEE Int. 
Conf. on Tools with AI, Toulouse, France. 
A. L. Gorin, G. Riccardi and J. Wright.  1997.  How 
May I Help You?  Speech Comm., 23 (1/2): 113?127. 
S. Seneff, E et al  1998.  Galaxy-II:  A Reference Ar-
chitecture for Conversational System Development.  
In Proc. of ICSLP 98, Sydney, Australia. 
                                                          
3 Some preliminary results of dialogue structure analysis are 
available but we lack space to include them in this note. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1169?1176,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Utilizing Co-Occurrence of Answers in Question Answering   
 
 
 
Abstract 
In this paper, we discuss how to utilize 
the co-occurrence of answers in building 
an automatic question answering system 
that answers a series of questions on a 
specific topic in a batch mode. Experi-
ments show that the answers to the many 
of the questions in the series usually have 
a high degree of co-occurrence in rele-
vant document passages. This feature 
sometimes can?t be easily utilized in an 
automatic QA system which processes 
questions independently. However it can 
be utilized in a QA system that processes 
questions in a batch mode. We have used 
our pervious TREC QA system as base-
line and augmented it with new answer 
clustering and co-occurrence maximiza-
tion components to build the batch QA 
system. The experiment results show that 
the QA system running under the batch 
mode get significant performance im-
provement over our baseline TREC QA 
system.   
1 Introduction 
Question answering of a series of questions on 
one topic has gained more and more research 
interest in the recent years. The current TREC 
QA test set contains factoid and list questions 
grouped into different series, where each series 
has the target of a definition associated with it 
(Overview of the TREC 2004 Question Answer-
ing Track, Voorhees 2005). Usually, the target is 
also called ?topic? by QA researchers. One of the 
restrictions of TREC QA is that ?questions 
within a series must be processed in order, with-
out looking ahead.? That is, systems are allowed 
to use answers to earlier questions to help answer 
later questions in the same series, but can not use 
later questions to help answer earlier questions. 
This requirement models the dialogue discourse 
between the user and the QA system. However 
our experiments on interactive QA system show 
that some impatient QA users will throw a bunch 
of questions to the system and waiting for the 
answers returned in all. This prompted us to con-
sider building a QA system which can accept as 
many questions as possible from users once in all 
and utilizing the relations between these ques-
tions to help find answers. We would also like to 
know the performance difference between the 
QA system processing the question series in an 
order and the QA system processing the question 
series as a whole. We call the second type of QA 
system as batch QA system to avoid the ambigu-
ity in the following description in this paper.  
  What kind of relations between questions 
could be utilized is a key problem in building the 
batch QA system. By observing the test ques-
tions of TREC QA, we found that the questions 
given under the same topic are not independent 
at all. Figure-1 shows a series of three questions 
proposed under the topic ?Russian submarine 
Kursk Sinks? and some relevant passages to this 
topic found in the TREC data set. These passages 
contain answers not to just one but to two or 
three of the questions. This indicates that the an-
swers to these questions have high co-occurrence.  
In an automatic QA system which processes 
the questions independently, the answers to the 
questions may or may not always be extracted 
due to algorithmic limitations or noisy informa-
tion around the correct answer. However in 
building a batch QA system, the inter-
dependence between the answers could be util-
ized to help to filter out the noisy information 
and pinpoint the correct answer for each question 
in the series.  
 
 
Min Wu1 and Tomek Strzalkowski1,2  
1 ILS Institute, University at Albany, State University of New York 
1400 Washington Ave SS261, Albany NY, 12222 
2Institute of Computer Science, Polish Academy of Sciences 
minwu@cs.albany.edu, tomek@csc.albany.edu  
1169
 
We will discuss later in this paper how to util-
ize the co-occurrence of answers to a series of 
questions in building a batch QA system. The 
remainder of this paper is organized as follows. 
In the next section, we review the current tech-
niques used in building an automatic QA system. 
Section 3 introduces the answers co-occurrence 
and how to cluster questions by the co-
occurrence of their answers. Section 4.1 de-
scribes our TREC QA system and section 4.2 
describes how to build a batch QA system by 
augmenting the TREC QA system with question 
clustering and answer co-occurrence maximiza-
tion. Section 4.3 describes the experiments and 
explains the experimental results. Finally we 
conclude with the discussion of future work.   
2 Related Work 
During recent years, many automatic QA sys-
tems have been developed and the techniques 
used in these systems cover logic inference, syn-
tactic relation analysis, information extraction 
and proximity search, some systems also utilize 
pre-compiled knowledge base and external 
online knowledge resource.  
The LCC system (Moldovan & Rus, 2001; 
Harabagiu et al 2004) uses a logic prover to se-
lect answer from related passages. With the aid 
of extended WordNet and knowledge base, the 
text terms are converted to logical forms that can 
be proved to match the question logical forms. 
The IBM?s PIQUANT system (Chu-Carroll et al 
2003; Prager et al 2004) adopts a QA-by-
Dossier-with-Constraints approach, which util-
izes the natural constraints between the answer to 
the main question and the answers to the auxil-
iary questions. Syntactic dependency matching 
has also been applied in many QA systems (Cui 
et al 2005; Katz and Lin 2003). The syntactic 
dependency relations of a candidate sentence are 
matched against the syntactic dependency rela-
tions in the question in order to decide if the can-
didate sentence contains the answer. Although 
surface text pattern matching is a comparatively 
simple method, it is very efficient for simple fac-
toid questions and is used by many QA systems 
(Hovy et al2001; Soubbotin, M. and S. Soub-
botin 2003). As a powerful web search engine 
and external online knowledge resource, Google 
has been widely adopted in QA systems (Hovy et 
al 2001; Cui 2005) as a tool to help passage re-
trieval and answer validation. 
Current QA systems mentioned above and 
represented at TREC have been developed to 
answer one question at the time. This may par-
tially be an artifact of the earlier TREC QA 
evaluations which used large sets of independent 
questions. It may also partially reflect the inten-
tion of the current TREC QA Track that the 
question series introduced in TREC QA 2004 
(Voorhees 2005) simulate an interaction with a 
human, thus expected to arrive one at a time. 
The co-occurrence of answers of a series of 
highly related questions has not yet been fully 
utilized in current automatic QA systems partici-
pating TREC. In this situation, we think it 
worthwhile to find out whether a series of highly 
related questions on a specific topic such as the 
TREC QA test questions can be answered to-
gether in a batch mode by utilizing the co-
occurrences of the answers and how much it will 
help improve the QA system performance.    
3 Answer Co-Occurrence and Question 
Clustering 
Many QA systems utilize the co-occurrence of 
question terms in passage retrieval (Cui 2005). 
Topic Russian submarine Kursk sinks 
 
1. When did the submarine sink?   August 12 
2. How many crewmen were lost in the disaster?   118 
3. In what sea did the submarine sink?    Barents Sea 
 
Some Related Passages 
 
Russian officials have speculated that the Kursk col-
lided with another vessel in the Barents Sea, and usu-
ally blame an unspecified foreign submarine. All 118 
officers and sailors aboard were killed. 
 
The Russian governmental commission on the acci-
dent of the submarine Kursk sinking in the Barents 
Sea on August 12 has rejected 11 original explana-
tions for the disaster. 
 
.... as the same one carried aboard the nuclear subma-
rine Kursk, which sank in the Barents Sea on Aug. 12, 
killing all 118 crewmen aboard. 
 
The navy said Saturday that most of the 118-man 
crew died Aug. 12 when a huge explosion .... 
 
Chief of Staff of the Russian Northern Fleet Mikhail 
Motsak Monday officially confirmed the deaths of 
118 crewmen on board the Kursk nuclear submarine 
that went to the bottom of the Barents Sea on August 
12. 
Figure-1 Questions and Related Passages 
1170
Some QA systems utilize the co-occurrence of 
question terms and answer terms in answer vali-
dation. These methods are based on the assump-
tion that the co-occurrences of question terms 
and answer terms are relatively higher than the 
co-occurrences of other terms. Usually the co-
occurrence are measured by pointwise mutual 
information between terms.  
During the development of our TREC QA sys-
tem, we found the answers of some questions in 
a series have higher co-occurrence. For example, 
in a series of questions on a topic of disaster 
event, the answers to questions such as ?when 
the event occurred?, ?where the event occurred? 
and ?how many were injured in the event? have 
high co-occurrence in relatively short passages. 
Also, in a series of questions on a topic of some 
person, the answers to questions such as ?when 
did he die?, ?where did he die? and ?how did he 
die? have high co-occurrence. To utilize this an-
swers co-occurrence effectively in a batch QA 
system, we need to know which questions are 
expected to have higher answers co-occurrence 
and cluster these questions to maximize the an-
swers co-occurrence among the questions in the 
cluster.  
Currently, the topics used in TREC QA test 
questions fall into four categories: ?Person?, 
?Organization?, ?Event? and ?Things?. The topic 
can be viewed as an object and the series of 
questions can be viewed as asking for the attrib-
utes of the object. In this point of view, to find 
out which questions have higher answers co-
occurrence is to find out which attributes of the 
object (topic) have high co-occurrence. 
We started with three categories of TREC QA 
topics: ?Event?, ?Person? and ?Organization?. 
For ?Event? topic category, we divided it into 
two sub-categories: ?Disaster Event? and ?Sport 
Event?. From the 2004 & 2005 TREC QA test 
questions, we manually collected frequently 
asked questions on each topic category and 
mapped these questions to the corresponding 
attributes of the topic. We focused on frequently 
asked questions because these questions are eas-
ier to be classified and thus served as a good 
starting point for our work. However for this 
technique to scale in the future, we are expecting 
to integrate automatic topic model detection into 
the system. For topic category ?Person?, the at-
tributes and corresponding named entity (NE) 
tags list as follows.   
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
For each topic category, we collected 20 sam-
ple topics as well as the corresponding attributes 
information about these topics. The sample topic 
?Rocky Marciano? and the attributes are listed as 
follows: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
From each attribute of the sample topic, an 
appropriate question can be formulated and rele-
vant passages about this question were retrieved 
from TREC data (AQUAINT Data) and the web. 
A topic-related passages collection was formed 
by the relevant passages of questions on all at-
tributes under the topic. Among the topic-related 
passages, the pointwise mutual information (PMI) 
of attribute values were calculated which conse-
quently formed a symmetric mutual information 
matrix. The PMI of two attribute values x and y 
was calculated by the following equation. 
)()(
),(log),(
ypxp
yxpyxPMI =  
All the mutual information matrixes under the 
topic category were added up and averaged in 
order to get one mutual information matrix 
which reflects the general co-occurrence rela-
Attribute      Attribute Value                        
 
Birth Date   September 1, 1923         
Birth Place  Brockton, MA                                                 
Death Date  August 31, 1969    
Death Place  Iowa  
Death Reason  airplane crash 
Death Age   45 
Buried Place  Fort Lauderdale, FL 
Nationality   American                      
Occupation  heavyweight champion boxer            
Father    Pierino Marchegiano 
Mother    Pasqualena Marchegiano 
Wife   Barbara Cousins 
Children   Mary Ann, Rocco Kevin 
No. of Children  two 
Real Name  Rocco Francis Marchegiano 
Nick Name  none 
Affiliation   none                         
Education   none  
Attribute                                     Attribute?s NE tag 
 
Birth Date   Date 
Birth Place  Location 
Death Date  Date 
Death Place  Location 
Death Reason  Disease, Accident 
Death Age   Number 
Nationality  Nationality 
Occupation  Occupation 
Father   Person 
Mother   Person 
Wife   Person 
Children   Person 
Number of Children  Number 
Real Name  Person, Other 
Nick Name  Person, Other 
Affiliation   Organization 
Education   Organization 
1171
tions between attributes under the topic category. 
We clustered the attributes by their mutual in-
formation value. Our clustering strategy was to 
cluster attributes whose pointwise mutual infor-
mation is greater than a threshold ?. We choose ? 
as equal to 60% of the maximum value in the 
matrix. 
The operations described above were auto-
matically carried out by our carefully designed 
training system. The clusters learned for each 
topic category is listed as follows. 
The reason for the clustering of attributes of 
topic category is for the convenience of building 
a batch QA system. When a batch QA system is 
processing a series of questions under a topic, 
some of the questions in the series are mapped to 
the attributes of the topic and thus grouped to-
gether according to the attribute clusters. Then 
questions in the same group are processed to-
gether to obtain a maximum of answers co-
occurrence. More details are given in section 4.2. 
4 Experiment  Setup and Evaluation 
4.1 Baseline System 
The baseline system is an automatic IE-driven 
(Information Extraction) QA system. We call it 
IE-driven because the main techniques used in 
the baseline system: surface pattern matching 
and N-gram proximity search need to be applied 
to NE-tagged (Named Entity) passages. The sys-
tem architecture is illustrated in Figure-2. The 
components indicated by dash lines are not in-
cluded in the baseline system and they are added 
to the baseline system to build a batch QA sys-
tem. As shown in the figure with light color, the 
two components are question classification and 
co-occurrence maximization. Both our baseline 
system and batch QA system didn?t utilize any 
pre-compiled knowledge base. 
In the question analysis component, questions 
are classified by their syntactic structure and an-
swer target. The answer targets are classified as 
named entity types. The retrieved documents are 
segmented into passages and filtered by topic 
keywords, question keywords and answer target. 
The answer selection methods we used are 
surface text pattern matching and n-gram prox-
imity search. We build a pattern learning system 
to automatically extract answer patterns from the 
TREC data and the web. These answer patterns 
are scored by their frequency, sorted by question 
type and represented as regular expressions with 
terms of ?NP?, ?VP?, ?VPN?, ?ADVP?, ?be?, 
?in?, ?of?, ?on?, ?by?, ?at?, ?which?, ?when?, 
?where?, ?who?, ?,?, ?-?, ?(?. Some sample an-
swer patterns of question type ?when_be_np_vp? 
are listed as follows. 
 
 
 
 
 
 
 
 
 
When applying these answer patterns to ex-
tract answer from candidate passages, the terms 
such as ?NP?, ?VP?, ?VPN?, ?ADVP? and ?be? 
are replaced with the corresponding question 
terms. The replaced patterns can be matched di-
rectly to the candidate passages and answer can-
didate be extracted.  
Some similar proximity search methods have 
been applied in document and passage retrieval 
in the previous research. We applied n-gram 
proximity search to answer questions whose an-
swers can?t be extracted by surface text pattern 
matching. Around every named entity in the fil-
tered candidate passages, question terms as well 
as topic terms are matched as n-grams. A ques-
tion term is tokenized by word. We matched the 
longest possible sequence of tokenized word 
within the 100 word sliding window around the 
named entity. Once a sequence is matched, the 
corresponding word tokens are removed from the 
ADVP1 VP in <Date>([^<>]+?)<\/Date>                             
NP1.{1,15}VP.{1,30} in <Date>([^<>]+?)<\/Date>                          
NP1.{1,30} be VP in <Date>([^<>]+?)<\/Date>                             
NP1, which be VP in <Date>([^<>]+?)<\/Date>                             
VP NP1.{1,15} at .{1,15}<Date>([^<>]+?)<\/Date>                         
ADVP1.{1,80}NP1.{1,80}<Date>([^<>]+?)<\/Date>                           
NP1, VP in <Date>([^<>]+?)<\/Date>                              
NP1 of <Date>([^<>]+?)<\/Date>                           
NP1 be VP in <Date>([^<>]+?)<\/Date>           
?Person? Topic  
 
Cluster1: Birth Date; Birth Place 
Cluster2a: Death Date; Death Place;  
   Death Reason; Death Age 
Cluster2b: Death Date; Birth Date 
Cluster3: Father; Mother 
Cluster4: Wife; Children; Number of Children 
Cluster5: Nationality; Occupation 
 
?Disaster Event? Topic  
 
Cluster1: Event Date; Event Location; Event Casualty;  
Cluster2:  Organization Involved, Person Involved 
 
?Sport Event? Topic 
 
Cluster1: Winner; Winning Score 
Cluster2: Location, Date 
 
?Organization? Topic 
 
Cluster1: Founded Date; Founded Location; Founder 
Cluster2: Headquarters; Number of Members 
1172
token list and the same searching and matching is 
repeated until the token list is empty or no se-
quence of tokenized word can be matched. The 
named entity is scored by the average weighted 
distance score of question terms and topic terms. 
Let Num(ti...tj) denotes the number of all 
matched n-grams, d(E, ti...tj) denotes the word 
distance between the named entity and the 
matched n-gram, W1(ti...tj) denotes the topic 
weight of the matched n-gram, W2(ti...tj) denotes 
the length weight of the matched n-gram. If ti...tj 
contains topic terms or question verb phrase, 0.5 
is assigned to W1, otherwise 1.0 is assigned. The 
value assigned to length weight W2 is deter-
mined by ?, the ratio value of matched n-gram 
length to question term length. How to assign W2 
is illustrated as follows.  
The weighted distance score D(E,QTerm) of 
the question term and the final score S(E) of the 
named entity are calculated by the following 
equations. 
)...(
)...(2
)...(1)...,(
),( ...
ji
tt ji
jiji
ttNum
ttW
ttWttEd
QTermED ji
? ?
=  
N
QTermED
ES
N
i
i?
=
),(
)(  
4.2 Batch QA System 
The batch QA system is built from the base-
line system and two added components: question 
classification and co-occurrence maximization. 
In a batch QA system, questions are classified 
before they are syntactically and semantically 
analyzed. The classification process consists of 
two steps: topic categorization and question 
mapping.  Firstly the topic of the series questions 
is classified into appropriate topic category and 
then the questions can be mapped to the corre-
sponding attribute and clustered according to the 
mapped attributes. Since the attributes of topic 
category is collected from frequently asked ques-
tions, there are some questions in the question 
series which can?t be mapped to any attribute. 
These unmapped questions are processed indi-
vidually.    
The topic categorization is done by a Na?ve 
Bayes classifier which employs features such as 
stemmed question terms and named entities in 
the question. The training data is a collection of 
85 question series labeled as one of four topic 
categories: ?Person?, ?Disaster Event?, ?Sport 
Event? and ?Organization?. The mapping of 
question to topic attribute is an example-based 
syntactic pattern matching and keywords match-
ing.   
 The questions grouped together are processed 
as a question cluster. After the processing of an-
swer selection and ranking, each question in the 
cluster gets top 10 scored candidate answers 
which forms an answer vector A(a1, ?, a10). 
 W2(ti...tj)=0.4   if ?<0.4; 
 W2(ti...tj)=0.6       if 0.4? ?? 0.6;  
W2(ti...tj)=0.8        if ?>0.6; 
W2(ti...tj)= 0.9      if ?>0.75. 
Answers 
 
Syntactic Chunking 
Type Categorization 
Query Generation 
Target Classification  
Questions Document 
Retrieval 
 Passage Filtering 
Surface Text Pattern Matching 
  N-Gram Proximity Search 
          Answer Ranking 
Pattern Files 
 
Tagged Corpus 
(AQUAINT 
/Web) 
 Question 
Clustering 
Co-occurrence 
Maximization 
Figure-2  Baseline QA System & Batch QA System (dashed lines and light colored component) 
1173
Suppose there are n questions in the cluster, the 
task of answer co-occurrence maximization is to 
retrieve a combination of n answers which has 
maximum pointwise mutual information (PMI). 
This combination is assumed to be the answers to 
the questions in the cluster. 
There are a total of 10n possible combinations 
among all the candidate answers. If the PMI of 
every combination should be calculated, it is 
computationally inefficient. Also, some combi-
nations containing noisy information may have 
higher co-occurrence than the correct answer 
combination. For example, the correct answers 
combination to questions showed in figure-1 is 
?August 12; 118; Barents Sea?. However, there 
is also a combination of ?Aug. 12, two; U.S.? 
which has higher pointwise mutual information 
due to the frequently occurred noisy information 
of ?two U.S. submarines? and ?two explosions in 
the area Aug. 12 at the time?.  
To reduce this negative effect brought by the 
noisy information, we started from the highest 
scored answer and put it in the final answer list. 
Then we added the answers one by one to the 
final answer list. The added answer has the high-
est PMI with the answers in the final answer list. 
It is important here to choose the first answer 
added to the final answer list correctly. Other-
wise, the following added answers will be nega-
tively affected. So in our batch QA system, a 
correct answer should be scored highest among 
all the answer candidates of the questions in the 
cluster. Although this can?t be always achieved, 
it can be approximated by setting higher thresh-
old both in passage scoring and answer ranking. 
However, in the baseline system, passages are 
not scored. They are equally processed because 
we wanted to retrieve as many answer candidates 
as possible and answer candidates are ranked by 
their matching score and redundancy score.   
4.3 Performance Evaluation 
The data corpus we used is TREC QA data 
(AQUAINT Corpus). The test questions are 
TREC QA 2004 and TREC QA 2005 questions. 
Each topic is followed with a series of factoid 
questions. The number of questions selected 
from TREC 2004 collection is 230 and the num-
ber of question series is 65. The number of ques-
tions selected from TREC 2005 collection is 362 
and the number of question series is 75.  
    We performed 4 different experiments: (1). 
Baseline system. (2). Batch QA system (Baseline 
system with co-occurrence maximization). (3). 
Baseline system with web supporting.  (4). Batch 
QA with web supporting. We introduced web 
supporting into the experiments because usually 
the information on the web tends to share more 
co-occurrence and redundancy which is also 
proved by our results.   
    Compared between the baseline system and 
batch system, the experiment results show that 
the overall accuracy score has been improved 
from 0.34 to 0.39 on TREC 2004 test questions 
and from 0.31 to 0.37 on TREC 2005 test ques-
tions. Compared between the baseline system 
and batch system with web supporting, the accu-
racy score can be improved up to 0.498.  We also 
noticed that the average number of questions un-
der each topic in TREC 2004 test questions is 
3.538, which is significantly lower than the 
4.8267 average in TREC 2005 questions series. 
This may explain why the improvement we ob-
tained on TREC2004 data is not as significant as 
the improvement obtained on TREC 2005 ques-
tions. 
The accuracy score of each TREC2005 ques-
tion series is also calculated. Figure3-4 shows the 
comparisons between 4 different experiment 
methods. We also calculate the number of ques-
tion series with accuracy increased, unchanged 
and decreased. It is also shown in the following 
table. (?+? means number of question series with 
accuracy increased, ?=? unchanged and ?-? de-
creased.)   
 
TREC2005 Question Series 
(75 question series) 
    + - = 
Baseline + Co-occurrence 25 5 45 
Baseline + Web 40 2 33 
Baseline + Co-occurrence + 
Web 
49 2 24 
Accuracy Comparison on Different 
Methods
0
0.1
0.2
0.3
0.4
0.5
0.6
1 2 3 4
TREC2004 TREC2005
1174
Some question series get unchanged accuracy 
because the questions can?t be clustered accord-
ing to our clustering template so that it can?t util-
ize the co-occurrence of answers in the cluster. 
Some question series get decreased accuracy be-
cause the questions because the noisy informa-
tion had even higher co-occurrence, the error 
occurred during the question clustering and the 
answers didn?t show any co-relations in the re-
trieved passages at all. A deep and further error 
analysis is necessary for this answer co-
occurrence maximization technique to be applied 
topic independently.   
 
5 Discussion and Future Work 
We have demonstrated that in a QA system, 
answering a series of inter-related questions can 
be improved by grouping the questions by ex-
pected co-occurrence of answers in text. The im-
provement can be made without exploiting the 
pre-compiled knowledge base. 
Although our system can cluster frequently 
asked questions on topics of ?Events?, ?Persons? 
and ?Organizations?, there are still some highly 
related questions which can?t be clustered by our 
method. Here are some examples.  
 
 
 
 
 
 
 
 
To cluster these questions, we plan to utilize 
event detection techniques and set up an event 
topic ?Carlos the Jackal captured? during the 
answering process, which will make it easier to 
cluster ?When was the Carlos the Jackal cap-
tured?? and ?Where was the Carlos the Jackal 
captured?? 
Can this answers co-occurrence maximization 
approach be applied to improve QA performance 
Topic Carlos the Jackal 
1. When was he captured?  
2. Where was he captured?    
 
Topic boxer Floyd Patterson 
1. When did he win the title? 
2. How old was he when he won the title? 
3. Who did he beat to win the title? 
 
Accuracy on TREC2005 Test Questions
0
0.2
0.4
0.6
0.8
1
1.2
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70 73
question series
ac
cu
ra
cy
baseline baseline+co_occurrence baseline+w eb baseline+w eb+co_occurrence
Accuracy on TREC2004 Test Questions
0
0.2
0.4
0.6
0.8
1
1.2
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64
question series
ac
cu
ra
cy
baseline baseline+co_occurrence baseline+w eb baseline+w eb+co_occurrence
Figure 3-4 Comparison of TREC2004/2005 Question Series Accuracy 
1175
on single questions (i.e. 1-series)? As suggested 
in the reference paper (Chu-Carrol and Prager), 
we may be able to add related (unasked) ques-
tions to form a cluster around the single question. 
Another open issue is what kind of effect will 
this technique bring to answering series of ?list? 
questions, i.e., where each question expects a list 
of items as answer.  As we know that the an-
swers of some ?list? questions have pretty high 
co-occurrence while others don?t have co-
occurrence at all. Future work involves experi-
ments conducted on these aspects.   
Acknowledgement 
The Authors wish to thank BBN for the use of 
NE tagging software IdentiFinder, CIIR at 
University of Massachusetts for the use of 
Inquery search engine, Stanford University NLP 
group for the use of Stanford parser. Thanks also 
to the anonymous reviewers for their helpful 
comments. 
References  
Chu-Carrol, J., J. Prager, C. Welty, K. Czuba and 
D. Ferrucci. ?A Multi-Strategy and Multi-
Source Approach to Question Answering?, In 
Proceedings of the 11th TREC, 2003. 
Cui, H., K. Li, R. Sun, T.-S. Chua and M.-Y. 
Kan. ?National University of Singapore at the 
TREC 13 Question Answering Main Task?. In 
Proceedings of the 13th TREC, 2005. 
Han, K.-S., H. Chung, S.-B. Kim, Y.-I. Song, J.-
Y. Lee, and H.-C. Rim. ?Korea University 
Question Answering System at TREC 2004?. 
In Proceedings of the 13th TREC, 2005.  
Harabagiu, S., D. Moldovan, C. Clark, M. Bow-
den, J. Williams and J. Bensley. ?Answer 
Mining by Combining Extraction Techniques 
with Abductive Reasoning?. In Proceedings of 
12th TREC, 2004.  
Hovy, E. L. Gerber, U. Hermjakob, M. Junk and 
C.-Y. Lin. ?Question Answering  in Webclo-
pedia?. In Proceedings of the 9th TREC, 2001.  
Lin, J., D. Quan, V. Sinha, K. Bakshi, D. Huynh, 
B. Katz and D. R. Karger. ?The Role of Con-
text in Question Answering Systems?. In CHI 
2003. 
Katz, B. and J. Lin. ?Selectively Using Relations 
to Improve Precision in Question Answering?. 
In Proceedings of the EACL-2003 Workshop 
on Natural Language Processing for Question 
Answering.  2003. 
Moldovan, D. and V. Rus. ?Logical Form Trans-
formation of WordNet and its Applicability to 
Question Answering?. In Proceedings of the 
ACL, 2001. 
Monz. C. ?Minimal Span Weighting Retrieval 
for Question Answering? In Proceedings of 
the SIGIR Workshop on Information Retrieval 
for Question Answering. 2004.  
Prager, J., E. Brown, A. Coden and D. Radev. 
?Question-Answering by Predictive Annota-
tion?. In Proceedings of SIGIR 2000, pp. 184-
191. 2000. 
Prager, J., J. Chu-Carroll and K. Czuba. ?Ques-
tion Answering Using Constraint Satisfaction: 
QA-By-Dossier-With-Constraints?. In Pro-
ceedings of the 42nd ACL. 2004. 
Ravichandran, D. and E. Hovy.  ?Learning Sur-
face   Text Patterns for a Question Answering 
System?. In Proceedings of 40th ACL. 2002. 
Soubbotin, M. and S. Soubbotin. ?Patterns of 
Potential Answer Expressions as Clues to the 
Right Answers?. In Proceedings of 11th TREC. 
2003.  
Voorhees, E. ?Using Question Series to Evaluate 
Question Answering System Effectiveness?. 
In Proceedings of HLT 2005. 2005.  
1176
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038?1046,
Beijing, August 2010
Modeling Socio-Cultural Phenomena in Discourse 
Tomek Strzalkowski1,2, George Aaron Broadwell1, Jennifer Stromer-Galley1, Samira Shaikh1, Sarah Taylor3 and Nick Webb1 1ILS Institute, University at Albany, SUNY 2IPI, Polish Academy of Sciences 3Lockheed Martin Corporation tomek@albany.edu 
 
Abstract In this paper, we describe a novel ap-proach to computational modeling and understanding of social and cul-tural phenomena in multi-party dia-logues. We developed a two-tier ap-proach in which we first detect and classify certain social language uses, including topic control, disagreement, and involvement, that serve as first order models from which presence the higher level social constructs such as leadership, may be inferred.  1. Introduction We investigate the language dynamics in small group interactions across various set-tings. Our focus in this paper is on English online chat conversations; however, the mod-els we are developing are more universal and applicable to other conversational situations: informal face-to-face interactions, formal meetings, moderated discussions, as well as interactions conducted in languages other than English, e.g., Urdu and Mandarin.  Multi-party online conversations are particu-larly interesting because they become a per-vasive form of communication within virtual communities, ubiquitous across all age groups. In particular, a great amount of communica-tion online occurs in virtual chat-rooms, typi-cally conducted using a highly informal text dialect. At the same time, the reduced-cue environment of online interaction necessitates more explicit linguistic devices to convey social and cultural nuances than is typical in face-to-face or even voice conversations.  Our objective is to develop computational models of how certain social phenomena such as leadership, power, and conflict are signaled and reflected in language through the choice of lexical, syntactic, semantic and conversa-tional forms by discourse participants. In this 
paper we report the results of an initial phase of our work during which we constructed a prototype system called DSARMD-1 (De-tecting Social Actions and Roles in Multi-party Dialogue). Given a representative seg-ment of multiparty task-oriented dialogue, DSARMD-1 automatically classifies all dis-course participants by the degree to which they deploy selected social language uses, such as topic control, task control, involve-ment, and disagreement. These are the mid-level social phenomena, which are de-ployed by discourse participants in order to achieve or assert higher-level social con-structs, including leadership. In this work we adopted a two-tier empirical approach where social language uses are modeled through observable linguistic features that can be automatically extracted from dialogue. The high-level social constructs are then inferred from a combination of language uses attrib-uted to each discourse participant; for exam-ple, a high degree of influence and a high de-gree of involvement by the same person may indicate a leadership role. In this paper we limit our discussion to the first tier only: how to effectively model and classify social lan-guage uses in multi-party dialogue.  2. Related Research Issues related to linguistic manifestation of social phenomena have not been systemati-cally researched before in computational lin-guistics; indeed, most of the effort thus far was directed towards the communicative di-mension of discourse. While the Speech Acts theory (Austin, 1962; Searle, 1969) provides a generalized framework for multiple levels of discourse analysis (locution, illocution and perlocution), most current approaches to dia-logue focus on information content and structural components (Blaylock, 2002; Car-berry & Lambert, 1999; Stolcke, et al, 2000) in dialogue; few take into account the effects that speech acts may have upon the social 
1038
roles of discourse participants. Also relevant is research on modeling sequences of dia-logue acts ? to predict the next one (Samuel et al 1998; Ji & Bilmes, 2006 inter alia) ? or to map them onto subsequences or ?dialogue games? (Carlson 1983; Levin et al, 1998), which are attempts to formalize participants? roles in conversation (e.g., Linell, 1990; Poe-sio &?Mikheev, 1998; Field et al, 2008). There is a body of literature in anthropology, linguistics, sociology, and communication on the relationship between language and power, as well as other social phenomena, e.g., con-flict, leadership; however, existing ap-proaches typically look at language use in situations where the social relationships are known, rather than using language predic-tively. For example, conversational analysis (Sacks et al, 1974) is concerned with the structure of interaction: turn-taking, when interruptions occur, how repairs are signaled, but not what they reveal about the speakers. Research in anthropology and communication has concentrated on how certain social norms and behaviors may be reflected in language (e.g., Scollon and Scollon, 2001; Agar, 1994) with few systematic studies attempting to ex-plore the reverse, i.e., what the linguistic phenomena tell us about social norms and behaviors.  3. Data & Annotation Our initial focus has been on on-line chat dialogues. While chat data is plentiful on-line, its adaptation for research purposes presents a number of challenges that include users? pri-vacy issues on the one hand, and their com-plete anonymity on the other. Furthermore, most data that may be obtained from public chat-rooms is of limited value for the type of modeling tasks we are interested in due to its high-level of noise, lack of focus, and rapidly shifting, chaotic nature, which makes any longitudinal studies virtually impossible. To derive complex models of conversational be-havior, we need the interaction to be reasona-bly focused on a task and/or social objectives within a group. Few data collections exist covering multiparty dialogue, and even fewer with on-line chat. Moreover, the few collections that exist were built primarily for the purpose of training dialogue act tagging and similar linguistic phenomena; few if any of these corpora are 
suitable for deriving pragmatic models of conversation, including socio-linguistic phe-nomena. Existing resources include a multi-person meeting corpus ICSI-MRDA and the AMI Meeting Corpus (Carletta, 2007), which contains 100 hours of meetings cap-tured using synchronized recording devices. Still, all of these resources look at spoken language rather than on-line chat. There is a parallel interest in the online chat environ-ment, although the development of useful re-sources has progressed less. Some corpora exist such as the NPS Internet chat corpus (Forsyth and Martell, 2007), which has been hand-anonymized and labeled with part-of-speech tags and dialogue act labels. The StrikeCom corpus (Twitchell et al, 2007) consists of 32 multi-person chat dialogues between players of a strategic game, where in 50% of the dialogues one participant has been asked to behave ?deceptively?. It is thus more typical that those interested in the study of Internet chat compile their own corpus on an as needed basis, e.g., Wu et al (2002), Khan et al (2002), Kim et al (2007).  Driven by the need to obtain a suitable dataset we designed a series of experiments in which recruited subjects were invited to participate in a series of on-line chat sessions in a spe-cially designed secure chat-room. The ex-periments were carefully designed around topics, tasks, and games for the participants to engage in so that appropriate types of behav-ior, e.g., disagreement, power play, persuasion, etc. may emerge spontaneously. These ex-periments and the resulting corpus have been described elsewhere (Shaikh et al, 2010b), and we refer the reader to this source. Ulti-mately a corpus of 50 hours of English chat dialogue was collected comprising more than 20,000 turns and 120,000 words. In addition we also assembled a corpus of 20 hours of Urdu chat.  A subset of English language dataset has been annotated at four levels: communication links, dialogue acts, local topics and meso-topics (which are essentially the most persistent lo-cal topics). Although full details of these an-notations are impossible to explain within the scope of this article, we briefly describe them below. Annotated datasets were used to de-velop and train automatic modules that detect and classify social uses of language in dis-course. It is important to note that the annota-
1039
tion has been developed to support the objec-tives of our project and does not necessarily conform to other similar annotation systems used in the past.  ? Communicative links. In a multi-party dia-logue an utterance may be directed towards a specific participant, a subgroup of par-ticipants or to everyone.  ? Dialogue Acts. We developed a hierarchy of 15 dialogue acts for annotating the func-tional aspect of the utterance in discussion.  The tagset we adopted is based on DAMSL (Allen & Core, 1997) and SWBD (Jurafsky et al, 1997), but compressed to 15 tags tuned significantly towards dialogue prag-matics and away from more surface char-acteristics of utterances (Shaikh et al, 2010a).  ? Local topics. Local topics are defined as nouns or noun phrases introduced into dis-course that are subsequently mentioned again via repetition, synonym, or pronoun.  ? Topic reference polarity. Some topics, which we call meso-topics, persist through a number of turns in conversation. A selec-tion of meso-topics is closely associated with the task in which the discourse par-ticipants are engaged. Meso-topics can be distinguished from the local topics because the speakers often make polarized state-ments about them.  4. Socio-linguistic Phenomena We are interested in modeling the social phe-nomena of Leadership and Power in discourse. These high-level phenomena (or Social Roles, SR) will be detected and attributed to dis-course participants based on their deployment of selected Language Uses (LU) in multi-party dialogue. Language Uses are mid-level socio-linguistic devices that link linguistic components deployed in discourse (from lexical to pragmatic) to social con-structs obtaining for and between the partici-pants. The language uses that we are currently studying are Agenda Control, Disagreement, and Involvement (Broadwell et al, 2010). Our research so far is focused on the analysis of English-language synchronous chat, and we are looking for correlations between vari-ous metrics that can be used to detect LU in multiparty dialogue. We expect that some of these correlations may be culturally specific or language-specific, as we move into the 
analysis of Urdu and Mandarin discourse in the next phase of this project. 4.1 Agenda Control in Dialogue Agenda Control is defined as efforts by a member or members of the group to advance the group?s task or goal. This is a complex LU that we will model along two dimensions: (1) Topic Control and (2) Task Control. Topic Control refers to attempts by any discourse participants to impose the topic of conversa-tion. Task Control, on the other hand, is an effort by some members of the group to de-fine the group?s project or goal and/or steer the group towards that goal. We believe that both behaviors can be detected using scalar measures per participant based on certain linguistic features of their utterances. For example, one hypothesis is that topic control is indicated by the rate of local topic introductions (LTI) per participant (Givon, 1983). Local topics may be defined quite simply as noun phrases introduced into dis-course, which are subsequently mentioned again via repetition, synonym, pronoun, or other form of co-reference. Thus, one meas-ure of topic control is the number of local topics introduced by each participant as per-centage of all local topics in a discourse.  Using an LTI index we can construct asser-tions about topic control in a discourse. For example, suppose the following information is discovered about the speaker LE in a multi-party discussion dialogue-11 where 90 local topics are identified: 1. LE introduces 23/90 (25.6%) of local top-ics in this dialogue. 2. The mean rate of local topic introductions is this dialogue is 14.29%, and standard deviation is 8.01. 3. LE is in the top quintile of participants for introducing new local topics We can now claim the following, with a de-gree of confidence (to be determined): TopicControlLTI (LE, 5, dialogue-1) We read this as follows: speaker LE exerts the highest degree of topic control in dialogue-1. Of course, LTI is just one source of evidence and we developed other metrics to comple-ment it. We mention three of them here:                                                 1 Dialogue-1 refers to an actual dataset of 90-minute chat among 7 participants, covering approximately 700 turns. The task is to select a candidate for a job given a set of resumes. 
1040
? SMT Index. This is a measure of topic con-trol suggested in (Givon, 1983) and it is based on subsequent mentions of already introduced local topics. Speakers who in-troduce topics that are discussed at length by the group tend to control the topic of the discussion. The subsequent mentions of lo-cal topics (SMT) index calculates the per-centage of second and subsequent refer-ences to the local topics, by repetition, synonym, or pronoun, relative to the speakers who introduced them.  ? Cite Score. This index measures the extent to which other participants discuss topics introduced by that speaker. The difference between SMT and CiteScore is that the lat-ter reflect to what degree a speaker?s efforts to control the topic are assented to by other participants in a conversation. ? TL Index (TL). This index stipulates that more influential speakers take longer turns than those who are less influential. The TL index is defined as the average number of words per turn for each speaker. Turn length also reflects the extent to which other participants are willing to ?yield the floor? in conversation. Like LTI, all the above indices are mapped into a degree of topic control, based on quin-tiles in normal distribution (Table 1).   
 
LTI SMT CS TL AVG LE 5 5 5 5 5.00 JR 4 4 4 3 3.75 KI 4 3 3 1 2.75 KN 3 5 4 4 4.00 KA 2 2 2 4 2.50 CS 2 2 2 2 2.00 JY 1 1 1 2 1.25 Table 1: Topic Control distribution in dialogue-1. Each row represents a speaker in the group (LE, JR, etc.). Columns show indices used, with degrees per speaker on 5-point scale based on quintiles in normal distribu-tion, and the average value. Ideally, all the above indices (and others yet to be defined) should predict the same out-come, i.e., for each dialogue participant they should assign the same degree of topic control, relative to other speakers. This is not always the case, and where the indices divert in their predictions, our level of confidence in the generated claims decreases. We are currently 
working on how these different metrics cor-relate to each other and how they should be weighted to maximize accuracy of making Topic Control claims. Nonetheless, we can already output a Topic Control map (shown in Table 1) that captures a sense of internal so-cial dynamics within the group.  The other aspect of Agenda Control phe-nomenon is Task Control. It is defined as an effort to determine the group's goal and/or steer the group towards that goal. Unlike Topic Control, which is imposed by influenc-ing the subject of conversation, Task Control is gained by directing other participants to perform certain tasks or accept certain opin-ions. Consequently, Task Control is detected by observing the usage of certain dialogue acts, including Action-Directive, Agree-Accept, Disagree-Reject, and related categories. Here again, we define several in-dices that allow us to compute a degree of Task Control in dialogue for each participant: ? Directive Index (DI). The participant who directs others is attempting to control the course of the task that the group is per-forming. We count the number of directives, i.e., utterances classified as Ac-tion-Directive, made by each participant as a percentage of all directives in discourse. ? Directed Topic Shift Index (DTSI). When a participant who controls the task offers a directive on the task, then the topic of con-versation shifts. In order to detect this con-dition, we calculate the ratio of coincidence of directive dialogue acts by each partici-pant with topic shifts following them.  ? Process Management index (PMI). Another measure of Task Control is the proportion of turns each participant has that explicitly address the problem solving process. This includes utterances that involve coordinat-ing the activities of the participants, plan-ning the order of activities, etc. These fall into the category of Task (or Process) Management in most DA tagging systems.  ? Process Management Success Index (PMSI). This index measures the degree of success by each speaker at controlling the task. A credit is given to the speaker whose suggested curse of action is supported by other speakers for each response that sup-ports the suggestion. Conversely, a credit is taken away for each response that rejects or 
1041
qualifies the suggestion. PMSI is computed as distribution of task management credits among the participants over all dialogue utterances classified as Task/Process Man-agement. 2 As an example, let?s consider the following information computed for the PMI index over dialogue-1:  1. Dialogue-1 contains 246 utterances classi-fied as Task/Process Management rather than doing the task. 2. Speaker KI makes 65 of these utterances for a PMI of 26.4%. 3. Mean PMI for participants is 14.3%; 80th percentile is >21.2%. PMI for KI is in the top quintile for all participants. Based on this evidence we may claim (with yet to be determined confidence) that: TaskControlPMI(KI, 5, dialogue-1) This may be read as follows: speaker KI ex-erts the highest degree of Task Control in dialogue-1. We note that Task Control and Topic Control do not coincide in this dis-course, at least based on the PMI index. Other index values for Task Control may be com-puted and tabulated in a way similar to LTI in Table 1. We omit these here due to space limitations. 4.2 Disagreement in Dialogue Disagreement is another language use that correlates with speaker?s power and leader-ship. There are two ways in which disagree-ment is realized: expressive disagreement and topical disagreement (Stromer-Galley, 2007; Price, 2002). Both can be detected using sca-lar measures applied to subsets of participants, typically any two participants. In addition, we can also measure for each participant the rate with which he or she generates disagreement (with any and all other speakers). Expressive Disagreement is normally understood at the level of dialogue acts, i.e., when discourse participants make explicit utterances of dis-agreement, disapproval, or rejection in re-sponse to a prior speaker?s utterance. Here is an example (KI and KA are two speakers in a multiparty dialogue in which participants                                                 2 The exact structure of the credit function is still being deter-mined experimentally. For example, more credit may be given to first supporting response and less for subsequent responses; more credit may be given for unprompted suggestions than for those that were responding to questions from others. 
discuss candidates for a youth counselor job): KA: CARLA... women are always better with kids KI: That?s not true! KI: Men can be good with kids too While such exchanges are vivid examples of expressive disagreement, we are interested in more sustained phenomenon where two speakers repeatedly disagree, thus revealing a social relationship between them. Therefore, one measure of Expressive Disagreement that we consider is the number of Disagree-Reject dialogue acts between any two speakers as a percentage of all utterances exchanged be-tween these two speakers. This becomes a basis for the Disagree-Reject Index (DRX). In dialogue-1 we have: 1. Speakers KI and KA have 47 turns between them. Among these there are 8 turns classi-fied as Disagree-Reject, for the DRX of 15.7%. 2. The mean DRX for speakers who make any Disagree-Reject utterances is 9.5%. The pair of speakers KI-KA is in the top quin-tile (>13.6%). Based on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which may be read as follows: speakers KI and KA have the highest level of expressive disagreement in dialogue-1. This measure is complemented by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a percentage of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of speakers, the CDX values are as-signed to each group participant and indicate the degree of disagreement that each person generates. While Expressive Disagreement is based on the use of more overt linguistic devices, Topical Disagreement is defined as a differ-ence in referential valence in utterances (statements, opinions, questions, etc.) made on a topic. Referential valence of an utterance is determined by the type of statement made about the topic in question, which can be positive (+), negative (?), or neutral (0). A positive statement is one in favor of (express advocacy) or in support of (supporting infor-mation) the topic being discussed. A negative statement is one that is against or negative on 
1042
the topic being discussed. A neutral statement is one that does not indicate the speaker?s po-sition on the topic. Here is an example of op-posing polarity statements about the same topic in discourse: Sp-1: I like that he mentions ?Volunteerism and Leadership? Sp-2: but if they?re looking for someone who is experienced then I?d cross him off Detecting topical disagreement in discourse is more complicated because its strength may vary from one topic in a conversation to the next. A reasonable approach is thus to meas-ure the degree of disagreement between two speakers on one topic first, and then extrapo-late over the entire discourse. Accordingly, our measure of topical disagreement is valua-tion differential between any two speakers as expressed in their utterances about a topic. Here, the topic (or an ?issue?) is understood more narrowly than the local topic defined in the previous section (as used in Topic Control, for example), and may be assumed to cover only the most persistent local topics, i.e., top-ics with the largest number of references in dialogue, or what we call the meso-topics. For example, in a discussion of job applicants, each of the applicants becomes a meso-topic, and there may be additional meso-topics pre-sent, such as qualifications required, etc.  The resulting Topical Disagreement Metric (TDM) captures the degree to which any two speakers advocate the opposite sides of a meso-topic. TDM is computed as an average of P-valuation differential for one speaker (advocating for a meso-topic) and (?P)-valuation differential for the other speaker (advocating against the meso-topic).  Using TDM we can construct claims related to disagreement in a given multiparty dia-logue of sufficient duration (exactly what constitutes a sufficient duration is still being researched). Below is an example based on a 90-minute chat dialogue-1 about several job candidates for a youth counselor. The discus-sion involved 7 participants, including KI and KA. Topical disagreement is measured on 5 points scale (corresponding to quintiles in normal distribution): TpDisAgreeTDM(KI,KA,?Carla?,4,dialogue-1) This may be read as follows: speakers KI and KA topically disagree to degree 4 on topic [job candidate] ?Carla? in dialogue-1. In or-
der to calculate this we compute the value of TDM index between these two speakers. We find that KA makes 30% of all positive utter-ances made by anyone about Carla (40), while KI makes 45% of all negative utterances against Carla. This places these two speakers in the top quintiles in the ?for Carla? polarity distribution and ?against Carla? distribution, respectively. Taking into account any oppos-ing polarity statements made by KA against Carla and any statements made by KI for Carla, we calculate the level of topical dis-agreement between KA and KI to be 4 on the 1-5 scale. TDM allows us to compute topical disagree-ment between any two speakers in a discourse, which may also be represented in a 2-dimensional table revealing another inter-esting aspect of internal group dynamics.  4.3 Involvement in Dialogue The third type of social language use that we discuss in this paper is Involvement. In-volvement is defined as a degree of engage-ment or participation in the discussion of a group. It is an important element of leader-ship, although its importance is expected to differ between cultures; in Western cultures, high involvement and influence (topic control) often correlates with group leadership. In order to measure Involvement we designed several indices based on turn characteristics for each speaker. Four of the indices are briefly explained below:  ? The NP index (NPI) is a measure of gross informational content contributed by each speaker in discourse. NPI counts the ratio of third-person nouns and pronouns used by a speaker to the total number of nouns and pronouns in the discourse.  ? The Turn index (TI) is a measure of inter-actional frequency; it counts the ratio of turns per participant to the total number of turns in the discourse.  ? The Topic Chain Index (TCI) counts the degree to which participants discuss of the most persistent topics. In order to calculate TCI values, we define a topic chains for all local topics. We compute frequency of mentions of these longest topics for each participant.  ? The Allotopicality Index (ATP) counts the number of mentions of local topics that were introduced by other participants. An 
1043
ATP value is the proportion of a speaker's allotopical mentions, i.e., excluding ?self-citations?, to all allotopical mentions in a discourse.  As an example, we may consider the follow-ing situation in dialogue-1: 1. Dialogue-1 contains 796 third person nouns and pronouns, excluding mentions of participants? names. 2. Speaker JR uses 180 nouns and pronouns for an NPI of 22.6%.  3. The median NPI is 14.3%; JR are in the upper quintile of participants (> 19.9%). From the above evidence we can draw the following claim: InvolvementNPI(JR, 5, dialogue-1) This may be read as: speaker JR is the most involved participant in dialogue-1. As with other language uses, multiple indices for Involvement can be combined into a 2-dimensional map capturing the group in-ternal dynamics.  5. Implementation & Evaluation We developed a prototype automated DSARMD system that comprises a series of modules that create automated annotation of the source dialogue for all the language ele-ments discussed above, including communi-cative links, dialogue acts, local/meso topics, and polarity. Automatically annotated dia-logue is then used to generate language use degree claims. In order to evaluate accuracy of the automated process we conducted a pre-liminary evaluation comparing the LU claims generated from automatically annotated data to the claims generated from manually coded dialogues. Below we briefly describe the methodology and metrics used. Each language use is asserted per a partici-pant in a discourse (or per each pair of par-ticipants, e.g., for Disagreement) on a 5-point ?strength? scale. This can be represented as an ordered sequence LUX(d1, d2, ? dn), where LU is the language use being asserted, X is the index used, di is the degree of LU attrib-uted to speaker i. This assignment is therefore a 5-way classification of all discourse par-ticipants and its correctness is measured by dividing the number of correct assignments by the total number of elements to be classi-fied, which gives the micro-averaged preci-sion. The accuracy metric is computed with 
several variants as follows: 1. Strict mapping: each complete match is counted as 1; all mismatches are counted as 0. For example, the outputs LUX (5,4,3,2,1) and LUX (4,5,3,1,1) produce two exact matches (for the third and the last speaker) for a precision of 0.4. 2. Weighted mapping: since each degree value di in LUX(d1, d2, ? dn) represents a quintile in normal distribution, we consider the po-sition of the value within the quintile. If two mismatched values are less than ? quintile apart we assign a partial credit (currently 0.5). 3. Highest ? Rest: we measure accuracy with which the highest LU degree (but not nec-essarily the same degree) is assigned to the right speaker vs. any other score. This re-sults in binary classification of scores. The sequences in (1) produce 0.6 match score. 4. High ? Low: An alternative binary classifi-cation where scores 5 and 4 are considered High, while the remaining scores are con-sidered Low. Under this metric, the se-quences in (1) match with 100% precision. The process of automatic assignment of lan-guage uses derived from automatically proc-essed dialogues was evaluated against the control set of assignments based on hu-man-annotated data. In order to obtain a reli-able ?ground truth?, each test dialogue was annotated by at least three human coders (linguistics and communication graduate stu-dents, trained). Since human annotation was done at the linguistic component level, a strict inter-annotator agreement was not required; instead, we were interested whether in each case a comparable statistical distribution of the corresponding LU index was obtained. Annotations that produced index distributions dissimilar from the majority were eliminated. Automated dialogue processing involved the following modules: ? Local topics detection identifies first men-tions by tracking occurrences of noun phrases. Subsequent mentions are identi-fied using fairly simple pronoun resolution (based mostly on lexical features), with Wordnet used to identify synonyms, etc. ? Meso-topics are identified as longest-chain local topics. Their polarity is assessed at the utterance level by noting presence of positive or negative cue words and phrases. ? Dialogue acts are tagged based on presence 
1044
of certain cue phrases derived from a train-ing corpus (Webb et al, 2008).  ? Communicative links are mapped by com-puting inter-utterance similarity based on n-gram overlap. Preliminary evaluation results are shown in Tables 3-5 with average performance over 3 chat sessions (approx 4.5 hours) involving three groups of speakers and different tasks (job candidates, political issues). Topic Con-trol and Involvement tables show average accuracy per index. For example, the LTI in-dex, computed over automatically extracted local topics, produces Topic Control assign-ments with the average precision of 80% when compared to assignments derived from human-annotated data using the strict accu-racy metric. However, automated prediction of Involvement based on NPI index is far less reliable, although we can still pick the most involved speaker with 67% accuracy. We omit the indices based on turn length (TL) and turn count (TI) because their values are trivially computed. At this time we do not combine indices into a single LU prediction. Addi-tional experiments are needed to determine how much each of these indices contributes to LU prediction. Topic  Control LTI? SMT? CS?Strict? 0.80? 0.40? 0.40?Weighted? 0.90? 0.53? 0.53?Highest?Rest? 0.90? 0.67? 0.67?High?Low? 1.00? 0.84? 0.90?Table 3: Topic Control LU assignment performance averages of selected indices over a subset of data cov-ering three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, 7 per session). 
Involvement NPI? TCI? ATP?Strict? 0.31? 0.42? 0.39?Weighted? 0.46? 0.49? 0.42?Highest?Rest? 0.67? 0.77? 0.68?High?Low? 0.58? 0.74? 0.48?Table 4: Involvement LU assignment performance av-erages for selected indices over the same subset of data as in Table 3. Topical Disagreement performance is shown in Table 5. We calculated precision and recall of assigning a correct degree of disagreement 
to each pair of speakers who are members of a group. Precision and recall averages are then computed over all meso-topics identified in the test dataset, which consists of three separate 90-minute dialogues involving 7, 5 and 7 speakers, respectively. Our calculation includes the cases where different sets of meso-topics were identified by the system and by the human coder. A strict mapping of levels of disagreement between speakers is hard to compute accurately; however, finding the speakers who disagree the most, or the least, is significantly more robust. 
Topical Disagreement Prec.? Recall?Strict? 0.33? 0.32?Weighted? 0.54? 0.54?Highest?Rest? 0.89? 0.85?High?Low? 0.77? 0.73?Table 5: Topical Disagreement LU assignment per-formance averages over 13 meso-topics discussed in three dialogues with combined duration of 4.5 hours with total of 19 participants (7, 5, and 7 per session). 6. Conclusion In this paper we presented a preliminary design for modeling certain types of social phenomena in multi-party on-line dialogues. Initial, limited-scale evaluation indicates that the model can be effectively automated. Much work lies ahead, including large scale evaluation, testing index stability and resilience to NL component level error. Current performance of the system is based on only preliminary versions of linguistic modules (topic extraction, polarity assignments, etc.) which perform at only 70-80% accuracy, so these need to be improved as well. Research on Urdu and Chinese dialogues is just starting. Acknowledgements This research was funded by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the U.S. Army Research Lab. All statements of fact, opinion or conclusions contained herein are those of the authors and should not be construed as representing the official views or policies of IARPA, the ODNI or the U.S. Government. 
1045
References Agar, Michael. 1994. Language Shock, Under-standing the Culture of Conversation. Quill, William Morrow, New York. Allen, J. M. Core. 1997. Draft of DAMSL: Dialog Act Markup in Several Layers. www.cs. roch-ester.edu/research/cisd/resources/damsl/  Anderson, A., et al 1991. The HCRC Map Task Corpus. Language and Speech 34(4), 351--366. Austin, J. L. 1962. How to do Things with Words. Clarendon Press, Oxford. Bird, Steven, et al 2009. Natural Language Proc-essing with Python: Analyzing Text with the Natural Language Toolkit. O'Reilly Media.  Blaylock, Nate. 2002. Managing Communicative Intentions in Dialogue Using a Collaborative Problem-Solving Model. Technical Report 774, University of Rochester, CS Dept. Broadwell, G. A et al (2010). Social Phenomena and Language Use. ILS Technical report. Carberry, Sandra and Lynn Lambert. 1999. A Process Model for Recognizing Communicative Acts and Modeling Negotiation Dialogue. Computational Linguistics, 25(1), pp. 1-53. Carletta, J. (2007). Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus. Language Resources and Evaluation Journal 41(2): 181-190 Carlson, Lauri. 1983. Dialogue Games: An Ap-proach to Discourse Analysis. D. Reidel. Eric N. Forsyth and Craig H. Martell. 2007. Lexi-cal and Discourse Analysis of Online Chat Dia-log. First IEEE International Conference on Semantic Computing (ICSC 2007), pp. 19-26. Field, D., et al 2008. Automatic Induction of Dia-logue Structure from the Companions Dialogue Corpus, 4th Int. Workshop on Human-Computer Conversation, Bellagio. Givon, Talmy. 1983. Topic continuity in discourse: A quantitative cross-language study. Amster-dam: John Benjamins.  Ivanovic, Edward. 2005. Dialogue Act Tagging for Instant Messaging Chat Sessions. In Proceed-ings of the ACL Student Research Workshop. 79?84. Ann Arbor, Michigan. Ji, Gang Jeff Bilmes. 2006. Backoff Model Train-ing using Partially Observed Data: Application to Dialog Act Tagging. HLT-NAACL Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-asca. 1997. Switchboard SWBD-DAMSL Shal-low-Discourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/ manual.august1.html Jurafsky, D., et al 1997. Automatic detection of discourse structure for speech recognition and understanding. IEEE Workshop on Speech Recognition and Understanding, Santa Barbara. Khan, Faisal M., et al 2002. Mining Chat-room Conversations for Social and Semantic Interac-
tions. Computer Science and Engineering, Le-high University. Kim, Jihie., et al 2007. An Intelligent Discus-sion-Bot for Guiding Student Interactions in Threaded Discussions. AAAI Spring Sympo-sium on Interaction Challenges for Intelligent Assistants Levin, L., et al (1998). A discourse coding scheme for conversational Spanish. Interna-tional Conference on Speech and Language Processing. Levin, L., et al (2003). Domain specific speech acts for spoken language translation. 4th SIG-dial Workshop on Discourse and Dialogue. Linell, Per. 1990. The power of dialogue dynamics. In Ivana Markov?a and Klaus Foppa, editors, The Dynamics of Dialogue. Harvester, 147?177. Poesio, Massimo and Andrei Mikheev. 1998. The predictive power of game structure in dialogue act recognition. International Conference on Speech and Language Processing (ICSLP-98). Price, V., Capella, J. N., & Nir, L. (2002). Does disagreement contribute to more deliberative opinion? Political Communication, 19, 95-112. Sacks, H. and Schegloff, E., Jefferson, G. 1974. A simplest systematic for the organization of turn-taking for conversation. In: Language 50(4), 696-735.  Samuel, K. et al 1998. Dialogue Act Tagging with Transformation-Based Learning. 36th Annual Meeting of the ACL. Scollon, Ron and Suzanne W. Scollon. 2001. Intercultural Communication, A Discourse Ap-proach. Blackwell Publishing, Second Edition. Searle, J. R. 1969. Speech Acts. Cambridge Uni-versity Press, London-New York. Shaikh, S. et al 2010. DSARMD Annotation Guidelines, V. 2.5. ILS Technical Report.  Shaikh S. et al 2010. MPC: A Multi-Party Chat Corpus for Modeling Social Phenomena in Discourse, Proc. LREC-2010, Malta. Stolcke, Andreas et al 2000. Dialogue Act Mod-eling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguis-tics, 26(3). Stromer-Galley, J. 2007. Measuring deliberation?s content: A coding scheme. Journal of Public Deliberation, 3(1).  Tianhao Wu, et al 2002. Posting Act Tagging Us-ing Transformation-Based Learning. Founda-tions of Data Mining and Discovery, IEEE In-ternational Conference on Data Mining Twitchell, Douglas P., Jay F. Nunamaker Jr., and Judee K. Burgoon. 2004. Using Speech Act Profiling for Deception Detection. Intelligence and Security Informatics, LNCS, Vol. 3073 Webb, N., T. Liu, M. Hepple and Y. Wilks. 2008. Cross-Domain Dialogue Act Tagging. 6th In-ternational Conference on Language Resources and Evaluation (LREC-2008), Marrakech. 
1046
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 296?305,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Bootstrapping Events and Relations from Text 
Ting Liu ILS, University at Albany, USA tliu@albany.edu 
Tomek Strzalkowski ILS, University at Albany, USA Polish Academy of Sciences tomek@albany.edu 
Abstract 
In this paper, we describe a new approach to semi-supervised adaptive learning of event extraction from text. Given a set of exam-ples and an un-annotated text corpus, the BEAR system (Bootstrapping Events And Relations) will automatically learn how to recognize and understand descriptions of complex semantic relationships in text, such as events involving multiple entities and their roles. For example, given a series of descriptions of bombing and shooting inci-dents (e.g., in newswire) the system will learn to extract, with a high degree of accu-racy, other attack-type events mentioned elsewhere in text, irrespective of the form of description. A series of evaluations using the ACE data and event set show a signifi-cant performance improvement over our baseline system. 
1 Introduction We constructed a semi-supervised machine learning process that effectively exploits statisti-cal and structural properties of natural language discourse in order to rapidly acquire rules to de-tect mentions of events and other complex rela-tionships in text, extract their key attributes, and construct template-like representations. The learning process exploits descriptive and struc-tural redundancy, which is common in language; it is often critical for achieving successful com-munication despite distractions, different con-texts, or incompatible semantic models between a speaker/writer and a hearer/reader. We also take advantage of the high degree of referential consistency in discourse (e.g., as observed in word sense distribution by (Gale, et al1992), and arguably applicable to larger linguistic units), which enables the reader to efficiently correlate different forms of description across coherent spans of text.  The method we describe here consists of two steps: (1) supervised acquisition of initial extrac-tion rules from an annotated training corpus, and 
(2) self-adapting unsupervised multi-pass boot-strapping by which the system learns new rules as it reads un-annotated text using the rules learnt in the first step and in the subsequent learning passes. When a sufficient quantity and quality of text material is supplied, the system will learn many ways in which a specific class of events can be described. This includes the capability to detect individual event mentions using a system of context-sensitive triggers and to isolate perti-nent attributes such as agent, object, instrument, time, place, etc., as may be specific for each type of event. This method produces an accurate and highly adaptable event extraction that significant-ly outperforms current information extraction techniques both in terms of accuracy and robust-ness, as well as in deployment cost. 2 Learning by bootstrapping  As a semi-supervised machine learning method, bootstrapping can start either with a set of prede-fined rules or patterns, or with a collection of training examples (seeds) annotated by a domain expert on a (small) data set. These are normally related to a target application domain and may be regarded as initial ?teacher instructions? to the learning system. The training set enables the sys-tem to derive initial extraction rules, which are applied to un-annotated text data in order to pro-duce a much larger set of examples. The exam-ples found by the initial rules will occur in a variety of linguistic contexts, and some of these contexts may provide support for creating alter-native extraction rules. When the new rules are subsequently applied to the text corpus, addition-al instances of the target concepts will be identi-fied, some of which will be positive and some not. As this process continues to iterate over, the system acquires more extraction rules, fanning out from the seed set until no new rules can be learned.  Thus defined, bootstrapping has been used in natural language processing research, notably in word sense disambiguation (Yarowsky, 1995). Strzalkowski and Wang (1996) were first to demonstrate that the technique could be applied to adaptive learning of named entity extraction 
296
 Figure 1. Skeletal dependency structure representation of an event mention. 
rules. For example, given a ?na?ve? rule for iden-tifying company names in text, e.g., ?capitalized NP followed by Co.?, their system would first find a large number of (mostly) positive instanc-es of company names, such as ?Henry Kauffman Co.? From the context surrounding each of these instances it would isolate alternative indicators, such as ?the president of?, which is noted to oc-cur in front of many company names, as in ?The president of American Electric Automobile Co. ??. Such alternative indicators give rise to new extraction rules, e.g., ?president of + CNAME?. The new rules find more entities, including com-pany names that do not end with Co., and the process iterates until no further rules are found. The technique achieved a very high performance (95% precision and 90% recall), which encour-aged more research in IE area by using boot-strapping techniques. Using a similar approach, (Thelen and Riloff, 2002) generated new syntac-tic patterns by exploiting the context of known seeds for learning semantic categories.  In Snowball (Agichtein and Gravano, 2000 ) and Yangarber?s IE system (2000), bootstrapping technique was applied for extraction of binary relations, such as Organization-Location, e.g., between Microsoft and Redmond, WA. Then, Xu (2007) extended the method for more complex relations extraction by using sentence syntactic structure and a data driven pattern generation. In this paper, we describe a different approach on building event patterns and adapting to the dif-ferent structures of unseen events. 3 Bootstrapping applied to event learn-ing  Our objective in this project was to expand the bootstrapping technique to learn extraction of events from text, irrespective of their form of description, a property essential for successful adaptability to new domains and text genres. The major challenge in advancing from entities and binary relations to event learning is the complex-ity of structures involved that not only consist of multiple elements but their linguistic context may now extend well beyond a few surrounding words, even past sentence boundaries. These considerations guided the design of the BEAR system (Bootstrapping Events And Relations), which is described in this paper. 3.1 Event representation  An event description can vary from very concise, newswire-style to very rich and complex as may 
be found in essays and other narrative forms. The system needs to recognize any of these forms and to do so we need to distill each description to a basic event pattern. This pattern will capture the heads of key phrases and their dependency struc-ture while suppressing modifiers and certain oth-er non-essential elements. Such skeletal representations cannot be obtained with keyword analysis or linear processing of sentences at word level (e.g., Agichtein and Gravano, 2000), be-cause such methods cannot distinguish a phrase head from its modifier. A shallow dependency parser, such as Minipar (Lin, 1998), that recog-nizes dependency relations between words is quite sufficient for deriving head-modifier rela-tions and thus for construction of event tem-plates. Event templates are obtained by stripping the parse tree of modifiers while preserving the basic dependency structure as shown in Figure 1, which is a stripped down parse tree of, ?Also Monday, Israeli soldiers fired on four diplomatic vehicles in the northern Gaza town of Beit Hanoun, said diplomats? The model proposed here represents a signifi-cant advance over the current methods for rela-tion extraction, such as the SVO model (Yangarber, et al2000) and its extension, e.g., the chain model (Sudo, et al2001) and other related variants (Riloff, 1996) all of which lack the expressive power to accurately recognize and represent complex event descriptions and to sup-port successful machine learning. While Sudo?s subtree model (2003) overcomes some of the limitations of the chain models and is thus con-ceptually closer to our method, it nonetheless lacks efficiency required for practical applica-tions.  We represent complex relations as tree-like structures anchored at an event trigger (which is usually but not necessarily the main verb) with branches extending to the event attributes (which are usually named entities). Unlike the singular concepts (i.e., named entities such as ?person? or 
297
?location?) or linear relations (i.e., tuples such as ?Gates ? CEO ? Microsoft?), an event description consists of elements that form non-linear de-pendencies, which may not be apparent in the word order and therefore require syntactic and semantic analysis to extract. Furthermore, an ar-rangement of these elements in text can vary greatly from one event mention to the next, and there is usually other intervening material in-volved. Consequently, we construe event repre-sentation as a collection of paths linking the trigger to the attributes through the nodes of a parse tree1.  To create an event pattern (which will be part of an extraction rule), we generalize the depend-ency paths that connect the event trigger with each of the event key attributes (the roles). A dependency path consists of lexical and syntactic relations (POS and phrase dependencies), as well as semantic relations, such as entity tags (e.g., Person, Company, etc.) of event roles and word sense designations (based on Wordnet senses) of event triggers. In addition to the trigger-role paths (which we shall call the sub-patterns), an event pattern also contains the following: ? Event Type and Subtype ? which is inher-ited from seed examples; ? Trigger class ? an instance of the trigger must be found in text before any patterns are applied; ? Confidence score ? expected accuracy of the pattern established during training process; ? Context profile ? additional features col-lected from the context surrounding the event description, including references of other types of events near this event, in the same sentence, same paragraph, or ad-jacent paragraphs. We note that the trigger-attribute sub-patterns are defined over phrase structures rather than over linear text, as shown in Figure 2. In order to compose a complete event pattern, sub-patterns are collected across multiple mentions of the same-type event. 
                                                            1 Details of how to derive the skeletal tree representation are described in (Liu, 2009). 2 t ? the type of the event, w_pos ? the lemma of a word and its POS. 3 In this figure we omit the parse tree trimming step which was explained in the previous section. 
3.2 Designating the sense of event triggers  An event trigger may have multiple senses but only one of them is for the event representation. If the correct sense can be determined, we would be able to use its synonyms and hyponym as al-ternative event triggers, thus enabling extraction of more events. This, in turn, requires sense dis-ambiguation to be performed on the event trig-gers. In MUC evaluations, participating groups ( Yangarber and Grishman, 1998) used human experts to decide the correct sense of event trig-gers and then manually added correct synonyms to generalize event patterns. Although accurate, the process is time consuming and not portable to new domains. We developed a new approach for utilizing Wordnet to decide the correct sense of an event trigger. The method is based on the hypothesis that event triggers will share same sense when represent same type of event. For example, when the verbs, attack, assail, strike, gas, bomb, are trigger words of Conflict-Attack event, they share same sense. This process is described in the following steps: 1)  From training corpus, collect all triggers, which specify the lemma, POS tag, the type of event and get alpossible senses of them from Wordnet. 2)  Order the triggers by the trigger frequency TrF(t, w_pos),2 which is calculated by divid-ing number of times each word (w_pos) is used as a trigger for the event of type (t) by the total number of times this word occurs in the training corpus. Clearly, the greater trig-ger frequency of a word, the more discrimi-native it is as a trigger for the given type of event. When the senses of the triggers with high accuracy are defined, they can be the reference for the triggers in low accuracy. 3)  From the top of the trigger list, select the first none-sense defined trigger (Tr1) 4)  Again, beginning from the top of the trigger list, for every trigger Tr2 (other than Tr1), we look for a pair of compatible senses be-tween Tr1 and Tr2. To do so, traverse Syno-nym, Hypernym, and Hyponym links starting from the sense(s) of Tr2 (use either the sense already assigned to Tr2 if has or all its possi-ble senses) and see whether there are paths which can reach the senses of Tr1. If such converging paths exist, the compatible senses                                                             2 t ? the type of the event, w_pos ? the lemma of a word and its POS. 
Attacker:  <N(subj, PER): Attacker> <V(fire): trigger> Place:  <V(fire): trigger> <Prep> <N> <Prep(in)> <N(GPE): Place> Target:  <V(fire): trigger> <Prep(on)> <N(VEH): Target> Time-Within:<N(timex2): Time-Within><SentHead><V(fire): trigger> Figure 2. Trigger-attribute sub-patterns for key roles in a Conflict-Attack event pattern. 
298
are identified and assigned to Tr1 and Tr2 (if Tr2?s sense wasn?t assigned before). Then go back to step 3. However, if no such path ex-ist between Tr1 senses with other triggers senses, the first sense listed in Wordnet will be assigned to Tr1 This algorithm tries to assign the most proper sense to every trigger for one type of event. For example, the sense of fire as trigger of Conflict-Attack event is ?start firing a weapon?; while it is used in Personal-End_Position, its sense is ?ter-minate the employment of?. After the trigger sense is defined, we can expand event triggers by adding their synonyms and hyponyms during the event extraction. 3.3 Deriving initial rules from seed exam-ples  Extraction rules are construed as transformations from the event patterns derived from text onto a formal representation of an event. The initial rules are derived from a manually annotated training text corpus (seed data), supplied as part of an application task. Each rule contains the type of events it extracts, trigger, a list of role sub-patterns, and the confidence score obtained through a validation process (see section 3.6). Figure 3 shows an extraction pattern for the Con-flict-Attack event derived from the training cor-pus (but not validated yet)3.  3.4 Learning through pattern mutation  Given an initial set of extraction rules, a variety of pattern mutation techniques are applied to de-rive new patterns and new rules. This is done by selecting elements of previously learnt patterns, based on the history of partial matches and com-bining them into new patterns. This form of learning, which also includes conditional rule                                                             3 In this figure we omit the parse tree trimming step which was explained in the previous section. 
relaxation, is particularly useful for rapid adapta-tion of extraction capability to slightly altered, partly ungrammatical, or otherwise variant data.  The basic idea is as follows: the patterns ac-quired in prior learning iterations (starting with those obtained from the seed examples) are matched against incoming text to extract new events. Along the way there will be a number of partial matches, i.e., when no existing pattern fully matches a span of text. This may simply mean that no event is present; however, depend-ing upon the degree of the partial match we may also consider that a novel structural variant was found. BEAR would automatically test this hy-pothesis by attempting to construe a new pattern, out of the elements of existing patterns, in order to achieve a full match. If a match is achieved, the new ?mutated? pattern will be added to BEAR learned collection, subject to a validation step. The validation step (discussed later in this paper) is to assure that the added pattern would not introduce an unacceptable drop in overall system precision. Specific pattern mutation tech-niques include the following: ? Adding a role subpattern: When a pattern matches an event mention while there is a sufficient linguistic evidence (e.g., pres-ence of certain types of named entities) that additional roles may be present in text, then appropriate role subpatterns can be "imported" from other, non-matching patterns (Figure 4). ?  Replacing a role subpattern: When a pat-tern matches but for one role, the system can replace this role subpattern by another subpattern for the same role taken from a different pattern for the same event type. ?  Adding or replacing a trigger: When a pattern matches but for the trigger, a new trigger can be added if it either is already present in another pattern for the same event type or the syno-nym/hyponym/hypernym of the trigger (found in section 3.2). We should point out that some of the same ef-fects can be obtained by making patterns more general, i.e., adding "optional" attributes (i.e., optional sub-patterns), etc. Nonetheless, the pat-tern mutation is more efficient because it will automatically learn such generalization on an as-needed basis in an entirely data-driven fashion, while also maintaining high precision of the re-sulting pattern set. It is thus a more general method. Figure 4 illustrated the use of the ele-ments combination technique. In this example, 
 Figure 3. A Conflict-Attack event pattern derived from a positive example in the training corpus 
299
 Figure 5. A new extraction pattern is derived by iden-tifying an alternative trigger for an event. 
Pattern ID: 1286 Type: Conflict   Subtype: Attack Trigger:  attack_N Target:  <N(FAC): Target> <Prep(in)> <N(attack): trigger> Attacker:  <N(PER): Attacker> <V> <N> <Prep> <N> <Prep(in)> <N(attack): trigger> Time-Within: <N(attack): trigger> <E0> <V> <N(timex2): Time-within> Figure 5B. A new pattern is derived for event in Fig 5, with an attack as the trigger. 
Pattern ID: 1207 Type: Conflict    Subtype: Attack Trigger:  bombing_N Target:  <N(bombing): trigger> <Prep(of)> <N(FAC): Target>  Attacker:  <N(PER): Attacker> <V> <N(bombing): trigger>  Time-Within: <N(bombing): trigger> <Prep> <N> <Prep> <N> <E0> <V> <N(timex2): Time-within> Figure 5A. A pattern with the bombing trigger matches the event mention in Fig. 5. 
 Figure 4. Deriving a new pattern by importing a role from another pattern neither of the two existing patterns can fully match the new event description; however, by combining the first pattern with the Place role sub-pattern from the second pattern we obtain a new pattern that fully matches the text. While this adjustment is quite simple, it is nonetheless performed automatically and without any human assistance. The new pattern is then ?learned? by BEAR, subject to a verification step explained in a later section. 3.5 Learning by exploiting structural duali-ty  As the system reads through new text extracting more events using already learnt rules, each ex-tracted event mention is analyzed for presence of alternative trigger elements that can consistently predict the presence of a subset of events that includes the current one. Subsequently, an alter-native sub-pattern structure will be built with branches extending from the new trigger to the already identified attributes, as shown schemati-cally in Figure 5.  In this example, a Conflict-Attack-type event is extracted using a pattern (shown in Figure 5A) anchored at the ?bombing? trigger. Nonetheless, an alternative trigger structure is discovered, which is anchored at ?an attack? NP, as shown on the right side of Figure 5. This ?discovery? is based upon seeing the new trigger repeatedly ? it needs to ?explain? a subset of previously seen events to be adopted. The new trigger will prompt BEAR to derive additional event pat-terns, by computing alternative trigger-attribute paths in the dependency tree. The new pattern 
(shown in Figure 5B) is of course subject to con-fidence validation, after which it will be immedi-ately applied to extract more events.  Another way of getting at this kind of struc-tural duality is to exploit co-referential con-sistency within coherent spans of discourse, e.g., a single news article or a similar document. Such documents may contain references to multiple events, but when the same type of event is men-tioned along with the same attributes, it is more likely than not in reference to the same event.  This hypothesis is a variant of an argument ad-vanced in (Gale, et al2000) that a polysemous word used multiple times within a single docu-ment, is consistently used in the same sense. So if we extract an event mention (of type T) with trigger t in one part of a document, and then find that t occurs in another part of the same docu-ment, then we may assume that this second oc-currence of t has the same sense as the first. Since t is a trigger for an event of type T, we can hypothesize its subsequent occurrences indicate additional mentions of type T events that were not extracted by any of the existing patterns. Our objective is to exploit these unextracted mentions and then automatically generate additional event patterns. Indeed, Ji (2008) showed that trigger co-occurrence helps finding new mentions of the 
300
Pattern ID: -1 Type: Personnel  Subtype: End-Position Trigger: resign_V Person: <N(PER, subj): Person> <V(resign): trigger> Entity: <V(resign):trigger> <E0> <N(ORG): Entity> <N> <V> Figure 7A. A new pattern for End-Position learned by exploiting event co-reference. 
 Figure 7. Two event mentions have different triggers and sub-patterns structures  
 Figure 6. The probability of a sentence containing a mention of the same type of event within a single document same event; however, we found that if using enti-ty co-reference as another factor, more new men-tions could be identified when the trigger has low projected accuracy (Liu, 2009; Yu Hong, et al2011). Our experiments (Figure 64), which com-pared the triggers and the roles across all event mentions within each document on ACE training corpus, showed that when the trigger accuracy is 0.5 or higher, each of its occurrences within the document indicates an event mention of the same type with a very high probability (mostly > 0.9). For triggers with lower accuracy, this high prob-ability is only achieved when the two mentions share at least 60% of their roles, in addition to having a common trigger. Thus our approach uses co-occurrence of both trigger and event ar-gument for detecting new event mentions.  In Figure 7, an End-Position event is extracted from left sentence (L), with ?resign? as the trig-ger and ?Capek? and ?UBS? assigned Person and Entity roles, respectively5. The right sentence (R), taken from the same document, contains the same trigger word, ?resigned? and also the same 
                                                            4 The X-axis is the percentage of entities coreferred between the EVMs (Event mentions) and the SEs (Sentences); while the Y-axis shows the probability that the SE contains a men-tion that is the same type as the EVM. 5 Entity is the employer in the event 
entities, ?Howard G. Capek? and ?UBS?. The projected accuracy of resign_V as an End-Position trigger is 0.88. With 100% argument overlap rate, we estimate the probability that sen-tence R contains an event mention of the same type as sentence L (and in fact co-referential mention) at 97% (We set 80% as the threshold). Thus a new event mention is found and a new pattern for End-Position is automatically derived from R, as shown in Figure 7A. 3.6 Pattern validation  Extraction patterns are validated after each learn-ing cycle against the already annotated data. In the first supervised learning step, patterns accu-racy is tested against the training corpus based on the similarity between the extracted events and human annotated events:  ? A Full match is achieved when the event type is correctly identified and all its roles are correctly matched. A full credit is added to the pattern score. ? A Partial match is achieved when the event type is correctly identified but only a subset of roles is correctly extracted. A partial score, which is the ratio of the matched roles to the whole roles, is add-ed. ? A False Alarm occurs when a wrong type of event is extracted (including when no event is present in text). No credit is add-ed to the pattern score. In the subsequent steps, the validation is ex-tended over parts of the unannotated corpus. In Riloff (1996) and Sudo et al(2001), the pattern accuracy is mainly dependent on its occurrences in the relevant documents6 vs. the whole corpus. However, one document may contain multiple types of events, thus we set a more restricted val-idation measure on new rules: ? Good Match If a new rule ?rediscovers? already extracted events of the same type, then it will be counted as either a Full Match or Partial Match based on previ-ous rules ? Possible Match If an already extracted event of same type of a rule contains same entities and trigger as the candidate extracted by the rule. This candidate is a possible match, so it will get a partial 
                                                            6 If a document contains same type of events extracted from previous steps, the document is a relevant document to the pattern. 
301
Victim pattern: <N(obj, PER): Victim> <V(kill): trigger> (Life-Die) Projected Accuracy: 0.9390243902439024 Number of negative matches: 5 Number of Positive matches: 77  Attacker pattern: <N(subj, PE/PER/ORG): Attacker> <V> <V(use): trigger>  (Conflict-Attack) Projected Accuracy: 0.025210084033613446  Number of negative matches: 116  Number of positive matches: 3  Attacker pattern: <N(subj, GPE/PER): Attacker> <V(attack): trig-ger>  (Conflict-Attack) Projected Accuracy: 0.4166666666666667  Number of negative matches: 7  Number of positive matches: 5 categories of posi-tive matches: GPE: 4  GPE_Nation: 4  PER: 1 PER_Individual: 1 categories of nega-tive matches: GPE: 1  GPE_Nation: 1  PER: 6  PER_Group: 1 PER_Individual: 5 Figure 9. sub-patterns with projected accuracy scores 
Event id: 27 from: sample Projected Accuracy: 0.1765 Adjusted Projected Accuracy: 0.91 Type: Justice Subtype: Arrest-Jail Trigger: capture Person sub-pattern:  <N(obj, PER): Person> <V(capture): trigger> Co-occurrence ratio: {para_Conflict_Demonstrate=100%,  ?} Mutually exclusive ratio: {sent_Conflict_Attack=100%, pa-ra_Conflict_Attack=96.3%,  ?} Figure 8. An Arrest-Jail pattern with context profile information score based on the statistics result from Figure 6. ? False Alarm If a new rule picks up an al-ready extracted event in different type Thus, event patterns are validated for overall expected precision by calculating the ratio of positive matches to all matches against known events. This produces pattern confidence scores, which are used to decide if a pattern is to be learned or not. Learning only the patterns with sufficiently high confidence scores helps to guard the bootstrapping process from spinning off track; nonetheless, the overall objective is to maximize the performance of the resulting set of extraction rules, particularly by expanding its recall rate. For the patterns where the projected accuracy score falls under the cutoff threshold, we may still be able to make some ?repairs? by taking into account their context profile. To do so, we applied a similar approach as (Liao, 2010), which showed that some types of events can appeared frequently with each other. We collected all the matches produced by such a failed pattern and created a list of all other events that occur in their immediate vicinity: in the same sentence, as well as the sentences before and after it7. These other events, of different types and detected by differ-ent patterns, may be seen as co-occurring near the target event: these that co-occur near positive matches of our pattern will be added to the posi-tive context support of this pattern; conversely, events co-occurring near false alarms will be added to the negative context support for this pattern. By collecting such contextual infor-mation, we can find contextually-based indica-tors and non-indicators for occurrence of event mentions. When these extra constraints are in-cluded in a previously failed pattern, its projected 
                                                            7 If a known event is detected in the same sentence (sent_?), the same paragraph (para_?), or an adjacent paragraph (adj_para_...) as the candidate event, it be-comes an element of the pattern context support. 
accuracy is expected to increase, in some cases above the threshold.  For example, the pattern in Figure 8 has an in-itially low projected accuracy score; however, we find that positive matches of this pattern show a very high (100% in fact) degree of correlation with mentions of Demonstrate events. Therefore, limiting the application of this pattern to situa-tions where a Justice-Arrest-Jail event is men-tioned in a nearby text improves its projected accuracy to 91%, which is well above the re-quired threshold.  In addition to the confidence rate of each new pattern, we also calculate projected accuracy of each of the role sub-patterns, because they may be used in the process of detecting new patterns, and it will be necessary to score partial matches, as a function confidence weights for pattern components. To validate a sub-pattern we apply it to the training corpus and calculate its project-ed accuracy score by dividing the number of cor-rectly matched roles by the total number of matches returned. The projected accuracy score will tell us how well a sub-pattern can distin-guish a specific event role from other infor-mation, when used independently from other elements of the complete pattern. Figure 9 shows three sub-pattern examples. The first sub-pattern extracts the Victim role in a Life-Die event with very high projected accuracy. This sub-pattern is also a good candidate for generations of additional patterns for this type of event, a process which we describe in section D. The second sub-pattern was built to extract the Attacker role in Conflict-Attack events, but it has very low projected accuracy. The third one shows another Attacker sub-pattern whose pro-jected accuracy score is 0.417 after the first step 
302
 Figure 10. BEAR cross-validated scores 
Table 1. Sub-patterns whose projected accuracy is significantly increased after noisy samples are removed Sub-patterns Projected Accuracy Additional con-straints Revised Accu-racy Movement-Transport: <N(obj, PER/VEH): Artifact> <V(send): trigger> 0.475 removing PER 0.667 <V(bring): trigger> <N(obj)> <Prep = to> <N(FAC/GPE): Destina-tion> 0.375 removing GPE 1.0 ?    Conflict Attack: <N(PER/ORG/GPE):Attacker><N(attack):trigger> 0.682 removing PER 0.8 <N(subj,GPE/PER):Attacker><V(attack): trigger> 0.417 removing GPE 0.8 <N(obj,VEH/PER/FAC):Target><V(target):trigger> 0.364 removing PER_Individual 0.667 ?     
 Figure 11. BEAR?s unsupervised learning curve. 
in validation process. This is quite low; however, it can be repaired by constraining its entity type to GPE. This is because we note that with a GPE entity, the subpattern is 80% on target, while with PER entity it is 85% a false alarm. After this sub-pattern is restricted to GPE its projected accuracy becomes 0.8. Table 1 lists example sub-patterns for which the projected accuracy increases significantly after adding more constrains. When the projected accuracy of a sub-pattern is improved, all pat-terns containing this sub-pattern will also im-prove their projected accuracy. If the adjusted projected accuracy rises above the predefined threshold, the repaired pattern will be saved.  In the following section, we will discuss the experiments conducted to evaluate the perfor-mance of the techniques underlying BEAR: how effectively it can learn and how accurately it can perform its extraction task. 4 Evaluation  We test the system learning effectiveness by comparing its performance immediately follow-ing the first iteration (i.e., using rules derived from the training data) with its performance after N cycles of unsupervised learning. We split ACE training corpus 8  randomly into 5 folders and trained BEAR on the four folders and evaluated it on the left one. Then, we did 5 fold cross vali-dation. Our experiments showed that BEAR 
                                                            8 ACE training data contains 599 documents from news, weblog, usenet, and conversational telephone speech. Total 33 types of events are defined in ACE corpus.  
reached the best cross-validated score, 66.72%, when pattern accuracy threshold is set at 0.5. The highest score of single run is 67.62%. In the fol-lowing of this section, we will use results of one single run to display the learning behavior of BEAR.  In Figure 10, X-axis shows values of the learning threshold (in descending order), while Y-axis is the average F-score achieved by the automatically learned patterns for all types of events against the test corpus. The red (lower) line represents BEAR?s base run immediately after the first iteration (supervised learning step); the blue (upper) line represents BEAR?s perfor-mance after an additional 10 unsupervised learn-ing cycles9 are completed. We note that the final performance of the bootstrapped system steadily increases as the learning threshold is lowered, peaking at about 0.5 threshold value, and then declines as the threshold value is further de-creased, although it remains solidly above the base run. Analyzing more closely a few selected points on this chart we note, for example, that the base run at threshold of 0 has F-score of 34.5%, which represents 30.42% recall, 40% precision. On the other end of the curve, at the threshold of 0.9, the base run precision is 91.8% but recall at only 21.5%, which produces F-score of 34.8%. It is interesting to observe that at neither of these two extremes the system learning effectiveness is particularly good, and is significantly less than at 
                                                            9 The learning process for one type of event will stop when no new patterns can be generated, so the number of learning cycles for each event type is different. The highest number of learning cycles is 10 and lowest one is 2. 
303
Table 2. BEAR performance following different selections of learning steps  Precision Recall F-score Base1 0.89 0.22 0.35 Base2 0.87 0.28 0.42 All 0.84 0.56 0.67 PMM 0.84 0.48 0.61 CBM 0.86 0.37 0.52  
 Figure 13. Event mention extraction after learning: recall for each type of event 
 Figure 12. Event mention extraction after learning: preci-sion for each type of event 
the median threshold of 0.5 (based on the exper-iments conducted thus far), where the system performance improves from 42% to 66.86% F-score, which represents 83.9% precision and 55.57% recall.   Figure 11 explains BEAR?s learning effec-tiveness at what we determined empirically to be the optimal confidence threshold (0.5) for pattern acquisition. We note that the performance of the system steadily increases until it reaches a plat-eau after about 10 learning cycles.  Figure 12 and Figure 13 show a detailed breakdown of BEAR extraction performance after 10 learning cycles for different types of events. We note that while precision holds steady across the event types, recall levels vary signifi-cantly. The main reason for low recall in some types of events is the failure to find a sufficient number of high-confidence patterns. This may point to limitations of the current pattern discov-ery methods and may require new ways of reach-ing outside of the current feature set. In the previous section we described several learning methods that BEAR uses to discover, validate and adapt new event extraction rules. Some of them work by manipulating already learnt patterns and adapting them to new data in order to create new patterns, and we shall call these pattern-mutation methods (PMM). Other described methods work by exploiting a broader linguistic context in which the events occur, or context-based methods (CBM). CB methods look for structural duality in text surrounding the events and thus discover alternative extraction patterns.  In Table 2, we report the results of running BEAR with each of these two groups of learning methods separately and then in combination to 
see how they contribute to the end performance. Base1 and Base2 showed the result without and with adding trigger synonyms in event extrac-tion. By introducing trigger synonyms, 27% more good events were extracted at the first it-eration and thus, BEAR had more resources to use in the unsupervised learning steps.  The ALL is the combination of PMM and CBM, which demonstrate both methods have the contribution to the final results. Furthermore, as explained before, new extraction rules are learned in each iteration cycle based on what was learned in prior cycles and that new rules are adopted only after they are tested for their pro-jected accuracy (confidence score), so that the overall precision of the resulting rule set is main-tained at a high level relative to the base run. 5 Conclusion and future work  In this paper, we presented a semi-supervised method for learning new event extraction pat-terns from un-annotated text. The techniques de-scribed here add significant new tools that increase capabilities of information extraction technology in general, and more specifically, of systems that are built by purely supervised meth-ods or from manually designed rules. Our eval-uation using ACE dataset demonstrated that that bootstrapping can be effectively applied to learn-ing event extraction rules for 33 different types of events and that the resulting system can out-perform supervised system (base run) significant-ly.  Some follow-up research issues include: ? New techniques are needed to recognize event descriptions that still evade the cur-rent pattern derivation techniques, espe-cially for the events defined in Personnel, Business, and Transactions classes. ? Adapting the bootstrapping method to ex-tract events in a different language, e.g. Chinese or Arabic. ? Expanding this method to extraction of larger ?scenarios?, i.e., groups of correlat-ed events that form coherent ?stories? of-ten described in larger sections of text, e.g., an event and its immediate conse-quences. 
304
References  Agichtein, E. and Gravano, L. 2000. Snowball: Extracting Relations from Large Plain-Text Collections. In Proceedings of the Fifth ACM International Conference on Digital Libraries  Gale, W. A., Church, K. W., and Yarowsky, D. 1992. One sense per discourse. In Proceedings of the workshop on Speech and Natural Lan-guage, 233-237. Harriman, New York: Asso-ciation for Computational Linguistics.  Hong, Y., Zhang, J., Ma, B., Yao, J., Zhou, G., and Zhu, Q,. 2011. Using Cross-Entity Infer-ence to Improve Event Extraction. In Proceed-ings of the Annual Meeting of the Association of Computational Linguistics (ACL 2011). Portland, Oregon, USA. Ji, H. and Grishman, R. 2008. Refining Event Extraction Through Unsupervised Cross-document Inference. In Proceedings of the Annual Meeting of the Association of Compu-tational Linguistics (ACL 2008).Ohio, USA. Liao, S. and Grishman R. 2010. Using Document Level Cross-Event Inference to Improve Event Extraction. In Proc. ACL-2010, pages 789-797, Uppsala, Sweden, July. Lin, D. 1998. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing System, Granada, Spain. Liu Ting. 2009. BEAR: Bootstrap Event and Re-lations from Text. Ph.D. Thesis Riloff, E. 1996. Automatically Generating Ex-traction Patterns from Untagged Text. In Pro-ceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044?1049. The AAAI Press/MIT Press. Sudo, K., Sekine, S., Grishman, R. 2001. Auto-matic Pattern Acquisition for Japanese Infor-mation Extraction. In Proceedings of Human Language Technology Conference (HLT2001). Sudo, K., Sekine, S., Grishman, R. 2003. An im-proved extraction pattern representation model for automatic IE pattern acquisition. Proceed-ings of ACL 2003 , 224 ? 231. Tokyo. Strzalkowski, T., and Wang, J. 1996. A self-learning universal concept spotter. In Proceed-ings of the 16th conference on Computational linguistics - Volume 2, 931-936, Copenhagen, Denmark: Association for Computational Lin-guistics 
Thelen, M., Riloff, E. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10. 214-222. Morristown, NJ: Association for Computational Linguistics    Xu, F., Uszkoreit, H., &amp; Li, H. (2007). A seed-driven bottom-up machine learning framework for extracting relations of various complexity. In Proc. of the 45th Annual Meet-ing of the Association of Comp. Linguistics, pp. 584?591, Prague, Czech Republic. Yangarber, R., and Grishman, R. 1998. NYU: Description of the Proteus/PET System as Used for MUC-7 ST. In Proceedings of the 7th conference on Message understanding. Yangarber, R., Grishman, R., Tapanainen, P., and Huttunen, S.  2000. Unsupervised discov-ery of scenario-level patterns for information extraction. In Proceedings of the Sixth Confer-ence on Applied Natural Language Pro-cessing, (ANLP-NAACL 2000), 282-289  Yarowsky, D. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, 189-196, Cambridge, Massachusetts: Associa-tion for Computational Linguistics    
305
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 171?179,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Multi-Modal Annotation of Quest Games in Second Life  
 
Sharon Gower Small, Jennifer Stromer-Galley and Tomek Strzalkowski 
ILS Institute 
State University of New York at Albany 
Albany, NY 12222 
small@albany.edu, jstromer@albany.edu, tomek@albany.edu  
 
 
 
Abstract 
We describe an annotation tool developed to as-
sist in the creation of multimodal action-
communication corpora from on-line massively 
multi-player games, or MMGs. MMGs typically 
involve groups of players (5-30) who control 
their avatars1, perform various activities (quest-
ing, competing, fighting, etc.) and communicate 
via chat or speech using assumed screen names. 
We collected a corpus of 48 group quests in 
Second Life that jointly involved 206 players 
who generated over 30,000 messages in quasi-
synchronous chat during approximately 140 
hours of recorded action. Multiple levels of co-
ordinated annotation of this corpus (dialogue, 
movements, touch, gaze, wear, etc) are required 
in order to support development of automated 
predictors of selected real-life social and demo-
graphic characteristics of the players. The anno-
tation tool presented in this paper was developed 
to enable efficient and accurate annotation of all 
dimensions simultaneously. 
 
1 Introduction 
The aim of our project is to predict the real world 
characteristics of players of massively-multiplayer 
online games, such as Second Life (SL). We sought 
to predict actual player attributes like age or educa-
tion levels, and personality traits including leader-
ship or conformity. Our task was to do so using 
only the behaviors, communication, and interaction 
among the players produced during game play. To 
do so, we logged all players? avatar movements, 
                                                          
1 All avatar names seen in this paper have been changed to 
protect players? identities.  
?touch events? (putting on or taking off clothing 
items, for example), and their public chat messages 
(i.e., messages that can be seen by all players in the 
group). Given the complex nature of interpreting 
chat in an online game environment, we required a 
tool that would allow annotators to have a synchro-
nized view of both the event action as well as the 
chat utterances.  This would allow our annotators to 
correlate the events and the chat by marking them 
simultaneously. More importantly, being able to 
view game events enables more accurate chat anno-
tation; and conversely, viewing chat utterances 
helps to interpret the significance of certain events 
in the game, e.g., one avatar following another.  For 
example, an exclamation of: ?I can?t do it!? could 
be simply a response (rejection) to a request from 
another player; however, when the game action is 
viewed and the speaker is seen attempting to enter a 
building without success, another interpretation 
may arise (an assertion, a call for help, etc.).  
The Real World (RW) characteristics of SL 
players (and other on-line games) may be inferred 
to varying degrees from the appearance of their 
avatars, the behaviors they engage in, as well as 
from their on-line chat communications. For exam-
ple, the avatar gender generally matches the gender 
of the owner; on the other hand, vocabulary choices 
in chat are rather poor predictors of a player?s age, 
even though such correlation is generally seen in 
real life conversation.  
Second Life2 was the chosen platform because 
of the ease of creating objects, controlling the play 
environment, and collecting players? movement, 
chat, and other behaviors. We generated a corpus of 
chat and movement data from 48 quests comprised 
of 206 participants who generated over 30,000 
                                                          
2 An online Virtual World developed and launched in 2003, by 
Linden Lab, San Francisco, CA.  http://secondlife.com 
171
messages and approximately 140 hours of recorded 
action. We required an annotation tool to help us 
efficiently annotate dialogue acts and communica-
tion links in chat utterances as well as avatar 
movements from such a large corpus.  Moreover, 
we required correlation between these two dimen-
sions of chat and movement since movement and 
other actions may be both causes and effects of 
verbal communication. We developed a multi-
modal event and chat annotation tool (called RAT, 
the Relational Annotation Tool), which will simul-
taneously display a 2D rendering of all movement 
activity recorded during our Second Life studies, 
synchronized with the chat utterances. In this way 
both chat and movements can be annotated simul-
taneously: the avatar movement actions can be re-
viewed while making dialogue act annotations.  
This has the added advantage of allowing the anno-
tator to see the relationships between chat, behav-
ior, and location/movement. This paper will 
describe our annotation process and the RAT tool. 
2 Related Work 
Annotation tools have been built for a variety of 
purposes. The CSLU Toolkit (Sutton et al, 1998) is 
a suite of tools used for annotating spoken lan-
guage. Similarly, the EMU System (Cassidy and 
Harrington, 2001) is a speech database management 
system that supports multi-level annotations.  Sys-
tems have been created that allow users to readily 
build their own tools such as AGTK (Bird et al, 
2001).  The multi-modal tool DAT (Core and Al-
len, 1997) was developed to assist testing of the 
DAMSL annotation scheme.  With DAT, annota-
tors were able to listen to the actual dialogues as 
well as view the transcripts. While these tools are 
all highly effective for their respective tasks, ours is 
unique in its synchronized view of both event ac-
tion and chat utterances. 
Although researchers studying online communi-
cation use either off-the shelf qualitative data anal-
ysis programs like Atlas.ti or NVivo, a few studies 
have annotated chat using custom-built tools. One 
approach uses computer-mediated discourse analy-
sis approaches and the Dynamic Topic Analysis 
tool (Herring, 2003; Herring & Nix; 1997; Stromer-
Galley & Martison, 2009), which allows annotators 
to track a specific phenomenon of online interaction 
in chat: topic shifts during an interaction. The 
Virtual Math Teams project (Stahl, 2009) created a 
ated a tool that allowed for the simultaneous play-
back of messages posted to a quasi-synchronous 
discussion forum with whiteboard drawings that 
student math team members used to illustrate their 
ideas or visualize the math problem they were try-
ing to solve (?akir, 2009).  
A different approach to data capture of complex 
human interaction is found in the AMI Meeting 
Corpus (Carletta, 2007). It captures participants? 
head movement information from individual head-
mounted cameras, which allows for annotation of 
nodding (consent, agreement) or shaking (dis-
agreement), as well as participants? locations within 
the room; however, no complex events involving 
series of movements or participant proximity are 
considered. We are unaware of any other tools that 
facilitate the simultaneous playback of multi-modes 
of communication and behavior.  
3 Second Life Experiments 
To generate player data, we rented an island in 
Second Life and developed an approximately two 
hour quest, the Case of the Missing Moonstone.  In 
this quest, small groups of 4 to 5 players, who were 
previously unacquainted, work their way together 
through the clues and puzzles to solve a murder 
mystery. We recruited Second Life players in-game 
through advertising and setting up a shop that inter-
ested players could browse. We also used Facebook 
ads, which were remarkably effective.  
The process of the quest experience for players 
started after they arrived in a starting area of the 
island (the quest was open only to players who 
were made temporary members of our island) 
where they met other players, browsed quest-
appropriate clothing to adorn their avatars, and re-
ceived information from one of the researchers. 
Once all players arrived, the main quest began, 
progressing through five geographic areas in the 
island. Players were accompanied by a ?training 
sergeant?, a researcher using a robot avatar, that 
followed players through the quest and provided 
hints when groups became stymied along their in-
vestigation but otherwise had little interaction with 
the group.  
The quest was designed for players to encounter 
obstacles that required coordinated action, such as 
all players standing on special buttons to activate a 
door, or the sharing of information between players, 
such as solutions to a word puzzle, in order to ad-
vance to the next area of the quest (Figure 1). 
172
Slimy Roastbeef: ?who?s got the square gear?? 
Kenny Superstar: ?I do, but I?m stuck? 
Slimy Roastbeef: ?can you hand it to me?? 
Kenny Superstar: ?i don?t know how? 
Slimy Roastbeef: ?open your inventory, click 
and drag it onto me? 
 
Figure 1: Excerpt of dialogue during a coor-
dination activity 
Quest activities requiring coordination among the 
players were common and also necessary to ensure 
a sufficient degree of movement and message traf-
fic to provide enough material to test our predic-
tions, and to allow us to observe particular social 
characteristics of players. Players answered a sur-
vey before and then again after the quest, providing 
demographic and trait information and evaluating 
other members of their group on the characteristics 
of interest. 
3.1 Data Collection 
We recorded all players? avatar movements as they 
purposefully moved avatars through the virtual 
spaces of the game environment, their public chat, 
and their ?touch events?, which are the actions that 
bring objects out of player inventories, pick up ob-
jects to put in their inventories, or to put objects, 
such as hats or clothes, onto the avatars, and the 
like. We followed Yee and Bailenson?s (2008) 
technical approach for logging player behavior. To 
get a sense of the volume of data generated, 206 
players generated over 30,000 messages into the 
group?s public chat from the 48 sessions.  We com-
piled approximately 140 hours of recorded action. 
The avatar logger was implemented to record each 
avatar?s location through their (x,y,z) coordinates, 
recorded at two second intervals. This information 
was later used to render the avatar?s position on our 
2D representation of the action (section 4.1).   
4 RAT 
The Relational Annotation Tool (RAT) was built to 
assist in annotating the massive collection of data 
collected during the Second Life experiments.  A 
tool was needed that would allow annotators to see 
the textual transcripts of the chat while at the same 
time view a 2D representation of the action.  Addi-
tionally, we had a textual transcript for a select set 
of events: touch an object, stand on an object, at-
tach an object, etc., that we needed to make avail-
able to the annotator for review.  
These tool characteristics were needed for 
several reasons. First, in order to fully understand 
the communication and interaction occurring be-
tween players in the game environment and accu-
rately annotate those messages, we needed 
annotators to have as much information about the 
context as possible. The 2D map coupled with the 
events information made it easier to understand. 
For example, in the quest, players in a specific 
zone, encounter a dead, maimed body. As annota-
tors assigned codes to the chat, they would some-
times encounter exclamations, such as ?ew? or 
?gross?. Annotators would use the 2D map and the 
location of the exclaiming avatar to determine if the 
exclamation was a result of their location (in the 
zone with the dead body) or because of something 
said or done by another player. Location of avatars 
on the 2D map synchronized with chat was also 
helpful for annotators when attempting to disam-
biguate communicative links. For example, in one 
subzone, mad scribblings are written on a wall. If 
player A says ?You see that scribbling on the 
wall?? the annotator needs to use the 2D map to see 
who the player is speaking to. If player A and 
player C are both standing in that subzone, then the 
annotator can make a reasonable assumption that 
player A is directing the question to player C, and 
not player B who is located in a different subzone. 
Second, we annotated coordinated avatar move-
ment actions (such as following each other into a 
building or into a room), and the only way to read-
ily identify such complex events was through the 
2D map of avatar movements. 
The overall RAT interface, Figure 2, allows 
the annotator to simultaneously view all modes of 
representation.  There are three distinct panels in 
this interface. The left hand panel is the 2D repre-
sentation of the action (section 4.1).  The upper 
right hand panel displays the chat and event tran-
scripts (section 4.2), while the lower right hand por-
tion is reserved for the three annotator sub-panels 
(section 4.3).   
 
173
Figure 2: RAT interface 
4.1 The 2D Game Representation 
The 2D representation was the most challenging of 
the panels to implement.  We needed to find the 
proper level of abstraction for the action, while 
maintaining its usefulness for the annotator.  Too 
complex a representation would cause cognitive 
overload for the annotator, thus potentially deterio-
rating the speed and quality of the annotations.  
Conversely, an overly abstract representation would 
not be of significant value in the annotation proc-
ess.   
There were five distinct geographic areas on our 
Second Life Island: Starting Area, Mansion, Town 
Center, Factory and Apartments. An overview of 
the area in Second Life is displayed in Figure 3. We 
decided to represent each area separately as each 
group moves between the areas together, and it was 
therefore never necessary to display more than one 
area at a time.  The 2D representation of the Man-
sion Area is displayed in Figure 4 below.  Figure 5 
is an exterior view of the actual Mansion in Second 
Life. Each area?s fixed representation was rendered 
using Java Graphics, reading in the Second Life 
(x,y,z) coordinates from an XML data file. We rep-
resented the walls of the buildings as connected 
solid black lines with openings left for doorways.  
Key item locations were marked and labeled, e.g. 
Kitten, maid, the Idol, etc. Even though annotators 
visited the island to familiarize themselves with the 
layout, many mansion rooms were labeled to help 
the annotator recall the layout of the building, and 
minimize error of annotation based on flawed re-
call. Finally, the exact time of the action that is cur-
rently being represented is displayed in the lower 
left hand corner. 
 
 
 
Figure 3: Second Life overview map 
  
174
 
 
Figure 4: 2D representation of Second Life action 
inside the Mansion/Manor 
 
 
 
 
Figure 5: Second Life view of Mansion exterior 
 
Avatar location was recorded in our log files as an 
(x,y,z) coordinate at a two second interval.  Avatars 
were represented in our 2D panel as moving solid 
color circles, using the x and y coordinates. A color 
coded avatar key was displayed below the 2D rep-
resentation.  This key related the full name of every 
avatar to its colored circle representation. The z 
coordinate was used to determine if the avatar was 
on the second floor of a building.  If the z value 
indicated an avatar was on a second floor, their icon 
was modified to include the number ?2? for the du-
ration of their time on the second floor. Also logged 
was the avatar?s degree of rotation.  Using this we 
were able to represent which direction the avatar 
was looking by a small black dot on their colored 
circle. 
As the annotators stepped through the chat and 
event annotation, the action would move forward, 
in synchronized step in the 2D map.  In this way at 
any given time the annotator could see the avatar 
action corresponding to the chat and event tran-
scripts appearing in the right panels.  The annotator 
had the option to step forward or backward through 
the data at any step interval, where each step corre-
sponded to a two second increment or decrement, to 
provide maximum flexibility to the annotator in 
viewing and reviewing the actions and communica-
tions to be annotated.  Additionally, ?Play? and 
?Stop? buttons were added to the tool so the anno-
tator may simply watch the action play forward ra-
ther than manually stepping through. 
4.2 The Chat & Event Panel 
Avatar utterances along with logged Second Life 
events were displayed in the Chat and Event Panel 
(Figure 6). Utterances and events were each dis-
played in their own column.  Time was recorded for 
every utterance and event, and this was displayed in 
the first column of the Chat and Event Panel. All 
avatar names in the utterances and events were 
color coded, where the colors corresponded to the 
avatar color used in the 2D panel. This panel was 
synchronized with the 2D Representation panel and 
as the annotator stepped through the game action on 
the 2D display, the associated utterances and events 
populated the Chat and Event panel. 
 
175
 
 
Figure 6: Chat & Event Panel  
4.3 The Annotator Panels  
The Annotator Panels (Figures 7 and 10) contains 
all features needed for the annotator to quickly 
annotate the events and dialogue. Annotators could 
choose from a number of categories to label each 
dialogue utterance. Coding categories included 
communicative links, dialogue acts, and selected 
multi-avatar actions.   In the following we briefly 
outline each of these. A more detailed description 
of the chat annotation scheme is available in 
(Shaikh et al, 2010).   
4.3.1 Communicative Links 
One of the challenges in multi-party dialogue is to 
establish which user an utterance is directed to-
wards. Users do not typically add addressing in-
formation in their utterances, which leads to 
ambiguity while creating a communication link be-
tween users. With this annotation level, we asked 
the annotators to determine whether each utterance 
was addressed to some user, in which case they 
were asked to mark which specific user it was ad-
dressed to; was in response to another prior utter-
ance by a different user, which required marking 
the specific utterance responded to; or a continua-
tion of the user?s own prior utterance.  
Communicative link annotation allows for accu-
rate mapping of dialogue dynamics in the multi-
party setting, and is a critical component of tracking 
such social phenomena as disagreements and lead-
ership. 
4.3.2 Dialogue Acts 
We developed a hierarchy of 19 dialogue acts for 
annotating the functional aspect of the utterance in 
the discussion.  The tagset we adopted is loosely 
based on DAMSL (Allen & Core, 1997) and 
SWBD (Jurafsky et al, 1997), but greatly reduced 
and also tuned significantly towards dialogue 
pragmatics and away from more surface character-
istics of utterances. In particular, we ask our anno-
tators what is the pragmatic function of each 
utterance within the dialogue, a decision that often 
depends upon how earlier utterances were classi-
fied. Thus augmented, DA tags become an impor-
tant source of evidence for detecting language uses 
and such social phenomena as conformity. Exam-
ples of dialogue act tags include Assertion-Opinion, 
Acknowledge, Information-Request, and Confirma-
tion-Request. 
Using the augmented DA tagset alo presents a 
fairly challenging task to our annotators, who need 
to be trained for many hours before an acceptable 
rate of inter-annotator agreement is achieved. For 
this reason, we consider our current DA tagging as 
a work in progress. 
4.3.3 Zone coding 
Each of the five main areas had a correspond-
ing set of subzones. A subzone is a building, a 
room within a building, or any other identifiable 
area within the playable spaces of the quest, e.g. the 
Mansion has the subzones: Hall, Dining Room, 
Kitchen, Outside, Ghost Room, etc. The subzone 
was determined based on the avatar(s) (x,y,z) coor-
dinates and the known subzone boundaries. This 
additional piece of data allowed for statistical 
analysis at different levels: avatar, dialogue unit, 
and subzone. 
 
176
 
 
Figure 7: Chat Annotation Sub-Panel
4.3.4 Multi-avatar events 
As mentioned, in addition to chat we also were in-
terested in having the annotators record composite 
events involving multiple avatars over a span of 
time and space.  While the design of the RAT tool 
will support annotation of any event of interest with 
only slight modifications, for our purposes, we 
were interested in annotating two types of events 
that we considered significant for our research hy-
potheses. The first type of event was the multi-
avatar entry (or exit) into a sub-zone, including the 
order in which the avatars moved.  
Figure 8 shows an example of a ?Moves into 
Subzone? annotation as displayed in the Chat & 
Event Panel. Figure 9 shows the corresponding se-
ries of progressive moments in time portraying en-
try into the Bank subzone as represented in RAT. In 
the annotation, each avatar name is recorded in or-
der of its entry into the subzone (here, the Bank).  
Additionally, we record the subzone name and the 
time the event is completed3.  
The second type of event we annotated was the 
?follow X? event, i.e., when one or more avatars 
appeared to be following one another within a sub-
zone. These two types of events were of particular 
interest because we hypothesized that players who 
are leaders are likely to enter first into a subzone 
and be followed around once inside.  
In addition, support for annotation of other types 
of composite events can be added as needed; for 
example, group forming and splitting, or certain 
                                                          
3 We are also able to record the start time of any event but for 
our purposes we were only concerned with the end time. 
joint activities involving objects, etc. were fairly 
common in quests and may be significant for some 
analyses (although not for our hypotheses). 
For each type of event, an annotation subpanel is 
created to facilitate speedy markup while minimiz-
ing opportunities for error (Figure 10).  A ?Moves 
Into Subzone? event is annotated by recording the  
ordinal (1, 2, 3, etc.) for each avatar.  Similarly, a 
?Follows? event is coded as avatar group ?A? fol-
lows group ?B?, where each group will contain one 
or more avatars. 
 
 
 
Figure 8: The corresponding annotation for Figure 
9 event, as displayed in the Chat & Event Panel 
5 The Annotation Process 
To annotate the large volume of data generated 
from the Second Life quests, we developed an an-
notation guide that defined and described the anno-
tation categories and decision rules annotators were 
to follow in categorizing the data units (following 
previous projects (Shaikh et al, 2010). Two stu-
dents were hired and trained for approximately 60 
hours, during which time they learned how to use 
the annotation tool and the categories and rules for 
the annotation process. After establishing a satisfac-
tory level of interrater reliability (average Krippen-
dorff?s alpha of all measures was <0.8. 
Krippendorff?s alpha accounts for the probability of 
177
chance agreement and is therefore a conservative 
measure of agreement), the two students then anno-
tated the 48 groups over a four-month period. It 
took approximately 230 hours to annotate the ses-
sions, and they assigned over 39,000 dialogue act 
tags. Annotators spent roughly 7 hours marking up 
the movements and chat messages per 2.5 hour 
quest session. 
 
 
Figure 9: A series of progressive moments in time portraying avatar entry into the Bank subzone 
  
 
 
Figure 10: Event Annotation Sub-Panel, currently showing the ?Moves Into Subzone? event from 
figure 9, as well as: ?Kenny follows Elliot in Vault?
5.1 The Annotated Corpus 
The current version of the annotated corpus consists 
of thousands of tagged messages including: 4,294 
action-directives, 17,129 assertion-opinions, 4,116 
information requests, 471 confirmation requests, 
394 offer-commits, 3,075 responses to information 
requests, 1,317 agree-accepts, 215 disagree-rejects, 
and 2,502 acknowledgements, from 30,535 pre-
split utterances (31,801 post-split). We also as-
signed 4,546 following events.  
6 Conclusion 
In this paper we described the successful imple-
mentation and use of our multi-modal annotation 
tool, RAT.  Our tool was used to accurately and 
simultaneously annotate over 30,000 messages and 
approximately 140 hours of action.  For each hour 
spent annotating, our annotators were able to tag 
approximately 170 utterances as well as 36 minutes 
of action. 
The annotators reported finding the tool highly 
functional and very efficient at helping them easily 
assign categories to the relevant data units, and that 
they could assign those categories without produc-
ing too many errors, such as accidentally assigning 
the wrong category or selecting the wrong avatar. 
The function allowing for the synchronized play-
back of the chat and movement data coupled with 
the 2D map increased comprehension of utterances 
178
and behavior of the players during the quest, im-
proving validity and reliability of the results. 
 
Acknowledgements  
This research is part of an Air Force Research 
Laboratory sponsored study conducted by Colorado 
State University, Ohio University, the University at 
Albany, SUNY, and Lockheed Martin. 
 
References  
Steven Bird, Kazuaki Maeda, Xiaoyi Ma and Haejoong 
Lee. 2001. annotation tools based on the annotation 
graph API.  In Proceedings of ACL/EACL 2001 
Workshop on Sharing Tools and Resources for Re-
search and Education. 
M. P. ?akir. 2009. The organization of graphical, narra-
tive and symbolic interactions. In Studying virtual 
math teams (pp. 99-140). New York, Springer. 
J. Carletta. 2007. Unleashing the killer corpus: experi-
ences in creating the multi-everything AMI Meeting 
Corpus. Language Resources and Evaluation Journal 
41(2): 181-190. 
Mark G. Core and James F. Allen. 1997. Coding dia-
logues with the DAMSL annotation scheme.  In Pro-
ceedings of AAAI Fall 1997 Symposium. 
Steve Cassidy and Jonathan Harrington.  2001. Multi-
level annotation in the Emu speech database man-
agement system.  Speech Communication, 33:61-77. 
S. C. Herring. 2003. Dynamic topic analysis of synchro-
nous chat. Paper presented at the New Research for 
New Media: Innovative Research Symposium. Min-
neapolis, MN. 
S. C. Herring and Nix, C. G. 1997. Is ?serious chat? an 
oxymoron? Pedagogical vs. social use of internet re-
lay chat. Paper presented at the American Association 
of Applied Linguistics, Orlando, FL. 
Samira Shaikh, Strzalkowski, T., Broadwell, A., Stro-
mer-Galley, J., Taylor, S., and Webb, N. 2010. MPC: 
A Multi-party chat corpus for modeling social phe-
nomena in discourse. Proceedings of the Seventh 
Conference on International Language Resources and 
Evaluation. Valletta, Malta: European Language Re-
sources Association. 
G. Stahl. 2009. The VMT vision. In G. Stahl, (Ed.), 
Studying virtual math teams (pp. 17-29). New York, 
Springer. 
Stephen Sutton, Ronald Cole,  Jacques De 
Villiers,  Johan Schalkwyk,  Pieter Vermeulen,  Mike 
Macon,  Yonghong Yan,  Ed Kaiser,  Brian Run-
Rundle,  Khaldoun Shobaki,  Paul Hosom,  Alex 
Kain,  Johan Wouters,  Dominic Massaro,  Michael 
Cohen. 1998. Universal Speech Tools: The CSLU 
toolkit. Proceedings of the 5th ICSLP, Australia. 
 
Jennifer Stromer-Galley and Martinson, A. 2009. Coher-
ence in political computer-mediated communication: 
Comparing topics in chat. Discourse & Communica-
tion, 3, 195-216. 
 
N. Yee and Bailenson, J. N. 2008. A method for longitu-
dinal behavioral data collection in Second Life. Pres-
ence, 17, 594-596. 
 
 
179
Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 43?48,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
VCA: An Experiment With A Multiparty Virtual Chat Agent 
Samira Shaikh1, Tomek Strzalkowski1, 2, Sarah Taylor3, Nick Webb1 1ILS Institute, University at Albany, State University of New York 2Institute of Computer Science, Polish Academy of Sciences 3Advancded Technology Office, Lockheed Martin IS&GS E-mail: ss578726@albany.edu, tomek@albany.edu   Abstract 
The purpose of this research was to advance the understanding of the behavior of small groups in online chat rooms. The research was conducted using Internet chat data collected through planned exercises with recruited par-ticipants. Analysis of the collected data led to construction of preliminary models of social behavior in online discourse. Some of these models, e.g., how to effectively change the topic of conversation, were subsequently im-plemented into an automated Virtual Chat Agent (VCA) prototype. VCA has been dem-onstrated to perform effectively and convinc-ingly in Internet conversation in multiparty chat environments.  1 Introduction Internet chat rooms provide a ready means of communication for people of most age groups these days. More often than not, these virtual chat rooms have multiple participants conversing on a wide variety of topics, using a highly infor-mal and free-form text dialect. An increasing use of virtual chat rooms by a variety of demograph-ics such as small children and impressionable youth leads to the risk of exploitation by deceit-ful individuals or organizations. Such risks might be reduced by presence of virtual chat agents that could keep conversations from progressing into certain topics by changing the topic of conversa-tion.  Our aim was to study the behavior of small groups of online chat participants and derive models of social phenomena that occur fre-quently in a virtual chat environment. We used the MPC chat corpus (Shaikh et al, 2010), which is 20 hours of multi-party chat data collected through a series of carefully designed online chat sessions. Chat data collected from public chat rooms, while easily available, presents signifi-cant concerns regarding its adaptability for our research use. Publicly available chat data is com-
pletely anonymous, has a high level of noise and lack of focus, in addition to engendering user privacy issues for its use in modeling tasks. The MPC corpus was used in (1) understanding how certain social behaviors are reflected in language and (2) building an automated chat agent that could effectively achieve certain (initially lim-ited) social objectives in the chat-room. A brief description of the MPC corpus and its relevant characteristics is given in Section 3 of this paper. One specific phenomenon of social behavior we wanted to model was an effective change of conversation topic, when a participant or a group of participants deliberately (if perhaps only tem-porarily) shift the discussion to a different, pos-sibly related topic. Both success and failure of these actions was of interest because the outcome depended upon the choice of utterance, the per-sons to whom it was addressed, their reaction, and the time when it was produced. Our analysis of the corpus for such phenomena led to the use of an annotation scheme that allows us to anno-tate for topic and focus change in conversation. We describe the annotation scheme used in Sec-tion 4.  We constructed an autonomous virtual chat agent (VCA) that could achieve initially limited social goals in a chat room with human partici-pants. We used a novel approach of exploiting the topic of conversation underway to search the web and find related topics that could be inserted in the conversation to change its flow. We tested the first prototype with the capability to opportu-nistically change to topic of conversation using a combination of linguistic, dialogic, and topic reference devices, which we observed effectively deployed by the most influential chat participants in the MPC corpus.  The VCA design, architec-ture and mode of operation are described in de-tail in Section 5 of this paper. 2 Related Work Automated dialogue agents such as the early ELIZA (Weizenbaum, 1966) and PARRY 
43
(Colby, 1974) could conduct a one-on-one ?con-versation? with a human using rules and pattern-matching algorithms. More recently, the addition of heuristic pattern matching in A.L.I.C.E (Wallace, 2008) led to development of chat bots using AIML1 and its variations, such as Project CyN2. Most of the work on conversational agents was limited to one-on-one situations, where a single agent converses with a human user, whether to perform a transaction (such as book-ing a flight or banking transactions) (Hardy et al, 2006) or for companionship (e.g., browsing of family photographs) (Wilks, 2010). Many of these systems were inspired by the challenge of the Turing Test or its more limited variants such as Loebner Prize.   Research in the field of developing a multi-user chat-room agent has been limited. This is some-what surprising because a multi-user setting makes the agent?s task of maintaining conversa-tion far less onerous than in one-on-one situa-tions. In a chat-room, with many users engaged in conversations, it is much easier for an agent to pass as just another user. Indeed, a skillfully de-signed agent may be able to influence an ongoing conversation. 3 MPC Chat Corpus The MPC chat corpus is a collection of 20 hours of chat sessions with multiple participants (on average 4), conversing for about 90 minutes in a secure online chat room. The topics of conversa-tion vary from free-flowing chat in the initial collection phase to allow participants to build comfortable a rapport with each other, to specific task-oriented dialogues in the latter phase; such as choosing the right candidate for a job inter-view from a list of given resumes. This corpus is suitable for our research purposes since the chat sessions were designed around enabling the so-cial phenomena we were interested in modeling. 4 Annotation Scheme We wished to annotate the data we collected to derive models from language use for social phe-nomena. These represent complex pragmatic concepts that are difficult to annotate directly, let alne detect automatically. Our approach was to build a multi-level annotation scheme.  In this paper we briefly outline our annotation scheme that consists of three layers: communica-                                                1 http://www.alicebot.org/aiml.html 2 http://www.daxtron.com/123start.htm?Cyn 
tive links, dialogue acts, and topic/focus changes. A more detailed description of the annotation scheme will be presented in a future publication.  4.1 Communicative Links Annotators are asked to mark each utterance in one of three categories ? utterance is addressed to a participant or a set of participants, it is in response to a specific prior utterance by another participant or it is a continuation of the partici-pant?s own prior utterance. By an utterance, we mean the set of words in a single turn by a par-ticipant. In multi-party chat, participants do not generally add addressing information in their utterances and it is often ambiguous to whom they are speaking. Communicative link annota-tion allows us to accurately map who is speaking to whom in the conversation, which is required for tracking social phenomena across partici-pants.  4.2 Dialogue Acts At this annotation level, we developed a hierar-chy of 20 dialogue acts, based loosely on DAMSL (Allen & Core, 1997) and SWBD-DAMSL (Jurafsky et al, 1997), but greatly re-duced and more tuned to dialogue pragmatics. For example, the utterance ?It is cold here today? may function as a Response-Answer when given in response to a question about the weather, and would act as an Assertion-Opinion if it is evalu-ated alone. The dialogue acts, thus augmented, become an important feature in modeling partici-pant behavior for our research purpose. A de-tailed description of the tags is beyond the scope of this paper. 4.3 Topic and Focus boundaries The flow of discussion in chat shifts quite rapidly from one topic to another. Furthermore, within each topic (e.g., music bands) the focus of conver-sation (e.g., dc for cutie) moves just as rapidly. We distinguish between topic and focus to accom-modate both broader thematic shifts and more narrow aspect changes of the topic being dis-cussed. For example, participants might discuss the topic of healthcare reform, by focusing on President Obama, and then switch the focus to some particulars of the reform, such as the ?public op-tion?. Similarly, topics may shift while the focus remains the same (e.g., moving on to Obama?s economic policies), although such changes are less common. Annotators typically marked the first mention of a substantive noun phrase as a topic or focus introduction. 
44
The effect of topic change is apparent when a subsequent utterance by another participant is about the same topic. This is a successful attempt at changing the topic. Shown in Figure 1 is an example of topic shift annotated in our data col-lection.  
 Figure 1. A topic change in dialogue, with three participants (AA, KA and KN)  We found this model of topic change fairly con-sistently exhibited, where the participants would ask an open question, in order to get other par-ticipants to respond to them, thereby changing the course of conversation. We collected all ut-terances marked topic shifts and focus shifts and created a set of templates from them.  These templates served as a model for the VCA to util-ize when creating a response.  Another model of behavior that we found as a consequence of topic change is topic sustain. This is an instance where the utterance is marked to be on the same topic as the one currently being discussed, for example, utterance 5 in Figure 1. These may be in the form of offering support or agreement with a previous utterance or asking a question about a new in-topic aspect. We gave our annotators a fair amount of lev-erage on how to label the topics and how to rec-ognize the focus. Our primary interest was in an accurate detection of topic/focus boundaries and shifts. Of the 14 sessions we selected from the MPC corpus, we selected 10 for annotation, with at least 3 annotators for each session. In Table 1 some of the overall statistics computed from this set are shown. We computed inter-annotator agreement on all three levels of our annotation, i.e. Communication Links, Dialogue Acts and 
Topic/Focus Shifts. Topic and Focus shifts had the highest inter-annotator agreement scores on different measures such as Krippendorf?s Alpha (Krippendorff, 1980) and Fliess? Kappa (Fliess, 1971). In Figure 2, we show inter-annotator agreement measures on Topic/Focus shift anno-tation for four of the annotated sessions. Krip-pendorff?s Alpha and Fleiss? Kappa measures show inter-annotator agreement on topic shift alone, and Conflated Krippendorff?s Alpha measures show the agreement when topic and focus are conflated as one category. With such high degree of agreement, we can reliably derive models of topic shift behavior from our anno-tated data.  Total Number of Sessions Annotated 10 Number of annotators per file 3 Total Utterances Annotated 4640 Average number of utterances per ses-sion ~520 Total topics identified per session 174 Total topic shifts identified per ses-sion 344 Table 1. Selected statistics from annotated data set  
 Figure 2. Inter-annotator agreement measures for Topic/Focus shifts 5 VCA Design A virtual chat agent is an automated program with the ability to respond to utterances in chat. Our VCA is distinctive in its ability to participate in multi-party chat and manage to steer the flow of conversation to a new topic. We exploit the dialogue mechanism underlying HITIQA (Small et al 2009) to drive the dialogue in VCA.  The topic as defined by the information con-tained in the participant?s utterance is used to mine outside data sources (e.g., a corpus, the web) in order to locate and learn additional in-formation about that topic. The objective is to identify some of the salient concepts that appear 
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
Krippendorff	 ?
's	 ?Alpha	 ?
Con?ated	 ?
Krippendorff	 ?
's	 ?Alpha	 ?
Fleiss	 ?'	 ?Kappa	 ?
AA 1: did anyone watch the morning talk shows today (MTP, for example)? KA 2: nope! AA 3: I missed them ? I was hoping someone else had. AA 4: My kids tell me the band you?re going to hear (dc for cutie) is great. (TOPIC: music bands, FOCUS: dc for cutie) KA 5: oh cool! Their lyrics are nice, I think. (TOPIC: music bands, FOCUS: dc for cutie) KA 6. what kind of music do you guys listen to? (TOPIC: music, FOCUS: none) KN 7: I don?t really have a favorite genre?.you on youtube right now? (TOPIC: music, FOCUS: youtube)  
45
associated with the topic, but are not directly mentioned in the utterance. Such associations may be postulated because additional concepts are repeatedly found near the concepts men-tioned in the utterance.  An illustrative example found in our annotated corpus is the utterance, ?Lars Ulrich might have a thing or two to say about technology.? Here, the topic of conversation prior to this utterance was ?tech-nology? and it was changed to ?music? after this utterance. Here, ?Lars Ulrich? is the bridge that connects the two concepts ?technology? and ?mu-sic? together. 5.1 VCA Architecture The VCA is composed of the following modules that interact as shown in Figure 3.   5.1.1 Chat Analyzer Every utterance in chat is first analyzed by the Chat Analyzer component. This process removes stop words, emoticons and punctuation, as well as any participant nicknames from the utterance. We postulate that the remaining content bearing words in the utterance represent the topic of that utterance. We call this analyzed utterance our chat ?query? which is sent in parallel to the Document Retrieval and NL Processing compo-nent.   5.1.2 Document Retrieval The document retrieval process retrieves docu-ments from either the web or a test document 
corpus. We use Google AJAX api for our web retrieval process and InQuery (Callan et al, 1992) retrieval engine for our offline mode of operation to retrieve documents from the test corpus. The test document corpus was collected by mining the web for all utterances in our data 
collection, creating a stable document set for ex-perimental purposes. Currently, the document corpus contains about 1Gb of text data.   5.1.3 Clustering We cluster the paragraphs in documents retrieved using clustering method in Hardy et al (Hardy et al, 2009) This process groups the paragraphs containing salient entities into sets of closely as-sociated concepts. From each cluster, we choose the most representative paragraph, usually called the ?seed? paragraph for further NL processing. Each seed paragraph and the chat query undergo the same further NL processing sequence.    5.1.4 Natural Language Processing We process each chat query by performing stemming, part-of-speech tagging and named-entity recognition on it. Each seed paragraph is also run through same three natural language processing tasks. We are using Stanford POS tagger for our part-of-speech tagging. For named entity recognition, we have the ability to choose between BBN?s IdentiFinder and AeroText? (Taylor, 2004).  5.1.5 Framing We build frames from the entities and attributes found in both the chat query and the paragraphs.. This work extends the concept of framing devel-oped for HITIQA (Small et al 2009) and COL-LANE (Strzalkowski, 2009). Framing provides an informative handle on text, which can be ex-
ploited to compare the underlying textual repre-sentations, as we explain in the next section.  5.1.6 Scoring and Frame Matching Using the information in the frames built in the previous step; we compare the chat query frame 
Figure 3. VCA Architecture 
46
built from the chat query, to the frames created from the paragraphs, called paragraph frames. We assign a score for each paragraph frame based on how many attributes and their corre-sponding values match; in the current version of VCA a very basic approach to counting how many attribute-value pairs match is taken. Of all the paragraph frames we select the highest scor-ing frames and select the attribute-value pairs that are not part of the chat query frame. For ex-ample, as shown in Figure 4a below, the chat utterance ?Aruba might be nice!? created the fol-lowing chat query frame.  
 a. Example chat query frame  
 b. Frame Matching, Scoring and Template  Selection  Figure 4. From frames to VCA responses  Correspondingly, we select all PLACE type en-tities from the highest-ranking paragraph frames. These are shown in Figure 4b as Aruba Entity list.  The entities ?NASCAR?, ?Women Seeking Men? and ?Mateo? are not of entity type ? PLACE, we assign them a score of 0. The score is the fre-quency of occurrence of that entity in the para-graph; in this example it is found to be 1. Assign-ing scores by frequency of occurrence ensures that the most commonly occurring concept around the one that is being discussed in the chat query utterance will be used to respond with.  5.1.7 Template Selection Once we have chosen the entity to respond with, we select a template from the set of templates for that entity. These are templates that are created based on the models created from topic change utterances annotated in our data set. For a select group of entities, which are quite frequently en-
countered in our data collection such as PLACE, PERSON, ORGANIZATION etc., we have a set of templates specific to that entity type. We also have several generic templates that may be used if the entity type does not match the ones that we have selected. For example, a PLACE specific template is ?Have you ever been to __?? and a PER-SON specific template is ?You heard about __??. Not all templates are formulated as questions. An-other example of a generic template is ?__rules!?.  6 Example of VCA Interaction Figure 5 represents an example of the VCA in action in a simulated environment; the VCA is the participant ?renee?. We can see how the con-versation changes from ?gun laws? to ?hunting? after renee?s utterance at 11:48 AM.  
  Figure 5. Topic change example 7 Evaluation  We ran two tests of this initial VCA prototype in a public chat-room. VCA was inserted into a public chat-room with multiple participants on two separate occasions. The general topic of dis-cussion during both instances was ?anime?. We have developed an evaluation protocol in order to test the effectiveness of the VCA prototype in a realistic setting. The initial metric of VCA ef-fectiveness is the rate of involvement measured in the number of utterances generated by the VCA during the test period. These utterances are subsequently judged for appropriateness using the metric developed for the Companions Project (Webb, 2010). The actual appropriateness anno-tation scheme can be quite involved, but for this simple test we reduced the coding to only binary assessment, so that the VCA utterances were an-notated as either appropriate or inappropriate, given the content of the utterance and the flow of dialogue thus far. Using this coarse grain evalua-tion on a live chat segment we noted that the VCA made 9 appropriate utterances and 7 inap-
[POS] NNP, Aruba JJ, nice [ENT] PLACE  
Aruba Entity List: VALUE = NASCAR and TYPE = ORGANIZATION and SCORE = 0 VALUE = Dallas and TYPE = PLACE and SCORE = 1 VALUE = Mateo and TYPE = PERSON and SCORE = 0  VCA: How about Dallas? 
47
propriate utterances, which gives the appropri-ateness score of 56%. While some of VCA utter-ances seem inappropriate (i.e., not related to the conversation topic), we noted also that other posters generally tolerated these inappropriate utterances that occurred early in the dialogue. Moreover, these early inappropriate utterances did generate appropriate responses from the hu-man users. This ?positive? dynamic changed gradually as the dialogue progressed, when the participants began to ignore VCA?s utterances.  While this coarse grained evaluation is useful, our plan is to conduct evaluation experiments by recruiting subjects for chat sessions and inserting the VCA in the discussion. We will measure the impact of the VCA in the chat session by having participants fill out post-session questionnaires, which can elicit their responses regarding (a) if they detect presence of a VCA at any time during the dialogue; (b) who was the VCA; (c) who changed the topic of conversation most often; and so on. Another metric of interest is the level of engagement of the VCA, which can be meas-ured by the number of direct responses to an ut-terance by the VCA. We are developing the evaluation process, and report on the results in a separate publication. References  Allen, J. M. Core. (1997). Draft of DAMSL: Dialog Act Markup in Several Layers. http://www.cs.rochester.edu/research/cisd/resources/damsl/  Callan, J. P., W. B. Croft, and S. M. Harding. 1992. The INQUERY Retrieval System, in Proceedings of the 3rd Inter- national Conference on Database and Expert Systems.  Colby, K.M, Hilf, F.D, and S. Weber. 1972. Turing-like indistinguishability tests for the validation of a computer simulation of paranoid processes. In: Ar-tificial Intelligence , Vol. 3, p. 199-221. Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bul-letin, 74(5):378{382. Hardy, Hilda, Nobuyuki Shimizu, Tomek Strzalk-owski, Ting Liu, Bowden Wise and Xinyang Zhang. 2002. Cross-document summarization by concept classification. In Proceedings of ACM SIGIR '02 Conference, pages 121-128, Tampere, Finland. Hardy, H., A Biermann, R. Bryce Inouye, A. McKenzie, T. Strzalkowski, C. Ursu, N. Webb and M. Wu. 2006. The AMITIES System: Data-Driven Techniques for Automated Dialogue. In 
Speech Communication 48 (3-4), pages 354-373.  Elsevier. Jurafsky, Dan, Elizabeth Shriberg, and Debra Biasca. (1997). Switchboard SWBD-DAMSL Shallow-Discourse-Function Annotation Coders Manual. http://stripe.colorado.edu/~jurafsky/manual.august1.html Krippendorff, Klaus. 1980. Content Analysis, an In-troduction to its Methodology. Sage Publications, Thousand Oaks, CA. Samira S., Tomek Strzalkowski, Sarah Taylor and Jonathan Smith (2009) Comparing an Integrated QA system performance - A Preliminary Model. Proceedings of PACLING Conference, Sapporo, Japan. Shaikh, S., Strzalkowski, T.,  Broadwell, A., Stromer-Galley, J., Taylor, Sarah and Webb, N. 2010. Pro-ceedings of LREC Conference, Malta. Sharon Small and Tomek Strzalkowski. 2009. HITIQA: High-Quality Intelligence through Inter-active Question Answering. Journal of Natural Language Engineering, Vol. 15 (1), pp. 31?54. Cambridge.  Tomek Strzalkowski, Sarah Taylor, Samira Shaikh, Ben-Ami Lipetz, Hilda Hardy, Nick Webb, Tony Cresswell, Min Wu, Yu Zhan, Ting Liu, and Song Chen. 2009. COLLANE: An experiment in com-puter-mediated tacit collaboration. In Aspects of Natural Language Processing (M. Marciniak and A. Mykowiecka, editors). Springer.  Taylor, Sarah M. 2004. "Information Extraction Tools: Deciphering Human Language." IT Profes-sional. Vol. 06, no. 6, pages: 28-34. Novem-ber/December, 2004. Online. http://ieeexplore.ieee.org/iel5/6294/30282/01390870.pdf?tp=&arnumber=1390870&isnumber=30282 Wallace, R. 2008. The Anatomy of A.L.I.C.E. In Parsing the Turing Test. (Robert Epstein, Gary Roberts and Grace Beber, editors). Springer. Webb, N., D. Benyon, P. Hansen and O. Mival. 2010. Evaluating Human-Machine Conversation for Ap-propriateness. In Proceedings of the 7th Interna-tional Conference on Language Resources and Evaluation (LREC2010), Valletta, Malta. Weizenbaum, Joseph. January 1966. "ELIZA ? A Computer Program For the Study of Natural Lan-guage Communication Between Man And Ma-chine", Communications of the ACM 9 (1): 36?45. Wilks, Y. 2010. Artificial Companions. In: Y.Wilks (ed.) Close Engagement with Companions: scien-tific, economic, psychological and philosophical perspectives. John Benjamins: Amsterdam. 
48
Proceedings of the First Workshop on Metaphor in NLP, pages 67?76,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Robust Extraction of Metaphors from Novel Data   Tomek Strzalkowski1, George Aaron Broadwell1, Sarah Taylor2, Laurie Feldman1, Boris Yamrom1, Samira Shaikh1, Ting Liu1, Kit Cho1, Umit Boz1, Ignacio Cases1 and Kyle El-liott3 1State University of New York 2Sarah M. Taylor Consulting LLC 3Plessas Experts University at Albany 121 South Oak St.  Network Inc. Albany NY USA 12222 Falls Church VA USA 22046 Herndon VA 20171 tomek@albany.edu talymail59@gmail.com  kelliot@plessas.net     Abstract 
This article describes our novel approach to the automated detection and analysis of meta-phors in text. We employ robust, quantitative language processing to implement a system prototype combined with sound social science methods for validation. We show results in 4 different languages and discuss how our methods are a significant step forward from previously established techniques of metaphor identification. We use Topical Structure and Tracking, an Imageability score, and innova-tive methods to build an effective metaphor identification system that is fully automated and performs well over baseline.  1 Introduction The goal of this research is to automatically identi-fy metaphors in textual data.  We have developed a prototype system that can identify metaphors in naturally occurring text and analyze their seman-tics, including the associated affect and force. Met-aphors are mapping systems that allow the semantics of a familiar Source domain to be ap-plied to a Target domain so that new frameworks of reasoning can emerge in the Target domain. Metaphors are pervasive in discourse, used to con-vey meanings indirectly. Thus, they provide criti-cal insights into the preconceptions, assumptions and motivations underlying discourse, especially valuable when studied across cultures. When met-aphors are thoroughly understood within the con-text of a culture, we gain substantial knowledge about cultural values. These insights can help bet-ter shape cross-cultural understanding and facili-
tate discussions and negotiations among different communities.  A longstanding challenge, however, is the large-scale, automated identification of metaphor in vol-umes of data, and especially the interpretation of their complex, underlying semantics.  We propose a data-driven computational ap-proach that can be summarized as follows: Given textual input, we first identify any sentence that contains references to Target concepts in a given Target Domain (Target concepts are elements that belong to a particular domain; for instance ?gov-ernment bureaucracy? is a Target concept in the ?Governance? domain). We then extract a passage of length 2N+1, where N is the number of sentenc-es preceding (or succeeding) the sentence with Target Concept. We employ dependency parsing to determine the syntactic structure of each input sen-tence. Topical structure and imageability analysis are then combined with dependency parsing output to locate the candidate metaphorical expressions within a sentence. For this step, we identify nouns and verbs in the passage (of length 2N+1) and link their occurrences ? including repetitions, pronomi-nal references, synonyms and hyponyms. This linking uncovers the topical structure that holds the narrative together.  We then locate content words that are outside the topical structure and compute their imageability scores. Any nouns or adjectives outside the main topical structure that also have high imageability scores and are dependency-linked in the parse structure to the Target Concept are identified as candidate source relations, i.e., expressions borrowed from a Source domain to describe the Target concept. In addition, any verbs that have a direct dependency on the Target Con-
67
cept are considered as candidate relations. These candidate relations are then used to compute and rank proto-sources. We search for their arguments in a balanced corpus, assumed to represent stand-ard use of the language, and cluster the results. Proto-source clusters and their ranks are exploited to determine whether the candidate relations are metaphorical or literal. Finally, we compute the affect and force associated with the metaphor.    Our approach is shown to work in four lan-guages ? American English, Mexican Spanish, Russian Russian and Iranian Farsi. We detail in this paper the application of our approach to detec-tion of metaphors using specific examples from the ?Governance? domain. However, our approach can be expanded to work on extracting metaphors in any domain, even unspecified ones. We shall brief-ly explain this in Section 5; we defer the details of the expanded version of the algorithm to a separate larger publication. In addition, we shall primarily present examples in English to illustrate details of our algorithms. However, modules for all four lan-guages have the same implementation in our sys-tem.  The rest of the paper is organized as follows: in Section 2, we discuss related research in this field. Section 3 presents our approach in detail; Section 4 describes our evaluation and results. In Section 5 we discuss our conclusions and future directions.  2 Related Work Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as defined by Lakoff & Johnson, 1980; and their fol-lowers) that generally look at metaphors as abstract language constructs with complex semantic prop-erties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that at-tempt to correlate metaphor semantics with their usage in naturally occurring text but generally lack robust tools to do so; and (3) social science ap-proaches, particularly in psychology and anthro-pology that seek to explain how people deploy and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated exam-ples.     Metaphor study in yet other disciplines has in-cluded cognitive psychologists (e.g., Allbritton, McKoon & Gerrig, 1995) who have focused on the 
way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on refugees (1992), see metaphor as a tool to help out-siders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Polit-ical Science underpinning (Musolff, 2008; Lakoff, 2001).      In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of meta-phors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al(2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example).     Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feld-man & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al(2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However they self-report their work to be an initial explora-tion and hence, inconclusive. Shutova et al(2010a) employ an unsupervised method of metaphor iden-tification using nouns and verb clustering to auto-matically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quanti-ties and may not be easily generated in different languages.  By contrast, we propose an approach that is fully automated and can be validated using empirical social science methods. Details of our algorithm follow next.  3 Our Approach In this section, we walk through the steps of meta-phor identification in detail. Our overall algorithm 
68
consists of five main steps from obtaining textual input to classification of input as metaphorical or literal.  3.1 Passage Identification The input to our prototype system is a piece of text. This text may be taken from any genre ? news articles, blogs, magazines, official announcements, broadcast transcripts etc.  Given the text, we first identify sentences that contain Target concepts in the domain we are in-terested in. Target concepts are certain keywords that occur within the given domain and represent concepts that may be targets of metaphor. For in-stance, in the ?Governance? domain, concepts such as ?federal bureaucracy? and ?state mandates? serve as Target concepts. We keep a list of Target concepts to search through when analyzing given input. This list can be automatically created by mining Target Concepts from resource such as Wikipedia, given the Target domain, or manually constructed. Space limits the discussion of how such lists may be automatically created; a separate larger publication addresses our approach to this task in greater detail.  In Figure 1, we show a piece of text drawn from a 2008 news article. The sentence in italics con-tains one of our Target concepts: ?federal bureau-cracy?. We extract the sentence containing Target concepts that match any of those in our list, includ-ing N sentences before and N sentences after the sentence if they exist, to yield a passage of at most 2N+1 sentences. For the example shown in Figure 1, the Target concept is ?federal bureaucracy?. In current system prototype, N=2. Hence, we extract two sentences prior to the sentence containing ?federal bureaucracy? (in Figure 1 example, these are omitted for ease of presentation) and two sen-tences following the given sentence.       Once this passage is extracted, we need to de-termine whether a metaphor is present in the mid-dle sentence. To accomplish that, we follow the steps as described in the next section.    
 Figure 1. Excerpt from news article. Passage containing target concept highlighted in italics. The callouts 1, 2 etc., indicate topic chains (see next section).      3.2 Topical Structure and Imageability Anal-ysis Our hypothesis is that metaphorically used terms are typically found outside the topical structure of the text. This is an entirely novel method of effec-tively selecting candidate relations. It draws on  Broadwell et al (2012), who proposed a method to establish the topic chains in discourse as a means of modeling associated socio-linguistic phenomena such as topic control and discourse cohesiveness. We adapted this method to identify and exclude any words that serve to structure the core discus-sion, since the metaphorical words, except in the cases of extended and highly elaborated meta-phors, are not the main subject, and thus unlikely to be repeated or referenced in the context sur-rounding the sentence.  We link the occurrences of each noun and verb in the passage (5 sentence length). Repetitions via synonyms, hyponyms, lexical variants and pronoun references are linked together. These words, as elements of the several topic chains in a text, are then excluded from further consideration. WordNet (Felbaum, 1998) is used to look up synonyms and hyponyms of the remaining content words. We 
These qualities1 have helped him4 navigate the labyrinthine federal bureaucracy in his demand-ing $191,300-a-year job as the top federal offi-cial3 responsible for bolstering airline, border2, port and rail security against a second cata-strophic terrorist attack.  But those same personal qualities1 also explain why the 55-year-old Cabinet officer3 has alienat-ed so many Texans along the U.S.-Mexico bor-der2 with his4 relentless implementation of the Bush administration's hard-nosed approach to immigration enforcement - led by his unyielding push to construct 670 miles of border2 fencing by the end of the year.  Some Texas officials are so exasperated that they say they'll just await the arrival of the next presi-dent before revisiting border enforcement with the federal government.  Copyright 2008. The Houston Chronicle Publishing Company. All Rights Reserved.  
69
illustrate this in Figure 1. We show the two sen-tences that form the latter context in the example passage. We show four of the topic chains discov-ered in this passage. These have been labeled via superscripts in Figure 1. 1 and 2 are the repetitions of word ?qualities? and ?border?. The 3 identifies repetition via lexical variants ?officer? and ?offi-cial? and 4 identifies the pronoun co-references  ?him? and ?his?. We shall exclude these words from consideration when searching for candidate metaphorical relations in the middle sentence of the passage.  To further narrow the pool of candidate relations in this sentence, we compute the imageability scores of the remaining words. The hypothesis is metaphors use highly imageable words to convey their meaning. The use of imageability scores for the primary purpose of metaphor detection distin-guishes our approach from other research on this problem. While Turney et al (2011) explored the use of word concreteness (a concept related but not identical to imageability) in an attempt to disam-biguate between abstract and concrete verb senses, their method was not specifically applied to detec-tion of metaphors; rather it was used to classify verb senses for the purpose of resolving textual entailment. Broadwell et al (2013) present a de-tailed description of our approach and how we use imageability scores to detect metaphors. Our assertion is that any highly imageable word is more likely to be a metaphorical relation. We use the MRCPD (Coltheart 1981, Wilson 1988) expanded lexicon to look up the imageability scores of words not excluded via the topic chains. Although the MRCPD contains data for over 150,000 words, a major limitation of the database for our purposes is that the MRCPD has imageabil-ity ratings (i.e., how easily and quickly the word evokes a mental image) for only ~9,240  (6%) of the total words in its database. To fill this gap, we expanded the MRCPD database by adding imagery ratings for an further 59,989 words. This was done by taking the words for which the MRCPD data-base has an imageability rating and using that word as an index to synsets determined using WordNet (Miller, 1995). The expansion and validation of the expanded MRCPD imageability rating is presented in a separate, future publication.  Words that have an imageability rating lower than an experimentally determined threshold are further excluded from consideration. In the exam-
ple shown in Figure 1, words that have sufficiently high imageability scores are ?labyrinthine?, ?port?, ?rail? and ?airline?. We shall consider them as candidate relations, to be further investigated, as explained in the dependency parsing step described next.   3.3 Relation Extraction Dependency parsing reveals the syntactic structure of the sentence with the Target concept. We use the Stanford parser (Klein and Manning, 2003) for English language data. We identify candidate met-aphorical relations to be any verbs that have the Target concept in direct dependency path (other than auxiliary and modal verbs). We exclude verbs of attitude (?think?, ?say?, ?consider?), since these have been found to be more indicative of metony-my than of metaphor. This list of attitude verbs is automatically derived from WordNet. From the example shown in Figure 1, one of the candidate relations extracted would be the verb ?navigate?.      In addition, we have a list of candidate relations from Step 3.2, which are the highly imageable nouns and adjectives that remain after topical structure analysis. Since ?port?, ?rail? and ?airline? do not have a direct dependency path to our Target concept of ?federal bureaucracy?, we drop these from further consideration. The highly imageable word remaining in this list is ?labyrinthine?.      Thus, two candidate relations are extracted from this passage ? ?navigate? and ?labyrinthine?. We shall now show how we use these to discover pro-to-sources for the potential metaphor.  3.4 Discovery of Proto-sources Once candidate relations are identified, we exam-ine whether the usage of these relations is meta-phorical or literal. To determine this, we search for all uses of these relations in a balanced corpus and examine in which contexts the candidate relations occur. To demonstrate this via our example, we shall consider one of the candidate relations identi-fied in Figure 1 ? ?navigate?; the search method is the same for all candidate relations identified. In the case of the verb ?navigate? we search a bal-anced corpus for the collocated words, that is, those that occur within a 4-word window following the verb, with high mutual information (>3) and occurring together in the corpus with a frequency 
70
at least 3. This search returns a list of words, most-ly nouns in this case, that are the objects of the verb ?navigate?, just as ?federal bureaucracy? is the object in the given example. However, since the search occurs in a balanced corpus, given the parameters we search for, we discover words where the objects are literally navigated. Given these search parameters, the top results we get are generally literal uses of the word ?navigate?. We cluster the resulting literal uses as semantically related words using WordNet and corpus statistics. Each such cluster is an emerging prototype source domain, or a proto-source, for the potential meta-phor. In Figure 2, we show three of the clusters ob-tained when searching for the literal usage of the verb ?navigate?. We use elements of the clusters to give names or label the proto-source domains. WordNet hypernyms or synonyms are used in most cases. The clusters shown in Figure 2 represent three potential source domains for the given exam-ple, the labels ?MAZE?, ?WAY? and ?COURSE? are derived from WordNet.  
 Figure 2. Three of several clusters obtained from bal-anced corpus search for objects of verb ?navigate?.       We rank the clusters according to the combined frequency of cluster elements in the balanced cor-pus. In a similar fashion, clusters are obtained for the candidate relation ?labyrinthine?; however here we search for the nouns modified by the adjective ?labyrinthine?.       
3.5 Estimation of Linguistic Metaphor A ranked list of proto-sources from the previous step serves as evidence for the presence of a meta-phor.   If any Target domain elements are found in the top two ranked clusters, we consider the phrase being investigated to be literal. This eliminates examples where one of the most frequently en-countered sources is within the target domain.  If neither of the top two most frequent clusters contains any elements from the target domain, we then compute the average imageability scores for each cluster from the mean imageability score of the cluster elements. If no cluster has a sufficiently high imageability score (experimentally deter-mined to be >.50 in the current prototype), we again consider the given input to be literal. This step reinforces the claim that metaphors use highly imageable language to convey their meaning. If a proto-source cluster is found to meet both criteria, we consider the given phrase to be metaphorical. For the example shown in Figure 1, our system finds ?navigate the ?federal bureaucracy? to be metaphorical. One of the top Source domains iden-tified for this metaphor is ?MAZE?. Hence the conceptual metaphor output for this example can be: ?FEDERAL BUREAUCRACY IS A MAZE?. Our system can thus classify input sentences as metaphorical or literal by the series of steps out-lined above. In addition, we have modules that can determine a more complex conceptual metaphor, based upon evidence of one or more metaphorical passages as identified above. We do not discuss those modules in this article. Once a metaphor is identified, we compute associated Mappings, Af-fect and Force. 3.6 Mappings In the current prototype system, we assign meta-phors to one of three types of mappings. Propertive mappings ? which state what the domain objects  
1. Proto-source Name: MAZE Proto-source Elements: [mazes, system, net-works] IMG Score: 0.74 2. Proto-source Name: WAY Proto-source Elements: [way, tools] IMG Score: 0.60 3. Proto-source Name: COURSE Proto-source Elements: [course, streams] IMG: 0.55 
Table 1. Algorithm assigns affect of metaphor based upon mappings. 
Rel  < Negative Rel  = Neutral 
Rel ? Positive 
71
are and descriptive features; Agentive mappings ? which describe what the domain elements do to other objects in the same or different domains; and Patientive mappings ? which describe what is done to the objects in these domains. These are broad categories to which relations can, with some ex-ceptions be assigned at the linguistic metaphor lev-el by the parse tag of the relation. Relations that take Target concepts as objects are usually Pa-tientive relations. Similarly, relations that are Agentive take Target concepts as subjects. Proper-tive relations are usually determined by adjectival relations.     Once mappings are assigned, we can use them to group linguistic metaphors. A set of linguistic met-aphors on the same or semantically equivalent Target concepts can be grouped together if the re-lations are all agentive, patientive or propertive. The mapping assigned to set of examples in Figure 3 is Patientive.      One immediate consequence of the proposed approach is the simplicity with which we can rep-resent domains, their elements, and the metaphoric mappings between domains. Regardless of what specific relations may operate within a domain (be it Source or Target), they can be classified into just 3 categories. We are further expanding this module to include semantically richer distinctions within the mappings. This includes the determination of the sub-dimensions of mappings i.e. assigning groups of relations to a semantic category.  3.7 Affect and Force  Affect of a metaphor may be positive, negative or neutral. Our affect estimation module computes an affect score taking into account the relation, Target concept and the subject or object of the relation based on the dependency between relation and Target concept. The algorithm is applied according to the categories shown in Table 1.      The expanded ANEW lexicon (Bradley and Lang, 2010) is used to look up affect scores of words. ANEW assigns scores from 0 (highly nega-tive) to 9 (highly positive); 5 being neutral. We compute the affect of a metaphorical phrase within a sentence by summing the affect scores of the re-lation and its object or subject.  If the relation is agentive, we then look at the object in source do-main that the Target concept is acting upon. If the object (denoted in above table as X) has an affect 
score that is greater than neutral, and the relation itself has an affect score that is greater than neutral, then a POSITIVE affect is assigned to the meta-phor. This is denoted by the cell at the intersection of the row labeled ?Rel > Positive? and the 3rd col-umn in Table 1. Similarly affect for the other map-ping categories can be assigned.   
 Figure 3. Four metaphors for the Target concept ?feder-al bureaucracy?.   We also seek to determine the impact of metaphor on the reader. This is explored using the concept of Force in our system. The force of a metaphor is estimated currently by the commonness of the ex-pression in the given Target domain. We compute the frequency of the relation co-occurring with Target concept in a corpus of documents in the given Target domain. This frequency represents the commonness of expression, which is the in-verse of Force. The more common a metaphorical expression is, the lesser its force.     For the example shown below in Figure 4, the affect is computed to be positive (?navigate? and ?veterans? are both found to have positive affect scores, the relation is patientive). The force of this expression is low, since its commonness is 742 (commonness score > 100 is high commonness, determined experimentally).   
 Figure 4. Example of metaphor with positive affect and low force. 
1. His attorney described him as a family man who was lied to by a friend and who got tangled in federal bureaucracy he knew nothing about. 2. The chart, composed of 207 boxes illustrates the maze of federal bureaucracy that would have been created by then-President Bill Clinton's rela-tion health reform plan in the early 1990s. 3. "Helping my constituents navigate the federal bureaucracy is one of the most important things I can do," said Owens. 4. A Virginia couple has donated $1 million to help start a center at Arkansas State University meant to help wounded veterans navigate the federal bureaucracy as they return to civilian life.  
A Virginia couple has donated $1 million to help start a center at Arkansas State University meant to help wounded veterans navigate the federal bureaucracy as they return to civilian life.  
72
   The focus of this article is the automatic identifi-cation of metaphorical sentences in naturally oc-curring text. Affect and force modules are utilized to understand metaphors in context and contrast them across cultures, if feasible. We defer more detailed discussion of affect and force and their implications to a future, larger article.  4 Evaluation and Results In order to determine the efficacy of our system in classifying metaphors as well as to validate various system modules such as affect and force, we per-formed a series of experiments to collect human validation of metaphors in a large set of examples.  4.1 Experimental Setup  We constructed validation tasks that aimed at per-forming evaluation of linguistic metaphor extrac-tion accuracy. The first task ? Task 1, consists of a series of examples, typically 50, split more or less equally between those proposed by the system to be metaphorical and those proposed to be literal. This task was designed to elicit subject and expert judgments on several aspects related to the pres-ence or absence of linguistic metaphors in text. Subjects are presented with brief passages where a Target concept and a relation are highlighted. They are asked to rank their responses on a 7-point scale for the following questions:  Q1: To what degree does the above passage use metaphor to describe the highlighted concept? Q2: To what degree does this passage convey an idea that is either positive or negative?  Q3: To what degree is it a common way to express this idea?      There are additional questions that ask subjects to judge the imageability and arousal of a given pas-sage, which we do not discuss in this article. Q1 deals with assessing the metaphoricity of the ex-ample, Q2 deals with affect and Q3 deals with force.   Each instance of Task 1 consists of a set of instruc-tions, training examples, and a series of passages to be judged. Instructions provide training examples whose ratings fall at each end the rating continu-um. Following the task, participants take a gram-
mar test to demonstrate native language proficien-cy in the target language. All task instances are then posted on Amazon?s Mechanical Turk. The goal is to collect at least 30 valid judgments per task instance. We typically collect ~50 judgments from Mechanical Turkers, so that after filtering for invalid data which includes turkers selecting items at random, taking too little time to complete the task, grammar test failures, and other inconsistent data, we would still retain 30 valid judgments per passage. In addition to grammar test and time fil-ter, we also inserted instance of known metaphors and known literal passages randomly within the Task. Any turker judgments that classify these known instance incorrectly more than 30% of the total known instance size are discarded.     The valid turker judgments are then converted to a binary judgment for the questions we presented. For example, for question Q1, the anchors to 7-point scale are 0 (none at all i.e. literal) to 7 (highly i.e metaphorical). We take [0, 2] as a literal judg-ment and [4, 6] as metaphorical and take a majority vote. If the majority vote is 3, we discard that pas-sage from our test set, since it is undetermined whether the passage is literal or metaphorical.         We have collected human judgments on hun-dreds of metaphors in all four languages of inter-est. In Section 4.3, we explain our performance and compare our results to baseline where appro-priate.  4.2 Test Reliability The judgments collected from subjects are tested for reliability and validity. Reliability among the raters is computed by measuring intra-class corre-lation (ICC) (McGraw & Wong, 1996; Shrout & Fleiss, 1979). A coefficient value above 0.7 indi-cates strong reliability.  Table 3 shows the current reliability coefficients established for the selected Task 1 questions in all 4 languages. In general, our analyses have shown that with approximately 30 or more subjects we obtain a reliability coefficient of at least 0.7. We note that Russian and Farsi reliability scores are low in some categories, primarily due to lack of sufficient subject rating data. However, reliability of subject ratings for metaphor question (Q1) is sufficiently high in three of the four languages we are interested in.  
73
Dimension English Spanish Russian  Farsi Metaphor .908 .882 .838 .606 Affect  .831 .776 .318 .798 Commonness .744 .753 .753 .618 Table 3. Intraclass correlations for linguistic metaphor assessment by Mechanical Turk subjects (Task 1) 4.3 Results In Table 4, we show our performance at classifying metaphors across four different languages. The baseline in this table assigns all given examples in the test set to be metaphorical. We note that per-formance of the system at the linguistic metaphor level when compared to human gold standard is significantly over baseline for all four languages. The system performances cited in Table 4 validate the system against test sets that contain the distri-bution of metaphorical vs. literal examples as out-lined in Table 5.   English Spanish Russian Farsi Baseline 45.8% 41.7% 56.4% 50% System 71.3% 80% 69.2% 78% Table 4. Performance accuracy of system when com-pared to baseline for linguistic metaphor classification.   English Spanish Russian Farsi Metaphor 50 50 22 25 Literal 59 70 17 25 Total 109 120 39 50 Table 5. Number of metaphorical and literal examples in test sets across all four languages.  Table 6 shows the accuracy in classification by the Affect and Force modules. We note that the low performance of affect and force for languages oth-er than English. Our focus has been on improving NLP tools for Spanish, Russian and Farsi, so that a similar robust performance for those language can be achieved as we can demonstrate in English.  Accuracy English Spanish Russian Farsi Affect  72% 54% 51% 40% Force 67% 50% 33% 66% Table 6. Affect and force performance of system on linguistic metaphor level.  5 Discussion and Future Work In this article, we described in detail our approach to detecting metaphors in text. We have developed 
an automated system that does not require the ex-istence of annotated training data or a knowledge base of predefined metaphors. We have described the various steps for detecting metaphors from re-ceiving an input, to selecting candidate relations, to the discovery of prototypical source domains, and leading to the identification of a metaphor as well as the discovery of the potential source domain being applied in the metaphor. We presented two novel concepts that have heretofore not been fully explored in computational metaphor identification systems. The first is the exclusion of words that form the thread of the discussion in the text, by the application of a Topic Tracking module. The se-cond is the application of Imageability scores in the selection of salient candidate relations.  Our evaluation consists first of validating the eval-uation task itself. Once we ensure that sufficient reliability has been established on the various di-mensions we seek to evaluate ? metaphoricity, af-fect and force ? we compare our system performance to the human gold standard. The per-formance of our system as compared to baseline is quite high, across all four languages of interest when measured against human assessed gold standard.  In this article, we discuss examples of metaphors belonging to a specific Target domain ? ?Govern-ance?. However, we can run our system through data in any domain perform the same kind of met-aphor identification. In cases where the Target do-main is unknown, we plan to use our Topic tracking module to recognize content words that may form part of a metaphorical phrase. This is essentially a process that is the reverse of that de-scribed in Section 3.3. We will find the salient Target concepts where there are directly dependent relations with the imageable verbs or adjectives.  In a separate larger publication, we plan to discuss in detail revisions to our Mapping module as well as the discovery and analyses of more complex conceptual metaphors. Such complex metaphors are based upon evidence from one or more instance of linguistic metaphors. Additional modules would recognize the manifold mappings, affect and force associated with the complex conceptual metaphors.   Acknowledgments This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of 
74
Defense US Army Research Laboratory contract num-ber W911NF-12-C-0024. The U.S. Government is au-thorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.  Disclaimer: The views and conclu-sions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Govern-ment. References  Allbritton, David W., Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-Based Schemas and Text Representations: Making Connections Through Con-ceptual Metaphors, Journal of Experimental Psy-chology: Learning, Memory, and Cognition, Vol. 21, No. 3, pp. 612-625. Baumer, Erik. P.S., White, James., Tomlinson, Bill. 2010. Comparing Semantic Role Labeling with Typed Dependency Parsing in Computational Meta-phor Identification. Proceedings of the NAACL HLT 2010 Second Workshop on Computational Ap-proaches to Linguistic Creativity, pages 14?22, Los Angeles, California, June 2010.  Bradley, M.M. & Lang, P.J. 2010. Affective Norms for English Words (ANEW): Instruction manual and af-fective ratings. Technical Report C-2. University of Florida, Gainesville, FL. Broadwell George A., Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Taylor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu and Nick Webb. 2012. Modeling Socio-Cultural Phenomena in Discourse. Journal of Natural Language Engineer-ing, Cambridge Press. Broadwell, George A., Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Taylor, Samira Shaikh, Ting Liu, Kit Cho, aand Nick Webb. 2013. Using imageability and topic chaining to locate met-aphors in linguistic corpora. in Ariel M. Greenberg, William G. Kennedy, Nathan D. Bos and Stephen Marcus, eds. Proceedings of the 6th International Conference on Social Computing, Behavioral-Cultural Modeling and Prediction SBP 2013. Carbonell, Jaime. 1980. Metaphor: a key to extensible semantic analysis. Proceedings of the 18th Annual Meeting on Association for Computational Linguis-tics. Charteris-Black, Jonathan 2002 Second Language Fig-urative Proficiency: A Comparative Study of Malay and English. Applied Linguistics 23/1: 104-133. 
Coltheart, M. 1981. The MRC Psycholinguistic Data-base. Quarterly Journal of Experimental Psychology, 33A, 497-505. Fass, Dan. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computa-tional Linguistics, Vol 17:49-90 Feldman, J. and S. Narayanan. 2004. Embodied mean-ing in a neural theory of language. Brain and Lan-guage, 89(2):385?392. Fellbaum, C. editor. 1998. WordNet: An Electronic Lexical Database (ISBN: 0-262-06197-X). MIT Press, first edition. Gedigian, M., Bryant, J., Narayanan, S., & Ciric, B. (2006). Catching Metaphors. Proceedings of the Third Workshop on Scalable Natural Language Un-derstanding ScaNaLU 06 (pp. 41-48). Association for Computational Linguistics. Klein, Dan and Manning, Christoper D. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430. Krishnakumaran, S. and X. Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the Workshop on Computational Approaches to Fig-urative Language, pages 13?20, Rochester, NY. Lakoff, George and Johnson, Mark. 1980. Metaphors We Live By. University Of Chicago Press. Lakoff, George. 2001. Moral Politics: what Conserva-tives Know that Liberals Don?t. University of Chica-go Press. Malkki, Liisa. 1992. National Geographic: The Rooting of People and the Territorialization of National Iden-tity Among Scholars and Refugees. Society for Cul-tural Anthropology 7(1):24-44 Martin, James. 1988. A Computational Theory of Meta-phor. PH.D. Dissertation McGraw, K. O., & Wong, S. P. 1996. Forming infer-ences about some intraclass correlation coefficients. Psychological Methods, 1(1), 30-46. Musolff, Andreas. 2008. What can Critical Metaphor Analysis Add to the Understanding of Racist Ideolo-gy? Recent Studies of Hitler?s Anti-Semitic Meta-phors, Critical Approaches to Discourse Analysis across Disciplines, http://cadaad.org/ejournal, Vol. 2(2): 1-10. O?Halloran, Kieran. 2007. Critical Discourse Analysis and the Corpus-informed Interpretation of Metaphor at the Register Level. Oxford University Press 
75
Shrout, P. E., & Fleiss, J. L. 1979. Intraclass correla-tions: Uses in assessing rater reliability. Psychologi-cal Bulletin, 86 (2), 420-428. Shutova, E. 2010. Models of Metaphors in NLP. In Proceedings of ACL 2010, Uppsala, Sweden. Shutova, E. and S. Teufel. 2010a. Metaphor corpus an-notated for source - target domain mappings. In Pro-ceedings of LREC 2010, Malta. Shutova, E., T. Van de Cruys and A. Korhonen. 2012. Unsupervised Metaphor Paraphrasing Using a Vector Space Model, In Proceedings of COLING 2012, Mumbai, India Turney, Peter., Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identifi-cation through concrete and abstract context. In Pro-ceedings of EMNLP, pages 680?690, Edinburgh, UK Wilks, Yorick. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cambridge University Press, Cambridge, U.K., 329--348. Wilson, M.D. (1988) The MRC Psycholinguistic Data-base: Machine Readable Dictionary, Version 2. Be-havioural Research Methods, Instruments and Computers, 20(1), 6-11.     
76
Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 41?48,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Topical Positioning: A New Method for Predicting Opinion Changes in Conversation  Ching-Sheng Lin1, Samira Shaikh1, Jennifer Stromer-Galley1,2,  Jennifer Crowley1, Tomek Strzalkowski1,3, Veena Ravishankar1   1State University of New York - University at Albany, NY 12222 USA 2Syracuse University 3Polish Academy of Sciences clin3@albany.edu, sshaikh@albany.edu, tomek@albany.edu    Abstract 
In this paper, we describe a novel approach to automatically detecting and tracking discus-sion dynamics in Internet social media by fo-cusing on attitude modeling of topics. We characterize each participant?s attitude to-wards topics as Topical Positioning, employ Topical Positioning Map to represent the posi-tions of participants with respect to each other and track attitude shifts over time. We also discuss how we used participants? attitudes towards system-detected meso-topics to re-flect their attitudes towards the overall topic of conversation. Our approach can work across different types of social media, such as Twitter discussion and online chat room. In this article, we show results on Twitter data. 
1 Introduction The popularity of social networks and the new kinds of communication they support provides never before available opportunities to examine people behaviors, ideas, and sentiments in various forms of interaction. One of the active research subjects is to automatically identify sentiment, which has been adopted in many different applica-tions such as text summarization and product re-view. In general, people express their stances and rationalize their thoughts on the topics in social media discussion platform. Moreover, some of them explicitly or implicitly establish strategies to persuade others to embrace his/her belief. For ex-ample, in the discussion of the topic ?Should the legal drinking age be lowered to 18?, the partici-pants who are against it may state their views ex-plicitly and list negative consequences of lowering 
drinking age to 18 in an attempt to change opinions of those who appear to support the change. This phenomenon actually involves two research prob-lems which have been of great interest in Natural Language Processing: opinion identification and sociolinguistic modeling of discourse. The first problem can be addressed by traditional opinion analysis that recognizes which position or stance a person is taking for the given topics (So-masundaran and Wiebe, 2009). The second part requires modeling the sociolinguistic aspects of interactions between participants to detect more subtle opinion shifts that may be revealed by changes in interpersonal conversational dynamics. In this paper, we bring these two research avenues together and describe a prototype automated sys-tem that: (1) discovers each participant?s position polarities with respect to various topics in conver-sation, (2) models how participants? positions change over the course of conversation, and (3) measures the distances between participants? rela-tive positions on all topics. We analyzed discus-sions on Twitter to construct a set of meso-topics based on the persistence of certain noun phrases and co-referential expressions used by the partici-pants. A meso-topic is any local topic in conversa-tion referred to by a noun phrase and subsequently mentioned again at least 5 times via repetition, pronoun or synonym. Meso-topics do not neces-sarily represent actual topics of conversations, but certainly are important interactive handles used by the speakers. It is our hypothesis that meso-topics can be effectively used to track and predict polarity changes in speakers? positions towards the overall topic of conversation. Once the meso-topics and their polarities for each participant are determined, we can generate a topical positioning map (or net-work) (TPN) showing relative distances between 
41
participants based on all meso-topics in discourse. Comparing different snapshots of the TPN over time, we can observe how the group?s dynamic changes, i.e., how some participants move closer to one another while others drift apart in the discus-sion. In particular, we suggest that TPN changes can track and predict participants? changes of opin-ion about the overall topic of conversation. The remainder of this paper is organized as follows. In Section 2, we review related work. In Section 3, we describe the components of the pro-posed technique and the way they are used to im-plement the system. In Section 4, we discuss initial empirical studies, including data collection and evaluation. In final section, we present conclusions and some future work. 2 Related Work While systematic research on opinion tracking and influence in dialogues is a relatively new area of computational linguistics, related research includes automatic opinion mining and sentiments extrac-tion from text (Wiebe et al, 2005; Strapparava and Mihalcea, 2008), speech (Vogt et al, 2008) and social networking sites (Martineau and Finin, 2009). Much of the recent work was focused on automatic analysis of product reviews (books, movies, etc.) and extracting customers? opinions from them (Hu and Liu, 2004; David and Pinch, 2006; Zhuang et al, 2006). A typical approach is to count the number of ?opinion? words within a text window around the product names, possibly augmented with syntactic parsing to get dependen-cies right. An opinion mining application can ex-tract either full opinion sentences (Philip et al, 2003) or may generate a more structured represen-tation (Hu and Liu, 2004). Another recent applica-tion of sentiment analysis is ECO system (Effective Communication Online) (Small et al, 2010) that constructs a model of a community-wide sentiment towards certain common issues discussed in social media, particularly forums and open blogs. This model is then used to assess whether a new post would fit into the targeted community by comparing the sentiment polarities about the concepts in the message and in the model. Potential posters are then guided in ways to shape their communication so that it minimizes the num-ber of conflicting concept sentiments, while still preserving the intended message.  
Another related research domain is about modeling the social phenomena in discourse. (Strzalkowski et al, 2010, Broadwell et al, 2012) proposed a two-tier approach that relies on extract-ing observable linguistic features of conversational text to detect mid-level social behaviors such as Topic Control, Disagreement and Involvement. These social behaviors are then used to infer high-er-level social roles such as Leader and Influencer, which may have impact on how other participants? opinions form and change. 3 System Modules  In this section, we describe a series of modules in our system, which include meso-topic extraction, topical positioning and topical positioning map, and explain how we capture opinion shifts. 3.1 Meso-Topic Extraction Participants mention many ideas and subjects in dialogue. We call these Local Topics, which are any noun phrases introduced that are subsequently mentioned via repetition, synonym, or pronoun (Strzalkowski et al, 2010) by the same participant or different participants. Some local topics persist for only a couple of turns, others for much longer; some are closely relevant to the overall discussion, while others may appear to be digressions. We identify local topics, their first mentions and sub-sequent mentions, and track participants who make these mentions. Once local topics have been intro-duced into the dialogue we track their persistence as topic chains, through repetitions of the noun phrase as well as references via pronouns and the use of synonyms. Topic chains do not have to be continuous, they may contain gaps. The lengths of these gaps are also important to measures for some behaviors. Meso-topics are the most persistent lo-cal topics, topics that are widely cited through long stretches of discourse. A selection of meso-topics is closely associated with the task in which the dis-course participants are engaged. Short ?gaps? in the chain are permitted (up to 10 turns, to accom-modate digressions, obscure references, noise, etc.). Meso-topics can be distinguished from the local topics because the participants often make polar-ized statements about them. We use the Stanford part-of-speech tagger (Klein and Manning, 2003) to automatically detect nouns and noun phrases in dialogue and select those with subsequent men-
42
tions as local topics using a fairly simple pronoun resolution method based primarily on presence of specific lexical features as well as temporal dis-tance between utterances. Princeton Wordnet (Fellbaum et al, 2006) is consulted to identify synonyms and other related words commonly used in co-references. The local topics that form suffi-ciently long co-reference chains are designated as meso-topics. 3.2 Topical Positioning Topical Positioning is defined as the attitude a speaker has towards the meso-topics of discussion. Speakers in a dialogue, when discussing issues, especially ones with some controversy, will estab-lish their attitude on each topic, classified as for, against, or neutral/undecided. In so doing, they establish their positions on the issue or topic, which shapes the agenda of the discussion and also shapes the outcomes or conclusions of the discus-sion. Characterizing topical positioning allows us to see the speakers who are for, who are against, and who are neutral/undecided on a given topic or issue. To establish topical positioning, we first identify meso-topics that are present in a discourse. For each utterance made by a speaker on a meso-topic we then establish its polarity, i.e., if this ut-terance is ?for? (positive) or ?against? (negative), or neutral on the topic. We distinguish three forms of meso-topic valuation that may be present: (a) ex-press advocacy/disadvocacy, when the valuation is applied directly to the topic (e.g., ?I?m for Carla?); (b) supporting/dissenting information, when the valuation is made indirectly by offering additional information about the topic (e.g., ?He's got experi-ence with youngsters.?); and (c) express agree-ment/disagreement with a polarized statement made by another speaker. The following measures of Topical Position-ing are defined: Topic Polarity Index, which estab-lishes the polarity of a speaker?s attitude towards the topic, and Polarity Strength Index, which measures the magnitude of this attitude.   [Topic Polarity Index (TPX)] In order to detect the polarity of Topical Positioning on meso-topic T, we count for each speaker: - All utterances on T using statements with po-larity P applied directly to T using appropriate 
adverb or adjective phrases, or when T is a di-rect object of a verb. Polarities of adjectives and adverbs are taken from the expanded ANEW lexicon (Bradley and Lang, 1999). - All utterances that offer information with po-larity P about topic T. - All responses to other speakers? statements with polarity P applied to T. In the Twitter environment (and the like), for now we in-clude a re-tweet in this category. Given these counts we can calculate TPX for each speaker as a proportion of positive, negative and neutral polarity utterances made by this speaker about topic T. A speaker whose utterances are overwhelmingly positive (80% or more) has a pro-topic position (TPX = +1); a speaker whose utter-ances are overwhelmingly negative takes an against-topic position (TPX = ?1); a speaker whose utterances are largely neutral or whose utterances vary in polarity, has a neutral/undecided position on the topic (TPX = 0).  [Polarity Strength Index (PSX)] In addition to the valence of the Topical Positioning, we also wish to calculate its strength. To do so, we calculate the proportion of utterances on the topic made by each speaker to all utterances made about this topic by all speakers in the discourse. Speakers, who make most utterances on the topic relative to other speakers, take a stronger position on this topic. PSX is measured on a 5-point scale corresponding to the quintiles in normal distribution.   Topical Positioning Measure (TPM)  In order to establish the value of Topical Position-ing for a given topic we combine the values of TPX*PSX. Topical Positioning takes values be-tween +5 (strongest pro) to 0 (neutral/undecided) to ?5 (strongest against). For example, a speaker who makes 25% of all utterances on the topic ?Carla? (group mean is 12%) and whose most statements are positive, has the strongest pro Topi-cal Positioning on Carla: +5 (for fifth quintile on the positive side). 3.3 Topical Positioning Map (TPN) Given the combined values of TPM for each par-ticipant in a group, we can calculate distances be-tween the speakers on each meso-topic as well as on all meso-topics in a conversation. For meso-
43
topics (t1, ? tN), the distance is calculated using a cosine between speakers? ?vectors? (TPMt1(A) ? TPMtN(A)) and (TPMt1(B) ? TPMtN(B)). Specifi-cally, we use (1-Cosine(V1, V2)) to represent dis-tance between node V1 and V2 in the network, where the range becomes 0 to 2.  With the aid of TPN, we can detect the opin-ion shifts and model the impact of speakers with specific social roles in the group, which in our case is the influencer. An influencer is a group partici-pant who has credibility in the group and introduc-es ideas that others pick up on or support. An influencer model is generated from mid-level soci-olinguistic behaviors, including Topic Control, Disagreement and Involvement (Shaikh et al, 2012). In order to calculate effect of the influencer on a group, we track changes in the TPN distances between speakers, and particularly between the influencer and other speakers. We want to know if the other speakers in the group moved closer to or further away from the influencer, who may be promoting a particular position on the overall sub-ject of discussion. Our hypothesis is that other par-ticipants will move closer (as a group, though not necessarily individually) to an influential speaker. We may also note that some speakers move closer while others move away, indicating a polarizing effect of an influential speaker. If there is more than one influencer in the group these effects may be still more complex. 4 Data Collection and Experiment  Our initial focus has been on Twitter discussions which enable users to create messages, i.e., ?tweets?. There are plenty of tweet messages gen-erated all the time and it is reported that Twitter has surpassed 400 million tweets per day. With the Twitter API, it is easy to collect those tweets for research, as the communications are considered public. However, most of data obtained publicly is of limited value due to its complexity, lack of fo-cus, and inability to control for many independent variables. In order to derive reliable models of conversational behavior that fulfill our interests in opinion change, we needed a controlled environ-ment with participants whose initial opinions were known and with conversation reasonably focused on a topic of interest. To do so, we recruited partic-ipants for a two-week Twitter debates on a variety of issues, one of the topics was ?Should the mini-
mum legal drinking age be lowered to 18?? We captured participants? initial positions through sur-veys before each debate, and their exit positions through surveys after the debate was completed two weeks later. The surveys were designed to col-lect both the participants? opinions about the over-all topic of conversation as well as about the roles they played in it. These data were then compared to the automatically computed TPN changes. 4.1 Data Collection To obtain a suitable dataset, we conducted two groups of controlled and secured experiments with Twitter users. The experiment was specially de-signed to ensure that participants stay on topic of discussion and that there was a minority opinion represented in the group. We assigned the same overall topic for both groups: ?lowering the drink-ing age from 21 to 18?. Before the discussion, the participants completed an 11-question survey to determine their pre-discussion attitudes toward overall topic. One participant with the minority opinion was then asked to act as an influencer in the discussion, i.e., to try to convince as many people as possible to adopt his or her position. Af-ter the discussion, the participants were asked the same 11 questions to determine if their positions have changed. All 11 questions probed various aspects of the overall topic, thus providing a relia-ble measure of participant?s opinion. All responses were on a 7-point scale from ?strongly agree? to ?strongly disagree?. The orientation of individual questions vs. the overall topic was varied to make sure that the participants did not mechanically fill their responses. Some of the questions were:  (1) Lowering the drinking age to 18 would make alcohol less of a taboo, making alcohol consump-tion a more normalized activity to be done in mod-eration.   +3 strongly agree ----- -3 strongly disagree (2) 18 year olds are more susceptible to binge drinking and other risky/irresponsible behaviors than people who are 21 and older. -3 strongly agree ----- +3 strongly disagree  (note reversed polarity) The basic statistical information about the two ex-perimental groups is given in Table 1 and the tweet distribution of each participant in Group-1 is shown in Figure 1. Participants are denoted by a 
44
two-letter abbreviation (WS, EP and so on). The current data set is only a fraction of a larger cor-pus, which is currently under development. Addi-tional datasets cover a variety of discussion topics and involve different groups of participants.  Group # participants # tweets Influencer 1 20 225 WS 2 14 222 EP Table 1: Selected details of two experimental groups.   
 Figure 1: Tweet distribution for each participant in Group-1 where participants with asterisk are against ?lowering drinking age?.  As we would like to know the participants? pre- and post-discussion attitudes about the overall top-ic, we used the responses on 11 survey questions to calculate how strongly participants feel on the overall topic of discussion. Each question is given on a seven-point scale ranging from ?+3? to ?-3?, where ?+3? implies strongly agree to keep drinking age at 21 and ?-3? means strongly disagree. Posi-tions of participants are determined by adding the scores of the 11 questions according to their re-sponses on pre- or post- discussion questionnaires. Figure 2 is an example of pre-discussion responses for two participants in Group-1. WS largely agrees that drinking age should be kept at 21 whereas EK has an opposing opinion. The pre- and post-discussion attitudes of participants in Group-1 to-wards the overall topic are shown in Figure 3.  
 Figure 2: Pre-discussion survey scores of WS and EK. 
Subsequently, we computed relative pre-discussion attitude distance between each participant and the influencer based on the pre-discussion surveys and their post-discussion attitude distance based on the post-discussion surveys. We normalized these dis-tances to a [0, 2] interval to be consistent with co-sine distance computation scale used in the TPN module. The changes from pre-discussion attitude distance to post-discussion attitude distance based on the surveys are considered the gold standard against which the system-computed TPN values are measured. As shown in Figure 4(a), the pre-discussion distance between WS and EK is 1.43 (first bar) and the post-discussion distance is 0.07 (second bar), which implies their positions on the overall topic moved significantly closer. We also note that WS?s position did not change much throughout the discussion (Figure 3). This was just as we expected since WS was our designated influ-encer, and this fact was additionally confirmed in the post survey: in response to the question ?Who was the influencer in the discussion?? the majority of participants selected WS. The post survey re-sponses from the other group also confirmed our selected influencer. In addition, we used the auto-mated DSARMD system (Strzalkowski et al, 2013) to compute the most influential participants in each group, and again the same people were identified.  
 Figure 3: Pre- and post-discussion attitudes of partici-pants in Group-1 where the left bar of the participant is their pre-discussion attitude and right bar of the partici-pant is their post-discussion attitude. 
45
  Figure 4: (a) Relative position change between speakers WS (the influencer) and EK based on surveys and au-tomatically computed TPN distance. The first bar in each par corresponds to their pre-discussion distance and second bar is post-discussion distance. We note that TPN predicts correctly that WS and EK move closer together. (b) Relative position change between partici-pants WS and BC.   
4.2 Experiment After detailed analysis of participants? opinion be-fore and after the discussion, two twitter discus-sions are run through our system to extract the required information in order to compute topical positioning as explained in section 3. In Group-1, ten meso-topics were generated by our system (in-cluding, e.g., ?drinking age?, ?teens? and ?alco-hol?). Each participant?s polarity associated with these meso-topics was computed by our system to form ten-dimensional topical positioning vectors for Group-1. In our experiment, we used the first quarter of discussion to compute initial topical po-sitioning of the group and last-three quarters to compute the final topical positioning. Once the pre- and post-topical positioning were determined, the topical positioning map between participants was calculated accordingly, i.e., pre- and post-TPN. We used the first quarter of discussion for the initial TPN because we required a sufficient amount of data to compute a stable measure; how-ever, we expected it would not fully represent par-ticipants? initial positions. Nonetheless, we should still see the change when compared with post-TPN, which was computed on the last three-quarters of the discussion. In order to detect the opinion shifts and also to measure the effect of the influencer on a group, we tracked the changes in the TPN with respect to the influencer. As shown in Figure 4(a), the pre-TPN between WS and EK is 1.33 (third bar) and post-TPN is 0.72 (fourth bar). Hence, the system determines that their opinions are moving closer which conforms to the survey results. Figure 4(b) is another example of WS and BC that system result shows the same tendency as the survey result. The pre-discussion distance between WS and BC is 1.87 (first bar) and the post-discussion distance is 1.42 (second bar), which implies their positions on the overall topic moved closer after discussion. In system detection, the pre-TPN between is 1.56 (third bar) and post-TPN is 1.02 (fourth bar), which also concludes their attitudes are closer. An-other examples showing that speaker moved away from influencer are in Figure 5(a) and 5(b). Ac-cording to the survey, the pre-discussion attitude distance between WS and CC is 0.62 (first bar) and post-discussion attitude distance is 1.35 (second bar), which implies their positions diverged after the discussion. Our system determined pre-TPN between WS and CC is 1.0 (third bar) and post-
46
TPN is 1.29 (fourth bar), which shows their diver-gence.  
Figure 5: (a) Relative position change between WS and CC based on surveys and TPN. (b) Relative position change between participants WS and RF. We note that RF moves away from WS, which is correctly predicted by TPN.   In a separate exercise we also explored differ-ent parts of the Twitter session to compute pre-TPN and post-TPN, in addition to the ? vs. ? split discussed above. In particular, we computed TPN distances between speakers at first ? vs. second ?, 
first ? vs. last ?, first ? vs. last ?, etc. Experiment results show that using the first quarter of discus-sion as initial topical positioning and last quarter as final topical positioning (? vs. ?) produces the most accurate prediction of opinion changes for all group participants: 87.5% in Group-1 and 76% in Group-2.  We should also note here that there is no specific correlation between the meso-topics and the overall topic other than the meso-topics arise spontaneously in the conversation. The set of me-so-topics in the second discussion on the same top-ic was different than the in the first discussion. In particular, meso-topics are not necessarily correlat-ed with the aspects of the overall topic that are ad-dressed in the surveys. Nonetheless, the TPN changes appear to predict the changes in surveys in both discussions. At this time the results is indica-tive only. Further experiments need to be run on additional data (currently being collected) to con-firm this finding. 5 Conclusion In this paper, we described an automated approach to detect participant?s Topical Positioning and cap-ture the opinion shifts by Topical Position Maps. This work is still in progress and we intend to pro-cess more genres of data, including Twitter and on-line chat, to confirm effects seen in the data we currently have. The future work should be able to account for the relationship between meso-topic and overall topic (i.e., supporting meso-topic means for or against overall topic). A potential so-lution could be determined by aligning with TPN of influencers who are known strongly pro- or against- overall topic. Another avenue of future work is to apply proposed model on virtual chat-room agent to guide the discussion and change par-ticipants? attitudes.  References  Bradley, M. M., & Lang, P. J. 1999. Affective norms for English words (ANEW): Instruction manual and af-fective ratings(Tech. Rep. No. C-1). Gainesville, FL: University of Florida, The Center for Research in Psychophysiology. Broadwell, George, Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Taylor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu, and Nick Webb. "Modeling Socio-Cultural Phenomena in Dis-
47
course." Journal of Natural Language Engineering (2012): 1-45. David, S., and Pinch, T. J. 2006. Six degrees of reputa-tion: The use and abuse of online review and recom-mendation systems. First Monday. Special Issue on Commercial Applications of the Internet.  Fellbaum, C., B. Haskell, H. Langone, G. Miller, R. Poddar, R. Tengi and P. Wakefield. 2006. WordNet 2.1. Hu, M., and Liu, B. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI, 755?760. Klein, D., & Manning, C. D. 2003. Accurate unlexical-ized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguis-tics-Volume 1 , 423-430. Association for Computa-tional Linguistics. Martineau, J., & Finin, T. 2009. Delta tfidf: An im-proved feature space for sentiment analysis. In Pro-ceedings of the 3rd AAAI International Conference on Weblogs and Social Media, 258-261. Philip Beineke, Trevor Hastie, Christopher Manning and Shivakumar Vaithyanathan 2003. An exploration of sentiment summarization. In Proceedings of AAAI, 12-15. Shaikh, Samira, et al2012. Modeling Influence in Online Multi-party Discourse. Cloud and Green Computing (CGC), 2012 Second International Con-ference on. IEEE. Small, S., Strzalkowski, T. and Webb, N. 2010. ECO: Effective Communication Online. Technical Report ILS-015, University at Albany, SUNY Somasundaran, S., & Wiebe, J. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP. Strapparava, C., and Mihalcea, R. 2008. Learning to Identify Emotions in Text. In Proceedings of the ACM Conference on Applied Computing ACM-SAC. Strzalkowski, T.; Broadwell, G. A.; Stromer-Galley, J.; Shaikh, S.; Taylor, S.; and Webb, N. 2010. Modeling socio-cultural phenomena in discourse. In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics, 1038-1046. Strzalkowski, T., Samira Shaikh, Ting Liu, George Aa-ron Broadwell, Jennifer Stromer-Galley, Sarah M. Taylor, Veena Ravishankar, Umit Boz, Xiaoai Ren: Influence and Power in Group Interactions. SBP 2013: 19-27 Vogt, T., Andre?, E., & Bee, N. 2008. EmoVoice?A framework for online recognition of emotions from voice. Perception in Multimodal Dialogue Systems, 188-199. Wiebe, J., Wilson, T., and Cardie, C. 2005. Annotating expressions of opinions and emotions in language. 
Journal of Language Resources and Evaluation 39(2-3):165?210 Zhuang, L., Jing, F., Zhu, X. Y., & Zhang, L. 2006. Movie review mining and summarization. In Confer-ence on Information and Knowledge Management: Proceedings of the 15 th ACM international confer-ence on Information and knowledge management, 43-50. 
48
Proceedings of the Second Workshop on Metaphor in NLP, pages 42?51,
Baltimore, MD, USA, 26 June 2014. c?2014 Association for Computational Linguistics
Computing Affect in Metaphors 
Tomek Strzalkowski1,2, Samira Shaikh1, Kit Cho1, George Aaron Broadwell1, Laurie Feldman1, Sarah Taylor3, Boris Yamrom1, Ting Liu1, Ignacio Cases1, Yuliya Peshkova1 and Kyle Elliot4 1State University of New York - Univer-sity at Albany  
2Polish Academy of Sciences 3Sarah M. Taylor Consulting LLC 4Plessas Experts Network tomek@albany.edu     Abstract 
This article describes a novel approach to automated determination of affect associ-ated with metaphorical language. Affect in language is understood to mean the at-titude toward a topic that a writer at-tempts to convey to the reader by using a particular metaphor. This affect, which we will classify as positive, negative or neutral with various degrees of intensity, may arise from the target of the meta-phor, from the choice of words used to describe it, or from other elements in its immediate linguistic context. We attempt to capture all these contributing elements in an Affect Calculus and demonstrate experimentally that the resulting method can accurately approximate human judgment. The work reported here is part of a larger effort to develop a highly ac-curate system for identifying, classifying, and comparing metaphors occurring in large volumes of text across four differ-ent languages: English, Spanish, Russian, and Farsi. 1 Introduction We present an approach to identification and val-idation of affect in linguistic metaphors, i.e., metaphorical expressions occurring in written language. Our method is specifically aimed at isolating the affect conveyed in metaphors as opposed to more broad approaches to sentiment classification in the surrounding text. We demonstrate experimentally that our basic Affect Calculus captures metaphor-related affect with a high degree of accuracy when applied to neutral metaphor targets. These are targets that them-selves do not carry any prior valuations. We sub-
sequently expanded and refined this method to properly account for the contribution of the prior affect associated with the target as well as its immediate linguistic context.  2 Metaphor in Language Metaphors are mapping systems that allow the semantics of a familiar Source domain to be ap-plied to a new Target domain so as to invite new frameworks for reasoning (usually by analogy) to emerge in the target domain. The purpose of a metaphor is (a) to simplify or enable reasoning and communication about the target domain that would otherwise be difficult (because of tech-nical complexity) or impossible (due to lack of agreed upon vocabulary) (e.g., Lakoff & John-son, 1980; 2004); or (b) to frame the target do-main in a particular way that enables one form of reasoning while inhibiting another (e.g., Thibodeau & Boroditsky, 2011). The two rea-sons for using metaphors are not necessarily mu-tually exclusive, in other words, (a) and (b) can operate at the same time. The distinction sug-gested above has to do with affect: a metaphor formed through (a) alone is likely to be neutral (e.g., client/server, messenger DNA), while a metaphor formed using (b) is likely to have a polarizing affect (e.g., tax?s burden).  The Source and Target domains that serve as endpoints of a metaphoric mapping can be repre-sented in a variety of ways; however, in a nut-shell they are composed of two kinds of things: concepts and relations. In a Target domain the concepts are typically abstract, disembodied, of-ten fuzzy concepts, such as crime, mercy, or vio-lence, but may also include more concrete, novel, or elaborate concepts such as democracy or eco-nomic inequality. In a Source domain, the con-cepts are typically concrete and physical; howev-er, mapping between two abstract domains is 
42
also possible. (E.g., crime may be both a target and a source domain.)  The relations of interest are those that operate between the concepts within a Source domain and can be ?borrowed? to link concepts within the Target domain, e.g., ?Crime(TARGET) spread to(RELATION) previously safe areas? may be bor-rowing from a DISEASE or a PARASITE source domain.  3 Related Research: metaphor detection Most current research on metaphor falls into three groups: (1) theoretical linguistic approach-es (as defined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that attempt to correlate met-aphor semantics with their usage in naturally oc-curring text but generally lack robust tools to do so; and (3) social science approaches, particular-ly in psychology and anthropology that seek to explain how people produce and understand met-aphors in interaction, but which lack the neces-sary computational tools to work with anything other than relatively isolated examples. In computational investigations of metaphor, knowledge-based approaches include MetaBank (Martin, 1994), a large knowledge base of meta-phors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the ex-istence of lexical resources that may not always be present or satisfactorily robust in different languages. Gedigan et al. (2006) identify a sys-tem that can recognize metaphor; however their approach is only shown to work in a narrow do-main (The Wall Street Journal, for example).  Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an over-view). Baumer et al. (2010) used semantic role labels and typed dependency parsing in an at-tempt towards computational metaphor identifi-cation. However, they describe their own work as an initial exploration and hence, inconclusive. Shutova et al. (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute met-
aphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is difficult to produce in large quantities and may not be easily generated in different languages. Several other similar approaches were recently reported at the Meta4NLP 1  workshop, e.g., (Mohler et al., 2013; Wilks et al., 2013; Hovy et al., 2013). Most recently, a significantly different ap-proach to metaphor understanding based on lexi-cal semantics and discourse analysis was intro-duced by Strzalkowski et al. (2013). Space con-straints limit our discussion about their work in this article, however in the foregoing, our discus-sion is largely consistent with their framework. 4 Affect in Metaphors Affect in language is understood to mean the atti-tude toward a topic that a speaker/writer attempts to convey to the reader or audience via text or speech (van der Sluis and Mellish 2008).  It is expressed through multiple means, many of which are unrelated to metaphor. While affect in text is often associated, at least in theory, with a variety of basic emotions (anger, fear, etc.), it is generally possible to classify the set of possible affective states by polarity: positive, negative, and sometimes neutral. Affect is also considered to have a graded strength, sometimes referred to as intensity.  Our approach to affect in metaphor has been vetted not only by our core linguistic team but also by an independent team of linguist-analysts with whom we work to understand metaphor across several language-culture groups. Our re-search continues to show no difficulties in com-prehension or disagreement across languages concerning the concept of linguistic affect, of its application to metaphor, and of its having both polarity and intensity.  5 Related Research: sentiment and af-fect There is a relatively large volume of research on sentiment analysis in language (Kim and Hovy, 2004; Strapparava and Mihalcea, 2007; Wiebe and Cardie, 2005; inter alia) that aim at detecting polarity of text, but is not specifically concerned with metaphors. A number of systems were de-veloped to automatically extract writer?s senti-                                                1 The First Workshop on Metaphor in NLP. http://aclweb.org/anthology//W/W13/W13-09.pdf 
43
ment towards specific products or services such as movies or hotels, from online reviews (e.g., Turney, 2002; Pang and Lee, 2008) or social me-dia messages (e.g., Thelwall et al., 2010). None of these techniques has been applied specifically to metaphorical language, and it is unclear if the-se alone would be sufficient due to the relatively complex semantics involved in metaphor inter-pretation. Socher et al. (2013 cite) have recently used recursive neural tensor networks to classify sentences into positive/negative categories. However, the presence of largely negative con-cepts such as ?poverty? in a given sentence overwhelms the sentiment for the sentence in their method. Other relevant efforts in sentence level sentiment analysis include Sem-Eval Task2.  While presence of affect in metaphorical lan-guage is well documented in linguistic and psy-cholinguistic literature (e.g., Osgood, 1980; Pavio and Walsh, 1993; Caffi and Janney, 1994; Steen, 1994), relatively little work was done to detect affect automatically. Some notable recent efforts include Zhang and Barnden (2010), Veale and Li (2012), and Kozareva (2013), who pro-posed various models of metaphor affect classifi-cation based primarily on lexical features of the surrounding text: specifically the word polarity information. In these and other similar approach-es, which are closely related to sentiment analy-sis, affect is attributed to the entire text fragment: a sentence or utterance containing a metaphor, or in some cases the immediate textual context around it.  In contrast, our objective is to isolate affect due to the metaphor itself, independently of its particular context, and also to determine how various elements of the metaphoric expression contribute to its polarity and strength. For exam-ple, we may want to know what is the affect conveyed about the Government as a target con-cept of the metaphor in ?Government regulations are crushing small businesses.? and how it dif-fers in  ?Government programs help to eradicate poverty in rural areas.? or in ?Feds plan to raise the tax on the rich.? In all these examples, there is a subtle interplay between the prior affect as-sociated with certain words (e.g., ?crush?, ?pov-erty?) and the semantic role they occupy in the sentence (e.g., agent vs. patient vs. location, etc.). Our objective is to develop an approach that can better explain such differences. Not sur-prisingly, in one of the target domains we are investigating, the Economic Inequality domain,                                                 2 https://www.cs.york.ac.uk/semeval-2013/task2/ 
there is considerable agreement on the basic atti-tudes across cultures towards the key target con-cepts: poverty is negative, wealth is positive, taxation is largely negative, and so on. This is in a marked contrast with another Target domain, the Governance domain where the target con-cepts tend to be neutral (e.g. bureaucracy, regula-tions etc.) Another important motivation in developing our approach (although not discussed in this pa-per) is to obtain a model of affect that would help to explain empirically why metaphorically rich language is considered highly influential. Persua-sion and influence literature (Soppory and Dillard, 2002) indicates messages containing metaphorical language produce somewhat great-er attitude change than messages that do not. However, some recent studies (e.g., Broadwell et al., 2012) found that lexical models of affect, sentiment, or emotion in language do not corre-late with established measures of influence, con-trary to expectations. Therefore, a different ap-proach to affect is needed based both on lexical and semantic features. We describe this new model below, and show some preliminary results in applications to metaphors interpretation. 6 Basic Affect Calculus The need for a new approach to affect arises from the inability of the current methods of sen-timent analysis to capture the affect that is con-veyed by the metaphor itself, which may be only a part of the overall affect expressed in a text. Affect conveyed in metaphors, while often more polarized than in literal language, is achieved using subtler, less explicit, and more modulated expressions. This presents a challenge for NLP approaches that base affect determination upon the presence of explicit sentiment markers in language that may mask affect arising from a metaphor. This problem becomes more challeng-ing when strong, explicit sentiment markers are present in a surrounding context or when the atti-tude of the speaker/writer towards the target con-cept is considered.  Our initial objective is thus to detect and clas-sify the portion of affect that the speaker/writer is trying to convey by choosing a specific meta-phor. The observables here are the linguistic metaphors that are actually uttered or written; therefore, our method must be able to determine affect present in the linguistic metaphors first and then extrapolate to the conceptual metaphor based on evidence across multiple uses of the 
44
same metaphor. Conceptual metaphors are posit-ed by instances of linguistic metaphors that point to the same source domain. We choose initially to model the speaker/writer perspective; howev-er, it may also be important to determine the ef-fect that a metaphor has on the reader/listener, which we do not address here. Affect in metaphor arises from the juxtaposi-tion of a Source and a Target domain through the relations explicated in linguistic metaphors. The-se relations typically involve one or more predi-cates from the source domain that are applied to a target concept. For example, in ?Government regulations are crushing small businesses.? the relation ?crushing? is borrowed from a concrete source domain (e.g., Physical Burden), and used with an abstract target concept of ?government regulation? which becomes the agentive argu-ment, i.e., crushed(GovReg, X), where X is an optional patientive argument, in this case ?small businesses?. Thus, government regulation is said to be doing something akin to ?crushing?, a harmful and negative activity according to the Affective Norms in English (ANEW) psycholin-guistic database (Bradley and Lang, 1999). Since ?government regulation? is doing something negative, the polarity of affect conveyed about it is also negative. The ANEW lexicon we are us-ing contains ratings of ~100K words. The origi-nal ANEW lexicon by Bradley and Lang was expanded following the work done by Liu et al. (2014) in expanding the MRC imageability lexi-con. While other sources of valence judgments exist such as NRC (Mohammad et al., 2013) and MPQA (Weibe and Cardie, 2005), there are limi-tations ? for instance ? NRC lexicon rates each words on a positive or negative scale, which does not allow for more fine-grained analysis of strength of valence.  Calculation from Table 1 is further general-ized by incorporating the optional second argu-ment of the relation and the role of the target concept (i.e., agentive or patientive). Thus, if X=?small business? as in the example above, the complete relation becomes crushed(GovReg, 
SmBus), which retains negative affect assuming that ?small business? is considered positive or at least neutral, an assessment that needs to be es-tablished independently. The above calculations are captured in the Af-fect Calculus (AC), which was derived from the sociolinguistic models of topical positioning and disagreement in discourse (Broadwell et al., 2013).     The Affect Calculus was conceived as a hypo-thetical model of metaphorical affect, involving the metaphor target, the source relation, as well as the arguments of this relation, one of which is the target itself. The basic version of the AC is shown in Table 1. We should note that the AC allows us to make affect inferences about any of the elements of the metaphoric relation given the values of the remaining elements. We should also note that this calculus does not yet incorpo-rate any discernable prior affect that the target concept itself may carry. When the target con-cept may be considered neutral (as is ?govern-ment regulation? when taken out of context) this table allows us to compute the affect value of any linguistic metaphor containing it. This is un-like the target concepts such as ?poverty? which bring their prior affect into the metaphor. We will return to this issue later. In the Affect Calculus table, Relation denotes a unary or binary predicate (typically a verb, an adjective, or a noun). In the extended version of the AC (Section 6) Relation may also denote a compound consisting of a predicate and one or more satellite arguments, i.e., arguments other than AGENT or PATIENT, such as ORIGIN or DES-TINATION for motion verbs, etc.  7 Extended Affect Calculus The basic Affect Calculus does not incorporate any prior affect that the target concept might bring into a metaphor. This is fine in some do-mains (e.g., Government), where most target concepts may be considered neutral. But in other target domains, such as the Economic Inequality domain, many of the target concepts have a 
Relation type Type 1 (proper-tive) Rel(Target) Type 2 (agentive) Rel (Target, X) Type 3 (patientive) Rel(X, Target) Relation/X  X ? neutral X < neutral X ? neutral X < neutral Positive POSITIVE POSITIVE ? UNSYMP POSITIVE ? SYMPAT Negative NEGATIVE ? UNSYMP ? SYMPAT ? SYMPAT ? SYMPAT Neutral NEUTRAL NEUTRAL ? NEUTRAL NEUTRAL ? NEUTRAL Table 1.  A simple affect calculus specifies affect polarity for linguistic metaphors using a 5-point polar-ity scale [negative < unsympathetic < neutral < sympathetic < positive]. X is the second argument. 
45
strong prior affect in most cultures (e.g., ?pov-erty? is universally considered negative). We thus need to incorporate this prior affect into our calculation whenever an affect-loaded target concept is invoked in a metaphor. Where the basic Affect Calculus simply imposes a context-borne affect upon a neutral target concept, the Advanced Affect Calculus must combine it with the prior affect carried by the target concept, de-pending upon the type of semantic context. As already discussed, we differentiate 3 basic se-mantic contexts (and additional contexts in the extended Affect Calculus discussed in the next section) where the target concept is positioned with respect to other arguments in a metaphorical expression:  ? Propertive context is when a property of a Target is specified (e.g. deep poverty, sea of wealth) ? Agentive context is when the Target appears as an agent of a relation that may involve an-other concept (Argument X) in the patient role (e.g. Government regulations are crush-ing?, Government programs help?) ? Patientive context is when the Target ap-pears in the patient role that involves another concept (possibly implicit, Argument X) in the agent role. (e.g. ?eradicate poverty., ?.navigate government bureaucracy)  Table 1 (in the previous section) specifies how to calculate the affect expressed towards the tar-get depending upon the affect associated with the Relation and the Argument X. In the Advanced Affect Calculus, this table specifies the context-borne affect that interacts with the affect associ-ated with the target. When the target prior affect is unknown or assumed neutral, the AC table is applied directly, as explained previously. When the target has a known polarized affect, either positive or negative, the values in the AC table are used to calculate the final affect by combin-ing the prior affect of the target with an appro-priate value from the table. This is necessary for affect-loaded target concepts such as ?poverty? or ?wealth? that have strong prior affect and can-not be considered neutral.  In order to calculate the combined affect we define two operators ? and ?. These operators form simple polarity algebra shown in Table 2. When the Target is in a Patientive relation, we use ?  to combine its affect with the context val-ue from the AC table; otherwise, we use ? .  In the table for ? operator, we note that combining opposing affects from the Target and the Rela-
tion causes the final affect to be undetermined (UND). In such cases we will take the affect of the stronger element (more polarized score) to prevail. ?  pos neg neu  ?  pos neg neu 
pos pos neg pos pos pos UND pos 
neg neg pos neg neg UND neg neg 
neu pos neg neu neu pos neg neu Table 2: Polarity algebra for extended affect calculus  More specifically, in order to determine the combined polarity score in these cases, we com-pute the distance between each element?s ANEW score and the closest boundary of the neutral range of scores. For example, ANEW scores are assigned on a 10-point continuum (derived from human judgments on 10-point Likert scale) from most negative (0) to most positive (9). Values in the range of 3.0 to 5.0 may be considered neutral (this range can be set differently for target con-cepts and relations): ? Poverty affect score = 1.67 (ANEW) ? 3 (neutral lower) = -1.33 ? Grasp affect score = 5.45 (ANEW) ? 5 (neutral upper)= +0.45 Consider the expression ?poverty?s grasp?. Since poverty is a polarized target concept in Propertive position, we use ? operator to com-bine its affect value with that of Relation (grasp). The result is negative: ? ?Poverty?s grasp? affect score (via AC?) = -1.33 + 0.45 = -0.82 (negative) When the combined score is close to 0 (-0.5 to +0.5) the final affect is neutral. 7.1 Exceptions The above calculus works in a majority of cases, but there are exceptions requiring specialized handling. An incomplete list of these is below (and cases will be added as we encounter them): Reflexive relations. In some cases the target is in the agentive position but semantically it is also a patient, as in ?poverty is spreading?. These cases need to be handled carefully ? although the current AC may be able to handle them in some contexts. When interpreted as an agentive rela-
46
tion, the affect of ?poverty is spreading? comes out as undetermined but would likely be output as negative on the basis of the strong negative affect associated with poverty (vs. weaker posi-tive affect of ?spreading?). When handled as a patientive relation (an unknown force is spread-ing poverty), it comes out clearly and strongly negative. Similarly, ?wealth is declining? is best handled through patientive relation. Therefore, for this AC we will treat intransitive relations as patientive.  Causative relations. Some relations denoted by causative verbs such as ?alleviate?, ?mitigate? or ?ease? appear to presuppose that their patient argument has negative affect, and their positive polarity already incorporates this assumption. Thus, ?alleviate? is best interpreted as ?reduce the negative of?, which inserts an extra negation into the calculation. Without considering this extra negation we would calculate ?alleviate(+) poverty(-)? as negative (doing something posi-tive to a negative concept), which is not the ex-pected reading. Therefore, the proposed special handling is to treat ?alleviate? and similar rela-tions as always producing positive affect when applied to negative targets.  8 Extensions to Basic Affect Calculus The basic model presented in the preceding sec-tion oversimplifies certain more complex cases where the metaphoric relation involves more than 2 arguments. Consequently, we are consid-ering several extensions to the basic Affect Cal-culus as suggested below. The foregoing should be treated as hypotheses subject to validation.  One possible extension involves relations rep-resented by verbs of motion (which is a common source domain) that involve satellite arguments such as ORIGIN and DESTINATION in addition to the main AGENT and PATIENT roles. Any polarity associated with these arguments may impact af-fect directed at the target concept appearing in one of the main role positions. Likewise, we need a mechanism to calculate affect for target concepts found in one of the satellite roles. In ?Federal cuts could push millions into poverty? the relation ?push into? involves three arguments: AGENT (Federal cuts), PATIENT (millions [peo-ple]) and DESTINATION (poverty). In calculating affect towards ?Federal cuts? it is not sufficient to consider the polarity of the predicate ?push? (or ?push into?), but instead one must consider the polarity of ?push into (poverty)? as the compo-site agentive relation involving ?federal cuts?. 
The polarity of this composite, in turn, depends upon the polarity of its destination argument. In other words: polarity(Rel(DEST)) = polarity (DEST) Thus, if ?poverty? is negative, then pushing someone or something into poverty is a harmful relation. Assuming that ?millions [people]? is considered at least neutral, we obtain negative affect for ?Federal cuts? from the basic Affect Calculus table. An analogous situation holds for the ?ORIGIN? argument, with the polarity reversed. Thus: polarity (Rel (ORIGIN)) = ~polarity (ORIGIN) In other words, the act of removing something from a bad place is helpful and positive. For ex-ample, in ?Higher retail wages would lift Ameri-cans out of poverty? the relation compound ?lift out of (poverty)? is considered helpful/positive. Again, once the polarity of the relation com-pound is established, the basic affect calculus applies as usual, thus we obtain positive affect towards ?higher retail wages?. In situations when both arguments are present at the same time and point towards potentially conflicting outcomes, we shall establish a precedence order based on the evidence from human validation data. Another class of multi-argument relations we are considering includes verbs that take an IN-STRUMENT argument, typically signaled by ?with? preposition. In this case, affect inference for the relation compound is postulated as fol-lows: polarity (Rel (INSTR))    = polarity (INSTR) if polarity(INSTR) < neutral   = polarity (Rel) otherwise In other words, using a negative (bad) instru-ment always makes the relation harmful, while using a positive or neutral instrument has no ef-fect on the base predicate polarity.  Other types of multi-argument relations may require similar treatment, and we are currently investigating further possible extensions. In all cases not explicitly covered in the extended Af-fect Calculus, we shall assume the default condi-tion that other satellite arguments (such as TIME, LOCATION, etc.) will have no impact on the po-larity of the source relation compound. In other words: polarity (Rel (s-role)) =default polarity (Rel) 9 Evaluation and Results For an evaluation, our objective is to construct a test that can evaluate the ability of an automated system to correctly identify and classify the af-
47
fect associated with linguistic and conceptual metaphors. A series of naturally occurring text samples containing a linguistic metaphor about a target concept are presented as input to the sys-tem. The system outputs the affect associated with the metaphor, as positive, negative, or neu-tral. The system output is then compared to hu-man generated answer key resulting in an accu-racy score. The evaluation thus consists of two components:  1. Determining the ground truth about affect in test samples;  2. Measuring the automated system?s ability to identify affect correctly.  Step 1 is done using human assessors who judge affect in a series of test samples. Assessors are presented with brief passages where a target concept and a relation are highlighted. They are asked to rank their responses on a 7-point scale for the following questions, among others: ? To what degree does the above passage use metaphor to describe the highlighted concept? ? To what degree does this passage convey an idea that is either positive or negative? It is strictly necessary that input to the system be metaphorical sentences, since affect may be associated with non-metaphoric expressions as well; in fact, some direct expressions may carry stronger affect than subtle and indirect meta-phors. This is why both questions on the survey are necessary: the first focuses the assessor?s at-tention on the highlighted metaphor before ask-ing about affect. If the purpose of the test is to measure the accuracy of assigning affect to a metaphor, then accuracy should be measured against the subset of expressions judged to be metaphorical.  The judgments collected from human asses-sors are tested for reliability and validity. Relia-bility among the raters is computed by measuring intra-class correlation (ICC) (McGraw & Wong, 1996; Shrout & Fleiss, 1979). Typically, a coef-ficient value above 0.7 indicates strong agree-ment. In general, our analyses have shown that we need approximately 30 or more subjects in order to obtain a reliability coefficient of at least 0.7. In addition, certain precautions were taken to ensure quality control in the data. We used the following criteria to discard a subject?s data: (1) completed the task too quickly (i.e., averaged fewer than 10 seconds for each passage); (2) gave the same answer to 85% or more of the test items; (3) did not pass a simple language profi-ciency test; or (4) did not provide correct an-swers to a set of randomly inserted control pas-
sages which have been previously judged by ex-perts to be unequivocally literal or metaphorical. Human judgments are collected using Amazon?s Mechanical Turk services. For each passage in surveys, we would collect at least 30 viable judgments. In addition, we have native language speakers who have been rigorously trained to provide expert judgments on metaphor and affect identification task. Table 3 shows the intra-class correlations for affect determination amongst Mechanical Turk subjects. Experiments were conducted in 4 languages: English, Spanish, Rus-sian, and Farsi.   English Spanish Russian Farsi Metaphor 0.864 0.853 0.916 0.720 Affect 0.924 0.791 0.713 0.797 Table 3: Intra-class correlations for metaphor and affect assessment by Mechanical Turk sub-jects In Figure 1, we present partial evidence that the human assessment collection method cap-tures the phenomenon of affect associated with metaphors. The chart clearly shows that affect tends to be more polarized in metaphors than in literal expressions. The chart is based on more than 11,000 affect judgments for English linguis-tic metaphors and literal expressions about Gov-ernance concepts. We see a highly pronounced tendency towards the polarization of affect (both positive and negative). Ratings of affect (y-axis) in metaphoric expressions (columns 5-7) are judged to be stronger, and in particular more negative than the literal expressions (columns 1-3). A similar trend occurs with other target con-cepts as well as other languages, although the data are less reliable due to smaller test samples. Once an answer key is established using the aforementioned procedures, system accuracy can be determined from a confusion matrix as shown in Table 4. In Table 4, we show system assign-ment of affect versus answer key for English Governance and Economic Inequality target metaphors. Overall accuracy across positive, negative and neutral affect for English test set of 220 samples is 74.5%. Analogous confusion ma-trices have been constructed for Spanish, Russian and Farsi. NLP resources such as parser and lex-icons for the languages other than English are not as robust or well rounded; therefore affect classi-fication accuracy in those languages is impacted.   
48
 Figure 1: Distribution of affect polarity in hu-man judgment of English literal and metaphori-cal expressions from the Governance domain. Metaphoricity of an expression (x-axis) is judged from highly literal (1) to highly metaphorical (7)   Table 5 shows the accuracy of affect detection for expressions that the system determined to be metaphors across all four languages under inves-tigation. Evaluation set for numbers reported in Table 5 contains a total of 526 linguistic meta-phors in these four languages.   English Affect Sample size = 220 System identified as Positive Negative Neutral 
Answ
er Key 
Positive 40 16 3 Nega-tive 12 109 1 Neutral 10 14 15  Table 4: Confusion matrix for affect classifi-cation in English linguistic metaphors in Gov-ernance and Economic Inequality Domain. Accu-racy is 74.5%   English Spanish Russian Farsi 
Accuracy 74.5% 71% 59% 64% Table 5: Performance on affect classification for linguistic metaphors in four languages 10 Conclusion In this paper we presented a new approach to automatic computing of affect in metaphors that exploits both lexical and semantic information in metaphorical expressions. Our method was eval-uated through a series of rigorous experiments 
where more than several dozen of qualified as-sessors judged hundreds of sentences (extracted from online sources) that contained metaphorical expressions. The objective was to capture affect associated with the metaphor itself. Our system can approximate human judgment with accuracy ranging from 59% for Russian to 74% for Eng-lish. These results are quite promising. The dif-ferences are primarily due to varied robustness of the language processing tools (such as parsers and morphological analyzers) that are available for each language. We note that a direct compar-ison to lexical approaches such as described by Kozareva (2013) is not possible at this time due to differences in assessment methodology, alt-hough it remains one of our objectives.  Our next step is to demonstrate that the new way of calculating affect can lead to a reliable model of affective language use that correlates with other established measures of influence.  Acknowledgements Supported by the Intelligence Advanced Re-search Projects Activity (IARPA) via Depart-ment of Defense US Army Research Laboratory contract number W911NF-12-C-0024. The U.S. Government is authorized to reproduce and dis-tribute reprints for Governmental purposes not-withstanding any copyright annotation thereon.  Disclaimer: The views and conclusions con-tained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either ex-pressed or implied, of IARPA, DoD/ARL, or the U.S. Government. References David W. Allbritton, Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-based schemas and text Representations: making connections through conceptual metaphors, Journal of Experimental Psychology: Learning, Memory, and Cognition, 21(3):612-625. Eric P. S. Baumer, James P. White, and Bill Tomlin-son. 2010. Comparing semantic role labeling with typed dependency parsing in computational meta-phor identification. In Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14?22, Los Angeles, California.  Margaret M. Bradley, and Peter Lang. 1999. Affective norms for English words (ANEW): Instruction manual and affective ratings. Technical Report C-2. University of Florida, Gainesville, FL. George Aaron Broadwell, Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Tay-
49
lor, Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb. 2013. Using imageability and topic chain-ing to locate metaphors in linguistic corpora. In Proceedings of International Conference on So-cial Computing, Behavioral-Cultural Modeling, & Prediction, pages 102?109. Washington D.C. George Aaron Broadwell, Jennifer Stromer-Galley, Tomek Strzalkowski, Samira Shaikh, Sarah Tay-lor, Umit Boz, Alana Elia, Laura Jiao, Ting Liu and Nick Webb. 2012. Modeling socio-cultural phenomena in discourse. Journal of Natural Lan-guage Engineering, pages 1?45. Cambridge Press. Claudia Caffi, and Richard W. Janney. 1994. Towards a pragmatics of emotive communication. Jour-nal of Pragmatics, 22:325?373. Jaime Carbonell. 1980. Metaphor: A key to extensible semantic analysis. In Proceedings of the 18th An-nual Meeting on Association for Computational Linguistics. Jonathan, Charteris-Black. 2002. Second language figurative proficiency: A comparative study of Malay and English. Applied Linguistics 23(1):104?133. Dan, Fass. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computa-tional Linguistics, 17:49-90 Jerome Feldman, and Srinivas Narayanan. 2004. Em-bodied meaning in a neural theory of language. Brain and Language, 89(2):385?392. Christiane D. Fellbaum. 1998. WordNet: An electron-ic lexical database (1st ed.). MIT Press. Matt Gedigian, John Bryant, Srini Narayanan and Branimir Ciric. 2006. Catching Metaphors. In Proceedings of the Third Workshop on Scalable Natural Language Understanding ScaNaLU 2006, pages 41?48. New York City: NY. Dirk Hovy, Shashank Shrivastava, Sujay Kumar Jau-har, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney Sanders and Eduard Hovy. 2013. Identi-fying Metaphorical Word Use with Tree Kernels. In the Proceedings of the First Workshop on Met-aphor in NLP, (NAACL). Atlanta. Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th international conference on Computational Linguistics, COLING ?04. Zornitsa Kozareva. 2013. Multilingual Affect Polarity and   Valence Prediction in Metaphor-Rich Texts. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013) Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Proceedings of the Workshop on Computation-al Approaches to Figurative Language, pages 13?20. Rochester, NY. George Lakoff, and Mark Johnson. 1980. Metaphors we live by. University Of Chicago Press, Chicago, Illinois. 
George, Lakoff. 2001. Moral politics: what conserva-tives know that liberals don?t. University of Chi-cago Press, Chicago, Illinois. Ting Liu, Kit Cho, George Aaron Broadwell, Samira Shaikh, Tomek Strzalkowski, John Lien, Sarah Taylor, Laurie Feldman, Boris Yamrom, Nick Webb, Umit Boz and Ignacio Cases. 2014. Auto-matic Expansion of the MRC Psycholinguistic Da-tabase Imageability Ratings. In Proceedings of 9th Language Resources and Evaluation Conference, (LREC 2014)Reykjavik, Iceland. Liisa, Malkki.  1992. National geographic: The root-ing of people and the territorialization of national identity among scholars and refugees. Society for Cultural Anthropology, 7(1):24?44. James Martin. 1988. A computational theory of meta-phor. Ph.D. Dissertation. Kenneth O. McGraw and S. P. Wong. 1996. Forming inferences about some intraclass correlation coef-ficients. Psychological Methods, 1(1): 30?46. Mohammad, S.M., S. Kiritchenko, and X. Zhu. 2013. NRC-Canada: Building the state-of-the-art insen-timent analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation Exercises (SemEval-2013), Atlanta, Georgia, USA, June 2013. Michael Mohler, David Bracewell, David Hinote, and Marc Tomlinson. 2013. Semantic signatures for example-based linguistic metaphor detection. In The Proceedings of the First Workshop on Meta-phor in NLP, (NAACL), pages 46?54. Musolff, Andreas. 2008. What can critical metaphor analysis add to the understanding of racist ideolo-gy? Recent studies of Hitler?s anti-semitic meta-phors, critical approaches to discourse analysis across disciplines. Critical Approaches to Dis-course Analysis Across Disciplines, 2(2):1?10. Kieran, O?Halloran. 2007. Critical discourse analysis and the corpus-informed interpretation of meta-phor at the register level. Oxford University Press Charles E. Osgood. 1981. The cognitive dynamics of synaesthesia and metaphor. In Proceedings of the National Symposium for Research in Art. Learn-ing in Art: Representation and Metaphor, pages 56-80. University of Illinois Press. Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135, January. Allan Pavio and Mary Walsh. 1993. Psychological processes in metaphor comprehension and memory. In Andrew Ortony, editor, Meta-phor and thought (2nd ed.). Cambridge: Cambridge University Press. Patrick E Shrout and Joseph L Fleiss. 1979. Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2):420?428. Ekaterina Shutova. 2010. Models of metaphors in NLP. In Proceedings of ACL 2010. Uppsala, Swe-den. 
50
Ekaterina Shutova and Simone Teufel. 2010a. Meta-phor corpus annotated for source - target domain mappings. In Proceedings of Language Resources and Evaluation Conference 2010. Malta. Ekaterina Shutova. 2010b. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages 688?697. Ekaterina Shutova, Tim Van de Cruys, and Anna Korhonen. 2012. Unsupervised metaphor para-phrasing using a vector space model In Proceed-ings of COLING 2012, Mumbai, India Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Chris Manning, Andrew Ng and Chris Potts. 2013. In Proceedings Conference on Empir-ical Methods in Natural Language Processing (EMNLP 2013). Seattle, USA.  Sopory, P. and Dillard, J. P. (2002), The Persuasive Effects of Metaphor: A Meta-Analysis. Human Communication Research, 28: 382?419. doi: 10.1111/j.1468-2958.2002.tb00813.x Gerard Steen. 1994. Understanding metaphor in lit-erature: An empirical approach. London: Long-man. Carlo, Strapparava, and Rada Mihalcea. 2007. Semeval-2007 task 14: Affective text. In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations, pages 70?74. Association for Computational Linguistics. Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases and Kyle Elliott. 2013. Robust extraction of metaphor from novel data. In Proceedings of Workshop on Metaphor in NLP, NAACL. Atlanta. Mike Thelwall, Kevan Buckley, and Georgios Pato-glou. Sentiment in Twitter events. 2011. Journal of the American Society for Information Science and Technology, 62(2):406?418. Paul H. Thibodeau and Lera Boroditsky. 2011. Meta-phors We Think With: The Role of Metaphor in Reasoning. PLoS ONE 6(2): e16782. Peter D, Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised clas-sification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ?02, pages 417?424. Ielka van der Sluis,  and C. Mellish 2008. Toward affective natural language deneration: Empirical investigations. affective language in human and machine. AISB 2008 Proceedings Volume 2. Tony Veale and Guofu Li. 2012. Specifying view-point and information need with affective meta-phors: a system demonstration of the metaphor magnet web app/service. In Proceedings of the ACL 2012 System Demonstrations, ACL ?12, pag-es 7?12. Janyce, Wiebe and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. In Language Resources and Evaluation. 
Yorick, Wilks. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cambridge University Press, Cambridge, U.K., 329?348. Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton. 2013. Automatic Metaphor Detection us-ing Large-Scale Lexical Resources and Conven-tional Metaphor Extraction. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta.  Wiebe, J., Wilson, T., and Cardie, C.: Annotating expressions of opinions and emotions in  lan-guage. Language Resources and Evaluation, 39(2-3), pp. 165-210 (2005). Li Zhang and John Barnden. 2010. Affect and meta-phor sensing in virtual drama. International Journal of Computer Games Technology. Vol. 2010.  
51
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 210?220,
Dublin, Ireland, August 23, 2014.
Discovering Conceptual Metaphors Using Source Domain Spaces 
  Samira Shaikh1, Tomek Strzalkowski1, Kit Cho1, Ting Liu1, George Aaron Broadwell1, Laurie Feldman1, Sarah Taylor2, Boris Yamrom1, Ching-Sheng Lin1, Ning Sa1, Ignacio Cases1, Yuli-ya Peshkova1 and Kyle Elliot3  1State University of New York  ? University at Albany 2Sarah M. Taylor Consulting LLC samirashaikh@gmail.com     
3Plessas Experts Network 
 Abstract This article makes two contributions towards the use of lexical resources and corpora; specifically making use of them for gaining access to and using word associations. The direct application of our approach is for detecting linguistic and conceptual metaphors automatically in text. We describe our method of building conceptual spaces, that is, defining the vocabulary that characterizes a Source Domain (e.g., Disease) of a conceptual metaphor (e.g., Poverty is a Disease). We also describe how these conceptual spaces are used to group linguistic metaphors into conceptual metaphors. Our method works in multiple languages, including English, Spanish, Russian and Farsi. We provide details of how our method can be evaluated and evaluation results that show satisfactory performance across all languages. 1 Introduction Metaphors are communicative devices that are pervasive in discourse. When understood in a cultural context, they provide insights into how a culture views certain salient concepts, typically broad, abstract concepts such as poverty or democracy. In our research, we are focusing on metaphors on targets of governance, economic inequality and democracy, although our approach works for metaphors on any target. Suppose it is found in a culture that its people use metaphors when speaking of poverty; for example, they may talk about ?symptom of poverty? or that ?poverty infects areas of the city?. These expressions are linguistic metaphors that are instances of a broader conceptual metaphor: Poverty is a Disease. Similarly, if it is found that common linguistic metaphors about poverty for peoples of a culture include ?deep hole of poverty? and ?fall into poverty?, it would lead to the conceptual metaphor: Poverty is an Abyss. A communicator wishing to speak of ways to deal with poverty would use metaphors such as ?treat poverty? and ?cure poverty? to make their framing consistent with the conceptual metaphor of Disease, whereas she would use metaphors such as ?lift out of poverty? when speaking to people who are attuned to the Abyss conceptual metaphor. Here Disease and Abyss are source domains, and poverty is the target domain. Relations, like ?symptom of?, ?infect? and ?fall into? from the respective source domains are mapped onto the target domain of poverty. In order to discover conceptual metaphors and group linguistic metaphors together, we make use of corpora to define the conceptual space that characterizes a source domain. We wish to discover the set of relations that are used literally for a given source domain, and would create metaphors if applied to some other target domain. That is, we wish to automatically discover that relations such as ?symptom?, ?infect?, ?treat? and ?cure? characterize the source domain of Disease, for example. To create the conceptual spaces, we employ a fully automated method in which we search a balanced corpus using specific search patterns. Search patterns are so created as to look for co-occurence of                                                 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/  
210
relations with members of a given source domain. Relations could be nouns, verbs, verb phrases and adjectives that are frequently used literally within a source domain. In addition, we calculate the frequency with which relations occur in a given source domain, or Relation Frequency. We then calculate the Inverse Domain Frequency (IDF), a variant of the inverse document frequency measure quite commonly used in field of information retrieval; the IDF captures the degree of distribution of relations across all source domains under consideration. Using these two measures, the relation frequency and inverse domain frequency, we are able to rank relations within a source domain. This ranked list of relations are then used to group linguistic metaphors belonging to the same source domain together. A group of linguistic metaphors so formed is a conceptual metaphor.  2 Related Research Most current research on metaphor falls into three groups: (1) theoretical linguistic approaches (as de-fined by Lakoff & Johnson, 1980; and their followers) that generally look at metaphors as abstract language constructs with complex semantic properties; (2) quantitative linguistic approaches (e.g., Charteris-Black, 2002; O?Halloran, 2007) that attempt to correlate metaphor semantics with their us-age in naturally occurring text but generally lack robust tools to do so; and (3) social science ap-proaches, particularly in psychology and anthropology that seek to explain how people deploy and understand metaphors in interaction, but which lack the necessary computational tools to work with anything other than relatively isolated examples.     Metaphor study in yet other disciplines has included cognitive psychologists (e.g., Allbritton, McKoon & Gerrig, 1995) who have focused on the way metaphors may signify structures in human memory and human language processing. Cultural anthropologists, such as Malkki in her work on ref-ugees (1992), see metaphor as a tool to help outsiders interpret the feelings and mindsets of the groups they study, an approach also reflective of available metaphor case studies, often with a Political Sci-ence underpinning (Musolff, 2008; Lakoff, 2001).      In computational investigations of metaphor, knowledge-based approaches include MetaBank (Mar-tin, 1994), a large knowledge base of metaphors empirically collected. Krishnakumaran and Zhu (2007) use WordNet (Felbaum, 1998) knowledge to differentiate between metaphors and literal usage. Such approaches entail the existence of lexical resources that may not always be present or satisfacto-rily robust in different languages. Gedigan et al (2006) identify a system that can recognize metaphor. However their approach is only shown to work in a narrow domain (Wall Street Journal, for example).     Computational approaches to metaphor (largely AI research) to date have yielded only limited scale, often hand designed systems (Wilks, 1975; Fass, 1991; Martin, 1994; Carbonell, 1980; Feldman & Narayan, 2004; Shutova & Teufel, 2010; inter alia, also Shutova, 2010b for an overview). Baumer et al (2010) used semantic role labels and typed dependency parsing in an attempt towards computational metaphor identification. However, they self-report their work to be an initial exploration and hence, inconclusive. Shutova et al (2010a) employ an unsupervised method of metaphor identification using nouns and verb clustering to automatically impute metaphoricity in a large corpus using an annotated training corpus of metaphors as seeds. Their method relies on annotated training data, which is diffi-cult to produce in large quantities and may not be easily generated in different languages.  More recently, several important approaches to metaphor extraction have emerged from the IARPA Metaphor program, including Broadwell et al (2013), Strzalkowski et al. (2014), Wilks et al (2013), Hovy et al (2013) inter alia. These papers concentrate on the algorithms for detection and classification of individual linguistic metaphors in text rather than formation of conceptual metaphors in a broader cultural context. Taylor et al (2014) outlines the rationale why conceptual level metaphors may provide important insights into cross-cultural contrasts. Our work described here is a first attempt at automatic discovery of conceptual metaphors operating within a culture directly from the linguistic evidence in language. 3 Our Approach The process of discovering conceptual metaphors is necessarily divided into two phases: (1) collecting evidence about potential source domains that may be invoked when metaphorical expressions are used; and (2) building a conceptual space for each sufficiently evidenced source domain so that linguistic metaphors can be accurately classified as instances of appropriate conceptual metaphors. In 
211
this paper, we concentrate on the second phase only. Strzalkowski et al (2013) in their work have described a data-driven linguistic metaphor extraction method and our approach builds upon their work. During the source domain evidencing phase, we established a set of 50 source domains that operate frequently with the target concepts we are focusing on (government, bureaucracy, poverty, wealth, taxation, democracy and elections). These domains were a joint effort of several teams participating in the Metaphor program and we are taking this set as a starting point. These are shown in Table 1.   A_GOD	 ? CONFINEMENT	 ? GAME	 ? MONSTER	 ? PLANT	 ?A_RIGHT	 ? CRIME	 ? GAP	 ? MORAL_DUTY	 ? PORTAL	 ?ABYSS	 ? CROP	 ? GEOGRAPHIC_FEATURE	 ? MOVEMENT	 ? POSITION	 ?AND	 ?CHANGE	 ?OF	 ?	 ?POSITION	 ?ON	 ?A	 ?SCALE	 ?ADDICTION	 ? DARKNESS	 ? GREED	 ? NATURAL_PHYSICAL_FORCE	 ? RACE	 ?ANIMAL	 ? DESTROYER	 ?	 ? HUMAN_BODY	 ? OBESITY	 ? RESOURCE	 ?BATTLE	 ? DISEASE	 ? IMPURITY	 ? PARASITE	 ? STAGE	 ?BLOOD_STREAM	 ? ENERGY	 ? LIGHT	 ? PATHWAY	 ? STRUGGLE	 ?BODY_OF_WATER	 ? ENSLAVEMENT	 ? MACHINE	 ? PHYSICAL_BURDEN	 ? THEFT	 ?BUILDING	 ? FOOD	 ? MAZE	 ? PHYSICAL_HARM	 ? VISION	 ?COMPETITION	 ? FORCEFUL_EXTRACTION	 ? MEDICINE	 ? PHYSICAL_LOCATION	 ? WAR	 ?Table 1. Set of 50 source domains that operate frequently with target concepts being investigated. Only English names are shown for ease of presentation, equivalent sets in Spanish, Russian and Farsi have been created. Some of the domains are self explanatory, while others require a further specification since the labels are sometimes ambiguous. For example, PLANT represents things that grow in the soil, not factories; similarly, BUILDING represents artifacts such as houses or edifices, but not the act of constructing something; RACE refers to a running competition, not skin color, etc.  Consequently, each of these domains need to be seeded with the prototypical representative elements to make the meaning completely clear. This seeding occurs during the first phase of the process when a linguistic expression, such as ?cure poverty? is classified as a linguistic metaphor. This process of classifying ?cure poverty? as metaphorical is described in detail in Strzalkowski et al. (2013). Part of the seeding process is to establish that a source domain different than the target domain (here: poverty) is invoked by the relation (here: cure). To find the source domain where ?cure? is typically used literally, we form a linguistic pattern [cure [OBJ: X/nn]] (derived automatically from the parsed metaphoric expression) which is subsequently run through a balanced language corpus. Arguments matching the variable X are then clustered into semantic categories, using lexical resources such as Wordnet (Felbaum, 1998) and the most frequent and concrete category is selected as a possible source domain (proto-source domain). From the balanced language corpus, it is possible to compute the frequency with which the arguments resulting from search appear with relation (?cure?). We determine concreteness by looking up concreteness score in MRC psycholinguistic database (Coltheart 1981, Wilson 1988). As may be expected, the initial elements of the proto-source obtained from the above patterns will include: disease, cancer, plague, etc. These become the seeds of the source domain DISEASE in our list. The same process was performed for each of the 50 domains listed here, for each of the 4 languages under consideration. Additional Source Domains are continously generated bottom-up fashion by this phase 1 process elaborated above. In Table 2, we show seeds so obtained for a few source domains.    DISEASE	 ? disease,	 ?cancer,	 ?plague	 ?ABYSS	 ? abyss,	 ?chasm,	 ?crevasse	 ?BODY_OF_WATER	 ? ocean,	 ?lake	 ?river,	 ?pond,	 ?sea	 ?PLANT	 ? plant,	 ?tree,	 ?flower,	 ?weed,	 ?shrub,	 ?vegetable	 ?GEOGRAPHIC_FEATURE	 ? land,	 ?land	 ?form,	 ?earth,	 ?mountain,	 ?plateau,	 ?island,	 ?valley	 ?Table 2. Example of seeds corresponding to a few source domains 
212
Once such seeds are obtained, we perform another search through a balanced corpus in the corresponding language to discover relations that characterize the source domains. The purpose of source domain spaces in our research is two-fold: a) to provide a sufficiently complete characterization of a source domain via a list of relations ; and b) such a list of relations should sufficiently distinguish between different source domains. Creating these spaces is phase 2 of the conceptual metaphor discovery process. We search for nouns, verbs and verb phrases, and adjectives that co-occur with seeds of given source domain with sufficiently high frequency and sufficiently high mutual information. Our goal with this process is to approximate normal usage patterns of relations within source domains. The results of balanced corpora search form our conceptual spaces. The balanced corpora we use are English: Corpus of Contemporary American English (Davies, 2008), Spanish:  Corpus del Espa?ol Actual (Davies, 2002), Russian: Russian National Corpus2 and Farsi: Bijankhan Corpus (Oroumchian et al., 2006). In addition to retrieving the relations, we retrieve the frequency with which these relations can be found to co-occur with seeds of a source domain, Relation Frequency (RF). We calculate Inverse Domain Frequency (IDF) of all relations across all 50 source domains using a variant of the inverse document frequency measure. The formula for IDF is as given below:  IDF = log (total number of source domains / total number of source domains a relation appears in)  For example, if a relation such as ?dive into? is found to appear in two source domains, BODY_OF_WATER and GEOGRAPHIC_FEATURE, then the IDF for ?dive into? would be log (50/2). The rank of a relation is computed as the product of RF and IDF. However, computing rank using RF without normalization results in inflated ranks for relations that are quite common across domains even when they do not sufficiently disambiguate between the domains. We assume a normal distribution of frequencies of relations within a source domain and normalize RF by taking its logarithm. We also normalize with respect to seeds within a source domain. If a relation frequency is disproportionately high with a specific seed, we disregard that frequency. For example, one of the seeds for the source domain of BUILDING is ?house?. A search through balanced corpus for nouns adjacent to ?house? revealed a disproportionately large number for ?white?, which is meant to be the White House, and would be disregarded.  In Table 3, we show a few top ranked relations for the source domains DISEASE and BODY_OF_WATER. In columns 1 and 2, we show the source domain and the relation. Column 3 shows the relation frequency and column 4 shows the part of speech of relation (V=verb or verb phrase, N=noun, ADJ=adjective). An RF score of 800 for row 1 indicates that the relation ?diagnose with? appears 800 times with one or more of the seeds we search for source domain DISEASE (?diagnose with cancer?, ?diagnose with disease? and so on. In column 5, we show the position where the relation is commonly found to co-occur with the source domain. For example, ?afflict? in row 2 has a position ?after? which means it appears after DISEASE: ?DISEASE afflict(s)?; whereas row 3 would be read as ?affict with DISEASE? since it appears ?before?. In column 6, we show the normalized RF*IDF score. The highest RF*IDF score for a relation across our spaces is 2.165. From Table 3, we can see that even if  frequency for some relations may be relatively low, their rank would be high if they are strongly associated with a single source domain.    	 ? 1.	 ?Source	 ?Domain	 ? 2.	 ?Relation	 ? 3.	 ?RF	 ? 4.	 ?Type	 ? 5.	 ?Position	 ? 6.	 ?Norm	 ?RF*IDF	 ?1	 ? DISEASE	 ? diagnose	 ?with	 ? 800	 ? V	 ? before	 ? 1.94	 ?2	 ? DISEASE	 ? afflict	 ? 85	 ? V	 ? after	 ? 1.67	 ?3	 ? DISEASE	 ? afflict	 ?with	 ? 33	 ? V	 ? before	 ? 1.52	 ?4	 ? DISEASE	 ? cure	 ?of	 ? 29	 ? N	 ? before	 ? 1.46	 ?5	 ? BODY_OF_WATER	 ? dive	 ?into	 ? 49	 ? V	 ? before	 ? 2.01	 ?6	 ? BODY_OF_WATER	 ? wade	 ?through	 ? 44	 ? V	 ? before	 ? 1.88	 ?7	 ?	 ? BODY_OF_WATER	 ? wade	 ?into	 ? 42	 ? V	 ? before	 ? 1.84	 ?8	 ? BODY_OF_WATER	 ? rinse	 ?in	 ? 41	 ? V	 ? before	 ? 1.80	 ?Table 3. A few top ranking relations for the source domains DISEASE and BODY_OF_WATER. Relations are ranked by their normalized RF*IDF score.                                                 2 http://ruscorpora.ru/en/ 
213
With the conceptual spaces defined in this manner, we can now use them to group linguistic metaphors together. Shaikh et al (2014) have created a repository of thousands of automatically extracted lingusitic metaphors in all four languages, which we are using to create conceptual metaphors. To discover which conceptual metaphors exist within such large sets of linguistic metaphors would be quite challenging, if not impossible, for a human expert. We automatically assign each linguistic metaphor to ranked list of source domains.  Consider the linguistic metaphor ?plunge into poverty?, where the relation is ?plunge into?. We search through our conceptual spaces and retrieve a list of source domains where the relation ?plunge into? may appear. From this list, only the domains that have this relation RF*IDF score higher than a threshold are considered. This threshold is currently assigned to be 0.40, although it is subject to further experimentation. The source domain where the RF*IDF score of ?plunge into? is the highest is chosen as the source domain, along with the next source domains only if the difference in scores is 5% or lower. Tables 4 and 5 depicts this part of algorithm for two relations, ?plunge into? and ?explorar? (from Spanish ? ?explore?). The relation ?plunge into? is thus assigned to BODY_OF_WATER source domain. ?explorar? is assigned to GEOGRAPHIC_FEATURE and BODY_OF_WATER since difference in RF*IDF scores is less than 5%.  Relation	 ? Source	 ?Domains	 ? RF*IDF	 ?	 ? 	 ? Relation	 ? Source	 ?Domains	 ? RF*IDF	 ?
plunge	 ?into	 ?	 ?
BODY_OF_WATER	 ? 1.82	 ? 	 ?
explorar	 ?
GEOGRAPHIC_FEATURE	 ? 0.77	 ?DARKNESS	 ? 1.28	 ? 	 ? BODY_OF_WATER	 ? 0.76	 ?ABYSS	 ? 0.68	 ? 	 ? PHYSICAL_LOCATION	 ? 0.56	 ?WAR	 ? 0.57	 ? 	 ? PATHWAY	 ? 0.56	 ?GEOGRAPHIC_FEATURE	 ? 0.48	 ? 	 ? BUILDING	 ? 0.41	 ?Table 4 and Table 5. Assigning relations of linguistic metaphor to source domains. ?plunge into? is assigned to BODY_OF_WATER; ?explorar? is assigned to GEOGRAPHIC_FEATURE and BODY_OF_WATER Once this process of assigning linguistic metaphors to source domains is accomplished for all linguistic metaphors in our repository, we validate the resulting conceptual metaphors. A small percentage of metaphors cannot be assigned to any of the 50 Source Domains. We explain the validation process in Section 4. In Tables 6 and 7, we show sample conceptual metaphors in English and Spanish. Our validation process revealed an interesting insight regarding forming conceptual metaphor, wherein they should contain relations that are anchors for that given source domain that we shall describe next.  
 Table 6. A conceptual metaphor in English: POVERTY is a BODY_OF_WATER 
214
 Table 7. A conceptual metaphor in Spanish: POVERTY is a DISEASE 3.1 Anchor relations in Conceptual Metaphors When human assessors are presented with a set of linguistic metaphors and the task to assign them into a source domain, some relations will have stronger impact on their decision that others. For example, ?cure? would almost invariably be assigned to DISEASE domain, while ?dive in? would invoke BODY_OF_WATER domain. Other relations, such as ?spread? or ?fall into? are less specific, however, when paired with highly evocative relations above are likely to be classified the same way. Thus, there are two types of metaphorical relations in linguistic metaphors: (1) the highly evocative relations that unambigously point to a specific source domain ? we shall call them anchors; and (2) the relations that are compatible with the anchor but are not anchors themselves. We can add another class: (3) the relations that are not compatible with a given anchor. Thus, a set of linguistic metaphors that provides evidence for a conceptual metaphor should contain at least some anchor relations and the balance of the set may be composed of anchor-compatible relations. Our current hypothesis is that there should be at least one anchor for each 7 anchor compatible relations for a group of linguistic metaphors to provide a sufficient evidence for a conceptual metaphor.  As part of our validation process, we conducted a series of experiments with human assessors. One of the tasks was to assign a single linguistic metaphor to one of 50 source domains. As an illustrative example, we show in Table 8, one linguistic metaphor. When presented with this example, a majority of assessors chose ENEMY source domain, while DISEASE was selected second. Additionally, there was greater variance among their selections, only 31% chose the top source domain of ENEMY.  Subsequently, human assessors were presented a set of linguistic metaphors where at least one anchor relation was present. In this case, the majority of assessors chose the DISEASE source domain. Even though the ?fight against poverty? example was included in the set, the presence of anchors such as ?cure poverty? and ?treat poverty? lead assessors to choose DISEASE source domain. The variance in selection was also less, a 70% majority choosing DISEASE. We show the conceptual metaphor in Table 9.  The	 ?summit	 ?has	 ?proven	 ?that	 ?there	 ?is	 ?a	 ?renewed	 ?appetite	 ?for	 ?the	 ?fight	 ?against	 ?poverty.	 ?	 ?ENEMY:	 ?31%;	 ?DISEASE:	 ?17%;	 ?ANIMAL,	 ?MONSTER,?.<10%	 ?Table 8. A single linguistic metaphor was assigned a varied number of source domains by human assessors.   Of	 ?course,	 ?many	 ?government	 ?programs	 ?aim	 ?to	 ?alleviate	 ?poverty.	 ?We	 ?seek	 ?to	 ?stimulate	 ?true	 ?prosperity	 ?rather	 ?than	 ?simply	 ?treat	 ?poverty.	 ?Unless	 ?the	 ?fight	 ?against	 ?poverty	 ?is	 ?honestly	 ?addressed	 ?by	 ?the	 ?West,	 ?there	 ?will	 ?be	 ?many	 ?more	 ?Afghanistans.	 ?Above	 ?all,	 ?he	 ?knows	 ?that	 ?the	 ?only	 ?way	 ?to	 ?cure	 ?poverty	 ?is	 ?to	 ?grow	 ?the	 ?economy.	 ?	 ?DISEASE:	 ?70%;	 ?ENEMY:	 ?30%	 ?Table 9. A conceptual metaphor containing anchors. When sample metaphor from Table 8 is included in this set, human assessors still choose the source domain to be DISEASE. 
215
4 Evaluation and Results A group of human experts who are native speakers and have been substantively trained to achieve high levels of agreement (0.78 Krippendorf?s alpha (1970) or higher) form our validation team. In addition, we aim to run crowd-sourced experiments on Amazon Mechanical Turk. In Figure 1, we show a web interface we built to present our human assessors. The task shown here is the assignment of a single linguistic metaphor to one of 50 source domains. Then, we present our validation team with conceptual metaphors we created. Each conceptual metaphor is validated by at least two language experts. This interface is shown in Figure 2. These interfaces are carefully created by our team of social scientists and psychologists, designed to elicit proper responses from native speakers of the language.  
 Figure 1. Interface of task where human assessors select source domain for a single linguistic metaphor.  
216
 Figure 2. Interface of task where human assessors select source domains for a conceptual metaphor. Assessors provide their top two choices along with a description detailing how they made their decision.  In Table 10, we show the number of conceptual metaphors currently in the repository and the accuracy of our method across four languages, as computed by using validation data. We show the number of conceptual metaphors present in the Governance target domain (metaphors about government and bureaucracy), Economic Inequality (dealing with metaphors of poverty, wealth and taxation) and Democracy (democracy and elections metaphors). These conceptual metaphors on the three target domains of Governace, Economic Inequality and Democracy, when compared across cultures could provide deep insight about peoples? perceptions regarding salient concepts. We note that Russian and Farsi performance is lower than that in English and Spanish. The size of balanced corpus and accuracy of lexical tools such as stemmers and morphological analyzers affect performance of our algorithm.  The Farsi balanced corpus is relatively small when compared to English balanced corpus. The smaller size affects computation of statistics such as Relation Frequency and subsequently the thresholds of RF*IDF scores. One improvement we are currently investigating is that the thresholds may be set specifically for a language.   	 ? ENGLISH	 ? SPANISH	 ? RUSSIAN	 ? FARSI	 ?#	 ?of	 ?Governance	 ?Conceptual	 ?Metaphors	 ? 27	 ? 7	 ? 8	 ? 7	 ?#	 ?of	 ?Economic	 ?Inequality	 ?Conceptual	 ?Metaphors	 ? 32	 ? 26	 ? 57	 ? 7	 ?#	 ?of	 ?Democracy	 ?	 ?Conceptual	 ?Metaphors	 ? 51	 ? 16	 ? 18	 ? 8	 ?Total	 ?#	 ?of	 ?	 ?Conceptual	 ?Metaphors	 ? 110	 ? 49	 ? 83	 ? 22	 ?Accuracy	 ?(%)	 ?	 ? 85%	 ? 76%	 ? 67%	 ? 62%	 ?Table 10. Number of conceptual metaphors discovered thus far and performance of our approach across four languages. 
217
5 Conclusion and Future Work In this article, we presented our approach towards automatic discovery of conceptual metaphors directly from linguistic evidence in a given language. We make use of corpora in two unique ways: the first is to discover prototypical seeds that form the basis of source domains and second is to create conceptual spaces that allow us to characterize the relations that operate within source domains automatically. In addition, our approach also allows us to distinguish between source domains as necessary. The validation results show that this is indeed a promising first attempt of tackling a challenging research problem.  We note that the assignment of source domains is limited to the set of 50 in our current prototype. This assumes a closed set of 50 source domains, whereas in reality, there might be many others that operate in the realm of metaphors we are investigating. Although additional source domains are continually being discovered in a bottom-up fashion by the linguistic metaphor extraction process, we cannot account for every source domain that may be relevant. One way of overcoming this limitation would be to define a source domain ?OTHER? that would be the all-encompassing domain accounting for any yet undiscovered domains. The details of how it would be represented are still under investigation.  Another potential improvement to our method is to experimentally refine the threshold score of RF*IDF. Through large scale validation experiments, we could learn the optimal thresholds automatically by using machine learning. 6 Acknowledgements This paper is based on work supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Defense US Army Research Laboratory contract number W911NF-12-C-0024. The U.S. Government is authorized to reproduce and distribute reprints for Governmental pur-poses notwithstanding any copyright annotation thereon.  Disclaimer: The views and conclusions con-tained herein are those of the authors and should not be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Gov-ernment. References David W. Allbritton, Gail McKoon, and Richard J. Gerrig. 1995. Metaphor-based schemas and text Representa-tions: making connections through conceptual metaphors, Journal of Experimental Psychology: Learning, Memory, and Cognition, 21(3):612-625. Jonathan, Charteris-Black. 2002. Second language figurative proficiency: A comparative study of Malay and English. Applied Linguistics 23(1):104?133. George Aaron Broadwell, Umit Boz, Ignacio Cases, Tomek Strzalkowski, Laurie Feldman, Sarah Taylor, Samira Shaikh, Ting Liu and Kit Cho. 2013. Using Imageability and Topic Chaining to Locate Metaphors in Linguis-tic Corpora. In Proceedings of The 2013 International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction (SBP 2013), Washington D.C., USA. Jaime Carbonell. 1980. Metaphor: A key to extensible semantic analysis. In Proceedings of the 18th Annual Meeting on Association for Computational Linguistics. M. Coltheart. 1981. The MRC Psycholinguistic Database. Quarterly Journal of Experimental Psychology, 33A: 497-505. Davies, Mark. 2008-. The Corpus of Contemporary American English: 450 million words, 1990-present. Availa-ble online at http://corpus.byu.edu/coca/. Davies, Mark. 2002-. Corpus del Espa?ol: 100 million words, 1200s-1900s. Available online at http://www.corpusdelespanol.org. Dan, Fass. 1991. met*: A Method for Discriminating Metonymy and Metaphor by Computer. Computational Linguistics, 17:49-90 Jerome Feldman, and Srinivas Narayanan. 2004. Embodied meaning in a neural theory of language. Brain and Language, 89(2):385?392. 
218
Christiane D. Fellbaum. 1998. WordNet: An electronic lexical database (1st ed.). MIT Press. Matt Gedigian, John Bryant, Srini Narayanan and Branimir Ciric. 2006. Catching Metaphors. In Proceedings of the Third Workshop on Scalable Natural Language Understanding ScaNaLU 2006, pages 41?48. New York City: NY. Dirk Hovy, Shashank Shrivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney Sanders and Eduard Hovy. 2013. Identifying Metaphorical Word Use with Tree Kernels. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta. Krippendorff, Klaus. 1970. Estimating the reliability, systematic error, and random error of interval da-ta. Educational and Psychological Measurement, 30 (1),61-70. Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Pro-ceedings of the Workshop on Computational Approaches to Figurative Language, pages 13?20. Rochester, NY. George Lakoff, and Mark Johnson. 1980. Metaphors we live by. University Of Chicago Press, Chicago, Illinois. George, Lakoff. 2001. Moral politics: what conservatives know that liberals don?t. University of Chicago Press, Chicago, Illinois. Liisa, Malkki.  1992. National geographic: The rooting of people and the territorialization of national identity among scholars and refugees. Society for Cultural Anthropology, 7(1):24?44. James Martin. 1988. A computational theory of metaphor. Ph.D. Dissertation. Musolff, Andreas. 2008. What can critical metaphor analysis add to the understanding of racist ideology? Recent studies of Hitler?s anti-semitic metaphors, critical approaches to discourse analysis across disciplines. Critical Approaches to Discourse Analysis Across Disciplines, 2(2):1?10. Kieran, O?Halloran. 2007. Critical discourse analysis and the corpus-informed interpretation of metaphor at the register level. Oxford University Press Farhad Oroumchian, Samira Tasharofi, Hadi Amiri, Hossein Hojjat, Fahime Raja. 2006. Creating a Feasible Corpus for Persian POS Tagging.Technical Report, no. TR3/06, University of Wollongong in Dubai. Samira Shaikh, Tomek Strzalkowski, Ting Liu, George Aaron Broadwell, Boris Yamrom, Sarah Taylor, Laurie Feldman, Kit Cho, Umit Boz, Ignacio Cases, Yuliya Peshkova and Ching-Sheng Lin. 2014. A Multi-Cultural Repository of Automatically Discovered Linguistic and Conceptual Metaphors. In Proceedings of the The 9th edition of the Language Resources and Evaluation Conference , Reykjavik, Iceland.  Ekaterina Shutova and Simone Teufel. 2010a. Metaphor corpus annotated for source - target domain mappings. In Proceedings of Language Resources and Evaluation Conference 2010. Malta. Ekaterina Shutova. 2010b. Models of metaphor in nlp. In Proceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, ACL ?10, pages 688?697. Ekaterina Shutova, Tim Van de Cruys, and Anna Korhonen. 2012. Unsupervised metaphor paraphrasing using a vector space model In Proceedings of COLING 2012, Mumbai, India Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases and Kyle Elliott. 2013. Robust extraction of metaphor from novel data. In Proceedings of Workshop on Metaphor in NLP, NAACL. Atlanta. Tomek Strzalkowski, Samira Shaikh, Kit Cho, George Aaron Broadwell, Laurie Feldman, Sarah Taylor, Boris Yamrom, Ting Liu, Ignacio Cases, Yuliya Peshkova and Kyle Elliot. 2014. Computing Affect in Metaphors. In Proceedings of the Second Workshop on Metaphor in NLP, Baltimore Maryland.  Sarah Taylor, Laurie Beth Feldman, Kit Cho, Samira Shaikh, Ignacio Cases,Yuliya  Peshkiva, George Aaron Broadwell Ting Liu, Umit Boz, Kyle Elliott. Boris Yamrom, and Tomek Strzalkowski. 2014. Extracting Un-derstanding from automated metaphor identification: Contrasting Concepts of Poverty across Cultures and Languages. AHFE Conference, Cracow, Poland. Yorick, Wilks. 1975. Preference semantics. Formal Semantics of Natural Language, E. L. Keenan, Ed. Cam-bridge University Press, Cambridge, U.K., 329?348. Yorick Wilks, Lucian Galescu, James Allen, Adam Dalton. 2013. Automatic Metaphor Detection using Large-Scale Lexical Resources and Conventional Metaphor Extraction. In the Proceedings of the First Workshop on Metaphor in NLP, (NAACL). Atlanta.  
219
Wilson, M. D. 1988. The MRC Psycholinguistic Database: Machine Readable Dictionary, Version 2. Behav-ioural Research Methods, Instruments and Computers, 20(1): 6-11. 
220
