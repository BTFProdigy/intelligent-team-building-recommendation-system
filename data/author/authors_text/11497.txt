Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 64?72,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Topological Field Parsing of German
Jackie Chi Kit Cheung
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
jcheung@cs.toronto.edu
Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gpenn@cs.toronto.edu
Abstract
Freer-word-order languages such as Ger-
man exhibit linguistic phenomena that
present unique challenges to traditional
CFG parsing. Such phenomena produce
discontinuous constituents, which are not
naturally modelled by projective phrase
structure trees. In this paper, we exam-
ine topological field parsing, a shallow
form of parsing which identifies the ma-
jor sections of a sentence in relation to
the clausal main verb and the subordinat-
ing heads. We report the results of topo-
logical field parsing of German using the
unlexicalized, latent variable-based Berke-
ley parser (Petrov et al, 2006) Without
any language- or model-dependent adapta-
tion, we achieve state-of-the-art results on
the Tu?Ba-D/Z corpus, and a modified NE-
GRA corpus that has been automatically
annotated with topological fields (Becker
and Frank, 2002). We also perform a qual-
itative error analysis of the parser output,
and discuss strategies to further improve
the parsing results.
1 Introduction
Freer-word-order languages such as German ex-
hibit linguistic phenomena that present unique
challenges to traditional CFG parsing. Topic focus
ordering and word order constraints that are sen-
sitive to phenomena other than grammatical func-
tion produce discontinuous constituents, which are
not naturally modelled by projective (i.e., with-
out crossing branches) phrase structure trees. In
this paper, we examine topological field parsing, a
shallow form of parsing which identifies the ma-
jor sections of a sentence in relation to the clausal
main verb and subordinating heads, when present.
We report the results of parsing German using
the unlexicalized, latent variable-based Berkeley
parser (Petrov et al, 2006). Without any language-
or model-dependent adaptation, we achieve state-
of-the-art results on the Tu?Ba-D/Z corpus (Telljo-
hann et al, 2004), with a F1-measure of 95.15%
using gold POS tags. A further reranking of
the parser output based on a constraint involv-
ing paired punctuation produces a slight additional
performance gain. To facilitate comparison with
previous work, we also conducted experiments on
a modified NEGRA corpus that has been automat-
ically annotated with topological fields (Becker
and Frank, 2002), and found that the Berkeley
parser outperforms the method described in that
work. Finally, we perform a qualitative error anal-
ysis of the parser output on the Tu?Ba-D/Z corpus,
and discuss strategies to further improve the pars-
ing results.
German syntax and parsing have been studied
using a variety of grammar formalisms. Hocken-
maier (2006) has translated the German TIGER
corpus (Brants et al, 2002) into a CCG-based
treebank to model word order variations in Ger-
man. Foth et al (2004) consider a version of de-
pendency grammars known as weighted constraint
dependency grammars for parsing German sen-
tences. On the NEGRA corpus (Skut et al, 1998),
they achieve an accuracy of 89.0% on parsing de-
pendency edges. In Callmeier (2000), a platform
for efficient HPSG parsing is developed. This
parser is later extended by Frank et al (2003)
with a topological field parser for more efficient
parsing of German. The system by Rohrer and
Forst (2006) produces LFG parses using a manu-
ally designed grammar and a stochastic parse dis-
ambiguation process. They test on the TIGER cor-
pus and achieve an F1-measure of 84.20%. In
Dubey and Keller (2003), PCFG parsing of NE-
GRA is improved by using sister-head dependen-
cies, which outperforms standard head lexicaliza-
tion as well as an unlexicalized model. The best
64
performing model with gold tags achieve an F1
of 75.60%. Sister-head dependencies are useful in
this case because of the flat structure of NEGRA?s
trees.
In contrast to the deeper approaches to parsing
described above, topological field parsing identi-
fies the major sections of a sentence in relation
to the clausal main verb and subordinating heads,
when present. Like other forms of shallow pars-
ing, topological field parsing is useful as the first
stage to further processing and eventual seman-
tic analysis. As mentioned above, the output of
a topological field parser is used as a guide to
the search space of a HPSG parsing algorithm in
Frank et al (2003). In Neumann et al (2000),
topological field parsing is part of a divide-and-
conquer strategy for shallow analysis of German
text with the goal of improving an information ex-
traction system.
Existing work in identifying topological fields
can be divided into chunkers, which identify the
lowest-level non-recursive topological fields, and
parsers, which also identify sentence and clausal
structure.
Veenstra et al (2002) compare three approaches
to topological field chunking based on finite state
transducers, memory-based learning, and PCFGs
respectively. It is found that the three techniques
perform about equally well, with F1 of 94.1% us-
ing POS tags from the TnT tagger, and 98.4% with
gold tags. In Liepert (2003), a topological field
chunker is implemented using a multi-class ex-
tension to the canonically two-class support vec-
tor machine (SVM) machine learning framework.
Parameters to the machine learning algorithm are
fine-tuned by a genetic search algorithm, with a
resulting F1-measure of 92.25%. Training the pa-
rameters to SVM does not have a large effect on
performance, increasing the F1-measure in the test
set by only 0.11%.
The corpus-based, stochastic topological field
parser of Becker and Frank (2002) is based on
a standard treebank PCFG model, in which rule
probabilities are estimated by frequency counts.
This model includes several enhancements, which
are also found in the Berkeley parser. First,
they use parameterized categories, splitting non-
terminals according to linguistically based intu-
itions, such as splitting different clause types (they
do not distinguish different clause types as basic
categories, unlike Tu?Ba-D/Z). Second, they take
into account punctuation, which may help iden-
tify clause boundaries. They also binarize the very
flat topological tree structures, and prune rules
that only occur once. They test their parser on a
version of the NEGRA corpus, which has been
annotated with topological fields using a semi-
automatic method.
Ule (2003) proposes a process termed Directed
Treebank Refinement (DTR). The goal of DTR is
to refine a corpus to improve parsing performance.
DTR is comparable to the idea of latent variable
grammars on which the Berkeley parser is based,
in that both consider the observed treebank to be
less than ideal and both attempt to refine it by split-
ting and merging nonterminals. In this work, split-
ting and merging nonterminals are done by consid-
ering the nonterminals? contexts (i.e., their parent
nodes) and the distribution of their productions.
Unlike in the Berkeley parser, splitting and merg-
ing are distinct stages, rather than parts of a sin-
gle iteration. Multiple splits are found first, then
multiple rounds of merging are performed. No
smoothing is done. As an evaluation, DTR is ap-
plied to topological field parsing of the Tu?Ba-D/Z
corpus. We discuss the performance of these topo-
logical field parsers in more detail below.
All of the topological parsing proposals pre-
date the advent of the Berkeley parser. The exper-
iments of this paper demonstrate that the Berke-
ley parser outperforms previous methods, many of
which are specialized for the task of topological
field chunking or parsing.
2 Topological Field Model of German
Topological fields are high-level linear fields in
an enclosing syntactic region, such as a clause
(Ho?hle, 1983). These fields may have constraints
on the number of words or phrases they contain,
and do not necessarily form a semantically co-
herent constituent. Although it has been argued
that a few languages have no word-order con-
straints whatsoever, most ?free word-order? lan-
guages (even Warlpiri) have at the very least some
sort of sentence- or clause-initial topic field fol-
lowed by a second position that is occupied by
clitics, a finite verb or certain complementizers
and subordinating conjunctions. In a few Ger-
manic languages, including German, the topology
is far richer than that, serving to identify all of
the components of the verbal head of a clause,
except for some cases of long-distance dependen-
65
cies. Topological fields are useful, because while
Germanic word order is relatively free with respect
to grammatical functions, the order of the topolog-
ical fields is strict and unvarying.
Type Fields
VL (KOORD) (C) (MF) VC (NF)
V1 (KOORD) (LV) LK (MF) (VC) (NF)
V2 (KOORD) (LV) VF LK (MF) (VC) (NF)
Table 1: Topological field model of German.
Simplified from Tu?Ba-D/Z corpus?s annotation
schema (Telljohann et al, 2006).
In the German topological field model, clauses
belong to one of three types: verb-last (VL), verb-
second (V2), and verb-first (V1), each with a spe-
cific sequence of topological fields (Table 1). VL
clauses include finite and non-finite subordinate
clauses, V2 sentences are typically declarative
sentences and WH-questions in matrix clauses,
and V1 sentences include yes-no questions, and
certain conditional subordinate clauses. Below,
we give brief descriptions of the most common
topological fields.
? VF (Vorfeld or ?pre-field?) is the first con-
stituent in sentences of the V2 type. This is
often the topic of the sentence, though as an
anonymous reviewer pointed out, this posi-
tion does not correspond to a single function
with respect to information structure. (e.g.,
the reviewer suggested this case, where VF
contains the focus: ?Wer kommt zur Party?
?Peter kommt zur Party. ?Who is coming to
the Party? ?Peter is coming to the party.)
? LK (Linke Klammer or ?left bracket?) is the
position for finite verbs in V1 and V2 sen-
tences. It is replaced by a complementizer
with the field label C in VL sentences.
? MF (Mittelfeld or ?middle field?) is an op-
tional field bounded on the left by LK and
on the right by the verbal complex VC or
by NF. Most verb arguments, adverbs, and
prepositional phrases are found here, unless
they have been fronted and put in the VF, or
are prosodically heavy and postposed to the
NF field.
? VC is the verbal complex field. It includes
infinite verbs, as well as finite verbs in VL
sentences.
? NF (Nachfeld or ?post-field?) contains
prosodically heavy elements such as post-
posed prepositional phrases or relative
clauses.
? KOORD1 (Koordinationsfeld or ?coordina-
tion field?) is a field for clause-level conjunc-
tions.
? LV (Linksversetzung or ?left dislocation?) is
used for resumptive constructions involving
left dislocation. For a detailed linguistic
treatment, see (Frey, 2004).
Exceptions to the topological field model as de-
scribed above do exist. For instance, parenthetical
constructions exist as a mostly syntactically inde-
pendent clause inside another sentence. In our cor-
pus, they are attached directly underneath a clausal
node without any intervening topological field, as
in the following example. In this example, the par-
enthetical construction is highlighted in bold print.
Some clause and topological field labels under the
NF field are omitted for clarity.
(1) (a) (SIMPX ?(VF Man) (LK mu?) (VC verstehen) ?
, (SIMPX sagte er), ? (NF da? diese
Minderheiten seit langer Zeit massiv von den
Nazis bedroht werden)). ?
(b) Translation: ?One must understand,? he said,
?that these minorities have been massively
threatened by the Nazis for a long time.?
3 A Latent Variable Parser
For our experiments, we used the latent variable-
based Berkeley parser (Petrov et al, 2006). La-
tent variable parsing assumes that an observed
treebank represents a coarse approximation of
an underlying, optimally refined grammar which
makes more fine-grained distinctions in the syn-
tactic categories. For example, the noun phrase
category NP in a treebank could be viewed as a
coarse approximation of two noun phrase cate-
gories corresponding to subjects and object, NP?S,
and NP?VP.
The Berkeley parser automates the process of
finding such distinctions. It starts with a simple bi-
narized X-bar grammar style backbone, and goes
through iterations of splitting and merging non-
terminals, in order to maximize the likelihood of
the training set treebank. In the splitting stage,
1The Tu?Ba-D/Z corpus distinguishes coordinating and
non-coordinating particles, as well as clausal and field co-
ordination. These distinctions need not concern us for this
explanation.
66
Figure 1: ?I could never have done that just for aesthetic reasons.? Sample Tu?Ba-D/Z tree, with topolog-
ical field annotations and edge labels. Topological field layer in bold.
an Expectation-Maximization algorithm is used to
find a good split for each nonterminal. In the
merging stage, categories that have been over-
split are merged together to keep the grammar size
tractable and reduce sparsity. Finally, a smoothing
stage occurs, where the probabilities of rules for
each nonterminal are smoothed toward the prob-
abilities of the other nonterminals split from the
same syntactic category.
The Berkeley parser has been applied to the
Tu?BaD/Z corpus in the constituent parsing shared
task of the ACL-2008 Workshop on Parsing Ger-
man (Petrov and Klein, 2008), achieving an F1-
measure of 85.10% and 83.18% with and without
gold standard POS tags respectively2. We chose
the Berkeley parser for topological field parsing
because it is known to be robust across languages,
and because it is an unlexicalized parser. Lexi-
calization has been shown to be useful in more
general parsing applications due to lexical depen-
dencies in constituent parsing (e.g. (Ku?bler et al,
2006; Dubey and Keller, 2003) in the case of Ger-
man). However, topological fields explain a higher
level of structure pertaining to clause-level word
order, and we hypothesize that lexicalization is un-
likely to be helpful.
4 Experiments
4.1 Data
For our experiments, we primarily used the Tu?Ba-
D/Z (Tu?binger Baumbank des Deutschen / Schrift-
sprache) corpus, consisting of 26116 sentences
(20894 training, 2611 development, 2089 test,
with a further 522 sentences held out for future ex-
2This evaluation considered grammatical functions as
well as the syntactic category.
periments)3 taken from the German newspaper die
tageszeitung. The corpus consists of four levels
of annotation: clausal, topological, phrasal (other
than clausal), and lexical. We define the task of
topological field parsing to be recovering the first
two levels of annotation, following Ule (2003).
We also tested the parser on a version of the NE-
GRA corpus derived by Becker and Frank (2002),
in which syntax trees have been made projec-
tive and topological fields have been automatically
added through a series of linguistically informed
tree modifications. All internal phrasal structure
nodes have also been removed. The corpus con-
sists of 20596 sentences, which we split into sub-
sets of the same size as described by Becker and
Frank (2002)4. The set of topological fields in
this corpus differs slightly from the one used in
Tu?Ba-D/Z, making no distinction between clause
types, nor consistently marking field or clause
conjunctions. Because of the automatic anno-
tation of topological fields, this corpus contains
numerous annotation errors. Becker and Frank
(2002) manually corrected their test set and eval-
uated the automatic annotation process, reporting
labelled precision and recall of 93.0% and 93.6%
compared to their manual annotations. There are
also punctuation-related errors, including miss-
ing punctuation, sentences ending in commas, and
sentences composed of single punctuation marks.
We test on this data in order to provide a bet-
ter comparison with previous work. Although we
could have trained the model in Becker and Frank
(2002) on the Tu?Ba-D/Z corpus, it would not have
3These are the same splits into training, development, and
test sets as in the ACL-08 Parsing German workshop. This
corpus does not include sentences of length greater than 40.
416476 training sentences, 1000 development, 1058 test-
ing, and 2062 as held-out data. We were unable to obtain
the exact subsets used by Becker and Frank (2002). We will
discuss the ramifications of this on our evaluation procedure.
67
Gold tags Edge labels LP% LR% F1% CB CB0% CB ? 2% EXACT%
- - 93.53 93.17 93.35 0.08 94.59 99.43 79.50
+ - 95.26 95.04 95.15 0.07 95.35 99.52 83.86
- + 92.38 92.67 92.52 0.11 92.82 99.19 77.79
+ + 92.36 92.60 92.48 0.11 92.82 99.19 77.64
Table 2: Parsing results for topological fields and clausal constituents on the Tu?Ba-D/Z corpus.
been a fair comparison, as the parser depends quite
heavily on NEGRA?s annotation scheme. For ex-
ample, Tu?Ba-D/Z does not contain an equiva-
lent of the modified NEGRA?s parameterized cat-
egories; there exist edge labels in Tu?BaD/Z, but
they are used to mark head-dependency relation-
ships, not subtypes of syntactic categories.
4.2 Results
We first report the results of our experiments on
the Tu?Ba-D/Z corpus. For the Tu?Ba-D/Z corpus,
we trained the Berkeley parser using the default
parameter settings. The grammar trainer attempts
six iterations of splitting, merging, and smoothing
before returning the final grammar. Intermediate
grammars after each step are also saved. There
were training and test sentences without clausal
constituents or topological fields, which were ig-
nored by the parser and by the evaluation. As
part of our experiment design, we investigated the
effect of providing gold POS tags to the parser,
and the effect of incorporating edge labels into the
nonterminal labels for training and parsing. In all
cases, gold annotations which include gold POS
tags were used when training the parser.
We report the standard PARSEVAL measures
of parser performance in Table 2, obtained by the
evalb program by Satoshi Sekine and Michael
Collins. This table shows the results after five it-
erations of grammar modification, parameterized
over whether we provide gold POS tags for pars-
ing, and edge labels for training and parsing. The
number of iterations was determined by experi-
ments on the development set. In the evaluation,
we do not consider edge labels in determining
correctness, but do consider punctuation, as Ule
(2003) did. If we ignore punctuation in our evalu-
ation, we obtain an F1-measure of 95.42% on the
best model (+ Gold tags, - Edge labels).
Whether supplying gold POS tags improves
performance depends on whether edge labels are
considered in the grammar. Without edge labels,
gold POS tags improve performance by almost
two points, corresponding to a relative error reduc-
tion of 33%. In contrast, performance is negatively
affected when edge labels are used and gold POS
tags are supplied (i.e., + Gold tags, + Edge la-
bels), making the performance worse than not sup-
plying gold tags. Incorporating edge label infor-
mation does not appear to improve performance,
possibly because it oversplits the initial treebank
and interferes with the parser?s ability to determine
optimal splits for refining the grammar.
Parser LP% LR% F1%
Tu?Ba-D/Z
This work 95.26 95.04 95.15
Ule unknown unknown 91.98
NEGRA - from Becker and Frank (2002)
BF02 (len. ? 40) 92.1 91.6 91.8
NEGRA - our experiments
This work (len. ? 40) 90.74 90.87 90.81
BF02 (len. ? 40) 89.54 88.14 88.83
This work (all) 90.29 90.51 90.40
BF02 (all) 89.07 87.80 88.43
Table 3: BF02 = (Becker and Frank, 2002). Pars-
ing results for topological fields and clausal con-
stituents. Results from Ule (2003) and our results
were obtained using different training and test sets.
The first row of results of Becker and Frank (2002)
are from that paper; the rest were obtained by our
own experiments using that parser. All results con-
sider punctuation in evaluation.
To facilitate a more direct comparison with pre-
vious work, we also performed experiments on the
modified NEGRA corpus. In this corpus, topo-
logical fields are parameterized, meaning that they
are labelled with further syntactic and semantic in-
formation. For example, VF is split into VF-REL
for relative clauses, and VF-TOPIC for those con-
taining topics in a verb-second sentence, among
others. All productions in the corpus have also
been binarized. Tuning the parameter settings on
the development set, we found that parameterized
categories, binarization, and including punctua-
tion gave the best F1 performance. First-order
horizontal and zeroth order vertical markoviza-
68
tion after six iterations of splitting, merging, and
smoothing gave the best F1 result of 91.78%. We
parsed the corpus with both the Berkeley parser
and the best performing model of Becker and
Frank (2002).
The results of these experiments on the test set
for sentences of length 40 or less and for all sen-
tences are shown in Table 3. We also show other
results from previous work for reference. We
find that we achieve results that are better than
the model in Becker and Frank (2002) on the test
set. The difference is statistically significant (p =
0.0029, Wilcoxon signed-rank).
The results we obtain using the parser of Becker
and Frank (2002) are worse than the results de-
scribed in that paper. We suggest the following
reasons for this discrepancy. While the test set
used in the paper was manually corrected for eval-
uation, we did not correct our test set, because it
would be difficult to ensure that we adhered to the
same correction guidelines. No details of the cor-
rection process were provided in the paper, and de-
scriptive grammars of German provide insufficient
guidance on many of the examples in NEGRA on
issues such as ellipses, short infinitival clauses,
and expanded participial constructions modifying
nouns. Also, because we could not obtain the ex-
act sets used for training, development, and test-
ing, we had to recreate the sets by randomly split-
ting the corpus.
4.3 Category Specific Results
We now return to the Tu?Ba-D/Z corpus for a
more detailed analysis, and examine the category-
specific results for our best performing model (+
Gold tags, - Edge labels). Overall, Table 4 shows
that the best performing topological field cate-
gories are those that have constraints on the type
of word that is allowed to fill it (finite verbs in
LK, verbs in VC, complementizers and subordi-
nating conjunctions in C). VF, in which only one
constituent may appear, also performs relatively
well. Topological fields that can contain a vari-
able number of heterogeneous constituents, on the
other hand, have poorer F1-measure results. MF,
which is basically defined relative to the positions
of fields on either side of it, is parsed several points
below LK, C, and VC in accuracy. NF, which
contains different kinds of extraposed elements, is
parsed at a substantially worse level.
Poorly parsed categories tend to occur infre-
quently, including LV, which marks a rare re-
sumptive construction; FKOORD, which marks
topological field coordination; and the discourse
marker DM. The other clause-level constituents
(PSIMPX for clauses in paratactic constructions,
RSIMPX for relative clauses, and SIMPX for
other clauses) also perform below average.
Topological Fields
Category # LP% LR% F1%
PARORD 20 100.00 100.00 100.00
VCE 3 100.00 100.00 100.00
LK 2186 99.68 99.82 99.75
C 642 99.53 98.44 98.98
VC 1777 98.98 98.14 98.56
VF 2044 96.84 97.55 97.20
KOORD 99 96.91 94.95 95.92
MF 2931 94.80 95.19 94.99
NF 643 83.52 81.96 82.73
FKOORD 156 75.16 73.72 74.43
LV 17 10.00 5.88 7.41
Clausal Constituents
Category # LP% LR% F1%
SIMPX 2839 92.46 91.97 92.21
RSIMPX 225 91.23 92.44 91.83
PSIMPX 6 100.00 66.67 80.00
DM 28 59.26 57.14 58.18
Table 4: Category-specific results using grammar
with no edge labels and passing in gold POS tags.
4.4 Reranking for Paired Punctuation
While experimenting with the development set
of Tu?Ba-D/Z, we noticed that the parser some-
times returns parses, in which paired punctuation
(e.g. quotation marks, parentheses, brackets) is
not placed in the same clause?a linguistically im-
plausible situation. In these cases, the high-level
information provided by the paired punctuation is
overridden by the overall likelihood of the parse
tree. To rectify this problem, we performed a sim-
ple post-hoc reranking of the 50-best parses pro-
duced by the best parameter settings (+ Gold tags,
- Edge labels), selecting the first parse that places
paired punctuation in the same clause, or return-
ing the best parse if none of the 50 parses satisfy
the constraint. This procedure improved the F1-
measure to 95.24% (LP = 95.39%, LR = 95.09%).
Overall, 38 sentences were parsed with paired
punctuation in different clauses, of which 16 were
reranked. Of the 38 sentences, reranking improved
performance in 12 sentences, did not affect perfor-
mance in 23 sentences (of which 10 already had a
perfect parse), and hurt performance in three sen-
tences. A two-tailed sign test suggests that rerank-
69
ing improves performance (p = 0.0352). We dis-
cuss below why sentences with paired punctuation
in different clauses can have perfect parse results.
To investigate the upper-bound in performance
that this form of reranking is able to achieve, we
calculated some statistics on our (+ Gold tags, -
Edge labels) 50-best list. We found that the aver-
age rank of the best scoring parse by F1-measure
is 2.61, and the perfect parse is present for 1649
of the 2088 sentences at an average rank of 1.90.
The oracle F1-measure is 98.12%, indicating that
a more comprehensive reranking procedure might
allow further performance gains.
4.5 Qualitative Error Analysis
As a further analysis, we extracted the worst scor-
ing fifty sentences by F1-measure from the parsed
test set (+ Gold tags, - Edge labels), and compared
them against the gold standard trees, noting the
cause of the error. We analyze the parses before
reranking, to see how frequently the paired punc-
tuation problem described above severely affects a
parse. The major mistakes made by the parser are
summarized in Table 5.
Problem Freq.
Misidentification of Parentheticals 19
Coordination problems 13
Too few SIMPX 10
Paired punctuation problem 9
Other clause boundary errors 7
Other 6
Too many SIMPX 3
Clause type misidentification 2
MF/NF boundary 2
LV 2
VF/MF boundary 2
Table 5: Types and frequency of parser errors in
the fifty worst scoring parses by F1-measure, us-
ing parameters (+ Gold tags, - Edge labels).
Misidentification of Parentheticals Parentheti-
cal constructions do not have any dependencies on
the rest of the sentence, and exist as a mostly syn-
tactically independent clause inside another sen-
tence. They can occur at the beginning, end, or
in the middle of sentences, and are often set off
orthographically by punctuation. The parser has
problems identifying parenthetical constructions,
often positing a parenthetical construction when
that constituent is actually attached to a topolog-
ical field in a neighbouring clause. The follow-
ing example shows one such misidentification in
bracket notation. Clause internal topological fields
are omitted for clarity.
(2) (a) Tu?Ba-D/Z: (SIMPX Weder das Ausma? der
Scho?nheit noch der fru?here oder spa?tere
Zeitpunkt der Geburt macht einen der Zwillinge
fu?r eine Mutter mehr oder weniger echt /
authentisch / u?berlegen).
(b) Parser: (SIMPX Weder das Ausma? der
Scho?nheit noch der fru?here oder spa?tere
Zeitpunkt der Geburt macht einen der Zwillinge
fu?r eine Mutter mehr oder weniger echt)
(PARENTHETICAL / authentisch /
u?berlegen.)
(c) Translation: ?Neither the degree of beauty nor
the earlier or later time of birth makes one of the
twins any more or less real/authentic/superior to
a mother.?
We hypothesized earlier that lexicalization is
unlikely to give us much improvement in perfor-
mance, because topological fields work on a do-
main that is higher than that of lexical dependen-
cies such as subcategorization frames. However,
given the locally independent nature of legitimate
parentheticals, a limited form of lexicalization or
some other form of stronger contextual informa-
tion might be needed to improve identification per-
formance.
Coordination Problems The second most com-
mon type of error involves field and clause coordi-
nations. This category includes missing or incor-
rect FKOORD fields, and conjunctions of clauses
that are misidentified. In the following example,
the conjoined MFs and following NF in the cor-
rect parse tree are identified as a single long MF.
(3) (a) Tu?Ba-D/Z: Auf dem europa?ischen Kontinent
aber hat (FKOORD (MF kein Land und keine
Macht ein derartiges Interesse an guten
Beziehungen zu Ru?land) und (MF auch kein
Land solche Erfahrungen im Umgang mit
Ru?land)) (NF wie Deutschland).
(b) Parser: Auf dem europa?ischen Kontinent aber
hat (MF kein Land und keine Macht ein
derartiges Interesse an guten Beziehungen zu
Ru?land und auch kein Land solche
Erfahrungen im Umgang mit Ru?land wie
Deutschland).
(c) Translation: ?On the European continent,
however, no land and no power has such an
interest in good relations with Russia (as
Germany), and also no land (has) such
experience in dealing with Russia as Germany.?
Other Clause Errors Other clause-level errors
include the parser predicting too few or too many
clauses, or misidentifying the clause type. Clauses
are sometimes confused with NFs, and there is one
case of a relative clause being misidentified as a
70
main clause with an intransitive verb, as the finite
verb appears at the end of the clause in both cases.
Some clause errors are tied to incorrect treatment
of elliptical constructions, in which an element
that is inferable from context is missing.
Paired Punctuation Problems with paired
punctuation are the fourth most common type of
error. Punctuation is often a marker of clause
or phrase boundaries. Thus, predicting paired
punctuation incorrectly can lead to incorrect
parses, as in the following example.
(4) (a) ? Auch (SIMPX wenn der Krieg heute ein
Mobilisierungsfaktor ist) ? , so Pau , ? (SIMPX
die Leute sehen , da? man fu?r die Arbeit wieder
auf die Stra?e gehen mu?) . ?
(b) Parser: (SIMPX ? (LV Auch (SIMPX wenn der
Krieg heute ein Mobilisierungsfaktor ist)) ? , so
Pau , ? (SIMPX die Leute sehen , da? man fu?r
die Arbeit wieder auf die Stra?e gehen mu?)) . ?
(c) Translation: ?Even if the war is a factor for
mobilization,? said Pau, ?the people see, that
one must go to the street for employment again.?
Here, the parser predicts a spurious SIMPX
clause spanning the text of the entire sentence, but
this causes the second pair of quotation marks to
be parsed as belonging to two different clauses.
The parser also predicts an incorrect LV field. Us-
ing the paired punctuation constraint, our rerank-
ing procedure was able to correct these errors.
Surprisingly, there are cases in which paired
punctuation does not belong inside the same
clause in the gold parses. These cases are ei-
ther extended quotations, in which each of the
quotation mark pair occurs in a different sen-
tence altogether, or cases where the second of the
quotation mark pair must be positioned outside
of other sentence-final punctuation due to ortho-
graphic conventions. Sentence-final punctuation
is typically placed outside a clause in this version
of Tu?Ba-D/Z.
Other Issues Other incorrect parses generated
by the parser include problems with the infre-
quently occurring topological fields like LV and
DM, inability to determine the boundary between
MF and NF in clauses without a VC field sepa-
rating the two, and misidentifying appositive con-
structions. Another issue is that although the
parser output may disagree with the gold stan-
dard tree in Tu?Ba-D/Z, the parser output may be
a well-formed topological field parse for the same
sentence with a different interpretation, for ex-
ample because of attachment ambiguity. Each of
the authors independently checked the fifty worst-
scoring parses, and determined whether each parse
produced by the Berkeley parser could be a well-
formed topological parse. Where there was dis-
agreement, we discussed our judgments until we
came to a consensus. Of the fifty parses, we de-
termined that nine, or 18%, could be legitimate
parses. Another five, or 10%, differ from the gold
standard parse only in the placement of punctua-
tion. Thus, the F1-measures we presented above
may be underestimating the parser?s performance.
5 Conclusion and Future Work
In this paper, we examined applying the latent-
variable Berkeley parser to the task of topological
field parsing of German, which aims to identify the
high-level surface structure of sentences. Without
any language or model-dependent adaptation, we
obtained results which compare favourably to pre-
vious work in topological field parsing. We further
examined the results of doing a simple reranking
process, constraining the output parse to put paired
punctuation in the same clause. This reranking
was found to result in a minor performance gain.
Overall, the parser performs extremely well in
identifying the traditional left and right brackets
of the topological field model; that is, the fields
C, LK, and VC. The parser achieves basically per-
fect results on these fields in the Tu?Ba-D/Z corpus,
with F1-measure scores for each at over 98.5%.
These scores are higher than previous work in the
simpler task of topological field chunking. The fo-
cus of future research should thus be on correctly
identifying the infrequently occuring fields and
constructions, with parenthetical constructions be-
ing a particular concern. Possible avenues of fu-
ture research include doing a more comprehensive
discriminative reranking of the parser output. In-
corporating more contextual information might be
helpful to identify discourse-related constructions
such as parentheses, and the DM and LV topolog-
ical fields.
Acknowledgements
We are grateful to Markus Becker, Anette Frank,
Sandra Kuebler, and Slav Petrov for their invalu-
able help in gathering the resources necessary for
our experiments. This work is supported in part
by the Natural Sciences and Engineering Research
Council of Canada.
71
References
M. Becker and A. Frank. 2002. A stochastic topo-
logical parser for German. In Proceedings of the
19th International Conference on Computational
Linguistics, pages 71?77.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, pages 24?41.
U. Callmeier. 2000. PET?a platform for experimen-
tation with efficient HPSG processing techniques.
Natural Language Engineering, 6(01):99?107.
A. Dubey and F. Keller. 2003. Probabilistic parsing
for German using sister-head dependencies. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 96?103.
K.A. Foth, M. Daum, and W. Menzel. 2004. A
broad-coverage parser for German based on defea-
sible constraints. Constraint Solving and Language
Processing.
A. Frank, M. Becker, B. Crysmann, B. Kiefer, and
U. Schaefer. 2003. Integrated shallow and deep
parsing: TopP meets HPSG. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 104?111.
W. Frey. 2004. Notes on the syntax and the pragmatics
of German Left Dislocation. In H. Lohnstein and
S. Trissler, editors, The Syntax and Semantics of the
Left Periphery, pages 203?233. Mouton de Gruyter,
Berlin.
J. Hockenmaier. 2006. Creating a CCGbank and a
Wide-Coverage CCG Lexicon for German. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 505?512.
T.N. Ho?hle. 1983. Topologische Felder. Ph.D. thesis,
Ko?ln.
S. Ku?bler, E.W. Hinrichs, and W. Maier. 2006. Is it re-
ally that difficult to parse German? In Proceedings
of EMNLP.
M. Liepert. 2003. Topological Fields Chunking for
German with SVM?s: Optimizing SVM-parameters
with GA?s. In Proceedings of the International Con-
ference on Recent Advances in Natural Language
Processing (RANLP), Bulgaria.
G. Neumann, C. Braun, and J. Piskorski. 2000. A
Divide-and-Conquer Strategy for Shallow Parsing
of German Free Texts. In Proceedings of the sixth
conference on Applied natural language processing,
pages 239?246. Morgan Kaufmann Publishers Inc.
San Francisco, CA, USA.
S. Petrov and D. Klein. 2008. Parsing German with
Latent Variable Grammars. In Proceedings of the
ACL-08: HLT Workshop on Parsing German (PaGe-
08), pages 33?39.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
C. Rohrer and M. Forst. 2006. Improving coverage
and parsing quality of a large-scale LFG for Ger-
man. In Proceedings of the Language Resources
and Evaluation Conference (LREC-2006), Genoa,
Italy.
W. Skut, T. Brants, B. Krenn, and H. Uszkoreit.
1998. A Linguistically Interpreted Corpus of Ger-
man Newspaper Text. Proceedings of the ESSLLI
Workshop on Recent Advances in Corpus Annota-
tion.
H. Telljohann, E. Hinrichs, and S. Kubler. 2004.
The Tu?Ba-D/Z treebank: Annotating German with a
context-free backbone. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC 2004), pages 2229?2235.
H. Telljohann, E.W. Hinrichs, S. Kubler, and H. Zins-
meister. 2006. Stylebook for the Tubingen Tree-
bank of Written German (Tu?Ba-D/Z). Seminar fur
Sprachwissenschaft, Universitat Tubingen, Tubin-
gen, Germany.
T. Ule. 2003. Directed Treebank Refinement for PCFG
Parsing. In Proceedings of Workshop on Treebanks
and Linguistic Theories (TLT) 2003, pages 177?188.
J. Veenstra, F.H. Mu?ller, and T. Ule. 2002. Topolog-
ical field chunking for German. In Proceedings of
the Sixth Conference on Natural Language Learn-
ing, pages 56?62.
72
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 7?14,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Optimization-based Content Selection for Opinion Summarization
Jackie Chi Kit Cheung
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
jcheung@cs.toronto.edu
Giuseppe Carenini and Raymond T. Ng
Department of Computer Science
University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{carenini,rng}@cs.ubc.ca
Abstract
We introduce a content selection method
for opinion summarization based on a
well-studied, formal mathematical model,
the p-median clustering problem from fa-
cility location theory. Our method re-
places a series of local, myopic steps to
content selection with a global solution,
and is designed to allow content and re-
alization decisions to be naturally inte-
grated. We evaluate and compare our
method against an existing heuristic-based
method on content selection, using human
selections as a gold standard. We find that
the algorithms perform similarly, suggest-
ing that our content selection method is
robust enough to support integration with
other aspects of summarization.
1 Introduction
It is now possible to find a large amount of in-
formation on people?s opinions on almost every
subject online. The ability to analyze such infor-
mation is critical in complex, high-stakes decision
making processes. At the individual level, some-
one wishing to buy a laptop may read customer
reviews from others who have purchased and used
the product. At the corporate level, customer feed-
back on a newly launched product may help to
identify weaknesses and features that are in need
of improvement (Dellarocas et al, 2004).
Effective summarization systems are thus
needed to convey people?s opinions to users. A
challenging problem in implementing this ap-
proach in a particular domain is to devise a con-
tent selection strategy that identifies what key in-
formation should be presented. In general, content
selection is a critical task at the core of both sum-
marization and NLG and it represents a promising
area for cross-fertilization.
Existing NLG systems tend to approach con-
tent selection by defining a heuristic based on sev-
eral relevant factors, and maximizing this heuristic
function. ILEX (Intelligent Labelling Explorer) is
a system for generating labels for sets of objects
defined in a database, such as for museum arti-
facts (O?Donnell et al, 2001). Its content selection
strategy involves computing a heuristic relevance
score for knowledge elements, and returning the
items with the highest scores.
In GEA (Generator of Evaluative Arguments),
evaluative arguments are generated to describe an
entity as positive or negative (Carenini and Moore,
2006). An entity is decomposed into a hierarchy
of features, and a relevance score is independently
calculated for each feature, based on the prefer-
ences of the user and the value of that feature for
the product. Content selection involves selecting
the most relevant features for the current user.
There is also work in sentiment analysis relying
on optimization or clustering-based approaches.
Pang and Lee (2004) frame the problem of detect-
ing subjective sentences as finding the minimum
cut in a graph representation of the sentences.
They produce compressed versions of movie re-
views using just the subjective sentences, which
retain the polarity information of the review. Ga-
mon et al (2005) use a heuristic approach to
cluster sentences drawn from car reviews, group-
ing sentences that share common terms, especially
those salient in the domain such as ?drive? or ?han-
dling?. The resulting clusters are displayed by a
Treemap visualization.
Our work is most similar to the content se-
lection method of the multimedia conversation
system RIA (Responsive Information Architect)
(Zhou and Aggarwal, 2004). In RIA, content
selection involves selecting dimensions (such as
price in the real estate domain) in response to a
query such that the desirability of the dimensions
selected for the query is maximized while respect-
7
ing time and space constraints. The maximization
of desirability is implemented as an optimization
problem similar to a knapsack problem. RIA?s
content selection method performs similarly to ex-
pert human designers, but the evaluation is limited
in scale (two designers, each annotating two se-
ries of queries to the system), and no heuristic al-
ternative is compared against it. Our work also
frames content selection as a formal optimization
problem, but we apply this model to the domain of
opinion summarization.
A key advantage of formulating a content selec-
tion strategy as a p-median optimization problem
is that the resulting framework can be extended to
select other characteristics of the summary at the
same time as the information content, such as the
realization strategy with which the content is ex-
pressed. The p-median clustering works as a mod-
ule separate from its interpretation as the solution
to a content selection problem, so we can freely
modify the conversion process from the selection
problem to the clustering problem. Work in NLG
and summarization has shown that content and
realization decisions (including media allocation)
are often dependent on each other, which should
be reflected in the summarization process. For
example, in multi-modal summarization, complex
information can be more effectively conveyed by
combining graphics and text (Tufte et al, 1998).
While graphics can present large amounts of data
compactly and support the discovery of trends and
relationships, text is much more effective at ex-
plaining key points about the data. In another case
specific to opinion summarization, the controver-
siality of the opinions in a corpus was found to cor-
relate with the type of text summary, with abstrac-
tive summarization being preferred when the con-
troversiality is high (Carenini and Cheung, 2008).
We first test whether our optimization-based
approach can achieve reasonable performance on
content selection alone. As a contribution of this
paper, we compare our optimization-based ap-
proach to a previously proposed heuristic method.
Because our approach replaces a set of myopic de-
cisions with an extensively studied procedure (the
p-median problem) that is able to find a global so-
lution, we hypothesized our approach would pro-
duce better selections. The results of our study
indicate that our optimization-based content selec-
tion strategy performs about as well as the heuris-
tic method. These results suggest that our frame-
work is robust enough for integrating other aspects
of summarization with content selection.
2 Previous Heuristic Approach
2.1 Assumed Input Information
We now define the expected input into the summa-
rization process, then describe a previous greedy
heuristic method. The first phase of the summa-
rization process is to extract opinions about an en-
tity from free text or some other source, such as
surveys. and express the extracted information in a
structured format for further processing. We adopt
the approach to opinion extraction described by
Carenini et al (2006), which we summarize here.
Given a corpus of documents expressing opin-
ions about an entity, the system extracts a set of
evaluations on aspects or features of the product.
An evaluation consists of a polarity, a score for
the strength of the opinion, and the feature be-
ing evaluated. The polarity expresses whether the
opinion is positive or negative, and the strength
expresses the degree of the sentiment, which is
represented as an integer from 1 to 3. Possi-
ble polarity/strength (P/S) scores are thus [-3,-
2,-1,+1,+2,+3], with +3 being the most positive
evaluation, and -3 the most negative. For exam-
ple, using a DVD player as the entity, the com-
ment ?Excellent picture quality?on par with my
Pioneer, Panasonic, and JVC players.? contains an
opinion on the picture quality, and is a very posi-
tive evaluation (+3).
The features and their associated opinions are
organized into a hierarchy of user-defined features
(UDFs), so named because they can be defined by
a user according to the user?s needs or interests.
1
The outcome of the process of opinion extraction
and structuring is a UDF hierarchy in which each
node is annotated with all the evaluations it re-
ceived in the corpus (See Figure 1 for an example).
2.2 Heuristic Content Selection Strategy
Using the input information described above, con-
tent selection is framed as the process of selecting
a subset of those features that are deemed more
1
Actually, the system first extracts a set of surface-level
crude features (CFs) on which opinions were expressed, us-
ing methods described by Hu and Liu (2004). Next, the CFs
are mapped onto the UDFs using term similarity scores. The
process of mapping CFs to UDFs groups together semanti-
cally similar CFs and reduces redundancy. Our study ab-
stracts away from this mapping process, as well as the pro-
cess of creating the UDF structure. We leave the explanation
of the details to the original papers.
8
Camera
Lens [+1,+1,+3,-
2,+2]
Digital Zoom
Optical Zoom
. . .
Editing/Viewing
[+1,+1]
Viewfinder [-2,-
2,-1]
. . .
Flash
[+1,+1,+3,+2,+2]
. . .
Image
Image Type
TIFF
JPEG
. . .
Resolution
Effective Pixels
Aspect Ratio
. . .
Figure 1: Partial view of assumed input informa-
tion (UDF hierarchy annotated with user evalua-
tions) for a digital camera.
important and relevant to the user. This is done
using an importance measure defined on the avail-
able features (UDFs). This measure is calculated
from the P/S scores of the evaluations associated
to each UDF. Let PS(u) be the set of P/S scores
that UDF u receives. Then, a measure of im-
portance is defined as some function of the P/S
scores. Previous work considered only summing
the squares of the scores. In this work, we also
consider summing the absolute value of the scores.
So, the importance measure is defined as
dir moi(u) =
?
psPS(u)
ps
2
or
?
psPS(u)
|ps|
where the term ?direct? means the importance is
derived only from that feature and not from its
descendant features. The basic premises of these
metrics are that a feature?s importance should be
proportional to the number of evaluations of that
feature in the corpus, and that stronger evaluations
should be given more weight. The two versions
implement the latter differently, using the sum of
squares or the absolute values respectively. No-
tice that each non-leaf node in the feature hierar-
chy effectively serves a dual purpose. It is both a
feature upon which a user might comment, as well
as a category for grouping its sub-features. Thus,
a non-leaf node should be important if either its
descendants are important or the node itself is im-
portant. To this end, a total measure of importance
moi(u) is defined as
moi(u) =
?
?
?
?
?
?
?
dir moi(u) if CH(u) = ?
[? dir moi(u) +
(1? ?)?
?
v?CH(u)
moi(v)] otherwise
where CH(u) refers to the children of u in
the hierarchy and ? is some real parameter in the
range [0.5, 1] that adjusts the relative weights of
the parent and children. We found in our experi-
mentation that the parameter setting does not sub-
stantially change the performance of the system,
so we select the value 0.9 for ?, following previ-
ous work. As a result, the total importance of a
node is a combination of its direct importance and
of the importance of its children.
The selection procedure proceeds as follows.
First, the most obvious simple greedy selection
strategy was considered?sort the nodes in the UDF
by the measure of importance and select the most
important node until a desired number of features
is included. However, since a node derives part
of its ?importance? from its children, it is possible
for a node?s importance to be dominated by one or
more of its children. Including both the child and
parent node would be redundant because most of
the information is contained in the child. Thus, a
dynamic greedy selection algorithm was devised
in which the importance of each node was recal-
culated after each round of selection, with all pre-
viously selected nodes removed from the tree. In
this way, if a node that dominates its parent?s im-
portance is selected, its parent?s importance will
be reduced during later rounds of selection. No-
tice, however, that this greedy selection consists of
a series of myopic steps to decide which features
to include in the summary next, based on what has
been selected already and what remains to be se-
lected at this step. Although this series of local
decisions may be locally optimal, it may result in
a suboptimal choice of contents overall.
3 Clustering-Based Optimization
Strategy
To address the limitation of local optimality of
this initial strategy, we explore if the content se-
lection problem for opinion summarization can
be naturally and effectively solved by a global
optimization-based approach. Our approach as-
sumes the same input information as the previ-
ous approach, and we also use the direct measure
9
of importance defined above. Our framework is
UDF-based in the following senses. First, a UDF
is the basic unit of content that is selected for in-
clusion in the summary. Also, the information
content that needs to be ?covered? by the summary
is the sum of the information content in all of the
UDFs in the UDF hierarchy.
To reduce content selection to a clustering prob-
lem, we need the following components. First, we
need a cost function to quantify how well a UDF
(if selected) can express the information content
in another UDF. We call this measure the infor-
mation coverage cost. To define this cost func-
tion, we need to define the semantic relatedness
between the selected content and the covered con-
tent, which is domain-dependent. For example, we
can rely on similarity metrics such as ones based
on WordNet similarity scores (Fellbaum and oth-
ers, 1998). In the consumer product domain in
which we test our method, we use the UDF hi-
erarchy of the entity being summarized.
Second, we need a clustering paradigm that de-
fines the quality of a proposed clustering; that is,
a way to globally quantify how well all the infor-
mation content is represented by the set of UDFs
that we select. The clustering paradigm that we
found to most naturally fit our task is the p-median
problem (also known as the k-median problem),
from facility location theory. In its original in-
terpretation, p-median is used to find optimal lo-
cations for opening facilities which provide ser-
vices to customers, such that the cost of serving
all of the customers with these facilities is mini-
mized. This matches our intuition that the quality
of a summary of opinions depends on how well it
represents all of the opinions to be summarized.
Formally, given a set F of m potential locations
for facilities, a set U of n customers, a cost func-
tion d : F ? U ?? < representing the cost of
serving a customer u ? U with a facility f ? F ,
and a constant p ? m, an optimal solution to the
p-median problem is a subset S of F , such that the
expression
?
u?U
min
f?S
d(f, u)
is minimized, and |S| = p. The subset S is exactly
the set of UDFs that we would include in the sum-
mary, and the parameter p can be set to determine
the summary length.
Although solving the p-median problem is NP-
hard in general (Kariv and Hakimi, 1979), viable
approximation methods do exist. We use POP-
STAR, an implementation of an approximate so-
lution (Resende and Werneck, 2004) which has
an average error rate of less than 0.4% on all the
problem classes it was tested on in terms of the p-
median problem value. As an independent test of
the program?s efficacy, we compare the program?s
output to solutions which we obtained by brute-
force search on 12 of the 36 datasets we worked
with which are small enough such that an exact so-
lution can be feasibly found. POPSTAR returned
the exact solution in all 12 instances.
We now reinterpret the p-median problem for
summarization content selection by specifying the
sets U , F , and the information coverage cost d in
terms of properties of the summarization process.
We define the basic unit of the summarization pro-
cess to be UDFs, so the sets U and F correspond
to the set of UDFs describing the product. The
constant p is a parameter to the p-median prob-
lem, determining the summary size in terms of the
number of features.
The cost function is d(u, v), where u is a UDF
that is being considered for inclusion in the sum-
mary, and v is the UDF to be ?covered? by u. To
specify this cost, we need to consider both the to-
tal amount of information in v as well as the se-
mantic relationship between the two features. We
use the importance measure defined earlier, based
on the number and strength of evaluations of the
covered feature to quantify the former. The raw
importance score is modified by multipliers which
depend on the relationship between u and v. One
is the semantic relatedness between the two fea-
tures, which is modelled by the UDF tree hierar-
chy. We hypothesize that it is easier for a more
general feature to cover information about a more
specific feature than the reverse, and that features
that are not in a ancestor-descendant relationship
cannot cover information about each other because
of the tenuous semantic connection between them.
For example, knowing that a camera is well-liked
in general provides stronger evidence that its dura-
bility is also well-liked than the reverse. Based on
these assumptions, we define a multiplier for the
above measure of importance based on the UDF
tree structure, T (u, v), as follows.
T (u, v) =
?
?
?
T
up
? k, if u is a descendant of v
k, if u is an ancestor of v
?, otherwise
k is the length of the path from u to v in the UDF
10
hierarchy. T
up
is a parameter specifying the rela-
tive difficulty of covering information in a feature
that is an ancestor in the UDF hierarchy. Mirror-
ing our experience with the heuristic method, the
value of the parameter does not affect performance
very much. In our experiments and the example to
follow, we pick the values T
up
= 3, meaning that
covering information in an ancestor node is three
times more difficult than covering information in
a descendant node.
Another multiplier to the opinion domain is the
distribution of evaluations of the features. Cover-
age is expected to be less if the features are evalu-
ated differently; for example, if users rated a cam-
era well overall but the feature zoom poorly, a sen-
tence about how well the camera is rated in gen-
eral does not provide much evidence that the zoom
is not well liked, and vice versa. Since evalua-
tions are labelled with P/S ratings in our data, it is
natural to define this multiplier based on the dis-
tributions of ratings for the features. Given these
P/S ratings between -3 and +3, we first aggregate
the positive and negative evaluations. As before,
we test both summing absolute values and squared
values. Define:
imp pos(u) =
?
ps?PS(u)?ps>0
ps
2
or |ps|
imp neg(u) =
?
ps?PS(u)?ps<0
ps
2
or |ps|
Then, we calculate the parameter to the Bernoulli
distribution corresponding to the ratio of the im-
portance of the two polarities. That is, Bernoulli
with parameter
?(u) = imp pos(u)/(imp pos(u)+imp neg(u))
The distribution-based multiplier E(u, v) is the
Jensen-Shannon divergence from Ber(?(u)) to
Ber(?(v)), plus one for multiplicative identity
when the divergence is zero.
E(u, v) = JS(?(u), ?(v)) + 1
The final formula for the information coverage
cost is thus
d(u, v) = dir moi(v)? T (u, v)? E(u, v)
Consider the following example consisting of
four-node UDF tree and importance scores.
i. Covered ii. Solutions
A B C D p Selected Val.
C
o
v
e
r
i
n
g
A 0 50 30 240 1 A 320
B 165 0 ? 120 2 A,D 80
C 165 ? 0 ? 3 A,B,D 30
D 330 150 ? 0 4 A,B,C,D 0
Table 1: i. Information coverage cost scores for
the worked example. Rows represent the covering
feature, while columns represent the covered fea-
ture. ii. Optimal solution to p-median problem in
the worked example at different numbers of fea-
tures selected.
A dir moi(A) = 55
??
B C dir moi(B) = 50, dir moi(C) = 30
?
D dir moi(D) = 120
With parameter T
up
= 3 and setting the
distribution-based multiplier E to 1 to simplify
calculations (or for example, if the features re-
ceived the same distributions of evaluations), this
tree yields the information coverage cost scores
found in Table 1i. Running p-median on these val-
ues produces the optimal results found in Table 1ii.
This method trades off selecting centrally located
nodes near the root of the UDF tree and the im-
portance of the individual nodes. In this example,
D is selected after the root node A even though D
has a greater importance value.
4 Comparative Evaluation
4.1 Stochastic Data Generation
In our experiments we wanted to compare the two
content selection strategies (heuristic vs. p-median
optimization) on datasets that were both realistic
and diverse. Despite the widespread adoption of
user reviews in online websites, there is to our
knowledge no publicly available corpus of cus-
tomer reviews of sufficient size which is annotated
with features arranged in a hierarchy. While small-
scale corpora do exist for a small number of prod-
ucts, the size of the corpora is too small to be rep-
resentative of all possible distributions of evalu-
ations and feature hierarchies of products, which
limits our ability to draw any meaningful conclu-
sion from the dataset.
2
Thus, we stochastically
2
Using a constructed dataset based on real data where no
resources or agreed-upon evaluation methodology yet exists
has been done in other NLP tasks such as topic boundary de-
tection (Reynar, 1994) and local coherence modelling (Barzi-
lay and Lapata, 2005). We are encouraged, however, that sub-
sequent to our experiment, more resources for opinion anal-
11
mean std.
# Features 55.3889 8.5547
# Evaluated Features 21.6667 5.9722
# Children (depth 0) 11.3056 0.7753
# Children (depth 1 fertile) 5.5495 1.7724
Table 2: Statistics on the 36 generated data sets.
At depth 1, 134 of the 407 features in total across
the trees were barren. The generated tree hierar-
chies were quite flat, with a maximum depth of 2.
generated the data for the products to mimic real
product feature hierarchies and evaluations. We
did this by gathering statistics from existing cor-
pora of customer reviews about electronics prod-
ucts (Hu and Liu, 2004), which contain UDF hier-
archies and evaluations that have been defined and
annotated. Using these statistics, we created dis-
tributions over the characteristics of the data, such
as the number of nodes in a UDF hierarchy, and
sampled from these distributions to generate new
UDF hierarchies and evaluations. In total, we gen-
erated 36 sets of data, which covered a realistic set
of possible scenarios in term of feature hierarchy
structures as well as in term of distribution of eval-
uations for each feature. Table 2 presents some
statistics on the generated data sets.
4.2 Building a Human Performance Model
We adopt the evaluation approach that a good con-
tent selection strategy should perform similarly to
humans, which is the view taken by existing sum-
marization evaluation schemes such as ROUGE
(Lin, 2004) and the Pyramid method (Nenkova et
al., 2007). For evaluating our content selection
strategy, we conducted a user study asking human
participants to perform a selection task to create
?gold standard? selections. Participants viewed
and selected UDF features using a Treemap infor-
mation visualization. See Figure 2 for an example.
We recruited 25 university students or gradu-
ates, who were each presented with 19 to 20 of
the cases we generated as described above. Each
case represented a different hypothetical product,
which was represented by a UDF hierarchy, as
well as P/S evaluations from -3 to +3. These were
displayed to the participants by a Treemap visual-
ization (Shneiderman, 1992), which is able to give
an overview of the feature hierarchy and the eval-
uations that each feature received. Treemaps have
been shown to be a generally successful tool for
ysis such as a user review corpus by Constant et al (2008)
have been released, as an anonymous reviewer pointed out.
visualizing data in the customer review domain,
even for novice users (Carenini et al, 2006). In
a Treemap, the feature hierarchy is represented by
nested rectangles, with parent features being larger
rectangles, and children features being smaller
rectangles contained within its parent rectangle.
The size of the rectangles depends on the number
of evaluations that this feature received directly,
as well as indirectly through its children features.
Each evaluation is also shown as a small rectangle,
coloured according to its P/S rating, with -3 being
bright red, and +3 being bright green.
Participants received 30 minutes of interactive
training in using Treemaps, and were presented
with a scenario in which they were told to take the
role of a friend giving advice on the purchase of
an electronics product based on existing customer
reviews. They were then shown 22 to 23 scenar-
ios corresponding to different products and eval-
uations, and asked to select features which they
think would be important to include in a summary
to send to a friend. We discarded the first three
selections that participants made to allow them to
become further accustomed to the visualization.
The number of features that participants were
asked to select from each tree was 18% of the
number of selectable features. A feature is con-
sidered selectable if it appears in the Treemap vi-
sualization; that is, the feature receives at least
one evaluation, or one of its descendant features
does. This proportion was the average propor-
tion at which the selections made by the heuristic
greedy strategy and p-median diverged the most
when we were initially testing the algorithms. Be-
cause each tree contained a different number of
features, the actual number of features selected
ranged from two to seven. Features were given
generic labels like Feature 34, so that participants
cannot rely on preexisting knowledge about that
Figure 2: A sample Treemap visualization of the
customer review data sets shown to participants.
12
Selection method Cohen?s Kappa
heuristic, squared moi 0.4839
heuristic, abs moi 0.4841
p-median, squared moi 0.4679
p-median, abs moi 0.4821
Table 3: Cohen?s kappa for heuristic greedy and
p-median methods against human selections. Two
versions of the measure of importance were tested,
one using squared P/S scores, the other using ab-
solute values.
kind of product in their selections.
4.3 Evaluation Metrics
Using this human gold standard, we can now com-
pare the greedy heuristic and the p-median strate-
gies. We report the agreement between the hu-
man and machine selections in terms of kappa
and a version of the Pyramid method. The Pyra-
mid method is a summarization evaluation scheme
built upon the observation that human summaries
can be equally informative despite being divergent
in content (Nenkova et al, 2007). In the Pyramid
method, Summary Content Units (SCUs) in a set
of human-written model summaries are manually
identified and annotated. These SCUs are placed
into a pyramid with different tiers, corresponding
to the number of model (i.e. human) summaries
in which each SCU appears. A summary to be
evaluated is similarly annotated by SCUs and is
scored by the scores of its SCUs, which are the
tier of the pyramid in which the SCU appears. The
Pyramid score is defined as the sum of the weights
of the SCUs in the evaluated summary divided by
the maximum score achievable with this number
of SCUs, if we were to take SCUs starting from
the highest tier of the pyramid. Thus, a summary
scores highly if its SCUs are found in many of
the model summaries. We use UDFs rather than
text passages as SCUs, since UDFs are the ba-
sic units of content in our selections. Moderate
inter-annotator agreement between human feature
selections shows that our data fits the assumption
of the Pyramid method (i.e. diversity of human an-
notations); the Fleiss? kappa (1971) scores for the
human selections ranged from 0.2984 to 0.6151,
with a mean of 0.4456 among all 33 sets which
were evaluated. A kappa value above 0.6 is gener-
ally taken to indicate substantial agreement (Lan-
dis and Koch, 1977).
Figure 3: Pyramid scores for the two selection ap-
proaches at different numbers of features i. using
the squared importance measure, ii. using the ab-
solute value importance measure.
4.4 Results
The greedy heuristic method and p-median per-
form similarly at the number of features that the
human participants were asked to select. The dif-
ference is not statistically significant by a two-
tailed t-test. Table 3 shows that using absolute
values of P/S scores in the importance measure
is better than using squares. Squaring seems to
give too much weight to extreme evaluations over
more neutral evaluations. P-median is particu-
larly affected, which is not surprising as it uses the
measure of importance both in the raw importance
score and in the distribution-based multiplier.
The Pyramid method allows us to compare the
algorithms at different numbers of features. Fig-
ure 3 shows the average pyramid score for the
two methods over the proportion of features that
are selected. Overall, both algorithms perform
well, and reach a score of about 0.9 at 10% of
features selected. The heuristic method performs
slightly better when the proportion is below 25%,
but slightly worse above that proportion.
We consider several possible explanations for
the surprising result that the heuristic greedy
method and p-median methods perform similarly.
One possibility is that the approximate p-median
solution we adopted (POPSTAR) is error-prone on
this task, but this is unlikely as the approximate
method has been rigorously tested both externally
on much larger problems and internally on a sub-
set of our data. Another possibility is that the au-
tomatic methods have reached a ceiling in perfor-
mance by these evaluation metrics.
Nevertheless, these results are encouraging in
showing that our optimization-based method is a
viable alternative to a heuristic strategy for con-
tent selection, and validate that incorporating other
13
summarization decisions into content selection is
an option worth exploring.
5 Conclusions and Future Work
We have proposed a formal optimization-based
method for summarization content selection based
on the p-median clustering paradigm, in which
content selection is viewed as selecting clusters
of related information. We applied the frame-
work to opinion summarization of customer re-
views. An experiment evaluating our p-median
algorithm found that it performed about as well
as a comparable existing heuristic approach de-
signed for the opinion domain in terms of similar-
ity to human selections. These results suggest that
the optimization-based approach is a good starting
point for integration with other parts of the sum-
marization/NLG process, which is a promising av-
enue of research.
6 Acknowledgements
We would like to thank Lucas Rizoli, Gabriel Mur-
ray and the anonymous reviewers for their com-
ments and suggestions.
References
R. Barzilay and M. Lapata. 2005. Modeling Local Co-
herence: An Entity-based Approach. In Proc. 43rd
ACL, pages 141?148.
G. Carenini and J.C.K. Cheung. 2008. Extractive vs.
NLG-based abstractive summarization of evaluative
text: The effect of corpus controversiality. In Proc.
5th INLG.
G. Carenini and J.D. Moore. 2006. Generating and
evaluating evaluative arguments. Artificial Intelli-
gence, 170(11):925?952.
G. Carenini, R.T. Ng, and A. Pauls. 2006. Interac-
tive multimedia summaries of evaluative text. In
Proc. 11th Conference on Intelligent User Inter-
faces, pages 124?131.
N. Constant, C. Davis, C. Potts, and F. Schwarz.
2008. The pragmatics of expressive content: Evi-
dence from large corpora. Sprache und Datenverar-
beitung.
C. Dellarocas, N. Awad, and X. Zhang. 2004. Explor-
ing the Value of Online Reviews to Organizations:
Implications for Revenue Forecasting and Planning.
In Proc. 24th International Conference on Informa-
tion Systems.
C. Fellbaum et al 1998. WordNet: an electronic lexi-
cal database. Cambridge, Mass: MIT Press.
J.L. Fleiss et al 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
M. Gamon, A. Aue, S. Corston-Oliver, and E. Ringger.
2005. Pulse: Mining customer opinions from free
text. Lecture Notes in Computer Science, 3646:121?
132.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. 2004 ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 168?177. ACM Press New
York, NY, USA.
O. Kariv and S.L. Hakimi. 1979. An algorithmic
approach to network location problems. II: the p-
medians. SIAM Journal on Applied Mathematics,
37(3):539?560.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
C.Y. Lin. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. Workshop on
Text Summarization Branches Out, pages 74?81.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The Pyramid Method: Incorporating human con-
tent selection variation in summarization evaluation.
ACM Transactions on Speech and Language Pro-
cessing (TSLP), 4(2).
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: an architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7(03):225?250.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. 42nd ACL, pages
271?278.
M.G.C. Resende and R.F. Werneck. 2004. A Hy-
brid Heuristic for the p-Median Problem. Journal
of Heuristics, 10(1):59?88.
J.C. Reynar. 1994. An automatic method of finding
topic boundaries. In Proc. 32nd ACL, pages 331?
333.
B. Shneiderman. 1992. Tree visualization with tree-
maps: 2-d space-filling approach. ACM Transac-
tions on Graphics (TOG), 11(1):92?99.
E.R. Tufte, S.R. McKay, W. Christian, and J.R Matey.
1998. Visual Explanations: Images and Quanti-
ties, Evidence and Narrative. Computers in Physics,
12(2):146?148.
M.X. Zhou and V. Aggarwal. 2004. An optimization-
based approach to dynamic data content selection
in intelligent multimedia interfaces. In Proc. 17th
annual ACM symposium on User interface software
and technology, pages 227?236. ACM Press New
York, NY, USA.
14
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 23?33,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Utilizing Extra-sentential Context for Parsing
Jackie Chi Kit Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
Syntactic consistency is the preference to
reuse a syntactic construction shortly after its
appearance in a discourse. We present an anal-
ysis of the WSJ portion of the Penn Tree-
bank, and show that syntactic consistency is
pervasive across productions with various left-
hand side nonterminals. Then, we implement
a reranking constituent parser that makes use
of extra-sentential context in its feature set.
Using a linear-chain conditional random field,
we improve parsing accuracy over the gen-
erative baseline parser on the Penn Treebank
WSJ corpus, rivalling a similar model that
does not make use of context. We show that
the context-aware and the context-ignorant
rerankers perform well on different subsets of
the evaluation data, suggesting a combined ap-
proach would provide further improvement.
We also compare parses made by models, and
suggest that context can be useful for parsing
by capturing structural dependencies between
sentences as opposed to lexically governed de-
pendencies.
1 Introduction
Recent corpus linguistics work has produced ev-
idence of syntactic consistency, the preference to
reuse a syntactic construction shortly after its ap-
pearance in a discourse (Gries, 2005; Dubey et al,
2005; Reitter, 2008). In addition, experimental stud-
ies have confirmed the existence of syntactic prim-
ing, the psycholinguistic phenomenon of syntactic
consistency1. Both types of studies, however, have
1Whether or not corpus-based studies of consistency have
any bearing on syntactic priming as a reality in the human mind
limited the constructions that are examined to partic-
ular syntactic constructions and alternations. For in-
stance, Bock (1986) and Gries (2005) examine spe-
cific constructions such as the passive voice, dative
alternation and particle placement in phrasal verbs,
and Dubey et al (2005) deal with the internal struc-
ture of noun phrases. In this work, we extend these
results and present an analysis of the distribution of
all syntactic productions in the Penn Treebank WSJ
corpus. We provide evidence that syntactic consis-
tency is a widespread phenomenon across produc-
tions of various types of LHS nonterminals, includ-
ing all of the commonly occurring ones.
Despite this growing evidence that the probability
of syntactic constructions is not independent of the
extra-sentential context, current high-performance
statistical parsers (e.g. (Petrov and Klein, 2007; Mc-
Closky et al, 2006; Finkel et al, 2008)) rely solely
on intra-sentential features, considering the partic-
ular grammatical constructions and lexical items
within the sentence being parsed. We address this
by implementing a reranking parser which takes ad-
vantage of features based on the context surrounding
the sentence. The reranker outperforms the genera-
tive baseline parser, and rivals a similar model that
does not make use of context. We show that the
context-aware and the context-ignorant models per-
form well on different subsets of the evaluation data,
suggesting a feature set that combines the two mod-
els would provide further improvement. Analysis of
the rerankings made provides cases where contex-
tual information has clearly improved parsing per-
is a subject of debate. See (Pickering and Branigan, 1999) and
(Gries, 2005) for opposing viewpoints.
23
prior denominator
prior numerator pos_adapt numerator
pos_adapt denominator
fp,tf?p,t
f?p,?t fp,?t
?p p
?t
t
Figure 1: Visual representation of calculation of prior and
positive adaptation probabilities. t represents the pres-
ence of a construction in the target set. p represents the
presence of the construction in the prime set.
formance, indicating the potential of extra-sentential
contextual information to aid parsing, especially for
structural dependencies between sentences, such as
parallelism effects.
2 Syntactic Consistency in the Penn
Treebank WSJ
Syntactic consistency has been examined by Dubey
et al (2005) for several English corpora, including
the WSJ, Brown, and Switchboard corpora. They
have provided evidence that syntactic consistency
exists not only within coordinate structures, but also
in a variety of other contexts, such as within sen-
tences, between sentences, within documents, and
between speaker turns in the Switchboard corpus.
However, their analysis rests on a selected number
of constructions concerning the internal structure of
noun phrases. We extend their result here to arbi-
trary syntactic productions.
There have also been studies into syntactic con-
sistency that consider all syntactic productions in
dialogue corpora (Reitter, 2008; Buch and Pietsch,
2010). These studies find an inverse correlation be-
tween the probability of the appearance of a syn-
less frequent        Production-type deciles        more frequent
Pr
op
or
tio
n 
of
 c
on
si
st
en
t p
ro
du
ct
io
n-
ty
pe
s
Figure 2: Production-types (singletons removed) catego-
rized into deciles by frequency and the proportion of the
production-types in that bin that is consistent to a signifi-
cant degree.
tactic structure and the distance since its last occur-
rence, which indicates syntactic consistency. These
studies, however, do not provide consistency results
on subsets of production-types, such as by produc-
tion LHS as our study does, so the implications that
can be drawn from them for improving parsing are
less apparent.
We adopt the measure used by Dubey et al (2005)
to quantify syntactic consistency, adaptation prob-
ability. This measure originates in work on lexical
priming (Church, 2000), and quantifies the probabil-
ity of a target word or construction w appearing in a
?primed? context. Specifically, four frequencies are
calculated, based on whether the target construction
appears in the previous context (the prime set), and
whether the construction appears after this context
(the target set):
fp,?t(w) = # of times w in prime set only
f?p,t(w) = # of times w in target set only
f?p,?t(w) = # of times w in neither set
fp,t(w) = # of times w in both sets
We also define N to be the sum of the four fre-
24
LHS prior pos adapt ratio + > prior sig. insig. + < prior sig.
ADJP 0.03 0.05 1.96 26 251 0
ADVP 0.21 0.24 1.15 26 122 0
NP 0.17 0.22 1.27 281 2284 0
PP 0.56 0.58 1.04 32 125 0
PRN 0.01 0.03 4.60 12 82 0
PRT 0.06 0.08 1.40 3 3 0
QP 0.03 0.18 5.41 24 147 0
S 0.30 0.34 1.13 42 689 1
SBAR 0.15 0.20 1.31 13 68 0
SINV 0.01 0.01 1.00 3 77 0
VP 0.08 0.12 1.56 148 1459 0
WHADVP 0.04 0.08 1.84 2 8 0
WHNP 0.07 0.10 1.39 3 47 0
WHPP 0.01 0.02 2.65 1 1 0
Table 1: Weighted average by production frequency among non-singleton production-types of prior and positive adap-
tation probabilities, and the ratio between them. The columns on the right show the number of production-types
for which the positive adaptation probability is significantly greater than, not different from, or less than the prior
probability. We exclude LHSs with a weighted average prior of less than 0.005, due to the small sample size.
quencies. Then, we define the prior and the positive
adaptation probability of a construction as follows
(See also Figure 1):
prior(w) = fp,t(w) + f?p,t(w)
N
pos adapt(w) = fp,t(w)
fp,t(w) + fp,?t(w)
A positive adaptation probability that is greater
than the prior probability would be interpreted as
evidence for syntactic consistency for that construc-
tion. We conduct ?2 tests for statistical signif-
icance testing. We analyze the Penn Treebank
WSJ corpus according this schema for all produc-
tions that occur in sections 2 to 22. These are the
standard training and development sets for train-
ing parsers. We did not analyze section 23 in or-
der not to use its characteristics in designing our
reranking parser so that we can use this section as
our evaluation test set. Our analysis focuses on the
consistency of rules between sentences, so we take
the previous sentence within the same article as the
prime set, and the current sentence as the target set
in calculating the probabilities given above. The
raw data from which we produced our analysis are
available at http://www.cs.toronto.edu/
?
jcheung/wsj_parallelism_data.txt.
We first present results for consistency in all the
production-types2, grouped by the LHS of the pro-
duction. Table 1 shows the weighted average prior
and positive adaptation probabilities for productions
by LHS, where the weighting is done by the num-
ber of occurrence of that production. Production-
types that only occur once are removed. It also
shows the number of production-types in which the
positive adaptation probability is statistically signif-
icantly greater than, not significantly different from,
and significantly lower than the prior probability.
Quite remarkably, very few production-types are
significantly less likely to reoccur compared to the
prior probability. Also note the wide variety of LHSs
for which there is a large number of production-
types that are consistent to a significant degree.
While a large number of production-types appears
not to be significantly more likely to occur in a
primed context, this is due to the large number of
production-types which only appear a few times.
Frequently occurring production-types mostly ex-
hibit syntactic consistency.
We show this in Figure 2, in which we put
non-singleton production-types into ten bins by fre-
2That is, all occurrences of a production with a particular
LHS and RHS.
25
Ten most frequent production-types
production f?p,t fp,t fp,?t prior pos adapt ratio
PP ? IN NP 5624 26224 5793 0.80 0.82 1.02
NP ? NP PP 9033 12451 9388 0.54 0.57 1.05
NP ? DT NN 9198 10585 9172 0.50 0.54 1.07
S ? NP VP 8745 9897 9033 0.47 0.52 1.11
S ? NP VP . 8576 8501 8888 0.43 0.49 1.13
S ? VP 8717 7867 9042 0.42 0.47 1.11
NP ? PRP 7208 5309 7285 0.32 0.42 1.33
ADVP ? RB 7986 3949 7905 0.30 0.33 1.10
NP ? NN 7630 3390 7568 0.28 0.31 1.11
VP ? TO VP 7039 3552 7250 0.27 0.33 1.23
Ten most consistent among 10% most frequent production-types
production f?p,t fp,t fp,?t prior pos adapt ratio
QP ? # CD CD 51 18 45 0.00 0.29 163.85
NP ? JJ NNPS 52 7 53 0.00 0.12 78.25
NP ? NP , ADVP 109 24 99 0.00 0.20 58.05
NP ? DT JJ CD NN 63 6 67 0.00 0.08 47.14
PP ? IN NP NP 83 10 87 0.00 0.10 43.86
QP ? IN $ CD 51 3 49 0.00 0.06 42.28
NP ? NP : NP . 237 128 216 0.01 0.37 40.34
INTJ ? UH 59 4 60 0.00 0.06 39.26
ADVP ? IN NP 108 11 83 0.00 0.12 38.91
NP ? CD CD 133 21 128 0.00 0.14 36.21
Table 2: Some instances of consistency effects of productions. All productions? pos adapt probability is significantly
greater than its prior probability at p < 10?6.
quency and calculated the proportion of production-
types in that bin for which the positive adaptation
probability is significantly greater than the prior. It is
clear that the most frequently occurring production-
types are also the ones most likely to exhibit evi-
dence of syntactic consistency.
Table 2 shows the breakdown of the prior and
positive adaptation calculation components for the
ten most frequent production-types and the ten most
consistent (by the ratio pos adapt / prior) produc-
tions among the top decile of production-types. Note
that all of these production-types are consistent to a
statistically significant degree. Interestingly, many
of the most consistent production-types have NP as
the LHS, but overall, productions with many differ-
ent LHS parents exhibit consistency.
3 A Context-Aware Reranker
Having established evidence for widespread syntac-
tic consistency in the WSJ corpus, we now investi-
gate incorporating extra-sentential context into a sta-
tistical parser. The first decision to make is whether
to incorporate the context into a generative or a dis-
criminative parsing model.
Employing a generative model would allow us to
train the parser in one step, and one such parser
which incorporates the previous context has been
implemented by Dubey et al (2006). They imple-
ment a PCFG, learning the production probabilities
by a variant of standard PCFG-MLE probability es-
timation that conditions on whether a rule has re-
cently occurred in the context or not:
P (RHS|LHS,Prime) = c(LHS ? RHS,Prime)
c(LHS,Prime)
LHS and RHS represent the left-hand side and
26
right-hand side of a production, respectively. Prime
is a binary variable which is True if and only if
the current production has occurred in the prime set
(the previous sentence). c represents the frequency
count.
The drawback of such a system is that it doubles
the state space of the model, and hence likely in-
creases the amount of data needed to train the parser
to a comparable level of performance as a more com-
pact model, or would require elaborate smoothing.
Dubey et al (2006) find that this system performs
worse than the baseline PCFG-MLE model, drop-
ping F1 from 73.3% to 71.6%3.
We instead opt to incorporate the extra-sentential
context into a discriminative reranking parser, which
naturally allows additional features to be incorpo-
rated into the statistical model. Many discriminative
models of constituent parsing have been proposed in
recent literature. They can be divided into two broad
categories?those that rerank the N-best outputs of a
generative parser, and those that make all parsing de-
cisions using the discriminative model. We choose
to implement an N-best reranking parser so that we
can utilize state-of-the-art generative parsers to en-
sure a good selection of candidate parses to feed
into our reranking module. Also, fully discrimina-
tive models tend to suffer from efficiency problems,
though recent models have started to overcome this
problem (Finkel et al, 2008).
Our approach is similar to N-best reranking
parsers such as Charniak and Johnson (2005)
and Collins and Koo (2005), which implement a va-
riety of features to capture within-sentence lexical
and structural dependencies. It is also similar to
work which focuses on coordinate noun phrase pars-
ing (e.g. (Hogan, 2007; Ku?bler et al, 2009)) in that
we also attempt to exploit syntactic parallelism, but
in a between-sentence setting rather than in a within-
sentence setting that only considers coordination.
As evidence of the potential of an N-best rerank-
ing approach with respect to extra-sentential con-
text, we considered the 50-best parses in the devel-
opment set produced by the generative parser, and
categorized each into one of nine bins depending
on whether this candidate parse exhibits more, less,
3A similar model which conditions on whether productions
have previously occurred within the same sentence, however,
improves F1 to 73.6%.
Overlap
less equal more
worse F1 32519 7224 17280(81.8%) (69.3%) (75.4%)
equal F1 1023 1674 540(2.6%) (16.1%) (2.4%)
better F1 6224 1527 5106(15.7%) (14.6%) (22.3%)
Table 3: Correlation between rule overlap and F1 com-
pared to the generative baseline for the 50-best parses in
the development set.
or the same amount of rule overlap with the previ-
ous correct parse than the generative baseline, and
whether the candidate parse has a better, worse, or
the same F1 measure than the generative baseline
(Table 3). We find that a larger percentage of candi-
date parses which share more productions with the
previous parse are better than the generative base-
line parse than for the other categories, and this dif-
ference is statistically significant (?2 test).
3.1 Conditional Random Fields
For our statistical reranker, we implement a linear-
chain conditional random field (CRF). CRFs are a
very flexible class of graphical models which have
been used for various sequence and relational la-
belling tasks (Lafferty et al, 2001). They have been
used for tree labelling, in XML tree labelling (Jousse
et al, 2006) and semantic role labelling tasks (Cohn
and Blunsom, 2005). They have also been used for
shallow parsing (Sha and Pereira, 2003), and full
constituent parsing (Finkel et al, 2008; Tsuruoka et
al., 2009). We exploit the flexibility of CRFs by in-
corporating features that depend on extra-sentential
context.
In a linear-chain CRF, the conditional probabil-
ity of a sequence of labels y = y{t=1...T} given a se-
quence of observed output x = x{t=1...T} and weight
vector ? = ?{k=1...K} is given as follows:
P (y|x) = 1
Z
exp(
T
?
t=1
?
k
?kfk(yt?1, yt, x, t))
27
where Z is the partition function. The feature func-
tions fk(yt?1, yt, x, t) can depend on two neighbour-
ing parses, the sentences in the sequence, and the
position of the sentence in the sequence. Since our
feature functions do not depend on the words or
the time-step within the sequence, however, we will
write fk(yt?1, yt) from now on.
We treat each document in the corpus as one CRF
sequence, and each sentence as one time-step in
the sequence. The label sequence then is the se-
quence of parses, and the outputs are the sentences
in the document. Since there is a large number of
parses possible for each sentence and correspond-
ingly many possible states for each label variable,
we restrict the possible label state-space by extract-
ing the N-best parses from a generative parser, and
rerank over the sequences of candidate parses thus
provided. We use the generative parser of Petrov
and Klein (2007), a state-splitting parser that uses an
EM algorithm to find splits in the nonterminal sym-
bols to maximize training data likelihood. We use
the 20-best parses, with an oracle F1 of 94.96% on
section 23.
To learn the weight vector, we employ a stochastic
gradient ascent method on the conditional log like-
lihood, which has been shown to perform well for
parsing tasks (Finkel et al, 2008). In standard gra-
dient ascent, the conditional log likelihood with a L2
regularization term for a Gaussian prior for a train-
ing corpus of N sequences is
L(?) =
N
?
i=1
?
t,k
?kfk(y(i)t?1, y
(i)
t )
?
N
?
i=1
log Z(i) ?
?
k
?2k
2?2
And the partial derivatives with respect to the
weights are
?L
??k
=
N
?
i=1
?
t
fk(y(i)t?1, y
(i)
t )
?
N
?
i=1
?
t
?
y,y?
fk(y, y?)P (y, y?|x(i))
?
?
k
?k
?2
The first term is the feature counts in the train-
ing data, and the second term is the feature expecta-
tions according to the current weight vector. The
third term corresponds to the penalty to non-zero
weight values imposed by regularization. The prob-
abilities in the second term can be efficiently calcu-
lated by the CRF-version of the forward-backward
algorithm.
In standard gradient ascent, we update the weight
vector after iterating through the whole training cor-
pus. Because this is computationally expensive, we
instead use stochastic gradient ascent, which ap-
proximates the true gradient by the gradient calcu-
lated from a single sample from the training corpus.
We thus do not have to sum over the training set in
the above expressions. We also employ a learning
rate multiplier on the gradient. Thus, the weight up-
date for the ith encountered training sequence during
training is
? = ? + ?i?Lstochastic(?)
?i = ?
? ?N
? ?N + i
The learning rate function is modelled on the one
used by Finkel et al (2008). It is designed such that
?i is halved after ? passes through the training set.
We train the model by iterating through the train-
ing set in a randomly permuted order, updating the
weight vector after each sequence. The parameters
?, ? , and ? are tuned to the development set. The fi-
nal settings we use are ? = 0.08, ? = 5, and ? = 50.
We use sections 2?21 of the Penn Treebank WSJ for
training, 22 for development, and 23 for testing. We
conduct 20-fold cross validation to generate the N-
best parses for the training set, as is standard for N-
best rerankers.
To rerank, we do inference with the linear-chain
CRF for the most likely sequence of parses using
the Viterbi algorithm.
3.2 Feature Functions
We experiment with various feature functions that
depend on the syntactic and lexical parallelism be-
tween yt?1 and yt. We use the occurrence of a rule
in yt that occurred in yt?1 as a feature. Based on the
results of the corpus analysis, the first representation
28
(1) (S (NP (DT NN)) (VP (VBD)))
(2) (S (NP (NNS)) (VP (VBD)))
Phrasal features:
Template: (parent, childL, childR, repeated)
(S, edge, NP, +), (S, NP, VP, +), (S, VP, edge, +), (NP, edge,
NNS,?), (NP, NNS, edge,?), (VP, edge, VBD, +), (VP, VBD,
edge, +)
Lexical features:
Template: (parent, POSL, POSR, repeated)
(S, edge, NNS, ?), (S, NNS, VBD, ?), (S, VBD, edge, +),
(NP, edge, NNS, ?), (NP, NNS, edge, ?), (VP, edge, VBD,
+), (VP, VBD, edge, +)
Figure 3: Example of features extracted from a parse se-
quence specified down to the POS level.
we tried was to simply enumerate the (non-lexical)
productions in yt along with whether that production
is found in yt?1. However, we found that our most
successful feature function is to consider overlaps in
partial structures of productions.
Specifically, we decompose a tree into all of the
nonlexical vertically and horizontally markovized
subtrees. Each of the subtrees in yt marked by
whether that same subtree occurs in the previous
tree is a feature. The simple production represen-
tation corresponds to a vertical markovization of 1
and a horizontal markovization of infinite. We found
that a vertical markovization of 1 and a horizontal
markovization of 2 produced the best results on our
data. We will call this model the phrasal model.
This schema so far only considers local substruc-
tures of parse trees, without being informed by the
lexical information found in the leaves of the tree.
We try another schema which considers the POS tag
sequences found in each subtree. A feature then is
the node label of the root of the subtree with the POS
tag sequence it dominates, again decomposed into
sequences of length 2 by markovization. We will
call this model the lexical model.
To extract features from this sequence, we con-
sider the substructures in the second parse, and mark
whether they are found in the first parse as well. We
add edge markers to mark the beginning and end of
constituents. See Figure 3 for an example of features
Method F1 (%)
Model-averaged 90.47
Combined, jointly trained ?Context 90.33
Combined, jointly trained 90.31
Model-averaged ?Context 90.22
lexical ?Context 90.21
lexical 90.20
phrasal 90.12
phrasal ?Context 89.74
Generative 89.70
Table 4: Development set (section 22) results of various
models that we trained. Italicized are the models we use
for the test set.
extracted by the two models.
We will consider various ways of combining the
two schemata above in the next section. In addition,
we also add a feature corresponding to the scaled log
probability of a parse tree derived from the genera-
tive parsing baseline. Scaling is necessary because
of the large differences in the magnitude of the log
probability for different sentences. The scaling for-
mula that we found to work best is to scale the max-
imum log probability among the N-best candidate
parses to be 1.0 and the minimum to be 0.0.
3.3 Results
We train the two models which make use of extra-
sentential context described in the previous section,
and use the model to parse the development and
test set. We also trained a model which combines
both sets of features, but we found that we get better
performance by training the two models separately,
then averaging the models by computing the respec-
tive averages of their features? weights. Thus, we
use the model-averaged version of the models that
consider context in the test set experiments. The
generative parser forms the first baseline method
to which we compare our results. We also train a
reranker which makes use of the same features as we
described above, but without marking whether each
substructure occurs in the previous sentence. This is
thus a reranking method which does not make use
of the previous context. Again, we tried model aver-
aging, but this produces less accurate parses on the
29
LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB
development set ? length ? 40 development set ? all sentences
Generative 90.33 90.20 90.27 39.92 0.68 71.99 89.64 89.75 89.70 37.76 0.82 68.65
+Context 91.25 90.71 90.98 41.25 0.61 73.45 90.62 90.33 90.47 38.88 0.74 70.47
?Context 90.85 90.78 90.82 40.62 0.62 73.00 90.28 90.38 90.22 38.24 0.74 70.00
Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative
is the generative baseline of Petrov and Klein (2007), +Context is the best performing reranking model using previous
context (model-averaged phrasal and lexical), ?Context is the best performing reranking model not using
previous context (jointly trained phrasal and lexical).
LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB
test set ? length ? 40 test set ? all sentences
Generative 90.04 89.84 89.94 38.31 0.80 68.33 89.60 89.35 89.47 36.05 0.94 65.81
+Context 90.63 90.11 90.37 39.02 0.73 69.40 90.17 89.64 89.91 36.84 0.87 67.09
?Context 90.64 90.43 90.54 38.62 0.72 69.84 90.20 89.97 90.08 36.47 0.85 67.55
Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
development set, so we use the jointly trained model
on the test set. We will refer to this model as the
context-ignorant or ?Context model, as opposed to
the previous context-aware or +Context model. The
results of these experiments on the development set
are shown in Table 4.
PARSEVAL results4 on the development and test
set are presented in Tables 5 and 6. We see that
the reranked models outperform the generative base-
line model in terms of F1, and that the reranked
model that uses extra-sentential context outperforms
the version that does not use extra-sentential context
in the development set, but not in the test set. Us-
ing Bikel?s randomized parsing evaluation compara-
tor5, we find that both reranking models outperform
the baseline generative model to statistical signifi-
cance for recall and precision. The context-ignorant
reranker outperforms the context-aware reranker on
recall (p < 0.01), but not on precision (p = 0.42).
However, the context-aware model has the highest
exact match scores in both the development and the
test set.
The F1 result suggests two possibilities?either the
context-aware model captures the same information
as the context-ignorant model, but less effectively, or
the two models capture different information about
4This evaluation ignores punctuation and corresponds to the
new.prm parameter setting on evalb.
5http://www.cis.upenn.edu/
?
dbikel/
software.html
Sec. ?Context better same +Context better
22 157 1357 186
23 258 1904 254
Table 7: Context-aware vs. context-ignorant reranking
results, by sentential F1.
the parses. Two pieces of evidence point to the
latter possibility. First, if the context-aware model
were truly inferior, then we would expect it to out-
perform the context-ignorant model on almost no
sentences. Otherwise, we would expect them to
do well on different sentences. Table 7 shows that
the context-aware model outperforms the context-
ignorant model on nearly as many trees in the test
section as the reverse. Second, if we hypotheti-
cally had an oracle that could determine whether the
context-ignorant or the context-aware model would
be more accurate on a sentence and if the two models
were complementary to each other, we would expect
to achieve a gain in F1 over the generative baseline
which is roughly the sum of the gain achieved by
each model separately. This is indeed the case, as
we are able to achieve F1s of 91.23% and 90.89%
on sections 22 and 23 respectively, roughly twice the
improvement that the individual models obtain.
To put our results in perspective, we now compare
the magnitude of the improvement in F1 our context-
30
System Baseline Best Imp. (rel.)
Dubey et al (2006) 73.3 73.6 0.3 (1.1%)
Hogan (2007) 89.4 89.6 0.2 (1.9%)
This work 89.5 89.9 0.4 (3.8%)
Table 8: A comparison of parsers specialized to exploit
intra- or extra-sentential syntactic parallelism on section
23 in terms of the generative baseline they compare them-
selves against, the best F1 their non-baseline models
achieve, and the absolute and relative improvements.
aware model achieves over the generative baseline
to that of other systems specialized to exploit intra-
or extra-sentential parallelism. We achieve a greater
improvement despite the fact that our generative
baseline provides a higher level of performance, and
is presumably thus more difficult to improve upon
(Table 8). These systems do not compare themselves
against a reranked model that does not use paral-
lelism as we do in this work.
During inference, the Viterbi algorithm recov-
ers the most probable sequence of parses, and this
means that we are relying on the generative parser to
provide the context (i.e. the previous parses) when
analyzing any given sentence. We do another type of
oracle analysis in which we provide the parser with
the correct, manually annotated parse tree of the
previous sentence when extracting features for the
current sentence during training and parsing. This
?perfect context? model achieves F1s of 90.42% and
90.00% on sections 22 and 23 respectively, which is
comparable to the best results of our reranking mod-
els. This indicates that the lack of perfect contextual
information is not a major obstacle to further im-
proving parsing performance.
3.4 Analysis
We now analyze several specific cases in the devel-
opment set in which the reranker makes correct use
of contextual information. They concretely illustrate
how context can improve parsing performance, and
confirm our initial intuition that extra-sentential con-
text can be useful for parsing. The sentence in (3)
and (4) is one such case.
(3) Generative/Context-ignorant: (S (S A BMA
spokesman said ?runaway medical costs? have
made health insurance ?a significant
challenge) ,? and (S margins also have been
pinched ...) (. .))
(4) Context-aware: (S (NP A BMA spokesman)
(VP said ?runaway medical costs? have made
health insurance ?a significant challenge,? and
margins also have been pinched ...) (. .))
The baseline and the context-ignorant models
parse the sentence as a conjunction of two S clauses,
misanalyzing the scope of what is said by the BMA
spokesman to the first part of the conjunct. By an-
alyzing the features and feature weight values ex-
tracted from the parse sequence, we determined that
the context-aware reranker is able to correct the
analysis of the scoping due to a parallelism in the
syntactic structure. Specifically, the substructure
S ? V P. is present in both this sentence and the
previous sentence of the reranked sequence, which
also contains a reporting verb.
(5) (S (NP BMA Corp., Kansas City, Mo.,) (VP
said it?s weighing ?strategic alternatives? ...
and is contacting possible buyers ...) (. .))
As a second example, consider the following sen-
tence.
(6) Generative/Context-ignorant: To achieve
maximum liquidity and minimize price
volatility, (NP either all markets) (VP should
be open to trading or none).
(7) Context-aware: To achieve maximum liquidity
and minimize price volatility, (CC either) (S
(NP all markets) should be open to trading or
none).
The original generative and context-ignorant
parses posit that ?either all markets? is a noun
phrase, which is incorrect. Syntactic parallelism cor-
rects this for two reasons. First, the reranker prefers
a determiner to start an NP in a consistent context,
as both surround sentences also contain this sub-
structure. Also, the previous sentence also contains
a conjunction CC followed by a S node under a S
node, which the reranker prefers.
While these examples show contextual features to
be useful for parsing coordinations, we also found
31
context-awareness to be useful for other types of
structural ambiguity such as PP attachment ambi-
guity. Notice that the method we employ to cor-
rect coordination errors is different from previous
approaches which usually rely on lexical or syntac-
tic similarity between conjuncts rather than between
sentences. Our approach can thus broaden the range
of sentences that can be usefully reranked. For ex-
ample, there is little similarity between conjuncts to
avail of in the second example (Sentences 6 and 7).
Based on these analyses, it appears that con-
text awareness provides a source of information for
parsing which is not available to context-ignorant
parsers. We should thus consider integrating both
types of features into the reranking parser to build
on the advantages of each. Specifically, within-
sentence features are most appropriate for lexi-
cal dependencies and some structural dependencies.
Extra-sentential features, on the other hand, are ap-
propriate for capturing the syntactic consistency ef-
fects as we have demonstrated in this paper.
4 Conclusions
In this paper, we have examined evidence for syn-
tactic consistency between neighbouring sentences.
First, we conducted a corpus analysis of the Penn
Treebank WSJ, and shown that parallelism exists
between sentences for productions with a variety
of LHS types, generalizing previous results for
noun phrase structure. Then, we explored a novel
source of features for parsing informed by the extra-
sentential context. We improved on the parsing ac-
curacy over a generative baseline parser, and rival a
similar reranking model that does not rely on extra-
sentential context. By examining the subsets of
the evaluation data on which each model performs
best and also individual cases, we argue that con-
text allows a type of structural ambiguity resolution
not available to parsers which only rely on intra-
sentential context.
Acknowledgments
We would like to thank the anonymous reviewers
and Timothy Fowler for their comments. This work
is supported in part by the Natural Sciences and En-
gineering Research Council of Canada.
References
J.K. Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18(3):355?387.
A. Buch and C. Pietsch. 2010. Measuring syntactic
priming in dialog corpora. In Proceedings of the Con-
ference on Linguistic Evidence 2010: Empirical, The-
oretical and Computational Perspectives.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd ACL, pages 173?180. Association
for Computational Linguistics.
K.W. Church. 2000. Empirical estimates of adaptation:
the chance of two Noriegas is closer to p/2 than p2. In
Proceedings of 18th COLING, pages 180?186. Asso-
ciation for Computational Linguistics.
T. Cohn and P. Blunsom. 2005. Semantic role labelling
with tree conditional random fields. In Ninth Confer-
ence on Computational Natural Language Learning,
pages 169?172.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
A. Dubey, P. Sturt, and F. Keller. 2005. Parallelism in
coordination as an instance of syntactic priming: Evi-
dence from corpus-based modeling. In Proceedings of
HLT/EMNLP 2005, pages 827?834.
A. Dubey, F. Keller, and P. Sturt. 2006. Integrating syn-
tactic priming into an incremental probabilistic parser,
with an application to psycholinguistic modeling. In
Proceedings of the 21st COLING and the 44th ACL,
pages 417?424. Association for Computational Lin-
guistics.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proceedings of ACL-08: HLT, pages 959?967.
S.T. Gries. 2005. Syntactic priming: A corpus-based
approach. Journal of Psycholinguistic Research,
34(4):365?399.
D. Hogan. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In Proceedings of
45th ACL, volume 45, pages 680?687.
F. Jousse, R. Gilleron, I. Tellier, and M. Tommasi. 2006.
Conditional random fields for XML trees. In ECML
Workshop on Mining and Learning in Graphs.
S. Ku?bler, W. Maier, E. Hinrichs, and E. Klett. 2009.
Parsing coordinations. In Proceedings of the 12th
EACL, pages 406?414. Association for Computational
Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International
Conference on Machine Learning, pages 282?289.
32
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
HLT-NAACL 2006.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of HLT-NAACL
2007, pages 404?411. Association for Computational
Linguistics.
M.J. Pickering and H.P. Branigan. 1999. Syntactic prim-
ing in language production. Trends in Cognitive Sci-
ences, 3(4):136?141.
D. Reitter. 2008. Context Effects in Language Produc-
tion: Models of Syntactic Priming in Dialogue Cor-
pora. Ph.D. thesis, University of Edinburgh.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL,
pages 213?220.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Fast full
parsing by linear-chain conditional random fields. In
Proceedings of the 12th EACL, pages 790?798. Asso-
ciation for Computational Linguistics.
33
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775?786,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Unsupervised Sentence Enhancement for Automatic Summarization
Jackie Chi Kit Cheung
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
jcheung@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
We present sentence enhancement as a
novel technique for text-to-text genera-
tion in abstractive summarization. Com-
pared to extraction or previous approaches
to sentence fusion, sentence enhancement
increases the range of possible summary
sentences by allowing the combination of
dependency subtrees from any sentence
from the source text. Our experiments in-
dicate that our approach yields summary
sentences that are competitive with a sen-
tence fusion baseline in terms of con-
tent quality, but better in terms of gram-
maticality, and that the benefit of sen-
tence enhancement relies crucially on an
event coreference resolution algorithm us-
ing distributional semantics. We also
consider how text-to-text generation ap-
proaches to summarization can be ex-
tended beyond the source text by exam-
ining how human summary writers incor-
porate source-text-external elements into
their summary sentences.
1 Introduction
Sentence fusion is the technique of merging sev-
eral input sentences into one output sentence
while retaining the important content (Barzilay
and McKeown, 2005; Filippova and Strube, 2008;
Thadani and McKeown, 2013). For example, the
input sentences in Figure 1 may be fused into one
output sentence.
As a text-to-text generation technique, sentence
fusion is attractive because it provides an avenue
for moving beyond sentence extraction in auto-
matic summarization, while not requiring deep se-
Input: Bil Mar Foods Co., a meat processor
owned by Sara Lee, announced a recall of
certain lots of hot dogs and packaged meat.
Input: The outbreak led to the recall on Tues-
day of 15 million pounds of hot dogs and cold
cuts produced at the Bil Mar Foods plant.
Output: The outbreak led to the recall on Tues-
day of lots of hot dogs and packaged meats
produced at the Bil Mar Foods plant.
Figure 1: An example of fusing two input sen-
tences into an output sentence. The sections of the
input sentences that are retained in the output are
shown in bold.
mantic analysis beyond, say, a dependency parser
and lexical semantic resources.
The overall trajectory pursued in the field can
be characterized as a move away from local con-
texts relying heavily on the original source text to-
wards more global contexts involving reformula-
tion of the text. Whereas sentence extraction and
sentence compression (Knight and Marcu, 2000,
for example) involve taking one sentence and per-
haps removing parts of it, traditional sentence fu-
sion involves reformulating a small number of rel-
atively similar sentences in order to take the union
or intersection of the information present therein.
In this paper, we move further along this path
in the following ways. First, we present sen-
tence enhancement as a novel technique which
extends sentence fusion by combining the subtrees
of many sentences into the output sentence, rather
than just a few. Doing so allows relevant informa-
tion from sentences that are not similar to the orig-
inal input sentences to be added during fusion. As
775
Source text: This fact has been underscored in
the last few months by two unexpected out-
breaks of food-borne illness.
Output: The outbreak of food-borne illness led
to the recall on Tuesday of lots of hot dogs
and meats produced at the Bil Mar Foods
plant.
Figure 2: An example of sentence enhancement,
in which parts of dissimilar sentences are incorpo-
rated into the output sentence.
shown in Figure 2, the phrase of food-borne illness
can be added to the previous output sentence, de-
spite originating in a source text sentence that is
quite different overall.
Elsner and Santhanam (2011) proposed a super-
vised method to fuse disparate sentences, which
takes as input a small number of sentences with
compatible information that have been manually
identified by editors of articles. By contrast, our
algorithm is unsupervised, and tackles the prob-
lem of identifying compatible event mergers in the
entire source text using an event coreference mod-
ule. Our method outperforms a previous syntax-
based sentence fusion baseline on measures of
summary content quality and grammaticality.
Second, we analyze how text-to-text genera-
tion systems may make use of text that is not in
the source text itself, but in articles on a related
topic in the same domain. By examining the parts
of human-written summaries that are not found
in the source text, we find that using in-domain
text allows summary writers to more precisely ex-
press some target semantic content, but that more
sophisticated computational semantic techniques
will be required to enable automatic systems to
likewise do so.
A more general argument of this paper is that
the apparent dichotomy between text-to-text gen-
eration and semantics-to-text generation can be
resolved by viewing them simply as having dif-
ferent starting points towards the same end goal
of precise and wide-coverage NLG. The statisti-
cal generation techniques developed by the text-
to-text generation community have been success-
ful in many domains. Yet the results of our ex-
periments and studies demonstrate the following:
as text-to-text generation techniques move beyond
using local contexts towards more dramatic refor-
mulations of the kind that human writers perform,
more semantic analysis will be needed in order to
ensure that the reformulations preserve the infer-
ences that can be drawn from the input text.
2 Related Work
A relatively large body of work exists in sentence
compression (Knight and Marcu, 2000; McDon-
ald, 2006; Galley and McKeown, 2007; Cohn
and Lapata, 2008; Clarke and Lapata, 2008, in-
ter alia), and sentence fusion (Barzilay and McK-
eown, 2005; Marsi and Krahmer, 2005; Filippova
and Strube, 2008; Filippova, 2010; Thadani and
McKeown, 2013). Unlike this work, our sentence
enhancement algorithm considers the entire source
text and is not limited to the initial input sentences.
Few previous papers focus on combining the con-
tent of diverse sentences into one output sentence.
Wan et al. (2008) propose sentence augmentation
by identifying ?seed? words in a single original
sentence, then adding information from auxiliary
sentences based on word co-occurrence counts.
Elsner and Santhanam (2011) investigate the idea
of fusing disparate sentences with a supervised al-
gorithm, as discussed above.
Previous studies on cut-and-paste summariza-
tion (Jing and McKeown, 2000; Saggion and La-
palme, 2002) investigate the operations that hu-
man summarizers perform on the source text in
order to produce the summary text. Our previ-
ous work argued that current extractive systems
rely too heavily on notions of information central-
ity (Cheung and Penn, 2013). This paper extends
this work by identifying specific linguistic factors
correlated with the use of source-text-external ele-
ments.
3 A Sentence Enhancement Algorithm
The basic steps in our sentence expansion algo-
rithm are as follows: (1) clustering to identify ini-
tial input sentences, (2) sentence graph creation,
(3) sentence graph expansion, (4) tree generation,
and (5) linearization.
At a high level, our method for sentence en-
hancement is inspired by the syntactic sentence
fusion approach of Filippova and Strube (2008)
(henceforth, F&S) originally developed for Ger-
man, in that it operates over the dependency parses
of a small number of input sentences to produce
an output sentence which fuses parts of the in-
776
put sentences. We adopt the same assumption as
F&S that these initial core sentences have a high
degree of similarity with each other, and should
form the core of a new sentence to be generated
(Step 1). While fusion from highly disparate in-
put sentences is possible, Elsner and Santhanam
(2011) showed how difficult it is to do so cor-
rectly, even where such cases are manually iden-
tified. We thus aim for a more targeted type of
fusion initially. Next, the dependency trees of
the core sentences are fused into an intermediate
sentence graph (Step 2), a directed acyclic graph
from which the final sentence will be generated
(Steps 4 and 5). We will compare against our im-
plementation of F&S, adapted to English.
However, unlike F&S or other previous ap-
proaches to sentence fusion, the sentence enhance-
ment algorithm may also avail itself of the de-
pendency parses of all of the other sentences in
the source text, which expands the range of pos-
sible sentences that may be produced. This is ac-
complished by expanding the sentence graph with
parts of these sentences (Step 3). One important
issue here is that the expansion must be modulated
by an event coreference component to ensure that
the merging of information from different points
in the source text is valid and does not result in
incorrect or nonsensical inferences.
3.1 Core sentence identification
To generate the core sentence clusters, we first
identify clusters of similar sentences, then rank the
clusters according to their salience. The top clus-
ter in the source text is then selected to be the input
to the sentence fusion algorithms.
Sentence alignment is performed by complete-
link agglomerative clustering, which requires a
measure of similarity between sentences and a
stopping criterion. We define the similarity be-
tween two sentences to be the standard cosine
similarity between the lemmata of the sentences,
weighted by IDF and excluding stopwords, and
clustering is run until a similarity threshold of
0.5 is reached. Since complete-link clustering
prefers small coherent clusters and we select the
top-scoring cluster in each document collection,
the method is somewhat robust to different choices
of the stopping threshold.
The clusters are scored according to the signa-
ture term method of Lin and Hovy (2000), which
assigns an importance score to each term accord-
BMFoods    announce    recall    certain lots...
outbreak    led    recall    Tuesday    15M pounds...
nsubj dobj
nsubj dobj
prep_of
prep_of
prep_on
(a) Abbreviated dependency trees.
BMFoods    announce                 certain lots...
outbreak    led                 Tuesday    15M pounds...
nsubj dobj
nsubj
dobj
prep_of
prep_of
prep_onrecall
food-borne illness
prep_of
(b) Sentence graph after merging the nodes with lemma recall
(in bold), and expanding the node outbreak (dashed outgoing
edge).
Figure 3: An example of the input dependency
trees for sentence graph creation and expansion,
using the input sentences of Figure 1.
ing to how much more often it appears in the
source text compared to some irrelevant back-
ground text using a log likelihood ratio. Specifi-
cally, the score of a cluster is equal to the sum of
the importance scores of the set of lemmata in the
cluster.
3.2 Sentence graph creation
After core sentence identification, the next step
is to align the nodes of the dependency trees of
the core input sentences in order to create the ini-
tial sentence graph. The input to this step is the
collapsed dependency tree representations of the
core sentences produced by the Stanford parser
1
.
In this representation, preposition nodes are col-
lapsed into the label of the dependency edge be-
tween the functor of the prepositional phrase and
the prepositional object. Chains of conjuncts are
also split, and each argument is attached to the
parent. In addition, auxiliary verbs, negation par-
ticles, and noun-phrase-internal elements
2
are col-
lapsed into their parent nodes. Figure 3a shows
the abbreviated dependency representations of the
input sentences from Figure 1.
Then, a sentence graph is created by merging
nodes that share a common lemma and part-of-
1
As part of the CoreNLP suite: http://nlp.
stanford.edu/software/corenlp.shtml
2
As indicated by the dependency edge label nn.
777
speech tag. In addition, we allow synonyms to
be merged, defined as being in the same Word-
Net synset. Merging is blocked if the word is a
stop word, which includes function words as well
as a number of very common verbs (e.g., be, have,
do). Throughout the sentence graph creation and
expansion process, the algorithm disallows the ad-
dition of edges that would result in a cycle in the
graph.
3.3 Sentence graph expansion
The initial sentence graph is expanded by merg-
ing in subtrees from dependency parses of non-
core sentences drawn from the source text. First,
expansion candidates are identified for each node
in the sentence graph by finding all of the depen-
dency edges in the source text from non-core sen-
tences in which the governor of the edge shares
the same lemma and POS tag as the node in the
sentence graph.
Then, these candidate edges are pruned accord-
ing to two heuristics. The first is to keep only one
candidate edge of each dependency relation type
according to the edge that has the highest informa-
tiveness score (Section 3.4.1), with ties being bro-
ken according to which edge has a subtree with a
fewer number of nodes. The second is to perform
event coreference in order to prune away those
candidate edges which are unlikely to be describ-
ing the same event as the core sentences, as ex-
plained in the next section. Finally, any remaining
candidate edges are fused into the sentence graph,
and the subtree rooted at the dependent of the can-
didate edge is added to the sentence graph as well.
See Figure 3b for an example of sentence graph
creation and expansion.
3.3.1 Event coreference
One problem of sentence fusion is that the differ-
ent inputs of the fusion may not refer to the same
event, resulting in an incorrect merging of infor-
mation, as would be the case in the following ex-
ample:
S1: Officers pled not guilty but risked 25 years to
life.
S2: Officers recklessly engaged in conduct which
seriously risked the lives of others.
Here, the first usage of risk refers to the potential
sentence imposed if the officers are convicted in
a trial, whereas the second refers to the potential
harm caused by the officer.
Context 1: Officers ... risked 25 years to life...
(nsubj, officers)   (dobj, life)
(nsubj, conduct)   (advmod, seriously)   (dobj, life)
sim1((risk, dobj), (risk, dobj))
    ? sim2(life, life) = 1.0
sim1((risk, nsubj), (risk, nsubj))
  ? sim2(officer, conduct) = 0.38
Context 2: ...conduct seriously risked the lives...
Figure 4: Event coreference resolution as a
maximum-weight bipartite graph matching prob-
lem. All the nodes share the predicate risk.
In order to ensure that sentence enhancement
does not lead to the merging of such incompati-
ble events, we designed a simple method to ap-
proximate event coreference resolution that does
not require event coreference labels. This method
is based on the intuition that different mentions of
an event should contain many of the same partic-
ipants. Thus, by measuring the similarity of the
arguments and the syntactic contexts between the
node in the sentence graph and the candidate edge,
we can have a measure of the likelihood that they
refer to the same event.
We would be interested in integrating existing
event coreference resolution systems into this step
in the future, such as the unsupervised method
of Bejan and Harabagiu (2010). Existing event
coreference systems tend to focus on cases with
different heads (e.g., X kicked Y, then Y was in-
jured), which could increase the possibilities for
sentence enhancement, if the event coreference
module is sufficiently accurate. However, since
our method currently only merges identical heads,
we require a more fine-grained method based on
distributional measures of similarity.
We measure the similarity of these syntactic
contexts by aligning the arguments in the syn-
tactic contexts and computing the similarity of
the aligned arguments. These problems can be
jointly solved as a maximum-weight bipartite
graph matching problem (Figure 4). Formally, let
a syntactic context be a list of dependency triples
(h, r, a), consisting of a governor or head node h
and a dependent argument a in the dependency re-
lation r, where head node h is fixed across each
778
element of the list. Then, each of the two in-
put syntactic contexts forms one of the two dis-
joint sets in a complete weighted bipartite graph
where each node corresponds to one dependency
triple. We define the edge weights according to
the similarities of the edge?s incident nodes; i.e.,
between two dependency triples (h
1
, r
1
, a
1
) and
(h
2
, r
2
, a
2
). We also decompose the similarity
into the similarities between the head and relation
types ((h
1
, r
1
) and (h
2
, r
2
)), and between the ar-
guments (a
1
and a
2
). The edge weight function is
thus:
sim((h
1
, r
1
, a
1
), (h
2
, r
2
, a
2
)) = (1)
sim
1
((h
1
, r
1
), (h
2
, r
2
))? sim
2
(a
1
, a
2
),
where sim
1
and sim
2
are binary functions that rep-
resent the similarities between governor-relation
pairs and dependents, respectively. We train mod-
els of distributional semantics using a large back-
ground corpus; namely, the Annotated Gigaword
corpus (Napoles et al., 2012). For sim
1
, we cre-
ate a vector of counts of the arguments that are
seen filling each (h, r) pair, and define the similar-
ity between two such pairs to be the cosine simi-
larity between their argument vectors. For sim
2
,
we create a basic vector-space representation of
a word d according to words that are found in
the context of word d within a five-word context
window, and likewise compute the cosine simi-
larity between the word vectors. These methods
of computing distributional similarity are well at-
tested in lexical semantics for measuring the re-
latedness of words and syntactic structures (Tur-
ney and Pantel, 2010), and similar methods have
been applied in text-to-text generation by Ganitke-
vitch et al. (2012), though the focus of that work is
to use paraphrase information thus learned to im-
prove sentence compression.
The resulting graph matching problem is solved
using the NetworkX package for Python
3
. The fi-
nal similarity score is an average of the similarity
scores from Equation 1 that participate in the se-
lected matching, weighted by the product of the
IDF scores of the dependent nodes of each edge.
This final score is used as a threshold that candi-
date contexts from the source text must meet in
order to be eligible for being merged into the sen-
tence graph. This threshold was tuned by cross-
validation, and can remain constant, although re-
3
http://networkx.github.io/
tuning to different domains (a weakly supervised
alternative) is likely to be beneficial.
3.4 Tree generation
The next major step of the algorithm is to extract
an output dependency tree from the expanded sen-
tence graph. We formulate this as an integer linear
program, in which variables correspond to edges
of the sentence graph, and a solution to the linear
program determines the structure of an output de-
pendency tree. We use ILOG CPLEX to solve all
of the integer linear programs in our experiments.
A good dependency tree must at once express
the salient or important information present in the
input text as well as be grammatically correct and
of a manageable length. These desiderata are en-
coded into the linear program as constraints or as
part of the objective function.
3.4.1 Objective function
We designed an objective function that considers
the importance of the words and syntactic rela-
tions that are selected as well as accounts for re-
dundancy in the output sentence. Let X be the set
of variables in the program, and let each variable
in X take the form x
h,r,a
, a binary variable that
represents whether an edge in the sentence graph
from a head node with lemma h to an argument
with lemma a in relation r is selected. For a lexi-
con ?, our objective function is:
max
?
w??
max
x
h,r,a
?Xs.t.a=w
(x
h,r,w
? P (r|h) ? I(w)),
(2)
where P (r|h) is the probability that head h
projects the dependency relation r, and I(w) is
the informativeness score for word w as defined
by Clarke and Lapata (2008). This formulation
encourages the selection of words that are infor-
mative according to I(w) and syntactic relations
that are probable. The inner max function for each
w in the lexicon encourages non-redundancy, as
each word may only contribute once to the objec-
tive value. This function can be rewritten into a
form compatible with a standard linear program by
the addition of auxiliary variables and constraints.
For more details of how this and other aspects of
the linear program are implemented, see the sup-
plementary document.
3.4.2 Constraints
Well-formedness constraints, taken directly from
F&S, ensure that the set of selected edges pro-
779
duces a tree. Another constraint limits the number
of content nodes in the tree to 11, which corre-
sponds to the average number of content nodes in
human-written summary sentences in the data set.
Syntactic constraints aim to ensure grammatical-
ity of the output sentence. In addition to the con-
straint proposed by F&S regarding subordinating
conjunctions, we propose two other ones. The first
ensures that a nominal or adjectival predicate must
be selected with a copular construction at the top
level of a non-finite clause. The second ensures
that transitive verbs retain both of their comple-
ments in the output
4
. Semantic constraints ensure
that only noun phrases of sufficiently high simi-
larity which are not in a hyperonym-hyponym or
holonym-meronym relation with each other may
be joined by coordination.
3.5 Linearization
The final step of our method is to linearize the de-
pendency tree from the previous step into the final
sequence of words. We implemented our own lin-
earization method to take advantage of the order-
ing information can be inferred from the original
source text sentences.
Our linearization algorithm proceeds top-down
from the root of the dependency tree to the leaves.
At each node of the tree, linearization consists of
realizing the previously collapsed elements such
as prepositions, determiners and noun compound
elements, then ordering the dependent nodes with
respect to the root node and each other. Restoring
the collapsed elements is accomplished by simple
heuristics. For example, prepositions and deter-
miners precede their accompanying noun phrase.
The dependent nodes are ordered by a sort-
ing algorithm, where the order between two syn-
tactic relations and dependent nodes (r
1
, a
1
) and
(r
2
, a
2
) is determined as follows. First, if a
1
and
a
2
originated from the same source text sentence,
then they are ordered according to their order of
appearance in the source text. Otherwise, we con-
sider the probability P (r
1
precedes r
2
), and order
a
1
before a
2
iff P (r
1
precedes r
2
) > 0.5. This
distribution, P (r
1
precedes r
2
), is estimated by
counting and normalizing the order of the relation
types in the source text corpus. For the purposes
of ordering, the governor node is treated as if it
4
We did not experiment with changing the grammatical
voice in the output tree, such as introducing a passive con-
struction if only a direct object is selected, but this is one
possible extension of the algorithm.
were a dependent node with a special syntactic re-
lation label self. This algorithm always produces
an output ordering with a projective dependency
tree, which is a reasonable assumption for English.
4 Experiments
4.1 Method
Recent approaches to sentence fusion have of-
ten been evaluated as isolated components. For
example, F&S evaluate the output sentences by
asking human judges to rate the sentences? in-
formativeness and grammaticality according to a
1?5 Likert scale rating. Thadani and McKe-
own (2013) combine grammaticality ratings with
an automatic evaluation which compares the sys-
tem output against gold-standard sentences drawn
from summarization data sets. However, this eval-
uation setting still does not reflect the utility of
sentence fusion in summarization, because the
input sentences come from human-written sum-
maries rather than the original source text.
We adopt a more realistic setting of using sen-
tence fusion in automatic summarization by draw-
ing the input or core sentences automatically from
the source text, then evaluating the output of the
fusion and expansion algorithm directly as one-
sentence summaries according to standard sum-
marization evaluation measures of content quality.
Data preparation. Our experiments are con-
ducted on the TAC 2010 and TAC 2011 Guided
Summarization corpus (Owczarzak and Dang,
2010), on the initial summarization task. Each
document cluster is summarized by one sentence,
generated from an initial cluster of core sentences
as described in Section 3.1.
Evaluation measures. We evaluate summary
content quality using the word-overlap measures
ROUGE-1 and ROUGE-2, as is standard in the
summarization community. We also measure the
quality of sentences at a syntactic or shallow se-
mantic level that operates at the level of depen-
dency triples by a measure that we call Pyra-
mid BE. Specifically, we extract all of the depen-
dency triples of the form t = (h, r, a) from the
sentence under evaluation and the gold-standard
summaries, where h and a are the lemmata of
the head and the argument, and r is the syntac-
tic relation, normalized for grammatical voice and
excluding the collapsed edges which are mostly
noun-phrase-internal elements and grammatical
780
Method Pyramid BE ROUGE-1 ROUGE-2 Log Likelihood Oracle Pyramid BE
Fusion (F&S) 10.61 10.07 2.15 -159.31 28.00
Expansion 8.82 9.41 1.82 -157.46 52.97
+Event coref 11.00 9.76 1.93 -156.20 40.30
Table 1: Results of the sentence enhancement and fusion experiments.
particles. Then, we perform a matching between
the set of triples in the sentence under evalua-
tion and in a reference summary following the
Transformed BE method of Tratz and Hovy (2008)
with the total weighting scheme. This match-
ing is performed between the sentence and ev-
ery gold-standard summary, and the maximum of
these scores is taken. This score is then divided
by the maximum score that is achievable using the
number of triples present in the input sentence, as
inspired by the Pyramid method. This denom-
inator is more appropriate than the one used in
Transformed BE, which is designed for the case
where the evaluated summary and the reference
summaries are of comparable length.
For grammaticality, we parse the output sen-
tences using the Stanford parser
5
, and use the log
likelihood of the most likely parse of the sentence
as a coarse estimate of grammaticality. Parse log
likelihoods have been shown to be useful in deter-
mining grammaticality (Wagner et al., 2009), and
many of the problems associated with using it do
not apply in our evaluation, because our sentences
have a fixed number of content nodes, and contain
similar content. While we could have conducted
a user study to elicit Likert-scale grammaticality
judgements, such results are difficult to interpret
and the scores depend heavily on the set of judges
and the precise evaluation setting, as is the case for
sentence compression (Napoles et al., 2011).
4.2 Results and discussion
As shown in Table 1, sentence enhancement with
coreference outperforms the sentence fusion algo-
rithm of F&S in terms of the Pyramid BE measure
and the baseline expansion algorithm, though only
the latter difference is statistically significant (p =
0.019
6
). In terms of the ROUGE word overlap
5
The likelihoods are obtained by the PCFG model of
CoreNLP version 1.3.2. We experimented with the Berke-
ley parser (Petrov et al., 2006) as well, with similar results
that favour the sentence enhancement with event coreference
method, but because the parser failed to parse a number of
cases, we do not report those results here.
6
All statistical significance results in this section are for
Wilcoxon signed-rank tests.
measures, fusion achieves a better performance,
but it only outperforms the expansion baseline
significantly (ROUGE-1: p = 0.021, ROUGE-
2: p = 0.012). Note that the ROUGE scores
are low because they involve comparing a one-
sentence summary against a paragraph-long gold
standard. The average log likelihood result sug-
gests that sentence enhancement with event coref-
erence produces sentences that are more grammat-
ical than traditional fusion does, and this differ-
ence is statistically significant (p = 0.044). These
results show that sentence enhancement with event
coreference is competitive with a strong previous
sentence fusion method in terms of content, de-
spite having to combine information from more
diverse sentences. This does not come at the ex-
pense of grammaticality; in fact, it seems that hav-
ing a greater possible range of output sentences
may even improve the grammaticality of the out-
put sentences.
Oracle score. To examine the potential of sen-
tence enhancement, we computed an oracle score
that provides an upper bound to the best possi-
ble sentence that may be extracted from the sen-
tence graph. First, we ranked all of dependency
triples found in each gold-standard summary by
their score (i.e., the number of gold-standard sum-
maries they appear in). Then, we took the high-
est scoring triples from this ranking that are found
in the sentence graph until the length limit was
reached, and divided by the Pyramid-based de-
nominator as above
7
. The oracle score is the max-
imum of these scores over the gold-standard sum-
maries. The resulting oracle scores are shown
in the rightmost column of Table 1. While it
is no surprise that the oracle score improves af-
ter the sentence graph is expanded, the large in-
crease in the oracle score indicates the potential of
sentence enhancement for generating high-quality
summary sentences.
7
There is no guarantee that these dependency triples form
a tree structure. Hence, this is an upper bound.
781
Grammaticality. There is still room for im-
provement in the grammaticality of the generated
sentences, which will require modelling contexts
larger than individual predicates and their argu-
ments. Consider the following output of the sen-
tence enhancement with event coreference system:
(3) The government has launched an
investigation into Soeharto?s wealth by the
Attorney General?s office on the wealth of
former government officials.
This sentence suffers from coherence problems
because two pieces of information are duplicated.
The first is the subject of the investigation, which
is expressed by two prepositional objects of in-
vestigation with the prepositions into and on.
The second, more subtle incoherence concerns
the body that is responsible for the investigation,
which is expressed both by the subject of launch
(The government has launched an investigation),
and the by-prepositional object of investigation (an
investigation ... by the Attorney General?s office).
Clearly, a model that makes fewer independence
assumptions about the relation between different
edges in the sentence graph is needed.
5 A Study of Source-External Elements
The sentence enhancement algorithm presented
above demonstrates that it is possible to use the
entire source text to produce an informative sen-
tence. Yet it is still limited by the particular pred-
icates and dependency relations that are found
in the source. The next step towards develop-
ing abstractive systems that exhibit human-like be-
haviour is to try to incorporate elements into the
summary that are not found in the source text at
all.
Despite its apparent difficulty, there is reason to
be hopeful for text-to-text generation techniques
even in such a scenario. In particular, we showed
in earlier work that almost all of the caseframes,
or pairs of governors and relations, in human-
written summaries can be found in the source text
or in a small set of additional related articles that
belong to the same domain as the source text (e.g.,
natural disasters) (Cheung and Penn, 2013). What
that study lacks, however, is a detailed analysis
of the factors surrounding why human summary
writers use non-source-text elements in their sum-
maries, and how these may be automatically iden-
tified in the in-domain text. In this section, we
supply such an analysis and provide evidence that
human summary writers actually do incorporate
elements external to the source text for a reason,
namely, that these elements are more specific to
the semantic content that they wish to convey. We
also identify a number of features that may be use-
ful for automatically determining the appropriate-
ness of these in-domain elements in a summary.
5.1 Method
We performed our analysis on the predicates
present in text, such as kill and computer. We also
analyzed predicate-relation pairs (PR pairs) such
as (kill, nsubj) or (computer, amod). This choice
is similar to the caseframes used by Cheung and
Penn (2013), and we similarly apply transforma-
tions to normalize for grammatical voice and other
syntactic alternations, but we consider PR pairs of
all relation types, unlike caseframes, which only
consider verb complements and prepositional ob-
jects. PR pairs are extracted from the prepro-
cessed corpus. We use the TAC 2010 Guided
Summarization data set for our analyses, which
we organize into two sub-studies. In the prove-
nance study, we divide the PR pairs in human-
written summaries according to whether they are
found in the source text (source-internal) or not
(source-external). In the domain study, we divide
in-domain but source-external predicate-relation
pairs according to whether they are used in a
human-written summary (gold-standard) or not
(non-gold-standard).
5.2 Provenance Study
In the first study, we compare the characteristics
of gold-standard predicates and PR pairs accord-
ing to their provenance; that is, are they found in
the source text itself? The question that we try to
answer is why human summarizers need to look
beyond the source text at all when writing their
summaries. We will provide evidence that they do
so because they can find predicates that are more
appropriate to the content that is being expressed
according to two quantitative measures.
Predicate provenance. Source-external PR
pairs may be external to the source text for two
reasons. Either the predicate (i.e., the actual word)
is found in the source text, but the dependency
relation (i.e., the semantic predication that holds
between the predicate and its arguments) is
not found with that particular predicate, or the
782
Average freq (millions)
Source-internal 1.77 (1.57, 2.08)
Source-external 1.15 (0.99, 1.50)
(a) The average predicate frequency of source-internal vs.
source-external gold-standard predicates in an external corpus.
Arg entropy
Source-internal 7.94 (7.90, 7.97)
Source-external 7.42 (7.37, 7.48)
(b) The average argument entropy of source-internal vs. source-
external PR pairs in bits.
Table 2: Results of the provenance study. 95%
confidence intervals are estimated by the bootstrap
method and indicated in parentheses.
predicate itself may be external to the source text
altogether. If the former is true, then a generalized
version of the sentence enhancement algorithm
presented in this paper could in principle capture
these PR-pairs. We thus compute the proportion
of source-external PR pairs where the predicate
already exists in the source text.
We find that 2413 of the 4745 source-external
PR pairs, or 51% have a predicate that can be
found in the source text. This indicates that an
extension of the sentence enhancement with event
coreference approach presented in this paper could
capture a substantial portion of the source-external
PR pairs in its hypothesis space already.
Predicate frequency. What factors then can ac-
count for the remaining predicates that are not
found in the source text at all? The first such fac-
tor we identify is the frequency of the predicates.
Here, we take frequency to be the number of oc-
currences of the predicate in an external corpus;
namely the Annotated Gigaword, which gives us
a proxy for the specificity or informativeness of a
word. In this comparison, we take the set of pred-
icates in human-written summaries, divide them
according to whether they are found in the source
text or not, and then look up their frequency of ap-
pearance in the Annotated Gigaword corpus.
As Table 2a shows, the predicates that are not
found in the source text consist of significantly less
frequent words on average (Wilcoxon rank-sums
test, p < 10
?17
). This suggests that human sum-
mary writers are motivated to use source-external
predicates, because they are able to find a more in-
formative or apposite predicate than the ones that
are available in the source text.
Entropy of argument distribution. Another
measure of the informativeness or appropriateness
of a predicate is to examine the range of arguments
that it tends to take. A more generic word would
be expected to take a wider range of arguments,
whereas a more particular word would take a nar-
rower range of arguments, for example those of
a specific entity type. We formalize this notion
by measuring the entropy of the distribution of ar-
guments that a predicate-relation pair takes as ob-
served in Annotated Gigaword. Given frequency
statistics f(h, r, a) of predicate head h taking an
argument word a in relation r, we define the argu-
ment distribution of predicate-relation pair (h, r)
as:
P (a|h, r) = f(h, r, a)/
?
a
?
f(h, r, a
?
) (4)
We then compute the entropy of P (a|h, r) for the
gold-standard predicate-relation pairs, and com-
pare the average argument entropies of the source-
internal and the source-external subsets.
Table 2b shows the result of this comparison.
Source-external PR pairs exhibit a lower average
argument entropy, taking a narrower range of pos-
sible arguments. Together these two findings indi-
cate that human summary writers look beyond the
source text not just for the sake of diversity or to
avoid copying the source text; they do so because
they can find predicates that more specifically con-
vey some desired semantic content.
5.3 Domain study
The second study examines how to distinguish
those source-external predicates and PR pairs in
in-domain articles that are used in a summary from
those that are not. For this study, we rely on the
topic category divisions in the TAC 2010 data set,
and define the in-domain text to be the documents
that belong to the same topic category as the target
document cluster (but not including the target doc-
ument cluster itself). This study demonstrates the
importance of better semantic understanding for
developing a text-to-text generation system that
uses in-domain text, and identifies potentially use-
ful features for training such a system.
Nearest neighbour similarity. In the event-
coreference step of the sentence enhancement al-
gorithm, we relied on distributional semantics to
783
N NN sim
GS 2202 0.493 (0.486, 0.501)
Non-GS 789K 0.443 (0.442, 0.443)
(a) Average similarity of gold-standard (GS) and
non-gold-standard (non-GS) PR pairs to the near-
est neighbour in the source text.
N Freq. (millions) Fecundity
GS 1568 2.44 (2.05, 2.94) 21.6 (20.8, 22.5)
non-GS 268K 0.85 (0.83, 0.87) 6.43 (6.41, 6.47)
(b) Average frequency and fecundity of GS and non-GS predicates in
an external corpus. The differences are statistically significant (p <
10
?10
).
Table 3: Results of the domain study. 95% confidence intervals are given in parentheses.
measure the similarity of arguments. Here, we
examine how well distributional similarity deter-
mines the appropriateness of a source-external PR
pair in a summary. Specifically, we measure its
similarity to the nearest PR pair in the source text.
To determine the similarity between two PR pairs,
we compute the cosine similarity between their
vector representations. The vector representation
of a PR pair is the concatenation of a context vec-
tor for the predicate itself and a selectional pref-
erences vector for the PR pair; that is, the vector
of counts with elements f(h, r, a) for fixed h and
r. These vectors are trained from the Annotated
Gigaword corpus.
The average nearest-neighbour similarities of
PR pairs are shown in Table 3a. While the dif-
ference between the gold-standard and non-gold-
standard PR pairs is indeed statistically signifi-
cant, the magnitude of the difference is not large.
This illustrates the challenge of mining source-
external text for abstractive summarization, and
demonstrates the need for a more structured or
detailed semantic representation in order to deter-
mine the PR pairs that would be appropriate. In
other words, the kind of simple event coreference
method based solely on distributional semantics
that we used in Section 3.3.1 is unlikely to be suf-
ficient when moving beyond the source text.
Frequency and fecundity. We also explore sev-
eral features that would be relevant to identifying
predicates in in-domain text that are used in the
automatic summary. This is a difficult problem, as
less than 0.6% of such predicates are actually used
in the source text. As a first step, we consider sev-
eral simple measures of the frequency and charac-
teristics of the predicates.
The first measure that we compute is the aver-
age predicate frequency of the gold-standard and
non-gold-standard predicates in an external cor-
pus, as in Section 5.2. A second, related mea-
sure is to compute the number of possible relations
that may occur with a given predicate. We call
this measure the fecundity of a predicate. Both
of these are computed with respect to the external
Annotated Gigaword corpus, as before.
As shown in Table 3b, there is a dramatic dif-
ference in both measures between gold-standard
and non-gold-standard predicates in in-domain ar-
ticles. Gold-standard predicates tend to be more
common words compared to non-gold-standard
ones. This result is not in conflict with the re-
sult in the provenance study that source-external
predicates are less common words. Rather, it is
a reminder that the background frequencies of the
predicates matter, and must be considered together
with the semantic appropriateness of the candidate
word.
6 Conclusions
This paper introduced sentence enhancement as
a method to incorporate information from multi-
ple points in the source text into one output sen-
tence in a fashion that is more flexible than previ-
ous sentence fusion algorithms. Our results show
that sentence enhancement improves the content
and grammaticality of summary sentences com-
pared to previous syntax-based sentence fusion ap-
proaches. Then, we presented studies on the com-
ponents of human-written summaries that are ex-
ternal to the source text. Our analyses suggest that
human summary writers look beyond the source
text to find predicates and relations that more pre-
cisely express some target semantic content, and
that more sophisticated semantic techniques are
needed in order to exploit in-domain articles for
text-to-text generation in summarization.
Acknowledgments
We would like to thank the anonymous reviewers
for valuable suggestions. The first author was sup-
ported by a Facebook PhD Fellowship during the
completion of this research.
784
References
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?
328.
Cosmin A. Bejan and Sanda Harabagiu. 2010. Unsu-
pervised event coreference resolution with rich lin-
guistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1412?1422. Association for Compu-
tational Linguistics.
Jackie Chi Kit Cheung and Gerald Penn. 2013. To-
wards robust abstractive multi-document summa-
rization: A caseframe analysis of centrality and do-
main. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1233?1242, August.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research(JAIR), 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 137?144,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Micha Elsner and Deepak Santhanam. 2011. Learn-
ing to fuse disparate sentences. In Proceedings of
the Workshop on Monolingual Text-To-Text Gener-
ation, pages 54?63. Association for Computational
Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 177?
185, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 322?
330, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Monolingual distributional
similarity for text-to-text generation. In Proceedings
of *SEM 2012: The First Joint Conference on Lex-
ical and Computational Semantics, pages 256?264,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference,
pages 178?185.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization?step one: Sentence compres-
sion. In Proceedings of the National Conference on
Artificial Intelligence, pages 703?710.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In COLING 2000 Volume 1: The 18th In-
ternational Conference on Computational Linguis-
tics, COLING ?00, pages 495?501, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European
Workshop on Natural Language Generation, pages
109?117.
Ryan T. McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97. Association for Com-
putational Linguistics.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated Gigaword. In Pro-
ceedings of the NAACL-HLT Joint Workshop on Au-
tomatic Knowledge Base Construction & Web-scale
Knowledge Extraction (AKBC-WEKEX), pages 95?
100.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC
2010 guided summarization task guidelines.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with SumUM.
Computational Linguistics, 28(4):497?526.
Kapil Thadani and Kathleen McKeown. 2013. Super-
vised sentence fusion with single-stage inference. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 1410?
1418, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Stephen Tratz and Eduard Hovy. 2008. Summariza-
tion evaluation using transformed Basic Elements.
In Proceedings of the First Text Analysis Conference
(TAC).
785
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experi-
ments in sentence classification. CALICO Journal,
26(3):474?490.
Stephen Wan, Robert Dale, Mark Dras, and Cecile
Paris. 2008. Seed and grow: Augmenting statisti-
cally generated summary sentences using schematic
word patterns. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 543?552, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
786
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?43,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Evaluating Distributional Models of Semantics for Syntactically
Invariant Inference
Jackie CK Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
A major focus of current work in distri-
butional models of semantics is to con-
struct phrase representations composition-
ally from word representations. However,
the syntactic contexts which are modelled
are usually severely limited, a fact which
is reflected in the lexical-level WSD-like
evaluation methods used. In this paper, we
broaden the scope of these models to build
sentence-level representations, and argue
that phrase representations are best eval-
uated in terms of the inference decisions
that they support, invariant to the partic-
ular syntactic constructions used to guide
composition. We propose two evaluation
methods in relation classification and QA
which reflect these goals, and apply several
recent compositional distributional models
to the tasks. We find that the models out-
perform a simple lemma overlap baseline
slightly, demonstrating that distributional
approaches can already be useful for tasks
requiring deeper inference.
1 Introduction
A number of unsupervised semantic models
(Mitchell and Lapata, 2008, for example) have re-
cently been proposed which are inspired at least
in part by the distributional hypothesis (Harris,
1954)?that a word?s meaning can be character-
ized by the contexts in which it appears. Such
models represent word meaning as one or more
high-dimensional vectors which capture the lex-
ical and syntactic contexts of the word?s occur-
rences in a training corpus.
Much of the recent work in this area has, fol-
lowing Mitchell and Lapata (2008), focused on
the notion of compositionality as the litmus test of
a truly semantic model. Compositionality is a nat-
ural way to construct representations of linguistic
units larger than a word, and it has a long history
in Montagovian semantics for dealing with argu-
ment structure and assembling rich semantical ex-
pressions of the kind found in predicate logic.
While compositionality may thus provide a
convenient recipe for producing representations
of propositionally typed phrases, it is not a nec-
essary condition for a semantic representation.
Rather, that distinction still belongs to the crucial
ability to support inference. It is not the inten-
tion of this paper to argue for or against composi-
tionality in semantic representations. Rather, our
interest is in evaluating semantic models in order
to determine their suitability for inference tasks.
In particular, we contend that it is desirable and
arguably necessary for a compositional semantic
representation to support inference invariantly, in
the sense that the particular syntactic construction
that guided the composition should not matter rel-
ative to the representations of syntactically differ-
ent phrases with the same meanings. For example,
we can assert that John threw the ball and The ball
was thrown by John have the same meaning for
the purposes of inference, even though they differ
syntactically.
An analogy can be drawn to research in image
processing, in which it is widely regarded as im-
portant for the representations of images to be in-
variant to rotation and scaling. What we should
want is a representation of sentence meaning that
is invariant to diathesis, other regular syntactic al-
ternations in the assignment of argument struc-
ture, and, ideally, even invariant to other meaning-
preserving or near-preserving paraphrases.
33
Existing evaluations of distributional semantic
models fall short of measuring this. One evalua-
tion approach consists of lexical-level word sub-
stitution tasks which primarily evaluate a sys-
tem?s ability to disambiguate word senses within a
controlled syntactic environment (McCarthy and
Navigli, 2009, for example). Another approach is
to evaluate parsing accuracy (Socher et al 2010,
for example), which is really a formalism-specific
approximation to argument structure analysis.
These evaluations may certainly be relevant to
specific components of, for example, machine
translation or natural language generation sys-
tems, but they tell us little about a semantic
model?s ability to support inference.
In this paper, we propose a general framework
for evaluating distributional semantic models that
build sentence representations, and suggest two
evaluation methods that test the notion of struc-
turally invariant inference directly. Both rely on
determining whether sentences express the same
semantic relation between entities, a crucial step
in solving a wide variety of inference tasks like
recognizing textual entailment, information re-
trieval, question answering, and summarization.
The first evaluation is a relation classification
task, where a semantic model is tested on its abil-
ity to recognize whether a pair of sentences both
contain a particular semantic relation, such as
Company X acquires Company Y. The second task
is a question answering task, the goal of which is
to locate the sentence in a document that contains
the answer. Here, the semantic model must match
the question, which expresses a proposition with a
missing argument, to the answer-bearing sentence
which contains the full proposition.
We apply these new evaluation protocols to
several recent distributional models, extending
several of them to build sentence representa-
tions. We find that the models outperform a sim-
ple lemma overlap model only slightly, but that
combining these models with the lemma overlap
model can improve performance. This result is
likely due to weaknesses in current models? abil-
ity to deal with issues such as named entities,
coreference, and negation, which are not empha-
sized by existing evaluation methods, but it does
suggest that distributional models of semantics
can play a more central role in systems that re-
quire deep, precise inference.
2 Compositionality and Distributional
Semantics
The idea of compositionality has been central to
understanding contemporary natural language se-
mantics from an historiographic perspective. The
idea is often credited to Frege, although in fact
Frege had very little to say about compositional-
ity that had not already been repeated since the
time of Aristotle (Hodges, 2005). Our modern
notion of compositionality took shape primarily
with the work of Tarski (1956), who was actu-
ally arguing that a central difference between for-
mal languages and natural languages is that nat-
ural language is not compositional. This in turn
was the ?the contention that an important theo-
retical difference exists between formal and nat-
ural languages,? that Richard Montague so fa-
mously rejected (Montague, 1974). Composi-
tionality also features prominently in Fodor and
Pylyshyn?s (1988) rejection of early connection-
ist representations of natural language semantics,
which seems to have influenced Mitchell and La-
pata (2008) as well.
Logic-based forms of compositional semantics
have long strived for syntactic invariance in mean-
ing representations, which is known as the doc-
trine of the canonical form. The traditional justifi-
cation for canonical forms is that they allow easy
access to a knowledge base to retrieve some de-
sired information, which amounts to a form of in-
ference. Our work can be seen as an extension of
this notion to distributional semantic models with
a more general notion of representational similar-
ity and inference.
There are many regular alternations that seman-
tics models have tried to account for such as pas-
sive or dative alternations. There are also many
lexical paraphrases which can take drastically dif-
ferent syntactic forms. Take the following exam-
ple from Poon and Domingos (2009), in which the
same semantic relation can be expressed by a tran-
sitive verb or an attributive prepositional phrase:
(1) Utah borders Idaho.
Utah is next to Idaho.
In distributional semantics, the original sen-
tence similarity test proposed by Kintsch (2001)
served as the inspiration for the evaluation per-
formed by Mitchell and Lapata (2008) and most
later work in the area. Intransitive verbs are given
34
in the context of their syntactic subject, and can-
didate synonyms are ranked for their appropri-
ateness. This method targets the fact that a syn-
onym is appropriate for only some of the verb?s
senses, and the intended verb sense depends on
the surrounding context. For example, burn and
beam are both synonyms of glow, but given a par-
ticular subject, one of the synonyms (called the
High similarity landmark) may be a more appro-
priate substitution than the other (the Low similar-
ity landmark). So, if the fire is the subject, glowed
is the High similarity landmark, and beamed the
Low similarity landmark.
Fundamentally, this method was designed as
a demonstration that compositionality in com-
puting phrasal semantic representations does not
interfere with the ability of a representation to
synthesize non-compositional collocation effects
that contribute to the disambiguation of homo-
graphs. Here, word-sense disambiguation is im-
plicitly viewed as a very restricted, highly lexi-
calized case of inference for selecting the appro-
priate disjunct in the representation of a word?s
meaning.
Kintsch (2001) was interested in sentence sim-
ilarity, but he only conducted his evaluation on
a few hand-selected examples. Mitchell and La-
pata (2008) conducted theirs on a much larger
scale, but chose to focus only on this single case
of syntactic combination, intransitive verbs and
their subjects, in order to ?factor out inessential
degrees of freedom? to compare their various al-
ternative models more equitably. This was not
necessary?using the same, sufficiently large, un-
biased but syntactically heterogeneous sample of
evaluation sentences would have served as an ade-
quate control?and this decision furthermore pre-
vents the evaluation from testing the desired in-
variance of the semantic representation.
Other lexical evaluations suffer from the same
problem. One uses the WordSim-353 dataset
(Finkelstein et al 2002), which contains hu-
man word pair similarity judgments that seman-
tic models should reproduce. However, the word
pairs are given without context, and homography
is unaddressed. Also, it is unclear how reliable
the similarity scores are, as different annotators
may interpret the integer scale of similarity scores
differently. Recent work uses this dataset mostly
for parameter tuning. Another is the lexical para-
phrase task of McCarthy and Navigli (2009), in
which words are given in the context of the sur-
rounding sentence, and the task is to rank a given
list of proposed substitutions for that word. The
list of substitutions as well as the correct rankings
are elicited from annotators. This task was origi-
nally conceived as an applied evaluation of WSD
systems, not an evaluation of phrase representa-
tions.
Parsing accuracy has been used as a prelimi-
nary evaluation of semantic models that produce
syntactic structure (Socher et al 2010; Wu and
Schuler, 2011). However, syntax does not always
reflect semantic content, and we are specifically
interested in supporting syntactic invariance when
doing semantic inference. Also, this type of eval-
uation is tied to a particular grammar formalism.
The existing evaluations that are most similar in
spirit to what we propose are paraphrase detection
tasks that do not assume a restricted syntactic con-
text. Washtell (2011) collected human judgments
on the general meaning similarity of candidate
phrase pairs. Unfortunately, no additional guid-
ance on the definition of ?most similar in mean-
ing? was provided, and it appears likely that sub-
jects conflated lexical, syntactic, and semantic re-
latedness. Dolan and Brockett (2005) define para-
phrase detection as identifying sentences that are
in a bidirectional entailment relation. While such
sentences do support exactly the same inferences,
we are also interested in the inferences that can
be made from similar sentences that are not para-
phrases according to this strict definition ? a sit-
uation that is more often encountered in end ap-
plications. Thus, we adopt a less restricted notion
of paraphrasis.
3 An Evaluation Framework
We now describe a simple, general framework
for evaluating semantic models. Our framework
consists of the following components: a seman-
tic model to be evaluated, pairs of sentences that
are considered to have high similarity, and pairs
of sentences that are considered to have low simi-
larity.
In particular, the semantic model is a binary
function, s = M(x, x?), which returns a real-
valued similarity score, s, given a pair of arbitrary
linguistic units (that is, words, phrases, sentences,
etc.), x and x?. Note that this formulation of the
semantic model is agnostic to whether the models
use compositionality to build a phrase represen-
35
tation from constituent representations, and even
to the actual representation used. The model is
tested by applying it to each element in the fol-
lowing two sets:
H = {(h, h?)|h and h? are linguistic units (2)
with high similarity}
L = {(l, l?)|l and l? are linguistic units (3)
with low similarity}
The resulting sets of similarity scores are:
SH =
{
M(h, h?)|(h, h?) ? H
} (4)
SL =
{
M(l, l?)|(l, l?) ? L
} (5)
The semantic model is evaluated according to
its ability to separate SH and SL. We will de-
fine specific measures of separation for the tasks
that we propose shortly. While the particular def-
initions of ?high similarity? and ?low similarity?
depend on the task, at the crux of both our evalu-
ations is that two sentences are similar if they ex-
press the same semantic relation between a given
entity pair, and dissimilar otherwise. This thresh-
old for similarity is closely tied to the argument
structure of the sentence, and allows considerable
flexibility in the other semantic content that may
be contained in the sentence, unlike the bidirec-
tional paraphrase detection task. Yet it ensures
that a consistent and useful distinction for infer-
ence is being detected, unlike unconstrained sim-
ilarity judgments.
Also, compared to word similarity assessments
or paraphrase elicitation, determining whether a
sentence expresses a semantic relation is a much
easier task cognitively for human judges. This bi-
nary judgment does not involve interpreting a nu-
merical scale or coming up with an open-ended
set of alternative paraphrases. It is thus easier to
get reliable annotated data.
Below, we present two tasks that instantiate
this evaluation framework and choice of similar-
ity threshold. They differ in that the first is tar-
geted towards recognizing declarative sentences
or phrases, while the second is targeted towards a
question answering scenario, where one argument
in the semantic relation is queried.
3.1 Task 1: Relation Classification
The first task is a relation classification task. Rela-
tion extraction and recognition are central to a va-
riety of other tasks, such as information retrieval,
ontology construction, recognizing textual entail-
ment and question answering.
In this task, the high and the low similarity sen-
tence pairs are constructed in the following man-
ner. First, a target semantic relation, such as Com-
pany X acquires Company Y is chosen, and enti-
ties are chosen for each slot in the relation, such as
Company X=Pfizer and Company Y=Rinat Neu-
roscience. Then, sentences containing these enti-
ties are extracted and divided into two subsets. In
one of them, E, the entities are in the target se-
mantic relation, while in the other, NE, they are
not. The evaluation sets H and L are then con-
structed as follows:
H = E ? E \ {(e, e)|e ? E} (6)
L = E ?NE (7)
In other words, the high similarity sentence
pairs are all the pairs where both express the tar-
get semantic relation, except the pairs between a
sentence and itself, while the low similarity pairs
are all the pairs where exactly one of the two sen-
tences expresses the target relation.
Several sentences expressing the relation Pfizer
acquires Rinat Neuroscience are shown in Exam-
ples 8 to 10. These sentences illustrate the amount
of syntactic and lexical variation that the semantic
model must recognize as expressing the same se-
mantic relation. In particular, besides recognizing
synonymy or near-synonymy at the lexical level,
models must also account for subcategorization
differences, extra arguments or adjuncts, and part-
of-speech differences due to nominalization.
(8) Pfizer buys Rinat Neuroscience to extend
neuroscience research and in doing so
acquires a product candidate for OA.
(lexical difference)
(9) A month earlier, Pfizer paid an estimated
several hundred million dollars for biotech
firm Rinat Neuroscience. (extra argument,
subcategorization)
(10) Pfizer to Expand Neuroscience Research
With Acquisition of Biotech Company Rinat
Neuroscience (nominalization)
Since our interest is to measure the models?
ability to separate SH and SL in an unsuper-
vised setting, standard supervised classification
accuracy is not applicable. Instead, we employ
36
the area under a ROC curve (AUC), which does
not depend on choosing an arbitrary classification
threshold. A ROC curve is a plot of the true pos-
itive versus false positive rate of a binary classi-
fier as the classification threshold is varied. The
area under a ROC curve can thus be seen as the
performance of linear classifiers over the scores
produced by the semantic model. The AUC can
also be interpreted as the probability that a ran-
domly chosen positive instance will have a higher
similarity score than a randomly chosen negative
instance. A random classifier is expected to have
an AUC of 0.5.
3.2 Task 2: Restricted QA
The second task that we propose is a restricted
form of question answering. In this task, the sys-
tem is given a question q and a document D con-
sisting of a list of sentences, in which one of the
sentences contains the answer to the question. We
define:
H = {(q, d)|d ? D and d answers q} (11)
L = {(q, d)|d ? D and d does not answer q}
(12)
In other words, the sentences are divided into two
subsets; those that contain the answer to q should
be similar to q, while those that do not should be
dissimilar. We also assume that only one sentence
in each document contains the answer, so H con-
tains only one sentence.
Unrestricted question answering is a difficult
problem that forces a semantic representation to
deal sensibly with a number of other semantic is-
sues such as coreference and information aggre-
gation which still seem to be out of reach for
contemporary distributional models of meaning.
Since our focus in this work is on argument struc-
ture semantics, we restrict the question-answer
pairs to those that only require dealing with para-
phrases of this type.
To do so, we semi-automatically restrict the
question-answer pairs by using the output of an
unsupervised clustering semantic parser (Poon
and Domingos, 2009). The semantic parser clus-
ters semantic sub-expressions derived from a de-
pendency parse of the sentence, so that those sub-
expressions that express the same semantic re-
lations are clustered. The parser is used to an-
swer questions, and the output of the parser is
manually checked. We use only those cases that
have thus been determined to be correct question-
answer pairs. As a result of this restriction, this
task is rather more like Task 1 in how it tests a
model?s ability to recognize lexical and syntac-
tic paraphrases. This task also involves recog-
nizing voicing alternations, which were automati-
cally extracted by the semantic parser.
An example of a question-answer pair involv-
ing a voicing alternation that is used in this task is
presented in Example 13.
(13) Q: What does il-2 activate?
A: PI3K
Sentence: Phosphatidyl inositol 3-kinase
(PI3K) is activated by IL-2.
Since there is only one element in H and hence
SH for each question and document, we measure
the separation between SH and SL using the rank
of the score of answer-bearing sentence among
the scores of all the sentences in the document.
We normalize the rank so that it is between 0
(ranked least similar) and 1 (ranked most simi-
lar). Where ties occur, the sentence is ranked as
if it were in the median position among the tied
sentences. If the question-answer pairs are zero-
indexed by i, answer(i) is the index of the sen-
tence containing the answer for the ith pair, and
length(i) is the number of sentences in the doc-
ument, then the mean normalized rank score of a
system is:
norm rank = E
i
[
1? answer(i)length(i) ? 1
]
(14)
4 Experiments
We drew a number of recent distributional seman-
tic models to compare in this paper. We first de-
scribe the models and our reimplementation of
them, before describing the tasks and the datasets
used in detail and the results.
4.1 Distributional Semantic Models
We tested four recent distributional models and a
lemma overlap baseline, which we now describe.
We extended several of the models to compo-
sitionally construct phrase representations using
component-wise vector addition and multiplica-
tion, as we note below. Since the focus of this pa-
per is on evaluation methods for such models, we
did not experiment with other compositionality
37
operators. We do note, however, that component-
wise operators have been popular in recent liter-
ature, and have been applied across unrestricted
syntactic contexts (Mitchell and Lapata, 2009),
so there is value in evaluating the performance of
these operators in itself. The models were trained
on the Gigaword corpus (2nd ed., ~2.3B words).
All models use cosine similarity to measure the
similarity between representations, except for the
baseline model.
Lemma Overlap This baseline simply repre-
sents a sentence as the counts of each lemma
present in the sentence after removing stop
words. Let a sentence x consist of lemma-tokens
m1, . . . ,m|x|. The similarity between two sen-
tences is then defined as
M(x, x?) = #In(x, x?) + #In(x?, x) (15)
#In(x, x?) =
|x|
?
i=1
1x?(mi) (16)
where 1x?(mi) is an indicator function that returns
1 if mi ? x?, and 0 otherwise. This definition
accounts for multiple occurrences of a lemma.
M&L Mitchell and Lapata (2008) propose a
framework for compositional distributional se-
mantics using a standard term-context vector
space word representation. A phrase is repre-
sented as a vector of context-word counts (actu-
ally, pmi-scaled values), which is derived compo-
sitionally by a function over constituent vectors,
such as component-wise addition or multiplica-
tion. This model ignores syntactic relations and
is insensitive to word-order.
E&P Erk and Pado? (2008) introduce a struc-
tured vector space model which uses syntactic de-
pendencies to model the selectional preferences
of words. The vector representation of a word in
context depends on the inverse selectional prefer-
ences of its dependents, and the selectional pref-
erences of its head. For example, suppose catch
occurs with a dependent ball in a direct object
relation. The vector for catch would then be in-
fluenced by the inverse direct object preferences
of ball (e.g. throw, organize), and the vector for
ball would be influenced by the selectional pref-
erences of catch (e.g. cold, drift). More formally,
given words a and b in a dependency relation r,
a distributional representation of a, va, the repre-
sentation of a in context, a?, is given by
a? = va ?Rb(r?1) (17)
Rb(r) =
?
c:f(c,r,b)>?
f(c, r, b) ? vc, (18)
where Rb(r) is the vector describing the selec-
tional preference of word b in relation r, f(c, r, b)
is the frequency of this dependency triple, ? is a
frequency threshold to weed out uncommon de-
pendency triples (10 in our experiments), and ?
is a vector combination operator, here component-
wise multiplication. We extend the model to com-
pute sentence representations from the contextu-
alized word vectors using component-wise addi-
tion and multiplication.
TFP Thater et al(2010)?s model is also sensi-
tive to selectional preferences, but to two degrees.
For example, the vector for catch might contain
a dimension labelled (OBJ,OBJ-1,throw),
which indicates the strength of connection be-
tween the two verbs through all of the co-
occurring direct objects which they share. Unlike
E&P, TFP?s model encodes the selectional prefer-
ences in a single vector using frequency counts.
We extend the model to the sentence level with
component-wise addition and multiplication, and
word vectors are contextualized by the depen-
dency neighbours. We use a frequency threshold
of 10 and a pmi threshold of 2 to prune infrequent
word and dependencies.
D&L Dinu and Lapata (2010) (D&L) assume
a global set of latent senses for all words, and
models each word as a mixture over these latent
senses. The vector for a word ti in the context of
a word cj is modelled by
v(ti, cj) = P (z1|ti, cj), ...P (zK |ti, cj) (19)
where z1...K are the latent senses. By mak-
ing independence assumptions and decomposing
probabilities, training becomes a matter of esti-
mating the probability distributions P (zk|ti) and
P (cj |zk) from data. While Dinu and Lapata
(2010) describe two methods to do so, based
on non-negative matrix factorization and latent
Dirichlet alcation, the performances are similar,
so we tested only the latent Dirichlet alcation
method. Like the two previous models, we ex-
tend the model to build sentence representations
38
Pfizer/Rinat N. Yahoo/Inktomi Besson/Paris Antoinette/Vienna Average
Overlap 0.7393 0.6007 0.7395 0.8914 0.7427
Models trained on the entire GigaWord
M&L add 0.6196 0.5387 0.5259 0.7275 0.6029
M&L mult 0.9036 0.6099 0.6443 0.8467 0.7511
D&L add 0.9214 0.8168 0.6989 0.8932 0.8326
D&L mult 0.7732 0.6734 0.6527 0.7659 0.7163
Models trained on the AFP section
E&P add 0.7536 0.4933 0.2780 0.6408 0.5414
E&P mult 0.5268 0.5328 0.5252 0.8421 0.6067
TFP add 0.4357 0.5325 0.8725 0.7183 0.6398
TFP mult 0.5554 0.5524 0.7283 0.6917 0.6320
M&L add 0.5643 0.5504 0.4594 0.7640 0.5845
M&L mult 0.8679 0.6324 0.4356 0.8258 0.6904
D&L add 0.8143 0.9062 0.6373 0.8664 0.8061
D&L mult 0.8429 0.7461 0.645 0.5948 0.7072
Table 1: Task 1 results in AUC scores. The values in bold indicate the best performing model for a particular
training corpus. The expected random baseline performance is 0.5.
Entities: {X, Y} + N
Relation: acquires
{Pfizer, Rinat Neuroscience} 41 50
{Yahoo, Inktomi} 115 433
Relation: was born in
{Luc Besson, Paris} 6 126
{Marie Antoinette, Vienna} 39 105
Table 2: Task 1 dataset characteristics. N is the total
number of sentences. + is the number of sentences
that express the relation.
from the contextualized representations. We set
the number of latent senses to 1200, and train for
600 Gibbs sampling iterations.
4.2 Training and Parameter Settings
We reimplemented these four models, following
the parameter settings described by previous work
where possible, though we also aimed for consis-
tency in parameter settings between models (for
example, in the number of context words). For the
non-baseline models, we followed previous work
and model only the 30000 most frequent lemmata.
Context vectors are constructed using a symmet-
ric window of 5 words, and their dimensions rep-
resent the 3000 most frequent lemmatized context
words excluding stop words. Due to resource lim-
itations, we trained the syntactic models over the
AFP subset of Gigaword (~338M words). We also
trained the other two models on just the AFP por-
tion for comparison. Note that the AFP portion
of Gigaword is three times larger than the BNC
corpus (~100M words), on which several previ-
ous syntactic models were trained. Because our
main goal is to test the general performance of the
models and to demonstrate the feasibility of our
evaluation methods, we did not further tune the
parameter settings to each of the tasks, as doing
so would likely only yield minor improvements.
4.3 Task 1
We used the dataset by Bunescu and Mooney
(2007), which we selected because it contains
multiple realizations of an entity pair in a target
semantic relation, unlike similar datasets such as
the one by Roth and Yih (2002). Controlling for
the target entity pair in this manner makes the task
more difficult, because the semantic model cannot
make use of distributional information about the
entity pair in inference. The dataset is separated
into subsets depending on the target binary rela-
tion (Company X acquires Company Y or Person
X was born in Place Y) and the entity pair (e.g.,
Yahoo and Inktomi) (Table 2).
The dataset was constructed semi-
automatically using a Google search for the
two entities in order with up to seven content
words in between. Then, the extracted sentences
were hand-labelled with whether they express the
target relation. Because the order of the entities
has been fixed, passive alternations do not appear
39
Pure models Mixed models
All Subset All Subset
Overlap 0.8770 0.7291 0.8770 0.7291
Models trained on the entire GigaWord
M&L add 0.7467 0.6106 0.8782 0.7523
M&L mult 0.5331 0.5690 0.8841 0.7678
D&L add 0.6552 0.5716 0.8791 0.7539
D&L mult 0.5488 0.5255 0.8841 0.7466
Models trained on the AFP section
E&P add 0.4589 0.4516 0.8748 0.7375
E&P mult 0.5201 0.5584 0.8882 0.7719
TFP add 0.6887 0.6443 0.8940 0.7871
TFP mult 0.5210 0.5199 0.8785 0.7432
M&L add 0.7588 0.6206 0.8710 0.7371
M&L mult 0.5710 0.5540 0.8801 0.7540
D&L add 0.6358 0.5402 0.8713 0.7305
D&L mult 0.5647 0.5461 0.8856 0.7683
Table 3: Task 2 results, in normalized rank scores.
Subset is the cases where lemma overlap does not
achieve a perfect score. The two columns on the right
indicate performance using the sum of the scores from
the lemma overlap and the semantic model. The ex-
pected random baseline performance is 0.5.
in this dataset.
The results for Task 1 indicate that the D&L ad-
dition model performs the best (Table 1), though
the lemma overlap model presents a surprisingly
strong baseline. The syntax-modulated E&P and
TFP models perform poorly on this task, even
when compared to the other models trained on the
AFP subset. The M&L multiplication model out-
performs the addition model, a result which cor-
roborates previous findings on the lexical substi-
tution task. The same does not hold in the D&L
latent sense space. Overall, some of the datasets
(Yahoo and Antoinette) appear to be easier for the
models than others (Pfizer and Besson), but more
entity pairs and relations would be needed to in-
vestigate the models? variance across datasets.
4.4 Task 2
We used the question-answer pairs extracted by
the Poon and Domingos (2009) semantic parser
from the GENIA biomedical corpus that have
been manually checked to be correct (295 pairs).
Because our models were trained on newspaper
text, they required adaptation to this specialized
domain. Thus, we also trained the M&L, E&P
and TFP models on the GENIA corpus, back-
ing off to word vectors from the GENIA corpus
when a word vector could not be found in the
Gigaword-trained model. We could not do this
for the D&L model, since the global latent senses
that are found by latent Dirichlet alcation train-
ing do not have any absolute meaning that holds
across multiple runs. Instead, we found the 5
words in the Gigaword-trained D&L model that
were closest to each novel word in the GENIA
corpus according to cosine similarity over the co-
occurrence vectors of the words in the GENIA
corpus, and took their average latent sense distri-
butions as the vector for that word.
Unlike in Task 1, there is no control for the
named entities in a sentence, because one of the
entities in the semantic relation is missing. Also,
distributional models have problems in dealing
with named entities which are common in this
corpus, such as the names of genes and proteins.
To address these issues, we tested hybrid models
where the similarity score from a semantic model
is added to the similarity score from the lemma
overlap model.
The results are presented in Table 3. Lemma
overlap again presents a strong baseline, but the
hybridized models are able to outperform simple
lemma overlap. Unlike in Task 1, the E&P and
TFP models are comparable to the D&L model,
and the mixed TFP addition model achieves the
best result, likely due to the need to more pre-
cisely distinguish syntactic roles in this task. The
D&L addition model, which achieved the best
performance in Task 1, does not perform as well
in this task. This could be due to the domain adap-
tation procedure for the D&L model, which could
not be reasonably trained on such a small, special-
ized corpus.
5 Related Work
Turney and Pantel (2010) survey various types of
vector space models and applications thereof in
computational linguistics. We summarize below
a number of other word- or phrase-level distribu-
tional models.
Several approaches are specialized to deal with
homography. The top-down multi-prototype ap-
proach determines a number of senses for each
word, and then clusters the occurrences of the
word (Reisinger and Mooney, 2010) into these
senses. A prototype vector is created for each
of these sense clusters. When a new occurrence
40
of a word is encountered, it is represented as a
combination of the prototype vectors, with the de-
gree of influence from each prototype determined
by the similarity of the new context to the exist-
ing sense contexts. In contrast, the bottom-up ex-
emplar-based approach assumes that each occur-
rence of a word expresses a different sense of the
word. The most similar senses of the word are ac-
tivated when a new occurrence of it is encountered
and combined, for example with a kNN algorithm
(Erk and Pado?, 2010).
The models we compared and the above work
assume each dimension in the feature vector cor-
responds to a context word. In contrast, Washtell
(2011) uses potential paraphrases directly as di-
mensions in his expectation vectors. Unfortu-
nately, this approach does not outperform vari-
ous context word-based approaches in two phrase
similarity tasks.
In terms of the vector composition function,
component-wise addition and multiplication are
the most popular in recent work, but there ex-
ist a number of other operators such as tensor
product and convolution product, which are re-
viewed by Widdows (2008). Instead of vector
space representations, one could also use a matrix
space representation with its much more expres-
sive matrix operators (Rudolph and Giesbrecht,
2010). So far, however, this has only been ap-
plied to specific syntactic contexts (Baroni and
Zamparelli, 2010; Guevara, 2010; Grefenstette
and Sadrzadeh, 2011), or tasks (Yessenalina and
Cardie, 2011).
Neural networks have been used to learn both
phrase structure and representations. In Socher et
al. (2010), word representations learned by neu-
ral network models such as (Bengio et al 2006;
Collobert and Weston, 2008) are fed as input into
a recursive neural network whose nodes represent
syntactic constituents. Each node models both the
probability of the input forming a constituent and
the phrase representation resulting from composi-
tion.
6 Conclusions
We have proposed an evaluation framework for
distributional models of semantics which build
phrase- and sentence-level representations, and
instantiated two evaluation tasks which test for
the crucial ability to recognize whether sen-
tences express the same semantic relation. Our
results demonstrate that compositional distribu-
tional models of semantics already have some
utility in the context of more empirically complex
semantic tasks than WSD-like lexical substitution
tasks, in which compositional invariance is a req-
uisite property. Simply computing lemma over-
lap, however, is a very competitive baseline, due
to issues in these protocols with named entities
and domain adaptivity. The better performance
of the mixture models in Task 2 shows that such
weaknesses can be addressed by hybrid seman-
tic models. Future work should investigate more
refined versions of such hybridization, as well as
extend this idea to other semantic phenomena like
coreference, negation and modality.
We also observe that no single model or com-
position operator performs best for all tasks and
datasets. The latent sense mixture model of Dinu
and Lapata (2010) performs well in recognizing
semantic relations in general web text. Because
of the difficulty of adapting it to a specialized
domain, however, it does less well in biomedi-
cal question answering, where the syntax-based
model of Thater et al(2010) performs the best.
A more thorough investigation of the factors that
can predict the performance and/or invariance of
a given composition operator is warranted.
In the future, we would like to evaluate other
models of compositional semantics that have been
recently proposed. We would also like to collect
more comprehensive test data, to increase the ex-
ternal validity of our evaluations.
Acknowledgments
We would like to thank Georgiana Dinu and Ste-
fan Thater for help with reimplementing their
models. Saif Mohammad, Peter Turney, and
the anonymous reviewers provided valuable com-
ments on drafts of this paper. This project was
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193.
Yoshua Bengio, Holger Schwenk, Jean-Se?bastien
Sene?cal, Fre?deric Morin, and Jean-Luc Gauvain.
41
2006. Neural probabilistic language models. In-
novations in Machine Learning, pages 137?186.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 576?583.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, page 160?167.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 1162?1172.
William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proceedings of the Third International
Workshop on Paraphrasing, pages 9?16.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?
906.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-
based models for word meaning in context. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 92?97.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Jerry A. Fodor and Zenon W. Pylyshyn. 1988. Con-
nectionism and cognitive architecture: A critical
analysis. Cognition, 28:3?71.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011. Experimental support for a categorical com-
positional distributional model of meaning. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
1394?1404.
Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distributional
semantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 33?37.
Zeller S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Wilfred Hodges. 2005. The interplay of fact and the-
ory in separating syntax from meaning. In Work-
shop on Empirical Challenges and Analytical Al-
ternatives to Strict Compositionality.
Walter Kintsch. 2001. Predication. Cognitive Sci-
ence, 25(2):173?202.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
430?439.
Richard Montague. 1974. English as a formal lan-
guage. Formal Philosophy, pages 188?221.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word
meaning. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Dan Roth and Wen-tau Yih. 2002. Probabilistic rea-
soning for entity & relation recognition. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics, pages 835?841.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
907?916.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. Proceedings of the Deep Learn-
ing and Unsupervised Feature Learning Workshop
of NIPS 2010, pages 1?9.
Alfred Tarski. 1956. The concept of truth in formal-
ized languages. Logic, Semantics, Metamathemat-
ics, pages 152?278.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948?957.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Justin Washtell. 2011. Compositional expectation:
A purely distributional model of compositional se-
mantics. In Proceedings of the Ninth International
Conference on Computational Semantics (IWCS
2011), pages 285?294.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sym-
posium on Quantum Interaction.
42
Stephen Wu and William Schuler. 2011. Structured
composition of semantic vectors. In Proceedings
of the Ninth International Conference on Computa-
tional Semantics (IWCS 2011), pages 295?304.
Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment analy-
sis. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 172?182.
43
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 696?705,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Unsupervised Detection of Downward-Entailing Operators By
Maximizing Classification Certainty
Jackie CK Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
We propose an unsupervised, iterative
method for detecting downward-entailing
operators (DEOs), which are important for
deducing entailment relations between sen-
tences. Like the distillation algorithm of
Danescu-Niculescu-Mizil et al(2009), the
initialization of our method depends on the
correlation between DEOs and negative po-
larity items (NPIs). However, our method
trusts the initialization more and aggres-
sively separates likely DEOs from spuri-
ous distractors and other words, unlike dis-
tillation, which we show to be equivalent
to one iteration of EM prior re-estimation.
Our method is also amenable to a bootstrap-
ping method that co-learns DEOs and NPIs,
and achieves the best results in identifying
DEOs in two corpora.
1 Introduction
Reasoning about text has been a long-standing
challenge in NLP, and there has been consider-
able debate both on what constitutes inference and
what techniques should be used to support infer-
ence. One task involving inference that has re-
cently received much attention is that of recog-
nizing textual entailment (RTE), in which the goal
is to determine whether a hypothesis sentence can
be entailed from a piece of source text (Bentivogli
et al 2010, for example).
An important consideration in RTE is whether
a sentence or context produces an entailment re-
lation for events that are a superset or subset of
the original sentence (MacCartney and Manning,
2008). By default, contexts are upward-entailing,
allowing reasoning from a set of events to a su-
perset of events as seen in (1). In the scope of
a downward-entailing operator (DEO), however,
this entailment relation is reversed, such as in
the scope of the classical DEO not (2). There
are also operators which are neither upward- nor
downward entailing, such as the expression ex-
actly three (3).
(1) She sang in French. ? She sang.
(upward-entailing)
(2) She did not sing in French. ? She did not
sing. (downward-entailing)
(3) Exactly three students sang. 6? Exactly
three students sang in French. (neither
upward- nor downward-entailing)
Danescu-Niculescu-Mizil et al(2009) (hence-
forth DLD09) proposed the first computational
methods for detecting DEOs from a corpus. They
proposed two unsupervised algorithms which rely
on the correlation between DEOs and negative
polarity items (NPIs), which by the definition of
Ladusaw (1980) must appear in the context of
DEOs. An example of an NPI is yet, as in the
sentence This project is not complete yet. The
first baseline method proposed by DLD09 sim-
ply calculates a ratio of the relative frequencies
of a word in NPI contexts versus in a general
corpus, and the second is a distillation method
which appears to refine the baseline ratios using a
task-specific heuristic. Danescu-Niculescu-Mizil
and Lee (2010) (henceforth DL10) extend this ap-
proach to Romanian, where a comprehensive list
of NPIs is not available, by proposing a bootstrap-
ping approach to co-learn DEOs and NPIs.
DLD09 are to be commended for having iden-
tified a crucial component of inference that nev-
ertheless lends itself to a classification-based ap-
696
proach, as we will show. However, as noted
by DL10, the performance of the distillation
method is mixed across languages and in the
semi-supervised bootstrapping setting, and there
is no mathematical grounding of the heuristic to
explain why it works and whether the approach
can be refined or extended. This paper supplies
the missing mathematical basis for distillation and
shows that, while its intentions are fundamentally
sound, the formulation of distillation neglects an
important requirement that the method not be
easily distracted by other word co-occurrences
in NPI contexts. We call our alternative cer-
tainty, which uses an unusual posterior classifica-
tion confidence score (based on the max function)
to favour single, definite assignments of DEO-
hood within every NPI context. DLD09 actually
speculated on the use of max as an alternative,
but within the context of an EM-like optimization
procedure that throws away its initial parameter
settings too willingly. Certainty iteratively and
directly boosts the scores of the currently best-
ranked DEO candidates relative to the alternatives
in a Na??ve Bayes model, which thus pays more re-
spect to the initial weights, constructively build-
ing on top of what the model already knows. This
method proves to perform better on two corpora
than distillation, and is more amenable to the co-
learning of NPIs and DEOs. In fact, the best
results are obtained by co-learning the NPIs and
DEOs in conjunction with our method.
2 Related work
There is a large body of literature in linguis-
tic theory on downward entailment and polar-
ity items1, of which we will only mention the
most relevant work here. The connection between
downward-entailing contexts and negative polar-
ity items was noticed by Ladusaw (1980), who
stated the hypothesis that NPIs must be gram-
matically licensed by a DEO. However, DEOs
are not the sole licensors of NPIs, as NPIs can
also be found in the scope of questions, certain
numeric expressions (i.e., non-monotone quanti-
fiers), comparatives, and conditionals, among oth-
ers. Giannakidou (2002) proposes that the prop-
erty shared by these constructions and downward
entailment is non-veridicality. If F is a propo-
1See van der Wouden (1997) for a comprehensive refer-
ence.
sitional operator for proposition p, then an oper-
ator is non-veridical if Fp 6? p. Positive opera-
tors such as past tense adverbials are veridical (4),
whereas questions, negation and other DEOs are
non-veridical (5, 6).
(4) She sang yesterday. ? She sang.
(5) She denied singing. 6? She sang.
(6) Did she sing? 6? She sang.
While Ladusaw?s hypothesis is thus accepted
to be insufficient from a linguistic perspective, it
is nevertheless a useful starting point for compu-
tational methods for detecting NPIs and DEOs,
and has inspired successful techniques to detect
DEOs, like the work by DLD09, DL10, and also
this work. In addition to this hypothesis, we fur-
ther assume that there should only be one plausi-
ble DEO candidate per NPI context. While there
are counterexamples, this assumption is in prac-
tice very robust, and is a useful constraint for our
learning algorithm. An analogy can be drawn to
the one sense per discourse assumption in word
sense disambiguation (Gale et al 1992).
The related?and as we will argue, more
difficult?problem of detecting NPIs has also
been studied, and in fact predates the work on
DEO detection. Hoeksema (1997) performed the
first corpus-based study of NPIs, predominantly
for Dutch, and there has also been work on de-
tecting NPIs in German which assumes linguistic
knowledge of licensing contexts for NPIs (Lichte
and Soehn, 2007). Richter et al(2010) make
this assumption as well as use syntactic structure
to extract NPIs that are multi-word expressions.
Parse information is an especially important con-
sideration in freer-word-order languages like Ger-
man where a MWE may not appear as a contigu-
ous string. In this paper, we explicitly do not as-
sume detailed linguistic knowledge about licens-
ing contexts for NPIs and do not assume that a
parser is available, since neither of these are guar-
anteed when extending this technique to resource-
poor languages.
3 Distillation as EM Prior Re-estimation
Let us first review the baseline and distillation
methods proposed by DLD09, then show that dis-
tillation is equivalent to one iteration of EM prior
697
re-estimation in a Na??ve Bayes generative proba-
bilistic model up to constant rescaling. The base-
line method assigns a score to each word-type
based on the ratio of its relative frequency within
NPI contexts to its relative frequency within a
general corpus. Suppose we are given a corpus C
with extracted NPI contexts N and they contain
tokens(C) and tokens(N ) tokens respectively.
Let y be a candidate DEO, countC(y) be the uni-
gram frequency of y in a corpus, and countN (y)
be the unigram frequency of y in N . Then, we
define S(y) to be the ratio between the relative
frequencies of y within NPI contexts and in the
entire corpus2:
S(y) = count
N (y)/tokens(N )
countC(y)/tokens(C) . (7)
The scores are then used as a ranking to de-
termine word-types that are likely to be DEOs.
This method approximately captures Ladusaw?s
hypothesis by highly ranking words that appear
in NPI contexts more often than would be ex-
pected by chance. However, the problem with
this approach is that DEOs are not the only words
that co-occur with NPIs. In particular, there exist
many piggybackers, which, as defined by DLD09,
collocate with DEOs due to semantic relatedness
or chance, and would thus incorrectly receive a
high S(y) score.
Examples of piggybackers found by DLD09 in-
clude the proper noun Milken, and the adverb vig-
orously, which collocate with DEOs like deny in
the corpus they used. DLD09?s solution to the
piggybacker problem is a method that they term
distillation. Let Ny be the NPI contexts that con-
tain word y; i.e., Ny = {c ? N|c ? y}. In dis-
tillation, each word-type is given a distilled score
according to the following equation:
Sd(y) =
1
|Ny|
?
p?Ny
S(y)
?
y??p S(y?)
. (8)
where p indexes the set of NPI contexts which
contain y3, and the denominator is the number of
2DLD09 actually use the number of NPI contexts con-
taining y rather than countN (y), but we find that using the
raw count works better in our experiments.
3In DLD09, the corresponding equation does not indicate
that p should be the contexts that include y, but it is clear
from the surrounding text that our version is the intended
meaning. If all the NPI contexts were included in the sum-
mation, Sd(y) would reduce to inverse relative frequency.
Y
L
DEO
Context wordsX
Figure 1: Na??ve Bayes formulation of DEO detection.
NPI contexts which contain y.
DLD09 find that distillation seems to improve
the performance of DEO detection in BLLIP.
Later work by DL10, however, shows that distil-
lation does not seem to improve performance over
the baseline method in Romanian, and the authors
also note that distillation does not improve perfor-
mance in their experiments on co-learning NPIs
and DEOs via bootstrapping.
A better mathematical grounding of the distilla-
tion method?s apparent heuristic in terms of exist-
ing probabilistic models sheds light on the mixed
performance of distillation across languages and
experimental settings. In particular, it turns out
that the distillation method of DLD09 is equiva-
lent to one iteration of EM prior re-estimation in
a Na??ve Bayes model. Given a lexicon L of L
words, let each NPI context be one sample gen-
erated by the model. One sample consists of a
latent categorical (i.e., a multinomial with one
trial) variable Y whose values range over L, cor-
responding to the DEO that licenses the context,
and observed Bernoulli variables ~X = Xi=1...L
which indicate whether a word appears in the NPI
context (Figure 1). This method does not attempt
to model the order of the observed words, nor the
number of times each word appears. Formally, a
Na??ve Bayes model is given by the following ex-
pression:
P ( ~X, Y ) =
L
?
i=1
P (Xi|Y )P (Y ). (9)
The probability of a DEO given a particular
NPI context is
P (Y | ~X) ?
L
?
i=1
P (Xi|Y )P (Y ). (10)
698
The probability of a set of observed NPI con-
texts N is the product of the probabilities for each
sample:
P (N ) =
?
~X?N
P ( ~X) (11)
P ( ~X) =
?
y?L
P ( ~X, y). (12)
We first instantiate the baseline method of
DLD09 by initializing the parameters to the
model, P (Xi = 1|y) and P (Y = y), such that
P (Y = y) is proportional to S(y). Recall that this
initialization utilizes domain knowledge about the
correlation between NPIs and DEOs, inspired by
Ladusaw?s hypothesis:
P (Y = y) = S(y)/
?
y?
S(y?) (13)
P (Xi = 1|y) =
{
1 if Xi corresponds to y
0.5 otherwise.
(14)
This initialization of P (Xi = 1|y) ensures that
the the value of y corresponds to one of the words
in the NPI context, and the initialization of P (Y )
is simply a normalization of S(y).
Since we are working in an unsupervised set-
ting, there are no labels for Y available. A com-
mon and reasonable assumption about learning
the parameter settings in this case is to find the pa-
rameters that maximize the likelihood of the ob-
served training data; i.e., the NPI contexts:
?? = argmax
?
P (N ; ?). (15)
The EM algorithm is a well-known iterative al-
gorithm for performing this optimization. Assum-
ing that the prior P (Y = y) is a categorical distri-
bution, the M-step estimate of these parameters
after one iteration through the corpus is as fol-
lows:
P t+1(Y = y) =
?
~X?N
P t(y| ~X)
?
y? P t(y?| ~X)
(16)
We do not re-estimate P (Xi = 1|y) because
their role is simply to ensure that the DEO re-
sponsible for an NPI context exists in the context.
Estimating these parameters would exacerbate the
problems with EM for this task which we will dis-
cuss shortly.
P (Y ) gives a prior probability that a certain
word-type y is a DEO in an NPI context, without
normalizing for the frequency of y in NPI con-
texts. Since we are interested in estimating the
context-independent probability that y is a DEO,
we must calculate the probability that a word is
a DEO given that it appears in an NPI context.
Let Xy be the observed variable corresponding to
y. Then, the expression we are interested in is
P (y|Xy = 1). We now show that P (y|Xy =
1) = P (y)/P (Xy = 1), and that this expression
is equivalent to (8).
P (y|Xy = 1) =
P (y,Xy = 1)
P (Xy = 1)
(17)
Recall that P (y,Xy = 0) = 0 because of the
assumption that a DEO appears in the NPI context
that it generates. Thus,
P (y,Xy = 1) = P (y,Xy = 1) + P (y,Xy = 0)
= P (y) (18)
One iteration of EM to calculate this proba-
bility is equivalent to the distillation method of
DLD09. In particular, the numerator of (17),
which we just showed to be equal to the estimate
of P (Y ) given by (16), is exactly the sum of the
responsibilities for a particular y, and is propor-
tional to the summation in (8) modulo normaliza-
tion, because P ( ~X |y) is constant for all y in the
context. The denominator P (Xy = 1) is simply
the proportion of contexts containing y, which is
proportional to |Ny|. Since both the numerator
and denominator are equivalent up to a constant
factor, an identical ranking is produced by distil-
lation and EM prior re-estimation.
Unfortunately, the EM algorithm does not pro-
vide good results on this task. In fact, as more
iterations of EM are run, the performance drops
drastically, even though the corpus likelihood
is increasing. The reason is that unsupervised
EM learning is not constrained or biased towards
learning a good set of DEOs. Rather, a higher data
likelihood can be achieved simply by assigning
high prior probabilities to frequent word-types.
This can be seen qualitatively by consider-
ing the top-ranking DEOs after several itera-
tions of EM/distillation (Figure 2). The top-
ranking words are simply function words or other
words common in the corpus, which have noth-
ing to do with downward entailment. In effect,
699
1 iteration 2 iterations 3 iterations
denies the the
denied to to
unaware denied that
longest than than
hardly that and
lacking if has
deny has if
nobody denies of
opposes and denied
highest but denies
Figure 2: Top 10 DEOs after iterations of EM on
BLLIP.
EM/distillation overrides the initialization based
on Ladusaw?s hypothesis and finds another solu-
tion with a higher data likelihood. We will also
provide a quantitative analysis of the effects of
EM/distillation in Section 5.
4 Alternative to EM: Maximizing the
Posterior Classification Certainty
We have seen that in trying to solve the piggy-
backer problem, EM/distillation too readily aban-
dons the initialization based on Ladusaw?s hy-
pothesis, leading to an incorrect solution. Instead
of optimizing the data likelihood, what we need is
a measure of the number of plausible DEO candi-
dates there are in an NPI context, and a method
that refines the scores towards having only one
such plausible candidate per context. To this end,
we define the classification certainty to be the
product of the maximum posterior classification
probabilities over the DEO candidates. For a set
of hidden variables yN for NPI contexts N , this
is the expression:
Certainty(yN |N ) =
?
~X?N
max
y
P (y| ~X). (19)
To increase this certainty score, we propose
a novel iterative heuristic method for refining
the baseline initializations of P (Y ). Unlike
EM/distillation, our method biases learning to-
wards trusting the initialization, but refines the
scores towards having only one plausible DEO
per context in the training corpus. This is accom-
plished by treating the problem as a DEO classi-
fication problem, and then maximizing an objec-
tive ratio that favours one DEO per context. Our
method is not guaranteed to increase classification
certainty between iterations, but we will show that
it does increase certainty very quickly in practice.
The key observation that allows us to resolve
the tension between trusting the initialization and
enforcing one DEO per NPI context is that the
distributions of words that co-occur with DEOs
and piggybackers are different, and that this dif-
ference follows from Ladusaw?s hypothesis. In
particular, while DEOs may appear with or with-
out piggybackers in NPI contexts, piggybackers
do not appear without DEOs in NPI contexts, be-
cause Ladusaw?s hypothesis stipulates that a DEO
is required to license the NPI in the first place.
Thus, the presence of a high-scoring DEO candi-
date among otherwise low-scoring words is strong
evidence that the high-scoring word is not a pig-
gybacker and its high score from the initialization
is deserved. Conversely, a DEO candidate which
always appears in the presence of other strong
DEO candidates is likely a piggybacker whose
initial high score should be discounted.
We now describe our heuristic method that is
based on this intuition. For clarity, we use scores
rather than probabilities in the following explana-
tion, though it is equally applicable to either. As
in EM/distillation, the method is initialized with
the baseline S(y) scores. One iteration of the
method proceeds as follows. Let the score of the
strongest DEO candidate in an NPI context p be:
M(p) = max
y?p
Sth(y), (20)
where Sth(y) is the score of candidate y at the tth
iteration according to this heuristic method.
Then, for each word-type y in each context p,
we compare the current score of y to the scores of
the other words in p. If y is currently the strongest
DEO candidate in p, then we give y credit equal
to the proportional change to M(p) if y were re-
moved (Context p without y is denoted p \ y). A
large change means that y is the only plausible
DEO candidate in p, while a small change means
that there are other plausible DEO candidates. If
y is not currently the strongest DEO candidate, it
receives no credit:
cred(p, y) =
{
M(p)?M(p\y)
M(p) if Sth(y) = M(p)
0 otherwise.
(21)
700
NPI contexts
A B C,B C,B C,D C
Original scores
S(A) = 5, S(B) = 4, S(C) = 1, S(D) = 2
Updated scores
Sh(A) = 5? (5? 4)/5 = 1
Sh(B) = 4? (0 + 2? (4? 1)/4)/3 = 2
Sh(C) = 1? (0 + 0 + 0) = 0
Sh(D) = 2? (2? 1)/2 = 1
Figure 3: Example of one iteration of the certainty-
based heuristic on four NPI contexts with four words
in the lexicon.
Then, the average credit received by each y is
a measure of how much we should trust the cur-
rent score for y. The updated score for each DEO
candidate is the original score multiplied by this
average:
St+1h (y) =
Sth(y)
|Ny|
?
?
p?Ny
cred(p, y). (22)
The probability P t+1(Y = y) is then simply
St+1h (y) normalized:
P t+1(Y = y) = S
t+1
h (y)
?
y??L
St+1h (y?)
. (23)
We iteratively reduce the scores in this fashion
to get better estimates of the relative suitability of
word-types as DEOs.
An example of this method and how it solves
the piggybacker problem is given in Figure 3. In
this example, we would like to learn that B and
D are DEOs, A is a piggybacker, and C is a fre-
quent word-type, such as a stop word. Using the
original scores, piggybacker A would appear to
be the most likely word to be a DEO. However,
by noticing that it never occurs on its own with
words that are unlikely to be DEOs (in the exam-
ple, word C), our heuristic penalizes A more than
B, and ranks B higher after one iteration. EM
prior re-estimation would not correctly solve this
example, as it would converge on a solution where
C receives all of the probability mass because it
appears in all of the contexts, even though it is
unlikely to be a DEO according to the initializa-
tion.
5 Experiments
We evaluate the performance of these methods on
the BLLIP corpus (?30M words) and the AFP
portion of the Gigaword corpus (?338M words).
Following DLD09, we define an NPI context to
be all the words to the left of an NPI, up to the
closest comma or semi-colon, and removed NPI
contexts which contain the most common DEOs
like not. We further removed all empty NPI con-
texts or those which only contain other punctua-
tion. After this filtering, there were 26696 NPI
contexts in BLLIP and 211041 NPI contexts in
AFP, using the same list of 26 NPIs defined by
DLD09.
We first define an automatic measure of per-
formance that is common in information retrieval.
We use average precision to quantify how well a
system separates DEOs from non-DEOs. Given a
list of known DEOs, G, and non-DEOs, the aver-
age precision of a ranked list of items, X, is de-
fined by the following equation:
AP (X) =
?n
k=1 P (X1...k)? 1(xk ? G)
|G| ,
(24)
where P (X1...k) is the precision of the first k
items and 1(xk ? G) is an indicator function
which is 1 if x is in the gold standard list of DEOs
and 0 otherwise.
DLD09 simply evaluated the top 150 output
DEO candidates by their systems, and qualita-
tively judged the precision of the top-k candidates
at various values of k up to 150. Average preci-
sion can be seen as a generalization of this evalu-
ation procedure that is sensitive to the ranking of
DEOs and non-DEOs. For development purposes,
we use the list of 150 annotations by DLD09. Of
these, 90 were DEOs, 30 were not, and 30 were
classified as ?other? (they were either difficult to
classify, or were other types of non-veridical oper-
ators like comparatives or conditionals). We dis-
carded the 30 ?other? items and ignored all items
not in the remaining 120 items when evaluating a
ranked list of DEO candidates. We call this mea-
sure AP120.
In addition, we annotated DEO candidates from
the top-150 rankings produced by our certainty-
701
absolve, abstain, banish, bereft, boycott, cau-
tion, clear, coy, delay, denial, desist, devoid,
disavow, discount, dispel, disqualify, down-
play, exempt, exonerate, foil, forbid, forego,
impossible, inconceivable, irrespective, limit,
mitigate, nip, noone, omit, outweigh, pre-
condition, pre-empt, prerequisite, refute, re-
move5, repel, repulse, scarcely, scotch, scuttle,
seldom, sensitive, shy, sidestep, snuff, thwart,
waive, zero-tolerance
Figure 4: Lemmata of DEOs identified in this work not
found by DLD09.
based heuristic on BLLIP and also by the dis-
tillation and heuristic methods on AFP, in order
to better evaluate the final output of the meth-
ods. This produced an additional 68 DEOs (nar-
rowly defined) (Figure 4), 58 non-DEOs, and 31
?other? items4. Adding the DEOs and non-DEOs
we found to the 120 items from above, we have
an expanded list of 246 items to rank, and a corre-
sponding average precision which we call AP246.
We employ the frequency cut-offs used by
DLD09 for sparsity reasons. A word-type must
appear at least 10 times in an NPI context and
150 times in the corpus overall to be considered.
We treat BLLIP as a development corpus and use
AP120 on AFP to determine the number of itera-
tions to run our heuristic (5 iterations for BLLIP
and 13 iterations for AFP). We run EM/distillation
for one iteration in development and testing, be-
cause more iterations hurt performance, as ex-
plained in Section 3.
We first report the AP120 results of our ex-
periments on the BLLIP corpus (Table 1 sec-
ond column). Our method outperforms both
EM/distillation and the baseline method. These
results are replicated on the final test set from
AFP using the full set of annotations AP246 (Ta-
ble 1 third column). Note that the scores are lower
when using all the annotations because there are
more non-DEOs relative to DEOs in this list, mak-
ing the ranking task more challenging.
A better understanding of the algorithms can
4The complete list will be made publicly available.
5We disagree with DLD09 that remove is not downward-
entailing; e.g., The detergent removed stains from his cloth-
ing. ? The detergent removed stains from his shirts.
Method BLLIP AP120 AFP AP246
Baseline .879 .734
Distillation .946 .785
This work .955 .809
Table 1: Average precision results on the BLLIP and
AFP corpora.
be obtained by examining the data likelihood and
the classification certainty at each iteration of the
algorithms (Figure 5). Whereas EM/distillation
maximizes the former expression, the certainty-
based heuristic method actually decreases data
likelihood for the first couple of iterations before
increasing it again. In terms of classification cer-
tainty, EM/distillation converges to a lower classi-
fication certainty score compared to our heuristic
method. Thus, our method better captures the as-
sumption of one DEO per NPI context.
6 Bootstrapping to Co-Learn NPIs and
DEOs
The above experiments show that the heuristic
method outperforms the EM/distillation method
given a list of NPIs. We would like to extend
this result to novel domains, corpora, and lan-
guages. DLD09 and DL10 proposed the follow-
ing bootstrapping algorithm for co-learning NPIs
and DEOs given a much smaller list of NPIs as a
seed set.
1. Begin with a small set of seed NPIs
2. Iterate:
(a) Use the current list of NPIs to learn a
list of DEOs
(b) Use the current list of DEOs to learn a
list of NPIs
Interestingly, DL10 report that while this
method works in Romanian data, it does not work
in the English BLLIP corpus. They speculate that
the reason might be due to the nature of the En-
glish DEO any, which can occur in all classes of
DE contexts according to an analysis by Haspel-
math (1997). Further, they find that in Romanian,
distillation does not perform better than the base-
line method during Step (2a). While this linguis-
tic explanation may certainly be a factor, we raise
702
0 1 2 3 4 5 6 7 8 9 10
-2.5
-2
-1.5
-1
-0.5
0
x 106
Iterations
Lo
g 
pr
ob
ab
ilit
y
(a) Data log likelihood.
0 1 2 3 4 5 6 7 8 9 10
-2.5
-2
-1.5
-1
-0.5
0
x 105
Iterations
Lo
g 
pr
ob
ab
ilit
y
(b) Log classification certainty probabilities.
Figure 5: Log likelihood and classification certainty probabilities of NPI contexts in two corpora. Thinner lines
near the top are for BLLIP; thicker lines for AFP. Blue dotted: baseline; red dashed: distillation; green solid:
our certainty-based heuristic method. P ( ~X|y) probabilities are not included since they would only result in a
constant offset in the log domain.
a second possibility that the distillation algorithm
itself may be responsible for these results. As ev-
idence, we show that the heuristic algorithm is
able to work in English with just the single seed
NPI any, and in fact the bootstrapping approach in
conjunction with our heuristic even outperforms
the above approaches when using a static list of
NPIs.
In particular, we use the methods described in
the previous sections for Step (2a), and the follow-
ing ratio to rank NPI candidates in Step (2b), cor-
responding to the baseline method to detect DEOs
in reverse:
T (x) = count
D(x)/tokens(D)
countC(x)/tokens(C) . (25)
Here, countD(x) refers to the number of oc-
currences of NPI candidate x in DEO contexts
D, defined to be the words to the right of a DEO
operator up to a comma or semi-colon. We do
not use the EM/distillation or heuristic methods in
Step (2b). Learning NPIs from DEOs is a much
harder problem than learning DEOs from NPIs.
Because DEOs (and other non-veridical opera-
tors) license NPIs, the majority of occurrences of
NPIs will be in the context of a DEO, modulo am-
biguity of DEOs such as the free-choice any and
other spurious correlations such as piggybackers
as discussed earlier. In the other direction, it is
not the case that DEOs always or nearly always
appear in the context of an NPI. Rather, the most
common collocations of DEOs are the selectional
preferences of the DEO, such as common argu-
ments to verbal DEOs, prepositions that are part
of the subcategorization of the DEO, and words
that together with the surface form of the DEO
comprise an idiomatic expression or multi-word
expression. Further, NPIs are more likely to be
composed of multiple words, while many DEOs
are single words, possibly with PP subcategoriza-
tion requirements which can be filled in post hoc.
Because of these issues, we cannot trust the ini-
tialization to learn NPIs nearly as much as with
DEOs, and cannot use the distillation or certainty
methods for this step. Rather, the hope is that
learning a noisy list of ?pseudo-NPIs?, which of-
ten occur in negative contexts but may not actu-
ally be NPIs, can still improve the performance of
DEO detection.
There are a number of parameters to the method
which we tuned to the BLLIP corpus using
AP120. At the end of Step (2a), we use the cur-
rent top 25 DEOs plus 5 per iteration as the DEO
list for the next step. To the initial seed NPI of
703
Method BLLIP AP120 AFP AP246
Baseline .889 (+.010) .739 (?.005)
Distillation .930 (?.016) .804 (+.019)
This work .962 (+.007) .821 (+.012)
Table 2: Average precision results with bootstrapping
on the BLLIP and AFP corpora. Absolute gain in av-
erage precision compared to using a fixed list of NPIs
given in brackets.
anymore, anything, anytime, avail, bother,
bothered, budge, budged, countenance, faze,
fazed, inkling, iota, jibe, mince, nor, whatso-
ever, whit
Figure 6: Probable NPIs found by bootstrapping using
the certainty-based heuristic method.
any, we add the top 5 ranking NPI candidates at
the end of Step (2b) in each subsequent iteration.
We ran the bootstrapping algorithm for 11 itera-
tions for all three algorithms. The final evaluation
was done on AFP using AP246.
The results show that bootstrapping can indeed
improve performance, even in English (Table 2).
Using bootstrapping to co-learn NPIs and DEOs
actually results in better performance than spec-
ifying a static list of NPIs. The certainty-based
heuristic in particular achieves gains with boot-
strapping in both corpora, in contrast to the base-
line and distillation methods. Another factor that
we found to be important is to add a sufficient
number of NPIs to the NPI list each iteration, as
adding too few NPIs results in only a small change
in the NPI contexts available for DEO detection.
DL10 only added one NPI per iteration, which
may explain why they did not find any improve-
ment with bootstrapping in English. It also ap-
pears that learning the pseudo-NPIs does not hurt
performance in detecting DEO, and further, that
a number of true NPIs are learned by our method
(Figure 6).
7 Conclusion
We have proposed a novel unsupervised method
for discovering downward-entailing operators
from raw text based on their co-occurrence with
negative polarity items. Unlike the distilla-
tion method of DLD09, which we show to
be an instance of EM prior re-estimation, our
method directly addresses the issue of piggyback-
ers which spuriously correlate with NPIs but are
not downward-entailing. This is achieved by
maximizing the posterior classification certainty
of the corpus in a way that respects the initializa-
tion, rather than maximizing the data likelihood
as in EM/distillation. Our method outperforms
distillation and a baseline method on two corpora
as well as in a bootstrapping setting where NPIs
and DEOs are jointly learned. It achieves the best
performance in the bootstrapping setting, rather
than when using a fixed list of NPIs. The perfor-
mance of our algorithm suggests that it is suitable
for other corpora and languages.
Interesting future research directions include
detecting DEOs of more than one word as well as
distinguishing the particular word sense and sub-
categorization that is downward-entailing. An-
other problem that should be addressed is the
scope of the downward entailment, generalizing
work being done in detecting the scope of nega-
tion (Councill et al 2010, for example).
Acknowledgments
We would like to thank Cristian Danescu-
Niculescu-Mizil for his help with replicating his
results on the BLLIP corpus. This project was
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
References
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T.
Dang, and Danilo Giampiccolo. 2010. The sixth
pascal recognizing textual entailment challenge. In
The Text Analysis Conference (TAC 2010).
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not:
Learning to classify the scope of negation for im-
proved sentiment analysis. In Proceedings of the
Workshop on Negation and Speculation in Natural
Language Processing, pages 51?59. Association for
Computational Linguistics.
Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2010. Don?t ?have a clue??: Unsupervised co-
learning of downward-entailing operators. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 247?252. Association for Computational Lin-
guistics.
Cristian Danescu-Niculescu-Mizil, Lillian Lee, and
Richard Ducott. 2009. Without a ?doubt??: Un-
supervised discovery of downward-entailing oper-
704
ators. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the Workshop on Speech and Natural
Language, pages 233?237. Association for Compu-
tational Linguistics.
Anastasia Giannakidou. 2002. Licensing and sensitiv-
ity in polarity items: from downward entailment to
nonveridicality. CLS, 38:29?53.
Martin Haspelmath. 1997. Indefinite pronouns. Ox-
ford University Press.
Jack Hoeksema. 1997. Corpus study of negative po-
larity items. IV-V Jornades de corpus linguistics
1996?1997.
William A. Ladusaw. 1980. On the notion ?affective?
in the analysis of negative-polarity items. Journal
of Linguistic Research, 1(2):1?16.
Timm Lichte and Jan-Philipp Soehn. 2007. The re-
trieval and classification of negative polarity items
using statistical profiles. Roots: Linguistics in
Search of Its Evidential Base, pages 249?266.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of the
22nd International Conference on Computational
Linguistics.
Frank Richter, Fabienne Fritzinger, and Marion Weller.
2010. Who can see the forest for the trees? ex-
tracting multiword negative polarity items from
dependency-parsed text. Journal for Language
Technology and Computational Linguistics, 25:83?
110.
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity and Multiple Negation. Rout-
ledge.
705
Proceedings of NAACL-HLT 2013, pages 837?846,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Probabilistic Frame Induction
Jackie Chi Kit Cheung?
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
jcheung@cs.toronto.edu
Hoifung Poon
One Microsoft Way
Microsoft Research
Redmond, WA 98052, USA
hoifung@microsoft.com
Lucy Vanderwende
One Microsoft Way
Microsoft Research
Redmond, WA 98052, USA
lucyv@microsoft.com
Abstract
In natural-language discourse, related events
tend to appear near each other to describe a
larger scenario. Such structures can be formal-
ized by the notion of a frame (a.k.a. template),
which comprises a set of related events and
prototypical participants and event transitions.
Identifying frames is a prerequisite for infor-
mation extraction and natural language gen-
eration, and is usually done manually. Meth-
ods for inducing frames have been proposed
recently, but they typically use ad hoc proce-
dures and are difficult to diagnose or extend.
In this paper, we propose the first probabilistic
approach to frame induction, which incorpo-
rates frames, events, and participants as latent
topics and learns those frame and event transi-
tions that best explain the text. The number
of frame components is inferred by a novel
application of a split-merge method from syn-
tactic parsing. In end-to-end evaluations from
text to induced frames and extracted facts, our
method produces state-of-the-art results while
substantially reducing engineering effort.
1 Introduction
Events with causal or temporal relations tend to oc-
cur near each other in text. For example, a BOMB-
ING scenario in an article on terrorism might be-
gin with a DETONATION event, in which terrorists
set off a bomb. Then, a DAMAGE event might en-
sue to describe the resulting destruction and any
casualties, followed by an INVESTIGATION event
?This research was undertaken during the author?s internship
at Microsoft Research.
covering subsequent police investigations. After-
wards, the BOMBING scenario may transition into
a CRIMINAL-PROCESSING scenario, which begins
with police catching the terrorists, and proceeds to
a trial, sentencing, etc. A common set of partici-
pants serves as the event arguments; e.g., the agent
(or subject) of DETONATION is often the same as
the theme (or object) of INVESTIGATION and corre-
sponds to a PERPETRATOR.
Such structures can be formally captured by the
notion of a frame (a.k.a. template, scenario), which
consists of a set of events with prototypical transi-
tions, as well as a set of slots representing the com-
mon participants. Identifying frames is an explicit
or implicit prerequisite for many NLP tasks. Infor-
mation extraction, for example, stipulates the types
of events and slots that are extracted for a frame or
template. Online applications such as dialogue sys-
tems and personal-assistant applications also model
users? goals and subgoals using frame-like represen-
tations. In natural-language generation, frames are
often used to represent contents to be expressed as
well as to support surface realization.
Until recently, frames and related representations
have been manually constructed, which has limited
their applicability to a relatively small number of do-
mains and a few slots within a domain. Furthermore,
additional manual effort is needed after the frames
are defined in order to extract frame components
from text (e.g., in annotating examples and design-
ing features to train a supervised learning model).
This paradigm makes generalizing across tasks dif-
ficult, and might suffer from annotator bias.
Recently, there has been increasing interest in au-
837
tomatically inducing frames from text. A notable
example is Chambers and Jurafsky (2011), which
first clusters related verbs to form frames, and then
clusters the verbs? syntactic arguments to identify
slots. While Chambers and Jurafsky (2011) repre-
sents a major step forward in frame induction, it is
also limited in several aspects. The clustering used
ad hoc steps and customized similarity metrics, as
well as an additional retrieval step from a large ex-
ternal text corpus for slot generation. This makes it
hard to replicate their approach or adapt it to new
domains. Lacking a coherent model, it is also diffi-
cult to incorporate additional linguistic insights and
prior knowledge.
In this paper, we present PROFINDER (PROba-
bilistic Frame INDucER), the first probabilistic ap-
proach to frame induction. PROFINDER defines
a joint distribution over the words in a document
and their frame assignments by modeling frame
and event transitions, correlations among events and
slots, and their surface realizations. Given a set of
documents, PROFINDER outputs a set of induced
frames with learned parameters, as well as the most
probable frame assignments that can be used for
event and entity extraction. The numbers of events
and slots are dynamically determined by a novel
application of the split-merge approach from syn-
tactic parsing (Petrov et al, 2006). In end-to-end
evaluations from text to entity extraction using stan-
dard MUC and TAC datasets, PROFINDER achieved
state-of-the-art results while significantly reducing
engineering effort and requiring no external data.
2 Related Work
In information extraction and other semantic pro-
cessing tasks, the dominant paradigm requires two
stages of manual effort. First, the target representa-
tion is defined manually by domain experts. Then,
manual effort is required to construct an extractor
or to annotate examples to train a machine-learning
system. Recently, there has been a burgeoning body
of work in reducing such manual effort. For exam-
ple, a popular approach to reduce annotation effort is
bootstrapping from seed examples (Patwardhan and
Riloff, 2007; Huang and Riloff, 2012). However,
this still requires prespecified frames or templates,
and selecting seed words is often a challenging task
(Curran et al, 2007). Filatova et al (2006) construct
simple domain templates by mining verbs and the
named entity type of verbal arguments that are topi-
cal, whereas Shinyama and Sekine (2006) identify
query-focused slots by clustering common named
entities and their syntactic contexts. Open IE (Banko
and Etzioni, 2008) limits the manual effort to de-
signing a few domain-independent relation patterns,
which can then be applied to extract relational triples
from text. While extremely scalable, this approach
can only extract atomic factoids within a sentence,
and the resulting triples are noisy, non-canonicalized
text fragments.
More relevant to our approach is the recent work
in unsupervised semantic induction, such as un-
supervised semantic parsing (Poon and Domingos,
2009), unsupervised semantical role labeling (Swier
and Stevenson, 2004) and induction (Lang and Lap-
ata, 2011, e.g.), and slot induction from web search
logs (Cheung and Li, 2012). As in PROFINDER,
they model distributional contexts for slots and
roles. However, these approaches focus on the se-
mantics of independent sentences or queries, and do
not capture discourse-level dependencies.
The modeling of frame and event transitions in
PROFINDER is similar to a sequential topic model
(Gruber et al, 2007), and is inspired by the suc-
cessful applications of such topic models in sum-
marization (Barzilay and Lee, 2004; Daume? III and
Marcu, 2006; Haghighi and Vanderwende, 2009, in-
ter alia). There are, however, two main differences.
First, PROFINDER contains not a single sequential
topic model, but two (for frames and events, respec-
tively). In addition, it also models the interdepen-
dencies among events, slots, and surface text, which
is analogous to the USP model (Poon and Domin-
gos, 2009). PROFINDER can thus be viewed as a
novel combination of state-of-the-art models in un-
supervised semantics and discourse modeling.
In terms of aim and capability, PROFINDER is
most similar to Chambers and Jurafsky (2011),
which culminated from a series of work for iden-
tifying correlated events and arguments in narratives
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). By adopting a probabilistic approach,
PROFINDER has a sound theoretical underpinning,
and is easy to modify or extend. For example, in
Section 3, we show how PROFINDER can easily be
838
augmented with additional linguistically-motivated
features. Likewise, PROFINDER can easily be used
as a semi-supervised system if some slot designa-
tions and labeled examples are available.
The idea of representing and capturing stereotyp-
ical knowledge has a long history in artificial in-
telligence and psychology, and has assumed vari-
ous names such as frames (Minsky, 1974), schemata
(Rumelhart, 1975), and scripts (Schank and Abel-
son, 1977). In the linguistics and computational
linguistics communities, frame semantics (Fillmore,
1982) uses frames as the central representation of
word meaning, culminating in the development of
FrameNet (Baker et al, 1998), which contains over
1000 manually annotated frames. A similarly rich
lexical resource is the MindNet project (Richard-
son et al, 1998). Our notion of frame is related to
these representations, but there are also subtle differ-
ences. For example, Minsky?s frame emphasizes in-
heritance, which we do not model in this paper1. As
in semantic role labeling, FrameNet focuses on se-
mantic roles and does not model event or frame tran-
sitions, so the scope of its frames is often no more
than an event in our model. Perhaps the most sim-
ilar to our frame is Roger Schank?s scripts, which
capture prototypical events and participants in a sce-
nario such as restaurant dining. In their approach,
however, scripts are manually defined, making it
hard to generalize. In this regard, our work may be
viewed as an attempt to revive a long tradition in AI
and linguistics, by leveraging the recent advances in
computational power, NLP, and machine learning.
3 Probabilistic Frame Induction
In this section, we present PROFINDER, a proba-
bilistic model for frame induction. Let F be a set of
frames, where each frame F = (EF , SF ) comprises
a unique set of events EF and slots SF . Given a
document D and a word w in D, Zw = (f, e) repre-
sents an assignment of w to frame f ? F and frame
element e ? Ef ? Sf . At the heart of PROFINDER
is a generative model P?(D,Z) that defines a joint
distribution over document D and the frame assign-
ment to its words Z. Given a set of documents D,
1This should be a straightforward extension ? using the
split-and-merge approach, PROFINDER already produces a hi-
erarchy of events and slots in learning, although currently it
makes no use of the intermediate levels.
frame induction in PROFINDER amounts to deter-
mining the number of events and slots in each frame,
as well as learning the parameters ? by summing out
the latent assignments Z to maximize the likelihood
of the document set
?
D?D
P?(D).
The induced frames identify the key event structures
in the document set. Additionally, PROFINDER can
conduct event and entity extraction by computing
the most probable frame assignment Z. In the re-
mainder of the section, we first present the base
model for PROFINDER. We then introduce sev-
eral linguistically motivated refinements, as well as
efficient algorithms for learning and inference in
PROFINDER.
3.1 Base Model
The probabilistic formulation of PROFINDER makes
it extremely flexible for incorporating linguistic in-
tuition and prior knowledge. In this paper, we design
our PROFINDER model to capture three types of de-
pendencies.
Frame transitions between clauses A sentence
contains one or more clauses, each of which is a
minimal unit expressing a proposition. A clause is
unlikely to straddle different frames, so we stipu-
late that the words in a clause be assigned to the
same frame. On the other hand, frame transitions
can happen between clauses, and we adopt the com-
mon Markov assumption that the frame of a clause
only depends on the previous clause in the docu-
ment. Clauses are automatically extracted from the
dependency parse and further decomposed into an
event head and its syntactic arguments.
Event transitions within a frame Events tend to
transition into related events in the same frame, as
determined by their causal or temporal relations.
Each clause is assigned an event compatible with
its frame assignment (i.e., the event is in the given
frame). Like frame transitions, we assume that the
event assignment of a clause depends only on the
event of the previous clause.
Emission of event heads and slot words Simi-
lar to topics in topic models, each event determines
839
a multinomial from which the event head is gener-
ated; e.g., a DETONATION event might use verbs
such as detonate, set off or nouns such as denota-
tion, bombing as its event head. Additionally, as
in USP (Poon and Domingos, 2009), an event also
contains a multinomial of slots for each of its argu-
ment types2; e.g., the agent argument of a DETONA-
TION event is generally the PERPETRATOR slot of
the BOMBING frame. Finally, each slot has its own
multinomials for generating the argument head and
dependency label, regardless of the event.
Formally, let D be a document and C1, ? ? ? , Cl be
its clauses, the PROFINDER model is defined by
P?(D,Z) = PF?INIT(F1)?
?
i
PF?TRAN(Fi+1|Fi)
? PE?INIT(E1|F1)
?
?
i
PE?TRAN(Ei+1|Ei, Fi+1, Fi)
?
?
i
PE?HEAD(ei|Ei)
?
?
i,j
PSLOT(Si,j |Ei,j , Ai,j)
?
?
i,j
PA?HEAD(ai,j |Si,j)
?
?
i,j
PA?DEP(depi,j |Si,j)
Here, Fi, Ei denote the frame and event assign-
ment to clause Ci, respectively, and ei denotes the
event head. For the j-th argument of clause i,
Si,j denotes the slot assignment, Ai,j the argument
type, ai,j the head word, and depi,j the dependency
from the event head. PE?TRAN(Ei+1|Ei, Fi+1, Fi) =
PE?INIT(Ei+1|Fi+1) if Fi+1 6= Fi.
Essentially, PROFINDER combines a frame HMM
with an event HMM, where the first models frame
transition and emits events, and the second models
event transition within a frame and emits argument
slots.
3.2 Model refinements
The base model captures the main dependencies in
event narrative, but it can be easily extended to lever-
2USP generates the argument types along with events from
clustering. For simplicity, in PROFINDER we simply classify
a syntactic argument into subject, object, and prepositional ob-
ject, according to its Stanford dependency to the event head.
age additional linguistic intuition. PROFINDER in-
corporates three such refinements.
Background frame Event narratives often con-
tain interjections of general content common to all
frames. For example, in newswire articles, ATTRI-
BUTION is commonplace to describe who said or
reported a particular quote or fact. To avoid con-
taminating frames with generic content, we intro-
duce a background frame with its own events, slots,
and emission distributions, and a binary switch vari-
able Bi ? {BKG,CNT} that determines whether
clause i is generated from the actual content frame
Fi (CNT ) or background (BKG). We also stipu-
late that if BKG is chosen, the nominal frame stays
the same as the previous clause.
Stickiness in frame and event transitions Prior
work has demonstrated that promoting topic coher-
ence in natural-language discourse helps discourse
modeling (Barzilay and Lee, 2004). We extend
PROFINDER to leverage this intuition by incorporat-
ing a ?stickiness? prior (Haghighi and Vanderwende,
2009) to encourage neighboring clauses to stay in
the same frame. Specifically, along with introducing
the background frame, the frame transition compo-
nent now becomes
PF?TRAN(Fi+1|Fi, Bi+1) = (1)
?
??
??
1(Fi+1 = Fi), if Bi+1 = BKG
?1(Fi+1 = Fi)+
(1? ?)PF?TRAN(Fi+1|Fi),
if Bi+1 = CNT
where ? is the stickiness parameter, and the event
transition component correspondingly becomes
PE?TRAN(Ei+1|Ei, Fi+1, Fi, Bi+1) = (2)
?
??
??
1(Ei+1 = Ei), if Bi+1 = BKG
PE?TRAN(Ei+1|Ei), if Bi+1 = CNT,Fi = Fi+1
PE?INIT(Ei+1), if Bi+1 = CNT,Fi 6= Fi+1
Argument dependencies as caseframes As no-
ticed in previous work such as Chambers and Juraf-
sky (2011), the combination of an event head and a
dependency relation often gives a strong signal of
the slot that is indicated. For example, bomb >
nsubj (subject argument of bomb) often indicates
a PERPETRATOR. Thus, rather than simply emitting
840
Frame 
Event 
Background 
Event 
head 
?1 
?1 
?1 
?1 
?1 ?1 
???1 ?1 
? ?? 
?? 
?? 
?? 
?? ?? 
???? ?? 
. . .  
. . .  
|?| |?| |?| 
???????  
Arguments 
???????  ??????  
Figure 1: Graphical representation of our model. Hyper-
parameters, the stickiness factor, and the frame and event
initial and transition distributions are not shown for clar-
ity.
the dependency from the event head to an event ar-
gument depi,j , our model instead emits the pair of
event head and dependency relation, which we call
a caseframe following Bean and Riloff (2004).
3.3 Full generative story
To summarize, the distributions that are learned by
our model are the default distributions PBKG(B),
PF?INIT(F ), PE?INIT(E); the transition distri-
butions PF?TRAN(Fi+1|Fi), PE?TRAN(Ei+1|Ei);
and the emission distributions PSLOT(S|E,A,B),
PE?HEAD(e|E,B), PA?HEAD(a|S), PA?DEP(dep|S).
We used additive smoothing with uniform Dirich-
let priors for all the multinomials. The overall
generative story of our model is as follows:
1. Draw a Bernoulli distribution for PBKG(B)
2. Draw the frame, event, and slot distributions
3. Draw an event head emission distribution
PE?HEAD(e|E,B) for each frame including the
background frame
4. Draw event argument lemma and caseframe
emission distributions for each slot in each
frame including the background frame
5. For each clause in each document, generate the
clause-internal structure.
The clause-internal structure at clause i is gener-
ated by the following steps:
1. Generate whether this clause is background
(Bi ? {CNT,BKG} ? PBKG(B))
2. Generate the frame Fi and event Ei from
PF?INIT(F ), PE?INIT(E), or according to
equations 1 and 2
3. Generate the observed event head ei from
PE?HEAD(ei|Ei).
4. For each event argument:
(a) Generate the slot Si,j from
PSLOT(S|E,A,B).
(b) Generate the dependency/caseframe emis-
sion depi,j ? PA?DEP(dep|S) and the
lemma of the head word of the event ar-
gument ai,j ? PA?HEAD(a|S).
3.4 Learning and Inference
Our generative model admits efficient inference by
dynamic programming. In particular, after collaps-
ing the latent assignment of frame, event, and back-
ground into a single hidden variable for each clause,
the expectation and most probable assignment can
be computed using standard forward-backward and
Viterbi algorithms on fixed tree structures.
Parameter learning can be done using EM by al-
ternating the computation of expected counts and the
maximization of multinomial parameters. In par-
ticular, PROFINDER uses incremental EM, which
has been shown to have better and faster con-
vergence properties than standard EM (Liang and
Klein, 2009).
Determining the optimal number of events and
slots is challenging. One solution is to adopt a non-
parametric Bayesian method by incorporating a hi-
erarchical prior over the parameters (e.g., a Dirich-
let process). However, this approach can impose
unrealistic restrictions on the model choice and re-
sult in intractability which requires sampling or ap-
proximate inference to overcome. Additionally, EM
learning can suffer from local optima due to its non-
convex learning objective, especially when dealing
with a large number hidden states without a good
initialization.
To address these issues, we adopt a novel appli-
cation of the split-merge method previously used in
syntactic parsing for inferring refined latent syntac-
tic categories (Petrov et al, 2006). First, the model
is initialized with a number of frames, which is a
hyperparameter, and each frame is associated with
841
one event and two slots. Starting from this mini-
mal structure, EM training begins. After a number
of iterations, each event and slot state is ?split? in
two; that is, each original state now becomes two
new states. Each of the new states is generated with
half of the probability of the original, and contains
a duplicate of the associated emission distributions.
Some perturbation is then added to the probabilities
to break symmetry. After splitting, we merge back
a portion of the newly split events and slots that re-
sult in the least improvement in the likelihood of the
training data. For more details on split-merge, see
Petrov et al (2006)
By adjusting the number of split-merge cycles and
the merge parameters, our model learns the number
of events and slots in a dynamical fashion that is tai-
lored to the data. Moreover, our model starts with a
small number of frame elements, which reduces the
number of local optima and facilitates initial learn-
ing. After each split, the subsequent learning starts
with (a perturbed version of) the previously learned
parameters, which makes a good initialization that
is crucial for EM. Finally, it is also compatible with
the hierarchical nature of events and slots. For ex-
ample, slots can first be coarsely split into persons
versus locations, and later refined into subcategories
such as perpetrators and victims.
4 MUC-4 Entity Extraction Experiments
We first evaluate our model on a standard entity
extraction task, using the evaluation settings from
Chambers and Jurafsky (2011) (henceforth, C&J)
to enable a head-to-head comparison. Specifically,
we use the MUC-4 data set (1992) , which contains
1300 training and development documents on ter-
rorism in South America, with 200 additional doc-
uments for testing. MUC-4 contains four templates:
ATTACK, KIDNAPPING, BOMBING, and ARSON.3
All templates share the same set of predefined slots,
with the evaluation focusing on the following four:
PERPETRATOR, PHYSICAL TARGET, HUMAN TAR-
GET, and INSTRUMENT.
For each slot in a MUC template, the system
first identifies an induced slot that best maps to it
by F1 on the development set. As in C&J, tem-
3Two other templates have negligible counts and are ignored
as in C&J.
plate is ignored in final evaluation, so all the clusters
that belong to the same slot are then merged across
the templates; e.g., the PERPETRATOR clusters for
KIDNAPPING and BOMBING are merged. The fi-
nal precision, recall, and F1 are computed based on
these merged clusters. Correctness is determined by
matching head words, and slots marked as optional
in MUC are ignored when computing recall. All hy-
perparameters are tuned on the development set (see
Appendix A for their values).
Named entity type Named entity type is a useful
feature to filter out entities for particular slots; e.g. a
location cannot be an INSTRUMENT. We thus divide
each induced cluster into four clusters by named
entity type before performing the mapping, follow-
ing C&J?s heuristic and using a named entity recog-
nizer and word lists derived from WordNet: PER-
SON/ORGANIZATION, PHYSICAL OBJECT, LOCA-
TION, and OTHER.
Document classification The MUC-4 dataset
contains many documents that have words related
to MUC slots (e.g., plane and aviation), but are not
about terrorism. To reduce precision errors, C&J
first filtered irrelevant documents based on the speci-
ficity of event heads to learned frames. To estimate
the specificity, they used additional data retrieved
from a large external corpus. In PROFINDER, how-
ever, specificity can be easily estimated using the
probability distributions learned during training. In
particular, we define the probability of an event head
in a frame j as:
PF (w) =
?
EF?F
PE?HEAD(w|E)/|F |, (3)
and the probability of a frame given an event head
as:
P (F |w) = PF (w)/
?
F ??F
PF ?(w). (4)
We then follow the rest of C&J?s procedure to
score each learned frame with each MUC document.
Specifically, a document is mapped to a frame if the
average PF (w) in the document is above a threshold
and the document contains at least one trigger word
w? with P (F |w?) > 0.2. The threshold and the in-
duced frame were determined on the development
set, and were used to filter irrelevant documents in
the test set.
842
Unsupervised methods P R F1
PROFINDER (This work) 32 37 34
Chambers and Jurafsky (2011) 48 25 33
With additional information
PROFINDER +doc. classification 41 44 43
C&J 2011 +granularity 44 36 40
Table 1: Results on MUC-4 entity extraction. C&J 2011
+granularity refers to their experiment in which they
mapped one of their templates to five learned clusters
rather than one.
Results Compared to C&J, PROFINDER is con-
ceptually much simpler, using a single probabilis-
tic model and standard learning and inference algo-
rithms, and not requiring multiple processing steps
or customized similarity metrics. It only used the
data in MUC-4, whereas C&J required additional
text to be retrieved from a large external corpus (Gi-
gaword (Graff et al, 2005)) for each event cluster.
It currently does not make use of coreference infor-
mation, whereas C&J did. Remarkably, despite all
these, PROFINDER was still able to outperform C&J
on entity extraction, as shown in Table 1. We also
evaluated PROFINDER?s performance assuming per-
fect document classification (+doc. classification).
This led to a substantially higher precision, suggest-
ing that further improvement is possible from better
document classification.
Figure 2 shows part of a frame learned by
PROFINDER, which includes some slots and events
annotated in MUC. PROFINDER is also able to iden-
tify events and slots not annotated in MUC, a de-
sirable characteristic of unsupervised methods. For
example, it found a DISCUSSION event, an AR-
REST event (call, arrest, express, meet, charge), a
PEACE AGREEMENT slot (agreement, rights, law,
proposal), and an AUTHORITIES slot (police, gov-
ernment, force, command). The background frame
was able to capture many verbs related to attribu-
tion, such as say, continue, add, believe, although it
missed report.
5 Evaluating Frame Induction Using
Guided Summarization Templates
The MUC-4 dataset was originally designed for
information extraction and focuses on a limited
number of template and slot types. To evalu-
Event: Attack Event: Discussion
report, participate, kid-
nap, kill, release
hold, meeting, talk, dis-
cuss, investigate
Slot: Perpetrator Slot: Victim
PERSON/ORG PERSON/ORG
Words: guerrilla, po-
lice, source, person,
group
Words: people, priest,
leader, member, judge
Caseframes:
report>nsubj,
kidnap>nsubj,
kill>nsubj,
participate>nsubj,
release>nsubj
Caseframes:
kill>dobj,
murder>dobj,
release>dobj,
report>dobj,
kidnap>dobj
Figure 2: A partial frame learned by PROFINDER from
the MUC-4 data set, with the most probable emissions for
each event and slot. Labels are assigned by the authors
for readability.
ate PROFINDER?s capabilities in generalizing to
a greater variety of text, we designed and con-
ducted a novel evaluation based on the TAC guided-
summarization dataset. This evaluation was inspired
by the connection between summarization and infor-
mation extraction (White et al, 2001), and reflects a
conceptualization of summarization as inducing and
extracting structured information from source text.
Essentially, we adapted the TAC summarization an-
notation to create gold-standard slots, and used them
to evaluate entity extraction as in MUC-4.
Dataset We used the TAC 2010 guided-
summarization dataset in our experiments
(Owczarzak and Dang, 2010). This data set con-
sists of text from five domains (termed categories
in TAC), each with a template defined by TAC
organizers. In total, there are 46 document clusters
(termed topics in TAC), each of which contains 20
documents and has eight human-written summaries.
Each summary was manually segmented using
the Pyramid method (Nenkova and Passonneau,
2004) and each segment was annotated with a slot
(termed aspect in TAC) from the corresponding
template. Figure 3 shows an example and the full
set of templates is available at http://www.
nist.gov/tac/2010/Summarization/
Guided-Summ.2010.guidelines.html. In
843
(a) Accidents and Natural Disasters:
WHAT: what happened
WHEN: date, time, other temporal markers
WHERE: physical location
WHY: reasons for accident/disaster
WHO AFFECTED: casualties...
DAMAGES: ... caused by the disaster
COUNTERMEASURES: rescue efforts...
(b) (WHEN During the night of July 17,)
(WHAT a 23-foot <WHAT tsunami) hit the
north coast of Papua New Guinea (PNG)>,
(WHY triggered by a 7.0 undersea earth-
quake in the area).
(c) WHEN: night WHAT: tsunami, coast
WHY: earthquake
Figure 3: (a) A frame from the TAC Guided Summariza-
tion task with abbreviated slot descriptions. (b) A TAC
text span, segmented into several contributors with slot
labels. Note that the two WHAT contributors overlap, and
are demarcated by different bracket types. (c) The entities
that are extracted for evaluation.
TAC, each annotated segment (Figure 3b) is called
a contributor.
Evaluation Method We converted the contribu-
tors into a form that is more similar to the previ-
ous MUC evaluation, so that we can fairly compare
against previous work such as C&J that were de-
signed to extract information into that form. Specif-
ically, we extracted the head lemma from all the
maximal noun phrases found in the contributor (Fig-
ure 3c) and treated them as gold-standard entity slots
to extract. While this conversion may not be ideal in
some cases, it simplifies the TAC slots and enables
automatic evaluation. We leave the refinement of
this conversion to future work, and believe it could
be done by crowdsourcing.
For each TAC slot in a TAC category, we extract
entities from the summaries that belong to the given
TAC category. A system-induced entity is consid-
ered a match to a TAC-derived entity from the same
document if the head lemma in the former matches
one in the latter. Based on this matching criterion,
the system-induced slots are mapped to the TAC
slots in a way that achieves the best F1 for each
TAC slot. We allow a system slot to map to mul-
tiple TAC slots, due to potential overlaps in entities
1-best 5-best
Systems P R F1 P R F1
PROFINDER 24 25 24 21 38 27
C&J 58 6.1 11 50 12 20
Table 2: Results on TAC 2010 entity extraction with N -
best mapping for N = 1 and N = 5. Intermediate values
of N produce intermediate results, and are not shown for
brevity.
among TAC slots. For example, in a document about
a tsunami, earthquake may appear both in the WHAT
slot as a disaster itself, and in the CAUSE slot as a
cause for the tsunami.
One salient difference between TAC and MUC
slots is that TAC slots are often more general than
MUC slots. For example, TAC slots such as WHY
and COUNTERMEASURES likely correspond to mul-
tiple slots at the granularity of MUC. As a result, we
also consider mapping the N -best system-induced
slots to each TAC slot, for N up to 5.
Experiments We trained PROFINDER and a reim-
plementation of C&J on the 920 full source texts of
TAC 2010, and tested them on the 368 model sum-
maries. We did not provide C&J?s model with access
to external data, in order to enable fair comparison
with our model. Since all of the summary sentences
are expected to be relevant, we did not conduct doc-
ument or sentence relevance classification in C&J or
PROFINDER. We tuned all parameters by two-fold
cross validation on the summaries. We computed the
overall precision, recall, and F1 by taking a micro-
average over the results for each TAC slot.
Results The results are shown in Table 2.
PROFINDER substantially outperformed C&J in F1,
in both 1-best and N -best cases. As in MUC-4, the
precision of C&J is higher, partly because C&J often
did not do much in clustering and produced many
small clusters. For example, in the 1-best setting, the
average number of entities mapped to each TAC slot
by C&J is 21, whereas it is 208 for PROFINDER. For
both systems, the results are generally lower com-
pared to that in MUC-4, which is expected since this
task is harder given the greater diversity in frames
and slots to be induced.
844
6 Conclusion
We have presented PROFINDER, the first probabilis-
tic approach to frame induction and shown that it
achieves state-of-the-art results on end-to-end entity
extraction in standard MUC and TAC data sets. Our
model is inspired by recent advances in unsuper-
vised semantic induction and content modeling in
summarization. Our probabilistic approach makes
it easy to extend the model with additional linguistic
insights and prior knowledge. While we have made
a case for unsupervised methods and the importance
of robustness across domains, our method is also
amenable to semi-supervised or supervised learn-
ing if annotated data is available. In future work,
we would like to further investigate frame induction
evaluation, particularly in evaluating event cluster-
ing.
Acknowledgments
We would like to thank Nate Chambers for answer-
ing questions about his system. We would also like
to thank Chris Quirk for help with preprocessing the
MUC corpus, and the members of the NLP group at
Microsoft Research for useful discussions.
Appendix A. Hyperparameter Settings
We document below the hyperparameter settings for
PROFINDER that were used to generate the results
in the paper.
Hyperparameter MUC TAC
Number of frames, |F| 9 8
Frame stickiness, ? 0.125 0.5
Smoothing (frames, events, slots) 0.5 2
Smoothing (emissions) 0.05 0.2
Number of split-merge cycles 4 2
Iterations per cycle 10 10
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 17th International Conference on Compu-
tational linguistics.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. Pro-
ceedings of ACL-08: HLT, pages 28?36.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004.
David Bean and Ellen Riloff. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference reso-
lution. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of ACL-08: HLT, pages 789?797, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 976?986, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Jackie C. K. Cheung and Xiao Li. 2012. Sequence clus-
tering and labeling for unsupervised query intent dis-
covery. In Proceedings of the 5th ACM International
Conference on Web Search and Data Mining, pages
383?392.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
Query-Focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 305?312, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions, pages 207?
214, Sydney, Australia, July. Association for Compu-
tational Linguistics.
845
Charles J. Fillmore. 1982. Frame semantics. Linguistics
in the Morning Calm, pages 111?137.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2005. English gigaword second edition. Linguistic
Data Consortium, Philadelphia.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic markov models. Artificial Intelligence
and Statistics (AISTATS).
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362?370, Boulder, Colorado, June. Association
for Computational Linguistics.
Ruihong Huang and Ellen Riloff. 2012. Bootstrapped
training of event extraction classifiers. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 286?295, Avignon, France, April. Association
for Computational Linguistics.
Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 1117?1126, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Percy Liang and Dan Klein. 2009. Online EM for un-
supervised models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 611?619, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
Marvin Minsky. 1974. A framework for representing
knowledge. Technical report, Cambridge, MA, USA.
1992. Proceedings of the Fourth Message Understanding
Conference (MUC-4). Morgan Kaufmann.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004, volume 2004, pages 145?152.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC 2010
guided summarization task guidelines.
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 717?
727, Prague, Czech Republic, June. Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Stephen D. Richardson, William B. Dolan, and Lucy Van-
derwende. 1998. MindNet: Acquiring and structuring
semantic information from text. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Conference
on Computational Linguistics, Volume 2, pages 1098?
1102, Montreal, Quebec, Canada, August. Association
for Computational Linguistics.
David Rumelhart, 1975. Notes on a schema for stories,
pages 211?236. Academic Press, Inc.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry Into Hu-
man Knowledge Structures. Lawrence Erlbaum, July.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
New York City, USA, June. Association for Computa-
tional Linguistics.
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labelling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 95?102, Barcelona, Spain, July. Association for
Computational Linguistics.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Multidoc-
ument summarization via information extraction. In
Proceedings of the First International Conference on
Human Language Technology Research. Association
for Computational Linguistics.
846
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 186?195,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Entity-based local coherence modelling using topological fields
Jackie Chi Kit Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
One goal of natural language generation is
to produce coherent text that presents in-
formation in a logical order. In this pa-
per, we show that topological fields, which
model high-level clausal structure, are an
important component of local coherence
in German. First, we show in a sen-
tence ordering experiment that topologi-
cal field information improves the entity
grid model of Barzilay and Lapata (2008)
more than grammatical role and simple
clausal order information do, particularly
when manual annotations of this informa-
tion are not available. Then, we incor-
porate the model enhanced with topolog-
ical fields into a natural language gen-
eration system that generates constituent
orders for German text, and show that
the added coherence component improves
performance slightly, though not statisti-
cally significantly.
1 Introduction
One type of coherence modelling that has captured
recent research interest is local coherence mod-
elling, which measures the coherence of a docu-
ment by examining the similarity between neigh-
bouring text spans. The entity-based approach,
in particular, considers the occurrences of noun
phrase entities in a document (Barzilay and Lap-
ata, 2008). Local coherence modelling has been
shown to be useful for tasks like natural language
generation and summarization, (Barzilay and Lee,
2004) and genre classification (Barzilay and Lap-
ata, 2008).
Previous work on English, a language with rel-
atively fixed word order, has identified factors that
contribute to local coherence, such as the gram-
matical roles associated with the entities. There is
good reason to believe that the importance of these
factors vary across languages. For instance, freer-
word-order languages exhibit word order patterns
which are dependent on discourse factors relating
to information structure, in addition to the gram-
matical roles of nominal arguments of the main
verb. We thus expect word order information to be
particularly important in these languages in dis-
course analysis, which includes coherence mod-
elling.
For example, Strube and Hahn (1999) introduce
Functional Centering, a variant of Centering The-
ory which utilizes information status distinctions
between hearer-old and hearer-new entities. They
apply their model to pronominal anaphora reso-
lution, identifying potential antecedents of sub-
sequent anaphora by considering syntactic and
word order information, classifying constituents
by their familiarity to the reader. They find that
their approach correctly resolves more pronomi-
nal anaphora than a grammatical role-based ap-
proach which ignores word order, and the differ-
ence between the two approaches is larger in Ger-
man corpora than in English ones. Unfortunately,
their criteria for ranking potential antecedents re-
quire complex syntactic information in order to
classify whether proper names are known to the
hearer, which makes their algorithm hard to auto-
mate. Indeed, all evaluation is done manually.
We instead use topological fields, a model of
clausal structure which is indicative of information
structure in German, but shallow enough to be au-
tomatically parsed at high accuracy. We test the
hypothesis that they would provide a good com-
plement or alternative to grammatical roles in lo-
cal coherence modelling. We show that they are
superior to grammatical roles in a sentence or-
dering experiment, and in fact outperforms sim-
ple word-order information as well. We further
show that these differences are particularly large
when manual syntactic and grammatical role an-
186
Millionen von Mark   verschwendet   der Senat jeden Monat,   weil   er   sparen will. 
LK MF VCVF LK MF
S
NF
S
?The senate wastes millions of marks each month, because it wants to save.?
Figure 1: The clausal and topological field structure of a German sentence. Notice that the subordinate
clause receives its own topology.
notations are not available.
We then embed these topological field annota-
tions into a natural language generation system to
show the utility of local coherence information in
an applied setting. We add contextual features
using topological field transitions to the model
of Filippova and Strube (2007b) and achieve a
slight improvement over their model in a con-
stituent ordering task, though not statistically sig-
nificantly. We conclude by discussing possible
reasons for the utility of topological fields in lo-
cal coherence modelling.
2 Background and Related Work
2.1 German Topological Field Parsing
Topological fields are sequences of one or more
contiguous phrases found in an enclosing syntac-
tic region, which is the clause in the case of the
German topological field model (Ho?hle, 1983).
These fields may have constraints on the number
of words or phrases they contain, and do not nec-
essarily form a semantically coherent constituent.
In German, the topology serves to identify all of
the components of the verbal head of a clause, as
well as clause-level structure such as complemen-
tizers and subordinating conjunctions. Topologi-
cal fields are a useful abstraction of word order,
because while Germanic word order is relatively
free with respect to grammatical functions, the or-
der of the topological fields is strict and unvarying.
A German clause can be considered to be an-
chored by two ?brackets? which contain modals,
verbs and complementizers. The left bracket (linke
Klammer, LK) may contain a complementizer,
subordinating conjunction, or a finite verb, de-
pending on the clause type, and the right bracket
contains the verbal complex (VC). The other topo-
logical fields are defined in relation to these two
brackets, and contain all other parts of the clause
such as verbal arguments, adjuncts, and discourse
cues.
The VF (Vorfeld or ?pre-field?) is so-named be-
cause it occurs before the left bracket. As the first
constituent of most matrix clauses in declarative
sentences, it has special significance for the coher-
ence of a passage, which we will further discuss
below. The MF (Mittelfeld or ?middle field?) is
the field bounded by the two brackets. Most verb
arguments, adverbs, and prepositional phrases are
found here, unless they have been fronted and put
in the VF, or are prosodically heavy and postposed
to the NF field. The NF (Nachfeld or ?post-field?)
contains prosodically heavy elements such as post-
posed prepositional phrases or relative clauses,
and occasionally postposed noun phrases.
2.2 The Role of the Vorfeld
One of the reasons that we use topological fields
for local coherence modelling is the role that the
VF plays in signalling the information structure of
German clauses, as it often contains the topic of
the sentence.
In fact, its role is much more complex than be-
ing simply the topic position. Dipper and Zins-
meister (2009) distinguish multiple uses of the VF
depending on whether it contains an element re-
lated to the surrounding discourse. They find that
45.1% of VFs are clearly related to the previous
context by a reference or discourse relation, and a
further 21.9% are deictic and refer to the situation
described in the passage in a corpus study. They
also run a sentence insertion experiment where
subjects are asked to place an extracted sentence
in its original location in a passage. The authors
remark that extracted sentences with VFs that are
referentially related to previous context (e.g., they
contain a coreferential noun phrase or a discourse
relation like ?therefore?) are reinserted at higher
accuracies.
187
a)
# Original Sentence and Translation
1
Einen Zufluchtsort fu?r Frauen, die von ihren Ma?nnern mi?handelt werden, gibt es nunmehr auch
in Treptow.
?There is now a sanctuary for women who are mistreated by their husbands in Treptow as well.?
2
Das Bezirksamt bietet Frauen (auch mit Kindern) in derartigen Notsituationen voru?bergehend
eine Unterkunft.
?The district office offers women (even with children) in this type of emergency temporary
accommodation.?
3
Zugleich werden die Betroffenen der Regelung des Unterhalts, bei Beho?rdenga?ngen und auch
bei der Wohnungssuche unterstu?tzt.
?At the same time, the affected are supported with provisions of necessities, in dealing with
authorities, and also in the search for new accommodations.?
b)
DE Zufluchtsort Frauen Ma?nnern Treptow Kindern
EN sanctuary women husbands Treptow children
1 acc oth oth oth ?
2 ? oth ? ? oth
3 ? nom ? ? ?
c)
? ? ? nom ? acc ? oth nom ? nom nom nom acc nom oth
0.3 0.0 0.0 0.1 0.0 0.0 0.0 0.0
acc ? acc nom acc acc acc oth oth ? oth nom oth acc oth oth
0.1 0.0 0.0 0.0 0.3 0.1 0.0 0.1
Table 1: a) An example of a document from Tu?Ba-D/Z, b) an abbreviated entity grid representation of
it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two.
Mentions of the entity Frauen are underlined. nom: nominative, acc: accusative, oth: dative, oblique,
and other arguments
Filippova and Strube (2007c) also examine the
role of the VF in local coherence and natural lan-
guage generation, focusing on the correlation be-
tween VFs and sentential topics. They follow Ja-
cobs (2001) in distinguishing the topic of addres-
sation, which is the constituent for which the
proposition holds, and frame-setting topics, which
is the domain in which the proposition holds, such
as a temporal expression. They show in a user
study that frame-setting topics are preferred to top-
ics of addressation in the VF, except when a con-
stituent needs to be established as the topic of ad-
dressation.
2.3 Using Entity Grids to Model Local
Coherence
Barzilay and Lapata (2008) introduce the entity
grid as a method of representing the coherence of a
document. Entity grids indicate the location of the
occurrences of an entity in a document, which is
important for coherence modelling because men-
tions of an entity tend to appear in clusters of
neighbouring or nearby sentences in coherent doc-
uments. This last assumption is adapted from Cen-
tering Theory approaches to discourse modelling.
In Barzilay and Lapata (2008), an entity grid is
constructed for each document, and is represented
as a matrix in which each row represents a sen-
tence, and each column represents an entity. Thus,
a cell in the matrix contains information about an
entity in a sentence. The cell is marked by the
presence or absence of the entity, and can also be
augmented with other information about the en-
tity in this sentence, such as the grammatical role
of the noun phrase representing that entity in that
sentence, or the topological field in which the noun
phrase appears.
Consider the document in Table 1. An entity
grid representation which incorporates the syntac-
tic role of the noun phrase in which the entity ap-
188
pears is also shown (not all entities are listed for
brevity). We tabulate the transitions of entities be-
tween different syntactic positions (or their non-
occurrence) in sentences, and convert the frequen-
cies of transitions into a feature vector representa-
tion of transition probabilities in the document.
To calculate transition probabilities, we divide
the frequency of a particular transition by the total
number of transitions of that length.
This model of local coherence was investigated
for German by Filippova and Strube (2007a). The
main focus of that work, however, was to adapt
the model for use in a low-resource situation when
perfect coreference information is not available.
This is particularly useful in natural language un-
derstanding tasks. They employ a semantic clus-
tering model to relate entities. In contrast, our
work focuses on improving performance by anno-
tating entities with additional linguistic informa-
tion, such as topological fields, and is geared to-
wards natural language generation systems where
perfect information is available.
Similar models of local coherence include vari-
ous Centering Theory accounts of local coherence
((Kibble and Power, 2004; Poesio et al, 2004)
inter alia). The model of Elsner and Charniak
(2007) uses syntactic cues to model the discourse-
newness of noun phrases. There are also more
global content models of topic shifts between sen-
tences like Barzilay and Lee (2004).
3 Sentence Ordering Experiments
3.1 Method
We test a version of the entity grid representa-
tion augmented with topological fields in a sen-
tence ordering experiment corresponding to Ex-
periment 1 of Barzilay and Lapata (2008). The
task is a binary classification task to identify the
original version of a document from another ver-
sion which contains the sentences in a randomly
permuted order, which is taken to be incoherent.
We solve this problem in a supervised machine
learning setting, where the input is the feature vec-
tor representations of the two versions of the doc-
ument, and the output is a binary value indicating
the document with the original sentence ordering.
We use SVMlight?s ranking module for classifi-
cation (Joachims, 2002).
The corpus in our experiments consists of the
last 480 documents of Tu?Ba-D/Z version 4 (Telljo-
hann et al, 2004), which contains manual corefer-
ence, grammatical role and topological field infor-
mation. This set is larger than the set that was used
in Experiment 1 of Barzilay and Lapata (2008),
which consists of 400 documents in two English
subcorpora on earthquakes and accidents respec-
tively. The average document length in the Tu?Ba-
D/Z subcorpus is also greater, at 19.2 sentences
compared to about 11 for the two subcorpora. Up
to 20 random permutations of sentences were gen-
erated from each document, with duplicates re-
moved.
There are 216 documents and 4126 original-
permutation pairs in the training set, and 24 docu-
ments and 465 pairs in the development set. The
remaining 240 documents are in the final test set
(4243 pairs). The entity-based model is parame-
terized as follows.
Transition length ? the maximum length of the
transitions used in the feature vector representa-
tion of a document.
Representation ? when marking the presence of
an entity in a sentence, what information about
the entity is marked (topological field, grammat-
ical role, or none). We will describe the represen-
tations that we try in section 3.2.
Salience ? whether to set a threshold for the fre-
quency of occurrence of entities. If this is set, all
entities below a certain frequency are treated sep-
arately from those reaching this frequency thresh-
old when calculating transition probabilities. In
the example in Table 1, with a salience thresh-
old of 2, Frauen would be treated separately from
Ma?nnern or Kindern.
Transition length, salience, and a regularization
parameter are tuned on the development set. We
only report results using the setting of transition
length ? 4, and no salience threshold, because
they give the best performance on the development
set. This is in contrast to the findings of Barzi-
lay and Lapata (2008), who report that transition
length ? 3 and a salience threshold of 2 perform
best on their data.
3.2 Entity Representations
The main goal of this study is to compare word
order, grammatical role and topological field in-
formation, which is encoded into the entity grid at
each occurrence of an entity. Here, we describe
the variants of the entity representations that we
compare.
189
Baseline Representations We implement sev-
eral baseline representations against which we test
our topological field-enhanced model. The sim-
plest baseline representation marks the mere ap-
pearance of an entity without any additional infor-
mation, which we refer to as default.
Another class of baseline representations mark
the order in which entities appear in the clause.
The correlation between word order and informa-
tion structure is well known, and has formed the
basis of some theories of syntax such as the Prague
School?s (Sgall et al, 1986). The two versions
of clausal order we tried are order 1/2/3+,
which marks a noun phrase as the first, the sec-
ond, or the third or later to appear in a clause, and
order 1/2+, which marks a noun phrase as the
first, or the second or later to appear in a clause.
Since noun phrases can be embedded in other
noun phrases, overlaps can occur. In this case, the
dominating noun phrase takes the smallest order
number among its dominated noun phrases.
The third class of baseline representations we
employ mark an entity by its grammatical role
in the clause. Barzilay and Lapata (2008) found
that grammatical role improves performance in
this task for an English corpus. Because Ger-
man distinguishes more grammatical roles mor-
phologically than English, we experiment with
various granularities of role labelling. In particu-
lar, subj/obj distinguishes the subject position,
the object position, and another category for all
other positions. cases distinguishes five types of
entities corresponding to the four morphological
cases of German in addition to another category
for noun phrases which are not complements of
the main verb.
Topological Field-Based These representations
mark the topological field in which an entity ap-
pears. Some versions mark entities which are
prepositional objects separately. We try versions
which distinguish VF from non-VF, as well as
more general versions that make use of a greater
set of topological fields. vfmarks the noun phrase
as belonging to a VF (and not in a PP) or not.
vfpp is the same as above, but allows preposi-
tional objects inside the VF to be marked as VF.
topf/pp distinguishes entities in the topological
fields VF, MF, and NF, contains a separate cat-
egory for PP, and a category for all other noun
phrases. topf distinguishes between VF, MF, and
NF, on the one hand, and everything else on the
other. Prepositional objects are treated the same
as other noun phrases here.
Combined We tried a representation which
combines grammatical role and topological field
into a single representation, subj/obj?vf,
which takes the Cartesian product of subj/obj
and vf above.
Topological fields do not map directly to topic-
focus distinctions. For example, besides the topic
of the sentence, the Vorfeld may contain discourse
cues, expletive pronouns, or the informational or
contrastive focus. Furthermore, there are addi-
tional constraints on constituent order related to
pronominalization. Thus, we devised additional
entity representations to account for these aspects
of German.
topic attempts to identify the sentential topic
of a clause. A noun phrase is marked as TOPIC
if it is in VF as in vfpp, or if it is the first
noun phrase in MF and also the first NP in the
clause. Other noun phrases in MF are marked
as NONTOPIC. Categories for NF and miscella-
neous noun phrases also exist. While this repre-
sentation may appear to be very similar to sim-
ply distinguishing the first entity in a clause as for
order 1/2+ in that TOPIC would correspond
to the first entity in the clause, they are in fact dis-
tinct. Due to issues related to coordination, appos-
itive constructions, and fragments which do not
receive a topology of fields, the first entity in a
clause is labelled the TOPIC only 80.8% of the
time in the corpus. This representation also distin-
guishes NFs, which clausal order does not.
topic+pron refines the above by taking into
account a word order restriction in German that
pronouns appear before full noun phrases in the
MF field. The following set of decisions repre-
sents how a noun phrase is marked: If the first NP
in the clause is a pronoun in an MF field and is the
subject, we mark it as TOPIC. If it is not the sub-
ject, we mark it as NONTOPIC. For other NPs, we
follow the topic representation.
3.3 Automatic annotations
While it is reasonable to assume perfect annota-
tions of topological fields and grammatical roles in
many NLG contexts, this assumption may be less
appropriate in other applications involving text-to-
text generation where the input to the system is
text such as paraphrasing or machine translation.
Thus, we test the robustness of the entity repre-
190
Representation Manual Automatic
topf/pp 94.44 94.89
topic 94.13 94.53
topic+pron 94.08 94.51
topf 93.87 93.11
subj/obj 93.831 91.7++
cases 93.312 90.93++
order 1/2+ 92.51++ 92.1+
subj/obj?vf 92.32++ 90.74++
default 91.42++ 91.42++
vfpp 91.37++ 91.68++
vf 91.21++ 91.16++
order 1/2/3+ 91.16++ 90.71++
Table 2: Accuracy (%) of the permutation de-
tection experiment with various entity represen-
tations using manual and automatic annotations
of topological fields and grammatical roles. The
baseline without any additional annotation is un-
derlined. Two-tailed sign tests were calculated for
each result against the best performing model in
each column (1: p = 0.101; 2: p = 0.053; +: statis-
tically significant, p < 0.05; ++: very statistically
significant, p < 0.01 ).
sentations to automatic extraction in the absence
of manual annotations. We employ the following
two systems for extracting topological fields and
grammatical roles.
To parse topological fields, we use the Berke-
ley parser of Petrov and Klein (2007), which has
been shown to perform well at this task (Cheung
and Penn, 2009). The parser is trained on sections
of Tu?Ba-D/Z which do not overlap with the sec-
tion from which the documents for this experiment
were drawn, and obtains an overall parsing per-
formance of 93.35% F1 on topological fields and
clausal nodes without gold POS tags on the section
of Tu?Ba-D/Z it was tested on.
We tried two methods to obtain grammatical
roles. First, we tried extracting grammatical roles
from the parse trees which we obtained from the
Berkeley parser, as this information is present in
the edge labels that can be recovered from the
parse. However, we found that we achieved bet-
ter accuracy by using RFTagger (Schmid and
Laws, 2008), which tags nouns with their morpho-
logical case. Morphological case is distinct from
grammatical role, as noun phrases can function as
adjuncts in possessive constructions and preposi-
Annotation Accuracy (%)
Grammatical role 83.6
Topological field (+PP) 93.8
Topological field (?PP) 95.7
Clausal order 90.8
Table 3: Accuracy of automatic annotations of
noun phrases with coreferents. +PP means that
prepositional objects are treated as a separate cate-
gory from topological fields. ?PP means they are
treated as other noun phrases.
tional phrases. However, we can approximate the
grammatical role of an entity using the morpho-
logical case. We follow the annotation conven-
tions of Tu?Ba-D/Z in not assigning a grammati-
cal role when the noun phrase is a prepositional
object. We also do not assign a grammatical role
when the noun phrase is in the genitive case, as
genitive objects are very rare in German and are
far outnumbered by the possessive genitive con-
struction.
3.4 Results
Table 2 shows the results of the sentence ordering
permutation detection experiment. The top four
performing entity representations are all topologi-
cal field-based, and they outperform grammatical
role-based and simple clausal order-based mod-
els. These results indicate that the information
that topological fields provide about clause struc-
ture, appositives, right dislocation, etc. which is
not captured by simple clausal order is important
for coherence modelling. The representations in-
corporating linguistics-based heuristics do not out-
perform purely topological field-based models.
Surprisingly, the VF-based models fare quite
poorly, performing worse than not adding any an-
notations, despite the fact that topological field-
based models in general perform well. This result
may be a result of the heterogeneous uses of the
VF.
The automatic topological field annotations are
more accurate than the automatic grammatical role
annotations (Table 3), which may partly explain
why grammatical role-based models suffer more
when using automatic annotations. Note, how-
ever, that the models based on automatic topolog-
ical field annotations outperform even the gram-
matical role-based models using manual annota-
tion (at marginal significance, p < 0.1). The topo-
191
logical field annotations are accurate enough that
automatic annotations produce no decrease in per-
formance.
These results show the upper bound of entity-
based local coherence modelling with perfect
coreference information. The results we obtain
are higher than the results for the English cor-
pora of Barzilay and Lapata (2008) (87.2% on the
Earthquakes corpus and 90.4% on the Accidents
corpus), but this is probably due to corpus differ-
ences as well as the availability of perfect corefer-
ence information in our experiments1.
Due to the high performance we obtained, we
calculated Kendall tau coefficients (Lapata, 2006)
over the sentence orderings of the cases in which
our best performing model is incorrect, to deter-
mine whether the remaining errors are instances
where the permuted ordering is nearly identical to
the original ordering. We obtained a ? of 0.0456
in these cases, compared to a ? of ?0.0084 for all
the pairs, indicating that this is not the case.
To facilitate comparison to the results of Filip-
pova and Strube (2007a), we rerun this experiment
on the same subsections of the corpus as in that
work for training and testing. The first 100 arti-
cles of Tu?Ba-D/Z are used for testing, while the
next 200 are used for training and development.
Unlike the previous experiments, we do not do
parameter tuning on this set of data. Instead, we
follow Filippova and Strube (2007a) in using tran-
sition lengths of up to three. We do not put in
a salience threshold. We see that our results are
much better than the ones reported in that work,
even for the default representation. The main
reason for this discrepancy is probably the way
that entities are created from the corpus. In our
experiments, we create an entity for every single
noun phrase node that we encounter, then merge
the entities that are linked by coreference. Filip-
pova and Strube (2007a) convert the annotations
of Tu?Ba-D/Z into a dependency format, then ex-
tract entities from the noun phrases found there.
They may thus annotate fewer entities, as there
1Barzilay and Lapata (2008) use the coreference sys-
tem of Ng and Cardie (2002) to obtain coreference anno-
tations. We are not aware of similarly well-tested, pub-
licly available coreference resolution systems that handle all
types of anaphora for German. We considered adapting the
BART coreference resolution toolkit (Versley et al, 2008) to
German, but a number of language-dependent decisions re-
garding preprocessing, feature engineering, and the learning
paradigm would need to be made in order to achieve rea-
sonable performance comparable to state-of-the-art English
coreference resolution systems.
Representation Accuracy (%)
topf/pp 93.83
topic 93.31
topic+pron 93.31
topf 92.49
subj/obj 88.99
order 1/2+ 88.89
order 1/2/3+ 88.84
cases 88.63
vf 87.60
vfpp 88.17
default 87.55
subj/obj?vf 87.71
(Filippova and Strube, 2007) 75
Table 4: Accuracy (%) of permutation detection
experiment with various entity representations us-
ing manual and automatic annotations of topolog-
ical fields and grammatical roles on subset of cor-
pus used by Filippova and Strube (2007a).
may be nested NP nodes in the original corpus.
There may also be noise in the dependency con-
version process.
The relative rankings of different entity repre-
sentations in this experiment are similar to the
rankings of the previous experiment, with topolog-
ical field-based models outperforming grammati-
cal role and clausal order models.
4 Local Coherence for Natural Language
Generation
One of the motivations of the entity grid-based
model is to improve surface realization decisions
in NLG systems. A typical experimental design
would pass the contents of the test section of a
corpus as input to the NLG system with the order-
ing information stripped away. The task is then to
regenerate the ordering of the information found
in the original corpus. Various coherence models
have been tested in corpus-based NLG settings.
For example, Karamanis et al (2009) compare
several versions of Centering Theory-based met-
rics of coherence on corpora by examining how
highly the original ordering found in the corpus
is ranked compared to other possible orderings of
propositions. A metric performs well if it ranks
the original ordering better than the alternative or-
derings.
In our next experiment, we incorporate local co-
192
herence information into the system of Filippova
and Strube (2007b). We embed entity topologi-
cal field transitions into their probabilistic model,
and show that the added coherence component
slightly improves the performance of the baseline
NLG system in generating constituent orderings in
a German corpus, though not to a statistically sig-
nificant degree.
4.1 Method
We use the WikiBiography corpus2 for our exper-
iments. The corpus consists of more than 1100 bi-
ographies taken from the German Wikipedia, and
contains automatic annotations of morphological,
syntactic, and semantic information. Each article
also contains the coreference chain of the subject
of the biography (the biographee). The first 100
articles are used for testing, the next 200 for de-
velopment, and the rest for training.
The baseline generation system already incor-
porates topological field information into the con-
stituent ordering process. The system operates in
two steps. First, in main clauses, one constituent
is selected as the Vorfeld (VF). This is done us-
ing a maximum entropy model (call it MAXENT).
Then, the remaining constituents are ordered using
a second maximum entropy model (MAXENT2).
Significantly, Filippova and Strube (2007b) found
that selecting the VF first, and then ordering the
remaining constituents results in a 9% absolute
improvement over the corresponding model where
the selection is performed in one step by the sort-
ing algorithm alone.
The maximum entropy model for both steps rely
on the following features:
? features on the voice, valency, and identity of
the main verb of the clause
? features on the morphological and syntactic
status of the constituent to be ordered
? whether the constituent occurs in the preced-
ing sentence
? features for whether the constituent contains
a determiner, an anaphoric pronoun, or a rel-
ative clause
? the size of the constituent in number of mod-
ifiers, in depth, and in number of words
2http://www.eml-research.de/english/
research/nlp/download/wikibiography.php
? the semantic class of the constituent (per-
son, temporal, location, etc.) The biographee,
in particular, is marked by its own semantic
class.
In the first VF selection step, MAXENT simply
produces a probability of each constituent being a
VF, and the constituent with the highest probabil-
ity is selected. In the second step, MAXENT2 takes
the featural representation of two constituents, and
produces an output probability of the first con-
stituent preceding the second constituent. The fi-
nal ordering is achieved by first randomizing the
order of the constituents in a clause (besides the
first one, which is selected to be the VF), then
sorting them according to the precedence proba-
bilities. Specifically, a constituent A is put before
a constituent B if MAXENT2(A,B) > 0.5. Because
this precedence relation is not antisymmetric (i.e.,
MAXENT2(A,B) > 0.5 and MAXENT2(B,A) >
0.5 may be simultaneously true or simultaneously
false), different initializations of the order pro-
duce different sorted results. In our experiments,
we correct this by defining the precedence rela-
tion to be A precedes B iff MAXENT2(A,B) >
MAXENT2(B,A). This change does not greatly im-
pact the performance, and removes the random-
ized element of the algorithm.
The baseline system does not directly model the
context when ordering constituents. All of the
features but one in the original maximum entropy
models rely on local properties of the clause. We
incorporate local coherence information into the
model by adding entity transition features which
we found to be useful in the sentence ordering ex-
periment in Section 3 above.
Specifically, we add features indicating the
topological fields in which entities occur in the
previous sentences. We found that looking back
up to two sentences produces the best results (by
tuning on the development set). Because this cor-
pus does not come with general coreference in-
formation except for the coreference chain of the
biographee, we use the semantic classes instead.
So, all constituents in the same semantic class are
treated as one coreference chain. An example of a
feature may be biog-last2, which takes on a value
such as ?v??, meaning that this constituent refers
to the biographee, and the biographee occurs in
the VF two clauses ago (v), but does not appear in
the previous clause (?). For a constituent which is
not the biographee, this feature would be marked
193
Method VF Acc (%) Acc (%) Tau
Baseline 68.7 60.9 0.72
+Coherence 69.2 61.5 0.72
Table 5: Results of adding coherence features into
a natural language generation system. VF Acc%
is the accuracy of selecting the first constituent in
main clauses. Acc % is the percentage of per-
fectly ordered clauses, tau is Kendall?s ? on the
constituent ordering. The test set contains 2246
clauses, of which 1662 are main clauses.
?na? (not applicable).
4.2 Results
Table 5 shows the results of adding these contex-
tual features into the maximum entropy models.
We see that we obtain a small improvement in the
accuracy of VF selection, and in the accuracy of
correctly ordering the entire clause. These im-
provements are not statistically significant by Mc-
Nemar?s test. We suggest that the lack of coref-
erence information for all entities in the article
may have reduced the benefit of the coherence
component. Also, the topline of performance is
substantially lower than 100%, as multiple order-
ings are possible and equally valid. Human judge-
ments on information structuring for both inter-
and intra-sentential units are known to have low
agreement (Barzilay et al, 2002; Filippova and
Strube, 2007c; Lapata, 2003; Chen et al, 2007).
Thus, the relative error reduction is higher than the
absolute reduction might suggest.
5 Conclusions
We have shown that topological fields are a use-
ful source of information for local coherence mod-
elling. In a sentence-order permutation detection
task, models which use topological field infor-
mation outperform both grammatical role-based
models and models based on simple clausal or-
der, with the best performing model achieving a
relative error reduction of 40.4% over the original
baseline without any additional annotation. Ap-
plying our local coherence model in another set-
ting, we have embedded topological field transi-
tions of entities into an NLG system which orders
constituents in German clauses. We find that the
coherence-enhanced model slightly outperforms
the baseline system, but this was not statistically
significant.
We suggest that the utility of topological fields
in local coherence modelling comes from the in-
teraction between word order and information
structure in freer-word-order languages. Crucially,
topological fields take into account issues such
as coordination, appositives, sentential fragments
and differences in clause types, which word or-
der alone does not. They are also shallow enough
to be accurately parsed automatically for use in
resource-poor applications.
Further refinement of the topological field an-
notations to take advantage of the fact that they
do not correspond neatly to any single information
status such as topic or focus could provide addi-
tional performance gains. The model also shows
promise for other discourse-related tasks such as
coreference resolution and discourse parsing.
Acknowledgements
We are grateful to Katja Filippova for providing us
with source code for the experiments in Section 4
and for answering related questions, and to Tim-
othy Fowler for useful discussions and comments
on a draft of the paper. This work is supported in
part by the Natural Sciences and Engineering Re-
search Council of Canada.
References
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to gen-
eration and summarization. In Proc. HLT-NAACL
2004, pages 113?120.
R. Barzilay, N. Elhadad, and K. McKeown. 2002. In-
ferring strategies for sentence ordering in multidoc-
ument news summarization. Journal of Artificial In-
telligence Research, 17:35?55.
E. Chen, B. Snyder, and R. Barzilay. 2007. Incremen-
tal text structuring with online hierarchical ranking.
In Proceedings of EMNLP, pages 83?91.
J.C.K. Cheung and G. Penn. 2009. Topological Field
Parsing of German. In Proc. 47th ACL and 4th IJC-
NLP, pages 64?72. Association for Computational
Linguistics.
S. Dipper and H. Zinsmeister. 2009. The Role of
the German Vorfeld for Local Coherence: A Pi-
lot Study. In Proceedings of the Conference of the
German Society for Computational Linguistics and
Language Technology (GSCL), pages 69?79. Gunter
Narr.
194
M. Elsner and E. Charniak. 2007. A generative
discourse-new model for text coherence. Technical
report, Technical Report CS-07-04, Brown Univer-
sity.
K. Filippova and M. Strube. 2007a. Extending the
entity-grid coherence model to semantically related
entities. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
139?142. Association for Computational Linguis-
tics.
K. Filippova and M. Strube. 2007b. Generating con-
stituent order in German clauses. In Proc. 45th ACL,
pages 320?327.
K. Filippova and M. Strube. 2007c. The German Vor-
feld and Local Coherence. Journal of Logic, Lan-
guage and Information, 16(4):465?485.
T.N. Ho?hle. 1983. Topologische Felder. Ph.D. thesis,
Ko?ln.
J. Jacobs. 2001. The dimensions of topiccomment.
Linguistics, 39(4):641?681.
T. Joachims. 2002. Learning to Classify Text Using
Support Vector Machines. Kluwer.
N. Karamanis, C. Mellish, M. Poesio, and J. Oberlan-
der. 2009. Evaluating centering for information or-
dering using corpora. Computational Linguistics,
35(1):29?46.
R. Kibble and R. Power. 2004. Optimizing referential
coherence in text generation. Computational Lin-
guistics, 30(4):401?416.
M. Lapata. 2003. Probabilistic text structuring: Exper-
iments with sentence ordering. In Proc. 41st ACL,
pages 545?552.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendall?s tau. Computational Linguistics,
32(4):471?484.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc.
40th ACL, pages 104?111.
S. Petrov and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
M. Poesio, R. Stevenson, B.D. Eugenio, and J. Hitze-
man. 2004. Centering: A parametric theory
and its instantiations. Computational Linguistics,
30(3):309?363.
H. Schmid and F. Laws. 2008. Estimation of condi-
tional probabilities with decision trees and an appli-
cation to fine-grained POS tagging. In Proc. 22nd
COLING, pages 777?784. Association for Compu-
tational Linguistics.
P. Sgall, E. Hajic?ova?, J. Panevova?, and J. Mey. 1986.
The meaning of the sentence in its semantic and
pragmatic aspects. Springer.
M. Strube and U. Hahn. 1999. Functional center-
ing: Grounding referential coherence in information
structure. Computational Linguistics, 25(3):309?
344.
H. Telljohann, E. Hinrichs, and S. Kubler. 2004.
The Tu?Ba-D/Z treebank: Annotating German with
a context-free backbone. In Proc. Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), pages 2229?2235.
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proc. 46th ACL-HLT Demo Session, pages 9?12.
Association for Computational Linguistics.
195
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392?401,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Probabilistic Domain Modelling With Contextualized Distributional
Semantic Vectors
Jackie Chi Kit Cheung
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
jcheung@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
Generative probabilistic models have been
used for content modelling and template
induction, and are typically trained on
small corpora in the target domain. In
contrast, vector space models of distribu-
tional semantics are trained on large cor-
pora, but are typically applied to domain-
general lexical disambiguation tasks. We
introduce Distributional Semantic Hidden
Markov Models, a novel variant of a hid-
den Markov model that integrates these
two approaches by incorporating contex-
tualized distributional semantic vectors
into a generative model as observed emis-
sions. Experiments in slot induction show
that our approach yields improvements in
learning coherent entity clusters in a do-
main. In a subsequent extrinsic evalua-
tion, we show that these improvements are
also reflected in multi-document summa-
rization.
1 Introduction
Detailed domain knowledge is crucial to many
NLP tasks, either as an input for language un-
derstanding, or as the goal itself, to acquire such
knowledge. For example, in information extrac-
tion, a list of slots in the target domain is given
to the system, and in natural language generation,
content models are trained to learn the content
structure of texts in the target domain for infor-
mation structuring and automatic summarization.
Generative probabilistic models have been one
popular approach to content modelling. An impor-
tant advantage of this approach is that the structure
of the model can be adapted to fit the assumptions
about the structure of the domain and the nature
of the end task. As this field has progressed, the
formal structures that are assumed to represent a
domain have increased in complexity and become
more hierarchical. Earlier work assumes a flat set
of topics (Barzilay and Lee, 2004), which are ex-
pressed as states of a latent random variable in the
model. Later work organizes topics into a hierar-
chy from general to specific (Haghighi and Van-
derwende, 2009; Celikyilmaz and Hakkani-Tur,
2010). Recently, Cheung et al (2013) formalized
a domain as a set of frames consisting of proto-
typical sequences of events, slots, and slot fillers
or entities, inspired by classical AI work such as
Schank and Abelson?s (1977) scripts. We adopt
much of this terminology in this work. For exam-
ple, in the CRIMINAL INVESTIGATIONS domain,
there may be events such as a murder, an investi-
gation of the crime, an arrest, and a trial. These
would be indicated by event heads such as kill, ar-
rest, charge, plead. Relevant slots would include
VICTIM, SUSPECT, AUTHORITIES, PLEA, etc.
One problem faced by this line of work is that,
by their nature, these models are typically trained
on a small corpus from the target domain, on the
order of hundreds of documents. The small size of
the training corpus makes it difficult to estimate re-
liable statistics, especially for more powerful fea-
tures such as higher-order N-gram features or syn-
tactic features.
By contrast, distributional semantic models are
trained on large, domain-general corpora. These
methods model word meaning using the contexts
in the training corpus in which the word appears.
The most popular approach today is a vector space
representation, in which each dimension corre-
sponds to some context word, and the value at that
dimension corresponds to the strength of the as-
sociation between the context word and the target
word being modelled. A notion of word similarity
arises naturally from these models by comparing
the similarity of the word vectors, for example by
using a cosine measure. Recently, these models
have been extended by considering how distribu-
392
tional representations can be modified depending
on the specific context in which the word appears
(Mitchell and Lapata, 2008, for example). Con-
textualization has been found to improve perfor-
mance in tasks like lexical substitution and word
sense disambiguation (Thater et al, 2011).
In this paper, we propose to inject contextual-
ized distributional semantic vectors into genera-
tive probabilistic models, in order to combine their
complementary strengths for domain modelling.
There are a number of potential advantages that
distributional semantic models offer. First, they
provide domain-general representations of word
meaning that cannot be reliably estimated from the
small target-domain corpora on which probabilis-
tic models are trained. Second, the contextualiza-
tion process allows the semantic vectors to implic-
itly encode disambiguated word sense and syntac-
tic information, without further adding to the com-
plexity of the generative model.
Our model, the Distributional Semantic Hidden
Markov Model (DSHMM), incorporates contextu-
alized distributional semantic vectors into a gen-
erative probabilistic model as observed emissions.
We demonstrate the effectiveness of our model in
two domain modelling tasks. First, we apply it to
slot induction on guided summarization data over
five different domains. We show that our model
outperforms a baseline version of our method that
does not use distributional semantic vectors, as
well as a recent state-of-the-art template induction
method. Then, we perform an extrinsic evaluation
using multi-document summarization, wherein we
show that our model is able to learn event and slot
topics that are appropriate to include in a sum-
mary. From a modelling perspective, these results
show that probabilistic models for content mod-
elling and template induction benefit from distri-
butional semantics trained on a much larger cor-
pus. From the perspective of distributional seman-
tics, this work broadens the variety of problems to
which distributional semantics can be applied, and
proposes methods to perform inference in a prob-
abilistic setting beyond geometric measures such
as cosine similarity.
2 Related Work
Probabilistic content models were proposed by
Barzilay and Lee (2004), and related models have
since become popular for summarization (Fung
and Ngai, 2006; Haghighi and Vanderwende,
2009), and information ordering (Elsner et al,
2007; Louis and Nenkova, 2012). Other related
generative models include topic models and struc-
tured versions thereof (Blei et al, 2003; Gruber
et al, 2007; Wallach, 2008). In terms of domain
learning in the form of template induction, heuris-
tic methods involving multiple clustering steps
have been proposed (Filatova et al, 2006; Cham-
bers and Jurafsky, 2011). Most recently, Cheung
et al (2013) propose PROFINDER, a probabilis-
tic model for frame induction inspired by content
models. Our work is similar in that we assume
much of the same structure within a domain and
consequently in the model as well (Section 3), but
whereas PROFINDER focuses on finding the ?cor-
rect? number of frames, events, and slots with a
nonparametric method, this work focuses on in-
tegrating global knowledge in the form of distri-
butional semantics into a probabilistic model. We
adopt one of their evaluation procedures and use it
to compare with PROFINDER in Section 5.
Vector space models form the basis of modern
information retrieval (Salton et al, 1975), but only
recently have distributional models been proposed
that are compositional (Mitchell and Lapata, 2008;
Clark et al, 2008; Grefenstette and Sadrzadeh,
2011, inter alia), or that contextualize the meaning
of a word using other words in the same phrase
(co-compositionality) (Erk and Pado?, 2008; Dinu
and Lapata, 2010; Thater et al, 2011). We re-
cently showed how such models can be evaluated
for their ability to support semantic inference for
use in complex NLP tasks like question answering
or automatic summarization (Cheung and Penn,
2012).
Combining distributional information and prob-
abilistic models has actually been explored in pre-
vious work. Usually, an ad-hoc clustering step
precedes training and is used to bias the initializa-
tion of the probabilistic model (Barzilay and Lee,
2004; Louis and Nenkova, 2012), or the clustering
is interleaved with iterations of training (Fung et
al., 2003). By contrast, our method better modu-
larizes the two, and provides a principled way to
train the model. More importantly, previous ad-
hoc clustering methods only use distributional in-
formation derived from the target domain itself;
initializing based on domain-general distributional
information can be problematic because it can bias
training towards a local optimum that is inappro-
priate for the target domain, leading to poor per-
393
?1 
?1 
?1 
?1 
?1 
? 
?? 
?? 
?? 
?? 
?? 
. . .  
?? ????
?  ??????  ??  ????
?  ??????  
Figure 1: Graphical representation of our model.
Distributions that generate the latent variables and
hyperparameters are omitted for clarity.
formance.
3 Distributional Semantic Hidden
Markov Models
We now describe the DSHMM model. This model
can be thought of as an HMM with two layers
of latent variables, representing events and slots
in the domain. Given a document consisting of
a sequence of T clauses headed by propositional
heads ~H (verbs or event nouns), and argument
noun phrases ~A, a DSHMM models the joint prob-
ability of observations ~H , ~A, and latent random
variables ~E and ~S representing domain events and
slots respectively; i.e., P ( ~H, ~A, ~E, ~S).
The basic structure of our model is similar to
PROFINDER. Each timestep in the model gener-
ates one clause in the document. More specifi-
cally, it generates the event heads and arguments
which are crucial in identifying events and slots.
We assume that event heads are verbs or event
nouns, while arguments are the head words of their
syntactically dependent noun phrases. We also as-
sume that the sequence of clauses and the clause-
internal syntactic structure are fixed, for example
by applying a dependency parser. Within each
clause, a hierarchy of latent and observed variables
maps to corresponding elements in the clause (Ta-
ble 1), as follows:
Event Variables At the top-level, a categorical
latent variable Et with NE possible states repre-
sents the event that is described by clause t. Its
value is conditioned on the previous time step?s
event variable, following the standard, first-order
Markov assumption (PE(Et|Et?1), or PEinit(E1)
Node Component Textual unit
Et Event Clause
Sta Slot Noun phrase
Ht Event head Verb/event noun
Ata Event argument Noun phrase
Table 1: The correspondence between nodes in our
graphical model, the domain components that they
model, and the related elements in the clause.
for the first clause). The internal structure of the
clause is generated by conditioning on the state of
Et, including the head of the clause, and the slots
for each argument in the clause.
Slot Variables Categorical latent variables with
NS possible states represent the slot that an argu-
ment fills, and are conditioned on the event vari-
able in the clause, Et (i.e., PS(Sta|Et), for the
ath slot variable). The state of Sta is then used to
generate an argument Ata.
Head and Argument Emissions The head of
the clause Ht is conditionally dependent on Et,
and each argument Ata is likewise conditioned on
its slot variable Sta. Unlike in most applications of
HMMs in text processing, in which the represen-
tation of a token is simply its word or lemma iden-
tity, tokens in DSHMM are also associated with a
vector representation of their meaning in context
according to a distributional semantic model (Sec-
tion 3.1). Thus, the emissions can be decomposed
into pairs Ht = (lemma(Ht), sem(Ht)) and
Ata = (lemma(Ata), sem(Ata)), where lemma
and sem are functions that return the lemma iden-
tity and the semantic vector respectively. The
probability of the head of a clause is thus:
PH(Ht|Et) = PHlemm(lemma(Ht)|Et) (1)
? PHsem(sem(Ht)|Et),
and the probability of a clausal argument is like-
wise:
PA(Ata|Sta) = PAlemm(lemma(Ata)|Sta) (2)
? PAsem(sem(Ata)|Sta).
All categorical distributions are smoothed using
add-? smoothing (i.e., uniform Dirichlet priors).
Based on the independence assumptions described
above, the joint probability distribution can be fac-
394
tored into:
P ( ~H, ~A, ~E, ~S) = PEinit(E1) (3)
?
T?
t=2
PE(Et|Et?1)
T?
t=1
PH(Ht|Et)
?
T?
t=1
Ct?
a=1
PS(Sta|Et)PA(Ata|Sta).
3.1 Vector Space Models of Semantics
In this section, we describe several methods for
producing the semantic vectors associated with
each event head or argument; i.e., the function
sem. We chose several simple, but widely studied
models, to investigate whether they can be effec-
tively integrated into DSHMM. We start with a de-
scription of the training of a basic model without
any contextualization, then describe several con-
textualized models based on recent work.
Simple Vector Space Model In the basic ver-
sion of the model (SIMPLE), we train a term-
context matrix, where rows correspond to target
words, and columns correspond to context words.
Training begins by counting context words that ap-
pear within five words of the target word, ignor-
ing stopwords. We then convert the raw counts
to positive pointwise mutual information scores,
which has been shown to improve word similarity
correlation results (Turney and Pantel, 2010). We
set thresholds on the frequencies of words for in-
clusion as target and context words (given in Sec-
tion 4). Target words which fall below the thresh-
old are modelled as UNK. All the methods below
start from this basic vector representation.
Component-wise Operators Mitchell and Lap-
ata (2008) investigate using component-wise op-
erators to combine the vectors of verbs and their
intransitive subjects. We use component-wise op-
erators to contextualize our vectors, but by com-
bining with all of the arguments, and regardless
of the event head?s category. Let event head h
be the syntactic head of a number of arguments
a1, a2, ...am, and ~vh, ~va1 , ~va2 , ...~vam be their re-
spective vector representations according to the
SIMPLE method. Then, their contextualized vec-
tors ~cM&Lh ,~cM&La1 , ...~cM&Lam would be:
~cM&Lh = ~vh  (
m?
i=1
~vam) (4)
~cM&Lai = ~vai  ~vh,?i = 1...m, (5)
where  represents a component-wise operator,
addition or multiplication, and ? represents its
repeated application. We tested component-wise
addition (M&L+) and multiplication (M&L?).
Selectional Preferences Erk and Pado? (2008)
(E&P) incorporate inverse selectional preferences
into their contextualization function. The intu-
ition is that a word should be contextualized such
that its vector representation becomes more sim-
ilar to the vectors of other words that its depen-
dency neighbours often take in the same syntactic
position. For example, suppose catch is the head
of the noun ball, in the relation of a direct object.
Then, the vector for ball would be contextualized
to become similar to the vectors for other frequent
direct objects of catch, such as baseball, or cold.
Likewise, the vector for catch would be contextu-
alized to become similar to the vectors for throw,
hit, etc. Formally, let h take a as its argument in
relation r. Then:
~cE&Ph = ~vh ?
m?
i=1
?
w?L
freq(w, r, ai) ? ~vw, (6)
~cE&Pa = ~va ?
?
w?L
freq(h, r, w) ? ~vw, (7)
where freq(h, r, a) is the frequency of h occur-
ring as the head of a in relation r in the train-
ing corpus, L is the lexicon, and ? represents
component-wise multiplication.
Dimensionality Reduction and Vector Emission
After contextualization, we apply singular value
decomposition (SVD) for dimensionality reduc-
tion to reduce the number of model parameters,
keeping the k most significant singular values and
vectors. In particular, we apply SVD to the m-by-
n term-context matrix M produced by the SIM-
PLE method, resulting in the truncated matrices
M ? Uk?kV Tk , where Uk is a m-by-k matrix, ?k
is k-by-k, and Vk is n-by-k. This takes place af-
ter contextualization, so the component-wise op-
erators apply in the original semantic space. Af-
terwards, the contextualized vector in the original
space, ~c, can be transformed into a vector in the
reduced space, ~cR, by ~cR = ??1k V Tk ~c.Distributional semantic vectors are traditionally
compared by measures which ignore vector mag-
nitudes, such as cosine similarity, but a multivari-
ate Gaussian is sensitive to magnitudes. Thus, the
final step is to normalize ~cR into a unit vector by
dividing it by its L2 norm, ||~cR||.
395
We model the emission of these contextualized
vectors in DSHMM as multivariate Gaussian dis-
tributions, so the semantic vector emissions can be
written as PHsem, PAsem ? N (?,?), where ? ? Rk
is the mean and ? ? Rk?k is the covariance
matrix. To avoid overfitting, we regularize the
covariance using its conjugate prior, the Inverse-
Wishart distribution. We follow the ?neutral? set-
ting of hyperparameters given by Ormoneit and
Tresp (1995), so that the MAP estimate for the co-
variance matrix for (event or slot) state i becomes:
?i =
?
j rij(xj ? ?i)(xj ? ?i)T + ?I?
j rij + 1
, (8)
where j indexes all the relevant semantic vectors
xj in the training set, rij is the posterior respon-
sibility of state i for vector xj , and ? is the re-
maining hyperparameter that we tune to adjust the
amount of regularization. To further reduce model
complexity, we set the off-diagonal entries of the
resulting covariance matrix to zero.
3.2 Training and Inference
Inference in DSHMM is accomplished by the stan-
dard Inside-Outside and tree-Viterbi algorithms,
except that the tree structure is fixed, so there
is no need to sum over all possible subtrees.
Model parameters are learned by the Expectation-
Maximization (EM) algorithm. We tune the hy-
perparameters (NE , NS , ?, ?, k) and the number
of EM iterations by two-fold cross-validation1.
3.3 Summary and Generative Process
In summary, the following steps are applied to
train a DSHMM:
1. Train a distributional semantic model on a
large, domain-general corpus.
2. Preprocess and generate contextualized vec-
tors of event heads and arguments in the
small corpus in the target domain.
3. Train the DSHMM using the EM algorithm.
The formal generative process is as follows:
1. Draw categorical distributions PEinit;
PE , PS , PHlemm (one per event state);
PAlemm (one per slot state) from Dirichlet
priors.
2. Draw multivariate Gaussians PHsem, PAsem for
each event and slot state, respectively.
1The topic cluster splits and the hyperparameter set-
tings are available at http://www.cs.toronto.edu/
?jcheung/dshmm/dshmm.html.
3. Generate the documents, clause by clause.
Generating a clause at position t consists of
these steps:
1. Generate the event state Et ? PE (or PEinit).
2. Generate the event head components
lemm(Ht) ? PHlemm, sem(Ht) ? PHsem.
3. Generate a number of slot states Sta ? PS .
4. For each slot, generate the argument compo-
nents lemm(Ata) ? PAlemm, sem(Ata) ?
PAsem.
4 Experiments
We trained the distributional semantic models us-
ing the Annotated Gigaword corpus (Napoles et
al., 2012), which has been automatically prepro-
cessed and is based on Gigaword 5th edition. This
corpus contains almost ten million news articles
and more than 4 billion tokens. We used those ar-
ticles marked as ?stories? ? the vast majority of
them. We modelled the 50,000 most common lem-
mata as target words, and the 3,000 most common
lemmata as context words.
We then trained DSHMM and conducted our
evaluations on the TAC 2010 guided summa-
rization data set (Owczarzak and Dang, 2010).
Lemmatization and extraction of event heads and
arguments are done by preprocessing with the
Stanford CoreNLP tool suite (Toutanova et al,
2003; de Marneffe et al, 2006). This data set con-
tains 46 topic clusters of 20 articles each, grouped
into five topic categories or domains. For exam-
ple, one topic cluster in the ATTACK category is
about the Columbine Massacre. Each topic cluster
contains eight human-written ?model? summaries
(?model? here meaning a gold standard). Half of
the articles and model summaries in a topic cluster
are used in the guided summarization task, and the
rest are used in the update summarization task.
We chose this data set because it allows us
to conduct various domain-modelling evaluations.
First, templates for the domains are provided, and
the model summaries are annotated with slots
from the template, allowing for an intrinsic eval-
uation of slot induction (Section 5). Second, it
contains multiple domain instances for each of the
domains, and each domain instance comes anno-
tated with eight model summaries, allowing for an
extrinsic evaluation of our system (Section 6).
396
5 Guided Summarization Slot Induction
We first evaluated our models on their ability to
produce coherent clusters of entities belonging to
the same slot, adopting the experimental proce-
dure of Cheung et al (2013).
As part of the official TAC evaluation proce-
dure, model summaries were manually segmented
into contributors, and labelled with the slot in
the TAC template that the contributor expresses.
For example, a summary fragment such as On 20
April 1999, a massacre occurred at Columbine
High School is segmented into the contributors:
(On 20 April 1999, WHEN); (a massacre oc-
curred, WHAT); and (at Columbine High School,
WHERE).
In the slot induction evaluation, this annotation
is used as follows. First, the maximal noun phrases
are extracted from the contributors and clustered
based on the TAC slot of the contributor. These
clusters of noun phrases then become the gold
standard clusters against which automatic systems
are compared. Noun phrases are considered to be
matched if the lemmata of their head words are the
same and they are extracted from the same sum-
mary. This accounts for the fact that human an-
notators often only label the first occurrence of a
word that belongs to a slot in a summary, and fol-
lows the standard evaluation procedure in previ-
ous information extraction tasks, such as MUC-4.
Pronouns and demonstratives are ignored. This
extraction process is noisy, because the meaning
of some contributors depends on an entire verb
phrase, but we keep this representation to allow
a direct comparison to previous work.
Because we are evaluating unsupervised sys-
tems, the clusters produced by the systems are not
labelled, and must be matched to the gold stan-
dard clusters. This matching is performed by map-
ping to each gold cluster the best system cluster
according to F1. The same system cluster may be
mapped multiple times, because several TAC slots
can overlap. For example, in the NATURAL DIS-
ASTERS domain, an earthquake may fit both the
WHAT slot as well as the CAUSE slot, because it
generated a tsunami.
We trained a DSHMM separately for each of the
five domains with different semantic models, tun-
ing hyperparameters by two-fold cross-validation.
We then extracted noun phrase clusters from the
model summaries according to the slot labels pro-
duced by running the Viterbi algorithm on them.
Method P R F1
HMM w/o semantics 13.8 64.1 22.6*
DSHMM w/ SIMPLE 20.9 27.5 23.7
DSHMM w/ E&P 20.7 27.9 23.8
PROFINDER 23.7 25.0 24.3
DSHMM w/ M&L+ 19.7 36.3 25.6*
DSHMM w/ M&L? 22.1 33.2 26.5*
Table 2: Slot induction results on the TAC guided
summarization data set. Asterisks (*) indicate
that the model is statistically significantly differ-
ent from PROFINDER in terms of F1 at p < 0.05.
Results We compared DSHMM to two base-
lines. Our first baseline is PROFINDER, a state-
of-the-art template inducer which Cheung et al
(2013) showed to outperform the previous heuris-
tic clustering method of Chambers and Jurafsky
(2011). Our second baseline is our DSHMM
model, without the semantic vector component,
(HMM w/o semantics). To calculate statistical
significance, we use the paired bootstrap method,
which can accommodate complex evaluation met-
rics like F1 (Berg-Kirkpatrick et al, 2012).
Table 2 shows that performance of the mod-
els. Overall, PROFINDER significantly outper-
forms the HMM baseline, but not any of the
DSHMM models by F1. DSHMM with contextu-
alized semantic vectors achieves the highest F1s,
and are significantly better than PROFINDER. All
of the differences in precision and recall between
PROFINDER and the other models are significant.
The baseline HMM model has highly imbalanced
precision and recall. We think this is because the
model is unable to successfully produce coher-
ent clusters, so the best-case mapping procedure
during evaluation picked large clusters that have
high recall. PROFINDER has slightly higher preci-
sion, which may be due to its non-parametric split-
merge heuristic. We plan to investigate whether
this learning method could improve DSHMM?s
performance further. Importantly, the contextual-
ization of the vectors seems to be beneficial, at
least with the M&L component-wise operators.
In the next section, we show that the improve-
ment from contextualization transfers to multi-
document summarization results.
397
6 Multi-document Summarization: An
Extrinsic Evaluation
We next evaluated our models extrinsically in the
setting of extractive, multi-document summariza-
tion. To use the trained DSHMM for extractive
summarization, we need a decoding procedure for
selecting sentences in the source text to include in
the summary. Inspired by the KLSUM and HI-
ERSUM methods of Haghighi and Vanderwende
(2009), we develop a criterion based on Kullback-
Leibler (KL) divergence between distributions es-
timated from the source text, and those estimated
from the summary. The assumption here is that
these distributions should match in a good sum-
mary. We describe two methods to use this crite-
rion: a basic unsupervised method (Section 6.1),
and a supervised variant that makes use of in-
domain summaries to learn the salient slots and
events in the domain (Section 6.2).
6.1 A KL-based Criterion
There are four main component distributions from
our model that should be considered during extrac-
tion: (1) the distribution of events, (2) the distri-
bution of slots, (3) the distribution of event heads,
and (4) the distribution of arguments. We estimate
(1) as the context-independent probability of being
in a certain event state, which can be calculated
using the Inside-Outside algorithm. Given a col-
lection of documents D which make up the source
text, the distribution of event topics P?E(E) is es-
timated as:
P?E(E = e) = 1Z
?
d?D
?
t
Int(e)Outt(e)
P (d) , (9)
where Int(e) and Outt(e) are the values of the
inside and outside trellises at timestep t for some
event state e, and Z is a normalization constant.
The distribution for a set of sentences in a can-
didate summary, Q?E(E), is identical, except the
summation is over the clauses in the candidate
summary. Slot distributions P?S(S) and Q?S(S) (2)
are defined analogously, where the summation oc-
curs along all the slot variables.
For (3) and (4), we simply use the MLE es-
timates of the lemma emissions, where the esti-
mates are made over the source text and the can-
didate summary instead of over the entire train-
ing set. All of the candidate summary distribu-
tions (i.e., the ?Q? distributions?) are smoothed by
a small amount, so that the KL-divergence is al-
ways finite. Our KL criterion combines the above
components linearly, weighting the lemma distri-
butions by the probability of their respective event
or slot state:
KLScore = (10)
DKL(P?E ||Q?E) +DKL(P?S ||Q?S)
+
NE?
e=1
P?E(e)DKL(P?H(H|e)||Q?H(H|e))
+
NS?
s=1
P?S(s)DKL(P?A(A|s)||Q?A(A|s))
To produce a summary, sentences from the
source text are greedily added such thatKLScore
is minimized at each step, until the desired sum-
mary length is reached, discarding sentences with
fewer than five words.
6.2 Supervised Learning
The above unsupervised method results in sum-
maries that closely mirror the source text in terms
of the event and slot distributions, but this ig-
nores the fact that not all such topics should be
included in a summary. It also ignores genre-
specific, stylistic considerations about character-
istics of good summary sentences. For example,
Woodsend and Lapata (2012) find several factors
that indicate sentences should not be included in
an extractive summary, such as the presence of
personal pronouns. Thus, we implemented a sec-
ond method, in which we modify the KL criterion
above by estimating P?E and P?S from other model
summaries that are drawn from the same domain
(i.e. topic category), except for those summaries
that are written for the specific topic cluster to be
used for evaluation.
6.3 Method and Results
We used the best performing models from the slot
induction task and the above unsupervised and su-
pervised methods based on KL-divergence to pro-
duce 100-word summaries of the guided summa-
rization source text clusters. We did not com-
pare against PROFINDER, as its structure is dif-
ferent and would have required a different proce-
dure than the KL-criterion we developed above.
As shown in the previous evaluation, however, the
HMM baseline without semantics and DSHMM
with SIMPLE perform similarly in terms of F1,
398
Method ROUGE-1 ROUGE-2 ROUGE-SU4
unsup. sup. unsup. sup. unsup. sup.
Leading baseline 28.0 ? 5.39 ? 8.6 ?
HMM w/o semantics 32.3 32.7 6.45 6.49 10.1 10.2
DSHMM w/ SIMPLE 32.1 32.7 5.81 6.50 9.8 10.2
DSHMM w/ M&L+ 32.1 33.4 6.27 6.82 10.0 10.6
DSHMM w/ M&L? 32.4 34.3* 6.35 7.11? 10.2 11.0*
DSHMM w/ E&P 32.8 33.8* 6.38 7.31* 10.3 10.8*
Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the
model is statistically significantly better than the HMM model without semantics at a 95% confidence
interval, a caret ? indicates that the value is marginally so.
so we consider these competitive baselines. We
did not evaluate with the update summarization
task, because our method has not been adapted to
it. For the evaluation measure, we used the stan-
dard ROUGE suite of automatic evaluation mea-
sures (Lin, 2004). Note that the evaluation con-
ditions of TAC 2010 are different, and thus those
results are not directly comparable to ours. For in-
stance, top performing systems in TAC 2010 make
use of manually constructed lists of entities known
to fit the slots in the provided templates and sam-
ple topic statements, which our method automat-
ically learns. We include the leading baseline re-
sults from the competition as a point of reference,
as it is a well-known and non-trivial one for news
articles. This baseline summary consists of the
leading sentences from the most recent document
in the source text cluster up to the word length
limit.
Table 3 shows the summarization results for the
three most widely-used settings of ROUGE. All
of our models outperform the leading baseline by
a large margin, demonstrating the effective of the
KL-criterion. In terms of unsupervised perfor-
mance, all of our models perform similarly. Be-
cause the unsupervised method mimics the distri-
butions in the source text at all levels, the method
may negate the benefit of learning and simply pro-
duce summaries that match the source text in the
word distributions, thus being an approximation
of KLSUM. Looking at the supervised results,
however, the semantic vector models show clear
gains in ROUGE, whereas the baseline method
does not obtain much benefit from supervision. As
in the previous evaluation, the models with con-
textualized semantic vectors provide the best per-
formance. M&L? performs very well, as in slot
induction, but E&P also performs well, unlike in
the previous evaluation. This result reinforces the
importance of the contextualization procedure for
distributional semantic models.
Analysis To better understand what is gained by
supervision using in-domain summaries, we ana-
lyzed the best performing M&L? model?s output
summaries for one document cluster from each
domain. For each event state, we calculated the
ratio P?Esumm(e)/P?Esource(e), for the probability of
an event state e as estimated from the training
summaries and the the source text respectively.
Likewise, we calculated P?Ssumm(s)/P?Ssource(s) for
the slot states. This ratio indicates the change in
state?s probability after supervision; the greater the
ratio, the more preferred that state becomes after
training. We selected the most preferred and dis-
preferred event and slot for each document clus-
ter, and took the three most probable lemmata
from the associated lemma distribution (Table 4).
It seems that supervision is beneficial because it
picks out important event heads and arguments in
the domain, such as charge, trial, and murder in
the TRIALS domain. It also helps the summarizer
avoid semantically generic words (be or have),
pronouns, quotatives, and common but irrelevant
words (home, city, restaurant in TRIALS).
7 Conclusion
We have shown that contextualized distributional
semantic vectors can be successfully integrated
into a generative probabilistic model for domain
modelling, as demonstrated by improvements in
slot induction and multi-document summariza-
tion. The effectiveness of our model stems from
the use of a large domain-general corpus to train
the distributional semantic vectors, and the im-
plicit syntactic and word sense information pro-
399
Domain Event Heads Slot Arguments+ ? + ?
ATTACKS say
2, cause,
doctor say
2, be, have attack, hostage,troops he, it, they
TRIALS charge, trial,accuse say, be, have
prison, murder,
charge
home, city, restau-
rant
RESOURCES reduce, increase,university say, be, have
government,
effort, program he, they, it
DISASTERS flood, strengthen,engulf say, be, have
production,
statoil, barrel he, it, they
HEALTH be, department,have say, do, make
food, product,
meat she, people, way
Table 4: Analysis of the most probable event heads and arguments in the most preferred (+) and dispre-
ferred (?) events and slots after supervised training.
vided by the contextualization process. Our ap-
proach is modular, and allows principled train-
ing of the probabilistic model using standard tech-
niques. While we have focused on the overall clus-
tering of entities and the distribution of event and
slot topics in this work, we would also like to in-
vestigate discourse modelling and content struc-
turing. Finally, our work shows that the applica-
tion of distributional semantics to NLP tasks need
not be confined to lexical disambiguation. We
would like to see modern distributional semantic
methods incorporated into an even greater variety
of applications.
Acknowledgments
This work is supported by the Natural Sciences
and Engineering Research Council of Canada.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics: HLT-NAACL 2004.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statisti-
cal significance in NLP. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
2The event head say happens to appear in both the most
preferred and dispreferred events in the ATTACKS domain.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of
Machine Learning Research, 3.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 815?824, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 976?
986, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jackie Chi Kit Cheung and Gerald Penn. 2012. Evalu-
ating distributional models of semantics for syntacti-
cally invariant inference. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 33?43,
Avignon, France, April. Association for Computa-
tional Linguistics.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
400
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162?1172.
Micha Elsner, Joseph Austerweil, and Eugene Char-
niak. 2007. A unified local and global model for
discourse coherence. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
Rochester, New York, April. Association for Com-
putational Linguistics.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?
906.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of
domain templates. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 207?214, Sydney, Australia, July. Association
for Computational Linguistics.
Pascale Fung and Grace Ngai. 2006. One story, one
flow: Hidden markov story models for multilin-
gual multidocument summarization. ACM Transac-
tions on Speech and Language Processing (TSLP),
3(2):1?16.
Pascale Fung, Grace Ngai, and Chi-Shun Cheung.
2003. Combining optimal clustering and hidden
markov models for extractive summarization. In
Proceedings of the ACL 2003 Workshop on Multilin-
gual Summarization and Question Answering, pages
21?28, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394?
1404, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.
2007. Hidden topic markov models. Artificial In-
telligence and Statistics (AISTATS).
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 362?370, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Chin Y. Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Stan Szpakowicz and
Marie-Francine Moens, editors, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
1992. Proceedings of the Fourth Message Understand-
ing Conference (MUC-4). Morgan Kaufmann.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the NAACL-HLT Joint Workshop on Au-
tomatic Knowledge Base Construction & Web-scale
Knowledge Extraction (AKBC-WEKEX), pages 95?
100.
Dirk Ormoneit and Volker Tresp. 1995. Improved
gaussian mixture density estimates using bayesian
penalty terms and network averaging. In Advances
in Neural Information Processing, pages 542?548.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC
2010 guided summarization task guidelines.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry Into
Human Knowledge Structures. Lawrence Erlbaum,
July.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of IJCNLP.
Kristina Toutanova, Dan Klein, Christoper D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, page 180.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Hanna M. Wallach. 2008. Structured topic models for
language. Doctoral dissertation, University of Cam-
bridge.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea, July. Association for
Computational Linguistics.
401
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1233?1242,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Towards Robust Abstractive Multi-Document Summarization: A
Caseframe Analysis of Centrality and Domain
Jackie Chi Kit Cheung
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
jcheung@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
In automatic summarization, centrality is
the notion that a summary should contain
the core parts of the source text. Cur-
rent systems use centrality, along with re-
dundancy avoidance and some sentence
compression, to produce mostly extrac-
tive summaries. In this paper, we investi-
gate how summarization can advance past
this paradigm towards robust abstraction
by making greater use of the domain of
the source text. We conduct a series of
studies comparing human-written model
summaries to system summaries at the se-
mantic level of caseframes. We show that
model summaries (1) are more abstrac-
tive and make use of more sentence aggre-
gation, (2) do not contain as many topi-
cal caseframes as system summaries, and
(3) cannot be reconstructed solely from
the source text, but can be if texts from
in-domain documents are added. These
results suggest that substantial improve-
ments are unlikely to result from better
optimizing centrality-based criteria, but
rather more domain knowledge is needed.
1 Introduction
In automatic summarization, centrality has been
one of the guiding principles for content selection
in extractive systems. We define centrality to be
the idea that a summary should contain the parts
of the source text that are most similar or repre-
sentative of the source text. This is most trans-
parently illustrated by the Maximal Marginal Rel-
evance (MMR) system of Carbonell and Goldstein
(1998), which defines the summarization objective
to be a linear combination of a centrality term and
a non-redundancy term.
Since MMR, much progress has been made on
more sophisticated methods of measuring central-
ity and integrating it with non-redundancy (See
Nenkova and McKeown (2011) for a recent sur-
vey). For example, term weighting methods such
as the signature term method of Lin and Hovy
(2000) pick out salient terms that occur more often
than would be expected in the source text based on
frequencies in a background corpus. This method
is a core component of the most successful sum-
marization methods (Conroy et al, 2006).
While extractive methods based on centrality
have thus achieved success, there has long been
recognition that abstractive methods are ultimately
more desirable. One line of work is in text simpli-
fication and sentence fusion, which focus on the
ability of abstraction to achieve a higher compres-
sion ratio (Knight and Marcu, 2000; Barzilay and
McKeown, 2005). A less examined issue is that of
aggregation and information synthesis. A key part
of the usefulness of summaries is that they provide
some synthesis or analysis of the source text and
make a more general statement that is of direct rel-
evance to the user. For example, a series of related
events can be aggregated and expressed as a trend.
The position of this paper is that centrality is
not enough to make substantial progress towards
abstractive summarization that is capable of this
type of semantic inference. Instead, summariza-
tion systems need to make more use of domain
knowledge. We provide evidence for this in a se-
ries of studies on the TAC 2010 guided summa-
rization data set that examines how the behaviour
of automatic summarizers can or cannot be dis-
tinguished from human summarizers. First, we
confirm that abstraction is a desirable goal, and
1233
provide a quantitative measure of the degree of
sentence aggregation in a summarization system.
Second, we show that centrality-based measures
are unlikely to lead to substantial progress towards
abstractive summarization, because current top-
performing systems already produce summaries
that are more ?central? than humans do. Third, we
consider how domain knowledge may be useful as
a resource for an abstractive system, by showing
that key parts of model summaries can be recon-
structed from the source plus related in-domain
documents.
Our contributions are novel in the following re-
spects. First, our analyses are performed at the
level of caseframes, rather at the level of words or
syntactic dependencies as in previous work. Case-
frames are shallow approximations of semantic
roles which are well suited to characterizing a do-
main by its slots. Furthermore, we take a devel-
opmental rather than evaluative perspective?our
goal is not to develop a new evaluation measure as
defined by correlation with human responsiveness
judgments. Instead, our studies reveal useful cri-
teria with which to distinguish human-written and
system summaries, helping to guide the develop-
ment of future summarization systems.
2 Related Work
Domain-dependent template-based summariza-
tion systems have been an alternative to extractive
systems which make use of rich knowledge about
a domain and information extraction techniques to
generate a summary, possibly using a natural lan-
guage generation system (Radev and McKeown,
1998; White et al, 2001; McKeown et al, 2002).
This paper can be seen as a first step towards
reconciling the advantages of domain knowledge
with the resource-lean extraction approaches pop-
ular today.
As noted above, Lin and Hovy?s (2000) sig-
nature terms have been successful in discovering
terms that are specific to the source text. These
terms are identified by a log-likelihood ratio test
based on their relative frequencies in relevant and
irrelevant documents. They were originally pro-
posed in the context of single-document summa-
rization, where they were calculated using in-
domain (relevant) vs. out-of-domain (irrelevant)
text. In multi-document summarization, the in-
domain text has been replaced by the source text
cluster (Conroy et al, 2006), thus they are now
used as a form of centrality-based features. In
this paper, we use guided summarization data as
an opportunity to reopen the investigation into the
effect of domain, because multiple document clus-
ters from the same domain are available.
Summarization evaluation is typically done by
comparing system output to human-written model
summaries, and are validated by their correlation
with user responsiveness judgments. The compar-
ison can be done at the word level, as in ROUGE
(Lin, 2004), at the syntactic level, as in Basic
Elements (Hovy et al, 2006), or at the level of
summary content units, as in the Pyramid method
(Nenkova and Passonneau, 2004). There are also
automatic measures which do not require model
summaries, but compare against the source text in-
stead (Louis and Nenkova, 2009; Saggion et al,
2010).
Several studies complement this paper by ex-
amining the best possible extractive system us-
ing current evaluation measures, such as ROUGE
(Lin and Hovy, 2003; Conroy et al, 2006). They
find that the best possible extractive systems score
higher or as highly than human summarizers, but
it is unclear whether this means the oracle sum-
maries are actually as useful as human ones in
an extrinsic setting. Genest et al (2009) ask hu-
mans to create extractive summaries, and find that
they score in between current automatic systems
and human-written abstracts on responsiveness,
linguistic quality, and Pyramid score. In the lec-
ture domain, He et al (1999; 2000) find that
lecture transcripts that have been manually high-
lighted with key points improve students? quiz
scores more than when using automated summa-
rization techniques or when providing only the
lecture transcript or slides.
Jing and McKeown (2000) manually analyzed
30 human-written summaries, and find that 19%
of sentences cannot be explained by cut-and-paste
operations from the source text. Saggion and La-
palme (2002) similarly define a list of transfor-
mations necessary to convert source text to sum-
mary text, and manually analyzed their frequen-
cies. Copeck and Szpakowicz (2004) find that
at most 55% of vocabulary items found in model
summaries occur in the source text, but they do
not investigate where the other vocabulary items
might be found.
1234
Sentence:
At one point, two bomb squad trucks sped to
the school after a backpack scare.
Dependencies:
num(point, one) prep at(sped, point)
num(trucks, two) nn(trucks, bomb)
nn(trucks, squad) nsubj(sped, trucks)
root(ROOT, sped) det(school, the)
prep to(sped, school) det(scare, a)
nn(scare, backpack) prep after(sped, scare)
Caseframes:
(speed, prep at) (speed, nsubj)
(speed, prep to) (speed, prep after)
Table 1: A sentence decomposed into its depen-
dency edges, and the caseframes derived from
those edges that we consider (in black).
3 Theoretical basis of our analysis
Many existing summarization evaluation methods
rely on word or N-gram overlap measures, but
these measures are not appropriate for our anal-
ysis. Word overlap can occur due to shared proper
nouns or entity mentions. Good summaries should
certainly contain the salient entities in the source
text, but when assessing the effect of the domain,
different domain instances (i.e., different docu-
ment clusters in the same domain) would be ex-
pected to contain different salient entities. Also,
the realization of entities as noun phrases depends
strongly on context, which would confound our
analysis if we do not also correctly resolve corefer-
ence, a difficult problem in its own right. We leave
such issues to other work (Nenkova and McKe-
own, 2003, e.g.).
Domains would rather be expected to share slots
(a.k.a. aspects), which require a more semantic
level of analysis that can account for the various
ways in which a particular slot can be expressed.
Another consideration is that the structures to be
analyzed should be extracted automatically. Based
on these criteria, we selected caseframes to be the
appropriate unit of analysis. A caseframe is a shal-
low approximation of the semantic role structure
of a proposition-bearing unit like a verb, and are
derived from the dependency parse of a sentence1.
1Note that caseframes are distinct from (though directly
Relation Caseframe Pair Sim.
Degree (kill, dobj) (wound, dobj) 0.82
Causal (kill, dobj) (die, nsubj) 0.80
Type (rise, dobj) (drop, prep to) 0.81
Figure 1: Sample pairs of similar caseframes by
relation type, and the similarity score assigned to
them by our distributional model.
In particular, they are (gov, role) pairs, where gov
is a proposition-bearing element, and role is an
approximation of a semantic role with gov as its
head (See Figure 1 for examples). Caseframes do
not consider the dependents of the semantic role
approximations.
The use of caseframes is well grounded in a va-
riety of NLP tasks relevant to summarization such
as coreference resolution (Bean and Riloff, 2004),
and information extraction (Chambers and Juraf-
sky, 2011), where they serve the central unit of se-
mantic analysis. Related semantic representations
are popular in Case Grammar and its derivative
formalisms such as frame semantics (Fillmore,
1982).
We use the following algorithm to extract case-
frames from dependency parses. First, we extract
those dependency edges with a relation type of
subject, direct object, indirect object, or prepo-
sitional object (with the preposition indicated),
along with their governors. The governor must be
a verb, event noun (as defined by the hyponyms
of the WordNet EVENT synset), or nominal or ad-
jectival predicate. Then, a series of deterministic
transformations are applied to the syntactic rela-
tions to account for voicing alternations, control,
raising, and copular constructions.
3.1 Caseframe Similarity
Direct caseframe matches account for some vari-
ation in the expression of slots, such as voicing
alternations, but there are other reasons different
caseframes may indicate the same slot (Figure 1).
For example, (kill, dobj) and (wound, dobj) both
indicate the victim of an attack, but differ by
the degree of injury to the victim. (kill, dobj)
and (die, nsubj) also refer to a victim, but are
linked by a causal relation. (rise, dobj) and
inspired by) the similarly named case frames of Case Gram-
mar (Fillmore, 1968).
1235
(drop, prep to) on the other hand simply share a
named entity type (in this case, numbers). To ac-
count for these issues, we measure caseframe sim-
ilarity based on their distributional similarity in a
large training corpus.
First, we construct vector representations of
each caseframe, where the dimensions of the vec-
tor correspond to the lemma of the head word that
fills the caseframe in the training corpus. For ex-
ample, kicked the ball would result in a count of
1 added to the caseframe (kick, dobj) for the con-
text word ball. Then, we rescale the counts into
pointwise mutual information values, which has
been shown to be more effective than raw counts
at detecting semantic relatedness (Turney, 2001).
Similarity between caseframes can then be com-
pared by cosine similarity between the their vector
representations.
For training, we use the AFP portion of the
Gigaword corpus (Graff et al, 2005), which we
parsed using the Stanford parser?s typed depen-
dency tree representation with collapsed conjunc-
tions (de Marneffe et al, 2006). For reasons of
sparsity, we only considered caseframes that ap-
pear at least five times in the guided summariza-
tion corpus, and only the 3000 most common lem-
mata in Gigaword as context words.
3.2 An Example
To illustrate how caseframes indicate the slots in a
summary, we provide the following fragment of a
model summary from TAC about the Unabomber
trial:
(1) In Sacramento, Theodore Kaczynski faces a
10-count federal indictment for 4 of the 16
mail bomb attacks attributed to the
Unabomber in which two people were killed.
If found guilty, he faces a death penalty. ...
He has pleaded innocent to all charges. U.S.
District Judge Garland Burrell Jr. presides
in Sacramento.
All of the slots provided by TAC for the Inves-
tigations and Trials domain can be identified by
one or more caseframes. The DEFENDANT can be
identified by (face, nsubj), and (plead, nsubj);
the CHARGES by (face, dobj); the REASON
by (indictment, prep for); the SENTENCE by
(face, dobj); the PLEAD by (plead, dobj); and
the INVESTIGATOR by (preside, nsubj).
4 Experiments
We conducted our experiments on the data and re-
sults of the TAC 2010 summarization workshop.
This data set contains 920 newspaper articles in
46 topics of 20 documents each. Ten are used in
an initial guided summarization task, and ten are
used in an update summarization task, in which
a summary must be produced assuming that the
original ten documents had already been read. All
summaries have a word length limit of 100 words.
We analyzed the results of the two summarization
tasks separately in our experiments.
The 46 topics belong to five different cate-
gories or domains: Accidents and natural dis-
asters, Criminal or terrorist attacks, Health and
safety, Endangered resources, and Investigations
and trials. Each domain is associated with a tem-
plate specifying the type of information that is ex-
pected in the domain, such as the participants in
the event or the time that the event occurred.
In our study, we compared the characteristics of
summaries generated by the eight human summa-
rizers with those generated by the peer summaries,
which are basically extractive systems. There
are 43 peer summarization systems, including two
baselines defined by NIST. We refer to systems
by their ID given by NIST, which are alphabetical
for the human summarizers (A to H), and numeric
for the peer summarizers (1 to 43). We removed
two peer systems (systems 29 and 43) which did
not generate any summary text in the workshop,
presumably due to software problems. For each
measure that we consider, we compare the average
among the human-written summaries to the three
individual peer systems, which we chose in order
to provide a representative sample of the average
and best performance of the automatic systems ac-
cording to current evaluation methods. These sys-
tems are all primarily extractive, like most of the
systems in the workshop:
Peer average The average of the measure
among the 41 peer summarizers.
Peer 16 This system scored the highest in re-
sponsiveness scores on the original summarization
task and in ROUGE-2, responsiveness, and Pyra-
mid score in the update task.
Peer 22 This system scored the highest in
ROUGE-2 and Pyramid score in the original sum-
marization task.
1236
1
4
28
25
12
10
15
31
36
18
42
8
14
30
37
33
13
19
24
16
21
40
41
3
27
38
17
35
23
39
34
22
7
6
20
26
11
32
5
9
G
2
F
B
E
A
D
C
H
System IDs
0.0
0.5
1.0
1.5
2.0
N
u
m
be
r
o
fs
en
te
n
ce
s
(a) Initial guided summarization task
28
1
31
18
42
4
10
15
36
24
25
12
13
16
30
34
27
3
9
8
14
37
33
40
7
11
39
38
19
35
26
23
17
22
21
6
32
41
5
20
F
G
2
E
C
B
A
H
D
System IDs
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
N
u
m
be
r
o
fs
en
te
n
ce
s
(b) Update summarization task
Figure 2: Average sentence cover size: the average number of sentences needed to generate the case-
frames in a summary sentence (Study 1). Model summaries are shown in darker bars. Peer system
numbers that we focus on are in bold.
Condition Initial Update
Model average 1.58 1.57
Peer average 1.06 1.06
Peer 1 1.00 1.00
Peer 16 1.04 1.04
Peer 22 1.08 1.09
Table 2: The average number of source text sen-
tences needed to cover a summary sentence. The
model average is statistically significantly differ-
ent from all the other conditions p < 10?7
(Study 1).
Peer 1 The NIST-defined baseline, which is the
leading sentence baseline from the most recent
document in the source text cluster. This system
scored the highest on linguistic quality in both
tasks.
4.1 Study 1: Sentence aggregation
We first confirm that human summarizers are more
prone to sentence aggregation than system sum-
marizers, showing that abstraction is indeed a de-
sirable goal. To do so, we propose a measure to
quantify the degree of sentence aggregation exhib-
ited by a summarizer, which we call average sen-
tence cover size. This is defined to be the min-
imum number of sentences from the source text
needed to cover all of the caseframes found in a
summary sentence (for those caseframes that can
be found in the source text at all), averaged over all
of the summary sentences. Purely extractive sys-
tems would thus be expected to score 1.0, as would
systems that perform text compression by remov-
ing constituents of a source text sentence. Human
summarizers would be expected to score higher, if
they actually aggregate information from multiple
points in the source text.
To illustrate, suppose we assign arbitrary in-
dices to caseframes, a summary sentence con-
tains caseframes {1, 2, 3, 4, 5}, and the source
text contains three sentences with caseframes,
which can be represented as a nested set
{{1, 3, 4}, {2, 5, 6}, {1, 4, 7}}. Then, the sum-
mary sentence can be covered by two sentences
from the source text, namely {{1, 3, 4}, {2, 5, 6}}.
This problem is actually an instance of the min-
imum set cover problem, in which sentences are
sets, and caseframes are set elements. Minimum
set cover is NP-hard in general, but the standard
integer programming formulation of set cover suf-
ficed for our data set; we used ILOG CPLEX
12.4?s mixed integer programming mode to solve
all the set cover problems optimally.
Results Figure 2 shows the ranking of the sum-
marizers by this measure. Most peer systems have
a low average sentence cover size of close to 1,
which reflects the fact that they are purely or al-
most purely extractive. Human model summariz-
ers show a higher degree of aggregation in their
summaries. The averages of the tested condi-
tions are shown in Table 2, and are statistically
significant. Peer 2 shows a relatively high level
of aggregation despite being an extractive system.
Upon inspection of its summaries, it appears that
Peer 2 tends to select many datelines, and without
punctuation to separate them from the rest of the
summary, our automatic analysis tools incorrectly
merged many sentences together, resulting in in-
correct parses and novel caseframes not found in
1237
A
32
B
12
42
27
37
33
G
1
5
28
7
39
2
E
F
H
35
26
15
C
D
11
20
9
36
14
19
40
13
16
8
30
4
6
10
3
18
41
21
34
24
17
25
31
22
23
38
System IDs
0.00
0.02
0.04
0.06
0.08
0.10
0.12
Pe
r
w
o
rd
de
n
si
ty
(a) Initial guided summarization task
E
A
G
B
37
1
33
C
12
27
26
42
39
11
H
28
F
15
2
D
32
20
35
5
40
7
4
10
8
19
14
30
36
41
18
3
9
21
24
34
13
22
25
16
31
17
6
23
38
System IDs
0.00
0.02
0.04
0.06
0.08
0.10
Pe
r
w
o
rd
de
n
si
ty
(b) Update summarization task
Figure 3: Density of signature caseframes (Study 2).
Topic: Unabomber trial
(charge, dobj), (kill, dobj),
(trial, prep of), (bombing, prep in)
Topic: Mangrove forests
(beach, prep of), (save, dobj)
(development, prep of), (recover, nsubj)
Topic: Bird Flu
(infect, prep with), (die, nsubj)
(contact, dobj), (import, prep from)
Figure 4: Examples of signature caseframes found
in Study 2.
the source text.
4.2 Study 2: Signature caseframe density
Study 1 shows that human summarizers are more
abstractive in that they aggregate information from
multiple sentences in the source text, but how is
this aggregation performed? One possibility is
that human summary writers are able to pack a
greater number of salient caseframes into their
summaries. That is, humans are fundamentally re-
lying on centrality just as automatic summarizers
do, and are simply able to achieve higher compres-
sion ratios by being more succinct. If this is true,
then sentence fusion methods over the source text
alone might be able to solve the problem. Unfor-
tunately, we show that this is false and that system
summaries are actually more central than model
ones.
To extract topical caseframes, we use Lin and
Hovy?s (2000) method of calculating signature
terms, but extend the method to apply it at the
caseframe rather than the word level. We fol-
low Lin and Hovy (2000) in using a significance
Condition Initial Update
Model average 0.065 0.052
Peer average 0.080? 0.072?
Peer 1 0.066 0.050
Peer 16 0.083? 0.085?
Peer 22 0.101? 0.084?
Table 3: Signature caseframe densities for differ-
ent sets of summarizers, for the initial and update
guided summarization tasks (Study 2). ?: p <
0.005.
threshold of 0.001 to determine signature case-
frames2. Figure 4 shows examples of signature
caseframes for several topics. Then, we calculate
the signature caseframe density of each of the
summarization systems. This is defined to be the
number of signature caseframes in the set of sum-
maries divided by the number of words in that set
of summaries.
Results Figure 3 shows the density for all of the
summarizers, in ascending order of density. As
can be seen, the human abstractors actually tend to
use fewer signature caseframes in their summaries
than automatic systems. Only the leading baseline
is indistinguishable from the model average. Ta-
ble 3 shows the densities for the conditions that
we described earlier. The differences in density
between the human average and the non-baseline
conditions are highly statistically significant, ac-
cording to paired two-tailed Wilcoxon signed-rank
tests for the statistic calculated for each topic clus-
ter.
These results show that human abstractors do
2We tried various other thresholds, but the results were
much the same.
1238
Threshold 0.9 0.8
Condition Init. Up. Init. Up.
Model average 0.066 0.052 0.062 0.047
Peer average 0.080 0.071 0.071 0.063
Peer 1 0.068 0.050 0.060 0.044
Peer 16 0.083 0.086 0.072 0.077
Peer 22 0.100 0.086 0.084 0.075
Table 4: Density of signature caseframes after
merging to various threshold for the initial (Init.)
and update (Up.) summarization tasks (Study 2).
not merely repeat the caseframes that are indica-
tive of a topic cluster or use minor grammatical
alternations in their summaries. Rather, a genuine
sort of abstraction or distillation has taken place,
either through paraphrasing or semantic inference,
to transform the source text into the final informa-
tive summary.
Merging Caseframes We next investigate
whether simple paraphrasing could account for
the above results; it may be the case that human
summarizers simply replace words in the source
text with synonyms, which can be detected with
distributional similarity. Thus, we merged similar
caseframes into clusters according to the distribu-
tional semantic similarity defined in Section 3.1,
and then repeated the previous experiment. We
chose two relatively high levels of similarity (0.8
and 0.9), and used complete-link agglomerative
(i.e., bottom-up) clustering to merge similar
caseframes. That is, each caseframe begins as a
separate cluster, and the two most similar clusters
are merged at each step until the desired similarity
threshold is reached. Cluster similarity is defined
to be the minimum similarity (or equivalently,
maximum distance) between elements in the
two clusters; that is, maxc?C1,c??C2 ?sim(c, c?).
Complete-link agglomerative clustering tends to
form coherent clusters where the similarity be-
tween any pair within a cluster is high (Manning
et al, 2008).
Cluster Results Table 4 shows the results after
the clustering step, with similarity thresholds of
0.9 and 0.8. Once again, model summaries contain
a lower density of signature caseframes. The sta-
tistical significance results are unchanged. This in-
dicates that simple paraphrasing alone cannot ac-
count for the difference in the signature caseframe
densities, and that some deeper abstraction or se-
mantic inference has occurred.
Note that we are not claiming that a lower den-
sity of signature caseframes necessarily correlates
with a more informative summary. For example,
some automatic summarizers are comparable to
the human abstractors in their relatively low den-
sity of signature caseframes, but these turn out to
be the lowest performing summarization systems
by all measures in the workshop, and they are un-
likely to rival human abstractors in any reasonable
evaluation of summary informativeness. It does,
however, appear that further optimizing centrality-
based measures alone is unlikely to produce bet-
ter informative summaries, even if we analyze the
summary at a syntactic/semantic rather than lexi-
cal level.
4.3 Study 3: Summary Reconstruction
The above studies show that the higher degree
of abstraction in model summaries cannot be ex-
plained by better compression of topically salient
caseframes alone. We now switch perspectives to
ask how model summaries might be automatically
generated at all. We will show that they cannot
be reconstructed solely from the source text, ex-
tending Copeck and Szpakowicz (2004)?s result to
caseframes. However, we also show that if articles
from the same domain are added, reconstruction
then becomes possible. Our measure of whether
a model summary can be reconstructed is case-
frame coverage. We define this to be the propor-
tion of caseframes in a summary that is contained
by some reference set. This is thus a score be-
tween 0 and 1. Unlike in the previous study, we
use the full set of caseframes, not just signature
caseframes, because our goal now to create a hy-
pothesis space from which it is in principle possi-
ble to generate the model summaries.
Results We first calculated caseframe coverage
with respect to the source text alone (Figure 5).
As expected, automatic systems show close to per-
fect coverage, because of their basically extractive
nature, while model summaries show much lower
coverage. These statistics are summarized by Ta-
ble 5. These results present a fundamental limit
to extractive systems, and also text simplification
and sentence fusion methods based solely on the
source text.
The Impact of Domain Knowledge How might
automatic summarizers be able to acquire these
1239
A
G
E
B
H
F
C
D
38
17
2
32
20
6
39
40
5
9
34
14
23
35
19
7
33
41
12
11
37
26
42
21
27
3
24
28
10
4
8
13
16
31
30
25
22
1
15
18
36
System IDs
0.0
0.2
0.4
0.6
0.8
1.0
Co
ve
ra
ge
(a) Initial guided summarization task
G
A
B
E
H
C
F
D
2
38
17
32
11
41
39
20
35
19
26
21
5
23
14
37
40
27
42
12
25
4
6
33
7
8
30
22
31
10
24
13
34
15
28
1
3
9
16
18
36
System IDs
0.0
0.2
0.4
0.6
0.8
1.0
Co
ve
ra
ge
(b) Update summarization task
Figure 5: Coverage of summary text caseframes in source text (Study 3).
Condition Initial Update
Model average 0.77 0.75
Peer average 0.99 0.99
Peer 1 1.00 1.00
Peer 16 1.00 1.00
Peer 22 1.00 1.00
Table 5: Coverage of caseframes in summaries
with respect to the source text. The model aver-
age is statistically significantly different from all
the other conditions p < 10?8 (Study 3).
caseframes from other sources? Traditional sys-
tems that perform semantic inference do so from a
set of known facts about the domain in the form of
a knowledge base, but as we have seen, most ex-
tractive summarization systems do not make much
use of in-domain corpora. We examine adding
in-domain text to the source text to see how this
would affect coverage.
Recall that the 46 topics in TAC 2010 are cat-
egorized into five domains. To calculate the im-
pact of domain knowledge, we add all the docu-
ments that belong in the same domain to the source
text to calculate coverage. To ensure that coverage
does not increase simply due to increasing the size
of the reference set, we compare to the baseline of
adding the same number of documents that belong
to another domain. As shown in Table 6, the ef-
fect of adding more in-domain text on caseframe
coverage is substantial, and noticeably more than
using out-of-domain text. In fact, nearly all case-
frames can be found in the expanded set of arti-
cles. The implication of this result is that it may
be possible to generate better summaries by min-
ing in-domain text for relevant caseframes.
Reference corpus Initial Update
Source text only 0.77 0.75
+out-of-domain 0.91 0.91
+in-domain 0.98 0.97
Table 6: The effect on caseframe coverage of
adding in-domain and out-of-domain documents.
The difference between adding in-domain and out-
of-domain text is significant p < 10?3 (Study 3).
5 Conclusion
We have presented a series of studies to distin-
guish human-written informative summaries from
the summaries produced by current systems. Our
studies are performed at the level of caseframes,
which are able to characterize a domain in terms of
its slots. First, we confirm that model summaries
are more abstractive and aggregate information
from multiple source text sentences. Then, we
show that this is not simply due to summary writ-
ers packing together source text sentences contain-
ing topical caseframes to achieve a higher com-
pression ratio, even if paraphrasing is taken into
account. Indeed, model summaries cannot be re-
constructed from the source text alone. How-
ever, our results are also positive in that we find
that nearly all model summary caseframes can be
found in the source text together with some in-
domain documents.
Current summarization systems have been
heavily optimized towards centrality and lexical-
semantical reasoning, but we are nearing the bot-
tom of the barrel. Domain inference, on the other
hand, and a greater use of in-domain documents
as a knowledge source for domain inference, are
very promising indeed. Mining useful caseframes
1240
for a sentence fusion-based approach has the po-
tential, as our experiments have shown, to deliver
results in just the areas where current approaches
are weakest.
Acknowledgements
This work is supported by the Natural Sciences
and Engineering Research Council of Canada.
References
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?
328.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 335?336. ACM.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 976?
986, Portland, Oregon, USA, June. Association for
Computational Linguistics.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 152?159, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Terry Copeck and Stan Szpakowicz. 2004. Vocabu-
lary agreement among model summaries and source
documents. In Proceedings of the 2004 Document
Understanding Conference (DUC).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Charles Fillmore. 1968. The case for case. In E. Bach
and R. T. Harms, editors, Universals in Linguistic
Theory, pages 1?88. Holt, Reinhart, and Winston,
New York.
Charles J. Fillmore. 1982. Frame semantics. Linguis-
tics in the Morning Calm, pages 111?137.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi
Yousfi-Monod. 2009. Hextac: the creation of a
manual extractive run. In Proceedings of the Second
Text Analysis Conference, Gaithersburg, Maryland,
USA. National Institute of Standards and Technol-
ogy.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2005. English gigaword second edition.
Linguistic Data Consortium, Philadelphia.
Liwei He, Elizabeth Sanocki, Anoop Gupta, and
Jonathan Grudin. 1999. Auto-summarization of
audio-video presentations. In Proceedings of the
Seventh ACM International Conference on Multime-
dia. ACM.
Liwei He, Elizabeth Sanocki, Anoop Gupta, and
Jonathan Grudin. 2000. Comparing presentation
summaries: slides vs. reading vs. listening. In Pro-
ceedings of the SIGCHI Conference on Human Fac-
tors in Computing Systems, CHI ?00, pages 177?
184, New York, NY, USA. ACM.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with Basic Elements. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation (LREC), pages 899?902.
IBM. IBM ILOG CPLEX Optimization Studio V12.4.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference,
pages 178?185.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the National Conference on
Artificial Intelligence.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text sum-
marization. In Proceedings of the 18th Conference
on Computational Linguistics - Volume 1, COLING
?00, pages 495?501, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL
03 on Text Summarization Workshop. Association
for Computational Linguistics.
Chin Y. Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Stan Szpakowicz and
Marie-Francine Moens, editors, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization with-
out human models. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
1241
Processing. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze, 2008. Introduction to Information
Retrieval, chapter 17. Cambridge University Press.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with Columbia?s Newsblaster. In
Proceedings of the Second International Conference
on Human Language Technology Research, pages
280?285. Morgan Kaufmann Publishers Inc.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In Com-
panion Volume of the Proceedings of HLT-NAACL
2003 - Short Papers. Association for Computational
Linguistics.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2):103?233.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004, volume 2004, pages
145?152.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with SumUM.
Computational linguistics, 28(4):497?526.
Horacio Saggion, Juan-Manuel Torres-Moreno, Iria
Cunha, and Eric SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1059?
1067. Association for Computational Linguistics.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Mul-
tidocument summarization via information extrac-
tion. In Proceedings of the First International Con-
ference on Human Language Technology Research.
Association for Computational Linguistics.
1242
Extractive vs. NLG-based Abstractive Summarization of Evaluative 
Text: The Effect of Corpus Controversiality
Giuseppe Carenini and Jackie Chi Kit Cheung1
Department of Computer Science
University of British Columbia
Vancouver, B.C.  V6T 1Z4, Canada 
{carenini,cckitpw}@cs.ubc.ca
Abstract
Extractive  summarization  is  the  strategy  of 
concatenating  extracts  taken  from  a  corpus 
into a summary, while abstractive summariza-
tion  involves  paraphrasing  the  corpus  using 
novel sentences.  We define a novel  measure 
of  corpus  controversiality  of  opinions  con-
tained in evaluative text, and report the results 
of  a  user  study  comparing  extractive  and 
NLG-based abstractive summarization at dif-
ferent levels of controversiality. While the ab-
stractive summarizer performs better overall, 
the results suggest  that the margin by which 
abstraction  outperforms  extraction  is  greater 
when  controversiality  is  high,  providing  a 
context  in  which  the  need  for  generation-
based methods is especially great.
1 Introduction
There are two main approaches to the task of sum-
marization?extraction and abstraction (Hahn and 
Mani, 2000). Extraction involves concatenating ex-
tracts  taken  from  the  corpus  into  a  summary, 
whereas abstraction involves generating novel sen-
tences from information extracted from the corpus. 
It has been observed that in the context of multi-
document summarization of news articles, extrac-
tion may be inappropriate because it may produce 
summaries which are overly verbose or biased to-
wards some sources (Barzilay et al, 1999). How-
ever, there has been little work identifying specific 
factors which might affect the performance of each 
strategy  in  summarizing  evaluative  documents 
containing opinions and preferences, such as cus-
tomer reviews or blogs. This work aims to address 
this gap by exploring one dimension along which 
the effectiveness of the two paradigms could vary; 
namely,  the controversiality of  the opinions  con-
tained in the corpus.
In this paper, we make the following contribu-
tions.  Firstly,  we define a measure  of  controver-
siality of opinions in the corpus based on informa-
tion entropy. Secondly, we run a user study to test 
the  hypothesis  that  a  controversial  corpus  has 
greater  need  of  abstractive  methods  and  conse-
quently of NLG techniques. Intuitively, extracting 
sentences from multiple users whose opinions are 
diverse and wide-ranging may not reflect the over-
all  opinion,  whereas it  may be adequate content-
wise if opinions are roughly the same across users. 
As a secondary contribution, we propose a method 
for structuring text when summarizing controver-
sial corpora. This method is used in our study for 
generating abstractive summaries.
The results  of  the user  study support  our  hy-
pothesis  that  a  NLG summarizer  outperforms  an 
extractive summarizer by more  when the contro-
versiality is high.
2 Related Work
There  has  been  little  work  comparing  extractive 
and abstractive multi-document summarization. A 
previous  study  on  summarizing  evaluative  text 
(Carenini et. al, 2006) showed that extraction and 
abstraction performed about equally well,  though 
for different reasons. The study, however, did not 
1Authors are listed in alphabetical order.
33
look at the effect of the controversiality of the cor-
pus on the relative performance of the two strate-
gies.
To the best of our knowledge, the task of mea-
suring the controversiality of opinions in a corpus 
has  not  been  studied  before.  Some  well  known 
measures  are  related to  this  task,  including vari-
ance, information entropy,  and measures of inter-
rater reliability.  (e.g. Fleiss' Kappa (Fleiss, 1971), 
Krippendorff's Alpha (Krippendorff, 1980)). How-
ever, these existing measures do not satisfy certain 
properties that a sound measure of controversiality 
should possess, prompting us to develop our own 
based on information entropy.
Summary evaluation is a challenging open re-
search  area.  Existing  methods  include  soliciting 
human judgements, task-based approaches, and au-
tomatic approaches.
Task-based evaluation measures  the  effective-
ness of a summarizer for its intended purpose. (e.g. 
(McKeown et al, 2005)) This approach, however, 
is less applicable in this work because we are inter-
ested in evaluating specific properties of the sum-
mary such as the grammaticality and the content, 
which may be difficult to evaluate with an overall 
task-based  approach.  Furthermore,  the  design  of 
the task may intrinsically favour abstractive or ex-
tractive  summarization.  As  an  extreme  example, 
asking for a list of specific comments from users 
would clearly favour extractive summarization.
Another method for summary evaluation is the 
Pyramid method (Nenkova and Passonneau, 2004), 
which takes into account the fact that human sum-
maries with different content can be equally infor-
mative. Multiple human summaries are taken to be 
models, and chunks of meaning known as Summa-
ry Content  Units  (SCU)  are  manually  identified. 
Peer summaries are evaluated based on how many 
SCUs they share with the model  summaries,  and 
the  number  of  model  summaries  in  which  these 
SCUs are found.  Although this  method has been 
tested in DUC 2006 and DUC 2005 (Passonneau et 
al., 2006), (Passonneau et al, 2005) in the domain 
of news articles, it has not been tested for evalua-
tive text. A pilot study that we conducted on a set 
of customer reviews on a product using the Pyra-
mid method revealed several problems specific to 
the  evaluative  domain.  For  example,  summaries 
which  misrepresented  the  polarity of  the  evalua-
tions for a certain feature were not penalized, and 
human summaries sometimes produced contradic-
tory statements about the distribution of the opin-
ions. In one case, one model summary claimed that 
a feature is positively rated, while another claimed 
the opposite, whereas the machine summary indi-
cated that this feature drew mixed reviews. Clear-
ly, only one of these positions should be regarded 
as correct. Further work is needed to resolve these 
problems.
There are also automatic methods for summary 
evaluation,  such  as  ROUGE  (Lin,  2004),  which 
gives  a  score  based  on  the  similarity  in  the  se-
quences of words between a human-written model 
summary  and  the  machine  summary.  While 
ROUGE scores have been shown to often correlate 
quite well with human judgements (Nenkova et al, 
2007), they do not provide insights into the specif-
ic strengths and weaknesses of the summary.
The method of summarization evaluation used 
in this work is to ask users to complete a question-
naire about summaries that they are presented with. 
The questionnaire consists of questions asking for 
Likert  ratings  and  is  adapted  from the  question-
naire in (Carenini et al, 2006).
3 Representative Systems
In our user study, we compare an abstractive and 
an extractive multi-document summarizer that are 
both developed specifically for the evaluative do-
main. These summarizers have been found to pro-
duce quantitatively similar results, and both signif-
icantly outperform a baseline summarizer, which is 
the MEAD summarization framework with all op-
tions set to the default (Radev et al, 2000).
Both summarizers  rely on information  extrac-
tion from the corpus. First, sentences with opinions 
need to be identified, along with the features of the 
entity that are evaluated, the strength, and polarity 
(positive  or  negative)  of  the  evaluation.  For  in-
stance, in a corpus of customer reviews, the sen-
tence ?Excellent picture quality - on par with my 
Pioneer, Panasonic, and JVC players.? contains an 
opinion on the feature  picture quality of  a DVD 
player, and is a very positive evaluation (+3 on a 
scale from -3 to +3). We rely on methods from pre-
vious  work  for  these  tasks  (Hu  and  Liu,  2004). 
Once these features, called Crude Features (CFs), 
are extracted, they are mapped onto a taxonomy of 
User Defined Features (UDFs), so named because 
they can be defined by the user. This mapping pro-
vides a better conceptual organization of the CFs 
34
by  grouping  together  semantically  similar  CFs, 
such as jpeg picture and jpeg slide show under the 
UDF JPEG. For the purposes of our study, feature 
extraction,  polarity/strength identification and the 
mapping from CFs to UDFs are not done automati-
cally as in (Hu and Liu, 2004) and (Carenini et al 
2005). Instead, ?gold standard? annotations by hu-
mans are used in order to focus on the effect of the 
summarization strategy.
3.1 Abstractive Summarizer: SEA
The abstractive summarizer is the Summarizer of 
Evaluative Arguments (SEA), adapted from GEA, 
a system for generating evaluative text tailored to 
the user's preferences (Carenini and Moore, 2006).
In  SEA,  units  of  content  are  organized  by 
UDFs.  The importance of each UDF is based on 
the  number  and  strength  of  evaluations  of  CFs 
mapped to this UDF, as well as the importance of 
its children UDFs. Content selection consists of re-
peating the following two steps  until  the desired 
number of UDFs have been selected: (i)  greedily 
selecting the most important UDF (ii) recalculating 
the measure of importance scores for the remaining 
UDFs.
The content structuring, microplanning, and re-
alization  stages  of  SEA are  adapted  from GEA. 
Each selected UDF is realized in the final summary 
by one clause, generated from a template pattern 
based  on  the  number  and  distribution  of 
polarity/strength evaluations of the UDF. For ex-
ample, the UDF video output with an average po-
larity/strength of near -3 might be realized as ?sev-
eral customers found the video output to be terri-
ble.?
While experimenting with the SEA summariz-
er,  we  noticed  that  the  document  structuring  of 
SEA summaries, which is adapted from GEA and 
is based on guidelines from argumentation theory 
(Carenini  and Moore,  2000),  sometimes  sounded 
unnatural.  We  found  that  controversially  rated 
UDF features (roughly balanced positive and nega-
tive evaluations) were treated as contrasts to those 
which were uncontroversially rated (either mostly 
positive, or mostly negative evaluations). In SEA, 
contrast relations between features are realized by 
cue phrases signalling contrast such as ?however? 
and ?although?. These cue phrases appear to signal 
a contrast that is too strong for the relation between 
controversial and uncontroversial features. An ex-
ample of a SEA summary suffering from this prob-
lem can be found in Figure 1.
To solve this problem, we devised an alterna-
tive content structure for controversial corpora, in 
which  all  controversial  features  appear  first,  fol-
lowed by all  positively and negatively evaluated 
features.
3.2 Extractive Summarizer: MEAD*
The  extractive  approach  is  represented  by 
MEAD*, which is adapted from the open source 
summarization  framework  MEAD (Radev  et  al., 
2000).
After  information  extraction,  MEAD*  orders 
CFs  by the  number  of  sentences  evaluating  that 
CF, and selects a sentence from each CF until the 
word limit has been reached. The sentence that is 
selected for each CF is  the one with the highest 
sum of  polarity/strength evaluations  for  any fea-
ture, so sentences that mention more CFs tend to 
be  selected.  The  selected  sentences  are  then  or-
dered according to the UDF hierarchy by a depth-
first traversal through the UDF tree so that more 
abstract  features  tend  to  precede  more  specific 
ones.
MEAD* does not have a special mechanism to 
deal with controversial features. It is not clear how 
overall controversiality of a feature can be effec-
tively expressed with extraction, as each sentence 
conveys a specific and unique opinion. One could 
include two sentences of opposite polarity for each 
controversial  feature.  However,  in  several  cases 
that we considered, this produced extremely inco-
herent text that did not seem to convey the gist of 
the overall controversiality of the feature.
Customers had mixed opinions about the Apex AD2600. 
Although several customers found the video output to be 
poor and some customers disliked the user interface, cus-
tomers had mixed opinions about the range of compatible 
disc formats. However, users did agree on some things. 
Some users found the extra features to be very good even 
though customers had mixed opinions about the supplied 
universal remote control.
Figure 1: SEA summary of a controversial corpus with 
a document structuring problem. Controversial and un-
controversial features are interwoven. See Figure 3 for 
an example of a summary structured with our alterna-
tive strategy.
35
3.3 Links to the Corpus
In common with the previous study on which this 
is  based,  both  the  SEA and MEAD* summaries 
contain ?clickable footnotes? which are links back 
into an original user review, with a relevant sen-
tence highlighted. These footnotes serve to provide 
details  for  the  abstractive  SEA summarizer,  and 
context for the sentences chosen by the extractive 
MEAD*  summarizer.  They  also  aid  the  partici-
pants of the user study in checking the contents of 
the summary.  The sample sentences for SEA are 
selected by a method similar to the MEAD* sen-
tence selection algorithm. One of the questions in 
the questionnaire provided to users targets the ef-
fectiveness of the footnotes as an aid to the sum-
mary.
4 Measuring Controversiality
The opinion sentences in the corpus are annotated 
with  the  CF  that  they  evaluate  as  well  as  the 
strength, from 1 to 3, and polarity, positive or neg-
ative, of the evaluation. It is natural then, to base a 
measure  of  controversiality on these  annotations. 
To  measure  the  controversiality  of  a  corpus,  we 
first  measure  the  controversiality  of  each  of  the 
features in the corpus. We list two properties that a 
measure of feature controversiality should satisfy.
Strength-sensitivity:  The  measure  should  be 
sensitive to the strength of the evaluations. e.g. Po-
larity/strength  (P/S)  evaluations  of  -2  and  +2 
should be less controversial than -3 and +3
Polarity-sensitivity: The measure should be sen-
sitive the polarity of the evaluations. e.g. P/S eval-
uations of -1 and +1 should be more controversial 
than +1 and +3.
The rationale for this property is that positive 
and negative evaluations are fundamentally differ-
ent, and this distinction is more important than the 
difference in intensity.  Thus,  though a numerical 
scale would suggest that -1 and +1 are as distant as 
+1  and  +3,  a  suitable  controversiality  measure 
should not treat them so.
In addition, the overall measure of corpus con-
troversiality should also satisfy the following two 
features.
CF-weighting: CFs should be weighted by the 
number of evaluations they contain when calculat-
ing the overall value of controversiality for the cor-
pus.
CF-independence: The controversiality of indi-
vidual CFs should not affect each other.
An alternative is to calculate controversiality by 
UDFs  instead  of  CFs.  However,  not  all  CFs 
mapped to the same UDF represent the same con-
cept. For example, the CFs picture clarity and col-
or signal are both mapped to the UDF video out-
put.
4.1 Existing Measures of Variability
Since the problem of measuring the variability of a 
distribution has been well studied, we first exam-
ined existing metrics including variance, entropy, 
kappa, weighted kappa, Krippendorff?s alpha, and 
information  entropy.  Each  of  these,  however,  is 
problematic in their canonical form, leading us to 
devise a new metric based on information entropy 
which satisfies the above properties. Existing met-
rics will now be examined in turn.
Variance:  Variance  does  not  satisfy  polarity-
sensitivity, as the statistic only takes into account 
the difference of each data point to the mean, and 
the sign of the data point plays no role.
Information Entropy: The canonical form of in-
formation entropy does not satisfy strength or po-
larity  sensitivity,  because  the  measure  considers 
the discrete values of the distribution to be an un-
ordered set.
Measures of Inter-rater Reliability: Many mea-
sures exist  to assess inter-rater agreement or dis-
agreement,  which  is  the  task  of  measuring  how 
similarly two or more judges rate one or more sub-
jects beyond chance (dis)agreement.  Various ver-
sions  of  Kappa  and  Krippendorff's  Alpha  (Krip-
pendorff, 1980), which have shown to be equiva-
lent in their most generalized forms (Passonneau, 
1997), can be modified to satisfy all the properties 
listed above. However, there are important differ-
ences between the tasks of  measuring controver-
siality and measuring inter-rater reliability. Kappa 
and Krippendorff's Alpha correct for chance agree-
ment  between raters,  which is  appropriate  in  the 
context  of  inter-rater  reliability  calculations,  be-
cause judges are asked to give their  opinions on 
items that are given to them. In contrast,  expres-
sions  of  opinion  are  volunteered  by  users,  and 
users  self-select  the  features  they  comment  on. 
Thus,  it  is  reasonable  to  assume  that  they never 
randomly  select  an  evaluation  for  a  feature,  and 
chance agreement does not exist.
36
4.2 Entropy-based Controversiality
We define here  our novel  measure  of  controver-
siality, which is based on information entropy be-
cause  it  can  be  more  easily  adapted  to  measure 
controversiality. As has been stated, entropy in its 
original form over the evaluations of a CF is not 
sensitive to strength or polarity. To correct this, we 
first  aggregate  the  positive  and  negative  evalua-
tions for each CF separately, and then calculate the 
entropy based on the resultant Bernoulli distribu-
tion.
Let ps(cfj) be the set of polarity/strength evalua-
tions  for  cfj.  Let  the  importance  of  a  feature, 
imp(cfj), be the sum of the absolute values of the 
polarity/strength evaluations for cfj.
imp?cf j ?= ?
ps k? ps? cf j?
?psk?
Define:
imp_ pos ?cf j?= ?
psk ? ps? cf j?? psk?0
?psk?
imp_ neg ?cf j ?= ?
psk? ps ?cf j??psk?0
?psk?
Now, calculate the entropy of the Bernoulli dis-
tribution  corresponding  to  the  importance  of  the 
two polarities  to  satisfy polarity-sensitivity.  That 
is, Bernoulli with parameter 
? j=imp_ pos ?cf j? /imp?cf j ?
H ?? j?=?? j log2? j??1?? j ? log2 ?1?? j?
Next, we scale this score by the importance of 
the evaluations divided by the maximum possible 
importance for this number of evaluations to satis-
fy strength-sensitivity. Since our scale is from -3 to 
+3, the maximum possible importance for a feature 
is three times the number of evaluations.
max_imp?cf j ?=3??ps ?cf j ??
Then the controversiality of a feature is:
contro ?cf j ?= imp?cf j??H ?? j?max_imp?cf j ?
The case corresponding to the highest possible 
feature  controversiality,  then,  would  be  the  bi-
modal case with equal numbers of evaluations on 
the extreme positive and negative bins (Figure 2). 
Note, however, that controversiality is not simply 
bimodality. A unimodal normal-like distribution of 
evaluations  centred on zero,  for  example,  should 
intuitively  be  somewhat  controversial,  because 
there are equal numbers  of  positive and negative 
evaluations. Our entropy-based feature controver-
siality measure is able to take this into account.
To calculate the controversiality of the corpus, 
a weighted average is taken over the CF controver-
siality scores, with the weight being equal to one 
less  than the  number  of  evaluations  for  that  CF. 
We subtract one to eliminate any CF where only 
one evaluation is made, as that CF has an entropy 
score of one by default  before scaling by impor-
tance.  This  procedure  satisfies  properties  CF-
weighting and CF-independence.
w ?cf j?=?ps ?cf j ???1
contro ?corpus?=? w?cf j ??contro ?cf j?? w?cf j?
Although the annotations in this  corpus range 
from -3 to +3, it would be easy to rescale opinion 
annotations of different corpora to apply this met-
ric. Note that empirically,  this measure correlates 
highly with Kappa and Krippendorff's Alpha.
5 User Study
Our main hypothesis that extractive summarization 
is outperformed even more in the case of contro-
versial corpora was tested by a user study, which 
compared the results of MEAD* and the modified 
SEA. First, ten subsets of 30 user reviews were se-
lected from the corpus of 101 reviews of the Apex 
AD2600  DVD  player  from  amazon.com  by 
stochastic  local  search.  Five of  these  subsets  are 
controversial, with controversiality scores between 
0.83 and 0.88, and five of these are uncontrover-
sial, with controversiality scores of 0. A set of thir-
Figure 2: Sample feature controversiality scores for 
three different distributions of polarity/strength evalua-
tions.
37
ty user reviews per subcorpus was needed to create 
a summary of sufficient length, which in our case 
was about 80 words in length.
Twenty university students were recruited and 
presented with two summaries of the same subcor-
pus,  one  generated  from  SEA  and  one  from 
MEAD*. We generated ten subcorpora in total, so 
each subcorpus was assigned to two participants. 
One of these participants was shown the SEA sum-
mary first, and the other was shown the MEAD* 
summary first, to eliminate the order of presenta-
tion as a source of variation.
The participants were asked to take on the role 
of an employee of Apex, and told that they would 
have to write a summary for the quality assurance 
department  of  the  company about  the  product  in 
question. The purpose of this was to prime them to 
look for information that should be included in a 
summary  of  this  corpus.  They were  given  thirty 
minutes to read the reviews, and take notes.
They were then presented with a questionnaire 
on the summaries,  consisting of ten Likert rating 
questions. Five of these questions targeted the lin-
guistic quality of the summary, based on linguistic 
well-formedness questions used at DUC 2005, one 
targeted the ?clickable footnotes? linking to sample 
sentences  in  the  summary  (see  section  3.3),  and 
three evaluated the contents of the summary.  The 
three questions targeted Recall,  Precision, and the 
general Accuracy of the summary contents respec-
tively. The tenth question asked for a general over-
all quality judgement of the summary.
After  familiarizing  themselves  with  the  ques-
tionnaire, the participants were presented with the 
two summaries in sequence, and asked to fill out 
the questionnaire while reading the summary. They 
were  allowed to  return to  the  original  set  of  re-
views during this time. Lastly, they were given an 
additional questionnaire which asked them to com-
pare  the  two  summaries  that  they  were  shown. 
Questions  in  the  questionnaire  not  found  in 
(Carenini et al, 2006) are attached in Appendix A.
6 Results
6.1 Quantitative Results
We convert the Likert responses from a scale from 
Strongly Disagree to Strong Agree to a scale from 
1 to 5, with 1 corresponding to Strongly Disagree, 
and 5 to Strongly Agree. We group the ten ques-
tions into four categories: linguistic (questions 1 to 
5),  content  (questions 6 to 8),  footnote (question 
9),  and  overall  (question  10).  See  Table  1 for  a 
breakdown of the  responses for  each question at 
each controversiality level.
For our analysis, we adopt a two-step approach 
that has been applied in Computational Linguistics 
(Di Eugenio et al, 2002) as well as in HCI (Hinck-
ley et al, 1997).
First, we perform a two-way Analysis of Vari-
ance (ANOVA) test using the average response of 
the questions in each category. The two factors are 
controversiality of the corpus (high or low) as in-
dependent  samples,  and the summarizer  (SEA or 
MEAD*)  as  repeated  measures.  We  repeat  this 
procedure  for  the  average  of  the  ten  questions, 
termed  Macro below. The p-values of these tests 
are summarized in Table 2.
The results  of  the ANOVA tests  indicate that 
SEA significantly outperforms MEAD* in terms of 
linguistic and overall quality, as well as for all the 
questions combined. It does not significantly out-
perform MEAD* by content, or in the amount that 
the  included  sample  sentences  linked  to  by  the 
footnotes aid the summary.  No significant differ-
ences are found in the performance of the summa-
SEA
Customers had mixed opinions about the Apex AD2600 1,2 
possibly because users were divided on the range of compatible 
disc formats 3,4 and there was disagreement among the users 
about the video output 5,6. However, users did agree on some 
things. Some purchasers found the extra features 7 to be very 
good and some customers really liked the surround sound sup-
port 8 and thought the user interface 9 was poor.
MEAD*
When we tried to hook up the first one , it was broken - the 
motor would not eject discs or close the door . 1 The build 
quality feels solid , it does n't shake or whine while playing 
discs , and the picture and sound is top notch ( both dts and 
dd5.1 sound good ) . 2 The progressive scan option can be 
turned off easily by a button on the remote control which is 
one of the simplest and easiest remote controls i have ever 
seen or used . 3 It plays original dvds and cds and plays 
mp3s and jpegs . 4 
Figure 3: Sample SEA and MEAD* summaries for a controversial corpus. The numbers within the summaries are 
footnotes linking the summary to an original user review from the corpus.
38
rizers  over  the  two levels  of  controversiality  for 
any of the question sets .
While  the  average  differences  in  scores  be-
tween  the  SEA  and  MEAD*  summarizers  are 
greater in the controversial case for the linguistic, 
content, and macro averages as well as the ques-
tion on the overall quality, the p-values for interac-
tion between the two factors in the two-way ANO-
VA test are not significant.
For the second step of the analysis,  we use a 
one-tailed  sign  test  (Siegel  and  Castellan,  1988) 
over the difference in performance of the summa-
rizers at the two levels of controversiality for the 
questions in the questionnaire. We encode + in the 
case  where  the  difference  between  SEA  and 
MEAD* is greater for a question in the controver-
sial setting, ? if the difference is smaller, and we 
discard a question if the difference is the same (e.g. 
the Footnote question). Since the Overall question 
is likely correlated with the responses of the other 
questions, we did not include it in the test. After 
discarding the Footnote question, the p-value over 
the  remaining  eight  questions  is  0.0352,  which 
lends support to our hypothesis that the abstraction 
is better by more when the corpus is controversial.
We  also  analyze  the  users'  summary  prefer-
ences at the two levels of controversiality. A strong 
preference  for  SEA  is  encoded  as  a  5,  while  a 
strong preference for MEAD* is encoded as a 1, 
with 3 being neutral. Using a two-tailed unpaired 
two-sample t-test, we do not find a significant dif-
ference  in  the  participants'  summary  preferences 
(p=0.6237). However, participants sometimes pre-
ferred summaries for reasons other than linguistic 
or  content  quality,  or  may  base  their  judgement 
only on one aspect of the summary. For instance, 
one  participant  rated  SEA  at  least  as  well  as 
MEAD* in all questions except Footnote, yet pre-
ferred MEAD* to SEA overall  precisely because 
MEAD* was felt  to have made better use of the 
footnotes than SEA.
6.2 Qualitative Results
The  qualitative  comments  that  participants  were 
asked to provide along with the Likert scores con-
firmed the observations that led us to formulate the 
initial hypothesis.
In  the  controversial  subcorpora,  participants 
generally  agreed  that  the  abstractive  nature  of 
SEA's generated text was an advantage. For exam-
ple, one participant lauded SEA for attempting to 
?synthesize the reviews? and said that it  ?did re-
flect the mixed nature of the reviews, and covered 
some common complaints.? The participant, how-
ever, said that SEA ?was somewhat misleading in 
that it understated the extent to which reviews were 
negative. In particular, agreement was reported on 
Question 
Set
Controver-
siality
Summarizer Controversiality 
x Summarizer
Linguistic 0.7226 <0.0001 0.2639
Content 0.9215 0.1906 0.2277
Footnote 0.2457 0.7805 1
Overall 0.6301 0.0115 0.2000
Macro 0.7127 0.0003 0.1655
Table 2: Two-way ANOVA p-values.
Table 1: Breakdown of average Likert question responses for each summary at the two levels of controversiali-
ty as well as the difference between SEA and MEAD*.
Controversial Uncontroversial
SEA MEAD* (SEA ? MEAD*)SEA MEAD* (SEA ? MEAD*)
Question Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev. Mean St. Dev.
Grammaticality 4.5 0.53 3.4 1.26 1.1 0.99 4.2 0.92 2.78 1.3 1.56 1.51
Non-redundancy 4.2 0.92 4 1.07 0.25 1.58 3.7 0.95 3.8 1.14 -0.1 1.45
Referential clarity 4.5 0.53 3.44 1.33 1 1.22 4.2 1.03 3.5 1.18 0.7 1.34
Focus 4.11 1.27 2.1 0.88 2.22 0.83 3.9 1.1 2.6 1.35 1.3 1.57
Structure and Coherence 4.1 0.99 1.9 0.99 2.2 1.14 3.8 1.4 2.3 1.06 1.5 1.9
Linguistic 4.29 0.87 2.91 1.35 1.39 1.34 3.96 1.07 3 1.29 0.98 1.63
Recall 2.8 1.32 1.8 1.23 1 1.33 2.5 1.27 2.5 1.43 0 1.89
Precision 3.9 1.1 2.7 1.64 1.2 1.23 3.5 1.27 3.3 0.95 0.2 1.93
Accuracy 3.4 0.97 3.3 1.57 0.1 1.2 3.1 1.52 3.2 1.03 -0.1 2.28
Content 3.37 1.19 2.6 1.57 0.77 1.3 3.03 1.38 3 1.17 0.03 1.97
Footnote 4 1.05 3.9 0.88 0.1 1.66 3.6 1.07 3.5 1.35 0.1 1.6
Overall 3.8 0.79 2.4 1.17 1.4 1.07 3.2 1.23 2.7 0.82 0.5 1.84
Macro ? Footnote 3.92 1.06 2.75 1.41 1.17 1.32 3.57 1.26 2.97 1.2 0.61 1.81
Macro 3.93 1.05 2.87 1.4 1.06 1.39 3.57 1.24 3.02 1.22 0.56 1.79
39
some  features  where  none existed,  and problems 
with reliability were not mentioned.?
Participants disagreed on the information cover-
age of the MEAD* summary. One participant said 
that MEAD* includes ?almost all the information 
about the Apex 2600 DVD player?, while another 
said that it ?does not reflect all information from 
the customer reviews.?
In the  uncontroversial  subcorpora,  more  users 
criticized SEA for its inaccuracy in content selec-
tion. One participant felt that SEA ?made general-
izations that were not precise or accurate.? Partici-
pants  had  specific  comments  about  the  features 
that SEA mentioned that they did not consider im-
portant.  For  example,  one  comment  was  that 
?Compatibility with CDs was not a general prob-
lem,  nor were issues with the remote  control,  or 
video output (when it worked).? MEAD* was criti-
cized for being ?overly specific?, but users praised 
MEAD* for being ?not at all redundant?, and said 
that it ?included information I felt was important.?
7 Conclusion and Future Work
We have explored the controversiality of opinions 
in a corpus of evaluative text as an aspect which 
may determine how well abstractive and extractive 
summarization  strategies  perform.  We  have  pre-
sented a novel measure of controversiality, and re-
ported on the results of a user study which suggest 
that abstraction by NLG outperforms extraction by 
a larger amount in more controversial corpora. We 
have also presented a document structuring strate-
gy for summarization of controversial corpora.
Our  work  has  implications  in  practical  deci-
sions on summarization strategy choice; an extrac-
tive approach, which may be easier to implement 
because of its lack of requirement for natural lan-
guage generation, may suffice if the controversiali-
ty of opinions in a corpus is sufficiently low.
A future approach to summarization of evalua-
tive text might combine extraction and abstraction 
in  order  to  combine  the  different  strengths  that 
each bring to the summary. The controversiality of 
the corpus might be one factor determining the mix 
of abstraction and extraction in the summary. The 
footnotes linking to sample sentences in the corpus 
in SEA are already one form of this combined ap-
proach.  Further  work  is  needed  to  integrate  this 
text into the summary itself, possibly in a modified 
form.
As a final note to our user study, further studies 
should be done with different corpora and summa-
rization systems to increase the external validity of 
our results.
Acknowledgements
We would like to thank Raymond T. Ng, Gabriel 
Murray  and  Lucas  Rizoli  for  their  helpful  com-
ments. This work was partially funded by the Nat-
ural Sciences and Engineering Research Council of 
Canada.
References
Regina Barzilay,  Kathleen R. McKeown, and Michael 
Elhadad. 1999. Information fusion in the context of 
multi-document summarization. In  Proc. 37th ACL, 
pages 550?557.
Giuseppe Carenini and Johanna D. Moore. 2006. Gener-
ating and evaluating evaluative arguments.  Artificial  
Intelligence, 170(11):925-952.
Giuseppe  Carenini,  Raymond  Ng  and  Adam  Pauls. 
2006.  Multi-document  summarization  of  evaluative 
text. In Proc. 11th EACL 2006, pages 305-312.
Giuseppe  Carenini,  Raymond  T.  Ng  and  Ed  Zwart. 
2005.  Extracting Knowledge from Evaluative Text. 
In Proc. 3rd International Conference on Knowledge 
Capture, pages 11-18.
Giuseppe  Carenini  and  Johanna  D.  Moore.  2000.  A 
strategy for generating evaluative arguments. In First  
INLG, pages 47-54, Mitzpe Ramon, Israel.
Barbara Di Eugenio, Michael Glass, and Michael J. Tro-
lio. 2002. The DIAG experiments: Natural language 
generation  for  intelligent  tutoring  systems.  In  INL-
G02, The 2nd INLG, pages 120-127.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment  among  many  raters.  Psychological  Bulletin. 
76:378-382.
U. Hahn and I. Mani. 2000. The challenges of automatic 
summarization. IEEE Computer, 33(11):29-36.
Ken  Hinckley,  Randy  Pausch,  Dennis  Proffitt,  James 
Patten, and Neal Kassell. 1997. Cooperative bimanu-
al action. In Proc. CHI Conference.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In  Proc. 10th ACM SIGKDD 
conference, pages 168-177.
Klaus Krippendorff. 1980.  Content Analysis: An Intro-
duction to Its Methodology. Sage Publications, Bev-
erly Hills, CA.
Chin-Yew Lin. 2004. ROUGE: A Package for Automat-
ic Evaluation of Summaries.  In  Proc. of  Workshop 
on Text Summarization Branches Out, Post-Confer-
ence Workshop of ACL 2004, Barcelona, Spain.
40
2005. Linguistic quality questions from the 2005 docu-
ment  understanding  conference. 
http://duc.nist.gov/duc2005/quality-questions.txt
Kathleen McKeown, Rebecca Passonneau, David Elson, 
Ani Nenkova, and Julia Hirschberg. 2005. Do sum-
maries help? A task-based evaluation of multi-docu-
ment summarization. In Proc. SIGIR 2005.
Ani  Nenkova,  Rebecca  J.  Passonneau,  and K.  McKe-
own. 2007. The pyramid method: incorporating hu-
man  content  selection  variation  in  summarization 
evaluation.  ACM Transactions on Speech and Lan-
guage Processing, 4(2).
Ani Nenkova and Rebecca J. Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proc. NAACL/HLT.
Rebecca  J.  Passonneau,  Kathleen  McKeown,  Sergey 
Sigleman, and Adam Goodkind. 2006. Applying the 
pyramid method in the 2006 Document Understand-
ing Conference. In Proc. DUC'06.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the pyra-
mid method in DUC 2005. In Proc. DUC'05.
Rebecca J. Passonneau. 1997. Applying Reliability Met-
rics  to  Co-Reference  Annotation.  Department  of 
Computer  Science,  Columbia  University,  TR 
CUCS-017-97.
Dragomir  Radev,  Hongyan  Jing,  and  Malgorzata 
Budzikowska.  2000.  Centroid-based  summarization 
of  multiple  documents:  sentence  extraction,  utility-
based  evaluation,  and  user  studies.  In  Proc.  
ANLP/NAACL Workshop on Automatic Summariza-
tion.
S. Siegel and N. J. Castellan, Jr. 1988. Nonparametric 
statistics for the behaviorial sciences. McGraw Hill.
Appendix A. Additional Questions
Footnotes: a) Did you use the footnotes when re-
viewing the summary?
b) Answer this question only if you answered 
?Yes? to the previous question. The clickable foot-
notes were a helpful addition to the summary.
Summary Comparison Questions:
1) List any Pros and Cons you can think of for 
each of the summaries. Point form is okay.
2) Overall, which summary did you prefer?
3) Why did you  prefer  this  summary?  (If  the 
reason overlaps with some points from question 1, 
put a star next to those points in the chart.)
4) Do you have any other comments about the 
reviews or summaries, the tasks, or the experiment 
in general? If so, please write them below.
41
