Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 172?182, Dublin, Ireland, August 23-29 2014.
Building Large-Scale Twitter-Specific Sentiment Lexicon :
A Representation Learning Approach
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\?
, Ming Zhou
?
, Ting Liu
\
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach. We cast sentiment lexicon learning as a phrase-level sentiment classification
task. The challenges are developing effective feature representation of phrases and obtaining
training data with minor manual annotations for building the sentiment classifier. Specifical-
ly, we develop a dedicated neural architecture and integrate the sentiment information of tex-
t (e.g. sentences or tweets) into its hybrid loss function for learning sentiment-specific phrase
embedding (SSPE). The neural network is trained from massive tweets collected with positive
and negative emoticons, without any manual annotation. Furthermore, we introduce the Urban
Dictionary to expand a small number of sentiment seeds to obtain more training data for building
the phrase-level sentiment classifier. We evaluate our sentiment lexicon (TS-Lex) by applying
it in a supervised learning framework for Twitter sentiment classification. Experiment results
on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than
previously introduced sentiment lexicons.
1 Introduction
A sentiment lexicon is a list of words and phrases, such as ?excellent?, ?awful? and ?not bad?, each
of which is assigned with a positive or negative score reflecting its sentiment polarity and strength.
Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment in-
formation and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012;
Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to
estimate the sentiment score of each phrase. These methods typically employ parsing results, syntac-
tic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between
phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et
al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit
the dependency relations between sentiment words and aspect words. However, parsing information and
the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon
from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and
it is hard to have reliable tweet parsers due to the informal language style.
In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation
learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level classi-
fication task. Our method contains two part: (1) a representation learning algorithm to effectively learn
the continuous representation of phrases, which are used as features for phrase-level sentiment classifica-
tion, (2) a seed expansion algorithm that enlarge a small list of sentiment seeds to collect training data for
building the phrase-level classifier. Specifically, we learn sentiment-specific phrase embedding (SSPE),
which is a low-dimensional, dense and real-valued vector, by encoding the sentiment information and
?
This work was partly done when the first author was visiting Microsoft Research.
?
Corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
172
Sentiment 
Classifier 
Sentiment 
Lexicon 
Phrase Embedding 
NEG: goon looser 
Sentiment Seeds 
Tweets with Emoticons 
Soooo nice~ :) 
It?s horrible :( 
Seed 
Expansion 
Representation 
Learning 
POS: good :) 
NEG: poor :( 
NEU: when he 
Training Data 
POS: wanted fave 
NEU: again place 
[1.31,0.97] good: 
[0.99,1.17] coool: 
[-0.81,-0.7] bad: 
[-0.8,-0.72] mess: 
Learning 
Algorithm 
Figure 1: The representation learning approach for building Twitter-specific sentiment lexicon.
syntactic contexts into the continuous representation of phrases
1
. As a result, the nearest neighbors in the
embedding space of SSPE are favored to have similar semantic usage as well as the same sentiment po-
larity. To this end, we extend the existing phrase embedding learning algorithm (Mikolov et al., 2013b),
and develop a dedicated neural architecture with hybrid loss function to incorporate the supervision from
sentiment polarity of text (e.g. tweets). We learn SSPE from tweets, leveraging massive tweets con-
taining positive and negative emoticons as training set without any manual annotation. To obtain more
training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban
Dictionary
2
, which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we
utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in
the sentiment lexicon.
We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learn-
ing framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the bench-
mark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced
lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the top-
performed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by
regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu
and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding
learning algorithms. The main contributions of this work are as follows:
? To our best knowledge, this is the first work that leverages the continuous representation of phrases
for building large-scale sentiment lexicon from Twitter;
? We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from
massive tweets selected with positive and negative emoticons;
? We report the results that our lexicon outperforms existing sentiment lexicons by applying them in
a supervised learning framework for Twitter sentiment classification.
2 Related Work
In this section, we give a brief review about building sentiment lexicon and learning continuous repre-
sentation of words and phrases.
2.1 Sentiment Lexicon Learning
Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das
and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Tur-
ney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods
1
Word/unigram is also regarded as phrase in this paper.
2
http://www.urbandictionary.com/
173
and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by re-
garding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation
algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran,
2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score
of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are
mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples
from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between senti-
ment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet.
Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al.
(2010) represent words and phrases with their syntactic contexts within a window size from the web
documents. Unlike the dominated propagation based methods, we explore the classification framework
based on representation learning for building large-scale sentiment lexicon from Twitter.
To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual
information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and
:(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expres-
sions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and
sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase
embedding and propose a representation learning approach to build sentiment lexicon.
2.2 Learning Continuous Representation of Word and Phrase
Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al.,
2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al.,
2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et
al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al.,
2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend
the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the
embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing
embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentiment-
specific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike
previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike
Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we
integrate the sentiment information of text into our method. It is worth noting that we focus on learning
the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013)
that learn the compositionality of sentences.
3 Methodology
In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a
classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases
as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed
expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment
distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode
the sentiment information into the continuous representation of phrases, we extend an existing phrase
embedding learning algorithm (Mikolov et al., 2013b) and develop a tailored neural architecture to learn
sentiment-specific phrase embedding (SSPE), as described in subsection 3.1. To automatically obtain
more training data for building the phrase-level sentiment classifier, we use the similar words from Urban
Dictionary to expand a small list of sentiment seeds, as described in subsection 3.2.
3.1 Sentiment-Specific Phrase Embedding
Mikolov et al. (2013b) introduce Skip-Gram to learn phrase embedding based on the context words of
phrases, as illustrated in Figure 2(a).
Given a phrase w
i
, Skip-Gram maps it into its continuous representation e
i
. Then, Skip-Gram utilizes
174
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
ei 
wi-2 wi-1 wi+1 wi+2 
wi 
polj 
(a) Skip-Gram (b) Our Model 
sej 
sj 
Figure 2: The traditional Skip-Gram model and our neural architecture for learning sentiment-specific
phrase embedding (SSPE).
e
i
to predict the context words of w
i
, namely w
i?2
, w
i?1
, w
i+1
, w
i+2
, et al. Hierarchical softmax (Morin
and Bengio, 2005) is leveraged to accelerate the training procedure because the vocabulary size of phrase
table is typically huge. The objective of Skip-Gram is to maximize the average log probability:
f
syntactic
=
1
T
T
?
i=1
?
?c?j?c,j 6=0
log p(w
i+j
|e
i
) (1)
where T is the occurrence of each phrase in the corpus, c is the window size, e
i
is the embedding of the
current phrase w
i
, w
i+j
is the context words of w
i
, p(w
i+j
|e
i
) is calculated with hierarchical softmax.
The basic softmax unit is calculated as softmax
i
= exp(z
i
)/
?
k
exp(z
k
). We leave out the details
of hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013b) due to the page limit. It is
worth noting that, Skip-Gram is capable to learn continuous representation of words and phrases with
the identical model (Mikolov et al., 2013b).
To integrate sentiment information into the continuous representation of phrases, we develop a tailored
neural architecture to learn SSPE, as illustrated in Figure 2(b). Given a triple ?w
i
, s
j
, pol
j
? as input,
where w
i
is a phrase contained in the sentence s
j
whose gold sentiment polarity is pol
j
, our training
objective is to (1) utilize the embedding of w
i
to predict its context words, and (2) use the sentence
representation se
j
to predict the gold sentiment polarity of s
j
, namely pol
j
. We simply average the
embedding of phrases contained in a sentence as its continuous representation (Huang et al., 2012). The
objective of the sentiment part is to maximize the average of log sentiment probability:
f
sentiment
=
1
S
S
?
j=1
log p(pol
j
|se
j
) (2)
where S is the occurrence of each sentence in the corpus,
?
k
pol
jk
= 1. For binary classification
between positive and negative, the distribution of [0,1] is for positive and [1,0] is for negative. Our final
training objective is to maximize the linear combination of the syntactic and sentiment parts:
f = ? ? f
syntactic
+ (1? ?) ? f
sentiment
(3)
where ? weights the two parts. Accordingly, the nearest neighbors in the embedding space of SSPE are
favored to have similar semantic usage as well as the same sentiment polarity.
We train our neural model with stochastic gradient descent and use AdaGrad (Duchi et al., 2011) to
update the parameters. We empirically set embedding length as 50, window size as 3 and the learning
rate of AdaGrad as 0.1. Hyper-parameter ? is tuned on the development set. To obtain large-scale
training corpus, we collect tweets from April, 2013 through TwitterAPI. After filtering the tweets that
are too short (< 5 words) and removing @user and URLs, we collect 10M tweets (5M positive and 5M
negative) with positive and negative emoticons
3
, which is are utilized as the training data to train our
neural model. The vocabulary size is 750,000 after filtering the 1?4 grams through frequency.
3
We use the emoticons selected by Hu et al. (2013), namely :) : ) :-) :D =) as positive and :( : ( :-( as negative ones.
175
3.2 Seed Expansion with Urban Dictionary
Urban Dictionary is a web-based dictionary that contains more than seven million definitions until March,
2013
4
. It was intended as a dictionary of slang, cultural words or phrases not typically found in standard
dictionaries, but it is now used to define any word or phrase. For each item in Urban Dictionary, there is
a list of similar words contributed by volunteers. For example, the similar words of ?cooool? are ?cool?,
?awesome?, ?coooool?, et al
5
and the similar words of ?not bad? are ?good?, ?ok? and ?cool?, et al
6
.
These similar words are typically semantically close to and have the same sentiment polarity with the
target word. We conduct preliminary statistic on the items of Urban Dictionary from ?a? to ?z?, and
find that there are total 799,430 items containing similar words and each of them has about 10.27 similar
words on average.
We utilize Urban Dictionary to expand little sentiment seeds for collecting training data for building
the phrase-level sentiment classifier. We manually label the top frequent 500 words from the vocabulary
of SSPE as positive, negative or neutral. After removing the ambiguous ones, we obtain 125 positive, 109
negative and 140 neutral words, which are regarded as the sentiment seeds
7
. Afterwards, we leverage
the similar words from Urban Dictionary to expand the sentiment seeds. We first build a k-nearest
neighbors (KNN) classifier by regarding the sentiment seeds as gold standard. Then, we employ the KNN
classifier on the items of Urban Dictionary containing similar words, and predict a three-dimensional
discrete vector [knn
pos
, knn
neg
, knn
neu
] for each item, reflecting the hits numbers of sentiment seeds
with different sentiment polarity in its similar words. For example, the vector value of ?not bad? is
[10, 0, 0], which means that there are 10 positive seeds, 0 negative seeds and 0 neutral seeds occur in
its similar words. To ensure the quality of the expanded words, we set threshold for each category to
collect the items with high quality as expanded words. Take the positive category as an example, we
keep an item as positive expanded word if it satisfies knn
pos
> knn
neg
+ threshold
pos
and knn
pos
>
knn
neu
+ threshold
pos
simultaneously. We empirically set the thresholds of positive, negative and
neutral as 6,3,2 respectively by balancing the size of expanded words in three categories. After seed
expansion, we collect 1,512 positive, 1,345 negative and 962 neutral words, which are used as the training
data to build the phrase-level sentiment classifier. We also tried the propagation methods to expand the
sentiment seeds, namely iteratively added the similar words of sentiment seeds from Urban Dictionary
into the expanded word collection. However, the quantity of expanded words is less than the KNN-based
results and the quality is relatively poor.
After obtaining the training data and feature representation of phrases, we build the phrase-level clas-
sifier with softmax, whose length is two for the positive vs negative case:
y(w) = softmax(? ? e
i
+ b) (4)
where ? and b are the parameters of classifier, e
i
is the embedding of the current phrase w
i
, y(w) is the
predicted sentiment distribution of item w
i
. We employ the classifier to predict the sentiment distribution
of each phrase in the vocabulary of SSPE, and save the phrases as well as their sentiment probability in
the positive (negative) lexicon if the positive (negative) probability is larger than 0.5.
4 Experiment
In this section, we conduct experiments to evaluate the effectiveness of our sentiment lexicon (TS-Lex)
by applying it in the supervised learning framework for Twitter sentiment classification, as given in
subsection 4.1. We also directly evaluate the quality of SSPE as it forms the fundamental component for
building sentiment lexicon. We use SSPE as the feature for sentiment classification of items in existing
sentiment lexicons, as described in subsection 4.2.
4
http://en.wikipedia.org/wiki/Urban Dictionary
5
http://www.urbandictionary.com/define.php?term=cooool
6
http://www.urbandictionary.com/define.php?term=not+bad
7
We will publish the sentiment seeds later.
176
4.1 Twitter Sentiment Classification
Experiment Setup and Dataset We conduct experiments on the benchmark Twitter sentiment classi-
fication dataset (message-level) from SemEval 2013 (Nakov et al., 2013). The training and development
sets were completely released to task participants. However, we were unable to download all the training
and development sets because some tweets were deleted or not available due to modified authorization
status. The statistic of the positive and negative tweets in our dataset are given in Table 1(b). We train
positive vs negative classifier with LibLinear (Fan et al., 2008) with default settings on the training set,
tune parameters -c on the dev set and evaluate on the test set. The evaluation metric is Macro-F1.
(a) Sentiment Lexicons
Lexicon Positive Negative Total
HL 2,006 4,780 6,786
MPQA 2,301 4,150 6,451
NRC-Emotion 2,231 3,324 5,555
TS-Lex 178,781 168,845 347,626
HashtagLex 216,791 153,869 370,660
Sentiment140Lex 480,008 260,158 740,166
(b) SemEval 2013 Dataset
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 1: Statistic of sentiment lexicons and Twitter sentiment classification datasets.
Results and Analysis We compare TS-Lex with HL
8
(Hu and Liu, 2004), MPQA
9
(Wilson et al.,
2005), NRC-Emotion
10
(Mohammad and Turney, 2012), HashtagLex and Sentiment140Lex
11
(Moham-
mad et al., 2013). The statistics of TS-Lex and other sentiment lexicons are illustrated in Table 1(a). HL,
MPQA and NRC-Emotion are traditional sentiment lexicons with a relative small lexicon size. Hashta-
gLex and Sentiment140Lex are Twitter-specific sentiment lexicons. We can find that, TS-Lex is larger
than the traditional sentiment lexicons.
We evaluate the effectiveness of TS-Lex by applying it as the features for Twitter sentiment classifica-
tion in the supervised learning framework (Pang et al., 2002). We conduct experiments in two settings,
namely only utilizing the lexicon features (Unique) and appending lexicon feature to existing feature
sets (Appended). In the first setting, we design the lexicon features as same as the top-performed Twit-
ter sentiment classification system in SemEval2013
12
(Mohammad et al., 2013). For each sentiment
polarity (positive vs negative), the lexicon features are:
? total count of tokens in the tweet with score greater than 0;
? the sum of the scores for all tokens in the tweet;
? the maximal score;
? the non-zero score of the last token in the tweet;
In the second experiment setting, we append the lexicon features to the existing basic feature. We use
the feature sets of Mohammad et al. (2013) excluding the lexicon feature as the basic feature, including
bag-of-words, pos-tagging, emoticons, hashtags, elongated words, etc. Experiment results of the Unique
features and Appended features from different sentiment lexicons on Twitter sentiment classification are
given in Table 2(a).
From Table 2(a), we can find that TS-Lex yields best performance in both Unique and Appended
feature sets among all sentiment lexicons, including two large-scale Twitter-specific sentiment lexicons.
The reason is that the classifier for building TS-Lex utilize (1) the well developed feature representation
of phrases (SSPE), which captures the semantic and sentiment connections between phrases, and (2) the
enlarged sentiment words through web intelligence as training data. HashtagLex and Sentiment140Lex
8
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html#lexicon
9
http://mpqa.cs.pitt.edu/lexicons/subj lexicon/
10
http://www.saifmohammad.com/WebPages/ResearchInterests.html
11
We utilize the unigram and bigram lexicons from HashtagLex and Sentiment140Lex.
12
http://www.saifmohammad.com/WebPages/Abstracts/NRC-SentimentAnalysis.htm
177
(a)
Lexicon Unique Appended
HL 60.49 79.40
MPQA 59.15 76.54
NRC-Emotion 54.81 76.79
HashtagLex 65.30 76.67
Sentiment140Lex 72.51 80.68
TS-Lex 78.07 82.36
(b)
Lexicon Unique
Seed 57.92
Expand 60.69
Lexicon(seed) 74.64
TS-Lex 78.07
Table 2: Macro-F1 on Twitter sentiment classification with different lexicon features.
only utilize the relations between phrases and hashtag/emoticon seeds, yet do not well capture the con-
nections between phrases. In the Unique setting, the performances of the traditional lexicons (HL, MPQA
and NRC-Emotion) are lower than large-scale Twitter-specific lexicons (HashtagLex, Sentiment140Lex
and our lexicon). The reason is that, tweets have the informal language style and contain slangs and di-
verse multi-word phrases, which are not well covered by the traditional sentiment lexicons with a small
size. After incorporating the lexicon feature of TS-Lex into the top-performs system (Mohammad et al.,
2013), we further improve the macro-F1 from 84.70% to 85.65%.
Effect of Seed Expansion with Urban Dictionary To verify the effectiveness of seed expansion
through Urban Dictionary, we conduct experiments by applying (1) sentiment seeds (Seed), (2) words
after expansion (Expand), (3) sentiment lexicon generated from the classifier only utilizing sentiment
seeds as training data (Lexicon(seed)), (4) the final lexicon (TS-Lex) exploiting the expanded words as
training data to build sentiment classifier, to produce lexicon features, and only use them for Twitter
sentiment classification (Unique). From Table 2(b), we find that the performance of sentiment seeds and
expanded words are relatively poor due to their low coverage. Under this scenario, seed expansion yields
2.77% improvement (from 57.92% to 60.69%) on macro-F1. By utilizing the expanded words as training
data to build the phrase-level sentiment classifier, TS-Lex obtains 3.43% improvements on Twitter senti-
ment classification (from 74.64% to 78.07%), which verifies the effectiveness of seed expansion through
Urban Dictionary. In addition, we find that only using a small number of sentiment seeds as the training
data, we can obtain superior performance (74.64%) than all baseline lexicons. This indicates that the
representation learning approach effectively capture the semantic and sentimental connections between
phrases through SSPE, and leverage them for building the sentiment lexicon.
Effect of ? in SSPE We tune the hyper-parameter ? of SSPE on the development set of SemEval 2013,
and study its influence on the performance of Twitter sentiment classification by applying the generated
lexicon as features. We utilize the expanded words as training data to train softmax and only utilize the
lexicon features (Unique) for Twitter sentiment classification. Experiment results with different ? are
illustrated in Figure 3(a).
From Figure 3(a), we can see that that SSPE performs better when ? is in the range of [0.1, 0.3], which
is dominated by the sentiment information. The model with ? = 1 stands for Skip-Gram model. The
sharp decline at ? = 1 indicates the importance of sentiment information in learning sentiment-specific
phrase embedding for building sentiment lexicon.
Discussion In the experiment, we do not apply TS-Lex into the unsupervised learning framework for
Twitter sentiment classification. The reason is that the lexicon-based unsupervised method typically
require the sentiment lexicon to have high precision, yet our task is to build large-scale lexicon (TS-Lex)
with broad coverage. We leave this as the future work, although we may set higher threshold (e.g. larger
than 0.5) to increase the precision of TS-Lex and loose the recall.
4.2 Evaluation of Different Representation Learning Methods
Experiment Setup and Dataset We conduct sentiment classification of items in two traditional senti-
ment lexicons, HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005), to evaluate the effective of the
178
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.735
0.74
0.745
0.75
0.755
0.76
0.765
0.77
0.775
0.78
0.785
?
Ma
cro
?F1
 
 
TS?Lex
(a) SSPE with different ? on the development set for Twitter
sentiment classification.
0.66
0.68
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
C&W ReEmbed(C&W) W2V ReEmbed(W2V) MVSA SSPE
MPQAHL
(b) Sentiment classification of lexicons with different embed-
ding learning algrithms.
Figure 3: Experiment results with different settings.
sentiment-specific phrase embedding (SSPE). We train the positive vs negative classifier with LibLin-
ear (Fan et al., 2008). The evaluation metric is the macro-F1 of 5-fold cross validation. The statistics of
HL and MPQA are listed in Table 1(a).
Baseline Embedding Learning Algorithms We compare SSPE with the following embedding learn-
ing algorithms:
(1) C&W. C&W is one of the most representative embedding learning algorithms (Collobert et al.,
2011) for learning word embedding, which has been proven effective in many NLP tasks.
(2) W2V. Mikolov et al. (2013a) introduce Word2Vec for learning the continuous vectors for words
and phrases. We utilize Skip-Gram as it performs better than CBOW in the experiments.
(3) MVSA. Maas et al. (2011) learn word vectors for sentiment analysis with a probabilistic model of
documents utilizing the sentiment polarity of documents.
(4) ReEmbed. Lebret et al. (2013) learn task-specific embedding from existing embedding and task-
specific corpus. We utilize the training set of Twitter sentiment classification as the labeled corpus to
re-embed words. ReEmbed(C&W) and ReEmbed(W2V) stand for the use of different embedding results
as the reference word embedding.
The embedding results of the baseline algorithms and SSPE are trained with the same dataset and
parameter sets.
Results and Analysis Experiment results of the baseline embedding learning algorithms and SSPE are
given in Figure 3(b). We can see that SSPE yields best performance on both lexicons. The reason is that
SSPE effectively encode the sentiment information of tweets as well as the syntactic contexts of phrases
from massive data into the continuous representation of phrases. The performances of C&W and W2V
are relatively low because they only utilize the syntactic contexts of items, yet ignore the sentiment in-
formation of text, which is crucial for sentiment analysis. ReEmbed(C&W) and ReEmbed(W2V) achieve
better performance than C&W and W2V because the sentiment information of sentences are incorporated
into the continuous representation of phrases. There is a gap between ReEmbed and SSPE because SSPE
leverages more sentiment supervision from massive tweets collected by positive and negative emoticons.
5 Conclusion
In this paper, we propose building large-scale Twitter-specific sentiment lexicon with a representation
learning approach. Our method contains two parts: (1) a representation learning algorithm to effectively
learn the embedding of phrases, which are used as features for classification, (2) a seed expansion al-
gorithm that enlarge a small list of sentiment seeds to obtain training data for building the phrase-level
sentiment classifier. We introduce a tailored neural architecture and integrate the sentiment information
of tweets into its hybrid loss function for learning sentiment-specific phrase embedding (SSPE). We
learn SSPE from the tweets collected by positive and negative emoticons, without any manual annota-
179
tion. To collect more training data for building the phrase-level classifier, we utilize the similar words
from Urban Dictionary to expand a small list of sentiment seeds. The effectiveness of our sentiment
lexicon (TS-Lex) has been verified through applied in the supervised learning framework for Twitter
sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-
Lex outperforms previously introduced sentiment lexicons and further improves the top-perform system
in SemEval 2013 with feature combination. In future work, we plan to apply TS-Lex into the unsuper-
vised learning framework for Twitter sentiment classification.
Acknowledgements
We thank Nan Yang, Yajuan Duan and Yaming Sun for their great help. This research was partly sup-
ported by National Natural Science Foundation of China (No.61133012, No.61273321, No.61300113).
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource
for sentiment analysis and opinion mining. In LREC, volume 10, pages 2200?2204.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. Journal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspec-
tives. IEEE Trans. Pattern Analysis and Machine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P Sheth. 2012. Extracting diverse
sentiment expressions with target-dependent polarity from twitter. In ICWSM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537.
George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word
observations. ICML.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for amazon: Sentiment extraction from small talk on the web.
Management Science, 53(9):1375?1388.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochas-
tic optimization. The Journal of Machine Learning Research, pages 2121?2159.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss classifi-
cation. In Proceedings of the 14th ACM international conference on Information and knowledge management,
pages 617?624. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pageranking wordnet synsets: An application to opinion mining. In
ACL, volume 7, pages 442?431.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. The Journal of Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and applications for sentiment analysis. Communications of the ACM,
56(4):82?89.
Ming Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu. 2013. Unsupervised sentiment analysis with emotional signals.
In Proceedings of the International World Wide Web Conference, pages 607?618.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In ACL, pages 873?882. ACL.
180
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th
international conference on Computational Linguistics, page 1367. Association for Computational Linguistics.
Igor Labutov and Hod Lipson. 2013. Re-embedding words. In Annual Meeting of the Association for Computa-
tional Linguistics.
R?emi Lebret, Jo?el Legrand, and Ronan Collobert. 2013. Is deep learning really necessary for word embeddings?
NIPS workshop.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xiaoyan Zhu. 2012. Cross-domain co-extraction of
sentiment and topic lexicons. In Proceedings of the 50th ACL, pages 410?419. ACL, July.
Dekang Lin. 1994. Principar: an efficient, broad-coverage, principle-based parser. In Proceedings of the 15th
conference on COLING, pages 482?488. Association for Computational Linguistics.
Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies,
5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of
words and phrases and their compositionality. The Conference on Neural Information Processing Systems.
Saif M Mohammad and Peter D Turney. 2012. Crowdsourcing a word?emotion association lexicon. Computa-
tional Intelligence.
Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-the-art in
sentiment analysis of tweets. Proceedings of the International Workshop on Semantic Evaluation.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceed-
ings of the international workshop on artificial intelligence and statistics, pages 246?252.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic
Evaluation, volume 13.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information
retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine
learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 79?86.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double
propagation. In IJCAI, volume 9, pages 1199?1204.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational linguistics, 37(1):9?27.
Delip Rao and Deepak Ravichandran. 2009. Semi-supervised polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675?682.
Association for Computational Linguistics.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng, and C.D. Manning. 2011. Semi-supervised recursive
autoencoders for predicting sentiment distributions. In Conference on Empirical Methods in Natural Language
Processing, pages 151?161.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher
Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Conference
on Empirical Methods in Natural Language Processing, pages 1631?1642.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2014. Radical-enhanced chinese
character embedding. arXiv preprint arXiv:1404.4714.
181
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word
embedding for twitter sentiment classification. In Procedding of the 52th Annual Meeting of Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. Annual Meeting of the Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of
reviews. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 417?424.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics, pages 777?785. Association for Computational
Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives from corpora. In AAAI/IAAI, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
347?354.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion targets in a
two-stage framework. In Proceedings of the 51st ACL, pages 1764?1773. ACL.
182
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Joint Segmentation and Classification Framework
for Sentiment Analysis
Duyu Tang
\?
, Furu Wei
?
, Bing Qin
\
, Li Dong
]?
, Ting Liu
\
, Ming Zhou
?
\
Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
]
Beihang University, Beijing, China
\
{dytang, qinb, tliu}@ir.hit.edu.cn
?
{fuwei, mingzhou}@microsoft.com
]
donglixp@gmail.com
Abstract
In this paper, we propose a joint segmenta-
tion and classification framework for sen-
timent analysis. Existing sentiment clas-
sification algorithms typically split a sen-
tence as a word sequence, which does not
effectively handle the inconsistent senti-
ment polarity between a phrase and the
words it contains, such as ?not bad? and
?a great deal of ?. We address this issue
by developing a joint segmentation and
classification framework (JSC), which si-
multaneously conducts sentence segmen-
tation and sentence-level sentiment classi-
fication. Specifically, we use a log-linear
model to score each segmentation candi-
date, and exploit the phrasal information
of top-ranked segmentations as features to
build the sentiment classifier. A marginal
log-likelihood objective function is de-
vised for the segmentation model, which
is optimized for enhancing the sentiment
classification performance. The joint mod-
el is trained only based on the annotat-
ed sentiment polarity of sentences, with-
out any segmentation annotations. Experi-
ments on a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that, our joint model performs com-
parably with the state-of-the-art methods.
1 Introduction
Sentiment classification, which classifies the senti-
ment polarity of a sentence (or document) as posi-
tive or negative, is a major research direction in the
field of sentiment analysis (Pang and Lee, 2008;
Liu, 2012; Feldman, 2013). Majority of existing
approaches follow Pang et al. (2002) and treat sen-
?
This work was partly done when the first and fourth
authors were visiting Microsoft Research.
timent classification as a special case of text cate-
gorization task. Under this perspective, previous
studies typically use pipelined methods with two
steps. They first produce sentence segmentation-
s with separate text analyzers (Choi and Cardie,
2008; Nakagawa et al., 2010; Socher et al., 2013b)
or bag-of-words (Paltoglou and Thelwall, 2010;
Maas et al., 2011). Then, feature learning and sen-
timent classification algorithms take the segmenta-
tion results as inputs to build the sentiment classi-
fier (Socher et al., 2011; Kalchbrenner et al., 2014;
Dong et al., 2014).
The major disadvantage of a pipelined method
is the problem of error propagation, since sen-
tence segmentation errors cannot be corrected by
the sentiment classification model. A typical kind
of error is caused by the polarity inconsistency be-
tween a phrase and the words it contains, such
as ?not bad, bad? and ?a great deal of, great?.
The segmentations based on bag-of-words or syn-
tactic chunkers are not effective enough to han-
dle the polarity inconsistency phenomenons. The
reason lies in that bag-of-words segmentations re-
gard each word as a separate unit, which losses
the word order and does not capture the phrasal
information. The segmentations based on syntac-
tic chunkers typically aim to identify noun group-
s, verb groups or named entities from a sentence.
However, many sentiment indicators are phrases
constituted of adjectives, negations, adverbs or id-
ioms (Liu, 2012; Mohammad et al., 2013a), which
are splitted by syntactic chunkers. Besides, a bet-
ter approach would be to utilize the sentiment in-
formation to improve the segmentor. Accordingly,
the sentiment-specific segmentor will enhance the
performance of sentiment classification in turn.
In this paper, we propose a joint segmentation
and classification framework (JSC) for sentimen-
t analysis, which simultaneous conducts sentence
segmentation and sentence-level sentiment clas-
sification. The framework is illustrated in Fig-
477
Segmentations Input 
that is not bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
that  is  not  bad 
Polarity: +1 
-1 
-1 
+1 
+1 
<+1,-1>   NO 
Polarity Update 
<+1,-1>   NO 
<+1,+1>  YES 
<+1,+1>  YES 
SC SEG CG 
Update 
SC 
2.3  
1.6 
0.6 
0.4 
0.6 
0.4 
2.3 
1.6 
SEG 
Rank 
Top K  
Figure 1: The joint segmentation and classification framework (JSC) for sentiment classification. CG
represents the candidate generation model, SC means the sentiment classification model and SEG stands
for the segmentation ranking model. Down Arrow means the use of a specified model, and Up Arrow
indicates the update of a model.
ure 1. We develop (1) a candidate generation mod-
el to generate the segmentation candidates of a
sentence, (2) a segmentation ranking model to s-
core each segmentation candidate of a given sen-
tence, and (3) a classification model to predic-
t the sentiment polarity of each segmentation. The
phrasal information of top-ranked candidates from
the segmentation model are utilized as features to
build the sentiment classifier. In turn, the predict-
ed sentiment polarity of segmentation candidates
from classification model are leveraged to update
the segmentor. We score each segmentation can-
didate with a log-linear model, and optimize the
segmentor with a marginal log-likelihood objec-
tive. We train the joint model from sentences an-
notated only with sentiment polarity, without any
segmentation annotations.
We evaluate the effectiveness of our joint mod-
el on a benchmark Twitter sentiment classifica-
tion dataset in SemEval 2013. Results show that
the joint model performs comparably with state-
of-the-art methods, and consistently outperforms
pipeline methods in various experiment settings.
The main contributions of the work presented in
this paper are as follows.
? To our knowledge, this is the first work that
automatically produces sentence segmenta-
tion for sentiment classification within a joint
framework.
? We show that the joint model yields com-
parable performance with the state-of-the-art
methods on the benchmark Twitter sentiment
classification datasets in SemEval 2013.
2 Related Work
Existing approaches for sentiment classification
are dominated by two mainstream directions.
Lexicon-based approaches (Turney, 2002; Ding
et al., 2008; Taboada et al., 2011; Thelwall et
al., 2012) typically utilize a lexicon of sentiment
words, each of which is annotated with the sen-
timent polarity or sentiment strength. Linguis-
tic rules such as intensifications and negations are
usually incorporated to aggregate the sentimen-
t polarity of sentences (or documents). Corpus-
based methods treat sentiment classification as a
special case of text categorization task (Pang et al.,
2002). They mostly build the sentiment classifier
from sentences (or documents) with manually an-
notated sentiment polarity or distantly-supervised
corpora collected by sentiment signals like emoti-
cons (Go et al., 2009; Pak and Paroubek, 2010;
Kouloumpis et al., 2011; Zhao et al., 2012).
Majority of existing approaches follow Pang et
al. (2002) and employ corpus-based method for
sentiment classification. Pang et al. (2002) pi-
oneer to treat the sentiment classification of re-
views as a special case of text categorization prob-
lem and first investigate machine learning meth-
ods. They employ Naive Bayes, Maximum En-
tropy and Support Vector Machines (SVM) with a
diverse set of features. In their experiments, the
best performance is achieved by SVM with bag-
of-words feature. Under this perspective, many s-
tudies focus on designing or learning effective fea-
tures to obtain better classification performance.
On movie or product reviews, Wang and Man-
ning (2012) present NBSVM, which trades-off
478
between Naive Bayes and NB-feature enhanced
SVM. Kim and Zhai (2009) and Paltoglou and
Thelwall (2010) learn the feature weights by in-
vestigating variants weighting functions from In-
formation Retrieval. Nakagawa et al. (2010) uti-
lize dependency trees, polarity-shifting rules and
conditional random fields (Lafferty et al., 2001)
with hidden variables to compute the documen-
t feature. On Twitter, Mohammad et al. (2013b)
develop a state-of-the-art Twitter sentiment classi-
fier in SemEval 2013, using a variety of sentiment
lexicons and hand-crafted features.
With the revival of deep learning (representa-
tion learning (Hinton and Salakhutdinov, 2006;
Bengio et al., 2013; Jones, 2014)), more recen-
t studies focus on learning the low-dimensional,
dense and real-valued vector as text features for
sentiment classification. Glorot et al. (2011) inves-
tigate Stacked Denoising Autoencoders to learn
document vector for domain adaptation in sen-
timent classification. Yessenalina and Cardie
(2011) represent each word as a matrix and
compose words using iterated matrix multipli-
cation. Socher et al. propose Recursive Au-
toencoder (RAE) (2011), Matrix-Vector Recursive
Neural Network (MV-RNN) (2012) and Recur-
sive Neural Tensor Network (RNTN) (2013b) to
learn the composition of variable-length phrases
based on the representation of its children. To
learn the sentence representation, Kalchbrenner et
al. (2014) exploit Dynamic Convolutional Neu-
ral Network and Le and Mikolov (2014) inves-
tigate Paragraph Vector. To learn word vectors
for sentiment analysis, Maas et al. (2011) propose
a probabilistic document model following Blei et
al. (2003), Labutov and Lipson (2013) re-embed
words from existing word embeddings and Tang
et al. (2014b) develop three neural networks to
learn word vectors from tweets containing posi-
tive/negative emoticons.
Unlike most previous corpus-based algorithms
that build sentiment classifier based on splitting a
sentence as a word sequence, we produce sentence
segmentations automatically within a joint frame-
work, and conduct sentiment classification based
on the segmentation results.
3 The Proposed Approach
In this section, we first give the task definition
of two tasks, namely sentiment classification and
sentence segmentation. Then, we present the
overview of the proposed joint segmentation and
classification model (JSC) for sentiment analysis.
The segmentation candidate generation model and
the segmentation ranking model are described in
Section 4. The details of the sentiment classifica-
tion model are presented in Section 5.
3.1 Task Definition
The task of sentiment classification has been well
formalized in previous studies (Pang and Lee,
2008; Liu, 2012). The objective is to identify the
sentiment polarity of a sentence (or document) as
positive or negative
1
.
The task of sentence segmentation aims to s-
plit a sentence into a sequence of exclusive part-
s, each of which is a basic computational unit of
the sentence. An example is illustrated in Table 1.
The original text ?that is not bad? is segmented
as ?[that] [is] [not bad]?. The segmentation re-
sult is composed of three basic computational u-
nits, namely [that], [is] and [not bad].
Type Sample
Sentence that is not bad
Segmentation [that] [is] [not bad]
Basic units [that], [is], [not bad]
Table 1: Example for sentence segmentation.
3.2 Joint Model (JSC)
The overview of the proposed joint segmentation
and classification model (JSC) for sentiment anal-
ysis is illustrated in Figure 1. The intuitions of the
joint model are two-folds:
? The segmentation results have a strong influ-
ence on the sentiment classification perfor-
mance, since they are the inputs of the sen-
timent classification model.
? The usefulness of a segmentation can be
judged by whether the sentiment classifier
can use it to predict the correct sentence po-
larity.
Based on the mutual influence observation, we
formalize the joint model in Algorithm 1. The in-
puts contain two parts, training data and feature
extractors. Each sentence s
i
in the training data
1
In this paper, the sentiment polarity of a sentence is not
relevant to the target (or aspect) it contains (Hu and Liu, 2004;
Jiang et al., 2011; Mitchell et al., 2013).
479
Algorithm 1 The joint segmentation and classifi-
cation framework (JSC) for sentiment analysis
Input:
training data: T = [s
i
, pol
g
i
], 1 ? i ? |T |
segmentation feature extractor: sfe(?)
classification feature extractor: cfe(?)
Output:
sentiment classifier: SC
segmentation ranking model: SEG
1: Generate segmentation candidates ?
i
for each
sentence s
i
in T , 1 ? i ? |T |
2: Initialize sentiment classifier SC
(0)
based on
cfe(?
ij
), randomize j ? [1, |?
i
|], 1 ? i ?
|T |
3: Randomly initialize the segmentation ranking
model SEG
(0)
4: for r ? 1 ... R do
5: Predict the sentiment polarity pol
i
for ?
i
based on SC
(r?1)
and cfe(?
i?
)
6: Update the segmentation model SEG
(r)
with SEG
(r?1)
and [?
i
, sfe(?
i?
),
pol
i?
, pol
g
i
], 1 ? i ? |T |
7: for i? 1 ... |T | do
8: Calculate the segmentation score for ?
i?
based on SEG
(r)
and sfe(?
i?
)
9: Select the top-ranked K segmentation
candidates ?
i?
from ?
i
10: end for
11: Train the sentiment classifier SC
(r)
with
cfe(?
i?
), 1 ? i ? |T |
12: end for
13: SC? SC
(R)
14: SEG? SEG
(R)
T is annotated only with its gold sentiment po-
larity pol
g
i
, without any segmentation annotation-
s. There are two feature extractors for the task
of sentence segmentation (sfe(?)) and sentiment
classification (cfe(?)), respectively. The output-
s of the joint model are the segmentation ranking
model SEG and the sentiment classifier SC.
In Algorithm 1, we first generate segmentation
candidates ?
i
for each sentence s
i
in the training
set (line 1). Each ?
i
contains no less than one
segmentation candidates. We randomly select one
segmentation result from each ?
i
and utilize their
classification features to initialize the sentimen-
t classifier SC
(0)
(line 2). We randomly initialize
the segmentation model SEG
(0)
(line 3). Subse-
quently, we iteratively train the segmentation mod-
el SEG
(r)
and sentiment classifier SC
(r)
in a join-
t manner (line 4-12). At each iteration, we pre-
dict the sentiment polarity of each segmentation
candidate ?
i?
with the current sentiment classifi-
er SC
(r?1)
(line 5), and then leverage them to up-
date the segmentation model SEG
(r)
(line 6). Af-
terwards, we utilize the recently updated segmen-
tation ranking model SEG
(r)
to update the senti-
ment classifier SC
(r)
(line 7-11). We extract the
segmentation features for each segmentation can-
didate ?
i?
, and employ them to calculate the seg-
mentation score (line 8). The top-ranked K seg-
mentation results ?
i?
of each sentence s
i
is select-
ed (line 9), and further used to train the sentimen-
t classifier SC
(r)
(line 11). Finally, after training
R iterations, we dump the segmentation ranking
model SEG
(R)
and sentiment classifier SC
(R)
in
the last iteration as outputs (line 13-14).
At training time, we train the segmentation
model and classification model from sentences
with manually annotated sentiment polarity. At
prediction time, given a test sentence, we gener-
ate its segmentation candidates, and then calculate
segmentation score for each candidate. Afterward-
s, we select the top-ranked K candidates and vote
their predicted sentiment polarity from sentiment
classifier as the final result.
4 Segmentation Model
In this section, we present details of the segmenta-
tion candidate generation model (Section 4.1), the
segmentation ranking model (Section 4.2) and the
feature description for segmentation ranking mod-
el (Section 4.3).
4.1 Segmentation Candidate Generation
In this subsection, we describe the strategy to gen-
erate segmentation candidates for each sentence.
Since the segmentation results have an exponen-
tial search space in the number of words in a
sentence, we approximate the computation using
beam search with constrains on a phrase table,
which is induced from massive corpora.
Many studies have been previously proposed to
recognize phrases in the text. However, it is out
of scope of this work to compare them. We ex-
ploit a data-driven approach given by Mikolov et
al. (2013), which identifies phrases based on the
occurrence frequency of unigrams and bigrams,
freq(w
i
, w
j
) =
freq(w
i
, w
j
)? ?
freq(w
i
)? freq(w
j
)
(1)
480
where ? is a discounting coefficient that prevents
too many phrases consisting of very infrequen-
t words. We run 2-4 times over the corpora to get
longer phrases containing more words. We em-
pirically set ? as 10 in our experiment. We use
the default frequency threshold (value=5) in the
word2vec toolkit
2
to select bi-terms.
Given a sentence, we initialize the beam of each
index with the current word, and sequentially add
phrases into the beam if the new phrase is con-
tained in the phrase table. At each index of a sen-
tence, we rank the segmentation candidates by the
inverted number of items within a segmentation,
and save the top-ranked N segmentation candi-
dates into the beam. An example of the generated
segmentation candidates is given in Table 2.
Type Sample
Sentence that is not bad
Phrase Table [is not], [not bad], [is not bad]
Segmentations
[that] [is not bad]
[that] [is not] [bad]
[that] [is] [not bad]
[that] [is] [not] [bad]
Table 2: Example for segmentation candidate gen-
eration.
4.2 Segmentation Ranking Model
The objective of the segmentation ranking model
is to assign a scalar to each segmentation candi-
date, which indicates the usefulness of the seg-
mentation result for sentiment classification. In
this subsection, we describe a log-linear model to
calculate the segmentation score. To effectively
train the segmentation ranking model, we devise a
marginal log-likelihood as the optimization objec-
tive.
Given a segmentation candidate ?
ij
of the sen-
tence s
i
, we calculate the segmentation score
for ?
ij
with a log-linear model, as given in Equa-
tion 2.
?
ij
= exp(b+
?
k
sfe
ijk
? w
k
) (2)
where ?
ij
is the segmentation score of ?
ij
; sfe
ijk
is the k-th segmentation feature of ?
ij
; w and b are
the parameters of the segmentation ranking model.
During training, given a sentence s
i
and its gold
sentiment polarity pol
g
i
, the optimization objec-
2
Available at https://code.google.com/p/word2vec/
tive of the segmentation ranking model is to max-
imize the segmentation scores of the hit candi-
dates, whose predicted sentiment polarity equal-
s to the gold polarity of sentence pol
p
i
. The loss
function of the segmentation model is given in E-
quation 3.
loss = ?
|T |
?
i=1
log(
?
j?H
i
?
ij
?
j
?
?A
i
?
ij
?
) + ?||w||
2
2
(3)
where T is the training data; A
i
represents all the
segmentation candidates of sentence s
i
; H
i
mean-
s the hit candidates of s
i
; ? is the weight of the
L2-norm regularization factor. We train the seg-
mentation model with L-BFGS (Liu and Nocedal,
1989), running over the complete training data.
4.3 Feature
We design two kinds of features for sentence seg-
mentation, namely the phrase-embedding feature
and the segmentation-specific feature. The final
feature representation of each segmentation is the
concatenation of these two features. It is worth
noting that, the phrase-embedding feature is used
in both sentence segmentation and sentiment clas-
sification.
Segmentation-Specific Feature We empirically
design four segmentation-specific features to re-
flect the information of each segmentation, as list-
ed in Table 3.
Phrase-Embedding Feature We leverage
phrase embedding to generate the features of
segmentation candidates for both sentence seg-
mentation and sentiment classification. The
reason is that, in both tasks, the basic compu-
tational units of each segmentation candidate
might be words or phrases of variable length.
Under this scenario, phrase embedding is highly
suitable as it is capable to represent phrases with
different length into a consistent distributed vector
space (Mikolov et al., 2013). For each phrase,
phrase embedding is a dense, real-valued and
continuous vector. After the phrase embedding is
trained, the nearest neighbors in the embedding
space are favored to have similar grammatical us-
ages and semantic meanings. The effectiveness of
phrase embedding has been verified for building
large-scale sentiment lexicon (Tang et al., 2014a)
and machine translation (Zhang et al., 2014).
We learn phrase embedding with Skip-Gram
model (Mikolov et al., 2013), which is the state-of-
481
Feature Feature Description
#unit the number of basic computation units in the segmentation candidate
#unit / #word the ratio of units? number in a candidate to the length of original sentence
#word ? #unit the difference between sentence length and the number of basic computational units
#unit > 2 the number of basic component units composed of more than two words
Table 3: Segmentation-specific features for segmentation ranking.
Feature Feature Description
All-Caps the number of words with all characters in upper case
Emoticon the presence of positive (or negative) emoticons, whether the last unit is emoticon
Hashtag the number of hashtag
Elongated units the number of basic computational containing elongated words (with one character
repeated more than two times), such as gooood
Sentiment lexicon the number of sentiment words, the score of last sentiment words, the total sentiment
score and the maximal sentiment score for each lexicon
Negation the number of negations as individual units in a segmentation
Bag-of-Units an extension of bag-of-word for a segmentation
Punctuation the number of contiguous sequences of dot, question mark and exclamation mark.
Cluster the presence of units from each of the 1,000 clusters from Twitter NLP tool (Gimpel
et al., 2011)
Table 4: Classification-specific features for sentiment classification.
the-art phrase embedding learning algorithm. We
compose the representation (or feature) of a seg-
mentation candidate from the embedding of the
basic computational units (words or phrases) it
contains. In this paper, we explore min, max and
average convolution functions, which have been
used as simple and effective methods for composi-
tion learning in vector-based semantics (Mitchell
and Lapata, 2010; Collobert et al., 2011; Socher et
al., 2013a; Shen et al., 2014; Tang et al., 2014b),
to calculate the representation of a segmentation
candidate. The final phrase-embedding feature is
the concatenation of vectors derived from different
convolutional functions, as given in Equation 4,
pf(seg) = [pf
max
(seg), pf
min
(seg), pf
avg
(seg)]
(4)
where pf(seg) is the representation of the given
segmentation; pf
x
(seg) is the result of the con-
volutional function x ? {min,max, avg}. Each
convolutional function pf
x
(?) conducts the matrix-
vector operation of x on the sequence represented
by columns in the lookup table of phrase embed-
ding. The output of pf
x
(?) is calculated as
pf
x
(seg) = ?
x
?L
ph
?
seg
(5)
where ?
x
is the convolutional function of pf
x
;
?L
ph
?
seg
is the concatenated column vectors of
the basic computational units in the segmentation;
L
ph
is the lookup table of phrase embedding.
5 Classification Model
For sentiment classification, we follow the su-
pervised learning framework (Pang et al., 2002)
and build the classifier from sentences with man-
ually labelled sentiment polarity. We extend the
state-of-the-art hand-crafted features in SemEval
2013 (Mohammad et al., 2013b), and design the
classification-specific features for each segmenta-
tion. The detailed feature description is given in
Table 4.
6 Experiment
In this section, we conduct experiments to evaluate
the effectiveness of the joint model. We describe
the experiment settings and the result analysis.
6.1 Dataset and Experiment Settings
We conduct sentiment classification of tweets on a
benchmark Twitter sentiment classification dataset
in SemEval 2013. We run 2-class (positive vs neg-
ative) classification as sentence segmentation has a
great influence on the positive/negative polarity of
tweets due to the polarity inconsistency between a
phrase and its constitutes, such as ?not bad, bad?.
482
We leave 3-class classification (positive, negative,
neutral) and fine-grained classification (very neg-
ative, negative, neutral, positive, very positive) in
the future work.
Positive Negative Total
Train 2,642 994 3,636
Dev 408 219 627
Test 1,570 601 2,171
Table 5: Statistics of the SemEval 2013 Twitter
sentiment classification dataset (positive vs nega-
tive).
The statistics of our dataset crawled from Se-
mEval 2013 are given in Table 5. The evalua-
tion metric is the macro-F1 of sentiment classifi-
cation. We train the joint model on the training
set, tune parameters on the dev set and evaluate
on the test set. We train the sentiment classifier
with LibLinear (Fan et al., 2008) and utilize exist-
ing sentiment lexicons
3
to extract classification-
specific features. We randomly crawl 100M tweets
from February 1st, 2013 to April 30th, 2013 with
Twitter API, and use them to learn the phrase em-
bedding with Skip-Gram
4
. The vocabulary size
of the phrase embedding is 926K, from unigram
to 5-gram. The parameter -c in SVM is tuned on
the dev-set in both baseline and our method. We
run the L-BFGS for 50 iterations, and set the reg-
ularization factor ? as 0.003. The beam size N of
the candidate generation model and the top-ranked
segmentation number K are tuned on the dev-set.
6.2 Baseline Methods
We compare the proposed joint model with the fol-
lowing sentiment classification algorithms:
? DistSuper: We collect 10M balanced tweets
selected by positive and negative emoticons
5
as
training data, and build classifier using the Lib-
Linear and ngram features (Go et al., 2009; Zhao
et al., 2012).
? SVM: The n-gram features and Support Vec-
tor Machine are widely-used baseline methods to
build sentiment classifiers (Pang et al., 2002). We
use LibLinear to train the SVM classifier.
3
In this work, we use HL (Hu and Liu, 2004), M-
PQA (Wilson et al., 2005), NRC Emotion Lexicon (Moham-
mad and Turney, 2012), NRC Hashtag Lexicon and Senti-
ment140Lexicon (Mohammad et al., 2013b).
4
https://code.google.com/p/word2vec/
5
We use the emoticons selected by Hu et al. (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
? NBSVM: NBSVM (Wang and Manning,
2012) trades-off between Naive Bayes and NB-
features enhanced SVM. We use NBSVM-bi be-
cause it performs best on sentiment classification
of reviews.
? RAE: Recursive Autoencoder (Socher et al.,
2011) has been proven effective for sentiment clas-
sification by learning sentence representation. We
train the RAE using the pre-trained phrase embed-
ding learned from 100M tweets.
? SentiStrength: Thelwall et al. (2012) build a
lexicon-based classifier which uses linguistic rules
to detect the sentiment strength of tweets.
? SSWE
u
: Tang et al. (2014b) propose to learn
sentiment-specific word embedding (SSWE) from
10M tweets collected by emoticons. They apply
SSWE as features for Twitter sentiment classifica-
tion.
? NRC: NRC builds the state-of-the-art system
in SemEval 2013 Twitter Sentiment Classifica-
tion Track, incorporating diverse sentiment lexi-
cons and hand-crafted features (Mohammad et al.,
2013b). We re-implement this system because the
codes are not publicly available. We do not di-
rectly report their results in the evaluation task,
as our training and development sets are smaller
than their dataset. In NRC + PF, We concatenate
the NRC features and the phrase embeddings fea-
ture (PF), and build the sentiment classifier with
LibLinear.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al., 2013b) be-
cause the tweets in our dataset do not have accu-
rately parsed results. Another reason is that, due to
the differences between domains, the performance
of RNTN trained on movie reviews might be de-
creased if directly applied on the tweets (Xiao et
al., 2013).
6.3 Results and Analysis
Table 6 shows the macro-F1 of the baseline sys-
tems as well as our joint model (JSC) on senti-
ment classification of tweets (positive vs negative).
As is shown in Table 6, distant supervision is
relatively weak because the noisy-labeled tweets
are treated as the gold standard, which decreases
the performance of sentiment classifier. The result
of bag-of-unigram feature (74.50%) is not satisfied
as it losses the word order and does not well cap-
483
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + 5-gram 63.92
SVM + unigram 74.50
SVM + 5-gram 74.97
Recursive Autoencoder 75.42
NBSVM 75.28
SentiStrength 73.23
SSWE
u
84.98
NRC (Top System in SemEval 2013) 84.73
NRC + PF 84.75
JSC 85.51
Table 6: Macro-F1 for positive vs negative classi-
fication of tweets.
ture the semantic meaning of phrases. The integra-
tion of high-order n-ngram (up to 5-gram) does not
achieve significant improvement (+0.47%). The
reason is that, if a sentence contains a bigram ?not
bad?, they will use ?bad? and ?not bad? as par-
allel features, which confuses the sentiment clas-
sification model. NBSVM and Recursive Autoen-
coder perform comparatively and have a big gap
in comparison with JSC. In RAE, the representa-
tion of a sentence is composed from the represen-
tation of words it contains. Accordingly, ?great?
in ?a great deal of ? also contributes to the final
sentence representation via composition function.
JSC automatically conducts sentence segmenta-
tion by considering the sentiment polarity of sen-
tence, and utilize the phrasal information from the
segmentations. Ideally, JSC regards phrases like
?not bad? and ?a great deal of ? as basic compu-
tational units, and yields better classification per-
formance. JSC (85.51%) performs slightly better
than the state-of-the-art systems (SSWE
u
, 84.98%;
NRC+PF, 84.75%), which verifies its effective-
ness.
6.4 Comparing Joint and Pipelined Models
We compare the proposed joint model with
pipelined methods on Twitter sentiment classifi-
cation with different feature sets. Figure 2 gives
the experiment results. The tick [A, B] on x-
axis means the use of A as segmentation feature
and the use of B as classification feature. PF
represents the phrase-embedding feature; SF and
CF stand for the segmentation-specific feature and
classification-specific feature, respectively. We
use the bag-of-word segmentation result to build
sentiment classier in Pipeline 1, and use the seg-
mentation candidate with maximum phrase num-
ber in Pipeline 2.
0.7
0.72
0.74
0.76
0.78
0.8
0.82
0.84
0.86
Mac
ro?F
1
 
 
[PF, PF] [PF+SF, PF] [PF, PF+CF] [PF+SF, PF+CF]
Pipeline 1Pipeline 2Joint
Figure 2: Macro-F1 for positive vs negative classi-
fication of tweets with joint and pipelined models.
From Figure 2, we find that the joint model
consistently outperforms pipelined baseline meth-
ods in all feature settings. The reason is that
the pipelined methods suffer from error propaga-
tion, since the errors from linguistic-driven and
bag-of-word segmentations cannot be corrected by
the sentiment classification model. Besides, tra-
ditional segmentors do not update the segmenta-
tion model with the sentiment information of tex-
t. Unlike pipelined methods, the joint model is
capable to address these problems by optimizing
the segmentation model with the classification re-
sults in a joint framework, which yields better
performance on sentiment classification. We also
find that Pipeline 2 always outperforms Pipeline
1, which indicates the usefulness of phrase-based
segmentation for sentiment classification.
6.5 Effect of the beam size N
We investigate the influence of beam size N ,
which is the maximum number of segmentation
candidates of a sentence. In this part, we clamp the
feature set as [PF+SF, PF+CF], and vary the beam
size N in [1,2,4,8,16,32,64]. The experiment re-
sults of macro-F1 on the development set are il-
lustrated in Figure 3 (a). The time cost of each
training iteration is given in Figure 3 (b).
From Figure 3 (a), we can see that when larg-
er beam size is considered, the classification per-
formance is improved. When beam size is 1, the
model stands for the greedy search with the bag-
of-words segmentation. When the beam size is s-
mall, such as 2, beam search losses many phrasal
information of sentences and thus the improve-
ment is not significant. The performance remains
steady when beam size is larger than 16. From
484
1 2 4 8 16 32 640.81
0.82
0.83
0.84
0.85
0.86
Beam Size
Mac
ro?F
1
(a) Macro-F1 score for senti-
ment classification.
1 2 4 8 16 32 640
20
40
60
80
100
120
Beam Size
Run
time
 (Sec
ond)
 
 
(b) Time cost (seconds) of
each training iteration.
Figure 3: Sentiment classification of tweets with
different beam size N .
Figure 3 (b), we can find that the runtime of each
training iteration increases with larger beam size.
It is intuitive as the joint model with larger beam
considers more segmentation results, which in-
creases the training time of the segmentation mod-
el. We set beam size as 16 after parameter learn-
ing.
6.6 Effect of the top-ranked segmentation
number K
We investigate how the top-ranked segmentation
number K affects the performance of sentimen-
t classification. In this part, we set the feature as
[PF+SF, PF+CF], and the beam size as 16. The
results of macro-F1 on the development set are il-
lustrated in Figure 4.
1 3 5 7 9 11 13 150.82
0.83
0.84
0.85
0.86
Top?ranked candidate number
Ma
cro
?F1
Figure 4: Sentiment classification of tweets with
different top-ranked segmentation number K.
From Figure 4, we find that the classification
performance increases with K being larger. The
reason is that when a larger K is used, (1) at train-
ing time, the sentiment classifier is built by using
more phrasal information from multiple segmen-
tations, which benefits from the ensembles; (2) at
test time, the joint model considers several top-
ranked segmentations and get the final sentiment
polarity through voting. The performance remain-
s stable when K is larger than 7, as the phrasal
information has been mostly covered.
7 Conclusion
In this paper, we develop a joint segmentation
and classification framework (JSC) for sentiment
analysis. Unlike existing sentiment classification
algorithms that build sentiment classifier based
on the segmentation results from bag-of-words or
separate segmentors, the proposed joint model si-
multaneously conducts sentence segmentation and
sentiment classification. We introduce a marginal
log-likelihood function to optimize the segmenta-
tion model, and effectively train the joint mod-
el from sentences annotated only with sentiment
polarity, without segmentation annotations of sen-
tences. The effectiveness of the joint model has
been verified by applying it on the benchmark
dataset of Twitter sentiment classification in Se-
mEval 2013. Results show that, the joint model
performs comparably with state-of-the-art meth-
ods, and outperforms pipelined methods in various
settings. In the future, we plan to apply the join-
t model on other domains, such as movie/product
reviews.
Acknowledgements
We thank Nan Yang, Yajuan Duan, Yaming
Sun and Meishan Zhang for their helpful dis-
cussions. We thank the anonymous reviewers
for their insightful comments and feedbacks on
this work. This research was partly supported
by National Natural Science Foundation of Chi-
na (No.61133012, No.61273321, No.61300113).
The contact author of this paper, according to the
meaning given to this role by Harbin Institute of
Technology, is Bing Qin.
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 793?801.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
485
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reduc-
ing the dimensionality of data with neural networks.
Science, 313(5786):504?507.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics.
Nicola Jones. 2014. Computer science: The learning
machines. Nature, 505(7482):146.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A sentence model based on convolu-
tional neural networks. In Procedding of the 52th
Annual Meeting of Association for Computational
Linguistics.
Hyun Duk Kim and ChengXiang Zhai. 2009. Gener-
ating comparative summaries of contradictory opin-
ions in text. In Proceedings of CIKM 2009. ACM.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of international con-
ference on Machine learning. ACM.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents. Proceedings
of International Conference on Machine Learning.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503?528.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. Conference on Neural Information Processing
Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Margaret Mitchell, Jacqui Aguilar, Theresa Wilson,
and Benjamin Van Durme. 2013. Open domain tar-
geted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654.
Saif M Mohammad and Peter D Turney. 2012. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence.
Saif M Mohammad, Bonnie J Dorr, Graeme Hirst, and
Peter D Turney. 2013a. Computing lexical contrast.
Computational Linguistics, 39(3):555?590.
486
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013b. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 786?794.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Georgios Paltoglou and Mike Thelwall. 2010. A s-
tudy of information retrieval weighting schemes for
sentiment analysis. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1386?1395.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr?egoire Mesnil. 2014. Learning semantic rep-
resentations using convolutional neural networks for
web search. In Proceedings of the companion publi-
cation of the 23rd international conference on World
wide web companion, pages 373?374.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Y Ng. 2013a. Reasoning with neu-
ral tensor networks for knowledge base completion.
The Conference on Neural Information Processing
Systems.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and
Ting Liu. 2014a. Building large-scale twitter-
specific sentiment lexicon : A representation learn-
ing approach. In Proceedings of COLING 2014,
the 25th International Conference on Computation-
al Linguistics, pages 172?182.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014b. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 152?
162, October.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 111?121.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
487
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555?1565,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Nan Yang
\
, Ming Zhou
?
, Ting Liu
?
, Bing Qin
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
\
University of Science and Technology of China, Hefei, China
{dytang, tliu, qinb}@ir.hit.edu.cn
{fuwei, v-nayang, mingzhou}@microsoft.com
Abstract
We present a method that learns word em-
bedding for Twitter sentiment classifica-
tion in this paper. Most existing algorithm-
s for learning continuous word represen-
tations typically only model the syntactic
context of words but ignore the sentimen-
t of text. This is problematic for senti-
ment analysis as they usually map word-
s with similar syntactic context but oppo-
site sentiment polarity, such as good and
bad, to neighboring word vectors. We
address this issue by learning sentiment-
specific word embedding (SSWE), which
encodes sentiment information in the con-
tinuous representation of words. Specif-
ically, we develop three neural networks
to effectively incorporate the supervision
from sentiment polarity of text (e.g. sen-
tences or tweets) in their loss function-
s. To obtain large scale training corpora,
we learn the sentiment-specific word em-
bedding from massive distant-supervised
tweets collected by positive and negative
emoticons. Experiments on applying SS-
WE to a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that (1) the SSWE feature performs
comparably with hand-crafted features in
the top-performed system; (2) the perfor-
mance is further improved by concatenat-
ing SSWE with existing feature set.
1 Introduction
Twitter sentiment classification has attracted in-
creasing research interest in recent years (Jiang et
al., 2011; Hu et al, 2013). The objective is to clas-
sify the sentiment polarity of a tweet as positive,
?
This work was done when the first and third authors
were visiting Microsoft Research Asia.
negative or neutral. The majority of existing ap-
proaches follow Pang et al (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
designing effective features to obtain better clas-
sification performance. For example, Mohammad
et al (2013) build the top-performed system in the
Twitter sentiment classification track of SemEval
2013 (Nakov et al, 2013), using diverse sentiment
lexicons and a variety of hand-crafted features.
Feature engineering is important but labor-
intensive. It is therefore desirable to discover ex-
planatory factors from the data and make the learn-
ing algorithms less dependent on extensive fea-
ture engineering (Bengio, 2013). For the task of
sentiment classification, an effective feature learn-
ing method is to compose the representation of a
sentence (or document) from the representation-
s of the words or phrases it contains (Socher et
al., 2013b; Yessenalina and Cardie, 2011). Ac-
cordingly, it is a crucial step to learn the word
representation (or word embedding), which is a
dense, low-dimensional and real-valued vector for
a word. Although existing word embedding learn-
ing algorithms (Collobert et al, 2011; Mikolov et
al., 2013) are intuitive choices, they are not effec-
tive enough if directly used for sentiment classi-
fication. The most serious problem is that tradi-
tional methods typically model the syntactic con-
text of words but ignore the sentiment information
of text. As a result, words with opposite polari-
ty, such as good and bad, are mapped into close
vectors. It is meaningful for some tasks such as
pos-tagging (Zheng et al, 2013) as the two words
have similar usages and grammatical roles, but it
becomes a disaster for sentiment analysis as they
have the opposite sentiment polarity.
In this paper, we propose learning sentiment-
specific word embedding (SSWE) for sentiment
analysis. We encode the sentiment information in-
1555
to the continuous representation of words, so that
it is able to separate good and bad to opposite ends
of the spectrum. To this end, we extend the ex-
isting word embedding learning algorithm (Col-
lobert et al, 2011) and develop three neural net-
works to effectively incorporate the supervision
from sentiment polarity of text (e.g. sentences
or tweets) in their loss functions. We learn the
sentiment-specific word embedding from tweet-
s, leveraging massive tweets with emoticons as
distant-supervised corpora without any manual an-
notations. These automatically collected tweet-
s contain noises so they cannot be directly used
as gold training data to build sentiment classifier-
s, but they are effective enough to provide weak-
ly supervised signals for training the sentiment-
specific word embedding.
We apply SSWE as features in a supervised
learning framework for Twitter sentiment classi-
fication, and evaluate it on the benchmark dataset
in SemEval 2013. In the task of predicting posi-
tive/negative polarity of tweets, our method yields
84.89% in macro-F1 by only using SSWE as fea-
ture, which is comparable to the top-performed
system based on hand-crafted features (84.70%).
After concatenating the SSWE feature with ex-
isting feature set, we push the state-of-the-art to
86.58% in macro-F1. The quality of SSWE is al-
so directly evaluated by measuring the word sim-
ilarity in the embedding space for sentiment lexi-
cons. In the accuracy of polarity consistency be-
tween each sentiment word and its top N closest
words, SSWE outperforms existing word embed-
ding learning algorithms.
The major contributions of the work presented
in this paper are as follows.
? We develop three neural networks to learn
sentiment-specific word embedding (SSWE)
from massive distant-supervised tweets with-
out any manual annotations;
? To our knowledge, this is the first work that
exploits word embedding for Twitter senti-
ment classification. We report the results that
the SSWE feature performs comparably with
hand-crafted features in the top-performed
system in SemEval 2013;
? We release the sentiment-specific word em-
bedding learned from 10 million tweets,
which can be adopted off-the-shell in other
sentiment analysis tasks.
2 Related Work
In this section, we present a brief review of the
related work from two perspectives, Twitter senti-
ment classification and learning continuous repre-
sentations for sentiment classification.
2.1 Twitter Sentiment Classification
Twitter sentiment classification, which identifies
the sentiment polarity of short, informal tweets,
has attracted increasing research interest (Jiang et
al., 2011; Hu et al, 2013) in recent years. Gen-
erally, the methods employed in Twitter sentiment
classification follow traditional sentiment classifi-
cation approaches. The lexicon-based approaches
(Turney, 2002; Ding et al, 2008; Taboada et al,
2011; Thelwall et al, 2012) mostly use a dictio-
nary of sentiment words with their associated sen-
timent polarity, and incorporate negation and in-
tensification to compute the sentiment polarity for
each sentence (or document).
The learning based methods for Twitter sen-
timent classification follow Pang et al (2002)?s
work, which treat sentiment classification of texts
as a special case of text categorization issue. Many
studies on Twitter sentiment classification (Pak
and Paroubek, 2010; Davidov et al, 2010; Bar-
bosa and Feng, 2010; Kouloumpis et al, 2011;
Zhao et al, 2012) leverage massive noisy-labeled
tweets selected by positive and negative emoticon-
s as training set and build sentiment classifiers di-
rectly, which is called distant supervision (Go et
al., 2009). Instead of directly using the distant-
supervised data as training set, Liu et al (2012)
adopt the tweets with emoticons to smooth the lan-
guage model and Hu et al (2013) incorporate the
emotional signals into an unsupervised learning
framework for Twitter sentiment classification.
Many existing learning based methods on Twit-
ter sentiment classification focus on feature engi-
neering. The reason is that the performance of sen-
timent classifier being heavily dependent on the
choice of feature representation of tweets. The
most representative system is introduced by Mo-
hammad et al (2013), which is the state-of-the-
art system (the top-performed system in SemEval
2013 Twitter Sentiment Classification Track) by
implementing a number of hand-crafted features.
Unlike the previous studies, we focus on learning
discriminative features automatically from mas-
sive distant-supervised tweets.
1556
2.2 Learning Continuous Representations for
Sentiment Classification
Pang et al (2002) pioneer this field by using bag-
of-word representation, representing each word as
a one-hot vector. It has the same length as the size
of the vocabulary, and only one dimension is 1,
with all others being 0. Under this assumption,
many feature learning algorithms are proposed to
obtain better classification performance (Pang and
Lee, 2008; Liu, 2012; Feldman, 2013). However,
the one-hot word representation cannot sufficient-
ly capture the complex linguistic characteristics of
words.
With the revival of interest in deep learn-
ing (Bengio et al, 2013), incorporating the con-
tinuous representation of a word as features has
been proving effective in a variety of NLP tasks,
such as parsing (Socher et al, 2013a), language
modeling (Bengio et al, 2003; Mnih and Hin-
ton, 2009) and NER (Turian et al, 2010). In the
field of sentiment analysis, Bespalov et al (2011;
2012) initialize the word embedding by Laten-
t Semantic Analysis and further represent each
document as the linear weighted of ngram vec-
tors for sentiment classification. Yessenalina and
Cardie (2011) model each word as a matrix and
combine words using iterated matrix multiplica-
tion. Glorot et al (2011) explore Stacked Denois-
ing Autoencoders for domain adaptation in sen-
timent classification. Socher et al propose Re-
cursive Neural Network (RNN) (2011b), matrix-
vector RNN (2012) and Recursive Neural Tensor
Network (RNTN) (2013b) to learn the composi-
tionality of phrases of any length based on the
representation of each pair of children recursively.
Hermann et al (2013) present Combinatory Cate-
gorial Autoencoders to learn the compositionality
of sentence, which marries the Combinatory Cat-
egorial Grammar with Recursive Autoencoder.
The representation of words heavily relies on
the applications or tasks in which it is used (Lab-
utov and Lipson, 2013). This paper focuses
on learning sentiment-specific word embedding,
which is tailored for sentiment analysis. Un-
like Maas et al (2011) that follow the proba-
bilistic document model (Blei et al, 2003) and
give an sentiment predictor function to each word,
we develop neural networks and map each n-
gram to the sentiment polarity of sentence. Un-
like Socher et al (2011c) that utilize manually
labeled texts to learn the meaning of phrase (or
sentence) through compositionality, we focus on
learning the meaning of word, namely word em-
bedding, from massive distant-supervised tweets.
Unlike Labutov and Lipson (2013) that produce
task-specific embedding from an existing word
embedding, we learn sentiment-specific word em-
bedding from scratch.
3 Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
In this section, we present the details of learn-
ing sentiment-specific word embedding (SSWE)
for Twitter sentiment classification. We pro-
pose incorporating the sentiment information of
sentences to learn continuous representations for
words and phrases. We extend the existing word
embedding learning algorithm (Collobert et al,
2011) and develop three neural networks to learn
SSWE. In the following sections, we introduce the
traditional method before presenting the details of
SSWE learning algorithms. We then describe the
use of SSWE in a supervised learning framework
for Twitter sentiment classification.
3.1 C&W Model
Collobert et al (2011) introduce C&W model to
learn word embedding based on the syntactic con-
texts of words. Given an ngram ?cat chills on a
mat?, C&W replaces the center word with a ran-
dom wordw
r
and derives a corrupted ngram ?cat
chills w
r
a mat?. The training objective is that the
original ngram is expected to obtain a higher lan-
guage model score than the corrupted ngram by a
margin of 1. The ranking objective function can
be optimized by a hinge loss,
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(1)
where t is the original ngram, t
r
is the corrupted
ngram, f
cw
(?) is a one-dimensional scalar repre-
senting the language model score of the input n-
gram. Figure 1(a) illustrates the neural architec-
ture of C&W, which consists of four layers, name-
ly lookup ? linear ? hTanh ? linear (from
bottom to top). The original and corrupted ngram-
s are treated as inputs of the feed-forward neural
network, respectively. The output f
cw
is the lan-
guage model score of the input, which is calculat-
ed as given in Equation 2, where L is the lookup
table of word embedding,w
1
, w
2
, b
1
, b
2
are the pa-
rameters of linear layers.
f
cw
(t) = w
2
(a) + b
2
(2)
1557
so cooool :D 
lookup 
linear 
hTanh 
linear 
softmax 
(a) C&W 
so cooool :D 
(b) SSWEh 
so cooool :D 
(c) SSWEu 
syntactic 
sentiment 
positive 
negative 
Figure 1: The traditional C&W model and our neural networks (SSWE
h
and SSWE
u
) for learning
sentiment-specific word embedding.
a = hTanh(w
1
L
t
+ b
1
) (3)
hTanh(x) =
?
?
?
?
?
?1 if x < ?1
x if ? 1 ? x ? 1
1 if x > 1
(4)
3.2 Sentiment-Specific Word Embedding
Following the traditional C&W model (Collobert
et al, 2011), we incorporate the sentiment infor-
mation into the neural network to learn sentiment-
specific word embedding. We develop three neural
networks with different strategies to integrate the
sentiment information of tweets.
Basic Model 1 (SSWE
h
). As an unsupervised
approach, C&W model does not explicitly capture
the sentiment information of texts. An intuitive
solution to integrate the sentiment information is
predicting the sentiment distribution of text based
on input ngram. We do not utilize the entire sen-
tence as input because the length of different sen-
tences might be variant. We therefore slide the
window of ngram across a sentence, and then pre-
dict the sentiment polarity based on each ngram
with a shared neural network. In the neural net-
work, the distributed representation of higher lay-
er are interpreted as features describing the input.
Thus, we utilize the continuous vector of top layer
to predict the sentiment distribution of text.
Assuming there are K labels, we modify the di-
mension of top layer in C&W model as K and
add a softmax layer upon the top layer. The
neural network (SSWEh) is given in Figure 1(b).
Softmax layer is suitable for this scenario be-
cause its outputs are interpreted as conditional
probabilities. Unlike C&W, SSWE
h
does not gen-
erate any corrupted ngram. Let f
g
(t), where K
denotes the number of sentiment polarity label-
s, be the gold K-dimensional multinomial distri-
bution of input t and
?
k
f
g
k
(t) = 1. For pos-
itive/negative classification, the distribution is of
the form [1,0] for positive and [0,1] for negative.
The cross-entropy error of the softmax layer is :
loss
h
(t) = ?
?
k={0,1}
f
g
k
(t) ? log(f
h
k
(t)) (5)
where f
g
(t) is the gold sentiment distribution and
f
h
(t) is the predicted sentiment distribution.
Basic Model 2 (SSWE
r
). SSWE
h
is trained by
predicting the positive ngram as [1,0] and the neg-
ative ngram as [0,1]. However, the constraint of
SSWE
h
is too strict. The distribution of [0.7,0.3]
can also be interpreted as a positive label because
the positive score is larger than the negative s-
core. Similarly, the distribution of [0.2,0.8] indi-
cates negative polarity. Based on the above obser-
vation, the hard constraints in SSWE
h
should be
relaxed. If the sentiment polarity of a tweet is pos-
itive, the predicted positive score is expected to be
larger than the predicted negative score, and the
exact reverse if the tweet has negative polarity.
We model the relaxed constraint with a rank-
ing objective function and borrow the bottom four
layers from SSWE
h
, namely lookup? linear ?
hTanh ? linear in Figure 1(b), to build the re-
laxed neural network (SSWEr). Compared with
SSWE
h
, the softmax layer is removed because
SSWE
r
does not require probabilistic interpreta-
tion. The hinge loss of SSWE
r
is modeled as de-
1558
scribed below.
loss
r
(t) = max(0, 1? ?
s
(t)f
r
0
(t)
+ ?
s
(t)f
r
1
(t) )
(6)
where f
r
0
is the predicted positive score, f
r
1
is
the predicted negative score, ?
s
(t) is an indicator
function reflecting the sentiment polarity of a sen-
tence,
?
s
(t) =
{
1 if f
g
(t) = [1, 0]
?1 if f
g
(t) = [0, 1]
(7)
Similar with SSWE
h
, SSWE
r
also does not gen-
erate the corrupted ngram.
Unified Model (SSWE
u
). The C&W model
learns word embedding by modeling syntactic
contexts of words but ignoring sentiment infor-
mation. By contrast, SSWE
h
and SSWE
r
learn
sentiment-specific word embedding by integrating
the sentiment polarity of sentences but leaving out
the syntactic contexts of words. We develop a uni-
fied model (SSWEu) in this part, which captures
the sentiment information of sentences as well as
the syntactic contexts of words. SSWE
u
is illus-
trated in Figure 1(c).
Given an original (or corrupted) ngram and
the sentiment polarity of a sentence as the in-
put, SSWE
u
predicts a two-dimensional vector for
each input ngram. The two scalars (f
u
0
, f
u
1
) s-
tand for language model score and sentiment s-
core of the input ngram, respectively. The training
objectives of SSWE
u
are that (1) the original n-
gram should obtain a higher language model score
f
u
0
(t) than the corrupted ngram f
u
0
(t
r
), and (2) the
sentiment score of original ngram f
u
1
(t) should be
more consistent with the gold polarity annotation
of sentence than corrupted ngram f
u
1
(t
r
). The loss
function of SSWE
u
is the linear combination of t-
wo hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(8)
where loss
cw
(t, t
r
) is the syntactic loss as given
in Equation 1, loss
us
(t, t
r
) is the sentiment loss
as described in Equation 9. The hyper-parameter
? weighs the two parts.
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(9)
Model Training. We train sentiment-specific
word embedding from massive distant-supervised
tweets collected with positive and negative emoti-
cons
1
. We crawl tweets from April 1st, 2013 to
April 30th, 2013 with TwitterAPI. We tokenize
each tweet with TwitterNLP (Gimpel et al, 2011),
remove the @user and URLs of each tweet, and fil-
ter the tweets that are too short (< 7 words). Final-
ly, we collect 10M tweets, selected by 5M tweets
with positive emoticons and 5M tweets with nega-
tive emoticons.
We train SSWE
h
, SSWE
r
and SSWE
u
by
taking the derivative of the loss through back-
propagation with respect to the whole set of pa-
rameters (Collobert et al, 2011), and use Ada-
Grad (Duchi et al, 2011) to update the parame-
ters. We empirically set the window size as 3, the
embedding length as 50, the length of hidden lay-
er as 20 and the learning rate of AdaGrad as 0.1
for all baseline and our models. We learn embed-
ding for unigrams, bigrams and trigrams separate-
ly with same neural network and same parameter
setting. The contexts of unigram (bigram/trigram)
are the surrounding unigrams (bigrams/trigrams),
respectively.
3.3 Twitter Sentiment Classification
We apply sentiment-specific word embedding for
Twitter sentiment classification under a supervised
learning framework as in previous work (Pang et
al., 2002). Instead of hand-crafting features, we
incorporate the continuous representation of word-
s and phrases as the feature of a tweet. The senti-
ment classifier is built from tweets with manually
annotated sentiment polarity.
We explore min, average and max convolu-
tional layers (Collobert et al, 2011; Socher et
al., 2011a), which have been used as simple and
effective methods for compositionality learning
in vector-based semantics (Mitchell and Lapata,
2010), to obtain the tweet representation. The re-
sult is the concatenation of vectors derived from
different convolutional layers.
z(tw) = [z
max
(tw), z
min
(tw), z
average
(tw)]
where z(tw) is the representation of tweet tw and
z
x
(tw) is the results of the convolutional layer x ?
{min,max, average}. Each convolutional layer
1
We use the emoticons selected by Hu et al (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
1559
zx
employs the embedding of unigrams, bigrams
and trigrams separately and conducts the matrix-
vector operation of x on the sequence represented
by columns in each lookup table. The output of
z
x
is the concatenation of results obtained from
different lookup tables.
z
x
(tw) = [w
x
?L
uni
?
tw
, w
x
?L
bi
?
tw
, w
x
?L
tri
?
tw
]
where w
x
is the convolutional function of z
x
,
?L?
tw
is the concatenated column vectors of the
words in the tweet. L
uni
, L
bi
and L
tri
are the
lookup tables of the unigram, bigram and trigram
embedding, respectively.
4 Experiment
We conduct experiments to evaluate SSWE by in-
corporating it into a supervised learning frame-
work for Twitter sentiment classification. We also
directly evaluate the effectiveness of the SSWE by
measuring the word similarity in the embedding
space for sentiment lexicons.
4.1 Twitter Sentiment Classification
Experiment Setup and Datasets. We conduct
experiments on the latest Twitter sentiment clas-
sification benchmark dataset in SemEval 2013
(Nakov et al, 2013). The training and develop-
ment sets were completely in full to task partici-
pants. However, we were unable to download all
the training and development sets because some
tweets were deleted or not available due to mod-
ified authorization status. The test set is directly
provided to the participants. The distribution of
our dataset is given in Table 1. We train sentiment
classifier with LibLinear (Fan et al, 2008) on the
training set, tune parameter ?c on the dev set and
evaluate on the test set. Evaluation metric is the
Macro-F1 of positive and negative categories
2
.
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of the SemEval 2013 Twitter
sentiment classification dataset.
2
We investigate 2-class Twitter sentiment classifica-
tion (positive/negative) instead of 3-class Twitter sentiment
classification (positive/negative/neutral) in SemEval2013.
Baseline Methods. We compare our method
with the following sentiment classification algo-
rithms:
(1) DistSuper: We use the 10 million tweets se-
lected by positive and negative emoticons as train-
ing data, and build sentiment classifier with Lib-
Linear and ngram features (Go et al, 2009).
(2) SVM: The ngram features and Support Vec-
tor Machine are widely used baseline methods to
build sentiment classifiers (Pang et al, 2002). Li-
bLinear is used to train the SVM classifier.
(3) NBSVM: NBSVM (Wang and Manning,
2012) is a state-of-the-art performer on many sen-
timent classification datasets, which trades-off be-
tween Naive Bayes and NB-enhanced SVM.
(4) RAE: Recursive Autoencoder (Socher et al,
2011c) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. We run RAE with randomly initial-
ized word embedding.
(5) NRC: NRC builds the top-performed system
in SemEval 2013 Twitter sentiment classification
track which incorporates diverse sentiment lexi-
cons and many manually designed features. We
re-implement this system because the codes are
not publicly available
3
. NRC-ngram refers to the
feature set of NRC leaving out ngram features.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al, 2013b) be-
cause we cannot efficiently train the RNTN model.
The reason lies in that the tweets in our dataset do
not have accurately parsed results or fine grained
sentiment labels for phrases. Another reason is
that the RNTN model trained on movie reviews
cannot be directly applied on tweets due to the d-
ifferences between domains (Blitzer et al, 2007).
Results and Analysis. Table 2 shows the macro-
F1 of the baseline systems as well as the SSWE-
based methods on positive/negative sentimen-
t classification of tweets. Distant supervision is
relatively weak because the noisy-labeled tweet-
s are treated as the gold standard, which affects
the performance of classifier. The results of bag-
of-ngram (uni/bi/tri-gram) features are not satis-
fied because the one-hot word representation can-
not capture the latent connections between words.
NBSVM and RAE perform comparably and have
3
For 3-class sentiment classification in SemEval 2013,
our re-implementation of NRC achieved 68.3%, 0.7% low-
er than NRC (69%) due to less training data.
1560
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + uni/bi/tri-gram 63.84
SVM + unigram 74.50
SVM + uni/bi/tri-gram 75.06
NBSVM 75.28
RAE 75.12
NRC (Top System in SemEval) 84.73
NRC - ngram 84.17
SSWE
u
84.98
SSWE
u
+NRC 86.58
SSWE
u
+NRC-ngram 86.48
Table 2: Macro-F1 on positive/negative classifica-
tion of tweets.
a big gap in comparison with the NRC and SSWE-
based methods. The reason is that RAE and NB-
SVM learn the representation of tweets from the
small-scale manually annotated training set, which
cannot well capture the comprehensive linguistic
phenomenons of words.
NRC implements a variety of features and
reaches 84.73% in macro-F1, verifying the impor-
tance of a better feature representation for Twit-
ter sentiment classification. We achieve 84.98%
by using only SSWE
u
as features without borrow-
ing any sentiment lexicons or hand-crafted rules.
The results indicate that SSWE
u
automatically
learns discriminative features from massive tweets
and performs comparable with the state-of-the-art
manually designed features. After concatenating
SSWE
u
with the feature set of NRC, the perfor-
mance is further improved to 86.58%. We also
compare SSWE
u
with the ngram feature by inte-
grating SSWE into NRC-ngram. The concatenated
features SSWE
u
+NRC-ngram (86.48%) outperfor-
m the original feature set of NRC (84.73%).
As a reference, we apply SSWE
u
on subjec-
tive classification of tweets, and obtain 72.17% in
macro-F1 by using only SSWE
u
as feature. Af-
ter combining SSWE
u
with the feature set of NR-
C, we improve NRC from 74.86% to 75.39% for
subjective classification.
Comparision between Different Word Embed-
ding. We compare sentiment-specific word em-
bedding (SSWE
h
, SSWE
r
, SSWE
u
) with base-
line embedding learning algorithms by only us-
ing word embedding as features for Twitter sen-
timent classification. We use the embedding of u-
nigrams, bigrams and trigrams in the experimen-
t. The embeddings of C&W (Collobert et al,
2011), word2vec
4
, WVSA (Maas et al, 2011) and
our models are trained with the same dataset and
same parameter setting. We compare with C&W
and word2vec as they have been proved effective
in many NLP tasks. The trade-off parameter of
ReEmb (Labutov and Lipson, 2013) is tuned on
the development set of SemEval 2013.
Table 3 shows the performance on the pos-
itive/negative classification of tweets
5
. ReEm-
b(C&W) and ReEmb(w2v) stand for the use
of embeddings learned from 10 million distant-
supervised tweets with C&W and word2vec, re-
spectively. Each row of Table 3 represents a word
embedding learning algorithm. Each column s-
tands for a type of embedding used to compose
features of tweets. The column uni+bi denotes the
use of unigram and bigram embedding, and the
column uni+bi+tri indicates the use of unigram,
bigram and trigram embedding.
Embedding unigram uni+bi uni+bi+tri
C&W 74.89 75.24 75.89
Word2vec 73.21 75.07 76.31
ReEmb(C&W) 75.87 ? ?
ReEmb(w2v) 75.21 ? ?
WVSA 77.04 ? ?
SSWE
h
81.33 83.16 83.37
SSWE
r
80.45 81.52 82.60
SSWE
u
83.70 84.70 84.98
Table 3: Macro-F1 on positive/negative classifica-
tion of tweets with different word embeddings.
From the first column of Table 3, we can see that
the performance of C&W and word2vec are obvi-
ously lower than sentiment-specific word embed-
dings by only using unigram embedding as fea-
tures. The reason is that C&W and word2vec do
not explicitly exploit the sentiment information of
the text, resulting in that the words with oppo-
site polarity such as good and bad are mapped
to close word vectors. When such word embed-
dings are fed as features to a Twitter sentimen-
t classifier, the discriminative ability of sentiment
words are weakened thus the classification perfor-
mance is affected. Sentiment-specific word em-
4
Available at https://code.google.com/p/word2vec/. We
utilize the Skip-gram model because it performs better than
CBOW in our experiments.
5
MVSA and ReEmb are not suitable for learning bigram
and trigram embedding because their sentiment predictor
functions only utilize the unigram embedding.
1561
beddings (SSWE
h
, SSWE
r
, SSWE
u
) effectively
distinguish words with opposite sentiment polarity
and perform best in three settings. SSWE outper-
forms MVSA by exploiting more contextual infor-
mation in the sentiment predictor function. SSWE
outperforms ReEmb by leveraging more senti-
ment information from massive distant-supervised
tweets. Among three sentiment-specific word em-
beddings, SSWE
u
captures more context informa-
tion and yields best performance. SSWE
h
and
SSWE
r
obtain comparative results.
From each row of Table 3, we can see that the
bigram and trigram embeddings consistently im-
prove the performance of Twitter sentiment classi-
fication. The underlying reason is that a phrase,
which cannot be accurately represented by uni-
gram embedding, is directly encoded into the n-
gram embedding as an idiomatic unit. A typical
case in sentiment analysis is that the composed
phrase and multiword expression may have a dif-
ferent sentiment polarity than the individual word-
s it contains, such as not [bad] and [great] deal
of (the word in the bracket has different sentiment
polarity with the ngram). A very recent study by
Mikolov et al (2013) also verified the effective-
ness of phrase embedding for analogically reason-
ing phrases.
Effect of ? in SSWE
u
We tune the hyper-
parameter ? of SSWE
u
on the development set by
using unigram embedding as features. As given
in Equation 8, ? is the weighting score of syntac-
tic loss of SSWE
u
and trades-off the syntactic and
sentiment losses. SSWE
u
is trained from 10 mil-
lion distant-supervised tweets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
?
Ma
cro
?F1
 
 
SSWEu
Figure 2: Macro-F1 of SSWE
u
on the develop-
ment set of SemEval 2013 with different ?.
Figure 2 shows the macro-F1 of SSWE
u
on pos-
itive/negative classification of tweets with differ-
ent ? on our development set. We can see that
SSWE
u
performs better when ? is in the range
of [0.5, 0.6], which balances the syntactic context
and sentiment information. The model with ?=1
stands for C&W model, which only encodes the
syntactic contexts of words. The sharp decline at
?=1 reflects the importance of sentiment informa-
tion in learning word embedding for Twitter senti-
ment classification.
Effect of Distant-supervised Data in SSWE
u
We investigate how the size of the distant-
supervised data affects the performance of SSWE
u
feature for Twitter sentiment classification. We
vary the number of distant-supervised tweets from
1 million to 12 million, increased by 1 million.
We set the ? of SSWE
u
as 0.5, according to the
experiments shown in Figure 2. Results of posi-
tive/negative classification of tweets on our devel-
opment set are given in Figure 3.
1 2 3 4 5 6 7 8 9 10 11 12
x 106
0.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
# of distant?supervised tweets
Mac
ro?F
1
 
 
SSWEu
Figure 3: Macro-F1 of SSWE
u
with different size
of distant-supervised data on our development set.
We can see that when more distant-supervised
tweets are added, the accuracy of SSWE
u
con-
sistently improves. The underlying reason is that
when more tweets are incorporated, the word em-
bedding is better estimated as the vocabulary size
is larger and the context and sentiment informa-
tion are richer. When we have 10 million distant-
supervised tweets, the SSWE
u
feature increases
the macro-F1 of positive/negative classification of
tweets to 82.94% on our development set. When
we have more than 10 million tweets, the per-
formance remains stable as the contexts of words
have been mostly covered.
4.2 Word Similarity of Sentiment Lexicons
The quality of SSWE has been implicitly evaluat-
ed when applied in Twitter sentiment classification
in the previous subsection. We explicitly evaluate
it in this section through word similarity in the em-
1562
bedding space for sentiment lexicons. The evalua-
tion metric is the accuracy of polarity consistency
between each sentiment word and its topN closest
words in the sentiment lexicon,
Accuracy =
?
#Lex
i=1
?
N
j=1
?(w
i
, c
ij
)
#Lex?N
(10)
where #Lex is the number of words in the senti-
ment lexicon, w
i
is the i-th word in the lexicon, c
ij
is the j-th closest word tow
i
in the lexicon with co-
sine similarity, ?(w
i
, c
ij
) is an indicator function
that is equal to 1 if w
i
and c
ij
have the same sen-
timent polarity and 0 for the opposite case. The
higher accuracy refers to a better polarity consis-
tency of words in the sentiment lexicon. We set N
as 100 in our experiment.
Experiment Setup and Datasets We utilize
the widely-used sentiment lexicons, namely M-
PQA (Wilson et al, 2005) and HL (Hu and Liu,
2004), to evaluate the quality of word embedding.
For each lexicon, we remove the words that do
not appear in the lookup table of word embedding.
We only use unigram embedding in this section
because these sentiment lexicons do not contain
phrases. The distribution of the lexicons used in
this paper is listed in Table 4.
Lexicon Positive Negative Total
HL 1,331 2,647 3,978
MPQA 1,932 2,817 4,749
Joint 1,051 2,024 3,075
Table 4: Statistics of the sentiment lexicons. Join-
t stands for the words that occur in both HL and
MPQA with the same sentiment polarity.
Results. Table 5 shows our results com-
pared to other word embedding learning al-
gorithms. The accuracy of random result is
50% as positive and negative words are ran-
domly occurred in the nearest neighbors of
each word. Sentiment-specific word embed-
dings (SSWE
h
, SSWE
r
, SSWE
u
) outperform ex-
isting neural models (C&W, word2vec) by large
margins. SSWE
u
performs best in three lexicon-
s. SSWE
h
and SSWE
r
have comparable perfor-
mances. Experimental results further demonstrate
that sentiment-specific word embeddings are able
to capture the sentiment information of texts and
distinguish words with opposite sentiment polari-
ty, which are not well solved in traditional neural
Embedding HL MPQA Joint
Random 50.00 50.00 50.00
C&W 63.10 58.13 62.58
Word2vec 66.22 60.72 65.59
ReEmb(C&W) 64.81 59.76 64.09
ReEmb(w2v) 67.16 61.81 66.39
WVSA 68.14 64.07 67.12
SSWE
h
74.17 68.36 74.03
SSWE
r
73.65 68.02 73.14
SSWE
u
77.30 71.74 77.33
Table 5: Accuracy of the polarity consistency of
words in different sentiment lexicons.
models like C&W and word2vec. SSWE outper-
forms MVSA and ReEmb by exploiting more con-
text information of words and sentiment informa-
tion of sentences, respectively.
5 Conclusion
In this paper, we propose learning continuous
word representations as features for Twitter sen-
timent classification under a supervised learning
framework. We show that the word embedding
learned by traditional neural networks are not ef-
fective enough for Twitter sentiment classification.
These methods typically only model the contex-
t information of words so that they cannot dis-
tinguish words with similar context but opposite
sentiment polarity (e.g. good and bad). We learn
sentiment-specific word embedding (SSWE) by
integrating the sentiment information into the loss
functions of three neural networks. We train SS-
WE with massive distant-supervised tweets select-
ed by positive and negative emoticons. The ef-
fectiveness of SSWE has been implicitly evaluat-
ed by using it as features in sentiment classifica-
tion on the benchmark dataset in SemEval 2013,
and explicitly verified by measuring word similar-
ity in the embedding space for sentiment lexicon-
s. Our unified model combining syntactic context
of words and sentiment information of sentences
yields the best performance in both experiments.
Acknowledgments
We thank Yajuan Duan, Shujie Liu, Zhenghua Li,
Li Dong, Hong Sun and Lanjun Zhou for their
great help. This research was partly supported
by National Natural Science Foundation of China
(No.61133012, No.61273321, No.61300113).
1563
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy da-
ta. In Proceedings of International Conference on
Computational Linguistics, pages 36?44.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
Yoshua Bengio. 2013. Deep learning of represen-
tations: Looking forward. arXiv preprint arX-
iv:1305.0445.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings
of the Conference on Information and Knowledge
Management, pages 375?382.
Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shok-
oufandeh. 2012. Sentiment classification with su-
pervised sequence embedding. In Machine Learn-
ing and Knowledge Discovery in Databases, pages
159?174. Springer.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Annual Meeting of the Association for
Computational Linguistics, volume 7.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of International Con-
ference on Computational Linguistics, pages 241?
249.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894?904.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. E-
moticon smoothed language models for twitter sen-
timent analysis. In The Association for the Advance-
ment of Artificial Intelligence.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
1564
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-
do, and Jeffrey Dean. 2013. Distributed representa-
tions of words and phrases and their compositionali-
ty. The Conference on Neural Information Process-
ing Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Andriy Mnih and Geoffrey E Hinton. 2009. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the International Conference on Ma-
chine Learning, pages 129?136.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011c. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with composi-
tional vector grammars. In Annual Meeting of the
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Annual Meeting of the
Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmenta-
tion and pos tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 647?657.
1565
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49?54,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Adaptive Recursive Neural Network
for Target-dependent Twitter Sentiment Classification
Li Dong
??
Furu Wei
?
Chuanqi Tan
??
Duyu Tang
??
Ming Zhou
?
Ke Xu
?
?
Beihang University, Beijing, China
?
Microsoft Research, Beijing, China
?
Harbin Institute of Technology, Harbin, China
donglixp@gmail.com fuwei@microsoft.com {ysjtcq,tangduyu}@gmail.com
mingzhou@microsoft.com kexu@nlsde.buaa.edu.cn
Abstract
We propose Adaptive Recursive Neural
Network (AdaRNN) for target-dependent
Twitter sentiment classification. AdaRNN
adaptively propagates the sentiments of
words to target depending on the context
and syntactic relationships between them.
It consists of more than one composition
functions, and we model the adaptive sen-
timent propagations as distributions over
these composition functions. The experi-
mental studies illustrate that AdaRNN im-
proves the baseline methods. Further-
more, we introduce a manually annotated
dataset for target-dependent Twitter senti-
ment analysis.
1 Introduction
Twitter becomes one of the most popular social
networking sites, which allows the users to read
and post messages (i.e. tweets) up to 140 charac-
ters. Among the great varieties of topics, people
in Twitter tend to express their opinions for the
brands, celebrities, products and public events. As
a result, it attracts much attention to estimate the
crowd?s sentiments in Twitter.
For the tweets, our task is to classify their senti-
ments for a given target as positive, negative, and
neutral. People may mention several entities (or
targets) in one tweet, which affects the availabil-
ities for most of existing methods. For example,
the tweet ?@ballmer: windows phone is better
than ios!? has three targets (@ballmer, windows
phone, and ios). The user expresses neutral, pos-
itive, and negative sentiments for them, respec-
tively. If target information is ignored, it is diffi-
cult to obtain the correct sentiment for a specified
target. For target-dependent sentiment classifica-
tion, the manual evaluation of Jiang et al (2011)
?
Contribution during internship at Microsoft Research.
show that about 40% of errors are caused by not
considering the targets in classification.
The features used in traditional learning-based
methods (Pang et al, 2002; Nakagawa et al, 2010)
are independent to the targets, hence the results
are computed despite what the targets are. Hu and
Liu (2004) regard the features of products as tar-
gets, and sentiments for them are heuristically de-
termined by the dominant opinion words. Jiang
et al (2011) combine the target-independent fea-
tures (content and lexicon) and target-dependent
features (rules based on the dependency parsing
results) together in subjectivity classification and
polarity classification for tweets.
In this paper, we mainly focus on integrating
target information with Recursive Neural Network
(RNN) to leverage the ability of deep learning
models. The neural models use distributed repre-
sentation (Hinton, 1986; Rumelhart et al, 1986;
Bengio et al, 2003) to automatically learn fea-
tures for target-dependent sentiment classification.
RNN utilizes the recursive structure of text, and it
has achieved state-of-the-art sentiment analysis re-
sults for movie review dataset (Socher et al, 2012;
Socher et al, 2013). The recursive neural mod-
els employ the semantic composition functions,
which enables them to handle the complex com-
positionalities in sentiment analysis.
Specifically, we propose a framework which
learns to propagate the sentiments of words to-
wards the target depending on context and syn-
tactic structure. We employ a novel adaptive
multi-compositionality layer in recursive neural
network, which is named as AdaRNN (Dong et
al., 2014). It consists of more than one compo-
sition functions, and we model the adaptive sen-
timent propagations as learning distributions over
these composition functions. We automatically
learn the composition functions and how to select
them from supervisions, instead of choosing them
heuristically or by hand-crafted rules. AdaRNN
49
determines how to propagate the sentiments to-
wards the target and handles the negation or in-
tensification phenomena (Taboada et al, 2011) in
sentiment analysis. In addition, we introduce a
manually annotated dataset, and conduct extensive
experiments on it. The experimental results sug-
gest that our approach yields better performances
than the baseline methods.
2 RNN: Recursive Neural Network
RNN (Socher et al, 2011) represents the phrases
and words as D-dimensional vectors. It performs
compositions based on the binary trees, and obtain
the vector representations in a bottom-up way.
not very good
Negative
Softmax
very good
not very good
Figure 1: The composition process for ?not very
good? in Recursive Neural Network.
As illustrated in Figure 1, we obtain the repre-
sentation of ?very good? by the composition of
?very? and ?good?, and the representation of tri-
gram ?not very good? is recursively obtained by
the vectors of ?not? and ?very good?. The di-
mensions of parent node are calculated by linear
combination of the child vectors? dimensions. The
vector representation v is obtained via:
v = f (g (v
l
,v
r
)) = f
(
W
[
v
l
v
r
]
+ b
)
(1)
where v
l
,v
r
are the vectors of its left and right
child, g is the composition function, f is the non-
linearity function (such as tanh, sigmoid, softsign,
etc.), W ? R
D?2D
is the composition matrix, and
b is the bias vector. The dimension of v is the
same as its child vectors, and it is recursively used
in the next step. Notably, the word vectors in the
leaf nodes are regarded as the parameters, and will
be updated according to the supervisions.
The vector representation of root node is then
fed into a softmax classifier to predict the label.
The k-th element of softmax(x) is
exp{x
k
}?
j
exp{x
j
}
. For
a vector, the softmax obtains the distribution over
K classes. Specifically, the predicted distribution
is y = softmax (Uv), where y is the predicted
distribution, U ? R
K?D
is the classification ma-
trix, and v is the vector representation of node.
3 Our Approach
We use the dependency parsing results to find the
words syntactically connected with the interested
target. Adaptive Recursive Neural Network is pro-
posed to propagate the sentiments of words to the
target node. We model the adaptive sentiment
propagations as semantic compositions. The com-
putation process is conducted in a bottom-up man-
ner, and the vector representations are computed
recursively. After we obtain the representation of
target node, a classifier is used to predict the sen-
timent label according to the vector.
In Section 3.1, we show how to build recur-
sive structure for target using the dependency pars-
ing results. In Section 3.2, we propose Adaptive
Recursive Neural Network and use it for target-
dependent sentiment analysis.
3.1 Build Recursive Structure
The dependency tree indicates the dependency re-
lations between words. As described above, we
propagate the sentiments of words to the target.
Hence the target is placed at the root node to com-
bine with its connected words recursively. The de-
pendency relation types are remained to guide the
sentiment propagations in our model.
Algorithm 1 Convert Dependency Tree
Input: Target node, Dependency tree
Output: Converted tree
1: function CONV(r)
2: E
r
? SORT(dep edges connected with r)
3: v? r
4: for (r
t
?? u/u
t
?? r) in E
r
do
5: if r is head of u then
6: w? node with CONV(u), v as children
7: else
8: w? node with v, CONV(u) as children
9: v? w
10: return v
11: Call CONV(target node) to get converted tree
As illustrated in the Algorithm 1, we recursively
convert the dependency tree starting from the tar-
get node. We find all the words connected to the
target, and these words are combined with target
node by certain order. Every combination is con-
sidered as once propagation of sentiments. If the
target is head of the connected words, the target
vector is combined as the right node; if otherwise,
it is combined as the left node. This ensures the
50
child nodes in a certain order. We use two rules
to determine the order of combinations: (1) the
words whose head is the target in dependency tree
are first combined, and then the rest of connected
words are combined; (2) if the first rule cannot de-
termine the order, the connected words are sorted
by their positions in sentence from right to left.
Notably, the conversion is performed recursively
for the connected words and the dependency rela-
tion types are remained. Figure 2 shows the con-
verted results for different targets in one sentence.
3.2 AdaRNN: Adaptive Recursive Neural
Network
RNN employs one global matrix to linearly com-
bine the elements of vectors. Sometimes it is
challenging to obtain a single powerful function
to model the semantic composition, which moti-
vates us to propose AdaRNN. The basic idea of
AdaRNN is to use more than one composition
functions and adaptively select them depending on
the linguistic tags and the combined vectors. The
model learns to propagate the sentiments of words
by using the different composition functions.
Figure 2 shows the computation process for the
example sentence ?windows is better than ios?,
where the user expresses positive sentiment to-
wards windows and negative sentiment to ios. For
the targets, the order of compositions and the de-
pendency types are different. AdaRNN adap-
tively selects the composition functions g
1
. . . g
C
depending on the child vectors and the linguistic
types. Thus it is able to determine how to propa-
gate the sentiments of words towards the target.
Based on RNN described in Section 2, we de-
fine the composition result v in AdaRNN as:
v = f
(
C
?
h=1
P (g
h
|v
l
,v
r
, e) g
h
(v
l
,v
r
)
)
(2)
where g
1
, . . . , g
C
are the composition functions,
P (g
h
|v
l
,v
r
, e) is the probability of employing g
h
given the child vectors v
l
,v
r
and external feature
vector e, and f is the nonlinearity function. For
the composition functions, we use the same forms
as in Equation (1), i.e., we have C composition
matrices W
1
. . .W
C
. We define the distribution
over these composition functions as:
?
?
?
P (g
1
|v
l
,v
r
, e)
.
.
.
P (g
C
|v
l
,v
r
, e)
?
?
?
= softmax
?
?
?S
?
?
v
l
v
r
e
?
?
?
?
(3)
where ? is the hyper-parameter, S ? R
C?(2D+|e|)
is the matrix used to determine which composition
function we use, v
l
,v
r
are the left and right child
vectors, and e are external feature vector. In this
work, e is a one-hot binary feature vector which
indicates what the dependency type is. If relation
is the k-th type, we set e
k
to 1 and the others to 0.
Adding ? in softmax function is a widely used
parametrization method in statistical mechanics,
which is known as Boltzmann distribution and
Gibbs measure (Georgii, 2011). When ? = 0, this
function produces a uniform distribution; when
? = 1, it is the same as softmax function; when
? ??, it only activates the dimension with max-
imum weight, and sets its probability to 1.
3.3 Model Training
We use the representation of root node as the fea-
tures, and feed them into the softmax classifier to
predict the distribution over classes. We define the
ground truth vector t as a binary vector. If the k-th
class is the label, only t
k
is 1 and the others are
0. Our goal is to minimize the cross-entropy error
between the predicted distribution y and ground
truth distribution t. For each training instance, we
define the objective function as:
min
?
?
?
j
t
j
logy
j
+
?
???
?
?
???
2
2
(4)
where ? represents the parameters, and the L
2
-
regularization penalty is used.
Based on the converted tree, we employ back-
propagation algorithm (Rumelhart et al, 1986) to
propagate the errors from root node to the leaf
nodes. We calculate the derivatives to update the
parameters. The AdaGrad (Duchi et al, 2011) is
employed to solve this optimization problem.
4 Experiments
As people tend to post comments for the celebri-
ties, products, and companies, we use these key-
words (such as ?bill gates?, ?taylor swift?, ?xbox?,
?windows 7?, ?google?) to query the Twitter API.
After obtaining the tweets, we manually anno-
tate the sentiment labels (negative, neutral, posi-
tive) for these targets. In order to eliminate the
effects of data imbalance problem, we randomly
sample the tweets and make the data balanced.
The negative, neutral, positive classes account for
25%, 50%, 25%, respectively. Training data con-
sists of 6,248 tweets, and testing data has 692
51
w indows
is
better
io s than
g1 gC...
g 1 gC...
g1 gC...
g 1 gC...
n s u b j
cop
p rep
pobj
Positve
Softmax
io s
than
w indows
is better
g1 g C...
g1 gC...
g1 gC...
g 1 g C...
pobj
prep
ns u b j
cop
Negative
Softmax
w indows is better than io s
RO OT
cop
ns ub j
p rep pobj
( target ) ( target )
Dependency tree :
windows is target : ios is target :
Figure 2: For the sentence ?windows is better than ios?, we convert its dependency tree for the different
targets (windows and ios). AdaRNN performs semantic compositions in bottom-up manner and forward
propagates sentiment information to the target node. The g
1
, . . . , g
C
are different composition functions,
and the combined vectors and dependency types are used to select them adaptively. These composition
functions decide how to propagate the sentiments to the target.
tweets. We randomly sample some tweets, and
they are assigned with sentiment labels by two an-
notators. About 82.5% of them have the same la-
bels. The agreement percentage of polarity clas-
sification is higher than subjectivity classification.
To the best of our knowledge, this is the largest
target-dependent Twitter sentiment classification
dataset which is annotated manually. We make the
dataset publicly available
1
for research purposes.
We preprocess the tweets by replacing the tar-
gets with $T$ and setting their POS tags to NN.
Liblinear (Fan et al, 2008) is used for baselines.
A tweet-specific tokenizer (Gimpel et al, 2011)
is employed, and the dependency parsing results
are computed by Stanford Parser (Klein and Man-
ning, 2003). The hyper-parameters are chosen by
cross-validation on the training split, and the test
accuracy and macro-average F1-score score are re-
ported. For recursive neural models, the dimen-
sion of word vector is set to 25, and f = tanh
is used as the nonlinearity function. We employ
10 composition matrices in AdaRNN. The param-
eters are randomly initialized. Notably, the word
vectors will also be updated.
SVM-indep: It uses the uni-gram, bi-gram,
punctuations, emoticons, and #hashtags as the
content features, and the numbers of positive or
negative words in General Inquirer as lexicon fea-
tures. These features are all target-independent.
SVM-dep: We re-implement the method pro-
posed by Jiang et al (2011). It combines both
1
http://goo.gl/5Enpu7
the target-independent (SVM-indep) and target-
dependent features and uses SVM as the classifier.
There are seven rules to extract target-sensitive
features. We do not implement the social graph
optimization and target expansion tricks in it.
SVM-conn: The words, punctuations, emoti-
cons, and #hashtags included in the converted de-
pendency tree are used as the features for SVM.
RNN: It is performed on the converted depen-
dency tree without adaptive composition selection.
AdaRNN-w/oE: Our approach without using
the dependency types as features in adaptive se-
lection for the composition functions.
AdaRNN-w/E: Our approach with employing
the dependency types as features in adaptive se-
lection for the composition functions.
AdaRNN-comb: We combine the root vectors
obtained by AdaRNN-w/E with the uni/bi-gram
features, and they are fed into a SVM classifier.
Method Accuracy Macro-F1
SVM-indep 62.7 60.2
SVM-dep 63.4 63.3
SVM-conn 60.0 59.6
RNN 63.0 62.8
AdaRNN-w/oE 64.9 64.4
AdaRNN-w/E 65.8 65.5
AdaRNN-comb 66.3 65.9
Table 1: Evaluation results on target-dependent
Twitter sentiment classification dataset. Our ap-
proach outperforms the baseline methods.
52
As shown in the Table 1, AdaRNN achieves bet-
ter results than the baselines. Specifically, we find
that the performances of SVM-dep increase than
SVM-indep. It indicates that target-dependent fea-
tures help improve the results. However, the accu-
racy and F1-score do not gain significantly. This
is caused by mismatch of the rules (Jiang et al,
2011) used to extract the target-dependent fea-
tures. The POS tagging and dependency parsing
results are not precise enough for the Twitter data,
so these hand-crafted rules are rarely matched.
Further, the results of SVM-conn illustrate that us-
ing the words which have paths to target as bag-of-
words features does not perform well.
RNN is also based on the converted depen-
dency tree. It outperforms SVM-indep, and is
comparable with SVM-dep. The performances
of AdaRNN-w/oE are better than the above base-
lines. It shows that multiple composition functions
and adaptive selection help improve the results.
AdaRNN provides more powerful composition
ability, so that it achieves better semantic compo-
sition for recursive neural models. AdaRNN-w/E
obtains best performances among the above meth-
ods. Its macro-average F1-score rises by 5.3%
than the target-independent method SVM-indep.
It employs dependency types as binary features to
select the composition functions adaptively. The
results illustrate that the syntactic tags are helpful
to guide the model propagate sentiments of words
towards target. Although the dependency results
are also not precise enough, the composition se-
lection is automatically learned from data. Hence
AdaRNN is more robust for the imprecision of
parsing results than the hand-crafted rules. The
performances become better after adding the uni-
gram and bi-gram features (target-independent).
4.1 Effects of ?
We compare different ? for AdaRNN defined in
Equation (3) in this section. Different parameter ?
leads to different composition selection schemes.
As illustrated in Figure 3, the AdaRNN-w/oE
and AdaRNN-w/E achieve the best accuracies at
? = 2, and they have a similar trend. Specifi-
cally, ? = 0 obtains a uniform distribution over
the composition functions which does not help im-
prove performances. ? ? ? results in a max-
imum probability selection algorithm, i.e., only
the composition function which has the maximum
probability is used. This selection scheme makes
0 20 21 22 23 24 25 26?61
62
63
64
65
66
Accur
acy
RNNAdaRNN-w/oEAdaRNN-w/E
Figure 3: The curve shows the accuracy as the
hyper-parameter ? = 0, 2
0
, 2
1
, . . . , 2
6
increases.
AdaRNN achieves the best results at ? = 2
1
.
the optimization instable. The performances of
? = 1, 2 are similar and they are better than
other settings. It indicates that adaptive selection
method is useful to model the compositions. The
hyper-parameter ? makes trade-offs between uni-
form selection and maximum selection. It adjusts
the effects of these two perspectives.
5 Conclusion
We propose Adaptive Recursive Neural Network
(AdaRNN) for the target-dependent Twitter senti-
ment classification. AdaRNN employs more than
one composition functions and adaptively chooses
them depending on the context and linguistic tags.
For a given tweet, we first convert its dependency
tree for the interested target. Next, the AdaRNN
learns how to adaptively propagate the sentiments
of words to the target node. AdaRNN enables
the sentiment propagations to be sensitive to both
linguistic and semantic categories by using differ-
ent compositions. The experimental results illus-
trate that AdaRNN improves the baselines without
hand-crafted rules.
Acknowledgments
This research was partly supported by the National
863 Program of China (No. 2012AA011005), the
fund of SKLSDE (Grant No. SKLSDE-2013ZX-
06), and Research Fund for the Doctoral Pro-
gram of Higher Education of China (Grant No.
20111102110019).
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155, March.
53
Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In Twenty-Eighth AAAI Conference on Artificial In-
telligence (AAAI). AAAI.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 12:2121?2159,
July.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874, June.
H.O. Georgii. 2011. Gibbs Measures and Phase
Transitions. De Gruyter studies in mathematics. De
Gruyter.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies: Short Papers - Volume 2,
HLT ?11, pages 42?47, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Geoffrey E. Hinton. 1986. Learning distributed repre-
sentations of concepts. In Proceedings of the Eighth
Annual Conference of the Cognitive Science Society,
pages 1?12. Hillsdale, NJ: Erlbaum.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 151?160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 786?794.
Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86. As-
sociation for Computational Linguistics.
D.E. Rumelhart, G.E. Hinton, and R.J. Williams. 1986.
Learning representations by back-propagating er-
rors. Nature, 323(6088):533?536.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In ICML.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
EMNLP-CoNLL, pages 1201?1211.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. In EMNLP, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Comput. Lin-
guist., 37(2):267?307, June.
54
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 208?212,
Dublin, Ireland, August 23-24, 2014.
Coooolll: A Deep Learning System for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Bing Qin
?
, Ting Liu
?
, Ming Zhou
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
{dytang, qinb, tliu}@ir.hit.edu.cn
{fuwei, mingzhou}@microsoft.com
Abstract
In this paper, we develop a deep learn-
ing system for message-level Twitter sen-
timent classification. Among the 45 sub-
mitted systems including the SemEval
2013 participants, our system (Coooolll)
is ranked 2nd on the Twitter2014 test set
of SemEval 2014 Task 9. Coooolll is
built in a supervised learning framework
by concatenating the sentiment-specific
word embedding (SSWE) features with
the state-of-the-art hand-crafted features.
We develop a neural network with hybrid
loss function
1
to learn SSWE, which en-
codes the sentiment information of tweets
in the continuous representation of words.
To obtain large-scale training corpora, we
train SSWE from 10M tweets collected by
positive and negative emoticons, without
any manual annotation. Our system can
be easily re-implemented with the publicly
available sentiment-specific word embed-
ding.
1 Introduction
Twitter sentiment classification aims to classify
the sentiment polarity of a tweet as positive, nega-
tive or neutral (Jiang et al., 2011; Hu et al., 2013;
Dong et al., 2014). The majority of existing ap-
proaches follow Pang et al. (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
?
This work was partly done when the first author was
visiting Microsoft Research.
1
This is one of the three sentiment-specific word embed-
ding learning algorithms proposed in Tang et al. (2014).
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
designing effective features to obtain better clas-
sification performance (Pang and Lee, 2008; Liu,
2012; Feldman, 2013). For example, Mohammad
et al. (2013) implement diverse sentiment lexicons
and a variety of hand-crafted features. To leverage
massive tweets containing positive and negative e-
moticons for automatically feature learning, Tang
et al. (2014) propose to learn sentiment-specific
word embedding and Kalchbrenner et al. (2014)
model sentence representation with Dynamic Con-
volutional Neural Network.
In this paper, we develop a deep learning sys-
tem for Twitter sentiment classification. First-
ly, we learn sentiment-specific word embedding
(SSWE) (Tang et al., 2014), which encodes the
sentiment information of text into the continuous
representation of words (Mikolov et al., 2013; Sun
et al., 2014). Afterwards, we concatenate the SS-
WE features with the state-of-the-art hand-crafted
features (Mohammad et al., 2013), and build the
sentiment classifier with the benchmark dataset
from SemEval 2013 (Nakov et al., 2013). To
learn SSWE, we develop a tailored neural net-
work, which incorporates the supervision from
sentiment polarity of tweets in the hybrid loss
function. We learn SSWE from tweets, lever-
aging massive tweets with emoticons as distant-
supervised corpora without any manual annota-
tions.
We evaluate the deep learning system on the
test set of Twitter Sentiment Analysis Track in Se-
mEval 2014
2
. Our system (Coooolll) is ranked
2nd on the Twitter2014 test set, along with the
SemEval 2013 participants owning larger train-
ing data than us. The performance of only us-
ing SSWE as features is comparable to the state-
of-the-art hand-crafted features (detailed in Ta-
ble 3), which verifies the effectiveness of the
sentiment-specific word embedding. We release
the sentiment-specific word embedding learned
2
http://alt.qcri.org/semeval2014/task9/
208
Training 
Data 
Learning 
Algorithm 
Feature 
Representation 
Sentiment 
Classifier 
1 
2 
?. 
 
N 
 
N+1 
N+2 
? 
N+K 
STATE 
Feature 
SSWE 
Feature 
all-cap 
emoticon 
? 
?. 
dimension 1 
dimension 2 
dimension N 
elongated 
Massive 
Tweets 
Embedding 
Learning 
Figure 1: Our deep learning system (Coooolll) for
Twitter sentiment classification.
from 10 million tweets, which can be easily used
to re-implement our system and adopted off-the-
shell in other sentiment analysis tasks.
2 A Deep Learning System
In this section, we present the details of our deep
learning system for Twitter sentiment classifica-
tion. As illustrated in Figure 1, Coooolll is a su-
pervised learning method that builds the sentimen-
t classifier from tweets with manually annotated
sentiment polarity. In our system, the feature rep-
resentation of tweet is composed of two parts, the
sentiment-specific word embedding features (SS-
WE features) and the state-of-the-art hand-crafted
features (STATE features). In the following parts,
we introduce the SSWE features and STATE fea-
tures, respectively.
2.1 SSWE Features
In this part, we first describe the neural network
for learning sentiment-specific word embedding.
Then, we generate the SSWE features of a tweet
from the embedding of words it contains.
Our neural network is an extension of the tra-
ditional C&W model (Collobert et al., 2011), as
illustrated in Figure 2. Unlike C&W model that
learns word embedding by only modeling syntac-
tic contexts of words, we develop SSWEu, which
captures the sentiment information of sentences as
well as the syntactic contexts of words. Given an
original (or corrupted) ngram and the sentiment
polarity of a sentence as the input, SSWE
u
predict-
s a two-dimensional vector for each input ngram.
The two scalars (f
u
0
, f
u
1
) stand for language model
score and sentiment score of the input ngram, re-
so cooool :D 
syntactic 
sentiment 
Figure 2: Our neural network (SSWE
u
) for learn-
ing sentiment-specific word embedding.
spectively. The training objectives of SSWE
u
are
that (1) the original ngram should obtain a high-
er language model score f
u
0
(t) than the corrupted
ngram f
u
0
(t
r
), and (2) the sentiment score of orig-
inal ngram f
u
1
(t) should be more consistent with
the gold polarity annotation of sentence than cor-
rupted ngram f
u
1
(t
r
). The loss function of SSWE
u
is the linear combination of two hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(1)
where where t is the original ngram, t
r
is the cor-
rupted ngram which is generated from t with mid-
dle word replaced by a randomly selected one,
loss
cw
(t, t
r
) is the syntactic loss as given in E-
quation 2, loss
us
(t, t
r
) is the sentiment loss as
described in Equation 3. The hyper-parameter ?
weighs the two parts.
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(2)
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(3)
where ?
s
(t) is an indicator function reflecting the
sentiment polarity of a sentence, whose value is 1
if the sentiment polarity of tweet t is positive and
-1 if t?s polarity is negative. We train sentiment-
specific word embedding from 10M tweets col-
lected with positive and negative emoticons (Hu
et al., 2013). The details of training phase are de-
scribed in Tang et al. (2014).
After finish learning SSWE, we explore min,
average and max convolutional layers (Collobert
et al., 2011; Socher et al., 2011; Mitchell and Lap-
ata, 2010), to obtain the tweet representation. The
result is the concatenation of vectors derived from
different convolutional layers.
209
2.2 STATE Features
We re-implement the state-of-the-art hand-crafted
features (Mohammad et al., 2013) for Twitter sen-
timent classification. The STATE features are de-
scribed below.
? All-Caps. The number of words with all char-
acters in upper case.
? Emoticons. We use the presence of positive
(or negative) emoticons and whether the last
unit of a segmentation is emoticon
3
.
? Elongated Units. The number of elongated
words (with one character repeated more than
two times), such as gooood.
? Sentiment Lexicon. We utilize several senti-
ment lexicons
4
to generate features. We ex-
plore the number of sentiment words, the s-
core of last sentiment words, the total senti-
ment score and the maximal sentiment score
for each lexicon.
? Negation. The number of individual nega-
tions
5
within a tweet.
? Punctuation. The number of contiguous se-
quences of dot, question mark and exclama-
tion mark.
? Cluster. The presence of words from each
of the 1,000 clusters from the Twitter NLP
tool (Gimpel et al., 2011).
? Ngrams. The presence of word ngrams (1-4)
and character ngrams (3-5).
3 Experiments
We evaluate our deep learning system by applying
it for Twitter sentiment classification within a su-
pervised learning framework. We conduct exper-
iments on both positive/negative/neutral and posi-
tive/negative classification of tweets.
3
We use the positive and negative emoticons from Sen-
tiStrength, available at http://sentistrength.wlv.ac.uk/.
4
HL (Hu and Liu, 2004), MPQA (Wilson et al., 2005), N-
RC Emotion (Mohammad and Turney, 2013), NRC Hashtag
and Sentiment140Lexicon (Mohammad et al., 2013).
5
http://sentiment.christopherpotts.net/lingstruc.html
3.1 Dataset and Setting
We train the Twitter sentiment classifier on the
benchmark dataset in SemEval 2013 (Nakov et
al., 2013). The training and development sets were
completely in full to task participants of SemEval
2013. However, we were unable to download al-
l the training and development sets because some
tweets were deleted or not available due to modi-
fied authorization status. The distribution of our
dataset is given in Table 1. We train sentimen-
t classifiers with LibLinear (Fan et al., 2008) on
the training set and dev set, and tune parameter
?c,?wi of SVM on the test set of SemEval 2013.
In both experiment settings, the evaluation met-
ric is the macro-F1 of positive and negative class-
es (Nakov et al., 2013).
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of our SemEval 2013 Twitter
sentiment classification dataset.
The test sets of SemEval 2014 is directly pro-
vided to the participants, which is composed of
five parts. The statistic of test sets in SemEval
2014 is given in Table 2.
Positive Negative Neutral Total
T1 427 304 411 1,142
T2 492 394 1,207 2,093
T3 1,572 601 1,640 3,813
T4 982 202 669 1,939
T5 33 40 13 86
Table 2: Statistics of SemEval 2014 Twitter senti-
ment classification test set. T1 is LiveJournal2014,
T2 is SMS2013, T3 is Twitter2013, T4 is Twit-
ter2014, T5 is Twitter2014Sarcasm.
3.2 Results and Analysis
The experiment results of different methods
on positive/negative/neutral and positive/negative
Twitter sentiment classification are listed in Ta-
ble 3. The meanings of T1?T5 in each column are
described in Table 2. SSWE means the approach
that only utilizes the sentiment-specific word em-
bedding as features for Twitter sentiment classi-
fication. In STATE, we only utilize the existing
features (Mohammad et al., 2013) for building the
210
Method
Positive/Negative/Neutral Positive/Negative
T1 T2 T3 T4 T5 T1 T2 T3 T4 T5
SSWE 70.49 64.29 68.69 66.86 50.00 84.51 85.19 85.06 86.14 62.02
Coooolll 72.90 67.68 70.40 70.14 46.66 86.46 85.32 86.01 87.61 56.55
STATE 71.48 65.43 66.18 67.07 44.89 83.96 82.82 84.39 86.16 58.27
W2V 55.19 52.98 52.33 50.58 49.63 68.87 71.89 74.50 71.52 61.60
Top 74.84 70.28 72.12 70.96 58.16 - - - - - - - - - -
Average 63.52 55.63 59.78 60.41 45.44 - - - - - - - - - -
Table 3: Macro-F1 of positive and negative classes in positive/negative/neutral and positive/negative
Twitter sentiment classification on the test sets (T1-T5, detailed in Table 2) of SemEval 2014. The
performances of Coooolll on the Twitter-relevant test sets are bold.
sentiment classifier. In Coooolll, we use the con-
catenation of SSWE features and STATE features.
In W2V, we only use the word embedding learned
from word2vec
6
as features. Top and Average are
the top and average performance of the 45 team-
s of SemEval 2014, including the SemEval 2013
participants who owns larger training data.
On positive/negative/neutral classification of
tweets as listed in Table 3 (left table), we find
that the learned sentiment-specific word embed-
ding features (SSWE) performs comparable with
the state-of-the-art hand-crafted features (STATE),
especially on the Twitter-relevant test sets (T3
and T4)
7
. After feature combination, Coooolll
yields 4.22% and 3.07% improvement by macro-
F1 on T3 and T4,which verifies the effective-
ness of SSWE by learning discriminate features
from massive data for Twitter sentiment classifi-
cation. From the 45 teams, Coooolll gets the Rank
5/2/3/2 on T1-T4 respectively, along with the Se-
mEval 2013 participants owning larger training
data. We also comparing SSWE with the context-
based word embedding (W2V), which don?t cap-
ture the sentiment supervision of tweets. We find
that W2V is not effective enough for Twitter sen-
timent classification as there is a big gap between
W2V and SSWE on T1-T4. The reason is that W2V
does not capture the sentiment information of text,
which is crucial for sentiment analysis tasks and
effectively leveraged for learning the sentiment-
specific word embedding.
We also conduct experiments on the posi-
6
We utilize the Skip-gram model. The embedding is
trained from the 10M tweets collected by positive and neg-
ative emoticons, as same as the training data of SSWE.
7
The result of STATE on T3 is different from the results
reported in Mohammad et al. (2013) and Tang et al. (2014)
because we have different training data with the former and
different -wi of SVM with the latter.
tive/negative classification of tweets. The reason
is that the sentiment-specific word embedding is
learned from the positive/negative supervision of
tweets through emoticons, which is tailored for
positive/negative classification of tweets. From
Table 3 (right table), we find that the performance
of positive/negative Twitter classification is con-
sistent with the performance of 3-class classifica-
tion. SSWE performs comparable to STATE on T3
and T4, and yields better performance (1.62% and
1.45% improvements on T3 and T4, respectively)
through feature combination. SSWE outperform-
s W2V by large margins (more than 10%) on T3
and T4, which further verifies the effectiveness of
sentiment-specific word embedding.
4 Conclusion
We develop a deep learning system (Coooolll) for
message-level Twitter sentiment classification in
this paper. The feature representation of Cooool-
ll is composed of two parts, a state-of-the-art
hand-crafted features and the sentiment-specific
word embedding (SSWE) features. The SSWE
is learned from 10M tweets collected by posi-
tive and negative emoticons, without any manu-
al annotation. The effectiveness of Coooolll has
been verified in both positive/negative/neutral and
positive/negative classification of tweets. Among
45 systems of SemEval 2014 Task 9 subtask(b),
Coooolll yields Rank 2 on the Twitter2014 test set,
along with the SemEval 2013 participants owning
larger training data.
Acknowledgments
We thank Li Dong for helpful discussions. This
work was partly supported by National Natu-
ral Science Foundation of China (No.61133012,
No.61273321, No.61300113).
211
References
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 49?54.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDDConference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computation-
al Linguistics, pages 655?665.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word rep-
resentations in vector space. arXiv preprint arX-
iv:1301.3781.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion, pages 321?327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13, pages
312?320.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou
Ji, and Xiaolong Wang. 2014. Radical-enhanced
chinese character embedding. arXiv preprint arX-
iv:1404.4714.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 1555?1565.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
212
