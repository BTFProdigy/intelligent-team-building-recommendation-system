MPLUS:  A Probabilistic Medical Language Understanding  System 
Lee M. Christensen, Peter J. Haug, and Marcelo Fiszman 
Department of Medical Informatics, LDS Hospital/University of Utah, Salt Lake City, UT 
E-mail: ldlchris@ihc.com, ldphaug@ihc.com, ldmfiszm@ihc.com 
 
Abstract 
This paper describes the basic philosophy 
and implementation of MPLUS (M+), a 
robust medical text analysis tool that uses a 
semantic model based on Bayesian 
Networks (BNs).  BNs provide a concise 
and useful formalism for representing 
semantic patterns in medical text, and for 
recognizing and reasoning over those 
patterns. BNs are noise-tolerant, and 
facilitate the training of M+. 
1 
2 
Introduction 
In the field of medical informatics, 
computerized tools are being developed that 
depend on databases of clinical information.  
These include alerting systems for improved 
patient care, data mining systems for quality 
assurance and research, and diagnostic systems 
for more complex medical decision support.  
These systems require data that is appropriately 
structured and coded.  Since a large portion of 
the information stored in patient databases is in 
the form of free text, manually coding this 
information in a format accessible to these tools 
can be time consuming and expensive.  In recent 
years, natural language processing (NLP) 
methodologies have been studied as a means of 
automating this task. There have been many 
projects involving automated medical language 
analysis, including deciphering pathology 
reports (Smart and Roux, 1995), physical exam 
findings (Lin et al, 1991), and radiology reports 
(Friedman et al, 1994; Ranum, 1989; Koehler, 
1998).   
M+ is the latest in a line of NLP tools 
developed at LDS Hospital in Salt Lake City, 
Utah.  Its predecessors include SPRUS (Ranum, 
1989) and SymText (Koehler, 1998).  These 
tools have been used in the realm of radiology 
reports, admitting diagnoses (Haug et al, 1997), 
radiology utilization review (Fiszman, 2002) 
and syndromic detection (Chapman et al, 
2002).  Some of the character of these tools 
derives from common characteristics of 
radiology reports, their initial target domain.  
 Because of the off-the-cuff nature of 
radiology dictation, a report will frequently 
contain text that is telegraphic or otherwise not 
well formed grammatically.  Our desire was not 
only to take advantage of phrasal structure to 
discover semantic patterns in text, but also to be 
able to infer those patterns from lexical and 
contextual cues when necessary. 
Most NLP systems capable of semantic 
analysis employ representational formalisms 
with ties to classical logic, including semantic 
grammars (Friedman et al, 1994), unification-
based semantics (Moore, 1989), and description 
logics (Romacker and Hahn, 2000). M+ and its 
predecessors employ Bayesian Networks (Pearl, 
1988), a methodology outside this tradition.  
This study discusses the philosophy and 
implementation of M+, and attempts to show 
how Bayesian Networks can be useful in 
medical text analysis.  
The M+ Semantic Model 
2.1 Semantic Bayesian Networks 
M+ uses Bayesian Networks (BNs) to represent 
the basic semantic types and relations within a 
medical domain such as chest radiology reports.   
M+  BNs are structurally similar to semantic 
networks, in that they are implemented as 
directed acyclic graphs, with  nodes 
representing word and concept types, and links 
representing relations between those types.  BNs 
also have a character as frames or slot-filler 
representations (Minsky, 1975).  Each node is 
treated as a variable, with an associated  list of 
possible values.  For instance a node 
representing "disease severity" might include 
the possible values {"severe", "moderate", 
"mild"}. Each value  has a probability, either 
assigned or inferred, of being the true value of 
that node.   
In addition to providing a framework 
for representation, a BN is also a probabilistic 
inference engine.  The probability of each 
possible value of a node is conditioned on the 
probabilities of the values of neighboring nodes, 
                                            Association for Computational Linguistics.
                            the Biomedical Domain, Philadelphia, July 2002, pp. 29-36.
                         Proceedings of the Workshop on Natural Language Processing in
through a training process that learns a Bayesian 
joint probability function from a set of training 
cases.  After a BN is trained, a node can be 
assigned a value by setting the probability of 
that value to 1, and the probabilities of the 
alternate values to 0.  This results in a cascading 
update of the value probabilities in all 
unassigned nodes, in effect predicting what the 
values of the unassigned nodes should be, given 
the initial assignments.  The sum of the 
probabilities for the values of a given node is 
constrained to equal 1, making the values 
mutually exclusive, and reflecting uncertainty if 
more than one value has a nonzero probability.  
Please note that in this paper, "BN instance" 
refers to the state of a BN after assignments 
have been made.   
A training case for a BN is a list of node 
/ value assignments.  For instance, consider a 
simple BN for chest anatomy phrases, as shown 
in Figure 1. 
Figure 1.  BN for simple chest anatomy phrases. 
A training case for this BN applied to 
the phrase "right upper lobe" could be: 
side=right 
verticality=upper 
location=lobe 
interpretation= *right-upper-lobe 
 
In the context of the Bayesian learning, 
this case has an effect similar to a production 
rule which states "If  you find the words 'right', 
'upper' and 'lobe' together in a phrase, infer the 
meaning *right-upper-lobe".  After training on 
this case, assigning one or more values from this 
case would increase the probabilities of the 
other values; for instance assigning side= 
"right" would increase the probability of the 
value interpretation= *right-upper-lobe. 
Interpretive concepts such as *right-
upper-lobe are atomic symbols which are either 
invented by the human trainer, or else obtained 
from a medical knowledge database such as the 
UMLS metathesaurus.  By convention, concept 
names in M+ are preceded with an asterisk. 
A medical domain is represented in M+ 
as a network of BNs, with word-level and lower 
concept-level BNs providing input to higher 
concept-level BNs.  Figure 2 shows a partial 
view of the network of BNs used to model the 
M+ Head CT (Computerized Tomography) 
domain, instantiated with the phrase "temporal 
subdural hemorrhage".   Each BN instance is 
shown with a list of nodes and most probable 
values. Note that input nodes of higher BNs in 
this model have the same name as, and take 
input from, the summary nodes of lower BNs.  
Word level BNs have input nodes named 
"head", "mod1" and "mod2", corresponding to 
the syntactic head and modifiers of a phrase.  
Each node in a BN has a distinguished "null" 
value, whose meaning is that no information 
relevant to that node, explicit or inferable, is 
present in the represented phrase. 
Figure 2.  Network of M+ BNs, applied to 
"temporal subdural hemorrhage".  
One way in which M+ differs from its 
predecessor SymText (Koehler, 1998) is in the 
size and modularity of its semantic BNs.  The 
SymText BNs group observation and disease 
concepts together with state ("present", 
"absent"), change-of-state ("old", "chronic"), 
anatomic location and other concept types.  M+ 
trades the inferential advantages of such 
monolithic BNs for the modularity and 
composability of smaller BNs such as those 
shown in figure 2.  Figure 3 shows a single 
instance of the SymText Chest Radiology 
Findings BN, instantiated with the sentence 
"There is dense infiltrative opacity in the right 
upper lobe". 
*observations :  *localized upper lobe infiltrate (0.888) 
     *state :  *present (0.989) 
         state term :  null (0.966) 
     *topic concept :  *poorly-marginated opacity (0.877) 
         topic term :  opacity  (1.0) 
         topic modifier :  infiltrative (1.0) 
      *measurement concept :  *null (0.999) 
         measurement term :  null (0.990) 
         first value :  null (0.998) 
         second value :  null (0.999) 
         values link :  null (0.999) 
         size descriptor :  null (0.999) 
     *tissue concept :  *lung parenchyma (0.906) 
         tissue term : alveolar (1.0) 
     *severity concept :  *high severity (0.893) 
         severity term :  dense (1.0) 
     *anatomic concept :  *right upper lobe (0.999) 
         *anatomic link concept :  *involving (1.0) 
             anatomic link term :  in (1.0) 
         anatomic location term :  lobe (1.0) 
         anatomic location modifier :  null (0.999) 
         anatomic modifier side :  right (1.0) 
         anatomic modifier superior/inferior : upper (1.0) 
         anatomic modifier lateral/medial : null (0.999) 
         anatomic modifier anterior/posterior : null (0.999) 
         anatomic modifier central/peripheral : null (0.955) 
     *change concept :  *null (0.569) 
         change with time :  null (0.567) 
         change degree :  null (0.904) 
         change quality :  null (0.923) 
Figure 3.  SymText BN instantiation. 
2.2 Parse-Driven BN Instantiation 
M+ BNs are instantiated as part of the 
syntactic parse process.  M+ syntactic and 
semantic analyses are interleaved, in contrast 
with NLP systems that perform semantic 
analysis after the parse has finished. 
M+ uses a bottom-up chart parser, with 
a context free grammar (CFG).  As a word such 
as "right" is recognized by the parser, a word-
level phrase object is created and a BN instance 
containing the assignment side= "right" is 
attached to that phrase.  As larger grammatical 
patterns are recognized, the BN instances 
attached to subphrases within those patterns are 
unified and attached to the new phrases, as 
described in section 3.  The result of this 
process is a set of completed BN instances, as 
illustrated in figure 2.  Each BN instance is a 
template containing word and concept-level 
value assignments, and the interpretive concepts 
inferred from those assignments.  The templates 
themselves are nested in a symbolic expression, 
as described in section 2.3, to facilitate 
composing multiple BN instances in 
representations of arbitrary complexity. 
Each phrase recognized by the parser is 
assigned a probability, based on a weighted sum 
of the joint probabilities of its associated BN 
instances, and adjusted for various syntactic and 
semantic constraint violations.  Phrases are 
processed in order of probability; thus the parse 
involves a semantically-guided best-first search. 
Syntactic and semantic analysis in M+ 
are mutually constraining.  If a grammatically 
possible phrase is uninterpretable, i.e. if its 
subphrase interpretations cannot be unified, it is 
rejected.  If the interpretation has a low 
probability, the phrase is less likely to appear in 
the final parse tree.  On the other hand, 
interpretations are constructed as phrases are 
recognized. The exception to this rule is when 
an ungrammatical fragment of text is 
encountered.  M+ then uses a semantically-
guided phrase repair procedure not described in 
this paper. 
2.3 The M+ Abstract Semantic Language  
The probabilistic reasoning afforded by BNs is 
superior to classical logic in important ways 
(Pearl, 1988).  However, BNs are limited in 
expressive power relative to first-order logics 
(Koller and Pfeffer, 1997), and commercially 
available implementations lack the flexibility of 
symbolic languages.  Friedman et alhave made 
considerable headway in giving BNs many 
useful characteristics of first order languages, in 
what they call probabilistic relational models, or 
PRMs (e.g. Friedman at al.  1999).   
While we are waiting for industry-
standard PRMs, we have tried to make our 
semantic BNs more useful by combining them 
with a first-order language, called the M+ 
Abstract Semantic Language (ASL), 
implemented within M+.  Specifically, BNs are 
treated as object types within the ASL.  There is 
a "chest anatomy" type, for instance, and a 
"chest radiology findings" type, corresponding 
to BNs of those same names.  The interpretation 
of a phrase is an expression in the ASL, 
containing predicates that state the relation of 
BN instances to one another, and to the phrase 
they describe. For instance, the interpretation of 
"hazy right lower lobe opacity" could be the 
expression  
(and (head-of #phrase1 #find1) 
                       (located-at #find1 #loc1)) 
 
where #phrase1 identifies a syntactic phrase 
object, and #find1 and #loc1 are tokens 
representing instances of the findings BN 
(instanced with the words "hazy" and "opacity") 
and the anatomic BN (instanced with "right", 
"lower" and "lobe"), respectively.  The relation 
'head-of' denotes that the findings BN is the 
main or "head" BN for that phrase.  Conversely, 
"hazy right lower lobe opacity" can be thought 
of as a findings-type phrase, with an anatomic-
type modifier. 
This expression captures the abstract or 
"skeletal" structure of the interpretation, while 
the BN instances contain the details and specific 
inferences.   One can think of the meaning of an 
expression like (located-at #find1 #loc1) in 
abstract terms, e.g. "some-finding located-at 
some-location".  Alternatively, the meaning of a 
BN token might be thought of as the most 
probable interpretive concept within that BN 
instance.  In this case, (located-at #find1 #loc1) 
could mean "*localized-infiltrate located-at 
*left-lower-lobe". 
Because the object types in the ASL are 
the abstract concept types represented by the 
BNs, semantic rules formulated in this language 
constitute an "abstract semantic grammar" 
(ASG). The ASG recognizes patterns of 
semantic relations among the BNs, and supports 
analysis and inference based on those patterns.  
It also permits rule-based control over the 
creation, instantiation, and use of the BNs, 
including defining pathways for information 
sharing among BNs using virtual evidence 
(Pearl, 1988).    
One use of the ASG is in post-parse 
processing of interpretations.  After the M+ 
parser has constructed an interpretation, post-
parse ASG productions may augment or alter 
this interpretation.  One rule instructs "If two 
pathological conditions exist in a 'consistent-
with' relation, and the first condition has a state 
modifier (i.e. *present or *absent), and the 
second condition does not, apply the first 
condition's state to the second condition".   
For instance, in the ambiguous sentence 
"There is no opacity consistent with 
pneumonia", if the parser doesn't correctly 
determine the scope of "no", it may produce the 
an interpretation in which *pneumonia lacks a 
state modifier, and is therefore inferred (by 
default) to be present.  This rule correctly 
attaches (state-of *pneumonia *absent) to this 
interpretation. 
One important consequence of the 
modularity of the M+ BNs, and of the ability to 
nest them within the ASL, is that M+ can 
compose BN instances in expressions of 
arbitrary complexity.  For instance, it is 
straightforward to represent the multiple 
anatomic concepts in the phrase "opacity in the 
inferior segment of the left upper lobe, adjacent 
to the heart": 
(and (head-of #phrase1 #find1)  
                          (located-at #find1 #anat1) 
                          (qualified-by #anat1 #anat2)  
                          (adjacent-to #anat1 #anat3)) 
 
where the interpretive concepts of #anat1, 
#anat2 and #anat3 are *left-upper-lobe, 
*inferior-segment, and *heart, respectively. 
The set of binary predicates that 
constitutes a phrase interpretation in M+ forms a 
directed acyclic graph; thus we can refer to the 
interpretation as an interpretation graph.  The 
interpretation graph of a new phrase is formed 
by unifying the graphs of its subphrases, as 
described in section 3.  
2.4 Advantages of Bayesian Networks 
As mentioned, a BN training case bears a 
similarity to a production rule.  It would be 
straightforward to implement the training cases 
as a set of rules, and apply them to text analysis 
using a deductive reasoning engine.  However, 
Bayesian reasoning has important advantages 
over first order logic, including: 
1- BNs are able to respond gracefully to 
input "noise".  A semantic BN may produce 
reasonable inferences from phrasal patterns that 
only partially match any given training case, or 
that overlap different cases, or that contain 
words in an unexpected order.  For instance, 
having trained on multi-word phrases containing 
"opacity", the single word "opacity" could raise 
the probabilities of several interpretations such 
as *localized-infiltrate and *parenchymal-
abnormality, both of which are reasonable 
hypotheses for the underlying cause of opacity 
on a chest x-ray film. 
2- Bayesian inference works bi-
directionally; i.e. it is abductive as well as 
deductive.  If instead of assigning word-level 
nodes, one assigns the value of the summary 
node, the probability of word values having a 
high correlation with that summary will 
increase.  For instance, assigning the value 
*localized-infiltrate will raise the probability 
that the topic word is "opacity".   
Bi-directional inference provides a 
means for modeling the effects of lexical 
context.  A value assignment made to one word 
node can alter value probabilities at unassigned 
word nodes, in a path of inference that passes 
through the connecting concept nodes.  For 
instance, if a BN were trained on "right upper 
lobe" and "left upper lobe", but had never seen 
the term "bilateral", applying the BN to the 
phrase "bilateral upper lobes" would increase 
the  probabilities of both "left" and "right", 
suggesting that "bilateral" is semantically 
similar to "left" and "right".  This is one 
approach to guessing the node assignments of 
unknown words, a step in the direction of 
automated learning of new training cases.    
Similarly, if the system encounters a 
phrase with a misspelling such as "rght upper 
lobe", by noting the orthographic similarity of 
"rght" to "right" and the fact that "right" is 
highly predicted from surrounding words, it can 
determine that "rght" is a misspelling of "right".  
The spell checker currently used by M+ 
employs this technique. 
3 Generating  Interpretation 
Graphs 
As mentioned, in M+ the interpretation graph of 
a phrase is created by unifying the graphs of its 
child phrases.  High joint probabilities in the 
resulting BN instances are one source of 
evidence that the words thus brought together 
exist in the expected semantic pattern.  
However, corroborating evidence must be 
sought in the syntax of the text.  Words which 
appear together in a training phrase may not be 
in that same relation in a given text.  For 
instance, "no" and "pneumonia" support 
different conclusions in "no evidence of 
pneumonia" and "patient has pneumonia with no 
apparent complicating factors".  M+ therefore 
only attempts to unify sub-interpretations that 
appear, on syntactic grounds, to be talking about 
the same things.  This is less constraining than 
production rules that look for words in a 
specific order, but more constraining than 
simply pulling key words out of a string of text.  
The following are examples of rules 
used to guide the unification of ASL 
interpretation graphs.  For convenience, several 
shorthand functional notations are used:  If P 
represents a phrase on the parse chart, root-
bn(P) represents the root or head BN instance in 
P's interpretation graph, and type-of(root-bn(P)) 
is the BN type of root-bn(P).  If A and B are 
sibling child phrases of parent phrase C, then C 
= parent-phrase(A,B).  Note that for 
convenience, BN instances in the interpretation 
graphs in Figures 4 - 6 are represented 
alternately as the words slotted in those 
instances, and as the most probable interpretive 
concepts inferred by those instances. 
3.1 Same-type Unification 
If phrase A syntactically modifies phrase B, 
then M+ assumes that some semantic relation 
exists between A and B.  The nature of that 
relation is partly determinable from type-
of(root-bn(A)) and type-of(root-bn(B)).  If type-
of(root-bn(A)) = type-of(root-bn(B)), that 
relation is simply one where root-bn(A) and 
root-bn(B) are partial descriptions of a single 
concept.  If root-bn(A) and root-bn(B) are 
unifiable, M+ composes their input to form 
root-bn(parent-phrase(A,B)).   
If in addition there are two unifiable 
same-type BN instances X and Y linked to root-
bn(A) and root-bn(B) respectively, via arcs of 
the same name, then X and Y also describe a 
single concept, and the arcs describe a single 
relationship.  For instance, if X and Y describe 
the anatomic locations of  root-bn(A) and root-
bn(B), and if root-bn(A) and root-bn(B) are 
partial descriptions of a single "finding", then X 
and Y are partial descriptions of a single 
anatomic location, and ought to be unified. 
Figure 4:  Same-type unification 
In figure 4, in the Chest X-ray domain, 
the phrase "bilateral hazy lower lobe opacity" is 
interpreted by unifying the interpretations of its 
subphrases "bilateral hazy" and "lower lobe 
opacity".  Note that without any corresponding 
syntactic transformation, this rule brings about a 
"virtual transformation", whereby words are 
grouped together within BN instances in a 
manner that reflects the conceptual structure of 
the text.  In this example "bilateral hazy lower 
lobe opacity" is treated as ("bilateral lower 
lobe") ("hazy opacity"). 
Figure 6: Grammar rule - based unification. 
3.2 Different-type Unification 
If phrase A syntactically modifies phrase B, and 
type-of(root-bn(A)) <> type-of(root-bn(B)), 
then root-bn(A) and root-bn(B) represent 
different concepts within some semantic 
relation.  M+ uses the ASG to identify that 
relation and to add it to the interpretation graph 
in the form of a path of named arcs connecting 
root-bn(A) and root-bn(B).  This path may 
include implicit connecting BN instances.  
M+ Implementation 4 
5 
M+ is written in Common Lisp, with some C 
routines for BN access.  The M+ architecture 
consists of six basic components:  The parser, 
concept space, rule base, lexicon, ASL inference 
engine, and Bayesian network component.   
For instance, to interpret "subdural 
hemorrhage" in the Head CT domain, M+ 
attempts to unify the graphs for the subphrases 
"subdural" and "hemorrhage", where type-
of(root-bn("subdural")) = location, and type-
of(root-bn("hemorrhage")) = topic.  M+ 
identifies the connecting path for these two 
types as shown in figure 2, and adds that path to 
the interpretation as shown in figure 5.  Note 
that this path contains instances of the 
"observation" and "anatomy" BN types.  
As mentioned, the parser is an 
implementation of a bottom up chart parser with 
context free grammar.   
The concept space is a table of symbols 
representing types, objects and relations within 
the ASL.  These include BN names, BN node 
value names, inter-BN relation names, and a 
small ontology of useful concepts such as those 
related to time. 
Figure 5.  Different-type unification. 
The rule base contains rules, which 
comprise the syntactic grammar and ASG.  
The lexicon is a table of Lisp-readable 
word information entries, obtained in part from 
the UMLS Specialist Lexicon. 
The ASL inference engine combines 
symbolic unification with backward-chaining 
inference. It can be used to match an ASG 
pattern against an interpretation graph, and to 
perform tests associated with grammar rules.  
3.3 Grammar Rule Based Unification The Bayesian network component utilizes 
the Norsys Netica(TM) API, and includes a set of 
Lisp and C language routines for instantiating 
and retrieving probabilities from BNs. 
Individual grammar rules in M+ can recognize 
semantic relations, and add connecting arcs to 
the interpretation graph.  For instance, M+ has a 
rule which recognizes findings-type phrases 
connected with strings of the "suggesting" 
variety, and connects their graphs with a 
'consistent-with' arc.  This is used to interpret 
"opacity suggesting possible infarct" in the 
Head CT domain, as shown in figure 6. 
Training M+ 
Porting M+ to a new medical domain involves 
gathering a corpus of training sentences for the 
domain, using the Netica(TM) graphical interface 
to create domain-specific BNs, and generating 
training cases for the new BNs.   
The most time-consuming task is the 
creation of training cases.  We have developed a 
prototype version of a Web-based tool which 
largely automates this task.  The basic idea is to 
enable M+ to guess the BN value assignments 
of unknown words, then use it to parse phrases 
similar to phrases already seen.  For instance, 
having been trained on the phrase "right upper 
lobe", the parser is able to produce reasonable 
parses, with some "guessed" value assignments, 
for "left upper lobe", "right middle lobe", 
"bilateral lungs", etc.  The BN assignments 
produced by the parse are output as tentative 
new cases to be reviewed and corrected by the 
human trainer. 
The training process begins with an 
initial set of interpreted "seed" phrases.  From 
this set, the tool can apply the parser to phrases 
similar to this set, and so semi-automatically 
traverse ever widening semantically contiguous 
areas within the space of corpus phrases.  As the 
training proceeds, the role of the human trainer 
increasingly becomes one of providing 
correction and interpretations for semantic 
patterns the system is increasingly able to 
discover on its own. 
To parse phrases containing unknown 
words, M+ uses a technique based on a variation 
of the vector space model of lexical semantic 
similarity (Manning and Schutze, 1999).  As 
M+ encounters an unknown word, it gathers a 
list of training corpus words judged similar to 
that word, as predicted by the vector space 
measure.  It then identifies BN nodes whose 
known values significantly overlap with this list, 
and provisionally assigns the unknown word as 
a new value for those nodes.  The assignment 
resulting in the best parsetree is selected for the 
new provisional training case. 
6 Evaluation 
M+ was evaluated for the extraction of 
American College of Radiology (ACR) 
utilization review codes from Head CT reports 
(Fiszman, 2002). The ACR codes compare the 
outcome in a report with the suspected diagnosis 
provided by emergency department physicians. 
If the outcome relates to the suspected diagnosis 
then the report should be encoded as positive 
(P). If the outcome is negative and does not 
relate to the suspected diagnosis then the report 
should be encoded as negative  (N). In order to 
extract those ACR codes we trained M+ to 
extract eleven broad disease concepts, then 
inferred the ACR codes based on the application 
of a rule to the M+ output:  If any of the 
concepts was present, the report was considered 
positive, else the report was considered 
negative. 
Twenty six hundred head CT scan 
reports were used for this evaluation.  Six 
hundred reports were randomly selected for 
testing, and the rest were used to train M+ in 
this domain.  The performance of M+ on this 
task was measured against that of four board 
certified physicians, using a gold standard based 
on majority vote, as described in (Fiszman, 
2002).   For each subject we calculated recall, 
precision and specificity with their respective 95 
% confidence intervals for the capture of ACR 
utilization codes.  
From 600 head CT reports, 67 were 
judged to be positive (P) by the gold standard 
physicians and 534 were judged to be negative 
(N). Therefore the positive rate for head CT in 
this sample was 11%.  Recall, precision and 
specificity for every subject are presented with 
their respective 95% confidence intervals in 
Table 1. The physicians had an average recall of 
88% (CI, 84% to 92.%), an average precision of 
86% (CI, 81% to 90%), and average specificity 
of 98% (CI, 97% to 99%). M+  had recall of 
87% (CI, 78% to 95%), precision of 85% (CI, 
77% to 94%) and specificity of 98% (CI, 97% 
to 99). 
Table 1.  Results of  ACR utilization code study. 
Subject Recall Specificity Precision 
Physician1 0.83 
(0.74-0.92) 
0.99 
(0.98-1.00) 
0.91 
(0.84-0.99) 
Physician2 0.88 
(0.81-0.97) 
0.98 
(0.97-0.99) 
0.84 
(0.75-0.93) 
Physician3 0.93 
(0.87-1.00) 
0.98 
(0.97-0.99) 
0.86 
(0.78-0.95) 
Physician4 0.88 
(0.96-0.99) 
0.97 
(0.96-0.99) 
0.81 
(0.71-0.90) 
M+ 0.87 
(0.78-0.95) 
0.98 
(0.97-0.99) 
0.85 
(0.77-0.94) 
 
The results on Head CT reports are 
encouraging, but there are limitations. We only 
evaluated 600 reports, because it's very hard to 
get physicians to produce gold standard data for 
medical reports. The prevalence of positive 
reports is only 11% and reflects the fact that the 
individual brain conditions  have very low 
prevalence. 
7 
8 
Conclusions 
M+ and its predecessors have demonstrated that 
BNs provide a useful semantic model for 
medical text processing.  In practice, a medical 
NLP system will frequently encounter missing 
and unknown words,  unknown and 
ungrammatical phrase structures, and 
telegraphic usages.  Knowledge databases will 
be imperfect and incomplete.  Using BNs for 
semantic representation brings a noise-tolerant, 
partial match-tolerant, context-sensitive 
character to the recognition of semantic 
patterns, and to relevant inferences based on 
those patterns.  In addition, BNs can be used to 
guess the semantic types of unknown words, 
providing a basis for bootstrapping the system's 
semantic knowledge. 
Acknowledgements 
Many thanks to Wendy W. Chapman for her 
advice and input in this paper, and her efforts to 
make M+ a useful addition to the RODS project 
at the University of Pittsburgh. 
References 
 
Chapman W., Christensen L. M., Wagner M., Haug 
P. J., Ivanov O., Dowling J. N., Olszewski R. T. 
2002.  Syndromic Detection from Free-text 
Triage Diagnoses: Evaluation of a Medical 
Language Processing System before Deployment 
in the Winter Olympics. Proc AMIA Symp. 
(submitted). 
Chomsky, Noam. 1965. Aspects of the theory of 
syntax.  Special technical report (Massachusetts 
Institute of Technology, Research Laboratory of 
Electronics); no. 11. Cambridge, MA: MIT Press. 
Fiszman M., Blatter D.D., Christensen L.M., Oderich 
G., Macedo T., Eidelwein A.P., Haug P.J.  2002.  
Utilization review of head CT scans: value of a 
medical language processing system. American 
Journal of Roentgenology (AJR). (submitted) 
Friedman C, Alderson PO, Austin JH, Cimino JJ, 
Johnson SB.  1994,  A general natural-language 
text processor for clinical radiology.  J Am Med 
Inform Assoc. Mar-Apr;1(2) pp. 161-74. 
Friedman N., Getoor L., Koller D. and Pfeffer A. 
1999.  Learning Probabilistic Relational Models.  
Proceedings of the 16th International Joint 
Conference on Artificial Intelligence (IJCAI):  
pp. 1300-1307. 
Haug P. J., Christensen L., Gundersen M., Clemons 
B., Koehler S., Bauer K. 1997. A natural 
language parsing system for encoding admitting 
diagnoses. Proc AMIA Symp. 81: pp. 4-8. 
Koehler, S. B. 1998.  SymText: A natural language 
understanding system for encoding free text 
medical data. Ph.D. Dissertation, University of 
Utah. 
Koller D., and Pfeffer A.  1997. Object-Oriented 
Bayesian Networks.  Proceedings of the 13th 
Annual Conference on Uncertainty in AI:  pp. 
302-313.  
Lin R, Lenert L, Middleton B, Shiffman S. A free-
text processing system to capture physical 
findings: Canonical Phrase Identification System 
(CAPIS). Proc Annu Symp Comput Appl Med 
Care. pp. 843-7. 
Manning C. D. and Schutze H. 1999.  Foundations of 
Statistical Natural Language Processing.  MIT 
Press.  
Minsky, M. 1975.  A framework for representing 
knowledge.  In The Psychology of Human Vision, 
ed. P. H. Winston, pp. 211-277.  McGraw Hill. 
Moore, R. C. 1989. Unification-based Semantic 
Interpretation.  Proceedings of the 27th Annual 
Meeting of the Association for Computational 
Linguistics, pp33-41. 
Pearl, Judea.  1988.  Probabilistic inference in 
intelligent systems.  Networks of plausible 
inference:  Morgan Kaufmann. 
Ranum D.L. 1989. Knowledge-based understanding 
of radiology text.  Comput Methods Programs 
Biomed.  Oct-Nov;30(2-3) pp.209-215. 
Romacker, Martin and Hahn, Udo. 2000.  An 
empirical assessment of semantic interpretation. 
ANLP/NAACL 2000 -- Proceedings of the 6th 
Applied Natural Language Processing 
Conference & the 1st Conference of the North 
American Chapter of the Association for 
Computational Linguistics. pp. 327-334.  
 Schank, R.C. and R. Abelson. 1997.  Scripts, Plans, 
Goals, and Understanding.  Hillsdale, NJ: 
Lawrence Erlbaum. 
Smart, J. F. and M. Roux. 1995. A  model for 
medical knowledge representation application to 
the analysis of descriptive pathology reports.  
Methods Inf Med.  Sep;34(4) pp. 352-60. 
Proceedings of the Workshop on BioNLP, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
ONYX: A System for the Semantic Analysis of Clinical Text  Lee M. Christensen, Henk Harkema, Peter J. Haug,  Jeannie Y. Irwin, Wendy W. Chapman Department of Biomedical Informatics University of Pittsburgh University of Utah Pittsburgh, PA 15214, USA Salt Lake City, Utah, 84143, USA lmc61 heh23 rey3 wec6 @pitt.edu Peter.Haug@intermountainmail.org     Abstract 
This paper introduces ONYX, a sentence-level text analyzer that implements a number of innovative ideas in syntactic and semantic analysis. ONYX is being developed as part of a project that seeks to translate spoken dental examinations directly into chartable findings. ONYX integrates syntax and semantics to a high degree. It interprets sentences using a combination of probabilistic classifiers, graphical unification, and semantically anno-tated grammar rules. In this preliminary evaluation, ONYX shows inter-annotator agreement scores with humans of 86% for as-signing semantic types to relevant words, 80% for inferring relevant concepts from words, and 76% for identifying relations between concepts. 
1 Introduction This paper describes ONYX, a sentence-level medical language analyzer currently under devel-opment at the University of Pittsburgh. Since ONYX contains a number of innovative ideas at an early stage of development, the objective of this paper is to paint a broad picture of ONYX and to present preliminary evaluation results rather than analyzing any single aspect in detail.  ONYX is being developed as part of a project aimed at extracting information from spoken dental examinations. Currently, dental findings must be charted after an exam is completed or may be charted by an assistant who acts as a transcription-ist during the exam. Our goal is to design a system capable of automatically extracting chartable find-
ings directly from spoken exams, potentially also supporting automated decision support and quality control. We are also developing tools to enable the system to be ported to other clinical domains and settings.   Extracting information from unedited speech tran-scriptions presents a number of challenges. Sen-tences may be fragmented or telegraphic, and much of the speech may be irrelevant for our pur-poses. The following example illustrates some of these difficulties:  "Okay. Okay. Open. Okay. No. 1 is missing. Two oc-clusal distal amalgam. Actually, make that occlusal. Also, one palatal amalgam. Can you close just slightly? And perfect. Okay, now open again."  The relevant findings in this example are that tooth number one is missing and tooth number two has amalgam fillings on the occlusal and palatal sur-faces. Our ultimate challenge is to create a system that can recognize relevant sentences and perform competently in the face of the inherent ambiguity and noise commonly found in conversational speech. ONYX does not yet address all of these challenges, although we have clear directions we are pursuing as described in the Future Work sec-tion of this paper. Our goal in this paper is to de-scribe the current state of ONYX and the innovations we feel will enable it to be adapted to complex NLP tasks in the future. 2 Overview of ONYX  ONYX is the middle component of a pipelined architecture as illustrated in figure 1. The entry point to this architecture is a speech-to-text ana-lyzer, which takes input from a microphone worn 
19
by the dentist and produces a transcription that ONYX analyzes for semantic content. ONYX's output is then passed to a discourse analyzer that applies dental knowledge to assemble ONYX's sentence-level semantic representations into chartable exam findings.  
 Figure 1. Speech-to-chart pipeline.  ONYX looks for dental conditions such as caries, fractures and translucencies; restorations such as fillings and crowns; tooth locations; and modifiers such as tooth part, tooth surface, and condition ex-tent. It produces templates of words and concepts. Table 1 shows a summary of four templates (Den-tal Condition, Tooth Location, Surface and State) representing the meaning of "eight mesio might have a slight translucency."   
 ONYX?s interpretations are represented as binary predicates that take the templates as arguments (for convenience, only the summary concepts from the templates are shown): 
ConditionAt(*translucency, *numberEight) & LocationHasSurface(*numberEight, *mesial) &  StateOf(*translucency, *possible)   ONYX builds on ideas from MPLUS (Christensen et al 2002), which was used primarily to interpret radiology reports. MPLUS uses Bayesian networks (BNs) to produce filled templates. Through a train-ing process, words from the corpus of training documents are manually associated with states of terminal nodes in a BN, and concepts are associ-ated with states of nonterminal nodes. When MPLUS interprets a sentence, it instantiates the BNs with words from the sentence and infers the most probable concepts consistent with those words. It then generates templates filled with those words and concepts.   BNs have proven useful in semantic analysis (e.g. Ranum 1989, Koehler 1998, Christensen 2002): their performance degrades gracefully in the face of various types of lexical and syntactic noise. The main disadvantage with using BNs is their inherent computational complexity. ONYX employs a se-mantics-intensive form of parsing, interpreting each phrase as it is constructed rather than waiting until the syntactic analysis is completed to do the interpretation. For this reason we have developed an experimental probabilistic classifier for ONYX called a Concept Model (CM). CMs support a tree-structured representation of related words and con-cepts (figure 5), structurally similar to the BNs used by MPLUS, but using a more efficient model of computation. In essence CMs are trees of Na?ve Bayes classifiers, although they contain enhance-ments, not described in this study, which in general make them more accurate than strict Na?ve Bayes. Each node together with its children constitutes a single classifier. When a CM is applied to words in a sentence, word-level CM states are assigned a probability based on training data. Probabilities are propagated upwards through the CM, calculating probabilities for all concepts that depend directly or indirectly on the words of the sentence.  3 ONYX Syntactic Analyzer For this project we desired a parser that was fast, flexible and robust. We designed a variation on a bottom-up chart parser (Kay, 1980) and hand-crafted an initial set of 52 context-free grammar 
Dental Condition   Condition Concept *translucency   Condition Term "translucency"   Severity Concept *superficial   Severity Term "slight" Tooth Location   Location Concept *numberEight   Tooth Number "eight" Surface   Surface Concept *mesial   Front/Back Term "mesio" State   State Concept *possible   State Term "might" Table 1: ONYX templates for "eight mesio might have a slight translucency." Terms with an * are in-ferred concepts. 
20
rules. Chart parsers based on Kay?s algorithm maintain an agenda of ?edges,? which correspond to partially or completely instantiated grammar rules. In the original algorithm, for each new phrase added to the chart an edge is created for each rule that can begin with that phrase. In addi-tion, each existing edge that abuts and can be ex-tended with that phrase is duplicated with a pointer to the new phrase. When an edge has no more un-matched components, it is regarded as a new phrase that can begin or extend other edges. Since edges are used to anticipate all possible continua-tions of phrases vis-?-vis the grammar, the number of edges grows quickly relative to the number of words in the sentence. Charniak et al (1998) noted that exhaustively parsing maximum-40-word sen-tences from the Penn II treebank requires an aver-age of 1.2 million edges per sentence.   ONYX?s parse algorithm replaces edges with bi-nary links. We briefly describe this new algorithm. A set of binary link templates is defined for each grammar rule. For instance, the rule S->NP AUX VP (labeled S1) would produce the templates [s1:np,aux] and [s1:aux,vp]. When a phrase is added to the chart, binary links for all applicable rules are added from that phrase to juxtaposed phrases to the left and right on the chart. When a right or left-terminating link is added (all links for rules with two or three components are right or left terminating), a quick search is done in the other direction for links belonging to the same rule. Each complete set of links defines a new phrase of the target type, as shown in figure 2.  
 Figure 2. Binary links for the rule S1:S->NP AUX VP used to generate new phrases of type S1 from juxta-posed NP, AUX, and VP phrases on the chart.  Although we have not analyzed the time and space complexity of this algorithm, it has proven to be 
more efficient than the edge-based parser used by MPLUS. Time and space complexity for chart parsers is calculated based on the number of edges produced, which has been shown to be O(n3), with n words in a sentence. Since binary links, unlike edges, are only used to record grammatical rela-tions between juxtaposed phrases on the chart (rather than anticipating possible continuations), are not duplicated, and can participate in the crea-tion of multiple new phrases, the number of binary links grows more slowly than the number of edges.  On the other hand, the need to search for com-pleted link sets increases processing time. We plan to formally analyze the time and space require-ments of this algorithm in a future study. 4  ONYX Semantic Analyzer In ONYX syntax and semantics are highly inte-grated. Rather than waiting for a completed parse tree to begin the interpretation process, ONYX semantically interprets each phrase as it is created and before it is placed on the chart. Each phrase is assigned a ?goodness? score based in part on the goodness of its semantic interpretation, and this score is used in determining the order in which phrases are expanded, resulting in a semantically guided best-first search.  To represent semantic relations between templates, ONYX uses a custom-built first-order predicate language with a syntax based roughly on the Knowledge Interchange Format (Genesareth & Fikes, 1992). ONYX interpretations are conjuncts of binary predicates formulated in this language, with templates as arguments. This language is for internal use only; ONYX will use standard lan-guage protocols for communicating with external systems. We decided to implement our own lan-guage rather than using an existing implementation in order to have access to the underlying data structures, which we use in three ways not tradi-tionally applied to symbolic languages: 1- We have extended our language to include Java objects as constants and Java methods as functions and rela-tions. In particular, CM templates are treated as constants in the language, and CMs are semanti-cally typed functions that map words to templates. 2- As described next, ONYX's default mode of semantic interpretation is based on a form of graph unification. Binary predicates are treated as unifi-
21
able links in a graph as shown in figure 3. 3- ONYX uses the predicate structure of an interpre-tation to pass information between CMs. For in-stance, if an interpretation contains the relation ConditionAt(Condition, Location), ONYX inserts the summary concept from the Location CM into the Condition CM. This allows the Condition CM to factor tooth location into its determination of the most probable Condition concept.  Figure 3 illustrates ONYX?s unification-based in-terpretation process. ONYX relies on a semantic network that defines types and relations in the den-tal domain (figure 4). As dental concepts are brought together in a phrase, links connecting those concepts are extracted from the semantic network and formulated into binary predicates in an interpretation. As phrases are joined together in larger phrases, their relations and templates are merged, resulting in an interpretation tree denoting a dental object (e.g. dental condition, tooth loca-tion, tooth surface) with possibly multiple levels of modifiers. For instance, the interpretation for "eight mesio might have a slight translucency" can be generated from the partial interpretations of the phrases "eight mesio", "might" and "slight translu-cency" as shown in figure 3.  
 Figure 3. Interpreting ?eight mesio might have a slight trans-lucency? using graph unification.   There are two primary justifications for using uni-fication in this way. First, conjoined phrases, par-ticularly noun phrases, often contain unifiable partial descriptions of a single object. Second, if concepts appear together in a phrase, there is a good chance that relations connecting those con-cepts in the semantic network are captured, explic-itly or implicitly, in the meaning of the phrase.  
The dental semantic network is shown in figure 4. Terminal (white) nodes define concrete semantic types associated with dental CMs. For instance, the DentalCondition type is associated with the con-cept model shown in figure 5. 
 Figure 4. Semantic network for dental exams. Nonterminal (gray) nodes represent abstract types with no associated CMs. A concrete type may have more than one abstract parent type. For instance, a Restoration, such as a crown, is both a Condition and a Location. As such, it can exist at a tooth lo-cation, e.g., "the crown on tooth 5," and it can be the location of condition, e.g., "the crack on the crown on tooth 5." Since a concrete type can have multiple parent types, ONYX often produces mul-tiple alternative interpretations over words of a sentence. For instance, ONYX may produce two interpretations for "mesial amalgam"?one refer-ring to the mesial surface of an amalgam filling, and one referring to an amalgam filling on the me-sial surface of some unspecified tooth. ONYX uses probabilities derived from training cases to prefer the latter interpretation, which is the more likely of the two.  
 Figure 5. Dental Condition Concept Model. Each concept model has a tree structure as illus-trated in figure 5, which shows the structure of the Dental Condition CM. Nonterminal nodes repre-
22
sent concepts, and terminal nodes represent words, with the exception of stub nodes. The value of a stub node is the summary concept (i.e., root node) from the CM of the same name.   One problem with ONYX?s graph-based model of interpretation is that the semantic network does not capture all relations that might be expressed in a dental exam. The network was deliberately kept simple by including mostly relations that are cate-gorically true (e.g., all teeth have surfaces) or that are frequently talked about (e.g., restorations are frequently mentioned as being locations of other conditions). This restriction helps keep the unifica-tion process tractable and minimizes ambiguity, but interpretations may miss important points. For instance, the ONYX interpretation of "15 occlusal amalgam" is ConditionAt(*filling, *toothFifteen) & Loca-tionHasSurface(*toothFifteen, *occlusal) which can be paraphrased as "a filling at tooth 15 and tooth 15 has an occlusal surface". This interpretation misses the important fact that the filling is on the occlusal surface of tooth 15, which we would normally in-fer from the fact that ?occlusal? adjectivally modi-fies ?amalgam.? Another limitation is that although the semantic network as it stands can describe sin-gle objects with their modifiers, it cannot be used to build up complex descriptions involving multi-ple objects of the same type.  To address these limitations we have added a sec-ond, more specialized mode of interpretation that is contingent on lexical and syntactic information from the parse and that can introduce into an inter-pretation predicates that do not exist in the seman-tic network. This mode of interpretation uses semantic types and patterns attached to grammar rules. As an example, the rule NP -> AP NP can be semantically annotated thus:     NP<Restoration> -> AP<Surface> NP<Restoration>  => OnSurface(Restoration, Surface)  This rule captures the idea that if a Surface-type adjectival phrase modifies a Restoration-type noun phrase, the restoration exists on that surface. Ap-plied to ?occlusal amalgam? this rule would pro-duce an interpretation OnSurface(*filling, *occlusal), which is the relation missing from the previous example. Semantically annotated grammar rules 
can also connect objects of the same semantic type. For instance, we might define a rule      NP<Condition> -> NP<Condition1> "caused by"     NP<Condition2>      => CausedBy(condition1, condition2)  This rule can match phrases such as "leakage caused by a crack along the lingual surface", and link the two conditions (leakage and crack) with a CausedBy relation. This mechanism enables ONYX to construct complex descriptions with multiple objects.  We have added a mechanism to the ONYX train-ing tool that allows semantically annotated gram-mar rules to be generated semi-automatically during training. A human annotator with sufficient linguistic background can view the parse trees generated by ONYX for corpus sentences, repair those parse trees and/or add new semantic relations if necessary, then apply a function that creates cop-ies of the rules embodied in those trees with se-mantic types and predicates attached. 5  Integrating Syntax and Semantics Although most NLP systems apply semantic analy-sis to completed parse trees, in humans the two processes are more integrated. Syntactic expecta-tions are greatly influenced by word meanings, as illustrated by ?garden path? sentences such as ?The man whistling tunes pianos.? In ONYX, syntax and semantics are highly interleaved. This is ac-complished in several ways:  1- ONYX?s parse algorithm permits words to be processed in any order, rather than strictly left-to-right, since binary grammar links can be added to the phrase chart in any order. This allows ONYX to be instructed to focus on semantically interest-ing words first, which can be used, among other things, to gather useful information from ungram-matical speech or run-on sentences where attempt-ing to look for complete sentences in strict left-to-right fashion would be unsuccessful.  2- ONYX implements a variation on a probabilistic context free grammar (PCFG) (Charniak, 1997) that associates grammar rules with semantic types. Based on training, a conditional probability is cal-culated for each <rule, type> pair given specific 
23
<rule, type> assignments to the rule?s components. The probability of a phrase is then calculated as the product of the probabilities of the phrase rule and its semantic type, given the rule and type of each of its child phrases. ONYX is then able to prefer phrases that best accommodate the semantic types of their constituents. Specifically,  prob(phrase) =  ?(prob(rule(phrase) + semtype(phrase) |         rule(childPhrase) + semtype(childPhrase)))  3- One hard problem in parsing is determining the correct structure of conjunctive noun phrases. ONYX applies semantic guidance to solve this problem. For instance, in a chest radiology report the words "right and left lower lobe opacity" can be grouped in several different ways, and different groupings can produce different interpretations. The correct grouping should be something like: [[[right and left] [lower lobe]] opacity], rather than [[right and [left lower]] [lobe opacity]]. ONYX currently employs a simplistic representation of the meaning of a conjunctive phrase as a list of inter-pretations. The correct interpretations for "right and left lower lobe opacity" would be two predi-cate expressions covering the words (right, lower, lobe, opacity) and (left, lower, lobe, opacity). ONYX generates a measure of the similarity of these expressions based on the cosine similarity of the lists of non-null nodes in their CM templates. This measure is factored into the phrase's goodness score under the heuristic that semantically bal-anced conjunctive phrases are more likely to be correct than imbalanced ones.  4- As mentioned earlier, ONYX can utilize gram-mar rules annotated with semantic types and pat-terns. Semantically annotated rules constrain phrases to match particular semantic types, and can contribute predicates to the interpretation of those phrases. This gives ONYX's grammar the character of a semantic grammar.  5- Phrases are weighted and preferred by ONYX according to their goodness score, which is based on three measures: the probability of the phrase as determined by the PCFG formula, the conjunct cosine similarity score, if applicable, and the goodness score of the phrase's semantic interpreta-tion. The PCFG and conjunct similarity formulas 
are based on semantic criteria, as mentioned ear-lier. Interpretation goodness scores are calculated as a simple product of the probabilities of the se-mantic relation predicates they contain. Relation probabilities are in turn derived from training data, and are conditioned on the concepts they contain. The probability of a relation is calculated as the number of times a pair of concepts appears to-gether in the target relation divided by the number of times they appear together in any set of rela-tions. The goodness score of a phrase is thus highly semantically determined.  goodness(phrase) = F(prob(phrase, PCFG),     conjunctSimilarity(phrase),     goodness(interp(phrase)) goodness(interp(phrase)) =    ?prob(relations(interp(phrase))) prob(relation) =   count(relation + concepts(relation)) /  count(anyConnection(concepts(relation))) 6  Evaluation We performed a preliminary evaluation of ONYX for the extraction of relevant dental concepts and relations on a set of twelve documents in our cur-rent training corpus.   Reference Standard. Each document was inde-pendently annotated by three human annotators (authors LC, JI and HH), who used the ONYX training tool to fill in templates representing dental conditions, tooth locations and other relevant con-cepts, as well as to select the semantic relations linking those templates. The annotators then re-viewed disagreements and by consensus created a reference standard set of templates and relations. Where the annotators did not have sufficient dental knowledge to reach an agreement they consulted dental clinicians.  Outcome Metrics. To evaluate ONYX on the rela-tively small corpus of documents, we applied a leave-one-out approach: for each sentence in the reference standard, ONYX was trained using the templates from the remaining reference standard sentences. ONYX was then applied to the target sentence, and the resulting templates and relations were compared to the reference standard. We measured inter-annotator agreement (IAA) be-tween ONYX and the reference standard using the formula described in Roberts et al(2007): 
24
 IAA = (2 * correct) / (spurious + missing + correct)  We calculated IAA separately for CM words, con-cepts, and semantic relations. A correct match is a word, concept or relation generated by both the reference standard and ONYX; a spurious item is one ONYX generated that did not exist in the ref-erence standard; and a missing item is one that ex-isted in the reference standard but was not generated by ONYX. In addition to IAA we identi-fied the concepts and relations most commonly in error and calculated percentages for those errors.   We compared ONYX?s performance on the target documents with that of a simple baseline parser we created for this purpose. The baseline parser proc-esses the words of a sentence from left to right, creating phrases for sets of juxtaposed words that can be interpreted together using the semantic net-work. No grammar rules are employed, there is no analysis of conjunctive phrases, and goodness scores are not calculated. Our goal was to get a feel for how much these factors contribute to generat-ing correct interpretations. There is no precedence for this particular approach as far as we are aware, so we regard this comparison as informative but not definitive. 7 Results IAA results for ONYX and the baseline parser are shown in table 2. ONYX performs best at inserting words into appropriate nodes in the CMs, with IAA of 86%, and less well for inferring the best concept (80%) and identifying relations among concepts (76%). ONYX consistently out-performs the baseline parser.  Table 2: IAA for assignment of words, concepts, and relations.  IAA ONYX  86% Words (n = 904) Baseline  57% ONYX  80% Concepts (n = 1186) Baseline  53% ONYX  76% Relations (n = 297) Baseline  41%  Although this study does not examine all the rea-sons for the differences in performance between ONYX and the baseline parser, some reasons can 
be illustrated with an example. Conjunctive phrases are common in dental discourse, and a failure to handle conjuncts can result in both con-cept and relation errors. For instance, given the sentence "4, 5, 6, 7 fine" ONYX generates separate interpretations covering the word groupings (4, fine), (5, fine), (6, fine), and (7, fine), which would yield four ConditionAt relations, four Location concepts (*numberFour, *numberFive, *numberSix, *numberSeven) and one Condition concept (*normalTooth) appearing in each relation. The baseline parser in contrast does not discover this distribution of terms and so omits all but the ConditionAt relation over (7, fine). Trying to merge juxtaposed tooth numbers, the baseline parser also infers that at least some of these denote tooth ranges instead of individual teeth (e.g. inter-preting ?4, 5? as ?4 to 5? instead of ?4 and 5?), which causes it to misclassify Location concepts. The ability to generate correct parse trees and to use the structure of those parse trees in the inter-pretation process is important in generating correct interpretations.  Tables 3 and 4 show breakdowns by percentage of the concepts and relations most commonly in error in ONYX?s interpretations (errors accounting for more than 15%).  Table 3: Per-concept error percentages  Dental Condition Summary Concept 18% Tooth Location Summary Concept 17% Dental Condition Intermediate Concept 16% Surface Summary Concept 15% Total  66%  Table 4: Per-relation error percentages. Surface of Part 47% Location of Condition 23% Total 70%  8  Related Work ONYX is a new application inspired by SPRUS (Ranum, 1989), Symtext (Koehler, 1998), and MPLUS (Christensen, 2002), which all used Baye-sian Networks to infer relevant findings from text. Other medical language processing systems im-plement different approaches to encode clinical concepts and their modifiers, along with relations between concepts, including MedLEE (Friedman, 
25
1994), a largely statistical system by Taira and col-leagues (Taira, 2007), and MedSyndikate (Hahn, 2002).   Many of ONYX?s components leverage research in the general and clinical NLP domains, including the use of chart parsing (Kay, 1980) and probabil-istic context free grammars (Charniak, 1997). ONYX's use of semantically annotated grammar rules was inspired in part by MedLEE (Friedman et al 1994), which uses a semantic grammar.   Although incorporating ideas and approaches from others, we feel that ONYX is unique in several ways, including its high level of syntactic/semantic integration and the ways in which it blends sym-bolic and probabilistic representations of domain knowledge. We plan to make ONYX available through open source when the system is more complete.  9 Limitations There are several limitations to this study. Al-though ONYX introduces several innovations, these are not described in detail in this study and are not individually evaluated for their effect on ONYX?s performance. Instead, this study presents a broad overview of ONYX and evaluates ONYX's overall performance against a reference standard on a small test sample. Another limitation of our study is the baseline system?because similar sys-tems generate different output than ONYX and do not model the same domain, finding a competitive baseline application is difficult. In spite of its im-perfection, we believe the baseline we imple-mented to be reasonable. 10 Future Work One limitation of a system like ONYX is the over-head of manually creating complex training cases. To address this shortcoming, the ONYX training tool invokes ONYX to automatically create tem-plates and relations for corpus sentences, and hu-man trainers correct any mistakes. A semi-automated approach greatly speeds up the training process and facilitates agreement among human trainers. We plan to further automate this process using an approach derived from Thelen & Riloff (2002), which uses a classifier with features based 
on extraction patterns derived from Autoslog (Riloff, 1996). We plan to adapt this approach to automatically classify CM word assignments, and also to automatically classify semantic relations between CM templates. We will add this function-ality to the training tool to enable it to find and an-notate relevant sentences automatically where possible. We will also apply this functionality to enable ONYX to recognize relevant sentences in new documents based on their similarity to training sentences, and we will use semantic patterns stored with training sentences to aid in interpreting noisy segments of text that ONYX cannot parse. We plan to compare the performance of grammar-based and feature-based semantic analysis in future studies. With more fully automated training, we also hope to make ONYX more easily portable to new do-mains and clinical settings in the future.   Conclusions  This paper describes ONYX, which is being devel-oped as part of a system for extracting chartable findings from spoken dental examinations. ONYX contains a number of innovative ideas including a novel adaptation of Kay's (1980) parse algorithm; a symbolic language extended to include probabilis-tic and procedural elements; an integration of syn-tax and semantics that includes a semantically weighted probabilistic context free grammar and interpretation based both on a semantic network and a semantic grammar. Considering ONYX?s early stage of development it performed reasonably well in this limited evaluation but must be ex-tended to address challenges in extracting findings from spoken dental exams. Acknowledgments  This work was funded by NIDCR 1 R21DE018158-01A1 ?Feasibility of a Natural Language Processing-based Dental Charting Application. References  E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial In-telligence, pp. 598-603. E. Charniak, S. Goldwater and M. Johnson. 1998. Edge-Based Best-First Chart Parsing. In Proceedings of 
26
the Sixth Workshop on Very Large Corpora, pp. 127-133. Lee M. Christensen, Peter J. Haug, and Marcelo Fisz-man. 2002. MPLUS: A Probabilistic Medical Lan-guage Understanding System. Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain, Philadelphia, pp. 29 ? 36. Carol Friedman, Phil Alderson, John Austin, James Ci-mino, & Stephen Johnson. 1994. A general natural language text processor for clinical radiology. Jour-nal of American Medical Informatics Association 1(2), pp. 161?174.  M. R. Genesereth and R. E. Fikes. Knowledge Inter-change Format, Version 3.0 Reference Manual. Technical Report Logic-92-1, Stanford, CA, USA, 1992.  Hahn U, Romacker M, Schulz S. 2002. Medsyndikate-a natural language system for the extraction of medical information from findings reports. Int J Med Inf. 67(1-3), pp. 63-74.  M. Kay. 1980. Algorithm schemata and data structures in syntactic parsing. In Readings in Natural Lan-guage Processing, pp. 35 ? 70. Morgan Kaufmann Publishers Inc.  Koehler, S. B. 1998. SymText: A natural language un-derstanding system for encoding free text medical data. Ph.D. Dissertation, University of Utah.  Ranum D.L. 1989. Knowledge-based understanding of radiology text. Comput Methods ProBiomed. Oct-Nov;30(2-3) pp. 209-215. Ellen Riloff, 1996. Automatically Generating Extraction Patterns from Untagged Text. Proceedings of the Thirteenth National Conference on Artiticial Intelli-gence, pp. 1044 ? 1049. The AAAI Press/MIT Press. Angus Roberts, Robert Gaizauskas, Mark Hepple, Neil Davis, George Demetriou, Yikun Guo, Jay Kola, Ian Roberts, Andrea Setzer, Archana Tapuria, Bill Wheeldin. 2007. The CLEF Corpus: Semantic Anno-tation of Clinical Text. AMIA 2007, pp. 625 ? 629.  Taira R, Bashyam V, Kangarloo H. 2007. A field theory approach to medical natural language processing. IEEE Transactions in Inform Techn in Biomedicine 11(2). Michael Thelen and Ellen Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons using Ex-traction Pattern Contexts. Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing, pp. 214 ? 221. 
27
