Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 948?957,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Toward Completeness in Concept Extraction and Classification
Eduard Hovy and Zornitsa Kozareva
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292
hovy@isi.edu, zkozareva@gmail.com
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Abstract
Many algorithms extract terms from text to-
gether with some kind of taxonomic clas-
sification (is-a) link. However, the general
approaches used today, and specifically the
methods of evaluating results, exhibit serious
shortcomings. Harvesting without focusing on
a specific conceptual area may deliver large
numbers of terms, but they are scattered over
an immense concept space, making Recall
judgments impossible. Regarding Precision,
simply judging the correctness of terms and
their individual classification links may pro-
vide high scores, but this doesn?t help with the
eventual assembly of terms into a single coher-
ent taxonomy. Furthermore, since there is no
correct and complete gold standard to measure
against, most work invents some ad hoc evalu-
ation measure. We present an algorithm that is
more precise and complete than previous ones
for identifying from web text just those con-
cepts ?below? a given seed term. Comparing
the results to WordNet, we find that the algo-
rithm misses terms, but also that it learns many
new terms not in WordNet, and that it clas-
sifies them in ways acceptable to humans but
different from WordNet.
1 Collecting Information with Care
Over the past few years, many algorithms have been
published on automatically harvesting terms and
their conceptual types from the web and/or other
large corpora (Etzioni et al, 2005; Pasca, 2007;
Banko et al, 2007; Yi and Niblack, 2005; Snow et
al., 2005). But several basic problems limit the even-
tual utility of the results.
First, there is no standard collection of facts
against which results can be measured. As we show
in this paper, WordNet (Fellbaum, 1998), the most
obvious contender because of its size and popularity,
is deficient in various ways: it is neither complete
nor is its taxonomic structure inarguably perfect. As
a result, alternative ad hoc measures are invented
that are not comparable. Second, simply harvesting
facts about an entity without regard to its actual sub-
sequent organization inflates Recall and Precision
evaluation scores: while it is correct that a jaguar
is a animal, mammal, toy, sports-team, car-make,
and operating-system, this information doesn?t help
to create a taxonomy that, for example, places mam-
mal and animal closer to one another than to some
of the others. ((Snow et al, 2005) is an exception
to this.) As a result, this work may give a mislead-
ing sense of progress. Third, entities are of differ-
ent formal types, and their taxonomic treatment is
consequently different: some are at the level of in-
stances (e.g., Michelangelo was a painter) and some
at the level of concepts (e.g., a painter is a human).
The goal of our research is to learn terms for en-
tities (objects) and their taxonomic organization si-
multaneously, from text. Our method is to use a
single surface-level pattern with several open posi-
tions. Filling them in different ways harvests differ-
ent kinds of information, and/or confirms this infor-
mation. We evaluate in two ways: against WordNet,
since that is a commonly available and popular re-
source, and also by asking humans to judge the re-
sults since WordNet is neither complete nor exhaus-
tively taxonomized.
In this paper, we describe experiments with two
rich and common portions of an entity taxonomy:
Animals and People. The claim of this paper is: It is
possible to learn terms automatically to populate a
targeted portion of a taxonomy (such as below An-
948
imals or People) both at high precision compared
to WordNet and including additional correct ones as
well. We would like to also report on Recall rela-
tive to WordNet, but given the problems described
in Section 4, this turns out to be much harder than
would seem.
First, we need to define some basic terminology:
term: An English word (for our current purposes, a
noun or a proper name).
seed term: A word we use to initiate the algorithm.
concept: An item in the classification taxonomy we
are building. A concept may correspond to several
terms (singular form, plural form, the term?s syn-
onyms, etc.).
root concept: A concept at a fairly general (high)
level in the taxonomy, to which many others are
eventually learned to be subtypes/instances of.
basic-level concept: A concept at the ?basic level?,
corresponding approximately to the Basic Level cat-
egories defined in Prototype Theory in Psychology
(Rosch, 1978). For our purposes, a concept corre-
sponding to the (proto)typical level of generality of
its type; that is, a dog, not a mammal or a dachshund;
a singer, not a human or an opera diva.
instance: An item in the classification taxonomy
that is more specific than a concept; only one exam-
ple of the instance exists in ?the real world? at any
time. For example, Michelangelo is an instance, as
well as Mazda Miata with license plate 3HCY687,
while Mazda Miata is not.
classification link: We use a single relation, that,
depending on its arguments, is either is a type of
(when both arguments are concepts), or is an in-
stance of or is an example of (when the first argu-
ment is an instance/example of the second).
Section 2 describes our method for harvesting;
Section 3 discusses related work; and Section 4 de-
scribes the experiments and the results.
2 Term and Relation Extraction using the
Doubly-Anchored Pattern
Our goal is to develop a technique that automatically
?fills in? the concept space in the taxonomy below
any root concept, by harvesting terms through re-
peated web queries. We perform this in two alter-
nating stages.
Stage 1: Basic-level/Instance concept collec-
tion: We use the Doubly-Anchored Pattern DAP de-
veloped in (Kozareva et al, 2008):
DAP: [SeedTerm1] such as [SeedTerm2] and <X>
which learns a list of basic-level concepts or in-
stances (depending on whether SeedTerm2 ex-
presses a basic-level concept or an instance).1 DAP
is very reliable because it is instantiated with ex-
amples at both ?ends? of the space to be filled (the
higher-level (root) concept SeedTerm1 and a basic-
level term or instance (SeedTerm2)), which mutu-
ally disambiguate each other. For example, ?pres-
idents? for SeedTerm1 can refer to the leader of a
country, corporation, or university, and ?Ford? for
SeedTerm2 can refer to a car company, an automo-
bile pioneer, or a U.S. president. But when the two
terms co-occur in a text that matches the pattern
?Presidents such as Ford and <X>?, the text will
almost certainly refer to country presidents.
The first stage involves a series of repeated re-
placements of SeedTerm2 by newly-learned terms
in order to generate even more seed terms. That is,
each new basic-level concept or instance is rotated
into the pattern (becoming a new SeedTerm2) in a
bootstrapping cycle that Kozareva et al called reck-
less bootstrapping. This procedure is implemented
as exhaustive breadth-first search, and iterates until
no new terms are harvested. The harvested terms are
incorporated in a Hyponym Pattern Linkage Graph
(HPLG) G = (V,E), where each vertex v ? V is
a candidate term and each edge (u, v) ? E indi-
cates that term v was generated by term u. A term
u is ranked by Out-Degree(u) =
P
?(u,v)?E
w(u,v)
|V |?1
,
which represents the weighted sum of u?s outgoing
edges normalized by the total number of other nodes
in the graph. Intuitively, a term ranks highly if it
is frequently discovering many different terms dur-
ing the reckless bootstrapping cycle. This method is
very productive, harvesting a constant stream of new
terms for basic-level concepts or instances when the
taxonomy below the initial root concept SeedTerm1
is extensive (such as for Animals or People).
1Strictly speaking, our lowest-level concepts can be in-
stances, basic-level concepts, or concepts below the basic level
(e.g., dachsund). But for the sake of simplicity we will refer to
our lowest-level terms as basic-level concepts and instances.
949
Stage 2: Intermediate level concept collection:
Going beyond (Kozareva et al, 2008), we next apply
the Doubly-Anchored Pattern in the ?backward? di-
rection (DAP?1), for any two seed terms represent-
ing basic-level concepts or instances:
DAP?1: <X> such as [SeedTerm1] and [SeedTerm2]
which harvests a set of concepts, most of them inter-
mediate between the basic level or instance and the
initial higher-level seed.
This second stage (DAP?1) has not yet been de-
scribed in the literature. It proceeds analogously.
For pairs of basic-level concepts or instances be-
low the root concept that were found during the first
stage, we instantiate DAP?1 and issue a new web
query. For example, if the term ?cats? was harvested
by DAP in ?Animals such as dogs and <X>?, then
the pair < dogs, cats > forms the new Web query
?<X> such as dogs and cats?. We extract up to 2
consecutive nouns from the <X> position.
This procedure yields a large number of discov-
ered concepts, but they cannot all be used for fur-
ther bootstrapping. In addition to practical limita-
tions (such as limits on web querying), many of them
are too general?more general than the initial root
concept?and could derail the bootstrapping process
by introducing terms that stray every further away
from the initial root concept. We therefore rank the
harvested terms based on the likelihood that they
will be productive if they are expanded in the next
cycle. Ranking is based on two criteria: (1) the con-
cept should be prolific (i.e., produce many lower-
level concepts) in order to keep the bootstrapping
process energized, and (2) the concept should be
subordinate to the root concept, so that the process
stays within the targeted part of the search space.
To perform ranking, we incorporate both the har-
vested concepts and the basic-level/instance pairs
into a Hypernym Relation Graph (HRG), which we
define as a bipartite graph HRG = (V,E) with two
types of vertices. One set of vertices represents the
concepts (the category vertices (V
c
), and a second
set of vertices represents the basic-level/instance
pairs that produced the concepts (the member pair
vertices (V
mp
)). We create an edge e(u, v) ? E
between u ? V
c
and v ? V
mp
when the con-
cept represented by u was harvested by the basic-
level/instance pair represented by v, with the weight
of the edge defined as the number of times that the
lower pair found the concept on the web.
We use the Hypernym Relation Graph to rank
the intermediate concepts based on each node?s In-
Degree, which is the sum of the weights on the
node?s incoming edges. Formally, In-Degree(u) =
?
?(u,v)?E
w(u, v). Intuitively, a concept will be
ranked highly if it was harvested by many different
combinations of basic-level/instance terms.
However, this scoring function does not deter-
mine whether a concept is more or less general than
the initial root concept. For example, when har-
vesting animal categories, the system may learn the
word ?species?, which is a very common term asso-
ciated with animals, but also applies to non-animals
such as plants. To prevent the inclusion of over-
general terms and constrain the search to remain
?below? the root concept, we apply a Concept Posi-
tioning Test (CPT): We issue the following two web
queries:
(a) Concept such as RootConcept and <X>
(b) RootConcept such as Concept and <X>
If (b) returns more web hits than (a), then the con-
cept passes the test, otherwise it fails. The first (most
highly ranked) concept that passes CPT becomes the
new seed concept for the next bootstrapping cycle.
In principle, we could use all the concepts that pass
the CPT for bootstrapping2. However, for practical
reasons (primarily limitations on web querying), we
run the algorithm for 10 iterations.
3 Related Work
Many algorithms have been developed to automat-
ically acquire semantic class members using a va-
riety of techniques, including co-occurrence statis-
tics (Riloff and Shepherd, 1997; Roark and Char-
niak, 1998), syntactic dependencies (Pantel and
Ravichandran, 2004), and lexico-syntactic patterns
(Riloff and Jones, 1999; Fleischman and Hovy,
2002; Thelen and Riloff, 2002).
The work most closely related to ours is that of
(Hearst, 1992) who introduced the idea of apply-
ing hyponym patterns to text, which explicitly iden-
tify a hyponym relation between two terms (e.g.,
2The number of ranked concepts that pass CPT changes in
each iteration. Also, the wildcard * is important for counts, as
can be verified with a quick experiment using Google.
950
?such authors as <X>?). In recent years, sev-
eral researchers have followed up on this idea using
the web as a corpus. (Pasca, 2004) applies lexico-
syntactic hyponym patterns to the Web and use the
contexts around them for learning. KnowItAll (Et-
zioni et al, 2005) applies the hyponym patterns to
extract instances from the Web and ranks them by
relevance using mutual information. (Kozareva et
al., 2008) introduced a bootstrapping scheme using
the doubly-anchored pattern (DAP) that is guided
through graph ranking. This approach reported a
significant improvement from 5% to 18% over ap-
proaches using singly-anchored patterns like those
of (Pasca, 2004) and (Etzioni et al, 2005).
(Snow et al, 2005) describe a dependency path
based approach that generates a large number of
weak hypernym patterns using pairs of noun phrases
present in WordNet. They build a classifier using
the different hypernym patterns and find among the
highest precision patterns those of (Hearst, 1992).
Snow et al report performance of 85% precision
at 10% recall and 25% precision at 30% recall for
5300 hand-tagged noun phrase pairs. (McNamee et
al., 2008) use the technique of (Snow et al, 2005)
to harvest the hypernyms of the proper names. The
average precision on 75 automatically detected cat-
egories is 53%. The discovered hypernyms were
intergrated in a Question Answering system which
showed an improvement of 9% when evaluated on a
TREC Question Answering data set.
Recently, (Ritter et al, 2009) reported hypernym
learning using (Hearst, 1992) patterns and manually
tagged common and proper nouns. All hypernym
candidates matching the pattern are acquired, and
the candidate terms are ranked by mutual informa-
tion. However, they evaluate the performance of
their hypernym algorithm by considering only the
top 5 hypernyms given a basic-level concept or in-
stance. They report 100% precision at 18% recall,
and 66% precision at 72% recall, considering only
the top-5 list. Necessarily, using all the results re-
turned will result in lower precision scores. In con-
trast to their approach, our aim is to first acquire au-
tomatically with minimal supervision the basic-level
concepts for given root concept. Thus, we almost
entirely eliminate the need for humans to provide
hyponym seeds. Second, we evaluate the perfor-
mance of our approach not by measuring the top-
ranked 5 hypernyms given a basic-level concept, but
considering all harvested hypernyms of the concept.
Unlike (Etzioni et al, 2005), (Pasca, 2007) and
(Snow et al, 2005), we learn both instances and con-
cepts simultaneously.
Some researchers have also worked on reorga-
nizing, augmenting, or extending semantic concepts
that already exist in manually built resources such
as WordNet (Widdows and Dorow, 2002; Snow et
al., 2005) or Wikipedia (Ponzetto and Strube, 2007).
Work in automated ontology construction has cre-
ated lexical hierarchies (Caraballo, 1999; Cimiano
and Volker, 2005; Mann, 2002), and learned seman-
tic relations such as meronymy (Berland and Char-
niak, 1999; Girju et al, 2003).
4 Evaluation
The root concepts discussed in this paper are An-
imals and People, because they head large taxo-
nomic structures that are well-represented in Word-
Net. Throughout these experiments, we used as the
initial SeedTerm2 lions for Animals and Madonna
for People (by specifically choosing a proper name
for People we force harvesting down to the level of
individual instances). To collect data, we submitted
the DAP patterns as web queries to Google, retrieved
the top 1000 web snippets per query, and kept only
the unique ones. In total, we collected 1.1 GB of
snippets for Animals and 1.5 GB for People. The
algorithm was allowed to run for 10 iterations.
The algorithm learns a staggering variety of terms
that is much more diverse than we had antici-
pated. In addition to many basic-level concepts or
instances, such as dog and Madonna respectively,
and many intermediate concepts, such as mammals,
pets, and predators, it also harvested categories that
clearly seemed useful, such as laboratory animals,
forest dwellers, and endangered species. Many other
harvested terms were more difficult to judge, includ-
ing bait, allergens, seafood, vectors, protein, and
pests. While these terms have an obvious relation-
ship to Animals, we have to determine whether they
are legitimate and valuable subconcepts of Animals.
A second issue involves relative terms that are
hard to define in an absolute sense, such as native
animals and large mammals.
A complete evaluation should answer the following
three questions:
951
? Precision: What is the correctness of the har-
vested concepts? (How many of them are sim-
ply wrong, given the root concept?)
? Recall: What is the coverage of the harvested
concepts? (How many are missing, below a
given root concept?)
? How correct is the taxonomic structure
learned?
Given the number and variety of terms obtained,
we initially decided that an automatic evaluation
against existing resources (such as WordNet or
something similar) would be inadequate because
they do not contain many of our harvested terms,
even though many of these terms are clearly sensi-
ble and potentially valuable. Indeed, the whole point
of our work is to learn concepts and taxonomies that
go above and beyond what is currently available.
However, it is necessary to compare with
something, and it is important not to skirt the issue
by conducting evaluations that measure subsets of
results, or that perhaps may mislead. We therefore
decided to compare our results against WordNet and
to have human annotators judge as many results as
we could afford (to obtain a measure of Precision
and the legitimate extensions beyond WordNet).
Unfortunately, it proved impossible to measure
Recall against WordNet, because this requires as-
certaining the number of synsets in WordNet be-
tween the root and its basic-level categories. This
requires human judgment, which we could not af-
ford. We plan to address this question in future
work. Also, assessing the correctness of the learned
taxonomy structure requires the manual assessment
of each classification link proposed by the system
that is not already in WordNet, a task also beyond
our budget to complete in full. Some results?for
just basic-level terms and intermediate concepts, but
not among intermediate-level concepts?are shown in
Section 4.3.
We provide Precision scores using the following
measures, where terms refers to the harvested terms:
Pr
WN
=
#terms found in WordNet
#terms harvested by system
Pr
H
=
#terms judged correct by human
#terms harvested by system
NotInWN = #terms judged correct by human but
not in WordNet
We conducted three sets of experiments. Ex-
periment 1 evaluates the results of using DAP to
learn basic-level concepts for Animals and instances
for People. Experiment 2 evaluates the results of
using DAP?1 to harvest intermediate concepts be-
tween each root concept and its basic-level concepts
or instances. Experiment 3 evaluates the taxonomy
structure that is produced via the links between the
instances and intermediate concepts.
4.1 Experiment 1: Basic-Level Concepts and
Instances
In this section we discuss the results of harvest-
ing the basic-level Animal concepts and People in-
stances. The bootstrapping algorithm ranks the har-
vested terms by their Out-Degree score and consid-
ers as correct only those with Out-Degree > 0. In
ten iterations, the bootstrapping algorithm produced
913 Animal basic-level concepts and 1, 344 People
instances that passed this Out-Degree criterion.
4.1.1 Human Evaluation
The harvested terms were labeled by human
judges as either correct or incorrect with respect to
the root concept. Table 1 shows the Precision of the
top-ranked N terms, with N shown in increments
of 100. Overall, the Animal terms yielded 71%
(649/913) Precision and the People terms yielded
95% Precision (1,271/1,344). Figure 1 shows that
higher-ranked Animal terms are more accurate than
lower-ranked terms, which indicates that the scor-
ing function did its job. For People terms, accuracy
was very high throughout the ranked list. Overall,
these results show that the bootstrapping algorithm
generates a large number of correct instances of high
quality.
4.1.2 WordNet Evaluation
Table 1 shows a comparison of the harvested
terms against the terms present in WordNet.
Note that the Precision measured against WordNet
(Pr
WN
) for People is dramatically different from
the Precision based on human judgments (Pr
H
).
This can be explained by looking at the NotInWN
column, which shows that 48 correct Animal terms
952
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 100  200  300  400  500  600  700  800  900
Pr
ec
is
io
n
Rank
Animal Basic-level Concepts
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 200  400  600  800  1000  1200
Pr
ec
is
io
n
Rank
People Instances
Figure 1: Ranked Basic-Concepts and Instances.
and 986 correct People instances are not present in
WordNet (primarily, for people, because WordNet
contains relatively few proper names). These results
show that there is substantial room for improvement
in WordNet?s coverage of these categories. For Ani-
mals, the precision measured against WordNet is ac-
tually higher than the precision measured by human
judges, which may indicate that the judges failed to
recognize some correct animal terms.
Pr
WN
Pr
H
NotInWN
Animal .79 .71 48
People .23 .95 986
Table 1: Instance Evaluation.
4.1.3 Evaluation against Prior Work
To assess how well our algorithm compares with
previous semantic class learning methods, we com-
pared our results to those of (Kozareva et al, 2008).
Our work was inspired by that approach?in fact, we
use that previous algorithm as the first step of our
bootstrapping process. The novelty of our approach
is the insertion of an additional bootstrapping stage
that iteratively learns new intermediate concepts us-
ing DAP?1 and the Concept Positioning Test, fol-
lowed by the subsequent use of the newly learned
intermediate concepts in DAP to expand the search
space beyond the original root concept. This leads
to the discovery of additional basic-level terms or in-
stances, which are then recycled in turn to discover
new intermediate concepts, and so on.
Consequently, we can compare the results pro-
duced by the first iteration of our algorithm (be-
fore intermediate concepts are learned) to those of
(Kozareva et al, 2008) for the Animal and People
categories, and then compare again after 10 boot-
strapping iterations of intermediate concept learn-
ing. Figure 2 shows the number of harvested con-
cepts for Animals and People after each bootstrap-
ping iteration. Bootstrapping with intermediate con-
cepts produces nearly 5 times as many basic-level
concepts and instances than (Kozareva et al, 2008)
obtain, while maintaining similar levels of precision.
The intermediate concepts help so much because
they steer the learning process into new (yet still cor-
rect) regions of the search space after each iteration.
For instance, in the first iteration, the pattern ?ani-
mals such as lions and *? harvests about 350 basic-
level concepts, but only animals that are mentioned
in conjunction with lions are learned. Of these, an-
imals typically quite different from lions, such as
grass-eating kudu, are often not discovered.
However, in the second iteration, the intermediate
concept Herbivore is chosen for expansion. The pat-
tern ?herbivore such as antelope and *? discovers
many additional animals, including kudu, that co-
occur with antelope but do not co-occur with lions.
Table 2 shows examples of the 10 top-ranked
basic-level concepts and instances that were learned
for 3 randomly-selected intermediate Animal and
People concepts (IConcepts) that were acquired dur-
ing bootstrapping. In the next section, we present an
953
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 1  2  3  4  5  6  7  8  9  10
#I
te
m
s 
Le
ar
ne
d
Iterations
Animal Intermediate Concepts
Animal Basic-level Concepts
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 1  2  3  4  5  6  7  8  9  10
#I
te
m
s 
Le
ar
ne
d
Iterations
People Intermediate Concepts
People Instances
Figure 2: Learning Curves.
evaluation of the intermediate concept terms.
4.2 Experiment 2: Intermediate Concepts
In this section we discuss the results of harvesting
the intermediate-level concepts. Given the variety of
the harvested results, manual judgment of correct-
ness required an in-depth human annotation study.
We also compare our harvested results against the
concept terms in WordNet.
4.2.1 Human Evaluation
We hired 4 annotators (undergraduates at a dif-
ferent institution) to judge the correctness of the in-
termediate concepts. We created detailed annota-
tion guidelines that define 14 annotation labels for
each of the Animal and People classes, as shown
in Table 3. The labels are clustered into 4 major
PEOPLE
IConcept Instances
Dictators: Adolf Hitler, Joseph Stalin, Benito Mussolini, Lenin,
Fidel Castro, Idi Amin, Slobodan Milosevic,
Hugo Chavez, Mao Zedong, Saddam Hussein
Celebrities: Madonna, Paris Hilton, Angelina Jolie, Britney ,
Spears, Tom Cruise, Cameron Diaz, Bono,
Oprah Winfrey, Jennifer Aniston, Kate Moss
Writers: William Shakespeare, James Joyce, Charles Dickens,
Leo Tolstoy, Goethe, Ralph Waldo Emerson,
Daniel Defoe, Jane Austen, Ernest Hemingway,
Franz Kafka
ANIMAL
IConcept Basic-level Terms
Crustacean: shrimp, crabs, prawns, lobsters, crayfish, mysids,
decapods, marron, ostracods, yabbies
Primates: baboons, monkeys, chimpanzees, apes, marmosets,
chimps, orangutans, gibbons, tamarins, bonobos
Mammal: mice, whales, seals, dolphins, rats, deer, rabbits,
dogs, elephants, squirrels
Table 2: Learned People and Animals Terms.
types: Correct, Borderline, BasicConcept, and Not-
Concept. The details of our annotation guidelines,
the reasons for the intermediate labels, and the anno-
tation study can be found in (Kozareva et al, 2009).
ANIMAL
TYPE LABEL EXAMPLES
Correct GeneticAnimal reptile,mammal
BehavioralByFeeding predator, grazer
BehaviorByHabitat saltwater mammal
BehaviorSocialIndiv herding animal
BehaviorSocialGroup herd, pack
MorphologicalType cloven-hoofed animal
RoleOrFunction pet, parasite
Borderline NonRealAnimal dragons
EvaluativeTerm varmint, fox
OtherAnimal critter, fossil
BasicConcept BasicAnimal dog, hummingbird
NotConcept GeneralTerm model, catalyst
NotAnimal topic, favorite
GarbageTerm brates, mals
PEOPLE
TYPE LABEL EXAMPLES
Correct GeneticPerson Caucasian, Saxon
NonTransientEventRole stutterer, gourmand
TransientEventRole passenger, visitor
PersonState dwarf, schizophrenic
FamilyRelation aunt, mother
SocialRole fugitive, hero
NationOrTribe Bulgarian, Zulu
ReligiousAffiliation Catholic, atheist
Borderline NonRealPerson biblical figures
OtherPerson colleagues, couples
BasicConcept BasicPerson child, woman
RealPerson Barack Obama
NotConcept GeneralTerm image, figure
NotPerson books, events
Table 3: Intermediate Concept Annotation Labels
We measured pairwise inter-annotator agreement
across the four labels using the Fleiss kappa (Fleiss,
1971). The ? scores ranged from 0.61?0.71 for
Animals (average ?=0.66) and from 0.51?0.70 for
People (average ?=0.60). These agreement scores
seemed good enough to warrant using these human
judgments to estimate the accuracy of the algorithm.
The bootstrapping algorithm harvested 3, 549 An-
imal and 4, 094 People intermediate concepts in ten
iterations. After In-Degree ranking was applied,
954
we chose a random sample of intermediate concepts
with frequency over 1, which was given to four hu-
man judges for annotation. Table 4 summarizes the
labels assigned by the four annotators (A
1
? A
4
).
The top portion of Table 4 shows the results for all
the intermediate concepts (437 Animal terms and
296 People terms), and the bottom portion shows the
results only for the concepts that passed the Concept
Positioning Test (187 Animal terms and 139 People
terms). Accuracy is computed in two ways: Acc1 is
the percent of intermediate concepts labeled as Cor-
rect; Acc2 is the percent of intermediate concepts
labeled as either Correct or Borderline.
Without the CPT, accuracies range from 53?66%
for Animals and 75?85% for People. After ap-
plying the CPT, the accuracies increase to 71?84%
for animals and 82?94% for people. These results
confirm that the Concept Positioning Test is effec-
tive at removing many of the undesirable terms.
Overall, these results demonstrate that our algorithm
produced many high-quality intermediate concepts,
with good precision.
Figure 3 shows accuracy curves based on the
rankings of the intermediate concepts (based on In-
Degree scores). The CPT clearly improves accu-
racy even among the most highly ranked concepts.
For example, the Acc1 curves for animals show that
nearly 90% of the top 100 intermediate concepts
were correct after applying the CPT, whereas only
70% of the top 100 intermediate concepts were cor-
rect before. However, the CPT also eliminates many
desirable terms. For People, the accuracies are still
relatively high even without the CPT, and a much
larger set of intermediate concepts is learned.
Animals People
A
1
A
2
A
3
A
4
A
1
A
2
A
3
A
4
Correct 246 243 251 230 239 231 225 221
Borderline 42 26 22 29 12 10 6 4
BasicConcept 2 8 9 2 6 2 9 10
NotConcept 147 160 155 176 39 53 56 61
Acc1 .56 .56 .57 .53 .81 .78 .76 .75
Acc2 .66 .62 .62 .59 .85 .81 .78 .76
Animals after CPT People after CPT
A
1
A
2
A
3
A
4
A
1
A
2
A
3
A
4
Correct 146 133 144 141 126 126 114 116
Borderline 11 15 9 13 6 2 2 0
BasicConcept 2 8 9 2 0 1 7 7
NotConcept 28 31 25 31 7 10 16 16
Acc1 .78 .71 .77 .75 .91 .91 .82 .83
Acc2 .84 .79 .82 .82 .95 .92 .83 .83
Table 4: Human Intermediate Concept Evaluation.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300  350  400
Pr
ec
is
io
n
Rank
Animal Intermediate Concepts
noCPTC
noCPTCB
withCPTC
withCPTCB
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300
Pr
ec
is
io
n
Rank
People Intermediate Concepts
noCPTC
noCPTCB
withCPTC
withCPTCB
Figure 3: Intermediate Concept Precision at Rank N.
4.2.2 WordNet Evaluation
We also compared the intermediate concepts har-
vested by the algorithm to the contents of WordNet.
The results are shown in Table 5. WordNet contains
20% of the Animal concepts and 51% of the People
concepts learned by our algorithm, which confirms
that many of these concepts were considered to be
valuable taxonomic terms by the WordNet develop-
ers. However, our human annotators judged 57%
of the Animal and 84% of the People concepts to
be correct, which suggests that our algorithm gen-
erates a substantial number of additional concepts
that could be used to enrich taxonomic structure in
WordNet.
955
Pr
WN
Pr
H
NotInWN
Animal .20 (88/437) .57 (248/437) 204
People .51 (152/296) .85 (251/296) 108
Table 5: WordNet Intermediate Concept Evaluation.
4.3 Experiment 3: Taxonomic Links
In this section we evaluate the classification (taxon-
omy) that is learned by evaluating the links between
the intermediate concepts and the basic-level con-
cept/instance terms. That is, when our algorithm
claims that isa(X,Y), how often is X truly a subcon-
cept of Y? For example, isa(goat, herbivore) would
be correct, but isa(goat, bird) would not. Again,
since WordNet does not contain all the harvested
concepts, we conduct both a manual evaluation and
a comparison against WordNet.
4.3.1 Manual and WordNet Evaluations
Creating and evaluating the full taxonomic struc-
ture between the root and the basic-level or instance
terms is future work. Here we evaluate simply the
accuracy of the taxonomic links between basic-level
concepts/instances and intermediate concepts as har-
vested, but not between intermediate concepts. For
each pair, we extracted all harvested links and deter-
mined whether the same links appear in WordNet.
The links were also given to human judges. Table 6
shows the results.
ISA Pr
WN
Pr
H
NotInWN
Animal .47(912/1940) .88 (1716/1940) 804
People .23 (318/908) .94 (857/908) 539
Table 6: WordNet Taxonomic Evaluation.
The results show that WordNet lacks nearly half
of the taxonomic relations that were generated by
the algorithm: 804 Animal and 539 People links.
5 Conclusion
We describe a novel extension to the DAP approach
for discovering basic-level concepts or instances and
their superconcepts given an initial root concept. By
appropriate filling of different positions in DAP, the
algorithm alternates between ?downward? and ?up-
ward? learning. A key resulting benefit is that each
new intermediate-level term acquired restarts har-
vesting in a new region of the concept space, which
allows previously unseen concepts to be discovered
with each bootstrapping cycle.
We also introduce the Concept Positioning Test,
which serves to confirm that a harvested concept
falls into the desired part of the search space rela-
tive to either a superordinate or subordinate concept
in the growing taxonomy, before it is selected for
further harvesting using the DAP.
These algorithms can augment other term harvest-
ing algorithms recently reported. But in order to
compare different algorithms, it is important to com-
pare results to a standard. WordNet is our best can-
didate at present. But WordNet is incomplete. Our
results include a significantly large number of in-
stances of People (which WordNet does not claim
to cover), a number comparable to the results of (Et-
zioni et al, 2005; Pasca, 2007; Ritter et al, 2009).
Rather surprisingly, our results also include a large
number of basic-level and intermediate concepts for
Animals that are not present in WordNet, a category
WordNet is actually fairly complete about. These
numbers show clearly that it is important to conduct
manual evaluation of term harvesting algorithms in
addition to comparing to a standard resource.
Acknowledgments
This research was supported in part by grants from
the National Science Foundation (NSF grant no. IIS-
0429360), and the Department of Homeland Se-
curity, ONR Grant numbers N0014-07-1-0152 and
N00014-07-1-0149. We are grateful to the anno-
tators at the University of Pittsburgh who helped
us evaluate this work: Jay Fischer, David Halpern,
Amir Hussain, and Taichi Nakatani.
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O.Etzioni. 2007. Open information extraction
from the web. In Proceedings of International Joint
Conference on Artificial Itelligence, pages 2670?2676.
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proc. of the 37th Annual Meeting of
the Association for Computational Linguistics.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proceeding of RANLP-05, pages 166?172.
956
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication).
May.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of the
COLING conference, August.
J.L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In HLT-NAACL.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING, pages 539?545.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pat-
tern linkage graphs. In Proceedings of ACL-08: HLT,
pages 1048?1056. Association for Computational Lin-
guistics.
Z. Kozareva, E. Hovy, and E. Riloff. 2009. Learning and
evaluating the content and structure of a term taxon-
omy. In AAAI-09 Spring Symposium on Learning by
Reading and Learning to Read.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In COLING-02 on SEMANET,
pages 1?7.
P. McNamee, R. Snow, P. Schone, and J. Mayfield. 2008.
Learning named entity hyponyms for question answer-
ing. In Proceedings of the Third International Joint
Conference on Natural Language Processing.
P. Pantel and D. Ravichandran. 2004. Automatically la-
beling semantic classes. In HLT-NAACL, pages 321?
328.
M. Pasca. 2004. Acquisition of categorized named en-
tities for web search. In Proceedings of CIKM, pages
137?145.
M. Pasca. 2007. Weakly-supervised discovery of named
entities using web search queries. In CIKM, pages
683?690.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from wikipedia. In Proceedings of the 22nd
National COnference on Artificial Intelligence (AAAI-
07), pages 1440?1447.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
A. Ritter, S. Soderland, and O. Etzioni. 2009. What is
this, anyway: Automatic hypernym discovery. In Pro-
ceedings of AAAI-09 Spring Symposium on Learning
by Reading and Learning to Read, pages 88?93.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
E. Rosch, 1978. Principles of Categorization, pages 27?
48.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
D. Widdows and B. Dorow. 2002. A graph model for un-
supervised lexical acquisition. In Proceedings of the
19th international conference on Computational lin-
guistics, pages 1?7.
J. Yi and W. Niblack. 2005. Sentiment mining in web-
fountain. In ICDE ?05: Proceedings of the 21st In-
ternational Conference on Data Engineering, pages
1073?1083.
957
Bootstrapping Named Entity Recognition
with Automatically Generated Gazetteer Lists
Zornitsa Kozareva
Dept. de Lenguajes y Sistemas Informa?ticos
University of Alicante
Alicante, Spain
zkozareva@dlsi.ua.es
Abstract
Current Named Entity Recognition sys-
tems suffer from the lack of hand-tagged
data as well as degradation when mov-
ing to other domain. This paper explores
two aspects: the automatic generation of
gazetteer lists from unlabeled data; and the
building of a Named Entity Recognition
system with labeled and unlabeled data.
1 Introduction
Automatic information extraction and information
retrieval concerning particular person, location,
organization, title of movie or book, juxtaposes to
the Named Entity Recognition (NER) task. NER
consists in detecting the most silent and informa-
tive elements in a text such as names of people,
company names, location, monetary currencies,
dates. Early NER systems (Fisher et al, 1997),
(Black et al, 1998) etc., participating in Message
Understanding Conferences (MUC), used linguis-
tic tools and gazetteer lists. However these are dif-
ficult to develop and domain sensitive.
To surmount these obstacles, application of
machine learning approaches to NER became a
research subject. Various state-of-the-art ma-
chine learning algorithms such as Maximum En-
tropy (Borthwick, 1999), AdaBoost(Carreras et
al., 2002), Hidden Markov Models (Bikel et al, ),
Memory-based Based learning (Tjong Kim Sang,
2002b), have been used1. (Klein et al, 2003),
(Mayfield et al, 2003), (Wu et al, 2003),
(Kozareva et al, 2005c) among others, combined
several classifiers to obtain better named entity
coverage rate.
1For other machine learning methods, consult
http://www.cnts.ua.ac.be/conll2002/ner/
http://www.cnts.ua.ac.be/conll2003/ner/
Nevertheless all these machine learning algo-
rithms rely on previously hand-labeled training
data. Obtaining such data is labor-intensive, time
consuming and even might not be present for lan-
guages with limited funding. Resource limitation,
directed NER research (Collins and Singer, 1999),
(Carreras et al, 2003), (Kozareva et al, 2005a)
toward the usage of semi-supervised techniques.
These techniques are needed, as we live in a multi-
lingual society and access to information from var-
ious language sources is reality. The development
of NER systems for languages other than English
commenced.
This paper presents the development of a Span-
ish Named Recognition system based on machine
learning approach. For it no morphologic or syn-
tactic information was used. However, we pro-
pose and incorporate a very simple method for
automatic gazetteer2 construction. Such method
can be easily adapted to other languages and it is
low-costly obtained as it relies on n-gram extrac-
tion from unlabeled data. We compare the perfor-
mance of our NER system when labeled and unla-
beled training data is present.
The paper is organized in the following way:
brief explanation about NER process is repre-
sented in Section 2. In Section 3 follows feature
extraction. The experimental evaluation for the
Named Entity detection and classification tasks
with and without labeled data are in Sections 4 and
5. We conclude in Section 6.
2 The NER how to
A Named Entity Recognition task can be de-
scribed as composition of two subtasks, entity de-
2specialized lists of names for location and person names,
e.g. Madrid is in the location gazetteer, Mary is in the person
gazetteer
15
tection and entity classification. Entity delimita-
tion consist in determining the boundaries of the
entity (e.g. the place from where it starts and the
place it finishes). This is important for tracing
entities composed of two or more words such as
?Presidente de los Estados Unidos ?3, ?Universi-
dad Politecnica de Catalun?a?4. For this purpose,
the BIO scheme was incorporated. In this scheme,
tag B denotes the start of an entity, tag I continues
the entity and tag O marks words that do not form
part of an entity. This scheme was initially intro-
duced in CoNLL?s (Tjong Kim Sang, 2002a) and
(Tjong Kim Sang and De Meulder, 2003) NER
competitions, and we decided to adapt it for our
experimental work.
Once all entities in the text are detected, they
are passed for classification in a predefined set of
categories such as location, person, organization
or miscellaneous5 names. This task is known as
entity classification. The final NER performance
is measured considering the entity detection and
classification tasks together.
Our NER approach is based on machine learn-
ing. The two algorithms we used for the experi-
ments were instance-based and decision trees, im-
plemented by (Daelemans et al, 2003). They were
used with their default parameter settings. We
selected the instance-based model, because it is
known to be useful when the amount of training
data is not sufficient.
Important part in the NE process takes the lo-
cation and person gazetteer lists which were au-
tomatically extracted from unlabeled data. More
detailed explanation about their generation can be
found in Section 3.
To explore the effect of labeled and unlabeled
training data to our NER, two types of experiments
were conducted. For the supervised approach, the
labels in the training data were previously known.
For the semi-supervised approach, the labels in the
training data were hidden. We used bootstrapping
(Abney, 2002) which refers to a problem setting
in which one is given a small set of labeled data
and a large set of unlabeled data, and the task is to
induce a classifier.
? Goals:
- utilize a minimal amount of supervised ex-
amples;
3?President of the United States?
4?Technical University of Catalun?a?
5book titles, sport events, etc.
- obtain learning from many unlabeled ex-
amples;
? General scheme:
- initial supervision seed examples for train-
ing an initial model;
- corpus classification with seed model;
- add most confident classifications to train-
ing data and iterate.
In our bootstrapping, a newly labeled example
was added into the training data L, if the two clas-
sifiers C1 and C2 agreed on the class of that ex-
ample. The number n of iterations for our ex-
periments is set up to 25 and when this bound is
reached the bootstrapping stops. The scheme we
follow is described below.
1. for iteration = 0 . . . n do
2. pool 1000 examples from unlabeled data;
3. annotate all 1000 examples with classifier C1
and C2;
4. for each of the 1000 examples compare
classes of C1 and C2;
5. add example into L only if classes of C1 and
C2 agree;
6. train model with L;
7. calculate result
8. end for
Bootstrapping was previously used by (Carreras
et al, 2003), who were interested in recognizing
Catalan names using Spanish resources. (Becker
et al, 2005) employed bootstrapping in an ac-
tive learning method for tagging entities in an as-
tronomic domain. (Yarowsky, 1995) and (Mi-
halcea and Moldovan, 2001) utilized bootstrap-
ping for word sense disambiguation. (Collins and
Singer, 1999) classified NEs through co-training,
(Kozareva et al, 2005a) used self-training and co-
training to detect and classify named entities in
news domain, (Shen et al, 2004) conducted ex-
periments with multi-criteria-based active learning
for biomedical NER.
The experimental data we work with is taken
from the CoNLL-2002 competition. The Spanish
16
corpus6 comes from news domain and was previ-
ously manually annotated. The train data set con-
tains 264715 words of which 18798 are entities
and the test set has 51533 words of which 3558
are entities.
We decided to work with available NE anno-
tated corpora in order to conduct an exhaustive and
comparative NER study when labeled and unla-
beld data is present. For our bootstrapping experi-
ment, we simply ignored the presence of the labels
in the training data. Of course this approach can be
applied to other domain or language, the only need
is labeled test data to conduct correct evaluation.
The evaluation is computed per NE class by the
help of conlleval7 script. The evaluation measures
are:
Precision =
number of correct answers found by the system
number of answers given by the system
(1)
Recall =
number of correct answers found by the system
number of correct answers in the test corpus
(2)
F?=1 =
2? Precision? Recall
Precision + Recall
(3)
3 Feature extraction
Recently diverse machine learning techniques are
utilized to resolve various NLP tasks. For all of
them crucial role plays the feature extraction and
selection module, which leads to optimal classifier
performance. This section describes the features
used for our Named Entity Recognition task.
Feature vectors ?i={f1,...,fn} are constructed.
The total number of features is denoted by n, and
?i corresponds to the number of examples in the
data. In our experiment features represent contex-
tual, lexical and gazetteer information. Here we
number each feature and its corresponding argu-
ment.
f1: all letters of w08 are in capitals;
f2-f8: w?3, w?2, w?1, w0, w+1, w+2, w+3 ini-
tiate in capitals;
f9: position of w0 in the current sentence;
f10: frequency of w0;
f11-f17: word forms of w0 and the words in
[?3,+3] window;
f18: first word making up the entity;
f19: second word making up the entity, if
present;
6http://www.cnts.ua.ac.be/conll2002/ner/data/
7http://www.cnts.ua.ac.be/conll2002/ner/bin/
8w0 indicates the word to be classified.
f20: w?1 is trigger word for location, person or
organization;
f21: w+1 is trigger word for location, person or
organization;
f22: w0 belongs to location gazetteer list;
f23: w0 belongs to first person name gazetteer
list;
f24: w0 belongs to family name gazetteer list;
f25: 0 if the majority of the words in an entity
are locations, 1 if the majority of the words in an
entity are persons and 2 otherwise.
Features f22, f23, f24 were automatically ex-
tracted by a simple pattern validation method we
propose below.
The corpus from where the gazetteer lists were
extracted, forms part of Efe94 and Efe95 Spanish
corpora provided for the CLEF9 competitions. We
conducted a simple preprocessing, where all sgml
documents were merged in a single file and only
the content situated among the text tags was ex-
tracted and considered for further processing. As
a result, we obtained 1 Gigabyte of unlabeled data,
containing 173468453 words. The text was tok-
enized and the frequency of all unigrams in the
corpus was gathered.
The algorithm we propose and use to obtain
location and person gazetteer lists is very simple.
It consists in finding and validating common pat-
terns, which can be constructed and utilized also
for languages other than Spanish.
The location pattern ?prepi, wj?, looks for
preposition i which indicates location in the Span-
ish language and all corresponding right capital-
ized context words wj for preposition i. The de-
pendency relation between prepi and wj , con-
veys the semantic information on the selection re-
strictions imposed by the two related words. In
a walk through example the pattern ?en, ??, ex-
tracts all right capitalized context words wj as
{Argentina, Barcelona, Madrid, Valencia} placed
next to preposition ?en?. These words are taken
as location candidates. The selection restriction
implies searching for words appearing after the
preposition ?en? (e.g. en Madrid) and not before
the preposition (e.g. Madrid en).
The termination of the pattern extraction ?en,??,
initiates the extraction phase for the next preposi-
tions in prepi = {en, En, desde, Desde, hacia, Ha-
cia}. This processes is repeated until the complete
set of words in the preposition set are validated.
Table 1 represents the number of entities extracted
9http://www.clef-campaign.org/
17
by each one of the preposition patterns.
pi en En desde Desde hacia Hacia
wj 15567 2381 1773 320 1336 134
Table 1: Extracted entities
The extracted capitalized words are passed
through a filtering process. Bigrams ?prepi
Capitalized wordj? with frequency lower than
20 were automatically discarded, because we
saw that this threshold removes words that do
not tend to appear very often with the lo-
cation prepositions. In this way misspelled
words as Bacelona instead of Barcelona were
filtered. From another side, every capitalized
word composed of two or three characters, for
instance ?La, Las? was initiated in a trigram
?prepi, Capitalized wordj , Capitalized wordj+1? val-
idation pattern. If these words were seen in com-
bination with other capitalized words and their tri-
gram frequency was higher then 20 they were in-
cluded in the location gazetteer file. With this tri-
gram validation pattern, locations as ?Los Ange-
les?, ?Las Palmas?, ?La Corun?a? ,?Nueva York?10
were extracted.
In total 16819 entities with no repetition were
automatically obtained. The words represent
countries around the world, European capitals and
mostly Spanish cities. Some noisy elements found
in the file were person names, which were accom-
panied by the preposition ?en?. As person names
were capitalized and had frequency higher than the
threshold we placed, it was impossible for these
names to be automatically detected as erroneous
and filtered. However we left these names, since
the gazetteer attributes we maintain are mutually
nonexclusive. This means the name ?Jordan? can
be seen in location gazetteer indicating the coun-
try Jordan and in the same time can be seen in the
person name list indicating the person Jordan. In
a real NE application such case is reality, but for
the determination of the right category name en-
tity disambiguation is needed as in (Pedersen et
al., 2005).
Person gazetteer is constructed with graph ex-
ploration algorithm. The graph consists of:
1. two kinds of nodes:
? First Names
? Family Names
10New York
2. undirected connections between First Names
and Family Names.
The graph connects Family Names with First
Names, and vice versa. In practice, such a graph is
not necessarily connected, as there can be unusual
first names and surnames which have no relation
with other names in the corpus. Though, the cor-
pus is supposed to contain mostly common names
in one and the same language, names from other
languages might be present too. In this case, if
the foreign name is not connected with a Spanish
name, it will never be included in the name list.
Therefore, starting from some common Span-
ish name will very probably place us in the largest
connected component11. If there exist other differ-
ent connected components in the graph, these will
be outliers, corresponding to names pertaining to
some other language, or combinations of both very
unusual first name and family name. The larger
the corpus is, the smaller the presence of such ad-
ditional connected components will be.
The algorithm performs an uninformed breadth-
first search. As the graph is not a tree, the stop
condition occurs when no more nodes are found.
Nodes and connections are found following the
pattern ?First name, Family name?. The node
from which we start the search can be a common
Spanish first or family name. In our example we
started from the Spanish common first name Jose?.
The notation ?i, j? ? C refers to finding in the
corpus C the regular expression12
[A-Z][a-z]* [A-Z][a-z]*
This regular expression indicates a possible rela-
tion between first name and family name. The
scheme of the algorithm is the following:
Let C be the corpus, F be the set of first names,
and S be the set of family names.
1. F = {?Jose??}
2. ?i ? F do
Snew = Snew ? {j} ,?j | ?i, j? ? C
3. S = S ? Snew
4. ?j ? S do
Fnew = Fnew ? {i} ,?i | ?i, j? ? C
11A connected component refers to a maximal connected
subgraph, in graph theory. A connected graph, is a graph
containing only one connected component.
12For Spanish some other characters have to be added to
the regular expression, such as n? and accents.
18
Manolo
Jose
Maria
Garcia
Martinez
Fernandez
John Lennon
Fir
st
Fa
mi
ly
Re
lat
ion
s
na
m
e
no
des
na
m
e
no
des
Co
nn
ec
te
d
Co
m
po
ne
nt
Co
nn
ec
te
d
Co
m
po
ne
nt
Figure 1: An example of connected components.
5. F = F ? Fnew
6. if (Fnew 6= ?) ? (Snew 6= ?)
then goto 2.
else finish.
Suppose we have a corpus containing the fol-
lowing person names: {?Jose? Garc??a?, ?Jose?
Mart??nez?, ?Manolo Garc??a?, ?Mar??a Mart??nez?,
?Mar??a Ferna?ndez?, ?John Lennon?} ? C.
Initially we have F = {?Jose??} and S = ?. Af-
ter the 3rd step we would have S = {?Garc??a?,
?Mart??nez?}, and after the 5th step: F = {?Jose??,
?Manolo?, ?Mar??a?}. During the next iteration
?Ferna?ndez? would also be added to S, as ?Mar??a?
is already present in F . Neither ?John?, nor
?Lennon? are connected to the rest of the names,
so these will never be added to the sets. This can
be seen in Figure 1 as well.
In our implementation, we filtered relations ap-
pearing less than 10 times. Thus rare combina-
tions like ?Jose Madrid, Mercedes Benz? are fil-
tered. Noise was introduced from names related to
both person and organization names. For example
the Spanish girl name Mercedes, lead to the node
Benz, and as ?Mercedes Benz? refers also to the
car producing company, noisy elements started to
be added through the node ?Benz?. In total 13713
fist names and 103008 surnames have been auto-
matically extracted.
We believe and prove that constructing auto-
matic location and person name gazetteer lists
with the pattern search and validation model we
propose is a very easy and practical task. With
our approach thousands of names can be obtained,
especially given the ample presence of unlabeled
data and the World Wide Web.
The purpose of our gazetteer construction was
not to make complete gazetteer lists, but rather
generate in a quick and automatic way lists of
names that can help during our feature construc-
tion module.
4 Experiments for delimitation process
In this section we describe the conducted exper-
iments for named entity detection. Previously
(Kozareva et al, 2005b) demonstrated that in su-
pervised learning only superficial features as con-
text and ortografics are sufficient to identify the
boundaries of a Named Entity. In our experiment
the superficial features f1 ? f10 were used by the
supervised and semi-supervised classifiers. Table
2 shows the obtained results for Begin and Inside
tags, which actually detect the entities and the total
BIO tag performance.
experiment B I BIO
Supervised 94.40 85.74 91.88
Bootstrapped 87.47 68.95 81.62
Table 2: F-score of detected entities.
On the first row are the results of the super-
vised method and on the second row are the high-
est results of the bootstrapping achieved in its
seventeenth iteration. For the supervised learn-
ing 91.88% of the entity boundaries were cor-
rectly identified and for the bootstrapping 81.62%
were correctly detected. The lower performance
of bootstrapping is due to the noise introduced dur-
ing the learning. Some examples were learned
with the wrong class and others didn?t introduce
new information in the training data.
Figure 2 presents the learning curve of the boot-
strapping processes for 25 iterations. On each it-
eration 1000 examples were tagged, but only the
examples having classes that coincide by the two
classifiers were later included in the training data.
We should note that for each iteration the same
amount of B, I and O classes was included. Thus
the balance among the three different classes in the
training data is maintained.
According to z? statistics (Dietterich, 1998),
the highest score reached by bootstrapping can-
not outperform the supervised method, however if
both methods were evaluated on small amount of
data the results were similar.
19
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
60
65
70
75
80
85
iterations
f?
sc
or
e
Figure 2: Bootstrapping performance
5 Experiments for classification process
In a Named Entity classification process, to the
previously detected Named Entities a predefined
category of interest such as name of person, orga-
nization, location or miscellaneous names should
be assigned. To obtain a better idea of the perfor-
mance of the classification methods, several exper-
iments were conducted. The influence of the au-
tomatically extracted gazetteers was studied, and a
comparison of the supervised and semi-supervised
methods was done.
experiment PER LOC ORG MISC
NoGazetteerSup. 80.98 71.66 73.72 49.94
GazetteerSup. 84.32 75.06 77.83 53.98
Bootstrapped 62.59 51.19 50.18 33.04
Table 3: F-score of classified entities.
Table 3 shows the obtained results for each one
of the experimental settings. The first row indi-
cates the performance of the supervised classifier
when no gazetteer information is present. The
classifier used f1, f2, f3, f4, f5, f6, f7, f8, f18,
f19, f20, f21 attributes. The performance of the
second row concerns the same classifier, but in-
cluding the gazetteer information by adding f22,
f23, f24 and f25 attributes. The third row relates to
the bootstrapping process. The attributes used for
the supervised and semi-supervised learning were
the same.
Results show that among all classes, miscella-
neous is the one with the lowest performance. This
is related to the heterogeneous information of the
category. The other three categories performed
above 70%. As expected gazetteer information
contributed for better distinction of person and lo-
cation names. Organization names benefitted from
the contextual information, the organization trig-
ger words and the attribute validating if an entity
is not a person or location then is treated as an
organization. Bootstrapping performance was not
high, due to the previously 81% correctly detected
named entity boundaries and from another side to
the training examples which were incorrectly clas-
sified and included into the training data.
In our experiment, unlabeled data was used to
construct in an easy and effective way person and
location gazetteer lists. By their help supervised
and semi-supervised classifiers improved perfor-
mance. Although one semi-supervised method
cannot reach the performance of a supervised clas-
sifier, we can say that results are promising. We
call them promising in the aspect of constructing
NE recognizer for languages with no resources or
even adapting the present Spanish Named Entity
system to other domain.
6 Conclusions and future work
In this paper we proposed and implemented a
pattern validation search in an unlabeled corpus
though which gazetteer lists were automatically
generated. The gazetteers were used as features
by a Named Entity Recognition system. The per-
formance of this NER system, when labeled and
unlabeled training data was available, was mea-
sured. A comparative study for the information
contributed by the gazetteers in the entity classifi-
cation process was shown.
In the future we intend to develop automatic
gazetteers for organization and product names. It
is also of interest to divide location gazetteers in
subcategories as countries, cities, rivers, moun-
tains as they are useful for Geographic Informa-
tion Retrieval systems. To explore the behavior
of named entity bootstrapping, other domains as
bioinformatics will be explored.
Acknowledgements Many thanks to the three
anonymous reviewers for their useful comments
and suggestions.
This research has been partially funded by the
Spanish Government under project CICyT number
TIC2003-0664-C02-02 and PROFIT number FIT-
340100-2004-14 and by the Valencia Government
under project numbers GV04B-276 and GV04B-
268.
20
References
Steven P. Abney. 2002. Bootstrapping. In Proceedings
of Association of Computational Linguists, pages
360?367.
Markus Becker, Ben Hachey, Beatrice Alex, and Claire
Grover. 2005. Optimising selective sampling for
bootstrapping named entity recognition. In Pro-
ceedings of the Workshop on Learning with Multiple
View, ICML, pages 5?10. Bonn, Germany.
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. Nymble: a high-performance
learning name-finder. In Proceedings of Conference
on Applied Natural Language Processing.
William J Black, Fabio Rinaldi, and David Mowatt.
1998. Facile: Description of the ne system used for
muc-7. In Proceedings of MUC-7.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University., September.
Xavier Carreras, Llu??s Ma`rques, and Llu??s Padro?.
2002. Named entity extraction using adaboost.
In Proceedings of CoNLL-2002, pages 167?170.
Taipei, Taiwan.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?.
2003. Named entity recognition for catalan us-
ing only spanish resources and unlabelled data. In
EACL, pages 43?50.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2003. Timbl: Tilburg
memory-based learner. Technical Report ILK 03-
10, Tilburg University, November.
Thomas G. Dietterich. 1998. Approximate statistical
test for comparing supervised classification learning
algorithms. Neural Computation, 10(7):1895?1923.
David Fisher, Stephen Soderland, Joseph McCarthy,
Fangfang Feng, and Wendy Lehnert. 1997. De-
scription of the umass system as used for muc-6. In
Proceedings of MUC-6.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Walter Daelemans
and Miles Osborne, editors, Proceedings of CoNLL-
2003, pages 180?183. Edmonton, Canada.
Zornitsa Kozareva, Boyan Bonev, and Andres Mon-
toyo. 2005a. Self-training and co-training for span-
ish named entity recognition. In 4th Mexican Inter-
national Conference on Artificial Intelligence, pages
770?780.
Zornitsa Kozareva, Oscar Ferra?ndez, Andres Montoyo,
and Rafael Mun?oz. 2005b. Using language re-
source independent detection for spanish named en-
tity recognition. In Proceedings of the Conference
on Recent Advances in Natural Language Process-
ing (RANLP 2005), pages 279?283.
Zornitsa Kozareva, Oscar Ferra?ndez, Andre?s Montoyo,
Rafael Mun?oz, and Armando Sua?rez. 2005c. Com-
bining data-driven systems for improving named en-
tity recognition. In NLDB, pages 80?90.
James Mayfield, Paul McNamee, and Christine Pi-
atko. 2003. Named entity recognition using hun-
dreds of thousands of features. In Walter Daelemans
and Miles Osborne, editors, Proceedings of CoNLL-
2003, pages 184?187. Edmonton, Canada.
Rada Mihalcea and Dan I. Moldovan. 2001. A highly
accurate bootstrapping algorithm for word sense dis-
ambiguation. International Journal on Artificial In-
telligence Tools, 10(1-2):5?21.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering sim-
ilar contexts. In Computational Linguistics and In-
telligent Text Processing, 6th International Confer-
ence, CICLing 2005, Mexico City, pages 226?237.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew-Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In Proceed-
ings of Association of Computational Linguists.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmon-
ton, Canada.
Erik F. Tjong Kim Sang. 2002a. Introduction to
the conll-2002 shared task: Language-independent
named entity recognition. In Proceedings of
CoNLL-2002, pages 155?158. Taipei, Taiwan.
Erik F. Tjong Kim Sang. 2002b. Memory-based
named entity recognition. In Proceedings of
CoNLL-2002, pages 203?206. Taipei, Taiwan.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003.
A stacked, voted, stacked model for named entity
recognition. In Walter Daelemans and Miles Os-
borne, editors, Proceedings of CoNLL-2003, pages
200?203. Edmonton, Canada.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Meet-
ing of the Association for Computational Linguis-
tics, pages 189?196.
21
Proceedings of ACL-08: HLT, pages 1048?1056,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semantic Class Learning from the Web with Hyponym Pattern Linkage
Graphs
Zornitsa Kozareva
DLSI, University of Alicante
Campus de San Vicente
Alicante, Spain 03080
zkozareva@dlsi.ua.es
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
hovy@isi.edu
Abstract
We present a novel approach to weakly super-
vised semantic class learning from the web,
using a single powerful hyponym pattern com-
bined with graph structures, which capture
two properties associated with pattern-based
extractions: popularity and productivity. In-
tuitively, a candidate is popular if it was dis-
covered many times by other instances in the
hyponym pattern. A candidate is productive
if it frequently leads to the discovery of other
instances. Together, these two measures cap-
ture not only frequency of occurrence, but also
cross-checking that the candidate occurs both
near the class name and near other class mem-
bers. We developed two algorithms that begin
with just a class name and one seed instance
and then automatically generate a ranked list
of new class instances. We conducted exper-
iments on four semantic classes and consis-
tently achieved high accuracies.
1 Introduction
Knowing the semantic classes of words (e.g., ?trout?
is a kind of FISH) can be extremely valuable for
many natural language processing tasks. Although
some semantic dictionaries do exist (e.g., Word-
Net (Miller, 1990)), they are rarely complete, espe-
cially for large open classes (e.g., classes of people
and objects) and rapidly changing categories (e.g.,
computer technology). (Roark and Charniak, 1998)
reported that 3 of every 5 terms generated by their
semantic lexicon learner were not present in Word-
Net. Automatic semantic lexicon acquisition could
be used to enhance existing resources such as Word-
Net, or to produce semantic lexicons for specialized
categories or domains.
A variety of methods have been developed for
automatic semantic class identification, under the
rubrics of lexical acquisition, hyponym acquisition,
semantic lexicon induction, semantic class learn-
ing, and web-based information extraction. Many
of these approaches employ surface-level patterns to
identify words and their associated semantic classes.
However, such patterns tend to overgenerate (i.e.,
deliver incorrect results) and hence require addi-
tional filtering mechanisms.
To overcome this problem, we employed one sin-
gle powerful doubly-anchored hyponym pattern to
query the web and extract semantic class instances:
CLASS NAME such as CLASS MEMBER and *.
We hypothesized that a doubly-anchored pattern,
which includes both the class name and a class
member, would achieve high accuracy because of
its specificity. To address concerns about coverage,
we embedded the search in a bootstrapping process.
This method produced many correct instances, but
despite the highly restrictive nature of the pattern,
still produced many incorrect instances. This re-
sult led us to explore new ways to improve the ac-
curacy of hyponym patterns without requiring addi-
tional training resources.
The main contribution of this work is a novel
method for combining hyponym patterns with graph
structures that capture two properties associated
with pattern extraction: popularity and productivity.
Intuitively, a candidate word (or phrase) is popular
if it was discovered many times by other words (or
1048
phrases) in a hyponym pattern. A candidate word is
productive if it frequently leads to the discovery of
other words. Together, these two measures capture
not only frequency of occurrence, but also cross-
checking that the word occurs both near the class
name and near other class members.
We present two algorithms that use hyponym pat-
tern linkage graphs (HPLGs) to represent popularity
and productivity information. The first method uses
a dynamically constructed HPLG to assess the pop-
ularity of each candidate and steer the bootstrapping
process. This approach produces an efficient boot-
strapping process that performs reasonably well, but
it cannot take advantage of productivity information
because of the dynamic nature of the process.
The second method is a two-step procedure that
begins with an exhaustive pattern search that ac-
quires popularity and productivity information about
candidate instances. The candidates are then ranked
based on properties of the HPLG. We conducted ex-
periments with four semantic classes, achieving high
accuracies and outperforming the results reported by
others who have worked on the same classes.
2 Related Work
A substantial amount of research has been done in
the area of semantic class learning, under a variety
of different names and with a variety of different
goals. Given the great deal of similar work in infor-
mation extraction and ontology learning, we focus
here only on techniques for weakly supervised or
unsupervised semantic class (i.e., supertype-based)
learning, since that is most related to the work in
this paper.
Fully unsupervised semantic clustering (e.g.,
(Lin, 1998; Lin and Pantel, 2002; Davidov and Rap-
poport, 2006)) has the disadvantage that it may or
may not produce the types and granularities of se-
mantic classes desired by a user. Another related
line of work is automated ontology construction,
which aims to create lexical hierarchies based on se-
mantic classes (e.g., (Caraballo, 1999; Cimiano and
Volker, 2005; Mann, 2002)), and learning semantic
relations such as meronymy (Berland and Charniak,
1999; Girju et al, 2003).
Our research focuses on semantic lexicon induc-
tion, which aims to generate lists of words that be-
long to a given semantic class (e.g., lists of FISH
or VEHICLE words). Weakly supervised learning
methods for semantic lexicon generation have uti-
lized co-occurrence statistics (Riloff and Shepherd,
1997; Roark and Charniak, 1998), syntactic in-
formation (Tanev and Magnini, 2006; Pantel and
Ravichandran, 2004; Phillips and Riloff, 2002),
lexico-syntactic contextual patterns (e.g., ?resides
in <location>? or ?moved to <location>?) (Riloff
and Jones, 1999; Thelen and Riloff, 2002), and
local and global contexts (Fleischman and Hovy,
2002). These methods have been evaluated only on
fixed corpora1, although (Pantel et al, 2004) demon-
strated how to scale up their algorithms for the web.
Several techniques for semantic class induction
have also been developed specifically for learning
from the web. (Pas?ca, 2004) uses Hearst?s pat-
terns (Hearst, 1992) to learn semantic class instances
and class groups by acquiring contexts around the
pattern. Pasca also developed a second technique
(Pas?ca, 2007b) that creates context vectors for a
group of seed instances by searching web query
logs, and uses them to learn similar instances.
The work most closely related to ours is Hearst?s
early work on hyponym learning (Hearst, 1992)
and more recent work that has followed up on her
idea. Hearst?s system exploited patterns that explic-
itly identify a hyponym relation between a seman-
tic class and a word (e.g., ?such authors as Shake-
speare?). We will refer to these as hyponym pat-
terns. Pasca?s previously mentioned system (Pas?ca,
2004) applies hyponym patterns to the web and ac-
quires contexts around them. The KnowItAll system
(Etzioni et al, 2005) also uses hyponym patterns to
extract class instances from the web and then evalu-
ates them further by computing mutual information
scores based on web queries.
The work by (Widdows and Dorow, 2002) on lex-
ical acquisition is similar to ours because they also
use graph structures to learn semantic classes. How-
ever, their graph is based entirely on syntactic rela-
tions between words, while our graph captures the
ability of instances to find each other in a hyponym
pattern based on web querying, without any part-of-
speech tagging or parsing.
1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated
on web pages, but used a precompiled corpus of downloaded
web pages.
1049
3 Semantic Class Learning with Hyponym
Pattern Linkage Graphs
3.1 A Doubly-Anchored Hyponym Pattern
Our work was motivated by early research on hy-
ponym learning (Hearst, 1992), which applied pat-
terns to a corpus to associate words with semantic
classes. Hearst?s system exploited patterns that ex-
plicitly link a class name with a class member, such
as ?X and other Ys? and ?Ys such as X?. Relying
on surface-level patterns, however, is risky because
incorrect items are frequently extracted due to poly-
semy, idiomatic expressions, parsing errors, etc.
Our work began with the simple idea of using an
extremely specific pattern to extract semantic class
members with high accuracy. Our expectation was
that a very specific pattern would virtually eliminate
the most common types of false hits that are caused
by phenomena such as polysemy and idiomatic ex-
pressions. A concern, however, was that an ex-
tremely specific pattern would suffer from sparse
data and not extract many new instances. By using
the web as a corpus, we hoped that the pattern could
extract at least a few instances for virtually any class,
and then we could gain additional traction by boot-
strapping these instances.
All of the work presented in this paper uses just
one doubly-anchored pattern to identify candidate
instances for a semantic class:
<class name> such as <class member> and *
This pattern has two variables: the name of the se-
mantic class to be learned (class name) and a mem-
ber of the semantic class (class member). The aster-
isk (*) indicates the location of the extracted words.
We describe this pattern as being doubly-anchored
because it is instantiated with both the name of the
semantic class as well as a class member.
For example, the pattern ?CARS such as FORD
and *? will extract automobiles, and the pattern
?PRESIDENTS such as FORD and *? will extract
presidents. The doubly-anchored nature of the pat-
tern serves two purposes. First, it increases the like-
lihood of finding a true list construction for the class.
Our system does not use part-of-speech tagging or
parsing, so the pattern itself is the only guide for
finding an appropriate linguistic context.
Second, the doubly-anchored pattern virtually
Members = {Seed};
P0= ?Class such as Seed and *?;
P = {P0};
iter = 0;
While ((iter < Max Iters) and (P 6= {}))
iter++;
For each Pi ? P
Snippets = web query(Pi);
Candidates = extract words(Snippets,Pi);
Pnew = {};
For each Candidatek ? Candidates
If (Candidatek /? Members);
Members = Members ? {Candidatek};
Pk= ?Class such as Candidatek and *?;
Pnew = Pnew ? { Pk };
P = Pnew;
Figure 1: Reckless Bootstrapping
eliminates ambiguity because the class name and
class member mutually disambiguate each other.
For example, the word FORD could refer to an auto-
mobile or a person, but in the pattern ?CARS such as
FORD and *? it will almost certainly refer to an au-
tomobile. Similarly, the class ?PRESIDENT? could
refer to country presidents or corporate presidents,
and ?BUSH? could refer to a plant or a person. But
in the pattern ?PRESIDENTS such as BUSH?, both
words will surely refer to country presidents.
Another advantage of the doubly-anchored pat-
tern is that an ambiguous or underspecified class
name will be constrained by the presence of the class
member. For example, to generate a list of com-
pany presidents, someone might naively define the
class name as PRESIDENTS. A singly-anchored pat-
tern (e.g., ?PRESIDENTS such as *?) might gener-
ate lists of other types of presidents (e.g., country
presidents, university presidents, etc.). Because the
doubly-anchored pattern also requires a class mem-
ber (e.g., ?PRESIDENTS such as BILL GATES and
*?), it is likely to generate only the desired types of
instances.
3.2 Reckless Bootstrapping
To evaluate the performance of the doubly-anchored
pattern, we began by using the pattern to search the
web and embedded this process in a simple boot-
strapping loop, which is presented in Figure 1. As
input, the user must provide the name of the desired
1050
semantic class (Class) and a seed example (Seed),
which are used to instantiate the pattern. On the
first iteration, the pattern is given to Google as a
web query, and new class members are extracted
from the retrieved text snippets. We wanted the
system to be as language-independent as possible,
so we refrained from using any taggers or parsing
tools. As a result, instances are extracted using only
word boundaries and orthographic information. For
proper name classes, we extract all capitalized words
that immediately follow the pattern. For common
noun classes, we extract just one word, if it is not
capitalized. Examples are shown below, with the ex-
tracted items underlined:
countries such as China and Sri Lanka are ...
fishes such as trout and bass can ...
One limitation is that our system cannot learn
multi-word instances of common noun categories,
or proper names that include uncapitalized words
(e.g., ?United States of America?). These limita-
tions could be easily overcome by incorporating a
noun phrase (NP) chunker and extracting NPs.
Each new class member is then used as a seed in-
stance in the bootstrapping loop. We implemented
this process as breadth-first search, where each ?ply?
of the search process is the result of bootstrapping
the class members learned during the previous it-
eration as seed instances for the next one. During
each iteration, we issue a new web query and add
the newly extracted class members to the queue for
the next cycle. We run this bootstrapping process for
a fixed number of iterations (search ply), or until no
new class members are produced. We will refer to
this process as reckless bootstrapping because there
are no checks of any kind. Every term extracted by
the pattern is assumed to be a class member.
3.2.1 Results
Table 1 shows the results for 4 iterations of reck-
less bootstrapping for four semantic categories: U.S.
states, countries, singers, and fish. The first two
categories are relatively small, closed sets (our gold
standard contains 50 U.S. states and 194 countries).
The singers and fish categories are much larger, open
sets (see Section 4 for details).
Table 1 reveals that the doubly-anchored pattern
achieves high accuracy during the first iteration, but
Iter. countries states singers fish
1 .80 .79 .91 .76
2 .57 .21 .87 .64
3 .21 .18 .86 .54
4 .16 ? .83 .54
Table 1: Reckless Bootstrapping Accuracies
quality deteriorates rapidly as bootstrapping pro-
gresses. Figure 2 shows the recall and precision
curves for countries and states. High precision is
achieved only with low levels of recall for countries.
Our initial hypothesis was that such a specific pat-
tern would be able to maintain high precision be-
cause non-class members would be unlikely to co-
occur with the pattern. But we were surprised to find
that many incorrect entries were generated for rea-
sons such as broken expressions like ?Merce -dez?,
misidentified list constructions (e.g., ?In countries
such as China U.S. Policy is failing...?), and incom-
plete proper names due to insufficient length of the
retrieved text snippet.
Incorporating a noun phrase chunker would elim-
inate some of these cases, but far from all of them.
We concluded that even such a restrictive pattern is
not sufficient for semantic class learning on its own.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Country/State
Country
State
Figure 2: Recall/precision for reckless bootstrapping
In the next section, we present a new approach
that creates a Hyponym Pattern Linkage Graph to
steer bootstrapping and improve accuracy.
3.3 Using Dynamic Graphs to Steer
Bootstrapping
Intuitively, we expect true class members to occur
frequently in pattern contexts with other class mem-
1051
bers. To operationalize this intuition, we create a hy-
ponym pattern linkage graph, which represents the
frequencies with which candidate instances generate
each other in the pattern contexts.
We define a hyponym pattern linkage graph
(HPLG) as a G = (V,E), where each vertex v ? V
is a candidate instance and each edge (u, v) ? E
means that instance v was generated by instance u.
The weight w of an edge is the frequency with which
u generated v. For example, consider the following
sentence, where the pattern is italicized and the ex-
tracted instance is underlined:
Countries such as China and Laos have been...
In the HPLG, an edge e = (China, Laos) would
be created because the pattern anchored by China
extracted Laos as a new candidate instance. If this
pattern extracted Laos from 15 different snippets,
then the edge?s weight would be 15. The in-degree
of a node represents its popularity, i.e., the number
of instance occurrences that generated it.
The graph is constructed dynamically as boot-
strapping progresses. Initially, the seed is the only
trusted class member and the only vertex in the
graph. The bootstrapping process begins by instan-
tiating the doubly-anchored pattern with the seed
class member, issuing a web query to generate new
candidate instances, and adding these new instances
to the graph. A score is then assigned to every node
in the graph, using one of several different metrics
defined below. The highest-scoring unexplored node
is then added to the set of trusted class members, and
used as the seed for the next bootstrapping iteration.
We experimented with three scoring functions for
selecting nodes. The In-Degree (inD) score for ver-
tex v is the sum of the weights of all incoming edges
(u, v), where u is a trusted class member. Intuitively,
this captures the popularity of v among instances
that have already been identified as good instances.
The Best Edge (BE) score for vertex v is the maxi-
mum edge weight among the incoming edges (u, v),
where u is a trusted class member.
The Key Player Problem (KPP) measure is used in
social network analysis (Borgatti and Everett, 2006)
to identify nodes whose removal would result in a
residual network of minimum cohesion. A node re-
ceives a high value if it is highly connected and rel-
atively close to most other nodes in the graph. The
KPP score for vertex v is computed as:
KPP (v) =
?
u?V
1
d(u, v)
|V |?1
where d(u, v) is the shortest path between two ver-
tices, where u is a trusted node. For tie-breaking, the
distances are multiplied by the weight of the edge.
Note that all of these measures rely only on in-
coming edges because a node does not acquire out-
going edges until it has already been selected as a
trusted class member and used to acquire new in-
stances. In the next section, we describe a two-step
process for creating graphs that can take advantage
of both incoming and outgoing edges.
3.4 Re-Ranking with Precompiled Graphs
One way to try to confirm (or disconfirm) whether
a candidate instance is a true class member is to see
whether it can produce new candidate instances. If
we instantiate our pattern with the candidate (i.e.,
?CLASS NAME such as CANDIDATE and *?) and
successfully extract many new instances, then this
is evidence that the candidate frequently occurs with
the CLASS NAME in list constructions. We will re-
fer to the ability of a candidate to generate new in-
stances as its productivity.
The previous bootstrapping algorithm uses a dy-
namically constructed graph that is constantly evolv-
ing as new nodes are selected and explored. Each
node is scored based only on the set of instances
that have been generated and identified as ?trusted?
at that point in the bootstrapping process. To use
productivity information, we must adopt a different
procedure because we need to know not only who
generated each candidate, but also the complete set
of instances that the candidate itself can generate.
We adopted a two-step process that can use both
popularity and productivity information in a hy-
ponym pattern linkage graph to assess the quality of
candidate instances. First, we perform reckless boot-
strapping for a class name and seed until no new
instances are generated. Second, we assign a score
to each node in the graph using a scoring function
that takes into account both the in-degree (popular-
ity) and out-degree (productivity) of each node. We
experimented with four different scoring functions,
some of which were motivated by work on word
1052
sense disambiguation to identify the most ?impor-
tant? node in a graph containing its possible senses
(Navigli and Lapata, 2007).
The Out-degree (outD) score for vertex v is the
weighted sum of v?s outgoing edges, normalized by
the number of other nodes in the graph.
outD(v) =
?
?(v,p)?E
w(v, p)
|V |?1
This measure captures only productivity, while the
next three measures consider both productivity and
popularity. The Total-degree (totD) score for ver-
tex v is the weighted sum of both incoming and
outgoing edges, normalized by the number of other
nodes in the graph. The Betweenness (BT) score
(Freeman, 1979) considers a vertex to be important
if it occurs on many shortest paths between other
vertices.
BT (v) =
?
s,t?V :s 6=v 6=t
?st(v)
?st
where ?st is the number of shortest paths from s to t,
and ?st(v) is the number of shortest paths from s to
t that pass through vertex v. PageRank (Page et al,
1998) establishes the relative importance of a ver-
tex v through an iterative Markov chain model. The
PageRank (PR) score of a vertex v is determined
on the basis of the nodes it is connected to.
PR(v) = (1??)|V | + ?
?
u,v?E
PR(u)
outdegree(u)
? is a damping factor that we set to 0.85. We dis-
carded all instances that produced zero productivity
links, meaning that they did not generate any other
candidates when used in web queries.
4 Experimental evaluation
4.1 Data
We evaluated our algorithms on four semantic cat-
egories: U.S. states, countries, singers, and fish.
The states and countries categories are relatively
small, closed sets: our gold standards consist of 50
U.S. states and 194 countries (based on a list found
on Wikipedia). The singers and fish categories are
much larger, open classes. As our gold standard for
fish, we used a list of common fish names found on
Wikipedia.2 All the singer names generated by our
2We also counted as correct plural versions of items found
on the list. The total size of our fish list is 1102.
States
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
25 1.0 1.0 1.0 1.0 1.0 .88 .88
50 .96 .98 .98 1.0 1.0 .86 .82
64 .77 .78 .77 .78 .78 .77 .67
Countries
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
50 .98 .97 .98 1.0 1.0 .98 .97
100 .96 .97 .94 1.0 .99 .97 .95
150 .90 .92 .91 1.0 .95 .94 .92
200 .83 .81 .83 .90 .87 .82 .80
300 .60 .59 .61 .61 .62 .56 .60
323 .57 .55 .57 .57 .58 .52 .57
Singers
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
10 .92 .96 .92 1.0 1.0 1.0 1.0
25 .89 .90 .91 1.0 1.0 1.0 .99
50 .92 .85 .92 .97 .98 .95 .97
75 .89 .83 .91 .96 .95 .93 .95
100 .86 .81 .89 .96 .93 .94 .94
150 .86 .79 .88 .95 .92 .93 .87
180 .86 .80 .87 .91 .91 .91 .88
Fish
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
10 .90 .90 .90 1.0 1.0 .90 .70
25 .80 .88 .76 1.0 .96 .96 .72
50 .82 .80 .78 1.0 .94 .88 .66
75 .72 .69 .72 .93 .87 .79 .64
100 .63 .68 .66 .84 .80 .74 .62
116 .60 .65 .66 .80 .78 .71 .59
Table 2: Accuracies for each semantic class
algorithms were manually reviewed for correctness.
We evaluated performance in terms of accuracy (the
percentage of instances that were correct).3
4.2 Performance
Table 2 shows the accuracy results of the two al-
gorithms that use hyponym pattern linkage graphs.
We display results for the top-ranked N candidates,
for all instances that have a productivity value >
zero.4 The Popularity columns show results for the
3We never generated duplicates so the instances are distinct.
4Obviously, this cutoff is not available to the popularity-
based bootstrapping algorithm, but here we are just comparing
the top N results for both algorithms.
1053
bootstrapping algorithm described in Section 3.3,
using three different scoring functions. The re-
sults for the ranking algorithm described in Sec-
tion 3.4 are shown in the Productivity (Prd) and
Popularity&Productivity (Pop&Prd) columns. For
the states, countries, and singers categories, we ran-
domly selected 5 different initial seeds and then av-
eraged the results. For the fish category we ran each
algorithm using just the seed ?salmon?.
The popularity-based metrics produced good ac-
curacies on the states, countries, and singers cate-
gories under all 3 scoring functions. For fish, KPP
performed better than the others.
The Out-degree (outD) scoring function, which
uses only Productivity information, obtained the
best results across all 4 categories. OutD achieved
100% accuracy for the first 50 states and fish, 100%
accuracy for the top 150 countries, and 97% accu-
racy for the top 50 singers. The three scoring met-
rics that use both popularity and productivity also
performed well, but productivity information by it-
self seems to perform better in some cases.
It can be difficult to compare the results of differ-
ent semantic class learners because there is no stan-
dard set of benchmark categories, so researchers re-
port results for different classes. For the state and
country categories, however, we can compare our
results with that of other web-based semantic class
learners such as Pasca (Pas?ca, 2007a) and the Know-
ItAll system (Etzioni et al, 2005). For the U.S.
states category, our system achieved 100% recall
and 100% precision for the first 50 items generated,
and KnowItAll performed similarly achieving 98%
recall with 100% precision. Pasca did not evaluate
his system on states.
For the countries category, our system achieved
100% precision for the first 150 generated instances
(77% recall). (Pas?ca, 2007a) reports results of 100%
precision for the first 25 instances generated, and
82% precision for the first 150 instances gener-
ated. The KnowItAll system (Etzioni et al, 2005)
achieved 97% precision with 58% recall, and 79%
precision with 87% recall.5 To the best of our
knowledge, other researchers have not reported re-
sults for the singer and fish categories.
5(Etzioni et al, 2005) do not report exactly how many coun-
tries were in their gold standard.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  50  100  150  200  250  300  350  400
Ac
cu
ra
cy
Iterations
outD
inD
cutoff, t
Figure 3: Learning curve for Placido Domingo
Figure 3 shows the learning curve for both al-
gorithms using their best scoring functions on the
singer category with Placido Domingo as the initial
seed. In total, 400 candidate words were generated.
The Out-degree scoring function ranked the candi-
dates well. Figure 3 also includes a vertical line
indicating where the candidate list was cut (at 180
instances) based on the zero productivity cutoff.
One observation is that the rankings do a good
job of identifying borderline cases, which typically
are ranked just below most correct instances but just
above the obviously bad entries. For example, for
states, the 50 U.S. states are ranked first, followed
by 14 more entries (in order):
Russia, Ukraine, Uzbekistan, Azerbaijan,
Moldova, Tajikistan, Armenia, Chicago,
Boston, Atlanta, Detroit, Philadelphia, Tampa,
Moldavia
The first 7 entries are all former states of the So-
viet Union. In retrospect, we realized that we
should have searched for ?U.S. states? instead of just
?states?. This example illustrates the power of the
doubly-anchored hyponym pattern to correctly iden-
tify our intended semantic class by disambiguating
our class name based on the seed class member.
The algorithms also seem to be robust with re-
spect to initial seed choice. For the states, coun-
tries, and singers categories, we ran experiments
with 5 different initial seeds, which were randomly
selected. The 5 country seeds represented a diverse
set of nations, some of which are rarely mentioned in
the news: Brazil, France, Guinea-Bissau, Uganda,
1054
and Zimbabwe. All of these seeds obtained ? 92%
recall with ? 90% precision.
4.3 Error Analysis
We examined the incorrect instances produced by
our algorithms and found that most of them fell into
five categories.
Type 1 errors were caused by incorrect proper
name extraction. For example, in the sentence
?states such as Georgia and English speaking coun-
tries like Canada...?, ?English? was extracted as
a state. These errors resulted from complex noun
phrases and conjunctions, as well as unusual syn-
tactic constructions. An NP chunker might prevent
some of these cases, but we suspect that many of
them would have been misparsed regardless.
Type 2 errors were caused by instances that for-
merly belonged to the semantic class (e.g., Serbia-
Montenegro and Czechoslovakia are no longer coun-
tries). In this error type, we also include border-
line cases that could arguably belong to the semantic
class (e.g., Wales as a country).
Type 3 errors were spelling variants (e.g., Kyrgys-
tan vs. Kyrgyzhstan) and name variants (e.g., Bey-
once vs. Beyonce Knowles). Officially, every entity
has one official spelling and one complete name, but
in practice there are often variations that may occur
nearly as frequently as the official name. For exam-
ple, it is most common to refer to the singer Beyonce
by just her first name.
Type 4 errors were caused by sentences that were
just flat out wrong in their factual assertions. For ex-
ample, some sentences referred to ?North America?
as a country.
Type 5 errors were caused by broken expressions
found in the retrieved snippets (e.g. Michi -gan).
These errors may be fixable by cleaning up the web
pages or applying heuristics to prevent or recognize
partial words.
It is worth noting that incorrect instances of Types
2 and 3 may not be problematic to encounter in a
dictionary or ontology. Name variants and former
class members may in fact be useful to have.
5 Conclusions
Combining hyponym patterns with pattern linkage
graphs is an effective way to produce a highly ac-
curate semantic class learner that requires truly min-
imal supervision: just the class name and one class
member as a seed. Our results consistently produced
high accuracy and for the states and countries cate-
gories produced very high recall.
The singers and fish categories, which are much
larger open classes, also achieved high accuracy and
generated many instances, but the resulting lists are
far from complete. Even on the web, the doubly-
anchored hyponym pattern eventually ran out of
steam and could not produce more instances. How-
ever, all of our experiments were conducted using
just a single hyponym pattern. Other researchers
have successfully used sets of hyponym patterns
(e.g., (Hearst, 1992; Etzioni et al, 2005; Pas?ca,
2004)), and multiple patterns could be used with
our algorithms as well. Incorporating additional hy-
ponym patterns will almost certainly improve cover-
age, and could potentially improve the quality of the
graphs as well.
Our popularity-based algorithm was very effec-
tive and is practical to use. Our best-performing al-
gorithm, however, was the 2-step process that be-
gins with an exhaustive search (reckless bootstrap-
ping) and then ranks the candidates using the Out-
degree scoring function, which represents produc-
tivity. The first step is expensive, however, because
it exhaustively applies the pattern to the web until
no more extractions are found. In our evaluation, we
ran this process on a single PC and it usually finished
overnight, and we were able to learn a substantial
number of new class instances. If more hyponym
patterns are used, then this could get considerably
more expensive, but the process could be easily par-
allelized to perform queries across a cluster of ma-
chines. With access to a cluster of ordinary PCs,
this technique could be used to automatically create
extremely large, high-quality semantic lexicons, for
virtually any categories, without external training re-
sources.
Acknowledgments
This research was supported in part by the Department
of Homeland Security under ONR Grants N00014-07-1-014
and N0014-07-1-0152, the European Union Sixth Framework
project QALLME FP6 IST-033860, and the Spanish Ministry
of Science and Technology TEXT-MESS TIN2006-15265-C06-
01.
1055
References
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proc. of the 37th Annual Meeting of
the Association for Computational Linguistics.
S. Borgatti and M. Everett. 2006. A graph-theoretic per-
spective on centrality. Social Networks, 28(4).
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proc. of Recent Advances in Natural Lan-
guage Processing, pages 166?172.
D. Davidov and A. Rappoport. 2006. Efficient unsu-
pervised discovery of word categories using symmet-
ric patterns and high frequency words. In Proc. of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the ACL.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1?7.
C. Freeman. 1979. Centrality in social networks: Con-
ceptual clarification. Social Networks, 1:215?239.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In Proc. of Conference of HLT /
North American Chapter of the Association for Com-
putational Linguistics.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th confer-
ence on Computational linguistics, pages 539?545.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proc. of the 19th International Conference on Com-
putational linguistics, pages 1?7.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of the 17th international confer-
ence on Computational linguistics, pages 768?774.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In Proc. of the 19th International
Conference on Computational Linguistics, pages 1?7.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
R. Navigli and M. Lapata. 2007. Graph connectiv-
ity measures for unsupervised word sense disambigua-
tion. In Proc. of the 20th International Joint Confer-
ence on Artificial Intelligence, pages 1683?1688.
M. Pas?ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137?145.
M. Pas?ca. 2007a. Organizing and searching the world
wide web of facts ? step two: harnessing the wisdom
of the crowds. In Proc. of the 16th International Con-
ference on World Wide Web, pages 101?110.
M. Pas?ca. 2007b. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
the sixteenth ACM conference on Conference on infor-
mation and knowledge management, pages 683?690.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Library Tech-
nologies Project.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In Proc. of Conference of
HLT / North American Chapter of the Association for
Computational Linguistics, pages 321?328.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. To-
wards terascale knowledge acquisition. In Proc. of the
20th international conference on Computational Lin-
guistics, page 771.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proc. of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proc. of the Sixteenth National Conference on Arti-
ficial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proc. of
the Second Conference on Empirical Methods in Nat-
ural Language Processing, pages 117?124.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proc. of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1110?1116.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proc. of 11st
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proc. of the 2002 Conference on Em-
pirical Methods in Natural Language Processing.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1?7.
1056
Improving Name Discrimination: A Language Salad Approach
Ted Pedersen and Anagha Kulkarni
Department of Computer Science
University of Minnesota, Duluth
Duluth, MN 55812 USA
{tpederse,kulka020}@d.umn.edu
Roxana Angheluta
Attentio SA
B-1030 Brussels, Belgium
roxana@attentio.com
Zornitsa Kozareva
Dept. de Lenguajes y Sistemas Informa?ticos
University of Alicante
03080 Alicante, Spain
zkozareva@dlsi.ua.es
Thamar Solorio
Department of Computer Science
University of Texas at El Paso
El Paso, TX 79902 USA
tsolorio@utep.edu
Abstract
This paper describes a method of discrim-
inating ambiguous names that relies upon
features found in corpora of a more abun-
dant language. In particular, we discrim-
inate ambiguous names in Bulgarian, Ro-
manian, and Spanish corpora using infor-
mation derived from much larger quan-
tities of English data. We also mix to-
gether occurrences of the ambiguous name
found in English with the occurrences of
the name in the language in which we are
trying to discriminate. We refer to this as
a language salad, and find that it often re-
sults in even better performance than when
only using English or the language itself
as the source of information for discrimi-
nation.
1 Introduction
Name ambiguity is a problem that is increasing
in complexity and scope as online information
sources grow and expand their coverage. Like
words, names are often ambiguous and can refer
to multiple underlying entities or concepts. Web
searches for names can often return results asso-
ciated with multiple people or organizations in a
disorganized and unclear fashion. For example,
the top 10 results of a Google search for George
Miller includes a mixture of entries for two dif-
ferent entities, one a psychology professor from
Princeton University and the other the director of
the film Mad Max.1
Name discrimination takes some number of
contexts that include an ambiguous name, and di-
vides them into groups or clusters, where the con-
1Search conducted January 4, 2006.
texts in each cluster should ideally refer to the
same underlying entity (and each cluster should
refer to a different entity). Thus, if we are given
10,000 contexts that include the name John Smith,
we would want to divide those contexts into clus-
ters corresponding to each of the different under-
lying entities that share that name.
We have developed an unsupervised method of
name discrimination (Pedersen et al, 2005). We
have shown the method to be language indepen-
dent (Pedersen et al, 2006), which is to say we
can apply it to English contexts as easily as we
can apply it to Romanian or French. However,
we have observed that there are situations where
the number of contexts in which an ambiguous
name occurs is relatively small, perhaps because
the name itself is unusual, or because the quantity
of data available for language is limited in general.
These problems of scarcity can make it difficult to
apply these methods and discriminate ambiguous
names, especially in languages with fewer online
resources.
This paper presents a method of name discrim-
ination is based on using a larger number of con-
texts in English that include an ambiguous name,
and applying information derived from these con-
texts to the discrimination of that name in another
language, where there are many fewer contexts.
We also show that mixing English contexts with
the contexts to be discriminated can result in a
performance improvement over only using the En-
glish or the original contexts alone.
2 Discrimination by Clustering Contexts
Our method of name discrimination is described in
more detail in (Pedersen et al, 2005), but in gen-
eral is based on an unsupervised approach to word
sense discrimination introduced by (Purandare and
25
Pedersen, 2004), which builds upon earlier work
in word sense discrimination, including (Schu?tze,
1998) and (Pedersen and Bruce, 1997).
Our method treats each occurrence of an am-
biguous name as a context that is to be clustered
with other contexts that also include the same
name. In this paper, each context consists of about
50 words, where the ambiguous name is generally
in the middle of the context. The goal is to cluster
similar contexts together, based on the presump-
tion that the occurrences of a name that appear
in similar contexts will refer to the same underly-
ing entity. This approach is motivated by both the
distributional hypothesis (Harris, 1968) and the
strong contextual hypothesis (Miller and Charles,
1991).
2.1 Feature Selection
The contexts to be clustered are represented by
lexical features which may be selected from either
the contexts being clustered, or from a separate
corpus. In this paper we use both approaches. We
cluster the contexts based on features identified in
those very same contexts, and we also cluster the
contexts based on features identified in a separate
set of data (in this case English). We explore the
use of a mixed feature selection strategy where we
identify features both from the data to be clustered
and the separate corpus of English text. Thus, our
feature selection data may come from one of three
sources: the contexts to be clustered (which we
will refer to as the evaluation contexts), English
contexts which include the same name but are not
to be clustered, and the combination of these two
(our so-called Language Salad or Mix).
The lexical features we employ are bigrams,
that is consecutive words that occur together in the
corpora from which we are identifying features. In
this work we identify bigram features using Point-
wise Mutual Information (PMI). This is defined as
the log of the ratio of the observed frequency with
which the two words occur together in the feature
selection data, to the expected number of times
the two words would occur together in a corpus if
they were independent. This expected value is es-
timated simply by taking the product of the num-
ber of times the two words occur individually, and
dividing this by the total number of bigrams in the
feature selection data. Thus, larger values of PMI
indicate that the observed frequency of the bigram
is greater than would be expected if the two words
were independent.
In these experiments we take the top 500 ranked
bigrams that occur five or more times in the feature
selection data. We also exclude any bigram from
consideration that is made up of one or two stop
words, which are high frequency function words
that have been specified in a manually created list.
Note that with smaller numbers of contexts (usu-
ally 200 or fewer), we lower the frequency thresh-
old to two or more.
In general PMI is known to have a bias towards
pairs of words (bigrams) that occur a small num-
ber of times and only with each other. In this work
that is a desirable quality, since that will tend to
identify pairs of words that are very strongly as-
sociated with each other and also provide unique
discriminating information.
2.2 Context Representation
Once the bigram features have been identified,
then the contexts to be clustered are represented
using second order co-occurrences that are de-
rived from those bigrams. In general a second
order co-occurrence is a pair of words that may
not occur with each other, but that both occur fre-
quently with a third word. For example, garden
and fire may not occur together often, but both
commonly occur with hose. Thus, garden hose
and fire hose represent first order co?occurrences,
and garden and fire represent a second order co?
occurrence.
The process of creating the second order repre-
sentation has several steps. First, the bigram fea-
tures identified by PMI (the top ranked 500 bi-
grams that have occurred 5 or more times in the
feature selection data) are used to create a word
by word co?occurrence matrix. The first word in
each bigram represents a row in the matrix, and the
second word in each bigram represents a column.
The cells in the matrix contain the PMI scores.
Note that this matrix is not symmetric, and that
there are many words that only occur in either a
row or a column (and not both) because they tend
to occur as the first or second word in a bigram.
For example, President might tend to be a first
word in a bigram (e.g., President Clinton, Presi-
dent Putin), whereas last names will tend to be the
second word.
Once the co?occurrence matrix is created, then
the contexts to be clustered can be represented.
Each word in the context is checked to see if it
26
has a corresponding row (i.e., vector) in the co?
occurrence matrix. If it does, that word is replaced
in the context by the row from the matrix, so that
the word in the context is now represented by the
vector of words with which it occurred in the fea-
ture selection data. If a word does not have a corre-
sponding entry in the co?occurrence matrix, then
it is simply removed from the context. After all
the words in the context are checked, then all of
the vectors that are selected are averaged together
to create a vector representation of the context.
Then these contexts are clustered into a pre?
specified number of clusters using the k?means
algorithm. Note that we are currently develop-
ing methods to automatically select the number of
clusters in the data (e.g., (Pedersen and Kulkarni,
2006)), although we have not yet applied them to
this particular work.
3 The Language Salad
In this paper, we explore the creation of a second
order representation for a set of evaluation con-
texts using three different sets of feature selection
data. The co?occurrence matrix may be derived
from the evaluation contexts themselves, or from
a separate set of contexts in a different language,
or from the combination of these two (the Salad or
Mix).
For example, suppose we have 100 Romanian
evaluation contexts that include an ambiguous
name, and that same name also occurs 10,000
times in an English language corpus.2 Our goal
is to cluster the 100 Romanian contexts, which
contain all the information that we have about the
name in Romanian. While we could derive a sec-
ond order representation of the contexts, the re-
sulting co?occurrence matrix would likely be very
small and sparse, and insufficient for making good
discrimination decisions. We could instead rely
on first order features, that is look for frequent
words or bigrams that occur in the evaluation con-
texts, and try and find evaluation contexts that
share some of the same words or phrases, and clus-
ter them based on this type of information. How-
ever, again, the small number of contexts available
would likely result in very sparse representations
for the contexts, and unreliable clustering results.
Thus, our method is to derive a co?occurrence
matrix from a language for which we have many
2We assume that the names either have the same spelling
in both languages, or that translations are readily available.
occurrences of the ambiguous name, and then use
that co?occurrence matrix to represent the evalua-
tion contexts. This relies on the fact that the eval-
uation contexts will contain at least a few names
or words that are also used in the larger corpus (in
this case English). In general, we have found that
while this is not always true, it is often the case.
We have also experimented with combining the
English contexts with the evaluation contexts, and
building a co?occurrence matrix based on this
combined or mixed collection of contexts. This
is the language salad that we refer to, a mixture of
contexts in two different languages that are used to
derive a representation of the evaluation contexts.
4 Experimental Data
We use data in four languages in these experi-
ments, Bulgarian, English, Romanian, and Span-
ish.
4.1 Raw Corpora
The Romanian data comes from the 2004 archives
of the newspaper Adevarul (The Truth)3. This is a
daily newspaper that is among the most popular in
Romania. While Romanian normally has diacrit-
ical markings, this particular newspaper does not
include those in their online edition, so the alpha-
bet used was the same as English.
The Bulgarian data is from the Sega 2002 news
corpus, which was originally prepared for the
CLEF competition.4 This is a corpus of news arti-
cles from the Newspaper Sega5, which is based in
Sofia, Bulgaria. The Bulgarian text was translit-
erated (phonetically) from Cyrillic to the Roman
alphabet. Thus, the alphabet used was the same
as English, although the phonetic transliteration
leads to fewer cognates and borrowed English
words that are spelled exactly the same as in En-
glish text.
The Spanish corpora comes from the Spanish
news agency EFE from the year 1994 and 1995.
This collection was used in the Question Answer-
ing Track at CLEF-2003, and also for CLEF-2005.
This text is represented in Latin-1, and includes
the usual accents that appear in Spanish.
The English data comes from the GigaWord
corpus (2nd edition) that is distributed by the Lin-
guistic Data Consortium. This consists of more
3http://www.adevarulonline.ro/arhiva
4http://www.clef-campaign.org
5http://www.segabg.com
27
than 2 billion words of newspaper text that comes
from five different news sources between the years
1994 and 2004. In fact, we subdivide the English
data into three different corpora, where one is from
2004, another from 2002, and the third from 1994-
95, so that for each of the evaluation languages
(Bulgarian, Spanish, and Romanian) we have an
English corpus from the same time period.
4.2 Evaluation Contexts
Our experimental data consists of evaluation con-
texts derived from the Bulgarian, Romanian, and
Spanish corpora mentioned above. We also have
English corpora that includes the same ambiguous
names as found in the evaluation contexts.
In order to quickly generate a large volume of
experimental data, we created evaluation contexts
from the corpora for each of our four languages
by conflating together pairs of well known names
or places, and that are generally not highly am-
biguous (although some might be rather general).
For example, one of the pairs of names we con-
flate is George Bush and Tony Blair. To do that,
every occurrence of both of these names is con-
verted to an ambiguous form (GB TB, for exam-
ple), and the discrimination task is to cluster these
contexts such that their original and correct name
is re?discovered. We retain a record of the orig-
inal name for each occurrence, so as to evaluate
the results of our method. Of course we do not use
this information anywhere in the process outside
of evaluation.
The following pairs of names were conflated in
all four of the languages: George Bush-Tony Blair,
Mexico-India, USA-Paris, Ronaldo-David Beck-
ham (2002 and 2004), Diego Maradona-Roberto
Baggio (1994-95 only), and NATO-USA. Note
that some of these names have different spellings
in some of our languages, so we look for and con-
flate the native spelling of the names in the differ-
ent language corpora. These pairs were selected
because they occur in all four of our languages,
and they represent name distinctions that are com-
monly of interest, that is they represent ambiguity
in names of people and places. With these pairs
we are also following (Nakov and Hearst, 2003)
who suggest that if one is introducing ambiguity
by creating pseudo?words or conflating names,
then these words should be related in some way
(in order to avoid the creation of very sharp or ob-
vious sense distinctions).
4.3 Discussion
For each of the three evaluation languages (Bul-
garian, Romanian, and Spanish) we have contexts
for five different name conflate pairs that we wish
to discriminate. We have corresponding English
contexts for each evaluation language, where the
dates of both are approximately the same. This
temporal consistency between the evaluation lan-
guage and English is important because the con-
texts in which a name is used may change over
time. In 1994, for example, Tony Blair was not
yet Prime Minister of England (he became PM in
1997), and references to George Bush most likely
refer to the US President who served from 1988
until 1992, rather than the current US President
(who began his term in office in 2001). In 1994
the current (as of 2006) US President had just been
elected governor of Texas, and was not yet a na-
tional figure. This points out that George Bush is
an example of an ambiguous name, but our ob-
servation has been that in the 2002 and 2004 data
(Romanian and Bulgarian) nearly all occurrences
are associated with the current president, and that
most of the occurrences in 1994-95 (Spanish) re-
fer to the former US President. This illustrates
an important point: it is necessary to consider the
perspective represented by the different corpora.
There is little reason to expect that news articles
from Spain in 1994 and 1995 would focus much
attention on the newly elected governor of Texas
in the United States.
Tables 1, 2, and 3 show the number of contexts
that have been collected for each name conflate
pair. For example, in Table 1 we see that there are
746 Bulgarian contexts that refer to either Mex-
ico or India, and that of these 51.47% truly re-
fer to Mexico, and 48.53% to India. There are
149,432 English contexts that mention Mexico or
India, and the Mix value shown is simply the sum
of the number of Bulgarian and English contexts.
In general these tables show that the English
contexts are much larger in number, however,
there are a few exceptions with the Spanish data.
This is because the EFE corpus is relatively large
as compared to the Bulgarian and Romanian cor-
pora, and provides frequency counts that are in
some cases comparable to those in the English cor-
pus.
28
5 Experimental Methodology
For each of the three evaluation languages (Bul-
garian, Romanian, Spanish) there are five name
conflate pairs. The same name conflate pairs
are used for all three languages, except for
Diego Maradona-Roberto Baggio which is only
used with Spanish, and Ronaldo-David Beckham,
which is only used with Bulgarian and Romanian.
This is due to the fact that in 1994-95 (the era
of the Spanish data) neither Ronaldo nor David
Beckham were as famous as they later became, so
they were mentioned somewhat less often than in
the 2002 and 2004 corpora. The other four name
conflate pairs are used in all of the languages.
For each name conflate pair we create a second
order representation using three different sources
of features selection data: the evaluation contexts
themselves, the corresponding English contexts,
and then the mix of the evaluation contexts and the
English contexts (the Mix). The objective of these
experiments is to determine which of these sources
of feature selection data results in the highest F-
Measure, which is the harmonic mean of the pre-
cision and recall of an experiment.
The precision of each experiment is the num-
ber of evaluation contexts clustered correctly, di-
vided by the number of contexts that are clustered.
The clustering algorithm may choose not to assign
every context to a cluster, which is why that de-
nominator may not be the same as the number of
evaluation contexts. The recall of each experiment
is the the number of correctly clustered evaluation
contexts divided by the total number of evaluation
contexts. Note that for each of the three variations
for each name conflate pair experiment exactly the
same evaluation language contexts are being dis-
criminated, all that is changing in each experiment
is the source of the feature selection data. Thus the
F-measures for a name conflate pair in a particular
language can be compared directly. Note however
that the F-measures across languages are harder to
compare directly, since different evaluation con-
texts are used, and different English contexts are
used as well.
There is a simple baseline that can be used as a
point of comparison, and that is to place all of the
contexts for each name conflate pair into one clus-
ter, and say that there is no ambiguity. If that is
done, then the resulting F-Measure will be equal
to the majority percentage of the true underlying
entity as shown in Tables 1, 2, and 3. For exam-
ple, for Bulgarian, if the 746 Bulgarian contexts
for Mexico and India are all put into the same clus-
ter, the resulting F-Measure would be 51.47%, be-
cause we would simply assign all the contexts in
the cluster to the more common of the two entities,
which is Mexico in this case.
6 Experimental Results
Tables 1, 2, and 3 show the results for our exper-
iments, language by language. Each table shows
the results for the 15 experiments done for each
language: five name conflate pairs, each with
three different sources of feature selection data.
The row labeled with the name of the evalua-
tion language reports the F-Measure for the eval-
uation contexts (whose number of occurrences is
shown in the far right column) when the fea-
ture selection data is the evaluation contexts them-
selves. The rows labeled English and Mix report
the F-Measures obtained for the evaluation con-
texts when the feature selection data is the English
contexts, or the Mix of the English and evaluation
contexts.
6.1 Bulgarian Results
The Bulgarian results are shown in Table 1. Note
that the number of contexts for English is consid-
erably larger than for Bulgarian for all five name
conflate pairs. The Bulgarian and English data
came from 2002 news reports.
The Mix of feature selection data results in the
best performance for three of the five name con-
flate pairs: George Bush - Tony Blair, Ronaldo -
David Beckham, and NATO - USA. For remain-
ing two name conflate pairs, just using the Bul-
garian evaluation contexts results in the highest F-
Measure (Mexico-India, USA-Paris).
We believe that this may be partially due to the
fact that the two cases where Bulgarian leads to the
best results are for very general or generic underly-
ing entities: Mexico and India, and then the USA
and Paris. In both cases, contexts that mention
these entities could be discussing a wide range of
topics, and the larger volumes of English data may
simply overwhelm the process with a huge num-
ber of second order features. In addition, it may
be that the English and Bulgarian corpora contain
different content that reflects the different interests
of the original readership of this text. For example,
news that is reported about India might be rather
different in the United States (the source of most
29
Table 1: Bulgarian Results (2002): Feature Selec-
tion Data, F-Measure, and Number of Contexts
George Bush (73.43) - Tony Blair (26.57)
Mix 68.37 11,570
Bulgarian 55.78 651
English 36.15 10,919
Mexico (51.47) - India (48.53)
Bulgarian 70.97 746
Mix 55.01 150,178
English 48.15 149,432
USA (79.53) - Paris (20.47)
Bulgarian 58.67 3,283
Mix 51.68 56,044
English 49.66 52,761
Ronaldo (61.25) - David Beckham (38.75)
Mix 64.88 8,649
Bulgarian 52.75 320
English 48.11 8,329
NATO (87.37) - USA (12.63)
Mix 75.44 54,193
Bulgarian 65.92 3,770
English 60.44 50,423
of the English data) than in Bulgaria. Thus, the
use of the English corpora might not have been
as helpful in those cases where the names to be
discriminated are more global figures. For exam-
ple, Tony Blair and George Bush are probably in
the news in the USA and Bulgaria for many of the
same reasons, thus the underlying content is more
comparable than that of the more general entities
(like Mexico and India) that might have much dif-
ferent content associated with them.
We observed that Bulgarian tends to have fewer
cognates or shared names with English than do
Romanian and English. This is due to the fact
that the Bulgarian text is transliterated. This may
account for the fact that the English-only results
for Bulgarian are very poor, and it is only in com-
bination with the Bulgarian contexts that the En-
glish contexts show any positive effect. This sug-
gests that there are only a few words in the Bulgar-
ian contexts that also occur in English, but those
that do have a positive impact on clustering per-
formance.
6.2 Romanian Results
The Romanian results are shown in Table 2. The
Romanian and English contexts come from 2004.
Table 2: Romanian Results (2004): Feature Selec-
tion Data, F-Measure, and Number of Contexts
Tony Blair (72.00) - George Bush (28.00)
English 64.23 11,616
Mix 54.31 11,816
Romanian 50.75 200
India (53.66) - Mexico (46.34)
Romanian 50.93 82
English 47.30 88,247
Mix 42.55 88,329
USA (60.29) - Paris (39.71)
English 59.05 45,346
Romanian 58.76 700
Mix 57.91 46,046
David Beckham (55.56) - Ronaldo (44.44)
Mix 81.00 4,365
English 70.85 4,203
Romanian 52.47 162
NATO (58.05) - USA (41.95)
Mix 60.48 43,508
Romanian 51.20 1,168
English 38.91 42,340
The Mix of Romanian and English contexts for
feature selection results in improvements for two
of the five pairs (David Beckham - Ronaldo, and
NATO - USA). The use of English contexts only
provides the best results for two other pairs (Tony
Blair - George Bush, and USA - Paris, although in
the latter case the difference in the F-Measures that
result from the three sources of data is minimal).
There is one case (Mexico-India) where using the
Romanian contexts as feature selection data re-
sults in a slightly better F-measure than when us-
ing English contexts.
The improvement that the Mix shows for David
Beckham-Ronaldo is significant, and is perhaps
due to fact that in both English and Romanian text,
the content about Beckham and Ronaldo is simi-
lar, making it more likely that the mix of English
and Romanian contexts will be helpful. However,
it is also true that the Mix results in a significant
improvement for NATO-USA, and it seems likely
that the local perspective in Romania and the USA
would be somewhat different on these two entities.
However, NATO-USA has a relatively large num-
ber of contexts in Romanian as well as English, so
perhaps the difference in perspective had less of
an impact in those cases where the number of Ro-
30
Table 3: Spanish Results (1994-95): Feature Se-
lection Data, F-Measure, and Number of Contexts
George Bush (75.58) - Tony Blair (24.42)
Mix 78.59 2,353
Spanish 64.45 1,163
English 54.29 1,190
D. Maradona (51.55) - R. Baggio (48.45)
English 67.65 1,588
Mix 61.35 3,594
Spanish 60.70 2,006
India (92.34) - Mexico (7.66)
English 72.76 19,540
Spanish 66.57 2,377
Mix 61.54 21,917
USA (62.30) - Paris (37.70)
Spanish 69.31 1,000
English 64.30 17,344
Mix 59.40 18,344
NATO (63.86) - USA (36.14)
Spanish 62.04 2,172
Mix 58.47 27,426
English 56.00 25,254
manian contexts is much smaller (as is the case for
Beckham and Ronaldo).
6.3 Spanish Results
The Spanish results are shown in Table 3. The
Spanish and English contexts come from 1994-
1995, which puts them in a slightly different his-
torical era than the Bulgarian and Romanian cor-
pora.
Due to this temporal difference, we used Diego
Maradona and Roberto Baggio as a conflated pair,
rather than David Beckham and Ronaldo, who
were much younger and somewhat less famous at
that time. Also, Ronaldo is a highly ambiguous
name in Spanish, as it is a very common first name.
This is true in English text as well, although casual
inspection of the English text from 2002 and 2004
(where the Ronaldo-Beckham pair was included
experimentally) reveals that Ronaldo the soccer
player tends to occur more so than any other single
entity named Ronaldo, so while there is a bit more
noise for Ronaldo, there is not really a significant
ambiguity.
For the Spanish results we only note one pair
(George Bush - Tony Blair) where the Mix of En-
glish and Spanish results in the best performance.
This again suggests that the perspective of the
Spanish and English corpora were similar with re-
spect to these entities, and their combination was
helpful. In two other cases (Maradona-Baggio,
India-Mexico) English only contexts achieve the
highest F-Measure, and then in the two remaining
cases (USA-Paris, NATO-USA) the Spanish con-
texts are the best source of features.
Note that for Spanish we have reasonably large
numbers of contexts (as compared to Bulgarian
and Romanian). Given that, it is especially inter-
esting that English-only contexts are the most ef-
fective in two of five cases. This suggests that this
approach may have merit even when the evalua-
tion language does not suffer from problems of ex-
treme scarcity. It may simply be that the informa-
tion in the English corpora provides more discrim-
inating information than does the Spanish, and that
it is somewhat different in content than the Span-
ish, otherwise we would expect the Mix of English
and Spanish contexts to do better than being most
accurate for just one of five cases.
7 Discussion
Of the 15 name conflate experiments (five pairs,
three languages), in only five cases did the use of
the evaluation contexts as a source of feature se-
lection data result in better F-Measure scores than
did either using the English contexts alone or as a
Mix with the evaluation language contexts. Thus,
we conclude that there is a clear benefit to using
feature selection data that comes from a different
language than the one for which discrimination is
being performed.
We believe that this is due to the volume of
the English data, as well as to the nature of the
name discrimination task. For example, a per-
son is often best described or identified by observ-
ing the people he or she tends to associate with,
or the places he or she visits, or the companies
with which he or she does business. If we ob-
serve that George Miller and Mel Gibson occur
together, then it seems we can safely infer that
George Miller the movie director is being referred
to, rather than George Miller the psychologist and
father of WordNet.
This argument might suggest that first order
co?occurrences would be sufficient to discrimi-
nate among the names. That is, simply group the
evaluation contexts based on the features that oc-
cur within them, and essentially cluster evaluation
31
contexts based on the number of features they have
in common with other evaluation contexts. In fact,
results on word sense discrimination (Purandare
and Pedersen, 2004) suggest that first order rep-
resentations are more effective with larger number
of context than second order methods. However,
we see examples in these results that suggests this
may not always be the case. In the Bulgarian re-
sults, the largest number of Bulgarian contexts are
for NATO-USA, but the Mix performs quite a bit
better than Bulgarian only. In the case of Roma-
nian, again NATO-USA has the largest number of
contexts, but the Mix still does better than Roma-
nian only. And in Spanish, Mexico-India has the
largest number of contexts and English-only does
better. Thus, even in cases where we have an abun-
dant number of evaluation contexts, the indirect
nature of the second order representation provides
some added benefit.
We believe that the perspective of the news or-
ganizations providing the corpora certainly has an
impact on the results. For example, in Romanian,
the news about David Beckham and Ronaldo is
probably much the same as in the United States.
These are international figures that are both ex-
ternal to countries where the news originates, and
there is no reason to suppose there would be a
unique local perspective represented by any of the
news sources. The only difference among them
might be in the number of contexts available. In
this situation, the addition of the English contexts
may provide enough additional information to im-
prove discrimination performance in another lan-
guage.
For example, in the 162 Romanian contexts
for Ronaldo-Beckham, there is one occurrence of
Posh, which was the stage name of Beckham?s
wife Victoria. This is below our frequency cut-
off threshold for feature selection, so it would be
discarded when using Romanian?only contexts.
However, in the English contexts Posh is men-
tioned 6 times, and is included as a feature. Thus,
the one occurrence of Posh in the Romanian cor-
pus can be well represented by information found
in the English contexts, thus allowing that Roma-
nian context to be correctly discriminated.
8 Conclusions
This paper shows that a method of name discrim-
ination based on second order context representa-
tions can take advantage of English contexts, and
the mix of English and evaluation contexts, in or-
der to perform more accurate name discrimination.
9 Acknowledgments
This research is supported by a National Sci-
ence Foundation Faculty Early CAREER Devel-
opment Award (#0092784). All of the experiments
in this paper were carried out with version 0.71
SenseClusters package, which is freely available
from http://senseclusters.sourceforge.net.
References
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
G.A. Miller and W.G. Charles. 1991. Contextual cor-
relates of semantic similarity. Language and Cogni-
tive Processes, 6(1):1?28.
P. Nakov and M. Hearst. 2003. Category-based pseu-
dowords. In Companion Volume to the Proceedings
of HLT-NAACL 2003 - Short Papers, pages 67?69,
Edmonton, Alberta, Canada, May 27 - June 1.
T. Pedersen and R. Bruce. 1997. Distinguishing word
senses in untagged text. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 197?207, Providence,
RI, August.
T. Pedersen and A. Kulkarni. 2006. Selecting the
r?ightn?umber of senses based on clustering criterion
functions. In Proceedings of the Posters and Demo
Program of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Trento, Italy, April.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005.
Name discrimination by clustering similar contexts.
In Proceedings of the Sixth International Conference
on Intelligent Text Processing and Computational
Linguistics, pages 220?231, Mexico City, February.
T. Pedersen, A. Kulkarni, R. Angheluta, Z. Kozareva,
and T. Solorio. 2006. An unsupervised language in-
dependent method of name discrimination using sec-
ond order co-occurrence features. In Proceedings
of the Seventh International Conference on Intelli-
gent Text Processing and Computational Linguistics,
pages 208?222, Mexico City, February.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41?48, Boston, MA.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
32
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 19?26,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Language Independent Approach for Name Categorization and
Discrimination
Zornitsa Kozareva
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
zkozareva@dlsi.ua.es
Sonia Va?zquez
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
svazquez@dlsi.ua.es
Andre?s Montoyo
Departamento de Lenguajes
y Sistemas Informa?ticos
Universidad de Alicante
Alicante, Spain
montoyo@dlsi.ua.es
Abstract
We present a language independent ap-
proach for fine-grained categorization and
discrimination of names on the basis of text
semantic similarity information. The exper-
iments are conducted for languages from the
Romance (Spanish) and Slavonic (Bulgar-
ian) language groups. Despite the fact that
these languages have specific characteristics
as word-order and grammar, the obtained
results are encouraging and show that our
name entity method is scalable not only to
different categories, but also to different lan-
guages. In an exhaustive experimental eval-
uation, we have demonstrated that our ap-
proach yields better results compared to a
baseline system.
1 Introduction
1.1 Background
Named Entity (NE) recognition concerns the detec-
tion and classification of names into a set of cate-
gories. Presently, most of the successful NE ap-
proaches employ machine learning techniques and
handle simply the person, organization, location and
miscellaneous categories. However, the need of
the current Natural Language Applications impedes
specialized NE extractors which can help for in-
stance an information retrieval system to determine
that a query about ?Jim Henriques guitars? is related
to the person ?Jim Henriques? with the semantic cat-
egory musician, and not ?Jim Henriques? the com-
poser. Such classification can aid the system to rank
or return relevant answers in a more accurate and
appropriate way.
So far, the state-of-art NE recognizers identify
that ?Jim Henriques? is a person, but do not sub-
categorize it. There are numerous drawbacks re-
lated to the fine-grained NE issue. First, the sys-
tems need hand annotated data which are not avail-
able for multiple categories, because their creation is
time-consuming, requires supervision by experts, a
predefined fine-grained hierarchical structure or on-
tology. Second, there is a significant lack of freely
available or developed resources for languages other
than English, and especially for the Eastern Euro-
pean ones.
The World Wide Web is a vast, multilingual
source of unstructured information which we con-
sult daily in our native language to understand what
the weather in our city is or how our favourite soccer
team performed. Therefore, the need of multilingual
and specialized NE extractors remains and we have
to focus on the development of language indepen-
dent approaches.
Together with the specialized NE categorization,
we face the problem of name ambiguity which is
related to queries for different people, locations or
companies that share the same name. For instance,
Cambridge is a city in the United Kingdom, but
also in the United States of America. ACL refers
to ?The Association of Computational Linguistics?,
?The Association of Christian Librarians? or to the
?Automotive Components Limited?. Googling the
name ?Boyan Bonev? returns thousands of docu-
ments where some are related to a member of a robot
vision group in Alicante, a teacher at the School
19
of Biomedical Science, a Bulgarian schoolboy that
participated in computer science competition among
others. So far, we have to open the documents one
by one, skim the text and decide to which ?Boyan
Bonev? the documents are related to. However, if
we resolve the name disambiguation issue, this can
lead to an automatic clustering of web pages talking
about the same individual, location or ogranization.
1.2 Related Work
Previously, (Pedersen et al, 2005) tackled the name
discrimination task by developing a language inde-
pendent approach based on the context in which the
ambiguous name occurred. They construct second
order co-occurrence features according to which the
entities are clustered and associated to different un-
derlying names. The performance of this method
ranges from 51% to 73% depending on the pair of
named entities that have to be disambiguated. Simi-
lar approach was developed by (Bagga and Baldwin,
1998), who created first order context vectors that
represent the instance in which the ambiguous name
occurs. Their approach is evaluated on 35 different
mentions of John Smith, and the f-score is 84%.
For fine-grained person NE categorization, (Fleis-
chman and Hovy, 2002) carried out a supervised
learning for which they deduced features from the
local context in which the entity resides, as well as
semantic information derived from the topic signa-
tures and WordNet. According to their results, to
improve the 70% coverage for person name catego-
rization, more sophisticated features are needed, to-
gether with a more solid data generation procedure.
(Tanev and Magnini, 2006) classified geographic
location and person names into several subclasses.
They use syntactic information and observed how
often a syntactic pattern co-occurs with certain
member of a given class. Their method reaches 65%
accuracy. (Pasca, 2004) presented a lightly super-
vised lexico-syntactic method for named entity cat-
egorization which reaches 76% when evaluated with
unstructured text of Web documents.
(Mann, 2002) populated a fine-grained proper
noun ontology using common noun patterns and fol-
lowing the hierarchy of WordNet. They studied the
influence of the newly generated person ontology in
a Question Answering system. According to the ob-
tained results, the precision of the ontology is high,
but still suffers in coverage. A similar approach for
the population of the CyC Knowledge Base (KB)
was presented in (Shah et al, 2006). They used
information from the Web and other electronically
available text corpora to gather facts about particu-
lar named entities, to validate and finally to add them
to the CyC KB.
In this paper, we present a new text semantic simi-
larity approach for fine-grained person name catego-
rization and discrimination which is similar to those
of (Pedersen et al, 2005) and (Bagga and Baldwin,
1998), but instead of simple word co-occurrences,
we consider the whole text segment and relate the
deduced semantic information of Latent Seman-
tic Analysis (LSA) to trace the text cohesion be-
tween thousands of sentences containing named en-
tities which belong to different fine-grained cate-
gories or individuals. Our method is based on the
word sense discrimination hypothesis of Miller and
Charles (1991) according to which words with sim-
ilar meaning are used in similar context, hence in
our approach we assume that the same person or
the same fine-grained person category appears in the
similar context.
2 NE categorization and discrimination
with Latent Semantic Analysis
LSA has been applied successfully in many areas
of Natural Language Processing such as Informa-
tion Retrieval (Deerwester et al, 1990), Informa-
tion Filtering (Dumais, 1995) , Word Sense Disam-
biguation (Shu?tze, 1998) among others. This is pos-
sible because LSA is a fully automatic mathemati-
cal/statistical technique for extracting and inferring
relations of expected contextual usage of words in
discourse. It uses no humanly constructed dictionar-
ies or knowledge bases, semantic networks, syntac-
tic or morphological analyzers, because it takes only
as input raw text which is parsed into words and is
separated into meaningful passages. On the basis of
this information, LSA extracts a list of semantically
related word pairs or rank documents related to the
same topic.
LSA represents explicitly terms and documents
in a rich, highly dimensional space, allowing the
underlying ?latent?, semantic relationships between
terms and documents to be exploited. LSA relies
20
on the constituent terms of a document to suggest
the document?s semantic content. However, the LSA
model views the terms in a document as somewhat
unreliable indicators of the concepts contained in the
document. It assumes that the variability of word
choice partially obscures the semantic structure of
the document. By reducing the original dimen-
sionality of the term-document space with Singular
Value Decomposition to a matrix of 300 columns,
the underlying, semantic relationships between doc-
uments are revealed, and much of the ?noise? (dif-
ferences in word usage, terms that do not help distin-
guish documents, etc.) is eliminated. LSA statisti-
cally analyzes the patterns of word usage across the
entire document collection, placing documents with
similar word usage patterns near to each other in the
term-document space, and allowing semantically-
related documents to be closer even though they may
not share terms.
Taking into consideration these properties of
LSA, we thought that instead of constructing the
traditional term-document matrix, we can construct
a term-sentence matrix with which we can find a
set of sentences that are semantically related and
talk about the same person. The rows of the term-
sentence matrix correspond to the words of the sen-
tence where the NE has to be categorized or discrim-
inated (we call this sentence target sentence), while
the columns correspond to the rest of the sentences
with NEs. The cells of the matrix show the num-
ber of times a given word from the target sentence
co-occurs in the rest of the sentences. When two
columns of the term-sentence matrix are similar, this
means that the two sentences contain similar words
and are therefore likely to be semantically related.
When two rows are similar, then the corresponding
words occur in most of the same sentences and are
likely to be semantically related.
In this way, we can obtain semantic evidence
about the words which characterize a given person.
For instance, a football player is related to words
as ball, match, soccer, goal, and is seen in phrases
such as ?X scores a goal?, ?Y is penalized?. Mean-
while, a surgeon is related to words as hospital, pa-
tient, operation, surgery and is seen in phrases such
as ?X operates Y?, ?X transplants?. Evidently, the
category football player can be distinguished easily
from that of the surgeon, because both person names
occur and relate semantically to different words.
Another advantage of LSA is its property of lan-
guage independence, and the ability to link sev-
eral flexions or declanations of the same term.
This is especially useful for the balto-slavonic lan-
guages which have rich morphology. Once the term-
sentence approach is developed, practically there is
no restrain for LSA to be applied and extended to
other languages. As our research focuses not only
on the resolution of the NE categorization and dis-
crimination problems as a whole, but also on the lan-
guage independence issue, we considered the LSA?s
usage are very appropriate.
3 Development Data Set
For the development of our name discrimination and
classification approach, we used the Spanish lan-
guage. The corpora we worked with is the EFE94-95
Spanish news corpora, which were previously used
in the CLEF competitions1. In order to identify the
named entities in the corpora, we used a machine
learning based named entity recognizer (Kozareva et
al., 2007).
For the NE categorization and discrimination ex-
periments, we used six different named entities, for
which we assumed a-priory to belong to one of the
two fine-grained NE categories PERSON SINGER
and PERSON PRESIDENT. The president names
are Bill Clinton, George Bush and Fidel Castro, and
the singer names are Madonna, Julio Iglesias and
Enrique Iglesias. We have selected these names for
our experiment, because of their high frequency in
the corpora and low level of ambiguity.
Once we have selected the names, we have col-
lected a context of 10, 25, 50 and 100 words from
the left and from the right of the NEs. This is done
in order to study the influence of the context for the
NE discrimination and categorization tasks, and es-
pecially how the context window affects LSA?s per-
formance. We should note that the context for the
NEs is obtained from the text situated between the
text tags. During the creation of the context win-
dow, we used only the words that belong to the docu-
ment in which the NE is detected. This restriction is
imposed, because if we use words from previous or
following documents, this can influence and change
1http://www.clef-campaign.org/
21
the domain and the topic in which the NE is seen.
Therefore, NE examples for which the number of
context words does not correspond to 10, 25, 50 or
100 are directly discarded.
From the compiled data, we have randomly se-
lected different NE examples and we have created
two data sets: one with 100 and another with 200
examples per NE. In the fine-grained classification,
we have substituted the occurrence of the presi-
dent and singer names with the obfuscated form
President Singer. While for the NE discrim-
ination task, we have replaced the names with the
M EI JI BC GB FC label. The first label indicates
that a given sentence can belong to the president or
to the singer category, while the second label indi-
cates that behind it can stand one of the six named
entities. The NE categorization and discrimination
experiments are carried out in a completely unsuper-
vised way, meaning that we did not use the correct
name and name category until evaluation.
4 Experimental Evaluation
4.1 Experimental Settings
As mentioned in Section 2, to establish the semantic
similarity relation between a sentence with an ob-
fuscated name and the rest of the sentences, we use
LSA2. The output of LSA is a list of sentences that
best matches the target sentence (e.g. the sentence
with the name that has to be classified or discrim-
inated) ordered by their semantic similarity score.
Strongly similar sentences have values close to 1,
and dissimilar sentences have values close to 0.
In order to group the most semantically similar
sentences which we expect to refer to the same per-
son or the same fine-grained category, we apply the
graph-based clustering algorithm PoBOC (Cleuziou
et al, 2004). We construct a new quadratic sentence-
sentence similarity matrix where the rows stand for
the sentence we want to classify, the columns stand
for the sentences in the whole corpus and the values
of the cells represent the semantic similarity scores
derived from LSA.
On the basis of this information, PoBOC forms
two clusters whose performance is evaluated in
terms of precision, recall, f-score and accuracy
which can be derived from Table 1.
2http://infomap-nlp.sourceforge.net/
number of Correct PRESIDENT Correct SINGER
Assigned PRESIDENT a b
Assigned SINGER c d
Table 1: Contingency table
We have used the same experimental setting for
the name categorization and discrimination prob-
lems.
4.2 Spanish name categorization
In Table 2, we show the results for the Spanish fine-
grained categorization. The detailed results are for
the context window of 50 words with 100 and 200
examples. All runs, outperform a simple baseline
system which returns for half of the examples the
fine-grained category PRESIDENT and for the rest
SINGER. This 50% baseline performance is due to
the balanced corpus we have created. In the column
diff., we show the difference between the 50% base-
line and the f-score of the category. As can be seen
the f-scores reaches 90%, which is with 40% more
than the baseline. According to the z? statistics with
confidence level of 0.975, the improvement over the
baseline is statistically significant.
SPANISH
cont/ex Category P. R. A. F. diff.
50/100
PRESIDENT 90.38 87.67 88.83 89.00
SINGER 87.94 90.00 88.33 88.96 +39.00
50/200
PRESIDENT 90.10 94.33 91.92 92.18
SINGER 94.04 89.50 91.91 91.71 +42.00
Table 2: Spanish NE categorization
During the error analysis, we found out that the
PERSON PRESIDENT and PERSON SINGER cat-
egories are distinguishable and separable because
of the well-established semantic similarity relation
among the words with which the NE occurs.
A pair of president sentences has lots of strongly
related words such as president:meeting, presi-
dent:government, which indicates high text cohe-
sion, while the majority of words in a president?
singer pair are weakly related, for instance presi-
dent:famous, president:concert. But still we found
out ambiguous pairs such as president:company,
where the president relates to a president of a coun-
try, while the company refers to a musical enter-
22
name c10 c25 c50 c100
Madonna 63.63 61.61 63.16 79.45
Julio Iglesias 58.96 56.68 66.00 79.19
Enrique Iglesias 77.27 80.17 84.36 90.54
Bill Clinton 52.72 48.81 74.74 73.91
George Bush 49.45 41.38 60.20 67.90
Fidel Castro 61.20 62.44 77.08 82.41
Table 3: Spanish NE discrimination
prize. Such information confuses LSA?s categoriza-
tion process and decreases the NE categorization
performance.
4.3 Spanish name discrimination
In a continuation, we present in Table 3 the f-scores
for the Spanish NE discrimination task with the 10,
25, 50 and 100 context windows. The results show
that the semantic similarity method we employ is
very reliable and suitable not only for the NE cat-
egorization, but also for the NE discrimination. A
baseline which always returns one and the same per-
son name during the NE discrimination task is 17%.
From the table can be seen that all names outperform
this baseline. The f-score performance per individ-
ual name ranges from 42% to 90%. The results are
very good, as the conflated names (three presidents
and three singers) can be easily obfuscated, because
they share the same domain and occur with the same
semantically related words.
The three best discriminated names are Enrique
Iglesias, Fidel Castro and Madonna. The name Fidel
Castro is easily discriminated due to its characteriz-
ing words Cuba, CIA, Cuban president, revolution,
tyrant. All sentences having these words or syn-
onyms related to them are associated to Fidel Cas-
tro.
Bill Clinton occurred many times with the words
democracy, Boris Yeltsin, Halifax, Chelsea (the
daughter of Bill Clinton), White House, while
George Bush appeared with republican, Ronald
Reigan, Pentagon, war in Vietnam, Barbara Bush
(the wife of George Bush).
During the data compilation process, the exam-
ples for Enrique Iglesias are considered to belong to
the Spanish singer. However, in reality some exam-
ples of Enrique Iglesias talked about the president of
a financial company in Uruguay or political issues.
Therefore, this name was confused with Bill Clin-
ton, because they shared semantically related words
such as bank, general secretary, meeting, decision,
appointment.
The discrimination process for the singer names is
good, though Madonna and Julio Iglesias appeared
in the context of concerts, famous, artist, maga-
zine, scene, backstage. The characterizing words for
Julio Iglesias are Chabeli (the daughter of Julio Igle-
sias), Spanish, Madrid, Iberoamerican. The name
Madonna occurred with words related to a picture
of Madonna, a statue in a church of Madonna, the
movie Evita.
Looking at the effect of the context window for
the NE discrimination task, it can be seen that the
best performances of 90% for Enrique Iglesias, 82%
for Fidel Castro and 79% for Madonna are achieved
with 100 words from the left and from the right of
the NE. This shows that the larger context has better
discrimination power.
4.4 Discussion
After the error analysis, we saw that the performance
of our approach depends on the quality of the data
source we worked with. Although, we have selected
names with low degree of ambiguity, during the data
compilation process for which we assumed that they
refer 100% to the SINGER or PRESIDENT cate-
gories, during the experiments we found out that one
and the same name can refer to three different in-
dividuals. This was the case of Madonna and En-
rique Iglesias. From one side this impeded the fine-
grained categorization and discrimination processes,
but opened a new line for research.
In conclusion, the conducted experiments re-
vealed a series of important observations. The first
one is that the LSA?s term-sentence approach per-
forms better with a higher number of examples, be-
cause they provide more semantic information. In
addition to the number of examples, the experiments
show that the influence of the context window for the
name discrimination is significant. The discrimina-
tion power is better for larger context windows and
this is also related to the expressiveness of the lan-
guage.
Second, our name categorization and discrimina-
tion approach outperforms the baseline with 30%.
Finally, LSA is a very appropriate approximation
for the resolution of the NE categorization and dis-
23
crimination tasks. LSA also gives logical explana-
tion about the classification decision of the person
names, providing a set of words characterizing the
category or simply a list of words describing the in-
dividual we want to classify.
5 Adaptation to Bulgarian
5.1 Motivation
So far, we have discussed and described the develop-
ment and the performance of our approach with the
Spanish language. The obtained results and observa-
tions, serve as a base for the context extraction and
the experimental setup for the rest of the languages
which we want to study. However, to verify the mul-
tilingual performance of the approach, we decided
to carry out an experiment with a language which is
very different from the Romance family.
For this reason, we choose the Bulgarian lan-
guage, which is the earliest written Slavic language.
It dates back from the creation of the old Bulgarian
alphabet Glagolista, which was later replaced by the
Cyrillic alphabet. The most typical characteristics of
the Bulgarian language are the elimination of noun
declension, suffixed definite article, lack of a verb
infinitive and complicated verb system.
The Bulgarian name discrimination data is ex-
tracted from the news corpus Sega2002. This corpus
is originally prepared and used in the CLEF compe-
titions. The corpus consists of news articles orga-
nized in different XML files depending on the year,
month, and day of the publication of the news. We
merged all files into a single one, and considered
only the text between the text tags. In order to ease
the text processing and to avoid encoding problems,
we transliterated the Cyrillic characters into Latin
ones.
The discrimination data in this experiment con-
sists of the city, country, party, river and mountain
categories. We were interested in studying not only
the multilingual issue of our approach, but also how
scalable it is with other categories. The majority
of the categories are locations and only one corre-
sponds to organization. In Table 4, we shows the
number of names which we extracted for each one
of the categories.
5.2 Bulgarian data
The cities include the capital of Bulgaria ? Sofia, the
second and third biggest Bulgarian cities ? Plovdiv
and Varna, a city from the southern parts of Bulgaria
? Haskovo, the capital of England ? London and
the capital of Russia ? Moskva. The occurrences of
these examples are conflated in the ambiguous name
CITY.
For countries we choose Russia (Rusiya)3, Ger-
many (Germaniya), France (Franciya), Turkey (Tur-
ciya) and England (Angliya). The five names are
conflated into COUNTRY.
The organizations we worked with are the two
leading Bulgarian political parties. BSP (Balgar-
ska Socialisticeska Partija, or Bulgarian Socialist
Party) is the left leaning party and the successor to
the Bulgarian Communist Party. SDS (Sayuz na
demokratichnite sili, or The Union of Democratic
Forces) is the right leaning political party. The two
organizations are conflated into PARTY.
For the RIVER category we choose Danube
(Dunav) which is the second longest river in Eu-
rope and passes by Bulgaria, Maritsa which is the
longest river that runs solely in the interior of the
Balkans, Struma and Mesta which run in Bulgaria
and Greece.
The final category consists of the oldest Bulgarian
mountain situated in the southern part of Bulgaria ?
Rhodope (Rodopi), Rila which is the highest moun-
tain in Bulgaria and on the whole Balkan Penin-
sula, and Pirin which is the second highest Bulgarian
mountain after Rila. The three mountain names are
conflated and substituted with the label MOUNTAIN.
5.3 Bulgarian name discrimination
The experimental settings coincide with those pre-
sented in Section 4 and the obtained results are
shown in Table 4. The performance of our approach
ranges from 32 to 81%. For the five categories, the
best performance is achieved for those names that
have the majority number of examples.
For instance, for the CITY category, the best per-
formance of 79% is reached with Sofia. TAs we
have previously mentioned, this is due to the fact that
LSA has more evidence about the context in which
Sofia appears. It is interesting to note that the city
3this is the Bulgarian transliteration for Russia
24
Category Instance Total P R F
City
Plovdiv 1822 44.42 83.87 58.08
Sofiya 5633 71.39 89.79 79.54
Varna 1042 32.02 82.64 46.17
Haskovo 140 21.09 69.29 32.33
London 751 31.32 84.82 45.74
Moskva 1087 39.47 88.22 54.53
Country
Rusiya 2043 55.83 86.19 67.77
Germaniya 1588 40.72 77.96 53.50
Francia 1352 37.27 77.81 50.39
Turciya 1162 43.23 84.08 57.10
Angliya 655 29.67 72.67 42.14
Party
BSP 2323 42.54 99.35 59.57
SDS 3916 64.86 98.85 78.32
River
Dunav 403 85.39 76.92 80.94
Marica 203 77.88 83.25 80.47
Mesta 81 63.64 95.06 76.24
Struma 37 56.67 91.89 70.10
Mountain
Rila 101 70.22 91.09 79.31
Pirin 294 75.11 57.48 65.12
Rodopi 135 71.04 96.29 81.76
Table 4: Bulgarian NE discrimination
Varna forms part of weak named entities such as the
University of Varna, the Major house of Varna. Al-
though, this strong entity is embedded into the weak
ones, practically Varna changes its semantic cate-
gory from a city into university, major house. This
creates additional ambiguity in our already conflated
and ambiguous names. In order to improve the per-
formance, we need a better data generation process
where the mixture of weak and strong entities will
be avoided.
The same effect of best classification for major-
ity sense is observed with the COUNTRY category.
The best performance of 67% is obtained for Rus-
sia. The other country which is distinguished sig-
nificantly well is Turkey. The 57% performance is
from 5 to 10% higher compared to the performances
of Germany, England and France. This is due to the
context in which the names occur. Turkey is related
to trading with Bulgaria and emigration, meanwhile
the other countries appear in the context of the Eu-
ropean Union, the visit of the Bulgarian president in
these countries.
During the error analysis, we noticed that in the
context of the political parties, SDS appeared many
times in with the names of the political leader or the
representatives of the BSP party and vice versa. This
impeded LSA?s classification, because of the similar
context.
Among all categories, RIVER and MOUNTAIN
obtained the best performances. The rivers Dunav
and Maritsa reached 80%, while the mountains
Rodopi achieved 81.76% f-score. Looking at the
discrimination results for the other names in these
categories, it can be seen that their performances are
much higher compared to the names of the CITY,
COUNTY and PARTY categories. This experiment
shows that the discrimination power is related to the
type of the NE category we want to resolve.
6 Conclusions
In this paper, we have presented a language indepen-
dent approach for person name categorization and
discrimination. This approach is based on the sen-
tence semantic similarity information derived from
LSA. The approach is evaluated with different NE
examples for the Spanish and Bulgarian languages.
We have observed the discrimination performance of
LSA not only with the SINGER and PRESIDENT
companies, but also with the CITY, COUNTRY,
MOUNTAIN, RIVER and PARTY. This is the first
approach which focuses on the resolution of these
categories for the Bulgarian language.
The obtained results both for Spanish and Bulgar-
ian are very promising. The baselines are outper-
formed with 25%. The person fine-grained catego-
rization reaches 90% while the name discrimination
varies from 42% to 90%. This variability is related
to the degree of the name ambiguity among the con-
flated names and similar behaviour is observed in the
co-occurence approach of (Pedersen et al, 2005).
During the experimental evaluation, we found out
that the 100% name purity (e.g. that one name be-
longs only to one and the same semantic category)
which we accept during the data creation in real-
ity contains 9% noise. These observations are con-
firmed in the additional experimental study we have
conducted with the Bulgarian language. According
to the obtained results, our text semantic similarity
approach performs very well and practically there
is no restrain to be adapted to other languages, data
sets or even new categories.
7 Future Work
In the future, we want to relate the name discrimi-
nation and categorization processes, by first encoun-
tering the different underlying meanings of a name
25
and then grouping together the sentences that belong
to the same semantic category. This process will in-
crease the performance of the NE fine-grained cat-
egorization, and will reduce the errors we encoun-
tered during the classification of the singers Enrique
Iglesias and Madonna. In addition to this experi-
ment, we want to cluster web pages on the basis of
name ambiguity. For instance, we want to process
the result for the Google?s query George Miller, and
form three separate clusters obtained on the basis of
a fine-grained and name discrimination. Thus we
can form the clusters for GeorgeMiller the congress-
man, the movie director and the father of WordNet.
This study will include also techniques for automatic
cluster stopping.
Moreover, LSA?s ability of language indepen-
dence can be exploited to resolve cross-language
NE categorization and discrimination from which
we can extract cross-language pairs of semantically
related words characterizing a person e.g. George
Bush is seen with White House in English, la Casa
Blanca in Spanish, a Casa Branka in Portuguese and
Beliat Dom in Bulgarian.
With LSA, we can also observe the time consis-
tency property of a person which changes its se-
mantic category across time. For instance, a stu-
dent turns into a PhD student, teaching assistant and
then university professor, or as in the case of Arnold
Schwarzenegger from actor to governor.
Acknowledgements
We would like to thank the three anonymous re-
viewers for their useful comments and suggestions.
This work was partially funded by the European
Union under the project QALLME number FP6 IST-
033860 and by the Spanish Ministry of Science and
Technology under the project TEX-MESS number
TIN2006-15265-C06-01.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of the Thirty-Sixth Annual Meeting of
the ACL and Seventeenth International Conference on
Computational Linguistics, pages 79?85.
G. Cleuziou, L. Martin, and C. Vrain. 2004. Poboc: An
overlapping clustering algorithm, application to rule-
based classification and textual data. In ECAI, pages
440?444.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. In Journal of the American Society for Informa-
tion Science, volume 41, pages 391?407.
S. Dumais. 1995. Using lsi for information filtering:
Trec-3 experiments. In The Third Text Retrieval Con-
ference (TREC-3), pages 219?230.
M. Fleischman and E. Hovy. 2002. Fine grained classifi-
cation of named entities. In Proceedings of the 19th in-
ternational conference on Computational linguistics,
pages 1?7.
Z. Kozareva, O. Ferra?ndeza, A. Montoyo, R. Mun?oz,
A. Sua?rez, and J. Go?mez. 2007. Combining data-
driven systems for improving named entity recogni-
tion. Data and Knowledge Engineering, 61(3):449?
466, June.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In COLING-02 on SEMANET,
pages 1?7.
G. Miller and W. Charles. 1991. Contextual correlates of
semantic similarity. In Language and Cognitive Pro-
cesses, pages 1?28.
M. Pasca. 2004. Acquisition of categorized named enti-
ties for web search. In CIKM ?04: Proceedings of the
thirteenth ACM international conference on Informa-
tion and knowledge management, pages 137?145.
T. Pedersen, A. Purandare, and A. Kulkarni. 2005. Name
discrimination by clustering similar contexts. In CI-
CLing, pages 226?237.
P. Shah, D. Schneider, C. Matuszek, R.C. Kahlert,
B. Aldag, D. Baxter, J. Cabral, M. Witbrock, and
J. Curtis. 2006. Automated population of cyc: Ex-
tracting information about named-entities from the
web. In Proceedings of the Nineteenth International
FLAIRS Conference, pages 153?158.
H. Shu?tze. 1998. Automatic word sense discrimination.
In Journal of computational linguistics, volume 24.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proceeding of
11th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 17?24.
26
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 334?337,
Prague, June 2007. c?2007 Association for Computational Linguistics
UA-ZBSA: A Headline Emotion Classification through Web Information
Zornitsa Kozareva, Borja Navarro, Sonia Va?zquez, Andre?s Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
03080
zkozareva,borja,svazquez,montoyo@dlsi.ua.es
Abstract
This paper presents a headline emotion clas-
sification approach based on frequency and
co-occurrence information collected from
the World Wide Web. The content words of
a headline (nouns, verbs, adverbs and adjec-
tives) are extracted in order to form different
bag of word pairs with the joy, disgust, fear,
anger, sadness and surprise emotions. For
each pair, we compute the Mutual Informa-
tion Score which is obtained from the web
occurrences of an emotion and the content
words. Our approach is based on the hypoth-
esis that group of words which co-occur to-
gether across many documents with a given
emotion are highly probable to express the
same emotion.
1 Introduction
The subjective analysis of a text is becoming impor-
tant for many Natural Language Processing (NLP)
applications such as Question Answering, Informa-
tion Extraction, Text Categorization among others
(Shanahan et al, 2006). The resolution of this prob-
lem can lead to a complete, realistic and coher-
ent analysis of the natural language, therefore ma-
jor attention is drawn to the opinion, sentiment and
emotion analysis, and to the identification of be-
liefs, thoughts, feelings and judgments (Quirk et al,
1985), (Wilson and Wiebe, 2005).
The aim of the Affective Text task is to clas-
sify a set of news headlines into six types of emo-
tions: ?anger?, ?disgust?, ?fear?, ?joy?, ?sadness?
and ?surprise?. In order to be able to conduct
such multi-category analysis, we believe that first
we need a comprehensive theory of what a human
emotion is, and then we need to understand how the
emotion is expressed and transmitted within the nat-
ural language. These aspects rise the need of syn-
tactic, semantic, textual and pragmatic analysis of
a text (Polanyi and Zaenen, 2006). However, some
of the major drawbacks in this field are related to
the manual or automatic acquisition of subjective ex-
pressions, as well as to the lack of resources in terms
of coverage.
For this reason, our current emotion classification
approach is based on frequency and co-occurrence
bag of word counts collected from the World Wide
Web. Our hypothesis is that words which tend to co-
occur across many documents with a given emotion
are highly probable to express this emotion.
The rest of the paper is organized as follows. In
Section 2 we review some of the related work, in
Section 3 we describe our web-based emotion classi-
fication approach for which we show a walk-through
example in Section 4. A discussion of the obtained
results can be found in Section 5 and finally we con-
clude in Section 6.
2 Related work
Our approach for emotion classification is based on
the idea of (Hatzivassiloglou and McKeown, 1997)
and is similar to those of (Turney, 2002) and (Tur-
ney and Littman, 2003). According to Hatzivas-
siloglou and McKeown (1997), adjectives with the
same polarity tended to appear together. For exam-
ple the negative adjectives ?corrupt and brutal? co-
334
occur very often.
The idea of tracing polarity through adjective co-
occurrence is adopted by Turney (2002) for the bi-
nary (positive and negative) classification of text re-
views. They take two adjectives, for instance ?ex-
cellent? and ?poor? in a way that the first adjective
expresses positive meaning, meanwhile the second
one expresses negative. Then, they extract all ad-
jectives from the review text and combine them with
?excellent? and ?poor?. The co-occurrences of these
words are searched on the web, and then the Mutual
Information score for the two groups of adjectives
is measured. When the adjective of the review ap-
pear more often with ?excellent?, then the review is
classified as positive, and when the adjectives appear
more often with ?poor?, then the review is classified
as negative.
Following Hatzivassiloglou and McKeown (1997)
and Turney (2002), we decided to observe how often
the words from the headline co-occur with each one
of the six emotions. This study helped us deduce
information according to which ?birthday? appears
more often with ?joy?, while ?war? appears more
often with ?fear?.
Some of the differences between our approach
and those of Turney (2002) are mentioned below:
? objectives: Turney (2002) aims at binary text
classification, while our objective is six class
classification of one-liner headlines. Moreover,
we have to provide a score between 0 and 100
indicating the presence of an emotion, and not
simply to identify what the emotion in the text
is. Apart from the difficulty introduced by the
multi-category classification, we have to deal
with a small number of content words while
Turney works with large list of adjectives.
? word class: Turney (2002) measures polarity
using only adjectives, however in our approach
we consider the noun, the verb, the adverb and
the adjective content words. The motivation
of our study comes from (Polanyi and Zaenen,
2006), according to which each content word
can express sentiment and emotion. In addition
to this issue we saw that most of the headlines
contain only nouns and verbs, because they ex-
press objectivity.
? search engines: Turney (2002) uses the Al-
tavista web browser, while we consider and
combine the frequency information acquired
from three web search engines.
? word proximity: For the web searches, Tur-
ney (2002) uses the NEAR operator and con-
siders only those documents that contain the
adjectives within a specific proximity. In our
approach, as far as the majority of the query
words appear in the documents, the frequency
count is considered.
? queries: The queries of Turney (2002) are made
up of a pair of adjectives, and in our approach
the query contains the content words of the
headline and an emotion.
There are other emotion classification approaches
that use the web as a source of information. For
instance, (Taboada et al, 2006) extracted from the
web co-occurrences of adverbs, adjectives, nouns
and verbs. Gamon and Aue (2005) were looking
for adjectives that did not co-occur at sentence level.
(Baroni and Vegnaduzzo, 2004) and (Grefenstette
et al, 2004) gathered subjective adjectives from the
web calculating the Mutual Information score.
Other important works on sentiment analysis are
those of (Wilson et al, 2005) and (Wiebe et al,
2005; Wilson and Wiebe, 2005), who used linguistic
information such as syntax and negations to deter-
mine polarity. Kim and Hovy (2006) integrated verb
information from FrameNet and incorporated it into
semantic role labeling.
3 Web co-occurrences
In order to determine the emotions of a
headline, we measure the Pointwise Mu-
tual Information (MI) of ei and cwj as
MI(ei, cwj) = log2 hits(ei,cwj)hits(ei)hits(cwj) , where ei ?
{anger, disgust, fear, joy, sadness, surprise}
and cwj are the content words of the headline j.
For each headline, we have six MI scores which
indicate the presence of the emotion. MI is used
in our experiments because it provides information
about the independence of an emotion and a bag of
words.
To collect the frequency and co-occurrence counts
of the headline words, we need large and massive
335
data repositories. To surmount the data sparsity
problem, we used as corpus the World Wide Web
which is constantly growing and daily updated.
Our statistical information is collected from three
web search engines: MyWay1, AlltheWeb2 and Ya-
hoo3. It is interesting to note that the emotion dis-
tribution provided by each one of the search engines
for the same headline has different scores. For this
reason, we decided to compute an intermediate MI
score as aMI =
?n
s=1 MI(ei,cwj)
s .
In the trail data, besides the MI score of an emo-
tion and all headline content words, we have calcu-
lated the MI for an emotion and each one of the con-
tent words. This allowed us to determine the most
sentiment oriented word in the headline and then we
use this predominant emotion to weight the associ-
ation sentiment score for the whole text. Unfortu-
nately, we could not provide results for the test data
set, due to the high number of emotion-content word
pairs and the increment in processing time and re-
turned responses of the search engines.
4 Example for Emotion Classification
As a walk through example, we use the Mortar as-
sault leaves at least 18 dead headline which is taken
from the trial data. The first step in our emotion clas-
sification approach consists in the determination of
the part-of-speech tags for the one-liner. The non-
content words are stripped away, and the rest of the
words are taken for web queries. To calculate the MI
score of a headline, we query the three search en-
gines combining ?mortar, assault, leave, dead? with
the anger, joy, disgust, fear, sadness and surprise
emotions. The obtained results are normalized in a
range from 0 to 100 and are shown in Table 1.
MyWay AllWeb Yahoo Av. G.Stand.
anger 19 22 24 22 22
disgust 5 6 7 6 2
fear 44 50 53 49 60
joy 15 19 20 18 0
sadness 28 36 36 33 64
surprise 4 5 6 5 0
Table 1: Performance of the web-based emotion
classification for a trail data headline
1www.myway.com
2www.alltheweb.com
3www.yahoo.com
As can be seen from the table, the three search
engines provide different sentiment distribution for
the same headline, therefore in our final experiment
we decided to calculate intermediate MI. Comparing
our results to those of the gold standard, we can say
that our approach detects significantly well the fear,
sadness and angry emotions.
5 Results and Discussion
Table 2 shows the obtained results for the affective
test data. The low performance of our approach
is explainable by the minimal knowledge we have
used. An interesting conclusion deduced from the
trail and test emotion data is that the system detects
better the negative feelings such as anger, disgust,
fear and sadness, in comparison to the positive emo-
tions such as joy and surprise. This makes us believe
that according to the web most of the word-emotion
combinations we queried are related to the expres-
sion of negative emotions.
UA-ZBSA Fine-grained Coarse-grained
Pearson Acc. P. R.
Anger 23.20 86.40 12.74 21.66
Disgust 16.21 97.30 0.00 0.00
Fear 23.15 75.30 16.23 26.27
Joy 2.35 81.80 40.00 2.22
Sadness 12.28 88.90 25.00 0.91
Surprise 7.75 84.60 13.70 16.56
Table 2: Performance of the web-based emotion
classification for the whole test data set
In the test run, we could not apply the emotion-
word weighting, however we believe that it has
a significant impact over the final performance.
Presently, we were looking for the distribution of all
content words and the emotions, but in the future we
would like to transform all words into adjectives and
then conduct web queries.
Furthermore, we would like to combine the re-
sults from the web emotion classification with the
polarity information given by SentiWordNet4. A-
priory we want to disambiguate the headline content
words and to determine the polarities of the words
and their corresponding senses. For instance, the ad-
jective ?new? has eleven senses, where new#a#3 and
new#a#5 express negativism, new#a#4 and new#a#9
positivism and the rest of the senses are objective.
4http://sentiwordnet.isti.cnr.it/
336
So far we did not consider the impact of valence
shifter (Polanyi and Zaenen, 2006) and we were un-
able to detect that a negative adverb or adjective
transforms the emotion from positive into negative
and vice versa. We are also interested in studying
how to conduct queries not as a bag of words but
bind by syntactic relations (Wilson et al, 2005).
6 Conclusion
Emotion classification is a challenging and difficult
task in Natural Language Processing. For our first
attempt to detect the amount of angry, fear, sadness,
surprise, disgust and joy emotions, we have pre-
sented a simple web co-occurrence approach. We
have combined the frequency count information of
three search engines and we have measured the Mu-
tual Information score between a bag of content
words and emotion.
According to the yielded results, the presented ap-
proach can determine whether one sentiment is pre-
dominant or not, and most of the correct sentiment
assignments correspond to the negative emotions.
However, we need to improve the approach in many
aspects and to incorporate more knowledge-rich re-
sources, as well as to tune the 0-100 emotion scale.
Acknowledgements
This research has been funded by QALLME number
FP6 IST-033860 and TEX-MESS number TIN2006-
15265-C06-01.
References
Marco Baroni and Stefano Vegnaduzzo. 2004. Identi-
fying subjective adjectives through web-based mutual
information. In Ernst Buchberger, editor, Proceedings
of KONVENS 2004, pages 17?24.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the Workshop on Feature Engineering for Ma-
chine Learning in Natural Language Processing (ACL
2005), pages 57?64.
Gregory Grefenstette, Yan Qu, James G. Shanahana, and
David A. Evans. 2004. Coupling niche browsers and
affect analysis for an opinion mining application. In
Proceeding of RIAO-04.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on Eu-
ropean chapter of the Association for Computational
Linguistics (EACL).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, pages 1?8.
Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifter. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theory and Applications, chapter 1, pages 1?
10. Springer.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman.
James G. Shanahan, Yan Qu, and Janyce Wiebe. 2006.
Computing Attitude and Affect in Text: Theory and Ap-
plications. Springer.
Maite Taboada, Caroline Anthony, and Kimberly Voll.
2006. Methods for creating semantic orientation
databases. In Proceeding of LREC-06, the 5th Interna-
tional Conference on Language Resources and Evalu-
ation, pages 427?432.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 417?424.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation (for-
merly Computers and the Humanities), 39(2-3):165?
210.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In Ann Arbor, editor,
Proceedings of the Workshop on Frontiers in Corpus
Annotation II: Pie in the Sky, pages 53?60.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354.
337
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 338?341,
Prague, June 2007. c?2007 Association for Computational Linguistics
UA-ZSA: Web Page Clustering on the basis of Name Disambiguation
Zornitsa Kozareva, Sonia Vazquez, Andres Montoyo
DLSI, University of Alicante
Carretera de San Vicente S/N
Alicante, Spain
03080
zkozareva,svazquez,montoyo@dlsi.ua.es
Abstract
This paper presents an approach for web
page clustering. The different underlying
meanings of a name are discovered on the
basis of the title of the web page, the body
content, the common named entities across
the documents and the sub-links. This in-
formation is feeded into a K-Means cluster-
ing algorithm which groups together the web
pages that refer to the same individual.
1 Introduction
Ambiguity is the task of building up multiple alter-
native linguistic structures for a single input. Most
of the approaches focus on word sense disambigua-
tion (WSD), where the sense of a word has to be
determined depending on the context in which it is
used.
The same problem arises for named entities
shared by different people or for grandsons named
after their grandparents. For instance, querying the
name ?Michael Hammond? in the World Wide Web
where there are huge quantities of massive and un-
structured data, a search engine retrieves thousands
of documents related to this name. However, there
are several individuals sharing the name ?Michael
Hammon?. One is a biology professor at the Univer-
sity of Arizona, another is at the University of War-
wick, there is a mathematician from Toronto among
others. The question is which one of these refer-
ents we are actually looking for and interested in.
Presently, to be able to answer to this question, we
have to skim the content of the documents and re-
trieve the correct answers on our own.
To automate this process, the named entities
can be disambiguated and the different underlying
meanings of the name can be found. On the basis
of this information, the web pages can be clustered
together and organized in a hierarchical structure
which can ease the documents? browsing. This is
also the objective of the Web People Search (WePS)
task (Artiles et al, 2007). What makes the WePS
task even more challenging is the fact that in con-
trast to WSD where the number of senses of a word
are predefined, in WePS we do not know the exact
number of different individuals.
For the resolution of the WePS task, we have de-
veloped a web page clustering approach using the
title and the body content of the web pages. In ad-
dition, we group together the documents that share
many location, person and organization names, as
well as those that point out to the same sub-links.
The rest of the paper is organized as follows. In
Section 2 we describe various approaches for name
disambiguation and discrimination. Our approach
is shown in Section 3, the obtained results and a dis-
cussion are provided in Section 4 and finally we con-
clude in Section 5.
2 Related Work
Early work in the field of name disambiguation
is that of (Bagga and Baldwin, 1998) who pro-
posed cross-document coreference resolution algo-
rithm which uses vector space model to resolve the
ambiguities between people sharing the same name.
The approach is evaluated on 35 different mentions
of John Smith and reaches 85% f-score.
Mann and Yarowski (2003) developed an unsu-
338

HTML/XML cleaning 
Search 
Web 
Retrieved Documents 
Preprocessing 
Title 
Context information 
Body 
Text Proper names 
Links 
Clusters 
K-means Cluster analysis WEKA 
LSA matrix transformation 
Clustering 
On the basis of name disambiguation 
Matrix from context 
Figure 1: Architecture of the WePS System
pervised approach to name discrimination where bi-
ographical features (age, date of birth), familiar re-
lationships (wife, son, daughter) and associations
(country, company, organization) are considered.
Therefore, in our approach we use person, organiza-
tion and location names in order to construct a social
similarity network between two documents.
Another unsupervised clustering technique for
name discrimination of web pages is that of Peder-
sen and Kulkarni (2007). They used contextual vec-
tors derived from bigrams, and measured the impact
of several association measures. During the evalu-
ation, some names were easily discriminable com-
pared to others categories for which was even diffi-
cult to find and obtain discriminative feature. We
worked with their unigram model (Purandare and
Pedersen, 2004) to cluster the web pages using the
text content between the title tags.
3 Web Person Disambiguation
Our web people clustering approach is presented in
Figure 1 and consists of the following steps:
? HTML cleaning: all html tags are stripped
away, the javascript code is eliminated, the non
closed WePS tags are repaired, the missing be-
gin/end body tags are included and then the
content between the title, the body and the an-
chor tags is extracted.
? name matching: the location, person and orga-
nization names in the body texts are identified
with the GATE1 system (Cunningham, 2005).
Each named entity of a document is matched
with its corresponding named entity category
from the rest of the web pages. This infor-
mation is used to calculate the social semantic
similarity of the person, the location and the or-
ganization names. Our hypothesis is that doc-
uments with similar names tend to refer to the
same individual. The output of this module is
a matrix with binary values, where 1 stands for
the documents which share more than the half
of their proper names, and 0 otherwise.
? links: for each document, we extract the links
situated between the anchor tags. Since the
links are too specific, we wrote an url function
which transform a given web page d1 with URL
http://www.cs.ualberta.ca/?lindek/index.htm
into www.cs.ualberta.ca/?lindek,
and the web page d2 with URL
http://www.cs.ualberta.ca/?lindek/demos.htm
into www.cs.ualberta.ca/?lindek. According
to our approach, the two web pages d1 and d2
are linked to each other if their link structures
(LS) intersect, that is LS(d1)?LS(d2) 6= 0.
The output of this module is a matrix with
binary values, where 1 stands for two web
pages having more than 3 links in common and
0 otherwise.
? titles: for each document, we extract the text
between the title tags. We create a unigram
matrix which is feed into SenseClusters2. We
use automatic cluster stopping criteria with the
gap statistics which groups the web pages into
several clusters according to the context of the
titles. From the obtained clusters, we generate
a new matrix with binary values, where 1 corre-
sponds to the documents which were put in the
1http://sourceforge.net/projects/gate
2http://marimba.d.umn.edu/cgi-bin/SC-cgi/index.cgi
339
same cluster according to SenseClusters and 0
otherwise.
? bodies: the text between the body tags is ex-
tracted, tokenized and the part-of-speech (POS)
tags 3 are determined. The original text is trans-
formed by encoding the POS tag information as
follows: ?water#v the#det flowers#n and#conj
pass#v me#pron the#det glass#n of#prep wa-
ter#n?. This corpus transformation is done, be-
cause we want the Latent Semantic Analysis
(LSA) module to consider the syntactic cate-
gories of the words and to construct a more
reliable semantic space. For instance, in the
example above, there are two different repre-
sentations of water: the noun and the verb,
while without the corpus transformation LSA
sees only the string water.
? LSA4: the semantic similarity score for the
web-pages is calculated with Latent Semantic
Analysis (LSA). From the encoded body texts,
we build up a matrix, where the rows repre-
sent the words of the web-page collection, the
columns stand for the web-pages we want to
cluster and the cells show the number of times a
word of the corpus occurs in a web page. In or-
der to reduce the noise and the data sparsity, we
apply the Singular Value Decomposition algo-
rithm by reducing the original vector space into
300 dimensions. The output of the LSA mod-
ule is a matrix, which represents the semantic
similarity among the web pages.
? knowledge combination: the outputs of the
name matching, link, title and body modules
are combined into a new matrix 100 ? 400 di-
mensional matrix. The rows correspond to the
number of web pages and the columns repre-
sent the obtained values of the link, title, body
and name modules. This matrix is fed into
the K-means clustering algorithm which deter-
mines the final web page clustering.
? K-means5: the clustering of N web pages
into K disjoint subsets Sj containing Nj data
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4infomap-nlp.sourceforge.net/
5http://www.cs.waikato.ac.nz/ml/weka/
points is done by the minimization of the sum-
of-squares criterion J = ?Kj=1
?
n?Sj |xn ?
muj |2, where xn is a vector representing the
nth data point and muj is the geometric cen-
troid of the data points in Sj . The informa-
tion matrix from which the web page cluster-
ing is performed includes the similarity infor-
mation for the title, link, proper name and body.
The current implementation of K-means (Wit-
ten and Frank, 2005) does not have an au-
tomatic cluster stopping criteria, therefore the
number of clusters is set up manually.
4 Results and Discussion
Table 1 shows the obtained results for the test data
set. The average performance of our system is 56%
and we ranked on 10-th position from 16 participat-
ing teams. Although, we have used different sources
of information and various approximations, in the
future we have to surmount a number of obstacles.
One of the limitations comes from the usage of the
text snippets situated between the body tags. There
are a number of web pages which do not contain any
text. The semantic space for these documents cannot
be built with LSA and their similarity score is zero.
Despite the fact that we have eliminated the stop
words from the documents and we have transformed
the web pages by encoding the syntactic categories,
the classification power of LSA was different for the
ambiguous names and for the web pages. To some
extend this is due to the varying number of words
in the web pages. In the future, we want to con-
duct experiments with a fixed context windows for
all documents.
In this task, the number of senses (e.g. number
of different individuals that share the same name)
is unknown, and one of the major drawbacks in our
approach is related to the setting up of the number
of clusters. The K-Means clustering algorithm we
used, did not include an automatic cluster stopping
criteria, and we had to set up the number of clus-
ters manually. To be able to do that, we have ob-
served the average number of clusters per name in
the trial data. We have evaluated the performance
of our approach with several different numbers of
clusters. According to the obtained results, the best
clusters are 25 and 50. We used the same number
340
Name Purity Inverse
Purity
F
?=0.5
F
?=0.2
Mark Johnson 0,55 0,74 0,63 0,69
Sharon Goldwater 0,96 0,23 0,37 0,27
Robert Moore 0,36 0,67 0,47 0,57
Leon Barrett 0,62 0,51 0,56 0,52
Dekang Lin 0,99 0,43 0,60 0,49
Stephen Clark 0,52 0,75 0,62 0,69
Frank Keller 0,38 0,67 0,48 0,58
Jerry Hobbs 0,54 0,63 0,58 0,61
James Curran 0,53 0,61 0,57 0,59
Chris Brockett 0,73 0,40 0,51 0,44
Thomas Fraser 0,66 0,57 0,61 0,58
John Nelson 0,68 0,76 0,72 0,74
James Hamilton 0,56 0,60 0,58 0,59
William Dickson 0,59 0,78 0,67 0,73
James Morehead 0,36 0,64 0,46 0,56
Patrick Killen 0,56 0,69 0,62 0,66
George Foster 0,46 0,70 0,56 0,64
James Davidson 0,58 0,71 0,64 0,68
Arthur Morgan 0,77 0,47 0,59 0,51
Thomas Kirk 0,26 0,90 0,41 0,60
Patrick Killen 0,56 0,69 0,62 0,66
Harry Hughes 0,66 0,54 0,59 0,56
Jude Brown 0,64 0,63 0,64 0,63
Stephan Johnson 0,56 0,80 0,66 0,73
Marcy Jackson 0,40 0,73 0,52 0,63
Karen Peterson 0,56 0,72 0,63 0,68
Neil Clark 0,68 0,36 0,47 0,40
Jonathan Brooks 0,53 0,76 0,63 0,70
Violet Howard 0,58 0,75 0,65 0,71
Global average 0,58 0,64 0,58 0,60
Table 1: Evaluation results
of clusters for the test data, however this is a rough
parameter estimation.
5 Conclusion
Person name disambiguation is a very important task
whose resolution can improve the performance of
the search engine by grouping together web pages
which refer to different individuals that share the
same name.
For our participation in the WePS task, we pre-
sented a name disambiguation approach which uses
only the information extracted from the web pages.
We conducted an experimental study with the trail
data set, according to which the combination of
the title, the body, the proper names and sub-links
reaches the best performance. Our current approach
can be improved with the incorporation of automatic
cluster stopping criteria.
So far we did not take advantage of the document
ranking and the returned snippets, but we want to in-
corporate this information by measuring the snippet
similarity on the basis of relevant domain informa-
tion (Kozareva et al, 2007).
Acknowledgements
Many thanks to Ted Pedersen for useful comments
and suggestions. This work was partially funded
by the European Union under the project QALLME
number FP6 IST-033860 and by the Spanish Min-
istry of Science and Technology under the project
TEX-MESS number TIN2006-15265-C06-01.
References
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The semeval-
2007 weps evaluation: Establishing a benchmark for
the web people search task. In Proceedings of Semeval
2007, Association for Computational Linguistics.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of ACL, pages 79?85.
H. Cunningham. 2005. Information Extraction, Auto-
matic. Encyclopedia of Language and Linguistics, 2nd
Edition.
Z. Kozareva, S. Vazquez, and A. Montoyo. 2007. The
usefulness of conceptual representation for the iden-
tification of semantic variability expressions. In Pro-
ceedings of the Eighth International Conference on In-
telligent Text Processing and Computational Linguis-
tics, (CICLing-2007).
G. Mann and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In Proceedings of the sev-
enth conference on Natural language learning at HLT-
NAACL 2003, pages 33?40.
T. Pedersen and A. Kulkarni. 2007. Discovering identi-
ties in web contexts with unsupervised clustering. In
Proceedings of the IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data.
A. Purandare and T. Pedersen. 2004. Senseclusters -
finding clusters that represent word senses. In AAAI,
pages 1030?1031.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques, volume 2.
Morgan Kaufmann.
341
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx? , Su Nam Kim? , Zornitsa Kozareva? , Preslav Nakov? ,
Diarmuid O? Se?aghdha?, Sebastian Pado?? , Marco Pennacchiotti??,
Lorenza Romano??, Stan Szpakowicz??
Abstract
We present a brief overview of the main
challenges in the extraction of semantic
relations from English text, and discuss the
shortcomings of previous data sets and shared
tasks. This leads us to introduce a new
task, which will be part of SemEval-2010:
multi-way classification of mutually exclusive
semantic relations between pairs of common
nominals. The task is designed to compare
different approaches to the problem and to
provide a standard testbed for future research,
which can benefit many applications in
Natural Language Processing.
1 Introduction
The computational linguistics community has a con-
siderable interest in robust knowledge extraction,
both as an end in itself and as an intermediate step
in a variety of Natural Language Processing (NLP)
applications. Semantic relations between pairs of
words are an interesting case of such semantic
knowledge. It can guide the recovery of useful facts
about the world, the interpretation of a sentence, or
even discourse processing. For example, pears and
bowl are connected in a CONTENT-CONTAINER re-
lation in the sentence ?The bowl contained apples,
?University of Antwerp, iris.hendrickx@ua.ac.be
?University of Melbourne, snkim@csse.unimelb.edu.au
?University of Alicante, zkozareva@dlsi.ua.es
?National University of Singapore, nakov@comp.nus.edu.sg
?University of Cambridge, do242@cl.cam.ac.uk
?University of Stuttgart, pado@stanford.edu
??Yahoo! Inc., pennacc@yahoo-inc.com
??Fondazione Bruno Kessler, romano@fbk.eu
??University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
pears, and oranges.?, while ginseng and taste are in
an ENTITY-ORIGIN relation in ?The taste is not from
alcohol, but from the ginseng.?.
The automatic recognition of semantic relations
can have many applications, such as information
extraction (IE), document summarization, machine
translation, or construction of thesauri and seman-
tic networks. It can also facilitate auxiliary tasks
such as word sense disambiguation, language mod-
eling, paraphrasing or recognizing textual entail-
ment. For example, semantic network construction
can benefit from detecting a FUNCTION relation be-
tween airplane and transportation in ?the airplane
is used for transportation? or a PART-WHOLE rela-
tion in ?the car has an engine?. Similarly, all do-
mains that require deep understanding of text rela-
tions can benefit from knowing the relations that de-
scribe events like ACQUISITION between named en-
tities in ?Yahoo has made a definitive agreement to
acquire Flickr?.
In this paper, we focus on the recognition of se-
mantic relations between pairs of common nomi-
nals. We present a task which will be part of the
SemEval-2010 evaluation exercise and for which we
are developing a new benchmark data set. This data
set and the associated task address three significant
problems encountered in previous work: (1) the def-
inition of a suitable set of relations; (2) the incorpo-
ration of context; (3) the desire for a realistic exper-
imental design. We outline these issues in Section
2. Section 3 describes the inventory of relations we
adopted for the task. The annotation process, the
design of the task itself and the evaluation method-
ology are presented in Sections 4-6.
94
2 Semantic Relation Classification: Issues
2.1 Defining the Relation Inventory
A wide variety of relation classification schemes ex-
ist in the literature, reflecting the needs and granular-
ities of various applications. Some researchers only
investigate relations between named entities or in-
ternal to noun-noun compounds, while others have a
more general focus. Some schemes are specific to a
domain such as biomedical text.
Rosario and Hearst (2001) classify noun com-
pounds from the domain of medicine into 13 classes
that describe the semantic relation between the head
noun and the modifier. Rosario et al (2002) classify
noun compounds using the MeSH hierarchy and a
multi-level hierarchy of semantic relations, with 15
classes at the top level. Stephens et al (2001) pro-
pose 17 very specific classes targeting relations be-
tween genes. Nastase and Szpakowicz (2003) ad-
dress the problem of classifying noun-modifier rela-
tions in general text. They propose a two-level hier-
archy, with 5 classes at the first level and 30 classes
at the second one; other researchers (Kim and Bald-
win, 2005; Nakov and Hearst, 2008; Nastase et al,
2006; Turney, 2005; Turney and Littman, 2005)
have used their class scheme and data set. Moldovan
et al (2004) propose a 35-class scheme to classify
relations in various phrases; the same scheme has
been applied to noun compounds and other noun
phrases (Girju et al, 2005). Lapata (2002) presents a
binary classification of relations in nominalizations.
Pantel and Pennacchiotti (2006) concentrate on five
relations in an IE-style setting. In short, there is little
agreement on relation inventories.
2.2 The Role of Context
A fundamental question in relation classification is
whether the relations between nominals should be
considered out of context or in context. When one
looks at real data, it becomes clear that context does
indeed play a role. Consider, for example, the noun
compound wood shed : it may refer either to a shed
made of wood, or to a shed of any material used to
store wood. This ambiguity is likely to be resolved
in particular contexts. In fact, most NLP applica-
tions will want to determine not all possible relations
between two words, but rather the relation between
two instances in a particular context. While the in-
tegration of context is common in the field of IE (cf.
work in the context of ACE1), much of the exist-
ing literature on relation extraction considers word
pairs out of context (thus, types rather than tokens).
A notable exception is SemEval-2007 Task 4 Clas-
sification of Semantic Relations between Nominals
(Girju et al, 2007; Girju et al, 2008), the first to of-
fer a standard benchmark data set for seven semantic
relations between common nouns in context.
2.3 Style of Classification
The design of SemEval-2007 Task 4 had an im-
portant limitation. The data set avoided the chal-
lenge of defining a single unified standard classifi-
cation scheme by creating seven separate training
and test sets, one for each semantic relation. That
made the relation recognition task on each data set
a simple binary (positive / negative) classification
task.2 Clearly, this does not easily transfer to prac-
tical NLP settings, where any relation can hold be-
tween a pair of nominals which occur in a sentence
or a discourse.
2.4 Summary
While there is a substantial amount of work on re-
lation extraction, the lack of standardization makes
it difficult to compare different approaches. It is
known from other fields that the availability of stan-
dard benchmark data sets can provide a boost to the
advancement of a field. As a first step, SemEval-
2007 Task 4 offered many useful insights into the
performance of different approaches to semantic re-
lation classification; it has also motivated follow-
up research (Davidov and Rappoport, 2008; Ka-
trenko and Adriaans, 2008; Nakov and Hearst, 2008;
O? Se?aghdha and Copestake, 2008).
Our objective is to build on the achievements of
SemEval-2007 Task 4 while addressing its short-
comings. In particular, we consider a larger set of
semantic relations (9 instead of 7), we assume a
proper multi-class classification setting, we emulate
the effect of an ?open? relation inventory by means
of a tenth class OTHER, and we will release to the
research community a data set with a considerably
1http://www.itl.nist.gov/iad/mig/tests/
ace/
2Although it was not designed for a multi-class set-up, some
subsequent publications tried to use the data sets in that manner.
95
larger number of examples than SemEval-2007 Task
4 or other comparable data sets. The last point is cru-
cial for ensuring the robustness of the performance
estimates for competing systems.
3 Designing an Inventory of Semantic Re-
lations Between Nominals
We begin by considering the first of the problems
listed above: defining of an inventory of semantic
relations. Ideally, it should be exhaustive (should al-
low the description of relations between any pair of
nominals) and mutually exclusive (each pair of nom-
inals in context should map onto only one relation).
The literature, however, suggests no such inventory
that could satisfy all needs. In practice, one always
must decide on a trade-off between these two prop-
erties. For example, the gene-gene relation inven-
tory of Stephens et al (2001), with relations like X
phosphorylates Y, arguably allows no overlaps, but
is too specific for applications to general text.
On the other hand, schemes aimed at exhaus-
tiveness tend to run into overlap issues, due
to such fundamental linguistic phenomena as
metaphor (Lakoff, 1987). For example, in the sen-
tence Dark clouds gather over Nepal., the relation
between dark clouds and Nepal is literally a type of
ENTITY-DESTINATION, but in fact it refers to the
ethnic unrest in Nepal.
We seek a pragmatic compromise between the
two extremes. We have selected nine relations with
sufficiently broad coverage to be of general and
practical interest. We aim at avoiding ?real? overlap
to the extent that this is possible, but we include two
sets of similar relations (ENTITY-ORIGIN/ENTITY-
DESTINATION and CONTENT-CONTAINER/COM-
PONENT-WHOLE/MEMBER-COLLECTION), which
can help assess the models? ability to make such
fine-grained distinctions.3
As in Semeval-2007 Task 4, we give ordered two-
word names to the relations, where each word de-
scribes the role of the corresponding argument. The
full list of our nine relations follows4 (the definitions
we show here are intended to be indicative rather
than complete):
3COMPONENT-WHOLE and MEMBER-COLLECTION are
proper subsets of PART-WHOLE, one of the relations in
SemEval-2007 Task 4.
4We have taken the first five from SemEval-2007 Task 4.
Cause-Effect. An event or object leads to an effect.
Example: Smoking causes cancer.
Instrument-Agency. An agent uses an instrument.
Example: laser printer
Product-Producer. A producer causes a product to
exist. Example: The farmer grows apples.
Content-Container. An object is physically stored
in a delineated area of space, the container. Ex-
ample: Earth is located in the Milky Way.
Entity-Origin. An entity is coming or is derived
from an origin (e.g., position or material). Ex-
ample: letters from foreign countries
Entity-Destination. An entity is moving towards a
destination. Example: The boy went to bed.
Component-Whole. An object is a component of a
larger whole. Example: My apartment has a
large kitchen.
Member-Collection. A member forms a nonfunc-
tional part of a collection. Example: There are
many trees in the forest.
Communication-Topic. An act of communication,
whether written or spoken, is about a topic. Ex-
ample: The lecture was about semantics.
We add a tenth element to this set, the pseudo-
relation OTHER. It stands for any relation which
is not one of the nine explicitly annotated relations.
This is motivated by modelling considerations. Pre-
sumably, the data for OTHER will be very nonho-
mogeneous. By including it, we force any model of
the complete data set to correctly identify the deci-
sion boundaries between the individual relations and
?everything else?. This encourages good generaliza-
tion behaviour to larger, noisier data sets commonly
seen in real-world applications.
3.1 Semantic Relations versus Semantic Roles
There are three main differences between our task
(classification of semantic relations between nomi-
nals) and the related task of automatic labeling of
semantic roles (Gildea and Jurafsky, 2002).
The first difference is to do with the linguistic
phenomena described. Lexical resources for theo-
ries of semantic roles such as FrameNet (Fillmore et
96
al., 2003) and PropBank (Palmer et al, 2005) have
been developed to describe the linguistic realization
patterns of events and states. Thus, they target pri-
marily verbs (or event nominalizations) and their de-
pendents, which are typically nouns. In contrast,
semantic relations may occur between all parts of
speech, although we limit our attention to nominals
in this task. Also, semantic role descriptions typi-
cally relate an event to a set of multiple participants
and props, while semantic relations are in practice
(although not necessarily) binary.
The second major difference is the syntactic con-
text. Theories of semantic roles usually developed
out of syntactic descriptions of verb valencies, and
thus they focus on describing the linking patterns of
verbs and their direct dependents, phenomena like
raising and noninstantiations notwithstanding (Fill-
more, 2002). Semantic relations are not tied to
predicate-argument structures. They can also be es-
tablished within noun phrases, noun compounds, or
sentences more generally (cf. the examples above).
The third difference is that of the level of gen-
eralization. FrameNet currently contains more than
825 different frames (event classes). Since the se-
mantic roles are designed to be interpreted at the
frame level, there is a priori a very large number
of unrelated semantic roles. There is a rudimen-
tary frame hierarchy that defines mappings between
roles of individual frames,5 but it is far from com-
plete. The situation is similar in PropBank. Prop-
Bank does use a small number of semantic roles, but
these are again to be interpreted at the level of in-
dividual predicates, with little cross-predicate gen-
eralization. In contrast, all of the semantic relation
inventories discussed in Section 1 contain fewer than
50 types of semantic relations. More generally, se-
mantic relation inventories attempt to generalize re-
lations across wide groups of verbs (Chklovski and
Pantel, 2004) and include relations that are not verb-
centered (Nastase and Szpakowicz, 2003; Moldovan
et al, 2004). Using the same labels for similar se-
mantic relations facilitates supervised learning. For
example, a model trained with examples of sell re-
lations should be able to transfer what it has learned
to give relations. This has the potential of adding
5For example, it relates the BUYER role of the COM-
MERCE SELL frame (verb sell ) to the RECIPIENT role of the
GIVING frame (verb give).
1. People in Hawaii might be feeling
<e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.
2. My new <e1>apartment</e1> has a
<e2>large kitchen</e2>.
Figure 1: Two example sentences with annotation
crucial robustness and coverage to analysis tools in
NLP applications based on semantic relations.
4 Annotation
The next step in our study will be the actual annota-
tion of relations between nominals. For the purpose
of annotation, we define a nominal as a noun or a
base noun phrase. A base noun phrase is a noun and
its pre-modifiers (e.g., nouns, adjectives, determin-
ers). We do not include complex noun phrases (e.g.,
noun phrases with attached prepositional phrases or
relative clauses). For example, lawn is a noun, lawn
mower is a base noun phrase, and the engine of the
lawn mower is a complex noun phrase.
We focus on heads that are common nouns. This
emphasis distinguishes our task from much work in
IE, which focuses on named entities and on consid-
erably more fine-grained relations than we do. For
example, Patwardhan and Riloff (2007) identify cat-
egories like Terrorist organization as participants in
terror-related semantic relations, which consists pre-
dominantly of named entities. We feel that named
entities are a specific category of nominal expres-
sions best dealt with using techniques which do not
apply to common nouns; for example, they do not
lend themselves well to semantic generalization.
Figure 1 shows two examples of annotated sen-
tences. The XML tags <e1> and <e2> mark the
target nominals. Since all nine proper semantic re-
lations in this task are asymmetric, the ordering of
the two nominals must be taken into account. In
example 1, CAUSE-EFFECT(e1, e2) does not hold,
although CAUSE-EFFECT(e2, e1) would. In exam-
ple 2, COMPONENT-WHOLE(e2, e1) holds.
We are currently developing annotation guide-
lines for each of the relations. They will give a pre-
cise definition for each relation and some prototypi-
cal examples, similarly to SemEval-2007 Task 4.
The annotation will take place in two rounds. In
the first round, we will do a coarse-grained search
97
for positive examples for each relation. We will
collect data from the Web using a semi-automatic,
pattern-based search procedure. In order to ensure
a wide variety of example sentences, we will use
several dozen patterns per relation. We will also
ensure that patterns retrieve both positive and nega-
tive example sentences; the latter will help populate
the OTHER relation with realistic near-miss negative
examples of the other relations. The patterns will
be manually constructed following the approach of
Hearst (1992) and Nakov and Hearst (2008).6
The example collection for each relation R will
be passed to two independent annotators. In order to
maintain exclusivity of relations, only examples that
are negative for all relations but R will be included
as positive and only examples that are negative for
all nine relations will be included as OTHER. Next,
the annotators will compare their decisions and as-
sess inter-annotator agreement. Consensus will be
sought; if the annotators cannot agree on an exam-
ple it will not be included in the data set, but it will
be recorded for future analysis.
Finally, two other task organizers will look for
overlap across all relations. They will discard any
example marked as positive in two or more relations,
as well as examples in OTHER marked as positive in
any of the other classes. The OTHER relation will,
then, consist of examples that are negatives for all
other relations and near-misses for any relation.
Data sets. The annotated data will be divided into
a training set, a development set and a test set. There
will be 1000 annotated examples for each of the
ten relations: 700 for training, 100 for development
and 200 for testing. All data will be released under
the Creative Commons Attribution 3.0 Unported Li-
cense7. The annotation guidelines will be included
in the distribution.
5 The Classification Task
The actual task that we will run at SemEval-2010
will be a multi-way classification task. Not all pairs
of nominals in each sentence will be labeled, so the
gold-standard boundaries of the nominals to be clas-
sified will be provided as part of the test data.
6Note that, unlike in Semeval 2007 Task 4, we will not re-
lease the patterns to the participants.
7http://creativecommons.org/licenses/by/
3.0/
In contrast with Semeval 2007 Task 4, in which
the ordering of the entities was provided with each
example, we aim at a more realistic scenario in
which the ordering of the labels is not given. Par-
ticipants in the task will be asked to discover both
the relation and the order of the arguments. Thus,
the more challenging task is to identify the most
informative ordering and relation between a pair
of nominals. The stipulation ?most informative?
is necessary since with our current set of asym-
metrical relations that includes OTHER, each pair
of nominals that instantiates a relation in one di-
rection (e.g., REL(e1, e2)), instantiates OTHER in
the inverse direction (OTHER (e2, e1)). Thus, the
correct answers for the two examples in Figure 1
are CAUSE-EFFECT (earthquake, aftershocks) and
COMPONENT-WHOLE (large kitchen, apartment).
Note that unlike in SemEval-2007 Task 4, we will
not provide manually annotated WordNet senses,
thus making the task more realistic. WordNet senses
did, however, serve for disambiguation purposes in
SemEval-2007 Task 4. We will therefore have to
assess the effect of this change on inter-annotator
agreement.
6 Evaluation Methodology
The official ranking of the participating systems will
be based on their macro-averaged F-scores for the
nine proper relations. We will also compute and re-
port their accuracy over all ten relations, including
OTHER. We will further analyze the results quan-
titatively and qualitatively to gauge which relations
are most difficult to classify.
Similarly to SemEval-2007 Task 4, in order to
assess the effect of varying quantities of training
data, we will ask the teams to submit several sets of
guesses for the labels for the test data, using varying
fractions of the training data. We may, for example,
request test results when training on the first 50, 100,
200, 400 and all 700 examples from each relation.
We will provide a Perl-based automatic evalua-
tion tool that the participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
7 Conclusion
We have introduced a new task, which will be part of
SemEval-2010: multi-way classification of semantic
98
relations between pairs of common nominals. The
task will compare different approaches to the prob-
lem and provide a standard testbed for future re-
search, which can benefit many NLP applications.
The description we have presented here should
be considered preliminary. We invite the in-
terested reader to visit the official task web-
site http://semeval2.fbk.eu/semeval2.
php?location=tasks\#T11, where up-to-
date information will be published; there is also a
discussion group and a mailing list.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. EMNLP 2004, pages 33?40.
Dmitry Davidov and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals using
pattern clusters. In Proc. ACL-08: HLT, pages 227?
235.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Charles J. Fillmore. 2002. FrameNet and the linking be-
tween semantic and syntactic relations. In Proc. COL-
ING 2002, pages 28?36.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-
tohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. 4th Semantic Eval-
uation Workshop (SemEval-2007).
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2008.
Classification of semantic relations between nominals.
Language Resources and Evaluation. In print.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING
92, pages 539?545.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
types of some generic relation arguments: Detection
and evaluation. In Proc. ACL-08: HLT, Short Papers,
pages 185?188.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCAI, pages 945?956.
George Lakoff. 1987. Women, fire, and dangerous
things. University of Chicago Press, Chicago, IL.
Maria Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28:357?388.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 60?67.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. ACL-08: HLT, pages 452?460.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and WordNet-
based features. In Proc. AAAI, pages 781?787.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. COLING 2008, pages 649?656.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. COLING/ACL, pages
113?120.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proc. EMNLP-CoNLL), pages
717?727.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proc. EMNLP 2001,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in re-
lational semantics. In Proc. ACL-02, pages 247?254.
Matthew Stephens, Mathew Palakal, Snehasis
Mukhopadhyay, Rajeev Raje, and Javed Mostafa.
2001. Detecting gene relations from Medline ab-
stracts. In Pacific Symposium on Biocomputing, pages
483?495.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Peter D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. IJCAI, pages 1136?
1141.
99
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1110?1118,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Method to Learn and Construct Taxonomies using the
Web
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
Although many algorithms have been devel-
oped to harvest lexical resources, few organize
the mined terms into taxonomies. We pro-
pose (1) a semi-supervised algorithm that uses
a root concept, a basic level concept, and re-
cursive surface patterns to learn automatically
from the Web hyponym-hypernym pairs sub-
ordinated to the root; (2) a Web based concept
positioning procedure to validate the learned
pairs? is-a relations; and (3) a graph algorithm
that derives from scratch the integrated tax-
onomy structure of all the terms. Comparing
results with WordNet, we find that the algo-
rithm misses some concepts and links, but also
that it discovers many additional ones lacking
in WordNet. We evaluate the taxonomization
power of our method on reconstructing parts
of the WordNet taxonomy. Experiments show
that starting from scratch, the algorithm can
reconstruct 62% of the WordNet taxonomy for
the regions tested.
1 Introduction
A variety of NLP tasks, including inference, tex-
tual entailment (Glickman et al, 2005; Szpektor
et al, 2008), and question answering (Moldovan
et al, 1999), rely on semantic knowledge derived
from term taxonomies and thesauri such as Word-
Net. However, the coverage of WordNet is still lim-
ited in many regions (even well-studied ones such as
the concepts and instances below Animals and Peo-
ple), as noted by researchers such as (Pennacchiotti
and Pantel, 2006) and (Hovy et al, 2009) who per-
form automated semantic class learning. This hap-
pens because WordNet and most other existing tax-
onomies are manually created, which makes them
difficult to maintain in rapidly changing domains,
and (in the face of taxonomic complexity) makes
them hard to build with consistency. To surmount
these problems, it would be advantageous to have
an automatic procedure that can not only augment
existing resources but can also produce taxonomies
for existing and new domains and tasks starting from
scratch.
The main stages of automatic taxonomy induc-
tion are term extraction and term organization. In
recent years there has been a substantial amount of
work on term extraction, including semantic class
learning (Hearst, 1992; Riloff and Shepherd, 1997;
Etzioni et al, 2005; Pasca, 2004; Kozareva et al,
2008), relation acquisition between entities (Girju
et al, 2003; Pantel and Pennacchiotti, 2006; Davi-
dov et al, 2007), and creation of concept lists (Katz
and Lin, 2003). Various attempts have been made to
learn the taxonomic organization of concepts (Wid-
dows, 2003; Snow et al, 2006; Yang and Callan,
2009). Among the most common is to start with a
good ontology and then to try to position the miss-
ing concepts into it. (Snow et al, 2006) maximize
the conditional probability of hyponym-hypernym
relations given certain evidence, while (Yang and
Callan, 2009) combines heterogenous features like
context, co-occurrence, and surface patterns to pro-
duce a more-inclusive inclusion ranking formula.
The obtained results are promising, but the problem
of how to organize the gathered knowledge when
there is no initial taxonomy, or when the initial tax-
onomy is grossly impoverished, still remains.
1110
The major problem in performing taxonomy con-
struction from scratch is that overall concept po-
sitioning is not trivial. It is difficult to discover
whether concepts are unrelated, subordinated, or
parallel to each other. In this paper, we address the
following question: How can one induce the taxo-
nomic organization of concepts in a given domain
starting from scratch?
The contributions of this paper are as follows:
? An automatic procedure for harvesting
hyponym-hypernym pairs given a domain of
interest.
? A ranking mechanism for validating the learned
is-a relations between the pairs.
? A graph-based approach for inducing the taxo-
nomic organization of the harvested terms start-
ing from scratch.
? An experiment on reconstructing WordNet?s
taxonomy for given domains.
Before focusing on the harvesting and taxonomy
induction algorithms, we are going to describe some
basic terminology following (Hovy et al, 2009). A
term is an English word (for our current purposes,
a noun or a proper name). A concept is an item in
the classification taxonomy we are building. A root
concept is a fairly general concept which is located
on the high level of the taxonomy. A basic-level
concept corresponds to the Basic Level categories
defined in Prototype Theory in Psychology (Rosch,
1978). For example, a dog, not a mammal or a col-
lie. An instance is an item in the classification tax-
onomy that is more specific than a concept. For ex-
ample, Lassie, not a dog or collie .
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work. Section 3 describes the
taxonomization framework. Section 4 discusses the
experiments. We conclude in Section 5.
2 Related Work
The first stage of automatic taxonomy induction,
term and relation extraction, is relatively well-
understood. Methods have matured to the point of
achieving high accuracy (Girju et al, 2003; Pantel
and Pennacchiotti, 2006; Kozareva et al, 2008). The
produced output typically contains flat lists of terms
and/or ground instance facts (lion is-a mammal)
and general relation types (mammal is-a animal).
Most approaches use either clustering or patterns
to mine knowledge from structured and unstructured
text. Clustering approaches (Lin, 1998; Lin and Pan-
tel, 2002; Davidov and Rappoport, 2006) are fully
unsupervised and discover relations that are not di-
rectly expressed in text. Their main drawback is that
they may or may not produce the term types and
granularities useful to the user. In contrast, pattern-
based approaches harvest information with high ac-
curacy, but they require a set of seeds and surface
patterns to initiate the learning process. These meth-
ods are successfully used to collect semantic lex-
icons (Riloff and Shepherd, 1997; Etzioni et al,
2005; Pasca, 2004; Kozareva et al, 2008), encyclo-
pedic knowledge (Suchanek et al, 2007), concept
lists (Katz and Lin, 2003), and relations between
terms, such as hypernyms (Ritter et al, 2009; Hovy
et al, 2009) and part-of (Girju et al, 2003; Pantel
and Pennacchiotti, 2006).
However, simple term lists are not enough to solve
many problems involving natural language. Terms
may be augmented with information that is required
for knowledge-intensive tasks such as textual entail-
ment (Glickman et al, 2005; Szpektor et al, 2008)
and question answering (Moldovan et al, 1999). To
support inference, (Ritter et al, 2010) learn the se-
lectional restrictions of semantic relations, and (Pen-
nacchiotti and Pantel, 2006) ontologize the learned
arguments using WordNet.
Taxonomizing the terms is a very powerful
method to leverage added information. Subordi-
nated terms (hyponyms) inherit information from
their superordinates (hypernyms), making it unnec-
essary to learn all relevant information over and over
for every term in the language. But despite many at-
tempts, no ?correct? taxonomization has ever been
constructed for the terms of, say, English. Typically,
people build term taxonomies (and/or richer struc-
tures like ontologies) for particular purposes, using
specific taxonomization criteria. Different tasks and
criteria produce different taxonomies, even when us-
ing the same basic level concepts. This is because
most basic level concepts admit to multiple perspec-
tives, while each task focuses on one, or at most two,
perspectives at a time. For example, a dolphin is a
Mammal (and not a Fish) to a biologist, but is a Fish
1111
(and hence not a Mammal) to a fisherman or anyone
building or visiting an aquarium. More confusingly,
a tiger and a puppy are both Mammals and hence
belong close together in a typical taxonomy, but a
tiger is a WildAnimal (in the perspective of Animal-
Function) and a JungleDweller (in the perspective of
Habitat), while a puppy is a Pet (as function) and a
HouseAnimal (as habitat), which would place them
relatively far from one another. Attempts at pro-
ducing a single multi-perspective taxonomy fail due
to the complexity of interaction among perspectives,
and people are notoriously bad at constructing tax-
onomies adherent to a single perspective when given
terms from multiple perspectives. This issue and the
major alternative principles for taxonomization are
discussed in (Hovy, 2002).
It is therefore not surprising that the second
stage of automated taxonomy induction is harder to
achieve. As mentioned, most attempts to learn tax-
onomy structures start with a reasonably complete
taxonomy and then insert the newly learned terms
into it, one term at a time (Widdows, 2003; Pasca,
2004; Snow et al, 2006; Yang and Callan, 2009).
(Snow et al, 2006) guide the incremental approach
by maximizing the conditional probability over a
set of relations. (Yang and Callan, 2009) introduce
a taxonomy induction framework which combines
the power of surface patterns and clustering through
combining numerous heterogeneous features.
Still, one would like a procedure to organize the
harvested terms into a taxonomic structure starting
fresh (i.e., without using an initial taxonomic struc-
ture). We propose an approach that bridges the gap
between the term extraction algorithms that focus
mainly on harvesting but do not taxonomize, and
those that accept a new term and seek to enrich an al-
ready existing taxonomy. Our aim is to perform both
stages: to extract the terms of a given domain and to
induce their taxonomic organization without any ini-
tial taxonomic structure and information. This task
is challenging because it is not trivial to discover
both the hierarchically related and the parallel (per-
spectival) organizations of concepts. Achieving this
goal can provide the research community with the
ability to produce taxonomies for domains for which
currently there are no existing or manually created
ontologies.
3 Building Taxonomies from Scratch
3.1 Problem Formulation
We define our task as:
Task Definition: Given a root concept, a basic level
concept or an instance, and recursive lexico-syntactic
patterns, (1) harvest in bootstrapping fashion hy-
ponyms and hypernyms subordinated to the root; (2)
filter out erroneous information (extracted concepts
and isa relations); (3) organize the harvested con-
cepts into a taxonomy structure.
!"#!$%&
'%$()%*&
+%#!%,#-!%*& ./0#1-!%*&
/%#,(+0#%*&.-#)(+0#%*& 2-22-$*&
-)(2-$&
103&
10)4%5&
%$%6/-)!&
./%%!-/&
1%%#&73%#&
6"2-&
$(0)&
Figure 1: Taxonomy Induction from Scratch.
Figure 1 shows an example of the task. Start-
ing with the root concept animal and the basic
level concept lion, the algorithm learns new
terms like tiger, puma, deer, donkey of class
animal. Next for each basic level concept, the
algorithm harvests hypernyms and learns that a
lion is-a vertebrate, chordate, feline and mammal.
Finally, the taxonomic structure of each basic
level concept and its hypernyms is induced: ani-
mal?chordate?vertebrate?mammal?feline?lion.
3.2 Knowledge Harvesting
The main objective of our work is not the creation
of a new harvesting algorithm, but rather the or-
ganization of the harvested information in a tax-
onomy structure starting from scratch. There are
many algorithms for hyponym and hypernym har-
vesting from the Web. In our experiments, we use
the doubly-anchored lexico-syntactic patterns and
bootstrapping algorithm introduced by (Kozareva et
al., 2008) and (Hovy et al, 2009).
1112
We are interested in using this approach, because
it is: (1) simple and easy to implement; (2) requires
minimal supervision using only one root concept
and a term to learn new hyponyms and hypernyms
associated to the root; (3) reports higher precision
than current semantic class algorithms (Etzioni et
al., 2005; Pasca, 2004); and (4) adapts easily to dif-
ferent domains.
The general framework of the knowledge harvest-
ing algorithm is shown in Figure 2.
1. Given:
a hyponym pattern Pi={concept such as seed
and *}
a hypernym pattern Pc={* such as term1 and
term2}
a root concept root
a term called seed for Pi
2. build a query using Pi
3. submit Pi to Yahoo! or other search engine
4. extract terms occupying the * position
5. take terms from step 4 and go to step 2.
6. repeat steps 2?5 until no new terms are found
7. rank terms by outDegree
8. for ? terms with outDegree>0, build a query
using Pc
9. submit Pc to Yahoo! or other search engine
10. extract concepts (hypernyms) occupying the *
position
11. rank concepts by inDegree
Figure 2: Knowledge Harvesting Framework.
The algorithm starts with a root concept, seed
term1 of type root and a doubly-anchored pattern
(DAP) such as ?<root> such as <seed> and *?
which learns on the * position new terms of type
root. The newly learned terms, which can be either
instances, basic level or intermediate concepts, are
placed into the position of the seed in the DAP pat-
tern, and the bootstrapping process is repeated. The
process ceases when no new terms are found.
To separate the true from incorrect terms, we use
a graph-based algorithm in which each vertex u is
a term, and an each edge (u, v) ? E corresponds
to the direction in which the term u discovered the
term v. The graph is weighted w(u, v) according
1The input term can be an instance, a basic level or an in-
termediate concept. An intermediate concept is the one that is
located between the basic level and root concepts.
to the number of times the term pair u-v is seen
in unique web snippets. The terms are ranked by
outDegree(u)=
?
?(u,v)?E
w(u,v)
|V |?1 which counts the
number of outgoing links of node u normalized by
the total number of nodes in the graph excluding the
current. The algorithm considers as true terms with
outDegree>0.
All harvested terms are automatically fed into the
hypernym extraction phase. We use the natural or-
der in which the terms discovered each other and
place them into an inverse doubly-anchored pattern
(DAP?1) ?* such as <term1> and <term2>? to
learn hypernyms on the * position. Similarly we
build a graph with nodes h denoting the hypernyms
and nodes t1-t2 denoting the term pairs. The edges
(h, t1 ? t2) ? E? show the direction in which the
term pair discovered the hypernym. The hypernyms
are ranked by inDegree(h)=
?
?(t1?t2,h)?E? w(t1?
t2, h) which rewards hypernyms that are frequently
discovered by various term pairs. The output of
the algorithm is a list of is-a relations between the
learned terms (instances, basic level or intermediate
concepts) and their corresponding hypernyms. For
example, deer is-a herbivore, deer is-a ruminant,
deer is-a mammal.
3.3 Graph-Based Taxonomy Induction
In the final stage of our algorithm, we induce the
overall taxonomic structure using information about
the pairwise positioning of the terms. In the knowl-
edge harvesting and filtering phases, the algorithm
learned is-a relations between the root and the terms
(instances, basic level or intermediate concepts), as
well as the harvested hypernyms and the terms. The
only missing information is the positioning of the in-
termediate concepts located between the basic level
and the root such as mammals, vertibrates, felines,
chordates, among others.
We introduce a concept positioning (CP) proce-
dure that uses a set of surface patterns: ?X such as
Y?, ?X are Y that?, ?X including Y?, ?X like Y?,
?such X as Y? to learn the hierarchical relations for
all possible concept pairs. For each concept pair,
say chordates and vertebrates, we issue the two
following queries:
(a) chordates such as vertebrates
(b) vertebrates such as chordates
1113
If (a) returns more web hits than (b), then chordates
subsumes (or is broader than) vertebrates, other-
wise vertebrates subsumes chordates. For this
pair the such as pattern returned 7 hits for (a) and
0 hits for (b), so that the overall magnitude of the
direction of the relation is weak. To accumulate
stronger evidence, we issue web queries with the
remaining patterns. For the same concept pair, the
overall magnitude of ?X including Y? is 5820 hits
for (a) and 0 for (b).
As shown in Figure 3, the concept positioning pat-
terns cannot always determine the direct taxonomic
organization between two concepts as in the case
of felines and chordates, felines and vertebrates.
One reason is that the concepts are located on dis-
tant taxonomic levels. We humans typically exem-
plify concepts using more proximate ones. There-
fore, the concept positioning procedure can find ev-
idence for the relation ?mammals?felines?, but not
for ?chordates?felines?.
!"#$!%&
'()*(+)!*(,&
-(%#"(,&
$!$$!%,&
%#."&
/0.)1!*(,&
!"#$!%&
/0.)1!*(,&
'()*(+)!*(,&
$!$$!%,&
-(%#"(,&
%#."&
Figure 3: Concept Positioning and Induced Taxonomy.
After the concept positioning procedure has ex-
plored all concept pairs, we encounter two phenom-
ena: (1) direct links between some concepts are
missing and (2) multiple paths can be taken to reach
from one concept to another.
To surmount these problems, we employ a
graph based algorithm that finds the longest
path in the graph G??=(V ??, E??). The nodes
V ??={it1, h1, h2, .., hn, r} represent the input term,
its hypernyms, and the root. An edge (tm, tn) ? E??
indicates that there is a path between the terms tm
and tn. The direction tm ? tn indicates the term
subordination discovered during the CP procedure.
The objective is to find the longest path in G?? be-
tween the root and the input term. Intuitively, find-
ing the longest paths is equivalent to finding the tax-
onomic organization of all concepts.
First, if present, we eliminate all cycles from the
graph. Then, we find all nodes that have no prede-
cessor and those that have no successor. Intuitively,
a node with no predecessors p is likely to be posi-
tioned on the top of the taxonomy (e.g. animal),
while a node with no successor s is likely to be lo-
cated at the bottom (e.g. terms like lion, tiger, puma,
or concepts like krill predators that could not be re-
lated to an instance or a basic level concept during
the CP procedure). We represent the directed graph
as an adjacency matrix A = [am,n], where am,n is
1 if (tm, tn) is an edge of G??, and 0 otherwise. For
each (p, s) pair, we find the list of all paths connect-
ing p with s. In the end, from all discovered can-
didate paths, the algorithm returns the longest one.
The same graph-based taxonomization procedure is
repeated for the rest of the basic level concepts and
their hypernyms.
4 Experiments and Results
To evaluate the performance of a taxonomy induc-
tion algorithm, one can compare against a simple
taxonomy composed of 2?3 levels. However, one
cannot guarantee that the algorithm can learn larger
hierarchies completely or correctly.
Animals provide a good example of the true com-
plexity of concept organization: there are many
types, they are of numerous kinds, people take nu-
merous perspectives over them, and they are rela-
tively well-known to human annotators. In addition,
WordNet has a very rich and deep taxonomic struc-
ture for animals that can be used for direct compar-
ison. We further evaluate our algorithm on the do-
mains of Plants and Vehicles, which share some of
these properties.
4.1 Data Collection
We have run the knowledge harvesting algorithm on
the semantic classes Animals, Plants and Vehicles
starting with only one seed example such as lions,
cucumbers and cars respectively.
First, we formed and submitted the DAP pattern
as web queries to Yahoo!Boss. We retrieved the
top 1000 web snippets for each query. We kept
all unique terms and term pairs. Second, we used
the learned term pairs to form and submit new web
1114
queries DAP?1. In this step, the algorithm harvested
the hypernyms associated with each term. We kept
all unique triples composed of a hypernym and the
term pairs that extracted it. The algorithm ran until
complete exhaustion for 8 iterations for Animals, 10
iterations for Plants and 18 iterations of Vehicles.
Table 1 shows the total number of terms extracted
by the Web harvesting algorithm during the first
stage. In addition, we show the number of terms that
passed the outDegree threshold. We found that the
majority of the learned terms for Animals are basic
level concepts, while for Plants and Vehicles they are
a mixture of basic level and intermediate concepts.
Animals Plants Vehicles
#Extracted Terms 1855 2801 1425
#outDegree(Term)> 0 858 1262 581
Table 1: Learned Terms.
Since human based evaluation of all harvested
terms is time consuming and costly, we have se-
lected 90 terms located at the beginning, in the mid-
dle and in the end of the outDegree ranking. Table
2 summarizes the results.
Plants #CorrectByHand #inWN PrecByHand
rank[1-30] 29 28 .97
rank[420-450] 29 21 .97
rank[1232-1262] 27 19 .90
Vehicles #CorrectByHand #inWN PrecByHand
rank[1-30] 29 27 .97
rank[193-223] 22 18 .73
rank[551-581] 25 19 .83
Table 2: Term Evaluation.
Independently, we can say that the precision of the
harvesting algorithm is from 73 to 90%. In the case
of Vehicles, we found that the learned terms in the
middle ranking do not refer to the meaning of vehi-
cle as a transportation devise, but to the meaning of
vehicle as media (i.e. seminar, newspapers), com-
munication and marketing. For the same category,
the algorithm learned many terms which are missing
from WordNet such as BMW, bakkies, two-wheeler,
all-terrain-vehicle among others.
The second stage of the harvesting algorithm con-
cerns hypernym extraction. Table 3 shows the total
number of hypernyms harvested for all term pairs.
The top 20 highly ranked concepts by inDegree are
the most descriptive terms for the domain. However,
if we are interested in learning a larger set of hy-
pernyms, we found that inDegree is not sufficient
by itself. For example, highly frequent but irrele-
vant hypernyms such as meats, others are ranked
at the top of the list, while low frequent but rele-
vant ones such as protochordates, hooved-mammals,
homeotherms are discarded. This shows that we
need to develop additional and more sensitive mea-
sures for hypernym ranking.
Animals Plants Vehicles
#Extracted Hypernyms 1904 8947 2554
#inDegree(Hypernyms)> 10 110 294 100
Table 3: Learned Hypernyms.
Table 4 shows some examples of the learned an-
imal hypernyms which were annotated by humans
as: correct but not present in WordNet; borderline
which depending on the application could be valu-
able to have or exclude; and incorrect.
CorrectNotInWN {colony|social} insects, grazers, monogastrics
camelid, {mammalian|land|areal} predators
{australian|african} wildlife, filter feeders
hard shelled invertebrates, pelagics
bottom dwellers
Borderline prehistoric animals, large herbivores
pocket pets, farm raised fish, roaring cats
endangered mammals, mysterious hunters
top predators, modern-snakes, heavy game
Incorrect frozen foods, native mammals, red meats
furry predators, others, resources, sorts
products, items, protein
Table 4: Examples of Learned Animal Hypernyms.
The annotators found that 9% of the harvested is-
a relations are missing from WordNet. For example,
cartilaginous fish ? shark; colony insects? bees;
filter feeders? tube anemones among others. This
shows that despite its completeness, WordNet has
still room for improvement.
4.2 A Test: Reconstructing WordNet
As previously discussed in (Hovy et al, 2009), it is
extremely difficult even for expert to manually con-
struct and evaluate the correctness of the harvested
taxonomies. Therefore, we decided to evaluate the
performance of our taxonomization approach recon-
structing WordNet Animals, Plants and Vehicles tax-
onomies.
1115
Given a domain, we select from 140 to 170 of
the harvested terms. For each term, we retrieve all
WordNet hypernyms located on the path between the
input term and the root that is animal, plant or ve-
hicle depending on the domain of interest. We have
found that 98% of the WordNet terms are also har-
vested by our knowledge acquisition algorithm. This
means that being able to reconstruct WordNet?s tax-
onomy is equivalent to evaluating the performance
of our taxonomy induction approach.
Table 5 summarizes the characteristics of the tax-
onomies for the regions tested. For each domain,
we show the total number of terms that must be or-
ganized, and the total number of is-a relations that
must be induced.
Animals Plants Vehicles
#terms 684 554 140
#is-a 4327 2294 412
average depth 6.23 4.12 3.91
max depth 12 8 7
min depth 1 1 1
Table 5: Data for WordNet reconstruction.
Among the three domains we have tested, An-
imals is the most complex and richest one. The
maximum number of levels our algorithm must in-
fer is 11, the minimum is 1 and the average taxo-
nomic depth is 6.2. In total there are three basic level
concepts (longhorns, gaur and bullock) with maxi-
mum depth, twenty terms (basic level and intermedi-
ate concepts) with minimum depth and ninety-eight
terms (wombat, viper, rat, limpkin) with depth 6.
Plants is also a very challenging domain, because
it contains a mixture of scientific and general terms
such as magnoliopsida and flowering plant.
4.3 Evaluation
To evaluate the performance of our taxonomy induc-
tion approach, we use the following measures:
Precision = #is?a found in WordNet and by system#is?a found by system
Recall = #is?a found in WordNet and by system#is?a found in WordNet
Table 6 shows results of the taxonomy induction
of the Vehicles domain using different concept po-
sitioning patterns. The most productive ones are:
?X are Y that? and ?X including Y?. However, the
highest yield is obtained when we combine evidence
from all patterns.
Vehicles Precision Recall
X such as Y .99 (174/175) .42 (174/410)
X are Y that .99 (206/208) .50 (206/410)
X including Y .96 (165/171) .40 (165/410)
X like Y .96 (137/142) .33 (137/410)
such X as Y .98 (44/45) .11 (44/410)
AllPatterns .99 (246/249) .60 ( 246/410)
Table 6: Evaluation of the Induced Vehicle Taxonomy.
Table 7 shows results of the taxonomization of
the Animals and Plants domains. Overall, the ob-
tained results are very encouraging given the fact
that we started from scratch without the usage of
any taxonomic structure. Precision is robust, but we
must further improve recall. Our observation for the
lower recall is that some intermediate concepts re-
late mostly to the high level ones, but not to the basic
level concepts.
Precision Recall
Animals .98 (1643/1688) .38 (1643/4327)
Plants .97 (905/931) .39 (905/2294)
Table 7: Evaluation of the Induced Animal and Plant Tax-
onomies.
Figure 4 shows an example of the taxonomy in-
duced by our algorithm for the vipers, rats, wom-
bats, ducks, emus, moths and penguins basic level
concepts and their WordNet hypernyms.
animals
aquatic_vertebrates chordates invertebrates
vertebrates arthropods
aquatic_birds
duckspenguins
insects
moths
birds
emus
mammalsreptiles
marsupials placentalsrodents
wombats rats
metatherians
snakes
vipers
Figure 4: Induced Taxonomy for Animals.
The biggest challenge of the taxonomization pro-
cess is the merging of independent taxonomic per-
1116
spectives (a deer is a grazer in BehaviorByFeeding,
a wildlife in BehaviorByHabitat, a herd in Behavior-
SocialGroup and an even-toed ungulate in Morpho-
logicalType) into a single hierarchy.
5 Conclusions and Future Work
We are encouraged by the ability of the taxonomiza-
tion algorithm to reconstruct WordNet?s Animal hi-
erarchy, which is one of its most complete and elab-
orated. In addition, we have also evaluated the per-
formance of our algorithm with the Plant and Vehi-
cle WordNet hierarchies.
Currently, our automated taxonomization algo-
rithm is able to build some of the quasi-independent
perspectival taxonomies (Hovy et al, 2009). How-
ever, further research is required to develop methods
that reliably (a) identify the number of independent
perspectives a concept can take (or seems to take in
the domain text), and (b) classify any harvested term
into one or more of them. The result would greatly
simplify the task of the taxonomization stage.
We note that despite this richness, WordNet has
many concepts like camelid, filter feeder, mono-
gastrics among others which are missing, but the
harvesting algorithm can provide. Another promis-
ing line of research would investigate the combina-
tion of the two styles of taxonomization algorithms:
first, the one described here to produce an initial (set
of) taxonomies, and second, the term-insertion algo-
rithms developed in prior work.
Acknowledgments
We acknowledge the support of DARPA contract
number FA8750-09-C-3705. We thank Mark John-
son for the valuable discussions on taxonomy evalu-
ation. We thank the reviewers for their useful feed-
back and suggestions.
References
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
ACL, pages 297?304.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 232?239.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91?134,
June.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 1?8.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
A probabilistic classification approach for lexical tex-
tual entailment. In Proceedings, The Twentieth Na-
tional Conference on Artificial Intelligence and the
Seventeenth Innovative Applications of Artificial Intel-
ligence Conference, pages 1050?1055.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics, pages 539?
545.
Eduard H. Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction and
classification. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2009, pages 948?957.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tions in ontologies. The Semantics of Relationships:
An Interdisciplinary Perspective, pages 91?110.
Boris Katz and Jimmy Lin. 2003. Selectively using rela-
tions to improve precision in question answering. In In
Proceedings of the EACL-2003 Workshop on Natural
Language Processing for Question Answering, pages
43?50.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proceedings of the 19th international
conference on Computational linguistics, pages 1?7.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics, pages
768?774.
Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana Girju, and
Vasile Rus. 1999. Lasso: A tool for surfing the answer
net. In TREC.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
1117
44th Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2006.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137?145.
Marco Pennacchiotti and Patrick Pantel. 2006. Ontolo-
gizing semantic relations. In ACL-44: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 793?800.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 117?
124.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of AAAI Spring Symposium on Learn-
ing by Reading and Learning to Read.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional prefer-
ences. In to appear in Proceedings of the Association
for Computational Linguistics ACL2010.
Eleanor Rosch. 1978. Principles of categorization.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th international
conference on World Wide Web, pages 697?706.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In ACL
2008, Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics, pages
683?691.
Dominic Widdows. 2003. Unsupervised methods for de-
veloping taxonomies by combining syntactic and sta-
tistical information. In Proceedings of HLT-NAACL.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
ACL-IJCNLP ?09: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1, pages 271?279.
1118
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 118?128,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Class Label Enhancement via Related Instances
Zornitsa Kozareva
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
kozareva@isi.edu
Konstantin Voevodski
Boston University
111 Cummington St.
Boston, MA, 02215
kvodski@bu.edu
Shang-Hua Teng
University of Southern California
941 Bloom Walk, SAL 300
Los Angeles, CA 90089
shanghua@usc.edu
Abstract
Class-instance label propagation algorithms
have been successfully used to fuse informa-
tion from multiple sources in order to enrich
a set of unlabeled instances with class labels.
Yet, nobody has explored the relationships be-
tween the instances themselves to enhance an
initial set of class-instance pairs. We pro-
pose two graph-theoretic methods (centrality
and regularization), which start with a small
set of labeled class-instance pairs and use the
instance-instance network to extend the class
labels to all instances in the network. We carry
out a comparative study with state-of-the-art
knowledge harvesting algorithm and show that
our approach can learn additional class labels
while maintaining high accuracy. We conduct
a comparative study between class-instance
and instance-instance graphs used to propa-
gate the class labels and show that the latter
one achieves higher accuracy.
1 Introduction
Many natural language processing applications use
and rely on semantic knowledge resources. Since
manually built lexical repositories such as Word-
Net (Fellbaum, 1998) cover a limited amount of
knowledge and are tedious to maintain over time, re-
searchers have developed algorithms for automatic
knowledge extraction from structured and unstruc-
tured texts. There is a substantial body of work
on extracting is-a relations (Etzioni et al, 2005;
Kozareva et al, 2008), part-of relations (Girju et al,
2003; Pantel and Pennacchiotti, 2006) and general
facts (Lin and Pantel, 2001; Davidov and Rappoport,
2009; Jain and Pantel, 2010). The usefulness of the
generated resources has been shown to be valuable
to information extraction (Riloff and Jones, 1999),
question answering (Katz et al, 2003) and textual
entailment (Zanzotto et al, 2006) systems.
Among the most common knowledge acquisi-
tion approaches are those based on lexical patterns
(Hearst, 1992; Etzioni et al, 2005; Kozareva et al,
2008) and clustering (Lin and Pantel, 2002; Davidov
and Rappoport, 2008). While clustering can find in-
stances and classes that are not explicitly expressed
in text, they often may not generate the granularity
needed by the users. In contrast, pattern-based ap-
proaches generate highly accurate lists, but they are
constraint to the information matched by the pattern
and often suffer from recall. (Pas?ca, 2004; Snow
et al, 2006; Kozareva and Hovy, 2010) have shown
that complete lists of semantic classes and instances
are valuable for the enrichment of existing resources
like WordNet and for taxonomy induction. There-
fore, researchers have focused on the development
of methods that can automatically augment the ini-
tially extracted class-instance pairs.
(Pennacchiotti and Pantel, 2009) fused informa-
tion from pattern-based and distributional systems
using an ensemble method and a rich set of features
derived from query logs, web-crawl and Wikipedia.
(Talukdar et al, 2008) improved class-instance ex-
tractions exploring the relationships between the
classes and the instances to propagate the initial
class-labels to the remaining unlabeled instances.
Later on (Talukdar and Pereira, 2010) showed that
class-instance extraction with label propagation can
be further improved by adding semantic information
118
in the form of instance-attribute edges derived from
independently developed knowledge base. Similarly
to (Talukdar et al, 2008) and (Talukdar and Pereira,
2010), we are interested in enriching class-instance
extractions with label propagation. However, un-
like the previous work, we model the relationships
between the instances themselves to propagate the
initial set of class labels to the remaining unlabeled
instances. To our knowledge, this is the first work
to explore the connections between instances for the
task of class-label propagation.
Our work addresses the following question: Is it
possible to effectively explore the structure of the
text-mined instance-instance networks to enhance
an incomplete set of class labels? Our intuition is
that if an instance like bear belongs to a seman-
tic class carnivore, and the instance bear is con-
nected to the instance fox, then it is more likely that
the unlabeled instance fox is also of class carnivore.
To solve this problem, we propose two graph-based
approaches that use the structure of the instance-
instance graph to propagate the class labels. Our
methods are agnostic to the sources of semantic in-
stances and classes. In this work, we carried out ex-
periments with a state-of-the-art instance extraction
system and conducted a comparative study between
the original and the enhanced class-instance pairs.
The results show that this labeling procedure can be-
gin to bridge the gap between the extraction power
of the pattern-based approaches and the desired re-
call by finding class-instance pairs that are not ex-
plicitly mentioned in text. The contributions of the
paper are as follows:
? We use only the relationships between the in-
stances themselves to propagate class labels.
? We observe how often labels are propagated
along the edges of our semantic network, and
propose two ways to extend an initial set of
class labels to all the instance nodes in the net-
work. The first approach uses a linear sys-
tem to compute the network centrality relative
to the initially labeled instances. The second
approach uses a regularization framework with
respect to a random walk on the network.
? We evaluate the proposed approaches and show
that they discover many new class-instance
pairs compared to state-of-the-art knowledge
harvesting algorithm, while still maintaining
high accuracy.
? We conduct a comparative study between class-
instance and instance-instance graphs used
to propagate class labels. The experiments
show that considering relationships between in-
stances achieves higher accuracy.
The rest of the paper is organized as follows. In
Section 2, we review related work. Section 3 de-
scribes the Web-based knowledge harvesting algo-
rithm used to extract the instance network and the
class-instance pairs necessary for our experimen-
tal evaluation. Section 4 describes the two graph-
theoretic methods for class label propagation using
an instance-instance network. Section 5 shows a
comparative study between the proposed graph al-
gorithms and different baselines. We also show
a comparison between class-instance and instance-
instance graphs used in the label propagation. Fi-
nally, we conclude in Section 6.
2 Related Work
In the past decade, we have reached a good under-
standing on the knowledge harvesting technology
from structured (Suchanek et al, 2007) and unstruc-
tured text. Researchers have harvested with vary-
ing success semantic lexicons (Riloff and Shepherd,
1997) and concept lists (Katz et al, 2003). Many
efforts have also focused on the extraction of is-a
relations (Hearst, 1992; Pas?ca, 2004; Etzioni et al,
2005; Pas?ca, 2007; Kozareva et al, 2008), part-of re-
lations (Girju et al, 2003; Pantel and Pennacchiotti,
2006) and general facts (Etzioni et al, 2005; Davi-
dov and Rappoport, 2009; Jain and Pantel, 2010).
Various approaches have been proposed following
the patterns of (Hearst, 1992) and clustering (Lin
and Pantel, 2002; Davidov and Rappoport, 2008). A
substantial body of work has explored issues such as
reranking the harvested knowledge using mutual in-
formation (Etzioni et al, 2005) and graph algorithms
(Hovy et al, 2009), estimating the goodness of text-
mining seeds (Vyas et al, 2009), organizing the
extracted information (Cafarella et al, 2007a; Ca-
farella et al, 2007b) and inducing term taxonomies
with WordNet (Snow et al, 2006) or starting from
scratch (Kozareva and Hovy, 2010).
119
Since pattern-based approaches tend to be high-
precision and low-recall in nature, recently of great
interest to the research community is the develop-
ment of approaches that can increment the recall of
the harvested class-instance pairs. (Pennacchiotti
and Pantel, 2009) proposed an ensemble seman-
tic framework that mixes distributional and pattern-
based systems with a large set of features from a
web-crawl, query logs, and Wikipedia. (Talukdar
et al, 2008) combined extractions from free text
and structured sources using graph-based label prop-
agation algorithm. (Talukdar and Pereira, 2010)
conducted a comparative study of graph algorithms
and showed that class-instance extraction can be
improved using additional information that can be
modeled as instance-attribute edges.
Closest to our work is that of (Talukdar et al,
2008; Talukdar and Pereira, 2010) who model class-
instance relations to propagate class-labels. Al-
though these algorithms can be applied to other rela-
tions (Alfonseca et al, 2010), to our knowledge yet
nobody has modeled the connections between the in-
stances themselves for the task of class-label prop-
agation. We propose regularization and centrality
graph-theoretic methods, which exploit the instance-
instance network and a small set of class-instance
pairs to propagate the class-labels to the remaining
unlabeled instances. While objectives similar to reg-
ularization have been used for class-label propaga-
tion, the application of node centrality for this task is
also novel. The proposed solutions are intuitive and
almost parameter-free (both methods have a single
parameter, which is easy to interpret and does not
require careful tuning).
3 Knowledge Harvesting from the Web
Our proposed class-label enhancement approaches
are agnostic to the sources of semantic instances and
classes. Several methods have been developed to
harvest instances from the Web (Pas?ca, 2004; Et-
zioni et al, 2005; Pas?ca, 2007; Kozareva et al,
2008) and potentially we can use any of them.
In our experiments, we use the doubly-anchored
(DAP) method of (Kozareva et al, 2008), because it
achieves higher precision than (Etzioni et al, 2005;
Pas?ca, 2007), it is easy to implement and requires
minimum supervision (only one seed instance and a
lexico-syntactic pattern).
For a given semantic class of interest say ani-
mals, the algorithm starts with a seed example of
the class, say whales. The seed instance is fed into
a doubly-anchored pattern ?<semantic-class> such
as <seed> and *?, which extracts on the position
of the * new instances of the semantic class. Then,
the newly acquired instances are individually placed
on the position of the seed in the DAP pattern. The
bootstrapping procedure is repeated until no new in-
stances are found. We use the harvested instances to
build the instance-instance graph in which the nodes
are the learned instances and directed edges like
(whales,dolphins) indicate that the instance whales
extracted the instance dolphins. The edges between
the instances are weighted based on the number of
times the DAP pattern extracted the instances to-
gether.
Different strategies can be employed to acquire
semantic classes for each instance. We follow the
fully automated approach of (Hovy et al, 2009),
which takes the learned instance pairs from DAP and
feeds them into the pattern ?* such as <instance1>
and <instance2>?. The algorithm extracts on the
position of the * new semantic classes related to
instance1. According to (Hovy et al, 2009), the
usage of two instances acts as a disambiguator and
leads to much more accurate semantic class extrac-
tion compared to (Ritter et al, 2009).
4 Methods
We model the output of the instance harvesting al-
gorithm as a directed weighted graph that is given
by a set of vertices V and a set of edges E. We use
n to denote the number of vertices. A node u corre-
sponds to a learned instance, and an edge (u, v) ? E
indicates that the instance v was learned from the in-
stance u using the DAP pattern. The weight of the
edge w(u, v) specifies the number of times the pair
of instances were found by the DAP pattern. We de-
fine the adjacency matrix of the graph as:
A(u, v) =
{
w(u, v) if (u, v) ? E
0 otherwise.
We use dout(u) to specify the out-degree of u:
dout(u) =
?
(u,v)?E w(u, v), and din(v) to specify
the in-degree of v: din(v) = ?(u,v)?E w(u, v).
120
We represent the initial set of instances L that are
believed to belong to class C (the set of labeled in-
stances) by a row vector l ? {0, 1}n, where l(u) = 1
if u ? L. Our objective is to compute a vector l?
where l?(u) is proportional to how likely it is that u
belongs to C. We write all vectors as row vectors,
and use ~c to denote a 1 by n constant vector such
that ~c(u) = c for all u ? V .
4.1 Personalized Centrality
Our first approach is based on the intuition that if
u ? C and (u, v) ? E, then it is more likely that
v ? C. Moreover, the larger the weight of the edge
w(u, v), the more likely it is that v ? C. When we
extend this intuition to all the in-neighbors, we say
that the score of each node is proportional to the sum
of the scores of its in-neighbors scaled by the edge
weights: l?(v) = ??(u,v)?E l?(u)w(u, v). We can
verify that the vector l? must then satisfy l? = ?l?A,
so it is an eigenvector of the adjacency matrix of the
graph with an eigenvalue of ?.
However, this formulation is insufficient because
even though it captures our intuition that the nodes
get their scores from their in-neighbors, we are still
ignoring the initial scores of the nodes. A way to
take the initial scores into consideration is to com-
pute the following steady-state equation:
l? = l + ? ? l?A. (1)
Equation 1 specifies that the score l?(u) of each node
u is the sum of its initial score l(u) and the weighted
sum of the scores of its neighbors, which is scaled
by ?. This equation is known as ?-centrality, which
was first introduced by (Bonacich and Lloyd, 2001).
The ? parameter controls how much the score of
each node depends on the scores of its neighbors.
When ? = 0 the score of each node is equivalent to
its initial score, and does not depend on the scores
of its neighbors at all.
Alternately, we can think of the vector l? as the
fixed-point of the process in which in each iteration
some node v updates its score l?(v) by setting l?(v) =
l(v) + ??(u,v)?E w(u, v)l?(u).
Solving Equation 1 we can see that l? = l(I ?
?A)?1, where I is the identity matrix of size n.
The solution is also closely related to the following
expression, which is known as a Katz score (Katz,
1953):
s
??
t=1
?tAt.
We can verify that At(u, v) gives the number of
paths of length t between u and v. Katz proposed
using the above expression with the starting vector
s = ~1 to measure centrality in a network. Therefore,
the score of node v is given by the number of paths
from u to v for all u ? V , with longer paths given
less weight based on the value of ?. The method
proposed here measures a similar quantity with a
non-uniform starting vector. To show the relation-
ship between the two measures we use the identity
that??t=1 ?tAt = (I ??A)?1? I . It is easy to see
that l? = l(I ? ?A)?1
= l(??t=1 ?tAt + I)
= l??t=1 ?tAt + l
= l??t=0 ?tAt.
(2)
Equation 2 shows that l?(v) is given by the number
of paths from u to v for all u ? L (the initial labeled
set). Using a larger value of ? corresponds to giving
more weight to paths of longer length. The summa-
tion??t=0 ?tAt converges as long as |?| < 1/?max,
where ?max is the largest eigenvalue of A. There-
fore, we can only consider values of ? in this range.
4.2 Regularization Using Random Walks
Our second approach constrains l? to be as consistent
or smooth as possible with respect to the structure
of the graph. The simplest way to express this is
to require that for each edge (u, v) ? E, the scores
of the endpoints l?(u) and l?(v) must be as similar as
possible. Moreover, the greater the weight of the
edge w(u, v) the more important it is for the scores
to match. Using this intuition we can define the fol-
lowing optimization problem:
argminl??{0,1}n
?
(u,v)?E
(l?(u)? l?(v))2.
Setting l? = ~0 or l? = ~1 clearly optimizes this func-
tion, but does not give a meaningful solution. How-
ever, we can additionally constrain l? by requiring
that the initial labels cannot be modified, or more
generally penalizing the discrepancy between l?(u)
and l(u) for u ? L. The methods of (Talukdar and
Pereira, 2010) optimize objective functions of this
type.
121
Unlike the work of (Talukdar and Pereira, 2010),
here we use an objective function that considers
smoothness with respect to a random walk on the
graph. Performing a random walk allows us to take
more of the graph structure into account. For exam-
ple, if nodes u and v are part of the same cluster then
it is likely that the edge (u, v) is heavily traversed
during the random walk, and should have a lot of
probability in the stationary distribution of the walk.
Simply considering the weight of the edge w(u, v)
gives us no such information. Therefore if our objec-
tive function requires the scores to be consistent with
respect to the stationary probability of the edges in
the random walk, we can compute scores that are
consistent with the clustering structure of the graph.
Our semantic network is not strongly connected,
so we must make some modifications to the random
walk to ensure that it has a stationary distribution.
Section 4.2.1 describes our random walk and how
we compute the transition probability matrix P and
its stationary probability distribution pi. The defini-
tion of our objective function and the description of
how it is optimized is given in Section 4.2.2.
4.2.1 Teleporting Random Walk
Formally, a random walk is a process where at
each step we move from some node to one of its
neighbors. The transition probabilities are given
by edge weights, therefore the transition probability
matrix W is the normalized adjacency matrix where
each row sums to one:
W = D?1A.
Here the D matrix is the degree matrix, which is a
diagonal matrix given by
D(u, v) =
{
dout(u) if u = v
0 otherwise.
In our semantic network some nodes have no out-
neighbors, so in order to compute W we first add a
self-loop to any such node. In addition, we modify
the random walk to reset at each step with nonzero
probability ? to ensure that it has a steady-state
probability distribution. When the walk resets it
jumps or teleports to any node in the graph with
equal probability. The transition probability matrix
of this process is given by
P = ?K + (1? ?)W,
where K is an n by n matrix given by K(u, v) = 1nfor all u, v ? V . The stationary distribution pi must
satisfy pi = piP . Equivalently pi can be viewed as a
solution to the following PageRank equation:
pi = ?s+ (1? ?)piW.
Here the starting vector s = 1n~1 gives the prob-ability distribution for where the walk transitions
when it resets. In our computations we use a jump
probability ? = 0.15, which is standard for com-
putations of PageRank. The stationary distribution
pi can be computed by either solving the PageRank
equation or computing the eigenvector of P corre-
sponding to the eigenvalue of 1.
4.2.2 Regularization
(Zhou et al, 2005) propose the following function
to measure the smoothness of l? with respect to the
stationary distribution of the random walk:
?(l?) = 12
?
(u,v)?E
pi(u)P (u, v)
( l?(u)?
pi(u)
? l?(v)?
pi(v)
)2
.
Here pi(u)P (u, v) gives the steady-state proba-
bility of traversing the edge (u, v), and pi(u) and
pi(v) specify how much probability u and v have
in the stationary distribution pi. Zhou et al point
out that using this function gives better results than
smoothness with respect to the edge weights, which
can be formulated by replacing pi(u)p(u, v) with
w(u, v), and replacing pi(u) and pi(v) with dout(u)
and din(v), respectively. This observation is con-
sistent with our intuition that considering a random
walk takes more of the graph structure into account.
In addition to minimizing ?(l?), we also want l? to
be as close as possible to l, which gives the follow-
ing optimization problem:
argminl??Rn{?(y?) + ?||l? ? l||2}. (3)
Here the ? > 0 parameter specifies the tradeoff be-
tween the two terms: using a larger ? corresponds to
placing more emphasis on agreement with the initial
labels. (Zhou et al, 2005) show that this objective is
optimized by computing
l? = (I ? ??)?1l, (4)
where ? = (?1/2P??1/2 + ??1/2P?1/2)/2, and
? = 1/(1 + ?). ? is a diagonal matrix given by
122
?(u, v) =
{
pi(u) if u = v
0 otherwise.
Zhou et al propose this approach for semi-
supervised learning of labels on the graph, given an
initial vector l such that l(u) = 1 if vertex u has the
label, l(u) = ?1 if u does not have the label, and
l(u) = 0 if the vertex is unlabeled. They propose
taking the sign of l?(u) to classify u as positive or
negative. Using our labeling procedure we do not
have any negative examples, so our initial vector l
is non-negative, resulting in a non-negative vector l?.
This is not a problem because we can still interpret
l?(u) to be proportional to how likely it is that u has
the label. Rather than trying different settings of ?,
we directly vary ?, with a smaller ? placing more
emphasis on agreement with initial labels.
5 Experimental Evaluation
5.1 Data Collection
For our experimental study, we select three widely
used domains in the harvesting community (Et-
zioni et al, 2005; Pas?ca, 2007; Hovy et al, 2009;
Kozareva and Hovy, 2010): animals and vehicles.
For each domain we randomly selected different se-
mantic classes, which resulted in 20 classes alto-
gether. To generate the instance-instance seman-
tic network, we use the harvesting procedure de-
scribed in Section 3. For example, to learn instances
associated with animals, we instantiate the boot-
strapping algorithm with the semantic class animals,
the seed instance bears and the pattern ?animals
such as bears and *?. We submitted the pattern as
queries to Yahoo!Boss and collected new instances.
We ranked the instances following (Kozareva et al,
2008) which resulted in 397 animal, 4471 plant and
1425 vehicle instances. Table 1 shows the number
of nodes (instances) and directed edges for the con-
structed semantic networks.
class #instances #directed-edges
animals 397 2812
vehicles 1425 3191
Table 1: Nodes & Edges in the Instance Network.
Next, we use the harvested instances to auto-
matically learn the semantic classes associated with
them. For example, bears and wolves are animals
but also mammals, predators, vertebrates among
others. The obtained class harvesting results are
shown in Table 2. We indicate with Inst(Hovy et
al., 2009) the number of instances in the semantic
network that discovered the class during the pattern-
based harvesting, and with InstInWordNet the num-
ber of instances in the semantic network belonging
to the class according to WordNet.
ClassName Inst(Hovy et al, 2009) InstInWordNet
arthropods 12 50
carnivores 24 57
chordates 2 313
eutherians 3 193
insects 5 29
invertebrates 53 84
mammals 114 205
reptile 5 22
ruminants 14 34
ungulates 16 66
crafts 24 68
motor vehicles 27 127
self-propelled vehicles 36 145
vessels 11 36
wheeled vehicles 54 190
Table 2: Learned & Gold Standard Class-Instances.
We can see that the pattern-based approach of
(Hovy et al, 2009) does not recover a lot of the
class-instance relations present in WordNet. Be-
cause of this gap between the actual and the har-
vested class-instance pairs arises the objective of our
work, which is to explore the relationships between
the instances to propagate the initially learned class
labels to the remaining unlabeled instances. To eval-
uate the performance of our approach, we use as a
gold standard the WordNet class-instance mappings.
5.2 Testing Our Approach
Our approach is based on the intuition that given a
labeled instance u of class C, and an instance v in
our network, if there is an edge (u, v) then it is more
likely that v has the label C as well. For example,
if the instance bears is of class vertebrates and there
is an edge between the instances bears and wolves,
then it is likely that wolves are also vertebrates.
Before proceeding with the instance-instance class-
label propagation algorithms, first we study whether
this intuition is correct.
Individually for each class label C, we construct a
set TC that contains all instances in the network be-
longing to C according to WordNet. Then we com-
pute the probability that v belongs to C in WordNet
123
given that (u, v) is an edge in the instance network
and u belongs to C in WordNet: Prh = Pr[v ?
TC | (u, v) ? E and u ? TC ]. We compare
this to the background probability Prb = Pr[v ?
TC | u, v ? V and u ? TC ], which gives the proba-
bility that v belongs to C in WordNet if it is chosen
at random. In other words, if Prh = 1, this means
that whenever u has the label C and (u, v) is an
edge, then v is always labeled with C. If indeed this
is the case, then a good classifier can simply take the
initial set L and extend the labels to all nodes reach-
able from L in the semantic network. The larger the
difference between Prh and Prb, the more informa-
tion the links of the instance network carry for the
task of label propagation. Table 3 shows the Prh
and Prb values for each class.
CLASS Prh Prb
arthropods .46 .12
carnivores .49 .14
chordates .95 .80
eutherians .80 .49
insects .31 .07
invertebrates .74 .21
mammals .82 .52
reptile .27 .05
ruminants .39 .08
ungulates .60 .16
crafts .07 .05
motor vehicles .10 .09
self-propelled vehicles .11 .10
vessels .08 .02
wheeled vehicles .13 .13
Table 3: Learned & Gold Standard Class-Instances.
This study verifies our intuition that using the re-
lationships between the instances to extend a class
label to the remaining unlabeled nodes is an effec-
tive approach to enhancing an incomplete set of ini-
tial labels.
5.3 Comparative Study
The objective of our work is given a set of initially
labeled nodes L, to assign to each node a score
that indicates how likely it is to belong to L. The
simplest way to do this using the edges of the in-
stance network is to say that a node that has more
in-neighbors that have a certain label is more likely
to have this label. We define the in-neighbor score
i(v) of a node v as i(v) = |{u ? V |(u, v) ?
E and u ? L}|. We expect that the higher the in-
neighbor score of v, the more likely it is that v has
the label L. The personalized centrality method
that we proposed generalizes this intuition to indi-
rect neighbors (see Methods). Our regularization
using random walks technique further explores the
link structure of the instance network by considering
a random walk on it (see Methods). We compare our
approaches with a method that labels nodes at ran-
dom. The expected accuracy for class C is given by
|TC |
n , where n is the number of nodes in the network,and TC is the set containing all nodes that belong to
C according to WordNet. In other words, given that
there are 84 nodes in the network that are classified
as invertebrate according to WordNet, and there are
397 nodes in total, if we choose any number of nodes
at random our expected accuracy is 21%.
We evaluate the performance of our approaches
against the WordNet gold standard and show the ob-
tained results in Tables 4 and 5.
Invertebrates
rank centrality regularization in-neighbor random
5 1.0 1.0 .80 .21
10 1.0 1.0 .70 .21
20 .95 1.0 .75 .21
50 .96 .98 .76 .21
100 .69 .73 .67 .21
Mammals
rank centrality regularization in-neighbor random
5 .80 1.0 .80 .52
10 .90 1.0 .90 .52
20 .95 .95 .85 .52
50 .86 .96 .80 .52
100 .92 .92 .76 .52
Carnivores
rank centrality regularization in-neighbor random
5 1.0 1.0 .80 .14
10 .80 .80 .60 .14
20 .80 .85 .55 .14
50 .50 .68 .48 .14
100 .41 .44 .41 .14
Table 4: Accuracy @ Different Ranks.
Table 4 shows the accuracy at rank R calculated
as the number of correctly labeled instances with
class C at rank R divided by the total number of
instances with class C at rank R. Due to space limi-
tation, we show detailed ranking only for three of the
classes. We can see that using the semantic network
significantly enhances our ability to learn class la-
bels. Even the simple in-neighbor method produces
results that are very significant compared to chance.
Our centrality and regularization techniques further
explore the structure of the semantic network to give
124
better predictions.
Table 5 shows the accuracy of the class label prop-
agation algorithms for each class. For each class we
consider the top k ranked nodes, where k is the num-
ber of instances that belong to this class according
to WordNet. For example, the accuracy of central-
ity for carnivores is 80% showing that from the top
57 ranked animal instances, 80% belong to carni-
vores. In the final column we also report the per-
formance of a label propagation algorithm that uses
class-instance graph instead of an instance-instance
graph. To build the graph we remove the edges
between the instances and keep the class-instance
mappings discovered by the harvesting algorithm of
(Hovy et al, 2009). We use the modified adsorption
algorithm (MAD) of (Talukdar et al, 2008), which
is freely available from the Junto toolkit1. To rank
the instances for each class label produced by Junto,
we use the computed label scores as a ranking crite-
ria and measure accuracy similarly to centrality and
regularization.
class Centrality Regular. Rand MAD
arthropods .50 .60 .12 .56
carnivores .80 .85 .14 .44
chordates .81 .83 .80 .79
eutherians .54 .60 .49 .60
insects .38 .52 .07 .17
invertebrates .94 .96 .21 .64
mammals .82 .90 .52 .63
reptile .45 .55 .05 .14
ruminants .41 .44 .08 .41
ungulates .44 .61 .16 .32
crafts .47 .56 .05 .35
motor vehicle .45 .48 .09 .24
self-propelled vehicle .49 .47 .10 .27
vessel .33 .39 .02 .31
wheeled vehicle .51 .52 .13 .33
Table 5: Comparative Study.
The obtained results show that for almost all cases
the methods that use the structure of the instance net-
work significantly outperform predictions that use
the class-instance graph. This indicates that we
can indeed learn a lot form the instance-instance
relationships by exploring the structure of the in-
stance network. Among all approaches regulariza-
tion achieves the best results. We believe that reg-
ularization works well because it considers a ran-
dom walk on the semantic graph, and within-cluster
1http://code.google.com/p/junto/
edges are traversed more often in a random walk.
The regularization technique computes scores that
are consistent with the clustering structure of the
graph by requiring that the endpoints of highly tra-
versed edges, which are likely in the same cluster,
have similar scores (see Methods). Overall, regu-
larization enhanced the original output generated by
the pattern-based knowledge harvesting approach of
(Hovy et al, 2009) with 1219 new class-instance
pairs (75% additional information) while maintain-
ing 61.87% accuracy.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300  350
A
c
c
u
r
a
c
y
Rank
Centrality
_=0.01/hmax _=0.05/hmax _=0.10/hmax _=0.25/hmax _=0.50/hmax _=0.99/hmax Random
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300  350
A
c
c
u
r
a
c
y
Rank
Regularization
a=0.01 a=0.05a=0.10a=0.25a=0.50a=0.99Random
Figure 1: Parameter Tuning For Invertebrates.
5.4 Parameter Tuning
Both of our centrality and regularization methods
have a single tunable parameter. For centrality the
parameter ? controls how much the label of each
node depends on the labels of its neighbors in the
125
graph. The values range from 0 to 1/?max, where
?max is the largest eigenvalue of the adjacency ma-
trix of the semantic network. When ? = 0 the label
of each node is equivalent to its initial label, while
higher values of ? give more weight to the labels of
nodes that are further away.
For regularization the parameter ? controls how
much emphasis is placed on the agreement between
the initial and learned labels. The values of ? are
between 0 and 1. Smaller values require that the
learned labels be more consistent with the original
labels. When ? = 0 the learned labels will exactly
match the original labels.
For each method we try several parameter settings
and show the results in Figure 1 for the propagation
of the class label invertebrate. We can see that both
methods are quite insensitive to the parameter set-
tings, unless we choose very extreme values that ig-
nore the original labels.
5.5 Effect of number of labeled class-instances
We also study how the quality of the results is af-
fected by the number of initial class-instance pairs
used by our propagation methods. We conduct ex-
periments using only 25%, 50%, 75% and 100% of
the initial class-instance pairs learned by (Hovy et
al., 2009). Figure 2 shows the results for the label
propagation of the class invertebrate.
The performance of our methods significantly im-
proves when we incorporate more labels. Still, if we
are less concerned with recall and want to find small
sets of nodes with very high accuracy, the number
of initial labels is less important. For example, start-
ing with only 13 labeled nodes we can still achieve
100% accuracy for the top 30 nodes using regular-
ization, and 96% accuracy for the top 25 nodes using
centrality.
6 Conclusions
In this paper we proposed a centrality and regular-
ization graph-theoretic methods that explore the re-
lationships between the instances themselves to ef-
fectively extend a small set of class-instance labels
to all instances in a semantic network. The proposed
approaches are intuitive and almost parameter-free.
We conducted a series of experiments in which we
compared the effectiveness of the centrality and reg-
 0 0.1 0.2 0.3 0.4 0.5
 0.6 0.7 0.8 0.9 1
 50  100  150  200Ac
curacy
Rank
Centrality Random25% Initial Label50% Initial Label100% Initial Label
 0 0.1 0.2 0.3 0.4 0.5
 0.6 0.7 0.8 0.9 1
 50  100  150  200Ac
curacy
Rank
Regularization
Random25% Initial Label50% Initial Label100% Initial Label
Figure 2: Effect of Number of Initial Class-Instance
Pairs for Invertebrates.
ularization methods to learn new labels for the un-
labeled instances. We showed that the enhanced
class labels improve the original output generated by
the pattern-based knowledge harvesting approach of
(Hovy et al, 2009). Finally, we have studied the
impact of the class-instance and instance-instance
graphs for the class-label propagation task. The lat-
ter approach has shown to produce much more ac-
curate results. In the future, we want to apply our
approach to Web-based taxonomy induction, which
according to (Kozareva and Hovy, 2010) is stifled
due to the lacking relations between the instances
and the classes, and the classes themselves. The pro-
posed methods can be also applied to enhance fact
126
farms (Jain and Pantel, 2010).
Acknowledgments
We acknowledge the support of DARPA contract
number FA8750-09-C-3705 and NSF grant IIS-
0429360. We would like to thank the three anony-
mous reviewers for their useful comments and sug-
gestions and Partha Talukdar for the discussions on
modified adsorption.
References
Enrique Alfonseca, Marius Pasca, and Enrique Robledo-
Arnuncio. 2010. Acquisition of instance attributes
via labeled and related instances. In Proceeding of
the 33rd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?10, pages 58?65.
Phillip Bonacich and Paulette Lloyd. 2001. Social Net-
works, 23(3):191?201.
Michael J. Cafarella, Christopher R, Dan Suciu, Oren Et-
zioni, and Michele Banko. 2007a. Structured query-
ing of web text: A technical challenge. In in CIDR.
Michael J. Cafarella, Dan Suciu, and Oren Etzioni.
2007b. Navigating extracted data with schema discov-
ery. In Tenth International Workshop on the Web and
Databases, WebDB 2007WebDB.
Dmitry Davidov and Ari Rappoport. 2008. Classification
of semantic relationships between nominals using pat-
tern clusters. In Proceedings of ACL-08: HLT, pages
227?235, June.
Dmitry Davidov and Ari Rappoport. 2009. Geo-mining:
Discovery of road and transport networks using direc-
tional patterns. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP-09, pages 267?275.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91?134,
June.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 1?8.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics, pages 539?
545.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009.
Toward completeness in concept extraction and clas-
sification. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 948?957.
Alpa Jain and Patrick Pantel. 2010. Factrank: Random
walks on a web of facts. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 501?509.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003. In-
tegrating web-based and corpus-based techniques for
question answering. In Proceedings of the twelfth text
retrieval conference (TREC), pages 426?435.
Leo Katz. 1953. A new status index derived from socio-
metric analysis. Psychometrika, 18:39?43.
Zornitsa Kozareva and Eduard Hovy. 2010. A semi-
supervised method to learn and construct taxonomies
using the web. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1110?1118.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics ACL-08: HLT, pages 1048?1056.
Dekang Lin and Patrick Pantel. 2001. Dirt - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of the 19th international confer-
ence on Computational linguistics, pages 1?7.
Marius Pas?ca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137?145.
Marius Pas?ca. 2007. Organizing and searching the world
wide web of facts ? step two: harnessing the wisdom
of the crowds. In Proceedings of the 16th international
conference on World Wide Web, pages 101?110.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 113?120.
Marco Pennacchiotti and Patrick Pantel. 2009. Entity
extraction via ensemble semantics. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
127
Language Processing: Volume 1 - Volume 1, EMNLP
?09, pages 238?247.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In AAAI ?99/IAAI ?99: Proceedings of the Six-
teenth National Conference on Artificial intelligence.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Empirical Methods for Natural Language
Processing, pages 117?124.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discov-
ery. In Proceedings of the AAAI Spring Symposium on
Learning by Reading and Learning to Read.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, ACL-44, pages 801?808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th international
conference on World Wide Web, pages 697?706.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1473?1481.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 582?
590.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceeding of the 18th ACM conference
on Information and knowledge management, CIKM
?09, pages 225?234.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In ACL-44: Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 849?856.
Dengyong Zhou, Jiayuan Huang, and Bernhard
Scho?lkopf. 2005. Learning from labeled and
unlabeled data on a directed graph. In Proceedings
of the 22nd international conference on Machine
learning, ICML ?05, pages 1036?1043.
128
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 618?626,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Not All Seeds Are Equal: Measuring the Quality of Text Mining Seeds
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
Open-class semantic lexicon induction is of
great interest for current knowledge harvest-
ing algorithms. We propose a general frame-
work that uses patterns in bootstrapping fash-
ion to learn open-class semantic lexicons for
different kinds of relations. These patterns re-
quire seeds. To estimate the goodness (the po-
tential yield) of new seeds, we introduce a re-
gression model that considers the connectiv-
ity behavior of the seed during bootstrapping.
The generalized regression model is evaluated
on six different kinds of relations with over
10000 different seeds for English and Span-
ish patterns. Our approach reaches robust per-
formance of 90% correlation coefficient with
15% error rate for any of the patterns when
predicting the goodness of seeds.
1 Introduction: What is a Good Seed?
The automated construction of semantically typed
lexicons (terms classified into their appropriate se-
mantic class) from unstructured text is of great im-
portance for various kinds of information extraction
(Grishman and Sundheim, 1996), question answer-
ing (Moldovan et al, 1999), and ontology popu-
lation (Suchanek et al, 2007). Maintaining large
semantic lexicons is a time-consuming and tedious
task, because open classes (such as: all singers, all
types of insects) are hard to cover completely, and
even closed classes (such as: all countries, all large
software companies) change over time. Since it is
practically impossible for a human to collect such
knowledge adequately, many supervised, unsuper-
vised, and semi-supervised techniques have been de-
veloped.
All these techniques employ some sort of context
to specify the appearance in text of the desired in-
formation. This approach is based on the general
intuition, dating back at least to the distributional
similarity idea of (Harris, 1954), that certain con-
texts are specific enough to constrain terms or ex-
pressions within them to be specific classes or types.
Often, the context is a string of words with an empty
slot for the desired term(s); sometimes, it is a regu-
lar expression-like pattern that includes word classes
(syntactic or semantic); sometimes, it is a more ab-
stract set of features, including orthographic fea-
tures like capitalization, words, syntactic relations,
semantic types, and other characteristics, which is
the more complete version of the distributional sim-
ilarity approach.
In early information extraction work, these con-
texts were constructed manually, and resembled reg-
ular expressions (Appelt et al, 1995). More re-
cently, researchers have focused on learning them
automatically. Since unsupervised algorithms re-
quire large training data and may or may not produce
the types and granularities of the semantic class de-
sired by the user, and supervised algorithms may re-
quire a lot of manual oversight, semi-supervised al-
gorithms have become more popular. They require
only a couple of seeds (examples filling the desired
semantic context) to enable the learning mechanism
to learn patterns that extract from unlabeled texts
additional instances of the same class (Riloff and
Jones, 1999; Etzioni et al, 2005; Pasca, 2004).
Sometimes, the pattern(s) learned are satisfactory
618
enough to need no further elaboration. They are
applied to harvest as many additional terms of the
desired type as possible (for example, the instance-
learning pattern ?<type> such as ?? introduced in
(Hearst, 1992)). More often, the method is applied
recursively: once some pattern(s) have been learned,
they are used to find additional terms, which are then
used as new seeds in the patterns to search for addi-
tional new patterns, etc., until no further patterns are
found. At that point, the satisfactory patterns are se-
lected and large-scale harvesting proceeds as usual.
In an interesting variation of this method, (Kozareva
et al, 2008) describe the ?doubly-anchored pat-
tern? (DAP) that includes a seed term in conjunc-
tion with the open slot for the desired terms to be
learned, making the pattern itself recursive by al-
lowing learned terms to replace the initial seed terms
directly: ?<type> such as <seed> and ??.
Context-based information harvesting is well un-
derstood and has been the focus of extensive re-
search. The core unsolved problem is the selec-
tion of seeds. In current knowledge harvesting al-
gorithms, seeds are chosen either at random (Davi-
dov et al, 2007; Kozareva et al, 2008), by picking
the top N most frequent terms of the desired class
(Riloff and Jones, 1999; Igo and Riloff, 2009), or by
asking experts (Pantel et al, 2009). None of these
methods is quite satisfactory. (Etzioni et al, 2005)
report on the impact of seed set noise on the final
performance of semantic class learning, and Pan-
tel et al observe a tremendous variation in the en-
tity set expansion depending on the initial seed set
composition. These studies show that the selection
of ?good? seeds is very important. Recently, (Vyas
et al, 2009) proposed an automatic system for im-
proving the seeds generated by editors (Pantel et al,
2009). The results show 34% improvement in final
performance using the appropriate seed set. How-
ever, using editors to select seeds or to guide their
seed selection process is expensive and therefore not
always possible. Because of this, we address in this
paper two questions: ?What is a good seed?? and
?How can the goodness of seeds be automatically
measured without human intervention??.
The contributions of this paper are as follows:
? First, we use recursive patterns to automatically
learn seeds for open-class semantic lexicons.
? Second, we define what the ?goodness? of a
seed term is. Then we introduce a regression
model of seed quality measurement that, after
a certain amount of training, automatically es-
timates the goodness of new seeds with above
90% accuracy for bootstrapping with the given
relation.
? Next, importantly, we discover that training a
regression model on certain relations enables
one to predict the goodness of a seed even for
other relations that have never been seen be-
fore, with an accuracy rate of over 80%.
? We conduct experiments with six kinds of
relations and more than 10000 automatically
harvested seed examples in both English and
Spanish.
The rest of the paper is organized as follows.
In the next section, we review related work. Sec-
tion 3 describes the recursive pattern bootstrap-
ping (Kozareva et al, 2008). Section 4 presents our
seed quality measurement regression model. Sec-
tion 5 discusses experiments and results. Finally, we
conclude in Section 6.
2 Related Work
Seeds are used in automatic pattern extraction from
text corpora (Riloff and Jones, 1999) and from the
Web (Banko, 2009). Seeds are used to harvest in-
stances (Pasca, 2004; Etzioni et al, 2005; Kozareva
et al, 2008) or attributes of a given class (Pas?ca and
Van Durme, 2008), or to learn concept-specific re-
lations (Davidov et al, 2007), or to expand already
existing entity sets (Pantel et al, 2009). As men-
tioned above, (Etzioni et al, 2005) report that seed
set composition affects the correctness of the har-
vested instances, and (Pantel et al, 2009) observe an
increment of 42% precision and 39% recall between
the best and worst performing seed sets for the task
of entity set expansion.
Because of the large diversity of the usage of
seeds, there has been no general agreement regard-
ing exactly how many seeds are necessary for a
given task. According to (Pantel et al, 2009) 10 to
20 seeds are a sufficient starting set in a distribu-
tional similarity model to discover as many new cor-
rect instances as may ever be found. This observa-
tion differs from the claim of (Pas?ca and Van Durme,
2008) that 1 or 2 instances are sufficient to dis-
cover thousands of instance attributes. For some
619
pattern-based algorithms one to two seeds are suf-
ficient (Davidov et al, 2007; Kozareva et al, 2008),
some require ten seeds (Riloff and Jones, 1999; Igo
and Riloff, 2009), and others use a variation of 1, 5,
10 to 25 seeds (Talukdar et al, 2008).
As mentioned, seed selection is not yet well un-
derstood. Seeds may be chosen at random (Davi-
dov et al, 2007; Kozareva et al, 2008), by picking
the most frequent terms of the desired class (Riloff
and Jones, 1999; Igo and Riloff, 2009), or by ask-
ing humans (Pantel et al, 2009). The intuitions for
seed selection that experts develop over time seem
to prefer instances that are neither ambiguous nor
too frequent, but that at the same time are prolific
and quickly lead to the discovery of a diverse set of
instances. These criteria are vague and do not al-
ways lead to the discovery of good seeds. For some
approaches, infrequent and ambiguous seeds are ac-
ceptable while for others they lead to deterioration
in performance. For instance, the DAP (Kozareva et
al., 2008) performance is not affected by the ambi-
guity of the seed, because the class and the seed in
the pattern mutually disambiguate each other, while
for the distributional similarity model of (Pantel et
al., 2009), starting with an ambiguous seed leads
to ?leakage? and the harvesting of non-true class in-
stances. (Kozareva et al, 2008) show that for the
closed class country, both high-frequency seeds like
USA and low-frequency seeds like Burkina Faso
can equally well yield all remaining instances. An
open question to which no-one provides an answer
is whether and which high/low frequency seeds can
yield all instances of large, open classes like people
or singers.
3 Bootstrapping Recursive Patterns
There are many algorithms for harvesting informa-
tion from the Web. The main objective of our work
is not the creation of a new algorithm, but rather de-
termining the effect of seed selection on the gen-
eral class of recursive bootstrapping harvesting al-
gorithms for the acquisition of semantic lexicons for
open class relations. For our experiments, since it
is time-consuming and difficult for humans to pro-
vide large sets of seeds to start the bootstrapping
process, we employ the recursive DAP mechanism
introduced by (Kozareva et al, 2008) that produces
seeds on its own.
The algorithm starts with a seed of type class
which is fed into the doubly-anchored pattern
?<class> such as <seed> and *? and learns in the
* position new instances of type class. The newly
learned instances are then systematically placed into
the position of the seed in the DAP pattern, and the
harvesting process is repeated until no new instances
are found. The general framework is as follows:
1. Given:
a language L={English, Spanish}
a pattern Pi={e.g., [verb prep, noun, verb]}
a seed seed for Pi
2. Build a query in DAP-like fashion for Pi using
template Ti of the type ?class such as seed and
*?, ?* and seed verb prep?, ?* and seed noun?,
?* and seed verb?
3. submit Ti to Yahoo! or another search engine
4. extract instances occupying the * position
5. take instances from 4. and go to 2.
6. repeat steps 2?5 until no new instances are
found
At the end of bootstrapping, the harvested in-
stances can be considered to be seeds with which
the bootstrapping procedure could have been initi-
ated. We can now compare any of them to study
their relative ?goodness? as bootstrapping seeds.
4 Seed Quality Measurement
4.1 Problem Formulation
We define our task as:
Task Definition: Given a seed and a pattern in a
language (say English or Spanish), (1) use the boot-
strapping procedure to learn instances from the Web;
(2) build a predictive model to estimate the ?good-
ness? of seeds (whether generated by a human or
learned) .
Given a desired semantic class, a recursive harvest-
ing pattern expressing its context, and a seed term
for use in this pattern, we define the ?goodness? of
the seed as consisting of two measures:
? the yield: the total number of instances learned,
not counting duplicates, until the bootstrapping
procedure has run to exhaustion;
? the distance: the number of iterations required
by the process to reach exhaustion.
620
Our approach is to build a model of the behavior of
many seeds for the given pattern. Any new seed can
then be compared against this model, once its basic
characteristics have been determined, and its yield
and distance estimates produced. In order to deter-
mine the characteristics of the new seed, it first has
to be employed in the pattern for a small number of
iterations. The next subsection describes the regres-
sion model we employ in our approach.
4.2 Regression Model
Given a seed s, we seek to predict the yield g of s as
defined above. We do this via a parametrized func-
tion f :g? = f(s;w), where w ? Rd are the weights.
Our approach is to learn w from a collection of N
training examples {< si, gi >}Ni=1, where each si is
a seed and each gi ? R.
Support vector regression (Drucker et al, 1996)
is a well-known method for training a regression
model by solving the following optimization prob-
lem:
min
w?Rs
1
2
||w||2 + CN
N?
i=1
max(0, |gi ? f(si;w)| ? ?)
? ?? ?
?-insensitive loss function
where C is a regularization constant and ? con-
trols the training error. The training algorithm finds
weights w that define a function f minimizing the
empirical risk.
Let h be a function from seeds into some vector-
space representation ? Rd, then the function f takes
the form: f(s;w) = h(s)Tw = ?Ni=1 ?iK(s, si),
where f is re-parameterized in terms of a polyno-
mial kernel function K with dual weights ?i. K
measures the similarity between two seeds. Full de-
tails of the regression model and its implementation
are beyond the scope of this paper; for more de-
tails see (Scho?lkopf and Smola, 2001; Smola et al,
2003). In our experimental study, we use the freely
available implementation of SVM in Weka (Witten
and Frank, 2005).
To evaluate the quality of our prediction model,
we compare the actual yield of a seed with the pre-
dicted value obtained, and compute the correlation
coefficient and the relative absolute error.
5 Experiments and Results
5.1 Data Collection
We conducted an exhaustive evaluation study with
the open semantic classes people and city, initiated
with the seeds John and London. For each class, we
submitted the DAP patterns as web queries to Ya-
hoo!Boss and retrieved the top 1000 web snippets
for each query, keeping only unique instances. In
total, we collected 1.5GB of snippets for people and
1.9GB of snippets for cities. The algorithm ran un-
til complete exhaustion, requiring 19 iterations for
people and 12 for cities. The total number of unique
harvested instances was 3798 for people and 5090
for cities. We used all instances as seeds and instan-
tiated for each seed the bootstrapping process from
the very beginning. This resulted in 3798 and 5090
separate bootstrapping runs for people and cities re-
spectively. For each seed, we recorded the total
number of instances learned at the end of bootstrap-
ping, the number of iterations, and the number of
unique instances extracted on each iteration. After
the harvesting part terminated, we analyzed the con-
nectivity / bootstrapping behavior of the seeds, and
produced the regression model.
5.2 Seed Characteristics
For many knowledge harvesting algorithms, the se-
lection of a non-ambiguous seeds is of great impor-
tance. In the DAP bootstrapping framework, the am-
biguity of the seed is eliminated as the class and the
seed mutually disambiguate each other. Of great im-
portance to the bootstrapping algorithm is the selec-
tion of a seed that can yield a large number of in-
stances and can keep the bootstrapping process en-
ergized.
Figure 1: Seed Connectivity
Figure 1 shows the different kinds of seeds we
found on analyzing the results of the bootstrapping
process. Based on the yield learned on each iter-
ation, we identify four major kinds of seeds: her-
mit, one-step, mid, and high connectors. In the
figure, seed (a) is a hermit because it does not dis-
cover other instances. Seed (b) is a one-step connec-
tor as it discovers instances on the first iteration but
621
then becomes inactive. Seeds (d) and (e) are high
connectors because they find a rich population of in-
stances. Seed (c) is a mid connector because it has
lower yield than (d) and (e), but higher than (a) and
(b).
Table 1 shows the results of classifying the 3798
people and 5090 city seeds into the four kinds of
seed. The majority of the seeds for both patterns are
hermits, from 23 to 41% are high connectors, and
the rest are one-step and mid connectors. For each
kind of seed, we also show three examples.
people such as X and * examples
#hermit 2271 (60%) Leila, Anne Boleyn, Sophocles
#one-step 329 (9%) Helene, Frida Kahlo, Cornelius
#mid 315 (8%) Brent, Ferdinand, Olivia
#high 883 (23%) Diana, Donald Trump, Christopher
cities such as X and * examples
#hermit 2393 (47%) Belfast, Najafabad, El Mirador
#one-step 406 (8%) Durnstein, Wexford, Al-Qaim
#mid 207 (4%) Bialystok, Gori, New Albany
#high 2084 (41%) Vienna, Chicago, Marrakesh
Table 1: Connectivity-based Seed Classification.
This study shows that humans are very likely to
choose non-productive seeds for bootstrapping: it is
difficult for a human to know a priori that a name
like Diana will be more productive than Leila, He-
lene, or Olivia.
Another interesting characteristic of a seed is the
speed of learning. Some seeds, such as (e), ex-
tract large quantity of instances from the very be-
ginning, resulting in fewer bootstrapping iterations,
while others, such as (d), spike much later, resulting
in more. In our analysis, we found that some high
connector seeds of the people pattern can learn the
whole population in 12 iterations, while others re-
quire from 15 to 20 iterations. Figure 2 shows the
speed of learning of ten high connector seeds for
the people pattern. The y axis shows the number
of unique instances harvested on each iteration. In-
tuitively, a good seed is the one that produces a large
yield of instances in short distance. Thus the ?good-
ness? of seed (e) is better than that of seed (d).
As shown in Figure 2, for each seed, we observe
a single hump that corresponds to the point in which
a seed generates the maximum number of instances.
The peak occurs on different iterations because it is
dependent both on the yield learned with each iter-
ation and the total distance, for each seed. The oc-
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19
Yi
el
d
Iteration
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
Figure 2: Seed Learning Speed
currence of a single hump reveals regularity in the
connectivity behavior of seeds, and is discussed in
the Conclusion. We model this behavior as features
in our regression model and use it to measure the
quality of new seeds. The next subsection explains
the features of the regression model and the experi-
mental results obtained.
5.3 Predicting the Goodness of Seeds
Building a pattern specific model: For each pat-
tern, we build N different regression models, where
N corresponds to the total number of bootstrapping
iterations of the pattern. For regression model Ri,
we use the yield of a seed from iterations 1 to i as
features. This information is used to model the ac-
tivity of the seed in the bootstrapping process and
later on to predict the extraction power of new seeds.
For example, in Figure 1 on the first iteration seeds
(b), (c), and (d) have the same low connectivity com-
pared to seed (e). As bootstrapping progresses, seed
(d) reaches productive neighbors that discover more
instances, while seeds (b) and (c) become inactive.
This example shows that the yield in the initial stage
of bootstrapping is not sufficient to accurately pre-
dict the quality of the seeds. Since we do not know
exactly how many iterations are necessary to accu-
rately determine the ?goodness? of seeds, we model
the yield learned on each iteration by each seed and
subsequently include this information in the regres-
sion models.
The yield of a seed sk at iteration i is computed as
yield(sk)i =
?n
m=1(sm), where n is the total num-
ber of unique instances sm harvested on iteration i.
Y ield(sk)i is high when sk discovers a large number
of instances (new seeds), and small otherwise. For
hermit seeds, yield=0 at any iteration, because the
seeds are totally isolated and do not discover other
622
instances (seeds). For example, when building the
second regression model R2 using seeds (d) and (e)
from Figure 1, the feature values corresponding to
each seed in R2 are: yield(sd)1=1 and yield(sd)2=2
for seed (d), and yield(se)1=3 and yield(se)2=5 for
seed (e).
Results: Figure 3 shows the correlation coefficients
(cc) and the relative absolute errors of each regres-
sion model Ri for the people and city patterns. The
results are computed over ten-fold cross validation
of the 3798 people and 5090 city seeds. The x axis
shows the regression model Ri,. The y axis in the
two upper graphs shows the correlation coefficient
of the predicted and the actual total yield of the seeds
using Ri, and in the two lower graphs, the y axis
shows the error rate of each Ri.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Regression Model Ri
People
cut_off, t
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Regression Model Ri
Cities
cut_off, t
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Regression Model Ri
People
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Regression Model Ri
Cities
Figure 3: Regression for People and City.
We consider as a baseline model the regression
R1 which uses only the yield of the seeds on first
iteration. The prediction of R1 has cc=0.6 with
50% error for people and cc=0.4 with 70% error
for cities. These results confirm our previous obser-
vation that the quality of the seeds cannot be accu-
rately measured in the very beginning of bootstrap-
ping. However, by the ninth iteration, the regres-
sion models for people and cities reach cc=1.0 with
5% error rate. To make such an accurate prediction,
the model uses around one half of all bootstrapping
iterations?generally, just past the hump in Figure 2,
once the yield starts dropping.
Often in real applications or when under limited
resources (e.g., a fixed amount of Web queries per
day), running half the bootstrapping iterations is not
feasible. This problem can be resolved by employ-
ing different stopping criteria, at the cost of lower
cc and greater error. For example, one cut-off point
can be the (averaged) iteration number of the hump
for the given pattern. For people, the average hump
occurs at the seventh iteration, and for the city at
the fifth iteration. At this point, both patterns have a
cc=0.9 with 15% error rate. An alternative stopping
point can be the fourth iteration, where cc=0.7?0.8
with 35% error.
Overall, our study shows that it is possible to
model the behavior of seeds and use it to accurately
predict the ?goodness? of previously unseen seeds.
The results obtained for both people and city pat-
terns are very promising. However, a disadvantage
of this regression is that it requires training over the
whole extent of the given pattern. Also, each regres-
sion model is specific to the particular pattern it is
trained over. Next, we propose a generalized regres-
sion model which surmounts the problem of training
pattern-specific regression models.
5.4 Generalized Model for Goodness of Seeds
We built a generalized regression model (RG) com-
bining evidence from the people and city patterns.
We generated the features of each model as previ-
ously described in Section 5.3. From each pattern,
we randomly picked 1000 examples which resulted
in 30% of the people and 20% of the city seeds. We
used these seed examples to train the RGi models.
In total, we built 15 RGi, which is the maximum
number of overlapping iterations between the two
patterns. We tested our RG model with the remain-
ing 2798 people and 4090 city seeds.
Figure 4 shows the results of the RGi models for
the people and city patterns. In the first two itera-
tions, the predictions of the RG model are poorer
compared to the pattern-specific regression. On the
fourth iteration, both models have cc=0.7 and 0.8 for
the people and city patterns respectively. The error
rates of the generalized model are 41% and 35% for
people and city, while for the pattern-specific model
the errors are 37% and 32%. The early iterations
show a difference of around 4% in the error rate of
the two models, but around the ninth iteration both
models have comparable results.
623
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Generalized Regression Model RGi
Cities
People
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Generalized Regression Model RGi
Cities
People
Figure 4: Generalized Regression for People and City.
This study shows that it is possible to combine
evidence from two patterns harvesting different se-
mantic information to predict accurately the behav-
ior of unseen seed examples for either of the two
patterns.
5.5 Evaluating the Generalized Model on
Different Languages and Kinds of Patterns
So far, we have studied the performance of the gen-
eralized seed quality prediction method for specific
patterns in English. However, the connectivity be-
havior of the seeds might change for other languages
and kinds of patterns, making the generalized model
impractical to use in such cases. To verify this,
we evaluated the generalized model (RG) from Sec-
tion 5.4 with the people and city patterns in Spanish
(? gente como X y *? and ? ciudades como X y *?), as
well as with two new kinds of patterns (? * and X fly
to? and ? * and X work for?1). For each pattern, we
ran the bootstrapping process from Section 3 until
exhaustion and collected all seeds.
First, for each pattern we studied the connectivity
behavior of the seeds. Table 2 shows the obtained
results. The distribution is similar to the seed distri-
bution for the English people and cities patterns. Al-
though the total number of harvested instances (i.e.,
seeds) is different for each pattern, the proportion of
hermits to other seeds remains larger. From 20%
to 37% of the seeds are high connectors, and the
rest are one-step and mid connectors. This analysis
shows that the connectivity behavior of seeds across
different languages and patterns is similar, at least
for the examples studied. In addition to the seed
analysis, we show in the table the total number of
bootstrapping iterations for each pattern. The ?work
1The X indicates the position of the seed and (*) corresponds
to the instances learned during bootstrapping.
for? and ?fly to? patterns run for a longer distance
compared to the other patterns. While for the ma-
jority of the patterns the hump is observed on the
fifth or seventh iteration, for these two patterns the
average peak is observed on the fifteenth.
gente como X y ciudades como X y
#hermit 318 (56%) 1061 (51%)
#one-step 58 (10%) 150 (8%)
#mid 79 (14%) 79 (4%)
#high 117 (20%) 795 (38%)
tot#iter 20 16
and X fly to and X work for
#hermit 389 (45%) 1262 (48%)
#one-step 87 (9%) 238 (9%)
#mid 75 (8%) 214 (8%)
#high 322 (37%) 922 (35%)
tot#iter 26 33
Table 2: Seed Classification for Spanish and Verb-Prep
Patterns.
Second, we test the RGi models from Section 5.4,
which were trained on people and cities, to predict
the total yield of the seeds in the new patterns. Fig-
ure 5 shows the correlation coefficient and the rela-
tive absolute error results of each pattern for RGi.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Generalized Regression Model RGi
Work For
Fly To
Ciudades
Gente
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Generalized Regression Model RGi
Work For
Fly To
Ciudades
Gente
Figure 5: Generalized Regression for Different Lan-
guages and Patterns.
Interestingly, we found that our generalized
method has consistent performance across the dif-
ferent languages and patterns. On the twelfth iter-
ation, the model is able to predict the ?goodness?
of seeds with cc=1.0 and from 0.4% to 8.0% error
rate. Around the fifth and sixth iterations, all pat-
terns reach cc=0.8 with error of 5% to 15%. The
higher error bound is for patterns like ?work for? and
?fly to? which run for a longer distance. This experi-
mental study confirms the robustness of our general-
ized model which is trained on the behavior of seeds
from one kind of pattern and tested with seeds in dif-
ferent languages and on completely different kinds
of patterns.
624
6 Conclusions and Future Work
It would, a fortiori, seem impossible to estimate the
goodness of a seed term used in a recursive boot-
strapping pattern for harvesting information from
the web. After all, its eventual total yield and dis-
tance depend on the cumulation of the terms pro-
duced in each iteration of the bootstrapping, and
there are no external constraints or known web lan-
guage structure to be exploited.
We have shown that it is possible to create, using
regression, a model of the grown behavior of seeds
for a given pattern, and fitting it with an indication of
a new seed?s growth (considering its grown behavior
in a limited number of bootstrapping iterations) in
order to obtain a quite reliable estimate of the new
seed?s eventual yield and distance.
Going further, we are delighted to observe that
the regularity of the single-hump harvesting behav-
ior makes it possible to learn a regression model that
enables one to predict, with some accuracy, both the
yield and the distance of a new seed, even when the
pattern being considered is not yet seen. All that is
required is the indication of the seed?s growth be-
havior, obtained through a number of iterations us-
ing the pattern of interest.
Our ongoing analysis takes the following ap-
proach. Let Ti be the set of all new terms (terms
not yet found) harvested during iteration i. Then
T0 = {t0,1}, just the initial seed term. Let NY (ti,j)
be the novel yield of term ti,j , that is, the number
of as yet undiscovered terms produced by a single
application of the pattern using the term ti,j . Notice
that bootstrapping ceases when for some i = d (the
distance), ?j NY (td,j) = 0. Since the total number
of terms that can be learned,
?d
i=0
?
j NY (ti,j) =
N , is finite and fixed, there are exactly three al-
ternatives for the growth of the NY curve when
it is shown summed over each iteration: (i) either
?
j NY (ti,j) =
?
j NY (ti+1,j) and there is no
larger NY sum for any iteration; or (ii) ?j NY (ti,j)
grows to a maximal value for some iteration i =
m and then decreases again; or (iii) ?j NY (ti,j)
reaches more than one locally maximal value at dif-
ferent iterations. The first case, in which exactly
the same number of new terms is harvested every
iteration for several or all iterations, would require
that each new term once learned yields precisely and
only one subsequent new term, or that the number
of hermits is exactly balanced by the NY of one or
more of the other terms in that iteration. This situa-
tion is so unlikely as to be dismissed outright. Case
(ii), in which there is a single hump, appears to be
how text is written on the web, as shown in Fig-
ure 2. Case (iii), the multi-hump case, would re-
quire that the terms be linked in semi-disconnected
?islands?, with a relatively much smaller inter-island
connectivity than intra-island one. Given our stud-
ies, it appears that language on the web is not orga-
nized this way, at least not for the patterns we stud-
ied. However, it is not impossible: this two-hump
case would have to have occurred in (Kozareva et
al., 2008) when the ambiguous seed term Georgia
was used in the DAP ?states such as Georgia and *?,
where initially the US states were harvested but, at
some point, the learned term Georgia also initiated
harvesting of the ex-USSR states. Such ?leakage?
into a new semantic domain requires not only ambi-
guity of the seed but also parallel ambiguity of the
class term, which is highly unlikely as well.
Accepting case (ii), therefore, we postulate that
for any (or all regular) patterns there is some iter-
ation m in which ?j NY (tm,j) is maximal. The
question is how rapidly the summed NY curve ap-
proaches it and then abates again. This depends on
the out-degree connectivity of terms overall. In the
population of N terms for a given semantic pattern,
is the distribution of out-degrees Poisson (or Zip-
fian), or is it normal (Gaussian)? In the former case,
there will be a few high-degree connector terms and
a large number (the long tail) of one-step and hermit
terms; in the latter, there will be a small but equal
number of low-end and high-end connector terms,
with the bulk of terms falling in the mid-connector
range. One direction of our ongoing work is to deter-
mine this distribution, and to empirically derive its
parameters. It might be possible to discover some in-
teresting regularities about the (preferential) uses of
terms within semantic domains, as reflected in term
network connectivity.
Although not all seeds are equal, it appears to
be possible to treat them with a single regression
model, regardless of pattern, to predict their ?good-
ness?.
Acknowledgments: This research was supported by
NSF grant IIS-0705091.
625
References
Douglas E. Appelt, Jerry R. Hobbs, John Bear, David Is-
rael, Megumi Kameyama, Andy Kehler, David Martin,
Karen Myers, and Mabry Tyson. 1995. SRI Interna-
tional FASTUS system MUC-6 test results and analy-
sis. In Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 237?248.
Michele Banko. 2009. Open information extraction from
the web. In Ph.D. Dissertation from University of
Washington.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 232?239, June.
Harris Drucker, Chris J.C. Burges, Linda Kaufman, Alex
Smola, and Vladimir Vapnik. 1996. Support vector re-
gression machines. In Advances in NIPS, pages 155?
161.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91?134,
June.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: a brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics, pages 466?471.
Zellig S. Harris. 1954. Distributional structure. Word,
10:140?162.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th confer-
ence on Computational linguistics, pages 539?545.
Sean Igo and Ellen Riloff. 2009. Corpus-based seman-
tic lexicon induction with web-based corroboration.
In Proceedings of the Workshop on Unsupervised and
Minimally Supervised Learning of Lexical Semantics.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056.
Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana Girju, and
Vasile Rus. 1999. Lasso: A tool for surfing the answer
net. In TREC.
Marius Pas?ca and Benjamin Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of ACL-08: HLT.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 938?
947, August.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the thirteenth ACM
international conference on Information and knowl-
edge management, pages 137?145.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In AAAI ?99/IAAI ?99: Proceedings of the
sixteenth national conference on Artificial intelligence
and the eleventh Innovative applications of artificial
intelligence conference innovative applications of ar-
tificial intelligence.
Bernhard Scho?lkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond (Adaptive
Computation and Machine Learning). The MIT Press.
Alex J. Smola, Bernhard Schlkopf, and Bernhard Sch
Olkopf. 2003. A tutorial on support vector regression.
Technical report, Statistics and Computing.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th international
conference on World Wide Web, pages 697?706.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP 2008, pages
582?590.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Management,
CIKM, pages 225?234.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, second edition.
626
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482?1491,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Arguments and Supertypes of Semantic Relations using
Recursive Patterns
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
A challenging problem in open informa-
tion extraction and text mining is the learn-
ing of the selectional restrictions of se-
mantic relations. We propose a mini-
mally supervised bootstrapping algorithm
that uses a single seed and a recursive
lexico-syntactic pattern to learn the ar-
guments and the supertypes of a diverse
set of semantic relations from the Web.
We evaluate the performance of our algo-
rithm on multiple semantic relations ex-
pressed using ?verb?, ?noun?, and ?verb
prep? lexico-syntactic patterns. Human-
based evaluation shows that the accuracy
of the harvested information is about 90%.
We also compare our results with existing
knowledge base to outline the similarities
and differences of the granularity and di-
versity of the harvested knowledge.
1 Introduction
Building and maintaining knowledge-rich re-
sources is of great importance to information ex-
traction, question answering, and textual entail-
ment. Given the endless amount of data we have at
our disposal, many efforts have focused on mining
knowledge from structured or unstructured text,
including ground facts (Etzioni et al, 2005), se-
mantic lexicons (Thelen and Riloff, 2002), ency-
clopedic knowledge (Suchanek et al, 2007), and
concept lists (Katz et al, 2003). Researchers have
also successfully harvested relations between en-
tities, such as is-a (Hearst, 1992; Pasca, 2004) and
part-of (Girju et al, 2003). The kinds of knowl-
edge learned are generally of two kinds: ground
instance facts (New York is-a city, Rome is the cap-
ital of Italy) and general relational types (city is-a
location, engines are part-of cars).
A variety of NLP tasks involving inference or
entailment (Zanzotto et al, 2006), including QA
(Katz and Lin, 2003) and MT (Mt et al, 1988),
require a slightly different form of knowledge, de-
rived from many more relations. This knowledge
is usually used to support inference and is ex-
pressed as selectional restrictions (Wilks, 1975)
(namely, the types of arguments that may fill a
given relation, such as person live-in city and air-
line fly-to location). Selectional restrictions con-
strain the possible fillers of a relation, and hence
the possible contexts in which the patterns ex-
pressing that relation can participate in, thereby
enabling sense disambiguation of both the fillers
and the expression itself.
To acquire this knowledge two common ap-
proaches are employed: clustering and patterns.
While clustering has the advantage of being fully
unsupervised, it may or may not produce the types
and granularity desired by a user. In contrast
pattern-based approaches are more precise, but
they typically require a handful to dozens of seeds
and lexico-syntactic patterns to initiate the learn-
ing process. In a closed domain these approaches
are both very promising, but when tackling an un-
bounded number of relations they are unrealistic.
The quality of clustering decreases as the domain
becomes more continuously varied and diverse,
and it has proven difficult to create collections of
effective patterns and high-yield seeds manually.
In addition, the output of most harvesting sys-
tems is a flat list of lexical semantic expressions
such as ?New York is-a city? and ?virus causes
flu?. However, using this knowledge in inference
requires it to be formulated appropriately and or-
ganized in a semantic repository. (Pennacchiotti
and Pantel, 2006) proposed an algorithm for au-
tomatically ontologizing semantic relations into
WordNet. However, despite its high precision en-
tries, WordNet?s limited coverage makes it impos-
sible for relations whose arguments are not present
in WordNet to be incorporated. One would like a
procedure that dynamically organizes and extends
1482
its semantic repository in order to be able to ac-
commodate all newly-harvested information, and
thereby become a global semantic repository.
Given these considerations, we address in this
paper the following question: How can the selec-
tional restrictions of semantic relations be learned
automatically from the Web with minimal effort us-
ing lexico-syntactic recursive patterns?
The contributions of the paper are as follows:
? A novel representation of semantic relations
using recursive lexico-syntactic patterns.
? An automatic procedure to learn the se-
lectional restrictions (arguments and super-
types) of semantic relations from Web data.
? An exhaustive human-based evaluation of the
harvested knowledge.
? A comparison of the results with some large
existing knowledge bases.
The rest of the paper is organized as follows. In
the next section, we review related work. Section
3 addresses the representation of semantic rela-
tions using recursive patterns. Section 4 describes
the bootstrapping mechanism that learns the selec-
tional restrictions of the relations. Section 5 de-
scribes data collection. Section 6 discusses the ob-
tained results. Finally, we conclude in Section 7.
2 Related Work
A substantial body of work has been done in at-
tempts to harvest bits of semantic information, in-
cluding: semantic lexicons (Riloff and Shepherd,
1997), concept lists (Lin and Pantel, 2002), is-
a relations (Hearst, 1992; Etzioni et al, 2005;
Pasca, 2004; Kozareva et al, 2008), part-of re-
lations (Girju et al, 2003), and others. Knowl-
edge has been harvested with varying success both
from structured text such as Wikipedia?s infoboxes
(Suchanek et al, 2007) or unstructured text such
as the Web (Pennacchiotti and Pantel, 2006; Yates
et al, 2007). A variety of techniques have been
employed, including clustering (Lin and Pantel,
2002), co-occurrence statistics (Roark and Char-
niak, 1998), syntactic dependencies (Pantel and
Ravichandran, 2004), and lexico-syntactic pat-
terns (Riloff and Jones, 1999; Fleischman and
Hovy, 2002; Thelen and Riloff, 2002).
When research focuses on a particular relation,
careful attention is paid to the pattern(s) that ex-
press it in various ways (as in most of the work
above, notably (Riloff and Jones, 1999)). But it
has proven a difficult task to manually find ef-
fectively different variations and alternative pat-
terns for each relation. In contrast, when re-
search focuses on any relation, as in TextRun-
ner (Yates et al, 2007), there is no standardized
manner for re-using the pattern learned. TextRun-
ner scans sentences to obtain relation-independent
lexico-syntactic patterns to extract triples of the
form (John, fly to, Prague). The middle string de-
notes some (unspecified) semantic relation while
the first and third denote the learned arguments of
this relation. But TextRunner does not seek spe-
cific semantic relations, and does not re-use the
patterns it harvests with different arguments in or-
der to extend their yields.
Clearly, it is important to be able to specify both
the actual semantic relation sought and use its tex-
tual expression(s) in a controlled manner for max-
imal benefit.
The objective of our research is to combine the
strengths of the two approaches, and, in addition,
to provide even richer information by automati-
cally mapping each harvested argument to its su-
pertype(s) (i.e., its semantic concepts). For in-
stance, given the relation destination and the pat-
tern X flies to Y, automatically determining that
John, Prague) and (John, conference) are two
valid filler instance pairs, that (RyanAir, Prague)
is another, as well as that person and airline are
supertypes of the first argument and city and event
of the second. This information provides the se-
lectional restrictions of the given semantic rela-
tion, indicating that living things like people can
fly to cities and events, while non-living things like
airlines fly mainly to cities. This is a significant
improvement over systems that output a flat list
of lexical semantic knowledge (Thelen and Riloff,
2002; Yates et al, 2007; Suchanek et al, 2007).
Knowing the sectional restrictions of a semantic
relation supports inference in many applications,
for example enabling more accurate information
extraction. (Igo and Riloff, 2009) report that pat-
terns like ?attack on ?NP?? can learn undesirable
words due to idiomatic expressions and parsing er-
rors. Over time this becomes problematic for the
bootstrapping process and leads to significant de-
terioration in performance. (Thelen and Riloff,
2002) address this problem by learning multiple
semantic categories simultaneously, relying on the
often unrealistic assumption that a word cannot
belong to more than one semantic category. How-
1483
ever, if we have at our disposal a repository of se-
mantic relations with their selectional restrictions,
the problem addressed in (Igo and Riloff, 2009)
can be alleviated.
In order to obtain selectional restriction classes,
(Pennacchiotti and Pantel, 2006) made an attempt
to ontologize the harvested arguments of is-a,
part-of, and cause relations. They mapped each
argument of the relation into WordNet and identi-
fied the senses for which the relation holds. Un-
fortunately, despite its very high precision en-
tries, WordNet is known to have limited cover-
age, which makes it impossible for algorithms to
map the content of a relation whose arguments
are not present in WordNet. To surmount this
limitation, we do not use WordNet, but employ
a different method of obtaining superclasses of a
filler term: the inverse doubly-anchored patterns
DAP?1 (Hovy et al, 2009), which, given two ar-
guments, harvests its supertypes from the source
corpus. (Hovy et al, 2009) show that DAP?1 is
reliable and it enriches WordNet with additional
hyponyms and hypernyms.
3 Recursive Patterns
A singly-anchored pattern contains one example
of the seed term (the anchor) and one open posi-
tion for the term to be learned. Most researchers
use singly-anchored patterns to harvest semantic
relations. Unfortunately, these patterns run out of
steam very quickly. To surmount this obstacle, a
handful of seeds is generally used, and helps to
guarantee diversity in the extraction of new lexico-
syntactic patterns (Riloff and Jones, 1999; Snow et
al., 2005; Etzioni et al, 2005).
Some algorithms require ten seeds (Riloff and
Jones, 1999; Igo and Riloff, 2009), while others
use a variation of 5, 10, to even 25 seeds (Taluk-
dar et al, 2008). Seeds may be chosen at ran-
dom (Davidov et al, 2007; Kozareva et al, 2008),
by picking the most frequent terms of the desired
class (Igo and Riloff, 2009), or by asking humans
(Pantel et al, 2009). As (Pantel et al, 2009) show,
picking seeds that yield high numbers of differ-
ent terms is difficult. Thus, when dealing with
unbounded sets of relations (Banko and Etzioni,
2008), providing many seeds becomes unrealistic.
Interestingly, recent work reports a class of pat-
terns that use only one seed to learn as much infor-
mation with only one seed. (Kozareva et al, 2008;
Hovy et al, 2009) introduce the so-called doubly-
anchored pattern (DAP) that has two anchor seed
positions ??type? such as ?seed? and *?, plus one
open position for the terms to be learned. Learned
terms can then be replaced into the seed position
automatically, creating a recursive procedure that
is reportedly much more accurate and has much
higher final yield. (Kozareva et al, 2008; Hovy et
al., 2009) have successfully applied DAP for the
learning of hyponyms and hypernyms of is-a rela-
tions and report improvements over (Etzioni et al,
2005) and (Pasca, 2004).
Surprisingly, this work was limited to the se-
mantic relation is-a. No other study has described
the use or effect of recursive patterns for differ-
ent semantic relations. Therefore, going beyond
(Kozareva et al, 2008; Hovy et al, 2009), we here
introduce recursive patterns other than DAP that
use only one seed to harvest the arguments and su-
pertypes of a wide variety of relations.
(Banko and Etzioni, 2008) show that seman-
tic relations can be expressed using a handful
of relation-independent lexico-syntactic patterns.
Practically, we can turn any of these patterns into
recursive form by giving as input only one of the
arguments and leaving the other one as an open
slot, allowing the learned arguments to replace the
initial seed argument directly. For example, for
the relation ?fly to?, the following recursive pat-
terns can be built: ?* and ?seed? fly to *?, ??seed?
and * fly to *?, ?* fly to ?seed? and *?, ?* fly to *
and ?seed??, ??seed? fly to *? or ?* fly to ?seed??,
where ?seed? is an example like John or Ryanair,
and (?) indicates the position on which the ar-
guments are learned. Conjunctions like and, or
are useful because they express list constructions
and extract arguments similar to the seed. Poten-
tially, one can explore all recursive pattern varia-
tions when learning a relation and compare their
yield, however this study is beyond the scope of
this paper.
We are particularly interested in the usage of re-
cursive patterns for the learning of semantic re-
lations not only because it is a novel method,
but also because recursive patterns of the DAP
fashion are known to: (1) learn concepts with
high precision compared to singly-anchored pat-
terns (Kozareva et al, 2008), (2) use only one
seed instance for the discovery of new previously
unknown terms, and (3) harvest knowledge with
minimal supervision.
1484
4 Bootstrapping Recursive Patterns
4.1 Problem Formulation
The main goal of our research is:
Task Definition: Given a seed and a semantic relation ex-
pressed using a recursive lexico-syntactic pattern, learn in
bootstrapping fashion the selectional restrictions (i.e., the
arguments and supertypes) of the semantic relation from
an unstructured corpus such as the Web.
Figure 1 shows an example of the task and the
types of information learned by our algorithm.
* and John fly to *
seed = Johnrelation = fly to    
BrianKate
politicianspeopleartists
DeltaAlaskaairlinescarriers
beesanimals
party event
ItalyFrance countries
New York city
flowerstrees plants
Figure 1: Bootstrapping Recursive Patterns.
Given a seed John and a semantic relation fly to
expressed using the recursive pattern ?* and John
fly to *?, our algorithm learns the left side argu-
ments {Brian, Kate, bees, Delta, Alaska} and the
right side arguments {flowers, trees, party, New
York, Italy, France}. For each argument, the algo-
rithm harvests supertypes such as {people, artists,
politicians, airlines, city, countries, plants, event}
among others. The colored links between the right
and left side concepts denote the selectional re-
strictions of the relation. For instance, people fly
to events and countries, but never to trees or flow-
ers.
4.2 System Architecture
We propose a minimally supervised bootstrap-
ping algorithm based on the framework adopted in
(Kozareva et al, 2008; Hovy et al, 2009). The al-
gorithm has two phases: argument harvesting and
supertype harvesting. The final output is a ranked
list of interlinked concepts which captures the se-
lectional restrictions of the relation.
4.2.1 Argument Harvesting
In the argument extraction phase, the first boot-
strapping iteration is initiated with a seed Y and a
recursive pattern ?X? and Y verb+prep|verb|noun
Z??, where X? and Z? are the placeholders for the
arguments to be learned. The pattern is submit-
ted to Yahoo! as a web query and all unique snip-
pets matching the query are retrieved. The newly
learned and previously unexplored arguments on
the X? position are used as seeds in the subse-
quent iteration. The arguments on the Z? posi-
tion are stored at each iteration, but never used
as seeds since the recursivity is created using the
terms on X and Y . The bootstrapping process is
implemented as an exhaustive breadth-first algo-
rithm which terminates when all arguments are ex-
plored.
We noticed that despite the specific lexico-
syntactic structure of the patterns, erroneous in-
formation can be acquired due to part-of-speech
tagging errors or flawed facts on the Web. The
challenge is to identify and separate the erroneous
from the true arguments. We incorporate the har-
vested arguments on X and Y positions in a di-
rected graph G = (V,E), where each vertex
v ? V is a candidate argument and each edge
(u, v) ? E indicates that the argument v is gener-
ated by the argument u. An edge has weight w cor-
responding to the number of times the pair (u, v)
is extracted from different snippets. A node u
is ranked by u=
?
?(u,v)?E
w(u,v)+
?
?(v,u)?E
w(v,u)
|V |?1
which represents the weighted sum of the outgo-
ing and incoming edges normalized by the total
number of nodes in the graph. Intuitively, our con-
fidence in a correct argument u increases when the
argument (1) discovers and (2) is discovered by
many different arguments.
Similarly, to rank the arguments standing on
the Z position, we build a bipartite graph G? =
(V ?, E?) that has two types of vertices. One set
of vertices represents the arguments found on the
Y position in the recursive pattern. We will call
these Vy. The second set of vertices represents the
arguments learned on the Z position. We will call
these Vz . We create an edge e?(u?, v?) ? E? be-
tween u? ? Vy and v? ? Vz when the argument on
the Z position represented by v? was harvested by
the argument on the Y position represented by u?.
The weight w? of the edge indicates the number
of times an argument on the Y position found Z.
Vertex v? is ranked as v?=
?
?(u?,v?)?E?
w(u?,v?)
|V ?|?1 . In
a very large corpus, like the Web, we assume that
a correct argument Z is the one that is frequently
discovered by various arguments Y .
1485
4.2.2 Supertype Harvesting
In the supertype extraction phase, we take all
<X,Y> argument pairs collected during the argu-
ment harvesting stage and instantiate them in the
inverse DAP?1 pattern ?* such as X and Y?. The
query is sent to Yahoo! as a web query and all 1000
snippets matching the pattern are retrieved. For
each <X,Y> pair, the terms on the (*) position are
extracted and considered as candidate supertypes.
To avoid the inclusion of erroneous supertypes,
again we build a bipartite graph G?? = (V ??, E??).
The set of vertices Vsup represents the supertypes,
while the set of vertices Vp corresponds to the
?X,Y? pair that produced the supertype. An edge
e??(u??, v??) ? E??, where u?? ? Vp and v?? ? Vsup
shows that the pair ?X,Y? denoted as u?? harvested
the supertype represented by v??.
For example, imagine that the argument X?=
Ryanair was harvested in the previous phase by
the recursive pattern ?X? and EasyJet fly to Z??.
Then the pair ?Ryanair,EasyJet? forms a new Web
query ?* such as Ryanair and EasyJet? which
learns the supertypes ?airlines? and ?carriers?.
The bipartite graph has two vertices v??1 and v
??
2 for
the supertypes ?airlines? and ?carriers?, one ver-
tex u??3 for the argument pair ?Ryanair, EasyJet?,
and two edges e??1(u
??
3, v
??
1) and e
??
2(u
??
3, v
??
1). A vertex
v?? ? Vsup is ranked by v??=
?
?(u??,v??)?E??
w(u??,v??)
|V ??|?1 .
Intuitively, a supertype which is discovered mul-
tiple times by various argument pairs is ranked
highly.
However, it might happen that a highly ranked
supertype actually does not satisfy the selectional
restrictions of the semantic relation. To avoid such
situations, we further instantiate each supertype
concept in the original pattern1. For example,
?aircompanies fly to *? and ?carriers fly to *?. If
the candidate supertype produces many web hits
for the query, then this suggests that the term is a
relevant supertype.
Unfortunately, to learn the supertypes of the Z
arguments, currently we have to form all possi-
ble combinations among the top 150 highly ranked
concepts, because these arguments have not been
learned through pairing. For each pair of Z argu-
ments, we repeat the same procedure as described
above.
1Except for the ?dress? and ?person? relations, where
the targeted arguments are adjectives, and the supertypes are
nouns.
5 Semantic Relations
So far, we have described the mechanism that
learns from one seed and a recursive pattern the
selectional restrictions of any semantic relation.
Now, we are interested in evaluating the per-
formance of our algorithm. A natural question
that arises is: ?How many patterns are there??.
(Banko and Etzioni, 2008) found that 95% of the
semantic relations can be expressed using eight
lexico-syntactic patterns. Space prevents us from
describing all of them, therefore we focus on the
three most frequent patterns which capture a large
diversity of semantic relations. The relative fre-
quency of these patterns is 37.80% for ?verbs?,
22.80% for ?noun prep?, and 16.00% for ?verb
prep?.
5.1 Data Collection
Table 1 shows the lexico-syntactic pattern and the
initial seed we used to express each semantic rela-
tion. To collect data, we ran our knowledge har-
vesting algorithm until complete exhaustion. For
each query submitted to Yahoo!, we retrieved the
top 1000 web snippets and kept only the unique
ones. In total, we collected 30GB raw data which
was part-of-speech tagged and used for the argu-
ment and supertype extraction. Table 1 shows the
obtained results.
recursive pattern seed X arg Z arg #iter
X and Y work for Z Charlie 2949 3396 20
X and Y fly to Z EasyJet 772 1176 19
X and Y go to Z Rita 18406 27721 13
X and Y work in Z John 4142 4918 13
X and Y work on Z Mary 4126 5186 7
X and Y work at Z Scott 1084 1186 14
X and Y live in Z Harry 8886 19698 15
X and Y live at Z Donald 1102 1175 15
X and Y live with Z Peter 1344 834 11
X and Y cause Z virus 12790 52744 19
X and Y celebrate Jim 6033 ? 12
X and Y drink Sam 1810 ? 13
X and Y dress nice 1838 ? 8
X and Y person scared 2984 ? 17
Table 1: Total Number of Harvested Arguments.
An interesting characteristic of the recursive
patterns is the speed of leaning which can be mea-
sured in terms of the number of unique argu-
ments acquired during each bootstrapping itera-
tion. Figure 2 shows the bootstrapping process for
the ?cause? and ?dress? relations. Although both
relations differ in terms of the total number of it-
erations and harvested items, the overall behavior
of the learning curves is similar. Learning starts
of very slowly and as bootstrapping progresses a
1486
rapid growth is observed until a saturation point is
reached.
 0
 10000
 20000
 30000
 40000
 50000
 60000
 1  2  3  4  5  6  7  8  9  10  11 12  13 14  15 16  17 18  19
#
I
t
e
m
s
 
L
e
a
r
n
e
d
Iterations
X and Y Cause Z
XZ
 0
 500
 1000
 1500
 2000
 1  2  3  4  5  6  7  8
#
I
t
e
m
s
 
L
e
a
r
n
e
d
Iterations
X and Y Dress
X
Figure 2: Items extracted in 10 iterations.
The speed of leaning is related to the connectiv-
ity behavior of the arguments of the relation. In-
tuitively, a densely connected graph takes shorter
time (i.e., fewer iterations) to be learned, as in the
?work on? relation, while a weakly connected net-
work takes longer time to harvest the same amount
of information, as in the ?work for? relation.
6 Results
In this section, we evaluate the results of our
knowledge harvesting algorithm. Initially, we de-
cided to conduct an automatic evaluation compar-
ing our results to knowledge bases that have been
extracted in a similar way (i.e., through pattern ap-
plication over unstructured text). However, it is
not always possible to perform a complete com-
parison, because either researchers have not fully
explored the same relations we have studied, or for
those relations that overlap, the gold standard data
was not available.
The online demo of TextRunner2 (Yates et al,
2007) actually allowed us to collect the arguments
for all our semantic relations. However, due to
Web based query limitations, TextRunner returns
only the first 1000 snippets. Since we do not have
the complete and ranked output of TextRunner,
comparing results in terms of recall and precision
is impossible.
Turning instead to results obtained from struc-
tured sources (which one expects to have high
correctness), we found that two of our relations
overlap with those of the freely available ontology
Yago (Suchanek et al, 2007), which was harvested
from the Infoboxes tables in Wikipedia. In addi-
tion, we also had two human annotators judge as
many results as we could afford, to obtain Preci-
sion. We conducted two evaluations, one for the
arguments and one for the supertypes.
2http://www.cs.washington.edu/research/textrunner/
6.1 Human-Based Argument Evaluation
In this section, we discuss the results of the har-
vested arguments. For each relation, we selected
the top 200 highly ranked arguments. We hired
two annotators to judge their correctness. We cre-
ated detailed annotation guidelines that define the
labels for the arguments of the relations, as shown
in Table 2. (Previously, for the same task, re-
searchers have not conducted such an exhaustive
and detailed human-based evaluation.) The anno-
tation was conducted using the CAT system3.
TYPE LABEL EXAMPLES
Correct Person John, Mary
Role mother, president
Group team, Japanese
Physical yellow, shabby
NonPhysical ugly, thought
NonLiving airplane
Organization IBM, parliament
Location village, New York, in the house
Time at 5 o?clock
Event party, prom, earthquake
State sick, anrgy
Manner live in happiness
Medium work on Linux, Word
Fixed phrase go to war
Incorrect Error wrong part-of-speech tag
Other none of the above
Table 2: Annotation Labels.
We allow multiple labels to be assigned to the
same concept, because sometimes the concept can
appear in different contexts that carry various con-
ceptual representations. Although the labels can
be easily collapsed to judge correct and incorrect
terms, the fine-grained annotation shown here pro-
vides a better overview of the information learned
by our algorithm.
We measured the inter-annotator agreement for
all labels and relations considering that a single
entry can be tagged with multiple labels. The
Kappa score is around 0.80. This judgement is
good enough to warrant using these human judge-
ments to estimate the accuracy of the algorithm.
We compute Accuracy as the number of examples
tagged as Correct divided by the total number of
examples.
Table 4 shows the obtained results. The over-
all accuracy of the argument harvesting phase is
91%. The majority of the occurred errors are due
to part-of-speech tagging. Table 3 shows a sam-
ple of 10 randomly selected examples from the top
200 ranked and manually annotated arguments.
3http://cat.ucsur.pitt.edu/default.aspx
1487
Relation Arguments
(X) Dress: stylish, comfortable, expensive, shabby, gorgeous
silver, clean, casual, Indian, black
(X) Person: honest, caring, happy, intelligent, gifted
friendly, responsible, mature, wise, outgoing
(X) Cause: pressure, stress, fire, bacteria, cholesterol
flood, ice, cocaine, injuries, wars
GoTo (Z): school, bed, New York, the movies, the park, a bar
the hospital, the church, the mall, the beach
LiveIn (Z): peace, close proximity, harmony, Chicago, town
New York, London, California, a house, Australia
WorkFor (Z): a company, the local prison, a gangster, the show
a boss, children, UNICEF, a living, Hispanics
Table 3: Examples of Harvested Arguments.
6.2 Comparison against Existing Resources
In this section, we compare the performance of our
approach with the semantic knowledge base Yago4
that contains 2 million entities5, 95% of which
were manually confirmed to be correct. In this
study, we compare only the unique arguments of
the ?live in? and ?work at? relations. We provide
Precision scores using the following measures:
PrY ago =
#terms found in Y ago
#terms harvested by system
PrHuman =
#terms judged correct by human
#terms harvested by system
NotInY ago = #terms judged correct by human but not in Y ago
Table 5 shows the obtained results.
We carefully analyzed those arguments that
were found by one of the systems but were miss-
ing in the other. The recursive patterns learn infor-
mation about non-famous entities like Peter and
famous entities like Michael Jordan. In contrast,
Yago contains entries mostly about famous enti-
ties, because this is the predominant knowledge in
Wikipedia. For the ?live in? relation, both repos-
itories contain the same city and country names.
However, the recursive pattern learned arguments
like pain, effort which express a manner of living,
and locations like slums, box. This information is
missing from Yago. Similarly for the ?work at?
relation, both systems learned that people work
at universities. In addition, the recursive pattern
learned a diversity of company names absent from
Yago.
While it is expected that our algorithm finds
many terms not contained in Yago?specifically,
the information not deemed worthy of inclusion
in Wikipedia?we are interested in the relatively
large number of terms contained in Yago but not
found by our algorithm. To our knowledge, no
4http://www.mpi-inf.mpg.de/yago-naga/yago/
5Names of cities, people, organizations among others.
X WorkFor A1 A2 WorkFor Z A1 A2
Person 148 152 Organization 111 110
Role 5 7 Person 60 60
Group 12 14 Event 4 2
Organization 8 7 Time 4 5
NonPhysical 22 23 NonPhysical 18 19
Other 5 5 Other 3 4
Acc. .98 .98 Acc. .99 .98
X Cause A1 A2 Cause Z A1 A2
PhysicalObj 82 75 PhysicalObj 15 20
NonPhysicalObj 69 66 NonPhysicalObj 89 91
Event 21 24 Event 72 72
State 29 31 State 50 50
Other 3 4 Other 5 4
Acc. .99 .98 Acc. .98 .98
X GoTo A1 A2 GoTo Z A1 A2
Person 190 188 Location 163 155
Role 4 4 Event 21 30
Group 3 3 Person 11 13
NonPhysical 1 3 NonPhysical 2 1
Other 2 2 Other 3 1
Acc. .99 .99 Acc. .99 .99
X FlyTo A1 A2 FlyTo Z A1 A2
Person 140 139 Location 199 198
Organization 54 57 Event 1 2
NonPhysical 2 2 Person 0 0
Other 4 2 Other 0 0
Acc. .98 .99 Acc. 1 1
X WorkOn A1 A2 WorkOn Z A1 A2
Person 173 172 Location 110 108
Role 2 3 Organization 27 25
Group 4 5 Manner 38 40
Organization 6 6 Time 4 4
NonPhysical 15 14 NonPhysical 18 21
Error 1 1 Medium 8 8
Other 1 1 Other 13 15
Acc. .99 .99 Acc. .94 .93
X WorkIn A1 A2 WorkIn Z A1 A2
Person 117 118 Location 104 111
Group 10 9 Organization 10 25
Organization 3 3 Manner 39 40
Fixed 3 1 Time 4 4
NonPhysical 55 59 NonPhysical 22 21
Error 12 10 Medium 8 8
Other 0 0 Error 13 15
Acc. .94 .95 Acc. .94 .93
X WorkAt A1 A2 WorkAt Z A1 A2
Person 193 192 Organization 189 190
Role 1 1 Manner 5 4
Group 1 1 Time 3 3
Organization 0 0 Error 3 2
Other 5 6 Other 0 1
Acc. .98 .97 Acc. .99 .99
X LiveIn A1 A2 LiveIn Z A1 A2
Person 185 185 Location 182 186
Role 3 4 Manner 6 8
Group 9 8 Time 1 2
NonPhysical 1 2 Fixed 5 2
Other 2 1 Other 6 2
Acc. .99 .99 Acc. .97 .99
X LiveAt A1 A2 LiveAt Z A1 A2
Person 196 195 Location 158 157
Role 1 1 Person 5 7
NonPhysical 0 1 Manner 1 2
Other 3 3 Error 36 34
Acc. .99 .99 Acc. .82 .83
X LiveWith A1 A2 LiveWith Z A1 A2
Person 188 187 Person 165 163
Role 6 6 Animal 2 4
Group 2 2 Manner 15 15
NonPhysical 2 3 NonPhysical 15 15
Other 2 2 Other 3 3
Acc. .99 .99 Acc. .99 .99
X Dress A1 A2 X Person A1 A2
Physical 72 59 Physical 8 2
NonPhysical 120 136 NonPhysical 188 194
Other 8 5 Other 4 4
Acc .96 .98 Acc. .98 .98
X Drink A1 A2 X Celebrate A1 A2
Living 165 174 Living 157 164
NonLiving 8 2 NonLiving 42 35
Error 27 24 Error 1 1
Acc .87 .88 Acc. .99 .99
Table 4: Harvested Arguments.
1488
PrY ago PrHuman NotInYago
X LiveIn .19 (2863/14705) .58 (5165)/8886 2302
LiveIn Z .10 (495/4754) .72 (14248)/19698 13753
X WorkAt .12(167/1399) .88 (959)/1084 792
WorkAt Z .3(15/525) .95 (1128)/1186 1113
Table 5: Comparison against Yago.
other automated harvesting algorithm has ever
been compared to Yago, and our results here form
a baseline that we aim to improve upon. And in
the future, one can build an extensive knowledge
harvesting system combining the wisdom of the
crowd and Wikipedia.
6.3 Human-Based Supertype Evaluation
In this section, we discuss the results of harvest-
ing the supertypes of the learned arguments. Fig-
ure 3 shows the top 100 ranked supertypes for the
?cause? and ?work on? relations. The x-axis in-
dicates a supertype, the y-axis denotes the number
of different argument pairs that lead to the discov-
ery of the supertype.
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 10  20  30  40  50  60  70  80  90  100
#
P
a
i
r
s
 
D
i
s
c
o
v
e
r
i
n
g
 
t
h
e
 
S
u
p
e
r
t
y
p
e
Supertype
WorkOnCause
Figure 3: Ranked Supertypes.
The decline of the curve indicates that certain
supertypes are preferred and shared among differ-
ent argument pairs. It is interesting to note that the
text on the Web prefers a small set of supertypes,
and to see what they are. These most-popular har-
vested types tend to be the more descriptive terms.
The results indicate that one does not need an elab-
orate supertype hierarchy to handle the selectional
restrictions of semantic relations.
Since our problem definition differs from avail-
able related work, and WordNet does not contain
all harvested arguments as shown in (Hovy et al,
2009), it is not possible to make a direct compar-
ison. Instead, we conduct a manual evaluation of
the most highly ranked supertypes which normally
are the top 20. The overall accuracy of the super-
types for all relations is 92%. Table 6 shows the
Relation Arguments
(Supx) Celebrate: men, people, nations, angels, workers, children
countries, teams, parents, teachers
(Supx) Dress: colors, effects, color tones, activities, patterns
styles, materials, size, languages, aspects
(Supx) FlyTo: airlines, carriers, companies, giants, people
competitors, political figures, stars, celebs
Cause (Supz): diseases, abnormalities, disasters, processes, isses
disorders, discomforts, emotions, defects, symptoms
WorkFor (Supz) organizations, industries, people, markets, men
automakers, countries, departments, artists, media
GoTo (Supz) : countries, locations, cities, people, events
men, activities, games, organizations,
FlyTo (Supz) places, countries, regions, airports, destinations
locations, cities, area, events
Table 6: Examples of Harvested Supertypes.
top 10 highly ranked supertypes for six of our re-
lations.
7 Conclusion
We propose a minimally supervised algorithm that
uses only one seed example and a recursive lexico-
syntactic pattern to learn in bootstrapping fash-
ion the selectional restrictions of a large class of
semantic relations. The principal contribution of
the paper is to demonstrate that this kind of pat-
tern can be applied to almost any kind of se-
mantic relation, as long as it is expressible in
a concise surface pattern, and that the recursive
mechanism that allows each newly acquired term
to restart harvesting automatically is a signifi-
cant advance over patterns that require a handful
of seeds to initiate the learning process. It also
shows how one can combine free-form but undi-
rected pattern-learning approaches like TextRun-
ner with more-controlled but effort-intensive ap-
proaches like commonly used.
In our evaluation, we show that our algorithm is
capable of extracting high quality non-trivial in-
formation from unstructured text given very re-
stricted input (one seed). To measure the perfor-
mance of our approach, we use various semantic
relations expressed with three lexico-syntactic pat-
terns. For two of the relations, we compare results
with the freely available ontology Yago, and con-
duct a manual evaluation of the harvested terms.
We will release the annotated and the harvested
data to the public to be used for comparison by
other knowledge harvesting algorithms.
The success of the proposed framework opens
many challenging directions. We plan to use the
algorithm described in this paper to learn the se-
lectional restrictions of numerous other relations,
in order to build a rich knowledge repository
1489
that can support a variety of applications, includ-
ing textual entailment, information extraction, and
question answering.
Acknowledgments
This research was supported by DARPA contract
number FA8750-09-C-3705.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36, June.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 232?239, June.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134, June.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th international conference on Compu-
tational linguistics, pages 1?7.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proc. of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 1?8.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the
14th conference on Computational linguistics, pages
539?545.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 948?957.
Sean Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with web-based corrobora-
tion. In Proceedings of the Workshop on Unsuper-
vised and Minimally Supervised Learning of Lexical
Semantics.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
In In Proceedings of the EACL-2003 Workshop on
Natural Language Processing for Question Answer-
ing, pages 43?50.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003.
Integrating web-based and corpus-based techniques
for question answering. In Proceedings of the
twelfth text retrieval conference (TREC), pages 426?
435.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048?1056.
Dekang Lin and Patrick Pantel. 2002. Concept dis-
covery from text. In Proc. of the 19th international
conference on Computational linguistics, pages 1?7.
Characteristics Of Mt, John Lehrberger, Laurent
Bourbeau, Philadelphia John Benjamins, and Rita
Mccardell. 1988. Machine Translation: Linguistic
Characteristics of Mt Systems and General Method-
ology of Evaluation. John Benjamins Publishing
Co(1988-03).
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proc. of Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 321?328.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-
scale distributional similarity and entity set expan-
sion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 938?947, August.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the thirteenth
ACM international conference on Information and
knowledge management, pages 137?145.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 793?800.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI ?99/IAAI ?99: Proceedings
of the Sixteenth National Conference on Artificial in-
telligence.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons.
In Proc. of the Second Conference on Empirical
Methods in Natural Language Processing, pages
117?124.
Brian Roark and Eugene Charniak. 1998. Noun-
phrase co-occurrence statistics for semiautomatic
semantic lexicon construction. In Proceedings of the
17th international conference on Computational lin-
guistics, pages 1110?1116.
1490
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems 17, pages 1297?1304. MIT Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW ?07: Proceedings of the 16th inter-
national conference on World Wide Web, pages 697?
706.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pasca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2008, pages 582?590.
Michael Thelen and Ellen Riloff. 2002. A Bootstrap-
ping Method for Learning Semantic Lexicons Using
Extraction Pattern Contexts. In Proc. of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 214?221.
Yorick Wilks. 1975. A preferential pattern-seeking,
semantics for natural language inference. Artificial
Intelligence, 6(1):53?74.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information ex-
traction on the web. In NAACL ?07: Proceedings of
Human Language Technologies: The Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Demonstra-
tions on XX, pages 25?26.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In ACL-44: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 849?
856.
1491
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616?1625,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insights from Network Structure for Text Mining
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
Text mining and data harvesting algorithms
have become popular in the computational lin-
guistics community. They employ patterns
that specify the kind of information to be har-
vested, and usually bootstrap either the pat-
tern learning or the term harvesting process (or
both) in a recursive cycle, using data learned
in one step to generate more seeds for the next.
They therefore treat the source text corpus as
a network, in which words are the nodes and
relations linking them are the edges. The re-
sults of computational network analysis, espe-
cially from the world wide web, are thus ap-
plicable. Surprisingly, these results have not
yet been broadly introduced into the computa-
tional linguistics community. In this paper we
show how various results apply to text mining,
how they explain some previously observed
phenomena, and how they can be helpful for
computational linguistics applications.
1 Introduction
Text mining / harvesting algorithms have been ap-
plied in recent years for various uses, including
learning of semantic constraints for verb participants
(Lin and Pantel, 2002) related pairs in various rela-
tions, such as part-whole (Girju et al, 2003), cause
(Pantel and Pennacchiotti, 2006), and other typical
information extraction relations, large collections
of entities (Soderland et al, 1999; Etzioni et al,
2005), features of objects (Pasca, 2004) and ontolo-
gies (Carlson et al, 2010). They generally start with
one or more seed terms and employ patterns that
specify the desired information as it relates to the
seed(s). Several approaches have been developed
specifically for learning patterns, including guided
pattern collection with manual filtering (Riloff and
Shepherd, 1997) automated surface-level pattern in-
duction (Agichtein and Gravano, 2000; Ravichan-
dran and Hovy, 2002) probabilistic methods for tax-
onomy relation learning (Snow et al, 2005) and ker-
nel methods for relation learning (Zelenko et al,
2003). Generally, the harvesting procedure is recur-
sive, in which data (terms or patterns) gathered in
one step of a cycle are used as seeds in the following
step, to gather more terms or patterns.
This method treats the source text as a graph or
network, consisting of terms (words) as nodes and
inter-term relations as edges. Each relation type in-
duces a different network1. Text mining is a process
of network traversal, and faces the standard prob-
lems of handling cycles, ranking search alternatives,
estimating yield maxima, etc.
The computational properties of large networks
and large network traversal have been studied inten-
sively (Sabidussi, 1966; Freeman, 1979; Watts and
Strogatz, 1998) and especially, over the past years,
in the context of the world wide web (Page et al,
1999; Broder et al, 2000; Kleinberg and Lawrence,
2001; Li et al, 2005; Clauset et al, 2009). Surpris-
ingly, except in (Talukdar and Pereira, 2010), this
work has not yet been related to text mining research
in the computational linguistics community.
The work is, however, relevant in at least two
ways. It sometimes explains why text mining algo-
1These networks are generally far larger and more densely
interconnected than the world wide web?s network of pages and
hyperlinks.
1616
rithms have the limitations and thresholds that are
empirically found (or suspected), and it may suggest
ways to improve text mining algorithms for some
applications.
In Section 2, we review some related work. In
Section 3 we describe the general harvesting proce-
dure, and follow with an examination of the various
statistical properties of implicit semantic networks
in Section 4, using our implemented harvester to
provide illustrative statistics. In Section 5 we dis-
cuss implications for computational linguistics re-
search.
2 Related Work
The Natural Language Processing knowledge har-
vesting community has developed a good under-
standing of how to harvests various kinds of se-
mantic information and use this information to im-
prove the performance of tasks such as information
extraction (Riloff, 1993), textual entailment (Zan-
zotto et al, 2006), question answering (Katz et
al., 2003), and ontology creation (Suchanek et al,
2007), among others. Researchers have focused
on the automated extraction of semantic lexicons
(Hearst, 1992; Riloff and Shepherd, 1997; Girju et
al., 2003; Pasca, 2004; Etzioni et al, 2005; Kozareva
et al, 2008). While clustering approaches tend to
extract general facts, pattern based approaches have
shown to produce more constrained but accurate lists
of semantic terms. To extract this information, (Lin
and Pantel, 2002) showed the effect of using differ-
ent sizes and genres of corpora such as news and
Web documents. The latter has been shown to pro-
vide broader and more complete information.
Researchers outside computational linguistics
have studied complex networks such as the World
Wide Web, the Social Web, the network of scien-
tific papers, among others. They have investigated
the properties of these text-based networks with the
objective of understanding their structure and ap-
plying this knowledge to determine node impor-
tance/centrality, connectivity, growth and decay of
interest, etc. In particular, the ability to analyze net-
works, identify influential nodes, and discover hid-
den structures has led to important scientific and
technological breakthroughs such as the discovery
of communities of like-minded individuals (New-
man and Girvan, 2004), the identification of influ-
ential people (Kempe et al, 2003), the ranking of
scientists by their citation indexes (Radicchi et al,
2009), and the discovery of important scientific pa-
pers (Walker et al, 2006; Chen et al, 2007; Sayyadi
and Getoor, 2009). Broder et al (2000) demon-
strated that the Web link structure has a ?bow-tie?
shape, while (2001) classified Web pages into au-
thorities (pages with relevant information) and hubs
(pages with useful references). These findings re-
sulted in the development of the PageRank (Page et
al., 1999) algorithm which analyzes the structure of
the hyperlinks of Web documents to find pages with
authoritative information. PageRank has revolution-
ized the whole Internet search society.
However, no-one has studied the properties of the
text-based semantic networks induced by semantic
relations between terms with the objective of un-
derstanding their structure and applying this knowl-
edge to improve concept discovery. Most relevant
to this theme is the work of Steyvers and Tenen-
baum (Steyvers and Tenenbaum, 2004), who stud-
ied three manually built lexical networks (associa-
tion norms, WordNet, and Roget?s Thesaurus (Ro-
get, 1911)) and proposed a model of the growth of
the semantic structure over time. These networks are
limited to the semantic relations among nouns.
In this paper we take a step further to explore the
statistical properties of semantic networks relating
proper names, nouns, verbs, and adjectives. Under-
standing the semantics of nouns, verbs, and adjec-
tives has been of great interest to linguists and cog-
nitive scientists such as (Gentner, 1981; Levin and
Somers, 1993; Gasser and Smith, 1998). We imple-
ment a general harvesting procedure and show its re-
sults for these word types. A fundamental difference
with the work of (Steyvers and Tenenbaum, 2004)
is that we study very large semantic networks built
?naturally? by (millions of) users rather than ?artifi-
cially? by a small set of experts. The large networks
capture the semantic intuitions and knowledge of the
collective mass. It is conceivable that an analysis
of this knowledge can begin to form the basis of a
large-scale theory of semantic meaning and its inter-
connections, support observation of the process of
lexical development and usage in humans, and even
suggest explanations of how knowledge is organized
in our brains, especially when performed for differ-
1617
ent languages on the WWW.
3 Inducing Semantic Networks in the Web
Text mining algorithms such as those mentioned
above raise certain questions, such as: Why are some
seed terms more powerful (provide a greater yield)
than others?, How can one find high-yield terms?,
How many steps does one need, typically, to learn
all terms for a given relation?, Can one estimate the
total eventual yield of a given relation?, and so on.
On the face of it, one would need to know the struc-
ture of the network a priori to be able to provide an-
swers. But research has shown that some surpris-
ing regularities hold. For example, in the text min-
ing community, (Kozareva and Hovy, 2010b) have
shown that one can obtain a quite accurate estimate
of the eventual yield of a pattern and seed after only
five steps of harvesting. Why is this? They do not
provide an answer, but research from the network
community does.
To illustrate the properties of networks of the kind
induced by semantic relations, and to show the ap-
plicability of network research to text harvesting, we
implemented a harvesting algorithm and applied it
to a representative set of relations and seeds in two
languages.
Since the goal of this paper is not the development
of a new text harvesting algorithm, we implemented
a version of an existing one: the so-called DAP
(doubly-anchored pattern) algorithm (Kozareva et
al., 2008), because it (1) is easy to implement, (2)
requires minimum input (one pattern and one seed
example), (3) achieves very high precision com-
pared to existing methods (Pasca, 2004; Etzioni et
al., 2005; Pasca, 2007), (4) enriches existing se-
mantic lexical repositories such as WordNet and
Yago (Suchanek et al, 2007), (5) can be formulated
to learn semantic lexicons and relations for noun,
verb and verb+preposition syntactic constructions;
(6) functions equally well in different languages.
Next we describe the knowledge harvesting proce-
dure and the construction of the text-mined semantic
networks.
3.1 Harvesting to Induce Semantic Networks
For a given semantic class of interest say singers, the
algorithm starts with a seed example of the class, say
Madonna. The seed term is inserted in the lexico-
syntactic pattern ?class such as seed and *?, which
learns on the position of the ? new terms of type
class. The newly learned terms are then individually
placed into the position of the seed in the pattern,
and the bootstrapping process is repeated until no
new terms are found. The output of the algorithm
is a set of terms for the semantic class. The algo-
rithm is implemented as a breadth-first search and
its mechanism is described as follows:
1. Given:
a language L={English, Spanish}
a pattern Pi={such as, including, verb prep,
noun}
a seed term seed for Pi
2. Build a query for Pi using template Ti ?class such
as seed and *?, ?class including seed and *?, ?*
and seed verb prep?, ?* and seed noun?, ?seed
and * noun?
3. Submit Ti to Yahoo! or other search engine
4. Extract terms occupying the * position
5. Feed terms from 4. into 2.
6. Repeat steps 2?5. until no new terms are found
The output of the knowledge harvesting algorithm
is a network of semantic terms interconnected by
the semantic relation captured in the pattern. We
can represent the traversed (implicit) network as a
directed graph G(V,E) with nodes V (|V | = n)
and edges E(|E| = m). A node u in the net-
work corresponds to a term discovered during boot-
strapping. An edge (u, v) ? E represents an ex-
isting link between two terms. The direction of the
edge indicates that the term v was generated by the
term u. For example, given the sentence (where
the pattern is in italics and the extracted term is un-
derlined) ?He loves singers such as Madonna and
Michael Jackson?, two nodes Madonna and Michael
Jackson with an edge e=(Madonna, Michael Jack-
son) would be created in the graph G. Figure 1
shows a small example of the singer network. The
starting seed term Madonna is shown in red color
and the harvested terms are in blue.
3.2 Data
We harvested data from the Web for a representa-
tive selection of semantic classes and relations, of
1618
!"#$%%"&
'()$%&*$+%&!,-+".(&*"-/0$%&
1.(,%.&2,$%&
'33"& 45,%-.& 6.7$%-.& 8"57&9,+"%%"&
:5.##,.&!.5-;57&
<(,-,"&8.70&
=+"/,5"&
2>>& 9,-/.7&!"5?%&=).@,.&A$%#.5&
B,%"&B;5%.5&
2$((7&4")$%&
Figure 1: Harvesting Procedure.
the type used in (Etzioni et al, 2005; Pasca, 2007;
Kozareva and Hovy, 2010a):
? semantic classes that can be learned using dif-
ferent seeds (e.g., ?singers such as Madonna
and *? and ?singers such as Placido Domingo
and *?);
? semantic classes that are expressed through dif-
ferent lexico-syntactic patterns (e.g., ?weapons
such as bombs and *? and ?weapons including
bombs and *?);
? verbs and adjectives characterizing the seman-
tic class (e.g., ?expensive and * car?, ?dogs
run and *?);
? semantic relations with more complex lexico-
syntactic structure (e.g., ?* and Easyjet fly to?,
?* and Sam live in?);
? semantic classes that are obtained in differ-
ent languages, such as English and Spanish
(e.g., ?singers such as Madonna and *? and
?cantantes como Madonna y *?);
While most of these variations have been explored
in individual papers, we have found no paper that
covers them all, and none whatsoever that uses verbs
and adjectives as seeds.
Using the above procedure to generate the data,
each pattern was submitted as a query to Ya-
hoo!Boss. For each query the top 1000 text snippets
were retrieved. The algorithm ran until exhaustion.
In total, we collected 10GB of data which was part-
of-speech tagged with Treetagger (Schmid, 1994)
and used for the semantic term extraction. Table 1
summarizes the number of nodes and edges learned
for each semantic network using pattern Pi and the
initial seed shown in italics.
Lexico-Syntactic Pattern Nodes Edges
P1=?singers such as Madonna and *? 1115 1942
P2=?singers such as Placido Domingo and *? 815 1114
P3=?emotions including anger and *? 113 250
P4=?emotions such as anger and *? 748 2547
P5=?diseases such as malaria and *? 3168 6752
P6=?drugs such as ibuprofen and *? 2513 9428
P7=?expensive and * cars? 4734 22089
P8=?* and tasty fruits? 1980 7874
P9=?whales swim and *? 869 2163
P10=?dogs chase and *? 4252 20212
P11=?Britney Spears dances and *? 354 540
P12=?John reads and *? 3894 18545
P13=?* and Easyjet fly to? 3290 6480
P14=?* and Charlie work for? 2125 3494
P15=?* and Sam live in? 6745 24348
P16=?cantantes como Madonna y *? 240 318
P17=?gente como Jorge y *? 572 701
Table 1: Size of the Semantic Networks.
4 Statistical Properties of Text-Mined
Semantic Networks
In this section we apply a range of relevant mea-
sures from the network analysis community to the
networks described above.
4.1 Centrality
The first statistical property we explore is centrality.
It measures the degree to which the network struc-
ture determines the importance of a node in the net-
work (Sabidussi, 1966; Freeman, 1979).
We explore the effect of two centrality measures:
indegree and outdegree. The indegree of a node
u denoted as indegree(u)=
?
(v, u) considers the
sum of all incoming edges to u and captures the abil-
ity of a semantic term to be discovered by other se-
mantic terms. The outdegree of a node u denoted
as outdegree(u)=
?
(u, v) considers the number of
outgoing edges of the node u and measures the abil-
ity of a semantic term to discover new terms. In-
tuitively, the more central the node u is, the more
confident we are that it is a correct term.
Since harvesting algorithms are notorious for ex-
tracting erroneous information, we use the two cen-
trality measures to rerank the harvested elements.
Table 2 shows the accuracy2 of the singer seman-
tic terms at different ranks using the in and out
degree measures. Consistently, outdegree outper-
forms indegree and reaches higher accuracy. This
2Accuracy is calculated as the number of correct terms at
rank R divided by the total number of terms at rank R.
1619
shows that for the text-mined semantic networks, the
ability of a term to discover new terms is more im-
portant than the ability to be discovered.
@rank in-degree out-degree
10 .92 1.0
25 .91 1.0
50 .90 .97
75 .90 .96
100 .89 .96
150 .88 .95
Table 2: Accuracy of the Singer Terms.
This poses the question ?What are the terms with
high and low outdegree??. Table 3 shows the top
and bottom 10 terms of the semantic class.
Semantic Class top 10 outDegree bottom 10 outDegree
Singers Frank Sinatra Alanis Morisette
Ella Fitzgerald Christine Agulera
Billie Holiday Buffy Sainte-Marie
Britney Spears Cece Winans
Aretha Franklin Wolfman Jack
Michael Jackson Billie Celebration
Celine Dion Alejandro Sanz
Beyonce France Gall
Bessie Smith Peter
Joni Mitchell Sarah
Table 3: Singer Term Ranking with Centrality Measures.
The nodes with high outdegree correspond to fa-
mous or contemporary singers. The lower-ranked
nodes are mostly spelling errors such as Alanis
Morisette and Christine Agulera, less known singers
such as Buffy Sainte-Marie and Cece Winans, non-
American singers such as Alejandro Sanz and
France Gall, extractions due to part-of-speech tag-
ging errors such as Billie Celebration, and general
terms such as Peter and Sarah. Potentially, know-
ing which terms have a high outdegree allows one to
rerank candidate seeds for more effective harvesting.
4.2 Power-law Degree Distribution
We next study the degree distributions of the net-
works. Similarly to the Web (Broder et al, 2000)
and social networks like Orkut and Flickr, the text-
mined semantic networks also exhibit a power-law
distribution. This means that while a few terms have
a significantly high degree, the majority of the se-
mantic terms have small degree. Figure 2 shows the
indegree and outdegree distributions for different
semantic classes, lexico-syntactic patterns, and lan-
guages (English and Spanish). For each semantic
network, we plot the best-fitting power-law function
(Clauset et al, 2009) which fits well all degree dis-
tributions. Table 4 shows the power-law exponent
values for all text-mined semantic networks.
Patt. ?in ?out Patt. ?in ?out
P1 2.37 1.27 P10 1.65 1.12
P2 2.25 1.21 P11 2.42 1.41
P3 2.20 1.76 P12 1.60 1.13
P4 2.28 1.18 P13 2.26 1.20
P5 2.49 1.18 P14 2.43 1.25
P6 2.42 1.30 P15 2.51 1.43
P7 1.95 1.20 P16 2.74 1.31
P8 1.94 1.07 P17 2.90 1.20
P9 1.96 1.30
Table 4: Power-Law Exponents of Semantic Networks.
It is interesting to note that the indegree power-
law exponents for all semantic networks fall within
the same range (?in ? 2.4), and similarly for the
outdegree exponents (?out ? 1.3). However, the
values of the indegree and outdegree exponents
differ from each other. This observation is consistent
with Web degree distributions (Broder et al, 2000).
The difference in the distributions can be explained
by the link asymmetry of semantic terms: A discov-
ering B does not necessarily mean that B will dis-
cover A. In the text-mined semantic networks, this
asymmetry is caused by patterns of language use,
such as the fact that people use first adjectives of the
size and then of the color (e.g., big red car), or prefer
to place male before female proper names. Harvest-
ing patterns should take into account this tendency.
4.3 Sparsity
Another relevant property of the semantic networks
concerns sparsity. Following Preiss (Preiss, 1999), a
graph is sparse if |E| = O(|V |k) and 1 < k < 2,
where |E| is the number of edges and |V | is the num-
ber of nodes, otherwise the graph is dense. For the
studied text-semantic networks, k is ? 1.08. Spar-
sity can be also captured through the density of the
semantic network which is computed as |E|V (V?1) . All
networks have low density which suggests that the
networks exhibit a sparse connectivity pattern. On
average a node (semantic term) is connected to a
very small percentage of other nodes. Similar be-
havior was reported for the WordNet and Roget?s se-
mantic networks (Steyvers and Tenenbaum, 2004).
1620
 0 50 100 150 200 250 300 350
 400 450 500
 0  10  20  30  40  50  60  70  80  90Number of Nodes Indegree 'emotions'power-law exponent=2.28  0 20 40 60 80 100 120  0  20  40  60  80  100  120Number of Nodes Outdegree 'emotions'power-law exponent=1.18
 0 500 1000 1500 2000
 2500
 0  10  20  30  40  50  60Number of Nodes Indegree 'travel_to'power-law exponent=2.26  0 100 200 300 400 500 600 700  0  5  10  15  20  25  30  35Number of Nodes Outdegree 'fly_to'power-law exponent=1.20
 0 50 100 150 200 250 300 350 400
 450 500
 1  2  3  4  5  6  7  8Number of Nodes Indegree 'gente'power-law exponent=2.90  0 20 40 60 80 100 120  0  2  4  6  8  10  12  14Number of Nodes Outdegree 'gente'power-law exponent=1.20
Figure 2: Degree Distributions of Semantic Networks.
4.4 Connectedness
For every network, we computed the strongly con-
nected component (SCC) such that for all nodes (se-
mantic terms) in the SCC, there is a path from any
node to another node in the SCC considering the di-
rection of the edges between the nodes. For each
network, we found that there is only one SCC. The
size of the component is shown in Table 5. Un-
like WordNet and Roget?s semantic networks where
the SCC consists 96% of all semantic terms, in the
text-mined semantic networks only 12 to 55% of the
terms are in the SCC. This shows that not all nodes
can reach (discover) every other node in the net-
work. This also explains the findings of (Kozareva
et al, 2008; Vyas et al, 2009) why starting with a
good seed is important.
4.5 Path Lengths and Diameter
Next, we describe the properties of the shortest paths
between the semantic terms in the SCC. The dis-
tance between two nodes in the SCC is measured as
the length of the shortest path connecting the terms.
The direction of the edges between the terms is taken
into consideration. The average distance is the aver-
age value of the shortest path lengths over all pairs
of nodes in the SCC. The diameter of the SCC is
calculated as the maximum distance over all pairs of
nodes (u, v), such that a node v is reachable from
node u. Table 5 shows the average distance and the
diameter of the semantic networks.
Patt. #nodes in SCC SCC Average Distance SCC Diameter
P1 364 (.33) 5.27 16
P2 285 (.35) 4.65 13
P3 48 (.43) 2.85 6
P4 274 (.37) 2.94 7
P5 1249 (.38) 5.99 17
P6 1471 (.29) 4.82 15
P7 2255 (.46 ) 3.51 11
P8 1012 (.50) 3.87 11
P9 289 (.33) 4.93 13
P10 2342 (.55) 4.50 12
P11 87 (.24) 5.00 11
P12 1967 (.51) 3.20 13
P13 1249 (.38) 4.75 13
P14 608 (.29) 7.07 23
P15 1752 (.26) 5.32 15
P16 56 (.23) 4.79 12
P17 69 (.12 ) 5.01 13
Table 5: SCC, SCC Average Distance and SCC Diameter
of the Semantic Networks.
The diameter shows the maximum number of
steps necessary to reach from any node to any other,
while the average distance shows the number of
steps necessary on average. Overall, all networks
have very short average path lengths and small di-
ameters that are consistent with Watt?s finding for
small-world networks. Therefore, the yield of har-
vesting seeds can be predicted within five steps ex-
plaining (Kozareva and Hovy, 2010b; Vyas et al,
2009).
We also compute for any randomly selected node
in the semantic network on average how many hops
(steps) are necessary to reach from one node to an-
other. Figure 3 shows the obtained results for some
of the studied semantic networks.
4.6 Clustering
The clustering coefficient (C) is another measure
to study the connectivity structure of the networks
(Watts and Strogatz, 1998). This measure captures
the probability that the two neighbors of a randomly
selected node will be neighbors. The clustering co-
efficient of a node u is calculated as Cu=
|eij |
ku(ku?1)
1621
 0 10 20 30 40 50
 60 70
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15Number of Nodes Distance (Hops)Britney Spears (verb harvesting)  0 50 100 150 200 250 300 350 400  1  2  3  4  5  6  7  8  9  10  11Number of Nodes Distance (Hops)fruits (adjective harvesting)
 0 50 100 150 200
 250
 2  4  6  8  10  12  14  16  18  20  22  24Number of Nodes Distance (Hops)work for  0 5 10 15 20 25 30  2  4  6  8  10  12  14  16  18  20Number of Nodes Distance (Hops) gente
Figure 3: Hop Plot of the Semantic Networks.
: vi, vj ? Nu, eij ? E, where ku is the total degree
of the node u and Nu is the neighborhood of u. The
clustering coefficient C for the whole semantic net-
work is the average clustering coefficient of all its
nodes, C= 1n
?
Ci. The value of the clustering coef-
ficient ranges between [0, 1], where 0 indicates that
the nodes do not have neighbors which are them-
selves connected, while 1 indicates that all nodes are
connected. Table 6 shows the clustering coefficient
for all text-mined semantic networks together with
the number of closed and open triads3. The analysis
suggests the presence of a strong local cluster, how-
ever there are few possibilities to form overlapping
neighborhoods of nodes. The clustering coefficient
of WordNet (Steyvers and Tenenbaum, 2004) is sim-
ilar to those of the text-mined networks.
4.7 Joint Degree Distribution
In social networks, understanding the preferential at-
tachment of nodes is important to identify the speed
with which epidemics or gossips spread. Similarly,
we are interested in understanding how the nodes of
the semantic networks connect to each other. For
this purpose, we examine the Joint Degree Distribu-
tion (JDD) (Li et al, 2005; Newman, 2003). JDD
is approximated by the degree correlation function
knn which maps the outdegree and the average
3A triad is three nodes that are connected by either two (open
triad) or three (closed triad) directed ties.
Patt. C ClosedTriads OpenTriads
P1 .01 14096 (.97) 388 (.03)
P2 .01 6487 (.97) 213 (.03)
P3 .30 1898 (.94) 129 (.06)
P4 .33 60734 (.94) 3944 (.06)
P5 .10 79986 (.97) 2321 (.03)
P6 .11 78716 (.97) 2336 (.03)
P7 .17 910568 (.95) 43412 (.05)
P8 .19 21138 (.95) 10728 (.05)
P9 .20 27830 (.95) 1354 (.05)
P10 .15 712227 (.96) 62101(.04)
P11 .09 3407 (.98) 63 (.02)
P12 .15 734724 (.96) 32517 (.04)
P13 .06 66162 (.99) 858 (.01)
P14 .05 28216 (.99) 408 (.01)
P15 .09 1336679 (.97) 47110 (.03)
P16 .09 1525 (.98) 37 ( .02)
P17 .05 2222 (.99) 21 (.01)
Table 6: Clustering Coefficient of the Semantic Networks.
indegree of all nodes connected to a node with
that outdegree. High values of knn indicate that
high-degree nodes tend to connect to other high-
degree nodes (forming a ?core? in the network),
while lower values of knn suggest that the high-
degree nodes tend to connect to low-degree ones.
Figure 4 shows the knn for the singer, whale, live
in, cars, cantantes, and gente networks. The figure
plots the outdegree and the average indegree of the
semantic terms in the networks on a log-log scale.
We can see that for all networks the high-degree
nodes tend to connect to other high-degree ones.
This explains why text mining algorithms should fo-
cus their effort on high-degree nodes.
4.8 Assortivity
The property of the nodes to connect to other nodes
with similar degrees can be captured through the as-
sortivity coefficient r (Newman, 2003). The range of
r is [?1, 1]. A positive assortivity coefficient means
that the nodes tend to connect to nodes of similar
degree, while negative coefficient means that nodes
are likely to connect to nodes with degree very dif-
ferent from their own. We find that the assortivi-
tiy coefficient of our semantic networks is positive,
ranging from 0.07 to 0.20. In this respect, the se-
mantic networks differ from the Web, which has a
negative assortivity (Newman, 2003). This implies
a difference in text mining and web search traver-
sal strategies: since starting from a highly-connected
seed term will tend to lead to other highly-connected
terms, text mining algorithms should prefer depth-
first traversal, while web search algorithms starting
1622
 1 10
 100
 1  10  100knn Outdegreesinger (seed is Madonna)  1 10 100  1  10  100knn Outdegreewhale (verb harvesting)
 1 10
 100
 1  10  100knn Outdegree live in  1 10 100  1  10  100knn Outdegreecars (adjective harvesting)
 1
 10
 1  10knn Outdegreecantantes  1 10  1  10knn Outdegree gente
Figure 4: Joint Degree Distribution of the Semantic Net-
works.
from a highly-connected seed page should prefer a
breadth-first strategy.
5 Discussion
The above studies show that many of the proper-
ties discovered of the network formed by the web
hold also for the networks induced by semantic rela-
tions in text mining applications, for various seman-
tic classes, semantic relations, and languages. We
can therefore apply some of the research from net-
work analysis to text mining.
The small-world phenomenon, for example, holds
that any node is connected to any other node in at
most six steps. Since as shown in Section 4.5 the se-
mantic networks also exhibit this phenomenon, we
can explain the observation of (Kozareva and Hovy,
2010b) that one can quite accurately predict the rel-
ative ?goodness? of a seed term (its eventual total
yield and the number of steps required to obtain that)
within five harvesting steps. We have shown that due
to the strongly connected components in text min-
ing networks, not all elements within the harvested
graph can discover each other. This implies that har-
vesting algorithms have to be started with several
seeds to obtain adequate Recall (Vyas et al, 2009).
We have shown that centrality measures can be used
successfully to rank harvested terms to guide the net-
work traversal, and to validate the correctness of the
harvested terms.
In the future, the knowledge and observations
made in this study can be used to model the lexi-
cal usage of people over time and to develop new
semantic search technology.
6 Conclusion
In this paper we describe the implicit ?hidden? se-
mantic network graph structure induced over the text
of the web and other sources by the semantic rela-
tions people use in sentences. We describe how term
harvesting patterns whose seed terms are harvested
and then applied recursively can be used to discover
these semantic term networks. Although these net-
works differ considerably from the web in relation
density, type, and network size, we show, some-
what surprisingly, that the same power-law, small-
world effect, transitivity, and most other character-
istics that apply to the web?s hyperlinked network
structure hold also for the implicit semantic term
graphs?certainly for the semantic relations and lan-
guages we have studied, and most probably for al-
most all semantic relations and human languages.
This rather interesting observation leads us to sur-
mise that the hyperlinks people create in the web are
of essentially the same type as the semantic relations
people use in normal sentences, and that they form
an extension of normal language that was not needed
before because people did not have the ability within
the span of a single sentence to ?embed? structures
larger than a clause?certainly not a whole other
page?s worth of information. The principal excep-
tion is the academic citation reference (lexicalized
as ?see?), which is not used in modern webpages.
Rather, the ?lexicalization? now used is a formatting
convention: the hyperlink is colored and often un-
derlined, facilities offered by computer screens but
not available to speech or easy in traditional typeset-
ting.
1623
Acknowledgments
We acknowledge the support of DARPA contract
number FA8750-09-C-3705 and NSF grant IIS-
0429360. We would like to thank Sujith Ravi for
his useful comments and suggestions.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
pages 85?94.
Andrei Broder, Ravi Kumar, Farzin Maghoul, Prabhakar
Raghavan, Sridhar Rajagopalan, Raymie Stata, An-
drew Tomkins, and Janet Wiener. 2000. Graph struc-
ture in the web. Comput. Netw., 33(1-6):309?320.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. pages 101?110.
Peng Chen, Huafeng Xie, Sergei Maslov, and Sid Redner.
2007. Finding scientific gems with google?s pagerank
algorithm. Journal of Informetrics, 1(1):8?15, Jan-
uary.
Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. New-
man. 2009. Power-law distributions in empirical data.
SIAM Rev., 51(4):661?703.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91?134,
June.
Linton Freeman. 1979. Centrality in social networks
conceptual clarification. Social Networks, 1(3):215?
239.
Michael Gasser and Linda B. Smith. 1998. Learning
nouns and adjectives: A connectionist account. In
Language and Cognitive Processes, pages 269?306.
Demdre Gentner. 1981. Some interesting differences be-
tween nouns and verbs. Cognition and Brain Theory,
pages 161?178.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 1?8.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics, pages 539?
545.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003. In-
tegrating web-based and corpus-based techniques for
question answering. In Proceedings of the twelfth text
retrieval conference (TREC), pages 426?435.
David Kempe, Jon Kleinberg, and E?va Tardos. 2003.
Maximizing the spread of influence through a social
network. In KDD ?03: Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 137?146.
Jon Kleinberg and Steve Lawrence. 2001. The structure
of the web. Science, 29:1849?1850.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL 2010, pages 1482?1491, July.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not all
seeds are equal: Measuring the quality of text mining
seeds. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
618?626.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics ACL-08: HLT, pages 1048?1056.
Beth Levin and Harold Somers. 1993. English verb
classes and alternations: A preliminary investigation.
Lun Li, David Alderson, Reiko Tanaka, John C. Doyle,
and Walter Willinger. 2005. Towards a Theory of
Scale-Free Graphs: Definition, Properties, and Impli-
cations (Extended Version). Internet Mathematica,
2(4):431?523.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of the 19th international confer-
ence on Computational linguistics, pages 1?7.
Mark E. Newman and Michelle Girvan. 2004. Find-
ing and evaluating community structure in networks.
Physical Review, 69(2).
Mark Newman. 2003. Mixing patterns in networks.
Physical Review E, 67.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. pages 113?120.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137?145.
1624
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of the Sixteenth ACM Conference on Information
and Knowledge Management, CIKM 2007, pages 683?
690.
Bruno R. Preiss. 1999. Data structures and algorithms
with object-oriented design patterns in C++.
Filippo Radicchi, Santo Fortunato, Benjamin Markines,
and Alessandro Vespignani. 2009. Diffusion of scien-
tific credits and the ranking of scientists. In Phys. Rev.
E 80, 056103.
Deepack Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. pages 41?47.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Empirical Methods for Natural Language
Processing, pages 117?124.
Ellen Riloff. 1993. Automatically constructing a dictio-
nary for information extraction tasks. pages 811?816.
Peter Mark Roget. 1911. Roget?s thesaurus of English
Words and Phrases. New York Thomas Y. Crowell
company.
Gert Sabidussi. 1966. The centrality index of a graph.
Psychometrika, 31(4):581?603.
Hassan Sayyadi and Lise Getoor. 2009. Future rank:
Ranking scientific articles by predicting their future
pagerank. In 2009 SIAM International Conference on
Data Mining (SDM09).
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. pages 1297?1304.
Stephen Soderland, Claire Cardie, and Raymond
Mooney. 1999. Learning information extraction rules
for semi-structured and free text. Machine Learning,
34(1-3), pages 233?272.
Mark Steyvers and Joshua B. Tenenbaum. 2004. The
large-scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cognitive
Science, 29:41?78.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th international
conference on World Wide Web, pages 697?706.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Graph-based weakly-supervised methods for informa-
tion extraction and integration. pages 1473?1481.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Management,
CIKM, pages 225?234.
Dylan Walker, Huafeng Xie, Koon-Kiu Yan, and Sergei
Maslov. 2006. Ranking scientific publications using a
simple model of network traffic. December.
Duncan Watts and Steven Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature,
393(6684):440?442.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In ACL-44: Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 849?856.
Dmitry Zelenko, Chinatsu Aone, Anthony Richardella,
Jaz K, Thomas Hofmann, Tomaso Poggio, and John
Shawe-taylor. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research 3.
1625
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 682?691,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich
Texts
Zornitsa Kozareva
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
kozareva@isi.edu
Abstract
Metaphor is an important way of convey-
ing the affect of people, hence understand-
ing how people use metaphors to convey
affect is important for the communication
between individuals and increases cohe-
sion if the perceived affect of the con-
crete example is the same for the two in-
dividuals. Therefore, building computa-
tional models that can automatically iden-
tify the affect in metaphor-rich texts like
?The team captain is a rock.?, ?Time is
money.?, ?My lawyer is a shark.? is an
important challenging problem, which has
been of great interest to the research com-
munity.
To solve this task, we have collected
and manually annotated the affect of
metaphor-rich texts for four languages.
We present novel algorithms that integrate
triggers for cognitive, affective, perceptual
and social processes with stylistic and lex-
ical information. By running evaluations
on datasets in English, Spanish, Russian
and Farsi, we show that the developed af-
fect polarity and valence prediction tech-
nology of metaphor-rich texts is portable
and works equally well for different lan-
guages.
1 Introduction
Metaphor is a figure of speech in which a word
or phrase that ordinarily designates one thing is
used to designate another, thus making an implicit
comparison (Lakoff and Johnson, 1980; Martin,
1988; Wilks, 2007). For instance, in
?My lawyer is a shark?
the speaker may want to communicate that his/her
lawyer is strong and aggressive, and that he will
attack in court and persist until the goals are
achieved. By using the metaphor, the speaker ac-
tually conveys positive affect because having an
aggressive lawyer is good if one is being sued.
There has been a substantial body of work on
metaphor identification and interpretation (Wilks,
2007; Shutova et al, 2010). However, in this
paper we focus on an equally interesting, chal-
lenging and important problem, which concerns
the automatic identification of affect carried by
metaphors. Building such computational mod-
els is important to understand how people use
metaphors to convey affect and how affect is ex-
pressed using metaphors. The existence of such
models can be also used to improve the communi-
cation between individuals and to make sure that
the speakers perceived the affect of the concrete
metaphor example in the same way.
The questions we address in this paper are:
?How can we build computational models that can
identify the polarity and valence associated with
metaphor-rich texts?? and ?Is it possible to build
such automatic models for multiple languages??.
Our main contributions are:
? We have developed multilingual metaphor-
rich datasets in English, Spanish, Russian and
Farsi that contain annotations of the Positive
and Negative polarity and the valence (from
?3 to +3 scale) corresponding to the inten-
sity of the affect conveyed in the metaphor.
? We have proposed and developed automated
methods for solving the polarity and valence
tasks for all four languages. We model
the polarity task as a classification problem,
while the valence task as a regression prob-
lem.
? We have studied the influence of different in-
formation sources like the metaphor itself,
the context in which it resides, the source and
682
target domains of the metaphor, in addition to
contextual features and trigger word lists de-
veloped by psychologists (Tausczik and Pen-
nebaker, 2010).
? We have conducted in depth experimental
evaluation and showed that the developed
methods significantly outperform baseline
methods.
The rest of the paper is organized as follows.
Section 2 describes related work, Section 3 briefly
talks about metaphors. Sections 4 and 5 describe
the polarity classification and valence prediction
tasks for affect of metaphor-rich texts. Both sec-
tions have information on the collected data for
English, Spanish, Russian and Farsi, the con-
ducted experiments and obtained results. Finally,
we conclude in Section 6.
2 Related Work
A substantial body of work has been done on de-
termining the affect (sentiment analysis) of texts
(Kim and Hovy, 2004; Strapparava and Mihalcea,
2007; Wiebe and Cardie, 2005; Yessenalina and
Cardie, 2011; Breck et al, 2007). Various tasks
have been solved among which polarity and va-
lence identification are the most common. While
polarity identification aims at finding the Positive
and Negative affect, valence is more challenging
as it has to map the affect on a [?3,+3] scale
depending on its intensity (Polanyi and Zaenen,
2004; Strapparava and Mihalcea, 2007).
Over the years researchers have developed vari-
ous approaches to identify polarity of words (Esuli
and Sebastiani, 2006), phrases (Turney, 2002; Wil-
son et al, 2005), sentences (Choi and Cardie,
2009) even documents (Pang and Lee, 2008).
Multiple techniques have been employed, from
various machine learning classifiers, to clustering
and topic models. Various domains and textual
sources have been analyzed such as Twitter, Blogs,
Web documents, movie and product reviews (Tur-
ney, 2002; Kennedy and Inkpen, 2005; Niu et al,
2005; Pang and Lee, 2008), but yet what is miss-
ing is affect analyzer for metaphor-rich texts.
While the affect of metaphors is well stud-
ied from its linguistic and psychological aspects
(Blanchette et al, 2001; Tomlinson and Love,
2006; Crawdord, 2009), to our knowledge the
building of computational models for polarity and
valence identification in metaphor-rich texts is still
a novel task (Smith et al, 2007; Veale, 2012; Veale
and Li, 2012; Reyes and Rosso, 2012; Reyes et
al., 2013). Little (almost no) effort has been put
into multilingual computational affect models of
metaphor-rich texts. Our research specifically tar-
gets the resolution of these problems and shows
that it is possible to build such computational mod-
els. The experimental result provide valuable con-
tributions and fundings, which could be used by
the research community to build upon.
3 Metaphors
Although there are different views on metaphor in
linguistics and philosophy (Black, 1962; Lakoff
and Johnson, 1980; Gentner, 1983; Wilks, 2007),
the common among all approaches is the idea of
an interconceptual mapping that underlies the pro-
duction of metaphorical expressions. There are
two concepts or conceptual domains: the target
(also called topic in the linguistics literature) and
the source (or vehicle), and the existence of a link
between them gives rise to metaphors.
The texts ?Your claims are indefensible.? and
?He attacked every weak point in my argument.?
do not directly talk about argument as a war, how-
ever the winning or losing of arguments, the attack
or defense of positions are structured by the con-
cept of war. There is no physical battle, but there
is a verbal battle and the structure of an argument
(attack, defense) reflects this (Lakoff and Johnson,
1980).
As we mentioned before, there has been a lot of
work on the automatic identification of metaphors
(Wilks, 2007; Shutova et al, 2010) and their
mapping into conceptual space (Shutova, 2010a;
Shutova, 2010b), however these are beyond the
scope of this paper. Instead we focus on an equally
interesting, challenging and important problem,
which concerns the automatic identification of af-
fect carried by metaphors. To conduct our study,
we use human annotators to collect metaphor-rich
texts (Shutova and Teufel, 2010) and tag each
metaphor with its corresponding polarity (Posi-
tive/Negative) and valence [?3,+3] scores. The
next sections describe the affect polarity and va-
lence tasks we have defined, the collected and an-
notated metaphor-rich data for each one of the En-
glish, Spanish, Russian and Farsi languages, the
conducted experiments and obtained results.
683
4 Task A: Polarity Classification
4.1 Problem Formulation
Task Definition: Given metaphor-rich texts annotated with
Positive and Negative polarity labels, the goal is to build an
automated computational affect model, which can assign to
previously unseen metaphors one of the two polarity classes.
a tough pill to swallow  
values that gave our nation birth  
Clinton also came into office hoping to bridge Washington?s partisan divide.  
Thirty percent of our mortgages are underwater.  
The administration, in fact, could go further with the budget knife by eliminating the V-22 Osprey aircraft 
 the 'things' are going to make sure their ox doesn't get gored  
Figure 1: Polarity Classification
Figure 1 illustrates the polarity task in which the
metaphors were classified into Positive or Nega-
tive. For instance, the metaphor ?tough pill to
swallow? has Negative polarity as it stands for
something being hard to digest or comprehend,
while the metaphor ?values that gave our nation
birth? has a Positive polarity as giving birth is like
starting a new beginning.
4.2 Classification Algorithms
We model the metaphor polarity task as a classifi-
cation problem in which, for a given collection of
N training examples, where mi is a metaphor and
ci is the polarity of mi, the objective is to learn
a classification function f : mi ? ci in which 1
stands for positive polarity and 0 stands for neg-
ative polarity. We tested five different machine
learning algorithms such as Nave Bayes, SVM
with polynomial kernel, SVM with RBF kernel,
AdaBoost and Stacking, out of which AdaBoost
performed the best. In our experimental study, we
use the freely available implementations in Weka
(Witten and Frank, 2005).
Evaluation Measures: To evaluate the goodness
of the polarity classification algorithms, we cal-
culate the f-score and accuracy on 10-fold cross
validation.
4.3 Data Annotation
To conduct our experimental study, we have used
annotated data provided by the Language Com-
puter Corporation (LCC)1, which developed anno-
1http://www.languagecomputer.com/
tation toolkit specifically for the task of metaphor
detection, interpretation and affect assignment.
They hired annotators to collect and annotate data
for the English, Spanish, Russian and Farsi lan-
guages. The domain for which the metaphors were
collected was Governance. It encompasses elec-
toral politics, the setting of economic policy, the
creation, application and enforcement of rules and
laws. The metaphors were collected from polit-
ical speeches, political websites, online newspa-
pers among others (Mohler et al, 2013).
The annotation toolkit allowed annotators to
provide for each metaphor the following infor-
mation: the metaphor, the context in which the
metaphor was found, the meaning of the metaphor
in the source and target domains from the per-
spective of a native speaker. For example, in the
Context: And to all nations, we will speak for the
values that gave our nation birth.; the annotators
tagged the Metaphor: values that gave our nation
birth; and listed as Source: mother gave birth to
baby; and Target: values of freedom and equal-
ity motivated the creation of America. The same
annotators also provided the affect associated with
the metaphor. The agreements of the annotators as
measured by LCC are: .83, .87, .80 and .61 for the
English, Spanish, Russian and Farsi languages.
In our study, the maximum length of a metaphor
is a sentence, but typically it has the span of a
phrase. The maximum length of a context is three
sentences before and after the metaphor, but typ-
ically it has the span of one sentence before and
after. In our study, the source and target domains
are provided by the human annotators who agree
on these definitions, however the source and target
can be also automatically generated by an inter-
pretation system or a concept mapper. The gen-
eration of source and target information is beyond
the scope of this paper, but studying their impact
on affect is important. At the same time, we want
to show that if the technology for source/target de-
tection and interpretation is not yet available, then
how far can one reach by using the metaphor itself
and the context around it. Later depending on the
availability of the information sources and toolkits
one can decide whether to integrate such informa-
tion or to ignore it. In the experimental sections,
we show how the individual information sources
and their combination affects the resolution of the
metaphor polarity and valence prediction tasks.
Table 1 shows the positive and negative class
684
distribution for each one of the four languages.
Negative Positive
ENGLISH 2086 1443
SPANISH 196 434
RUSSIAN 468 418
FARSI 384 252
Table 1: Polarity Class Distribution for Four Lan-
guages
The majority of the the annotated examples are
for English. However, given the difficulty of find-
ing bilingual speakers, we still managed to collect
around 600 examples for Spanish and Farsi, and
886 examples for Russian.
4.4 N-gram Evaluation and Results
N-gram features are widely used in a variety of
classification tasks, therefore we also use them in
our polarity classification task. We studied the in-
fluence of unigrams, bigrams and a combination
of the two, and saw that the best performing fea-
ture set consists of the combination of unigrams
and bigrams. In this paper, we will refer from now
on to n-grams as the combination of unigrams and
bigrams.
Figure 2 shows a study of the influence of the
different information sources and their combina-
tion with n-gram features for English.
!"#$
!%#$
!&#$!'#$
!(#$!)#$
!*#$!+#$
!,#$%$
-./0123
4$
53647.$ /048./$53647.9/
048./$ 73:/.;/
$
-./012
3495364
7.9/048
./$
73:/.;/
953647.
9/048./
$
Figure 2: Influence of Information Sources for
Metaphor Polarity Classification of English Texts
For each information source (metaphor, context,
source, target and their combinations), we built a
separate n-gram feature set and model, which was
evaluated on 10-fold cross validation. The results
from this study show that for English, the more
information sources one combines, the higher the
classification accuracy becomes.
Table 2 shows the influence of the information
sources for Spanish, Russian and Farsi with the n-
gram features. The best f-scores for each language
are shown in bold. For Farsi and Russian high per-
formances are obtained both with the context and
with the combination of the context, source and
target information. While for Spanish they reach
similar performance.
SPANISH RUSSIAN FARSI
Metaphor 71.6 71.0 62.4
Source 67.1 62.4 55.4
Target 68.9 67.2 62.4
Context 73.5 77.1 67.4
S+T 76.6 68.7 62.4
M+S+T 76.0 75.4 64.2
C+S+T 76.5 76.5 68.4
Table 2: N-gram features, F-scores on 10-fold val-
idation for Spanish, Russian and Farsi
4.5 LIWC as a Proxy for Metaphor Polarity
LIWC Repository: In addition to the n-gram
features, we also used the Linguistic Inquiry and
Word Count (LIWC) repository (Tausczik and
Pennebaker, 2010), which has 64 word categories
corresponding to different classes like emotional
states, psychological processes, personal concerns
among other. Each category contains a list of
words characterizing it. For instance, the LIWC
category discrepancy contains words like should,
could among others, while the LIWC category in-
hibition contains words like block, stop, constrain.
Previously LIWC was successfully used to ana-
lyze the emotional state of bloggers and tweeters
(Quercia et al, 2011) and to identify deception and
sarcasm in texts (Ott et al, 2011; Gonza?lez-Iba?n?ez
et al, 2011). When LIWC analyzes texts it gener-
ates statistics like number of words found in cat-
egory Ci divided by the total number of words in
the text. For our metaphor polarity task, we use
LIWC?s statistics of all 64 categories and feed this
information as features for the machine learning
classifiers. LIWC repository contains conceptual
categories (dictionaries) both for the English and
Spanish languages.
LIWC Evaluation and Results: In our experi-
ments LIWC is applied to English and Spanish
metaphor-rich texts since the LIWC category dic-
tionaries are available for both languages. Table 3
shows the obtained accuracy and f-score results in
English and Spanish for each one of the informa-
tion sources.
685
ENGLISH SPANISH
Acc Fscore Acc Fscore
Metaphor 98.8 98.8 87.9 87.2
Source 98.6 98.6 97.3 97.3
Target 98.2 98.2 97.9 97.9
Context 91.4 91.4 93.3 93.2
S+T 98.0 98.0 76.3 75.5
M+S+T 95.8 95.7 86.8 86.0
C+S+T 87.9 88.0 79.2 78.5
Table 3: LIWC features, Accuracy and F-scores
on 10-fold validation for English and Spanish
The best performances are reached with indi-
vidual information sources like metaphor, context,
source or target instead of their combinations. The
classifiers obtain similar performance for both lan-
guages.
LIWC Category Relevance to Metaphor Polar-
ity: We also study the importance and relevance
of the LIWC categories for the metaphor polar-
ity task. We use information gain (IG) to mea-
sure the amount of information in bits about the
polarity class prediction, if the only information
available is the presence of a given LIWC cate-
gory (feature) and the corresponding polarity class
distribution. IG measures the expected reduction
in entropy (uncertainty associated with a random
feature) (Mitchell, 1997).
Figure 3 illustrates how certain categories occur
more with the positive (in red color) vs negative
(in green color) class. With the positive metaphors
we observe the LIWC categories for present tense,
social, affect and family, while for the negative
metaphors we see LIWC categories for past tense,
inhibition and anger.
!"#$%$&'#&%()*%!&)#+'%"',&)%-"$&,+).%
!)&#&'$%$&'#&%!&)#+'"/%!)+'+0'#%1"23/.%-"$&,+).%#+-3"/%!)&#&'-&%
45%
6%
5%
Figure 3: LIWC category relevance to Metaphor
Polarity
In addition, we show in Figure 4 examples of
the top LIWC categories according to IG ranking
for each one of the information sources.
!"#$%&'() *'+#",#) -'.(/") 0$(1"#)2) 2) !""#$ /'+34)/'+34) %&'$ 5+1"6#) $+1"()$+1"() ()"*)"$ ("7$895#:) $;"/#)+,(-."/01-%$ /0(2$2"1("$ &'<") ='(>)6="$()='(?6) 6="$()='(?6) 6="$()='(?6)5+&5@58'+) !.,"1+$ ."#,3,&1$4&+%$ $;"/#) (/0-"$("7$895#:) (0+$&'<") 5+&5@58'+)5+1"6#) 5+1"6#)='(>) ='(>)
@7'/>)/'+6#($5+)6#'%)
6&'.7?)/'.7?)='.7?)
&$#")>577)$++':"?)
Figure 4: Example of LIWC Categories and
Words
For metaphor texts, these categories are I, con-
juntion, anger, discrepancy, swear words among
others; for contexts the categories are pronouns
like I, you, past tense, friends, affect and so on.
Our study shows that some of the LIWC categories
are important across all information sources, but
overall different triggers activate depending on the
information source and the length of the text used.
4.6 Comparative study
Figure 5 shows a comparison of the accuracy of
our best performing approach for each language.
For English and Spanish these are the LIWC mod-
els, while for Russian and Farsi these are the n-
gram models. We compare the performance of the
algorithms with a majority baseline, which assigns
the majority class to each example. For instance,
in English there are 3529 annotated examples, of
which 2086 are positive and 1443 are negative.
Since the positive class is the predominant one
for this language and dataset, a majority classifier
would have .59 accuracy in returning the positive
class as an answer. Similarly, we compute the ma-
jority baseline for the rest of the languages.
!""#$%"&' (%)*$+,&'-%./0+1/' 2+3/$/1"/'''4150+.6' 7898:' ;79<<' =>79?7'@A%1+.6' 7B97:' ?8988' =C79:C'D#..+%1' BB9::' ;C98C' =CE9<8'F%$.+' BC9C:' ?:9>:' =<<97:'
Figure 5: Best Accuracy Model and Comparison
against a Majority Baseline for Metaphor Polarity
Classification
As we can see from Figure 5 that all classi-
fiers significantly outperform the majority base-
686
line. For Farsi the increment is +11.90, while for
English the increment is +39.69. This means that
the built classifiers perform much better than a ran-
dom classifier.
4.7 Lessons Learned
To summarize, in this section we have defined the
task of polarity classification and we have pre-
sented a machine learning solution. We have used
different feature sets and information sources to
solve the task. We have conducted exhaustive
evaluations for four different languages namely
English, Spanish, Russian and Farsi. The learned
lessons from this study are: (1) for n-gram us-
age, the larger the context of the metaphor, the
better the classification accuracy becomes; (2) if
present source and target information can further
boost the performance of the classifiers; (3) LIWC
is a useful resource for polarity identification in
metaphor-rich texts; (4) analyzing the usages of
tense like past vs. present and pronouns are im-
portant triggers for positive and negative polarity
of metaphors; (5) some categories like family, so-
cial presence indicate positive polarity, while oth-
ers like inhibition, anger and swear words are in-
dicative of negative affect; (6) the built models sig-
nificantly outperform majority baselines.
5 Task B: Valence Prediction
5.1 Problem Formulation
Task Definition: Given metaphor-rich texts annotated
with valence score (from ?3 to +3), where ?3 indicates
strong negativity, +3 indicates strong positivity, 0 indi-
cates neural, the goal is to build a model that can predict
without human supervision the valence scores of new pre-
viously unseen metaphors.
The administration, in fact, could go further with the budget knife by eliminating the V-22 Osprey aircraft 
Clinton also came into office hoping to bridge Washington?s partisan divide.  
values that gave our nation birth  !"#
!$#
!%#
a tough pill to swallow  &"#
 the 'things' are going to make sure their ox doesn't get gored  &$#
Thirty percent of our mortgages are underwater.  &%#
Figure 6: Valence Prediction
Figure 6 shows an example of the valence pre-
diction task in which the metaphor-rich texts must
be arranged by the intensity of the emotional
state provoked by the texts. For instance, ?3
corresponds to very strong negativity, ?2 strong
negativity, ?1 weak negativity (similarly for the
positive classes). In this task we also consider
metaphors with neutral affect. They are annotated
with the 0 label and the prediction model should be
able to predict such intensity as well. For instance,
the metaphor ?values that gave our nation birth?,
is considered by American people that giving birth
sets new beginning and has a positive score +1,
but ?budget knife? is more positive +3 since tax
cut is more important. As any sentiment analysis
task, affect assignment of metaphors is also a sub-
jective task and the produced annotations express
the values, believes and understanding of the an-
notators.
5.2 Regression Model
We model the valence task a regression prob-
lem, in which for a given metaphor m, we seek
to predict the valence v of m. We do this via
a parametrized function f :v? = f(m;w), where
w ? Rd are the weights. The objective is to
learn w from a collection of N training examples
{< mi, vi >}Ni=1, where mi are the metaphor ex-
amples and vi ? R is the valence score of mi.Support vector regression (Drucker et al, 1996)is a well-known method for training a regressionmodel by solving the following optimization prob-lem:
min
w?Rs
1
2 ||w||
2 + CN
N?
i=1
max(0, |vi ? f(mi;w)| ? )? ?? ?
-insensitive loss function
where C is a regularization constant and  controls
the training error. The training algorithm finds
weights w that define a function f minimizing the
empirical risk. Let h be a function from seeds into
some vector-space representation ? Rd, then the
function f takes the form: f(m;w) = h(m)Tw =?N
i=1 ?iK(m,mi), where f is re-parameterized
in terms of a polynomial kernel function K with
dual weights ?i. K measures the similarity be-
tween two metaphoric texts. Full details of the
regression model and its implementation are be-
yond the scope of this paper; for more details see
(Scho?lkopf and Smola, 2001; Smola et al, 2003).
In our experimental study, we use the freely avail-
able implementation of SVM in Weka (Witten and
Frank, 2005).
687
Evaluation Measures: To evaluate the quality of
the valence prediction model, we compare the ac-
tual valence score of the metaphor given by human
annotators denoted with y against those valence
scores predicted by the regression model denoted
with x. We estimate the goodness of the regres-
sion model calculating both the correlation coef-
ficient ccx,y = n
?
xiyi?
?
xi
?
yi?
n
?
x2i?(
?
xi)2
?
n
?
y2i?(
?
yi)2
and the mean squared error msex,y =
?n
i=i(x?x?)
n .The two evaluation measures should be interpreted
in the following manner. Intuitively the higher the
correlation score is, the better the correlation be-
tween the actual and the predicted valence scores
will be. Similarly the smaller the mean squared
error rate, the better the regression model fits the
valence predictions to the actual score.
5.3 Data Annotation
To conduct our valence prediction study, we used
the same human annotators from the polarity clas-
sification task for each one of the English, Span-
ish, Russian and Farsi languages. We asked the
annotators to map each metaphor on a [?3,+3]
scale depending on the intensity of the affect asso-
ciated with the metaphor.
Table 4 shows the distribution (number of ex-
amples) for each valence class and for each lan-
guage.
-3 -2 -1 0 +1 +2 +3
ENGLISH 1057 817 212 582 157 746 540
SPANISH 106 65 27 17 40 132 262
RUSSIAN 118 42 308 13 202 149 67
FARSI 147 117 120 49 91 63 98
Table 4: Valence Score Distribution for Each Lan-
guage
5.4 Empirical Evaluation and Results
For each language and information source we built
separate valence prediction regression models. We
used the same features for the regression task as
we have used in the classification task. Those in-
clude n-grams (unigrams, bigrams and combina-
tion of the two), LIWC scores. Table 5 shows
the obtained correlation coefficient (CC) and mean
squared error (MSE) results for each one of the
four languages (English, Spanish, Russian and
Farsi) using the dataset described in Table 4.
The Farsi and Russian regression models are
based only on n-gram features, while the English
and Spanish regression models have both n-gram
and LIWC features. Overall, the CC for English
and Spanish is higher when LIWC features are
used. This means that the LIWC based valence re-
gression model approximates the predicted values
better to those of the human annotators. The better
valence prediction happens when the metaphor it-
self is used by LIWC. The MSE for English and
Spanish is the lowest, meaning that the predic-
tion is the closest to those of the human annota-
tors. In Russian and Farsi the lowest MSE is when
the combined metaphor, source and target infor-
mation sources are used. For English and Spanish
the smallest MSE or so called prediction error is
1.52 and 1.30 respectively, while for Russian and
Farsi is 1.62 and 2.13 respectively.
5.5 Lessons Learned
To summarize, in this section we have defined
the task of valence prediction of metaphor-rich
texts and we have described a regression model
for its solution. We have studied different fea-
ture sets and information sources to solve the task.
We have conducted exhaustive evaluations in all
four languages namely English, Spanish, Russian
and Farsi. The learned lessons from this study
are: (1) valence prediction is a much harder task
than polarity classification both for human annota-
tion and for the machine learning algorithms; (2)
the obtained results showed that despite its dif-
ficulty this is still a plausible problem; (3) sim-
ilarly to the polarity classification task, valence
prediction with LIWC is improved when shorter
contexts (the metaphor/source/target information
source) are considered.
6 Conclusion
People use metaphor-rich language to express af-
fect and often affect is expressed through the usage
of metaphors. Therefore, understanding that the
metaphor ?I was boiling inside when I saw him.?
has Negative polarity as it conveys feeling of anger
is very important for interpersonal or multicultural
communications.
In this paper, we have introduced a novel corpus
of metaphor-rich texts for the English, Spanish,
Russian and Farsi languages, which was manu-
ally annotated with the polarity and valence scores
of the affect conveyed by the metaphors. We
have studied the impact of different information
sources such as the metaphor in isolation, the con-
text in which the metaphor was used, the source
and target domain meanings of the metaphor and
688
RUSSIAN N-gram FARSI N-gram ENGLISH N-gram SPANISH N-gram ENGLISH LIWC SPANISH LIWC
CC MSE CC MSE CC MSE CC MSE CC MSE CC MSE
Metaphor .45 1.71 .25 2.25 .36 2.50 .37 2.54 .74 1.52 .87 1.20
Source .22 1.89 .11 2.42 .40 2.27 .22 2.43 .81 1.30 .85 1.28
Target .25 1.91 .15 2.47 .37 2.41 .32 2.36 .72 1.56 .85 1.29
Context .43 1.83 .32 2.38 .37 2.59 .40 2.37 .40 2.16 .67 1.92
S+T .29 1.83 .18 2.38 .40 2.40 .41 2.19 .70 1.60 .78 1.53
M+S+T .45 1.62 .29 2.13 .43 2.34 .43 2.14 .67 1.67 .78 1.53
C+S+T .42 1.85 .26 2.61 .43 2.52 .39 2.41 .44 2.08 .64 1.96
Table 5: Valence Prediction, Correlation Coefficient and Mean Squared Error for English, Spanish, Rus-
sian and Farsi
their combination in order to understand how such
information helps and impacts the interpretation
of the affect associated with the metaphor. We
have conducted exhaustive evaluation with multi-
ple machine learning classifiers and different fea-
tures sets spanning from lexical information to
psychological categories developed by (Tausczik
and Pennebaker, 2010). Through experiments car-
ried out on the developed datasets, we showed that
the proposed polarity classification and valence
regression models significantly improve baselines
(from 11.90% to 39.69% depending on the lan-
guage) and work well for all four languages. From
the two tasks, the valence prediction problem was
more challenging both for the human annotators
and the automated system. The mean squared er-
ror in valence prediction in the range [?3,+3],
where ?3 indicates strong negative and +3 indi-
cates strong positive affect for English, Spanish
and Russian was around 1.5, while for Farsi was
around 2.
The current findings and learned lessons reflect
the properties of the collected data and its anno-
tations. In the future we are interested in study-
ing the affect of metaphors for domains differ-
ent than Governance. We want to conduct stud-
ies with the help of social sciences who would re-
search whether the tagging of affect in metaphors
depends on the political affiliation, age, gender or
culture of the annotators. Not on a last place, we
would like to improve the built valence prediction
models and to collect more data for Spanish, Rus-
sian and Farsi.
Acknowledgments
The author would like to thank the reviewers for
their helpful comments as well as the LCC anno-
tators who have prepared the data and made this
work possible. This research is supported by the
Intelligence Advanced Research Projects Activ-
ity (IARPA) via Department of Defense US Army
Research Laboratory contract number W911NF-
12-C-0025. The U.S. Government is authorized to
reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright anno-
tation thereon. Disclaimer: The views and con-
clusions contained herein are those of the authors
and should not be interpreted as necessarily rep-
resenting the official policies or endorsements, ei-
ther expressed or implied, of IARPA, DoD/ARL,
or the U.S. Government.
References
Max Black. 1962. Models and Metaphors.
Isabelle Blanchette, Kevin Dunbar, John Hummel, and
Richard Marsh. 2001. Analogy use in naturalis-
tic settings: The influence of audience, emotion and
goals. Memory and Cognition, pages 730?735.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI?07, pages 2683?
2688. Morgan Kaufmann Publishers Inc.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2 -
Volume 2, EMNLP ?09, pages 590?598.
Elizabeth Crawdord. 2009. Conceptual metaphors of
affect. Emotion Review, pages 129?139.
Harris Drucker, Chris J.C. Burges, Linda Kaufman,
Alex Smola, and Vladimir Vapnik. 1996. Support
vector regression machines. In Advances in NIPS,
pages 155?161.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06, pages 417?422.
Dedre Gentner. 1983. Structure-mapping: A theo-
retical framework for analogy. Cognitive Science,
7(2):155?170.
689
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresa n, and
Nina Wacholder. 2011. Identifying sarcasm in twit-
ter: a closer look. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 581?586.
Alistair Kennedy and Diana Inkpen. 2005. Sentiment
classification of movie and product reviews using
contextual valence shifters. Computational Intelli-
gence, pages 110?125.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, COLING ?04.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live By. University of Chicago Press, Chicago.
James H. Martin. 1988. Representing regularities in
the metaphoric lexicon. In Proceedings of the 12th
conference on Computational linguistics - Volume 1,
COLING ?88, pages 396?401.
Thomas M. Mitchell. 1997. Machine Learning.
McGraw-Hill, Inc., 1 edition.
Michael Mohler, David Bracewell, David Hinote, and
Marc Tomlinson. 2013. Semantic signatures for
example-based linguistic metaphor detection. In
The Proceedings of the First Workshop on Metaphor
in NLP, (NAACL), pages 46?54.
Yun Niu, Xiaodan Zhu, Jianhua Li, and Graeme Hirst.
2005. Analysis of polarity information in medical
text. In In: Proceedings of the American Medical
Informatics Association 2005 Annual Symposium,
pages 570?574.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 309?319.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135, January.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Yan Qu, James Shana-
han, and Janyce Wiebe, editors, Proceedings of the
AAAI Spring Symposium on Exploring Attitude and
Affect in Text: Theories and Applications. AAAI
Press. AAAI technical report SS-04-07.
Daniele Quercia, Jonathan Ellis, Licia Capra, and Jon
Crowcroft. 2011. In the mood for being influential
on twitter. In the 3rd IEEE International Conference
on Social Computing.
Antonio Reyes and Paolo Rosso. 2012. Making ob-
jective decisions from subjective data: Detecting
irony in customer reviews. Decis. Support Syst.,
53(4):754?760, November.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013.
A multidimensional approach for detecting irony in
twitter. Lang. Resour. Eval., 47(1):239?268, March.
Bernhard Scho?lkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond (Adap-
tive Computation and Machine Learning). The MIT
Press.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source - target do-
main mappings. In International Conference on
Language Resources and Evaluation.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics, COLING
?10, pages 1002?1010.
Ekaterina Shutova. 2010a. Automatic metaphor in-
terpretation as a paraphrasing task. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, HLT ?10, pages
1029?1037.
Ekaterina Shutova. 2010b. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 688?697.
Catherine Smith, Tim Rumbell, John Barnden, Bob
Hendley, Mark Lee, and Alan Wallington. 2007.
Don?t worry about metaphor: affect extraction for
conversational agents. In Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 37?
40. Association for Computational Linguistics.
Alex J. Smola, Bernhard Schlkopf, and Bernhard Sch
Olkopf. 2003. A tutorial on support vector regres-
sion. Technical report, Statistics and Computing.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 70?74. Association for
Computational Linguistics, June.
Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Com-
puterized Text Analysis Methods. Journal of Lan-
guage and Social Psychology, 29(1):24?54, March.
Marc T. Tomlinson and Bradley C. Love. 2006. From
pigeons to humans: grounding relational learning in
concrete examples. In Proceedings of the 21st na-
tional conference on Artificial intelligence - Volume
1, AAAI?06, pages 199?204. AAAI Press.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?02, pages 417?424.
690
Tony Veale and Guofu Li. 2012. Specifying viewpoint
and information need with affective metaphors: a
system demonstration of the metaphor magnet web
app/service. In Proceedings of the ACL 2012 System
Demonstrations, ACL ?12, pages 7?12.
Tony Veale. 2012. A context-sensitive, multi-faceted
model of lexico-conceptual affect. In The 50th An-
nual Meeting of the Association for Computational
Linguistics, Proceedings of the Conference, pages
75?79.
Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities.
Yorick Wilks. 2007. A preferential, pattern-seeking,
semantics for natural language inference. In Words
and Intelligence I, volume 35 of Text, Speech
and Language Technology, pages 83?102. Springer
Netherlands.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, second edition.
Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment analy-
sis. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 172?182.
691
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 394?398,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
SemEval-2012 Task 7: Choice of Plausible Alternatives:  An Evaluation of Commonsense Causal Reasoning   Andrew S. Gordon Zornitsa Kozareva Melissa Roemmele Institute for Creative Technologies Information Sciences Institute Department of Linguistics University of Southern California University of Southern California Indiana University Los Angeles, CA Marina del Rey, CA Bloomington, IN gordon@ict.usc.edu kozareva@isi.edu msroemme@gmail.com       Abstract 
SemEval-2012 Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise. In this paper, we de-scribe the development of this task and its mo-tivation. We describe the two systems that competed in this task as part of SemEval-2012, and compare their results to those achieved in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future. 
1 Motivation Open-domain commonsense reasoning is one of the grand challenges of artificial intelligence, and has been the subject of research since the inception of the field. Until recently, this research history has been dominated by formal approaches (e.g. Lenat, 1995), where logical formalizations of com-monsense theories were hand-authored by expert logicians and evaluated using a handful of com-monsense challenge problems (Morgenstern, 2012). Progress via this approach has been slow, both because of the inherent difficulties in author-ing suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions. 
Radically different approaches to the com-monsense reasoning problem have recently been explored by natural language processing research-ers. Speer et al (2008) describe a novel reasoning approach that applies dimensionality reduction to the space of millions of English-language com-monsense facts in a crowd-sourced knowledge base (Liu & Singh, 2004). Gordon et al, (2010) describe a method for extracting millions of com-monsense facts from parse trees of English sen-tences. Jung et al (2010) describe a novel approach to the extraction of commonsense knowledge about activities by mining online how-to articles. We believe that these new NLP-based approaches hold enormous potential for overcom-ing the knowledge acquisition bottleneck that has limited progress in commonsense reasoning in pre-vious decades. Given the growth and enthusiasm for these new approaches, there is increasing need for a common metric for evaluation. A common evaluation suite would allow researchers to gauge the performance of new versions of their own systems, and to com-pare their approaches with those of other research groups. Evaluations for these new NLP-based ap-proaches should themselves be based in natural language, and must be suitably large to truly eval-uate the breadth of different reasoning approaches. Still, each evaluation should be focused on one dimension of the overall commonsense reasoning task, so as not to create a new challenge that no single research group could hope to succeed.  In SemEval-2012 Task 7, we presented a new evaluation for open-domain commonsense reason-
394
ing, focusing specifically on commonsense causal reasoning about everyday events. 2 Choice of Plausible Alternatives Consider the following English sentence, describ-ing a hypothetical state of the world: The man lost his balance on the ladder.  In addition to parsing this sentence, resolving ambiguities, and constructing a semantic interpre-tation, human readers also imagine the causal ante-cedents and consequents that would follow if the statement were true. With such a brief description, readers are left with many questions. How high up on the ladder was this man? What was he doing on the ladder in the first place? How much experience does he have using ladders? Was he intoxicated? The answers to these questions help readers formu-late hypotheses for the two central concerns when reasoning about events: What was the cause of this? and What happened as a result?  As computational linguists, we imagine that our automated natural language processing algorithms will also, eventually, need to engage in similar rea-soning processes in order to achieve human-like performance on text understanding tasks. Progress toward the goal of deep semantic interpretation of text has been slow. However, the last decade of natural language processing research has shown that enormous gains can be achieved when there is a clear evaluation metric. A shared task with an automated scoring mechanism allows researchers to compare different approaches, tune system pa-rameters to maximize performance, and assess progress toward broader research objectives. De-veloping an evaluation metric for causal reasoning poses a number of challenges. It is necessary to formulate a question with answers that can be au-tomatically graded, but can still serve as a proxy for the complex, generative imagination of readers. Roemmele et al (2011) offered a solution in the form of a simple binary-choice question. Presented with an English sentence describing a premise, systems must select between two alternatives (also sentences) the one that more plausibly has a causal relation to the premise, as in the following exam-ple: Premise: The man lost his balance on the lad-der. What happened as a result? Alternative 1: He fell off the ladder. 
Alternative 2: He climbed up the ladder. Both of these alternatives are conceivable, and neither is entailed by the premise. However, hu-man readers have no difficulty selecting the alter-native that is the more plausible of the two. This question asks about a causal consequent, and a complimentary formulation asks for the causal an-tecedent, as in the following example: Premise: The man fell unconscious. What was the cause of this? Alternative 1: The assailant struck the man on the head. Alternative 2: The assailant took the man's wal-let. Roemmele et al describe their efforts to author a collection of 1000 questions of these two types to create a new causal reasoning evaluation tool: the Choice of Plausible Alternatives (COPA). When presented to humans to select the correct alterna-tive, the inter-rater agreement was extremely high (Cohen's kappa = 0.965). Where disagreements between two raters were found (in 26 of 1000 items), questions were removed and replaced with new ones with perfect agreement. To develop an automated evaluation tool, the 1000 questions were randomly ordered and sorted into two equally sized sets of 500 questions to serve as development and test sets. The order of the correct alternative was also randomized, such that the expected accuracy of a random baseline would be 50%. Gold-standard answers for each split are used to automatically evaluate a given system's performance.  The distribution of the COPA evaluation in-cludes an automated test of statistical significance of differences seen between two competing sys-tems. This software tool implements a compute-intensive randomized test of statistical significance using stratified shuffling, as described by Noreen (1989). By randomly sorting answers between two systems over thousands of trials, this test computes the likelihood that differences as great as observed differences could be obtained by random chance. The COPA evaluation is most similar in style to the Recognizing Textual Entailment challenge (Degan et al, 2006), but differs in its focus on causal implication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns 
395
the defeasible inferences that can be drawn from the interpretation of a sentence. In this respect, COPA overlaps in its aims with the task of recog-nizing causal relations in text through automated discourse processing (e.g. Marcu, 1999). Some progress in automated discourse processing has been made using supervised machine learning methods, where system learn the lexical-syntactic patterns that are most correlated with causal rela-tions from a large annotated corpus (Sagae, 2009). Lacking a dedicated training corpus, the COPA evaluation encourages competitors to capture commonsense causal knowledge from any availa-ble corpus or existing knowledge repository. 3 SemEval-2012 Systems and Results The COPA evaluation was accepted as Task 7 of the 6th International Workshop on Semantic Eval-uation (SemEval-2012). In several respects, the COPA evaluation was different than the typical shared task offered as part of this series of work-shops. First, the task materials were available and distributed long before the evaluation period be-gan, and there were published results of previous systems using this evaluation.1 Second, the task included no training data, only sets of development and test questions (500 each). Participants were encouraged to use any available text corpus or knowledge repositories in the construction of their systems. Success on the task would not be possible simply through the selection of machine learning algorithms and feature encodings. Instead, some creativity and ingenuity was needed to find a suita-ble source of commonsense causal information, and determine an automated mechanism for apply-ing this information to COPA questions. Only one team successfully completed the task and submitted results during the official two-week SemEval-2012 evaluation period. This team was Travis Goodwin, Bryan Rink, Kirk Roberts, and Sanda M. Harabagiu from the University of Texas at Dallas, Human Language Technology Research Institute. This team submitted results from two different systems (Goodwin et al, 2012), which they described to us as follows: UTDHLT Bigram PMI: The team's first ap-proach selects the alternative with the maximum Pointwise Mutual Information (PMI) statistic                                                             1 http://www.ict.usc.edu/~gordon/copa.html 
(Church & Hanks, 1990) over all pairs of bigrams (at the token level) between the candidate alterna-tive and the premise. PMI statistics were collected using 8.4 million documents from the LDC Giga-word corpus (Graff & Cieri, 2003). A window of 100 terms was used for finding pairs of co-occurring bigrams, and a window/slop size of 2 for the bigram itself. UTDHLT SVM Combined: The team's second approach augments the first by combining it with several other features and casting the task as a classification problem. To this end, they consider the PMI between events participating in a temporal link on a Time-ML annotated Gigaword corpus. That is, events that occur together frequently will have a higher PMI. They also consider the differ-ence between the number of positive and negative polarity words between an alternative and premise using information from the Harvard Inquisitor. In addition, they used the count of matching cause-effect pairs extracted using patterns on dependency structures from the Gigaword corpus. Combining all of these sources of information, they trained a support vector machine (SVM) learning algorithm to classify the alternative that is most causally re-lated to the premise. These systems were assessed based on their ac-curacy on the 500 questions in the test split of the COPA evaluation, presented in Table 1. Both sys-tems significantly outperformed the random base-line (50% accuracy), but the gains seen in the second approach were not significantly different than those of the first.   System Accuracy UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4%  Table 1. SemEval-2012 Task 7 system accuracy on 500 questions in the COPA test split 4 Comparison to Previous Results In order to better evaluate the success of these two systems, we compared these results with the pub-lished results of other systems that have used the COPA evaluation. Three other systems were con-sidered. PMI Gutenberg (W=5): Described in Roem-mele et al (2011), this approach calculated the PMI between words (unigrams) in the premise and 
396
each alternative, and selected the alternative with the stronger correlation. The PMI statistic was cal-culated using every English-language document in Project Gutenberg (16GB of text), using a window of 5 words. PMI Story 1M (W=25): Described in Gordon et al (2011), this approach was identical to that of Roemmele et al (2011) except that the PMI statis-tic was calculated using a corpus of nearly one mil-lion personal stories extracted from Internet weblogs (Gordon & Swanson, 2009), with 1.9 GB of text. Using this corpus instead of Project Guten-berg, the best results were obtained by using a window of 25 words for the PMI statistic.  PMI Story 10M (W=25): Also described in Gordon et al (2011), this approach explores the gains that can be achieved by calculating the PMI statistic using a much larger corpus of weblog sto-ries. The story extraction technology used by Gor-don and Swanson (2009) was applied to 621 million English-language weblog entries posted to the Internet in 2010 to create a corpus of 10.4 mil-lion personal stories (37GB of text). Again, the best results were obtained by using a window of 25 words for the PMI statistic.  Table 2 compares the results of these three pre-vious systems with the two SemEval-2012 sys-tems. Although the last two of these three previous systems achieved higher scores than both of the SemEval-2012 submissions, the differences are not statistically significant.  System Accuracy PMI Gutenberg (W=5) 58.8% UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4% PMI Story 1M (W=25) 65.2% PMI Story 10M (W=25) 65.4%  Table 2. Comparison of SemEval-2012 Task 7 sys-tems (in bold) with previously published results on the 500 questions in the COPA test split 5 Discussion The two systems from the University of Texas at Dallas make an important contribution to progress on open-domain commonsense reasoning. Some lessons are evident from the short descriptions of their systems that they provided to us. 
As in each of the previously successful systems, this team focused their efforts on calculating corre-lational statistics between words in COPA ques-tions using very large text corpora. In this case, the Gigaword corpus is used, and the calculation is based on bigrams rather than unigrams. We believe that the content of the news articles that comprise the Gigaword corpus is a step further away from the concerns of COPA questions than both the Pro-ject Gutenberg corpus and the weblog story corpo-ra used in previous efforts. Indeed, the gains achieved by Gordon et al (2011) appear to be en-tirely due to the relationship between COPA ques-tions and the personal stories that people write about in their public weblogs. However, the use of a large news corpus affords the use of more sophis-ticated analysis techniques that have been devel-oped for this genre. Here, the Gigaword corpus is annotated using Time-ML relationships, which in turn are used to modify the PMI strength between words. The use of bigrams is an additional enhancement explored by this team, as is the casting of COPA questions as a classification task using a diverse set of lexical and discourse features. Such an approach can facilitate the combining of diverse systems in the future, where correlational statistics are gath-ered from a diverse set of text corpora, each suited for specific domains of COPA questions or yield-ing complimentary feature sets. Still, the modest COPA performance seen from all existing systems is somewhat discouraging. With the best systems performing in the 60-65% range, we remain much closer to random perfor-mance (50%) than human performance (99%). These results cast some doubt that the information necessary to answer COPA questions can be readi-ly obtained from large text corpora. Certainly the use of simple correlational statistics between near-by words is not enough. In the best case, we might wish for perfect identification of causal relation-ships between events in an extremely large text corpus of narratives similar in content to COPA questions. Semantic similarity between these events and COPA sentences could be computed to gather evidence to select the best alternative. Even if it were possible to achieve this ideal, it is diffi-cult to imagine that such an approach could mirror human performance on this task. To move closer to human performance, systems may need to stretch beyond corpus statistics into 
397
the realm of automated reasoning. Just as human readers do when hearing that ?the man lost his bal-ance on the ladder,? successful systems may need to treat COPA premises as novel world states, and imagine a broad range of interconnected causal antecedents and consequents. Useful knowledge bases will be those that have adequate coverage over commonsense concerns, but also adequate competency to support generative inference of the sort more commonly seen in deductive and abduc-tive automated reasoning frameworks. This knowledge may or may not be represented as text, but any successful system must have the capacity to apply this knowledge to the understanding of COPA's textual premises and alternatives. We con-sider the successful application of commonsense inference to text understanding to be one of the grand challenges of natural language processing, and hope that the COPA evaluation continues to be a useful tool for benchmarking progress toward this goal. Acknowledgments The projects or efforts depicted were or are spon-sored by the U. S. Army. The content or infor-mation presented does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. References  Church, K. and Hanks, P. (1990) Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics, 16(1):22-29.  Dagan, I., Glickman, O., and Magnini, B. (2006) The PASCAL Recognising Textual Entailment Chal-lenge. In Qui?onero-Candela, J.; Dagan, I.; Magnini, B.; d'Alch?-Buc, F. (Eds.), Machine Learning Chal-lenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177-190, Springer, 2006. Goodwin, T., Rink, B., Roberts, K., and Harabagiu, S. (2012) UTDHLT: COPACETIC System for Choos-ing Plausible Alternatives. Proceedings of the 6th In-ternational Workshop on Semantic Evaluation (SemEval 2012), June 7-8, 2012, Montreal, Canada. Gordon, A., Bejan, C., and Sagae, K. (2011) Com-monsense Causal Reasoning Using Millions of Per-sonal Stories. Twenty-Fifth Conference on Artificial Intelligence (AAAI-11), August 7?11, 2011, San Francisco, CA.  
Gordon, A. and Swanson, R. (2009) Identifying Person-al Stories in Millions of Weblog Entries. Internation-al Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA.  Gordon, J., Van Durme, B., and K. Schubert, L. (2010) Learning from the Web: Extracting General World Knowledge from Noisy Text. Proceedings of the AAAI 2010 Workshop on Collaboratively-built Knowledge Sources and Artificial Intelligence (WikiAI 2010). Graff, D. and Cieri, C. (2003) English Gigaword. Lin-guistic Data Consortium, Philadelphia. Jung, Y., Ryu, J., Kim., K. and Myaeng, S.(2010). Au-tomatic Construction of a Large-Scale Situation On-tology by Mining How-to Instructions from the Web. Journal of Web Semantics 8(2-3):110-124. Lenat, D. (1995) CYC: A Large-Scale Investment in Knowledge Infrastructure, Communications of the ACM 38:33-38. Liu, H. and Singh, P. (2004) ConceptNet: A Practical Commonsense Reasoning Toolkit. BT Technology Journal 22(4):211-226. Marcu, D. (1999). A decision-based approach to rhetor-ical parsing. The 37th Annual Meeting of the Associ-ation for Computational Linguistics (ACL'99), pages 365-372, Maryland, June 1999. Morgenstern, L. (2012) Common Sense Problem Page. Retrieved April 2012 at http://www-formal.stanford.edu/leora/commonsense/ Noreen, E. (1989) Computer-Intensive Methods for Testing Hypotheses: An Introduction. New York: John Wiley & Sons. Roemmele, M., Bejan, C., and Gordon, A. (2011) Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. AAAI Spring Symposium on Logical Formalizations of Com-monsense Reasoning, Stanford University, March 21-23, 2011. Sagae, K. (2009) Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81--84. 2009.  Speer, R., Havasi, C. and Lieberman, H. (2008) Analo-gySpace: Reducing the Dimensionality of Common Sense Knowledge. Proceedings of AAAI 2008.  
398
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 138?143, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 4: Free Paraphrases of Noun Compounds
Iris Hendrickx
Radboud University Nijmegen &
Universidade de Lisboa
iris@clul.ul.pt
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Stan Szpakowicz
University of Ottawa &
Polish Academy of Sciences
szpak@eecs.uottawa.ca
Zornitsa Kozareva
University of Southern California
kozareva@isi.edu
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
In this paper, we describe SemEval-2013 Task
4: the definition, the data, the evaluation and
the results. The task is to capture some of the
meaning of English noun compounds via para-
phrasing. Given a two-word noun compound,
the participating system is asked to produce
an explicitly ranked list of its free-form para-
phrases. The list is automatically compared
and evaluated against a similarly ranked list
of paraphrases proposed by human annota-
tors, recruited and managed through Ama-
zon?s Mechanical Turk. The comparison of
raw paraphrases is sensitive to syntactic and
morphological variation. The ?gold? ranking
is based on the relative popularity of para-
phrases among annotators. To make the rank-
ing more reliable, highly similar paraphrases
are grouped, so as to downplay superficial dif-
ferences in syntax and morphology. Three
systems participated in the task. They all beat
a simple baseline on one of the two evalua-
tion measures, but not on both measures. This
shows that the task is difficult.
1 Introduction
A noun compound (NC) is a sequence of nouns
which act as a single noun (Downing, 1977), as in
these examples: colon cancer, suppressor protein,
tumor suppressor protein, colon cancer tumor sup-
pressor protein, etc. This type of compounding is
highly productive in English. NCs comprise 3.9%
and 2.6% of all tokens in the Reuters corpus and the
British National Corpus (BNC), respectively (Bald-
win and Tanaka, 2004).
The frequency spectrum of compound types fol-
lows a Zipfian distribution (O? Se?aghdha, 2008), so
many NC tokens belong to a ?long tail? of low-
frequency types. More than half of the two-noun
types in the BNC occur exactly once (Kim and Bald-
win, 2006). Their high frequency and high produc-
tivity make robust NC interpretation an important
goal for broad-coverage semantic processing of En-
glish texts. Systems which ignore NCs may give up
on salient information about the semantic relation-
ships implicit in a text. Compositional interpretation
is also the only way to achieve broad NC coverage,
because it is not feasible to list in a lexicon all com-
pounds which one is likely to encounter. Even for
relatively frequent NCs occurring 10 times or more
in the BNC, static English dictionaries provide only
27% coverage (Tanaka and Baldwin, 2003).
In many natural language processing applications
it is important to understand the syntax and seman-
tics of NCs. NCs often are structurally similar,
but have very different meaning. Consider caffeine
headache and ice-cream headache: a lack of caf-
feine causes the former, an excess of ice-cream ? the
latter. Different interpretations can lead to different
inferences, query expansion, paraphrases, transla-
tions, and so on. A question answering system may
have to determine whether protein acting as a tumor
suppressor is an accurate paraphrase for tumor sup-
pressor protein. An information extraction system
might need to decide whether neck vein thrombosis
and neck thrombosis can co-refer in the same doc-
ument. A machine translation system might para-
phrase the unknown compound WTO Geneva head-
quarters as WTO headquarters located in Geneva.
138
Research on the automatic interpretation of NCs
has focused mainly on common two-word NCs. The
usual task is to classify the semantic relation under-
lying a compound with either one of a small number
of predefined relation labels or a paraphrase from an
open vocabulary. Examples of the former take on
classification include (Moldovan et al, 2004; Girju,
2007; O? Se?aghdha and Copestake, 2008; Tratz and
Hovy, 2010). Examples of the latter include (Nakov,
2008b; Nakov, 2008a; Nakov and Hearst, 2008; But-
nariu and Veale, 2008) and a previous NC paraphras-
ing task at SemEval-2010 (Butnariu et al, 2010),
upon which the task described here builds.
The assumption of a small inventory of prede-
fined relations has some advantages ? parsimony and
generalization ? but at the same time there are lim-
itations on expressivity and coverage. For exam-
ple, the NCs headache pills and fertility pills would
be assigned the same semantic relation (PURPOSE)
in most inventories, but their relational semantics
are quite different (Downing, 1977). Furthermore,
the definitions given by human subjects can involve
rich and specific meanings. For example, Down-
ing (1977) reports that a subject defined the NC
oil bowl as ?the bowl into which the oil in the en-
gine is drained during an oil change?, compared to
which a minimal interpretation bowl for oil seems
very reductive. In view of such arguments, linguists
such as Downing (1977), Ryder (1994) and Coulson
(2001) have argued for a fine-grained, essentially
open-ended space of interpretations.
The idea of working with fine-grained para-
phrases for NC semantics has recently grown in pop-
ularity among NLP researchers (Butnariu and Veale,
2008; Nakov and Hearst, 2008; Nakov, 2008a). Task
9 at SemEval-2010 (Butnariu et al, 2010) was de-
voted to this methodology. In that previous work,
the paraphrases provided by human subjects were
required to fit a restrictive template admitting only
verbs and prepositions occurring between the NC?s
constituent nouns. Annotators recruited through
Amazon Mechanical Turk were asked to provide
paraphrases for the dataset of NCs. The gold stan-
dard for each NC was the ranked list of paraphrases
given by the annotators; this reflects the idea that a
compound?s meaning can be described in different
ways, at different levels of granularity and capturing
different interpretations in the case of ambiguity.
For example, a plastic saw could be a saw made
of plastic or a saw for cutting plastic. Systems par-
ticipating in the task were given the set of attested
paraphrases for each NC, and evaluated according to
how well they could reproduce the humans? ranking.
The design of this task, SemEval-2013 Task 4,
is informed by previous work on compound anno-
tation and interpretation. It is also influenced by
similar initiatives, such as the English Lexical Sub-
stitution task at SemEval-2007 (McCarthy and Nav-
igli, 2007), and by various evaluation exercises in
the fields of paraphrasing and machine translation.
We build on SemEval-2010 Task 9, extending the
task?s flexibility in a number of ways. The restric-
tions on the form of annotators? paraphrases was re-
laxed, giving us a rich dataset of close-to-freeform
paraphrases (Section 3). Rather than ranking a set of
attested paraphrases, systems must now both gener-
ate and rank their paraphrases; the task they perform
is essentially the same as what the annotators were
asked to do. This new setup required us to innovate
in terms of evaluation measures (Section 4).
We anticipate that the dataset and task will be of
broad interest among those who study lexical se-
mantics. We believe that the overall progress in the
field will significantly benefit from a public-domain
set of free-style NC paraphrases. That is why our
primary objective is the challenging endeavour of
preparing and releasing such a dataset to the re-
search community. The common evaluation task
which we establish will also enable researchers to
compare their algorithms and their empirical results.
2 Task description
This is an English NC interpretation task, which ex-
plores the idea of interpreting the semantics of NCs
via free paraphrases. Given a noun-noun compound
such as air filter, the participating systems are asked
to produce an explicitly ranked list of free para-
phrases, as in the following example:
1 filter for air
2 filter of air
3 filter that cleans the air
4 filter which makes air healthier
5 a filter that removes impurities from the air
. . .
139
Such a list is then automatically compared and
evaluated against a similarly ranked list of para-
phrases proposed by human annotators, recruited
and managed via Amazon?s Mechanical Turk. The
comparison of raw paraphrases is sensitive to syn-
tactic and morphological variation. The ranking
of paraphrases is based on their relative popular-
ity among different annotators. To make the rank-
ing more reliable, highly similar paraphrases are
grouped so as to downplay superficial differences in
syntax and morphology.
3 Data collection
We used Amazon?s Mechanical Turk service to
collect diverse paraphrases for a range of ?gold-
standard? NCs.1 We paid the workers a small fee
($0.10) per compound, for which they were asked to
provide five paraphrases. Each paraphrase should
contain the two nouns of the compound (in sin-
gular or plural inflectional forms, but not in an-
other derivational form), an intermediate non-empty
linking phrase and optional preceding or following
terms. The paraphrasing terms could have any part
of speech, so long as the resulting paraphrase was a
well-formed noun phrase headed by the NC?s head.
We gave the workers feedback during data col-
lection if they appeared to have misunderstood the
nature of the task. Once raw paraphrases had been
collected from all workers, we collated them into a
spreadsheet, and we merged identical paraphrases
in order to calculate their overall frequencies. Ill-
formed paraphrases ? those violating the syntactic
restrictions described above ? were manually re-
moved following a consensus decision-making pro-
cedure; every paraphrase was checked by at least
two task organizers. We did not require that the
paraphrases be semantically felicitous, but we per-
formed minor edits on the remaining paraphrases if
they contained obvious typos.
The remaining well-formed paraphrases were
sorted by frequency separately for each NC. The
most frequent paraphrases for a compound are as-
signed the highest rank 0, those with the next-
highest frequency are given a rank of 1, and so on.
1Since the annotation on Mechanical Turk was going slowly,
we also recruited four other annotators to do the same work,
following exactly the same instructions.
Total Min / Max / Avg
Trial/Train (174 NCs)
paraphrases 6,069 1 / 287 / 34.9
unique paraphrases 4,255 1 / 105 / 24.5
Test (181 NCs)
paraphrases 9,706 24 / 99 / 53.6
unique paraphrases 8,216 21 / 80 / 45.4
Table 1: Statistics of the trial and test datasets: the total
number of paraphrases with and without duplicates, and
the minimum / maximum / average per noun compound.
Paraphrases with a frequency of 1 ? proposed for
a given NC by only one annotator ? always occupy
the lowest rank on the list for that compound.
We used 174+181 noun-noun compounds from
the NC dataset of O? Se?aghdha (2007). The trial
dataset, which we initially released to the partici-
pants, consisted of 4,255 human paraphrases for 174
noun-noun pairs; this dataset was also the training
dataset. The test dataset comprised paraphrases for
181 noun-noun pairs. The ?gold standard? contained
9,706 paraphrases of which 8,216 were unique for
those 181 NCs. Further statistics on the datasets are
presented in Table 1.
Compared with the data collected for the
SemEval-2010 Task 9 on the interpretation of noun
compounds, the data collected for this new task have
a far greater range of variety and richness. For ex-
ample, the following (selected) paraphrases for work
area vary from parsimonious to expansive:
? area for work
? area of work
? area where work is done
? area where work is performed
? . . .
? an area cordoned off for persons responsible for
work
? an area where construction work is carried out
? an area where work is accomplished and done
? area where work is conducted
? office area assigned as a work space
? . . .
140
4 Scoring
Noun compounding is a generative aspect of lan-
guage, but so too is the process of NC interpretation:
human speakers typically generate a range of possi-
ble interpretations for a given compound, each em-
phasizing a different aspect of the relationship be-
tween the nouns. Our evaluation framework reflects
the belief that there is rarely a single right answer
for a given noun-noun pairing. Participating systems
are thus expected to demonstrate some generativity
of their own, and are scored not just on the accu-
racy of individual interpretations, but on the overall
breadth of their output.
For evaluation, we provided a scorer imple-
mented, for good portability, as a Java class. For
each noun compound to be evaluated, the scorer
compares a list of system-suggested paraphrases
against a ?gold-standard? reference list, compiled
and rank-ordered from the paraphrases suggested
by our human annotators. The score assigned to
each system is the mean of the system?s performance
across all test compounds. Note that the scorer re-
moves all determiners from both the reference and
the test paraphrases, so a system is neither punished
for not reproducing a determiner or rewarded for
producing the same determiners.
The scorer can match words identically or non-
identically. A match of two identical words Wgold
and Wtest earns a score of 1.0. There is a partial
score of (2 |P | / (|PWgold| + |PWtest|))2 for a
match of two words PWgold and PWtest that are
not identical but share a common prefix P , |P | > 2,
e.g., wmatch(cutting, cuts) = (6/11)2 = 0.297.
Two n-grams Ngold = [GW1, . . . , GWn] and
Ntest = [TW1, . . . , TWn] can be matched if
wmatch(GWi, TWi) > 0 for all i in 1..n. The
score assigned to the match of these two n-grams is
then
?
i wmatch(GWi, TWi). For every n-gram
Ntest = [TW1, . . . , TWn] in a system-generated
paraphrase, the scorer finds a matching n-gram
Ngold = [GW1, . . . , GWn] in the reference para-
phrase Paragold which maximizes this sum.
The overall n-gram overlap score for a reference
paraphrase Paragold and a system-generated para-
phrase Paratest is the sum of the score calculated
for all n-grams in Paratest, where n ranges from 1
to the size of Paratest.
This overall score is then normalized by dividing
by the maximum value among the n-gram overlap
score for Paragold compared with itself and the n-
gram overlap score for Paratest compared with it-
self. This normalization step produces a paraphrase
match score in the range [0.0 ? 1.0]. It punishes a
paraphrase Paratest for both over-generating (con-
taining more words than are found in Paragold)
and under-generating (containing fewer words than
are found in Paragold). In other words, Paratest
should ideally reproduce everything in Paragold,
and nothing more or less.
The reference paraphrases in the ?gold standard?
are ordered by rank; the highest rank is assigned to
the paraphrases which human judges suggested most
often. The rank of a reference paraphrase matters
because a good participating system will aim to re-
produce the top-ranked ?gold-standard? paraphrases
as produced by human judges. The scorer assigns
a multiplier of R/(R + n) to reference paraphrases
at rank n; this multiplier asymptotically approaches
0 for the higher values of n of ever lower-ranked
paraphrases. We choose a default setting of R = 8,
so that a reference paraphrase at rank 0 (the highest
rank) has a multiplier of 1, while a reference para-
phrase at rank 5 has a multiplier of 8/13 = 0.615.
When a system-generated paraphrase Paratest is
matched with a reference paraphrase Paragold, their
normalized n-gram overlap score is scaled by the
rank multiplier attaching to the rank of Paragold rel-
ative to the other reference paraphrases provided by
human judges. The scorer automatically chooses the
reference paraphrase Paragold for a test paraphrase
Paratest so as to maximize this product of normal-
ized n-gram overlap score and rank multiplier.
The overall score assigned to each system for
a specific compound is calculated in two differ-
ent ways: using isomorphic matching of suggested
paraphrases to the ?gold-standard?s? reference para-
phrases (on a one-to-one basis); and using non-
isomorphic matching of system?s paraphrases to the
?gold-standard?s? reference paraphrases (in a poten-
tially many-to-one mapping).
Isomorphic matching rewards both precision and
recall. It rewards a system for accurately reproduc-
ing the paraphrases suggested by human judges, and
for reproducing as many of these as it can, and in
much the same order.
141
In isomorphic mode, system?s paraphrases are
matched 1-to-1 with reference paraphrases on a first-
come first-matched basis, so ordering can be crucial.
Non-isomorphic matching rewards only preci-
sion. It rewards a system for accurately reproducing
the top-ranked human paraphrases in the ?gold stan-
dard?. A system will achieve a higher score in a non-
isomorphic match if it reproduces the top-ranked hu-
man paraphrases as opposed to lower-ranked human
paraphrases. The ordering of system?s paraphrases
is thus not important in non-isomorphic matching.
Each system is evaluated using the scorer in both
modes, isomorphic and non-isomorphic. Systems
which aim only for precision should score highly
on non-isomorphic match mode, but poorly in iso-
morphic match mode. Systems which aim for pre-
cision and recall will face a more substantial chal-
lenge, likely reflected in their scores.
A na??ve baseline
We decided to allow preposition-only paraphrases,
which are abundant in the paraphrases suggested
by human judges in the crowdsourcing Mechanical
Turk collection process. This abundance means that
the top-ranked paraphrase for a given compound is
often a preposition-only phrase, or one of a small
number of very popular paraphrases such as used for
or used in. It is thus straightforward to build a na??ve
baseline generator which we can expect to score
reasonably on this task, at least in non-isomorphic
matching mode. For each test compound M H,
the baseline system generates the following para-
phrases, in this precise order: H of M, H in M, H
for M, H with M, H on M, H about M, H has M, H to
M, H used for M, H used in M.
This na??ve baseline is truly unsophisticated. No
attempt is made to order paraphrases by their corpus
frequencies or by their frequencies in the training
data. The same sequence of paraphrases is generated
for each and every test compound.
5 Results
Three teams participated in the challenge, and all
their systems were supervised. The MELODI sys-
tem relied on semantic vector space model built
from the UKWAC corpus (window-based, 5 words).
It used only the features of the right-hand head noun
to train a maximum entropy classifier.
Team isomorphic non-isomorphic
SFS 23.1 17.9
IIITH 23.1 25.8
MELODI-Primary 13.0 54.8
MELODI-Contrast 13.6 53.6
Naive Baseline 13.8 40.6
Table 2: Results for the participating systems; the base-
line outputs the same paraphrases for all compounds.
The IIITH system used the probabilities of the
preposition co-occurring with a relation to identify
the class of the noun compound. To collect statis-
tics, it used Google n-grams, BNC and ANC.
The SFS system extracted templates and fillers
from the training data, which it then combined with
a four-gram language model and a MaxEnt reranker.
To find similar compounds, they used Lin?s Word-
Net similarity. They further used statistics from the
English Gigaword and the Google n-grams.
Table 2 shows the performance of the partici-
pating systems, SFS, IIITH and MELODI, and the
na??ve baseline. The baseline shows that it is rela-
tively easy to achieve a moderately good score in
non-isomorphic match mode by generating a fixed
set of paraphrases which are both common and
generic: two of the three participating systems,
SFS and IIITH, under-perform the na??ve baseline
in non-isomorphic match mode, but outperform it
in isomorphic mode. The only system to surpass
this baseline in non-isomorphic match mode is the
MELODI system; yet, it under-performs against the
same baseline in isomorphic match mode. No par-
ticipating team submitted a system which would out-
perform the na??ve baseline in both modes.
6 Conclusions
The conclusions we draw from the experience of or-
ganizing the task are mixed. Participation was rea-
sonable but not large, suggesting that NC paraphras-
ing remains a niche interest ? though we believe it
deserves more attention among the broader lexical
semantics community and hope that the availabil-
ity of our freeform paraphrase dataset will attract a
wider audience in the future.
142
We also observed a varied response from our an-
notators in terms of embracing their freedom to gen-
erate complex and rich paraphrases; there are many
possible reasons for this including laziness, time
pressure and the fact that short paraphrases are often
very appropriate paraphrases. The results obtained
by our participants were also modest, demonstrating
that compound paraphrasing is both a difficult task
and a novel one that has not yet been ?solved?.
Acknowledgments
This work has partially supported by a small but ef-
fective grant from Amazon; the credit allowed us
to hire sufficiently many Turkers ? thanks! And a
thank-you to our additional annotators Dave Carter,
Chris Fournier and Colette Joubarne for their com-
plete sets of paraphrases of the noun compounds in
the test data.
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of complex nominals: Getting it right.
Proc. ACL04 Workshop on Multiword Expressions: In-
tegrating Processing, Barcelona, Spain, 24-31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK, 81-
88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 Task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. Proc. 5th International ACL Workshop on Se-
mantic Evaluation, Uppsala, Sweden, 39-44.
Seana Coulson. 2001. Semantic Leaps: Frame-Shifting
and Conceptual Blending in Meaning Construction.
Cambridge University Press, Cambridge, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4): 810-842.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. Proc.
45th Annual Meeting of the Association of Computa-
tional Linguistics, Prague, Czech Republic, 568-575.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
semantic relations in noun compounds via verb seman-
tics. Proc. ACL-06 Main Conference Poster Session,
Sydney, Australia, 491-498.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. Proc.
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), Prague, Czech Republic, 48-53.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Dan Moldovan
and Roxana Girju, eds., HLT-NAACL 2004: Workshop
on Computational Lexical Semantics, Boston, MA,
USA, 60-67.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the Web as a corpus.
Proc. 46th Annual Meeting of the Association for Com-
putational Linguistics ACL-08, Columbus, OH, USA,
452-460.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. Proc. 18th
European Conference on Artificial Intelligence ECAI-
08, Patras, Greece, 338-342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. Proc.
13th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications AIMSA-
08, Varna, Bulgaria, Lecture Notes in Computer Sci-
ence 5253, Springer, 103-117.
Diarmuid O? Se?aghdha. 2007. Designing and Evaluating
a Semantic Annotation Scheme for Compound Nouns.
In Proceedings of the 4th Corpus Linguistics Confer-
ence, Birmingham, UK.
Diarmuid O? Se?aghdha. 2008. Learning compound
noun semantics. Ph.D. thesis, Computer Laboratory,
University of Cambridge. Published as University
of Cambridge Computer Laboratory Technical Report
735.
Diarmuid O? Se?aghdha and Ann Copestake. 2009. Using
lexical and relational similarity to classify semantic re-
lations. Proc. 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
EACL-09, Athens, Greece, 621-629.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. 22nd International Conference on Computa-
tional Linguistics (COLING-08), Manchester, UK.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA, USA.
Takaaki Tanaka and Tim Baldwin. 2003. Noun-noun
compound machine translation: A feasibility study
on shallow processing. Proc. ACL-2003 Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment, Sapporo, Japan, 17-24.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. Proc. 48th Annual Meeting of the As-
sociation for Computational Linguistics ACL-10, Up-
psala, Sweden, 678-687.
143
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 2: Sentiment Analysis in Twitter
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Zornitsa Kozareva
USC Information Sciences Institute
kozareva@isi.edu
Alan Ritter
University of Washington
aritter@cs.washington.edu
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Veselin Stoyanov
JHU HLTCOE
ves@cs.jhu.edu
Theresa Wilson
JHU HLTCOE
taw@jhu.edu
Abstract
In recent years, sentiment analysis in social
media has attracted a lot of research interest
and has been used for a number of applica-
tions. Unfortunately, research has been hin-
dered by the lack of suitable datasets, com-
plicating the comparison between approaches.
To address this issue, we have proposed
SemEval-2013 Task 2: Sentiment Analysis in
Twitter, which included two subtasks: A, an
expression-level subtask, and B, a message-
level subtask. We used crowdsourcing on
Amazon Mechanical Turk to label a large
Twitter training dataset alng with additional
test sets of Twitter and SMS messages for both
subtasks. All datasets used in the evaluation
are released to the research community. The
task attracted significant interest and a total
of 149 submissions from 44 teams. The best-
performing team achieved an F1 of 88.9% and
69% for subtasks A and B, respectively.
1 Introduction
In the past decade, new forms of communication,
such as microblogging and text messaging have
emerged and become ubiquitous. Twitter messages
(tweets) and cell phone messages (SMS) are often
used to share opinions and sentiments about the sur-
rounding world, and the availability of social con-
tent generated on sites such as Twitter creates new
opportunities to automatically study public opinion.
Working with these informal text genres presents
new challenges for natural language processing be-
yond those encountered when working with more
traditional text genres such as newswire.
Tweets and SMS messages are short in length: a
sentence or a headline rather than a document. The
language they use is very informal, with creative
spelling and punctuation, misspellings, slang, new
words, URLs, and genre-specific terminology and
abbreviations, e.g., RT for re-tweet and #hashtags.1
How to handle such challenges so as to automati-
cally mine and understand the opinions and senti-
ments that people are communicating has only very
recently been the subject of research (Jansen et al,
2009; Barbosa and Feng, 2010; Bifet et al, 2011;
Davidov et al, 2010; O?Connor et al, 2010; Pak and
Paroubek, 2010; Tumasjan et al, 2010; Kouloumpis
et al, 2011).
Another aspect of social media data, such as Twit-
ter messages, is that they include rich structured in-
formation about the individuals involved in the com-
munication. For example, Twitter maintains infor-
mation about who follows whom. Re-tweets (re-
shares of a tweet) and tags inside of tweets provide
discourse information. Modeling such structured in-
formation is important because it provides means for
empirically studying social interactions where opin-
ion is conveyed, e.g., we can study the properties of
persuasive language or those associated with influ-
ential users.
Several corpora with detailed opinion and senti-
ment annotation have been made freely available,
e.g., the MPQA corpus (Wiebe et al, 2005) of
newswire text. These corpora have proved very
valuable as resources for learning about the lan-
guage of sentiment in general, but they did not focus
on social media.
1Hashtags are a type of tagging for Twitter messages.
312
Twitter RT @tash jade: That?s really sad, Charlie RT ?Until tonight I never realised how fucked up I was? -
Charlie Sheen #sheenroast
SMS Glad to hear you are coping fine in uni... So, wat interview did you go to? How did it go?
Table 1: Examples of sentences from each corpus that contain subjective phrases.
While some Twitter sentiment datasets have al-
ready been created, they were either small and pro-
prietary, such as the i-sieve corpus (Kouloumpis
et al, 2011), or they were created only for Span-
ish like the TASS corpus2 (Villena-Roma?n et al,
2013), or they relied on noisy labels obtained from
emoticons and hashtags. They further focused on
message-level sentiment, and no Twitter or SMS
corpus with expression-level sentiment annotations
has been made available so far.
Thus, the primary goal of our SemEval-2013 task
2 has been to promote research that will lead to a
better understanding of how sentiment is conveyed
in Tweets and SMS messages. Toward that goal,
we created the SemEval Tweet corpus, which con-
tains Tweets (for both training and testing) and SMS
messages (for testing only) with sentiment expres-
sions annotated with contextual phrase-level polar-
ity as well as an overall message-level polarity. We
used this corpus as a testbed for the system evalua-
tion at SemEval-2013 Task 2.
In the remainder of this paper, we first describe
the task, the dataset creation process, and the evalu-
ation methodology. We then summarize the charac-
teristics of the approaches taken by the participating
systems and we discuss their scores.
2 Task Description
We had two subtasks: an expression-level subtask
and a message-level subtask. Participants could
choose to participate in either or both subtasks. Be-
low we provide short descriptions of the objectives
of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked instance
of a word or a phrase, determine whether that
instance is positive, negative or neutral in that
context. The boundaries for the marked in-
stance were provided: this was a classification
task, not an entity recognition task.
2http://www.daedalus.es/TASS/corpus.php
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment. For
messages conveying both a positive and a
negative sentiment, whichever is the stronger
one was to be chosen.
Each participating team was allowed to submit re-
sults for two different systems per subtask: one con-
strained, and one unconstrained. A constrained sys-
tem could only use the provided data for training,
but it could also use other resources such as lexi-
cons obtained elsewhere. An unconstrained system
could use any additional data as part of the training
process; this could be done in a supervised, semi-
supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to the
data used to train a classifier. For example, if other
data (excluding the test data) was used to develop
a sentiment lexicon, and the lexicon was used to
generate features, the system would still be con-
strained. However, if other data (excluding the test
data) was used to develop a sentiment lexicon, and
this lexicon was used to automatically label addi-
tional Tweet/SMS messages and then used with the
original data to train the classifier, then such a sys-
tem would be unconstrained.
3 Dataset Creation
In the following sections we describe the collection
and annotation of the Twitter and SMS datasets.
3.1 Data Collection
Twitter is the most common micro-blogging site on
the Web, and we used it to gather tweets that express
sentiment about popular topics. We first extracted
named entities using a Twitter-tuned NER system
(Ritter et al, 2011) from millions of tweets, which
we collected over a one-year period spanning from
January 2012 to January 2013; we used the public
streaming Twitter API to download tweets.
313
Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective,
positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark
the position of its start and end in the text boxes below. The number above each word indicates its position. The
word/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range.
Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral. If a
sentence is not subjective please select the checkbox indicating that ?There are no subjective words/phrases?. Please
read the examples and invalid responses before beginning if this is your first time answering this hit.
Figure 1: Instructions provided to workers on Mechanical Turk followed by a screenshot.
Average # of Total Phrase Count Vocabulary
Corpus Words Characters Positive Negative Neutral Size
Twitter - Training 25.4 120.0 5,895 3,131 471 20,012
Twitter - Dev 25.5 120.0 648 430 57 4,426
Twitter - Test 25.4 121.2 2,734 1,541 160 11,736
SMS - Test 24.5 95.6 1,071 1,104 159 3,562
Table 2: Statistics for Subtask A.
We then identified popular topics as those named
entities that are frequently mentioned in association
with a specific date (Ritter et al, 2012). Given this
set of automatically identified topics, we gathered
tweets from the same time period which mentioned
the named entities. The testing messages had differ-
ent topics from training and spanned later periods.
To identify messages that express sentiment to-
wards these topics, we filtered the tweets us-
ing SentiWordNet (Baccianella et al, 2010). We
removed messages that contained no sentiment-
bearing words, keeping only those with at least one
word with positive or negative sentiment score that
is greater than 0.3 in SentiWordNet for at least one
sense of the words. Without filtering, we found class
imbalance to be too high.3
Twitter messages are rich in social media features,
including out-of-vocabulary (OOV) words, emoti-
cons, and acronyms; see Table 1. A large portion of
the OOV words are hashtags (e.g., #sheenroast)
and mentions (e.g., @tash jade).
3Filtering based on an existing lexicon does bias the dataset
to some degree; however, note that the text still contains senti-
ment expressions outside those in the lexicon.
Corpus Positive Negative Objective
/ Neutral
Twitter - Training 3,662 1,466 4,600
Twitter - Dev 575 340 739
Twitter - Test 1,573 601 1,640
SMS - Test 492 394 1,208
Table 3: Statistics for Subtask B.
We annotated the same Twitter messages with an-
notations for subtask A and subtask B. However,
the final training and testing datasets overlap only
partially between the two subtasks since we had
to throw away messages with low inter-annotator
agreement, and this differed between the subtasks.
For testing, we also annotated SMS messages, taken
from the NUS SMS corpus4 (Chen and Kan, 2012).
Tables 2 and 3 show statistics about the corpora we
created for subtasks A and B.
4http://wing.comp.nus.edu.sg/SMSCorpus/
314
A B
Lower Avg. Upper Avg.
Twitter - Train 64.7 82.4 90.8 82.7
Twitter - Dev 51.2 74.7 87.8 78.4
Twitter - Test 68.8 83.6 90.9 76.9
SMS - Test 66.5 88.5 81.2 77.6
Table 4: Bounds for datasets in subtasks A and B.
3.2 Annotation Guidelines
The instructions provided to the annotators, along
with an example, are shown in Figure 1. We pro-
vided several additional examples to the annotators,
shown in Table 5.
In addition, we filtered spammers by considering
the following kinds of annotations invalid:
? containing overlapping subjective phrases;
? subjective but without a subjective phrase;
? marking every single word as subjective;
? not having the overall sentiment marked.
3.3 Annotation Process
Our datasets were annotated for sentiment on Me-
chanical Turk. Each sentence was annotated by five
Mechanical Turk workers (Turkers). In order to
qualify for the hits, the Turker had to have an ap-
proval rate greater than 95% and have completed 50
approved hits. Each Turker was paid three cents
per hit. The Turker had to mark all the subjec-
tive words/phrases in the sentence by indicating their
start and end positions and say whether each subjec-
tive word/phrase was positive, negative, or neutral
(subtask A). They also had to indicate the overall
polarity of the sentence (subtask B).
Figure 1 shows the instructions and an exam-
ple provided to the Turkers. The first five rows
of Table 6 show an example of the subjective
words/phrases marked by each of the workers.
For subtask A, we combined the annotations of
each of the workers using intersection as indicated
in the last row of Table 6. A word had to appear
in 2/3 of the annotations in order to be considered
subjective. Similarly, a word had to be labeled with
a particular polarity (positive, negative, or neutral)
2/3 of the time in order to receive that label.
We also experimented with combining annota-
tions by computing the union of the sentences, and
taking the sentence of the worker who annotated the
most hits, but we found that these methods were
not as accurate. Table 4 shows the lower, average,
and upper bounds for all the hits by computing the
bounds for each hit and averaging them together.
This gives a good indication about how well we can
expect the systems to perform. For example, even if
we used the best annotator each time, it would still
not be possible to get perfect accuracy.
For subtask B, the polarity of the entire sentence
was determined based on the majority of the labels.
If there was a tie, the sentence was discarded. In
order to reduce the number of sentences lost, we
combined the objective and the neutral labels, which
Turkers tended to mix up. Table 4 shows the aver-
age bound for subtask B by computing the bounds
for each hit and averaging them together. Since the
polarity is chosen based on the majority, the upper
bound is 100%.
4 Scoring
For both subtasks, the participating systems were
required to perform a three-way classification ? a
particular marked phrase (for subtask A) or an en-
tire message (for subtask B) was to be classified as
positive, negative, or objective. For each system,
we computed a score for predicting positive/negative
phrases/messages vs. the other two classes.
For instance, to compute positive precision, Ppos,
we find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we divide
that number by the total number of messages it pre-
dicted to be positive. To compute recall, for the pos-
itive class, Rpos, we find the number of messages
correctly predicted to be positive and we divide that
number by the total number of positive messages in
the gold standard.
We then calculate F-score for the positive labels,
the harmonic average of precision and recall as fol-
lows Fpos = 2
PposRpos
Ppos+Rpos
. We carry out a similar
computation to calculate Fneg, which is F1 for neg-
ative messages.
The overall score for each system run is then
given by the average of the F1-scores for the posi-
tive and negative classes: F = (Fpos + Fneg)/2.
315
Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of
the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the
frontiers.
Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas
markets and lower costs for material imports, he said.
?March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,? according
to Chen, also Taiwan?s chief WTO negotiator.
friday evening plans were great, but saturday?s plans didnt go as expected ? i went dancing & it was an ok club,
but terribly crowded :-(
WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE
AT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, while
T-Mobile makes that a normal everyday thin
obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies
what we had. #Coward #Traitor
My graduation speech: ?I?d like to thanks Google, Wikipedia and my computer! :D #iThingteens
Table 5: List of example sentences with annotations that were provided to the annotators. All subjective phrases are
italicized. Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.
Worker 1 I would love to watch Vampire Diaries :) and some Heroes! Great combination 9/13
Worker 2 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Worker 3 I would love to watch Vampire Diaries :) and some Heroes! Great combination 10/13
Worker 4 I would love to watch Vampire Diaries :) and some Heroes! Great combination 13/13
Worker 5 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Intersection I would love to watch Vampire Diaries :) and some Heroes! Great combination
Table 6: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked as
subjective are italicized and highlighted in bold. The first five rows are annotations provided by Turkers, and the final
row shows their intersection. The final column shows the accuracy for each annotation compared to the intersection.
Note that ignoring Fneutral does not reduce the
task to predicting positive vs. negative labels only
(even though some participants have chosen to do
so) since the gold standard still contains neutral
labels which are to be predicted: Fpos and Fneg
would suffer if these examples are labeled as posi-
tive and/or negative instead of neutral.
We provided participants with a scorer. In addi-
tion to outputting the overall F-score, it produced
a confusion matrix for the three prediction classes
(positive, negative, and objective), and it also vali-
dated the data submission format.
5 Participants and Results
The results for subtask A are shown in Tables 7 and
8 for Twitter and for SMS messages, respectively;
those for subtask B are shown in Table 9 for Twit-
ter and in Table 10 for SMS messages. Systems are
ranked by their scores for the constrained runs; the
ranking based on scores for unconstrained runs is
shown as a subindex.
For both subtasks, there were teams that only sub-
mitted results for the Twitter test set. Some teams
submitted both a constrained and an unconstrained
version (e.g., AVAYA and teragram). As one would
expect, the results on the Twitter test set tended to be
better than those on the SMS test set since the SMS
data was out-of-domain with respect to the training
(Twitter) data.
Moreover, the results for subtask A were signifi-
cantly better than those for subtask B, which shows
that it is a much easier task, probably because there
is less ambiguity at the phrase-level.
5.1 Subtask A: Contextual Polarity
Table 7 shows that subtask A, Twitter, attracted 23
teams, who submitted 21 constrained and 7 uncon-
strained systems. Five teams submitted both a con-
strained and an unconstrained system, and two other
teams submitted constrained systems that are on
the boundary between being constrained and uncon-
strained.
316
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 88.93 yes yes
AVAYA 86.98 87.38(1) yes yes
BOUNCE 86.79 yes yes
LVIC-LIMSI 85.70 yes yes
FBM 85.50 yes semi
GU-MLT-LT 85.19 yes yes
UNITOR 84.60 yes yes
USNA 81.31 yes yes
Serendio 80.04 yes yes
ECNUCS 79.48 80.15(2) yes yes
TJP 78.16 yes yes
?columbia-nlp 74.94 yes yes
teragram 74.89(3) yes yes
sielers 74.41 yes yes
KLUE 73.74 yes yes
OPTWIMA 69.17 36.91(6) yes yes
swatcs 67.19 63.86(5) no yes
Kea 63.94 yes yes
senti.ue-en 62.79 71.38(4) yes yes
uottawa 60.20 yes yes
IITB 54.80 yes yes
SenselyticTeam 53.88 yes yes
SU-sentilab 34.73(7) no yes
Majority Baseline 38.10 N/A N/A
Table 7: Results for subtask A on the Twitter dataset. The
? marks a team that includes a task coorganizer, and the
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
One system was semi-supervised, and the rest
were supervised. The supervised systems used clas-
sifiers such as SVM (8 systems), Naive Bayes (7 sys-
tems), and Maximum Entropy (3 systems). Other
approaches used include an ensemble of classifiers,
manual rules, and a linear classifier. Two of the sys-
tems chose not to predict neutral as a possible clas-
sification label.
The average F1-measure on the Twitter test set
was 74.1% for constrained systems and 60.5% for
unconstrained ones; this does not mean that using
additional data does not help, it just shows that the
best teams only participated with a constrained sys-
tem. NRC-Canada had the best constrained system
with an F1-measure of 88.9%, and AVAYA had the
best unconstrained one with F1=87.4%.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
GU-MLT-LT 88.37 yes yes
NRC-Canada 88.00 yes yes
?AVAYA 83.94 85.79(1) yes yes
UNITOR 82.49 yes yes
TJP 81.23 yes yes
LVIC-LIMSI 80.16 yes yes
USNA 79.82 yes yes
ECNUCS 76.69 77.34(2) yes yes
sielers 73.48 yes yes
FBM 72.95 no semi
teragram 72.83 72.83(4) yes yes
KLUE 70.54 yes yes
?columbia-nlp 70.30 yes yes
senti.ue-en 66.09 74.13(3) yes yes
swatcs 66.00 67.68(5) no yes
Kea 63.27 yes yes
uottawa 55.89 yes yes
SU-sentilab 55.38(6) no yes
SenselyticTeam 51.13 yes yes
OPTWIMA 37.32 36.38(7) yes yes
Majority Baseline 31.50 N/A N/A
Table 8: Results for subtask A on the SMS dataset. The
? indicates a late submission, the ? marks a team that
includes a task co-organizer, and the  indicates a sys-
tem submitted as constrained but which used additional
Tweets or additional sentiment-annotated text to collect
statistics that were then used as a feature.
Table 8 shows the results for the SMS test set,
where 20 teams submitted 19 constrained and 7 un-
constrained systems (again, this included two teams
that submitted boundary systems, marked accord-
ingly). The average F-measure on this test set
was 70.8% for constrained systems and 65.7% for
unconstrained systems. The best constrained sys-
tem was that of GU-MLT-LT with an F-measure of
88.4%, and AVAYA had the best unconstrained sys-
tem with an F1 of 85.8%.
5.2 Subtask B: Message Polarity
Table 9 shows that subtask B, Twitter, attracted 38
teams, who submitted 36 constrained and 15 uncon-
strained systems (and two boundary ones).
The average F1-measure was 53.7% for the con-
strained and 54.6% for the unconstrained systems.
317
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 69.02 yes yes
GU-MLT-LT 65.27 yes yes
teragram 64.86 64.86(1) yes yes
BOUNCE 63.53 yes yes
KLUE 63.06 yes yes
AMI&ERIC 62.55 61.17(3) yes yes/semi
FBM 61.17 yes yes
AVAYA 60.84 64.06(2) yes yes/semi
SAIL 60.14 61.03(4) yes yes
UT-DB 59.87 yes yes
FBK-irst 59.76 yes yes
nlp.cs.aueb.gr 58.91 yes yes
UNITOR 58.27 59.50(5) yes semi
LVIC-LIMSI 57.14 yes yes
Umigon 56.96 yes yes
NILC USP 56.31 yes yes
DataMining 55.52 yes semi
ECNUCS 55.05 58.42(6) yes yes
nlp.cs.aueb.gr 54.73 yes yes
ASVUniOfLeipzig 54.56 yes yes
SZTE-NLP 54.33 53.10(9) yes yes
CodeX 53.89 yes yes
Oasis 53.84 yes yes
NTNU 53.23 50.71(10) yes yes
UoM 51.81 45.07(15) yes yes
SSA-UO 50.17 yes no
SenselyticTeam 50.10 yes yes
UMCC DLSI (SA) 49.27 48.99(12) yes yes
bwbaugh 48.83 54.37(8) yes yes/semi
senti.ue-en 47.24 47.85(13) yes yes
SU-sentilab 45.75(14) yes yes
OPTWIMA 45.40 54.51(7) yes yes
REACTION 45.01 yes yes
uottawa 42.51 yes yes
IITB 39.80 yes yes
IIRG 34.44 yes yes
sinai 16.28 49.26(11) yes yes
Majority Baseline 29.19 N/A N/A
Table 9: Results for subtask B on the Twitter dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
These averages are much lower than those for sub-
task A, which indicates that subtask B is harder,
probably because a message can contain parts ex-
pressing both positive and negative sentiment.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 68.46 yes yes
GU-MLT-LT 62.15 yes yes
KLUE 62.03 yes yes
AVAYA 60.00 59.47(1) yes yes/semi
teragram 59.10(2) yes yes
NTNU 57.97 54.55(6) yes yes
CodeX 56.70 yes yes
FBK-irst 54.87 yes yes
AMI&ERIC 53.63 52.62(7) yes yes/semi
ECNUCS 53.21 54.77(5) yes yes
UT-DB 52.46 yes yes
SAIL 51.84 51.98(8) yes yes
UNITOR 51.22 48.88(10) yes semi
SZTE-NLP 51.08 55.46(3) yes yes
SenselyticTeam 51.07 yes yes
NILC USP 50.12 yes yes
REACTION 50.11 yes yes
SU-sentilab 49.57(9) no yes
nlp.cs.aueb.gr 49.41 55.28(4) yes yes
LVIC-LIMSI 49.17 yes yes
FBM 47.40 yes yes
ASVUniOfLeipzig 46.50 yes yes
senti.ue-en 44.65 46.72(12) yes yes
SSA UO 44.39 yes no
UMCC DLSI (SA) 43.39 40.67(14) yes yes
UoM 42.22 35.22(15) yes yes
OPTWIMA 40.98 47.15(11) yes yes
uottawa 40.51 yes yes
bwbaugh 39.73 43.43(13) yes yes/semi
IIRG 22.16 yes yes
Majority Baseline 19.03 N/A N/A
Table 10: Results for subtask B on the SMS dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
Once again, NRC-Canada had the best con-
strained system with an F1-measure of 69%, fol-
lowed by teragram, which had the best uncon-
strained system with an F1-measure of 64.9%.
As Table 10 shows, the average F1-measure on
the SMS test set was 50.2% for constrained and
50.3% for unconstrained systems. NRC-Canada had
the best constrained system with an F1=68.5%, and
AVAYA had the best unconstrained one with F1-
measure of 59.5%.
318
5.3 Overall
Overall, the results achieved by the best teams were
very strong, especially for the simpler subtask A:
? F1=88.93, NRC-Canada on subtask A, Twitter;
? F1=88.37, GU-MLT-LT on subtask A, SMS;
? F1=69.02, NRC-Canada on subtask B, Twitter;
? F1=68.46, NRC-Canada on subtask B, SMS.
We can see that the strongest team overall was that
of NRC-Canada, which was ranked first on three of
the four conditions; and it was second on subtask A,
SMS. There were two other teams that were strong
across both tasks and on both test sets: GU-MLT-LT
and AVAYA. Three other teams, namely teragram,
BOUNCE and KLUE, were ranked in the top-3 in at
least one subtask and test set.
6 Discussion
We have seen that most participants restricted them-
selves to the provided data and submitted con-
strained systems. Indeed, the best systems for each
of the two subtasks and for each of the two testing
datasets were constrained systems; of course, this
does not mean that additional data would not be use-
ful. Curiously, in some cases where a team submit-
ted a constrained and unconstrained run, the uncon-
strained run actually performed worse.
Not surprisingly, most systems were supervised;
there were only five semi-supervised systems, and
there was only one unsupervised system. One ad-
ditional team declared their system as unsupervised
since it was not making use of the training data; we
still classified it as supervised though since it did use
supervision ? in the form of manual rules.
Most participants predicted all three labels (posi-
tive, negative and neutral), even though some partic-
ipants opted for not predicting neutral, which made
some sense since the final F1-score was averaged
over the positive and the negative predictions only.
The most popular classifiers included SVM, Max-
Ent, linear classifier, Naive Bayes; in some cases,
manual rules or ensembles of classifiers were used.
A variety of features were used, including word-
related (e.g., words, stems, n-grams, word clus-
ters), word-shape (e.g., punctuation, capitalization),
syntactic (e.g., POS tags, dependency relations),
Twitter-specific (e.g., repeated characters, emoti-
cons, URLs, hashtags, slang, abbreviations), and
sentiment-related (e.g., negation); one team also
used discourse relations. Almost all participants re-
lied heavily of various sentiment lexicons, the most
popular ones being MPQA and SentiWordNet, as
well as AFINN and Bing Liu?s Opinion Lexicon;
some participants used their own lexicons ? preex-
isting or built from the provided data.
Given that Twitter messages are noisy, most par-
ticipants did some preprocessing, including tok-
enization, stemming, lemmatization, stopword re-
moval, normalization/removal of URLs, hashtags,
users, slang, emoticons, repeated vowels, punctua-
tion; some even did pronoun resolution.
7 Conclusion
We have described a new task that entered SemEval-
2013: task 2 on Sentiment Analysis on Twitter. The
task has attracted a very high number of participants:
149 submissions from 44 teams.
We believe that the datasets that we have created
as part of the task and which we have released to the
community5 under a Creative Commons Attribution
3.0 Unported License,6 will be found useful by re-
searchers beyond SemEval.
Acknowledgments
The authors would like to thank Kathleen McKeown
for her insight in creating the Amazon Mechanical
Turk annotation task.
Funding for the Amazon Mechanical Turk anno-
tations was provided by the JHU Human Language
Technology Center of Excellence and by the Of-
fice of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
5http://www.cs.york.ac.uk/semeval-2013/task2/
6http://creativecommons.org/licenses/by/3.0/
319
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Nicoletta Calzolari (chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, LREC ?10,
pages 2200?2204, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and
Ricard Gavalda`. 2011. Detecting sentiment change in
Twitter streaming data. Journal of Machine Learning
Research - Proceedings Track, 17:5?11.
Tao Chen and Min-Yen Kan. 2012. Creating a live, pub-
lic short message service corpus: the NUS SMS cor-
pus. Language Resources and Evaluation, pages 1?
37.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116, Upp-
sala, Sweden.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The Good
the Bad and the OMG! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media, ICWSM? 11, pages 538?541, Barcelona,
Spain.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter for disambiguating senti-
ment ambiguous adjectives. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 436?439, Los Angeles, CA, USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1524?1534, Edinburgh, United
Kingdom.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?12, pages 1104?1112, Beijing, China.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about po-
litical sentiment. In William W. Cohen and Samuel
Gosling, editors, Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, pages 178?185, Washington, DC, USA.
The AAAI Press.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez
Cristo?bal. 2013. TASS - Workshop on Sentiment
Analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50:37?44.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
320
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 105?112,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Name Ambiguity Resolution Using A Generative Model
Zornitsa Kozareva and Sujith Ravi
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,sravi}@isi.edu
Abstract
Resolving ambiguity associated with names
found on the Web, Wikipedia or medical texts
is a very challenging task, which has been
of great interest to the research community.
We propose a novel approach to disambiguat-
ing names using Latent Dirichlet Allocation,
where the learned topics represent the under-
lying senses of the ambiguous name. We con-
duct a detailed evaluation on multiple data sets
containing ambiguous person, location and or-
ganization names and for multiple languages
such as English, Spanish, Romanian and Bul-
garian. We conduct comparative studies with
existing approaches and show a substantial
improvement of 15 to 35% in task accuracy.
1 Introduction
Recently, ambiguity resolution for names found on
the Web (Artiles et al, 2007), Wikipedia articles
(Bunescu and Pasca, 2006), news texts (Pedersen et
al., 2005) and medical literature (Ginter et al, 2004)
has become an active area of research. Like words,
names are ambiguous and can refer to multiple enti-
ties. For example, a Web search for Jerry Hobbs on
Google returns a mixture of documents associated
with two different entities in the top 10 search re-
sults. One refers to a computational linguist at Uni-
versity of Southern California and the other refers to
a fugitive and murderer. Disambiguating the names
and identifying the correct entity is very important
especially for Web search applications since 11-17%
of the Web search queries are composed of person
name and a term (Artiles et al, 2009a).
In the past, there has been a substantial body of
work in the area of name disambiguation under a va-
riety of different names and using diverse set of ap-
proaches. Some refer to the task as cross-document
coreference resolution (Bagga and Baldwin, 1998),
name discrimination (Pedersen et al, 2005) or Web
People Search (WebPS) (Artiles et al, 2007). The
majority of the approaches focus on person name
ambiguity (Chen and Martin, 2007; Artiles et al,
2010), some have also explored organization and lo-
cation name disambiguation (Pedersen et al, 2006).
The intuition behind most approaches follows the
distributional hypothesis (Harris, 1954) according
to which ambiguous names sharing the same con-
texts tend to refer to the same individual. To model
these characteristics, Bunescu and Pasca (2006)
and Cucerzan (2007) incorporate information from
Wikipedia articles, Artiles et al (2007) use Web
page content, Mann and Yarowsky (2003) extract bi-
ographic facts. The approaches used in the WebPS
tasks mainly rely on bag-of-words representations
(Artiles et al, 2007; Chen and Martin, 2007; Artiles
et al, 2009b). Most methods suffer from a com-
mon drawback?they rely on surface features such
as word co-occurrences, which are insufficient to
capture hidden information pertaining to the entities
(senses) associated with the documents.
We take a novel approach for tackling the prob-
lem of name ambiguity using an unsupervised topic
modeling framework. To our knowledge, no one
has yet explored the disambiguation of names using
Latent Dirichlet Allocation (LDA) nor has shown
LDA?s behavior on multiple data sources and set-
tings. Our motivation for using an unsupervised
105
topic modeling framework for name disambiguation
is based on the advantages generative models offer
in contrast to the existing ones. For instance, topic
models such as Latent Dirichlet alocation (LDA)
method (Blei et al, 2003) have been widely used in
the literature for other applications to uncover hid-
den (or latent) groupings underlying a set of obser-
vations. Topic models are capable of handling ambi-
guity and distinguishing between uses of words with
multiple meanings depending on context. Thereby,
they provide a natural fit for our name disambigua-
tion task, where latent topics correspond to the en-
tities (name senses) representing the documents for
an ambiguous name. Identifying these latent topics
helps us identify the particular sense of a given am-
biguous name that is used in the context of a particu-
lar document and hence resolve name ambiguity. In
addition, this approach offers several advantages?
(1) entities (senses) can be learnt automatically from
a collection of documents in an unsupervised man-
ner, (2) efficient methods already exist for perform-
ing inference in this model so we can easily scale
to Web data, and (3) unlike typical approaches, we
can easily apply our learnt model to resolve name
ambiguity for unseen documents.
The main contributions of this paper are:
? We propose a novel model for name disam-
biguation using Latent Dirichlet Allocation.
? Unlike previous approaches, which are de-
signed for specific tasks, corpora and lan-
guages, we conduct a detailed evaluation taking
into consideration the multiple properties of the
data and names.
? Our experimental study shows that LDA can be
used as a general name disambiguation frame-
work, which can be successfully applied on
any corpora (i.e. Web, news, Wikipedia), lan-
guages (i.e. English, Spanish, Romanian and
Bulgarian) and types of ambiguous names (i.e.
people, organizations, locations).
? We conduct a comparative study with existing
state-of-the-art clustering approaches and show
substantial improvements of 15 to 35% in task
accuracy.
The rest of the paper is organized as follows. In Sec-
tion 2 we describe related work. Section 3 describes
the Latent Dirichlet Allocation model used to dis-
ambiguate the names. Section 4 describes the exper-
iments we have conducted on multiple data sets and
languages. Finally, we conclude in Section 5.
2 Related Work
Ambiguous names have been disambiguated with
varying success from structured texts (Pedersen et
al., 2006), semi-structured texts such as Wikipedia
articles (Bunescu and Pasca, 2006; Cucerzan, 2007)
or unstructured texts such as those found on the Web
(Pedersen and Kulkarni, 2007; Artiles et al, 2009b).
Most approaches (Artiles et al, 2009b; Chen et al,
2009; Lan et al, 2009) focus on person name dis-
ambiguation, while others (Pedersen et al, 2006)
also explore ambiguity in organization and location
names. In the medical domain, Hatzivassiloglou et
al. (2001) and Ginter et al (2004) tackle the problem
of gene and protein name disambiguation.
Due to the high interest in this task, researchers
have explored a wide range of approaches and fea-
tures. Among the most common and efficient ones
are those based on clustering and bag-of-words rep-
resentation (Pedersen et al, 2005; Artiles et al,
2009b). Mann and Yarowsky (2003) extract bio-
graphic facts such as date or place of birth, occu-
pation, relatives among others to help resolve am-
biguous names of people. Others (Bunescu and
Pasca, 2006; Cucerzan, 2007; Nguyen and Cao,
2008) work on Wikipedia articles, using infobox
and link information. Pedersen et al (2006) rely
on second order co-occurrence vectors. A few oth-
ers (Matthias, 2005; Wan et al, 2005; Popescu and
Magnini, 2007) identify names of people, locations
and organizations and use them as a source of evi-
dence to measure the similarity between documents
containing the ambiguous names. The most simi-
lar work to ours is that of Song et al (2007) who
use a topic-based modeling approach for name dis-
ambiguation. However, their method explicitly tries
to model the distribution of latent topics with regard
to person names and words appearing within docu-
ments whereas in our method, the latent topics rep-
resent the underlying entities (name senses) for an
ambiguous name.
Unlike the previous approaches which were
specifically designed and evaluated on the WebPS
106
task or a corpus such as Wikipedia or the Web, in
this paper we show a novel unsupervised topic mod-
eling approach for name disambiguation for any cor-
pora (i.e. Web, news, Wikipedia), languages (i.e.
English, Spanish, Romanian and Bulgarian) and se-
mantic categories (i.e. people, location and organi-
zation). The obtained results show substantial im-
provements over the existing approaches.
3 Name Disambiguation with LDA
Recently, topic modeling methods have found
widespread applications in NLP for various
tasks such as summarization (Daume? III and
Marcu, 2006), inferring concept-attribute attach-
ments (Reisinger and Pasca, 2009), selectional
preferences (Ritter et al, 2010) and cross-document
co-reference resolution (Haghighi and Klein, 2010).
Topic models such as LDA are generative models
for documents and represent hidden or latent top-
ics (where a topic is a probability distribution over
words) underlying the semantic structure of docu-
ments. An important use for methods such as LDA
is to infer the set of topics associated with a given
document (or a collection of documents). Next, we
present a novel approach for the task of name dis-
ambiguation using unsupervised topic models.
3.1 Method Description
Given a document corpus D associated with a cer-
tain ambiguous name, our task is to group the docu-
ments into K sets such that each document set cor-
responds to one particular entity (sense) for the am-
biguous name. We first formulate the name disam-
biguation problem as a topic modeling task and then
apply the standard LDA method to infer hidden top-
ics (senses). Our generative story is as follows:
for each name sense sk where k ? {1, ...,K} do
Generate ?sk according to Dir(?)
end for
for each document i in the corpus D do
Choose ?i ? Dir(?)
for each word wi,j where j ? {1, ..., Ni} do
Choose a sense zi,j ?Multinomial(?i)
Choose a word wi,j ?Multinomial(?zi,j )
end for
end for
3.2 Inference
We perform inference on this model using collapsed
Gibbs sampling, where each of the hidden sense
variables zi,j are sampled conditioned on an as-
signment for all other variables, while integrating
over all possible parameter settings (Griffiths and
Steyvers, 2002). We use the MALLET (McCallum,
2002) implementation of LDA for our experiments.
We ran LDA with different parameter settings on a
held out data set and found that the following con-
figuration resulted in the best performance. We set
the hyperparameter ? to the default value of 0.01.
For the name discrimination task, we have to choose
from a smaller set of name senses and each docu-
ment is representative of a single sense, so we use
a sparse prior (?=0.1). On the other hand, the Web
People Search data is more noisy and also involves
a large number of senses, so we use a higher prior
(?=50).
For the name discrimination task (Section 4.1),
we are given a set of senses to choose from and
hence we can use this value to fix the number of top-
ics (senses) K in LDA. However, it is possible that
the number of senses may be unknown to us apriori.
For example, it is difficult to identify all the senses
associated with names of people on the Web. In such
scenarios, we set the value ofK to a fixed value. For
experiments on Web People Search, we set K = 40,
which is roughly the average number of senses as-
sociated with people names on the Web. An alter-
native strategy is to automatically choose the num-
ber of senses based on the model that leads to the
highest posterior probability (Griffiths and Steyvers,
2004). It is easy to incorporate this technique into
our model, but we leave this for future work.
3.3 Interpreting Name Senses From Topics
As a result of training, our model outputs the topic
(sense) distributions for each document in the cor-
pus. Although the LDA model can assign multi-
ple senses to a document, the name disambiguation
task specifies that each document should be assigned
only to a single name sense. Hence, for each docu-
ment i we assign it the most probable sense from its
sense distribution. This allows us to cluster all the
documents in D into K sets.
To evaluate our results against the gold standard
107
data, we further need to find a mapping between our
document clusters and the true name sense labels.
For each cluster k, we identify the true sense labels
(using the gold data) for every document which was
assigned to sense k in our output, and pick the ma-
jority sense label labelkmaj. as being representative
of the entire cluster (i.e., all documents in cluster k
will be labeled as belonging to sense labelkmaj.). Fi-
nally, we evaluate our labeling against the gold data.
4 Experimental Evaluation
Our objective is to study LDA?s performance on
multiple datasets, name categories and languages.
For this purpose, we evaluate our approach on two
tasks: name discrimination and Web People Search,
which are described in the next subsections. We use
freely available data from (Pedersen et al, 2006) and
(Artiles et al, 2009b), which enable us to compare
performance against existing methods.
4.1 Name Discrimination
Pedersen et al (2006) create ambiguous data by
conflating together tuples of non-ambiguous well
known names. The goal is to cluster the contexts
containing the conflated names such that the origi-
nal and correct names are re-discovered. This task is
known as name discrimination.
An advantage of the name conflation process is
that data can be easily created for any type of names
and languages. In our study, we use the whole data
set developed by Pedersen et al (2006) for the En-
glish, Spanish, Romanian and Bulgarian languages.
Table 1 shows the conflated names and the seman-
tic category they belong to (i.e. person, organization
or location) together with the distribution of the in-
stances for each underlying entity in the name. In
total there are eight person, eight location and three
organization conflated name pairs which represent a
diverse set of names of politicians, countries, cities,
political parties and software companies. For four
conflated name pairs the data is balanced. For ex-
ample, there are 3800 examples in total for the con-
flated name Bill Clinton ? Tony Blair of which 1900
are for the underlying entity Bill Clinton and 1900
for Tony Blair. For the rest of the cases the data is
imbalanced. For example, there are 3344 examples
for the conflated name Yaser Arafat ? Bill Clinton of
which 1004 belong to Yaser Arafat and 2340 to Bill
Clinton. The balanced and imbalanced data also lets
us study whether LDA?s performance if affected by
the different sense distributions.
Next, we show in Table 2 the overall results from
the disambiguation process. For each name, we first
show the baseline score which is calculated as the
percentage of instances belonging to the most fre-
quent underlying entity over all instances of that
conflated name pair. For example, for the Bill Clin-
ton ? Tony Blair conflated name pair, the baseline
is 50% since both underlying entities have the same
number of examples. This baseline is equivalent to
a clustering method that would assign all of the con-
texts to exactly one cluster.
The second column corresponds to the results
achieved by the second order co-occurrence cluster-
ing approach of (Pedersen et al, 2006). This ap-
proach is considered as state-of-the-art in name dis-
crimination after numerous features like unigram,
bigram, co-occurrence and multiple clustering algo-
rithms were tested. We denote this approach in Table
2 as Pedersen and use it as a comparison. Note that
in this experiment (Pedersen et al, 2006) predefine
the exact number of clusters, therefore we also use
the exact number of senses for the LDA topics. The
third column shows the results obtained by our LDA
approach. The final two columns represent the dif-
ference between our LDA approach and the baseline
denoted as ?B , as well as the difference between
our LDA approach and those of Pedersen denoted as
?P . We have highlighted in bold the improvements
of LDA over these methods.
The obtained results show that for all experiments
independent of whether the name sense data was bal-
anced or imbalanced, LDA has a positive increase
over the baseline. For some conflated tuples like the
Spanish NATO?ETZIN, the improvement over the
baseline is 47%. For seventeen out of the twenty
name conflated pairs LDA has also improved upon
Pedersen. The improvements range from +1.29 to
+19.18.
Unfortunately, we are not deeply familiar with
Romanian to provide a detailed analysis of the con-
texts and the errors that occurred. However, we no-
ticed that for English, Spanish and Bulgarian often
the same context containing two or three of the con-
flated names is used multiple times. Imagine that
108
Category Name Distribution
ENGLISH
person/politician Bill Cinton ? Tony Blair 1900+1900=3800
person/politician Bill Clinton ? Tony Blair ? Ehud Barak 1900+1900+1900=5700
organization IBM ? Microsoft 2406+3401=5807
location/country Mexico ? Uganda 1256+1256=2512
location/country&state Mexico ? India ? California ? Peru 1500+1500+1500+1500=6000
SPANISH
person/politician Yaser Arafat ? Bill Clinton 1004+2340=3344
person/politician Juan Pablo II ? Boris Yeltsin 1447+1450=2897
organization OTAN (NATO) ? EZLN 1093+1093=2186
location/city New York ? Washington 1517+2418=3935
location/city&country New York ? Brasil ? Washington 1517+1748+2418=5863
ROMANIAN
person/politician Traian Basescu ? Adrian Nastase 1804+1932=3736
person/politician Traian Basescu ? Ion Illiescu ? Adrian Nastase 1948+1966+2301=6215
organization Romanian Democratic Party ? Socialist Party 2037+3264=5301
location/city Brasov ? Bucarest 2310+2559=4869
location/country France ? USA ? Romania 1370+2396+3890=7656
BULGARIAN
person/politician Petar Stoyanov ? Ivan Kostov ? Georgi Parvanov 318+524+811=1653
person/politician Nadejda Mihaylova ? Nikolay Vasilev ? Stoyan Stoyanov 645+849+976=2470
organization Bulgarian Socialist Party ? Union Democratic Forces 2921+4680=7601
location/country France ? Germany ?Russia 1726+2095+2645=6466
location/city Varna ? Bulgaria 1240+1261=2501
Table 1: Data Set Characteristics of the Name Discrimination Task.
Name Baseline Pedersen LDA ?B ?P
ENGLISH
Bill Cinton ? Tony Blair 50.00% 80.95% 81.13% +31.13 +0.18
Bill Clinton ? Tony Blair ? Ehud Barak 33.33% 47.93% 67.19% +33.86 +19.26
IBM ? Microsoft 58.57% 63.70% 65.44% +6.87 +1.74
Mexico ? Uganda 50.00% 59.16% 78.34% +28.35 +19.18
Mexico ? India ? California ? Peru 25.00% 28.78% 46.43% +21.43 +17.65
SPANISH
Yaser Arafat ? Bill Clinton 69.98% 77.72% 83.67% +13.69 +5.95
Juan Pablo II ? Boris Yeltsin 50.05% 87.75% 52.36% +2.31 -35.39
OTAN (NATO) ? EZLN 50.00% 69.81% 96.89% +46.89 +27.08
New York ? Washington 61.45% 54.66% 66.73% +5.28 +12.07
New York ? Brasil ? Washington 42.55% 42.88% 59.28% +16.73 +16.40
ROMANIAN
Traian Basescu ? Adrian Nastase 51.34% 51.34% 58.51% +7.17 +7.17
Traian Basescu ? Ion Illiescu ? Adrian Nastase 37.02% 39.31% 47.69% +10.67 +8.38
Romanian Democratic Party ? Socialist Party 61.57% 77.70% 61.57% 0.00 -16.13
Brasov ? Bucarest 52.56% 63.67% 64.96% +12.40 +1.29
France ? USA ? Romania 50.81% 52.66% 55.39% +4.58 +2.73
BULGARIAN
Petar Stoyanov ? Ivan Kostov ? Georgi Parvanov 49.06% 58.68% 57.96% +8.90 -0.72
Nadejda Mihaylova ? Nikolay Vasilev ? Stoyan Stoyanov 39.51% 59.39% 53.97% +14.46 -5.42
Bulgarian Socialist Party ? Union Democratic Forces 61.57% 57.31% 61.76% +0.19 +4.45
France ? Germany ?Russia 40.91% 41.60% 46.74% +5.83 +5.14
Varna ? Bulgaria 50.42% 50.38% 51.78% +1.36 +1.40
Table 2: Results on the Multilingual and Multi-category Name Discrimination Task.
109
there is a single context in which both names Nade-
jda Mihaylova and Stoyan Stoyanov are mentioned.
This context is used to create two name conflated
examples. In the first case only the name Nadejda
Mihaylova was hidden with the Nadejda Mihaylova
? Nikolay Vasilev ? Stoyan Stoyanov label while the
name Stoyan Stoyanov was preserved as it is. In
the second case, the name Stoyan Stoyanov was hid-
den with the label Nadejda Mihaylova ? Nikolay
Vasilev ? Stoyan Stoyanov while the name Nadejda
Mihaylova was preserved. Since the example con-
tains two name conflations of the same context, it
becomes very difficult for any algorithm to identify
this phenomenon and discriminate the names cor-
rectly.
According to a study conducted by (Pedersen et
al., 2006), the conflated entities in the automatically
collected data sets can be ambiguous and can be-
long to multiple semantic categories. For example,
they mention that the city Varna occurred in the col-
lection as part of other named entities such as the
University of Varna, the Townhall of Varna. There-
fore, by conflating the name Varna in the organiza-
tion named entity University of Varna, the context
starts to deviate the meaning of Varna as a city into
the meaning of university. Such cases transmit ad-
ditional ambiguity to the conflated name pair and
make the task even harder.
Finally, our current approach does not use stop-
words except for English. According to Pedersen et
al. (2006) the usage of stop-words is crucial for this
task and leads to a substantial improvement.
4.2 Web People Search
Recently, Artiles et al (2009b) introduced the Web
People Search task (WebPS), where given the top
100 web search results produced for an ambiguous
person name, the goal is to produce clusters that con-
tain documents referring to the same individual.
We have randomly selected from the WebPS-2
test data three names from the Wikipedia, ACL?08
and Census categories. Unlike the previous data,
WebPS has (1) names with higher ambiguity from
3 to 56 entities per name, (2) only person names and
(3) unstructured and semi-structured texts from the
Web and Wikipedia1. Table 3 shows the number of
1We clean all html tags and remove stopwords.
entities (senses) (#E) and the number of documents
for each ambiguous name (#Doc).
In contrast to the previous task where the number
of topics is equal to the exact number of senses, in
this task the number of topics is approximate to the
number of senses2. In our experiments we set the
number of topics to 40. We embarked on this exper-
imental set up in order to make our results compara-
ble with the rest of the systems in WebPS. However,
if we use the exact number of name senses then LDA
achieves higher results.
To evaluate the performance of our approach, we
use the official WebPS evaluation script. We re-
port BCubed Precision, Recall and F-scores for our
LDA approach, two baseline systems and the ECNU
(Lan et al, 2009) system from the WebPS-2 chal-
lenge. We compare our results against ECNL, be-
cause they use similar word representation but in-
stead of relying on LDA they use a clustering algo-
rithm. We denote in Table 3 the difference between
the F-score performances of LDA and the ECNU
system as ?F1 . We highlight the differences in bold.
Since a name disambiguation system must have
good precision and recall results, we decided to
compare our results against two baselines which rep-
resent the extreme case of a system that reaches
100% precision (called ONE-IN-ONE) or a sys-
tem that reaches 100% recall (called ALL-IN-ONE).
Practically ONE-IN-ONE corresponds to assign-
ing each document to a different cluster (individ-
ual sense), while the ALL-IN-ONE baseline groups
together all web pages into a single cluster corre-
sponding to one name sense (the majority sense). A
more detailed explanation about the evaluation mea-
sures and the intuition behind them can be found in
(Artiles et al, 2007) and (Artiles et al, 2009b).
For six out of the nine names, LDA outperformed
the two baselines and the ECNU system with 5 to
41% on F-score. Precision and recall scores for LDA
are comparable except for Tom Linton and Helen
Thomas where precision is much higher. The de-
crease in performance is due to the low number of
senses (entities associated with a name) and the fact
that LDA was tuned to produce 40 topics. To over-
come this limitation, in the future we plan to work
on estimating the number of topics automatically.
2Researchers use from 15 to 50 number of clusters/senses.
110
ONE-IN-ONE ALL-IN-ONE ECNU LDA
Name #E #Doc BEP BER F1 BEP BER F1 BEP BER F1 BEP BER F1 ?F1
Wikipedia Names
Louis Lowe 24 100 1.00 .32 .48 .23 1.00 .37 .39 .78 .52 .63 .52 .57 +5
Mike Robertson 39 123 1.00 .44 .61 .11 1.00 .19 .14 .96 .25 .59 .62 .61 +36
Tom Linton 10 135 1.00 .11 .19 .54 1.00 .70 .68 .48 .56 .89 .22 .35 -21
ACL ?08 Names
Benjamin Snyder 28 95 1.00 .51 .67 .08 1.00 .15 .16 .79 .27 .59 .81 .68 +41
Emily Bender 19 120 1.00 .21 .35 .24 1.00 .39 .45 .60 .51 .78 .42 .55 +4
Hao Zhang 24 100 1.00 .26 .41 .21 1.00 .35 .45 .78 .57 .72 .36 .48 -9
Census Names
Helen Thomas 3 127 1.00 .03 .06 .96 1.00 .98 .96 .24 .39 .97 .08 .15 -24
Jonathan Shaw 26 126 1.00 .32 .49 .10 1.00 .18 .18 .60 .34 .66 .51 .58 +24
Susan Jones 56 110 1.00 .70 .82 .03 1.00 .06 .13 .81 .22 .51 .79 .62 +40
Table 3: Results for Web People Search-2.
5 Conclusion
We have shown how ambiguity in names can be
modeled and resolved using a generative probabilis-
tic model. Our LDA approach learns a distribution
over topics which correspond to entities (senses) as-
sociated with an ambiguous name. We evaluate our
novel approach on two tasks: name discrimination
and Web People Search. We conduct a detailed eval-
uation on (1) Web, Wikipedia and news documents;
(2) English, Spanish, Romanian and Bulgarian lan-
guages; (3) people, location and organization names.
Our method achieves consistent performance and
substantial improvements over baseline and existing
state-of-the-art clustering methods.
In the future, we would like to model the bi-
ographical fact extraction approach of (Mann and
Yarowsky, 2003) in our LDA model. We plan to es-
timate the number of topics automatically from the
distributions. We want to explore variants of our
current model. For example, currently all words are
generated by multiple topics (senses), but ideally we
want them to be generated by a single topic. Finally,
we want to impose additional constraints within the
topic models using hierarchical topic models.
Acknowledgments
We acknowledge the support of DARPA contract
FA8750-09-C-3705 and NSF grant IIS-0429360.
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 weps evaluation: Establishing a
benchmark for the web people search task. In Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007, pages 64?69.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo.
2009a. The role of named entities in Web People
Search. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 534?542.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2009b.
WePS 2 evaluation campaign: overview of the web
people search clustering task. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference.
Javier Artiles, Andrew Borthwick, Julio Gonzalo, Satoshi
Sekine, and Enrique Amigo?. 2010. WePS-3 evalu-
ation campaign: Overview of the web people search
clustering and attribute extraction ta. In Conference
on Multilingual and Multimodal Information Access
Evaluation (CLEF).
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics - Volume 1, ACL ?98, pages 79?85.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Razvan C. Bunescu and Marius Pasca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In EACL 2006, 11st Conference of the European
Chapter of the Association for Computational Linguis-
tics, Proceedings of the Conference.
Ying Chen and James H. Martin. 2007. Cu-comsem:
Exploring rich features for unsupervised web per-
sonal name disambiguation. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 125?128, June.
111
Ying Chen, Sophia Yat Mei Lee, and Chu-Ren Huang.
2009. Polyuhk: A robust information extraction
system for web personal names. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL 2007, Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 708?716.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 305?312, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Filip Ginter, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2004. New techniques for disambiguation
in natural language and their application to biological
text. J. Mach. Learn. Res., 5:605?621, December.
Thomas L Griffiths and Mark Steyvers. 2002. A prob-
abilistic approach to semantic representation. In Pro-
ceedings of the Twenty-Fourth Annual Conference of
Cognitive Science Society.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101 Suppl 1(Suppl 1):5228?5235.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393.
Zellig Harris. 1954. Distributional structure.
10(23):146?162.
Vasileios Hatzivassiloglou, Pablo A. Duboue, and An-
drey Rzhetsky. 2001. Disambiguating proteins, genes,
and rna in text: A machine learning approach. In Pro-
ceedings of the 9th International Conference on Intel-
ligent Systems for Molecular Biology.
Man Lan, Yu Zhe Zhang, Yue Lu, Jian Su, and Chew Lim
Tan. 2009. Which who are they? people attribute ex-
traction and disambiguation in web search results. In
2nd Web People Search Evaluation Workshop (WePS
2009), 18th WWW Conference.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003 - Volume 4, CONLL ?03,
pages 33?40.
Matthias Blume Matthias. 2005. Automatic entity dis-
ambiguation: Benefits to ner, relation extraction, link
analysis, and inference. In International Conference
on Intelligence Analysis.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Hien T. Nguyen and Tru H. Cao. 2008. Named entity dis-
ambiguation: A hybrid statistical and rule-based incre-
mental approach. In Proceedings of the 3rd Asian Se-
mantic Web Conference on The Semantic Web, ASWC
?08, pages 420?433.
Ted Pedersen and Anagha Kulkarni. 2007. Unsuper-
vised discrimination of person names in web contexts.
In Computational Linguistics and Intelligent Text Pro-
cessing, 8th International Conference, CICLing 2007,
pages 299?310.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering simi-
lar contexts. In Computational Linguistics and Intel-
ligent Text Processing, 6th International Conference,
CICLing 2005, pages 226?237.
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zor-
nitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name
discrimination using second order co-occurrence fea-
tures. In Computational Linguistics and Intelligent
Text Processing, 7th International Conference, CI-
CLing 2006, pages 208?222.
Octavian Popescu and Bernardo Magnini. 2007. Irst-bp:
Web people search using name entities. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007, pages 195?198. Associa-
tion for Computational Linguistics.
Joseph Reisinger and Marius Pasca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 620?628. Association for Computa-
tional Linguistics, August.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434.
Association for Computational Linguistics, July.
Yang Song, Jian Huang, Isaac G. Councill, Jia Li, and
C. Lee Giles. 2007. Efficient topic-based unsuper-
vised name disambiguation. In Proceedings of the 7th
ACM/IEEE-CS Joint Conference on Digital libraries,
pages 342?351.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding.
2005. Person resolution in person search results: Web-
hawk. In Proceedings of the 14th ACM international
conference on Information and knowledge manage-
ment, CIKM ?05, pages 163?170.
112
Proceedings of the TextGraphs-7 Workshop at ACL, pages 39?43,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Cause-Effect Relation Learning
Zornitsa Kozareva
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
kozareva@isi.edu
Abstract
To be able to answer the question What
causes tumors to shrink?, one would re-
quire a large cause-effect relation repos-
itory. Many efforts have been payed on
is-a and part-of relation leaning, however
few have focused on cause-effect learn-
ing. This paper describes an automated
bootstrapping procedure which can learn
and produce with minimal effort a cause-
effect term repository. To filter out the
erroneously extracted information, we in-
corporate graph-based methods. To evalu-
ate the performance of the acquired cause-
effect terms, we conduct three evaluations:
(1) human-based, (2) comparison with ex-
isting knowledge bases and (3) applica-
tion driven (SemEval-1 Task 4) in which
the goal is to identify the relation between
pairs of nominals. The results show that
the extractions at rank 1500 are 89% ac-
curate, they comprise 61% from the terms
used in the SemEval-1 Task 4 dataset and
can be used in the future to produce addi-
tional training examples for the same task.
1 Introduction
Over the years, researchers have successfully
shown how to build ground facts (Etzioni et
al., 2005), semantic lexicons (Thelen and Riloff,
2002), encyclopedic knowledge (Suchanek et al,
2007), and concept lists (Katz et al, 2003).
Among the most well developed repositories are
those focusing on is-a (Hearst, 1992) and part-
of (Girju et al, 2003; Pennacchiotti and Pantel,
2006) relations. However, to be able to answer the
question ?What causes tumors to shrink??, one re-
quires knowledge about cause-effect relation.
Other applications that can benefit from cause-
effect knowledge are the relational search engines
which have to retrieve all terms relevant to a query
like: ?find all X such that X causes wrinkles? (Ca-
farella et al, 2006). Unfortunately to date, there
is no universal repository of cause-effect relations
that can be used or consulted. However, one would
still like to dispose of an automated procedure that
can accurately and quickly acquire the terms ex-
pressing this relation.
Multiple algorithms have been created to learn
relations. Some like TextRunner (Etzioni et al,
2005) rely on labeled data, which is used to train
a sequence-labeling graphical model (CRF) and
then the system uses the model to extract terms
and relations from unlabeled texts. Although very
accurate, such methods require labeled data which
is difficult, expensive and time consuming to cre-
ate. Other more simplistic methods that rely
on lexico-syntactic patterns (Hearst, 1992; Riloff
and Jones, 1999; Pasca, 2004) have shown to be
equally successful at learning relations, temporal
verb order (Chklovski and Pantel, 2004) and en-
tailment (Zanzotto et al, 2006). Therefore, in this
paper, we have incorporated an automated boot-
strapping procedure, which given a pattern rep-
resenting the relation of interest can quickly and
easily learn the terms associated with the relation.
In our case, the pattern captures the cause-effect
relation. After extraction, we apply graph-based
metrics to rerank the information and filter out the
erroneous terms.
The contributions of the paper are:
? an automated procedure, which can learn
terms expressing cause-effect relation.
? an exhaustive human-based evaluation.
? a comparison of the extracted knowledge
with the terms available in the SemEval-1
Task 4 dataset for interpreting the relation be-
tween pairs of nominals.
The rest of the paper is organized as follows.
The next section describes the term extraction pro-
cedure. Section 3 and 4 describe the extracted data
39
and its characteristics. Section 5 focuses on the
evaluation and finally we conclude in Section 6.
2 Cause-Effect Relation Learning
2.1 Problem Formulation
The objectives of cause-effect relation learning are
similar to those of any general open domain rela-
tion extraction problem (Etzioni et al, 2005; Pen-
nacchiotti and Pantel, 2006). The task is formu-
lated as:
Task: Given a cause-effect semantic relation expressed
through lexico-syntactic pattern and a seed example for
which the relation is true, the objective is to learn from
large unstructured amount of texts terms associated with
the relation.
For instance, given the relation cause and the
term virus for which we know that it can cause
something, we express the statement in a recursive
pattern1 ?* and virus cause *? and use the pattern
to learn new terms that cause or have been caused
by something. Following our example, the recur-
sive pattern learns from the Web on the left side
terms like {bacteria, worms, germs} and on the
right side terms like {diseases, damage, contami-
nation}.
2.2 Knowledge Extraction Procedure
For our study, we have used the general Web-
based class instance and relation extraction frame-
work introduced by (Kozareva et al, 2008; Hovy
et al, 2009). The procedure is minimally super-
vised and achieves high accuracy of the produced
extractions.
TermExtraction: To initiate the learning process,
the user must provide as input a seed term Y and a
recursive pattern ?X? and Y verb Z?? from which
terms on the X? and Z? positions can be learned.
The input pattern is submitted to Yahoo!Boss API
as a web query and all snippets matching the query
are retrieved, part-of-speech tagged and used for
term extraction. Only the previously unexplored
terms found on X? position are used as seeds
in the subsequent iteration, while the rest of the
terms2 are kept. The knowledge extraction termi-
nates when there are no new extractions.
Term Ranking: Despite the specific lexico-
syntactic construction of the pattern, erroneous
1A recursive pattern is a lexico-syntactic pattern for which
one of the terms is given as input and the other one is an
open slot, allowing the learned terms to replace the initial
term directly.
2Including the terms found on Z? position.
extractions are still produced. To filter out
the information, we incorporate the harvested
terms on X? and Y ? positions in a directed
graph G=(V,E), where each vertex v ? V is
a candidate term and each edge (u, v) ? E
indicates that the term v is generated by the term
u. An edge has weight w corresponding to the
number of times the term pair (u, v) is extracted
from different snippets. A node u is ranked
by u=(
?
?(u,v)?E w(u, v) +
?
?(v,u)?E w(v, u))
which represents the weighted sum of the outgo-
ing and incoming edges to a node. The confidence
in a correct argument u increases when the term
discovers and is discovered by many different
terms. Similarly, the terms found on Z? position
are ranked by the total number of incoming edges
from the XY pairs z=
?
?(xy,z)?E? w(xy, z).
We assume that in a large corpus as the Web, a
correct term Z? would be frequently discovered
by various XY term pairs.
3 Data Collection
To learn the terms associated with a cause-effect
relation, the user can use as input any verb ex-
pressing causality3. In our experiment, we used
the verb cause and the pattern ?* and <seed>
cause *?, which was instantiated with the seed
term virus. We submitted the pattern to Ya-
hoo!Boss API as a search query and collected all
snippets returned during bootstrapping. The snip-
pets were cleaned from the html tags and part-of-
speech tagged (Schmid, 1994). All nouns (proper
names) found on the left and right hand side of the
pattern were extracted and kept as potential candi-
date terms of the cause-effect relation.
Table 1 shows the total number of terms found
for the cause pattern on X? and Z? positions in 19
bootstrapping iterations. In the same table, we also
show some examples of the obtained extractions.
Term Position #Extractions Examples
X cause 12790 pressure, stress, fire,
cholesterol, wars, ice,
food, cocaine, injuries
bacteria
cause Z 52744 death, pain, diabetes,
heart disease, damage,
determination, nosebleeds
chain reaction
Table 1: Extracted Terms.
3The user can use any pattern from the thesauri of
http://demo.patrickpantel.com/demos/lexsem/thesaurus.htm
40
4 Characteristic of Learning Terms
An interesting characteristic of the bootstrapping
process is the speed of leaning, which can be mea-
sured in terms of the number of unique terms ac-
quired on each bootstrapping iteration. Figure 1
shows the bootstrapping process for the ?cause?
relation. The term extraction starts of very slowly
and as bootstrapping progresses a rapid growth is
observed until a saturation point is reached. This
point shows that the intensity with which new el-
ements are discovered is lower and practically the
bootstrapping process can be terminated once the
amount of newly discovered information does not
exceed a certain threshold. For instance, instead
of running the algorithm until complete exhaus-
tion (19 iterations), the user can terminate it on
the 12th iteration.
!"#$%&'(
)%'#*(+,-,&(./"*,01'(
2"1,3$,4#%1,(./"*,01'(
21,3%5/"'(
6(/7
(#1,
$'(
&,%3
",4
(
8(1,3$'(
9(1,3$'(
8:(%"4(;(.%<',(9:(
Figure 1: Learning Curve.
The speed of leaning depends on the way the
X and Y terms relate to each other in the lexico-
syntactic pattern. For instance, the more densely
connected the graph is, the shorter (i.e., fewer iter-
ations) it will take to acquire all terms.
5 Evaluation and Results
In this section, we evaluate the results of the term
extraction procedure. To the extend to which it
is possible, we conduct a human-based evalua-
tion, we compare results to knowledge bases that
have been extracted in a similar way (i.e., through
pattern application over unstructured text) and we
show how the extracted knowledge can be used
by NLP applications such as relation identification
between nominals.
5.1 Human-Based Evaluation
For the human based evaluation, we use two an-
notators to judge the correctness of the extracted
terms. We estimate the correctness of the pro-
duced extractions by measuring Accuracy as the
number of correctly tagged examples divided by
the total number of examples.
Figure 2, shows the accuracy of the bootstrap-
ping algorithm with graph re-ranking in blue and
without graph re-ranking in red. The figure shows
that graph re-ranking is effective and can separate
out the erroneous extractions. The overall extrac-
tions produced by the algorithm are very precise,
at rank 1500 the accuracy is 89%.
!"#$%&
&'(()
#'(*
&
!"#$
!"#%$
!"&$
!"&%$
'$
'$ ($ )$ *$"(!!$ "%!!$ '!!!$ '%!!$
""#%$
"'$
""&%$
""&!$
""#!$ "!$
Figure 2: Term Extraction Accuracy.
Next, in Table 2, we also show a detailed eval-
uation of the extracted X and Z terms. We de-
fine five types according to which the humans can
classify the extracted terms. The types are: Phys-
icalObject, NonPhysicalObject, Event, State and
Other. We used Other to indicate erroneous ex-
tractions or terms which do not belong to any of
the previous four types. The Kappa agreement for
the produced annotations is 0.80.
X Cause A1 A2 Cause Z A1 A2
PhysicalObj 82 75 PhysicalObj 15 20
NonPhysicalObj 69 66 NonPhysicalObj 89 91
Event 21 24 Event 72 72
State 29 31 State 50 50
Other 3 4 Other 5 4
Acc. .99 .98 Acc. .98 .98
Table 2: Term Classification.
5.2 Comparison against Existing Resources
To compare the performance of our approach with
knowledge bases that have been extracted in a
similar way (i.e., through pattern application over
unstructured text), we consult the freely avail-
able resources NELL (Carlson et al, 2009), Yago
41
(Suchanek et al, 2007) and TextRunner (Etzioni
et al, 2005). Although these bases contain mil-
lions of facts, it turns out that NELL and Yago
do not have information for the cause-effect rela-
tion. While the online demo of TextRunner has
query limitation, which returns only the top 1000
snippets. Since we do not have the complete and
ranked output of TextRunner, comparing results in
terms of relative recall and precision is impossible
and unfair. Therefore, we decided to conduct an
application driven evaluation and see whether the
extracted knowledge can aid an NLP system.
5.3 Application: Identifying Semantic
Relations Between Nominals
Task Description (Girju et al, 2007) introduced
the SemEval-1 Task 4 on the Classification of Se-
mantic Relations between Nominals. It consists
in given a sentence: ?People in Hawaii might be
feeling <e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.?, an NLP
system should identify that the relationship be-
tween the nominals earthquake and aftershocks is
cause-effect.
Data Set (Girju et al, 2007) created a dataset for
seven different semantic relations, one of which is
cause-effect. For each relation, the nominals were
manually selected. This resulted in the creation
of 140 training and 80 testing cause-effect exam-
ples. From the train examples 52.14% were pos-
itive (i.e. correct cause-effect relation) and from
the test examples 51.25% were positive.
Evaluation and Results The objective of our ap-
plication driven study is to measure the overlap of
the cause-effect terms learned by our algorithm
and those used by the humans for the creation
of the SemEval-1 Task4 dataset. There are 314
unique terms in the train and test dataset for which
the cause-effect relation must be identified. Out of
them 190 were also found by our algorithm.
The 61% overlap shows that either our cause-
effect extraction procedure can be used to auto-
matically identify the relationship of the nominals
or it can be incorporated as an additional feature
by a more robust system that relies on semantic
and syntactic information. In the future, the ex-
tracted knowledge can be also used to create addi-
tional training examples for the machine learning
systems working with this dataset.
Table 3 shows some of the overlapping terms in
our system and the (Girju et al, 2007) dataset.
tremor, depression, anxiety, surgery,
exposure, sore throat, fulfillment, yoga,
frustration, inhibition, inflammation, fear,
exhaustion, happiness, growth, evacuation,
earthquake, blockage, zinc, vapour,
sleep deprivation, revenue increase, quake
Table 3: Overlapping Terms.
6 Conclusion
We have described a simple web based procedure
for learning cause-effect semantic relation. We
have shown that graph algorithms can successfully
re-rank and filter out the erroneous information.
We have conduced three evaluations using human
annotators, comparing knowledge against existing
repositories and showing how the extracted knowl-
edge can be used for the identification of relations
between pairs of nominals.
The success of the described framework opens
up many challenging directions. We plan to ex-
pand the extraction procedure with more lexico-
syntactic patterns that express the cause-effect re-
lation4 such as trigger, lead to, result among oth-
ers and thus enrich the recall of the existing repos-
itory. We also want to develop an algorithm for
extracting cause-effect terms from non contigu-
ous positions like ?stress is another very impor-
tant cause of diabetes?. We are also interested
in studying how the extracted knowledge can aid
a commonsense causal reasoner (Gordon et al,
2011; Gordon et al, 2012) in understanding that
if a girl wants to wear earrings it is more likely for
her to get her ears pierced rather then get a tattoo.
This example is taken from the Choice of Plausi-
ble Alternatives (COPA) dataset5, which presents
a series of forced-choice questions such that each
question provides a premise and two viable cause
or effect scenarios. The goal is to choose a cor-
rect answer that is the most plausible cause or ef-
fect. Similarly, the cause-effect repository can be
used to support a variety of applications, includ-
ing textual entailment, information extraction and
question answering
Acknowledgments
We would like to thank the reviewers for their comments and
suggestions. The research was supported by DARPA contract
number FA8750-09-C-3705.
4These patterns can be acquired from an existing para-
phrase system.
5http://people.ict.usc.edu/ gordon/copa.html
42
References
Michael Cafarella, Michele Banko, and Oren Etzioni.
2006. Relational Web Search. In World Wide Web
Conference, WWW 2006.
Andrew Carlson, Justin Betteridge, Estevam R. Hr-
uschka Jr., and Tom M. Mitchell. 2009. Coupling
semi-supervised learning of categories and relations.
In Proceedings of the NAACL HLT 2009 Workskop
on Semi-supervised Learning for Natural Language
Processing.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP 2004, pages
33?40.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134, June.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proc. of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 1?8.
Roxana Girju, Preslav Nakov, Vivi Nastaste, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic
relations between nominals. In SemEval 2007.
Andrew Gordon, Cosmin Bejan, and Kenji Sagae.
2011. Commonsense causal reasoning using mil-
lions of personal stories. In Proceedings of the
Twenty-Fifth Conference on Artificial Intelligence
(AAAI-11).
Andrew Gordon, Zornitsa Kozareva, and Melissa
Roemmele. 2012. Semeval-2012 task 7: Choice
of plausible alternatives: An evaluation of common-
sense causal reasoning. In Proceedings of the 6th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012).
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the
14th conference on Computational linguistics, pages
539?545.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 948?957.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003.
Integrating web-based and corpus-based techniques
for question answering. In Proceedings of the
twelfth text retrieval conference (TREC), pages 426?
435.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048?1056.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the thirteenth
ACM international conference on Information and
knowledge management, pages 137?145.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 793?800.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI ?99/IAAI ?99: Proceedings
of the Sixteenth National Conference on Artificial in-
telligence.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW ?07: Proceedings of the 16th inter-
national conference on World Wide Web, pages 697?
706.
Michael Thelen and Ellen Riloff. 2002. A Bootstrap-
ping Method for Learning Semantic Lexicons Using
Extraction Pattern Contexts. In Proc. of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 214?221.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 849?856.
43
