Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342?350,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Concise Integer Linear Programming Formulations
for Dependency Parsing
Andre? F. T. Martins?? Noah A. Smith? Eric P. Xing?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith,epxing}@cs.cmu.edu
Abstract
We formulate the problem of non-
projective dependency parsing as a
polynomial-sized integer linear pro-
gram. Our formulation is able to handle
non-local output features in an efficient
manner; not only is it compatible with
prior knowledge encoded as hard con-
straints, it can also learn soft constraints
from data. In particular, our model is able
to learn correlations among neighboring
arcs (siblings and grandparents), word
valency, and tendencies toward nearly-
projective parses. The model parameters
are learned in a max-margin framework
by employing a linear programming
relaxation. We evaluate the performance
of our parser on data in several natural
languages, achieving improvements over
existing state-of-the-art methods.
1 Introduction
Much attention has recently been devoted to in-
teger linear programming (ILP) formulations of
NLP problems, with interesting results in appli-
cations like semantic role labeling (Roth and Yih,
2005; Punyakanok et al, 2004), dependency pars-
ing (Riedel and Clarke, 2006), word alignment
for machine translation (Lacoste-Julien et al,
2006), summarization (Clarke and Lapata, 2008),
and coreference resolution (Denis and Baldridge,
2007), among others. In general, the rationale for
the development of ILP formulations is to incorpo-
rate non-local features or global constraints, which
are often difficult to handle with traditional algo-
rithms. ILP formulations focus more on the mod-
eling of problems, rather than algorithm design.
While solving an ILP is NP-hard in general, fast
solvers are available today that make it a practical
solution for many NLP problems.
This paper presents new, concise ILP formu-
lations for projective and non-projective depen-
dency parsing. We believe that our formula-
tions can pave the way for efficient exploitation of
global features and constraints in parsing applica-
tions, leading to more powerful models. Riedel
and Clarke (2006) cast dependency parsing as
an ILP, but efficient formulations remain an open
problem. Our formulations offer the following
comparative advantages:
? The numbers of variables and constraints are
polynomial in the sentence length, as opposed to
requiring exponentially many constraints, elim-
inating the need for incremental procedures like
the cutting-plane algorithm;
? LP relaxations permit fast online discriminative
training of the constrained model;
? Soft constraints may be automatically learned
from data. In particular, our formulations han-
dle higher-order arc interactions (like siblings
and grandparents), model word valency, and can
learn to favor nearly-projective parses.
We evaluate the performance of the new parsers
on standard parsing tasks in seven languages. The
techniques that we present are also compatible
with scenarios where expert knowledge is avail-
able, for example in the form of hard or soft first-
order logic constraints (Richardson and Domin-
gos, 2006; Chang et al, 2008).
2 Dependency Parsing
2.1 Preliminaries
A dependency tree is a lightweight syntactic repre-
sentation that attempts to capture functional rela-
tionships between words. Lately, this formalism
has been used as an alternative to phrase-based
parsing for a variety of tasks, ranging from ma-
chine translation (Ding and Palmer, 2005) to rela-
tion extraction (Culotta and Sorensen, 2004) and
question answering (Wang et al, 2007).
Let us first describe formally the set of legal de-
pendency parse trees. Consider a sentence x =
342
?w0, . . . , wn?, where wi denotes the word at the i-
th position, and w0 = $ is a wall symbol. We form
the (complete1) directed graph D = ?V,A?, with
vertices in V = {0, . . . , n} (the i-th vertex corre-
sponding to the i-th word) and arcs in A = V 2.
Using terminology from graph theory, we say that
B ? A is an r-arborescence2 of the directed
graph D if ?V,B? is a (directed) tree rooted at r.
We define the set of legal dependency parse trees
of x (denoted Y(x)) as the set of 0-arborescences
of D, i.e., we admit each arborescence as a poten-
tial dependency tree.
Let y ? Y(x) be a legal dependency tree for
x; if the arc a = ?i, j? ? y, we refer to i as the
parent of j (denoted i = pi(j)) and j as a child of
i. We also say that a is projective (in the sense of
Kahane et al, 1998) if any vertex k in the span of
a is reachable from i (in other words, if for any k
satisfying min(i, j) < k < max(i, j), there is a
directed path in y from i to k). A dependency tree
is called projective if it only contains projective
arcs. Fig. 1 illustrates this concept.3
The formulation to be introduced in ?3 makes
use of the notion of the incidence vector associ-
ated with a dependency tree y ? Y(x). This is
the binary vector z , ?za?a?A with each compo-
nent defined as za = I(a ? y) (here, I(.) denotes
the indicator function). Considering simultane-
ously all incidence vectors of legal dependency
trees and taking the convex hull, we obtain a poly-
hedron that we call the arborescence polytope,
denoted by Z(x). Each vertex of Z(x) can be
identified with a dependency tree in Y(x). The
Minkowski-Weyl theorem (Rockafellar, 1970) en-
sures that Z(x) has a representation of the form
Z(x) = {z ? R|A| | Az ? b}, for some p-by-|A|
matrix A and some vector b in Rp. However, it is
not easy to obtain a compact representation (where
p grows polynomially with the number of words
n). In ?3, we will provide a compact represen-
tation of an outer polytope Z?(x) ? Z(x) whose
integer vertices correspond to dependency trees.
Hence, the problem of finding the dependency tree
that maximizes some linear function of the inci-
1The general case where A ? V 2 is also of interest; it
arises whenever a constraint or a lexicon forbids some arcs
from appearing in dependency tree. It may also arise as a
consequence of a first-stage pruning step where some candi-
date arcs are eliminated; this will be further discussed in ?4.
2Or ?directed spanning tree with designated root r.?
3In this paper, we consider unlabeled dependency parsing,
where only the backbone structure (i.e., the arcs without the
labels depicted in Fig. 1) is to be predicted.
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$ Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and eighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and P reira, 2006). How-
ever, i the data-dr ven parsing etting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our curr nt
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known t have exact n n-projective
implementations.
We th switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related W rk
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorit ms (Yamad and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). I the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, a
is the case for edge-factored mod ls (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A projective dependency parse (top), and a non-
projective dependency parse (bottom) for two English sen-
tences; examples from McDonald and Satta (2007).
dence vectors can be cast as an ILP. A similar idea
was applied to word alignment by Lacoste-Julien
et al (2006), where permutations (rather than ar-
borescences) were the combinatorial structure be-
ing requiring representation.
Letting X denote the set of possible sentences,
define Y ,
?
x?X Y(x). Given a labeled dataset
L , ??x1, y1?, . . . , ?xm, ym?? ? (X ? Y)m, we
aim to learn a parser, i.e., a function h : X ? Y
that given x ? X ou puts a legal dependency parse
y ? Y(x). T e f ct that t er e xponentially
ma y candidates in Y(x) makes dependency pars-
ing a structured classification problem.
2.2 Arc Factorizat o and L c lity
There has been much recent work on dependency
parsing using graph-based, transition-based, and
hybrid methods; see Nivre and McDonald (2008)
for an overview. Typical graph-based methods
consider linear classifiers of the fo m
hw(x) = argmaxy?Y w
>f(x, y), (1)
where f(x, y) is a vector of features and w is the
corresponding weight vector. One wants hw to
have small expected loss; the ty ical loss func-
tion is th Hamming loss, `(y?; y) , |{?i, j? ?
y? : ?i, j? /? y}|. Tractability is usually ensured
by strong factorization assumptions, like the one
underlying the arc-factored model (Eisner, 1996;
McDonald et al, 2005), which forbids any feature
that depends on two or more arcs. This induces a
decomposition of the feature vector f(x, y) as:
f(x, y) =
?
a?y fa(x). (2)
Under this decomposition, each arc receives a
score; parsing amounts to choosing the configu-
ration that maximizes the overall score, which, as
343
shown by McDonald et al (2005), is an instance
of the maximal arborescence problem. Combi-
natorial algorithms (Chu and Liu, 1965; Edmonds,
1967) can solve this problem in cubic time.4 If
the dependency parse trees are restricted to be
projective, cubic-time algorithms are available via
dynamic programming (Eisner, 1996). While in
the projective case, the arc-factored assumption
can be weakened in certain ways while maintain-
ing polynomial parser runtime (Eisner and Satta,
1999), the same does not happen in the nonprojec-
tive case, where finding the highest-scoring tree
becomes NP-hard (McDonald and Satta, 2007).
Approximate algorithms have been employed to
handle models that are not arc-factored (although
features are still fairly local): McDonald and
Pereira (2006) adopted an approximation based
on O(n3) projective parsing followed by a hill-
climbing algorithm to rearrange arcs, and Smith
and Eisner (2008) proposed an algorithm based on
loopy belief propagation.
3 Dependency Parsing as an ILP
Our approach will build a graph-based parser
without the drawback of a restriction to local fea-
tures. By formulating inference as an ILP, non-
local features can be easily accommodated in our
model; furthermore, by using a relaxation tech-
nique we can still make learning tractable. The im-
pact of LP-relaxed inference in the learning prob-
lem was studied elsewhere (Martins et al, 2009).
A linear program (LP) is an optimization prob-
lem of the form
minx?Rd c
>x
s.t. Ax ? b.
(3)
If the problem is feasible, the optimum is attained
at a vertex of the polyhedron that defines the con-
straint space. If we add the constraint x ? Zd, then
the above is called an integer linear program
(ILP). For some special parameter settings?e.g.,
when b is an integer vector and A is totally uni-
modular5?all vertices of the constraining polyhe-
dron are integer points; in these cases, the integer
constraint may be suppressed and (3) is guaran-
teed to have integer solutions (Schrijver, 2003).
Of course, this need not happen: solving a gen-
eral ILP is an NP-complete problem. Despite this
4There is also a quadratic algorithm due to Tarjan (1977).
5A matrix is called totally unimodular if the determinants
of each square submatrix belong to {0, 1,?1}.
fact, fast solvers are available today that make this
a practical solution for many problems. Their per-
formance depends on the dimensions and degree
of sparsity of the constraint matrix A.
Riedel and Clarke (2006) proposed an ILP for-
mulation for dependency parsing which refines
the arc-factored model by imposing linguistically
motivated ?hard? constraints that forbid some arc
configurations. Their formulation includes an ex-
ponential number of constraints?one for each
possible cycle. Since it is intractable to throw
in all constraints at once, they propose a cutting-
plane algorithm, where the cycle constraints are
only invoked when violated by the current solu-
tion. The resulting algorithm is still slow, and an
arc-factored model is used as a surrogate during
training (i.e., the hard constraints are only used at
test time), which implies a discrepancy between
the model that is optimized and the one that is ac-
tually going to be used.
Here, we propose ILP formulations that elim-
inate the need for cycle constraints; in fact, they
require only a polynomial number of constraints.
Not only does our model allow expert knowledge
to be injected in the form of constraints, it is also
capable of learning soft versions of those con-
straints from data; indeed, it can handle features
that are not arc-factored (correlating, for exam-
ple, siblings and grandparents, modeling valency,
or preferring nearly projective parses). While, as
pointed out by McDonald and Satta (2007), the
inclusion of these features makes inference NP-
hard, by relaxing the integer constraints we obtain
approximate algorithms that are very efficient and
competitive with state-of-the-art methods. In this
paper, we focus on unlabeled dependency parsing,
for clarity of exposition. If it is extended to labeled
parsing (a straightforward extension), our formu-
lation fully subsumes that of Riedel and Clarke
(2006), since it allows using the same hard con-
straints and features while keeping the ILP poly-
nomial in size.
3.1 The Arborescence Polytope
We start by describing our constraint space. Our
formulations rely on a concise polyhedral repre-
sentation of the set of candidate dependency parse
trees, as sketched in ?2.1. This will be accom-
plished by drawing an analogy with a network
flow problem.
Let D = ?V,A? be the complete directed graph
344
associated with a sentence x ? X , as stated in
?2. A subgraph y = ?V,B? is a legal dependency
tree (i.e., y ? Y(x)) if and only if the following
conditions are met:
1. Each vertex in V \ {0} must have exactly one
incoming arc in B,
2. 0 has no incoming arcs in B,
3. B does not contain cycles.
For each vertex v ? V , let ??(v) , {?i, j? ?
A | j = v} denote its set of incoming arcs, and
?+(v) , {?i, j? ? A | i = v} denote its set of
outgoing arcs. The two first conditions can be eas-
ily expressed by linear constraints on the incidence
vector z:
?
a???(j) za = 1, j ? V \ {0} (4)
?
a???(0) za = 0 (5)
Condition 3 is somewhat harder to express. Rather
than adding exponentially many constraints, one
for each potential cycle (like Riedel and Clarke,
2006), we equivalently replace condition 3 by
3?. B is connected.
Note that conditions 1-2-3 are equivalent to 1-2-
3?, in the sense that both define the same set Y(x).
However, as we will see, the latter set of condi-
tions is more convenient. Connectedness of graphs
can be imposed via flow constraints (by requir-
ing that, for any v ? V \ {0}, there is a directed
path in B connecting 0 to v). We adapt the single
commodity flow formulation for the (undirected)
minimum spanning tree problem, due to Magnanti
and Wolsey (1994), that requires O(n2) variables
and constraints. Under this model, the root node
must send one unit of flow to every other node.
By making use of extra variables, ? , ??a?a?A,
to denote the flow of commodities through each
arc, we are led to the following constraints in ad-
dition to Eqs. 4?5 (we denote U , [0, 1], and
B , {0, 1} = U ? Z):
? Root sends flow n:
?
a??+(0) ?a = n (6)
? Each node consumes one unit of flow:
?
a???(j)
?a ?
?
a??+(j)
?a = 1, j ? V \ {0} (7)
? Flow is zero on disabled arcs:
?a ? nza, a ? A (8)
? Each arc indicator lies in the unit interval:
za ? U, a ? A. (9)
These constraints project an outer bound of the ar-
borescence polytope, i.e.,
Z?(x) , {z ? R|A| | (z,?) satisfy (4?9)}
? Z(x). (10)
Furthermore, the integer points of Z?(x) are pre-
cisely the incidence vectors of dependency trees
in Y(x); these are obtained by replacing Eq. 9 by
za ? B, a ? A. (11)
3.2 Arc-Factored Model
Given our polyhedral representation of (an outer
bound of) the arborescence polytope, we can
now formulate dependency parsing with an arc-
factored model as an ILP. By storing the arc-
local feature vectors into the columns of a matrix
F(x) , [fa(x)]a?A, and defining the score vec-
tor s , F(x)>w (each entry is an arc score) the
inference problem can be written as
max
y?Y(x)
w>f(x, y) = max
z?Z(x)
w>F(x)z
= max
z,?
s>z
s.t. A
[
z
?
]
? b
z ? B
(12)
whereA is a sparse constraint matrix (withO(|A|)
non-zero elements), and b is the constraint vec-
tor; A and b encode the constraints (4?9). This
is an ILP with O(|A|) variables and constraints
(hence, quadratic in n); if we drop the integer
constraint the problem becomes the LP relaxation.
As is, this formulation is no more attractive than
solving the problem with the existing combinato-
rial algorithms discussed in ?2.2; however, we can
now start adding non-local features to build a more
powerful model.
3.3 Sibling and Grandparent Features
To cope with higher-order features of the form
fa1,...,aK (x) (i.e., features whose values depend on
the simultaneous inclusion of arcs a1, . . . , aK on
345
a candidate dependency tree), we employ a lin-
earization trick (Boros and Hammer, 2002), defin-
ing extra variables za1...aK , za1 ? . . .?zaK . This
logical relation can be expressed by the following
O(K) agreement constraints:6
za1...aK ? zai , i = 1, . . . ,K
za1...aK ?
?K
i=1 zai ?K + 1. (13)
As shown by McDonald and Pereira (2006) and
Carreras (2007), the inclusion of features that
correlate sibling and grandparent arcs may be
highly beneficial, even if doing so requires resort-
ing to approximate algorithms.7 Define Rsibl ,
{?i, j, k? | ?i, j? ? A, ?i, k? ? A} and Rgrand ,
{?i, j, k? | ?i, j? ? A, ?j, k? ? A}. To include
such features in our formulation, we need to add
extra variables zsibl , ?zr?r?Rsibl and z
grand ,
?zr?r?Rgrand that indicate the presence of sibling
and grandparent arcs. Observe that these indica-
tor variables are conjunctions of arc indicator vari-
ables, i.e., zsiblijk = zij ? zik and z
grand
ijk = zij ? zjk.
Hence, these features can be handled in our formu-
lation by adding the following O(|A| ? |V |) vari-
ables and constraints:
zsiblijk ? zij , z
sibl
ijk ? zik, z
sibl
ijk ? zij + zik ? 1
(14)
for all triples ?i, j, k? ? Rsibl, and
zgrandijk ? zij , z
grand
ijk ? zjk, z
grand
ijk ? zij+zjk?1
(15)
for all triples ?i, j, k? ? Rgrand. Let R , A ?
Rsibl ? Rgrand; by redefining z , ?zr?r?R and
F(x) , [fr(x)]r?R, we may express our inference
problem as in Eq. 12, with O(|A| ? |V |) variables
and constraints.
Notice that the strategy just described to han-
dle sibling features is not fully compatible with
the features proposed by Eisner (1996) for pro-
jective parsing, as the latter correlate only con-
secutive siblings and are also able to place spe-
cial features on the first child of a given word.
The ability to handle such ?ordered? features is
intimately associated with Eisner?s dynamic pro-
gramming parsing algorithm and with the Marko-
vian assumptions made explicitly by his genera-
tive model. We next show how similar features
6Actually, any logical condition can be encoded with lin-
ear constraints involving binary variables; see e.g. Clarke and
Lapata (2008) for an overview.
7By sibling features we mean features that depend on
pairs of sibling arcs (i.e., of the form ?i, j? and ?i, k?); by
grandparent features we mean features that depend on pairs
of grandparent arcs (of the form ?i, j? and ?j, k?).
can be incorporated in our model by adding ?dy-
namic? constraints to our ILP. Define:
znext siblijk ,
?
??
??
1 if ?i, j? and ?i, k? are
consecutive siblings,
0 otherwise,
zfirst childij ,
{
1 if j is the first child of i,
0 otherwise.
Suppose (without loss of generality) that i < j <
k ? n. We could naively compose the constraints
(14) with additional linear constraints that encode
the logical relation
znext siblijk = z
sibl
ijk ?
?
j<l<k ?zil,
but this would yield a constraint matrix with
O(n4) non-zero elements. Instead, we define aux-
iliary variables ?jk and ?ij :
?jk =
{
1, if ?l s.t. pi(l) = pi(j) < j < l < k
0, otherwise,
?ij =
{
1, if ?k s.t. i < k < j and ?i, k? ? y
0, otherwise.
(16)
Then, we have that znext siblijk = z
sibl
ijk ? (??jk) and
zfirst childij = zij?(??ij), which can be encoded via
znext siblijk ? z
sibl
ijk z
first child
ij ? zij
znext siblijk ? 1 ? ?jk z
first child
ij ? 1 ? ?ij
znext siblijk ? z
sibl
ijk ? ?jk z
first child
ij ? zij ? ?ij
The following ?dynamic? constraints encode the
logical relations for the auxiliary variables (16):
?j(j+1) = 0 ?i(i+1) = 0
?j(k+1) ? ?jk ?i(j+1) ? ?ij
?j(k+1) ?
?
i<j
zsiblijk ?i(j+1) ? zij
?j(k+1) ? ?jk +
?
i<j
zsiblijk ?i(j+1) ? ?ij + zij
Auxiliary variables and constraints are defined
analogously for the case n ? i > j > k. This
results in a sparser constraint matrix, with only
O(n3) non-zero elements.
3.4 Valency Features
A crucial fact about dependency grammars is that
words have preferences about the number and ar-
rangement of arguments and modifiers they ac-
cept. Therefore, it is desirable to include features
346
that indicate, for a candidate arborescence, how
many outgoing arcs depart from each vertex; de-
note these quantities by vi ,
?
a??+(i) za, for
each i ? V . We call vi the valency of the ith ver-
tex. We add valency indicators zvalik , I(vi = k)
for i ? V and k = 0, . . . , n? 1. This way, we are
able to penalize candidate dependency trees that
assign unusual valencies to some of their vertices,
by specifying a individual cost for each possible
value of valency. The following O(|V |2) con-
straints encode the agreement between valency in-
dicators and the other variables:
?n?1
k=0 kz
val
ik =
?
a??+(i) za, i ? V (17)
?n?1
k=0 z
val
ik = 1, i ? V
zvalik ? 0, i ? V, k ? {0, . . . , n? 1}
3.5 Projectivity Features
For most languages, dependency parse trees tend
to be nearly projective (cf. Buchholz and Marsi,
2006). We wish to make our model capable of
learning to prefer ?nearly? projective parses when-
ever that behavior is observed in the data.
The multicommodity directed flow model of
Magnanti and Wolsey (1994) is a refinement of the
model described in ?3.1 which offers a compact
and elegant way to indicate nonprojective arcs, re-
quiring O(n3) variables and constraints. In this
model, every node k 6= 0 defines a commodity:
one unit of commodity k originates at the root
node and must be delivered to node k; the vari-
able ?kij denotes the flow of commodity k in arc
?i, j?. We first replace (4?9) by (18?22):
? The root sends one unit of commodity to each
node:
?
a???(0)
?ka ?
?
a??+(0)
?ka = ?1, k ? V \ {0} (18)
? Any node consumes its own commodity and no
other:
?
a???(j)
?ka ?
?
a??+(j)
?ka = ?
k
j , j, k ? V \ {0} (19)
where ?kj , I(j = k) is the Kronecker delta.
? Disabled arcs do not carry any flow:
?ka ? za, a ? A, k ? V (20)
? There are exactly n enabled arcs:
?
a?A za = n (21)
? All variables lie in the unit interval:
za ? U, ?ka ? U, a ? A, k ? V (22)
We next define auxiliary variables ?jk that indi-
cate if there is a path from j to k. Since each ver-
tex except the root has only one incoming arc, the
following linear equalities are enough to describe
these new variables:
?jk =
?
a???(j) ?
k
a, j, k ? V \ {0}
?0k = 1, k ? V \ {0}. (23)
Now, define indicators znp , ?znpa ?a?A, where
znpa , I(a ? y and a is nonprojective).
From the definition of projective arcs in ?2.1, we
have that znpa = 1 if and only if the arc is active
(za = 1) and there is some vertex k in the span of
a = ?i, j? such that ?ik = 0. We are led to the
following O(|A| ? |V |) constraints for ?i, j? ? A:
znpij ? zij
znpij ? zij ? ?ik, min(i, j) ? k ? max(i, j)
znpij ? ?
?max(i,j)?1
k=min(i,j)+1 ?ik + |j ? i| ? 1
There are other ways to introduce nonprojectiv-
ity indicators and alternative definitions of ?non-
projective arc.? For example, by using dynamic
constraints of the same kind as those in ?3.3,
we can indicate arcs that ?cross? other arcs with
O(n3) variables and constraints, and a cubic num-
ber of non-zero elements in the constraint matrix
(omitted for space).
3.6 Projective Parsing
It would be straightforward to adapt the con-
straints in ?3.5 to allow only projective parse trees:
simply force znpa = 0 for any a ? A. But there are
more efficient ways of accomplish this. While it is
difficult to impose projectivity constraints or cycle
constraints individually, there is a simpler way of
imposing both. Consider 3 (or 3?) from ?3.1.
Proposition 1 Replace condition 3 (or 3?) with
3??. If ?i, j? ? B, then, for any k = 1, . . . , n
such that k 6= j, the parent of k must satisfy
(defining i? , min(i, j) and j? , max(i, j)):
?
??
??
i? ? pi(k) ? j?, if i? < k < j?,
pi(k) < i? ? pi(k) > j?, if k < i? or k > j?
or k = i.
347
Then, Y(x) will be redefined as the set of projec-
tive dependency parse trees.
We omit the proof for space. Conditions 1, 2, and
3?? can be encoded with O(n2) constraints.
4 Experiments
We report experiments on seven languages, six
(Danish, Dutch, Portuguese, Slovene, Swedish
and Turkish) from the CoNLL-X shared task
(Buchholz and Marsi, 2006), and one (English)
from the CoNLL-2008 shared task (Surdeanu et
al., 2008).8 All experiments are evaluated using
the unlabeled attachment score (UAS), using the
default settings.9 We used the same arc-factored
features as McDonald et al (2005) (included in the
MSTParser toolkit10); for the higher-order models
described in ?3.3?3.5, we employed simple higher
order features that look at the word, part-of-speech
tag, and (if available) morphological information
of the words being correlated through the indica-
tor variables. For scalability (and noting that some
of the models require O(|V | ? |A|) constraints and
variables, which, when A = V 2, grows cubically
with the number of words), we first prune the base
graph by running a simple algorithm that ranks the
k-best candidate parents for each word in the sen-
tence (we set k = 10); this reduces the number of
candidate arcs to |A| = kn.11 This strategy is sim-
ilar to the one employed by Carreras et al (2008)
to prune the search space of the actual parser. The
ranker is a local model trained using a max-margin
criterion; it is arc-factored and not subject to any
structural constraints, so it is very fast.
The actual parser was trained via the online
structured passive-aggressive algorithm of Cram-
mer et al (2006); it differs from the 1-best MIRA
algorithm of McDonald et al (2005) by solv-
ing a sequence of loss-augmented inference prob-
lems.12 The number of iterations was set to 10.
The results are summarized in Table 1; for the
sake of comparison, we reproduced three strong
8We used the provided train/test splits except for English,
for which we tested on the development partition. For train-
ing, sentences longer than 80 words were discarded. For test-
ing, all sentences were kept (the longest one has length 118).
9
http://nextens.uvt.nl/?conll/software.html
10
http://sourceforge.net/projects/mstparser
11Note that, unlike reranking approaches, there are still ex-
ponentially many candidate parse trees after pruning. The
oracle constrained to pick parents from these lists achieves
> 98% in every case.
12The loss-augmented inference problem can also be ex-
pressed as an LP for Hamming loss functions that factor over
arcs; we refer to Martins et al (2009) for further details.
baselines, all of them state-of-the-art parsers based
on non-arc-factored models: the second order
model of McDonald and Pereira (2006), the hy-
brid model of Nivre and McDonald (2008), which
combines a (labeled) transition-based and a graph-
based parser, and a refinement of the latter, due
to Martins et al (2008), which attempts to ap-
proximate non-local features.13 We did not repro-
duce the model of Riedel and Clarke (2006) since
the latter is tailored for labeled dependency pars-
ing; however, experiments reported in that paper
for Dutch (and extended to other languages in the
CoNLL-X task) suggest that their model performs
worse than our three baselines.
By looking at the middle four columns, we can
see that adding non-arc-factored features makes
the models more accurate, for all languages. With
the exception of Portuguese, the best results are
achieved with the full set of features. We can
also observe that, for some languages, the valency
features do not seem to help. Merely modeling
the number of dependents of a word may not be
as valuable as knowing what kinds of dependents
they are (for example, distinguishing among argu-
ments and adjuncts).
Comparing with the baselines, we observe that
our full model outperforms that of McDonald and
Pereira (2006), and is in line with the most ac-
curate dependency parsers (Nivre and McDonald,
2008; Martins et al, 2008), obtained by com-
bining transition-based and graph-based parsers.14
Notice that our model, compared with these hy-
brid parsers, has the advantage of not requiring an
ensemble configuration (eliminating, for example,
the need to tune two parsers). Unlike the ensem-
bles, it directly handles non-local output features
by optimizing a single global objective. Perhaps
more importantly, it makes it possible to exploit
expert knowledge through the form of hard global
constraints. Although not pursued here, the same
kind of constraints employed by Riedel and Clarke
(2006) can straightforwardly fit into our model,
after extending it to perform labeled dependency
parsing. We believe that a careful design of fea-
13Unlike our model, the hybrid models used here as base-
lines make use of the dependency labels at training time; in-
deed, the transition-based parser is trained to predict a la-
beled dependency parse tree, and the graph-based parser use
these predicted labels as input features. Our model ignores
this information at training time; therefore, this comparison
is slightly unfair to us.
14See also Zhang and Clark (2008) for a different approach
that combines transition-based and graph-based methods.
348
[M
P0
6]
[N
M0
8]
[M
DS
X0
8]
AR
C-F
AC
TO
RE
D
+S
IB
L/G
RA
ND
P.
+V
AL
EN
CY
+P
RO
J. (
FU
LL
)
FU
LL
, R
EL
AX
ED
DANISH 90.60 91.30 91.54 89.80 91.06 90.98 91.18 91.04 (-0.14)
DUTCH 84.11 84.19 84.79 83.55 84.65 84.93 85.57 85.41 (-0.16)
PORTUGUESE 91.40 91.81 92.11 90.66 92.11 92.01 91.42 91.44 (+0.02)
SLOVENE 83.67 85.09 85.13 83.93 85.13 85.45 85.61 85.41 (-0.20)
SWEDISH 89.05 90.54 90.50 89.09 90.50 90.34 90.60 90.52 (-0.08)
TURKISH 75.30 75.68 76.36 75.16 76.20 76.08 76.34 76.32 (-0.02)
ENGLISH 90.85 ? ? 90.15 91.13 91.12 91.16 91.14 (-0.02)
Table 1: Results for nonprojective dependency parsing (unlabeled attachment scores). The three baselines are the second order
model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al (2008). The
four middle columns show the performance of our model using exact (ILP) inference at test time, for increasing sets of features
(see ?3.2??3.5). The rightmost column shows the results obtained with the full set of features using relaxed LP inference
followed by projection onto the feasible set. Differences are with respect to exact inference for the same set of features. Bold
indicates the best result for a language. As for overall performance, both the exact and relaxed full model outperform the arc-
factored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) according
to Dan Bikel?s randomized method (http://www.cis.upenn.edu/?dbikel/software.html).
tures and constraints can lead to further improve-
ments on accuracy.
We now turn to a different issue: scalability. In
previous work (Martins et al, 2009), we showed
that training the model via LP-relaxed inference
(as we do here) makes it learn to avoid frac-
tional solutions; as a consequence, ILP solvers
will converge faster to the optimum (on average).
Yet, it is known from worst case complexity the-
ory that solving a general ILP is NP-hard; hence,
these solvers may not scale well with the sentence
length. Merely considering the LP-relaxed version
of the problem at test time is unsatisfactory, as it
may lead to a fractional solution (i.e., a solution
whose components indexed by arcs, z? = ?za?a?A,
are not all integer), which does not correspond to a
valid dependency tree. We propose the following
approximate algorithm to obtain an actual parse:
first, solve the LP relaxation (which can be done
in polynomial time with interior-point methods);
then, if the solution is fractional, project it onto the
feasible set Y(x). Fortunately, the Euclidean pro-
jection can be computed in a straightforward way
by finding a maximal arborescence in the directed
graph whose weights are defined by z? (we omit
the proof for space); as we saw in ?2.2, the Chu-
Liu-Edmonds algorithm can do this in polynomial
time. The overall parsing runtime becomes poly-
nomial with respect to the length of the sentence.
The last column of Table 1 compares the ac-
curacy of this approximate method with the ex-
act one. We observe that there is not a substantial
drop in accuracy; on the other hand, we observed
a considerable speed-up with respect to exact in-
ference, particularly for long sentences. The av-
erage runtime (across all languages) is 0.632 sec-
onds per sentence, which is in line with existing
higher-order parsers and is much faster than the
runtimes reported by Riedel and Clarke (2006).
5 Conclusions
We presented new dependency parsers based on
concise ILP formulations. We have shown how
non-local output features can be incorporated,
while keeping only a polynomial number of con-
straints. These features can act as soft constraints
whose penalty values are automatically learned
from data; in addition, our model is also compati-
ble with expert knowledge in the form of hard con-
straints. Learning through a max-margin frame-
work is made effective by the means of a LP-
relaxation. Experimental results on seven lan-
guages show that our rich-featured parsers outper-
form arc-factored and approximate higher-order
parsers, and are in line with stacked parsers, hav-
ing with respect to the latter the advantage of not
requiring an ensemble configuration.
Acknowledgments
The authors thank the reviewers for their com-
ments. Martins was supported by a grant from
FCT/ICTI through the CMU-Portugal Program,
and also by Priberam Informa?tica. Smith was
supported by NSF IIS-0836431 and an IBM Fac-
ulty Award. Xing was supported by NSF DBI-
0546594, DBI-0640543, IIS-0713379, and an Al-
fred Sloan Foundation Fellowship in Computer
Science.
349
References
E. Boros and P.L. Hammer. 2002. Pseudo-Boolean op-
timization. Discrete Applied Mathematics, 123(1?
3):155?225.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. TAG,
dynamic programming, and the perceptron for effi-
cient, feature-rich parsing. In Proc. of CoNLL.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of CoNLL.
M. Chang, L. Ratinov, and D. Roth. 2008. Constraints
as prior knowledge. In ICML Workshop on Prior
Knowledge for Text and Language Processing.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
J. Clarke and M. Lapata. 2008. Global inference
for sentence compression an integer linear program-
ming approach. JAIR, 31:399?429.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
A. Culotta and J. Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proc. of ACL.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In Proc. of HLT-NAACL.
Y. Ding and M. Palmer. 2005. Machine translation us-
ing probabilistic synchronous dependency insertion
grammar. In Proc. of ACL.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automaton
grammars. In Proc. of ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. of COL-
ING.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING-ACL.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. I. Jor-
dan. 2006. Word alignment via quadratic assign-
ment. In Proc. of HLT-NAACL.
T. L. Magnanti and L. A. Wolsey. 1994. Optimal
Trees. Technical Report 290-94, Massachusetts In-
stitute of Technology, Operations Research Center.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In Proc. of
EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Polyhedral outer approximations with application to
natural language parsing. In Proc. of ICML.
R. T. McDonald and F. C. N. Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of EACL.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of IWPT.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of HLT-EMNLP.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proc. of ACL-HLT.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear program-
ming inference. In Proc. of COLING.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1):107?136.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In Proc. of EMNLP.
R. T. Rockafellar. 1970. Convex Analysis. Princeton
University Press.
D. Roth and W. T. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In
ICML.
A. Schrijver. 2003. Combinatorial Optimization:
Polyhedra and Efficiency, volume 24 of Algorithms
and Combinatorics. Springer.
D. A. Smith and J. Eisner. 2008. Dependency parsing
by belief propagation. In Proc. of EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,
and J. Nivre. 2008. The conll-2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. Proc. of CoNLL.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? A quasi-synchronous gram-
mar for QA. In Proceedings of EMNLP-CoNLL.
Y. Zhang and S. Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proc. of EMNLP.
350
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Summarization with a Joint Model
for Sentence Extraction and Compression
Andre? F. T. Martins?? and Noah A. Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith}@cs.cmu.edu
Abstract
Text summarization is one of the oldest prob-
lems in natural language processing. Popu-
lar approaches rely on extracting relevant sen-
tences from the original documents. As a side
effect, sentences that are too long but partly
relevant are doomed to either not appear in the
final summary, or prevent inclusion of other
relevant sentences. Sentence compression is a
recent framework that aims to select the short-
est subsequence of words that yields an infor-
mative and grammatical sentence. This work
proposes a one-step approach for document
summarization that jointly performs sentence
extraction and compression by solving an in-
teger linear program. We report favorable ex-
perimental results on newswire data.
1 Introduction
Automatic text summarization dates back to the
1950s and 1960s (Luhn, 1958; Baxendale, 1958; Ed-
mundson, 1969). Today, the proliferation of digital
information makes research on summarization tech-
nologies more important than ever before. In the last
two decades, machine learning techniques have been
employed in extractive summarization of single
documents (Kupiec et al, 1995; Aone et al, 1999;
Osborne, 2002) and multiple documents (Radev and
McKeown, 1998; Carbonell and Goldstein, 1998;
Radev et al, 2000). Most of this work aims only
to extract relevant sentences from the original doc-
uments and present them as the summary; this sim-
plification of the problem yields scalable solutions.
Some attention has been devoted by the NLP
community to the related problem of sentence com-
pression (Knight and Marcu, 2000): given a long
sentence, how to maximally compress it into a gram-
matical sentence that still preserves all the rele-
vant information? While sentence compression is
a promising framework with applications, for exam-
ple, in headline generation (Dorr et al, 2003; Jin,
2003), little work has been done to include it as a
module in document summarization systems. Most
existing approaches (with some exceptions, like the
vine-growth model of Daume?, 2006) use a two-stage
architecture, either by first extracting a certain num-
ber of salient sentences and then feeding them into
a sentence compressor, or by first compressing all
sentences and extracting later. However, regardless
of which operation is performed first?compression
or extraction?two-step ?pipeline? approaches may
fail to find overall-optimal solutions; often the sum-
maries are not better that the ones produced by ex-
tractive summarization. On the other hand, a pilot
study carried out by Lin (2003) suggests that sum-
marization systems that perform sentence compres-
sion have the potential to beat pure extractive sys-
tems if they model cross-sentence effects.
In this work, we address this issue by merging the
tasks of sentence extraction and sentence compres-
sion into a global optimization problem. A careful
design of the objective function encourages ?sparse
solutions,? i.e., solutions that involve only a small
number of sentences whose compressions are to be
included in the summary. Our contributions are:
? We cast joint sentence extraction and compression
as an integer linear program (ILP);
? We provide a new formulation of sentence com-
pression using dependency parsing information
that only requires a linear number of variables,
and combine it with a bigram model;
? We show how the full model can be trained in a
max-margin framework. Since a dataset of sum-
maries comprised of extracted, compressed sen-
tences is unavailable, we present a procedure that
trains the compression and extraction models sep-
arately and tunes a parameter to interpolate the
1
two models.
The compression model and the full system are
compared with state-of-the-art baselines in standard
newswire datasets. This paper is organized as fol-
lows: ?2?3 provide an overview of our two building
blocks, sentence extraction and sentence compres-
sion. ?4 describes our method to perform one-step
sentence compression and extraction. ?5 shows ex-
periments in newswire data. Finally, ?6 concludes
the paper and suggests future work.
2 Extractive summarization
Extractive summarization builds a summary by ex-
tracting a few informative sentences from the docu-
ments. Let D , {t1, . . . , tM} be a set of sentences,
contained in a single or in multiple related docu-
ments.1 The goal is to extract the best sequence of
sentences ?ti1 , . . . , tiK ? that summarizes D whose
total length does not exceed a fixed budget of J
words. We describe some well-known approaches
that wil serve as our experimental baselines.
Extract the leading sentences (Lead). For
single-document summarization, the simplest
method consists of greedily extracting the leading
sentences while they fit into the summary. A sen-
tence is skipped if its inclusion exceeds the budget,
and the next is examined. This performs extremely
well in newswire articles, due to the journalistic
convention of summarizing the article first.
Rank by relevance (Rel). This method ranks sen-
tences by a relevance score, and then extracts the top
ones that can fit into the summary. The score is typ-
ically a linear function of feature values:
scorerel(ti) , ?>f(ti) = ?Dd=1 ?dfd(ti), (1)
Here, each fd(ti) is a feature extracted from sen-
tence ti, and ?d is the corresponding weight. In our
experiments, relevance features include (i) the recip-
rocal position in the document, (ii) a binary feature
indicating whether the sentence is the first one, and
(iii) the 1-gram and 2-gram cosine similarity with
the headline and with the full document.
1For simplicity, we describe a unified framework for single
and multi-document summarization, although they may require
specialized strategies. Here we experiment only with single-
document summarization and assume t1, ..., tM are ordered.
Maximal Marginal Relevance (MMR). For long
documents or large collections, it becomes impor-
tant to penalize the redundancy among the extracted
sentences. Carbonell and Goldstein (1998) proposed
greedily adding sentences to the summary S to max-
imize, at each step, a score of the form
? ? scorerel(ti) ? (1 ? ?) ? scorered(ti, S), (2)
where scorerel(ti) is as in Eq. 1 and scorered(ti, S)
accounts for the redundancy between ti and the cur-
rent summary S. In our experiments, redundancy is
the 1-gram cosine similarity between the sentence
ti and the current summary S. The trade-off be-
tween relevance and redundancy is controlled by
? ? [0, 1], which is tuned on development data.
McDonald (2007) proposed a non-greedy variant
of MMR that takes into account the redundancy be-
tween each pair of candidate sentences. This is cast
as a global optimization problem:
S? = argmax
S
? ??ti?S scorerel(ti) ?
(1 ? ?) ??ti,tj?S scorered(ti, tj), (3)
where scorerel(ti) , ?>relfrel(ti), scorered(ti, tj) ,
?>redfred(ti, tj), and frel(ti) and fred(ti, tj) are feature
vectors with corresponding learned weight vectors
?rel and ?red. He has shown how the relevance-based
method and the MMR framework (in the non-greedy
form of Eq. 3) can be cast as an ILP. By introducing
indicator variables ??i?i=1,...,M and ??ij?i,j=1,...,M
with the meanings
?i =
{ 1 if ti is to be extracted
0 otherwise
?ij =
{ 1 if ti and tj are both to be extracted
0 otherwise
(4)
one can reformulate Eq. 3 as an ILP with O(M2)
variables and constraints:
max
??i?,??ij?
? ??Mi=1 ?iscorerel(ti) ? (5)
(1 ? ?) ??Mi=1
?M
j=1 ?ijscorered(ti, tj),
subject to binary constraints ?i, ?ij ? {0, 1}, the
length constraint
?M
i=1 ?iNi ? J (where Ni is the
number of words of the ith sentence), and the fol-
lowing ?agreement constraints? for i, j = 1, . . . ,M
2
(that impose the logical relation ?ij = ?i ? ?j):
?ij ? ?i, ?ij ? ?j , ?ij ? ?i + ?j ? 1 (6)
Let us provide a compact representation of the pro-
gram in Eq. 5 that will be used later. Define our vec-
tor of parameters as ? , [??rel,?(1??)?red]. Pack-
ing all the feature vectors (one for each sentence, and
one for each pair of sentences) into a matrix F,
F ,
[ Frel 0
0 Fred
]
, (7)
with Frel , [frel(ti)]1?i?M and Fred ,
[fred(ti, tj)]1?i<j?M , and packing all the variables
?i and ?ij into a vector ?, the program in Eq. 5 can
be compactly written as
max
?
?>F?, (8)
subject to binary and linear constraints on ?. This
formulation requires O(M2) variables and con-
straints. If we do not penalize sentence redundancy,
the redundancy term may be dropped; in this simpler
case, F = Frel, the vector ? only contains the vari-
ables ??i?, and the program in Eq. 8 only requires
O(M) variables and constraints. Our method (to be
presented in ?4) will build on this latter formulation.
3 Sentence Compression
Despite its simplicity, extractive summarization has
a few shortcomings: for example, if the original sen-
tences are too long or embed several clauses, there
is no way of preventing lengthy sentences from ap-
pearing in the final summary. The sentence com-
pression framework (Knight and Marcu, 2000) aims
to select the best subsequence of words that still
yields a short, informative and grammatical sen-
tence. Such a sentence compressor is given a sen-
tence t , ?w1, . . . , wN ? as input and outputs a sub-
sequence of length L, c , ?wj1 , . . . , wjL?, with
1 ? j1 < . . . < jL ? N . We may represent
this output as a binary vector s of length N , where
sj = 1 iff word wj is included in the compression.
Note that there are O(2N ) possible subsequences.
3.1 Related Work
Past approaches to sentence compression include
a noisy channel formulation (Knight and Marcu,
2000; Daume? and Marcu, 2002), heuristic methods
that parse the sentence and then trim constituents ac-
cording to linguistic criteria (Dorr et al, 2003; Zajic
et al, 2006), a pure discriminative model (McDon-
ald, 2006), and an ILP formulation (Clarke and La-
pata, 2008). We next give an overview of the two
latter approaches.
McDonald (2006) uses the outputs of two parsers
(a phrase-based and a dependency parser) as fea-
tures in a discriminative model that decomposes
over pairs of consecutive words. Formally, given a
sentence t = ?w1, . . . , wN ?, the score of a compres-
sion c = ?wj1 , . . . , wjL? decomposes as:
score(c; t) = ?Ll=2 ?>f(t, jl?1, jl) (9)
where f(t, jl?1, jl) are feature vectors that depend
on the original sentence t and consecutive positions
jl?1 and jl, and ? is a learned weight vector. The
factorization in Eq. 9 allows exact decoding with dy-
namic programming.
Clarke and Lapata (2008) cast the problem as an
ILP. In their formulation, Eq. 9 may be expressed as:
score(c; t) =
N?
i=1
?i?>f(t, 0, i) +
N?
i=1
?i?>f(t, i, n + 1) +
N?1?
i=1
N?
j=i+1
?ij?>f(t, i, j), (10)
where ?i, ?i, and ?ij are additional binary variables
with the following meanings:
? ?i = 1 iff word wi starts the compression;
? ?i = 1 iff word wi ends the compression;
? ?ij = 1 iff words wi and wj appear consecutively
in the compression;
and subject to the following agreement constraints:
?N
i=1 ?i = 1?N
i=1 ?i = 1
sj = ?j +?j?1i=1 ?ij
si = ?i +?Nj=i+1 ?ij . (11)
3
This framework also allows the inclusion of con-
straints to enforce grammaticality.
To compress a sentence, one needs to maximize
the score in Eq. 10 subject to the constraints in
Eq. 11. Representing the variables through
? , ??1, . . . , ?N , ?1, . . . , ?N , ?11, . . . , ?NN ?
(12)
and packing the feature vectors into a matrix F, we
obtain the ILP
max
s,?
?>F? (13)
subject to linear and integer constraints on the vari-
ables s and ?. This particular formulation requires
O(N2) variables and constraints.
3.2 Proposed Method
We propose an alternative model for sentence com-
pression that may be formulated as an ILP, as in
Eq. 13, but with only O(N) variables and con-
straints. This formulation is based on the output of a
dependency parser.
Directed arcs in a dependency tree link pairs of
words, namely a head to its modifier. A dependency
parse tree is characterized by a set of labeled arcs
of the form (head, modifier, label); see Fig.1 for an
example. Given a sentence t = ?w1, . . . , wN ?, we
write i = pi(j) to denote that the ith word is the
head (the ?parent?) of the jth word; if j is the root,
we write pi(j) = 0. Let s be the binary vector de-
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al, 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al, 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm ? none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
1.1 Related Work
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and No?va?k, 2005; McDon-
ald et al, 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al, 2005a; McDonald et al, 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and Bo?ker (1997)
showing that the recognition problem for a mini-
$
Figure 1: A dependency parse for an English sentence;
example from McDonald and Satta (2007).
scribing a possible compression c for the sentence
t. For each word j, we consider four possible cases,
accounting for the inclusion or not of j and pi(j) in
the compression. We introduce (mutually exclusive)
binary variables ?j11, ?j10, ?j01, and ?j00 to indicate
eac of t ese cases, i.e., for a, b ? {0, 1},
?jab , sj = a ? spi(j) = b. (14)
Consider feature vectors f11(t, j), f10(t, j), f01(t, j),
and f00(t, j), that look at the surface sentence and at
the status of the word j and its head pi(j); these fea-
tures have corresponding weight vectors ?11, ?10,
?01, and ?00. The score of c is written as:
score(c; t) = ?Nj=1
?
a,b?{0,1} ?jab?>abfab(t, j)
= ?a,b?{0,1} ?>abFab?ab
= ?>F?, (15)
where Fab , [fab(t, 1), . . . , fab(t,N)], ?ab ,
(?jab)j=1,...,N , ? , (?11,?10,?01,?00), and F ,
Diag(F11,F10,F01,F00) (a block-diagonal matrix).
We have reached in Eq. 15 an ILP isomorphic to
the one in Eq. 13, but only with O(N) variables.
There are some agreement constraints between the
variables ? and s that reflect the logical relations in
Eq. 14; these may be written as linear inequalities
(cf. Eq. 6), yielding O(N) constraints.
Given this proposal and ?3.1, it is also straight-
forward to extend this model to include bigram fea-
tures as in Eq. 10; the combination of dependency
relation features and bigram features yields a model
that is more powerful than both models in Eq. 15 and
Eq. 10. Such a model is expressible as an ILP with
O(N2) variables and constraints, making use of the
variables s, ?, ?, ? and ?. In ?5, we compare the
performance of this model (called ?Bigram?) and the
model in Eq. 15 (called ?NoBigram?).2
4 Joint Compression and Extraction
We ext describe our joint mod l for sentenc com-
pression and extracti n. Let D , {t1, . . . , tM} be
a set of sentences as in ?2, each expressed as a se-
quence of words, ti , ?wi1, . . . , wiNi?. Following
?3, we represent a compression of ti as a binary vec-
tor si = ?si1, . . . , siNi?, where sij = 1 iff word wij
2It should be noted that more efficient decoders are possible
that do not require solving an ILP. In particular, inference in the
NoBigram variant can performed in polynomial time with dy-
namic programming algorithms that propagate messages along
the dependency parse tree; for the Bigram variant, dynamic pro-
gramming can till be employed with some additional storage.
Our ILP formulation, however, is more suited to the final goal
of performing document summarization (of which our sentence
compression model will be a component); furthermore, it also
allows the straightforward inclusion of global linguistic con-
straints, which, as shown by Clarke and Lapata (2008), can
greatly improve th grammaticality of the compressions.
4
is included in the compression. Now, define a sum-
mary of D as a set of sentences obtained by extract-
ing and compressing sentences from D. More pre-
cisely, let ?1, . . . , ?M be binary variables, one for
each sentence ti in D; define ?i = 1 iff a compres-
sion of sentence ti is used in the summary. A sum-
mary of D is then represented by the binary vari-
ables ??1, . . . , ?M , s1, . . . , sM ?. Notice that these
variables are redundant:
?i = 0 ? ?j ? {1, . . . , Ni} sij = 0, (16)
i.e., an empty compression means that the sentence
is not to be extracted. In the sequel, it will become
clear why this redundancy is convenient.
Most approaches up to now are concerned with ei-
ther extraction or compression, not both at the same
time. We will combine the extraction scores in Eq. 8
and the compression scores in Eq. 15 to obtain a sin-
gle, global optimization problem;3 we rename the
extraction features and parameters to Fe and ?e and
the compression features and parameters to Fc and
?c:
max
?,?,s
?Te Fe?+
?M
i=1 ?Tc Fci?i, (17)
subject to agreement constraints on the variables ?i
and si (see Eqs. 11 and 14), and new agreement con-
straints on the variables ? and s1, . . . , sM to enforce
the relation in Eq. 16:
sij ? ?i, ?i = 1, . . . ,M,?j = 1, . . . , Ni
?i ?
?Ni
j=1 sij , ?i = 1, . . . ,M
(18)
The constraint that the length of the summary cannot
exceed J words is encoded as:
?M
i=1
?Ni
j=1 sij ? J. (19)
All variables are further restricted to be binary. We
also want to avoid picking just a few words from
many sentences, which typically leads to ungram-
matical summaries. Hence it is desirable to obtain
?sparse? solutions with only a few sentences ex-
tracted and compressed (and most components of ?
are zero) To do so, we add the constraint
?Ni
j=1 sij ? ?i?Ni, i = 1, . . . ,M, (20)
3In what follows, we use the formulation in Eq. 8 with-
out the redundancy terms; however these can be included in
a straightforward way, naturally increasing the number of vari-
ables/constraints.
which states, for each sentence ti, that ti should be
ignored or have at least ?Ni words extracted. We fix
? = 0.8, enforcing compression rates below 80%.4
To learn the model parameters ? = ??e,?c?, we
can use a max-margin discriminative learning al-
gorithm like MIRA (Crammer and Singer, 2003),
which is quite effective and scalable. However, there
is not (to our knowledge) a single dataset of ex-
tracted and compressed sentences. Instead, as will
be described in Sec. 5.1, there are separate datasets
of extracted sentences, and datasets of compressed
sentences. Therefore, instead of globally learning
the model parameters, ? = ??e,?c?, we propose the
following strategy to learn them separately:
? Learn ??e using a corpus of extracted sentences,
? Learn ??c using a corpus of compressed sentences,
? Tune ? so that ? = ???e, ???c? has good perfor-
mance on development data. (This is necessary
since each set of weights is learned up to scaling.)
5 Experiments
5.1 Datasets, Evaluation and Environment
For our experiments, two datasets were used:
The DUC 2002 dataset. This is a collection of
newswire articles, comprised of 59 document clus-
ters. Each document within the collections (out of
a total of 567 documents) has one or two manually
created abstracts with approximately 100 words.5
Clarke?s dataset for sentence compression. This
is the dataset used by Clarke and Lapata (2008). It
contains manually created compressions of 82 news-
paper articles (1,433 sentences) from the British Na-
tional Corpus and the American News Text corpus.6
To evaluate the sentence compressor alone, we
measured the compression rate and the precision,
recall, and F1-measure (both macro and micro-
averaged) with respect to the ?gold? compressed
4There are alternative ways to achieve ?sparseness,? either
in a soft way, by adding a term ??Pi ?i to the objective, or
using a different hard constraint, like
P
i ?i ? K, to limit the
number of sentences from which to pick words.
5http://duc.nist.gov
6http://homepages.inf.ed.ac.uk/s0460084/data
5
Compression Micro-Av. Macro-Av.
Ratio P R F1 P R F1
HedgeTrimmer 57.64% 0.7099 0.5925 0.6459 0.7195 0.6547 0.6367
McDonald (2006) 71.40% 0.7444 0.7697 0.7568 0.7711 0.7852 0.7696
NoBigram 71.20% 0.7399 0.7626 0.7510 0.7645 0.7730 0.7604
Bigram 71.35% 0.7472 0.7720 0.7594 0.7737 0.7848 0.7710
Table 1: Results for sentence compression in the Clarke?s test dataset (441 sentences) for our implementation of the
baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model,
NoBigram and Bigram. The compression ratio associated with the reference compressed sentences in this dataset is
69.06%. In the rightmost column, the statistically indistinguishable best results are emboldened, based on a paired
t-test applied to the sequence of F1 measures (p < 0.01).
sentences, calculated on unigrams.7
To evaluate the full system, we used Rouge-N
(Lin and Hovy, 2002), a popular n-gram recall-
based automatic evaluation measure. This score
compares the summary produced by a system with
one or more valid reference summaries.
All our experiments were conducted on a PC with
a Intel dual-core processor with 2.66 GHz and 2 Gb
RAM memory. We used ILOG CPLEX, a commer-
cial integer programming solver. The interface with
CPLEX was coded in Java.
5.2 Sentence Compression
We split Clarke?s dataset into two partitions, one
used for training (1,188 sentences) and the other for
testing (441 sentences). This dataset includes one
manual compression for each sentence, that we use
as reference for evaluation purposes. Compression
ratio, i.e., the fraction of words included in the com-
pressed sentences, is 69.32% (micro-averaged over
the training partition).
For comparison, two baselines were imple-
mented: a simple compressor based on Hedge Trim-
mer, the headline generation system of Dorr et al
(2003) and Zajic et al (2006),8 and the discrimina-
7Notice that this evaluation score is not able to properly cap-
ture the grammaticality of the compression; this is a known is-
sue that typically is addressed by requiring human judgments.
8Hedge Trimmer applies a deterministic compression proce-
dure whose first step is to identify the lowest leftmost S node in
the parse tree that contains a NP and a VP; this node is taken as
the root of the compressed sentence (i.e., all words that are not
spanned by this node are discarded). Further steps described
by Dorr et al (2003) include removal of low content units, and
an ?iterative shortening? loop that keeps removing constituents
until a desired compression ratio is achieved. The best results
were obtained without iterative shortening, which is explained
by the fact that the selection of the lowest leftmost S node (first
tive model described by McDonald (2006), which
captures ?soft syntactic evidence? (we reproduced
the same set of features). Both systems require
a phrase-structure parser; we used Collins? parser
(Collins, 1999);9 the latter system also derives fea-
tures from a dependency parser; we used the MST-
Parser (McDonald et al, 2005).10
We implemented the two variants of our compres-
sor described in ?3.2.
NoBigram. This variant factors the compression
score as a sum over individual scores, each depend-
ing on the inclusion or not of each word and its head
in the compression (see Eq. 15). An upper bound of
70% was placed on the compression ratio. As stated
in ?3.2, inference amounts to solving an ILP with
O(N) variables and constraints, N being the sen-
tence length. We also used MSTParser to obtain the
dependency parse trees.
Bigram. This variant includes an extra term stand-
ing for a bigram score, which factors as a sum over
pairs of consecutive words. As in McDonald (2006),
we include features that depend on the ?in-between?
words in the original sentence that are to be omitted
in the compression.11 As stated in ?3.2, inference
through this model can be done by solving an ILP
with O(N2) variables and constraints.
step of the algorithm) already provides significant compression,
as illustrated in Table 1.
9http://people.csail.mit.edu/mcollins/code.
html
10http://sourceforge.net/projects/mstparser
11The major difference between this variant and model of
McDonald (2006) is that the latter employs ?soft syntactic ev-
idence? as input features, while we make the dependency rela-
tions part of the output features. All the non-syntactic features
are the same. Apart from this, notice that our variant does not
employ a phrase-structure parser.
6
For both variants, we used MSTParser to obtain
the dependency parse trees. The model parameters
are learned in a pure discriminative way through a
max-margin approach. We used the 1-best MIRA
algorithm (Crammer and Singer, 2003; McDonald
et al, 2005) for training; this is a fast online algo-
rithm that requires solving the inference problem at
each step. Although inference amounts to solving
an ILP, which in the worst case scales exponentially
with the size of the sentence, training the model is
in practice very fast for the NoBigram model (a few
minutes in the environment described in ?5.1) and
fast enough for the Bigram model (a couple of hours
using the same equipment). This is explained by the
fact that sentences don?t usually exceed a few tens
of words, and because of the structure of the ILPs,
whose constraint matrices are very sparse.
Table 1 depicts the micro- and macro-averaged
precision, recall and F1-measure. We can see that
both variants outperform the Hedge Trimmer base-
line by a great margin, and are in line with the sys-
tem of McDonald (2006); however, none of our vari-
ants employ a phrase-structure parser. We also ob-
serve that our simpler NoBigram variant, which uses
a linear-sized ILP, achieves results similar to these
two systems.
5.3 Joint Compression and Extraction
For the summarization task, we split the DUC 2002
dataset into a training partition (427 documents) and
a testing partition (140 documents). The training
partition was further split into a training and a de-
velopment set. We evaluated the performance of
Lead, Rel, and MMR as baselines (all are described
in ?2). Weights for Rel were learned via the SVM-
Rank algorithm;12 to create a gold-standard ranking,
we sorted the sentences by Rouge-2 score13 (with re-
spect to the human created summaries). We include
a Pipeline baseline as well, which ranks all sentences
by relevance, then includes their compressions (us-
ing the Bigram variant) while they fit into the sum-
mary.
We tested two variants of our joint model, com-
bining the Rel extraction model with (i) the NoBi-
12SVMRank is implemented in the SVMlight toolkit
(Joachims, 1999), http://svmlight.joachims.org.
13A similar system was implemented that optimizes the
Rouge-1 score instead, but it led to inferior performance.
Rouge-1 Rouge-2
Lead 0.384 ? 0.080 0.177 ? 0.083
Rel 0.389 ? 0.074 0.178 ? 0.080
MMR ? = 0.25 0.392 ? 0.071 0.178 ? 0.077
Pipeline 0.380 ? 0.073 0.173 ? 0.073
Rel + NoBigr ? = 1.5 0.403 ? 0.080 0.180 ? 0.082
Rel + Bigr ? = 4.0 0.403 ? 0.076 0.180 ? 0.076
Table 2: Results for sentence extraction in the DUC2002
dataset (140 documents). Bold indicates the best results
with statistical significance, according to a paired t-test
(p < 0.01); Rouge-2 scores of all systems except Pipeline
are indistinguishable according to the same test, with p >
0.05.
gram compression model (?3.2) and (ii) the Bigram
variant. Each variant was trained with the proce-
dure described in ?4. To keep tractability, the in-
ference ILP problem was relaxed (the binary con-
straints were relaxed to unit interval constraints) and
non-integer solution values were rounded to produce
a valid summary, both for training and testing.14
Whenever this procedure yielded a summary longer
than 100 words, we truncated it to fit the word limit.
Table 2 depicts the results of each of the above
systems in terms of Rouge-1 and Rouge-2 scores.
We can see that both variants of our system are able
to achieve the best results in terms of Rouge-1 and
Rouge-2 scores. The suboptimality of extracting and
compressing in separate stages is clear from the ta-
ble, as Pipeline performs worse than the pure ex-
tractive systems. We also note that the configuration
Rel + Bigram is not able to outperform Rel + No-
Bigram, despite being computationally more expen-
sive (about 25 minutes to process the whole test set,
against the 7 minutes taken by the Rel + NoBigram
variant). Fig. 2 exemplifies the summaries produced
by our system. We see that both variants were able
to include new pieces of information in the summary
without sacrificing grammaticality.
These results suggest that our system, being capa-
ble of performing joint sentence extraction and com-
pression to summarize a document, offers a power-
ful alternative to pure extractive systems. Finally, we
note that no labeled datasets currently exist on which
our full model could have been trained with super-
vision; therefore, although inference is performed
14See Martins et al (2009) for a study concerning the impact
of LP relaxations in the learning problem.
7
MMR baseline:
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, ?Oscar and Lu-
cinda?.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London?s ancient Guildhall.
Carey, who lives in Sydney with his wife and son, said in a brief
speech that like the other five finalists he had been asked to attend
with a short speech in his pocket in case he won.
Rel + NoBigram:
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, ?Oscar and Lu-
cinda?.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London?s ancient Guildhall.
The judges made their selection from 102 books published in Britain
in the past 12 months and which they read in their homes.
Carey, who lives in Sydney with his wife and son, said in a brief
speech that like the other five finalists he had been asked to attend
with a short speech in his pocket in case he won.
Rel + Bigram:
Australian novelist Peter Carey was awarded the coveted Booker
Prize for fiction Tuesday night for his love story, ?Oscar and Lu-
cinda?.
A panel of five judges unanimously announced the award of the
$26,250 prize after an 80-minute deliberation during a banquet at
London?s ancient Guildhall.
He was unsuccessful in the prize competition in 1985 when his
novel, ?Illywhacker,? was among the final six.
Carey called the award a ?great honor? and he thanked the prize
sponsors for ?provoking so much passionate discussion about liter-
ature perhaps there will be more tomorrow?.
Carey was the only non-Briton in the final six.
Figure 2: Summaries produced by the strongest base-
line (MMR) and the two variants of our system. Deleted
words are marked as such.
jointly, our training procedure had to learn sepa-
rately the extraction and the compression models,
and to tune a scalar parameter to trade off the two
models. We conjecture that a better model could
have been learned if a labeled dataset with extracted
compressed sentences existed.
6 Conclusion and Future Work
We have presented a summarization system that per-
forms sentence extraction and compression in a sin-
gle step, by casting the problem as an ILP. The sum-
mary optimizes an objective function that includes
both extraction and compression scores. Our model
encourages ?sparse? summaries that involve only a
few sentences. Experiments in newswire data sug-
gest that our system is a valid alternative to exist-
ing extraction-based systems. However, it is worth
noting that further evaluation (e.g., human judg-
ments) needs to be carried out to assert the quality
of our summaries, e.g., their grammaticality, some-
thing that the Rouge scores cannot fully capture.
Future work will address the possibility of in-
cluding linguistic features and constraints to further
improve the grammaticality of the produced sum-
maries.
Another straightforward extension is the inclusion
of a redundancy term and a query relevance term
in the objective function. For redundancy, a simi-
lar idea of that of McDonald (2007) can be applied,
yielding a ILP with O(M2 + N) variables and con-
straints (M being the number of sentences and N the
total number of words). However, such model will
take into account the redundancy among the origi-
nal sentences and not their compressions; to model
the redundancy accross compressions, a possibil-
ity is to consider a linear redundancy score (similar
to cosine similarity, but without the normalization),
which would result in an ILP with O(N +?i P 2i )
variables and constraints, where Pi ? M is the num-
ber of sentences in which word wi occurs; this is no
worse than O(M2N).
We also intend to model discourse, which, as
shown by Daume? and Marcu (2002), plays an im-
portant role in document summarization. Another
future direction is to extend our ILP formulations
to more sophisticated models that go beyond word
deletion, like the ones proposed by Cohn and Lapata
(2008).
Acknowledgments
The authors thank the anonymous reviewers for helpful
comments, Yiming Yang for interesting discussions, and
Dipanjan Das and Sourish Chaudhuri for providing their
code. This research was supported by a grant from FCT
through the CMU-Portugal Program and the Informa-
tion and Communications Technologies Institute (ICTI)
at CMU, and also by Priberam Informa?tica.
8
References
C. Aone, M. E. Okurowski, J. Gorlinsky, and B. Larsen.
1999. A trainable summarizer with knowledge ac-
quired from robust nlp techniques. In Advances in Au-
tomatic Text Summarization. MIT Press.
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature?an experiment. IBM Journal of Re-
search Development, 2(4):354?361.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proc. of SIGIR.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression an integer linear programming
approach. JAIR, 31:399?429.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proc. COLING.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
K. Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. J. Mach.
Learn. Res., 3:951?991.
H. Daume? and D. Marcu. 2002. A noisy-channel model
for document compression. In Proc. of ACL.
H. Daume?. 2006. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D. thesis,
University of Southern California.
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge trim-
mer: A parse-and-trim approach to headline gener-
ation. In Proc. of HLT-NAACL Text Summarization
Workshop and DUC.
H. P. Edmundson. 1969. New methods in automatic ex-
tracting. Journal of the ACM, 16(2):264?285.
R. Jin. 2003. Statistical Approaches Toward Title Gener-
ation. Ph.D. thesis, Carnegie Mellon University.
T. Joachims. 1999. Making large-scale SVM learning
practical. In Advances in Kernel Methods - Support
Vector Learning. MIT Press.
K. Knight and D. Marcu. 2000. Statistics-based
summarization?step one: Sentence compression. In
Proc. of AAAI/IAAI.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proc. of SIGIR.
C.-Y. Lin and E. Hovy. 2002. Manual and automatic
evaluation of summaries. In Proc. of the ACL Work-
shop on Automatic Summarization.
C.-Y. Lin. 2003. Improving summarization performance
by sentence compression-a pilot study. In Proc. of the
Int. Workshop on Inf. Ret. with Asian Languages.
H. P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBM Journal of Research Development,
2(2):159?165.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Polyhedral outer approximations with application to
natural language parsing. In Proc. of ICML.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. of EACL.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proc. of
ECIR.
M. Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proc. of the ACL Workshop on
Automatic Summarization.
D. R. Radev and K. McKeown. 1998. Generating natural
language summaries from multiple on-line sources.
Computational Linguistics, 24(3):469?500.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple documents:
sentence extraction, utility-based evaluation, and user
studies. In Proc. of the NAACL-ANLP Workshop on
Automatic Summarization.
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006.
Sentence compression as a component of a multi-
document summarization system. In Proc. of the ACL
DUC Workshop.
9
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34?44,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Turbo Parsers: Dependency Parsing by Approximate Variational Inference
Andre? F. T. Martins?? Noah A. Smith? Eric P. Xing?
?School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{afm,nasmith,epxing}@cs.cmu.edu
Pedro M. Q. Aguiar?
?Instituto de Sistemas e Robo?tica
Instituto Superior Te?cnico
Lisboa, Portugal
aguiar@isr.ist.utl.pt
Ma?rio A. T. Figueiredo?
?Instituto de Telecomunicac?o?es
Instituto Superior Te?cnico
Lisboa, Portugal
mtf@lx.it.pt
Abstract
We present a unified view of two state-of-the-
art non-projective dependency parsers, both
approximate: the loopy belief propagation
parser of Smith and Eisner (2008) and the re-
laxed linear program of Martins et al (2009).
By representing the model assumptions with
a factor graph, we shed light on the optimiza-
tion problems tackled in each method. We also
propose a new aggressive online algorithm to
learn the model parameters, which makes use
of the underlying variational representation.
The algorithm does not require a learning rate
parameter and provides a single framework for
a wide family of convex loss functions, includ-
ing CRFs and structured SVMs. Experiments
show state-of-the-art performance for 14 lan-
guages.
1 Introduction
Feature-rich discriminative models that break local-
ity/independence assumptions can boost a parser?s
performance (McDonald et al, 2006; Huang, 2008;
Finkel et al, 2008; Smith and Eisner, 2008; Martins
et al, 2009; Koo and Collins, 2010). Often, infer-
ence with such models becomes computationally in-
tractable, causing a demand for understanding and
improving approximate parsing algorithms.
In this paper, we show a formal connection be-
tween two recently-proposed approximate inference
techniques for non-projective dependency parsing:
loopy belief propagation (Smith and Eisner, 2008)
and linear programming relaxation (Martins et al,
2009). While those two parsers are differently moti-
vated, we show that both correspond to inference in
a factor graph, and both optimize objective functions
over local approximations of the marginal polytope.
The connection is made clear by writing the explicit
declarative optimization problem underlying Smith
and Eisner (2008) and by showing the factor graph
underlying Martins et al (2009). The success of
both approaches parallels similar approximations in
other fields, such as statistical image processing and
error-correcting coding. Throughtout, we call these
turbo parsers.1
Our contributions are not limited to dependency
parsing: we present a general method for inference
in factor graphs with hard constraints (?2), which
extends some combinatorial factors considered by
Smith and Eisner (2008). After presenting a geo-
metric view of the variational approximations un-
derlying message-passing algorithms (?3), and clos-
ing the gap between the two aforementioned parsers
(?4), we consider the problem of learning the model
parameters (?5). To this end, we propose an ag-
gressive online algorithm that generalizes MIRA
(Crammer et al, 2006) to arbitrary loss functions.
We adopt a family of losses subsuming CRFs (Laf-
ferty et al, 2001) and structured SVMs (Taskar et
al., 2003; Tsochantaridis et al, 2004). Finally, we
present a technique for including features not at-
tested in the training data, allowing for richer mod-
els without substantial runtime costs. Our experi-
ments (?6) show state-of-the-art performance on de-
pendency parsing benchmarks.
1The name stems from ?turbo codes,? a class of high-
performance error-correcting codes introduced by Berrou et al
(1993) for which decoding algorithms are equivalent to running
belief propagation in a graph with loops (McEliece et al, 1998).
34
2 Structured Inference and Factor Graphs
Denote by X a set of input objects from which we
want to infer some hidden structure conveyed in an
output set Y. Each input x ? X (e.g., a sentence)
is associated with a set of candidate outputs Y(x) ?
Y (e.g., parse trees); we are interested in the case
where Y(x) is a large structured set.
Choices about the representation of elements of
Y(x) play a major role in algorithm design. In
many problems, the elements of Y(x) can be rep-
resented as discrete-valued vectors of the form y =
?y1, . . . , yI?, each yi taking values in a label set Yi.
For example, in unlabeled dependency parsing, I is
the number of candidate dependency arcs (quadratic
in the sentence length), and each Yi = {0, 1}. Of
course, the yi are highly interdependent.
Factor Graphs. Probabilistic models like CRFs
(Lafferty et al, 2001) assume a factorization of the
conditional distribution of Y ,
Pr(Y = y | X = x) ?
?
C?C ?C(x,yC), (1)
where each C ? {1, . . . , I} is a factor, C is the set
of factors, each yC , ?yi?i?C denotes a partial out-
put assignment, and each ?C is a nonnegative po-
tential function that depends on the output only via
its restriction to C. A factor graph (Kschischang
et al, 2001) is a convenient representation for the
factorization in Eq. 1: it is a bipartite graph Gx com-
prised of variable nodes {1, . . . , I} and factor nodes
C ? C, with an edge connecting the ith variable
node and a factor node C iff i ? C. Hence, the fac-
tor graph Gx makes explicit the direct dependencies
among the variables {y1, . . . , yI}.
Factor graphs have been used for several NLP
tasks, such as dependency parsing, segmentation,
and co-reference resolution (Sutton et al, 2007;
Smith and Eisner, 2008; McCallum et al, 2009).
Hard and Soft Constraint Factors. It may be
the case that valid outputs are a proper subset of
Y1 ? ? ? ? ? YI?for example, in dependency pars-
ing, the entries of the output vector y must jointly
define a spanning tree. This requires hard constraint
factors that rule out forbidden partial assignments
by mapping them to zero potential values. See Ta-
ble 1 for an inventory of hard constraint factors used
in this paper. Factors that are not of this special kind
are called soft factors, and have strictly positive po-
tentials. We thus have a partition C = Chard ? Csoft.
We let the soft factor potentials take the form
?C(x,yC) , exp(?>?C(x,yC)), where ? ? R
d
is a vector of parameters (shared across factors) and
?C(x,yC) is a local feature vector. The conditional
distribution of Y (Eq. 1) thus becomes log-linear:
Pr?(y|x) = Zx(?)
?1 exp(?>?(x,y)), (2)
where Zx(?) ,
?
y??Y(x) exp(?
>?(x,y?)) is the
partition function, and the features decompose as:
?(x,y) ,
?
C?Csoft
?C(x,yC). (3)
Dependency Parsing. Smith and Eisner (2008)
proposed a factor graph representation for depen-
dency parsing (Fig. 1). The graph has O(n2) vari-
able nodes (n is the sentence length), one per candi-
date arc a , ?h,m? linking a head h and modifier
m. Outputs are binary, with ya = 1 iff arc a belongs
to the dependency tree. There is a hard factor TREE
connected to all variables, that constrains the overall
arc configurations to form a spanning tree. There is a
unary soft factor per arc, whose log-potential reflects
the score of that arc. There are also O(n3) pair-
wise factors; their log-potentials reflect the scores
of sibling and grandparent arcs. These factors cre-
ate loops, thus calling for approximate inference.
Without them, the model is arc-factored, and ex-
act inference in it is well studied: finding the most
probable parse tree takes O(n3) time with the Chu-
Liu-Edmonds algorithm (McDonald et al, 2005),2
and computing posterior marginals for all arcs takes
O(n3) time via the matrix-tree theorem (Smith and
Smith, 2007; Koo et al, 2007).
Message-passing algorithms. In general
factor graphs, both inference problems?
obtaining the most probable output (the MAP)
argmaxy?Y(x) Pr?(y|x), and computing the
marginals Pr?(Yi = yi|x)?can be addressed
with the belief propagation (BP) algorithm (Pearl,
1988), which iteratively passes messages between
variables and factors reflecting their local ?beliefs.?
2There is a faster but more involvedO(n2) algorithm due to
Tarjan (1977).
35
A general binary factor: ?C(v1, . . . , vn) =
?
1 v1, . . . , vn ? SC
0 otherwise,
where SC ? {0, 1}n.
?Message-induced distribution: ? , ?mj?C?j=1,...,n ? Partition function: ZC(?) ,
P
?v1,...,vn??SC
Qn
i=1m
vi
i?C
?Marginals: MARGi(?) , Pr?{Vi = 1|?V1, . . . , Vn? ? SC} ?Max-marginals: MAX-MARGi,b(?) , maxv?SC Pr?(v|vi = b)
? Sum-prod.: mC?i = m
?1
i?C ? MARGi(?)/(1? MARGi(?)) ?Max-prod.: mC?i = m
?1
i?C ? MAX-MARGi,1(?)/MAX-MARGi,0(?)
? Local agreem. constr.: z ? conv SC , where z = ??i(1)?ni=1 ? Entropy: HC = logZC(?)?
Pn
i=1 MARGi(?) logmi?C
TREE ?TREE(?ya?a?A) =
?
1 y ? Ytree (i.e., {a ? A | ya = 1} is a directed spanning tree)
0 otherwise,
where A is the set of candidate arcs.
? Partition function Ztree(?) and marginals ?MARGa(?)?a?A computed via the matrix-tree theorem, with ? , ?ma?TREE?a?A
? Sum-prod.: mTREE?a = m
?1
a?TREE ? MARGa(?)/(1? MARGa(?))
?Max-prod.: mTREE?a = m
?1
a?TREE ? MAX-MARGa,1(?)/MAX-MARGa,0(?), where MAX-MARGa,b(?) , maxy?Ytree Pr?(y|ya = b)
? Local agreem. constr.: z ? Ztree, where Ztree , convYtree is the arborescence polytope
? Entropy: Htree = logZtree(?)?
P
a?A MARGa(?) logma?TREE
XOR (?one-hot?) ?XOR(v1, . . . , vn) =
?
1
Pn
i=1 vi = 1
0 otherwise.
? Sum-prod.: mXOR?i =
?P
j 6=imj?XOR
??1
?Max-prod.: mXOR?i =
`
maxj 6=imj?XOR
??1
? Local agreem. constr.:
P
i zi = 1, zi ? [0, 1],?i ?HXOR = ?
P
i(mi?XOR/
P
j mj?XOR) log(mi?XOR/
P
j mj?XOR)
OR ?OR(v1, . . . , vn) =
?
1
Pn
i=1 vi ? 1
0 otherwise.
? Sum-prod.: mOR?i =
?
1?
Q
j 6=i(1 +mj?OR)
?1
??1
?Max-prod.: mOR?i = max{1,minj 6=im
?1
j?OR}
? Local agreem. constr.:
P
i zi ? 1, zi ? [0, 1],?i
OR-WITH-OUTPUT ?OR-OUT(v1, . . . , vn) =
?
1 vn =
Wn?1
i=1 vi
0 otherwise.
? Sum-prod.: mOR-OUT?i =
( ?
1? (1?m?1n?OR-OUT)
Q
j 6=i,n(1 +mj?OR-OUT)
?1
??1
i < n
Q
j 6=n(1 +mj?OR-OUT)? 1 i = n.
?Max-prod.: mOR-OUT?i =
(
min mn?OR-OUT
Q
j 6=i,n max{1,mj?OR-OUT},max{1,minj 6=i,nm
?1
j?OR-OUT}
o
i < n
Q
j 6=n max{1,mj?OR-OUT}min{1,maxj 6=nmj?OR-OUT} i = n.
Table 1: Hard constraint factors, their potentials, messages, and entropies. The top row shows expressions for a
general binary factor: each outgoing message is computed from incoming marginals (in the sum-product case), or
max-marginals (in the max-product case); the entropy of the factor (see ?3) is computed from these marginals and the
partition function; the local agreement constraints (?4) involve the convex hull of the set SC of allowed configurations
(see footnote 5). The TREE, XOR, OR and OR-WITH-OUTPUT factors allow tractable computation of all these quantities
(rows 2?5). Two of these factors (TREE and XOR) had been proposed by Smith and Eisner (2008); we provide further
information (max-product messages, entropies, and local agreement constraints). Factors OR and OR-WITH-OUTPUT
are novel to the best of our knowledge. This inventory covers many cases, since the above formulae can be extended
to the case where some inputs are negated: just replace the corresponding messages by their reciprocal, vi by 1? vi,
etc. This allows building factors NAND (an OR factor with negated inputs), IMPLY (a 2-input OR with the first input
negated), and XOR-WITH-OUTPUT (an XOR factor with the last input negated).
In sum-product BP, the messages take the form:3
Mi?C(yi) ?
?
D 6=CMD?i(yi) (4)
MC?i(yi) ?
?
yC?yi
?C(yC)
?
j 6=iMj?C(yj). (5)
In max-product BP, the summation in Eq. 5 is re-
placed by a maximization. Upon convergence, vari-
able and factor beliefs are computed as:
?i(yi) ?
?
CMC?i(yi) (6)
?C(yC) ? ?C(yC)
?
iMi?C(yi). (7)
BP is exact when the factor graph is a tree: in the
sum-product case, the beliefs in Eqs. 6?7 correspond
3We employ the standard ? notation, where a summa-
tion/maximization indexed by yC ? yi means that it is over
all yC with the i-th component held fixed and set to yi.
to the true marginals, and in the max-product case,
maximizing each ?i(yi) yields the MAP output. In
graphs with loops, BP is an approximate method, not
guaranteed to converge, nicknamed loopy BP. We
highlight a variational perspective of loopy BP in ?3;
for now we consider algorithmic issues. Note that
computing the factor-to-variable messages for each
factorC (Eq. 5) requires a summation/maximization
over exponentially many configurations. Fortu-
nately, for all the hard constraint factors in rows 3?5
of Table 1, this computation can be done in linear
time (and polynomial for the TREE factor)?this ex-
tends results presented in Smith and Eisner (2008).4
4The insight behind these speed-ups is that messages on
binary-valued potentials can be expressed as MC?i(yi) ?
36
TREE
1
ARC
(T,RE)
SIB
(T1RE1RE)1 2
SIB
(T1RE1RE)1 3
SIB
(T1RE1RE)2 3GRAND
(A,T1RE)1
2
ARC
(T,RE) 3
ARC
(T,RE)ARC(A,T)
Figure 1: Factor graph corresponding to the dependency
parsing model of Smith and Eisner (2008) with sibling
and grandparent features. Circles denote variable nodes,
and squares denote factor nodes. Note the loops created
by the inclusion of pairwise factors (GRAND and SIB).
In Table 1 we present closed-form expressions
for the factor-to-variable message ratios mC?i ,
MC?i(1)/MC?i(0) in terms of their variable-to-
factor counterparts mi?C , Mi?C(1)/Mi?C(0);
these ratios are all that is necessary when the vari-
ables are binary. Detailed derivations are presented
in an extended version of this paper (Martins et al,
2010b).
3 Variational Representations
Let Px , {Pr?(.|x) | ? ? Rd} be the family of all
distributions of the form in Eq. 2. We next present
an alternative parametrization for the distributions in
Px in terms of factor marginals. We will see that
each distribution can be seen as a point in the so-
called marginal polytope (Wainwright and Jordan,
2008); this will pave the way for the variational rep-
resentations to be derived next.
Parts and Output Indicators. A part is a pair
?C,yC?, where C is a soft factor and yC a partial
output assignment. We let R = {?C,yC? | C ?
Csoft,yC ?
?
i?C Yi} be the set of all parts. Given
an output y? ? Y(x), a part ?C,yC? is said to be ac-
tive if it locally matches the output, i.e., if yC = y?C .
Any output y? ? Y(x) can be mapped to a |R|-
dimensional binary vector ?(y?) indicating which
parts are active, i.e., [?(y?)]?C,yC? = 1 if yC = y
?
C
Pr{?C(YC) = 1|Yi = yi} and MC?i(yi) ?
max?C(yC)=1 Pr{YC = yC |Yi = yi}, respectively for the
sum-product and max-product cases; these probabilities are in-
duced by the messages in Eq. 4: for an event A ?
Q
i?C Yi,
Pr{YC ? A} ,
P
yC
I(yC ? A)
Q
i?CMi?C(yi).
and 0 otherwise; ?(y?) is called the output indicator
vector. This mapping allows decoupling the feature
vector in Eq. 3 as the product of an input matrix and
an output vector:
?(x,y) =
?
C?Csoft
?C(x,yC) = F(x)?(y), (8)
where F(x) is a d-by-|R| matrix whose columns
contain the part-local feature vectors ?C(x,yC).
Observe, however, that not every vector in {0, 1}|R|
corresponds necessarily to a valid output in Y(x).
Marginal Polytope. Moving to vector representa-
tions of outputs leads naturally to a geometric view
of the problem. The marginal polytope is the convex
hull5 of all the ?valid? output indicator vectors:
M(Gx) , conv{?(y) | y ? Y(x)}.
Note that M(Gx) only depends on the factor graph
Gx and the hard constraints (i.e., it is independent of
the parameters ?). The importance of the marginal
polytope stems from two facts: (i) each vertex of
M(Gx) corresponds to an output in Y(x); (ii) each
point in M(Gx) corresponds to a vector of marginal
probabilities that is realizable by some distribution
(not necessarily in Px) that factors according to Gx.
Variational Representations. We now describe
formally how the points in M(Gx) are linked to the
distributions in Px. We extend the ?canonical over-
complete parametrization? case, studied by Wain-
wright and Jordan (2008), to our scenario (common
in NLP), where arbitrary features are allowed and
the parameters are tied (shared by all factors). Let
H(Pr?(.|x)) , ?
?
y?Y(x) Pr?(y|x) log Pr?(y|x)
denote the entropy of Pr?(.|x), and E?[.] the ex-
pectation under Pr?(.|x). The component of ? ?
M(Gx) indexed by part ?C,yC? is denoted ?C(yC).
Proposition 1. There is a map coupling each distri-
bution Pr?(.|x) ? Px to a unique ? ? M(Gx) such
that E?[?(Y )] = ?. Define H(?) , H(Pr?(.|x))
if some Pr?(.|x) is coupled to ?, and H(?) = ??
if no such Pr?(.|x) exists. Then:
1. The following variational representation for the
log-partition function (mentioned in Eq. 2) holds:
logZx(?) = max
??M(Gx)
?>F(x)? +H(?). (9)
5The convex hull of {z1, . . . , zk} is the set of points that can
be written as
Pk
i=1 ?izi, where
Pk
i=1 ?i = 1 and each ?i ? 0.
37
Parameter?space Factor?log-potentials?
space???????
Marginal?polytope?
Figure 2: Dual parametrization of the distributions in
Px. Our parameter space (left) is first linearly mapped to
the space of factor log-potentials (middle). The latter is
mapped to the marginal polytope M(Gx) (right). In gen-
eral only a subset of M(Gx) is reachable from our param-
eter space. Any distribution in Px can be parametrized by
a vector ? ? Rd or by a point ? ?M(Gx).
2. The problem in Eq. 9 is convex and its solution
is attained at the factor marginals, i.e., there is a
maximizer ?? s.t. ??C(yC) = Pr?(YC = yC |x)
for each C ? C. The gradient of the log-partition
function is? logZx(?) = F(x)??.
3. The MAP y? , argmaxy?Y(x) Pr?(y|x) can be
obtained by solving the linear program
?? , ?(y?) = argmax
??M(Gx)
?>F(x)?. (10)
A proof of this proposition can be found in Mar-
tins et al (2010a). Fig. 2 provides an illustration of
the dual parametrization implied by Prop. 1.
4 Approximate Inference & Turbo Parsing
We now show how the variational machinery just
described relates to message-passing algorithms and
provides a common framework for analyzing two re-
cent dependency parsers. Later (?5), Prop. 1 is used
constructively for learning the model parameters.
4.1 Loopy BP as a Variational Approximation
For general factor graphs with loops, the marginal
polytope M(Gx) cannot be compactly specified and
the entropy term H(?) lacks a closed form, render-
ing exact optimizations in Eqs. 9?10 intractable. A
popular approximate algorithm for marginal infer-
ence is sum-product loopy BP, which passes mes-
sages as described in ?2 and, upon convergence,
computes beliefs via Eqs. 6?7. Were loopy BP exact,
these beliefs would be the true marginals and hence
a point in the marginal polytope M(Gx). However,
this need not be the case, as elucidated by Yedidia et
al. (2001) and others, who first analyzed loopy BP
from a variational perspective. The following two
approximations underlie loopy BP:
? The marginal polytope M(Gx) is approximated by
the local polytope L(Gx). This is an outer bound;
its name derives from the fact that it only imposes
local agreement constraints ?i, yi ? Yi, C ? C:
?
yi
?i(yi) = 1,
?
yC?yi
?C(yC) = ?i(yi). (11)
Namely, it is characterized by L(Gx) , {? ?
R|R|+ | Eq. 11 holds ?i, yi ? Yi, C ? C}. The
elements of L(Gx) are called pseudo-marginals.
Clearly, the true marginals satisfy Eq. 11, and
therefore M(Gx) ? L(Gx).
? The entropy H is replaced by its Bethe approx-
imation HBethe(? ) ,
?I
i=1(1 ? di)H(? i) +?
C?CH(?C), where di = |{C | i ? C}| is the
number of factors connected to the ith variable,
H(? i) , ?
?
yi
?i(yi) log ?i(yi) and H(?C) ,
?
?
yC
?C(yC) log ?C(yC).
Any stationary point of sum-product BP is a lo-
cal optimum of the variational problem in Eq. 9
with M(Gx) replaced by L(Gx) and H replaced by
HBethe (Yedidia et al, 2001). Note however that
multiple optima may exist, since HBethe is not nec-
essarily concave, and that BP may not converge.
Table 1 shows closed form expressions for the
local agreement constraints and entropies of some
hard-constraint factors, obtained by invoking Eq. 7
and observing that ?C(yC) must be zero if configu-
ration yC is forbidden. See Martins et al (2010b).
4.2 Two Dependency Turbo Parsers
We next present our main contribution: a formal
connection between two recent approximate depen-
dency parsers, which at first sight appear unrelated.
Recall that (i) Smith and Eisner (2008) proposed a
factor graph (Fig. 1) in which they run loopy BP,
and that (ii) Martins et al (2009) approximate pars-
ing as the solution of a linear program. Here, we
fill the blanks in the two approaches: we derive ex-
plicitly the variational problem addressed in (i) and
we provide the underlying factor graph in (ii). This
puts the two approaches side-by-side as approximate
methods for marginal and MAP inference. Since
both rely on ?local? approximations (in the sense
38
of Eq. 11) that ignore the loops in their graphical
models, we dub them turbo parsers by analogy with
error-correcting turbo decoders (see footnote 1).
Turbo Parser #1: Sum-Product Loopy BP. The
factor graph depicted in Fig. 1?call it Gx?includes
pairwise soft factors connecting sibling and grand-
parent arcs.6 We next characterize the local polytope
L(Gx) and the Bethe approximationHBethe inherent
in Smith and Eisner?s loopy BP algorithm.
Let A be the set of candidate arcs, and P ?
A2 the set of pairs of arcs that have factors. Let
? = ??A, ?P ? with ?A = ??a?a?A and ?P =
??ab??a,b??P . Since all variables are binary, we may
write, for each a ? A, ?a(1) = za and ?a(0) =
1 ? za, where za is a variable constrained to [0, 1].
Let zA , ?za?a?A; the local agreement constraints
at the TREE factor (see Table 1) are written as zA ?
Ztree(x), where Ztree(x) is the arborescence poly-
tope, i.e., the convex hull of all incidence vectors
of dependency trees (Martins et al, 2009). It is
straightforward to write a contingency table and ob-
tain the following local agreement constraints at the
pairwise factors:
?ab(1, 1) = zab, ?ab(0, 0) = 1? za ? zb + zab
?ab(1, 0) = za ? zab, ?ab(0, 1) = zb ? zab.
Noting that all these pseudo-marginals are con-
strained to the unit interval, one can get rid of all
variables ?ab and write everything as
za ? [0, 1], zb ? [0, 1], zab ? [0, 1],
zab ? za, zab ? zb, zab ? za + zb ? 1,
(12)
inequalities which, along with zA ? Ztree(x), de-
fine the local polytope L(Gx). As for the factor en-
tropies, start by noting that the TREE-factor entropy
Htree can be obtained in closed form by computing
the marginals z?A and the partition function Zx(?)
(via the matrix-tree theorem) and recalling the vari-
ational representation in Eq. 9, yielding Htree =
logZx(?)? ?>F(x)z?A. Some algebra allows writ-
ing the overall Bethe entropy approximation as:
HBethe(? ) = Htree(zA)?
?
?a,b??P
Ia;b(za, zb, zab), (13)
where we introduced the mutual information asso-
ciated with each pairwise factor, Ia;b(za, zb, zab) =
6Smith and Eisner (2008) also proposed other variants with
more factors, which we omit for brevity.
TRE1
ACTRTE(
TRE1
A1TRTE(
,)SIARTE(
,)SIB23GRNDARTE(
TRE1AATTE( TRE1AAT1TE(
,)SI
AATE(
TRE1BNDRS)AATE(
)
AATR(TRE1AATRTE(
TRE1BG,RGDB)AATRTE(
)ACTR(
)A1TR(
GRDB,)DS
AR(
E
E
E
Figure 3: Details of the factor graph underlying the parser
of Martins et al (2009). Dashed circles represent auxil-
iary variables. See text and Table 1.
?
ya,yb
?ab(ya, yb) log
?ab(ya,yb)
?a(ya)?b(yb)
. The approximate
variational expression becomes logZx(?) ?
maxz ?
>F(x)z +Htree(zA)?
?
?a,b??P
Ia;b(za, zb, zab)
s.t. zab ? za, zab ? zb,
zab ? za + zb ? 1, ??a, b? ? P,
zA ? Ztree,
(14)
whose maximizer corresponds to the beliefs re-
turned by the Smith and Eisner?s loopy BP algorithm
(if it converges).
Turbo Parser #2: LP-Relaxed MAP. We now
turn to the concise integer LP formulation of Mar-
tins et al (2009). The formulation is exact but NP-
hard, and so an LP relaxation is made there by drop-
ping the integer constraints. We next construct a fac-
tor graph G?x and show that the LP relaxation corre-
sponds to an optimization of the form in Eq. 10, with
the marginal polytope M(G?x) replaced by L(G
?
x).
G?x includes the following auxiliary variable
nodes: path variables ?pij?i=0,...,n,j=1,...,n, which
indicate whether word j descends from i in the de-
pendency tree, and flow variables ?fka ?a?A,k=1,...,n,
which evaluate to 1 iff arc a ?carries flow? to k,
i.e., iff there is a path from the root to k that passes
through a. We need to seed these variables imposing
p0k = pkk = 1,?k, f
h
?h,m? = 0, ?h,m; (15)
i.e., any word descends from the root and from it-
self, and arcs leaving a word carry no flow to that
39
word. This can be done with unary hard constraint
factors. We then replace the TREE factor in Fig. 1 by
the factors shown in Fig. 3:
? O(n) XOR factors, each connecting all arc vari-
ables of the form {?h,m?}h=0,...,n. These ensure
that each word has exactly one parent. Each factor
yields a local agreement constraint (see Table 1):
?n
h=0 z?h,m? = 1, m ? {1, . . . , n} (16)
? O(n3) IMPLY factors, each expressing that if an
arc carries flow, then that arc must be active. Such
factors are OR factors with the first input negated,
hence, the local agreement constraints are:
fka ? za, a ? A, k ? {1, . . . , n}. (17)
? O(n2) XOR-WITH-OUTPUT factors, which im-
pose the constraint that each path variable pmk is
active if and only if exactly one incoming arc in
{?h,m?}h=0,...,n carries flow to k. Such factors
are XOR factors with the last input negated, and
hence their local constraints are:
pmk =
?n
h=0 f
k
?h,m?, m, k ? {1, . . . , n} (18)
? O(n2) XOR-WITH-OUTPUT factors to impose the
constraint that words don?t consume other words?
commodities; i.e., if h 6= k and k 6= 0, then there
is a path from h to k iff exactly one outgoing arc
in {?h,m?}m=1,...,n carries flow to k:
phk =
?n
m=1 f
k
?h,m?, h, k ? {0, . . . , n}, k /? {0, h}.
(19)
L(G?x) is thus defined by the constraints in Eq. 12
and 15?19. The approximate MAP problem, that
replaces M(G?x) by L(G
?
x) in Eq. 10, thus becomes:
maxz,f ,p ?
>F(x)z
s.t. Eqs. 12 and 15?19 are satisfied.
(20)
This is exactly the LP relaxation considered by Mar-
tins et al (2009) in their multi-commodity flow
model, for the configuration with siblings and grand-
parent features.7 They also considered a config-
uration with non-projectivity features?which fire
if an arc is non-projective.8 That configuration
can also be obtained here if variables {n?h,m?} are
7To be precise, the constraints of Martins et al (2009) are
recovered after eliminating the path variables, via Eqs. 18?19.
8An arc ?h,m? is non-projective if there is some word in its
span not descending from h (Kahane et al, 1998).
added to indicate non-projective arcs and OR-WITH-
OUTPUT hard constraint factors are inserted to en-
force n?h,m? = z?h,m??
?
min(h,m)<j<min(h,m) ?phj .
Details are omitted for space.
In sum, although the approaches of Smith and Eis-
ner (2008) and Martins et al (2009) look very dif-
ferent, in reality both are variational approximations
emanating from Prop. 1, respectively for marginal
and MAP inference. However, they operate on dis-
tinct factor graphs, respectively Figs. 1 and 3.9
5 Online Learning
Our learning algorithm is presented in Alg. 1. It is a
generalized online learner that tackles `2-regularized
empirical risk minimization of the form
min??Rd
?
2???
2 + 1m
?m
i=1 L(?;xi,yi), (21)
where each ?xi,yi? is a training example, ? ? 0 is
the regularization constant, and L(?;x,y) is a non-
negative convex loss. Examples include the logistic
loss used in CRFs (? log Pr?(y|x)) and the hinge
loss of structured SVMs (maxy??Y(x) ?
>(?(x,y?)?
?(x,y)) + `(y?,y) for some cost function `). These
are both special cases of the family defined in Fig. 4,
which also includes the structured perceptron?s loss
(? ? ?, ? = 0) and the softmax-margin loss of
Gimpel and Smith (2010; ? = ? = 1).
Alg. 1 is closely related to stochastic or online
gradient descent methods, but with the key advan-
tage of not needing a learning rate hyperparameter.
We sketch the derivation of Alg. 1; full details can
be found in Martins et al (2010a). On the tth round,
one example ?xt,yt? is considered. We seek to solve
min?,? ?m2 ?? ? ?t?
2 + ?
s.t. L(?;xt,yt) ? ?, ? ? 0,
(23)
9Given what was just exposed, it seems appealing to try
max-product loopy BP on the factor graph of Fig. 1, or sum-
product loopy BP on the one in Fig. 3. Both attempts present se-
rious challenges: the former requires computing messages sent
by the tree factor, which requires O(n2) calls to the Chu-Liu-
Edmonds algorithm and hence O(n5) time. No obvious strat-
egy seems to exist for simultaneous computation of all mes-
sages, unlike in the sum-product case. The latter is even more
challenging, as standard sum-product loopy BP has serious is-
sues in the factor graph of Fig. 3; we construct in Martins et al
(2010b) a simple example with a very poor Bethe approxima-
tion. This might be fixed by using other variants of sum-product
BP, e.g., ones in which the entropy approximation is concave.
40
L?,?(?;x,y) , 1? log
?
y??Y(x) exp
[
?
(
?>
(
?(x,y?)? ?(x,y)
)
+ ?`(y?,y)
)]
(22)
Figure 4: A family of loss functions including as particular cases the ones used in CRFs, structured SVMs, and the
structured perceptron. The hyperparameter ? is the analogue of the inverse temperature in a Gibbs distribution, while
? scales the cost. For any choice of ? > 0 and ? ? 0, the resulting loss function is convex in ?, since, up to a scale
factor, it is the composition of the (convex) log-sum-exp function with an affine map.
Algorithm 1 Aggressive Online Learning
1: Input: {?xi,yi?}mi=1, ?, number of epochs K
2: Initialize ?1 ? 0; set T = mK
3: for t = 1 to T do
4: Receive instance ?xt, yt? and set ?t = ?(yt)
5: Solve Eq. 24 to obtain ??t and L?,?(?t, xt,yt)
6: Compute?L?,?(?t, xt,yt)=F(xt)(??t??t)
7: Compute ?t = min
{
1
?m ,
L?,?(?t;xt,yt)
??L?,?(?t;xt,yt)?2
}
8: Return ?t+1 = ?t ? ?t?L?,?(?t;xt,yt)
9: end for
10: Return the averaged model ?? ? 1T
?T
t=1 ?t.
which trades off conservativeness (stay close to the
most recent solution ?t) and correctness (keep the
loss small). Alg. 1?s lines 7?8 are the result of tak-
ing the first-order Taylor approximation of L around
?t, which yields the lower bound L(?;xt,yt) ?
L(?t;xt,yt) + (? ? ?t)>?L(?t;xt,yt), and plug-
ging that linear approximation into the constraint of
Eq. 23, which gives a simple Euclidean projection
problem (with slack) with a closed-form solution.
The online updating requires evaluating the loss
and computing its gradient. Both quantities can
be computed using the variational expression in
Prop. 1, for any loss L?,?(?;x,y) in Fig. 4.10 Our
only assumption is that the cost function `(y?,y)
can be written as a sum over factor-local costs; let-
ting ? = ?(y) and ?? = ?(y?), this implies
`(y?,y) = p>?? + q for some p and q which are
constant with respect to ??.11 Under this assump-
tion, L?,?(?;x,y) becomes expressible in terms of
the log-partition function of a distribution whose
log-potentials are set to ?(F(x)>? + ?p). From
Eq. 9 and after some algebra, we finally obtain
L?,?(?;x,y) =
10Our description also applies to the (non-differentiable)
hinge loss case, when ? ? ?, if we replace all instances of
?the gradient? in the text by ?a subgradient.?
11For the Hamming cost, this holds with p = 1 ? 2? and
q = 1>?. See Taskar et al (2006) for other examples.
max
???M(Gx)
?>F(x)(????)+
1
?
H(??)+?(p>??+q).
(24)
Let ?? be a maximizer in Eq. 24; from the second
statement of Prop. 1 we obtain ?L?,?(?;x,y) =
F(x)(????). When the inference problem in Eq. 24
is intractable, approximate message-passing algo-
rithms like loopy BP still allow us to obtain approx-
imations of the loss L?,? and its gradient.
For the hinge loss, we arrive precisely at the max-
loss variant of 1-best MIRA (Crammer et al, 2006).
For the logistic loss, we arrive at a new online learn-
ing algorithm for CRFs that resembles stochastic
gradient descent but with an automatic step size that
follows from our variational representation.
Unsupported Features. As datasets grow, so do
the sets of features, creating further computational
challenges. Often only ?supported? features?those
observed in the training data?are included, and
even those are commonly eliminated when their fre-
quencies fall below a threshold. Important infor-
mation may be lost as a result of these expedi-
ent choices. Formally, the supported feature set
is Fsupp ,
?m
i=1 supp?(xi,yi), where suppu ,
{j |uj 6= 0} denotes the support of vector u. Fsupp
is a subset of the complete feature set, comprised of
those features that occur in some candidate output,
Fcomp ,
?m
i=1
?
y?i?Y(xi)
supp?(xi,y?i). Features
in Fcomp\Fsupp are called unsupported.
Sha and Pereira (2003) have shown that training a
CRF-based shallow parser with the complete feature
set may improve performance (over the supported
one), at the cost of 4.6 times more features. De-
pendency parsing has a much higher ratio (around
20 for bilexical word-word features, as estimated in
the Penn Treebank), due to the quadratic or faster
growth of the number of parts, of which only a few
are active in a legal output. We propose a simple
strategy for handling Fcomp efficiently, which can
be applied for those losses in Fig. 4 where ? = ?.
(e.g., the structured SVM and perceptron). Our pro-
cedure is the following: keep an active set F contain-
41
CRF (TURBO PARS. #1) SVM (TURBO PARS. #2) SVM (TURBO #2)
ARC-FACT. SEC. ORD. ARC-FACT. SEC. ORD. |F| |F||Fsupp| +NONPROJ., COMPL.
ARABIC 78.28 79.12 79.04 79.42 6,643,191 2.8 80.02 (-0.14)
BULGARIAN 91.02 91.78 90.84 92.30 13,018,431 2.1 92.88 (+0.34) (?)
CHINESE 90.58 90.87 91.09 91.77 28,271,086 2.1 91.89 (+0.26)
CZECH 86.18 87.72 86.78 88.52 83,264,645 2.3 88.78 (+0.44) (?)
DANISH 89.58 90.08 89.78 90.78 7,900,061 2.3 91.50 (+0.68)
DUTCH 82.91 84.31 82.73 84.17 15,652,800 2.1 84.91 (-0.08)
GERMAN 89.34 90.58 89.04 91.19 49,934,403 2.5 91.49 (+0.32) (?)
JAPANESE 92.90 93.22 93.18 93.38 4,256,857 2.2 93.42 (+0.32)
PORTUGUESE 90.64 91.00 90.56 91.50 16,067,150 2.1 91.87 (-0.04)
SLOVENE 83.03 83.17 83.49 84.35 4,603,295 2.7 85.53 (+0.80)
SPANISH 83.83 85.07 84.19 85.95 11,629,964 2.6 87.04 (+0.50) (?)
SWEDISH 87.81 89.01 88.55 88.99 18,374,160 2.8 89.80 (+0.42)
TURKISH 76.86 76.28 74.79 76.10 6,688,373 2.2 76.62 (+0.62)
ENGLISH NON-PROJ. 90.15 91.08 90.66 91.79 57,615,709 2.5 92.13 (+0.12)
ENGLISH PROJ. 91.23 91.94 91.65 92.91 55,247,093 2.4 93.26 (+0.41) (?)
Table 2: Unlabeled attachment scores, ignoring punctuation. The leftmost columns show the performance of arc-
factored and second-order models for the CRF and SVM losses, after 10 epochs with 1/(?m) = 0.001 (tuned on the
English Non-Proj. dev.-set). The rightmost columns refer to a model to which non-projectivity features were added,
trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated,
the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the
difference w.r.t. a model trained with the supported features only). Entries marked with ? are the highest reported in
the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008),
Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which
achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different).
ing all features that have been instantiated in Alg. 1.
At each round, run lines 4?5 as usual, using only
features in F. Since the other features have not been
used before, they have a zero weight, hence can be
ignored. When ? = ?, the variational problem in
Eq. 24 consists of a MAP computation and the solu-
tion corresponds to one output y?t ? Y(xt). Only the
parts that are active in y?t but not in yt, or vice-versa,
will have features that might receive a nonzero up-
date. Those parts are reexamined for new features
and the active set F is updated accordingly.
6 Experiments
We trained non-projective dependency parsers for
14 languages, using datasets from the CoNLL-X
shared task (Buchholz and Marsi, 2006) and two
datasets for English: one from the CoNLL-2008
shared task (Surdeanu et al, 2008), which contains
non-projective arcs, and another derived from the
Penn Treebank applying the standard head rules of
Yamada and Matsumoto (2003), in which all parse
trees are projective.12 We implemented Alg. 1,
12We used the provided train/test splits for all datasets. For
English, we used the standard test partitions (section 23 of the
Wall Street Journal). We did not exploit the fact that some
datasets only contain projective trees and have unique roots.
which handles any loss function L?,? .13 When ? <
?, Turbo Parser #1 and the loopy BP algorithm of
Smith and Eisner (2008) is used; otherwise, Turbo
Parser #2 is used and the LP relaxation is solved with
CPLEX. In both cases, we employed the same prun-
ing strategy as Martins et al (2009).
Two different feature configurations were first
tried: an arc-factored model and a model with
second-order features (siblings and grandparents).
We used the same arc-factored features as McDon-
ald et al (2005) and second-order features that con-
join words and lemmas (at most two), parts-of-
speech tags, and (if available) morphological infor-
mation; this was the same set of features as in Mar-
tins et al (2009). Table 2 shows the results obtained
in both configurations, for CRF and SVM loss func-
tions. While in the arc-factored case performance is
similar, in second-order models there seems to be a
consistent gain when the SVM loss is used. There
are two possible reasons: first, SVMs take the cost
function into consideration; second, Turbo Parser #2
is less approximate than Turbo Parser #1, since only
the marginal polytope is approximated (the entropy
function is not involved).
13The code is available at http://www.ark.cs.cmu.edu/
TurboParser.
42
? 1 1 1 1 3 5 ?
? 0 (CRF) 1 3 5 1 1 1 (SVM)
ARC-F. 90.15 90.41 90.38 90.53 90.80 90.83 90.66
2 ORD. 91.08 91.85 91.89 91.51 92.04 91.98 91.79
Table 3: Varying ? and ?: neither the CRF nor the
SVM is optimal. Results are UAS on the English Non-
Projective dataset, with ? tuned with dev.-set validation.
The loopy BP algorithm managed to converge for
nearly all sentences (with message damping). The
last three columns show the beneficial effect of un-
supported features for the SVM case (with a more
powerful model with non-projectivity features). For
most languages, unsupported features convey help-
ful information, which can be used with little extra
cost (on average, 2.5 times more features are instan-
tiated). A combination of the techniques discussed
here yields parsers that are in line with very strong
competitors?for example, the parser of Koo and
Collins (2010), which is exact, third-order, and con-
strains the outputs to be projective, does not outper-
form ours on the projective English dataset.14
Finally, Table 3 shows results obtained for differ-
ent settings of ? and ?. Interestingly, we observe
that higher scores are obtained for loss functions that
are ?between? SVMs and CRFs.
7 Related Work
There has been recent work studying efficient com-
putation of messages in combinatorial factors: bi-
partite matchings (Duchi et al, 2007), projective
and non-projective arborescences (Smith and Eis-
ner, 2008), as well as high order factors with count-
based potentials (Tarlow et al, 2010), among others.
Some of our combinatorial factors (OR, OR-WITH-
OUTPUT) and the analogous entropy computations
were never considered, to the best of our knowledge.
Prop. 1 appears in Wainwright and Jordan (2008)
for canonical overcomplete models; we adapt it here
for models with shared features. We rely on the vari-
ational interpretation of loopy BP, due to Yedidia et
al. (2001), to derive the objective being optimized
by Smith and Eisner?s loopy BP parser.
Independently of our work, Koo et al (2010)
14This might be due to the fact that Koo and Collins (2010)
trained with the perceptron algorithm and did not use unsup-
ported features. Experiments plugging the perceptron loss
(? ? ?, ? ? 0) into Alg. 1 yielded worse performance than
with the hinge loss.
recently proposed an efficient dual decomposition
method to solve an LP problem similar (but not
equal) to the one in Eq. 20,15 with excellent pars-
ing performance. Their parser is also an instance
of a turbo parser since it relies on a local approxi-
mation of a marginal polytope. While one can also
use dual decomposition to address our MAP prob-
lem, the fact that our model does not decompose as
nicely as the one in Koo et al (2010) would likely
result in slower convergence.
8 Conclusion
We presented a unified view of two recent approxi-
mate dependency parsers, by stating their underlying
factor graphs and by deriving the variational prob-
lems that they address. We introduced new hard con-
straint factors, along with formulae for their mes-
sages, local belief constraints, and entropies. We
provided an aggressive online algorithm for training
the models with a broad family of losses.
There are several possible directions for future
work. Recent progress in message-passing algo-
rithms yield ?convexified? Bethe approximations
that can be used for marginal inference (Wainwright
et al, 2005), and provably convergent max-product
variants that solve the relaxed LP (Globerson and
Jaakkola, 2008). Other parsing formalisms can be
handled with the inventory of factors shown here?
among them, phrase-structure parsing.
Acknowledgments
The authors would like to thank the reviewers for their
comments, and Kevin Gimpel, David Smith, David Son-
tag, and Terry Koo for helpful discussions. A. M. was
supported by a grant from FCT/ICTI through the CMU-
Portugal Program, and also by Priberam Informa?tica.
N. S. was supported in part by Qatar NRF NPRP-08-485-
1-083. E. X. was supported by AFOSR FA9550010247,
ONR N000140910758, NSF CAREER DBI-0546594,
NSF IIS-0713379, and an Alfred P. Sloan Fellowship.
M. F. and P. A. were supported by the FET programme
(EU FP7), under the SIMBAD project (contract 213250).
15The difference is that the model of Koo et al (2010)
includes features that depend on consecutive siblings?
making it decompose into subproblems amenable to dynamic
programming?while we have factors for all pairs of siblings.
43
References
C. Berrou, A. Glavieux, and P. Thitimajshima. 1993.
Near Shannon limit error-correcting coding and decod-
ing. In Proc. of ICC, volume 93, pages 1064?1070.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. NIPS, 19.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proc. of ACL.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
crfs: Training log-linear models with loss functions.
In Proc. of NAACL.
A. Globerson and T. Jaakkola. 2008. Fixing max-
product: Convergent message passing algorithms for
MAP LP-relaxations. NIPS, 20.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proc. of EMNLP.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 2001.
Factor graphs and the sum-product algorithm. IEEE
Trans. Inf. Theory, 47(2):498?519.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proc. of ACL-IJCNLP.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M. A. T. Figueiredo. 2010a.
Learning structured classifiers with dual coordinate
descent. Technical Report CMU-ML-10-109.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010b. Turbo parsers:
Dependency parsing by approximate variational infer-
ence (extended version).
A. McCallum, K. Schultz, and S. Singh. 2009. Fac-
torie: Probabilistic programming via imperatively de-
fined factor graphs. In NIPS.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. of CoNLL.
R. J. McEliece, D. J. C. MacKay, and J. F. Cheng. 1998.
Turbo decoding as an instance of Pearl?s ?belief prop-
agation? algorithm. IEEE Journal on Selected Areas
in Communications, 16(2).
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of HLT-NAACL.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
D. A. Smith and N. A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
CoNLL.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. JMLR, 8:693?723.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
D. Tarlow, I. E. Givoni, and R. S. Zemel. 2010. HOP-
MAP: Efficient message passing with high order po-
tentials. In Proc. of AISTATS.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
B. Taskar, S. Lacoste-Julien, and M. I. Jordan. 2006.
Structured prediction, dual extragradient and Bregman
projections. JMLR, 7:1627?1653.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proc. of ICML.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
M. J. Wainwright, T.S. Jaakkola, and A.S. Willsky. 2005.
A new class of upper bounds on the log partition func-
tion. IEEE Trans. Inf. Theory, 51(7):2313?2335.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2001. Gen-
eralized belief propagation. In NIPS.
44
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 238?249,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Dual Decomposition with Many Overlapping Components
Andre? F. T. Martins?? Noah A. Smith? Pedro M. Q. Aguiar? Ma?rio A. T. Figueiredo?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Sistemas e Robo?tica, Instituto Superior Te?cnico, Lisboa, Portugal
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.pt
Abstract
Dual decomposition has been recently pro-
posed as a way of combining complemen-
tary models, with a boost in predictive power.
However, in cases where lightweight decom-
positions are not readily available (e.g., due to
the presence of rich features or logical con-
straints), the original subgradient algorithm
is inefficient. We sidestep that difficulty by
adopting an augmented Lagrangian method
that accelerates model consensus by regular-
izing towards the averaged votes. We show
how first-order logical constraints can be han-
dled efficiently, even though the correspond-
ing subproblems are no longer combinatorial,
and report experiments in dependency pars-
ing, with state-of-the-art results.
1 Introduction
The last years have witnessed increasingly accurate
models for syntax, semantics, and machine transla-
tion (Chiang, 2007; Finkel et al, 2008; Petrov and
Klein, 2008; Smith and Eisner, 2008; Martins et
al., 2009a; Johansson and Nugues, 2008; Koo et al,
2010). The predictive power of such models stems
from their ability to break locality assumptions. The
resulting combinatorial explosion typically demands
some form of approximate decoding, such as sam-
pling, heuristic search, or variational inference.
In this paper, we focus on parsers built from lin-
ear programming relaxations, the so-called ?turbo
parsers? (Martins et al, 2009a; Martins et al, 2010).
Rush et al (2010) applied dual decomposition as
a way of combining models which alone permit
efficient decoding, but whose combination is in-
tractable. This results in a relaxation of the origi-
nal problem that is elegantly solved with the sub-
gradient algorithm. While this technique has proven
quite effective in parsing (Koo et al, 2010; Auli
and Lopez, 2011) as well as machine translation
(Rush and Collins, 2011), we show here that its
success is strongly tied to the ability of finding a
?good? decomposition, i.e., one involving few over-
lapping components (or slaves). With many compo-
nents, the subgradient algorithm exhibits extremely
slow convergence (cf. Fig. 2). Unfortunately, a
lightweight decomposition is not always at hand, ei-
ther because the problem does not factor in a natural
way, or because one would like to incorporate fea-
tures that cannot be easily absorbed in few tractable
components. Examples include features generated
by statements in first-order logic, features that vio-
late Markov assumptions, or history features such as
the ones employed in transition-based parsers.
To tackle the kind of problems above, we adopt
DD-ADMM (Alg. 1), a recently proposed algorithm
that accelerates dual decomposition (Martins et al,
2011). DD-ADMM retains the modularity of the
subgradient-based method, but it speeds up consen-
sus by regularizing each slave subproblem towards
the averaged votes obtained in the previous round
(cf. Eq. 14). While this yields more involved sub-
problems (with a quadratic term), we show that ex-
act solutions can still be efficiently computed for
all cases of interest, by using sort operations. As
a result, we obtain parsers that can handle very rich
features, do not require specifying a decomposition,
and can be heavily parallelized. We demonstrate the
success of the approach by presenting experiments
in dependency parsing with state-of-the-art results.
2 Background
2.1 Structured Prediction
Let x ? X be an input object (e.g., a sentence), from
which we want to predict a structured output y ?
Y (e.g., a parse tree). The output set Y is assumed
too large for exhaustive search to be tractable. We
assume to have a model that assigns a score f(y) to
each candidate output, based on which we predict
y? = arg max
y?Y
f(y). (1)
238
Designing the model must obey certain practical
considerations. If efficiency is the major concern,
a simple model is usually chosen so that Eq. 1 can
be solved efficiently, at the cost of limited expressive
power. If we care more about accuracy, a model with
richer features and more involved score functions
may be designed. Decoding, however, will be more
expensive, and approximations are often necessary.
A typical source of intractability comes from the
combinatorial explosion inherent in the composition
of two or more tractable models (Bar-Hillel et al,
1964; Tromble and Eisner, 2006). Recently, Rush
et al (2010) have proposed a dual decomposition
framework to address NLP problems in which the
global score decomposes as f(y) = f1(z1)+f2(z2),
where z1 and z2 are two overlapping ?views? of the
output, so that Eq. 1 becomes:
maximize f1(z1) + f2(z2)
w.r.t. z1 ? Y1, z2 ? Y2
s.t. z1 ? z2.
(2)
Above, the notation z1 ? z2 means that z1 and
z2 ?agree on their overlaps,? and an isomorphism
Y ' {?z1, z2? ? Y1?Y2 | z1 ? z2} is assumed. We
next formalize these notions and proceed to compo-
sitions of an arbitrary number of models. Of special
interest is the unexplored setting where this number
is very large and each component very simple.
2.2 Decomposition into Parts
A crucial step in the design of structured predictors
is that of decomposing outputs into parts (Taskar et
al., 2003). We assume the following setup:
Basic parts. We let R be a set of basic parts, such
that each element y ? Y can be identified with a
subset of R. The exact meaning of a ?basic part?
is problem dependent. For example, in dependency
parsing, R can be the set of all possible dependency
arcs (see Fig. 1); in phrase-based parsing, it can be
the set of possible spans; in sequence labeling, it can
be the set of possible labels at each position. Our
only assumption is that we can ?read out? y from
the basic parts it contains. For convenience, we rep-
resent y as a binary vector, y = ?y(r)?r?R, where
y(r) = 1 if part r belongs to y, and 0 otherwise.
Decomposition. We generalize the decomposition
in Eq. 2 by considering sets Y1, . . . ,YS for S ? 2.
Figure 1: Parts used by our parser. Arcs are the ba-
sic parts: any dependency tree can be ?read out? from
the arcs it contains. Consecutive siblings and grandpar-
ent parts introduce horizontal and vertical Markovization
(McDonald et al, 2006; Carreras, 2007). We break the
horizontal Markov assumption via all siblings parts and
the vertical one through parts which indicate a directed
path between two words. Inspired by transition-based
parsers, we also adopt head bigram parts, which look at
the heads attached to consecutive words. Finally, we fol-
low Martins et al (2009a) and have parts which indicate
if an arc is non-projective (i.e., if it spans words that do
not descend from its head).
Each Ys is associated with its own set of parts Rs, in
the same sense as above; we represent the elements
of Ys as binary vectors zs = ?zs(r)?r?Rs . Examples
are vectors indicating a tree structure, a sequence,
or an assignment of variables to a factor, in which
case it may happen that only some binary vectors
are legal. Some parts in Rs are basic, while others
are not. We denote by R?s = Rs ? R the subset of
the ones that are. In addition, we assume that:
? R1, . . . ,RS jointly cover R, i.e., R ? ?Ss=1 Rs;
? Only basic parts may overlap, i.e., Rs ? Rt ?
R, ?s, t ? {1, . . . , S};
? Each zs ? Ys is completely defined by its entries
indexed by elements of R?s, from which we can
guess the ones in Rs \ R?s. This implies that each
y ? Y has a unique decomposition ?z1, . . . , zS?.
Fig. 1 shows several parts used in dependency pars-
ing models; in phrase-based parsing, these could be
spans and production rules anchored in the surface
string; in sequence labeling, they can be unigram,
bigram, and trigram labels.1
1There is a lot of flexibility about how to decompose the
model into S components: each set Rs can correspond to a sin-
239
Global consistency. We want to be able to read
out y ? Y by ?gluing? together the components
?z1, . . . , zS?. This is only meaningful if they are
?globally consistent,? a notion which we make pre-
cise. Two components zs ? Ys and zt ? Yt are said
to be consistent (denoted zs ? zt) if they agree on
their overlaps, i.e., if zs(r) = zt(r), ?r ? Rs ? Rt.
A complete assignment ?z1, . . . , zS? is globally con-
sistent if all pairs of components are consistent. This
is equivalent to the existence of a witness vector
?u(r)?r?R such that zs(r) = u(r), ?s, r ? R?s.
With this setup, assuming that the score function
decomposes as f(z) = ?Ss=1 fs(zs), the decoding
problem (which extends Eq. 2 for S ? 2) becomes:
P : maximize
?S
s=1 fs(zs)w.r.t. zs ? Ys, ?s
?u(r)?r?R ? R|R|,
s.t. zs(r) = u(r), ?s, r ? R?s.
(3)
We call the equality constraints expressed in the last
line the ?agreement constraints.? It is these con-
straints that complicate the problem, which would
otherwise be exactly separable into S subproblems.
The dual decomposition method (Komodakis et al,
2007; Rush et al, 2010) builds an approximation by
dualizing out these constraints, as we describe next.
2.3 Dual Decomposition
We describe dual decomposition in a slightly differ-
ent manner than Rush et al (2010): we will first
build a relaxation of P (called P ?), in which the en-
tire approximation is enclosed. Then, we dualize P ?,
yielding problem D. In the second step, the duality
gap is zero, i.e., P ? and D are equivalent.2
Relaxation. For each s ? {1, . . . , S} we consider
the convex hull of Ys,
Zs =
{ ?
zs?Ys
p(zs)zs
???? p(zs) ? 0,
?
zs?Ys
p(zs) = 1
}
.
(4)
gle factor in a factor graph (Smith and Eisner, 2008), or to a
entire subgraph enclosing several factors (Koo et al, 2010), or
even to a formula in Markov logic (Richardson and Domingos,
2006). In these examples, the basic parts may correspond to
individual variable-value pairs.
2Instead of following the path P ? P ? ? D, Rush et al
(2010) go straight from P to D via a Lagrangian relaxation.
The two formulations are equivalent for linear score functions.
We have that Ys = Zs ? Z|Rs|; hence, problem P
(Eq. 3) is equivalent to one in which each Ys is re-
placed by Zs and the z-variables are constrained to
be integer. By dropping the integer constraints, we
obtain the following relaxed problem:
P ? : maximize
?S
s=1 fs(zs)w.r.t. zs ? Zs, ?s
?u(r)?r?R ? R|R|,
s.t. zs(r) = u(r), ?s, r ? R?s.
(5)
If the score functions fs are convex, P ? becomes a
convex program (unlike P , which is discrete); being
a relaxation, it provides an upper bound of P .
Lagrangian. Introducing a Lagrange multiplier
?s(r) for each agreement constraint in Eq. 5, one
obtains the Lagrangian function
L(z, u, ?) =
?S
s=1
(
fs(zs) +
?
r?R?s ?s(r)zs(r)
)
??r?R
(?
s:r?R?s ?s(r)
)
u(r), (6)
and the dual problem (the master)
D : minimize
?S
s=1 gs(?s)w.r.t. ? = ??1, . . . , ?S?
s.t. ?s:r?R?s ?s(r) = 0, ?r ? R,
(7)
where the gs(?s) are the solution values of the fol-
lowing subproblems (the slaves):
maximize fs(zs) +
?
r?R?s ?s(r)zs(r)w.r.t. zs ? Zs. (8)
We assume that strong duality holds (w.r.t. Eqs. 5?
7), hence we have P ? P ? = D.3
Solving the dual. Why is the dual formulation D
(Eqs. 7?8) more appealing than P ? (Eq. 5)? The an-
swer is that the components 1, . . . , S are now de-
coupled, which makes things easier provided each
slave subproblem (Eq. 8) can be solved efficiently.
In fact, this is always a concern in the mind of
the model?s designer when she chooses a decom-
position (the framework that we describe in ?3,
in some sense, alleviates her from this concern).
If the score functions are linear, i.e., of the form
fs(zs) =
?
r?Rs ?s(r)zs(r) for some vector ?s =
??s(r)?r?Rs , then Eq. 8 becomes a linear program,
for which a solution exists at a vertex of Zs (which
3This is guaranteed if the score functions fs are linear.
240
in turn is an element of Ys). Depending on the struc-
ture of the problem, Eq. 8 may be solved by brute
force, dynamic programming, or specialized combi-
natorial algorithms (Rush et al, 2010; Koo et al,
2010; Rush and Collins, 2011).
Applying the projected subgradient method (Ko-
modakis et al, 2007; Rush et al, 2010) to the mas-
ter problem (Eq. 7) yields a remarkably simple algo-
rithm, which at each round t solves the subproblems
in Eq. 8 for s = 1, . . . , S, and then gathers these
solutions (call them zt+1s ) to compute an ?averaged?
vote for each basic part,
ut+1(r) = 1?(r)
?
s:r?R?s zt+1s (r), (9)
where ?(r) = |{s : r ? Rs}| is the number of com-
ponents which contain part r. An update of the La-
grange variables follows,
?t+1s (r) = ?ts(r)? ?t(zt+1s (r)? ut+1(r)), (10)
where ?t is a stepsize. Intuitively, the algorithm
pushes for a consensus among the slaves (Eq. 9),
via an adjustment of the Lagrange multipliers which
takes into consideration deviations from the aver-
age (Eq. 10). The subgradient method is guaran-
teed to converge to the solution of D (Eq. 7), for
suitably chosen stepsizes (Shor, 1985; Bertsekas et
al., 1999); it also provides a certificate of optimal-
ity in case the relaxation is tight (i.e., P = D) and
the exact solution has been found. However, con-
vergence is slow when S is large (as we will show
in the experimental section), and no certificates are
available when there is a relaxation gap (P < P ?).
In the next section, we describe the DD-ADMM al-
gorithm (Martins et al, 2011), which does not have
these drawbacks and shares a similar simplicity.
3 Alternating Directions Method
There are two reasons why subgradient-based dual
decomposition is not completely satisfying:
? it may take a long time to reach a consensus;
? it puts all its resources in solving the dual problem
D, and does not attempt to make progress in the
primal P ?, which is closer to our main concern.4
4Our main concern is P ; however solving P ? is often a
useful step towards that goal, either because a good rounding
scheme exists, or because one may build tighter relaxations to
approach P (Sontag et al, 2008; Rush and Collins, 2011).
Taking a look back at the relaxed primal problem
P ? (Eq. 5), we see that any primal feasible solution
must satisfy the agreement constraints. This sug-
gests that penalizing violations of these constraints
could speed up consensus.
Augmented Lagrangian. By adding a penalty
term to Eq. 6, we obtain the augmented Lagrangian
function (Hestenes, 1969; Powell, 1969):
A?(z, u, ?) = L(z, u, ?)?
?
2
S?
s=1
?
r?R?s
(zs(r)? u(r))2,
(11)
where the parameter ? ? 0 controls the intensity
of the penalty. Augmented Lagrangian methods
are well-known in the optimization community (see,
e.g., Bertsekas et al (1999), ?4.2). They alternate
updates to the ?-variables, while seeking to maxi-
mize A? with respect to z and u. In our case, how-
ever, this joint maximization poses difficulties, since
the penalty term couples the two variables. The al-
ternating directions method of multipliers (ADMM),
coined by Gabay and Mercier (1976) and Glowinski
and Marroco (1975), sidesteps this issue by perform-
ing alternate maximizations,
zt+1 = arg max
z
A?(z, ut, ?t), (12)
ut+1 = arg max
u
A?(zt+1, u, ?t), (13)
followed by an update of the Lagrange multipliers
as in Eq. 10. Recently, ADMM has attracted inter-
est, being applied in a variety of problems; see the
recent book by Boyd et al (2011) for an overview.
As derived in the App. A, the u-updates in Eq. 13
have a closed form, which is precisely the averag-
ing operation performed by the subgradient method
(Eq. 9). We are left with the problem of comput-
ing the z-updates. Like in the subgradient approach,
the maximization in Eq. 12 can be separated into S
independent slave subproblems, which now take the
form:
maximize fs(zs) +
?
r?R?s ?s(r)zs(r)
??2
?
r?R?s(zs(r)? ut(r))2w.r.t. zs ? Zs(x).
(14)
Comparing Eq. 8 and Eq. 14, we observe that the
only difference is the presence in the latter of a
241
quadratic term which regularizes towards the pre-
vious averaged votes ut(r). Because of this term,
the solution of Eq. 14 for linear score functions may
not be at a vertex (in contrast to the subgradient
method). We devote ?4 to describing exact and effi-
cient ways of solving the problem in Eq. 14 for im-
portant, widely used slaves. Before going into de-
tails, we mention another advantage of ADMM over
the subgradient algorithm: it knows when to stop.
Primal and dual residuals. Recall that the sub-
gradient method provides optimality certificates
when the relaxation is tight (P = P ?) and an ex-
act solution of P has been found. While this is good
enough when tight relaxations are frequent, as in the
settings explored by Rush et al (2010), Koo et al
(2010), and Rush and Collins (2011), it is hard to
know when to stop when a relaxation gap exists.
We would like to have similar guarantees concern-
ing the relaxed primal P ?.5 A general weakness of
subgradient algorithms is that they do not have this
capacity, and so are usually stopped by specifying a
maximum number of iterations. In contrast, ADMM
allows to keep track of primal and dual residuals
(Boyd et al, 2011). This allows providing certifi-
cates not only for the exact solution of P (when the
relaxation is tight), but also to terminate when a near
optimal solution of the relaxed problem P ? has been
found. The primal residual rtP measures the amount
by which the agreement constraints are violated:
rtP =
?S
s=1
?
r?R?s(zts(r)? ut(r))2?
r?R ?(r)
; (15)
the dual residual rtD is the amount by which a dual
optimality condition is violated (see Boyd et al
(2011), p.18, for details). It is computed via:
rtD =
?
r?R ?(r)(ut(r)? ut?1(r))2?
r?R ?(r)
, (16)
Our stopping criterion is thus that these two residu-
als are below a threshold, e.g., 1 ? 10?3. The com-
plete algorithm is depicted as Alg. 1. As stated in
5This problem is more important than it may look. Problems
with many slaves tend to be less exact, hence relaxation gaps
are frequent. Also, when decoding is embedded in training, it is
useful to obtain the fractional solution of the relaxed primal P
(rather than an approximate integer solution). See Kulesza and
Pereira (2007) and Martins et al (2009b) for details.
Algorithm 1 ADMM-based Dual Decomposition
1: input: score functions ?fs(.)?Ss=1, parameters ?, ?,
thresholds P and D.
2: initialize t? 1
3: initialize u1(r)? 0.5 and ?1s(r)? 0, ?s, ?r ? R?s
4: repeat
5: for each s = 1, . . . , S do
6: make a zs-update, yielding zt+1s (Eq. 14)
7: end for
8: make a u-update, yielding ut+1 (Eq. 9)
9: make a ?-update, yielding ?t+1 (Eq. 10)
10: t? t+ 1
11: until rt+1P < P and rt+1D < D (Eqs. 15?16)
12: output: relaxed primal and dual solutions u, z, ?
Martins et al (2011), convergence to the solution of
P ? is guaranteed with a fixed stepsize ?t = ??, with
? ? [1, 1.618] (Glowinski and Le Tallec, 1989, Thm.
4.2). In our experiments, we set ? = 1.5, and adapt
? as described in (Boyd et al, 2011, p.20).6
4 Solving the Subproblems
In this section, we address the slave subproblems of
DD-ADMM (Eq. 14). We show how these subprob-
lems can be solved efficiently for several important
cases that arise in NLP applications. Throughout,
we assume that the score functions fs are linear, i.e.,
they can be written as fs(zs) = ?r?Rs ?s(r)zs(r).This is the case whenever a linear model is used, in
which case ?s(r) = 1?(r)w ? ?(x, r), where w is a
weight vector and ?(x, r) is a feature vector. It is
also the scenario studied in previous work in dual
decomposition (Rush et al, 2010). Under this as-
sumption, and discarding constant terms, the slave
subproblem in Eq. 14 becomes:
max
zs?Zs
?
r?Rs\R?s
?s(r)zs(r)?
?
2
?
r?R?s
(zs(r)? as(r))2.
(17)
where as(r) = ut(r)+??1(?s(r)+?ts(r)). Since Zs
is a polytope, Eq. 17 is a quadratic program, which
can be solved with a general purpose solver. How-
ever, that does not exploit the structure of Zs and is
inefficient when |Rs| is large. We next show that for
many cases, a closed-form solution is available and
6Briefly, we initialize ? = 0.03 and then increase/decrease
? by a factor of 2 whenever the primal residual becomes > 10
times larger/smaller than the dual residual.
242
can be computed inO(|Rs|) time, up to log factors.7
Pairwise Factors. This is the case where RPAIR =
{r1, r2, r12}, where r1 and r2 are basic parts and
r12 is their conjunction, i.e., we have YPAIR =
{?z1, z2, z12? | z12 = z1 ? z2}. This factor is use-
ful to make conjunctions of variables participate in
the score function (see e.g. the grandparent, sibling,
and head bigram parts in Fig. 1). The convex hull
of YPAIR is the polytope ZPAIR = {?z1, z2, z12? ?
[0, 1]3 | z12 ? z1, z12 ? z2, z12 ? z1 + z2 ? 1}, as
shown by Martins et al (2010). In this case, problem
(17) can be written as
max ?12z12 ? ?2 [(z1 ? a1)2 + (z2 ? a2)2]w.r.t. ?z1, z2, z12? ? [0, 1]3
s.t. z12 ? z1, z12 ? z2, z12 ? z1 + z2 ? 1
(18)
and has a closed form solution (see App. B).
Uniqueness Quantification and XOR. Many
problems involve constraining variables to take a
single value: for example, in dependency parsing,
a modifier can only take one head. This can be
expressed as the statement ?!y : Q(y) in first-order
logic,8 or as a one-hot XOR factor in a factor
graph (Smith and Eisner, 2008; Martins et al,
2010). In this case, RXOR = {r1, . . . , rn}, and
YXOR = {?z1, . . . , zn? ? {0, 1}n |
?n
i=1 zi = 1}.
The convex hull of YXOR is ZXOR = {?z1, . . . , zn? ?
[0, 1]n | ?ni=1 zi = 1}. Assume for the sake of
simplicity that all parts in RXOR are basic.9 Up to a
constant, the slave subproblem becomes:
minimize 12
?n
i=1(zi ? ai)2w.r.t. ?z1, . . . , zn? ? [0, 1]n
s.t. ?ni zi = 1.
(19)
This is the problem of projecting onto the probabil-
ity simplex, which can be done in O(n log n) time
via a sort operation (see App. C).10
7This matches the asymptotic time that would be necessary
to solve the corresponding problems in the subgradient method,
for which algorithms are straightforward to derive. The point is
that with ADMM fewer instances of these subproblems need to
be solved, due to faster convergence of the master problem.
8The symbol ?! means ?there is one and only one.?
9A similar derivation can be made otherwise.
10Also common is the need for constraining existence of ?at
most one? element. This can be reduced to uniqueness quantifi-
cation by adding a dummy NULL label.
Existential Quantification and OR. Sometimes,
only existence is required, not necessarily unique-
ness. This can be expressed with disjunctions, ex-
istential quantifiers in first-order logic (?y : Q(y)),
or as a OR factor. In this case, ROR = {r1, . . . , rn},
YOR = {?z1, . . . , zn? ? {0, 1}n |
?n
i=1 zi = 1},
and the convex hull is ZOR = {?z1, . . . , zn? ?
[0, 1]n | ?ni=1 zi ? 1} (see Tab. 1 in Martins et al
(2010)). The slave subproblem becomes:
minimize 12
?n
i=1(zi ? ai)2w.r.t. ?z1, . . . , zn? ? [0, 1]n
s.t. ?ni zi ? 1.
(20)
We derive a procedure in App. D to compute this
projection in O(n log n) runtime, also with a sort.
Negations. The two cases above can be extended
to allow some of their inputs to be negated. By a
change of variables in Eqs. 19?20 it is possible to
reuse the same black box that solves those problems.
The procedure is as follows:
1. For i = 1, . . . , n, set a?i = 1?ai if the ith variable
is negated, and a?i = ai otherwise.
2. Obtain ?z?1, . . . , z?n? as the solution of Eqs. 19 or
20 providing ?a?1, . . . , a?n? as input.
3. For i = 1, . . . , n, set zi = 1?z?i if the ith variable
is negated, and zi = z?i otherwise.
The ability to handle negated variables adds a
great degree of flexibility. From De Morgan?s
laws, we can now handle conjunctions and impli-
cations (since ?ni=1Qi(x) ? R(x) is equivalent to?n
i=1 ?Qi(x) ?R(x)).
Logical Variable Assignments. All previous ex-
amples involve taking a group of existing variables
and defining a constraint. Alternatively, we may
want to define a new variable which is the result of
an operation involving other variables. For exam-
ple, R(x) := ?!y : Q(x, y). This corresponds to the
XOR-WITH-OUTPUT factor in Martins et al (2010).
Interestingly, this can be expressed as a XOR where
R(x) is negated (i.e., either ?R(x) holds or exactly
one y satisfies Q(x, y), but not both).
A more difficult problem is that of the OR-WITH-
OUTPUT factor, expressed by the formula R(x) :=
?y : Q(x, y). We have ROR-OUT = {r0, . . . , rn},
and YOR-OUT = {?z0, . . . , zn? ? {0, 1}n | z0 =
243
# Slaves Runtime Description
Tree ?!h : arc(h,m), m 6= 0 O(n) O(n logn) Each non-root word has a head
flow(h,m, k)? arc(h,m) O(n3) O(1) Only active arcs may carry flow
path(m, d) := ?!h : flow(h,m, d), m 6= 0 O(n2) O(n logn)
path(h, d) := ?!m : flow(h,m, d) O(n2) O(n logn) Paths and flows are consistent
path(0,m) := TRUE, flow(h,m,m) := TRUE (see Martins et al (2010))
All siblings sibl(h,m, s) := arc(h,m) ? arc(h, s) O(n3) O(1) By definition
Grandp. grand(g, h,m) := arc(g, h) ? arc(h,m) O(n3) O(1) By definition
Head Bigram bigram(b, h,m) := arc(b,m? 1) ? arc(h,m), m 6= 0 O(n3) O(1) By definition
Consec. Sibl. lastsibl(h,m,m) := arc(h,m)
?!m ? [h, k] : lastsibl(h,m, k) O(n2) O(n logn) Head automaton model
lastsibl(h,m, k) := lastsibl(h,m, k + 1) (see supplementary material)
? nextsibl(h,m, k + 1) O(n3) O(1)
arc(h,m) := ?!s ? [h,m] : nextsibl(h, s,m) O(n2) O(n logn)
Nonproj. Arc nonproj(h,m) := arc(h,m) ? ?k ? [h,m] : ?path(h, k) O(n2) O(n logn) By definition
Table 1: First-order logic formulae underlying our dependency parser. The basic parts are the predicate variables
arc(h,m) (indicating an arc linking head h to modifier m), path(a, d) (indicating a directed path from ancestor
a to descendant d), nextsibl(h,m, s) (indicating that ?h,m? and ?h, s? are consecutive siblings), nonproj(h,m)
(indicating that ?h,m? is a non-projective arc), as well as the auxiliary variables flow(h,m, d) (indicating that arc
?h,m? carries flow to d), and lastsibl(h,m, k) (indicating that, up to position k, the last seen modifier of h occurred
at position m). The non-basic parts are the pairwise factors sibl(h,m, s), grand(g, h,m), and bigram(b, h,m); as
well as each logical formula. Columns 3?4 indicate the number of parts of each kind, and the time complexity for
solving each subproblem. For a sentence of length n, there are O(n3) parts and the total complexity is O(n3 log n).
?n
i=1 zi}. The convex hull of YOR-OUT is the follow-
ing set: ZOR-OUT = {?z0, . . . , zn? ? [0, 1]n | z0 ??n
i=1 zi, z0 ? zi, ?i = 1, . . . , n} (Martins et al,
2010, Tab.1). The slave subproblem is:
minimize 12
?n
i=0(zi ? ai)2w.r.t. ?z0, . . . , zn? ? [0, 1]n
s.t. z0 ??ni=1 zi; z0 ? zi, ?i = 1, . . . , n.(21)
The problem in Eq. 21 is more involved than the
ones in Eqs. 19?20. Yet, there is still an efficient
procedure with runtime O(n log n) (see App. E).
By using the result above for negated variables, we
are now endowed with a procedure for many other
cases, such that AND-WITH-OUTPUT and formu-
las with universal quantifiers (e.g., R(x) := ?y :
Q(x, y)). Up to a log-factor, the runtimes will be
linear in the number of predicates.
Larger Slaves. The only disadvantage of DD-
ADMM in comparison with the subgradient algo-
rithm is that there is not an obvious way of solving
the subproblem in Eq. 14 exactly for large combi-
natorial factors, such as the TREE constraint in de-
pendency parsing, or a sequence model. Hence, our
method seems to be more suitable for decomposi-
tions which involve ?simple slaves,? even if their
number is large. However, this does not rule out the
possibility of using this method otherwise. Eckstein
and Bertsekas (1992) show that the ADMM algo-
rithm may still converge when the z-updates are in-
exact. Hence the method may still work if the slaves
are solved numerically up to some accuracy. We de-
fer this to future investigation.
5 Experiments: Dependency Parsing
We used 14 datasets with non-projective depen-
dencies from the CoNLL-2006 and CoNLL-2008
shared tasks (Buchholz and Marsi, 2006; Surdeanu
et al, 2008). We also used a projective English
dataset derived from the Penn Treebank by applying
the standard head rules of Yamada and Matsumoto
(2003).11 We did not force the parser to output pro-
jective trees or unique roots for any of the datasets;
everything is learned from the data. We trained by
running 10 iterations of the cost-augmented MIRA
algorithm (Crammer et al, 2006) with LP-relaxed
decoding, as in Martins et al (2009b). Follow-
ing common practice (Charniak and Johnson, 2005;
Carreras et al, 2008), we employed a coarse-to-fine
procedure to prune away unlikely candidate arcs, as
described by Koo and Collins (2010). To ensure
valid parse trees at test time, we rounded fractional
11As usual, we train on sections ?02?21, use ?22 as validation
data, and test on ?23. We ran SVMTool (Gime?nez and Marquez,
2004) to obtain automatic part-of-speech tags for ?22?23.
244
solutions as described in Martins et al (2009a) (yet,
solutions were integral most of the time).
The parts used in our full model are the ones
depicted in Fig. 1. Note that a subgradient-based
method could handle some of those parts efficiently
(arcs, consecutive siblings, grandparents, and head
bigrams) by composing arc-factored models, head
automata, and a sequence labeler. However, no
lightweight decomposition seems possible for incor-
porating parts for all siblings, directed paths, and
non-projective arcs. Tab. 1 shows the first-order
logical formulae that encode the constraints in our
model. Each formula gives rise to a subproblem
which is efficiently solvable (see ?4). By ablating
some of rows of Tab. 1 we recover known methods:
? Resorting to the tree and consecutive sibling for-
mulae gives one of the models in Koo et al
(2010), with the same linear relaxation (a proof
of this fact is included in App. F);
? Resorting to tree, all siblings, grandparent, and
non-projective arcs, recovers a multi-commodity
flow configuration proposed by Martins et al
(2009a); the relaxation is also the same.12
The experimental results are shown in Tab. 2.
For comparison, we include the best published re-
sults for each dataset (at the best of our knowledge),
among transition-based parsers (Nivre et al, 2006;
Huang and Sagae, 2010), graph-based parsers (Mc-
Donald et al, 2006; Koo and Collins, 2010), hybrid
methods (Nivre and McDonald, 2008; Martins et al,
2008), and turbo parsers (Martins et al, 2010; Koo
et al, 2010). Our full model achieved the best re-
ported scores for 7 datasets. The last two columns
show a consistent improvement (with the exceptions
of Chinese and Arabic) when using the full set of
features over a second order model with grandparent
and consecutive siblings, which is our reproduction
of the model of Koo et al (2010).13
12Although Martins et al (2009a) also incorporated consec-
utive siblings in one of their configurations, our constraints are
tighter than theirs. See App. F.
13Note however that the actual results of Koo et al (2010)
are higher than our reproduction, as can be seen in the second
column. The differences are due to the features that were used
and on the way the models were trained. The cause is not search
error: exact decoding with an ILP solver (CPLEX) revealed no
significant difference with respect to our G+CS column. We
leave further analysis for future work.
Best known UAS G+CS Full
Arabic 80.18 [Ma08] 81.12 81.10 (-0.02)
Bulgar. 92.88 [Ma10] 93.04 93.50 (+0.46)
Chinese 91.89 [Ma10] 91.05 90.62 (-0.43)
Czech 88.78 [Ma10] 88.80 89.46 (+0.66)
English 92.57 [Ko10] 92.45 92.68 (+0.23)
Danish 91.78 [Ko10] 91.70 91.86 (+0.16)
Dutch 85.81 [Ko10] 84.77 85.53 (+0.76)
German 91.49 [Ma10] 91.29 91.89 (+0.60)
Japane. 93.42 [Ma10] 93.62 93.72 (+0.10)
Portug. 93.03 [Ko10] 92.05 92.29 (+0.24)
Slovene 86.21 [Ko10] 86.09 86.95 (+0.86)
Spanish 87.04 [Ma10] 85.99 86.74 (+0.75)
Swedish 91.36 [Ko10] 89.94 90.16 (+0.22)
Turkish 77.55 [Ko10] 76.24 76.64 (+0.40)
PTB ?23 93.04 [KC10] 92.19 92.53 (+0.34)
Table 2: Unlabeled attachment scores, excluding punc-
tuation. In the second column, [Ma08] denotes Martins
et al (2008), [KC10] is Koo and Collins (2010), [Ma10]
is Martins et al (2010), and [Ko10] is Koo et al (2010).
In columns 3?4, ?Full? is our full model, and ?G+CS? is
our reproduction of the model of Koo et al (2010), i.e.,
the same as ?Full? but with all features ablated excepted
for grandparents and consecutive siblings.
AF +G+CS +AS +NP Full
PTB ?22 91.02 92.13 92.32 92.36 92.41
PTB ?23 91.36 92.19 92.41 92.50 92.53
Table 3: Feature ablation experiments. AF is an arc-
factored model; +G+CS adds grandparent and consec-
utive siblings; +AS adds all-siblings; +NP adds non-
projective arcs; Full adds the bigram and directed paths.
Feature ablation and error analysis. We con-
ducted a simple ablation study by training several
models on the English PTB with different sets of
features. Tab. 3 shows the results. As expected, per-
formance keeps increasing as we use models with
greater expressive power. We show some concrete
examples in App. G of sentences that the full model
parsed correctly, unlike less expressive models.
Convergence speed and optimality. Fig. 2 com-
pares the performance of DD-ADMM and the sub-
gradient algorithms in the validation section of the
PTB.14 For the second order model, the subgradient
14The learning rate in the subgradient method was set as ?t =
?0/(1+Nincr(t)), as in Koo et al (2010), whereNincr(t) is the
number of dual increases up to the tth iteration, and ?0 is chosen
to maximize dual decrease after 20 iterations (in a per sentence
basis). Those preliminary iterations are not plotted in Fig. 2.
245
method has more slaves than in Koo et al (2010):
it has a slave imposing the TREE constraint (whose
subproblems consists on finding a minimum span-
ning tree) and several for the all-sibling parts, yield-
ing an average number of 310.5 and a maximum
of 4310 slaves. These numbers are still manage-
able, and we observe that a ?good? UAS is achieved
relatively quickly. The ADMM method has many
more slaves due to the multicommodity flow con-
straints (average 1870.8, maximum 65446), yet it
attains optimality sooner, as can be observed in the
right plot. For the full model, the subgradient-based
method becomes extremely slow, and the UAS score
severely degrades (after 1000 iterations it is 2%
less than the one obtained with the ADMM-based
method, with very few instances having been solved
to optimality). The reason is the number of slaves:
in this configuration and dataset the average number
of slaves per instance is 3327.4, and the largest num-
ber is 113207. On the contrary, the ADMM method
keeps a robust performance, with a large fraction of
optimality certificates in early iterations.
Runtime and caching strategies. Despite its suit-
ability to problems with many overlapping compo-
nents, our parser is still 1.6 times slower than Koo
et al (2010) (0.34 against 0.21 sec./sent. in PTB
?23), and is far beyond the speed of transition-based
parsers (e.g., Huang and Sagae (2010) take 0.04
sec./sent. on the same data, although accuracy is
lower, 92.1%). Our implementation, however, is not
fully optimized. We next describe how considerable
speed-ups are achieved by caching the subproblems,
following a strategy similar to Koo et al (2010).
Fig. 3 illustrates the point. After a few iterations,
many variables u(r) see a consensus being achieved
(i.e., ut(r) = zt+1s (r),?s) and enter an idle state:
they are left unchanged by the u-update in Eq. 9,
and so do the Lagrange variables ?t+1s (r) (Eq. 10).
If by iteration t all variables in a subproblem s are
idle, then zt+1s (r) = zts(r), hence the subproblem
does not need to be resolved.15 Fig. 3 shows that
15Even if not all variables are idle in s, caching may still be
useful: note that the z-updates in Eq. 14 tend to be sparse for the
subproblems described in ?4 (these are Euclidean projections
onto polytopes with 0/1 vertices, which tend to hit corners). An-
other trick that may accelerate the algorithm is warm-starting:
since many subproblems involve a sort operation, storing the
sorted indexes may speedup the next round.
200 400 600 800 1000Iterations0
20
40
60
80
100
% ac
tive
Full ADMM% active msgs% active subproblems% active vars
Figure 3: Fraction of active variables, subproblems and
messages along DD-ADMM iterations (full model). The
number of active messages denotes the total number of
variables (active or not) that participate in an active factor.
10-3 10-2 10-1 100 101Time ADMM (sec.)
10-3
10-2
10-1
100
101
Time
 CPLE
X (se
c.)
Elapsed Times
Figure 4: Runtimes of DD-ADMM and CPLEX on PTB
?22 (each point is a sentence). Average runtimes are
0.362 (DD-ADMM) and 0.565 sec./sent. (CPLEX).
many variables and subproblems are left untouched
after the first few rounds.
Finally, Fig. 4 compares the runtimes of our im-
plementation of DD-ADMM with those achieved by
a state-of-the-art LP solver, CPLEX, in its best per-
forming configuration: the simplex algorithm ap-
plied to the dual LP. We observe that DD-ADMM
is faster in some regimes but slower in others. For
short sentences (< 15 words), DD-ADMM tends to
be faster. For longer sentences, CPLEX is quite ef-
fective as it uses good heuristics for the pivot steps
in the simplex algorithm; however, we observed that
it sometimes gets trapped on large problems. Note
also that DD-ADMM is not fully optimized, and that
it is much more amenable to parallelization than the
simplex algorithm, since it is composed of many in-
dependent slaves. This suggests potentially signifi-
cant speed-ups in multi-core environments.
6 Related Work
Riedel and Clarke (2006) first formulated depen-
dency parsing as an integer program, along with
logical constraints. The multicommodity flow for-
246
0 200 400 600 800 1000Iterations85
8687
8889
9091
92
UAS 
(%)
Accuracy
ADMM FullSubgrad FullADMM Sec OrdSubgrad Sec Ord
0 200 400 600 800 1000Iterations0
20
40
60
80
100
Cert
ifica
tes (
%)
Stopping Criteria
ADMM Full (Tol<0.001)ADMM Full (Exact)Subgrad Full (Exact)ADMM Sec Ord (Tol<0.001)ADMM Sec Ord (Exact)Subgrad Sec Ord (Exact)
Figure 2: UAS including punctuation (left) and fraction of optimality certificates (right) accross iterations of the
subgradient and DD-ADMM algorithms, in PTB ?22. ?Full? is our full model; ?Sec Ord? is a second-order model
with grandparents and all siblings, for which the subgradient method uses a coarser decomposition with a TREE factor.
Since subgradient and DD-ADMM are solving the same problems, the solid lines (as the dashed ones) would meet in
the limit, however subgradient converges very slowly for the full model. The right plot shows optimality certificates
for both methods, indicating that an exact solution of P has been found; for DD-ADMM we also plot the fraction of
instances that converged to an accurate solution of P ? (primal and dual residuals < 10?3) and hence can be stopped.
mulation was introduced by Martins et al (2009a),
along with some of the parts considered here. Koo
et al (2010) proposed a subgradient-based dual de-
composition method that elegantly combines head
automata with maximum spanning tree algorithms;
these parsers, as well as the loopy belief propagation
method of Smith and Eisner (2008), are all instances
of turbo parsers (Martins et al, 2010).
DD-ADMM has been proposed and theoretically
analyzed by Martins et al (2011) for problems rep-
resentable as factor graphs. The general ADMM
method has a long-standing history in optimization
(Hestenes, 1969; Powell, 1969; Glowinski and Mar-
roco, 1975; Gabay and Mercier, 1976; Boyd et al,
2011). Other methods have been recently proposed
to accelerate dual decomposition, such as Jojic et al
(2010) and Meshi and Globerson (2011) (the latter
applying ADMM in the dual rather than the primal).
While our paper shows limitations of the sub-
gradient method when there are many overlapping
components, this method may still be advantageous
over ADMM in problems that are nicely decom-
posable, since it often allows reusing existing com-
binatorial machinery. Yet, the scenario we con-
sider here is realistic in NLP, where we often have
to deal with not-lightly-decomposable constrained
problems (e.g., exploiting linguistic knowledge).
7 Conclusion
We have introduced new feature-rich turbo parsers.
Since exact decoding is intractable, we solve an LP
relaxation through a recently proposed consensus al-
gorithm, DD-ADMM, which is suitable for prob-
lems with many overlapping components. We study
the empirical runtime and convergence properties of
DD-ADMM, complementing the theoretical treat-
ment in Martins et al (2011). DD-ADMM com-
pares favourably against the subgradient method in
several aspects: it is faster to reach a consensus, it
has better stopping conditions, and it works better
in non-lightweight decompositions. While its slave
subproblems are more involved, we derived closed-
form solutions for many cases of interest, such as
first-order logic formulas and combinatorial factors.
DD-ADMM may be useful in other frameworks
involving logical constraints, such as the models
for compositional semantics presented by Liang
et al (2011). Non-logical constraints may also
yield efficient subproblems, e.g., the length con-
straints in summarization and compression (Clarke
and Lapata, 2008; Martins and Smith, 2009; Berg-
Kirkpatrick et al, 2011). Finally, DD-ADMM can
be adapted to tighten its relaxations towards exact
decoding, as in Sontag et al (2008) and Rush and
Collins (2011). We defer this for future work.
Acknowledgments
We thank all reviewers for their comments, Eric Xing for
helpful discussions, and Terry Koo and Sasha Rush for
answering questions about their parser and for providing
code. A. M. was supported by a FCT/ICTI grant through
the CMU-Portugal Program, and by Priberam. This
work was partially supported by the FET programme
(EU FP7), under the SIMBAD project (contract 213250).
N. S. was supported by NSF CAREER IIS-1054319.
247
References
M. Auli and A. Lopez. 2011. A Comparison of Loopy
Belief Propagation and Dual Decomposition for Inte-
grated CCG Supertagging and Parsing. In Proc. of
ACL.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On for-
mal properties of simple phrase structure grammars.
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. of ACL.
D. Bertsekas, W. Hager, and O. Mangasarian. 1999.
Nonlinear programming. Athena Scientific.
D.P. Bertsekas, A. Nedic, and A.E. Ozdaglar. 2003. Con-
vex analysis and optimization. Athena Scientific.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein.
2011. Distributed Optimization and Statistical Learn-
ing via the Alternating Direction Method of Multipli-
ers. Now Publishers (to appear).
J.P. Boyle and R.L. Dykstra. 1986. A method for find-
ing projections onto the intersections of convex sets in
Hilbert spaces. In Advances in order restricted statis-
tical inference, pages 28?47. Springer Verlag.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In CONLL.
X. Carreras. 2007. Experiments with a higher-order pro-
jective dependency parser. In CoNLL.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
Proc. ACL, pages 173?180. Association for Computa-
tional Linguistics Morristown, NJ, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
computational linguistics, 33(2):201?228.
J. Clarke and M. Lapata. 2008. Global Inference for Sen-
tence Compression An Integer Linear Programming
Approach. JAIR, 31:399?429.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Algo-
rithms. JMLR, 7:551?585.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra.
2008. Efficient projections onto the L1-ball for learn-
ing in high dimensions. In ICML.
J. Eckstein and D. Bertsekas. 1992. On the Douglas-
Rachford splitting method and the proximal point al-
gorithm for maximal monotone operators. Mathemat-
ical Programming, 55(1):293?318.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proceedings of ACL-08: HLT, pages 959?967.
D. Gabay and B. Mercier. 1976. A dual algorithm for
the solution of nonlinear variational problems via finite
element approximation. Computers and Mathematics
with Applications, 2(1):17?40.
J. Gime?nez and L. Marquez. 2004. Svmtool: A gen-
eral pos tagger generator based on support vector ma-
chines. In Proc. of LREC.
R. Glowinski and P. Le Tallec. 1989. Augmented La-
grangian and operator-splitting methods in nonlinear
mechanics. Society for Industrial Mathematics.
R. Glowinski and A. Marroco. 1975. Sur
l?approximation, par e?le?ments finis d?ordre un, et la
re?solution, par penalisation-dualite?, d?une classe de
proble`mes de Dirichlet non line?aires. Rev. Franc. Au-
tomat. Inform. Rech. Operat., 9:41?76.
M. Hestenes. 1969. Multiplier and gradient methods.
Jour. Optim. Theory and Applic., 4:302?320.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of ACL,
pages 1077?1086.
R. Johansson and P. Nugues. 2008. Dependency-based
Semantic Role Labeling of PropBank. In EMNLP.
V. Jojic, S. Gould, and D. Koller. 2010. Accelerated dual
decomposition for MAP inference. In ICML.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL, pages 1?11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
A. Kulesza and F. Pereira. 2007. Structured Learning
with Approximate Inference. NIPS.
P. Liang, M.I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proc.
Association for Computational Linguistics (ACL).
A. F. T. Martins and N. A. Smith. 2009. Summarization
with a joint model for sentence extraction and com-
pression. In NAACL-HLT Workshop on Integer Linear
Programming for NLP.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009a.
Concise integer linear programming formulations for
dependency parsing. In ACL-IJCNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009b.
Polyhedral outer approximations with application to
natural language parsing. In ICML.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers:
Dependency parsing by approximate variational infer-
ence. In EMNLP.
248
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011. An Augmented
Lagrangian Approach to Constrained MAP Inference.
In ICML.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In CoNLL.
O. Meshi and A. Globerson. 2011. An Alternating Direc-
tion Method for Dual MAP LP Relaxation. In ECML
PKDD.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
ACL-HLT.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Procs. of CoNLL.
S. Petrov and D. Klein. 2008. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
Proc. of EMNLP.
M. Powell. 1969. A method for nonlinear constraints in
minimization problems. In R. Fletcher, editor, Opti-
mization, pages 283?298. Academic Press.
M. Richardson and P. Domingos. 2006. Markov logic
networks. Machine Learning, 62(1):107?136.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In EMNLP.
A. M. Rush and M. Collins. 2011. Exact decoding of
syntactic translation models through lagrangian relax-
ation. In ACL.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming relax-
ations for natural language processing. In EMNLP.
N. Shor. 1985. Minimization methods for non-
differentiable functions. Springer.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and
T Jaakkola. 2008. Tightening LP relaxations for MAP
using message-passing. In UAI.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
R.W. Tromble and J. Eisner. 2006. A fast finite-state
relaxation method for enforcing global constraints on
sequence decoding. In Proc. of NAACL, pages 423?
430.
M. Wainwright and M. Jordan. 2008. Graphical Models,
Exponential Families, and Variational Inference. Now
Publishers.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT.
249
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1500?1511,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structured Sparsity in Structured Prediction
Andre? F. T. Martins?? Noah A. Smith? Pedro M. Q. Aguiar? Ma?rio A. T. Figueiredo?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Instituto de Sistemas e Robo?tica, Instituto Superior Te?cnico, Lisboa, Portugal
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, Lisboa, Portugal
{afm,nasmith}@cs.cmu.edu, aguiar@isr.ist.utl.pt, mtf@lx.it.pt
Abstract
Linear models have enjoyed great success in
structured prediction in NLP. While a lot of
progress has been made on efficient train-
ing with several loss functions, the problem
of endowing learners with a mechanism for
feature selection is still unsolved. Common
approaches employ ad hoc filtering or L1-
regularization; both ignore the structure of the
feature space, preventing practicioners from
encoding structural prior knowledge. We fill
this gap by adopting regularizers that promote
structured sparsity, along with efficient algo-
rithms to handle them. Experiments on three
tasks (chunking, entity recognition, and de-
pendency parsing) show gains in performance,
compactness, and model interpretability.
1 Introduction
Models for structured outputs are in demand across
natural language processing, with applications in in-
formation extraction, parsing, and machine transla-
tion. State-of-the-art models usually involve linear
combinations of features and are trained discrim-
inatively; examples are conditional random fields
(Lafferty et al, 2001), structured support vector
machines (Altun et al, 2003; Taskar et al, 2003;
Tsochantaridis et al, 2004), and the structured per-
ceptron (Collins, 2002a). In all these cases, the un-
derlying optimization problems differ only in the
choice of loss function; choosing among them has
usually a small impact on predictive performance.
In this paper, we are concerned with model se-
lection: which features should be used to define the
prediction score? The fact that models with few
features (?sparse? models) are desirable for several
reasons (compactness, interpretability, good gener-
alization) has stimulated much research work which
has produced a wide variety of methods (Della Pietra
et al, 1997; Guyon and Elisseeff, 2003; McCallum,
2003). Our focus is on methods which embed this
selection into the learning problem via the regular-
ization term. We depart from previous approaches
in that we seek to make decisions jointly about all
candidate features, and we want to promote sparsity
patterns that go beyond the mere cardinality of the
set of features. For example, we want to be able to
select entire feature templates (rather than features
individually), or to make the inclusion of some fea-
tures depend on the inclusion of other features.
We achieve the goal stated above by employ-
ing regularizers which promote structured sparsity.
Such regularizers are able to encode prior knowl-
edge and guide the selection of features by model-
ing the structure of the feature space. Lately, this
type of regularizers has received a lot of attention
in computer vision, signal processing, and compu-
tational biology (Zhao et al, 2009; Kim and Xing,
2010; Jenatton et al, 2009; Obozinski et al, 2010;
Jenatton et al, 2010; Bach et al, 2011). Eisenstein
et al (2011) employed structured sparsity in com-
putational sociolinguistics. However, none of these
works have addressed structured prediction. Here,
we combine these two levels of structure: struc-
ture in the output space, and structure in the feature
space. The result is a framework that allows build-
ing structured predictors with high predictive power,
while reducing manual feature engineering. We ob-
tain models that are interpretable, accurate, and of-
ten much more compact than L2-regularized ones.
Compared with L1-regularized models, ours are of-
ten more accurate and yield faster runtime.
1500
2 Structured Prediction
We address structured prediction problems, which
involve an input set X (e.g., sentences) and an out-
put set Y, assumed large and structured (e.g., tags or
parse trees). We assume that each x ? X has a set
of candidate outputs Y(x) ? Y. We consider linear
models, in which predictions are made according to
y? = arg maxy?Y(x) ? ? ?(x, y), (1)
where ?(x, y) ? RD is a vector of features, and ? ?
RD is the vector of corresponding weights. Let D =
{?xi, yi?}Ni=1 be a training sample. We assume a cost
function is defined such that c(y?, y) is the cost of
predicting y? when the true output is y; our goal is to
learn ? with small expected cost on unseen data. To
achieve this goal, linear models are usually trained
by solving a problem of the form
?? = arg min? ?(?) + 1N
?N
i=1 L(?, xi, yi), (2)
where ? is a regularizer and L is a loss function.
Examples of losses are: the negative conditional log-
likelihood used in CRFs (Lafferty et al, 2001),
LCRF(?, x, y) = ? logP?(y|x), (3)
where P?(y|x) ? exp(? ? ?(x, y)) is a log-linear
model; the margin rescaled loss of structured SVMs
(Taskar et al, 2003; Tsochantaridis et al, 2004),
LSVM(?, x, y) = max
y??Y(x)
? ? ??(y?) + c(y?, y), (4)
where ??(y?) = ?(x, y?)??(x, y); and the loss un-
derlying the structured perceptron (Collins, 2002a),
LSP(?, x, y) = maxy??Y(x) ? ? ??(y?). (5)
Empirical comparison among these loss functions
can be found in the literature (see, e.g., Martins et al,
2010, who also consider interpolations of the losses
above). In practice, it has been observed that the
choice of loss has far less impact than the model de-
sign and choice of features. Hence, in this paper,
we focus our attention on the regularization term in
Eq. 2. We specifically address ways in which this
term can be used to help design the model by pro-
moting structured sparsity. While this has been a
topic of intense research in signal processing and
computational biology (Jenatton et al, 2009; Liu
and Ye, 2010; Bach et al, 2011), it has not yet re-
ceived much attention in the NLP community, where
the choice of regularization for supervised learning
has essentially been limited to the following:
? L2-regularization (Chen and Rosenfeld, 2000):
?L2? (?) , ?2???22 = ?2
?D
d=1 ?2d; (6)
? L1-regularization (Kazama and Tsujii, 2003;
Goodman, 2004):
?L1? (?) , ????1 = ??Dd=1 |?d|. (7)
The latter is known as ?Lasso,? as popularized by
Tibshirani (1996) in the context of sparse regres-
sion. In the two cases above, ? and ? are nonneg-
ative coefficients controlling the intensity of the reg-
ularization. ?L2? usually leads to easier optimizationand robust performance; ?L1? encourages sparser
models, where only a few features receive nonzero
weights; see Gao et al (2007) for an empirical com-
parison. More recently, Petrov and Klein (2008b)
applied L1 regularization for structure learning in
phrase-based parsing; a comparison with L2 appears
in Petrov and Klein (2008a). Elastic nets interpolate
between L1 and L2, having been proposed by Zou
and Hastie (2005) and used by Lavergne et al (2010)
to regularize CRFs.
Neither of the regularizers just described ?looks?
at the structure of the feature space, since they all
treat each dimension independently?we call them
unstructured regularizers, as opposed to the struc-
tured ones that we next describe.
3 Structured Sparsity
We are interested in regularizers that share with ?L1?
the ability to promote sparsity, so that they can be
used for selecting features. In addition, we want to
endow the feature space RD with additional struc-
ture, so that features are not penalized individually
(as in the L1-case) but collectively, encouraging en-
tire groups of features to be discarded. The choice of
groups will allow encoding prior knowledge regard-
ing the kind of sparsity patterns that are intended in
the model. This can be achieved with group-Lasso
regularization, which we next describe.
1501
3.1 The Group Lasso
To capture the structure of the feature space, we
group our D features into M groups G1, . . . , GM ,
where each Gm ? {1, . . . , D}. Ahead, we dis-
cuss meaningful ways of choosing group decompo-
sitions; for now, let us assume a sensible choice is
obvious to the model designer. Denote by ?m =
??d?d?Gm the subvector of those weights that cor-
respond to the features in the m-th group, and let
d1, . . . , dM be nonnegative scalars (one per group).
We consider the following group-Lasso regularizers:
?GLd =
?M
m=1 dm??m?2. (8)
These regularizers were first proposed by Bakin
(1999) and Yuan and Lin (2006) in the context of re-
gression. If d1 = . . . = dM , ?GLd becomes the ?L1
norm of the L2 norms.? Interestingly, this is also
a norm, called the mixed L2,1-norm.1 These regu-
larizers subsume the L1 and L2 cases, which corre-
spond to trivial choices of groups:
? If each group is a singleton, i.e., M = D and
Gd = {?d}, and d1 = . . . = dM = ? , we recover
L1-regularization (cf. Eqs. 7?8).
? If there is a single group spanning all the features,
i.e., M = 1 and G1 = {1, . . . , D}, then the right
hand side of Eq. 8 becomes d1???2. This is equiv-
alent to L2 regularization.2
We next present some non-trivial examples con-
cerning different topologies of G = {G1, . . . , GM}.
Non-overlapping groups. Let us first consider
the case where G is a partition of the feature
space: the groups cover all the features (?mGm =
{1, . . . , D}), and they do not overlap (Ga?Gb = ?,
?a 6= b). Then, ?GLd is termed a non-overlapping
group-Lasso regularizer. It encourages sparsity pat-
terns in which entire groups are discarded. A ju-
dicious choice of groups can lead to very compact
1In the statistics literature, such mixed-norm regularizers,
which group features and then apply a separate norm for each
group, are called composite absolute penalties (Zhao et al,
2009); other norms besides L2,1 can be used, such as L?,1
(Quattoni et al, 2009; Wright et al, 2009; Eisenstein et al,
2011).
2Note that Eqs. 8 and 6 do not become exactly the same: in
Eq. 6, the L2 norm is squared. However it can be shown that
both regularizers lead to identical learning problems (Eq. 2) up
to a transformation of the regularization constant.
models and pinpoint relevant groups of features.
The following examples lie in this category:
? The two cases above (L1 and L2 regularization).
? Label-based groups. In multi-label classification,
where Y = {1, . . . , L}, features are typically de-
signed as conjunctions of input features with la-
bel indicators, i.e., they take the form ?(x, y) =
?(x)? ey, where ?(x) ? RDX , ey ? RL has all
entries zero except the y-th entry, which is 1, and
? denotes the Kronecker product. Hence ?(x, y)
can be reshaped as aDX -by-Lmatrix, and we can
let each group correspond to a row. In this case,
all groups have the same size and we typically set
d1 = . . . = dM . A similar design can be made
for sequence labeling problems, by considering a
similar grouping for the unigram features.3
? Template-based groups. In NLP, features are com-
monly designed via templates. For example, a
template such as w0 ? p0 ? p?1 denotes the word
in the current position (w0) conjoined with its
part-of-speech (p0) and that of the previous word
(p?1). This template encloses many features cor-
responding to different instantiantions of w0, p0,
and p?1. In ?5, we learn feature templates from
the data, by associating each group to a feature
template, and letting that group contain all fea-
tures that are instantiations of this template. Since
groups have different sizes, it is a good idea to
let dm increase with the group size, so that larger
groups pay a larger penalty for being included.
Tree-structured groups. More generally, we may
let the groups in G overlap but be nested, i.e., we may
want them to form a hierarchy (two distinct groups
either have empty intersection or one is contained in
the other). This induces a partial order on G (the set
inclusion relation ?), endowing it with the structure
of a partially ordered set (poset).
A convenient graphical representation of the poset
?G,?? is its Hasse diagram. Each group is a node
in the diagram, and an arc is drawn from group Ga
to group Gb if Gb ? Ga and there is no b? s.t.
Gb ? Gb? ? Ga. When the groups are nested, this
diagram is a forest (a union of directed trees). The
corresponding regularizer enforces sparsity patterns
3The same idea is also used in multitask learning, where
labels correspond to tasks (Caruana, 1997).
1502
where a group of features is only selected if all its
ancestors are also selected.4 Hence, entire subtrees
in the diagram can be pruned away. Examples are:
? The elastic net. The diagram of G has a root node
for G1 = {1, . . . , D} and D leaf nodes, one per
each singleton group (see Fig. 1).
? The sparse group-Lasso. This regularizer was
proposed by Friedman et al (2010):
?SGLd,? (?) =
?M ?
m=1 (dm??m?2 + ?m??m?1) ,
(9)
where the total number of groups is M = M ? +
D, and the components ?1, . . . ,?M ? are non-
overlapping. This regularizer promotes sparsity
at both group and feature levels (i.e., it eliminates
entire groups and sparsifies within each group).
Graph-structured groups. In general, the groups
in G may overlap without being nested. In this case,
the Hasse diagram of G is a directed acyclic graph
(DAG). As in the tree-structured case, a group of
features is only selected if all its ancestors are also
selected. Based on this property, Jenatton et al
(2009) suggested a way of reverse engineering the
groups from the desired sparsity pattern. We next
describe a strategy for coarse-to-fine feature tem-
plate selection that directly builds on that idea.
Suppose that we are given M feature templates
T = {T1, . . . , TM} which are partially ordered ac-
cording to some criterion, such that if Ta  Tb we
would like to include Tb in our model only if Ta
is also included. This criterion could be a measure
of coarseness: we may want to let coarser part-of-
speech features precede finer lexical features, e.g.,
p0 ? p1  w0 ? w1, or conjoined features come af-
ter their elementary parts, e.g., p0  p0 ? p1. The
order does not need to be total, so some templates
may not be comparable (e.g., we may want p0 ? p?1
and p0 ? p1 not to be comparable). To achieve
the sparsity pattern encoded in ?T,?, we choose
G = ?G1, . . . , GM ? as follows: let I(Ta) be the
set of features that are instantiations of template Ta;
then define Ga = ?b:ab I(Tb), for a = 1, . . . ,M .
It is easy to see that ?G,?? and ?T,? are isomorph
posets (their Hasse diagrams have the same shape;
4We say that a group of features Gm is selected if some fea-
ture in Gm (but not necessarily all) has a nonzero weight.
see Fig. 1). The result is a ?coarse-to-fine? regular-
izer, which prefers to select feature templates that
are coarser before zooming into finer features.
3.2 Bayesian Interpretation
The prior knowledge encoded in the group-Lasso
regularizer (Eq. 8) comes with a Bayesian inter-
pretation, as we next describe. In a probabilistic
model (e.g. in the CRF case, where L = LCRF),
the optimization problem in Eq. 2 can be seen as
maximum a posteriori estimation of ?, where the
regularization term ?(?) corresponds to the neg-
ative log of a prior distribution (call it p(?)). It
is well-known that L2-regularization corresponds to
choosing independent zero-mean Gaussian priors,
?d ? N(0, ??1), and that L1-regularization results
from adopting zero-mean Laplacian priors, p(?d) ?
exp(? |?d|).
Figueiredo (2002) provided an alternative inter-
pretation of L1-regularization in terms of a two-
level hierarchical Bayes model, which happens to
generalize to the non-overlapping group-Lasso case,
where ? = ?GLd . As in the L2-case, we also assume
that each parameter receives a zero-mean Gaussian
prior, but now with a group-specific variance ?m,
i.e., ?m ? N(0, ?mI) for m = 1, . . . ,M . This
reflects the fact that some groups should have their
feature weights shrunk more towards zero than oth-
ers. The variances ?m ? 0 are not pre-specified but
rather generated by a one-sided exponential hyper-
prior p(?m|dm) ? exp(?d2m?m/2). It can be shown
that after marginalizing out ?m, we obtain
p(?m|dm) =
? ?
0
p(?m|?m)p(?m|dm)d?m
? exp (?dm??m?) . (10)
Hence, the non-overlapping group-Lasso corre-
sponds to the following two-level hierachical Bayes
model: independently for each m = 1, . . . ,M ,
?m ? Exp(d2m/2), ?m ? N(0, ?mI). (11)
3.3 Prox-operators
Before introducing our learning algorithm for han-
dling group-Lasso regularization, we need to define
the concept of a ?-proximity operator. This is the
function prox? : RD ? RD defined as follows:
prox?(?) = arg min?? 12??? ? ??2 + ?(??). (12)
1503
Figure 1: Hasse diagrams of several group-
based regularizers. For all tree-structured
cases, we use the same plate notation that
is traditionally used in probabilistic graphical
models. The rightmost diagram represents a
coarse-to-fine regularizer: each node is a tem-
plate involving contiguous sequences of words
(w) and POS tags (p); the symbol order ? 
p  w induces a template order (Ta  Tb
iff at each position i [Ta]i  [Tb]i). Digits
below each node are the group indices where
each template belongs.
Proximity operators generalize Euclidean projec-
tions and have many interesting properties; see Bach
et al (2011) for an overview. By requiring zero to be
a subgradient of the objective function in Eq. 12, we
obtain the following closed expression (called soft-
thresholding) for the ?L1? -proximity operator:
[prox?L1? (?)]d =
?
?
?
?d ? ? if ?d > ?
0 if |?d| ? ?
?d + ? if ?d < ?? .
(13)
For the non-overlapping group Lasso case, the prox-
imity operator is given by
[prox?GLd (?)]m =
{
0 if ??m?2 ? dm
??m?2?dm
??m?2 ?m otherwise.
(14)
which can be seen as a generalization of Eq. 13: if
the L2-norm of the m-th group is less than dm, the
entire group is discarded; otherwise it is scaled so
that its L2-norm decreases by an amount of dm.
When groups overlap, the proximity operator
lacks a closed form. When G is tree-structured, it
can still be efficiently computed by a recursive pro-
cedure (Jenatton et al, 2010). When G is not tree-
structured, no specialized procedure is known, and a
convex optimizer is necessary to solve Eq. 12.
4 Online Prox-Grad Algorithm
We now turn our attention to efficient ways of han-
dling group-Lasso regularizers. Several fast and
scalable algorithms having been proposed for train-
ing L1-regularized CRFs, based on quasi-Newton
optimization (Andrew and Gao, 2007), coordinate
descent (Sokolovska et al, 2010; Lavergne et al,
2010), and stochastic gradients (Carpenter, 2008;
Langford et al, 2009; Tsuruoka et al, 2009). The
algorithm that we use in this paper (Alg. 1) extends
the stochastic gradient methods for group-Lasso reg-
ularization; a similar algorithm was used by Martins
et al (2011) for multiple kernel learning.
Alg. 1 addresses the learning problem in Eq. 2 by
alternating between online (sub-)gradient steps with
respect to the loss term, and proximal steps with
respect to the regularizer. Proximal-gradient meth-
ods are very popular in sparse modeling, both in
batch (Liu and Ye, 2010; Bach et al, 2011) and on-
line (Duchi and Singer, 2009; Xiao, 2009) settings.
The reason we have chosen the algorithm of Martins
et al (2011) is that it effectively handles overlap-
ping groups, without the need of evaluating prox?
(which, as seen in ?3.3, can be costly if G is not tree-
structured). To do so, it decomposes ? as
?(?) = ?Jj=1 ?j?j(?) (15)
for some J ? 1, and nonnegative ?1, . . . , ?J ; each
?j-proximal operator is assumed easy to compute.
Such a decomposition always exists: if G does not
have overlapping groups, take J = 1. Otherwise,
find J ? M disjoint sets G1, . . . ,GJ such that?J
j=1 Gj = G and the groups on each Gj are non-
overlapping. The proximal steps are then applied
sequentially, one per each ?j . Overall, Alg. 1 satis-
fies the following important requirements:
? Computational efficiency. Each gradient step at
round t is linear in the number of features that
fire for that instance and independent of the total
number of features D. Each proximal step is lin-
ear in the number of groupsM , and does not need
be to performed every round (as we will see later).
1504
Algorithm 1 Online Sparse Prox-Grad Algorithm
1: input: D, ??j?Jj=1, T , gravity sequence
???jt?Jj=1?Tt=1, stepsize sequence ??t?Tt=1
2: initialize ? = 0
3: for t = 1 to T do
4: take training pair ?xt, yt? ? D
5: ? ? ? ? ?t?L(?;xt, yt) (gradient step)
6: for j = 1 to J do
7: ? = prox?t?jt?j (?) (proximal step)
8: end for
9: end for
10: output: ?
? Memory efficiency. Only a small active set of fea-
tures (those that have nonzero weights) need to
be maintained. Entire groups of features can be
deleted after each proximal step. Furthermore,
only the features which correspond to nonzero en-
tries in the gradient vector need to be inserted in
the active set; for some losses (LSVM and LSP)
many irrelevant features are never instantianted.
? Convergence. With high probability, Alg. 1 pro-
duces an -accurate solution after T ? O(1/2)
rounds, for a suitable choice of stepsizes and hold-
ing ?jt constant, ?jt = ?j (Martins et al, 2011).
This result can be generalized to any sequence
??jt?Tt=1 such that ?j = 1T
?T
t=1 ?jt.
We next describe several algorithmic ingredients
that make Alg. 1 effective in sparse modeling.
Budget-Driven Shrinkage. Alg. 1 requires the
choice of a ?gravity sequence.? We follow Lang-
ford et al (2009) and set ??jt?Jj=1 to zero for all t
which is not a multiple of some prespecified integer
K; this way, proximal steps need only be performed
eachK rounds, yielding a significant speed-up when
the number of groups M is large. A direct adop-
tion of the method of Langford et al (2009) would
set ?jt = K?j for those rounds; however, we have
observed that such a strategy makes the number of
groups vary substantially in early epochs. We use a
different strategy: for each Gj , we specify a budget
of Bj ? 0 groups (this may take into consideration
practical limitations, such as the available memory).
If t is a multiple of K, we set ?jt as follows:
1. If Gj does not have more than Bj nonzero
groups, set ?jt = 0 and do nothing.
2. Otherwise, sort the groups in Gj by decreasing
order of their L2-norms. Check the L2-norms
of the Bj-th and Bj+1-th entries in the list and
set ?jt as the mean of these two divided by ?t.
3. Apply a ?t?jt?j-proximal step using Eq. 14.
At the end of this step, no more than Bj groups
will remain nonzero.5
If the average of the gravity steps converge,
limT?? 1T
?T
t=1 ?jt ? ?j , then the limit points
?j implicitly define the regularizer, via ? =?J
j=1 ?j?j .6 Hence, we have shifted the control of
the amount of regularization to the budget constants
Bj , which unlike the ?j have a clear meaning and
can be chosen under practical considerations.
Space and Time Efficiency. The proximal steps
in Alg. 1 have a scaling effect on each group, which
affects all features belonging to that group (see
Eq. 14). We want to avoid explicitly updating each
feature in the active set, which could be time con-
suming. We mention two strategies that can be used
for the non-overlapping group Lasso case.
? The first strategy is suitable when M is large and
only a few groups ( M ) have features that fire
in each round; this is the case, e.g., of label-based
groups (see ?3.1). It consists of making lazy up-
dates (Carpenter, 2008), i.e., to delay the update
of all features in a group until at least one of
them fires; then apply a cumulative penalty. The
amount of the penalty can be computed if one as-
signs a timestamp to each group.
? The second strategy is suitable when M is small
and some groups are very populated; this is the
typical case of template-based groups (?3.1). Two
operations need to be performed: updating each
feature weight (in the gradient steps), and scaling
entire groups (in the proximal steps). We adapt
a trick due to Shalev-Shwartz et al (2007): repre-
sent the weight vector of them-th group, ?m, by a
5When overlaps exist (e.g. the coarse-to-fine case), we spec-
ify a total pseudo-budget B ignoring the overlaps, which in-
duces budgets B1, . . . , BJ which sum to B. The number of
actually selected groups may be less than B, however, since in
this case some groups can be shrunk more than once. Other
heuristics are possible.
6The convergence assumption can be sidestepped by freez-
ing the ?j after a fixed number of iterations.
1505
triple ??m, cm, ?m? ? R|Gm|?R+?R+, such that
?m = cm?m and ??m?2 = ?m. This representa-
tion allows performing the two operations above
in constant time, and it keeps track of the group
L2-norms, necessary in the proximal updates.
For sufficient amounts of regularization, our al-
gorithm has a low memory footprint. Only features
that, at some point, intervene in the gradient com-
puted in line 5 need to be instantiated; and all fea-
tures that receive zero weights after some proximal
step can be deleted from the model (cf. Fig. 2).
Sparseptron and Debiasing. Although Alg. 1 al-
lows to simultaneously select features and learn the
model parameters, it has been observed in the sparse
modeling literature that Lasso-like regularizers usu-
ally have a strong bias which may harm predictive
performance. A post-processing stage is usually
taken (called debiasing), in which the model is re-
fitted without any regularization and using only the
selected features (Wright et al, 2009). If a final de-
biasing stage is to be performed, Alg. 1 only needs
to worry about feature selection, hence it is appeal-
ing to choose a loss function that makes this pro-
cedure as simple as possible. Examining the input
of Alg. 1, we see that both a gravity and a stepsize
sequence need to be specified. The former can be
taken care of by using budget-driven shrinkage, as
described above. The stepsize sequence can be set
as ?t = ?0/
?
dt/Ne, which ensures convergence,
however ?0 requires tuning. Fortunately, for the
structured perceptron loss LSP (Eq. 5), Alg. 1 is in-
dependent of ?0, up to a scaling of ?, which does not
affect predictions (see Eq. 1).7 We call the instanti-
ation of Alg. 1 with a group-Lasso regularizer and
the loss LSP the sparseptron. Overall, we propose
the following two-stage approach:
1. Run the sparsepton for a few epochs and dis-
card the features with zero weights.
2. Refit the model without any regularization and
using the loss L which one wants to optimize.
7To see why this is the case, note that both gradient and
proximal updates come scaled by ?0; and that the gradient of
the loss is?LSP(?, xt, yt) = ?(xt, y?t)? ?(xt, yt), where y?t
is the prediction under the current model, which is insensitive to
the scaling of ?. This independence on ?0 does not hold when
the loss is LSVM or LCRF.
5 Experiments
We present experiments in three structured predic-
tion tasks for several group choices.
Text Chunking. We use the English dataset pro-
vided in the CoNLL 2000 shared task (Sang and
Buchholz, 2000), which consists of 8,936 training
and 2,012 testing sentences (sections 15?18 and 20
of the WSJ.) The input observations are the token
words and their POS tags; we want to predict the
sequences of IOB tags representing phrase chunks.
We built 96 contextual feature templates as follows:
? Up to 5-grams of POS tags, in windows of 5 to-
kens on the left and 5 tokens on the right;
? Up to 3-grams of words, in windows of 3 tokens
on the left and 3 tokens on the right;
? Up to 2-grams of word shapes, in windows of
2 tokens on the left and 2 tokens on the right.
Each shape replaces characters by their types
(case sensitive letters, digits, and punctuation),
and deletes repeated types?e.g., Confidence
and 2,664,098 are respectively mapped to Aa
and 0,0+,0+ (Collins, 2002b).
We defined unigram features by conjoining these
templates with each of the 22 output labels. An ad-
ditional template was defined to account for label
bigrams?features in this template do not look at the
input string, but only at consecutive pairs of labels.8
We evaluate the ability of group-Lasso regular-
ization to perform feature template selection. To
do that, we ran 5 epochs of the sparseptron algo-
rithm with template-based groups and budget-driven
shrinkage (budgets of 10, 20, 30, 40, and 50 tem-
plates were tried). For each group Gm, we set dm =
log2 |Gm|, which is the average number of bits nec-
essary to encode a feature in that group, if all fea-
tures were equiprobable. We set K = 1000 (the
number of instances between consecutive proximal
steps). Then, we refit the model with 10 iterations
of the max-loss 1-best MIRA algorithm (Crammer
et al, 2006).9 Table 1 compares the F1 scores and
8State-of-the-art models use larger output contexts, such as
label trigrams and 4-grams. We resort to bigram labels as we
are mostly interested in identifying relevant unigram templates.
9This variant optimizes theLSVM loss (Martins et al, 2010).
For the refitting, we used unregularized MIRA. For the baseline
1506
Table 1: Results for
text chunking.
MIRA Group Lasso B = 10 B = 20 B = 30 B = 40 B = 50
F1 (%) 93.10 92.99 93.28 93.59 93.42 93.40
model size (# features) 5,300,396 71,075 158,844 389,065 662,018 891,378
MIRA Lasso C = 0.1 C = 0.5 C = 1 Group-Lasso B = 100 B = 200 B = 300
Spa. dev/test 70.38/74.09 69.19/71.9 70.75/72.38 71.7/74.03 71.79/73.62 72.08/75.05 71.48/73.3
8,598,246 68,565 1,017,769 1,555,683 83,036 354,872 600,646
Dut. dev/test 69.15/71.54 64.07/66.35 66.82/69.42 70.43/71.89 69.48/72.83 71.03/73.33 71.2/72.59
5,727,004 164,960 565,704 953,668 128,320 447,193 889,660
Eng. dev/test 83.95/79.81 80.92/76.95 82.58/78.84 83.38/79.35 85.62/80.26 85.86/81.47 85.03/80.91
8,376,901 232,865 870,587 1,114,016 255,165 953,178 1,719,229
Table 2: Results for named entity recognition. Each cell shows F1 (%) and the number of features.
0 5 10 150
2
4
6 x 10
6
# Epochs
# F
eatu
res
 
 
MIRA
Sparceptron + MIRA (B=30)
Figure 2: Memory footprints of the MIRA and sparsep-
tron algorithms in text chunking. The oscillation in the
first 5 epochs (bottom line) comes from the proximal
steps each K = 1000 rounds. The features are then
frozen and 10 epochs of unregularized MIRA follow.
Overall, the sparseptron requires < 7.5% of the memory
as the MIRA baseline.
the model sizes obtained with the several budgets
against those obtained by running 15 iterations of
MIRA with the original set of features. Note that
the total number of iterations is the same; yet, the
group-Lasso approach has a much smaller memory
footprint (see Fig. 2) and yields much more com-
pact models. The small memory footprint comes
from the fact that Alg. 1 may entertain a large num-
ber of features without ever instantiating all of them.
The predictive power is comparable (although some
choices of budget yield slightly better scores for the
group-Lasso approach).10
Named Entity Recognition. We experiment with
the Spanish, Dutch, and English datasets pro-
vided in the CoNLL 2002/2003 shared tasks (Sang,
2002; Sang and De Meulder, 2003). For Span-
ish, we use the POS tags provided by Car-
(described next), we used L2-regularized MIRA and tuned the
regularization constant with cross-validation.
10We also tried label-based group-Lasso and sparse group-
Lasso (?3.1), with less impressive results (omitted for space).
reras (http://www.lsi.upc.es/?nlp/tools/
nerc/nerc.html); for English, we ignore the syn-
tactic chunk tags provided with the dataset. Hence,
all datasets have the same sort of input observations
(words and POS) and all have 9 output labels. We
use the feature templates described above plus some
additional ones (yielding a total of 452 templates):
? Up to 3-grams of shapes, in windows of size 3;
? For prefix/suffix sizes of 1, 2, 3, up to 3-grams of
word prefixes/suffixes, in windows of size 3;
? Up to 5-grams of case, punctuation, and digit in-
dicators, in windows of size 5.
As before, an additional feature template was de-
fined to account for label bigrams. We do feature
template selection (same setting as before) for bud-
get sizes of 100, 200, and 300. We compare with
both MIRA (using all the features) and the sparsep-
tron with a standard Lasso regularizer ?L1? , for sev-
eral values of C = 1/(?N). Table 2 shows the re-
sults. We observe that template-based group-Lasso
wins both in terms of accuracy and compactness.
Note also that the ability to discard feature tem-
plates (rather than individual features) yields faster
test runtime than models regularized with the stan-
dard Lasso: fewer templates will need to be instan-
tiated, with a speed-up in score computation.
Multilingual Dependency Parsing. We trained
non-projective dependency parsers for 6 languages
using the CoNLL-X shared task datasets (Buchholz
and Marsi, 2006): Arabic, Danish, Dutch, Japanese,
Slovene, and Spanish. We chose the languages with
the smallest datasets, because regularization is more
important when data is scarce. The output to be pre-
dicted from each input sentence is the set of depen-
dency links, which jointly define a spanning tree.
1507
2 4 6 8 10 12
x 106
76.5
77
77.5
78
78.5
Number of Features
UA
S (%
)
Arabic
 
 
0 5 10 15
x 106
89
89.2
89.4
89.6
89.8
90 Danish
0 2 4 6 8
x 106
92
92.5
93
93.5 Japanese
0 2 4 6 8 10
x 106
81
82
83
84 Slovene
0 0.5 1 1.5 2
x 107
82
82.5
83
83.5
84 Spanish
0 5 10 15
x 106
74
74.5
75
75.5
76 Turkish
 
 
Group?LassoGroup?Lasso (C2F)LassoFilter?based (IG)
Figure 3: Comparison between non-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-based
method based on information gain for selecting feature templates in multilingual dependency parsing. The x-axis is
the total number of features at different regularization levels, and the y-axis is the unlabeled attachment score. The
plots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method. The standard
Lasso (which does not select templates, but individual features) is also shown for comparison.
We use arc-factored models, for which exact infer-
ence is tractable (McDonald et al, 2005). We de-
fined M = 684 feature templates for each candi-
date arc by conjoining the words, shapes, lemmas,
and POS of the head and the modifier, as well as
the contextual POS, and the distance and direction
of attachment. We followed the same two-stage
approach as before, and compared with a baseline
which selects feature templates by ranking them ac-
cording to the information gain criterion. This base-
line assigns a score to each template Tm which re-
flects an empirical estimate of the mutual informa-
tion between Tm and the binary variable A that indi-
cates the presence/absence of a dependency link:
IGm ,
?
f?Tm
?
a?{0,1}
P (f, a) log2
P (f, a)
P (f)P (a) , (16)
where P (f, a) is the joint probability of feature f
firing and an arc being active (a = 1) or innactive
(a = 0), and P (f) and P (a) are the corresponding
marginals. All probabilities are estimated from the
empirical counts of events observed in the data.
The results are plotted in Fig. 3, for budget sizes
of 200, 300, and 400. We observe that for all
but one language (Spanish is the exception), non-
overlapping group-Lasso regularization is more ef-
fective at selecting feature templates than the in-
formation gain criterion, and slightly better than
coarse-to-fine group-Lasso. For completeness, we
also display the results obtained with a standard
Lasso regularizer. Table 3 shows what kind of
feature templates were most selected for each lan-
guage. Some interesting patterns can be observed:
morphologically-rich languages with small datasets
(such as Turkish and Slovene) seem to avoid lexi-
cal features, arguably due to potential for overfitting;
in Japanese, contextual POS appear to be specially
relevant. It should be noted, however, that some
of these patterns may be properties of the datasets
rather than of the languages themselves.
6 Related Work
A variant of the online proximal gradient algorithm
used in this paper was proposed by Martins et al
1508
Ara. Dan. Jap. Slo. Spa. Tur.
Bilexical ++ + +
Lex.? POS + +
POS? Lex. ++ + + + +
POS? POS ++ +
Middle POS ++ ++ ++ ++ ++ ++
Shape ++ ++ ++ ++
Direction + + + + +
Distance ++ + + + + +
Table 3: Variation of feature templates that were selected
accross languages. Each line groups together similar tem-
plates, involving lexical, contextual POS, word shape in-
formation, as well as attachment direction and length.
Empty cells denote that very few or none of the templates
in that category was selected; + denotes that some were
selected; ++ denotes that most or all were selected.
(2011), along with a theoretical analysis. The fo-
cus there, however, was multiple kernel learning,
hence overlapping groups were not considered in
their experiments. Budget-driven shrinkage and the
sparseptron are novel techniques, at the best of our
knowledge. Apart from Martins et al (2011), the
only work we are aware of which combines struc-
tured sparsity with structured prediction is Schmidt
and Murphy (2010); however, their goal is to pre-
dict the structure of graphical models, while we
are mostly interested in the structure of the feature
space. Schmidt and Murphy (2010) used to gener-
ative models, while our approach emphasizes dis-
criminative learning.
Mixed norm regularization has been used for a
while in statistics as a means to promote structured
sparsity. Group Lasso is due to Bakin (1999) and
Yuan and Lin (2006), after which a string of variants
and algorithms appeared (Bach, 2008; Zhao et al,
2009; Jenatton et al, 2009; Friedman et al, 2010;
Obozinski et al, 2010). The flat (non-overlapping)
case has tight links with learning formalisms such
as multiple kernel learning (Lanckriet et al, 2004)
and multi-task learning (Caruana, 1997). The tree-
structured case has been addressed by Kim and Xing
(2010), Liu and Ye (2010) and Mairal et al (2010),
along with L?,1 and L2,1 regularization. Graph-
structured groups are discussed in Jenatton et al
(2010), along with a DAG representation. In NLP,
mixed norms have been used recently by Grac?a et al
(2009) in posterior regularization, and by Eisenstein
et al (2011) in a multi-task regression problem.
7 Conclusions
In this paper, we have explored two levels of struc-
ture in NLP problems: structure on the outputs, and
structure on the feature space. We have shown how
the latter can be useful in model design, through the
use of regularizers which promote structured spar-
sity. We propose an online algorithm with mini-
mal memory requirements for exploring large fea-
ture spaces. Our algorithm, which specializes into
the sparseptron, yields a mechanism for selecting
entire groups of features. We apply sparseptron
for selecting feature templates in three structured
prediction tasks, with advantages over filter-based
methods, L1, and L2 regularization in terms of per-
formance, compactness, and model interpretability.
Acknowledgments
We would like to thank all reviewers for their comments,
Eric Xing for helpful discussions, and Slav Petrov for his
comments on a draft version of this paper. A. M. was sup-
ported by a FCT/ICTI grant through the CMU-Portugal
Program, and also by Priberam. This work was partially
supported by the FET programme (EU FP7), under the
SIMBAD project (contract 213250). N. S. was supported
by NSF CAREER IIS-1054319.
References
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In Proc. of
ICML.
G. Andrew and J. Gao. 2007. Scalable training of
L1-regularized log-linear models. In Proc. of ICML.
ACM.
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. 2011.
Convex optimization with sparsity-inducing norms. In
Optimization for Machine Learning. MIT Press.
F. Bach. 2008. Exploring large feature spaces with hier-
archical multiple kernel learning. NIPS, 21.
S. Bakin. 1999. Adaptive regression and model selec-
tion in data mining problems. Ph.D. thesis, Australian
National University.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
B. Carpenter. 2008. Lazy sparse stochastic gradient de-
scent for regularized multinomial logistic regression.
Technical report, Technical report, Alias-i.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
1509
S. F. Chen and R. Rosenfeld. 2000. A survey of
smoothing techniques for maximum entropy models.
IEEE Transactions on Speech and Audio Processing,
8(1):37?50.
M. Collins. 2002a. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
M. Collins. 2002b. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In
Proc. of ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Algo-
rithms. JMLR, 7:551?585.
S. Della Pietra, V. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19:380?393.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2873?2908.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
M.A.T. Figueiredo. 2002. Adaptive sparseness using Jef-
freys? prior. Advances in Neural Information Process-
ing Systems.
J. Friedman, T. Hastie, and R. Tibshirani. 2010. A note
on the group lasso and a sparse group lasso. Unpub-
lished manuscript.
J. Gao, G. Andrew, M. Johnson, and K. Toutanova. 2007.
A comparative study of parameter estimation methods
for statistical natural language processing. In Proc. of
ACL.
J. Goodman. 2004. Exponential priors for maximum en-
tropy models. In Proc. of NAACL.
J. Grac?a, K. Ganchev, B. Taskar, and F. Pereira. 2009.
Posterior vs. parameter sparsity in latent variable mod-
els. Advances in Neural Information Processing Sys-
tems.
I. Guyon and A. Elisseeff. 2003. An introduction to vari-
able and feature selection. Journal of Machine Learn-
ing Research, 3:1157?1182.
R. Jenatton, J.-Y. Audibert, and F. Bach. 2009. Struc-
tured variable selection with sparsity-inducing norms.
Technical report, arXiv:0904.3523.
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. 2010.
Proximal methods for sparse hierarchical dictionary
learning. In Proc. of ICML.
J. Kazama and J. Tsujii. 2003. Evaluation and exten-
sion of maximum entropy models with inequality con-
straints. In Proc. of EMNLP.
S. Kim and E.P. Xing. 2010. Tree-guided group lasso for
multi-task regression with structured sparsity. In Proc.
of ICML.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El
Ghaoui, and M. I. Jordan. 2004. Learning the kernel
matrix with semidefinite programming. JMLR, 5:27?
72.
J. Langford, L. Li, and T. Zhang. 2009. Sparse online
learning via truncated gradient. JMLR, 10:777?801.
T. Lavergne, O. Cappe?, and F. Yvon. 2010. Practical
very large scale CRFs. In Proc. of ACL.
J. Liu and J. Ye. 2010. Moreau-Yosida regularization for
grouped tree structure learning. In Advances in Neural
Information Processing Systems.
J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. 2010.
Network flow algorithms for structured sparsity. In
Advances in Neural Information Processing Systems.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011. Online learning of
structured predictors with multiple kernels. In Proc. of
AISTATS.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proc. of UAI.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
G. Obozinski, B. Taskar, and M.I. Jordan. 2010. Joint co-
variate selection and joint subspace selection for multi-
ple classification problems. Statistics and Computing,
20(2):231?252.
S. Petrov and D. Klein. 2008a. Discriminative log-linear
grammars with latent variables. Advances in Neural
Information Processing Systems, 20:1153?1160.
S. Petrov and D. Klein. 2008b. Sparse multi-scale gram-
mars for discriminative latent variable parsing. In
Proc. of EMNLP.
A. Quattoni, X. Carreras, M. Collins, and T. Darrell.
2009. An efficient projection for l1,? regularization.
In Proc. of ICML.
E.F.T.K. Sang and S. Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proceedings
of CoNLL-2000 and LLL-2000.
E.F.T.K. Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proc. of CoNLL.
E.F.T.K. Sang. 2002. Introduction to the CoNLL-
2002 shared task: Language-independent named entity
recognition. In Proc. of CoNLL.
1510
M. Schmidt and K. Murphy. 2010. Convex structure
learning in log-linear models: Beyond pairwise poten-
tials. In Proc. of AISTATS.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: Primal estimated sub-gradient solver for SVM.
In ICML.
N. Sokolovska, T. Lavergne, O. Cappe?, and F. Yvon.
2010. Efficient learning of sparse conditional random
fields for supervised sequence labelling. IEEE Journal
of Selected Topics in Signal Processing, 4(6):953?964.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society
B., pages 267?288.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In ICML.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL.
S.J. Wright, R. Nowak, and M.A.T. Figueiredo. 2009.
Sparse reconstruction by separable approximation.
IEEE Transactions on Signal Processing, 57(7):2479?
2493.
L. Xiao. 2009. Dual averaging methods for regular-
ized stochastic learning and online optimization. In
Advances in Neural Information Processing Systems.
M. Yuan and Y. Lin. 2006. Model selection and estima-
tion in regression with grouped variables. Journal of
the Royal Statistical Society (B), 68(1):49.
P. Zhao, G. Rocha, and B. Yu. 2009. Grouped and hi-
erarchical model selection through composite absolute
penalties. Annals of Statistics, 37(6A):3468?3497.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society Series B (Statistical Methodology),
67(2):301?320.
1511
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 196?206,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Robust Compressive Summarization
with Dual Decomposition and Multi-Task Learning
Miguel B. Almeida?? Andre? F. T. Martins??
?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, 1049-001 Lisboa, Portugal
{mba,atm}@priberam.pt
Abstract
We present a dual decomposition frame-
work for multi-document summarization,
using a model that jointly extracts and
compresses sentences. Compared with
previous work based on integer linear pro-
gramming, our approach does not require
external solvers, is significantly faster, and
is modular in the three qualities a sum-
mary should have: conciseness, informa-
tiveness, and grammaticality. In addition,
we propose a multi-task learning frame-
work to take advantage of existing data
for extractive summarization and sentence
compression. Experiments in the TAC-
2008 dataset yield the highest published
ROUGE scores to date, with runtimes that
rival those of extractive summarizers.
1 Introduction
Automatic text summarization is a seminal prob-
lem in information retrieval and natural language
processing (Luhn, 1958; Baxendale, 1958; Ed-
mundson, 1969). Today, with the overwhelming
amount of information available on the Web, the
demand for fast, robust, and scalable summariza-
tion systems is stronger than ever.
Up to now, extractive systems have been the
most popular in multi-document summarization.
These systems produce a summary by extracting
a representative set of sentences from the origi-
nal documents (Kupiec et al, 1995; Carbonell and
Goldstein, 1998; Radev et al, 2000; Gillick et al,
2008). This approach has obvious advantages: it
reduces the search space by letting decisions be
made for each sentence as a whole (avoiding fine-
grained text generation), and it ensures a grammat-
ical summary, assuming the original sentences are
well-formed. The typical trade-offs in these mod-
els (maximizing relevance, and penalizing redun-
dancy) lead to submodular optimization problems
(Lin and Bilmes, 2010), which are NP-hard but ap-
proximable through greedy algorithms; learning is
possible with standard structured prediction algo-
rithms (Sipos et al, 2012; Lin and Bilmes, 2012).
Probabilistic models have also been proposed to
capture the problem structure, such as determinan-
tal point processes (Gillenwater et al, 2012).
However, extractive systems are rather limited
in the summaries they can produce. Long, partly
relevant sentences tend not to appear in the sum-
mary, or to block the inclusion of other sen-
tences. This has motivated research in compres-
sive summarization (Lin, 2003; Zajic et al, 2006;
Daume?, 2006), where summaries are formed by
compressed sentences (Knight and Marcu, 2000),
not necessarily extracts. While promising results
have been achieved by models that simultaneously
extract and compress (Martins and Smith, 2009;
Woodsend and Lapata, 2010; Berg-Kirkpatrick et
al., 2011), there are still obstacles that need to
be surmounted for these systems to enjoy wide
adoption. All approaches above are based on in-
teger linear programming (ILP), suffering from
slow runtimes, when compared to extractive sys-
tems. For example, Woodsend and Lapata (2012)
report 55 seconds on average to produce a sum-
mary; Berg-Kirkpatrick et al (2011) report sub-
stantially faster runtimes, but fewer compressions
are allowed. Having a compressive summarizer
which is both fast and expressive remains an open
problem. A second inconvenience of ILP-based
approaches is that they do not exploit the modu-
larity of the problem, since the declarative specifi-
cation required by ILP solvers discards important
structural information. For example, such solvers
are unable to take advantage of efficient dynamic
programming routines for sentence compression
(McDonald, 2006).
196
This paper makes progress in two fronts:
? We derive a dual decomposition framework for
extractive and compressive summarization (?2?
3). Not only is this framework orders of mag-
nitude more efficient than the ILP-based ap-
proaches, it also allows the three well-known
metrics of summaries?conciseness, informa-
tiveness, and grammaticality?to be treated sep-
arately in a modular fashion (see Figure 1). We
also contribute with a novel knapsack factor,
along with a linear-time algorithm for the corre-
sponding dual decomposition subproblem.
? We propose multi-task learning (?4) as a prin-
cipled way to train compressive summarizers,
using auxiliary data for extractive summariza-
tion and sentence compression. To this end,
we adapt the framework of Evgeniou and Pon-
til (2004) and Daume? (2007) to train structured
predictors that share some of their parts.
Experiments on TAC data (?5) yield state-of-the-
art results, with runtimes similar to that of extrac-
tive systems. To our best knowledge, this had
never been achieved by compressive summarizers.
2 Extractive Summarization
In extractive summarization, we are given a set
of sentences D := {s1, . . . , sN} belonging to one
or more documents, and the goal is to extract a
subset S ? D that conveys a good summary of D
and whose total number of words does not exceed
a prespecified budget B.
We use an indicator vector y := ?yn?Nn=1 to rep-
resent an extractive summary, where yn = 1 if
sn ? S, and yn = 0 otherwise. Let Ln be the
number of words of the nth sentence. By design-
ing a quality score function g : {0, 1}N ? R, this
can be cast as a global optimization problem with
a knapsack constraint:
maximize g(y)
w.r.t. y ? {0, 1}N
s.t.
?N
n=1 Lnyn ? B. (1)
Intuitively, a good summary is one which selects
sentences that individually convey ?relevant? in-
formation, while collectively having small ?re-
dundancy.? This trade-off was explicitly mod-
eled in early works through the notion of max-
imal marginal relevance (Carbonell and Gold-
stein, 1998; McDonald, 2007). An alternative
are coverage-based models (?2.1; Filatova and
Hatzivassiloglou, 2004; Yih et al, 2007; Gillick
et al, 2008), which seek a set of sentences that
covers as many diverse ?concepts? as possible; re-
dundancy is automatically penalized since redun-
dant sentences cover fewer concepts. Both models
can be framed under the framework of submodular
optimization (Lin and Bilmes, 2010), leading to
greedy algorithms that have approximation guar-
antees. However, extending these models to allow
for sentence compression (as will be detailed in
?3) breaks the diminishing returns property, mak-
ing submodular optimization no longer applicable.
2.1 Coverage-Based Summarization
Coverage-based extractive summarization can be
formalized as follows. Let C(D) := {c1, . . . , cM}
be a set of relevant concept types which are
present in the original documents D.1 Let ?m be a
relevance score assigned to the mth concept, and
let the set Im ? {1, . . . , N} contain the indices of
the sentences in which this concept occurs. Then,
the following quality score function is defined:
g(y) =
?M
m=1 ?mum(y), (2)
where um(y) := ?n?Im yn is a Boolean functionthat indicates whether the mth concept is present
in the summary. Plugging this into Eq. 1, one ob-
tains the following Boolean optimization problem:
maximize
?M
m=1 ?mum
w.r.t. y ? {0, 1}N , u ? {0, 1}M
s.t. um =
?
n?Im yn, ?m ? [M ]?N
n=1 Lnyn ? B, (3)
where we used the notation [M ] := {1, . . . ,M}.
This can be converted into an ILP and addressed
with off-the-shelf solvers (Gillick et al, 2008). A
drawback of this approach is that solving an ILP
exactly is NP-hard. Even though existing commer-
cial solvers can solve most instances with a mod-
erate speed, they still exhibit poor worst-case be-
haviour; this is exacerbated when there is the need
to combine an extractive component with other
modules, as in compressive summarization (?3).
1Previous work has modeled concepts as events (Filatova
and Hatzivassiloglou, 2004), salient words (Lin and Bilmes,
2010), and word bigrams (Gillick et al, 2008). In the sequel,
we assume concepts are word k-grams, but our model can
handle other representations, such as phrases or predicate-
argument structures.
197
2.2 A Dual Decomposition Formulation
We next describe how the problem in Eq. 3 can be
addressed with dual decomposition, a class of op-
timization techniques that tackle the dual of com-
binatorial problems in a modular, extensible, and
parallelizable manner (Komodakis et al, 2007;
Rush et al, 2010). In particular, we employ al-
ternating directions dual decomposition (AD3;
Martins et al, 2011a, 2012) for solving a linear re-
laxation of Eq. 3. AD3 resembles the subgradient-
based algorithm of Rush et al (2010), but it enjoys
a faster convergence rate. Both algorithms split
the original problem into several components,
and then iterate between solving independent lo-
cal subproblems at each component and adjusting
multipliers to promote an agreement.2 The differ-
ence between the two methods is that the AD3 lo-
cal subproblems, instead of requiring the compu-
tation of a locally optimal configuration, require
solving a local quadratic problem. Martins et al
(2011b) provided linear-time solutions for several
logic constraints, with applications to syntax and
frame-semantic parsing (Das et al, 2012). We will
see that AD3 can also handle budget and knapsack
constraints efficiently.
To tackle Eq. 3 with dual decomposition, we
split the coverage-based summarizer into the fol-
lowing M + 1 components (one per constraint):
1. For each of the M concepts in C(D), one
component for imposing the logic constraint
in Eq. 3. This corresponds to the OR-WITH-
OUTPUT factor described by Martins et al
(2011b); the AD3 subproblem for themth factor
can be solved in time O(|Im|).
2. Another component for the knapsack con-
straint. This corresponds to a (novel) KNAP-
SACK factor, whose AD3 subproblem is solv-
able in time O(N). The actual algorithm is de-
scribed in the appendix (Algorithm 1).3
3 Compressive Summarization
We now turn to compressive summarization,
which does not limit the summary sentences to be
verbatim extracts from the original documents; in-
2For details about dual decomposition and Lagrangian re-
laxation, see the recent tutorial by Rush and Collins (2012).
3The AD3 subproblem in this case corresponds to com-
puting an Euclidean projection onto the knapsack polytope
(Eq. 11). Others addressed the related, but much harder, inte-
ger quadratic knapsack problem (McDonald, 2007).
stead, it allows the extraction of compressed sen-
tences where some words can be deleted.
Formally, let us express each sentence of D
as a sequence of word tokens, sn := ?tn,`?Ln`=0,where tn,0 ? $ is a dummy symbol. We rep-
resent a compression of sn as an indicator vec-
tor zn := ?zn,`?Ln`=0, where zn,` = 1 if the `thword is included in the compression. By conven-
tion, the dummy symbol is included if and only if
the remaining compression is non-empty. A com-
pressive summary can then be represented by an
indicator vector z which is the concatenation of
N such vectors, z = ?z1, . . . ,zN ?; each position
in this indicator vector is indexed by a sentence
n ? [N ] and a word position ` ? {0} ? [Ln].
Models for compressive summarization were
proposed by Martins and Smith (2009) and Berg-
Kirkpatrick et al (2011) by combining extraction
and compression scores. Here, we follow the lat-
ter work, by combining a coverage score function
g with sentence-level compression score functions
h1, . . . , hN . This yields the decoding problem:
maximize g(z) +
?N
n=1 hn(zn)
w.r.t. zn ? {0, 1}Ln , ?n ? [N ]
s.t.
?N
n=1
?Ln
`=1 zn,` ? B. (4)
3.1 Coverage Model
We use a coverage function similar to Eq. 2, but
taking a compressive summary z as argument:
g(z) =
?M
m=1 ?mum(z), (5)
where we redefine um as follows. First, we
parametrize each occurrence of the mth concept
(assumed to be a k-gram) as a triple ?n, `s, `e?,
where n indexes a sentence, `s indexes a start po-
sition within the sentence, and `e indexes the end
position. We denote by Tm the set of triples repre-
senting all occurrences of the mth concept in the
original text, and we associate an indicator vari-
able zn,`s:`e to each member of this set. We then
define um(z) via the following logic constraints:
? A concept type is selected if some of its k-gram
tokens are selected:
um(y) :=
?
?n,`s,`e??Tm zn,`s:`e . (6)
? A k-gram concept token is selected if all its
words are selected:
zn,`s:`e :=
?`e
`=`s zn,`. (7)
198
Sentences $     The      leader    of   moderate  Kashmiri  separatists warned   Thursday   that ...
$     Talks    with   Kashmiri  separatists began    last       year ...
"Kashmiri separatists"
Budget
Concept tokens
Concept type
Figure 1: Components of our compressive summarizer. Factors depicted in blue belong to the compres-
sion model, and aim to enforce grammaticality. The logic factors in red form the coverage component.
Finally, the budget factor, in green, is connected to the word nodes; it ensures that the summary fits the
word limit. Shaded circles represent active variables while white circles represent inactive variables.
We set concept scores as ?m := w ? ?cov(D, cm),
where ?cov(D, cm) is a vector of features (de-
scribed in ?3.5) and w the corresponding weights.
3.2 Compression Model
For the compression score function, we follow
Martins and Smith (2009) and decompose it as a
sum of local score functions ?n,` defined on de-
pendency arcs:
hn(zn) :=
?Ln
`=1 ?n,`(zn,`, zn,pi(`)), (8)
where pi(`) denotes the index of the word which
is the parent of the `th word in the dependency
tree (by convention, the root of the tree is the
dummy symbol). To model the event that an
arc is ?cut? by disconnecting a child from its
head, we define arc-deletion scores ?n,`(0, 1) :=
w ? ?comp(sn, `, pi(`)), where ?comp is a feature
map, which is described in detail in ?3.5. We set
?n,`(0, 0) = ?n,`(1, 1) = 0, and ?n,`(1, 0) = ??,
to allow only the deletion of entire subtrees.
A crucial fact is that one can maximize Eq. 8
efficiently with dynamic programming (using the
Viterbi algorithm for trees); the total cost is linear
in Ln. We will exploit this fact in the dual decom-
position framework described next.4
3.3 A Dual Decomposition Formulation
In previous work, the optimization problem in
Eq. 4 was converted into an ILP and fed to an off-
the-shelf solver (Martins and Smith, 2009; Berg-
Kirkpatrick et al, 2011; Woodsend and Lapata,
2012). Here, we employ the AD3 algorithm, in a
4The same framework can be readily adapted to other
compression models that are efficiently decodable, such as
the semi-Markov model of McDonald (2006), which would
allow incorporating a language model for the compression.
similar manner as described in ?2, but with an ad-
ditional component for the sentence compressor,
and slight modifications in the other components.
We have the following N +M +?Mm=1 |Tm|+ 1
components in total, illustrated in Figure 1:
1. For each of the N sentences, one component
for the compression model. The AD3 quadratic
subproblem for this factor can be addressed by
solving a sequence of linear subproblems, as de-
scribed by Martins et al (2012). Each of these
subproblems corresponds to maximizing an ob-
jective function of the same form as Eq. 8; this
can be done in O(Ln) time with dynamic pro-
gramming, as discussed in ?3.2.
2. For each of the M concept types in C(D),
one OR-WITH-OUTPUT factor for the logic con-
straint in Eq. 6. This is analogous to the one
described for the extractive case.
3. For each k-gram concept token in Tm, one
AND-WITH-OUTPUT factor that imposes the
constraint in Eq. 7. This factor was described
by Martins et al (2011b) and its AD3 subprob-
lem can be solved in time linear in k.
4. Another component linked to all the words im-
posing that at most B words can be selected;
this is done via a BUDGET factor, a particular
case of KNAPSACK. The runtime of this AD3
subproblem is linear in the number of words.
In addition, we found it useful to add a second
BUDGET factor limiting the number of sentences
that can be selected to a prescribed value K. We
set K = 6 in our experiments.
199
3.4 Rounding Strategy
Recall that the problem in Eq. 4 is NP-hard, and
that AD3 is solving a linear relaxation. While
there are ways of wrapping AD3 in an exact search
algorithm (Das et al, 2012), such strategies work
best when the solution of the relaxation has few
fractional components, which is typical of pars-
ing and translation problems (Rush et al, 2010;
Chang and Collins, 2011), and attractive networks
(Taskar et al, 2004). Unfortunately, this is not the
case in summarization, where concepts ?compete?
with each other for inclusion in the summary, lead-
ing to frustrated cycles. We chose instead to adopt
a fast and simple rounding procedure for obtaining
a summary from a fractional solution.
The procedure works as follows. First, solve
the LP relaxation using AD3, as described above.
This yields a solution z?, where each component
lies in the unit interval [0, 1]. If these components
are all integer, then we have a certificate that this
is the optimal solution. Otherwise, we collect the
K sentences with the highest values of z?n,0 (?pos-
teriors? on sentences), and seek the feasible sum-
mary which is the closest (in Euclidean distance)
to z?, while only containing those sentences. This
can be computed exactly in timeO(B?Kk=1 Lnk),
through dynamic programming.5
3.5 Features and Hard Constraints
As Berg-Kirkpatrick et al (2011), we used
stemmed word bigrams as concepts, to which we
associate the following concept features (?cov):
indicators for document counts, features indicat-
ing if each of the words in the bigram is a stop-
word, the earliest position in a document each con-
cept occurs, as well as two and three-way conjunc-
tions of these features.
For the compression model, we include the fol-
lowing arc-deletion features (?comp):
? the dependency label of the arc being deleted, as
well as its conjunction with the part-of-speech
tag of the head, of the modifier, and of both;
? the dependency labels of the arc being deleted
and of its parent arc;
? the modifier tag, if the modifier is a function
word modifying a verb ;
5Briefly, if we link the roots of theK sentences to a super-
root node, the problem above can be transformed into that
of finding the best configuration in the resulting binary tree
subject to a budget constraint. We omit details for space.
? a feature indicating whether the modifier or any
of its descendants is a negation word;
? indicators of whether the modifier is a temporal
word (e.g., Friday) or a preposition pointing to
a temporal word (e.g., on Friday).
In addition, we included hard constraints to pre-
vent the deletion of certain arcs, following pre-
vious work in sentence compression (Clarke and
Lapata, 2008). We never delete arcs whose de-
pendency label is SUB, OBJ, PMOD, SBAR, VC, or
PRD (this makes sure we preserve subjects and ob-
jects of verbs, arcs departing from prepositions or
complementizers, and that we do not break verb
chains or predicative complements); arcs linking
to a conjunction word or siblings of such arcs (to
prevent inconsistencies in handling coordinative
conjunctions); arcs linking verbs to other verbs,
to adjectives (e.g., make available), to verb parti-
cles (e.g., settle down), to the word that (e.g., said
that), or to the word to if it is a leaf (e.g., allowed
to come); arcs pointing to negation words, cardinal
numbers, or determiners; and arcs connecting two
proper nouns or words within quotation marks.
4 Multi-Task Learning
We next turn to the problem of learning the model
from training data. Prior work in compressive
summarization has followed one of two strategies:
Martins and Smith (2009) and Woodsend and La-
pata (2012) learn the extraction and compression
models separately, and then post-combine them,
circumventing the lack of fully annotated data.
Berg-Kirkpatrick et al (2011) gathered a small
dataset of manually compressed summaries, and
trained with full supervision. While the latter
approach is statistically more principled, it has
the disadvantage of requiring fully annotated data,
which is difficult to obtain in large quantities. On
the other hand, there is plenty of data contain-
ing manually written abstracts (from the DUC and
TAC conferences) and user-generated text (from
Wikipedia) that may provide useful weak supervi-
sion.
With this in mind, we put together a multi-task
learning framework for compressive summariza-
tion (which we name task #1). The goal is to
take advantage of existing data for related tasks,
such as extractive summarization (task #2), and
sentence compression (task #3). The three tasks
are instances of structured predictors (Bak?r et
200
Tasks Features Decoder
Comp. summ. (#1) ?cov, ?comp AD3 (solve Eq. 4)
Extr. summ. (#2) ?cov AD3 (solve Eq. 3)
Sent. comp. (#3) ?comp dyn. prg. (max. Eq. 8)
Table 1: Features and decoders used for each task.
al., 2007), and for all of them we assume feature-
based models that decompose over ?parts?:
? For the compressive summarization task, the
parts correspond to concept features (?3.1) and
to arc-deletion features (?3.2).
? For the extractive summarization task, there are
parts for concept features only.
? For the sentence compression task, the parts
correspond to arc-deletion features only.
This is summarized in Table 1. Features for
the three tasks are populated into feature vectors
?1(x, y), ?2(x, y), and ?3(x, y), respectively,
where ?x, y? denotes a task-specific input-output
pair. We assume the feature vectors are all D di-
mensional, where we place zeros in entries cor-
responding to parts that are absent. Note that
this setting is very general and applies to arbi-
trary structured prediction problems (not just sum-
marization), the only assumption being that some
parts are shared between different tasks.
Next, we associate weight vectors v1,v2,v3 ?
RD to each task, along with a ?shared? vector w.
Each task makes predictions according to the rule:
y? := arg max
y
(w + vk) ? ?k(x, y), (9)
where k ? {1, 2, 3}. This setting is equiva-
lent to the approach of Daume? (2007) for domain
adaptation, which consists in splitting each fea-
ture into task-component features and a shared
feature; but here we do not duplicate features ex-
plicitly. To learn the weights, we regularize the
weight vectors separately, and assume that each
task has its own loss function Lk, so that the to-
tal loss L is a weighted sum L(w,v1,v2,v3) :=?3
k=1 ?kLk(w + vk). This yields the following
objective function to be minimized:
F (w,v1,v2,v3) =
?
2 ?w?
2 +
3?
k=1
?k
2 ?vk?
2
+ 1N
3?
k=1
?kLk(w + vk), (10)
where ? and the ?k?s are regularization constants,
andN is the total number of training instances.6 In
our experiments (?5), we let the Lk?s be structured
hinge losses (Taskar et al, 2003; Tsochantaridis et
al., 2004), where the corresponding cost functions
are concept recall (for task #2), precision of arc
deletions (for task #3), and a combination thereof
(for task #1).7 These losses were normalized, and
we set ?k = N/Nk, where Nk is the number of
training instances for the kth task. This ensures
all tasks are weighted evenly. We used the same
rationale to set ? = ?1 = ?2 = ?3, choosing this
value through cross-validation in the dev set.
We optimize Eq. 10 with stochastic subgradient
descent. This leads to update rules of the form
w ? (1? ?t?)w ? ?t?k??Lk(w + vk)
vj ? (1? ?t?j)vj ? ?t?jk?k??Lk(w + vk),
where ??Lk are stochastic subgradients for the kth
task, that take only a single instance into account,
and ?jk = 1 if and only if j = k. Stochastic sub-
gradients can be computed via cost-augmented de-
coding (see footnote 7).
Interestingly, Eq. 10 subsumes previous ap-
proaches to train compressive summarizers. The
limit ??? (keeping the ?k?s fixed) forces w ?
0, decoupling all the tasks. In this limit, inference
for task #1 (compressive summarization) is based
solely on the model learned from that task?s data,
recovering the approach of Berg-Kirkpatrick et al
(2011). In the other extreme, setting ?1 = 0 sim-
ply ignores task #1?s training data. As a result, the
optimal v1 will be a vector of zeros; since tasks
#2 and #3 have no parts in common, the objective
will decouple into a sum of two independent terms
6Note that, by substituting uk := w+vk and solving for
w, the problem in Eq. 10 becomes that of minimizing the sum
of the losses with a penalty for the (weighted) variance of the
vectors {0,u1,u2,u3}, regularizing the difference towards
their average, as in Evgeniou and Pontil (2004). This is sim-
ilar to the hierarchical joint learning approach of Finkel and
Manning (2010), except that our goal is to learn a new task
(compressive summarization) instead of combining tasks.
7Let Yk denote the output set for the kth task. Given
a task-specific cost function ?k : Yk ? Yk ? R,
and letting ?xt, yt?Tt=1 be the labeled dataset for this
task, the structured hinge loss takes the form Lk(uk) :=?
tmaxy??Yk(uk ? (?k(xt, y?)? ?k(xt, yt)) + ?k(y?, yt)).The inner maximization over y? is called the cost-augmented
decoding problem: it differs from Eq. 9 by the inclusion
of the cost term ?k(y?, yt). Our costs decompose over the
model?s factors, hence any decoder for Eq. 9 can be used
for the maximization above: for tasks #1?#2, we solve a
relaxation by running AD3 without rounding, and for task #3
we use dynamic programming; see Table 1.
201
involving v2 and v3, which is equivalent to train-
ing the two tasks separately and post-combining
the models, as Martins and Smith (2009) did.
5 Experiments
5.1 Experimental setup
We evaluated our compressive summarizers on
data from the Text Analysis Conference (TAC)
evaluations. We use the same splits as previ-
ous work (Berg-Kirkpatrick et al, 2011; Wood-
send and Lapata, 2012): the non-update portions
of TAC-2009 for training and TAC-2008 for test-
ing. In addition, we reserved TAC-2010 as a dev-
set. The test partition contains 48 multi-document
summarization problems; each provides 10 related
news articles as input, and asks for a summary
with up to 100 words, which is evaluated against
four manually written abstracts. We ignored all
the query information present in the TAC datasets.
Single-Task Learning. In the single-task exper-
iments, we trained a compressive summarizer on
the dataset disclosed by Berg-Kirkpatrick et al
(2011), which contains manual compressive sum-
maries for the TAC-2009 data. We trained a struc-
tured SVM with stochastic subgradient descent;
the cost-augmented inference problems are re-
laxed and solved with AD3, as described in ?3.3.8
We followed the procedure described in Berg-
Kirkpatrick et al (2011) to reduce the number of
candidate sentences: scores were defined for each
sentence (the sum of the scores of the concepts
they cover), and the best-scored sentences were
greedily selected up to a limit of 1,000 words. We
then tagged and parsed the selected sentences with
TurboParser.9 Our choice of a dependency parser
was motivated by our will for a fast system; in par-
ticular, TurboParser attains top accuracies at a rate
of 1,200 words per second, keeping parsing times
below 1 second for each summarization problem.
Multi-Task Learning. For the multi-task ex-
periments, we also used the dataset of Berg-
Kirkpatrick et al (2011), but we augmented the
training data with extractive summarization and
sentence compression datasets, to help train the
8We use the AD3 implementation in http://www.
ark.cs.cmu.edu/AD3, setting the maximum number of
iterations to 200 at training time and 1000 at test time. We
extended the code to handle the knapsack and budget factors;
the modified code will be part of the next release (AD3 2.1).
9http://www.ark.cs.cmu.edu/TurboParser
compressive summarizer. For extractive sum-
marization, we used the DUC 2003 and 2004
datasets (a total of 80 multi-document summariza-
tion problems). We generated oracle extracts by
maximizing bigram recall with respect to the man-
ual abstracts, as described in Berg-Kirkpatrick et
al. (2011). For sentence compression, we adapted
the Simple English Wikipedia dataset of Wood-
send and Lapata (2011), containing aligned sen-
tences for 15,000 articles from the English and
Simple English Wikipedias. We kept only the
4,481 sentence pairs corresponding to deletion-
based compressions.
5.2 Results
Table 2 shows the results. The top rows refer
to three strong baselines: the ICSI-1 extractive
coverage-based system of Gillick et al (2008),
which achieved the best ROUGE scores in the
TAC-2008 evaluation; the compressive summa-
rizer of Berg-Kirkpatrick et al (2011), denoted
BGK?11; and the multi-aspect compressive sum-
marizer of Woodsend and Lapata (2012), denoted
WL?12. All these systems require ILP solvers.
The bottom rows show the results achieved by
our implementation of a pure extractive system
(similar to the learned extractive summarizer of
Berg-Kirkpatrick et al, 2011); a system that post-
combines extraction and compression components
trained separately, as in Martins and Smith (2009);
and our compressive summarizer trained as a sin-
gle task, and in the multi-task setting.
The ROUGE and Pyramid scores show that the
compressive summarizers (when properly trained)
yield considerable benefits in content coverage
over extractive systems, confirming the results of
Berg-Kirkpatrick et al (2011). Comparing the
two bottom rows, we see a clear benefit by train-
ing in the multi-task setting, with a consistent
gain in both coverage and linguistic quality. Our
ROUGE-2 score (12.30%) is, to our knowledge,
the highest reported on the TAC-2008 dataset,
with little harm in grammaticality with respect to
an extractive system that preserves the original
sentences. Figure 2 shows an example summary.
5.3 Runtimes
We conducted another set of experiments to com-
pare the runtime of our compressive summarizer
based on AD3 with the runtimes achieved by
GLPK, the ILP solver used by Berg-Kirkpatrick et
al. (2011). We varied the maximum number of it-
202
System R-2 R-SU4 Pyr LQ
ICSI-1 11.03 13.96 34.5? ?
BGK?11 11.71 14.47 41.3? ?
WL?12 11.37 14.47 ? ?
Extractive 11.16 14.07 36.0 4.6
Post-comb. 11.07 13.85 38.4 4.1
Single-task 11.88 14.86 41.0 3.8
Multi-task 12.30 15.18 42.6 4.2
Table 2: Results for compressive summarization.
Shown are the ROUGE-2 and ROUGE SU-4 re-
calls with the default options from the ROUGE
toolkit (Lin, 2004); Pyramid scores (Nenkova and
Passonneau, 2004); and linguistic quality scores,
scored between 1 (very bad) to 5 (very good). For
Pyramid, the evaluation was performed by two
annotators, each evaluating half of the problems;
scores marked with ? were computed by different
annotators and are not directly comparable. Lin-
guistic quality was evaluated by two linguists; we
show the average of the reported scores.
Solver Runtime (sec.) ROUGE-2
ILP Exact 10.394 12.40
LP-Relax. 2.265 12.38
AD3-5000 0.952 12.38
AD3-1000 0.406 12.30
AD3-200 0.159 12.15
Extractive (ILP) 0.265 11.16
Table 3: Runtimes of several decoders on a Intel
Core i7 processor @2.8 GHz, with 8GB RAM. For
each decoder, we show the average time taken to
solve a summarization problem in TAC-2008. The
reported runtimes of AD3 and LP-Relax include
the time taken to round the solution (?3.4), which
is 0.029 seconds on average.
erations of AD3 in {200, 1000, 5000}, and clocked
the time spent by GLPK to solve the exact ILPs
and their relaxations. Table 3 depicts the results.10
We see that our proposed configuration (AD3-
1000) is orders of magnitude faster than the ILP
solver, and 5 times faster than its relaxed variant,
while keeping similar accuracy levels.11 The gain
when the number of iterations in AD3 is increased
to 5000 is small, given that the runtime is more
10Within dual decomposition algorithms, we verified ex-
perimentally that AD3 is substantially faster than the subgra-
dient algorithm, which is consistent with previous findings
(Martins et al, 2011b).
11The runtimes obtained with the exact ILP solver seem
slower than those reported by Berg-Kirkpatrick et al (2011).
(around 1.5 sec. on average, according to their Fig. 3). We
conjecture that this difference is due to the restricted set of
subtrees that can be deleted by Berg-Kirkpatrick et al (2011),
which greatly reduces their search space.
Japan dispatched four military ships to help Russia res-
cue seven crew members aboard a small submarine
trapped on the seabed in the Far East. The Russian
Pacific Fleet said the crew had 120 hours of oxygen
reserves on board when the submarine submerged at
midday Thursday (2300 GMT Wednesday) off the Kam-
chatka peninsula, the stretch of Far Eastern Russia fac-
ing the Bering Sea. The submarine, used in rescue,
research and intelligence-gathering missions, became
stuck at the bottom of the Bay of Berezovaya off Rus-
sia?s Far East coast when its propeller was caught in a
fishing net. The Russian submarine had been tending
an underwater antenna mounted to the sea floor when it
became snagged on a wire helping to stabilize a ventila-
tion cable attached to the antenna. Rescue crews low-
ered a British remote-controlled underwater vehicle to a
Russian mini-submarine trapped deep under the Pacific
Ocean, hoping to free the vessel and its seven trapped
crewmen before their air supply ran out.
Figure 2: Example summary from our compres-
sive system. Removed text is grayed out.
than doubled; accuracy starts to suffer, however, if
the number of iterations is reduced too much. In
practice, we observed that the final rounding pro-
cedure was crucial, as only 2 out of the 48 test
problems had integral solutions (arguably because
of the ?repulsive? nature of the network, as hinted
in ?3.4). For comparison, we also report in the bot-
tom row the average runtime of the learned extrac-
tive baseline. We can see that our system?s runtime
is competitive with this baseline. To our knowl-
edge, this is the first time a compressive sum-
marizer achieves such a favorable accuracy/speed
tradeoff.
6 Conclusions
We presented a multi-task learning framework for
compressive summarization, leveraging data for
related tasks in a principled manner. We decode
with AD3, a fast and modular dual decomposition
algorithm which is orders of magnitude faster than
ILP-based approaches. Results show that the state
of the art is improved in automatic and manual
metrics, with speeds close to extractive systems.
Our approach is modular and easy to extend.
For example, a different compression model could
incorporate rewriting rules to enable compres-
sions that go beyond word deletion, as in Cohn
and Lapata (2008). Other aspects may be added
as additional components in our dual decom-
position framework, such as query information
(Schilder and Kondadadi, 2008), discourse con-
203
straints (Clarke and Lapata, 2007), or lexical pref-
erences (Woodsend and Lapata, 2012). Our multi-
task approach may be used to jointly learn pa-
rameters for these aspects; the dual decomposi-
tion algorithm ensures that optimization remains
tractable even with many components.
A Projection Onto Knapsack
This section describes a linear-time algorithm (Al-
gorithm 1) for solving the following problem:
minimize ?z ? a?2
w.r.t. zn ? [0, 1], ?n ? [N ],
s.t.
?N
n=1 Lnzn ? B, (11)
where a ? RN and Ln ? 0,?n ? [N ]. This in-
cludes as special cases the problems of projecting
onto a budget constraint (Ln = 1,?n) and onto
the simplex (same, plus B = 1).
Let clip(t) := max{0,min{1, t}}. Algorithm 1
starts by clipping a to the unit interval; if that
yields a z satisfying ?Nn=1 Lnzn ? B, we are
done. Otherwise, the solution of Eq. 11 must sat-
isfy ?Nn=1 Lnzn = B. It can be shown from the
KKT conditions that the solution is of the form
z?n := clip(an+ ??Ln) for a constant ?? lying in a
particular interval of split-points (line 11). To seek
this constant, we use an algorithm due to Pardalos
and Kovoor (1990) which iteratively shrinks this
interval. The algorithm requires computing medi-
ans as a subroutine, which can be done in linear
time (Blum et al, 1973). The overall complexity
in O(N) (Pardalos and Kovoor, 1990).
Acknowledgments
We thank all reviewers for their insightful com-
ments; Trevor Cohn for helpful discussions about
multi-task learning; Taylor Berg-Kirkpatrick for
answering questions about their summarizer and
for providing code; and Helena Figueira and Pedro
Mendes for helping with manual evaluation. This
work was partially supported by the EU/FEDER
programme, QREN/POR Lisboa (Portugal), under
the Discooperio project (contract 2011/18501),
and by a FCT grant PTDC/EEI-SII/2312/2012.
References
G. Bak?r, T. Hofmann, B. Scho?lkopf, A. Smola,
B. Taskar, and S. Vishwanathan. 2007. Predicting
Structured Data. The MIT Press.
Algorithm 1 Projection Onto Knapsack.
1: input: a := ?an?Nn=1, costs ?Ln?Nn=1, maximum cost B
2:
3: {Try to clip into unit interval:}
4: Set zn ? clip(an) for n ? [N ]
5: if?Nn=1 Lnzn ? B then6: Return z and stop.
7: end if
8:
9: {Run Pardalos and Kovoor (1990)?s algorithm:}
10: Initialize working set W? {1, . . . ,K}
11: Initialize set of split points:
P? {?an/Ln, (1? an)/Ln}Nn=1 ? {??}
12: Initialize ?L ? ??, ?R ??, stight ? 0, ? ? 0.
13: while W 6= ? do
14: Compute ? ? Median(P)
15: Set s? stight + ?? +?n?W Lnclip(an + ?Ln)16: If s ? B, set ?L ? ? ; if s ? B, set ?R ? ?
17: Reduce set of split points: P? P ? [?L, ?R]
18: Define the sets:
WL := {n ?W | (1? an)/Ln < ?L}
WR := {n ?W | ? an/Ln > ?R}
WM :=
{
n ?W
???? ?
an
Ln
? ?L ? 1? anLn
? ?R
}
19: Update working set: W?W \ (WL ?WR ?WM)
20: Update tight-sum:
stight ? stight+
?
n?WL Ln(1?an)?
?
n?WR Lnan
21: Update slack-sum: ? ? ? +?n?WM L2n22: end while
23: Define ?? ? (B ??Ni=1 Liai ? stight)/?24: Set zn ? clip(an + ??Ln), ?n ? [N ]
25: output: z := ?zn?Nn=1.
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature?an experiment. IBM Journal of Re-
search Development, 2(4):354?361.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. of Annual Meeting of the Association for Com-
putational Linguistics.
Manuel Blum, Robert W Floyd, Vaughan Pratt,
Ronald L Rivest, and Robert E Tarjan. 1973. Time
bounds for selection. Journal of Computer and Sys-
tem Sciences, 7(4):448?461.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In SIGIR.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through lagrangian
relaxation. In Proc. of Empirical Methods for Natu-
ral Language Processing.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proc. of
Empirical Methods in Natural Language Process-
ing.
J. Clarke and M. Lapata. 2008. Global Inference for
Sentence Compression An Integer Linear Program-
204
ming Approach. Journal of Artificial Intelligence
Research, 31:399?429.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proc. COLING.
D. Das, A. F. T. Martins, and N. A. Smith. 2012. An
Exact Dual Decomposition Algorithm for Shallow
Semantic Parsing with Constraints. In Proc. of First
Joint Conference on Lexical and Computational Se-
mantics (*SEM).
H. Daume?. 2006. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D. the-
sis, University of Southern California.
H. Daume?. 2007. Frustratingly easy domain adapta-
tion. In Proc. of Annual Meeting of the Association
for Computational Linguistics.
H. P. Edmundson. 1969. New methods in automatic
extracting. Journal of the ACM, 16(2):264?285.
T. Evgeniou and M. Pontil. 2004. Regularized multi?
task learning. In Proc. of ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 109?117. ACM.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proc. of International
Conference on Computational Linguistics.
J.R. Finkel and C.D. Manning. 2010. Hierarchical
joint learning: Improving joint parsing and named
entity recognition with non-jointly labeled data. In
Proc. of Annual Meeting of the Association for Com-
putational Linguistics.
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Dis-
covering diverse and salient threads in document
collections. In Proc. of Empirical Methods in Natu-
ral Language Processing.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.
2008. The icsi summarization system at tac 2008.
In Proc. of Text Understanding Conference.
K. Knight and D. Marcu. 2000. Statistics-based
summarization?step one: Sentence compression.
In AAAI/IAAI.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition:
Message-passing revisited. In Proc. of International
Conference on Computer Vision.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In SIGIR.
H. Lin and J. Bilmes. 2010. Multi-document summa-
rization via budgeted maximization of submodular
functions. In Proc. of Annual Meeting of the North
American chapter of the Association for Computa-
tional Linguistics.
H. Lin and J. Bilmes. 2012. Learning mixtures of sub-
modular shells with application to document sum-
marization. In Proc. of Uncertainty in Artificial In-
telligence.
C.-Y. Lin. 2003. Improving summarization perfor-
mance by sentence compression-a pilot study. In the
Int. Workshop on Inf. Ret. with Asian Languages.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research Development,
2(2):159?165.
A. F. T. Martins and N. A. Smith. 2009. Summariza-
tion with a Joint Model for Sentence Extraction and
Compression. In North American Chapter of the As-
sociation for Computational Linguistics: Workshop
on Integer Linear Programming for NLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011a. An Aug-
mented Lagrangian Approach to Constrained MAP
Inference. In Proc. of International Conference on
Machine Learning.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011b. Dual Decomposition
with Many Overlapping Components. In Proc. of
Empirical Methods for Natural Language Process-
ing.
Andre F. T. Martins, Mario A. T. Figueiredo, Pedro
M. Q. Aguiar, Noah A. Smith, and Eric P. Xing.
2012. Alternating Directions Dual Decomposition.
Arxiv preprint arXiv:1212.6550.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proc. of
Annual Meeting of the European Chapter of the As-
sociation for Computational Linguistics.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In ECIR.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarization: The pyramid
method. In Proceedings of NAACL, pages 145?152.
Panos M. Pardalos and Naina Kovoor. 1990. An al-
gorithm for a singly constrained class of quadratic
programs subject to upper and lower bounds. Math-
ematical Programming, 46(1):321?328.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evaluation,
and user studies. In the NAACL-ANLP Workshop on
Automatic Summarization.
205
A.M. Rush and M. Collins. 2012. A Tutorial on Dual
Decomposition and Lagrangian Relaxation for In-
ference in Natural Language Processing. Journal of
Artificial Intelligence Research, 45:305?362.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming re-
laxations for natural language processing. In Proc.
of Empirical Methods for Natural Language Pro-
cessing.
Frank Schilder and Ravikumar Kondadadi. 2008. Fast-
sum: Fast and accurate query-based multi-document
summarization. In Proc. of Annual Meeting of the
Association for Computational Linguistics.
R. Sipos, P. Shivaswamy, and T. Joachims. 2012.
Large-margin learning of submodular summariza-
tion models.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Proc. of Neural Infor-
mation Processing Systems.
B. Taskar, V. Chatalbashev, and D. Koller. 2004.
Learning associative Markov networks. In Proc. of
International Conference of Machine Learning.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Proc.
of International Conference of Machine Learning.
K. Woodsend and M. Lapata. 2010. Automatic gener-
ation of story highlights. In Proc. of Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 565?574.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proc. of Em-
pirical Methods in Natural Language Processing.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proc. of Empirical Methods in Natu-
ral Language Processing.
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proc. of International Joint Conference on Artifi-
cal Intelligence.
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006.
Sentence compression as a component of a multi-
document summarization system. In the ACL DUC
Workshop.
206
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617?622,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers
Andre? F. T. Martins?? Miguel B. Almeida?? Noah A. Smith#
?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, 1049-001 Lisboa, Portugal
#School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
{atm,mba}@priberam.pt, nasmith@cs.cmu.edu
Abstract
We present fast, accurate, direct non-
projective dependency parsers with third-
order features. Our approach uses AD3,
an accelerated dual decomposition algo-
rithm which we extend to handle special-
ized head automata and sequential head
bigram models. Experiments in fourteen
languages yield parsing speeds competi-
tive to projective parsers, with state-of-
the-art accuracies for the largest datasets
(English, Czech, and German).
1 Introduction
Dependency parsing has become a prominent ap-
proach to syntax in the last few years, with in-
creasingly fast and accurate models being devised
(Ku?bler et al, 2009; Huang and Sagae, 2010;
Zhang and Nivre, 2011; Rush and Petrov, 2012).
In projective parsing, the arcs in the dependency
tree are constrained to be nested, and the problem
of finding the best tree can be addressed with dy-
namic programming. This results in cubic-time
decoders for arc-factored and sibling second-order
models (Eisner, 1996; McDonald and Pereira,
2006), and quartic-time for grandparent models
(Carreras, 2007) and third-order models (Koo and
Collins, 2010). Recently, Rush and Petrov (2012)
trained third-order parsers with vine pruning cas-
cades, achieving runtimes only a small factor
slower than first-order systems. Third-order fea-
tures have also been included in transition systems
(Zhang and Nivre, 2011) and graph-based parsers
with cube-pruning (Zhang and McDonald, 2012).
Unfortunately, non-projective dependency
parsers (appropriate for languages with a more
flexible word order, such as Czech, Dutch, and
German) lag behind these recent advances. The
main obstacle is that non-projective parsing is
NP-hard beyond arc-factored models (McDonald
and Satta, 2007). Approximate parsers have there-
fore been introduced, based on belief propagation
(Smith and Eisner, 2008), dual decomposition
(Koo et al, 2010), or multi-commodity flows
(Martins et al, 2009, 2011). These are all in-
stances of turbo parsers, as shown by Martins et
al. (2010): the underlying approximations come
from the fact that they run global inference in
factor graphs ignoring loop effects. While this
line of research has led to accuracy gains, none of
these parsers use third-order contexts, and their
speeds are well behind those of projective parsers.
This paper bridges the gap above by presenting
the following contributions:
? We apply the third-order feature models of Koo
and Collins (2010) to non-projective parsing.
? This extension is non-trivial since exact dy-
namic programming is not applicable. Instead,
we adapt AD3, the dual decomposition algo-
rithm proposed by Martins et al (2011), to han-
dle third-order features, by introducing special-
ized head automata.
? We make our parser substantially faster than the
many-components approach of Martins et al
(2011). While AD3 requires solving quadratic
subproblems as an intermediate step, recent re-
sults (Martins et al, 2012) show that they can be
addressed with the same oracles used in the sub-
gradient method (Koo et al, 2010). This enables
AD3 to exploit combinatorial subproblems like
the the head automata above.
Along with this paper, we provide a free distribu-
tion of our parsers, including training code.1
2 Dependency Parsing with AD3
Dual decomposition is a class of optimization
techniques that tackle the dual of combinatorial
1Released as TurboParser 2.1, and publicly available at
http://www.ark.cs.cmu.edu/TurboParser.
617
Figure 1: Parts considered in this paper. First-
order models factor over arcs (Eisner, 1996; Mc-
Donald et al, 2005), and second-order models in-
clude also consecutive siblings and grandparents
(Carreras, 2007). Our parsers add also arbitrary
siblings (not necessarily consecutive) and head bi-
grams, as in Martins et al (2011), in addition
to third-order features for grand- and tri-siblings
(Koo and Collins, 2010).
problems in a modular and extensible manner (Ko-
modakis et al, 2007; Rush et al, 2010). In this
paper, we employ alternating directions dual de-
composition (AD3; Martins et al, 2011). Like
the subgradient algorithm of Rush et al (2010),
AD3 splits the original problem into local sub-
problems, and seeks an agreement on the over-
lapping variables. The difference is that the AD3
subproblems have an additional quadratic term to
accelerate consensus. Recent analysis (Martins et
al., 2012) has shown that: (i) AD3 converges at
a faster rate,2 and (ii) the quadratic subproblems
can be solved using the same combinatorial ma-
chinery that is used in the subgradient algorithm.
This opens the door for larger subproblems (such
as the combination of trees and head automata in
Koo et al, 2010) instead of a many-components
approach (Martins et al, 2011), while still enjoy-
ing faster convergence.
2.1 Our Setup
Given a sentence with L words, to which we
prepend a root symbol $, let A := {?h,m? | h ?
{0, . . . , L}, m ? {1, . . . , L}, h 6= m} be the
set of possible dependency arcs. We parame-
terize a dependency tree via an indicator vector
u := ?ua?a?A, where ua is 1 if the arc a is in the
tree, and 0 otherwise, and we denote by Y ? R|A|
the set of such vectors that are indicators of well-
2Concretely, AD3 needs O(1/) iterations to converge to
a -accurate solution, while subgradient needs O(1/2).
formed trees. Let {As}Ss=1 be a cover of A, where
each As ? A. We assume that the score of a parse
tree u ? Y decomposes as f(u) :=?Ss=1 fs(zs),
where each zs := ?zs,a?a?As is a ?partial view? of
u, and each local score function fs comes from a
feature-based linear model.
Past work in dependency parsing considered ei-
ther (i) a few ?large? components, such as trees
and head automata (Smith and Eisner, 2008; Koo
et al, 2010), or (ii) many ?small? components,
coming from a multi-commodity flow formulation
(Martins et al, 2009, 2011). Let Ys ? R|As| de-
note the set of feasible realizations of zs, i.e., those
that are partial views of an actual parse tree. A tu-
ple of views ?z1, . . . ,zS? ??Ss=1 Ys is said to be
globally consistent if zs,a = zs?,a holds for every
a, s and s? such that a ? As?As? . We assume each
parse u ? Y corresponds uniquely to a globally
consistent tuple of views, and vice-versa. Follow-
ing Martins et al (2011), the problem of obtaining
the best-scored tree can be written as follows:
maximize ?Ss=1 fs(zs)
w.r.t. u ? R|A|, zs ? Ys, ?s
s.t. zs,a = ua, ?s, ?a ? As, (1)
where the equality constraint ensures that the par-
tial views ?glue? together to form a coherent parse
tree.3
2.2 Dual Decomposition and AD3
Dual decomposition methods dualize out the
equality constraint in Eq. 1 by introducing La-
grange multipliers ?s,a. In doing so, they solve a
relaxation where the combinatorial sets Ys are re-
placed by their convex hulls Zs := conv(Ys).4 All
that is necessary is the following assumption:
Assumption 1 (Local-Max Oracle). Every s ?
{1, . . . , S} has an oracle that solves efficiently any
instance of the following subproblem:
maximize fs(zs) +
?
a?As ?s,azs,a
w.r.t. zs ? Ys. (2)
Typically, Assumption 1 is met whenever the max-
imization of fs over Ys is tractable, since the ob-
jective in Eq. 2 just adds a linear function to fs.
3Note that any tuple ?z1, . . . , zS? ? ?Ss=1 Ys satisfyingthe equality constraints will be globally consistent; this fact,
due the assumptions above, will imply u ? Y.
4Let ?|Ys| := {? ? R|Ys| |? ? 0, ?ys?Ys ?ys = 1}be the probability simplex. The convex hull of Ys is the set
conv(Ys) := {
?
ys?Ys ?ysys | ? ? ?
|Ys|}. Its members
represent marginal probabilities over the arcs in As.
618
The AD3 algorithm (Martins et al, 2011) alter-
nates among the following iterative updates:
? z-updates, which decouple over s = 1, . . . , S,
and solve a penalized version of Eq. 2:
z(t+1)s := argmax
zs?Zs
fs(zs) +
?
a?As ?
(t)
s,azs,a
??2
?
a?As(zs,a ? u
(t)
a )2. (3)
Above, ? is a constant and the quadratic term
penalizes deviations from the current global so-
lution (stored in u(t)).5 We will see (Prop. 2)
that this problem can be solved iteratively using
only the Local-Max Oracle (Eq. 2).
? u-updates, a simple averaging operation:
u(t+1)a := 1|{s : a?As}|
?
s : a?As z
(t+1)
s,a . (4)
? ?-updates, where the Lagrange multipliers are
adjusted to penalize disagreements:
?(t+1)s,a := ?(t)s,a ? ?(z(t+1)s,a ? u(t+1)a ). (5)
In sum, the only difference between AD3 and
the subgradient method is in the z-updates, which
in AD3 require solving a quadratic problem.
While closed-form solutions have been developed
for some specialized components (Martins et al,
2011), this problem is in general more difficult
than the one arising in the subgradient algorithm.
However, the following result, proved in Martins
et al (2012), allows to expand the scope of AD3
to any problem which satisfies Assumption 1.
Proposition 2. The problem in Eq. 3 admits a
solution z?s which is spanned by a sparse basis
W ? Ys with cardinality at most |W| ? O(|As|).
In other words, there is a distribution ? with sup-
port in W such that z?s =
?
ys?W ?ysys.6
Prop. 2 has motivated an active set alorithm
(Martins et al, 2012) that maintains an estimate
of W by iteratively adding and removing elements
computed through the oracle in Eq. 2.7 Typically,
very few iterations are necessary and great speed-
ups are achieved by warm-starting W with the ac-
tive set computed in the previous AD3 iteration.
This has a huge impact in practice and is crucial to
obtain the fast runtimes in ?4 (see Fig. 2).
5In our experiments (?4), we set ? = 0.05.
6Note that |Ys| = O(2|As|) in general. What Prop. 2
tells us is that the solution of Eq. 3 can be represented as a
distribution over Ys with a very sparse support.
7The algorithm is a specialization of Nocedal and Wright
(1999), ?16.4, which effectively exploits the sparse represen-
tation of z?s . For details, see Martins et al (2012).
0 10 20 30 40 50sentence length (words)0.00
0.10
0.20
avera
geru
ntime
(sec.) AD3Subgrad.
Figure 2: Comparison between AD3 and subgra-
dient. We show averaged runtimes in PTB ?22 as
a function of the sentence length. For subgradi-
ent, we chose for each sentence the most favorable
stepsize in {0.001, 0.01, 0.1, 1}.
3 Solving the Subproblems
We next describe the actual components used in
our third-order parsers.
Tree component. We use an arc-factored score
function (McDonald et al, 2005): f TREE(z) =?L
m=1 ?ARC(pi(m),m), where pi(m) is the parent
of the mth word according to the parse tree z,
and ?ARC(h,m) is the score of an individual arc.
The parse tree that maximizes this function can be
found in time O(L3) via the Chu-Liu-Edmonds?
algorithm (Chu and Liu, 1965; Edmonds, 1967).8
Grand-sibling head automata. Let Ainh and
Aouth denote respectively the sets of incoming and
outgoing candidate arcs for the hth word, where
the latter subdivides into arcs pointing to the right,
Aouth,?, and to the left, Aouth,?. Define the sets
AGSIBh,? = Ainh ?Aouth,? andAGSIBh,? = Ainh ?Aouth,?. We
describe right-side grand-sibling head automata;
their left-side counterparts are analogous. For
each head word h in the parse tree z, define
g := pi(h), and let ?m0,m1, . . . ,mp+1? be the se-
quence of right modifiers of h, with m0 = START
and mp+1 = END. Then, we have the following
grand-sibling component:
fGSIBh,? (z|AGSIBh,?) =
?p+1
k=1
(
?SIB(h,mk?1,mk)
?GP(g, h,mk) + ?GSIB(g, h,mk?1,mk)
)
,
where we use the shorthand z|B to denote the
subvector of z indexed by the arcs in B ? A.
Note that this score function absorbs grandparent
and consecutive sibling scores, in addition to the
grand-sibling scores.9 For each h, fGSIBh,? can be
8In fact, there is an asymptotically fasterO(L2) algorithm
(Tarjan, 1977). Moreover, if the set of possible arcs is reduced
to a subset B ? A (via pruning), then the fastest known al-
gorithm (Gabow et al, 1986) runs in O(|B|+L logL) time.
9Koo et al (2010) used an identical automaton for their
second-order model, but leaving out the grand-sibling scores.
619
No pruning |Ainm| ? K same, + |Aouth | ? J
TREE O(L2) O(KL+ L logL) O(KL+ L logL)
GSIB O(L4) O(K2L2) O(JK2L)
TSIB O(L4) O(KL3) O(J2KL)
SEQ O(L3) O(K2L) O(K2L)
ASIB O(L3) O(KL2) O(JKL)
Table 1: Theoretical runtimes of each subproblem
without pruning, limiting the number of candidate
heads, and limiting (in addition) the number of
modifiers. Note the O(L logL) total runtime per
AD3 iteration in the latter case.
maximized in time O(L3) with dynamic program-
ming, yielding O(L4) total runtime.
Tri-sibling head automata. In addition, we de-
fine left and right-side tri-sibling head automata
that remember the previous two modifiers of a
head word. This corresponds to the following
component function (for the right-side case):
f TSIBh,? (z|Aouth,?) =
?p+1
k=2 ?TSIB(h,mk?2,mk?1,mk).
Again, each of these functions can be maximized
in time O(L3), yielding O(L4) runtime.
Sequential head bigram model. Head bigrams
can be captured with a simple sequence model:
f SEQ(z) =
?L
m=2 ?HB(m,pi(m), pi(m? 1)).
Each score ?HB(m,h, h?) is obtained via features
that look at the heads of consecutive words (as in
Martins et al (2011)). This function can be maxi-
mized in time O(L3) with the Viterbi algorithm.
Arbitrary siblings. We handle arbitrary siblings
as in Martins et al (2011), definingO(L3) compo-
nent functions of the form fASIBh,m,s(z?h,m?, z?h,s?) =
?ASIB(h,m, s). In this case, the quadratic problem
in Eq. 3 can be solved directly in constant time.
Tab. 1 details the time complexities of each sub-
problem. Without pruning, each iteration of AD3
has O(L4) runtime. With a simple strategy that
limits the number of candidate heads per word to
a constant K, this drops to cubic time.10 Further
speed-ups are possible with more pruning: by lim-
iting the number of possible modifiers to a con-
stant J , the runtime would reduce to O(L logL).
10In our experiments, we employed this strategy withK =
10, by pruning with a first-order probabilistic model. Fol-
lowing Koo and Collins (2010), for each word m, we also
pruned away incoming arcs ?h,m? with posterior probability
less than 0.0001 times the probability of the most likely head.
UAS Tok/sec
PTB-YM ?22, 1st ord 91.38 4,063
PTB-YM ?22, 2nd ord 93.15 1,338
PTB-YM ?22, 2nd ord, +ASIB, +HB 93.28 1,018
PTB-YM ?22, 3rd ord 93.29 709
PTB-YM ?22, 3rd ord, gold tags 94.01 722
This work (PTB-YM ?23, 3rd ord) 93.07 735
Koo et al (2010) 92.46 112?
Huang and Sagae (2010) 92.1? 587?
Zhang and Nivre (2011) 92.9? 680?
Martins et al (2011) 92.53 66?
Zhang and McDonald (2012) 93.06 220
This work (PTB-S ?23, 3rd ord) 92.82 604
Rush and Petrov (2012) 92.7? 4,460
Table 2: Results for the projective English dataset.
We report unlabeled attachment scores (UAS) ig-
noring punctuation, and parsing speeds in tokens
per second. Our speeds include the time necessary
for pruning, evaluating features, and decoding, as
measured on a Intel Core i7 processor @3.4 GHz.
The others are speeds reported in the cited papers;
those marked with ? were converted from times per
sentence.
4 Experiments
We first evaluated our non-projective parser in a
projective English dataset, to see how its speed and
accuracy compares with recent projective parsers,
which can take advantage of dynamic program-
ming. To this end, we converted the Penn Tree-
bank to dependencies through (i) the head rules
of Yamada and Matsumoto (2003) (PTB-YM) and
(ii) basic dependencies from the Stanford parser
2.0.5 (PTB-S).11 We trained by running 10 epochs
of cost-augmented MIRA (Crammer et al, 2006).
To ensure valid parse trees at test time, we rounded
fractional solutions as in Martins et al (2009)?
yet, solutions were integral ? 95% of the time.
Tab. 2 shows the results in the dev-set (top
block) and in the test-set (two bottom blocks). In
the dev-set, we see consistent gains when more ex-
pressive features are added, the best accuracies be-
ing achieved with the full third-order model; this
comes at the cost of a 6-fold drop in runtime com-
pared with a first-order model. By looking at the
two bottom blocks, we observe that our parser
has slightly better accuracies than recent projec-
tive parsers, with comparable speed levels (with
the exception of the highly optimized vine cascade
approach of Rush and Petrov, 2012).
11We train on sections ?02?21, use ?22 as validation data,
and test on ?23. We trained a simple 2nd-order tagger with
10-fold jackknifing to obtain automatic part-of-speech tags
for ?22?23, with accuracies 97.2% and 96.9%, respectively.
620
First Ord. Sec. Ord. Third Ord. Best published UAS RP12 ZM12
UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS
Arabic 77.23 2,481 78.50 388 79.64 197 81.12 - Ma11 - - -
Bulgarian 91.76 5,678 92.82 2,049 93.10 1,273 93.50 - Ma11 91.9 3,980 93.08
Chinese 88.49 18,094 90.14 4,284 89.98 2,592 91.89 - Ma10 90.9 7,800 -
Czech 87.66 1,840 90.00 751 90.32 501 89.46 - Ma11 - - -
Danish 89.42 4,110 91.20 1,053 91.48 650 91.86 - Ma11 - -
Dutch 83.61 3,884 86.37 1,294 86.19 599 85.81 121 Ko10 - - -
German 90.52 5,331 91.85 1,788 92.41 965 91.89 - Ma11 90.8 2,880 91.35
English 91.21 3,127 93.03 1,317 93.22 785 92.68 - Ma11 - - -
Japanese 92.78 23,895 93.14 5,660 93.52 2,996 93.72 - Ma11 92.3 8,600 93.24
Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69
Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - -
Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48
Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44
Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - -
Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008.
?Best Published UAS? includes the most accurate parsers among Nivre et al (2006), McDonald et al
(2006), Martins et al (2010, 2011), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald
(2012). The last two are shown separately in the rightmost columns.
In our second experiment (Tab. 3), we used 14
datasets, most of which are non-projective, from
the CoNLL 2006 and 2008 shared tasks (Buch-
holz and Marsi, 2006; Surdeanu et al, 2008).
Our third-order model achieved the best reported
scores for English, Czech, German, and Dutch?
which includes the three largest datasets and the
ones with the most non-projective dependencies?
and is on par with the state of the art for the
remaining languages. To our knowledge, the
speeds are the highest reported among higher-
order non-projective parsers, and only about 3?
4 times slower than the vine parser of Rush and
Petrov (2012), which has lower accuracies.
5 Conclusions
We presented new third-order non-projective
parsers which are both fast and accurate. We de-
coded with AD3, an accelerated dual decomposi-
tion algorithm which we adapted to handle large
components, including specialized head automata
for the third-order features, and a sequence model
for head bigrams. Results are above the state of
the art for large datasets and non-projective lan-
guages. In the hope that other researchers may find
our implementation useful or are willing to con-
tribute with further improvements, we made our
parsers publicly available as open source software.
Acknowledgments
We thank all reviewers for their insightful com-
ments and Lingpeng Kong for help in converting
the Penn Treebank to Stanford dependencies. This
work was partially supported by the EU/FEDER
programme, QREN/POR Lisboa (Portugal), under
the Intelligo project (contract 2012/24803), by a
FCT grant PTDC/EEI-SII/2312/2012, and by NSF
grant IIS-1054319.
References
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Inter-
national Conference on Natural Language Learn-
ing.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In International Con-
ference on Natural Language Learning.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc.
of International Conference on Computational Lin-
guistics, pages 340?345.
H. N. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan.
1986. Efficient algorithms for finding minimum
spanning trees in undirected and directed graphs.
Combinatorica, 6(2):109?122.
621
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of An-
nual Meeting of the Association for Computational
Linguistics, pages 1077?1086.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition:
Message-passing revisited. In Proc. of International
Conference on Computer Vision.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of Annual Meeting of the
Association for Computational Linguistics, pages 1?
11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and
D. Sontag. 2010. Dual decomposition for parsing
with non-projective head automata. In Proc. of Em-
pirical Methods for Natural Language Processing.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Morgan & Claypool Publishers.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proc. of Annual Meeting
of the Association for Computational Linguistics.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo
parsers: Dependency parsing by approximate vari-
ational inference. In Proc. of Empirical Methods for
Natural Language Processing.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Dual decomposition
with many overlapping components. In Proc. of Em-
pirical Methods for Natural Language Processing.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2012. Alternat-
ing directions dual decomposition. Arxiv preprint
arXiv:1212.6550.
R. T. McDonald and F. C. N. Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of Annual Meeting of the European
Chapter of the Association for Computational Lin-
guistics.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of International Conference on Parsing
Technologies.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proc. of Empirical
Methods for Natural Language Processing.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of International Confer-
ence on Natural Language Learning.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Procs. of In-
ternational Conference on Natural Language Learn-
ing.
J. Nocedal and S. J. Wright. 1999. Numerical opti-
mization. Springer-Verlag.
Alexander M Rush and Slav Petrov. 2012. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proc. of Conference of the North American Chapter
of the Association for Computational Linguistics.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming re-
laxations for natural language processing. In Proc.
of Empirical Methods for Natural Language Pro-
cessing.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of Empirical Methods
for Natural Language Processing.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez,
and J. Nivre. 2008. The CoNLL-2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. Proc. of International Conference on Natural
Language Learning.
R.E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?36.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proc. of International Conference on Parsing Tech-
nologies.
H. Zhang and R. McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of Empirical Methods in Natural Language
Processing.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
of the Annual Meeting of the Association for Com-
putational Linguistics.
622
