Evaluating Cross-Language Annotation Transfer in the MultiSemCor Corpus  
Luisa BENTIVOGLI, Pamela FORNER, Emanuele PIANTA 
ITC-irst 
Via Sommarive, 18 
38050 Povo ? Trento 
 Italy 
{bentivo, forner, pianta}@itc.it 
 
Abstract 
In this paper we illustrate and evaluate an approach 
to the creation of high quality linguistically 
annotated resources based on the exploitation of 
aligned parallel corpora. This approach is based on 
the assumption that if a text in one language has 
been annotated and its translation has not, 
annotations can be transferred from the source text 
to the target using word alignment as a bridge. The 
transfer approach has been tested in the creation of 
the MultiSemCor corpus, an English/Italian 
parallel corpus created on the basis of the English 
SemCor corpus. In MultiSemCor texts are aligned 
at the word level and semantically annotated with a 
shared inventory of senses. We present some 
experiments carried out to evaluate the different 
steps involved in the methodology. The results of 
the evaluation suggest that the cross-language 
annotation transfer methodology is a promising 
solution allowing for the exploitation of existing 
(mostly English) annotated resources to bootstrap 
the creation of annotated corpora in new (resource-
poor) languages with greatly reduced human effort.  
1 Introduction 
Large-scale language resources play a crucial role 
for a steady progress in the field of Natural 
Language Processing (NLP), as they are essential 
for carrying out basic research and for building 
portable and robust systems with broad coverage. 
More specifically, given the advances of machine 
learning statistical methods for NLP, with 
supervised training methods leading the way to 
major improvements in performance on different 
tasks, a particularly valuable resource is now 
represented by large linguistically annotated 
corpora. 
Up until some years ago, linguistically annotated 
corpora were only produced through manual 
annotation, or by manual check of automatically 
produced annotations. Unfortunately, manual 
annotation is a very difficult and time-consuming 
task, and this fact has led to a shortage of manual-
quality annotated data. The scarcity of large size 
annotated corpora is more acute for languages 
different from English, for which even minimal 
amounts of data are still missing. This state of 
affairs makes it clear that any endeavour aiming at 
reducing the human effort needed to produce 
manual-quality labelled data will be highly 
beneficial to the field. 
Recent studies have shown that a valuable 
opportunity for breaking the annotated resource 
bottleneck is represented by parallel corpora, 
which can be exploited in the creation of resources 
for new languages via projection of annotations 
available in another language. This paper 
represents our contribution to the research in this 
field. We present a novel methodology to create a 
semantically annotated corpus by exploiting 
information contained in an already annotated 
corpus, using word alignment as a bridge. The 
methodology has been applied in the creation of 
the MultiSemCor corpus. MultiSemCor is an 
English/Italian parallel corpus which is being 
created on the basis of the English SemCor corpus 
and where the texts are aligned at the word level 
and semantically annotated with a shared inventory 
of senses.  
Given the promising results of a pilot study 
presented in (Bentivogli and Pianta, 2002), the 
MultiSemCor corpus is now under development. In 
this paper we focus on a thorough evaluation of the 
steps involved in the transfer methodology. We 
evaluate the performance of a new version of the 
word alignment system and the final quality of the 
annotations transferred from English to Italian. In 
Section 2 we lay out the annotation transfer 
methodology and summarize some related work. In 
Section 3 we discuss some problematic issues 
related to the methodology which will be 
extensively tested and evaluated in Section 4. In 
Section 5 we report about the state of development 
of the MultiSemCor corpus and, finally, in Section 
6 we present conclusions and our thoughts on 
future work. 
2 The Annotation Transfer Methodology 
The MultiSemCor project (Bentivogli and Pianta, 
2002) aims at building an English/Italian parallel 
corpus, aligned at the word level and annotated 
with PoS, lemma and word sense. The parallel 
corpus is created by exploiting the SemCor corpus 
(Landes et al, 1998), which is a subset of the 
English Brown corpus containing about 700,000 
running words. In SemCor all the words are tagged 
by PoS, and more than 200,000 content words are 
also lemmatized and sense-tagged with reference 
to the WordNet lexical database1 (Fellbaum, 1998).  
The main hypothesis underlying this 
methodology is that, given a text and its translation 
into another language, the semantic information is 
mostly preserved during the translation process. 
Therefore, if the texts in one language have been 
semantically annotated and their translations have 
not, annotations can be transferred from the source 
language to the target using word alignment as a 
bridge.  
The first problem to be solved in the creation of 
MultiSemCor was the fact that the Italian 
translations of the SemCor texts did not exist. Our 
solution was to have the translations made by 
professional translators. Given the high costs of 
building semantically annotated corpora, requiring 
specific skills and very specialized training, we 
think that manually translating the annotated 
corpus and automatically transferring the 
annotations may be preferable to hand-labelling a 
corpus from scratch. Not only are translators more 
easily available than linguistic annotators, but 
translations may be a more flexible and durable 
kind of annotation. Moreover, the annotation 
transfer methodology has the further advantage of 
producing a parallel corpus.  
With respect to a situation in which the 
translation of a corpus is already available, a 
corpus translated on purpose presents the 
advantage that translations can be ?controlled?, 
i.e. carried out following criteria aiming at 
maximizing alignment and annotation transfer. Our 
professional translators are asked to use, 
preferably, the same dictionaries used by the word 
aligner, and to maximize, whenever possible, the 
lexical correspondences between source and target 
texts. The translators are also told that the 
controlled translation criteria should never be 
followed to the detriment of a good Italian prose. 
Controlled translations cost the same as free 
translations, while having the advantage of 
                                                     
1
 WordNet is an English lexical database, developed 
at Princeton University, in which nouns, verbs, 
adjectives, and adverbs are organized into sets of 
synonyms (synsets) and linked to each other by means 
of various lexical and semantic relationships. In the last 
years, within the NLP community WordNet has become 
the reference lexicon for almost all tasks involving word 
sense disambiguation (see, for instance, the Senseval 
competition). 
 
enhancing the performances of the annotation 
transfer procedure. 
Once the SemCor texts have been translated, the 
strategy for creating MultiSemCor consists of (i) 
automatically aligning Italian and English texts at 
the word level, and (ii) automatically transferring 
the word sense annotations from English to the 
aligned Italian words. The final result of the 
MultiSemCor project is an Italian corpus annotated 
with PoS, lemma and word sense, but also an 
aligned parallel corpus lexically annotated with a 
shared inventory of word senses. More 
specifically, the sense inventory used is 
MultiWordNet (Pianta et al, 2002), a multilingual 
lexical database in which the Italian component is 
strictly aligned with the English WordNet. 
2.1 Related Work 
The idea of obtaining linguistic information about 
a text in one language by exploiting parallel or 
comparable texts in another language has been 
explored in the field of Word Sense 
Disambiguation (WSD) since the early 90?s, the 
most representative works being (Brown et al, 
1991), (Gale et al, 1992), and (Dagan and Itai, 
1994).  
In more recent years, Ide et al (2002) present a 
method to identify word meanings starting from a 
multilingual corpus. A by-product of applying this 
method is that once a word in one language is 
word-sense tagged, the translation equivalents in 
the parallel texts are also automatically annotated. 
Cross-language tagging is the goal of the work 
by Diab and Resnik (2002), who present a method 
for word sense tagging both the source and target 
texts of parallel bilingual corpora with the 
WordNet sense inventory.  
Parallel to the studies regarding the projection of 
semantic information, more recently the NLP 
community has also explored the possibility of 
exploiting translation to project more syntax-
oriented annotations. Yarowsky et al (2001) 
describe a successful method consisting of (i) 
automatic annotation of English texts, (ii) cross-
language projection of annotations onto target 
language texts, and (iii) induction of noise-robust 
taggers for the target language. A further step is 
made in (Hwa et al, 2002) and (Cabezas et al, 
2001), which address the task of acquiring a 
dependency treebank by bootstrapping from 
existing linguistic resources for English. Finally, in 
(Riloff et al, 2002) a method is presented for 
rapidly creating Information Extraction (IE) 
systems for new languages by exploiting existing 
IE systems via cross-language projection. 
The results of all the above mentioned studies 
show how previous major investments in English 
annotated corpora and tool development can be 
effectively leveraged across languages, allowing 
the development of accurate resources and tools in 
other languages without comparable human effort. 
3 Quality Issues 
The MultiSemCor project raises a number of 
theoretical and practical issues. For instance: is 
translational language fully representative of the 
general use of language in the same way as 
original language is? To what extent are the lexica 
of different languages comparable? These 
theoretical issues have already been presented in 
(Pianta and Bentivogli, 2003) and will not be 
discussed here. In the following, we address the 
issue of the quality of the annotation resulting from 
the application of the methodology.  
As opposed to automatic word sense 
disambiguation tasks, the MultiSemCor project 
specifically aims at producing manual-quality 
annotated data. Therefore, a potential risk which 
needs to be faced is represented by the possible 
degradation of the Italian annotation quality 
through the various steps of the annotation transfer 
procedure. A number of factors must be taken into 
account. First, annotation errors can be found in 
the original English texts. Then, the word aligner 
may align words incorrectly, and finally the 
transfer of the semantic annotations may not be 
applicable to certain translation pairs.  
SemCor quality. The English SemCor corpus has 
been manually annotated. However, some 
annotation errors can be found in the texts (see 
Fellbaum et al, 1998, for SemCor taggers? 
confidence ratings). As an example, the word 
pocket in the sentence ?He put his hands on his 
pockets? was incorrectly tagged with the WordNet 
synset {pouch, sac, sack, pocket -- an enclosed 
space} instead of the correct one {pocket -- a small 
pouch in a garment for carrying small articles}. 
Word alignment quality. The feasibility of the 
entire MultiSemCor project heavily depends on the 
availability of an English/Italian word aligner with 
very good performance in terms of recall and, 
more importantly, precision.  
Transfer quality. Even when both the original 
English annotations and the word alignment are 
correct, a number of cases still remain for which 
the transfer of the annotation is not applicable. An 
annotation is not transferable from the source 
language to the target when the translation 
equivalent does not preserve the lexical meaning of 
the source language. In these cases, if the 
alignment process puts the two expressions in 
correspondence, then the transfer of the sense 
annotation from the source to the target language is 
not correct.  
The first main cause of incorrect transfer is 
represented by translation equivalents which are 
not cross-language synonyms of the source 
language words. For example, in a sentence of the 
corpus the English word meaning is translated with 
the Italian word motivo (reason, grounds) which is 
suitable in that specific context but is not a 
synonymic translation of the English word. In this 
case, if the two words are aligned, the transfer of 
the sense annotation from English is not correct as 
the English sense annotation is not suitable for the 
Italian word. A specific case of non-synonymous 
translation occurs when a translation equivalent 
does not belong to the same lexical category of the 
source word. For example, the English verb to 
coexist in the sentence ?the possibility for man to 
coexist with animals? has been translated with the 
Italian noun coesistenza (coexistence) in ?le 
possibilit? di coesistenza tra gli uomini e gli 
animali?. Even if the translation is suitable for that 
context, the English sense of the verb cannot be 
transferred to the Italian noun. Sometimes, non-
synonymous translations are due to errors in the 
Italian translation, as in pull translated as spingere 
(push).  
A second case which offers challenge to the 
sense annotation transfer is phrasal 
correspondence, occurring when a target phrase 
has globally the same meaning as the 
corresponding source phrase, but the single words 
of the phrase are not cross-language synonyms of 
their corresponding source words. For example, the 
expression a dreamer sees has been translated as 
una persona sogna (a person dreams). The Italian 
translation maintains the synonymy at the phrase 
level but the single component words do not. 
Therefore, if the single words were aligned any 
transfer from English to Italian would be incorrect. 
Another example of phrasal correspondence, in 
which the semantic equivalence between words in 
the source and target phrase is even fuzzier, is 
given by the English phrase the days would get 
shorter and shorter translated as imminente fine 
dei tempi (imminent end of times). 
Another controversial cause of possible incorrect 
transfer is represented by the case in which the 
translation equivalent is indeed a cross-language 
synonym of the source expression but it is not a 
lexical unit. This usually happens with lexical 
gaps, i.e. when a language expresses a concept 
with a lexical unit whereas the other language 
expresses the same concept with a free 
combination of words, as for instance the English 
word successfully which can only be translated 
with the Italian free combination of words con 
successo (with success). However, it can also be 
the result of a choice made by the translator who 
decides to use a free combination of words instead 
of a possible lexical unit, as in empirically 
translated as in modo empirico (in an empirical 
manner) instead of empiricamente. In these cases 
the problem arises because in principle if the target 
expression is not a lexical unit it cannot be 
annotated as a whole. On the contrary, each 
component of the free combination of words 
should be annotated with its respective sense. 
In the next Section we will address these quality 
issues in order to assess the extent to which they 
affect the cross-language annotation transfer 
methodology. 
4 Evaluation of the Annotation Transfer 
Methodology 
A number of experiments have been carried out in 
order to test the various steps involved in the 
annotation transfer methodology. More precisely, 
we evaluated the performances of the word 
alignment system and the quality of the final 
annotation of the Italian corpus. 
4.1 Word Alignment 
Word alignment is the first crucial step in the 
methodology applied to build MultiSemCor. The 
word aligner used in the project is KNOWA 
(KNOwledge-intensive Word Aligner), an 
English/Italian word aligner, developed at ITC-irst, 
which relies mostly on information contained in 
the Collins bilingual dictionary, available in 
electronic format. KNOWA also exploits a 
morphological analyzer and a multiword 
recognizer for both English and Italian. For a 
detailed discussion of the characteristics of this 
tool, see (Pianta and Bentivogli, 2004). 
Some characteristics of the MultiSemCor 
scenario make the alignment task easier for 
KNOWA. First, in SemCor all multiwords 
included in WordNet are explicitly marked. Thus 
KNOWA does not need to recognize English 
multiwords, although it still needs to recognize the 
Italian ones. Second, within MultiSemCor word 
alignment is done with the final aim of transferring 
lexical annotations from English to Italian. Since 
only content words have word sense annotations in 
SemCor, it is more important that KNOWA 
behaves correctly on content words, which are 
easier to align than functional words. 
To evaluate the word aligner performance on the 
MultiSemCor task we created a gold standard 
composed of three English unseen texts (br-f43, 
br-l10, br-j53) taken randomly from the 
SemCor corpus. For each English text both a 
controlled and a free translation were made. Given 
the expectation that free translations are less 
suitable for word alignment, we decided to test 
KNOWA also on them in order to verify if the 
annotation transfer methodology can be applied to 
already existing parallel corpora. 
The six resulting pairs of texts were manually 
aligned following a set of alignment guidelines 
which have been defined taking into account the 
work done in similar word alignment projects 
(Melamed, 2001). Annotators were asked to align 
different kinds of units (simple words, segments of 
more than one word, parts of words) and to mark 
different kinds of semantic correspondence 
between the aligned units, e.g. full correspondence 
(synonymic), non synonymic, changes in lexical 
category, phrasal correspondence. Inter-annotator 
agreement was measured with the Dice coefficient 
proposed in (V?ronis and Langlais, 2000) and can 
be considered satisfactory as it turned out to be 
87% for free translations and 92% for controlled 
translations. As expected, controlled translations 
produced a better agreement rate between 
annotators. 
For assessing the performance of KNOWA, the 
standard notions of Precision, Recall, and 
Coverage have been used following (V?ronis and 
Langlais, 2000). See (Och and Ney, 2003) and 
Arenberg et al, 2000) for different evaluation 
metrics. The performance of KNOWA applied to 
the MultiSemCor gold standard in a full-text 
alignment task is shown in Table 1. These results, 
which compare well with those reported in the 
literature (V?ronis, 2000) show that, as expected, 
controlled translations allow for a better alignment 
but also that free translations may be satisfactorily 
aligned.  
The evaluation of KNOWA with respect to the 
English content words which have a semantic tag 
in SemCor is reported in Tables 2 and 3, for both 
free and controlled translations and broken down 
by Part of Speech.  
 Precision Recall Coverage 
Free 83.5 57.9 60.0 
Controlled 88.4 67.5  74.9 
Table 1: KNOWA on Full-text  
 Precision Recall Coverage 
Nouns 93.7 81.1 86.5 
Verbs 85.6 70.3 82.1 
Adjectives 95.6 64.7 67.7 
Adverbs 88.4 38.5 43.5 
Total 91.2 68.2 74.8 
Table 2: KNOWA on sense-tagged words only 
(Free translations)  
 Precision Recall Coverage 
Nouns 95.9 82.5 86.1 
Verbs 90.7 76.8 84.7 
Adjectives 95.2 69.9 73.5 
Adverbs 90.4 51.6 57.1 
Total 93.9 74.6 79.5 
Table 3: KNOWA on sense-tagged words only 
(Controlled translations) 
We can see that ignoring function words the 
performance of the word aligner improves in both 
precision and recall. 
4.2 Italian Annotation Quality 
As pointed out in Section 3, even in the case of a 
perfect word alignment the transfer of the 
annotations from English to the correctly aligned 
Italian words can still be a source of errors in the 
resulting Italian annotations. In order to evaluate 
the quality of the annotations automatically 
transferred to Italian, a new gold standard was 
created starting from SemCor text br-g11. The 
English text, containing 2,153 tokens and 1,054 
semantic annotations, was translated into Italian in 
a controlled modality. The resulting Italian text is 
composed of 2,351 tokens, among which 1,085 are 
content words to be annotated. The English text 
and its Italian translation were manually aligned 
and the Italian text was manually semantically 
annotated taking into account the annotations of 
the English words. Each time an English 
annotation was appropriate for the Italian 
corresponding word, the annotator used it also for 
Italian. Otherwise, the annotator did not use the 
original English annotation for the Italian word and 
looked in WordNet for a suitable annotation. 
Moreover, when the English annotations were 
not suitable for annotating the Italian words, the 
annotator explicitly distinguished between wrong 
English annotations and English annotations that 
could not be transferred to the Italian translation 
equivalents. The errors in the English annotations 
amount to 24 cases. Non-transferable annotations 
amount to 155, among which 143 are due to lack 
of synonymy at lexical level and 12 to translation 
equivalents which are not lexical units. 
The differences between the English and Italian 
text with respect to the number of tokens and 
annotations have also been analysed. The Italian 
text has about 200 tokens and 31 annotated words 
more than the English text. The difference in the 
number of tokens is due to various factors. First, 
there are grammatical characteristics specific to the 
Italian language, such as a different usage of 
articles, or a greater usage of reflexive verbs which 
leads to a higher number of clitics. For example, 
the English sentence ?as cells coalesced? must be 
translated into Italian as ?quando le cellule si 
unirono?. Then, we have single English words 
translated into Italian with free combinations of  
words (ex: down translated as verso il basso) and 
multiwords which are recognized in English and 
not recognized in Italian (e.g. one token for 
nucleic_acid in the English text and two tokens in 
the Italian text, one for acido and one for 
nucleico). As regards content words to be 
annotated, we would have expected that their 
number was the same both in English and Italian. 
In fact, the difference we found is much lower than 
the difference between tokens. This difference is 
explained by the fact that some English content 
words have not been annotated. For example, 
modal and auxiliary verbs (to have, to be, can, 
may, to have to, etc.) and partitives (some, any) 
where systematically left unannotated in the 
English text whereas they have been annotated for 
Italian. 
The automatic procedures for word alignment 
and annotation transfer were run on text br-g11 
and evaluated against the gold standard. The total 
number of transferred senses amounts to 879. 
Among them, 756 are correct and 123 are incorrect 
for the Italian words. Table 4 summarizes the 
results in terms of precision, recall and coverage 
with respect to both English annotations available 
(1,054) and Italian words to be annotated (1,085).  
We can see that the final quality of the Italian 
annotations is acceptable, the precision amounting 
to 86.0%. The annotation error rate of 14.0% has 
been analyzed in order to classify the different 
factors affecting the transfer methodology. Table 5 
reports the data about the composition of the 
incorrect transfer. 
Comparing the number of annotation errors in 
the English source, as marked up during the 
creation of the gold standard (24), with the number 
of errors in the Italian annotation due to errors in 
the original annotation (22), we can see that almost 
all of the source errors have been transferred, 
contributing in a consistent way to the overall 
Italian annotation error rate. 
As regards word alignment, br-g11 was a 
relatively easy text as the performance of KNOWA 
(i.e. 96.5%) is higher than that obtained with the 
test set (see Table 3). 
 Precision Recall Coverage 
Wrt English 86.0 71.7 83.4 
Wrt Italian 86.0 69.7 81.0 
Table 4: Annotation evaluation on text br-g11 
 # % 
English annotation errors 22 2.5 
Word alignment errors 31 3.5 
Non-transferable annotations 70 8.0 
Total incorrect transfers 123 14.0 
Table 5: Composition of the incorrect transfer  
The last source of annotation errors is 
represented by words which have been correctly 
aligned but whose word sense annotation cannot be 
transferred. This happens with (i) translation 
equivalents which are lexical units but are not 
cross-language synonyms, and (ii) translation 
equivalents which are cross-language synonyms 
but are not lexical units. In practice, given the 
difficulty in deciding what is a lexical unit and 
what is not, we decided to accept the transfer of a 
word sense from an English lexical unit to an 
Italian free combination of words (see for instance 
occhiali da sole annotated with the sense of 
sunglasses). Therefore, only the lack of synonymy 
at lexical level has been considered an annotation 
error. 
The obtained results are encouraging. Among 
the 143 non-synonymous translations marked in 
the gold standard, only 70 have been aligned by the 
word alignment system, showing that KNOWA is 
well suited to the MultiSemCor task. The reason is 
that it relies on bilingual dictionaries where non-
synonymous translations are quite rare. This can be 
an advantage with respect to statistics-based word 
aligners, which are expected to be able to align a 
great number of non-synonymous translations, thus 
introducing more errors in the transfer procedure.  
A final remark about the evaluation concerns the 
proportion of non-transferable word senses with 
respect to errors in the original English 
annotations. It is sometimes very difficult to 
distinguish between annotation errors and non-
transferable word senses, also because we are not 
English native speakers. Thus, we preferred to be 
conservative in marking English annotations as 
errors unless in very clear cases. This approach 
may have reduced the number of the errors in the 
original English corpus and augmented the number 
of non-transferable word senses, thus penalizing 
the transfer procedure itself. 
Summing up, the cross-language annotation 
transfer methodology produces an Italian corpus 
which is tagged with a final precision of 86.0%. 
After the application of the methodology 19.0% of 
the Italian words still need to be annotated (see the 
annotation coverage of 81.0%). We think that, 
given the precision and coverage rates obtained 
from the evaluation, the corpus as it results from 
the automatic procedure can be profitably used. 
However, even in the case that a manual revision is 
envisaged, we think that hand-checking the 
automatically tagged corpus and manually 
annotating the remaining 19% still results to be 
cost effective with respect to annotating the corpus 
from scratch. 
5 The MultiSemCor Corpus Up to Now 
We are currently working at the extensive 
application of the annotation transfer methodology 
for the creation of the MultiSemCor corpus. Up to 
now, MultiSemCor is composed of 29 English 
texts aligned at the word level with their 
corresponding Italian translations. Both source and 
target texts are annotated with POS, lemma, and 
word sense. More specifically, as regards English 
we have 55,935 running words among which 
29,655 words are semantically annotated (from 
SemCor). As for Italian, the corpus amounts to 
59,726 running words among which 23,095 words 
are annotated with word senses that have been 
automatically transferred from English. 
MultiSemCor can be a useful resource for a 
variety of tasks, both as a monolingual 
semantically annotated corpus and as a parallel 
aligned corpus. As an example, we are already 
using it to automatically enrich the Italian 
component of MultiWordNet, the reference lexicon 
of MultiSemCor. As a matter of fact, out of the 
23,095 Italian words automatically sense-tagged, 
5,292 are not yet present in MultiWordNet and will 
be added to it. Moreover, the Italian component of 
MultiSemCor is being used as a gold standard for 
the evaluation of Word Sense Disambiguation 
systems working on Italian. Besides NLP 
applications, MultiSemCor is also suitable to be 
consulted by humans through a Web interface 
(Ranieri et al, 2004) which is available at: 
http://tcc.itc.it/projects/multisemcor.  
6 Conclusion and future directions 
We have presented and evaluated an approach to 
the creation of high quality semantically annotated 
resources based on the exploitation of aligned 
parallel corpora. The results obtained from the 
thorough evaluation of the different steps involved 
in the methodology confirm the feasibility of the 
MultiSemCor project. The cross-lingual annotation 
transfer methodology is going to be applied also to 
the remaining 157 SemCor texts, which are 
currently being translated into Italian. 
As regards future research directions within the 
transfer annotation paradigm, it would be 
interesting to extend the methodology to other 
languages, e.g. Spanish, for which a WordNet 
exists and can be aligned with MultiWordNet. 
Moreover, as the Brown Corpus, used to create 
SemCor, has been syntactically annotated within 
the English Penn Treebank, the syntactic 
annotations of the SemCor texts are also available. 
We are planning to explore the possibility of 
transferring the syntactic annotations from the 
English to the Italian texts of MultiSemCor. 
References  
L. Ahrenberg, M. Merkel, H. Sagvall and A. J. 
Tiedemann. 2000. Evaluation of word alignment 
systems. In Proceedings of LREC 2000, Athens, 
Greece. 
L. Bentivogli and E. Pianta. 2002. Opportunistic 
Semantic Tagging. In Proceedings of LREC-
2002, Las Palmas, Canary Islands, Spain. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1991. Word-Sense 
Disambiguation using Statistical Methods. In 
Proceedings of ACL?91, Berkeley, California, 
USA. 
C. Cabezas, B. Dorr and P. Resnik. 2001. Spanish 
Language Processing at University of Maryland: 
Building Infrastructure for Multilingual 
Applications. In Proceedings of the 2nd  
International Workshop on Spanish Language 
Processing and Language Technologies, Jaen, 
Spain. 
I. Dagan and A. Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics: 
20(4):563-596. 
M. Diab and P. Resnik. 2002. An unsupervised 
method for word sense tagging using parallel 
corpora. In In Proceedings of ACL 2002, 
Philadelphia, USA . 
C. Fellbaum, J. Grabowski and S. Landes. 1998. 
Performance and confidence in a semantic 
annotation task. In Fellbaum, C. (ed.). 1998. 
WordNet: An Electronic Lexical Database. The 
MIT Press, Cambridge (Mass.). 
C. Fellbaum (ed.). 1998. WordNet: An Electronic 
Lexical Database. The MIT Press, Cambridge 
(Mass.). 
W. A. Gale, K. W. Church and D. Yarowsky. 
1992. Using Bilingual Materials to Develop 
Word Sense Disambiguation Methods. In 
Proceedings of the Fourth International 
Conference on Theoretical and Methodological 
Issues in Machine Translation. Montreal, 
Canada. 
R. Hwa, P. Resnik and A. Weinberg. 2002. 
Breaking the Resource Bottleneck for 
Multilingual Parsing. In Proceedings of the 
LREC-2002 Workshop on "Linguistic Knowledge 
Acquisition and Representation: Bootstrapping 
Annotated Language Data'', Las Palmas, Canary 
Islands, Spain. 
N. Ide, T. Erjavec, and D. Tufis. 2002. Sense 
Discrimination with Parallel Corpora. In 
Proceedings of ACL'02 Workshop on Word 
Sense Disambiguation: Recent Successes and 
Future Directions, Philadelphia, USA.  
S. Landes C. Leacock, and R.I. Tengi. 1998. 
Building semantic concordances. In Fellbaum, C. 
(ed.) (1998) WordNet: An Electronic Lexical 
Database. The MIT Press, Cambridge (Mass.). 
I. D. Melamed. 2001. Empirical Methods for 
Exploiting Parallel Texts. The MIT Press, 
Cambridge (Mass.).  
F.J. Och and H. Ney. 2003. A systematic 
comparison of various statistical alignment 
models. Computational Linguistics, 29(1):19-53. 
E. Pianta and L. Bentivogli. 2004. Knowledge 
intensive word alignment with KNOWA. In 
Proceedings of Coling 2004, Geneva, 
Switzerland. 
E. Pianta and L. Bentivogli. 2003. Translation as 
Annotation. In Proceedings of the AI*IA 2003 
Workshop ?Topics and Perspectives of Natural 
Language Processing in Italy?, Pisa, Italy. 
E. Pianta, L. Bentivogli and C. Girardi. 2002. 
MultiWordNet: developing an aligned 
multilingual database. In Proceedings of the 
First Global WordNet Conference, Mysore, 
India.  
M. Ranieri, E. Pianta and L. Bentivogli. 2004. 
Browsing Multilingual Information with the 
MultiSemCor Web Interface. In Proceedings of 
the LREC-2004 Workshop ?The amazing utility 
of parallel and comparable corpora?, Lisbon, 
Portugal. 
E. Riloff, C. Schafer and D. Yarowsky. 2002. 
Inducing information extraction systems for new 
languages via cross-language projection. In 
Proceedings of Coling 2002, Taipei, Taiwan. 
J. V?ronis and  P. Langlais. 2000. Evaluation of 
parallel text alignment systems. In V?ronis, J. 
(ed.). 2000. Parallel Text Processing, Kluwer 
Academic Publishers, Dordrecht. 
J. V?ronis (ed.). 2000. Parallel Text Processing. 
Kluwer Academic Publishers, Dordrecht. 
D. Yarowsky, G. Ngai and R. Wicentowski. 2001. 
Inducing Multilingual Text Analysis Tools via 
Robust Projection across Aligned Corpora. In 
Proceedings of HLT 2001, San Diego, 
California, USA.  
Knowledge Intensive Word Alignment with KNOWA 
Emanuele PIANTA and Luisa BENTIVOGLI 
ITC-irst 
Via Sommarie, 18 
38050 Povo - Trento 
Italy 
{pianta,bentivo}@itc.it 
 
Abstract 
In this paper we present KNOWA, an 
English/Italian word aligner, developed at ITC-irst, 
which relies mostly on information contained in 
bilingual dictionaries. The performances of 
KNOWA are compared with those of GIZA++, a 
state of the art statistics-based alignment algorithm. 
The two algorithms are evaluated on the EuroCor 
and MultiSemCor tasks, that is on two 
English/Italian publicly available parallel corpora. 
The results of the evaluation show that, given the 
nature and the size of the available English-Italian 
parallel corpora, a language-resource-based word 
aligner such as KNOWA can outperform a fully 
statistics-based algorithm such as GIZA++. 
1 Introduction 
Aligning a text and its translation (also known as 
bitext) at the word level is a basic Natural 
Language Processing task that has found various 
applications in recent years. Word level alignments 
can be used to build bilingual concordances for 
human browsing, to feed machine learning-based 
translation algorithms, or as a basis for sense 
disambiguation algorithms or for automatic 
projection of linguistic annotations from one 
language to another.   
A number of word alignment algorithms have 
been presented in the literature, see for instance 
(V?ronis, 2000) and (Melamed, 2001). Shared 
evaluation procedures have been established, 
although there are still open issues on some 
evaluation details (Ahrenberg et al 2000).  
Most of the known alignment algorithms are 
statistics-based and do not exploit external 
linguistic resources, or use them to a very limited 
extent. The main attractive of such algorithms is 
that they are language independent, and only 
require a parallel corpus of reasonable size to be 
trained.  
However, word alignment can be used for 
different purposes and in different application 
scenarios; different kinds of alignment strategies 
produce different kinds of results (for instance in 
terms of precision/recall) which can be more or 
less suitable to the goal to be achieved. The 
requirement of having a parallel corpus of 
adequate size available for training the  statistics-
based algorithms may be difficult to meet, given 
that parallel corpora are a precious but often rare 
resource. For the most common languages, such as 
English, French, German, Chinese, etc., reference 
parallel corpora of adequate size are available, and 
indeed statistics-based algorithms are evaluated on 
such reference corpora. Unfortunately, if one needs 
to replicate in a different corpus the results 
obtained for the reference corpora, finding a 
parallel corpus of adequate size can be difficult 
even for the most common languages. Consider 
that one of the most appealing features of statistics-
based algorithms is their ability to induce 
alignment models for bitexts belonging to very 
specific domains, an ability which seems to be out 
of reach for algorithms based on generic linguistic 
resources. However, for the statistics-based 
algorithms to achieve their objective, a parallel 
corpus for the specific domain needs to be 
available, a requirement that in some cases cannot 
be met easily.  
For these reasons, we claim that in some cases 
algorithms based on external, linguistics resources, 
if available, can be a useful alternative to statistics- 
based algorithms. In the rest of this paper we will 
compare the results obtained by a statistics-based 
and a linguistic resource-based algorithm when 
applied to the EuroCor and MultiSemCor 
English/Italian corpora.  
The statistics-based algorithm to be evaluated is 
described in  (Och and Ney, 2003). For its 
evaluation we used an implementation by the 
authors themselves, called GIZA++, which is 
freely available to the scientific community (Och, 
2003). The second algorithm to be evaluated is 
crucially based on a bilingual dictionary and a 
morphological analyzer. It is called KNOWA 
(KNowledge intensive Word Aligner) and has been 
developed at ITC-irst by the authors of this paper. 
The results of the comparative evaluation show 
that, given specific application goals, and given the 
availability of Italian/English resources, KNOWA 
obtains results that are comparable or better than 
the results obtained with GIZA++. 
Section 2 describes the basic KNOWA 
algorithm. Sections 3 and 4 illustrate two enhanced 
versions of the KNOWA algorithm. Section 5 
reports an experiment in which both KNOWA and 
GIZA++ are first applied to the alignment of a 
reference parallel corpus, EuroCor, and then to the 
MultiSemCor corpus. Section 6 adds some 
conclusive remarks. 
2 KNOWA ? the basic algorithm 
KNOWA is an English/Italian word aligner, which 
relies mostly on information contained in the 
Collins bilingual dictionary, available in electronic 
format. KNOWA also exploits a morphological 
analyzer and a multiword recognizer, for both 
Italian and English. It does not require any corpus 
for training. However the input bitext must be 
sentence-aligned. 
For each sentence pair, KNOWA produces word 
alignments according to the following strategy: 
 
? The morphological analysis produces a set 
of candidate lemmas for each English and 
Italian word. 
? The candidate lemmas are ordered from the 
most to the least probable by means of a 
rule-based PoS ordering algorithm. 
? A three phase incremental alignment 
procedure takes as input the two sentences 
annotated with sets of ordered candidate 
lemmas and outputs a set of pairwise word 
alignments. 
 
The alignment procedure is crucially based on 
the relation of potential correspondence between 
English and Italian words: 
 
Given an English word wE and an Italian word 
wI, wI is the potential correspondent of wE if one of 
the candidate lemmas of wI is the translation 
equivalent of one of the candidate lemmas of wE, 
according to a bilingual dictionary. 
 
The potential correspondence relation holds 
between words, but is relative to a lemma pair. For 
instance we say that the words dreams and sogna 
are potential correspondents relative to the lemma 
pair <dream/verb, sognare/verb>. Two words can 
be potential correspondents relative to more than 
one lemma pair. For instance the words dream and 
sogno are potential correspondents relative to the 
two lemmas pairs <dream/verb, sognare/verb> and 
<dream/ noun, sogno/noun>. In fact dream and 
sogno can be either first singular person of the verb 
to dream and sognare, or singular forms of the 
noun dream and sogno respectively. 
The correspondence relation is called potential 
because in real texts, tokens that are potential 
correspondents may not in fact be translations of 
each other. Take for instance the following 
translation pair: ?ll cane e il gatto?, ?the dog and 
the cat?. The first occurrence of the Italian article 
?il? is a potential correspondent of both 
occurrences of the word ?the? in the English 
sentence, but is the translation of only the first one.  
In the first phase of the alignment procedure the 
potential correspondence relation is exploited in 
the English to Italian direction: 
 
For each English word wE in a certain position p: 
 
1. Get the most probable candidate lemma of wE. 
2. Get the Italian word wI in the same position p. 
3. Check if there is a candidate lemma of wI 
which is a potential correspondent of wE 
relative to the current English candidate 
lemma, on the basis of a bilingual lexicon. 
4. If yes, align wE and wI and record their 
lemmas. 
5. Otherwise consider the next probable 
candidate lemma of wE and go back to step 2. 
6. If no aligment is found, progressively extend 
the Italian word window and go back to step 1. 
 
By extending the Italian word window we mean 
considering Italian words in position p ? Delta, 
where p is the position of the English word and 
Delta can vary from 1 to a MaxDelta value. The 
value of MaxDelta is adjustable, but a number of 
experiments have shown that the best results are 
obtained when MaxDelta=14. Note that if the 
alignment is not found within the Italian word 
window, the English word is left unaligned. In 
Table 1 the box in the Italian column shows the 
maximal text window in which the potential 
correspondent of dream is searched (MaxDelta=5).  
The search starts from 15-precedente and ends 
after the first extension of the text window as 
sogno can be found in position p-1. 
In the second phase of the alignment procedure 
the potential correspondence relation is exploited 
from Italian to English. For each Italian word 
which has not been aligned in the first phase, the 
same procedure is applied as above. 
In the third and last phase, the algorithm tries to 
align the words which are still unaligned, resorting 
to the graphemic similarity of the Italian and 
English words. See (Yzaguirre et al, 2000) for a 
similar approach. 
Note that given the way in which the alignment 
procedure works, finding an alignment implies also 
selecting a PoS and a lemma for both English and 
Italian words. The selected PoS and lemma can be 
different from the ones that were considered most 
probable by the PoS ordering algorithm, due to the 
constraints added by the potential correspondence 
relation. 
? ? 
9-the 9-l' 
10-exact 10-esatta 
11-pattern 11-riproduzione 
12-of 12-di 
13-a 13-un 
14-previous 14-sogno 
15-dream 15-precedente 
16-we 16-abbiamo 
17-have 17-un 
18-an 18-caso 
19-instance 19-di 
20-of 20-deja_vu 
21-deja_vu 21-, 
? ? 
Table 1: An example of a maximal text window 
The KNOWA algorithm needs to be able to cope 
with at least two problematic aspects. The first are 
multiwords. To work properly, KNOWA needs to 
identify them in the source and target sentences, 
and needs knowledge about their translation 
equivalents. We have tried to exploit the 
information about multiwords contained in the 
Collins bilingual dictionary. However it is well 
known that dictionaries contain only a small part of 
multiwords actually used in language. Thus, there 
is still wide room to improve KNOWA's capability 
to handle multiwords.  
The second problematic aspect has to do with 
multiple potential correspondence relations. Given 
a source word in one language, more than one 
potential correspondent can be found within the 
maximal word window in the target language. This 
is particularly true in a full text alignment task, that 
is trying to align also functional words. Articles 
and determiners can occur repeatedly in any 
sentence, and almost any Italian preposition can be 
the translation of any English preposition; this 
makes the task of aligning determiners and 
preposition on the basis of the potential 
correspondence relation and the absolute position 
in the sentence hard. Whatever the number of 
potential correspondents, the alignment procedure 
selects the potential correspondent whose position 
is nearest to the position of the source word by first 
considering the most probable PoS of the source 
word. Unfortunately, the potential correspondent 
selected in this way is not always the right one. 
Thus multiple potential correspondents can be a 
source of alignment errors for KNOWA. In the 
following section we describe an extension of the 
basic KNOWA algorithm that tries to cope with 
this limitation. 
3 KNOWA ? the pivot extension 
In this section we illustrate a variation of the basic 
KNOWA algorithm, which tries to solve the 
problem of multiple potential correspondence 
relations. To illustrate the problem, let us consider 
the example in Table 2, where wrong alignments 
are marked with a cross. 
1-the 1-il 
2-boy 2-cane 
3-likes 3-piace 
4-the 4-al 
5-dog 5-bambino 
Table 2: Errors due to multiple potential 
correspondence relations 
In the Italian translation the order of the English 
noun phrase is inverted. This is due to the fact that 
the Italian translation of ?likes? follows a different 
verb subcategorization pattern. What is an object in 
English becomes a subject in Italian, causing a 
problem to the basic KNOWA algorithm. In fact, 
KNOWA correctly aligns 2-boy with 5-bambino, 
and 5-dog with 2-cane, even if the English and 
Italian nouns are not in the same position in the 
respective sentences, thanks to a search in the 
Italian word window. However, KNOWA would 
also align 1-the with 1-il, and 4-the with 4-al. 
Actually 1-the is a potential correspondent of both 
1-il, and 4-al (the correct translation), but 
KNOWA chooses 1-il because its position is 
nearest to 1-the. 
To solve these problems we need to use a 
different strategy. The solution is based on the 
observation that content words tend to be less 
involved in multiple potential correspondences 
than function words, and that function words tend 
to be attached to content words. Thus the basic 
idea amounts to trying first the alignment of 
content words, and only in a second phase trying 
the alignment of function words relative to the 
position of content words to which they are 
attached.  Alignments between content words act 
as pivots, around which the alignment of function 
words is tried. 
In the example above, first the algorithm finds 
the following correct alignments:  
 
2-boy <> 5-bambino 
3-likes <> 3-piace 
5-dog <> 2-cane 
 
Then, it takes the first alignment and tries to align 
the word before 2-boy and the word before 5-
bambino, finding the correct alignment between 1-
the and 4-al, and so on. 
We do not expect that all content words are 
equally good pivots. To assess the goodness of 
nouns, verbs, adjectives, and adverbs as pivot 
words, we run various experiments, taking only the 
content words of a specific PoS and some 
combinations of them as pivot words. The results 
of these experiments show that nouns, taken alone 
as pivots, produce the best results in comparison 
with other PoS or combinations of PoS. 
We also considered an alternative strategy for 
selecting pivots words. Instead of using the PoS as 
a predictor for the goodness of a word as pivot, 
which actually amounts to saying that words in a 
certain PoS can be aligned with a lower error rate 
than others, we selected as pivots the words for 
which the potential correspondence relation with 
their translation equivalents in the other language 
is one-to-one. Given a word wE in the English 
sentence and a word wI in its Italian translation, we 
select wE as a pivot word if, and only if,  wI is the 
only potential correspondent of wE, and wE is the 
only potential correspondent of wI. Of course, 
content words, and nouns in particular, tend to 
have such property much more frequently than 
words with other PoS. However, not all nouns have 
this characteristics. On the other hand certain 
function words, for instance conjunctions, may be 
involved in a one-to-one potential correspondence 
relation.  
Table 3 shows a complete English sentence with 
its translation, taken from MultiSemCor. All the 
pivot words involved in one-to-one potential 
correspondence relations, according the Collins 
dictionary, are connected by a solid line. Note that 
the relation between 2-temperatures and 2-clima is 
indeed one-to-one, but is not recorded in the 
reference dictionary, so it is marked with a dotted 
line in the table.  
Table 4 exemplifies instead typical cases of non-
pivot words: 9-rovente is the only potential 
translation of 1-sizzling, but 9-rovente can also 
translate 2-hot, so neither 1-sizzling nor 4-hot are 
selected as pivot words.  
The pivot extension of KNOWA has strong 
similarities with a strategy that is used by various 
statistics-based algorithms, aiming at selecting at 
first the translation correspondents that are most 
probably correct. Once these pivotal 
correspondences have been established, the 
remaining alignments are derived using the pivots 
as fixed points. Given that fact that these 
algorithms do not exploit bilingual dictionaries, the 
selection of the pivotal translation correspondent 
may be based on cognates, or specific frequency 
configurations. See among others (Simmard and 
Plamondon, 1998) and (Ribeiro et al, 2000). 
The results obtained by applying the one-to-one 
potential correspondence as criterion for selecting 
pivot words are illustrated further on in Section 5. 
 
1-Sizzling 
2-temperatures 
3-and 
4-hot 
5-summer 
6-pavements 
7-are 
8-anything 
9-but 
10-kind 
11-to 
12-the 
13-feet 
1-Il 
2-clima 
3-torrido 
4-e 
5-i 
6-marciapiedi 
7-dell? 
8-estate 
9-rovente 
10-non 
11-sono 
12-niente 
13-di 
14-buono 
15-per 
16-i 
17-piedi 
Table 3: pivot words involved in one-to-one 
potential correspondences 
1-Sizzling 
2-temperatures 
3-and 
4-hot 
5-summer 
6-pavements 
7-are 
8-anything 
9-but 
10-kind 
? 
1-Il 
2-clima 
3-torrido 
4-e 
5-i 
6-marciapiedi 
7-dell? 
8-estate 
9-rovente 
10-non 
? 
Table 4: typical potential correspondences for 
non-pivot words 
4 KNOWA - the breadth-first extension 
The pivot extension to the basic KNOWA 
algorithm is based on two main hypotheses: first, 
certain words, which we call pivot words and 
which are mainly content words, are easier to align 
than others; second, the position of the other 
words, mainly function words, is anchored to the 
position of pivot words. This means for instance 
that if an article is near to a noun in Italian, we 
expect the English translation of the article to be 
near the English translation of the noun.  
However if we look closer to the way the basic 
algorithm explores the search space of the potential 
correspondent in the word window, we will see 
that such strategy is inconsistent with the above 
two hypotheses. Suppose that we start from a pivot 
word wE1, in position pE1, as illustrated in Table 5, 
where pivot words are included in box. Then, we 
try to align a non-pivot word wE2 occurring in 
position pE1+1. If the correspondent of wE1, that is 
wI1, occurs in position pI1, then we expect the 
correspondent of wE2, to occur in position pI1+1. 
Now, if wI2 turns out not to be the  potential 
correspondent of wE2, possibly because wE2 has not 
been translated, KNOWA will extend the word 
window of wI2, and search the potential 
correspondents in position pI1 ? 2, pI1 ? 3, and so 
on, up to MaxDelta. We describe this by saying 
that the basic algorithm searches potential 
correspondents in the word window following a 
depth-first search strategy. Unfortunately, such 
strategy can cause alignment errors. Suppose that 
wE3 is another pivot word in position pE3, to be 
aligned with wI3 in position pI3, and that wE4 is a 
non-pivot word in position pE3+1, to be aligned 
with wI4, in position pI3+1. Suppose also that wE2 is 
a potential correspondent of wI4. Because of the 
depth-first search strategy, the basic KNOWA 
algorithm will align wE2 and wI4 wrongly. This kind 
of error can be avoided by adopting what can be 
called a breadth-first search strategy. In practice, 
for each pivot word we first search the potential 
correspondent in a word window of 0, that is in the 
expected initial position, then for each pivot word 
we search potential correspondents in a window of 
?1, and so on up to the MaxDelta. The results of 
testing these strategy are reported in the following 
section. 
 
1-  
2- wE1 
3- wE2 
4- 
5- 
6- wE3 
7- wE4 
8- 
9- 
10- 
? 
1- 
2- 
3-  
4- wI1 
5- wI2 
6- 
7- 
8- wI3 
9- wI4 
10- 
? 
Table 5: Wrong alignment caused by the first-
depth search strategy in the word window 1-9. 
5 The experiments 
We have run the experiments on two tasks, the 
EuroCor and the MultiSemCor alignment tasks. 
We call EuroCor a reduced and revised version of 
EuroParl, a multilingual corpus extracted from the 
proceedings of the European Parliament, see 
(Koehn, unpublished). EuroParl includes texts in 
11 European languages, automatically aligned at 
the sentence level, whereas EuroCor includes only 
a part of the texts in EuroParl and only for English 
and Italian. On the other hand, MultiSemCor is a 
reference English/Italian corpus being developed at 
ITC-irst, including SemCor (part of the Brown 
Corpus) and its Italian translations. MultiSemCor 
has been created with the purpose of automatically 
transfer lexical semantic annotations from English 
to Italian (Bentivogli and Pianta, 2002).  
For our experiments on EuroCor, we used as  
gold standard (and test set) a text that,  following 
the EuroParl naming conventions, can be  
identified as ep-98-09-18. The revised version of 
this text includes 385 sentences, and has been 
manually aligned at the word level. Also sentence 
alignment has been manually revised.  
For our experiments on MultiSemCor we used a 
gold standard composed of 6 files, manually 
aligned. Three of them have been exploited as 
development set and three as test set. In order to 
keep the test set as unseen as possible, the 
experiments whose main goal is tuning the 
algorithm by comparing various alignment 
strategies or parameters have been run on the 
development set. Once the best configuration has 
been obtained on the development set, we gave the 
results of running the algorithm with such 
configuration on the test set. 
In our first experiment we run GIZA++ on both 
EuroCor and MultiSemCor. At first, we run 
GIZA++ on the entire English/Italian part of 
EuroParl, including around 694,000 sentences. The 
training of GIZA++ on this big corpus took around 
two weeks only for the English-to-Italian direction, 
on a high-level Sun Spark with 4 GB of memory. 
For this reason we decided to run the subsequent 
experiments on EuroCor, a reduced version of 
EuroParl, including around 21,000 sentences. 
EuroCor includes the following texts from 
EuroParl: ep-96-05-08, ep-97-04-07, ep-98-04-01, 
ep-90-11-04, ep-99-01-14, ep-99-10-05, ep-00-06-
13, ep-00-09-04, ep-01-04-02, ep-01-04-03. the 
file in the gold standard, ep-98-09-18, should be 
added to these texts. These texts where chosen 
randomly, sampling them from as diverse periods 
of time as possible. Note that GIZA++ cannot be 
tested on a test set distinct from the training set. 
Thus we trained GIZA++ on the whole EuroCor 
corpus, including the file in the test set. Given the 
fact that we are simply using GIZA++ as a black 
box without having access to the internals of the 
alignment program, this seems acceptably safe 
from a methodological point of view. In all our 
experiments with GIZA++ we adopted a 
configuration of the system which is reported by 
the  authors to produce optimal results, that is 
15H5344454, where the number in the base refers to 
the IBM models 1, 3, 4, and 5, H refers to the 
HMM training, and the superscript figures refer to 
the number of iterations. 
5.1 The EuroCor task 
The first training of GIZA++ on EuroCor gave the 
following disappointing results on all-words 
alignment: 59.7% precision, 14.1% recall. After 
inspection of the corpus, we realized that the 
original files in EuroParl contain tokenization 
errors, and what counts more, a big number of 
sentence alignment errors. For this reason we 
produced a revised version of EuroCor, fixing 
these errors as extensively as possible.  
A new run of GIZA++ on the revised EuroCor 
gave the following result: P:62.0%, R:34.7% on all 
word alignment; P:53.2%, R:38.3% on content 
words only. These results compare badly with 
those reported by (Och and Ney, 2003) on the 
Hansard alignment task. For this task, the authors 
report a precision of 79.6%, for a training on a 
corpus of 8,000 sentences. Explaining such a 
difference is not easy. A first explanation can be 
the fact the EuroCor task is inherently harder than 
the Hansard task. Whereas in the Hansard corpus 
the texts are direct translations of each other, in the 
EuroCor corpus it happens quite frequently that the 
English and Italian texts are both translation of a 
text in a third language. As a consequence, the 
texts are much more difficult to align. A better and 
more systematic revision of the sentence 
alignments could also improve the performance of 
GIZA++. 
The basic version of KNOWA run on the 
EuroCor test file gives the results reported in Table 
6. These results confirm the difficulty of the 
EuroCor task, but are quite encouraging for 
KNOWA, given that no special tuning was made 
to obtain them. It is interesting to note that whereas 
GIZA++ performs better on the all-word task than 
on the content-only-word task, KNOWA gets 
better results on the content-word-only task. 
Although it is true that aligning function words 
seems inherently more difficult than aligning 
content word, the worse result obtained by a 
statistics-based algorithm such as GIZA++ on the 
content-words-only task may be explained by the 
fact that data about content words are more sparse 
that data about function words. 
  Precision Recall 
all 62.0 34.7 GIZA++ 
22k content 53.2 38.3 
all 63.4 41.6 KNOWA 
basic content 85.5 53.2 
Table 6: GIZA++ and KNOWA-basic on the 
EuroCor task 
 
5.2 The MultiSemCor task 
The training of GIZA++ on MultiSemCor has been 
quite problematic, due to the small dimensions of 
MultiSemCor. In the current phase of the project, 
only 2,948 sentences are available. This is a small 
corpus which allows for only an approximate 
comparison with the experiment reported by Och 
and Ney (2003) on a set of 8,000 sentences from 
the Hansard corpus. Also, the authors report an 
improvement of around 7 points in precision, in 
passing from a corpus of 8,000 to 128,000 
sentences. As the ultimate version of MultiSemCor 
is expected to include more than 20,000 sentences, 
we can expect a non negligible improvement in 
precision when GIZA++ will be applied to the 
final version of MultiSemCor.  
To simulate at least partly the improvement that 
one can expect from an increase in the size of 
MultiSemCor, we trained GIZA++ on the union of 
the available MultiSemCor and EuroCor. The 
results of the training on MultiSemCor only, and 
on the union of MultiSemCor and EuroCor are 
reported in Table 7. Besides the row for the all-
word task, the table contains also a SemCor row. 
This task concerns all the words that have been 
manually tagged in SemCor, and roughly 
corresponds to the content-word task. As the 
purpose of MultiSemCor is transferring lexical 
annotations from the English annotated words to 
the corresponding Italian words, it is particularly 
important that the alignment for the annotated 
words be correct. The results showed that GIZA++ 
works consistently better in the Italian-to-English 
direction, rather than vice versa, so we report the 
former direction. Only for the training on the union 
of the MultiSemCor and EuroCor data, we also 
report the results calculated by resorting to the 
symmetrization by intersection of the two 
alignments. Table 7 below shows that the 
MultiSemCor task is less difficult than the 
EuroCor Task; that GIZA++ consistently performs 
worse on content words; and finally that the 
increase in the dimensions of the training corpus 
produces a non marginal improvement in the 
precision, although not in the recall measure. 
Symmetrization produces a big improvement in 
precision but also an unacceptable worsening of 
the recall measure for GIZA++. 
The two last rows in the table report the 
performances of the basic version of KNOWA in 
the same two tasks. These results show that given 
the available resources, KNOWA outperforms 
GIZA++ in all tasks. This is even clearer if we 
consider the extended versions of KNOWA, as 
reported in Table 8. Finally Table 9 reports the 
results of KNOWA on the test set. 
 task Prec. Recall 
all 68.9 53.5 GIZA++ 3k 
(MSC) It ->En semcor 60.4 55.1 
all 73.4 55.2 GIZA++ 25k 
(MSC+EC) It ->En semcor 81.9 52.9 
all 95.2 38.8 GIZA++ 25k 
(MSC+EC) intersec semcor 95.8 37.1 
all 84.5 63.7 KNOWA 
basic semcor 92.0 73.4 
Table 7: GIZA++ and KNOWA-basic on the 
MultiSemCor task (development set) 
KNOWA version task Prec. Recall 
all 86.8 65.3 pivot (nouns) 
depth-first semcor 92.5 73.6 
all 88.1 66.5 pivot (1-to-1) 
depth-first semcor 92.8 74.4 
all 89.4 67.5 pivot (1-to-1) 
breadth-first semcor 93.0 74.6 
Table 8: KNOWA-enhanced on constrained 
translation (development-set) 
KNOWA version task Prec. Recall 
all 82.1 56.9 best 
(on free tran.) semcor 89.1 66.5 
all 87.0 66.6 best 
(constr. tran.) semcor 91.8 72.8 
Table 9: KNOWA-best on test set (free and 
constrained translation) 
6 Conclusion 
In this paper we compared the performances of two 
word aligners, one exclusively based on statistical 
principles, and the other intensively based on 
linguistic resources. Although statistics-based 
algorithms are very appealing, because they are 
language independent, and only need a parallel 
corpus of reasonable size to be trained, we have 
shown that, from a practical point of view, the lack 
of parallel corpora with the necessary 
characteristics can hamper the performances of the 
statistical algorithms. In these cases, an algorithm 
based on linguistic resources, if available, can 
outperform a statistics-based algorithm. 
Also, knowledge-intensive word aligners may be 
more effective when word alignment is needed for 
special purposes such as annotation transfer from 
one language to another. This is  the case for 
instance of the MultiSemCor project, in which, 
apart from a better performance in terms of 
precision and recall, a word aligner based on 
dictionaries, such as KNOWA, has the advantage 
that it will fail to align words that are not 
synonyms. The alignment of non-synonymous 
translation equivalents, which are hardly found in 
bi-lingual dictionaries, is usually a strength of 
corpus-based word aligners, but turns out to be a 
disadvantage in the MultiSemCor case, where the 
alignment of non synonyoums words causes the 
transfer of wrong word sense annotations from one 
language to the other. 
References  
Lars Ahrenberg, Magnus Merkel, Anna S?gvall 
Hein and J?rg Tiedemann. 2000. Evaluation of 
word alignment systems. In Proceedings of 
LREC 2000, Athens, Greece.  
Luisa Bentivogli and Emanuele Pianta. 2002. 
Opportunistic Semantic Tagging. In Proceedings 
of LREC-2002, Las Palmas, Canary Islands, 
Spain (2002). 
Philipp Koehn. Unpublished. Europarl: A 
Multilingual Corpus for Evaluation of Machine 
Translation, unpublised draft, available at http: 
//www.isi.edu/~koehn/publications/europarl.ps. 
Dan I. Melamed. 2001. Empirical Methods for 
Exploiting Parallel Texts. The MIT Press, 
Cambridge, Massachussets. 
Franz J. Och. 2003. GIZA++: Training of 
statistical translation models. Available at 
http://www.isi.edu/~och/GIZA++.html. 
Franz. J. Och and H. Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1):19-51. 
Ant?nio Ribeiro, Gabriel Lopes and Jo?o Mexia. 
2000. Using Confidence Bands for Parallel Texts 
Alignment. In Proceedings of the 38th 
Conference of the Association for Computational 
Linguistics (ACL 2000), Hong Kong, China, 
2000 October 3?6. pp. 432?439. 
Michel Simard and Pierre Plamondon. 1998. 
Bilingual Sentence Alignment: Balancing 
Robustness and Accuracy. In Machine 
Translation, 13(1):59-80. 
Jean V?ronis (ed.). 2000. Parallel Text Processing. 
Dordrecht: Kluwer Academic Publishers. 
Llu?s de Yzaguirre, M. Ribas, J. Vivaldi and M. T. 
Cabr?. 2000. Some technical aspects about 
aligning near languages. In Proceedings of 
LREC 2000, Athens, Greece 
67
68
69
70
Revising the WORDNET DOMAINS Hierarchy: semantics, coverage and 
balancing 
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, Emanuele Pianta 
ITC-irst ? Istituto per la Ricerca Scientifica e Tecnologica 
Via Sommarive 18, Povo ? Trento, Italy, 38050 
email:{bentivo, forner, magnini, pianta}@itc.it 
 
Abstract 
The continuous expansion of the multilingual 
information society has led in recent years to a pressing 
demand for multilingual linguistic resources suitable to 
be used for different applications.  
In this paper we present the WordNet Domains 
Hierarchy (WDH), a language-independent resource 
composed of 164, hierarchically organized, domain 
labels (e.g. Architecture, Sport, Medicine). Although 
WDH has been successfully applied to various Natural 
Language Processing tasks, the first available version 
presented some problems, mostly related to the lack of a 
clear semantics of the domain labels. Other correlated 
issues were the coverage and the balancing of the 
domains. We illustrate a new version of WDH 
addressing these problems by an explicit and systematic 
reference to the Dewey Decimal Classification. The new 
version of WDH has a better defined semantics and is 
applicable to a wider range of tasks. 
1 Introduction 
The continuous expansion of the multilingual 
information society with a growing number of new 
languages present on the Web has led in recent 
years to a pressing demand for multilingual 
applications. To support such applications, 
multilingual language resources are needed, which 
however require a lot of human effort to be built. 
For this reason, the development of language-
independent resources which factorize what is 
common to many languages, and are possibly 
linked to the language-specific resources, could 
bring great advantages to the development of the 
multilingual information society. 
A language-independent resource, usable in 
many automatic and human applications, is 
represented by domain hierarchies. The notion of 
domain is related to similar notions such as 
semantic field, subject matter, broad topic, subject 
code, subject domain, category. These notions are 
used, sometimes interchangeably, sometimes with 
significant distinctions, in various fields such as 
linguistics, lexicography, cataloguing, text 
categorization. As far as this work is concerned, 
we define a domain as an area of knowledge which 
is somehow recognized as unitary. A domain can 
be characterized by the name of a discipline where 
a certain knowledge area is developed (e.g. 
chemistry) or by the specific object of the 
knowledge area (e.g. food). Although objects of 
knowledge and disciplines that study them are 
clearly related, the relation between these two 
points of view on domains is sometimes blurred 
and may be a source of uncertainty on their exact 
definition. 
Another interesting duality when speaking about 
domains is related to the fact that knowledge 
manifests itself in both words and texts. So the 
notion of domain can be applied both to the study 
of words, where a domain is the area of knowledge 
to which a certain lexical concept belongs, or to the 
study of texts, where the domain of a text is its 
broad topic. In this work we will assume that also 
these two points of view on domains are strictly 
intertwined.  
By their nature, domains can be organized in 
hierarchies based on a relation of specificity. For 
instance we can say that TENNIS is a more specific 
domain than SPORT, or that ARCHITECTURE is more 
general than TOWN PLANNING. 
Domain hierarchies can be usefully integrated 
into other linguistic resources and are also 
profitably used in many Natural Language 
Processing (NLP) tasks such as Word Sense 
Disambiguation (Magnini et al 2002), Text 
Categorization (Schutze, 1998), Information 
Retrieval (Walker and Amsler, 1986).  
As regards the usage of Domain hierarchies in 
the field of multilingual lexicography, an example 
is given by the EuroWordNet Domain-ontology, a 
language independent domain hierarchy to which 
interlingual concepts (ILI-records) can be assigned 
(Vossen, 1998). In the same line, see also the 
SIMPLE domain hierarchy (SIMPLE, 2000).  
Large domain hierarchies are also available on 
the Internet, mainly meant for classifying web 
documents. See for instance the Google and Yahoo 
directories. 
A large-scale application of a domain hierarchy 
to a lexicon is represented by WORDNET DOMAINS 
(Magnini and Cavagli?, 2000). WORDNET 
DOMAINS is a lexical resource developed at ITC-
irst where each WordNet synset (Fellbaum, 1998) 
is annotated with one or more domain labels 
selected from a domain hierarchy which was 
specifically created to this purpose. As the 
WORDNET DOMAINS Hierarchy (WDH) is 
language-independent, it has been possible to 
exploit it in the framework of MultiWordNet 
(Pianta et al, 2002), a multilingual lexical database 
developed at ITC-irst in which the Italian 
component is strictly aligned with the English 
WordNet. In MultiWordNet, the domain 
information has been automatically transferred 
from English to Italian, resulting in a Italian 
version of WORDNET DOMAINS. For instance, as 
the English synset {court, tribunal, judicature} was 
annotated with the domain LAW, also the Italian 
synset {corte, tribunale}, which is aligned with the 
corresponding English synset, results automatically 
annotated with the LAW domain. This procedure 
can be applied to any other WordNet (or part of it) 
aligned with Princeton WordNet (see for instance 
the Spanish WordNet). 
It is worth noticing that two of the main on-
going projects addressing the construction of 
multilingual resources, that is MEANING (Rigau 
et al 2002) and BALKANET (see web site), make 
use of WORDNET DOMAINS. Finally, WORDNET 
DOMAINS is being profitably used by the NLP 
community mainly for Word Sense 
Disambiguation tasks in various languages. 
Another application of domain hierarchies can 
be found in the field of corpus creation. In many 
existing corpora (see for instance the BNC, the 
ANC, the Brown and LOB Corpora) domain is one 
of the most used criteria for text selection and/or 
classification. Given that a domain hierarchy is 
language independent, if the same domain 
hierarchy is used to build reference corpora for 
different languages, then it would be easy to create 
(a first approximation of) comparable corpora by 
putting in correspondence corpora sections 
belonging to the same domain. 
An example of a corpus in which the complete 
representation of domains is pursued in a 
systematic way is represented by the MEANING 
Italian corpus, a large size corpus of written 
contemporary Italian in which a subset of the 
WDH labels has been chosen as the fundamental 
criterion for the selection of the texts to be 
included in the corpus (Bentivogli et al, 2003). 
Given the relevance of language-independent 
domain hierarchies for multilingual applications, it 
is of primary importance that these resources have 
a well-defined semantics and structure in order to 
be useful in various application fields. This paper 
reports the work done to improve the WDH so that 
it complies with such requirements. In particular, 
the WDH revision has been carried out with 
reference to the Dewey Decimal Classification. 
The paper is organized as follows. Section 2 
briefly introduces the WORDNET DOMAINS 
Hierarchy and its main characteristics, with a short 
overview of the Dewey Decimal Classification 
system. Section 3 describes features and properties 
of the revision. Finally, in section 4, conclusions 
are reported. 
2 The WordNet Domains Hierarchy 
The first version of the WDH was composed of 
164 domain labels selected starting from the 
subject field codes used in current dictionaries, and 
the subject codes contained in the Dewey Decimal 
Classification (DDC), a general knowledge 
organization tool which is the most widely used 
taxonomy for library organization purposes. 
Domain labels were organized in five main trees, 
reaching a maximum depth of four. Figure 1 shows 
a fragment of one of the five main trees in the 
WORDNET DOMAINS original hierarchy. 
Doctrines
Psychology
Art
Religion
Psychoanalysis
Dance
Drawing
Music
Photography
Plastic Arts
Sculpture
Numismatics
Jewellery
Painting
Philately
Philosophy
Theatre
Mythology
Occultism
Roman Catholic
Theology
Figure 1: Fragment of the original WDH 
Domain labels were initially conceived to be 
application-oriented, that is, they have been 
integrated in WordNet with the main purpose of 
allowing the categorization of word senses and to 
provide useful information during the 
disambiguation process. 
The second level of WDH, where the so-called 
Basic Domains are represented, includes labels 
such as ART, SPORT, RELIGION and HISTORY, 
while in the third level a degree of major 
specialization is reproduced, and domains, like for 
example, DRAWING, PAINTING, TENNIS, 
VOLLEYBALL, and ARCHAEOLOGY can be found. For 
NLP tasks, the set of Basic Domains has proved to 
possess a suitable level of abstraction and 
granularity. 
Although the first version of WDH found many 
applications in different scenarios, it presented 
some problems. First, the domain labels did not 
have a defined semantics. The content of the labels 
could be suggested by the lexical meaning of their 
name, but there was no explicit indication about 
their intended interpretation. 
Second, it was not clear whether the Basic 
Domains met certain requirements such as 
knowledge coverage and balancing. In fact, the 
Basic Domains are supposed to possess a 
comparable degree of granularity and, at the same 
time, to cover all human knowledge. However, 
they did not always posses such characteristics. For 
instance VETERINARY was put at the same level as 
ECONOMY, although these two domains obviously 
do not posses the same level of granularity. 
Moreover not all branches of human knowledge 
were represented (see for instance the HOME 
domain). 
The purpose of the work presented here was, 
therefore, to find a solution for such problems, in 
order to improve the applicability of WDH in a 
wider range of fields. The solution we propose is 
crucially based on the Dewey Decimal 
Classification (edition 21), which has been used as 
a reference point for defining a clear semantics, 
preventing overlapping among domains, and 
assessing the Basic Domains coverage and 
granularity issues.  
2.1 The Dewey Decimal Classification (DDC)  
The Dewey Decimal Classification (DDC) system 
(Mitchell et al 1996) is the most widely used 
taxonomy for library classification purposes 
providing a logical system for the organization of 
every item of knowledge through well-defined 
subject codes hierarchically organized. The 
semantics of each subject code is determined by a 
numeric code, a short lexical description associated 
to it, and by the hierarchical relations with the 
other subject codes. Another characteristic of the 
DDC is that a handbook is available explaining 
how texts should be classified under subject codes. 
The DDC is not just for organizing book 
collections; it has also been licensed for 
cataloguing internet resources (see for example 
BUBL http://bubl.ac.uk/link/) and it was conceived 
to accommodate the expansion and evolution of 
the body of human knowledge.  
The DDC hierarchy is arranged by disciplines 
(or fields of study), and this entails that a subject 
may appear in more than one discipline, depending 
on the aspect of the topic discussed.  
The DDC hierarchical structure allows a topic to 
be defined as part of the broader topic above it, and 
that determines the meaning of the class and its 
relation to other classes. At the broadest level, 
called Main Classes (or First summary), the DDC 
is composed of ten mutually exclusive main 
classes, which together cover the entire world of 
knowledge. Each main class is sub-divided into ten 
divisions, (the Hundred Divisions, or Second 
Summary) and each division is split into ten 
sections (the Thousand Section, also called Third 
Summary). 
Each category in the DDC is represented by a 
numeric code as the example below shows.  
 
700  Art 
 730  Plastic Arts 
  736 Carving 
   736.2 Precious Stones 
    736.23 Diamonds 
    736.25 Sapphires 
   736.4 Wood 
  738 Ceramic Arts 
  739 Art Metalwork 
 740  Drawing 
 750  Painting 
 
The first digit of the numbers indicates the main 
class, (700 is used for all Arts) the second digit 
indicates the hundred division, (730 corresponds to 
Plastic arts, 740 to Drawing, 750 to Painting) and 
the third digit indicates the section (736 represents 
Carving, 738 Ceramic arts, 739 Art metalwork). 
Moreover, almost all sub-classes are further 
subdivided. A decimal point follows the third digit 
until the degree of specification needed (736.23 
Diamonds, 736.25 Sapphires).  
3 The Revision of the WDH 
The revision of the first version of the WDH aimed 
at satisfying the following properties and 
characteristics:  
 
o semantics: each WDH label should have an 
explicit semantics and should be 
unambiguously identified; 
o disjunction: the interpretation of all WDH 
labels should not overlap; 
o basic coverage: all human knowledge should 
be covered  by the Basic Domains; 
o basic balancing: most Basic Domains should 
have a comparable degree of granularity. 
 
In the following sections we are going to show 
how a systematic mapping between WDH and 
DDC can be used to enforce each of the above 
characteristics.  
3.1 Semantics 
To give the domain labels a clear semantics so that 
they can be unambiguously identified and 
interpreted, we decided to associate each domain 
label to one or more DDC codes as shown below in 
Table 1.  
WDH Domains 
 
DDC Codes 
 
 Art 
 
[700-(790-(791.43,792,793.3), 
          710,720,745.5)] 
       Plastic arts 730 
                   Sculpture [731:735] 
                   Numismatics 737 
       Jewellery 739.27 
       Drawing [740-745.5] 
       Painting 750 
       Graphic arts 760 
                   Philately 769.56 
       Photography 770 
       Music 780 
       Cinema 791.43 
       Theatre [792-792.8] 
       Dance [792.8,793.3] 
Table 1: Fragment of the new WDH with the 
respective DDC codes 
In many cases we found a one-to-one mapping 
between a WDH label and a DDC code (e.g. 
PAINTING mapped onto 750 or CINEMA onto 
791.43). When one-to-one mappings were not 
found, artificial DDC codes were created. An 
artificial code, represented within square brackets, 
is created with reference to various DDC codes or 
parts of them. To describe artificial nodes, certain 
conventions have been adopted.  
(i) A series of non-consecutive codes is listed 
separated by a comma (see DANCE). 
(ii) A series of consecutive codes is indicated by a 
range. For instance, the series [731, 732, 733, 734, 
735] is abbreviated as [731:735] (see SCULPTURE). 
(iii) A part of a tree is represented as the difference 
between a tree and one or more of its subtrees, 
where the tree and the subtrees are identified by 
their roots (see DRAWING). 
(iv) The square brackets should be interpreted as 
meaning ?the generalities? of the composition of 
codes contained in the brackets. So, for instance, 
[731:735] should be interpreted as the generalities 
of the codes going from 731 to 735. In the original 
DDC, generalities are identified by the 0 decimal. 
For instance, the code 700 refers to the generalities 
of the codes from 710 to 790. 
To establish a mapping between labels and codes 
we exploited the names of the DDC categories and 
their description in the DDC manual. This worked 
pretty well in most cases, but there are some 
exceptions. Take for instance the TOURISM domain. 
Apparently tourism does not occur as a category in 
the DDC. On a closer inspection it came out that 
the categories which are most clearly related to 
tourism are 910.202:World travel guides and 
910.4:Accounts of travel. 
Note that a WDH domain can be mapped onto 
codes included in different DDC main classes, i.e. 
disciplines. For example ARTISANSHIP 
(745.5:Handicrafts, 338.642:Small business) maps 
onto categories located partly under 700:Art and 
partly under 300:Social Sciences. The same 
happens with SEXUALITY, a domain that following 
the DDC is studied by many different disciplines, 
e.g. philosophy, medicine, psychology, body care. 
As a consequence of the systematic specification 
of the semantics of the WDH domains, some of 
them have been re-labeled with regard to the 
previous version of the hierarchy. For instance, the 
domain BOTANY has been changed to PLANTS, 
ZOOLOGY to ANIMALS, and ALIMENTATION to FOOD. 
This change of focus from the name of the 
discipline to the name of the object of the 
discipline is not only in compliance with the new 
edition of the DDC, but it also reflects current and 
international usage (see, for example, Google 
categories). In some cases the change of the 
domain name comes along with a change of its 
intended interpretation. For instance, we have 
decided to enlarge the semantics of the domain 
ZOOTECHNICS and to call it ANIMAL HUSBANDRY, a 
more generic domain which was missing in the 
previous hierarchy.  
In most cases the hierarchical relations between 
the WDH domains are the same as the relations 
holding between the corresponding DDC codes: 
MUSIC is more specific than ART in the same way 
as 780:Music is more specific than 700:The Arts. 
To reinforce the hierarchical parallelism between 
the WDH and the DCC, we re-located some 
domains with regard to the previous WDH 
hierarchy. For example, OCCULTISM, which was 
placed under RELIGION in the old hierarchy, has 
been moved under the newly created domain 
PARANORMAL. Also, TOPOGRAPHY, previously placed 
under ASTRONOMY, has now been moved under 
GEOGRAPHY.  
In a few cases however we did not respect the 
hierarchical relations specified by the DDC, as in 
the case of the ARCHITECTURE domain shown in 
Table 2. ARCHITECTURE has been mapped onto 
720:Architecture and TOWN PLANNING onto 
710:Civic & landscape art.  
WDH Domains DDC Codes 
 Architecture  [645,690,710,720] 
 
Town Planning 710 
 
Buildings 690 
 
Furniture 645 
Table 2: A fragment of WDH for ARCHITECTURE 
However, whereas the 710 code is sibling of 720 
in the DDC, TOWN PLANNING is child of 
ARCHITECTURE in WDH. Also, ARCHITECTURE and 
TOWN PLANNING should be under ART according to 
the DDC, but they have been placed under 
APPLIED SCIENCE in WDH. 
3.2 Disjunction 
This property requires that no DDC code is 
associated to more than one WDH label. In only 
one case this requirement has not been met. 
Apparently, the DDC does not distinguish between 
the disciplines of Sociology and Anthropology, 
and reserves the codes that go from 301 to 307 to 
both of them. Although these two disciplines are 
strictly connected, it seems to us that in the current 
practice they are considered as distinct. So the 
WDH contains two distinct domains for 
SOCIOLOGY and ANTHROPOLOGY, which partially 
overlap because they both map onto the same DDC 
codes 301:307. 
3.3 Basic Coverage 
The term basic coverage refers to the ideal 
requirement that all human knowledge be covered 
by the totality of the Basic Domains (i.e. the 
domains composing the second level of WDH). 
Also in this case, we used the DDC as a gold 
standard to measure the coverage of WDH. Given 
the fact that the DDC has been used for more than 
a century to classify books and written documents 
all over the world, we can assume that the DDC 
guarantees a complete representation of all 
branches of knowledge. So the basic coverage has 
been manually checked by verifying that all (or 
almost all) the DDC categories can be assigned to 
at least one Basic Domain.  
From a practical point of view, it would be very 
complicated to check all the thousands of codes 
contained in the DDC. Thus, our check relied on 
two assumptions. First, when the Basic Domains 
are taken as a stand alone set, the semantics of a 
Basic Domain is given by its specific code together 
with the codes of its subdomains. Second, once a 
DDC code is covered by a Basic Domain, 
inductively, all the more specific categories are 
covered as well. These assumptions allowed us to 
actually check only the topmost DDC codes. For 
example, let?s take the 300 main class of the DDC. 
Table 3 below shows that all the sub-codes of the 
300 class are covered by one or more domains.  
In order to improve the overall WDH coverage, 
5 completely new domains have been introduced 
(the first three are Basic): PARANORMAL, HOME, 
HEALTH, FINANCE and GRAPHIC ARTS. 
Codes DDC Categories WDH Domains 
300 ? Social sciences 
? SOCIAL SCIENCE 
? SOCIOLOGY 
? ANTHROPOLOGY 
310 ? General statistics ? SOCIOLOGY 
320 ? Political science ? POLITICS 
330 ? Economics ? ECONOMY 
340 ? Law ? LAW 
350 ? Public administration & military service 
? ADMINISTRATION  
? MILITARY 
360 ? Social problems & 
services 
? SOCIOLOGY 
? ECONOMY 
? SEXUALITY 
370 ? Education ? PEDAGOGY 
380 
? Commerce, 
communication, 
transport 
? COMMERCE  
? TELECOMMUNICATION  
? TRANSPORT 
390 ? Customs, etiquette, folklore 
? FASHION  
? ANTHROPOLOGY 
? SEXUALITY  
Table 3: Coverage of the 300 DDC class 
We can now assume that the domain-coverage of 
the new version of WDH is almost equivalent to 
that of the DDC, thus ensuring the complete 
representation of all branches of knowledge. 
The new WDH allowed us to fix a number of 
synset classifications that were unsatisfactory in 
the previous version of WORDNET DOMAINS. For 
instance, in the first version of WORDNET 
DOMAINS the English/Italian synset {microwave 
oven, microwave}/{forno a microonde, 
microonde} was annotated with the FURNITURE 
domain, while the synset {detergent}/{detersivo} 
was annotated with FACTOTUM (i.e. no specific 
domain) as no better solution was available. The 
new WDH hierarchy allows for a more appropriate 
classification of both synsets within the new HOME 
domain. 
A few DDC codes are not covered by the new 
list of domains either. These are the codes under 
the 000:Generalities class which includes 
disciplines such as 010:Bibliography, 020:Library 
& information sciences, 030:Encyclopedic works, 
080:General collections. This section has been 
specifically created for cataloguing general and 
encyclopedic works and collections. So it is a 
idiosyncratic category which is not based on 
subject but on the genre of texts. 
Another set of codes which remains not covered 
by WDH are those going from 420 to 490 and from 
810 to 890. These DDC codes are devoted to 
specific languages and literatures of different 
countries, for example, 430:Germanic Languages, 
440:Romance Languages, 810:American Literature 
in English, etc. These codes are undoubtedly 
relevant for the classification of books, but are not 
compatible with the rationale of WDH, which is 
meant to be a language-independent resource. 
3.4 Basic Balancing 
The requirement about basic balancing is meant to 
assure that all Basic Domains have a comparable 
degree of granularity. 
Defining a granularity metrics for domains is a 
complex issue, for which only a tentative solution 
is provided here. At a first glance, three aspects 
could be taken into consideration: the number of 
publications about a domain, the number of sub-
codes in the DDC, and the relevance of a domain 
in the social life.  
As a first attempt, balancing could be evaluated 
referring to the number of publications classified 
under each Basic Domain. In fact, data are 
available about the number of texts classified 
under each of the DDC codes. Unfortunately, the 
number of books published under a certain 
category may not be indicative of its social 
relevance: very specialized domains may include a 
high number of publications, which however 
circulate in a restricted circle, with low social 
impact. For example, the number of texts classified 
in the History domain turns out to be more then ten 
times the number of texts catalogued under the 
Computer Science domain. However, if one looks 
at the number of HTML pages available on the 
Internet, or the number of magazines sold in a 
newspaper stand, or the number of terms used in 
everyday life, one cannot maintain that History is 
ten times more relevant than Computer Science. 
Another approach for evaluating the granularity 
of domains could be to take into account the 
number of DDC sub-codes corresponding to each 
Basic Domain. Unfortunately, also this approach 
gives results which are far from being satisfactory. 
The fact that a discipline has many subdivisions 
seems not to be clearly correlated with its 
relevance. For instance in the DDC manual 
(version 21) 105 pages can be put in 
correspondence with the ENGINEERING domain, 
whereas only 26 correspond to SPORT. It should 
also be said that there is no correlation between the 
number of publications and the number of sub-
categories in the DDC. For instance, 
ARCHITECTURE has a great number of publications 
classified under it, but on the contrary, the number 
of sub-categories in the DDC is very limited. 
The third criterion to evaluate the granularity of 
domains is their social relevance, which seems not 
to be captured adequately by the previous two 
criteria. Of course, social relevance is very difficult 
to evaluate. We tentatively took into consideration 
the organization of Internet hierarchies such as the 
Google and Yahoo directories, which seem to be 
closer than the DDC to represent the current social 
relevance of certain domains. See for instance the 
huge number of HTML pages classified in Google 
under the topic Television Programs. Of course 
Internet is only a partial view of the organization 
of human knowledge, so we cannot simply rely on 
the Internet to evaluate the granularity of the 
domains. 
None of the approaches analyzed so far seems to 
fit our needs. Thus we took into consideration a 
fourth criterion, which is based on the DDC as 
well. Instead of counting the number of 
subdivisions under a certain DDC code, we 
measured the depth of the code from the top of the 
hierarchy. For instance we can say that 700:Art has 
depth 1, 780:Music has depth 2, 782:Vocal Music 
has depth 3, and so on. We make the assumption 
that two DDC codes with the same depth have the 
same granularity. For instance we assume that 
782:Vocal Music and 382:Foreign Trade have the 
same granularity (both have depth 3).  
In order to evaluate the granularity of the Basic 
Domains against the DDC, we can compare WDH 
labels and DDC codes with the same depth. Given 
that the Basic Domains have depth 2, we should 
compare them to the so called Hundred Divisions 
(000, 010, 020, 030, ?, 100, 110, 120, etc.). 
Summing up, we will say that the Basic Domains 
are balanced if they can all be mapped onto the 
Hundred Divisions. Also, in the comparison we 
should take into account that the Basic Domains 
are 45, whereas the Hundred Divisions are 100. So, 
we expect that in the average, one Basic Domain 
maps onto two Hundred Divisions with a small 
degree of variance with respect to the average.  
What we have obtained from the analysis of the 
new WDH is the following: out of 45 Basic 
Domains 
 
o 4 domains map onto a Main Class (depth 1) 
o 18 domains are mapped at the Hundred 
Divisions level (depth 2) 
o 6 domains are mapped at different DDC levels, 
with the majority of DDC codes at depth 2 
o 17 domains map onto subdivisions of depth 3 
and 4. 
 
As for the average number of DDC codes 
covered by each Basic Domain, the variance is 
quite high. Certain Basic Domains cover a big 
number of codes from the Hundred Divisions. For 
instance HISTORY, and ART cover 6 codes each. 
Instead, in  most cases, one Basic Domain covers 
only one DDC code (e.g. LAW and 340:Law). 
The evaluation of the granularity of the Basic 
Domains according to the proposed criterion can 
be considered satisfactory even if the results 
diverge somewhat from what expected in principle.  
To explain this partial divergence in the 
granularity of domains, one should take into 
consideration that the DDC has been created 
relying heavily on the academic organization of 
knowledge disciplines. On the other side, in the 
practical WDH reorganization process we tried to 
balance somehow this discipline-oriented 
approach, by taking into account also the social 
relevance of domains. This has been done by 
relying on the organization of Internet directories 
and on our personal intuitions. 
Such an approach led us to put at the Basic level 
WDH labels corresponding to DDC codes with 
depth higher than 2 (more specific than the 
Hundreds Divisions). See for instance the 
positioning of RADIO+TV, FOOD, HEALTH, and 
ENVIRONMENT at the Basic level, even if they 
correspond to DDC codes of level 3 and 4.  
Instead, ANIMALS and PLANTS were not Basic in 
the previous version of WDH, but have been 
promoted to the Basic level in accordance with the 
granularity level they have in the DDC.  
Other domain labels have been placed at a lower 
level then expected with reference to the DDC. For 
instance PHILOSOPHY, ART, RELIGION, and 
LITERATURE have been put at the Basic Level, 
even if they correspond to DDC codes belonging to 
the Main Classes (depth 1). On the other side 
ASTROLOGY, ARCHAEOLOGY,  BODY CARE, and  
VETERINARY which were Basic in the previous 
version of the WDH, have been demoted at a lower 
level in accordance with the granularity they have 
in the DDC. Only in one case this process of 
demotion has led to the elimination of a sub-
domain, that is TEXTILE.  
4 Conclusions 
In this paper we described the revision of the 
WORDNET DOMAINS Hierarchy (WDH), with the 
aim of providing it with a clear semantics, and 
evaluating the coverage and balancing of a subset 
of the WDH, called Basic Domains. This has been 
done mostly by relying on the information 
available in the Dewy Decimal Classification 
(DDC). A semantics has been provided to the 
WDH labels by defining one or more pointers to 
DDC codes. The coverage of the Basic Domains 
has been evaluated by checking that each DDC 
code is covered by at least one Basic Domain. 
Finally, balancing has been evaluated mostly by 
comparing the granularity of the Basic Domains 
with the granularity of a subset of the DDC called 
the Hundred Divisions. Balancing is the aspect of 
the Basic Domains which diverges more clearly 
from the DDC. This is explained by the fact that 
we took in higher consideration the social 
relevance of domains. 
We think that the new version of the WDH is 
better suited to act as a useful language-
independent resource in the fields of computational 
lexicography, corpus building, and various NLP 
applications.  
5 Acknowledgements 
Thanks to Alfio Gliozzo for his useful comments 
and suggestions about how to improve the 
WORDNET DOMAINS Hierarchy. 
References  
BALKANET http://www.ceid.upatras.gr/Balkanet/ 
L. Bentivogli, C. Girardi and E. Pianta. 2003. The 
MEANING Italian Corpus. In Proceedings of the 
Corpus Linguistics 2003 Conference. Lancaster, 
United Kingdom. 
C. Fellbaum. 1998. WordNet. An Electronic 
Lexical Database. The MIT Press, Boston. 
B. Magnini and G. Cavagli?. 2000. Integrating 
Subject Field Codes into WordNet. In 
Proceedings of LREC-2000. Athens, Greece. 
B. Magnini, C. Strapparava, G. Pezzulo and A. 
Gliozzo. 2002. The Role of Domain Information 
in Word Sense Disambiguation. Journal of 
Natural Language Engineering (Special Issue on 
evaluating Word Sense Disambiguation 
Systems), 9(1):359:373. 
J.S. Mitchell, J. Beall, W.E. Matthews and G.R. 
New (eds). 1996. Dewey Decimal Classification  
Edition 21 (DDC 21). Forest Press, Albany, New 
York. 
E. Pianta, L. Bentivogli and C. Girardi. 2002. 
MultiWordNet: developing an aligned 
multilingual database. In Proceedings of the 
First Global WordNet Conference. Mysore, 
India. 
G. Rigau, B. Magnini, E. Agirre, P. Vossen and J. 
Carrol. 2002. MEANING: a Roadmap to 
Knowledge Technologies. In Proceedings of the 
COLING-2002 workshop "A Roadmap for 
Computational Linguistics". Taipei, Taiwan. 
H. Schutze. 1998. Automatic Word Sense 
Discrimination. Computational Linguistics, 
24(1):97-123. 
SIMPLE. 2000. Linguistic Specifications. 
Deliverable D2.1, March 2000.  
P. Vossen (ed). 1998. Computers and the 
Humanities (Special Issue on EuroWordNet), 
32(2-3). 
D.E. Walker and R.A. Amsler. 1986. Analyzing 
Language in Restricted Domain. Sublanguage 
description and Processing. Lawrence Earlbaum, 
Hillsdale NJ.  
Appendix : The first two levels of the WDH new version with the corresponding DDC codes 
 
TOP-LEVEL BASIC DOMAINS DDC 
Humanities   
 History [920:990] 
 Linguistics 410 
 Literature [800, 400] 
 Philosophy [100-(130, 150, 176)] 
 Psychology 150 
 Art [700-(710, 720, 745.5, 790-(791.43, 792, 793.3))] 
 Paranormal 130 
 Religion 200 
   
Free_Time  [790-(791.43, 792, 793.3)] 
 Radio-Tv [791.44, 791.45] 
 Play [793.4:795-794.6] 
 Sport [794.6, 796:799] 
   
Applied_Science  600 
 Agriculture [338.1, 630] 
 Food [613.2, 613.3, 641, 642] 
 Home [640-(641, 642, 645)] 
 Architecture [645, 690, 710, 720] 
 Computer_Science [004:006] 
 Engineering 620 
 Telecommunication [383, 384] 
 Medicine [610-(611, 612, 613)] 
   
Pure_Science  500 
 Astronomy  520 
 Biology [570-577, 611, 612-612.6] 
 Animals  590 
 Plants 580 
 Environment  577 
 Chemistry  540 
 Earth  [550, 560, 910-(910.4, 910.202)] 
 Mathematics 510 
 Physics  530 
   
Social_Science  [300.1:300.9] 
 Anthropology [301:307, 395, 398] 
 Health [613-(613.2, 613.3, 613.8, 613.9)] 
 Military [355:359] 
 Pedagogy 370 
 Publishing 070 
 Sociology [301:319-(305.8, 306.7), 360-(363.4, 368)] 
 Artisanship [338.642, 745.5] 
 Commerce [381, 382] 
 Industry [338-(338.1, 338.642), 660, 670, 680] 
 Transport [385:389] 
 Economy [330-(334, 338), 368, 650] 
 Administration [351:354] 
 Law 340 
 Politics 320 
 Tourism [910.202, 910.4] 
 Fashion [390-(392.6, 395, 398), 687] 
 Sexuality [155.3, 176, 306.7, 363.4, 392.6, 612.6, 613.96] 
   
 Factotum  
 
 
Representing and Accessing Multilevel Linguistic Annotation using 
the MEANING Format  
 
 
Emanuele Pianta 
ITC-irst 
38050, Povo 
Trento, Italy 
pianta@itc.it 
Luisa Bentivogli 
ITC-irst 
38050, Povo 
Trento, Italy 
bentivo@itc.it 
Christian Girardi 
ITC-irst 
38050, Povo 
Trento, Italy 
cgirardi@itc.it 
Bernardo Magnini 
ITC-irst 
38050, Povo 
Trento, Italy 
magnini@itc.it 
 
  
 
Abstract 
We present an XML annotation format 
(MEANING Annotation Format, MAF) 
specifically designed to represent and in-
tegrate different levels of linguistic anno-
tations and a tool that provides flexible 
access to them (MEANING Browser). 
We describe our experience in integrating 
linguistic annotations coming from dif-
ferent sources, and the solutions we 
adopted to implement efficient access to 
corpora annotated with the Meaning 
Format. 
1 Introduction 
It is well known that when using XML-based 
annotation schemes to represent multi layer an-
notations, it can be difficult to handle partially 
overlapping annotations. Annotating discontinu-
ous elements may be considered as a variant of 
the same problem (Pianta and Bentivogli, 2004). 
Other difficulties can arise from the necessity of 
integrating manual and automatic annotations, as 
we will show in this paper. 
One of the most effective solutions to the 
above mentioned problems is the so called stand-
off annotation, based on the separation between 
textual data and annotations, and between vari-
ous types of annotation, possibly pointing to 
same text. This approach has been systematically 
adopted in the design of MAF, a multilayer XML 
format developed for the EU-funded MEANING 
project, in the context of the creation of the Ital-
ian MEANING Corpus (Bentivogli et al, 2003).  
In this paper we will describe our experience 
in the use of MAF, with special emphasis on how 
we solved issues related to representing annota-
tion levels which come from different sources, 
and can possibly overlap. We will also give de-
tails about the solutions we adopted to allow for 
efficient access and human browsing of MAF 
standoff annotations. 
The rest of the paper is organized as follows. 
Section 2 describes MAF and the types of anno-
tations which have been represented with it. Sec-
tion 3 reports on the integration into MAF of lin-
guistic annotations coming from different 
sources. Section 4 illustrates the strategies 
adopted to make the information encoded in 
MAF quickly accessible. Finally, Section 5 pre-
sents the MEANING Browser, a tool for access-
ing and navigating corpora linguistically anno-
tated with MAF. 
2 The MEANING Format 
Following the proposals for the ISO/TC 37/SC 4 
standard for linguistic resources (Ide and 
Romary, 2002), the MAF scheme is based on 
annotation structures and data categories. Each 
type of annotation structure (nestable <struct> 
elements) corresponds to a specific kind of lin-
guistic object (e.g. tokens, lexical units, multi-
words), and each instance of a linguistic object is 
identified by a unique identifier. Data categories 
(<feat> tags) represent attributes of the linguistic 
objects. Different representation levels are con-
tained in separate documents, or document sec-
tions. The XLink and XPointer syntax is used to 
represent relations between elements in different 
XML documents, and IDREFs attributes for rela-
tions within the same document. 
2.1 First  version 
The first version of the MEANING Format has 
been used to represent seven kinds of informa-
tion: orthographic features, the structure of the 
77
text, morphosyntactic information, multiwords, 
syntactic information, named entities, and word 
senses. 
Annotation levels are related to each other fol-
lowing a hierarchy of annotation levels, which 
reflects a theoretically grounded hierarchy of 
linguistic objects. The basic (orthographic) anno-
tation level, representing tokens, is implemented 
with pointers to the character positions in the hub 
corpus. Then the morphosyntactic level, repre-
senting word-related morphological information, 
contains pointers to the tokens, whereas the mul-
tiword level points to the words described at 
morphosyntactic level. 
The following example shows how the mor-
phosyntactic features of the Italian word ?an-
dare? (to go) are represented. 
 
<struc   type="w-level" id="w_12" 
             xlink:href="#xpointer(id('t_10'))"> 
     <feat type="lemma">andare</feat> 
     <feat type="stem">and</feat> 
     <feat type="pos">v</feat> 
     <feat type="elra-tag">VF</feat> 
     <feat type="mood">inf</feat> 
     <feat type="tense">pres</feat> 
 </struc> 
 
MAF also specifically addresses the problem  
of discontinuous units, such as for instance non-
contiguous multiwords; see ?andarci veramente 
piano? (take it really easy). A detailed study of 
how standoff annotation allows for an elegant 
treatment of this phenomenon can be found in 
(Pianta and Bentivogli 2004). 
2.2 Second version 
The first version of the MEANING Format has 
recently been extended within the FU-PAT ON-
TOTEXT project (Magnini et al 2005). 
Within this project, we are creating the Italian 
Content Annotation Bank (I-CAB), a corpus of 
Italian news stories annotated with different 
kinds of semantic information. Annotation is be-
ing carried out manually, as we intend I-CAB to 
become a benchmark for automatic Information 
Extraction and Ontology Population tasks, in-
cluding recognition and normalization of various 
types of entities, temporal expressions, relations 
between entities, and relations between entities 
and temporal expressions (e.g. the relation date-
of-birth connecting a person to a date). 
To fulfill I-CAB annotation needs, we ex-
tended MAF, by adding a number of new lin-
guistic annotation levels, i.e.: 
 
? temporal expressions  
? entities of type person and organization 
? mentions (i.e. the textual expressions re-
ferring to the entities)  
 
According to the hierarchical approach to rep-
resenting relations between annotation levels in 
the first version of the MEANING Format, tem-
poral expressions and entity mentions are repre-
sented with pointers to morphosyntactic level 
entities. Entities, instead, are represented with 
pointers to entity mentions.  
To manually annotate temporal expressions 
we followed the TIMEX2 markup standard, 
while to mark entities and mentions we relied on 
the ACE entity detection task guidelines. To per-
form the annotation task we used Callisto 
(http://callisto.mitre.org). 
3 Converting linguistic annotations into 
MAF  
The manual annotations produced through Cal-
listo, which is related to novel annotation levels 
such  as temporal expressions and entity men-
tions, had to be integrated with more traditional 
annotations which are performed automatically 
with the TextPro tool, an automatic linguistic 
analysis Tool Suite developed at ITC-irst. 
News
MEANING
Annotation 
Format
Callisto
Lucene
AIF format
Manual 
annotation
Automatic 
annotation
TextPro
TextPro format
Indexing
Data Base
Conversion
MEANING
Browser
I-CAB 
Corpus
 
As one can see in the above figure, two different 
annotation processes (automatic and manual) 
produce two different formats which must be 
converted and integrated into MAF in order to be 
accessed by the MEANING Browser (or any 
other NLP tool). 
3.1 From TextPro format to MEANING 
Format 
TextPro takes a raw text as input and carries out 
basic processing tasks such as tokenization, mor-
78
phological analysis, PoS tagging, lemmatization, 
and multiword recognition. The results of 
TextPro analyses are represented in a table, 
where each token is on a row, and columns con-
tain multiple annotation levels. Converting from 
the TextPro to the MEANING Format requires 
retrieving the character positions of tokens in the 
hub corpus, which are not directly available in 
the TextPro output. 
3.2 From AIF format to MEANING format 
The Callisto manual annotation tool produces a 
coding format called AIF (Atlas Interchange For-
mat), which implements a stand-off XML anno-
tation scheme. 
When using the Callisto graphical interface, 
all annotations of temporal expressions and en-
tity mentions are carried out by selecting a se-
quence of contiguous characters. As a conse-
quence, all AIF annotations make reference to 
character positions. 
However, from Section 2.2 we know that in 
MAF temporal expressions and entity mentions 
make reference to morphosyntactic linguistic 
objects, not characters. This implies that, to go 
from AIF to the MEANING Format, we need to 
translate annotations making reference to the po-
sition of characters into annotations that point to 
morphological entities. More precisely, we need 
to substitute pointers to character positions with 
pointers to morphosyntactic objects which have 
been marked automatically by TextPro. Carrying 
out this step will also achieve the integration of 
manual and automatic annotations.  
The integration step is possible because the 
MAF hierarchy of annotation levels points, at the 
lowest level, to character positions. By following 
the hierarchy of links relating the various annota-
tion levels it is always possible to trace back a 
linguistic object to some sequence of characters 
in the raw text, and in the opposite direction, 
given a string, we know what linguistic objects 
correspond to it. Summing up, the integration of 
AIF annotations into MAF requires that, given 
the character positions contained in the AIF an-
notation of some string, we substitute the point-
ers to characters with the pointers to the linguis-
tic objects that cover the same string. 
 
4 Data Access 
MAF turned out to be a flexible and expressive 
means to represent and integrate multiple levels 
of linguistic annotation. This was achieved 
mainly thanks to the adoption of the standoff 
annotation approach.  However accessing and 
retrieving information spread in possibly very 
large repositories (hundreds of thousands) of 
XML files may be a challenging task even for 
Database Management Systems specifically de-
signed to handle XML. To solve this problem we 
first analyzed existing native XML databases 
such as eXist, and Apache Xindice, but found 
that what was available at the time did not suited 
our needs. For this reason we approached the 
access problem through a two-fold strategy: 
? converting  XML data into a relational 
database 
? indexing XML data and accessing them 
through a  search engine (LUCENE) 
The conversion of MAF data into a relational 
database is based on the following strategy. Each 
annotation level is mapped into a table, where 
rows represent instances of the relevant linguistic 
object (e.g. words), and columns represent its 
attributes (e.g. lemma, PoS, etc). Specific col-
umns contain the object identifiers and the point-
ers to objects of other types/tables. 
Once MAF data are stored in a relational data-
base, they can be accessed quite efficiently. 
However, when the access to data requires joins 
of many tables, access times become incompati-
ble with various kinds of applications, such as 
on-line corpus browsing. For this reason we tried 
to complement the use of a relational database 
with the exploitation of the indexing capability 
of the LUCENE search engine 
(http://lucene.apache.org/). To this extent we 
modified the LUCENE analyzer so as to be able 
to parse XML structures. In this way LUCENE 
can be configured in order to index any XML 
structure. 
The fast access capabilities of a relational da-
tabase combined with the extended indexing ca-
pabilities of LUCENE enabled us to implement a 
browser of MAF annotated corpora.  
5 The MEANING Browser 
The MEANING Browser can be used by humans 
to navigate any corpus encoded with MAF. The 
browser is built upon an API which can be used 
by any automatic system.  
In the following, we are going to demonstrate 
how I-CAB texts and their annotations can be 
accessed through the MEANING Browser. 
The first kind of access to the corpus is word-
oriented, and amounts to a concordancer, i.e. a 
79
tool able to provide all the occurrences of a cer-
tain word in the corpus. The user can alterna-
tively search for all occurrences of a word form, 
or a lemma, possibly constraining the search to a 
certain PoS. Free combinations between these 
constraints are allowed. The system will return a 
KWIC-like concordance of all the tokens in the 
corpus that match the request, within a chosen 
word window. By clicking on the magnifying 
glass, one can see the sentence in which the 
searched word occurs (see Appendix 1). 
By clicking on a specific icon a new window 
is opened where the whole text is displayed and 
its linguistic annotations are made accessible. A 
number of graphical widgets allow the user to 
highlight the desired annotations: e.g. nouns, 
verbs, multiwords, temporal expressions, men-
tions of a specific entity. 
In Appendix 2 the browser is used to show 
both nouns (automatically annotated) and entity 
mentions (from manual annotation). Appendix 3 
shows time expressions and discontinuous mul-
tiwords; see how the multiword ?ha rassegnato 
? le dimissioni? (he resigned) is made discon-
tinuous by the occurrence of a time expression 
ieri (yesterday). The browser will also give mor-
phosyntactic information about single words 
composing multiwords (governo, government). 
From the same window one can access the XML 
files encoding multiple annotation levels for the 
same document. 
References  
Bentivogli, L., Girardi, C., Pianta, E. 2003. The ME-
ANING Italian Corpus. In Proceedings of the Cor-
pus Linguistics 2003 conference, Lancaster, UK. 
Ide, N. & Romary, L. 2002. Standards for Language 
Resources. In Proceedings of LREC 2002, Las 
Palmas, Canary Islands, Spain.  
Magnini, B., Negri, M., Pianta, E., Romano, L., Sper-
anza, M., Serafini, L., Girardi, C., Bartalesi, V., 
Sprugnoli, R. 2005. From Text to Knowledge for 
the Semantic Web: the ONTOTEXT Project. In 
Proceedings of  SWAP 2005 Workshop, Trento, I-
taly. 
Pianta, E. and Bentivogli, L. 2004. Annotating Dis-
continuous Structures in XML: the Multiword 
Case. In Proceedings of the LREC 2004 Satellite 
Workshop on "XML-based richly annotated cor-
pora", Lisbon, Portugal. 
 
 
 
Appendix 1   
Kwic Concordancer 
 
 
Appendix 2 
Browsing nouns (in grey, automatic annotation)  and 
entity mentions (Tony Blair, manual annotation) 
 
 
Appendix 3  
Browsing discontinuous multiwords (ha rassegnato ? 
le dimissioni, he resigned), time expressions (ieri, 
yesterday) and word information (governo) 
 
80
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 120?123, Dublin, Ireland, August 23-29 2014.
MT-EQuAl: a Toolkit for Human Assessment of Machine Translation
Output
Christian Girardi
(1)
Luisa Bentivogli
(1)
Mohammad Amin Farajian
(1,2)
Marcello Federico
(1)
(1)
FBK - Fondazione Bruno Kessler, Trento, Italy
(2)
University of Trento, Italy
{cgirardi,bentivo,farajian,federico}@fbk.eu
Abstract
MT-EQuAl (Machine Translation Errors, Quality, Alignment) is a toolkit for human assessment
of Machine Translation (MT) output. MT-EQuAl implements three different tasks in an inte-
grated environment: annotation of translation errors, translation quality rating (e.g. adequacy
and fluency, relative ranking of alternative translations), and word alignment. The toolkit is web-
based and multi-user, allowing large scale and remotely managed manual annotation projects. It
incorporates a number of project management functions and sophisticated progress monitoring
capabilities. The implemented evaluation tasks are configurable and can be adapted to several
specific annotation needs. The toolkit is open source and released under Apache 2.0 license.
1 Introduction
It is widely recognized within the MT field that human evaluation can play a crucial role in improv-
ing MT technology. Despite the well-known difficulties in collecting human annotations (the process is
time-consuming, costly and often subjective), state of the art MT research is now moving towards inte-
grating as much as possible human quality assessment into the MT workflow. The most commonly used
human evaluation methodologies are based on absolute adequacy and fluency scores, relative ranking of
alternative MT outputs, and, more recently, human post-editing. Although very useful, these methods do
not provide information about the specific problems of MT systems. To address this limitation, new ap-
proaches based on human error analysis have emerged, where annotators identify and classify translation
errors thus giving precise indications about specific deficiencies of the evaluated MT systems. Given the
outlined trend, it is of the utmost importance to make available to the MT community tools (i) able to
support large-scale annotation projects involving a great variety of languages, (ii) addressing the most
required MT assessment tasks, and (iii) designed in a way to reduce as much as possible the problems re-
lated to manual annotation. MT-EQuAl is a toolkit for the manual assessment of MT output, created with
the aim of addressing the above requirements. The main characteristics of MT-EQuAl are the following:
? Web-based and multi-user: allows large-scale and remotely managed annotation projects. It incor-
porates project management functions and sophisticated progress monitoring capabilities.
? Three different MT assessment tasks in an integrated environment: annotation of translation errors,
translation quality rating (e.g. adequacy, fluency, relative ranking), and word alignment. An inte-
grated environment offering different tasks can address the needs of a higher number of potential
users within the MT field.
? Highly configurable tasks: possibility to evaluate a single MT output as well as two or more au-
tomatic translations in parallel, which is useful if the purpose of the annotation is to compare MT
systems. Furthermore, all tasks can be adapted to specific annotation needs (see Section 2).
? Fast and well-designed annotation interfaces: particular attention was paid to the usability of the
interfaces, especially for the error annotation task where a lot of annotations, often overlapping and
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
120
covering long sequences of words, have to be made. A fast and easy-to-use interface can reduce the
problems related to manual evaluation and ensure annotation speed and data quality.
? Open source: released under Apache 2.0 license at http://github.com/hltfbk/mt-equal .
We think that these features give to MT-EQuAl an added value with respect to other existing annotation
tools which only partially fulfill the requirements illustrated above.
Over the years, various annotation tools with different characteristics have been made available for
the assessment tasks offered by our toolkit. However, none of them incorporates all the features of MT-
EQuAl: either the integration in a multi-task platform, or a web-based interface, or the implementation
of the error annotation task which is the most needed to support the upcoming research. The most com-
parable tools to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis,
2013), TAUS DQF framework,
1
translate5,
2
Blast (Stymne, 2011), and Appraise (Federmann, 2012),
since they all implement translation error annotation. These tools were created for different purposes and
differ in various ways among each other and with respect to MT-EQuAl. All of them except Appraise
do not support multiple MT outputs, and PET, COSTA, and Blast are stand-alone tools. From the error
analysis point of view, their interfaces show different levels of flexibility. PET and COSTA permit only
sentence-level annotation, which is not the suitable granularity for that kind of information. Appraise
offers word-level annotation but displays the MT output word by word, which does not facilitate the
annotator in getting a global view of the sentence and of the errors. Finally, the translate5 and Blast in-
terfaces show the whole MT output and allow the annotator to mark the specific portion(s) of text where
an error occurs. This type of annotation is the same implemented in MT-EQuAl. However, with respect
to these tools, MT-EQuAl represents a step further as one of our main design goals was usability. The
MT-EQuAl error analysis interface is simple and very intuitive, and offers visualization functions aimed
at reducing annotators? cognitive load, so to enable them to focus on the task itself (see Section 2.2).
2 System Overview
MT-EQuAl is a web-based application implemented using PHP and JavaScript. It takes as input several
UTF-8 encoded csv files: the source text, the reference translation (optional), and one file for each of the
MT outputs to be evaluated. This allows the evaluation of single systems as well as the comparison of
multiple systems. Each row in the input csv files contains one evaluation item, typically one sentence.
In order to annotate translation errors, the sentences must be tokenized. To this purpose, the tool ac-
cepts input files already tokenized by the user or - if needed - it applies a simple tokenization based on
spaces, punctuation, and other language-dependent rules (e.g. a character-based tokenization is applied
to Chinese texts). The source and target languages must be declared in the csv, so that the tool can ap-
ply the most suitable text tokenization and visualisation (e.g. the text can be displayed left to right and
viceversa). The annotated data can be exported both in csv and XML format.
As regards data storage, all recorded information is permanently stored in a MySQL database. An
interesting feature is that immediate persistence of data is achieved without an explicit action by the
user to save the data, since every annotation is immediately sent to the server and stored in the database.
Finally - being a web-based application - if the server encounters some problems, annotation is blocked
and the user is notified with a warning message.
The MT-EQuAl front-end is composed of a project management interface and three annotation inter-
faces, one for each evaluation task.
2.1 Project Management Interface
The various project management functions implemented in the tool are accessible to the project manager
through an interface which is composed of four tabs:
? Task. In this tab the project manager creates the task and sets its specific features. For the error
analysis task, a default error typology - based on (Vilar et al., 2006) - is available, but any alternative
1
https://evaluation.taus.net/tools
2
http://www.translate5.net
121
tagset can be adopted. In the rating task it is possible to decide the number of points in the rating
scale, while in the alignment task the number of alignment types (e.g. sure, possible) can be set.
? Data. In this tab the project manager can import the input files and apply the tokenization module
if desired. Moreover, a table summarizing the data stored in the database for each task is displayed.
? Users. In this tab the project manager can create accounts for users and assign them to different
tasks. Each user will see only the task(s) s/he has been assigned to. Users do not see other users?
annotations unless they are working in ?revision mode?, where an existing annotation is presented
for revision.
? Annotation. This tab contains the progress monitoring and export functions. As regards progress
monitoring, a report containing real-time information about the progress of the annotation is dis-
played both at the task level and the user level. Moreover, the project manager can monitor user
activity through the visualization of the remote client interface in read-only mode. This feature is
particularly useful as it addresses the typical problems related to training and supervision of remote
annotators. Regarding annotation export, data can be exported (i) for all the tasks, (ii) for each
single task, and (iii) for each user. Furthermore, the annotations carried out by a user can be directly
copied into another user account for revision.
2.2 Error Annotation Interface
The error annotation interface requires the annotator to identify the type of errors present in the MT out-
put, according to the adopted error typology, and to mark their position in the text. As shown in Figure
1, the annotator is presented with the source sentence, a reference translation (optional) and the MT out-
put(s) to be analyzed. Two buttons allow the annotator to mark the MT output as containing ?no errors?
or ?too many errors?. In order to annotate the errors, the annotator selects with the mouse the word(s)
to be annotated. The selected word(s) are highlighted and, by right-clicking, the error typology menu is
displayed and the suitable error type can be chosen. It is possible to annotate single words (including
punctuation), spaces (e.g. to indicate the correct place for missing words in the candidate translation),
and sequences of words (very useful especially for reordering problems which can involve entire portions
of the sentence). The annotated errors are listed at the right of the corresponding sentences, subdivided
by error type. If the mouse hovers over a given error instance, the corresponding word(s) appear under-
lined in the text. It is possible to delete single error instances (by clicking on the bin icon) or all the errors
of a give type (by clicking on the ?reset? button).
Figure 1: Error annotation interface configured for two MT outputs and with the default error typology.
2.3 Translation Quality Rating Interface
As shown in Figure 2(a), the quality rating interface displays the source sentence, a reference trans-
lation (optional) and the MT output(s) to be rated. When the assessor clicks on a point in the scale,
the annotation is automatically saved and the point is highlighted in red. By clicking on the button
?Done?, the assessor confirms that the evaluation item has been completed. This layout is suitable for
adequacy/fluency evaluation, ranking outputs relatively to each other, and in general all those assessment
tasks that require rating MT outputs.
122
(a) Quality Rating (b) Word Alignment
Figure 2: (a) Quality Rating interface with 3 systems and a 5-point scale (b) Word Alignment interface.
2.4 Word Alignment Annotation Interface
The word alignment interface displays a traditional alignment matrix, where the rows correspond to the
words of the sentence in one language and the columns to the words of its translation. Word alignments
can be edited by clicking the respective matrix cells to add or remove links between words. The interface
is designed to allow the alignment of discontinuous text segments. Figure 2(b) shows an alignment
example where light grey, dark grey, and black cells respectively represent unlinked words, possible and
sure alignments.
3 Applications of MT-EQuAl and Forthcoming Extensions
MT-EQuAl is currently being used by professional translators on English to Italian data to assess the
performance of the alignment models and annotate translation errors of the MT systems developed within
the MateCat project.
3
MT-EQuAl was also extensively used within an industrial project for the evaluation
of commercial MT systems. To this purpose, professional translators performed error annotation and
quality rating on data for three different language pairs (English to Arabic/Chinese/Russian). MT-EQuAl
is being actively developed on the basis of the feedback and requirements collected from its users. We
are also currently implementing the automatic computation of Inter-Annotator Agreement scores, as an
additional feature to further improve the toolkit.
Acknowledgments
This work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).
References
Wilker Aziz, Sheila Castilho Monteiro de Sousa, and Lucia Specia. 2012. PET: a tool for post-editing and
assessing machine translation. In Proceedings of the Eight International Conference on Language Resources
and Evaluation (LREC?12), Istanbul, Turkey, May. European Language Resources Association (ELRA).
Konstantinos Chatzitheodorou and Stamatis Chatzistamatis. 2013. COSTA MT Evaluation Tool: An Open Toolkit
for Human Machine Translation Evaluation. Prague Bulletin of Mathematical Linguistics, pages 83?89.
Christian Federmann. 2012. Appraise: an open-source toolkit for manual evaluation of mt output. Prague Bulletin
of Mathematical Linguistics, pages 25?35.
Sara Stymne. 2011. Blast: A tool for error analysis of machine translation output. In Proceedings of the ACL-HLT
2011 System Demonstrations, pages 56?61, Portland, Oregon, June. Association for Computational Linguistics.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney. 2006. Error Analysis of Machine Translation Out-
put. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC?06),
pages 697?702, Genoa, Italy, May. European Language Resources Association (ELRA).
3
www.matecat.com
123
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Divide and Conquer:
Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Yashar Mehdad
FBK-irst and University of Trento
Trento, Italy
mehdad@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Abstract
We address the creation of cross-lingual tex-
tual entailment corpora by means of crowd-
sourcing. Our goal is to define a cheap and
replicable data collection methodology that
minimizes the manual work done by expert
annotators, without resorting to preprocess-
ing tools or already annotated monolingual
datasets. In line with recent works empha-
sizing the need of large-scale annotation ef-
forts for textual entailment, our work aims to:
i) tackle the scarcity of data available to train
and evaluate systems, and ii) promote the re-
course to crowdsourcing as an effective way
to reduce the costs of data collection without
sacrificing quality. We show that a complex
data creation task, for which even experts usu-
ally feature low agreement scores, can be ef-
fectively decomposed into simple subtasks as-
signed to non-expert annotators. The resulting
dataset, obtained from a pipeline of different
jobs routed to Amazon Mechanical Turk, con-
tains more than 1,600 aligned pairs for each
combination of texts-hypotheses in English,
Italian and German.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of Textual Entailment
(Dagan and Glickman, 2004). The task consists of
deciding, given a text (T) and an hypothesis (H) in
different languages, if the meaning of H can be in-
ferred from the meaning of T. As in other NLP appli-
cations, both for monolingual and cross-lingual TE,
the availability of large quantities of annotated data
is an enabling factor for systems development and
evaluation. Until now, however, the scarcity of such
data on the one hand, and the costs of creating new
datasets of reasonable size on the other, have repre-
sented a bottleneck for a steady advancement of the
state of the art.
In the last few years, monolingual TE corpora for
English and other European languages have been
created and distributed in the framework of sev-
eral evaluation campaigns, including the RTE Chal-
lenge1, the Answer Validation Exercise at CLEF2,
and the Textual Entailment task at EVALITA3. De-
spite the differences in the design of the tasks, all
the released datasets were collected through simi-
lar procedures, always involving expensive manual
work done by expert annotators. Moreover, in the
data creation process, large amounts of hand-crafted
T-H pairs often have to be discarded in order to re-
tain only those featuring full agreement, in terms of
the assigned entailment judgements, among multiple
annotators. The amount of discarded pairs is usually
high, contributing to increase the costs of creating
textual entailment datasets4.
The issues related to the shortage of datasets and
the high costs for their creation are more evident
1http://www.nist.gov/tac/2011/RTE/
2http://nlp.uned.es/clef-qa/ave/
3http://www.evalita.it/2009/tasks/te
4For instance, in the first five RTE Challenges, the aver-
age effort needed to create 1,000 pairs featuring full agreement
among 3 annotators was around 2.5 person-months. Typically,
around 25% of the original pairs had to be discarded during the
process, due to low inter-annotator agreement (Bentivogli et al,
2009).
670
in the CLTE scenario, where: i) the only dataset
currently available is an English-Spanish corpus ob-
tained by translating the RTE-3 corpus (Negri and
Mehdad, 2010), and ii) the application of the stan-
dard methods adopted to build RTE pairs requires
proficiency in multiple languages, thus significantly
increasing the costs of the data creation process.
To address these issues, in this paper we devise
a cost-effective methodology to create cross-lingual
textual entailment corpora. In particular, we focus
on the following problems:
(1) Is it possible to collect T-H pairs minimizing
the intervention of expert annotators? To address
this question, we explore the feasibility of crowd-
sourcing the corpus creation process. As a contri-
bution beyond the few works on TE/CLTE data ac-
quisition, we define an effective methodology that:
i) does not involve experts in the most complex (and
costly) stages of the process, ii) does not require pre-
processing tools, and iii) does not rely on the avail-
ability of already annotated RTE corpora.
(2) How can we guarantee good quality of the col-
lected data at a low cost? We address the quality
control issue through the decomposition of a com-
plex task (i.e. creating and annotating entailment
pairs) into smaller sub-tasks. Complex tasks are usu-
ally hard to explain in a simple way understandable
to non-experts, difficult to accomplish, and not suit-
able for the application of the quality-check mecha-
nisms provided by current crowdsourcing services.
Our ?divide and conquer? solution represents the
first attempt to address a complex task involving
content generation and labelling through the defini-
tion of a cheap and reliable pipeline of simple tasks
which are easy to define, accomplish, and control.
(3) Can we adapt such methodology to collect
cross-lingual T-H pairs? We tackle this question
by separating the problem of creating and annotating
TE pairs from the issues related to the multilingual
dimension. Our solution builds on the assumption
that entailment annotations can be projected across
aligned T-H pairs in different languages. In this
case, a complex multilingual task is reduced to a se-
quence of simpler subtasks where the most difficult
one, the generation of entailment pairs, is entirely
monolingual. Besides ensuring cost-effectiveness,
our solution allows us to overcome the problem of
finding workers that are proficient in multiple lan-
guages. Moreover, since the core monolingual tasks
of the process are carried out by manipulating En-
glish texts, we are able to address the very large
community of English speaking workers, with a
considerable reduction of costs and execution time.
Finally, as a by-product of our method, the acquired
pairs are fully aligned for all language combinations,
thus enabling meaningful comparisons between sce-
narios of different complexity (monolingual TE, and
CLTE between close or distant languages).
We believe that, in the same spirit of recent works
promoting large-scale annotation efforts around en-
tailment corpora (Sammons et al, 2010; Bentivogli
et al, 2010), the proposed approach and the resulting
dataset5 will contribute to meeting the strong need
for resources to develop and evaluate novel solutions
for textual entailment.
2 Related Works
Crowdsourcing services, such as Amazon Mechan-
ical Turk6 (MTurk) and CrowdFlower7, have been
recently used with success for a variety of NLP ap-
plications (Callison-Burch and Dredze, 2010). The
idea is that the acquisition and annotation of large
amounts of data needed to train and evaluate NLP
tools can be carried out in a cost-effective manner
by defining simple Human Intelligence Tasks (HITs)
routed to a crowd of non-expert workers (aka ?Turk-
ers?) hired through on-line marketplaces.
As regards textual entailment, the first work ex-
ploring the use of crowdsourcing services for data
annotation is described in (Snow et al, 2008), which
shows high agreement between non-expert annota-
tions of the RTE-1 dataset and existing gold standard
labels assigned by expert labellers.
Focusing on the actual generation of monolingual
entailment pairs, (Wang and Callison-Burch, 2010)
experiments the use of MTurk to collect facts and
counter facts related to texts extracted from an ex-
isting RTE corpus annotated with named entities.
Taking a step beyond the task of annotating exist-
5The CLTE corpora described in this paper will be made
freely available for research purposes through the website of
the funding EU Project CoSyne (http://www.cosyne.eu/).
6https://www.mturk.com/
7Although MTurk is directly accessible only to US citizens,
the CrowdFlower service (http://crowdflower.com/) provides an
interface to MTurk for non-US citizens.
671
ing datasets, and showing the feasibility of involving
non-experts also in the generation of TE pairs, this
approach is more relevant to our objectives. How-
ever, at least two major differences with our work
have to be remarked. First, they still use avail-
able RTE data to obtain a monolingual TE corpus,
whereas we pursue the more ambitious goal of gen-
erating from scratch aligned CLTE corpora for dif-
ferent language combinations. To this aim, we do
not resort to already annotated data, nor language-
specific preprocessing tools. Second, their approach
involves qualitative analysis of the collected data
only a posteriori, after manual removal of invalid
and trivial generated hypotheses. In contrast, our
approach integrates quality control mechanisms at
all stages of the data collection/annotation process,
thus minimizing the recourse to experts to check the
quality of the collected material.
Related research in the CLTE direction is re-
ported in (Negri and Mehdad, 2010), which de-
scribes the creation of an English-Spanish corpus
obtained from the RTE-3 dataset by translating the
English hypotheses into Spanish. Translations have
been crowdsourced adopting a methodology based
on translation-validation cycles, defined as separate
HITs. Although simplifying the CLTE corpus cre-
ation problem, which is recast as the task of translat-
ing already available annotated data, this solution is
relevant to our work for the idea of combining gold
standard units and ?validation HITS? as a way to
control the quality of the collected data at runtime.
3 Quality Control of Crowdsourced Data
The design of data acquisition HITs has to take into
account several factors, each having a considerable
impact on the difficulty of instructing the workers,
the quality and quantity of the collected data, the
time and overall costs of the acquisition. A major
distinction has to be made between jobs requiring
data annotation, and those involving content gener-
ation. In the former case, Turkers are presented with
the task of labelling input data referring to a fixed
set of possible values (e.g. making a choice between
multiple alternatives, assigning numerical scores to
rank the given data). In the latter case, Turkers are
faced with creative tasks consisting in the production
of textual material (e.g. writing a correct translation,
or a summary of a given text).
The ease of controlling the quality of the acquired
data depends on the nature of the job. For annotation
jobs, quality control mechanisms can be easily set up
by calculating Turkers? agreement, by applying vot-
ing schemes, or by adding hidden gold units to the
data to be annotated8. In contrast, the quality of the
results of content generation jobs is harder to assess,
due to the fact that multiple valid results are accept-
able (e.g. the same content can be expressed, trans-
lated, or summarized in different ways). In such sit-
uations the standard quality control mechanisms are
not directly applicable, and the detection of errors
requires either costly manual verification at the end
of the acquisition process, or more complex and cre-
ative solutions integrating HITs for quality check.
Most of the approaches to content generation pro-
posed so far rely on post hoc verification to fil-
ter out undesired low-quality data (Mrozinski et al,
2008; Mihalcea and Strapparava, 2009; Wang and
Callison-Burch, 2010). The few solutions integrat-
ing validation HITs address the translation of sin-
gle sentences, a task that is substantially different
from ours (Negri and Mehdad, 2010; Bloodgood and
Callison-Burch, 2010). Compared to sentence trans-
lation, the task of creating CLTE pairs is both harder
to explain without recurring to notions that are dif-
ficult to understand to non-experts (e.g. ?seman-
tic equivalence?, ?unidirectional entailment?), and
harder to execute without mastering these notions.
To tackle these issues the ?divide and conquer? ap-
proach described in the next section consists in the
decomposition of a difficult content generation job
into easier subtasks that are: i) self-contained and
easy to explain, ii) easy to execute without any NLP
expertise, and iii) suitable for the integration of a va-
riety of runtime control mechanisms (regional qual-
ifications, gold units, ?validation HITs?) able to en-
sure a good quality of the collected material.
8Both MTurk and CrowdFlower provide means to check
workers? reliability, and weed out untrusted ones without money
waste. These include different types of qualification mecha-
nisms, the possibility of giving work only to known trusted
Turkers (only with MTurk), and the possibility of adding hid-
den gold standard units in the data to be annotated (offered as a
built-in mechanism only by CrowdFlower).
672
4 CLTE Corpus Creation Methodology
Our approach builds on a pipeline of HITs routed to
MTurk?s workforce through the CrowdFlower inter-
face. The objective is to collect aligned T-H pairs
for different language combinations, reproducing an
RTE-like annotation style. However, our annotation
is not limited to the standard RTE framework, where
only unidirectional entailment from T to H is con-
sidered. As a useful extension, we annotate any pos-
sible entailment relation between the two text frag-
ments, including: i) bidirectional entailment (i.e.
semantic equivalence between T and H), ii) unidi-
rectional entailment from T to H, and iii) unidirec-
tional entailment from H to T. The resulting pairs
can be easily used to generate not only standard RTE
datasets9, but also general-purpose collections fea-
turing multi-directional entailment relations.
4.1 Data Acquisition and Annotation
We collect large amounts of CLTE pairs carrying out
the most difficult part of the process (the creation of
entailment-annotated pairs) at a monolingual level.
Starting from a set of parallel sentences in n lan-
guages, (e.g. L1, L2, L3), n entailment corpora are
created: one monolingual (L1/L1), and n-1 cross-
lingual (L1/L2, and L1/L3).
The monolingual corpus is obtained by modify-
ing the sentences only in one language (L1). Orig-
inal and modified sentences are then paired and an-
notated to form an entailment dataset for L1. The
CLTE corpora are obtained by combining the mod-
ified sentences in L1 with the original sentences in
L2 and L3, and projecting to the multilingual pairs
the annotations assigned to the monolingual pairs.
In principle, only two stages of the process re-
quire crowdsourcing multilingual tasks, but do not
concern entailment annotations. The first one, at the
beginning of the process, aims to obtain a set of par-
allel sentences to start with, and can be done in dif-
ferent ways (e.g. crowdsourcing the translation of
a set of sentences). The second one, at the end of
the process, consists of translating the modified L1
sentences into other languages (e.g. L2) in order to
extend the corpus to cover new language combina-
9With the positive examples drawn from bidirectional and
unidirectional entailments from T to H, and the negative ones
drawn from unidirectional entailments from H to T.
tions (e.g. L2/L2, L2/L3).
The execution of the two ?multilingual? stages is
not strictly necessary but depends on: i) the avail-
ability of parallel sentences to start the process, and
ii) the actual objectives in terms of language combi-
nations to be covered10.
As regards the first stage, in this work we started
from a set of 467 English/Italian/German aligned
sentences extracted from parallel documents down-
loaded from the Cafebabel European Magazine11.
Concerning the second multilingual stage, we per-
formed only one round of translations from En-
glish to Italian to extend the 3 combinations ob-
tained without translations (ENG/ENG, ENG/ITA,
and ENG/GER) with the new language combina-
tions ITA/ITA, ITA/ENG, and ITA/GER.
STEP1:	 ?Sentence	 ?modifica?on	 ?(monolingual)	 ?
STEP3:	 ?Transla?on	 ?(mul?lingual)	 ?
GER	 ? ENG	 ?
ENG1	 ?
ITA	 ?
ITA1	 ? ITA	 ?ENG	 ? ENG1	 ?
STEP2:	 ?TE	 ?annota?on	 ?(monolingual)	 ?
Monolingual	 ?TE	 ?corpus	 ?
Cross-??lingual	 ?TE	 ?corpus	 ?
ENG1	 ?GER	 ?
ENG1	 ?ITA	 ?
TE	 ?annota?ns	 ?projec?n	 ?	 ?	 ?
ITA1	 ? GER	 ?
ITA1	 ? ENG	 ?
Figure 1: Corpus creation process.
The main steps of our corpus creation process,
depicted in Figure 1, can be summarized as follows:
Step1: Sentence modification. The original
English sentences (ENG) are modified through
(monolingual) generation HITs asking Turkers to:
i) preserve the meaning of the original sentences
using different surface forms, or ii) slightly change
their meaning by adding or removing content. Our
assumption, in line with (Bos et al, 2009), is that
10Starting from parallel sentences in n languages, the n cor-
pora obtained without recurring to translations can be aug-
mented, by means of translation HITs, to create the full set of
language combinations. Each round of translation adds 1 mono-
lingual corpus, and n-1 CLTE corpora.
11http://www.cafebabel.com/
673
another way to think about entailment is to consider
whether one text T1 adds new information to the
content of another text T: if so, then T is entailed by
T1.
The result of this phase is a set of texts (ENG1)
that can be of three types:
1. Paraphrases of the original ENG texts, that will
be used to create bidirectional entailment pairs
(ENG?ENG1);
2. More specific sentences (the outcome of
content addition operations), used to create
ENG?ENG1 unidirectional entailment pairs;
3. More general sentences (the outcome of
content removal operations), used to create
ENG?ENG1 unidirectional entailment pairs.
Step2: TE Annotation. Entailment pairs com-
posed of the original sentences (ENG) and the modi-
fied ones (ENG1) are used as input of (monolingual)
annotation HITs asking Turkers to decide which of
the two texts contains more information. As a re-
sult, each ENG/ENG1 pair is annotated as an ex-
ample of uni-/bidirectional entailment, and stored in
the monolingual English corpus. Since the original
ENG texts are aligned with the ITA and GER texts,
the entailment annotations of ENG/ENG1 pairs can
be projected to the other language pairs and the
ITA/ENG1 and GER/ENG1 pairs are stored in the
CLTE corpus. The possibility of projecting TE an-
notations is based on the assumption that the seman-
tic information is mostly preserved during the trans-
lation process. This particularly holds at the deno-
tative level (i.e. regarding the truth values of the
sentence) which is crucial to semantic inference. At
other levels (e.g. lexical) there might be slight se-
mantic variations which, however, are very unlikely
to play a crucial role in determining entailment rela-
tions.
Step3: Translation. The modified sentences
(ENG1) are translated into Italian (ITA1) through
(multilingual) generation HITs reproducing the ap-
proach described in (Negri and Mehdad, 2010). As
a result, three new datasets are produced by au-
tomatically projecting annotations: the monolin-
gual ITA/ITA1, and the cross-lingual ENG/ITA1 and
GER/ITA1.
Since the solution adopted for sentence transla-
tion does not present novelty factors, the remainder
of this paper will omit further details on it. Instead,
the following sections will focus on the more chal-
lenging tasks of sentence modification and TE anno-
tation.
4.2 Crowdsourcing Sentence Modification and
TE Annotation
Sentence modification and TE annotation have been
decomposed into a pipeline of simpler monolingual
English sub-tasks. Such pipeline, depicted in Figure
2, involves several types of generation/annotation
HITs designed to be easily understandable to non-
experts. Each HIT consists of: i) a set of instruc-
tions for a specific task (e.g. paraphrasing a text),
ii) the data to be manipulated (e.g. an English sen-
tence), and iii) a test to check workers? reliability.
To cope with the quality control issues discussed in
Section 3, such tests are realized using gold stan-
dard units, either hidden in the data to be annotated
(annotation HITs) or defined as test questions that
workers must correctly answer (generation HITs).
Moreover, regional qualifications are applied to all
HITs. As a further quality check, all the annotation
HITs consider Turkers? agreement as a way to filter
out low quality results (only annotations featuring
agreement among 4 out of 5 workers are retained).
The six HITs defined for each subtask can be de-
scribed as follows:
1. Paraphrase (generation). Modify an En-
glish text (ENG), in order to produce a semantically
equivalent variant (ENG1). As a reliability test, be-
fore creating the paraphrase workers are asked to
judge if two English sentences contain the same in-
formation.
2. Grammaticality (annotation). Decide if an
English sentence is grammatically correct. This val-
idation HIT represents a quality check of the out-
put of each generation task (i.e. paraphrasing, and
add/remove information HITs).
3. Bidirectional Entailment (annotation). De-
cide whether two English sentences, the original
ENG and the modified ENG1, contain the same in-
formation (i.e. are semantically equivalent).
4a. Add Information (generation). Modify an
English text to create a more specific one by adding
content. As a reliability test, before generating the
674
Figure 2: Sentence modification and TE annotation pipeline.
new sentence workers are asked to judge which of
two given English sentences is more detailed.
4b. Remove Information (generation). Mod-
ify an English text to create a more general one by
removing part of its content. As a reliability test, be-
fore generating the new sentence workers are asked
to judge which of two given English sentences is less
detailed.
5. Unidirectional Entailment (annotation). De-
cide which of two English sentences (the original
ENG, and a modified ENG1) provides more infor-
mation.
These HITs are combined in an iterative pro-
cess that alternates text generation, grammaticality
check, and entailment annotation steps. As a result,
for each original ENG text we obtain multiple ENG1
variants of the three types (paraphrases, more gen-
eral texts, and more specific texts) and, in turn, a set
of annotated monolingual (ENG/ENG1) TE pairs.
As described in Section 4.1, the resulting mono-
lingual English TE corpus (ENG/ENG1) is used to
create the following mono/cross-lingual TE corpora:
? ITA/ENG1, and GER/ENG1 (by projecting TE
annotations)
? ITA/ITA1, GER/ITA1, and ENG/ITA1 (by
translating the ENG1 texts into Italian, and pro-
jecting TE annotations)
5 The Resulting CLTE Corpora
This section provides a quantitative and qualita-
tive analysis of the results of our corpus creation
methodology, focusing on the collected ENG-ENG1
monolingual dataset. It has to be remarked that, as
an effect of the adopted methodology, all the obser-
vations and the conclusions drawn hold for the col-
lected CLTE corpora as well.
5.1 Quantitative Analysis
Table 1 provides some details about each step of the
pipeline shown in Figure 2. For each HIT the table
presents: i) the number of items (sentences, or pairs
of sentences) given in input, ii) the number of items
(sentences or annotations) produced as output, iii)
the number of items discarded when the agreement
threshold was not reached, iv) the number of entail-
ment pairs added to the corpus, v) the time (days and
hours) required by the MTurk workforce to complete
the job, and vi) the cost of the job.
In HIT-1 (Paraphrase) 1,414 paraphrases were
collected asking three different meaning-preserving
modifications of each of the 467 original sen-
tences12. From a practical point of view, such redun-
dancy aims to ensure a sufficient number of gram-
matically correct and semantically equivalent mod-
ified sentences. From a theoretical point of view,
12Often, crowdsourced jobs return a number of output items
that is slightly larger than required, due to the labour distribution
mechanism internal to MTurk.
675
HIT # Input items # Output items # Discarded items # Pairs to corpus MTurk time Cost ($)
1. Paraphrase 467 1,414 5d+10.5h 45.48
2. Grammaticality 1,414 1,326 88 (6.22%) 1d+15h 56.88
3. Bidirectional Ent. 1,326 1,213 113 (8.52%) 301 3d+2h 53.47
(yes=1,205 no=8)
4a. Add Info 452 916 3d 37.02
4b. Remove Info 452 923 2d+22h 29.73
2. Grammaticality 1,839 1,749 90 (4.89%) 2d+5h 64.37
3. Bidirectional Ent. 1,749 1,438 311 (17.78%) 148 3d+20.5h 70.52
(yes=148 no=1,290)
5. Unidirectional Ent. 1,298 1,171 127 (9.78%) 1,171 8.5h 78.24
(491 + 680)
TOTAL 721 1,620 22d+11h 435.71
Table 1: The monolingual dataset creation pipeline.
collecting many variants of a small pool of origi-
nal sentences aims to create pairs featuring different
entailment relations with similar superficial forms.
This, in principle, should allow to obtain a dataset
which requires TE systems to focus more on deeper
semantic phenomena than on the surface realization
of the pairs.
The collected paraphrases were sent as input to
HIT-2 (Grammaticality). After this validation HIT,
the number of acceptable paraphrases was reduced
to 1,326 (with 88 discarded sentences, correspond-
ing to 6.22% of the total).
The retained paraphrases were paired with their
corresponding original sentences, and sent to HIT-3
(Bidirectional Entailment) to be judged for semantic
equivalence. The pairs marked as bidirectional en-
tailments (1,205) were divided in three groups: 25%
of the pairs (301) were directly stored in the final
corpus, while the ENG1 paraphrases of the remain-
ing 75% (904) were equally distributed to the next
modification steps.
In both HIT-4a (Add Information) and HIT-4b
(Remove information) two new modified sentences
were asked for each of the 452 paraphrases received
as input. The sentences collected in these generation
tasks were respectively 916 and 923.
The new modified sentences were sent back to
HIT-2 (Grammaticality) and HIT-3 (Bidirectional
Entailment). As a result 1,438 new pairs were cre-
ated; out of these, 148 resulted to be bidirectional
entailments and were stored in the corpus.
Finally, the 1,298 entailment pairs judged as non-
bidirectional in the two previously completed HIT-
3 (8+1,290) were given as input to HIT-5 (Unidi-
rectional Entailment). The pairs which passed the
agreement threshold were classified according to the
judgement received, and stored in the corpus as uni-
directional entailment pairs.
The analysis of Table 1 allows to formulate
some considerations. First, the percentage of dis-
carded items confirms the effectiveness of decom-
posing complex generation tasks into simpler sub-
tasks that integrate validation HITs and quality
checks based on non-experts? agreement. In fact, on
average, around 9.5% of the generated items were
discarded without experts? intervention13. Second,
the amount of discarded items gives evidence about
the relative difficulty of each HIT. As expected,
we observe lower rejection rates, corresponding to
higher inter-annotator agreement, for grammatical-
ity HITs (5.55% on average) than for more complex
entailment-related tasks (12.02% on average).
Looking at costs and execution time, it is hard
to draw definite conclusions due to several factors
that influence the progress of the crowdsourced jobs
(e.g. the fluctuations of Turkers? performances, the
time of the day at which jobs are posted, the dif-
ficulty to set the optimal cost for a given HIT14).
On the one hand, as expected, the more creative
?Add Info? task proved to be more demanding than
the ?Remove Info?: even though it was paid more,
13Moreover, it is worthwhile noticing that around 20% of the
collected items were automatically rejected (and not paid) due
to failures on the gold standard controls created both for gener-
ation and annotation tasks.
14The payment for each HIT was set on the basis of a pre-
vious feasibility study aimed at determining the best trade-off
between cost and execution time. However, replicating our ap-
proach would not necessarily result in the same costs.
676
it still took little more time to be completed. On
the other hand, although the ?Unidirectional Entail-
ment? task was expected to be more difficult and
thus rewarded more than the ?Bidirectional Entail-
ment? one, in the end it took notably less time to
be completed. Nevertheless, the overall figures (435
USD, and about 22.5 days of MTurk work to com-
plete the process)15 clearly demonstrate the effec-
tiveness of the approach. Even considering the time
needed for an expert to manage the pipeline (i.e. one
week to prepare gold units, and to handle the I/O of
each HIT), these figures show that our methodology
provides a cheaper and faster way to collect entail-
ment data in comparison with the RTE average costs
reported in Section 1.
As regards the amount of data collected, the re-
sulting corpus contains 1,620 pairs with the fol-
lowing distribution of entailment relations: i) 449
bidirectional entailments, ii) 491 ENG?ENG1 uni-
directional entailments, and iii) 680 ENG?ENG1
unidirectional entailments.
It must be noted that our methodology does not
lead to the creation of pairs where some information
is provided in one text and not in the other, and vice-
versa, as Example 1 shows:
Example 1.
ENG: New theories were emerging in the field of psychology.
ENG1: New theories were rising, which announced a kind of
veiled racism.
These negative examples in both directions repre-
sent a natural extension of the dataset, relevant also
for specific application-oriented scenarios, and their
creation will be addressed in future work.
Besides the achievement of our primary objec-
tives, the adopted approach led to some interesting
by-products. First, the generated corpora are per-
fectly suitable to produce entailment datasets simi-
lar to those used in the traditional RTE evaluation
framework. In particular, considering any possible
entailment relation between two text fragments, our
annotation subsumes the one proposed in RTE cam-
paigns. This allows for the cost-effective genera-
tion of RTE-like annotations from the acquired cor-
15Although by projecting annotations the ENG1/ITA and
ENG1/GER CLTE corpora came for free, the ITA1/ITA,
ITA1/ENG, and ITA1/GER combinations created by crowd-
sourcing translations added 45 USD and approximately 5 days
to these figures.
pora by combining ENG?ENG1 and ENG?ENG1
pairs to form 940 positive examples (449+491),
keeping the 680 ENG?ENG1 as negative exam-
ples. Moreover, by swapping ENG and ENG1 in the
unidirectional entailment pairs, 491 additional nega-
tive examples and 680 positive examples can be eas-
ily obtained.
Finally, the output of HITs 1-2-3 in Table 1 rep-
resents per se a valuable collection of 1,205 para-
phrases. This suggests the great potential of crowd-
sourcing for paraphrase acquisition.
5.2 Qualitative Analysis
Through manual verification of more than 50% of
the corpus (900 pairs), a total number of 53 pairs
(5.9%) were found incorrect. The different errors
were classified as follows:
Type 1: Sentence modification errors. Generation
HITs are a minor source of errors, being responsible
for 10 problematic pairs. These errors are either in-
troduced by generating a false statement (Example
2), or by forming a not fully understandable, awk-
ward, or non-natural sentence (Example 3).
Example 2.
ENG: Kosovo was the subject of major riots in 1989.
ENG1: The Russian city of Kosovo was the subject of ...
Example 3.
ENG: Balat is the Kurdish-Armenian district of Instanbul.
ENG1: Balat is a place, which is the Kurdish-Armenian ...
Type 2: TE annotation errors. The notion of con-
taining more/less information, used in the ?Unidi-
rectional Entailment? HIT, can mostly be applied
straightforwardly to the entailment definition. How-
ever, the concept of ?more/less detailed?, which gen-
erally works for factual statements, in some cases is
not applicable. In fact, the MTurk workers have reg-
ularly interpreted the instructions about the amount
of information as concerning the quantity of con-
cepts contained in a sentence. This is not always cor-
responding to the actual entailment relation between
the sentences. As a consequence, 43 pairs featur-
ing wrong entailment annotations were encountered.
These errors can be classified as follows:
a) 13 pairs, where the added/removed information
changes the meaning of the sentence. In these cases,
the modified sentence was judged more/less specific
677
than the original one, leading to unidirectional en-
tailment annotation. On the contrary, in terms of
the standard entailment definition, the correct anno-
tation is ?no entailment? (as in Example 4, which
was annotated as ENG?ENG1):
Example 4.
ENG: If you decide to live in Bulgaria, you have to like
difficulties because they are not difficulties, they are challenges.
ENG1: You have to like difficulties as they are not difficulties,
they are challenges.
b) 10 pairs where the incorrect annotation is due to
a coreference problem, as in:
Example 5.
ENG: John Smith is the new CEO of the company.
ENG1: He is the new CEO of the company.
These pairs were labelled as unidirectional entail-
ments (in the example above ENG?ENG1), under
the assumption that a proper name is more specific
and informative than a pronoun. However, adher-
ing to the TE definition, co-referring expressions are
equivalent, and their realization does not play any
role in the entailment decision. This implies that the
correct entailment annotation is ?bidirectional?.
c) 9 pairs where the sentences are semantically
equivalent, but contain a piece of information which
is explicit in one sentence, and implicit in the other.
In these cases, Turkers judged the sentence contain-
ing the explicit mention as more specific, and thus
the pair was annotated as unidirectional entailment.
Example 6.
ENG: I hear the click of the trigger and the burst of bullets
reach me immediately.
ENG1: I hear the trigger and the burst of bullets reach me
instantly.
In Example 6, the expression ?the trigger? in ENG1
implicitly means ?the click of the trigger?, mak-
ing the two sentences equivalent, and the entailment
bidirectional (instead of ENG?ENG1).
d) 7 pairs where the information removed from or
added to the sentence is not relevant to the entail-
ment relation. In these cases, the modified sen-
tence was judged less/more specific than the origi-
nal one (and thus considered as unidirectional entail-
ment), even though the correct judgement is ?bidi-
rectional?, as in:
Example 7.
ENG: At the same time, AKP is struggling with its approach to
the EU.
ENG1: AKP is struggling with its approach to the European
Union.
e) 4 pairs where the added/removed information
concerns universally quantified general statements,
about which the interpretation of ?more/less spe-
cific? given by Turkers resulted in the wrong anno-
tation.
Example 8.
ENG: I think the success of multicultural couples depends on
the size of the cultural gap between the two partners
ENG1: I believe the success of the couples depends on the size
of the cultural gap between the 2 partners.
In Example 8, the additional information (?mul-
ticultural?) restricts the set to which it refers
(?couples?) making ENG entailed by ENG1, and
not vice versa as resulted from Turkers? annotation.
In light of this analysis, we conclude that the sen-
tence modification methodology proved to be suc-
cessful, as the low number of Type 1 errors shows.
Considering that the most expensive phase in the
creation of a TE dataset is the generation of the
pairs, this is a significant achievement. Differently,
the entailment assessment phase appears to be more
problematic, accounting for the majority of errors.
As shown by Type 2 errors, this is due to a par-
tial misalignment between the instructions given in
our HITs, and the formal definition of textual en-
tailment. For this reason, further experimentation
will explore different ways to instruct workers (e.g.
asking to consider proper names and pronouns as
equivalent) in order to reduce the amount of errors
produced. As a final remark, considering that in the
creation of a TE dataset the manual check of the an-
notated pairs represents a minor cost, even the in-
volvement of experts to filter out wrong annotations
would not decrease the cost-effectiveness of the pro-
posed methodology.
6 Conclusions
There is an increasing need of annotated data to
develop new solutions to the Textual Entailment
problem, explore new entailment-related tasks, and
set up experimental frameworks targeting real-world
applications. Following the recent trends promot-
ing annotation efforts that go beyond the estab-
lished RTE Challenge framework (unidirectional en-
tailment between monolingual T-H pairs), in this
678
paper we addressed the multilingual dimension of
the problem. Our primary goal was the creation of
large-scale collections of entailment pairs for differ-
ent language combinations. Besides that, we consid-
ered cost effectiveness and replicability as additional
requirements. To achieve our objectives, we devel-
oped a ?divide and conquer? methodology based on
crowdsourcing. Our approach presents several key
innovations with respect to the related works on TE
data acquisition. These include the decomposition
of a complex content generation task in a pipeline
of simpler subtasks accessible to a large crowd of
non-experts, and the integration of quality control
mechanisms at each stage of the process. The result
of our work is the first large-scale dataset contain-
ing both monolingual and cross-lingual corpora for
several combinations of texts-hypotheses in English,
Italian, and German. Among the advantages of our
method it is worth mentioning: i) the full alignment
between the created corpora, ii) the possibility to
easily extend the dataset to new languages, and iii)
the feasibility of creating general-purpose corpora,
featuring multi-directional entailment relations, that
subsume the traditional RTE-like annotation.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The au-
thors would like to thank Emanuele Pianta for the
helpful discussions, and Giovanni Moretti for the
valuable support in the creation of the CLTE dataset.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of TAC 2009.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. Proceedings of LREC 2010.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using Mechanical Turk to Build Machine Translation
Evaluation Sets. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
Johan Bos, Fabio Massimo Zanzotto, and Marco Pennac-
chiotti. 2009. Textual Entailment at EVALITA 2009.
Proceedings of EVALITA 2009.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazons Me-
chanical Turk. Proceedings NAACL-2010 Workshop
on Creating Speech and Language Data With Amazons
Mechanical Turk.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
Proceedings of NAACL-HLT 2010.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. Proceedings of ACL-HLT
2011.
Rada Mihalcea and Carlo Strapparava. 2009. The Lie
Detector: Explorations in the Automatic Recognition
of Deceptive Language. Proceedings of ACL 2009.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a Why-Question Corpus for Devel-
opment and Evaluation of an Automatic QA-System.
Proceedings of ACL 2008.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. Proceed-
ings of the NAACL 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk.
Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.
2010. Ask Not What Textual Entailment Can Do for
You... Proceedings of ACL 2010.
Rion Snow, Brendan O?Connor, Daniel Jurafsky and An-
drew Y. Ng. 2008. Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. Proceedings of EMNLP 2008.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
679
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1643?1653,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Assessing the Impact of Translation Errors
on Machine Translation Quality with Mixed-effects Models
Marcello Federico, Matteo Negri, Luisa Bentivogli, Marco Turchi
FBK - Fondazione Bruno Kessler
Via Sommarive 18, 38123 Trento, Italy
{federico,negri,bentivogli,turchi}@fbk.eu
Abstract
Learning from errors is a crucial aspect of
improving expertise. Based on this no-
tion, we discuss a robust statistical frame-
work for analysing the impact of different
error types on machine translation (MT)
output quality. Our approach is based on
linear mixed-effects models, which allow
the analysis of error-annotated MT out-
put taking into account the variability in-
herent to the specific experimental setting
from which the empirical observations are
drawn. Our experiments are carried out
on different language pairs involving Chi-
nese, Arabic and Russian as target lan-
guages. Interesting findings are reported,
concerning the impact of different error
types both at the level of human perception
of quality and with respect to performance
results measured with automatic metrics.
1 Introduction
The dominant statistical approach to machine
translation (MT) is based on learning from large
amounts of parallel data and tuning the result-
ing models on reference-based metrics that can
be computed automatically, such as BLEU (Pap-
ineni et al., 2001), METEOR (Banerjee and Lavie,
2005), TER (Snover et al., 2006), GTM (Turian
et al., 2003). Despite the steady progress in the
last two decades, especially for few well resourced
translation directions having English as target lan-
guage, this way to approach the problem is quickly
reaching a performance plateau. One reason is
that parallel data are a source of reliable informa-
tion but, alone, limit systems knowledge to ob-
served positive examples (i.e. how a sentence
should be translated) without explicitly modelling
any notion of error (i.e. how a sentence should
not be translated). Another reason is that, as a
development and evaluation criterion, automatic
metrics provide a holistic view of systems? be-
haviour without identifying the specific issues of a
translation. Indeed, the global scores returned by
MT evaluation metrics depend on comparisons be-
tween translation hypotheses and reference trans-
lations, where the causes and the nature of the dif-
ferences between them are not identified.
To cope with these issues and define system
improvement priorities, the focus of MT evalua-
tion research is gradually shifting towards profil-
ing systems? behaviour with respect to various ty-
pologies of errors (Vilar et al., 2006; Popovi?c and
Ney, 2011; Farr?us et al., 2012, inter alia). This
shift has enriched the traditional MT evaluation
framework with a new element, that is the actual
errors done by a system. Until now, most of the
research has focused on the relationship (i.e. the
correlation) between two elements of the frame-
work: humans and automatic evaluation metrics.
As a new element of the framework, which be-
comes a sort of ?evaluation triangle?, the analy-
sis of error annotations opens interesting research
problems related to the relationships between: i)
error types and human perception of MT quality
and ii) error types and the sensitivity of automatic
metrics.
Besides motivating further investigation on met-
rics featuring high correlation with human judge-
ments (a well-established MT research sub-field,
which is out of the scope of this paper), connecting
the vertices of this triangle raises new challenging
questions such as:
(1) Which types of MT errors have the high-
est impact on human perception of translation
quality? Surprisingly, little prior work focused
on this side of the triangle. Error annotations
have been considered to highlight strengths and
weaknesses of MT engines or to investigate the
influence of different error types on post-editors?
work. However, the direct connection between er-
1643
rors and users? preferences has been only partially
understood, mainly from a descriptive standpoint
and through rudimentary techniques unsuitable to
draw clear-cut conclusions or reliable inferences.
(2) To which types of errors are different MT
evaluation metrics more sensitive? This side of
the triangle has been even less explored. For in-
stance, little has been done to understand which
automatic metric is more suitable to assess sys-
tem improvements with respect to a specific issue
(e.g. word order or morphology) or to shed light
on the joint impact of different error types on per-
formance results calculated with different metrics.
To answer these questions, we propose a ro-
bust statistical framework to analyse the im-
pact of different error types, alone and in com-
bination, both on human perception of quality and
on MT evaluation metrics? results. Our analysis
is carried out by employing linear mixed-effects
models, a generalization of linear regression mod-
els suited to model responses with fixed and ran-
dom effects. Experiments are performed on data
covering three translation directions (English to
Chinese, Arabic and Russian). For each direc-
tion, two automatic translations were collected for
around 400 sentences and were manually evalu-
ated by expert translators through absolute quality
judgements and error annotation.
Building on the advantages offered by linear
mixed-effects models, our main contributions in-
clude:
? A rigorous method, novel to MT error anal-
ysis research, to relate MT issues to human
preferences and MT metrics? results;
? The application of such method to three
translation directions having English as
source and different languages as target;
? A number of findings, specific to each lan-
guage direction, which are out of the reach of
the few simpler methods proposed so far.
Overall, our study has clear practical implica-
tions for MT systems? development and evalu-
ation. Indeed, the proposed statistical analysis
framework represents an ideal instrument to: i)
identify translation issues having the highest im-
pact on human perception of quality and ii) choose
the most appropriate evaluation metric to measure
progress towards their solution.
2 Related Work
Error analysis, as a way to identify systems? weak-
nesses and define priorities for their improvement,
is gaining increasing interest in the MT com-
munity (Popovi?c and Ney, 2011; Popovic et al.,
2013). Along this direction, the initial efforts to
develop error taxonomies covering different levels
of granularity (Flanagan, 1994; Vilar et al., 2006;
Farr?us Cabeceran et al., 2010; Stymne and Ahren-
berg, 2012; Lommel et al., 2014) have been re-
cently complemented by investigations on how to
exploit error annotations for diagnostic purposes.
Error annotations of sentences produced by differ-
ent MT systems, in different target languages and
domains, have been used to determine the qual-
ity of translations according to the amount of er-
rors encountered (Popovic et al., 2013), to design
new automatic metrics that take into considera-
tion human annotations (Popovic, 2012; Bojar et
al., 2013), and to train classifiers that can auto-
matic identify fine-grained errors in the MT output
(Popovi?c and Ney, 2011). The impact of edit op-
erations on post-editors? productivity, which im-
plicitly connects the severity of different errors to
human activity, has also been studied (Temnikova,
2010; O?Brien, 2011; Blain et al., 2011), but
few attempts have been made to explicitly model
how fine-grained errors impact on human quality
judgements and automatic metrics.
Recently, the relation between different error
types, their frequency, and human quality judge-
ments has been investigated from a descriptive
standpoint in (Lommel et al., 2014; Popovi?c et al.,
2014). In both works, however, the underlying as-
sumption that the most frequent error has also the
largest impact on quality perception is not verified
(in general and, least of all, across language pairs,
domains, MT systems and post-editors). Another
limitation of the proposed (univariate) analysis lies
in the fact that it exclusively focuses on error types
taken in isolation. This simplification excludes the
possibility that humans, when assigning a global
quality score to a translation, may be influenced
not only by the error types but also by their inter-
action. The implications of such possibility call
for a multivariate analysis capable to model also
error interactions.
In (Kirchhoff et al., 2013), a statistically-
grounded approach based on conjoint analysis has
been used to investigate users? reactions to dif-
ferent types of translation errors. According to
1644
their results, word order is the most dispreferred
error type, and the count of the errors in a sen-
tence is not a good predictor of users? prefer-
ences. Though more sophisticated than methods
based on rough error counts, the conjoint model
is bound to several constraints that limit its us-
ability. In particular, the application of conjoint
analysis in this context requires to: i) operate with
semi-automatically created (hence artificial) data
instead of real MT output, ii) manually define dif-
ferent levels of severity for each error type (e.g.
high/medium/low), and iii) limit the number of er-
ror types considered to avoid the explosion of all
possible combinations. Finally, the conjoint anal-
ysis framework is not able to explicitly model vari-
ance in the translated sentences, the human anno-
tators, and the SMT systems used to translate the
source sentences. Our claim is that avoiding any
possible bias introduced by these factors should be
a priority in the analysis of empirical observations
in a given experimental setting.
So far, the relation between errors and auto-
matic metrics has been analysed by measuring the
correlation between single or total error frequen-
cies and automatic scores (Popovi?c and Ney, 2011;
Farr?us et al., 2012). Using two different error tax-
onomies, both works show that the sum of the er-
rors has a high correlation with BLEU and TER
scores. Similar to the aforementioned works ad-
dressing the impact of MT errors on human per-
ception, these studies disregard error interactions,
and their possible impact on automatic scores.
To overcome these issues, we propose a ro-
bust statistic analysis framework based on mixed-
effects models, which have been successfully ap-
plied to several NLP problems such as sentiment
analysis (Greene and Resnik, 2009), automatic
speech recognition (Goldwater et al., 2010), and
spoken language translation (Ruiz and Federico,
2014). Despite their effectiveness, the use of
mixed-effects models in the MT field is rather re-
cent and limited to the analysis of human post-
editions (Green et al., 2013; L?aubli et al., 2013).
In both studies, the goal was to evaluate the im-
pact of post-editing on the quality and productivity
of human translation assuming an ANOVA mixed
model for a between-subject design, in which hu-
man translators either post-edited or translated the
same texts. Our scenario is rather different as we
employ mixed models to measure the influence of
different MT error types - expressed as continu-
ous fixed effects - on quality judgements and auto-
matic quality metrics. Mixed models, having the
capability to absorb random variability due to the
specific experimental set-up, provide a robust mul-
tivariate method to efficiently analyse the impor-
tance of error types.
Finally, differently from all previous works, our
analysis is run on language pairs having English
as source and languages distant from English (in
term of morphology and word-order) as target.
3 Mixed-effects Models
Mixed-effects models - or simply mixed models
- like any regression model, express the relation-
ship between a response variable and some co-
variates and/or contrast factors. They enhance
conventional models by complementing fixed ef-
fects with so-called random effects. Random ef-
fects are introduced to absorb random variability
inherent to the specific experimental setting from
which the observations are drawn. In general, ran-
dom effects correspond to covariates that are not -
or cannot be - exhaustively observed in an experi-
ment, e.g. the human annotators and the evaluated
systems. Hence, mixed models permit to elegantly
cope with experimental design aspects that hinder
the applicability of conventional regression mod-
els. These are, in particular, the use of repeated
and/or clustered observations that introduce corre-
lations in the response variable that clearly violate
the independence and homoscedasticity assump-
tions of conventional linear, ANOVA, and logis-
tic regression models. Significance testing with
mixed models is in general more powerful, i.e. less
prone to Type II Errors, and also permits to reduce
the chance of Type I Errors in within-subject de-
signs, which are prone to the ?fallacy of language-
as-a-fixed-effect? (Clark, 1973).
Random effects can be directly associated to
the regression model parameters, as random in-
tercepts and random slopes, and have the same
form of the generic error component of the model,
i.e. normally distributed with zero mean and un-
known variance. As random effects introduce hid-
den variables, mixed models are trained with Ex-
pectation Maximization, while significance testing
is performed via likelihood-ratio (LR) tests.
In this work we employ mixed linear models to
measure the influence of different MT error types,
expressed as continuous fixed effects, on quality
1645
judgements or on automatic quality metrics.
1
We illustrate mixed linear models (Baayen et
al., 2008) by referring to our analysis, which ad-
dresses the relationships between a quality metric
(y) and different types of errors (e.g. A, B, and
C)
2
observed at the sentence level. For the sake of
simplicity, we assume to have balanced repeated
observations for one single crossed effect. That is,
we have i ? {1, . . . , I} MT systems (our groups)
each of which translated the same j ? {1, . . . , J}
test sentences. Our response variable y
ij
- a nu-
meric quality score - is computed on each (sen-
tence, system) pair, and we aim to investigate its
relationship with error statistics available for each
MT output, namely A
ij
, B
ij
and C
ij
. A (possible)
linear mixed model for our study would be:
y
ij
= ?
0
+ ?
1
A
ij
+ ?
2
B
ij
+ ?
3
C
ij
+ (1)
b
0,i
+ b
1,i
A
ij
+ b
2,i
B
ij
+ b
3,i
C
i
+ 
ij
The model is split into two lines on purpose. The
first line shows the fixed effect component, that is
intercept (?
0
) and slopes (?
1
, ?
2
, ?
3
) for each error
type. The second line specifies the random struc-
ture of the model, which includes random inter-
cept and slopes for each MT system and the resid-
ual error. Borrowing the notation from (Green
et al., 2013), we conveniently rewrite (1) in the
group-wise arranged matrix notation:
y
i
= x
T
i
? + z
T
i
b
i
+ 
i
(2)
where y
i
is the J ? 1 vector of responses, x
i
is the
J?p design matrix of covariates (including the in-
tercept) with fixed coefficients ? ? R
p?1
, z is the
random structure matrix defined by J ? q covari-
ates with random coefficients b
i
? R
q?1
, and 
i
is
the vector of residuals (in our example, p = 4 and
q = 4). By packing together vectors and matrices
indexed over groups i, we can rewrite the model
in a general form (Baayen et al., 2008), which can
represent any possible crossed-effects and random
structures defined over them allowing, at the same
time, for a compact model specification:
y = X
T
? + Z
T
b+  (3)
 ? N (0, ?
2
I), b ? N (0, ?
2
?), b ? 
1
Although mixed ordinal models (Tutz and Hennevogl,
1996) are in principle more appropriate to target quality
judgements, in our preliminary investigations mixed linear
models showed a significantly higher predictive power.
2
Here, A, B and C represent three generic error classes.
Their actual number in a given experimental setting will de-
pend on the granularity of the reference error taxonomy.
where ? is the relative variance-covariance q ? q
matrix of the random effects (now q = 4I), ?
2
is the variance of the per-observation term , the
symbol ? denotes independence of random vari-
ables, andN indicates the multivariate normal dis-
tribution. While b, ?, and ? are estimated via max-
imum likelihood, the single random intercept and
slope values for each group are calculated subse-
quently. They are referred to as Best Linear Un-
biased Predictors (BLUPS) and, formally, are not
parameters of the model.
The significance of the contribution of each sin-
gle parameter (e.g. single entries of ?) to the
goodness of fit can be tested via likelihood ratio.
In this way, both the fixed and random effect struc-
ture of the model can be investigated with respect
to its actual necessity to the model.
4 Dataset
For our analysis we used a dataset that covers
three translation directions, corresponding to En-
glish to Chinese, Arabic, and Russian. An inter-
national organization provided us a set of English
sentences together with their translation produced
by two anonymous MT systems. For each evalu-
ation item (source sentence and two MT outputs)
three experts were asked to assign quality scores to
the MT outputs, and a fourth expert was asked to
annotate translation errors. The four experts, who
were all professional translators native in the ex-
amined target languages, were carefully trained to
get acquainted with the evaluation guidelines and
the annotation tool specifically developed for these
evaluation tasks (Girardi et al., 2014). The anno-
tation process was carried out in parallel by all an-
notators over one week, resulting in a final dataset
composed of 312 evaluation items for the ENZH
direction, 393 for ENAR, and 437 for ENRU.
4.1 Quality Judgements
Quality judgements were collected by asking the
three experts to rate each automatic translation
according to a 1-5 Likert scale, where 1 means
?incomprehensible translation? and 5 means ?per-
fect translation?. The distribution of the collected
annotations with respect to each quality score is
shown in Figure 1. As we can see, this distri-
bution reflects different levels of perceived qual-
ity across languages. ENZH, for instance, has the
highest number of low quality scores (1 and 2),
while ENRU has the highest number of high qual-
1646
0%	 ?
20%	 ?
40%	 ?
60%	 ?
80%	 ?
100%	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
5	 ?
4	 ?
3	 ?
2	 ?
1	 ?
Figure 1: Distribution of quality scores.
ity scores (4 and 5).
Table 1 shows the average of all the qual-
ity scores assigned by each annototator as well
as the average score obtained for each MT sys-
tem. These values demonstrate the variability
of annotators and systems. A particularly high
variability among human judges is observed for
the ENAR language direction (also reflected by
the inter-annotator agreement scores discussed be-
low), while ENZH shows the highest variability
between systems. As we will see in ?5.1, we suc-
cessfully cope with this variability by considering
systems and annotators as random effects, which
allow the regression models to abstract from these
differences.
Ann1 Ann2 Ann3 Sys1 Sys2
ENZH 2.38 2.69 2.21 2.29 2.56
ENAR 2.76 2.77 1.84 2.39 2.53
ENRU 2.82 2.72 2.96 2.87 2.79
Table 1: Average quality scores per annotator and
per system.
Inter-annotator agreement was computed using
the Fleiss? kappa coefficient (Fleiss, 1971), and re-
sulted in 22.70% for ENZH, 5.24% for ENAR, and
21.80% for ENRU. While for ENZH and ENRU
the results fall in the range of ?fair? agreement
(Landis and Koch, 1977), for ENAR only ?slight?
agreement is reached, reflecting the higher anno-
tators? variability evidenced in Table 1.
A more fine-grained agreement analysis is pre-
sented in Figure 2, where the kappa values are
given for each score class. In general we no-
tice a lower agreement on the intermediate quality
scores, while annotators tend to agree on very bad
and, even more, on good translations. In partic-
ular, we see that the agreement for ENAR is sys-
tematically lower than the values measured for the
other languages on all the score classes.
-??0.1	 ?
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
FLE
ISS
'	 ?KA
PPA
	 ?
QUALITY	 ?SCORES	 ?
ENZH	 ?
ENAR	 ?
ENRU	 ?
Figure 2: Class specific inter-annotator agreement.
4.2 Error Annotation
This evaluation task was carried out by one ex-
pert for each language direction, who was asked to
identify the type of errors present in the MT output
and to mark their position in the text. Since the fo-
cus of our work is the analysis method rather than
the definition of an ideal error taxonomy, for the
difficult language directions addressed we opted
for the following general error classes, partially
overlapping with (Vilar et al., 2006): i) reordering
errors, ii) lexicon errors (including wrong lexical
choices and extra words), iii) missing words, iv)
morphology errors.
Figure 3 shows the distribution of the errors in
terms of affected tokens (words) for each error
type. Since token counts for Chinese are not word-
based but character-based, for readability purposes
the number of errors counted for Chinese trans-
lations have been divided by 2.5. Note also that
morphological errors annotated for ENZH involve
only 13 characters and thus are not visible in the
plot. The total number of errors amounts to 16,320
characters for ENZH, 4,926 words for ENAR, and
5,965 words for ENRU.
This distribution highlights some differences
between languages directions. For example, trans-
lations into Arabic and Russian present several
morphology errors, while word reordering is the
most frequent issue for translations into Chinese.
As we will see in ?5.1, error frequency does not
give a direct indication of their impact on trasla-
tion quality judgements.
4.3 Automatic Metrics
In our investigation we consider three popular au-
tomatic metrics: sentence-level BLEU (Lin and
Och, 2004), TER (Snover et al., 2006), and GTM
(Turian et al., 2003). We compute all automatic
scores by relying on a single reference and by
1647
0	 ?
500	 ?
1000	 ?
1500	 ?
2000	 ?
2500	 ?
3000	 ?
3500	 ?
4000	 ?
ENZH	 ? ENAR	 ? ENRU	 ?
LEX	 ?
MISS	 ?
MORPH	 ?
REO	 ?
Figure 3: Distribution of error types.
means of standard packages. In particular, auto-
matic scores on Chinese are computed at the char-
acter level. Moreover, as we use metrics as re-
sponse variables for our regression models, we
compute all metrics at the sentence level. The
overall mean scores for all systems and languages
are reported in Table 2. Differences in systems?
performance can be observed for all language
pairs; as we will observe in ?5.2 such variability
explains the effectiveness of considering the MT
systems as a random effect.
BLEU TER GTM
Sys1 Sys2 Sys1 Sys2 Sy1 Sys2
ENZH 27.95 44.11 64.52 48.13 62.15 72.30
ENAR 19.63 25.25 68.83 63.99 47.20 52.33
ENRU 27.10 31.07 60.89 54.41 53.74 56.41
Table 2: Overall automatic scores per system.
5 Experiments
To assess the impact of translation errors on MT
quality we perform two sets of experiments. The
first set (?5.1) addresses the relation between er-
rors and human quality judgements. The sec-
ond set (?5.2) focuses on the relation between er-
rors and automatic metrics. In both cases, be-
fore measuring the impact of different errors on
the response variable (respectively quality judge-
ments and metrics), we validate the effectiveness
of mixed linear models by comparing their predic-
tion capability with other methods.
In all experiments, error counts of each category
were normalized into percentages with respect to
the sentence length and mapped in a logarithmic
scale. In this way, we basically assume that the
impact of errors tends to saturate above a given
threshold, hypothesis that also results in better fits
by our models.
3
Notice that while the chosen log-
3
In other words, we assume that human sensitivity to er-
10 base is easy to interpret, linear models can im-
plicitly adjust it. Our analysis makes use of mixed
linear models incorporating, as fixed effects, the
four types of errors (lex, miss, morph and reo) and
their pairwise interactions (the product of the sin-
gle error log counts), while their random struc-
ture depends on each specific experiment. For
the experiments we rely on the R language (R
Core Team, 2013) implementation of linear mixed
model in the lme4 library (Bates et al., 2014).
We assess the quality of our mixed linear mod-
els (MLM) by comparing their prediction capabil-
ity with a sequence of simpler linear models in-
cluding only fixed effects. In particular, we built
five univariate models and two multivariate mod-
els. The univariate models use as covariates, re-
spectively, the sum of all error types (baseline),
and each of the four types of errors (lex, miss,
morph and reo). The two multivariate models in-
clude all the four error types, considering them
without interactions (FLM w/o Interact.) and with
interactions (FLM).
Prediction performance is computed in terms of
Mean Absolute Error (MAE),
4
which we estimate
by averaging over 1,000 random splits of the data
in 90% training and 10% test. In particular, for the
human quality classes we pick the integer between
1-5 that is closest to the predicted value.
5.1 Errors vs. Quality Judgements
The response variable we target in this experiment
is the quality score produced by human annotators.
Our measurements follow a typical within-subject
design in which all the 3 annotators are exposed
to the same conditions (levels of the independent
variables), corresponding in our case to perfectly
balanced observations from 2 MT systems and N
sentences. This setting results in repeated or clus-
tered observations (thus violating independence)
corresponding to groups which naturally identify
possible random effects,
5
namely the annotators
(3 levels with 2xN observations each), the systems
(2 levels and 3xN observations each), and the sen-
rors follows a log-scale law: e.g. more sensitive to variations
in the interval [1-10] that in the interval [30-40].
4
MAE is calculated as the average of the absolute errors
|f
i
? y
i
|, where f
i
is the prediction of the model and y
i
the
true value for the i
th
instance. As it is a measure of error,
lower MAE scores indicate that our predictions are closer to
the true values of each test instance.
5
In all our experiments, random effects are limited to ran-
dom shifts since preliminary experiments also including ran-
dom slopes did not provide consistent results.
1648
Model ENZH ENAR ENRU
baseline 0.58 0.73 0.67
lex 0.67 0.78 0.72
miss 0.72 0.89 0.74
morph 0.72 0.89 0.74
reo 0.70 0.82 0.76
FLM w/o Interact. 0.59 0.77 0.65
FLM 0.57 0.72 0.63
MLM 0.53 0.61 0.61
Table 3: Prediction capability of human judge-
ments (MAE).
tences (N levels with 6 observations each). In prin-
ciple, such random effects permit to remove sys-
tematic biases of individual annotators, single sys-
tems and even single sentences, which are mod-
elled as random variables sampled from distinct
populations.
Table 3 shows a comparison of the prediction
capability of the mixed model
6
with simpler ap-
proaches. While the good performance achieved
by our strong baseline cannot be outperformed
by separately counting the number of errors of a
single type, lower MAE results are obtained by
methods based on multivariate analysis. Among
them, FLM brings the first consistent improve-
ments over the baseline by considering error in-
teractions, while MLM leads to the lowest MAE
due to the addition of random effects. The impor-
tance of random effects is particularly evidenced
by ENAR (12 points below the baseline). Indeed,
as discussed in ?4.1, for this language combina-
tion human annotators show the lowest agreement
score. This variability, which hides the smaller
differences in systems? behaviour, demonstrates
the importance of accounting for the erratic fac-
tors that might influence empirical observations in
a given setting. The good performance achieved
by MLM, combined with their high descriptive
power,
7
motivates their adoption in our study.
Concerning the analysis of error impact, Ta-
ble 4 shows the statistically significant coefficients
for the full-fledged MLM models for each trans-
lation direction. By default, all reported coeffi-
cients have p-values ? 10
?4
, while those marked
with ? and ? have respectively p-values ? 10
?3
and ? 10
?2
. Slope coefficients basically show
6
Note that the mixed model used in prediction does not in-
clude the random effect on sentences since the training sam-
ples do not guarantee sufficient observations for each test sen-
tence.
7
Note that the strong baseline used for comparison is not
capable to describe the contribution of the different error
types.
Error ENZH ENAR ENRU
Intercept 4.29 3.79
?
4.21
lex -1.27 -0.96 -1.12
miss -1.76 -0.90 -1.30
morph -0.48
?
-0.83 -0.51
reo -1.01 -0.75 -0.18
lex:miss 1.00 0.39 0.68
lex:morph - 0.29 0.32
lex:reo 0.50 0.21 -
miss:morph - 0.35 -
miss:reo 0.54 0.33 -
morph:reo - 0.37 -
Table 4: Effect of translation errors on MT qual-
ity perception on all judged sentences. Reported
coefficients (?) are all statistically significant with
p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
and
?
(p ? 10
?2
).
the impact of different error types (alone and in
combination) on human quality scores. Those that
are not statistically significant are omitted as they
do not increase the fitting capability of our model.
As can be seen from the table, such impact varies
across the different language combinations. While
for ENZH and ENRU miss is the error having
the highest impact (highest decrement with respect
to the intercept), the most problematic error for
ENAR is lex. It is interesting to observe that pos-
itive values for error combinations indicate that
their combined impact is lower that the sum of the
impact of the single errors. For instance, while for
ENZH a one-step increment in lex and miss errors
would respectively cause a reduction in the human
judgement of 1.27 and 1.76, their occurrence in
the same sentence would be discounted by 1.00.
This would result in a global judgement of 2.26
(4.29 -1.27 -1.76 +1.00) instead of 1.26. While
for ENAR this phenomenon can be observed for
all error combinations, such discount effects are
not always significant for the other two language
pairs. The existence of discount effects of various
magnitude associated to the different error com-
binations is a novel finding made possible by the
adoption of mixed-effect models.
Another interesting observation is that, in con-
trast with the common belief that the most fre-
quent errors have the highest impact on human
quality judgements, our experiments do not re-
veal such strict correlation (at least for the exam-
ined language pairs). For instance, for ENZH and
ENRU the impact of miss errors is higher than the
impact of other more frequent issues.
1649
BLEU score TER GTM
Model ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
baseline 12.4 9.8 12.2 15.7 13.4 14.4 9.8 10.6 11.5
lex 12.9 10.4 13.0 16.3 13.8 14.9 9.7 10.9 12.1
miss 13.8 10.5 14.1 17.3 14.2 16.4 10.5 11.1 13.2
morph 13.9 10.3 13.6 17.5 13.8 16.3 10.5 10.9 13.1
reo 13.7 10.5 14.0 17.4 14.1 16.3 10.4 11.1 13.1
FLM w/o Interact. 12.9 9.9 12.2 16.3 13.5 14.4 9.7 10.7 11.7
FLM 12.3 9.7 12.1 15.6 13.4 14.3 9.4 10.6 11.6
MLM 10.8 9.5 12.0 14.7 13.0 14.2 8.9 10.5 11.6
Table 5: Prediction capability of BLEU score, TER and GTM (MAE).
5.2 Errors vs. Automatic Metrics
In this experiment, the response variable is an au-
tomatic metric which is computed on a sample of
MT outputs (which are again perfectly balanced
over systems and sentences) and a set of reference
translations. As no subjects are involved in the ex-
periment, random variability is assumed to come
from the involved systems, the tested sentences,
and the unknown missing link between the covari-
ates (error types) and the response variable which
is modelled by the residual noise. Notice that,
in this case, the random effect on the sentences
also incorporates in some sense the randomness
of the corresponding reference translations, which
are themselves representatives of larger samples.
The prediction capability of the mixed model,
in comparison with the simpler ones, is reported
in Table 5. Also in this case, the low MAE
achieved by the baseline is out of the reach of uni-
variate methods. Again, small improvements are
brought by FLM when considering error interac-
tions, whereas the most visible gains are achieved
by MLM due to their control of random effects.
This is more evident for some language combina-
tions and can be explained by the differences in
systems? performance, a variability factor easily
absorbed by random effects. Indeed, the largest
MAE decrements over the baseline are always ob-
served for ENZH (for which the overall mean re-
sults reported in Table 2 show the largest dif-
ferences) and the smallest decrements relate to
language/metric combinations where systems? be-
haviour is more similar (e.g. ENRU/GTM).
Concerning the analysis of error impact, Table
6 shows how different error types (alone and in
combination) influence performance results mea-
sured with automatic metrics. To ease interpre-
tation of the reported figures we also show Pear-
son and Spearman correlations of each set of coef-
ficients (excluding intercept estimates) with their
corresponding coefficients reported in Table 4. In
fact, our primary interest in this experiment is to
see which metrics show a sensitivity to specific er-
ror types similar to human perception. As we can
see, the coefficients for each metric significantly
vary depending on the language, for the simple
reason that also the distribution and co-occurrence
of errors vary significantly across the different lan-
guages and MT systems. Remarkably, for some
translation directions, some of the metrics show
a sensitivity to errors that is very similar to that
of human judges. In particular, BLEU for ENZH
and ENAR, and GTM for ENZH show a very high
correlation with the human sensitivity to transla-
tion errors, with Pearson correlation coefficient ?
0.97. For ENRU, the best Pearson correlation is
instead achieved by TER (-0.78).
Besides these general observations, a closer
look at the reported scores brings additional find-
ings. In three cases (BLEU for ENZH, GTM for
ENZH and ENAR) the analysed metrics are most
sensitive to the same error type that has the high-
est influence on human judgements (according to
Table 4, these are miss for ENZH and ENRU, lex
for ENAR). On the contrary, in one case (TER for
ENZH) the analysed metric is insensitive to the er-
ror type (miss) which has the highest impact on hu-
man quality scores. From a practical point of view,
these remarks provide useful indications about the
appropriateness of each metric to highlight the de-
ficiencies of a specific system and to measure im-
provements targeting specific issues. As a rule of
thumb, for instance, to measure improvements of
an ENZH system with respect to missing words,
it would be more advisable to use BLEU or GTM
instead of TER.
8
8
Note that this conclusion holds for our data sample, in
which different types of errors co-occur and only one refer-
ence translation is available. In such conditions, our regres-
sion model shows that TER is not influenced by miss errors in
a statistically significant way. This does not mean that TER
is insensitive to missing words when occurring in isolation,
1650
BLEU score TER GTM
Error ENZH ENAR ENRU ENZH ENAR ENRU ENZH ENAR ENRU
Intercept 60.55
2
38.45
?
51.73 32.41
2
52.25
?
33.4
?
83.57
?
60.11
?
75.38
lex -18.78 -9.25 -16.57 16.87 9.66 18.45 -13.63 -7.60 -16.13
miss -23.20 -10.41 -6.75 - - 8.24 -14.87 - -5.98
morph - -9.97 -12.65 - 8.90 11.41 - -6.60 -10.42
reo -13.27 -7.62 -10.57 14.44 9.81 6.39 -7.29 -5.50 -7.03
lex:miss 14.37 4.97
?
- - - - 8.24
?
- -
lex:morph - - 5.27
?
- - -5.22
?
- - 4.92
lex:reo 8.57 3.57
?
5.40
?
-7.24
?
-4.35
?
- 5.46 3.22
?
3.65
2
miss:morph - 4.44
?
- - - - - - -
miss:reo 6.74
?
- 4.30 - - -6.38
?
5.07
?
- 4.71
?
morph:reo - 3.81
?
- - -4.97
?
- - 2.57
?
-
Pearson 0.98 0.97 0.70 -0.58 -0.78 -0.78 0.98 0.78 0.74
Spearman 0.97 0.91 0.73 -0.57 -0.59 -0.80 0.97 0.59 0.76
Table 6: Effect of translation errors on BLEU score, TER and GTM on all judged sentences and correla-
tion with their corresponding effects on human quality scores (from Table 4). Reported coefficients (?)
are statistically significant with p ? 10
?4
, except those marked with
?
(p ? 10
?3
),
?
(p ? 10
?2
) and
2
(p ? 10
?1
).
Similar considerations also apply to the analysis
of the impact of error combinations. The same dis-
count effects that we noticed when analysing the
impact of errors? co-occurrence on human percep-
tion (?5.1) are evidenced, with different degrees of
sensitivity, by the automatic metrics. While some
of them substantially reflect human response (e.g.
BLEU and GTM for ENZH), in some cases we
observe either the insensitivity to specific combi-
nations (mostly for ENAR), or a higher sensitivity
compared to the values measured for human as-
sessors (mostly for ENRU, where the impact of
miss:reo combinations is discounted - hence un-
derestimated - by all the metrics).
Despite such small differences, the coherence of
our results with previous findings (?5.1) suggests
the reliability of the applied method. Complet-
ing the picture along the side of the MT evalua-
tion triangle which connects error annotations and
automatic metrics, our findings contribute to shed
light on the existing relationships between transla-
tion errors, their interaction, and the sensitivity of
widely used automatic metrics.
6 Conclusion
We investigated the MT evaluation triangle (hav-
ing as corners automatic metrics, human quality
judgements and error annotations) along the two
less explored sides, namely: i) the relation be-
tween MT errors and human quality judgements
but that TER becomes less sensitive to such errors when they
co-occur with other types of errors. Overall, our experiments
show that when MT outputs contain more than one error type,
automatic metrics show different levels of sensitivity to each
specific error type.
and ii) the relation between MT errors and auto-
matic metrics. To this aim we employed a ro-
bust statistical analysis framework based on lin-
ear mixed-effects models (the first contribution of
the paper), which have a higher descriptive power
than simpler methods based on the raw count of
translation errors and are less artificial compared
to previous statistically-grounded approaches.
Working on three translation directions having
Chinese, Arabic and Russian as target (our second
contribution), we analysed error-annotated trans-
lations considering the impact of specific errors
(alone and in combination) and accounting for the
variability of the experimental set-up that origi-
nated our empirical observations. This led us to
interesting findings specific to each language pair
(third contribution). Concerning the relation be-
tween MT errors and quality judgements, we have
shown that: i) the frequency of errors of a given
type does not correlate with human preferences,
ii) errors having the highest impact can be pre-
cisely isolated and iii) the impact of error inter-
actions is often subject to measurable and previ-
ously unknown ?discount? effects. Concerning the
relation between MT errors and automatic met-
rics (BLEU, TER and GTM), our analysis evi-
denced significant differences in the sensitivity of
each metric to different error types. Such differ-
ences provide useful indications about the most
appropriate metric to assess system improvements
with respect to specific weaknesses. If learning
from errors is a crucial aspect of improving exper-
tise, our method and the resulting empirical find-
ings represent a significant contribution towards a
1651
more informed approach to system development,
improvement and evaluation.
Acknowledgements
This work has been partially supported by the EC-
funded project MateCat (ICT-2011.4.2-287688).
References
Harald R. Baayen, Douglas J. Davidson, and Dou-
glas M. Bates. 2008. Mixed-effects modeling with
crossed random effects for subjects and items. Jour-
nal of memory and language, 59(4):390?412.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Douglas Bates, Martin Maechler, Ben Bolker, and
Steven Walker, 2014. lme4: Linear mixed-effects
models using Eigen and S4. R package version 1.1-
6.
Fr?ed?eric Blain, Jean Senellart, Holger Schwenk, Mirko
Plitt, and Johann Roturier. 2011. Qualitative analy-
sis of post-editing for high quality machine transla-
tion. In Asia-Pacific Association for Machine Trans-
lation (AAMT), editor, Machine Translation Summit
XIII, Xiamen (China), 19-23 sept.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Herbert H. Clark. 1973. The language-as-fixed-effect
fallacy: A critique of language statistics in psycho-
logical research. Journal of verbal learning and ver-
bal behavior, 12(4):335?359.
Mireia Farr?us, Marta R. Costa-juss`a, and Maja
Popovi?c. 2012. Study and correlation analysis of
linguistic, perceptual, and automatic machine trans-
lation evaluations. J. Am. Soc. Inf. Sci. Technol.,
63(1):174?184, January.
Mireia Farr?us Cabeceran, Marta Ruiz Costa-Juss`a,
Jos?e Bernardo Mari?no Acebal, Jos?e Adri?an
Rodr??guez Fonollosa, et al. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th
Annual Conference of the European Association for
Machine Translation (EAMT).
Mary Flanagan. 1994. Error classification for mt eval-
uation. In Technology Partnerships for Crossing the
Language Barrier: Proceedings of the First Confer-
ence of the Association for Machine Translation in
the Americas, pages 65?72.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5).
Christian Girardi, Luisa Bentivogli, Mohammad Amin
Farajian, and Marcello Federico. 2014. Mt-equal:
a toolkit for human assessment of machine trans-
lation output. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 120?
123, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.
Sharon Goldwater, Daniel Jurafsky, and Christopher D.
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181?200.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, pages 439?448. ACM.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 503?511, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Katrin Kirchhoff, Daniel Capurro, and Anne M. Turner.
2013. A conjoint analysis framework for evaluating
user preferences in machine translation. Machine
Translation, pages 1?17.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33 (1):159?174.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. Assess-
ing Post-Editing Efficiency in a Realistic Translation
Environment. In Michel Simard Sharon O?Brien
and Lucia Specia (eds.), editors, Proceedings of MT
Summit XIV Workshop on Post-editing Technology
and Practice, pages 83?91, Nice, France.
Chin-Yew Lin and Franz Josef Och. 2004. Orange:
a method for evaluating automatic evaluation met-
rics for machine translation. In Proceedings of Col-
ing 2004, pages 501?507, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Arle Lommel, Aljoscha Burchardt, Maja Popovi?c, Kim
Harris, Eleftherios Avramidis, and Hans Uszkoreit.
1652
2014. Using a new analytic measure for the anno-
tation and analysis of mt errors on real data. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Sharon O?Brien. 2011. Cognitive Explorations of
Translation. Bloomsbury Studies in Translation.
Bloomsbury Academic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Research Report
RC22176, IBM Research Division, Thomas J. Wat-
son Research Center.
Maja Popovi?c and Hermann Ney. 2011. Towards au-
tomatic error analysis of machine translation output.
Comput. Linguist., 37(4):657?688, December.
Maja Popovic, Eleftherios Avramidis, Aljoscha Bur-
chardt, Sabine Hunsicker, Sven Schmeier, Cindy
Tscherwinka, David Vilar, and Hans Uszkoreit.
2013. Learning from human judgments of machine
translation output. In Proceedings of the MT Summit
XIV. Proceedings of MT Summit XIV.
Maja Popovi?c, Arle Lommel, Aljoscha Burchardt,
Eleftherios Avramidis, and Hans Uszkoreit. 2014.
Relations between different types of post-editing op-
erations, cognitive effort and temporal effort. In
Proceedings of the 17th Conference of the Euro-
pean Association for Machine Translation (EAMT),
Dubrovnik, Croatia, June.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 71?75, Montr?eal, Canada, June. Associ-
ation for Computational Linguistics.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
Nick Ruiz and Marcello Federico. 2014. Assessing the
Impact of Speech Recognition Errors on Machine
Translation Quality. In 11th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), Vancouver, BC, Canada.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In 5th Conference of the Association for Machine
Translation in the Americas (AMTA), Boston, Mas-
sachusetts, August.
Sara Stymne and Lars Ahrenberg. 2012. On
the practice of error analysis for machine trans-
lation evaluation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC?12), Is-
tanbul, Turkey, may. European Language Resources
Association (ELRA).
Irina Temnikova. 2010. Cognitive evaluation approach
for a controlled language post-editing experiment.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Joseph P. Turian, I. Dan Melamed, and Luke Shen.
2003. Evaluation of machine translation and its
evaluation. In Proceedings of the MT Summit IX.
Gerhard Tutz and Wolfgang Hennevogl. 1996. Ran-
dom effects in ordinal regression models. Computa-
tional Statistics & Data Analysis, 22(5):537?557.
David Vilar, Jia Xu, Luis Fernando dHaro, and Her-
mann Ney. 2006. Error analysis of statistical ma-
chine translation output. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC?06), pages 697?702.
1653
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 399?407,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Semeval-2012 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
FBK-irst
Trento, Italy
mehdad@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the first round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2012. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (10 teams,
92 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization.
Cross-linguality represents a dimension of the TE
recognition problem that has been so far only par-
tially investigated. The great potential for integrat-
ing monolingual TE recognition components into
NLP architectures has been reported in several ar-
eas, including question answering, information re-
trieval, information extraction, and document sum-
marization. However, mainly due to the absence of
cross-lingual textual entailment (CLTE) recognition
components, similar improvements have not been
achieved yet in any cross-lingual application. The
CLTE task aims at prompting research to fill this
gap. Along such direction, research can now ben-
efit from recent advances in other fields, especially
machine translation (MT), and the availability of: i)
large amounts of parallel and comparable corpora in
many languages, ii) open source software to com-
pute word-alignments from parallel corpora, and iii)
open source software to set up MT systems. We
believe that all these resources can positively con-
tribute to develop inference mechanisms for multi-
lingual data.
Content synchronization represents a challenging
application scenario to test the capabilities of ad-
vanced NLP systems. Given two documents about
the same topic written in different languages (e.g.
Wiki pages), the task consists of automatically de-
tecting and resolving differences in the information
they provide, in order to produce aligned, mutually
enriched versions of the two documents. Towards
this objective, a crucial requirement is to identify the
information in one page that is either equivalent or
novel (more informative) with respect to the content
of the other. The task can be naturally cast as an
entailment recognition problem, where bidirectional
and unidirectional entailment judgments for two text
fragments are respectively mapped into judgments
about semantic equivalence and novelty. Alterna-
tively, the task can be seen as a machine translation
evaluation problem, where judgments about seman-
tic equivalence and novelty depend on the possibility
to fully or partially translate a text fragment into the
other.
399
Figure 1: ?bidirectional?, ?forward?, ?backward? and
?no entailment? judgments for SP/EN CLTE pairs.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as useful evidence to
draw entailment decisions), the standard sentence
and word alignment programs used in SMT offer a
strong baseline for CLTE. However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and large
room for mutual improvement.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments (see Figure 1 for
Spanish/English examples of each judgment):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in both direc-
tions;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset created for
the first round of the task.
3 Dataset description
Four CLTE corpora have been created for the fol-
lowing language combinations: Spanish/English
(SP-EN), Italian/English (IT-EN), French/English
(FR-EN), German/English (DE-EN). The datasets
are released in the XML format shown in Figure 1.
3.1 Data collection and annotation
The dataset was created following the crowdsourc-
ing methodology proposed in (Negri et al, 2011),
which consists of the following steps:
1. First, English sentences were manually ex-
tracted from copyright-free sources (Wikipedia
and Wikinews). The selected sentences repre-
sent one of the elements (T1) of each entail-
ment pair;
2. Next, each T1 was modified through crowd-
sourcing in various ways in order to ob-
tain a corresponding T2 (e.g. introduc-
ing meaning-preserving lexical and syntactic
changes, adding and removing portions of
text);
3. Each T2 was then paired to the original T1,
and the resulting pairs were annotated with one
of the four entailment judgments. In order to
reduce the correlation between the difference
in sentences? length and entailment judgments,
400
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.1 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgments, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were manually checked and corrected
when necessary. Only pairs with agreement between
two expert annotators were retained. The final result
is a multilingual parallel entailment corpus, where
T1s are in 5 different languages (i.e. English, Span-
ish, German, Italian, and French), and T2s are in En-
glish. It?s worth mentioning that the monolingual
English corpus, a by-product of our data collection
methodology, will be publicly released as a further
contribution to the research community.2
3.2 Dataset statistics
Each dataset consists of 1,000 pairs (500 for training
and 500 for test), balanced across the four entail-
ment judgments (bidirectional, forward, backward,
and no entailment).
For each language combination, the distribu-
tion of the four entailment judgments according to
length diff is shown in Figure 2. Vertical bars rep-
resent, for each length diff value, the proportion
of pairs belonging to the four entailment classes.
As can be seen, the length diff constraint applied
to the length difference in the monolingual English
1Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
2The cross-lingual datasets are already available for research
purposes at http://www.celct.it/resourcesList.
php. The monolingual English dataset will be publicly released
to non participants in July 2012.
pairs (step 3 of the creation process) is substantially
reflected in the cross-lingual datasets for all lan-
guage combinations. In fact, as shown in Table 1,
the majority of the pairs is always included in the
same length diff range (approximately [-5,+5]) and,
within this range, the distribution of the four classes
is substantially uniform. Our assumption is that such
data distribution makes entailment judgments based
on mere surface features such as sentence length in-
effective, thus encouraging the development of alter-
native, deeper processing strategies.
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
Table 1: CLTE pairs distribution within the -5/+5
length diff range.
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgments re-
turned by each system with those manually assigned
by human annotators. The metric used for systems?
ranking is accuracy over the whole test set, i.e. the
number of correct judgments out of the total number
of judgments in the test set. Additionally, we calcu-
lated precision, recall, and F1 measures for each of
the four entailment judgment categories taken sep-
arately. These scores aim at giving participants the
possibility to gain clearer insights into their system?s
behavior on the entailment phenomena relevant to
the task.
For each language combination, two baselines
considering the length difference between T1 and T2
have been calculated (besides the trivial 0.25 accu-
racy score obtained by assigning each test pair in the
balanced dataset to one of the four classes):
? Composition of binary judgments (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary entailment
decisions (?YES?, ?NO?). The classifier uses
length(T1)/length(T2) as a single feature to
check for entailment from T1 to T2, and
length(T2)/length(T1) for the opposite direc-
tion. For each test pair, the unidirectional
401
010
20
30
40
50
60
70
80
-21 -18 -15 -12 -9 -6 -3 0 3 6 9
no_entailmentforwardbidirectionalbackward
(a) SP-EN
0
10
20
30
40
50
60
70
80
-17 -14 -11 -8 -5 -2 1 4 7 10
no_entailmentforwardbidirectionalbackward
(b) IT-EN
0
10
20
30
40
50
60
70
80
90
-21 -18 -15 -12 -9 -6 -3 0 3 6 9 12
no_entailmentforwardbidirectionalbackward
(c) FR-EN
0
10
20
30
40
50
60
70
80
90
-14 -10 -7 -4 -1 2 5 8 11 14 17
no_entailmentforwardbidirectionalbackward
(d) DE-EN
Figure 2: CLTE pairs distribution for different length diff values across all datasets.
judgments returned by the two classifiers are
composed into a single multi-directional judg-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgments.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using a
linear kernel with default parameters. Baseline re-
sults are reported in Table 2.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages.
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.34 0.39 0.39 0.40
Multi-class 0.43 0.44 0.42 0.42
Table 2: Baseline accuracy results.
5 Submitted runs and results
Participants were allowed to submit up to five runs
for each language combination. A total of 17 teams
registered to participate in the task and downloaded
the training set. Out of them, 12 downloaded the
test set and 10 (including one of the task organizers)
submitted valid runs. Eight teams produced submis-
sions for all the language combinations, while two
teams participated only in the SP-EN task. In total,
92 runs have been submitted and evaluated (29 for
SP-EN, and 21 for each of the other language pairs).
402
Despite the novelty and the difficulty of the problem,
these numbers demonstrate the interest raised by the
task, and the overall success of the initiative.
System name SP-EN IT-EN FR-EN DE-EN
BUAP run1 0.350 0.336 0.334 0.330
BUAP run2 0.366 0.344 0.342 0.268
celi run1 0.276 0.278 0.278 0.280
celi run2 0.336 0.338 0.300 0.352
celi run3 0.322 0.334 0.298 0.350
celi run4 0.268 0.280 0.280 0.274
DirRelCond3 run1 0.300 0.280 0.362 0.336
DirRelCond3 run2 0.300 0.284 0.360 0.336
DirRelCond3 run3 0.300 0.338 0.384 0.364
DirRelCond3 run4 0.344 0.316 0.384 0.374
FBK run1* 0.502 - - -
FBK run2* 0.490 - - -
FBK run3* 0.504 - - -
FBK run4* 0.500 - - -
HDU run1 0.630 0.554 0.564 0.558
HDU run2 0.632 0.562 0.570 0.552
ICT run1 0.448 0.454 0.456 0.460
JU-CSE-NLP run1 0.274 0.316 0.288 0.262
JU-CSE-NLP run2 0.266 0.326 0.294 0.296
JU-CSE-NLP run3 0.272 0.314 0.296 0.264
Sagan run1 0.342 0.352 0.346 0.342
Sagan run2 0.328 0.352 0.336 0.310
Sagan run3 0.346 0.356 0.330 0.332
Sagan run4 0.340 0.330 0.310 0.310
SoftCard run1 0.552 0.566 0.570 0.550
UAlacant run1 LATE 0.598 - - -
UAlacant run2 0.582 - - -
UAlacant run3 LATE 0.510 - - -
UAlacant run4 0.514 - - -
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 3: Accuracy results (92 runs) over the 4 lan-
guage combinations. Highest, average, median and low-
est scores are calculated considering the best run for each
team (*task organizers? system).
Accuracy results are reported in Table 3. As can
be seen from the table, overall accuracy scores are
quite different across language pairs, with the high-
est result on SP-EN (0.632), which is considerably
higher than the highest score on DE-EN (0.558).
This might be due to the fact that most of the partic-
ipating systems rely on a ?pivoting? approach that
addresses CLTE by automatically translating T1 in
the same language of T2 (see Section 6). Regard-
ing the DE-EN dataset, pivoting methods might be
penalized by the lower quality of MT output when
German T1s are translated into English.
The comparison with baselines results leads to in-
teresting observations. First of all, while all systems
significantly outperform the lowest 1-class baseline
(0.25), both other baselines are surprisingly hard to
beat. This shows that, despite the effort in keep-
ing the distribution of the entailment classes uni-
form across different length diff values, eliminating
the correlation between sentences? length and cor-
rect entailment decisions is difficult. As a conse-
quence, although disregarding semantic aspects of
the problem, features considering such information
are quite effective.
In general, systems performed better on the SP-
EN dataset, with most results above the binary base-
line (8 out of 10), and half of the systems above the
multi-class baseline. For the other language pairs
the results are lower, with only 3 out of 8 partici-
pants above the two baselines in all datasets. Aver-
age results reflect this situation: the average scores
are always above the binary baseline, whereas only
the SP-EN average result is higher than the multi-
class baseline(0.44 vs. 0.43).
To better understand the behaviour of each sys-
tem (also in relation to the different language com-
binations), Table 4 provides separate precision, re-
call, and F1 scores for each entailment judgment,
calculated over the best runs of each participating
team. Overall, the results suggest that the ?bidi-
rectional? and ?no entailment? categories are more
problematic than ?forward? and ?backward? judg-
ments. For most datasets, in fact, systems? perfor-
mance on ?bidirectional? and ?no entailment? is sig-
nificantly lower, typically on recall. Except for the
DE-EN dataset (more problematic on ?forward?),
also average F1 results on these judgments are lower.
This might be due to the fact that, for all datasets, the
vast majority of ?bidirectional? and ?no entailment?
judgments falls in a length diff range where the dis-
tribution of the four classes is more uniform (see
Figure 2).
Similar reasons can justify the fact that ?back-
ward? entailment results are consistently higher on
all datasets. Compared with ?forward? entailment,
these judgments are in fact less scattered across the
entire length diff range (i.e. less intermingled with
the other classes).
403
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods as-
sign entailment judgments without preliminary
translation.
? Composition of binary judgments vs. Multi-
class classification. Compositional approaches
map unidirectional entailment decisions taken
separately into single judgments (similar to the
Binary baseline in Section 4). Methods based
on multi-class classification directly assign one
of the four entailment judgments to each test
pair (similar to our Multi-class baseline).
Concerning the former dimension, most of the
systems (6 out of 10) adopted a pivoting approach,
relying on Google Translate (4 systems), Microsoft
Bing Translator (1), or a combination of Google,
Bing, and other MT systems (1) to produce English
T2s. Regarding the latter dimension, the composi-
tional approach was preferred to multi-class classi-
fication (6 out of 10). The best performing system
relies on a ?hybrid? approach (combining monolin-
gual and cross-lingual alignments) and a compo-
sitional strategy. Besides the frequent recourse to
MT tools, other resources used by participants in-
clude: on-line dictionaries for the translation of sin-
gle words, word alignment tools, part-of-speech tag-
gers, NP chunkers, named entity recognizers, stem-
mers, stopwords lists, and Wikipedia as an external
multilingual corpus. More in detail:
BUAP [pivoting, compositional] (Vilarin?o et al,
2012) adopts a pivoting method based on translating
T1 into the language of T2 and vice versa (Google
Translate3 and the OpenOffice Thesaurus4). Simi-
larity measures (e.g. Jaccard index) and rules are
3http://translate.google.com/
4http://extensions.services.openoffice.
org/en/taxonomy/term/233
respectively used to annotate the two resulting sen-
tence pairs with entailment judgments and combine
them in a single decision.
CELI [cross lingual, compositional & multi-
class] (Kouylekov, 2012) uses dictionaries for word
matching, and a multilingual corpus extracted from
Wikipedia for term weighting. Word overlap and
similarity measures are then used in different ap-
proaches to the task. In one run (Run 1), they are
used to train a classifier that assigns separate en-
tailment judgments for each direction. Such judg-
ments are finally composed into a single one for each
pair. In the other runs, the same features are used for
multi-class classification.
DirRelCond3 [cross lingual, compositional]
(Perini, 2012) uses bilingual dictionaries (Freedict5
and WordReference6) to translate content words into
English. Then, entailment decisions are taken com-
bining directional relatedness scores between words
in both directions (Perini, 2011).
FBK [cross lingual, compositional & multi-
class] (Mehdad et al, 2012a) uses cross-lingual
matching features extracted from lexical phrase ta-
bles, semantic phrase tables, and dependency rela-
tions (Mehdad et al, 2011; Mehdad et al, 2012b;
Mehdad et al, 2012c). The features are used for
multi-class and binary classification using SVMs.
HDU [hybrid, compositional] (Wa?schle and
Fendrich, 2012) uses a combination of binary clas-
sifiers for each entailment direction. The classifiers
use both monolingual alignment features based on
METEOR (Banerjee and Lavie, 2005) alignments
(translations obtained from Google Translate), and
cross-lingual alignment features based on GIZA++
(Och and Ney, 2000) (word alignments learned on
Europarl).
ICT [pivoting, compositional] (Meng et al,
2012) adopts a pivoting method (using Google
Translate and an in-house hierarchical MT system),
and the open source EDITS system (Kouylekov and
Negri, 2010) to calculate similarity scores between
monolingual English pairs. Separate unidirectional
entailment judgments obtained from binary classi-
fier are combined to return one of the four valid
CLTE judgments.
5http://www.freedict.com/
6http://www.wordreference.com/
404
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP spa-eng run2 0,337 0,664 0,447 0,406 0,568 0,473 0,333 0,088 0,139 0,391 0,144 0,211
celi spa-eng run2 0,324 0,368 0,345 0,411 0,368 0,388 0,306 0,296 0,301 0,312 0,312 0,312
DirRelCond3 spa-eng run4 0,358 0,608 0,451 0,444 0,448 0,446 0,286 0,032 0,058 0,243 0,288 0,264
FBK spa-eng run3 0,515 0,704 0,595 0,546 0,568 0,557 0,447 0,304 0,362 0,482 0,440 0,460
HDU spa-eng run2 0,607 0,656 0,631 0,677 0,704 0,690 0,602 0,592 0,597 0,643 0,576 0,608
ICT spa-eng run1 0,750 0,240 0,364 0,440 0,472 0,456 0,395 0,560 0,464 0,436 0,520 0,474
JU-CSE-NLP spa-eng run1 0,211 0,288 0,243 0,272 0,296 0,284 0,354 0,232 0,280 0,315 0,280 0,297
Sagan spa-eng run3 0,225 0,200 0,212 0,269 0,224 0,245 0,418 0,448 0,432 0,424 0,512 0,464
SoftCard spa-eng run1 0,602 0,616 0,609 0,650 0,624 0,637 0,471 0,448 0,459 0,489 0,520 0,504
UAlacant spa-eng run1 LATE 0,689 0,568 0,623 0,645 0,728 0,684 0,507 0,544 0,525 0,566 0,552 0,559
AVG. 0,462 0,491 0,452 0,476 0,5 0,486 0,412 0,354 0,362 0,43 0,414 0,415
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP ita-eng run2 0,324 0,456 0,379 0,327 0,672 0,440 0,538 0,056 0,101 0,444 0,192 0,268
celi ita-eng run2 0,349 0,360 0,354 0,455 0,36 0,402 0,294 0,320 0,307 0,287 0,312 0,299
DirRelCond3 ita-eng run3 0,323 0,488 0,389 0,480 0,288 0,360 0,331 0,368 0,348 0,268 0,208 0,234
HDU ita-eng run2 0,564 0,600 0,581 0,628 0,648 0,638 0,551 0,520 0,535 0,500 0,480 0,490
ICT ita-eng run1 0,661 0,296 0,409 0,554 0,368 0,442 0,427 0,448 0,438 0,383 0,704 0,496
JU-CSE-NLP ita-eng run2 0,240 0,280 0,258 0,339 0,480 0,397 0,412 0,280 0,333 0,359 0,264 0,304
Sagan ita-eng run3 0,306 0,296 0,301 0,252 0,216 0,233 0,395 0,512 0,446 0,455 0,400 0,426
SoftCard ita-eng run1 0,602 0,616 0,609 0,617 0,696 0,654 0,560 0,448 0,498 0,481 0,504 0,492
AVG. 0,421 0,424 0,410 0,457 0,466 0,446 0,439 0,369 0,376 0,397 0,383 0,376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP fra-eng run2 0,447 0,272 0,338 0,291 0,760 0,420 0,250 0,016 0,030 0,449 0,320 0,374
celi fra-eng run2 0,316 0,296 0,306 0,378 0,360 0,369 0,270 0,296 0,282 0,244 0,248 0,246
DirRelCond3 fra-eng run3 0,393 0,576 0,468 0,441 0,512 0,474 0,387 0,232 0,290 0,278 0,216 0,243
HDU fra-eng run2 0,564 0,672 0,613 0,582 0,736 0,650 0,676 0,384 0,490 0,500 0,488 0,494
ICT fra-eng run1 0,750 0,192 0,306 0,517 0,496 0,506 0,385 0,656 0,485 0,444 0,480 0,462
JU-CSE-NLP fra-eng run3 0,215 0,208 0,211 0,289 0,296 0,292 0,341 0,496 0,404 0,333 0,184 0,237
Sagan fra-eng run1 0,244 0,168 0,199 0,297 0,344 0,319 0,394 0,568 0,466 0,427 0,304 0,355
SoftCard fra-eng run1 0,551 0,608 0,578 0,649 0,696 0,672 0,560 0,488 0,521 0,513 0,488 0,500
AVG. 0,435 0,374 0,377 0,431 0,525 0,463 0,408 0,392 0,371 0,399 0,341 0,364
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP deu-eng run1 0,395 0,120 0,184 0,248 0,224 0,235 0,344 0,688 0,459 0,364 0,288 0,321
celi deu-eng run2 0,347 0,416 0,378 0,402 0,392 0,397 0,339 0,312 0,325 0,319 0,288 0,303
DirRelCond3 deu-eng run4 0,429 0,312 0,361 0,408 0,552 0,469 0,367 0,320 0,342 0,298 0,312 0,305
HDU deu-eng run1 0,559 0,528 0,543 0,600 0,696 0,644 0,540 0,488 0,513 0,524 0,520 0,522
ICT deu-eng run1 0,718 0,224 0,341 0,493 0,552 0,521 0,390 0,512 0,443 0,439 0,552 0,489
JU-CSE-NLP deu-eng run2 0,182 0,048 0,076 0,307 0,496 0,379 0,315 0,560 0,403 0,233 0,080 0,119
Sagan deu-eng run1 0,250 0,168 0,201 0,239 0,256 0,247 0,405 0,600 0,484 0,443 0,344 0,387
SoftCard deu-eng run1 0,568 0,568 0,568 0,611 0,640 0,625 0,521 0,488 0,504 0,496 0,504 0,500
AVG. 0,431 0,298 0,332 0,414 0,476 0,440 0,403 0,496 0,434 0,390 0,361 0,368
Table 4: precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
JU-CSE-NLP [pivoting, compositional] (Neogi
et al, 2012) uses Microsoft Bing translator7 to pro-
duce monolingual English pairs. Separate lexical
mapping scores are calculated (from T1 to T2 and
vice-versa) considering different types of informa-
tion and similarity metrics. Binary entailment de-
7http://www.microsofttranslator.com/
cisions are then heuristically combined into single
decisions.
Sagan [pivoting, multi-class] (Castillo and Car-
denas, 2012) adopts a pivoting method using Google
Translate, and trains a monolingual system based on
a SVM multi-class classifier. A CLTE corpus de-
rived from the RTE-3 dataset is also used as a source
of additional training material.
405
SoftCard [pivoting, multi-class] (Jimenez et al,
2012) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity (computed with
edit-distance)
UAlacant [pivoting, multi-class] (Espla`-Gomis
et al, 2012) exploits translations obtained from
Google Translate, Microsoft Bing translator, and the
Apertium open-source MT platform (Forcada et al,
2011).8 Then, a multi-class SVM classifier is used
to take entailment decisions using information about
overlapping sub-segments as features.
7 Conclusion
Despite the novelty of the problem and the diffi-
culty to capture multi-directional entailment rela-
tions across languages, the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
tion task organized within SemEval-2012 was a suc-
cessful experience. This year a new interesting chal-
lenge has been proposed, a benchmark for four lan-
guage combinations has been released, baseline re-
sults have been proposed for comparison, and a
monolingual English dataset has been produced as
a by-product which can be useful for monolingual
TE research. The interest shown by participants
was encouraging: 10 teams submitted a total of 92
runs for all the language pairs proposed. Overall,
the results achieved on all datasets are encourag-
ing, with best systems significantly outperforming
the proposed baselines. It is worth observing that the
nature of the task, which lies between semantics and
machine translation, led to the participation of teams
coming from both these communities, showing in-
teresting opportunities for integration and mutual
improvement. The proposed approaches reflect this
situation, with teams traditionally working on MT
now dealing with entailment, and teams tradition-
ally participating in the RTE challenges now dealing
with cross-lingual alignment techniques. Our ambi-
tion, for the future editions of the CLTE task, is to
further consolidate the bridge between the semantics
and MT communities.
8http://www.apertium.org/
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The
authors would also like to acknowledge Giovanni
Moretti from CELCT for evaluation scripts and
technical assistance, and the volunteer translators
that contributed to the creation of the dataset:
Mar??a Sol Accossato, Laura Barthe?le?my, Clau-
dia Biacchi, Jane Brendler, Amandine Chantrel,
Hanna Cheda Patete, Ellen Clancy, Rodrigo Damian
Tejeda, Daniela Dold, Valentina Frattini, Debora
Hedy Amato, Geniz Hernandez, Be?ne?dicte Jean-
nequin, Beate Jones, Anne Kauffman, Marcia Laura
Zanoli, Jasmin Lewis, Alicia Lo?pez, Domenico Los-
eto, Sabrina Luja?n Sa?nchez, Julie Mailfait, Gabriele
Mark, Nunzio Pruiti, Lourdes Rey Cascallar, Sylvie
Martlew, Aleane Salas Velez, Monica Scalici, An-
dreas Schwab, Marianna Sicuranza, Chiara Sisler,
Stefano Tordazzi, Yvonne.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72.
Julio Castillo and Marina Cardenas. 2012. Sagan: A
Cross Lingual Textual Entailment system based on
Machine Traslation. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. UAlacant: Using Online
Machine Translation for Cross-Lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Mikel L. Forcada, Ginest??-Rosell Mireia, Nordfalk Jacob,
O?Regan Jim, Ortiz-Rojas Sergio, Pe?rez-Ortiz Juan A.,
Sa?nchez-Mart??nez Felipe, Ram??rez-Sa?nchez Gema,
406
and Tyers Francis M. 2011. Apertium: a Free/Open-
Source Platform for Rule-Based Machine Translation.
Machine Translation, 25(2):127?144. Special Issue:
Free/Open-Source Machine Translation.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality + ML: Learning Adap-
tive Similarity Functions for Cross-lingual Textual En-
tailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2012. CELI: An Experiment with
Cross Language Textual Entailment. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Jose? G. C. de Souza.
2012a. FBK: Cross-Lingual Textual Entailment With-
out Translation. In Proceedings of the 6th Interna-
tional Workshop on Semantic Evaluation (SemEval
2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012b. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012c. Match without a Referee: Evaluating MT Ade-
quacy without Reference Translations. In Proceedings
of the 7th Workshop on Statistical Machine Translation
(WMT 2012).
Fandong Meng, Hao Xiong, and Qun Liu. 2012. ICT:
A Translation based Cross-lingual Textual Entailment.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Snehasis Neogi, Partha Pakray, Sivaji Bandyopadhyay,
and Alexander Gelbukh. 2012. JU-CSE-NLP: Lan-
guage Independent Cross-lingual Textual Entailment
System. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Franz J. Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL 2000).
Alpa?r Perini. 2011. Detecting textual entailment
with conditions on directional text relatedness scores.
Studia Universitatis Babes-Bolyai Series Informatica,
LVI(2):13?18.
Alpa?r Perini. 2012. DirRelCond3: Detecting Textual
Entailment Across Languages With Conditions On Di-
rectional Text Relatedness Scores. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n,
and Esteban Castillo. 2012. BUAP: Lexical and
Semantic Similarity for Cross-lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Features.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
407
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 25?33, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Semeval-2013 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
UBC
Vancouver, Canada
mehdad@cs.ubc.ca
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the second round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2013. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (six teams,
61 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization. Given two texts in different languages, the
cross-lingual textual entailment (CLTE) task con-
sists of deciding if the meaning of one text can be
inferred from the meaning of the other text. Cross-
linguality represents an interesting direction for re-
search on recognizing textual entailment (RTE), es-
pecially due to its possible application in a vari-
ety of tasks. Among others (e.g. question answer-
ing, information retrieval, information extraction,
and document summarization), multilingual content
synchronization represents a challenging application
scenario to evaluate CLTE recognition components
geared to the identification of sentence-level seman-
tic relations.
Given two documents about the same topic writ-
ten in different languages (e.g. Wikipedia pages),
the content synchronization task consists of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions of the two docu-
ments (Monz et al, 2011; Bronner et al, 2012). To-
wards this objective, a crucial requirement is to iden-
tify the information in one page that is either equiv-
alent or novel (more informative) with respect to the
content of the other. The task can be naturally cast
as an entailment recognition problem, where bidi-
rectional and unidirectional entailment judgements
for two text fragments are respectively mapped into
judgements about semantic equivalence and novelty.
The task can also be seen as a machine translation
evaluation problem, where judgements about se-
mantic equivalence and novelty depend on the pos-
sibility to fully or partially translate a text fragment
into the other.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as features contributing
to the entailment decision), the standard sentence
and word alignment programs used in SMT offer
a strong baseline for CLTE (Mehdad et al, 2011;
25
Figure 1: Example of SP-EN CLTE pairs.
Mehdad et al, 2012). However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and of-
fers large room for mutual improvement.
Building on the success of the first CLTE evalua-
tion organized within SemEval-2012 (Negri et al,
2012a), the remainder of this paper describes the
second evaluation round organized within SemEval-
2013. The following sections provide an overview
of the datasets used, the participating systems, the
approaches adopted, the achieved results, and the
lessons learned.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgements (see Figure 1 for
Spanish/English examples of each judgement):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in either di-
rection;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset.
3 Dataset description
The CLTE-2013 dataset is composed of four CLTE
corpora created for the following language combi-
nations: Spanish/English (SP-EN), Italian/English
(IT-EN), French/English (FR-EN), German/English
(DE-EN). Each corpus consists of 1,500 sentence
pairs (1,000 for training and 500 for test), balanced
across the four entailment judgements.
In this year?s evaluation, as training set we used
the CLTE-2012 corpus1 that was created for the
SemEval-2012 evaluation exercise2 (including both
training and test sets). The CLTE-2013 test set was
created from scratch, following the methodology de-
scribed in the next section.
3.1 Data collection and annotation
To collect the entailment pairs for the 2013 test set
we adopted a slightly modified version of the crowd-
sourcing methodology followed to create the CLTE-
2012 corpus (Negri et al, 2011). The main differ-
ence with last year?s procedure is that we did not
take advantage of crowdsourcing for the whole data
collection process, but only for part of it.
As for CLTE-2012, the collection and annotation
process consists of the following steps:
1. First, English sentences were manually ex-
tracted from Wikipedia and Wikinews. The se-
lected sentences represent one of the elements
(T1) of each entailment pair;
1http://www.celct.it/resources.php?id page=CLTE
2http://www.cs.york.ac.uk/semeval-2012/task8/
26
2. Next, each T1 was modified in various ways
in order to obtain a corresponding T2. While
in the CLTE-2012 dataset the whole T2 cre-
ation process was carried out through crowd-
sourcing, for the CLTE-2013 test set we crowd-
sourced only the first phase of T1 modification,
namely the creation of paraphrases. Focusing
on the creation of high quality paraphrases, we
followed the crowdsourcing methodology ex-
perimented in Negri et al (2012b), in which
a paraphrase is obtained through an itera-
tive modification process of an original sen-
tence, by asking workers to introduce meaning-
preserving lexical and syntactic changes. At
each round of the iteration, new workers are
presented with the output of the previous iter-
ation in order to increase divergence from the
original sentence. At the end of the process,
only the more divergent paraphrases according
to the Lesk score (Lesk, 1986) are selected. As
for the second phase of T2 creation process,
this year it was carried out by expert annota-
tors, who followed the same criteria used last
year for the crowdsourced tasks, i.e. i) remove
information from the input (paraphrased) sen-
tence and ii) add information from sentences
surrounding T1 in the source article;
3. Each T2 was then paired to the original T1, and
the resulting pairs were annotated with one of
the four entailment judgements. In order to re-
duce the correlation between the difference in
sentences? length and entailment judgements,
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.3 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgements, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
3Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were cross-annotated and filtered to
retain only those pairs with full agreement in the
entailment judgement between two expert annota-
tors. The final result is a multilingual parallel en-
tailment corpus, where T1s are in 5 different lan-
guages (i.e. English, Spanish, German, Italian, and
French), and T2s are in English. It is worth men-
tioning that the monolingual English corpus, a by-
product of our data collection methodology, will be
publicly released as a further contribution to the re-
search community.
3.2 Dataset statistics
As described in section 3.1, the methodology fol-
lowed to create the training and test sets was the
same except for the crowdsourced tasks. This al-
lowed us to obtain two datasets with the same bal-
ance across the entailment judgements, and to keep
under control the distribution of the pairs for differ-
ent length diff values in each language combination.
Training Set. The training set is composed of
1,000 CLTE pairs for each language combina-
tion, balanced across the four entailment judge-
ments (bidirectional, forward, backward, and
no entailment). As shown in Table 1, our data col-
lection procedure led to a dataset where the major-
ity of the pairs falls in the +5 -5 length diff range
for each language pair (67.2% on average across the
four language pairs). This characteristic is partic-
ularly relevant as our assumption is that such data
distribution makes entailment judgements based on
mere surface features such as sentence length inef-
fective, thus encouraging the development of alter-
native, deeper processing strategies.
Test Set. The test set is composed of 500 entail-
ment pairs for each language combination, balanced
across the four entailment judgements. As shown
in Table 2, also in this dataset the majority of the
collected entailment pairs is uniformly distributed
27
(a) SP-EN (b) IT-EN
(c) FR-EN (d) DE-EN
Figure 2: Pair distribution in the 2013 test set: total number of pairs (y-axis) for different length diff values (x-axis).
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
% (out of 1,000) 64.4 68.6 67.4 68.5
Table 1: Training set pair distribution within the -5/+5
length diff range.
in the [-5,+5] length diff range (68.1% on average
across the four language pairs).
However, comparing training and test set for
each language pair, it can be seen that while the
Spanish-English and Italian-English datasets are ho-
mogeneous with respect to the length diff feature,
the French-English and German-English datasets
present noticeable differences between training and
test set. These figures show that, despite the consid-
erable effort spent to produce comparable training
SP-EN IT-EN FR-EN DE-EN
backward 82 89 82 102
bidirectional 89 92 90 106
forward 69 78 76 98
no entailment 71 80 59 100
ALL 311 339 307 406
% (out of 500) 62.2 67.8 61.4 81.2
Table 2: Test set pair distribution within the -5/+5
length diff range.
and test sets, the ideal objective of a full homogene-
ity between the datasets for these two languages was
difficult to reach.
Complete details about the distribution of the
pairs in terms of length diff for the four cross-
lingual corpora in the test set are provided in Figure
2. Vertical bars represent, for each length diff value,
the proportion of pairs belonging to the four entail-
ment classes.
28
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgements re-
turned by each system with those manually assigned
by human annotators in the gold standard. The met-
rics used for systems? ranking is accuracy over the
whole test set, i.e. the number of correct judge-
ments out of the total number of judgements in the
test set. Additionally, we calculated precision, re-
call, and F1 measures for each of the four entail-
ment judgement categories taken separately. These
scores aim at giving participants the possibility to
gain clearer insights into their system?s behaviour on
the entailment phenomena relevant to the task.
To allow comparison with the CLTE-2012 re-
sults, the same three baselines were calculated on the
CLTE-2013 test set for each language combination.
The first one is the 0.25 accuracy score obtained by
assigning each test pair in the balanced dataset to
one of the four classes. The other two baselines con-
sider the length difference between T1 and T2:
? Composition of binary judgements (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary en-
tailment decisions (?YES?, ?NO?). The
classifier uses length(T1)/length(T2) and
length(T2)/length(T1) as features respectively
to check for entailment from T1 to T2 and vice-
versa. For each test pair, the unidirectional
judgements returned by the two classifiers are
composed into a single multi-directional judge-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgements.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using de-
fault parameters. Baseline results are reported in Ta-
ble 3.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages. With respect to
last year?s evaluation, we can observe a slight drop
in the binary classification baseline results. This
might be due to the fact that the length distribution
of examples is slightly different this year. How-
ever, there are no significant differences between the
multi-class baseline results of this year in compar-
ison with the previous round results. This might
suggest that multi-class classification is a more ro-
bust approach for recognizing multi-directional en-
tailment relations. Moreover, both baselines failed
in capturing the ?no-entailment? examples in all
datasets (F1no?entailment = 0).
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.35 0.39 0.37 0.39
Multi-class 0.43 0.44 0.42 0.42
Table 3: Baseline accuracy results.
5 Submitted runs and results
Like in the 2012 round of the CLTE task, partici-
pants were allowed to submit up to five runs for each
language combination. A total of twelve teams reg-
istered for participation and downloaded the train-
ing set. Out of them, six4 submitted valid runs.
Five teams produced submissions for all the four
language combinations, while one team participated
only in the DE-EN task. In total, 61 runs have been
submitted and evaluated (16 for DE-EN, and 15 for
each of the other language pairs).
Accuracy results are reported in Table 4. As can
be seen from the table, the performance of the best
systems is quite similar across the four language
combinations, with the best submissions achieving
results in the 43.4-45.8% accuracy interval. Simi-
larly, also average and median results are close to
each other, with a small drop on DE-EN. This drop
might be explained by the difference between the
training and test set with respect to the length diff
feature. Moreover, the performance of DE-EN auto-
matic translation might affect approaches based on
?pivoting?, (i.e. addressing CLTE by automatically
translating T1 in the same language of T2, as de-
scribed in Section 6).
4Including the task organizers.
29
System name SP-EN IT-EN FR-EN DE-EN
altn run1* 0.428 0.432 0.420 0.388
BUAP run1 0.364 0.358 0.368 0.322
BUAP run2 0.374 0.358 0.364 0.318
BUAP run3 0.380 0.358 0.362 0.316
BUAP run4 0.364 0.388 0.392 0.350
BUAP run5 0.386 0.360 0.372 0.318
celi run1 0.340 0.324 0.334 0.342
celi run2 0.342 0.324 0.340 0.342
ECNUCS run1 0.428 0.426 0.438 0.422
ECNUCS run2 0.404 0.420 0.450 0.436
ECNUCS run3 0.408 0.426 0.458 0.432
ECNUCS run4 0.422 0.416 0.436 0.452
ECNUCS run5 0.392 0.402 0.442 0.426
SoftCard run1 0.434 0.454 0.416 0.414
SoftCard run2 0.432 0.448 0.426 0.402
umelb run1 ? ? ? 0.324
Highest 0.434 0.454 0.458 0.452
Average 0.404 0.404 0.401 0.378
Median 0.428 0.426 0.420 0.369
Lowest 0.342 0.324 0.340 0.324
Table 4: CLTE-2013 accuracy results (61 runs) over the
4 language combinations. Highest, average, median and
lowest scores are calculated considering only the best run
for each team (*task organizers? system).
Compared to the results achieved last year, shown
in Table 5, a sensible decrease in the highest scores
can be observed. While in CLTE-2012 the top sys-
tems achieved an accuracy well above 0.5 (with a
maximum of 0.632 in SP-EN), the results for this
year are far below such level (the peak is now at
45,8% for FR-EN). A slight decrease with respect
to 2012 can also be noted for average performances.
However, it?s worth remarking the general increase
of the lowest and median scores, which are less sen-
sitive to isolate outstanding results achieved by sin-
gle teams. This indicates that a progress in CLTE
research has been made building on the lessons
learned after the first round of the initiative.
To better understand the behaviour of each sys-
tem, Table 6 provides separate precision, recall, and
F1 scores for each entailment judgement, calculated
over the best runs of each participating team. In
contrast to CLTE-2012, where the ?bidirectional?
and ?no entailment? categories consistently proved
to be more problematic than ?forward? and ?back-
ward? judgements, this year?s results are more ho-
mogeneous across the different classes. Neverthe-
less, on average, the classification of ?bidirectional?
pairs is still worse for three language pairs (SP-EN,
IT-EN and FR-EN), and results for ?no entailment?
are lower for two of them (SP-EN and DE-EN).
SP-EN IT-EN FR-EN DE-EN
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 5: CLTE-2012 accuracy results. Highest, average,
median and lowest scores are calculated considering only
the best run for each team.
As regards the comparison with the baselines,
this year?s results confirm that the length diff -based
baselines are hard to beat. More specifically, most
of the systems are slightly above the binary classi-
fication baseline (with the exception of the DE-EN
dataset where only two systems out of six outper-
formed it), whereas for all the language combina-
tions the multi-class baseline was beaten only by the
best participating system.
This shows that, despite the effort in keeping the
distribution of the entailment classes uniform across
different length diff values, eliminating the correla-
tion between sentence length and correct entailment
decisions is difficult. As a consequence, although
disregarding semantic aspects of the problem, fea-
tures considering length information are quite ef-
fective in terms of overall accuracy. Such features,
however, perform rather poorly when dealing with
challenging cases (e.g. ?no-entailment?), which are
better handled by participating systems.
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods
assign entailment judgements without prelim-
inary translation.
? Composition of binary judgements vs.
Multi-class classification. Compositional ap-
proaches map unidirectional (?YES?/?NO?)
30
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full spa-eng 0.509 0.464 0.485 0.440 0.264 0.330 0.464 0.416 0.439 0.357 0.568 0.438
BUAP spa-eng run5 0.446 0.360 0.398 0.521 0.296 0.378 0.385 0.456 0.418 0.300 0.432 0.354
celi spa-eng run2 0.396 0.352 0.373 0.431 0.400 0.415 0.325 0.328 0.327 0.245 0.288 0.265
ECNUCS spa-eng run1 0.458 0.432 0.444 0.533 0.320 0.400 0.406 0.416 0.411 0.380 0.544 0.447
SoftCard spa-eng run1 0.462 0.344 0.394 0.619 0.480 0.541 0.418 0.472 0.444 0.325 0.440 0.374
AVG. 0.454 0.390 0.419 0.509 0.352 0.413 0.400 0.418 0.408 0.321 0.454 0.376
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full ita-eng 0.448 0.376 0.409 0.417 0.344 0.377 0.512 0.496 0.504 0.374 0.512 0.432
BUAP ita-eng run4 0.418 0.328 0.368 0.462 0.384 0.419 0.379 0.440 0.407 0.327 0.400 0.360
celi ita-eng run1 0.288 0.256 0.271 0.395 0.408 0.402 0.336 0.304 0.319 0.279 0.328 0.301
ECNUCS ita-eng run1 0.422 0.456 0.438 0.592 0.336 0.429 0.440 0.440 0.440 0.349 0.472 0.401
SoftCard ita-eng run1 0.514 0.456 0.483 0.612 0.480 0.538 0.392 0.464 0.425 0.364 0.416 0.388
AVG. 0.418 0.374 0.394 0.496 0.390 0.433 0.412 0.429 0.419 0.339 0.426 0.376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full fra-eng 0.405 0.392 0.398 0.420 0.296 0.347 0.500 0.440 0.468 0.381 0.552 0.451
BUAP fra-eng run4 0.407 0.472 0.437 0.431 0.376 0.402 0.379 0.376 0.378 0.352 0.344 0.348
celi fra-eng run2 0.394 0.344 0.368 0.364 0.376 0.370 0.352 0.352 0.352 0.263 0.288 0.275
ECNUCS fra-eng run3 0.422 0.432 0.427 0.667 0.352 0.461 0.514 0.432 0.470 0.383 0.616 0.472
SoftCard fra-eng run2 0.477 0.416 0.444 0.556 0.400 0.465 0.412 0.432 0.422 0.335 0.456 0.386
AVG. 0.421 0.411 0.415 0.488 0.360 0.409 0.431 0.406 0.418 0.343 0.451 0.386
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full deu-eng 0.432 0.408 0.420 0.378 0.272 0.316 0.445 0.392 0.417 0.330 0.480 0.391
BUAP deu-eng run4 0.364 0.344 0.354 0.389 0.280 0.326 0.352 0.352 0.352 0.317 0.424 0.363
celi deu-eng run1 0.346 0.352 0.349 0.414 0.424 0.419 0.351 0.264 0.301 0.272 0.328 0.297
ECNUCS deu-eng run4 0.429 0.432 0.430 0.611 0.352 0.447 0.415 0.392 0.403 0.429 0.632 0.511
SoftCard deu-eng run1 0.511 0.368 0.428 0.527 0.384 0.444 0.417 0.400 0.408 0.317 0.504 0.389
umelb deu-eng run1 0.323 0.320 0.321 0.240 0.184 0.208 0.362 0.376 0.369 0.347 0.416 0.378
AVG. 0.401 0.371 0.384 0.426 0.316 0.360 0.390 0.363 0.375 0.335 0.464 0.389
Table 6: Precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
entailment decisions taken separately into sin-
gle judgements (similar to the Binary baseline
in Section 4). Methods based on multi-class
classification directly assign one of the four en-
tailment judgements to each test pair (similar to
our Multi-class baseline).
In contrast with CLTE-2012, where the combina-
tion of pivoting and compositional methods was the
option adopted by the majority of the approaches,
this year?s solutions do not show a clear trend. Con-
cerning the former dimension, participating systems
are equally distributed in cross-lingual and pivoting
methods relying on external automatic translation
tools. Regarding the latter dimension, in addition
to compositional and multi-class strategies, also al-
ternative solutions that leverage more sophisticated
meta-classification strategies have been proposed.
Besides the recourse to MT tools (e.g. Google
Translate), other tools and resources used by partic-
ipants include: WordNet, word alignment tools (e.g.
Giza++), part-of-speech taggers (e.g. Stanford POS
Tagger), stemmers (e.g. Snowball), machine learn-
ing libraries (e.g. Weka, SVMlight), parallel corpora
(e.g. Europarl), and stopword lists. More in detail:
ALTN [cross-lingual, compositional] (Turchi
and Negri, 2013) adopts a supervised learning
method based on features that consider word align-
ments between the two sentences obtained with
GIZA++ (Och et al, 2003). Binary entailment
judgements are taken separately, and combined into
final CLTE decisions.
BUAP [pivoting, multi-class and meta-
classifier] (Vilarin?o et al, 2013) adopts a pivoting
method based on translating T1 into the language of
31
T2 and vice versa (using Google Translate5). Sim-
ilarity measures (e.g. Jaccard index) and features
based on n-gram overlap, computed at the level of
words and part of speech categories, are used (either
alone or in combination) by different classification
strategies including: multi-class, a meta-classifier
(i.e. combining the output of 2/3/4-class classifiers),
and majority voting.
CELI [cross-lingual, meta-classifier]
(Kouylekov, 2013) uses dictionaries for word
matching, and a multilingual corpus extracted
from Wikipedia for term weighting. A variety of
distance measures implemented in the RTE system
EDITS (Kouylekov and Negri, 2010; Negri et
al., 2009) are used to extract features to train a
meta-classifier. Such classifier combines binary
decisions (?YES?/?NO?) taken separately for each
of the four CLTE judgements.
ECNUCS [pivoting, multi-class] (Jiang and
Man, 2013) uses Google Translate to obtain the En-
glish translation of each T1. After a pre-processing
step aimed at maximizing the commonalities be-
tween the two sentences (e.g. abbreviation replace-
ment), a number of features is extracted to train
a multi-class SVM classifier. Such features con-
sider information about sentence length, text sim-
ilarity/difference measures, and syntactic informa-
tion.
SoftCard [pivoting, multi-class] (Jimenez et al,
2013) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity.
Umelb [cross-lingual, pivoting, compositional]
(Graham et al, 2013) adopts both pivoting and
cross-lingual approaches. For the latter, GIZA++
was used to compute word alignments between the
input sentences. Word alignment features are used
to train binary SVM classifiers whose decisions are
eventually composed into CLTE judgements.
7 Conclusion
Following the success of the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
5http://translate.google.com/
tion task organized within SemEval-2012, a second
evaluation task has been organized within SemEval-
2013. Despite the decrease in the number of partic-
ipants (six teams - four less than in the first round
- submitted a total of 61 runs) the new experience
is still positive. In terms of data, a new test set
has been released, extending the old one with 500
new CLTE pairs. The resulting 1,500 cross-lingual
pairs, aligned over four language combinations (in
addition to the monolingual English version), and
annotated with multiple entailment relations, repre-
sent a significant contribution to the research com-
munity and a solid starting point for further develop-
ments.6 In terms of results, in spite of a significant
decrease of the top scores, the increase of both me-
dian and lower results demonstrates some encour-
aging progress in CLTE research. Such progress is
also demonstrated by the variety of the approaches
proposed. While in the first round most of the
teams adopted more intuitive and ?simpler? solu-
tions based on pivoting (i.e. translation of T1 and
T2 in the same language) and compositional entail-
ment decision strategies, this year new ideas and
more complex solutions have emerged. Pivoting and
cross-lingual approaches are equally distributed, and
new classification methods have been proposed. Our
hope is that the large room for improvement, the in-
crease of available data, and the potential of CLTE
as a way to address complex NLP tasks and applica-
tions will motivate further research on the proposed
problem.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531). The
authors would also like to acknowledge Pamela
Forner and Giovanni Moretti from CELCT, and the
volunteer translators that contributed to the creation
of the dataset: Giusi Calo, Victoria D??az, Bianca
Jeremias, Anne Kauffman, Laura Lo?pez Ortiz, Julie
Mailfait, Laura Mora?n Iglesias, Andreas Schwab.
6Together with the datasets derived from translation of the
RTE data (Negri and Mehdad, 2010), this is the only material
currently available to train and evaluate CLTE systems.
32
References
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. Cosyne: Synchro-
nizing multilingual wiki content. In Proceedings of
WikiSym 2012.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Yvette Graham, Bahar Salehi, and Tim Baldwin. 2013.
Unimelb: Cross-lingual Textual Entailment with Word
Alignment and String Similarity Features. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013).
Zhao Jiang and Lan Man. 2013. ECNUCS: Recognizing
Cross-lingual Textual Entailment Using Multiple Fea-
ture Types. . In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. Soft Cardinality-CLTE: Learning to Iden-
tify Directional Cross-Lingual Entailmens from Car-
dinalities and SMT. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2013. Celi: EDITS and Generic Text
Pair Classification. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Michael Lesk. 1986. Automated Sense Disambiguation
Using Machine-readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings
of the 5th annual international conference on Systems
documentation (SIGDOC86).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Christof Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
Cosyne: a framework for multilingual content syn-
chronization of wikis. In Proceedings of WikiSym
2011.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazons? Me-
chanical Turk.
Matteo Negri, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards ex-
tensible textual entailment engines: the edits package.
In AI* IA 2009: Emergent Perspectives in Artificial In-
telligence, pages 314?323. Springer.
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012a.
Semeval-2012 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012b.
Chinese Whispers: Cooperative Paraphrase Acqui-
sition. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC12), volume 2, pages 2659?2665.
F. Och, H. Ney, F. Josef, and O. H. Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual Entail-
ment. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
Darnes Vilarin?o, David Pinto, Saul Leo?n, Yuridiana
Alema?n, and Helena Go?mez-Adorno. 2013. BUAP:
N -gram based Feature Evaluation for the Cross-
Lingual Textual Entailment Task. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
33
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1?8,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 1: Evaluation of Compositional Distributional
Semantic Models on Full Sentences through Semantic Relatedness and
Textual Entailment
Marco Marelli
(1)
Luisa Bentivogli
(2)
Marco Baroni
(1)
Raffaella Bernardi
(1)
Stefano Menini
(1,2)
Roberto Zamparelli
(1)
(1)
University of Trento, Italy
(2)
FBK - Fondazione Bruno Kessler, Trento, Italy
{name.surname}@unitn.it, {bentivo,menini}@fbk.eu
Abstract
This paper presents the task on the evalu-
ation of Compositional Distributional Se-
mantics Models on full sentences orga-
nized for the first time within SemEval-
2014. Participation was open to systems
based on any approach. Systems were pre-
sented with pairs of sentences and were
evaluated on their ability to predict hu-
man judgments on (i) semantic relatedness
and (ii) entailment. The task attracted 21
teams, most of which participated in both
subtasks. We received 17 submissions in
the relatedness subtask (for a total of 66
runs) and 18 in the entailment subtask (65
runs).
1 Introduction
Distributional Semantic Models (DSMs) approx-
imate the meaning of words with vectors sum-
marizing their patterns of co-occurrence in cor-
pora. Recently, several compositional extensions
of DSMs (CDSMs) have been proposed, with the
purpose of representing the meaning of phrases
and sentences by composing the distributional rep-
resentations of the words they contain (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Despite the ever increasing interest in the
field, the development of adequate benchmarks for
CDSMs, especially at the sentence level, is still
lagging. Existing data sets, such as those intro-
duced by Mitchell and Lapata (2008) and Grefen-
stette and Sadrzadeh (2011), are limited to a few
hundred instances of very short sentences with a
fixed structure. In the last ten years, several large
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
data sets have been developed for various com-
putational semantics tasks, such as Semantic Text
Similarity (STS)(Agirre et al., 2012) or Recogniz-
ing Textual Entailment (RTE) (Dagan et al., 2006).
Working with such data sets, however, requires
dealing with issues, such as identifying multiword
expressions, recognizing named entities or access-
ing encyclopedic knowledge, which have little to
do with compositionality per se. CDSMs should
instead be evaluated on data that are challenging
for reasons due to semantic compositionality (e.g.
context-cued synonymy resolution and other lexi-
cal variation phenomena, active/passive and other
syntactic alternations, impact of negation at vari-
ous levels, operator scope, and other effects linked
to the functional lexicon). These issues do not oc-
cur frequently in, e.g., the STS and RTE data sets.
With these considerations in mind, we devel-
oped SICK (Sentences Involving Compositional
Knowledge), a data set aimed at filling the void,
including a large number of sentence pairs that
are rich in the lexical, syntactic and semantic phe-
nomena that CDSMs are expected to account for,
but do not require dealing with other aspects of
existing sentential data sets that are not within
the scope of compositional distributional seman-
tics. Moreover, we distinguished between generic
semantic knowledge about general concept cate-
gories (such as knowledge that a couple is formed
by a bride and a groom) and encyclopedic knowl-
edge about specific instances of concepts (e.g.,
knowing the fact that the current president of the
US is Barack Obama). The SICK data set contains
many examples of the former, but none of the lat-
ter.
2 The Task
The Task involved two subtasks. (i) Relatedness:
predicting the degree of semantic similarity be-
tween two sentences, and (ii) Entailment: detect-
ing the entailment relation holding between them
1
(see below for the exact definition). Sentence re-
latedness scores provide a direct way to evalu-
ate CDSMs, insofar as their outputs are able to
quantify the degree of semantic similarity between
sentences. On the other hand, starting from the
assumption that understanding a sentence means
knowing when it is true, being able to verify
whether an entailment is valid is a crucial chal-
lenge for semantic systems.
In the semantic relatedness subtask, given two
sentences, systems were required to produce a re-
latedness score (on a continuous scale) indicating
the extent to which the sentences were expressing
a related meaning. Table 1 shows examples of sen-
tence pairs with different degrees of semantic re-
latedness; gold relatedness scores are expressed on
a 5-point rating scale.
In the entailment subtask, given two sentences
A and B, systems had to determine whether the
meaning of B was entailed by A. In particular, sys-
tems were required to assign to each pair either
the ENTAILMENT label (when A entails B, viz.,
B cannot be false when A is true), the CONTRA-
DICTION label (when A contradicted B, viz. B is
false whenever A is true), or the NEUTRAL label
(when the truth of B could not be determined on
the basis of A). Table 2 shows examples of sen-
tence pairs holding different entailment relations.
Participants were invited to submit up to five
system runs for one or both subtasks. Developers
of CDSMs were especially encouraged to partic-
ipate, but developers of other systems that could
tackle sentence relatedness or entailment tasks
were also welcome. Besides being of intrinsic in-
terest, the latter systems? performance will serve
to situate CDSM performance within the broader
landscape of computational semantics.
3 The SICK Data Set
The SICK data set, consisting of about 10,000 En-
glish sentence pairs annotated for relatedness in
meaning and entailment, was used to evaluate the
systems participating in the task. The data set
creation methodology is outlined in the following
subsections, while all the details about data gen-
eration and annotation, quality control, and inter-
annotator agreement can be found in Marelli et al.
(2014).
3.1 Data Set Creation
SICK was built starting from two existing data
sets: the 8K ImageFlickr data set
1
and the
SemEval-2012 STS MSR-Video Descriptions data
set.
2
The 8K ImageFlickr dataset is a dataset of
images, where each image is associated with five
descriptions. To derive SICK sentence pairs we
randomly chose 750 images and we sampled two
descriptions from each of them. The SemEval-
2012 STS MSR-Video Descriptions data set is a
collection of sentence pairs sampled from the short
video snippets which compose the Microsoft Re-
search Video Description Corpus. A subset of 750
sentence pairs were randomly chosen from this
data set to be used in SICK.
In order to generate SICK data from the 1,500
sentence pairs taken from the source data sets, a 3-
step process was applied to each sentence compos-
ing the pair, namely (i) normalization, (ii) expan-
sion and (iii) pairing. Table 3 presents an example
of the output of each step in the process.
The normalization step was carried out on the
original sentences (S0) to exclude or simplify in-
stances that contained lexical, syntactic or seman-
tic phenomena (e.g., named entities, dates, num-
bers, multiword expressions) that CDSMs are cur-
rently not expected to account for.
The expansion step was applied to each of the
normalized sentences (S1) in order to create up to
three new sentences with specific characteristics
suitable to CDSM evaluation. In this step syntac-
tic and lexical transformations with predictable ef-
fects were applied to each normalized sentence, in
order to obtain (i) a sentence with a similar mean-
ing (S2), (ii) a sentence with a logically contradic-
tory or at least highly contrasting meaning (S3),
and (iii) a sentence that contains most of the same
lexical items, but has a different meaning (S4) (this
last step was carried out only where it could yield
a meaningful sentence; as a result, not all normal-
ized sentences have an (S4) expansion).
Finally, in the pairing step each normalized
sentence in the pair was combined with all the
sentences resulting from the expansion phase and
with the other normalized sentence in the pair.
Considering the example in Table 3, S1a and S1b
were paired. Then, S1a and S1b were each com-
bined with S2a, S2b,S3a, S3b, S4a, and S4b, lead-
1
http://nlp.cs.illinois.edu/HockenmaierGroup/data.html
2
http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=data
2
Relatedness score Example
1.6
A: ?A man is jumping into an empty pool?
B: ?There is no biker jumping in the air?
2.9
A: ?Two children are lying in the snow and are making snow angels?
B: ?Two angels are making snow on the lying children?
3.6
A: ?The young boys are playing outdoors and the man is smiling nearby?
B: ?There is no boy playing outdoors and there is no man smiling?
4.9
A: ?A person in a black jacket is doing tricks on a motorbike?
B: ?A man in a black jacket is doing tricks on a motorbike?
Table 1: Examples of sentence pairs with their gold relatedness scores (on a 5-point rating scale).
Entailment label Example
ENTAILMENT
A: ?Two teams are competing in a football match?
B: ?Two groups of people are playing football?
CONTRADICTION
A: ?The brown horse is near a red barrel at the rodeo?
B: ?The brown horse is far from a red barrel at the rodeo?
NEUTRAL
A: ?A man in a black jacket is doing tricks on a motorbike?
B: ?A person is riding the bicycle on one wheel?
Table 2: Examples of sentence pairs with their gold entailment labels.
ing to a total of 13 different sentence pairs.
Furthermore, a number of pairs composed of
completely unrelated sentences were added to the
data set by randomly taking two sentences from
two different pairs.
The result is a set of about 10,000 new sen-
tence pairs, in which each sentence is contrasted
with either a (near) paraphrase, a contradictory or
strongly contrasting statement, another sentence
with very high lexical overlap but different mean-
ing, or a completely unrelated sentence. The ra-
tionale behind this approach was that of building
a data set which encouraged the use of a com-
positional semantics step in understanding when
two sentences have close meanings or entail each
other, hindering methods based on individual lex-
ical items, on the syntactic complexity of the two
sentences or on pure world knowledge.
3.2 Relatedness and Entailment Annotation
Each pair in the SICK dataset was annotated to
mark (i) the degree to which the two sentence
meanings are related (on a 5-point scale), and (ii)
whether one entails or contradicts the other (con-
sidering both directions). The ratings were col-
lected through a large crowdsourcing study, where
each pair was evaluated by 10 different subjects,
and the order of presentation of the sentences was
counterbalanced (i.e., 5 judgments were collected
for each presentation order). Swapping the order
of the sentences within each pair served a two-
fold purpose: (i) evaluating the entailment rela-
tion in both directions and (ii) controlling pos-
sible bias due to priming effects in the related-
ness task. Once all the annotations were collected,
the relatedness gold score was computed for each
pair as the average of the ten ratings assigned by
participants, whereas a majority vote scheme was
adopted for the entailment gold labels.
3.3 Data Set Statistics
For the purpose of the task, the data set was ran-
domly split into training and test set (50% and
50%), ensuring that each relatedness range and en-
tailment category was equally represented in both
sets. Table 4 shows the distribution of sentence
pairs considering the combination of relatedness
ranges and entailment labels. The ?total? column
3
Original pair
S0a: A sea turtle is hunting for fish S0b: The turtle followed the fish
Normalized pair
S1a: A sea turtle is hunting for fish S1b: The turtle is following the fish
Expanded pairs
S2a: A sea turtle is hunting for food S2b: The turtle is following the red fish
S3a: A sea turtle is not hunting for fish S3b: The turtle isn?t following the fish
S4a: A fish is hunting for a turtle in the sea S4b: The fish is following the turtle
Table 3: Data set creation process.
indicates the total number of pairs in each range
of relatedness, while the ?total? row contains the
total number of pairs in each entailment class.
SICK Training Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 0 (0%) 471 (10%) 471
2-3 range 59 (1%) 2 (0%) 638 (13%) 699
3-4 range 498 (10%) 71 (1%) 1344 (27%) 1913
4-5 range 155 (3%) 1344 (27%) 352 (7%) 1851
TOTAL 712 1417 2805 4934
SICK Test Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 1 (0%) 451 (9%) 452
2-3 range 59 (1%) 0 (0%) 615(13%) 674
3-4 range 496 (10%) 65 (1%) 1398 (28%) 1959
4-5 range 157 (3%) 1338 (27%) 326 (7%) 1821
TOTAL 712 1404 2790 4906
Table 4: Distribution of sentence pairs across the
Training and Test Sets.
4 Evaluation Metrics and Baselines
Both subtasks were evaluated using standard met-
rics. In particular, the results on entailment were
evaluated using accuracy, whereas the outputs on
relatedness were evaluated using Pearson correla-
tion, Spearman correlation, and Mean Squared Er-
ror (MSE). Pearson correlation was chosen as the
official measure to rank the participating systems.
Table 5 presents the performance of 4 base-
lines. The Majority baseline always assigns
the most common label in the training data
(NEUTRAL), whereas the Probability baseline
assigns labels randomly according to their rela-
tive frequency in the training set. The Overlap
baseline measures word overlap, again with
parameters (number of stop words and EN-
TAILMENT/NEUTRAL/CONTRADICTION
thresholds) estimated on the training part of the
data.
Baseline Relatedness Entailment
Chance 0 33.3%
Majority NA 56.7%
Probability NA 41.8%
Overlap 0.63 56.2%
Table 5: Performance of baselines. Figure of merit
is Pearson correlation for relatedness and accuracy
for entailment. NA = Not Applicable
5 Submitted Runs and Results
Overall, 21 teams participated in the task. Partici-
pants were allowed to submit up to 5 runs for each
subtask and had to choose the primary run to be in-
cluded in the comparative evaluation. We received
17 submissions to the relatedness subtask (for a
total of 66 runs) and 18 for the entailment subtask
(65 runs).
We asked participants to pre-specify a pri-
mary run to encourage commitment to a
theoretically-motivated approach, rather than
post-hoc performance-based assessment. Inter-
estingly, some participants used the non-primary
runs to explore the performance one could reach
by exploiting weaknesses in the data that are not
likely to hold in future tasks of the same kind
(for instance, run 3 submitted by The Meaning
Factory exploited sentence ID ordering informa-
tion, but it was not presented as a primary run).
Participants could also use non-primary runs to
test smart baselines. In the relatedness subtask
six non-primary runs slightly outperformed the
official winning primary entry,
3
while in the
entailment task all ECNU?s runs but run 4 were
better than ECNU?s primary run. Interestingly,
the differences between the ECNU?s runs were
3
They were: The Meaning Factory?s run3 (Pearson
0.84170) ECNU?s runs2 (0.83893) run5 (0.83500) and Stan-
fordNLP?s run4 (0.83462) and run2 (0.83103).
4
due to the learning methods used.
We present the results achieved by primary runs
against the Entailment and Relatedness subtasks in
Table 6 and Table 7, respectively.
4
We witnessed
a very close finish in both subtasks, with 4 more
systems within 3 percentage points of the winner
in both cases. 4 of these 5 top systems were the
same across the two subtasks. Most systems per-
formed well above the best baselines from Table
5.
The overall performance pattern suggests that,
owing perhaps to the more controlled nature of
the sentences, as well as to the purely linguistic
nature of the challenges it presents, SICK entail-
ment is ?easier? than RTE. Considering the first
five RTE challenges (Bentivogli et al., 2009), the
median values ranged from 56.20% to 61.75%,
whereas the average values ranged from 56.45%
to 61.97%. The entailment scores obtained on
the SICK data set are considerably higher, being
77.06% for the median system and 75.36% for
the average system. On the other hand, the re-
latedness task is more challenging than the one
run on MSRvid (one of our data sources) at STS
2012, where the top Pearson correlation was 0.88
(Agirre et al., 2012).
6 Approaches
A summary of the approaches used by the sys-
tems to address the task is presented in Table 8.
In the table, systems in bold are those for which
the authors submitted a paper (Ferrone and Zan-
zotto, 2014; Bjerva et al., 2014; Beltagy et al.,
2014; Lai and Hockenmaier, 2014; Alves et al.,
2014; Le?on et al., 2014; Bestgen, 2014; Zhao et
al., 2014; Vo et al., 2014; Bic?ici and Way, 2014;
Lien and Kouylekov, 2014; Jimenez et al., 2014;
Proisl and Evert, 2014; Gupta et al., 2014). For the
others, we used the brief description sent with the
system?s results, double-checking the information
with the authors. In the table, ?E? and ?R? refer
to the entailment and relatedness task respectively,
and ?B? to both.
Almost all systems combine several kinds of
features. To highlight the role played by com-
position, we draw a distinction between compo-
sitional and non-compositional features, and di-
vide the former into ?fully compositional? (sys-
4
ITTK?s primary run could not be evaluated due to tech-
nical problems with the submission. The best ITTK?s non-
primary run scored 78,2% accuracy in the entailment task and
0.76 r in the relatedness task.
ID Compose ACCURACY
Illinois-LH run1 P/S 84.6
ECNU run1 S 83.6
UNAL-NLP run1 83.1
SemantiKLUE run1 82.3
The Meaning Factory run1 S 81.6
CECL ALL run1 80.0
BUAP run1 P 79.7
UoW run1 78.5
Uedinburgh run1 S 77.1
UIO-Lien run1 77.0
FBK-TR run3 P 75.4
StanfordNLP run5 S 74.5
UTexas run1 P/S 73.2
Yamraj run1 70.7
asjai run5 S 69.8
haLF run2 S 69.4
RTM-DCU run1 67.2
UANLPCourse run2 S 48.7
Table 6: Primary run results for the entailment
subtask. The table also shows whether a sys-
tem exploits composition information at either the
phrase (P) or sentence (S) level.
tems that compositionally computed the meaning
of the full sentences, though not necessarily by as-
signing meanings to intermediate syntactic con-
stituents) and ?partially compositional? (systems
that stop the composition at the level of phrases).
As the table shows, thirteen systems used compo-
sition in at least one of the tasks; ten used compo-
sition for full sentences and six for phrases, only.
The best systems are among these thirteen sys-
tems.
Let us focus on such compositional methods.
Concerning the relatedness task, the fine-grained
analyses reported for several systems (Illinois-
LH, The Meaning Factory and ECNU) shows that
purely compositional systems currently reach per-
formance above 0.7 r. In particular, ECNU?s
compositional feature gives 0.75 r, The Meaning
Factory?s logic-based composition model 0.73 r,
and Illinois-LH compositional features combined
with Word Overlap 0.75 r. While competitive,
these scores are lower than the one of the best
5
ID Compose r ? MSE
ECNU run1 S 0.828 0.769 0.325
StanfordNLP run5 S 0.827 0.756 0.323
The Meaning Factory run1 S 0.827 0.772 0.322
UNAL-NLP run1 0.804 0.746 0.359
Illinois-LH run1 P/S 0.799 0.754 0.369
CECL ALL run1 0.780 0.732 0.398
SemantiKLUE run1 0.780 0.736 0.403
RTM-DCU run1 0.764 0.688 0.429
UTexas run1 P/S 0.714 0.674 0.499
UoW run1 0.711 0.679 0.511
FBK-TR run3 P 0.709 0.644 0.591
BUAP run1 P 0.697 0.645 0.528
UANLPCourse run2 S 0.693 0.603 0.542
UQeResearch run1 0.642 0.626 0.822
ASAP run1 P 0.628 0.597 0.662
Yamraj run1 0.535 0.536 2.665
asjai run5 S 0.479 0.461 1.104
Table 7: Primary run results for the relatedness
subtask (r for Pearson and ? for Spearman corre-
lation). The table also shows whether a system ex-
ploits composition information at either the phrase
(P) or sentence (S) level.
purely non-compositional system (UNAL-NLP)
which reaches the 4th position (0.80 r UNAL-NLP
vs. 0.82 r obtained by the best system). UNAL-
NLP however exploits an ad-hoc ?negation? fea-
ture discussed below.
In the entailment task, the best non-
compositional model (again UNAL-NLP)
reaches the 3rd position, within close reach of the
best system (83% UNAL-NLP vs. 84.5% obtained
by the best system). Again, purely compositional
models have lower performance. haLF CDSM
reaches 69.42% accuracy, Illinois-LH Word
Overlap combined with a compositional feature
reaches 71.8%. The fine-grained analysis reported
by Illinois-LH (Lai and Hockenmaier, 2014)
shows that a full compositional system (based
on point-wise multiplication) fails to capture
contradiction. It is better than partial phrase-based
compositional models in recognizing entailment
pairs, but worse than them on recognizing neutral
pairs.
Given our more general interest in the distri-
butional approaches, in Table 8 we also classify
the different DSMs used as ?Vector Space Mod-
els?, ?Topic Models? and ?Neural Language Mod-
els?. Due to the impact shown by learning methods
(see ECNU?s results), we also report the different
learning approaches used.
Several participating systems deliberately ex-
ploit ad-hoc features that, while not helping a true
understanding of sentence meaning, exploit some
systematic characteristics of SICK that should be
controlled for in future releases of the data set.
In particular, the Textual Entailment subtask has
been shown to rely too much on negative words
and antonyms. The Illinois-LH team reports that,
just by checking the presence of negative words
(the Negation Feature in the table), one can detect
86.4% of the contradiction pairs, and by combin-
ing Word Overlap and antonyms one can detect
83.6% of neutral pairs and 82.6% of entailment
pairs. This approach, however, is obviously very
brittle (it would not have been successful, for in-
stance, if negation had been optionally combined
with word-rearranging in the creation of S4 sen-
tences, see Section 3.1 above).
Finally, Table 8 reports about the use of external
resources in the task. One of the reasons we cre-
ated SICK was to have a compositional semantics
benchmark that would not require too many ex-
ternal tools and resources (e.g., named-entity rec-
ognizers, gazetteers, ontologies). By looking at
what the participants chose to use, we think we
succeeded, as only standard NLP pre-processing
tools (tokenizers, PoS taggers and parsers) and rel-
atively few knowledge resources (mostly, Word-
Net and paraphrase corpora) were used.
7 Conclusion
We presented the results of the first task on the
evaluation of compositional distributional seman-
tic models and other semantic systems on full sen-
tences, organized within SemEval-2014. Two sub-
tasks were offered: (i) predicting the degree of re-
latedness between two sentences, and (ii) detect-
ing the entailment relation holding between them.
The task has raised noticeable attention in the
community: 17 and 18 submissions for the relat-
edness and entailment subtasks, respectively, for a
total of 21 participating teams. Participation was
not limited to compositional models but the major-
ity of systems (13/21) used composition in at least
one of the subtasks. Moreover, the top-ranking
systems in both tasks use compositional features.
However, it must be noted that all systems also ex-
6
Participant ID Non composition features Comp features Learning Methods External Resources 
Ve
cto
r S
em
an
tic
s M
od
el 
To
pic
 M
od
el 
Ne
ura
l L
an
gu
ag
e M
od
el 
De
no
tat
ion
al 
Mo
de
l 
Wo
rd 
Ov
erl
ap
 
Wo
rd 
Sim
ila
rit
y 
Sy
nta
cti
c F
ea
tur
es
 
Se
nte
nc
e d
iffe
ren
ce
 
Ne
ga
tio
n F
ea
tur
es
 
Se
nte
nc
e C
om
po
sit
ion
 
Ph
ras
e c
om
po
sit
ion
  
SV
M 
an
d K
ern
el 
me
tho
ds
 
K-
Ne
are
st 
Ne
igh
bo
urs
 
Cla
ssi
fie
r C
om
bin
ati
on
 
Ra
nd
om
 Fo
res
t 
Fo
L/P
rob
ab
ilis
tic
 Fo
L 
Cu
rri
cu
lum
 ba
se
d l
ea
rni
ng
 
Ot
he
r 
Wo
rdN
et 
Pa
rap
hra
se
s D
B 
Ot
he
r C
orp
ora
 
Im
ag
eF
lick
er 
 ST
S M
SR
-V
ide
o 
De
scr
ipt
ion
 
ASAP R R R R R R R R R 
ASJAI B B B B B B B B E B R B 
BUAP B B B B E B E B 
UEdinburgh B B B B B E R B 
CECL B B B B B B 
ECNU B B B B B B B B B B B B B 
FBK-TR R R R E B E E B R E R R E 
haLF E E E E 
IITK B B B B B B B B B 
Illinois-LH B B B B B B B B B B B B 
RTM-DCU B B B B B 
SemantiKLUE B B B B B B B B 
StandfordNLP B B R R R B E 
The Meaning Factory R R R R R R B E R E B B R 
UANLPCourse B B B B B 
UIO-Lien E E 
UNAL-NLP B B B B R B B 
UoW B B B B B B 
UQeRsearch R R R R R R R 
UTexas B B B B B B B 
Yamarj B B B B 
Table 8: Summary of the main characteristics of the participating systems on R(elatedness), E(ntailment)
or B(oth)
ploit non-compositional features and most of them
use external resources, especially WordNet. Al-
most all the participating systems outperformed
the proposed baselines in both tasks. Further anal-
yses carried out by some participants in the task
show that purely compositional approaches reach
accuracy above 70% in entailment and 0.70 r for
relatedness. These scores are comparable with the
average results obtained in the task.
Acknowledgments
We thank the creators of the ImageFlickr, MSR-
Video, and SemEval-2012 STS data sets for grant-
ing us permission to use their data for the task. The
University of Trento authors were supported by
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), volume 2.
Ana O. Alves, Adirana Ferrugento, Mariana Lorenc?o,
and Filipe Rodrigues. 2014. ASAP: Automatica se-
mantic alignment for phrases. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin
Erk, and Raymon J. Mooney. 2014. UTexas: Nat-
ural language semantics using distributional seman-
tics and probablisitc logic. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
7
Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In The Text Analysis Conference (TAC 2009).
Yves Bestgen. 2014. CECL: a new baseline and a non-
compositional approach for the Sick benchmark. In
Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similar-
ity. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: For-
mal Semantics for Recognizing Textual Entailment
and Determining Semantic Similarity. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. Evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising textual entailment, pages 177?
190. Springer.
Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014.
haLF:comparing a pure CDSM approach and a stan-
dard ML system for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Rohit Gupta, Ismail El Maarouf Hannah Bechara, and
Costantin Oras?an. 2014. UoW: NLP techniques de-
veloped at the University of Wolverhampton for Se-
mantic Similarity and Textual Entailment. In Pro-
ceedings of SemEval 2014: International Workshop
on Semantic Evaluation.
Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of SemEval 2014: International Workshop on Se-
mantic Evaluation.
Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Sa?ul Le?on, Darnes Vilarino, David Pinto, Mireya To-
var, and Beatrice Beltr?an. 2014. BUAP:evaluating
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of SemEval 2014: Inter-
national Workshop on Semantic Evaluation.
Elisabeth Lien and Milen Kouylekov. 2014. UIO-
Lien: Entailment recognition using minimal recur-
sion semantics. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Thomas Proisl and Stefan Evert. 2014. SemantiK-
LUE: Robust semantic similarity at multiple levels
using maximum weight matching. In Proceedings of
SemEval 2014: International Workshop on Semantic
Evaluation.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
An N. P. Vo, Octavian Popescu, and Tommaso Caselli.
2014. FBK-TR: SVM for Semantic Relatedness and
Corpus Patterns for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: One Stone Two Birds: Ensemble of Het-
erogenous Measures for Semantic Relatedness and
Textual Entailment. In Proceedings of SemEval
2014: International Workshop on Semantic Evalu-
ation.
8
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 19?27,
Beijing, August 2010
Extending English ACE 2005 Corpus Annotation with Ground-truth
Links to Wikipedia
Luisa Bentivogli
FBK-Irst
bentivo@fbk.eu
Pamela Forner
CELCT
forner@celct.it
Claudio Giuliano
FBK-Irst
giuliano@fbk.eu
Alessandro Marchetti
CELCT
amarchetti@celct.it
Emanuele Pianta
FBK-Irst
pianta@fbk.eu
Kateryna Tymoshenko
FBK-Irst
tymoshenko@fbk.eu
Abstract
This paper describes an on-going annota-
tion effort which aims at adding a man-
ual annotation layer connecting an exist-
ing annotated corpus such as the English
ACE-2005 Corpus to Wikipedia. The an-
notation layer is intended for the evalua-
tion of accuracy of linking to Wikipedia in
the framework of a coreference resolution
system.
1 Introduction
Collaboratively Constructed Resources (CCR)
such as Wikipedia are starting to be used for a
number of semantic processing tasks that up to
few years ago could only rely on few manually
constructed resources such as WordNet and Sem-
Cor (Fellbaum, 1998). The impact of the new re-
sources can be multiplied by connecting them to
other existing datasets, e.g. reference corpora. In
this paper we will illustrate an on-going annota-
tion effort which aims at adding a manual anno-
tation layer connecting an existing annotated cor-
pus such as the English ACE-2005 dataset1 to a
CCR such as Wikipedia. This effort will produce
a new integrated resource which can be useful for
the coreference resolution task.
Coreference resolution is the task of identify-
ing which mentions, i.e. individual textual de-
scriptions usually realized as noun phrases or pro-
nouns, refer to the same entity. To solve this
task, especially in the case of non-pronominal co-
reference, researchers have recently started to ex-
ploit semantic knowledge, e.g. trying to calculate
1http://projects.ldc.upenn.edu/ace/
the semantic similarity of mentions (Ponzetto and
Strube, 2006) or their semantic classes (Ng, 2007;
Soon et al, 2001). Up to now, WordNet has been
one of the most frequently used sources of se-
mantic knowledge for the coreference resolution
task (Soon et al, 2001; Ng and Cardie, 2002). Re-
searchers have shown, however, that WordNet has
some limits. On one hand, although WordNet has
a big coverage of the English language in terms
of common nouns, it still has a limited coverage
of proper nouns (e.g. Barack Obama is not avail-
able in the on-line version) and entity descrip-
tions (e.g. president of India). On the other hand
WordNet sense inventory is considered too fine-
grained (Ponzetto and Strube, 2006; Mihalcea and
Moldovan, 2001). In alternative, it has been re-
cently shown that Wikipedia can be a promising
source of semantic knowledge for coreference res-
olution between nominals (Ponzetto and Strube,
2006).
Consider some possible uses of Wikipedia.
For example, knowing that the entity men-
tion ?Obama? is described on the Wikipedia
page Barack_Obama2, one can benefit from
the Wikipedia category structure. Categories as-
signed to the Barack_Obama page can be used
as semantic classes, e.g. ?21st-century presidents
of the United States?. Another example of a
useful Wikipedia feature are the links between
Wikipedia pages. For instance, some Wikipedia
pages contain links to the Barack_Obama page.
Anchor texts of these links can provide alterna-
2The links to Wikipedia pages are given displaying only
the last part of the link which corresponds to the title of the
page. The complete link can be obtained adding this part to
http://en.wikipedia.org/wiki/.
19
tive names of this entity, e.g. ?Barack Hussein
Obama? or ?Barack Obama Junior?.
Naturally, in order to obtain semantic knowl-
edge about an entity mention from Wikipedia
one should link this mention to an appropriate
Wikipedia page, i.e. to disambiguate it using
Wikipedia as a sense inventory. The accuracy
of linking entity mentions to Wikipedia is a very
important issue. For example, such linking is a
step of the approach to coreference resolution de-
scribed in (Bryl et al, 2010). In order to evaluate
this accuracy in the framework of a coreference
resolution system, a corpus of documents, where
entity mentions are annotated with ground-truth
links to Wikipedia, is required.
The possible solution of this problem is to ex-
tend the annotation of entity mentions in a corefer-
ence resolution corpus. In the recent years, coref-
erence resolution systems have been evaluated on
various versions of the English Automatic Content
Extraction (ACE) corpus (Ponzetto and Strube,
2006; Versley et al, 2008; Ng, 2007; Culotta et
al., 2007; Bryl et al, 2010). The latest publicly
available version is ACE 20053.
In this paper we present an extension of ACE
2005 non-pronominal entity mention annotations
with ground-truth links to Wikipedia. This exten-
sion is intended for evaluation of accuracy of link-
ing entity mentions to Wikipedia pages. The an-
notation is currently in progress. At the moment
of writing this paper we have completed around
55% of the work. The extension can be exploited
by coreference resolution systems, which already
use ACE 2005 corpus for development and testing
purposes, e.g. (Bryl et al, 2010). Moreover, En-
glish ACE 2005 corpus is multi-purpose and can
be used in other information extraction (IE) tasks
as well, e.g. relation extraction. Therefore, we
believe that our extension might also be useful for
other IE tasks, which exploit semantic knowledge.
In the following we start by providing a brief
overview of the existing corpora annotated with
links to Wikipedia. In Section 3 we describe some
characteristics of the English ACE 2005 corpus,
which are relevant to the creation of the extension.
Next, we describe the general annotation princi-
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
ples and the procedure adopted to carry out the
annotation. In Section 4 we present some anal-
yses of the annotation and statistics about Inter-
Annotator Agreement.
2 Related work
Recent approaches to linking terms to Wikipedia
pages (Cucerzan, 2007; Csomai and Mihalcea,
2008; Milne and Witten, 2008; Kulkarni et al,
2009) have used two kinds of corpora for eval-
uation of accuracy: (i) sets of Wikipedia pages
and (ii) manually annotated corpora. In Wikipedia
pages links are added to terms ?only where
they are relevant to the context?4. Therefore,
Wikipedia pages do not contain the full annotation
of all entity mentions. This observation applies
equally to the corpus used by (Milne and Wit-
ten, 2008), which includes 50 documents from the
AQUAINT corpus annotated following the same
strategy5. The corpus created by (Cucerzan, 2007)
contains annotation of named entities only6. It
contains 756 annotations, therefore for our pur-
poses it is limited in terms of size.
Kulkarni et al (2009) have annotated 109 doc-
uments collected from homepages of various sites
with as many links as possible7. Their annotation
is too extensive for our purposes, since they do not
limit annotation to the entity mentions. To tackle
this issue, one can use an automatic entity mention
detector, however it is likely to introduce noise.
3 Creating the extension
The task consists of manually annotating the
non-pronominal mentions contained in the En-
glish ACE 2005 corpus with links to appropriate
Wikipedia articles. The objective of the work is
to create an extension of ACE 2005, where all the
mentions contained in the ACE 2005 corpus are
disambiguated using Wikipedia as a sense reposi-
tory to point to. The extension is intended for the
4http://en.wikipedia.org/wiki/
Wikipedia:Manual_of_Style
5http://www.nzdl.org/wikification/
docs.html
6http://research.microsoft.com/en-us/
um/people/silviu/WebAssistant/TestData/
7http://soumen.cse.iitb.ac.in/?soumen/
doc/CSAW/
20
evaluation of accuracy of linking to Wikipedia in
the framework of a coreference resolution system.
3.1 The English ACE 2005 Corpus
The English ACE 2005 corpus is composed of
599 articles assembled from a variety of sources
selected from broadcast news programs, newspa-
pers, newswire reports, internet sources and from
transcribed audio. It contains the annotation of a
series of entities (person, location, organization)
for a total of 15,382 different entities and 43,624
mentions of these entities. A mention is an in-
stance of a textual reference to an object, which
can be either named (e.g. Barack Obama), nom-
inal (e.g. the president), or pronominal (e.g. he,
his, it). An entity is an aggregate of all the men-
tions which refer to one conceptual entity. Beyond
the annotation of entities and mentions, ACE 05
contains also the annotation of local co-reference
for the entities; this means that mentions which
refer to the same entity in a document have been
marked with the same ID.
3.2 Annotating ACE 05 with Wikipedia
Pages
For the purpose of our task, not all the
ACE 05 mentions are annotated, but only the
named (henceforth NAM) and nominal (hence-
forth NOM) mentions. The resulting additional
annotation layer will contain a total of 29,300
mentions linked to Wikipedia pages. As specif-
ically regards the annotation of NAM mentions,
information about local coreference contained in
ACE 05 has been exploited in order to speed up
the annotation process. In fact, only the first
occurrence of the NAM mentions in each doc-
ument has been annotated and the annotation is
then propagated to all the other co-referring NAM
mentions in the document.
Finally, it must be noted that in ACE 05, given
a complex entity description, both the full ex-
tent of the mention (e.g. president of the United
States) and its syntactic head (e.g. ?president?)
are marked. In our Wikipedia extension only the
head of the mention is annotated, while the full ex-
tent of the mention is available from the original
ACE 05 corpus.
3.3 General Annotation Principles
Depending on the mention type to be annotated,
i.e. NAM or NOM, a different annotation strategy
has been followed. Each mention of type NAM
is annotated with a link to a Wikipedia page de-
scribing the referred entity. For instance, ?George
Bush? is annotated with a link to the Wikipedia
page George_W._Bush.
NOM mentions are annotated with a link to the
Wikipedia page which provides a description of
its appropriate sense. For instance, in the exam-
ple ?I was driving Northwest of Baghdad and I
bumped into these guys going around the capi-
tal? the mention ?capital? is linked to the page
which provides a description of its meaning, i.e.
Capital_(political). Note that the object
of linking is the textual description of an entity,
and not the entity itself. In the example, even
though from the context it is clear that the mention
?capital? refers to Baghdad, we provide a link to
the concept of capital and not to the entity Bagdad.
As a term can have both a more generic sense
and a more specific one, depending on the context
in which it occurs, mentions of type NOM can of-
ten be linked to more than one Wikipedia page.
Whenever possible, the NOM mentions are anno-
tated with a list of links to appropriate Wikipedia
pages in the given context. In such cases, links
are sorted in order of relevance, where the first
link corresponds to the most specific sense for that
term in its context, and therefore is regarded as the
best choice. For instance, for the NOM mention
head ?President? which in the context identifies
the United States President George Bush the an-
notation?s purpose is to provide a description of
the item ?President?, so the following links are
selected as appropriate: President_of_the_
United_States and President.
The correct interpretation of the term is strictly
related to the context in which the term occurs.
While performing the annotation, the context of
the entire document has always been exploited in
order to correctly identify the specific sense of the
mention.
3.4 Annotation Procedure
The annotation procedure requires that the men-
tion string is searched in Wikipedia in order to
21
find the appropriate page(s) to be used for anno-
tating the mention. In the annotation exercise, the
annotators have always taken into consideration
the context where a mention occurs, searching for
both the generic and the most specific sense of the
mention disambiguated in the context. In fact, in
the example provided above, not only ?President?,
but also ?President of the United States? has been
queried in Wikipedia as required by the context.
Not only the context, but also some features of
Wikipedia must be mentioned as they affect the
annotation procedure:
a. One element which contributes to the choice
of the appropriate Wikipedia page(s) for
one mention is the list of links proposed in
Wikipedia?s Disambiguation pages. Disam-
biguation pages are non-article pages which
are intended to allow the user to choose from
a list of Wikipedia articles defining different
meanings of a term, when the term is am-
biguous. Disambiguation pages cannot be
used as links for the annotation as they are
not suitable for the purposes of this task. In
fact, the annotator?s task is to disambiguate
the meaning of the mention, so one link,
pointing to a specific sense, is to be cho-
sen. Disambiguation pages should always be
checked as they provide useful suggestions
in order to reach the appropriate link(s).
b. In the same way as Disambiguation pages,
Wikitionary cannot be used as linking page,
as it provides a list of possible senses for a
term and not only one specific sense which is
necessary to disambiguate the mention.
c. In Wikipedia, terms may be redirected to
other terms which are related in terms of
morphological derivation; i.e. searching for
the term ?Senator? you are automatically
redirected to ?Senate?; or querying ?citizen?
you are automatically redirected to ?citizen-
ship?. Redirections have always been con-
sidered appropriate links for the term.
Some particular rules have been followed in order
to deal with specific cases in the annotation, which
are described below:
1. As explained before in Section 3.2, as a gen-
eral rule the head of the ACE 05 mention
is annotated with Wikipedia links. In those
cases where the syntactic head of the men-
tion is a multiword lexical unit, the ACE 05
practice is to mark as head only the rightmost
item of the multiword. For instance, in the
case of the multiword ?flight attendant? only
?attendant? is marked as head of the men-
tion, although ?flight attendant? is clearly a
multiword lexical unit that should be anno-
tated as one semantic whole. In our anno-
tation we take into account the meaning of
the whole lexical unit; so, in the above exam-
ple, the generic sense of ?attendant? has not
been given, whereas Flight_attendant
is considered as the appropriate link.
2. In some cases, in ACE 2005 pronouns like
?somebody?, ?anybody?, ?anyone?, ?one?,
?others?, were incorrectly marked as NOM
(instead of PRO). Such cases, which amount
to 117, have been marked with the tag ?No
Annotation?.
3. When a page exists in Wikipedia for a given
mention but not for the specific sense in that
context the ?Missing sense? annotation has
been used. One example of ?Missing sense?
is for instance the term ?heart? which has 29
links proposed in the ?Disambiguation page?
touching different categories (sport, science,
anthropology, gaming, etc.), but there is no
link pointing to the sense of ?center or core of
something?; so, when referring to the heart
of a city, the term has been marked as ?Miss-
ing sense?.
4. When no article exists in Wikipedia for a
given mention, the tag ?No page? has been
adopted.
5. Nicknames, i.e. descriptive names used
in place of or in addition to the official
name(s) of a person, have been treated as
NAM. Thus, even if nicknames look like de-
scriptions of individuals (and their reference
should not be solved, following the general
rule), they are actually used and annotated as
22
Number of annotated mentions 16310
Number of single link mentions 13774
Number of multi-link mentions 1458
Number of ?No Page? annotations 481
Number of ?Missing Sense? 480
annotations
Number of ?No Annotation? 117
annotations
Total number of links 16851
Total number of links in multi-link 3077
mentions
Table 1: Annotation data
proper names aliases. For example, given the
mention ?Butcher of Baghdad?, whose head
?Butcher? is to be annotated, the appropriate
Wikipedia link is Saddam_Hussein, auto-
matically redirected from the searched string
?Butcher of Baghdad?. The link Butcher
is not appropriate as it provides a description
of the mention. It is interesting the fact that
Wikipedia itself redirects to the page of Sad-
dam Hussein.
4 The ACE05-WIKI Extension
Up to now, the 55% of the markable men-
tions have been annotated by one annotator,
amounting to 16,310 mentions. This annotation
has been carried out by CELCT in a period
of two months from February 22 to April 30,
2010, using the on-line version of Wikipedia,
while the remaining 45% of the ACE mentions
will be annotated during August 2010. The
complete annotation will be freely available
at: http://www.celct.it/resources.
php?id_page=acewiki2010, while the
ACE 2005 corpus is distributed by LDC8.
4.1 Annotation Data Analysis
Table 1 gives some statistics about the overall
annotation. In the following sections, mentions
annotated with one link are called ?single link?,
whereas, mentions annotated with more than one
link are named ?multi-link?.
These data refer to the annotation of each sin-
gle mention. It is not possible to give statis-
tics at the entity level, as mentions have differ-
8http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
Annotation Mention Type
NAM NOM
Single link mentions 6589 7185
Multi-link mentions 79 1379
Missing sense 96 384
No Page 440 41
Table 2: Distinction of NAM and NOM in the an-
notation
ent ID depending on the documents they belong
to, and the information about the cross-document
co-reference is not available. Moreover, mentions
of type NOM are annotated with different links
depending on their disambiguated sense, making
thus impossible to group them together.
Most mentions have been annotated with only
one link; if we consider multi-link mentions, we
can say that each mention has been assigned an
average of 2,11 links (3,077/1,458).
Data about ?Missing sense? and ?No page?
are important as they provide useful information
about the coverage of Wikipedia as sense in-
ventory. Considering both ?Missing sense? and
?No page? annotations, the total number of men-
tions which have not been linked to a Wikipedia
page amounts to 6%, equally distributed between
?Missing sense? and ?No page? annotations. This
fact proves that, regarded as a sense inventory,
Wikipedia has a broad coverage. As Table 2
shows, the mentions for which more than one link
was deemed appropriate are mostly of type NOM,
while NAM mentions have been almost exclu-
sively annotated with one link only. The very few
cases in which a NAM mention is linked to more
than one Wikipedia page are primarily due to (i)
mistakes in the ACE 05 annotation (for example,
the mention ?President? was erroneously marked
as a NAM); (ii) or to cases where nouns marked
as NAM could also be considered as NOMs (see
for instance the mention ?Marine?, to mean the
Marine Corps).
Table 2 provides also statistics about the ?Miss-
ing sense? and ?No page? cases provided on men-
tions divided among the NAM and NOM type.
The ?missing sense? annotation concerns mostly
the NOM category, whereas the NAM category
is hardly affected. This attests the fact that per-
sons, locations and organizations are well repre-
23
sented in Wikipedia. This is mainly due to the
encyclopedic nature of Wikipedia where an arti-
cle may be about a person, a concept, a place,
an event, a thing etc.; instead, information about
nouns (NOM) is more likely to be found in a
dictionary, where information about the meanings
and usage of a term is provided.
4.2 Inter-Annotator Agreement
About 3,100 mentions, representing more than
10% of the mentions to be annotated, have been
annotated by two annotators in order to calculate
Inter-Annotator Agreement.
Once the annotations were completed, the
two annotators carried out a reconciliation phase
where they compared the two sets of links pro-
duced. Discrepancies in the annotation were
checked with the aim of removing only the more
rough errors and oversights. No changes have
been made in the cases of substantial disagree-
ment, which has been maintained.
In order to measure Inter-Annotator Agree-
ment, two metrics were used: (i) the Dice coeffi-
cient to measure the agreements on the set of links
used in the annotation9 and (ii) two measures of
agreement calculated at the mention level, i.e. on
the group of links associated to each mention.
The Dice coefficient is computed as follows:
Dice = 2C/(A + B)
where C is the number of common links chosen by
the two annotators, while A and B are respectively
the total number of links selected by the first and
the second annotator. Table 3 shows the results
obtained both before and after the reconciliation
9The Dice coefficient is a typical measure used to com-
pare sets in IR and is also used to calculate inter-annotator
agreement in a number of tasks where an assessor is allowed
to select a set of labels to apply to each observation. In fact,
in these cases measures such as the widely used K are not
good to calculate agreement. This is because K only offers
a dichotomous distinction between agreement and disagree-
ment, whereas what is needed is a coefficient that also allows
for partial disagreement between judgments. In fact, in our
case we often have a partial agreement on the set of links
given for each mention. Also considering only the mentions
for which a single link has been chosen, it is not possible
to calculate K statistics in a straightforward way as the cate-
gories (i.e. the possible Wikipedia pages) in some cases can-
not be determined a priori and are different for each mention.
Due to these factors chance agreement cannot be calculated
in an appropriate way.
BEFORE AFTER
reconciliation reconciliation
DICE 0.85 0.94
Table 3: Statistics about Dice coefficient
BEFORE AFTER
reconciliation reconciliation
Complete 77.98% 91.82%
On first link 84.41% 95.58%
Table 4: Agreement at the mention level
process. Agreement before reconciliation is satis-
factory and shows the feasibility of the annotation
task and the reliability of the annotation scheme.
Two measures of agreement at the mention
level are also calculated. To this purpose, we
count the number of mentions where annotators
agree, as opposed to considering the agreement on
each link separately. Mention-level agreement is
calculated as follows:
Number of mentions with annotation in agreement
Total number of annotated mentions
We calculate both ?complete? agreement and
agreement on the first link. As regards the first
measure, a mention is considered in complete
agreement if (i) it has been annotated with the
same link(s) and (ii) in the case of multi-link men-
tions, links are given in the same order. As for the
second measure, there is agreement on a mention
if both the annotators chose the same first link (i.e.
the one judged as the most appropriate), regard-
less of other possible links assigned to that men-
tion. Table 4 provides data about both complete
agreement and first link agreement, calculated be-
fore and after the annotators reconciliation.
4.3 Disagreement Analysis
Considering the 3,144 double-annotated men-
tions, the cases of disagreements amount to 692
(22,02%) before the reconciliation while they are
reduced to 257 (8,18%) after that process. It is in-
teresting to point out that the disagreements affect
the mentions of type NOM in most of the cases,
whereas mentions of type NAM are involved only
in 3,8% of the cases.
Examining the two annotations after the recon-
ciliation, it is possible to distinguish three kinds
of disagreement which are shown in Table 5 to-
24
Number of
Disagreement type Disagreements
1) No matching in the link(s)
proposed
105 (40,85%)
2) No matching on the first link,
but at least one of the other links
is the same
14 (5,45%)
3) Matching on the first link and
mismatch on the number of ad-
ditional links
138 (53,70%)
Total Disagreements 257
Table 5: Types of disagreements
gether with the data about their distribution. An
example of disagreement of type (1) is the anno-
tation of the mention ?crossing?, in the following
context: ?Marines from the 1st division have se-
cured a key Tigris River Crossing?. Searching for
the word ?river crossing? in the Wikipedia search-
box, the Disambiguation Page is opened and a
list of possible links referring to more specific
senses of the term are offered, while the generic
?river crossing? sense is missing. The annota-
tors are required to choose just one of the possi-
ble senses provided and they chose two different
links pointing to pages of more specific senses:
{Ford_%28river%29} and {Bridge}.
Another example is represented by the annota-
tion of the mention ?area? in the context : ?Both
aircraft fly at 125 miles per hour gingerly over en-
emy area?. In Wikipedia no page exists for the
specific sense of ?area? appropriate in the con-
text. Searching for ?area? in Wikipedia, the page
obtained is not suitable, and the Disambiguation
page offers a list of various possible links to either
more specific or more general senses of the term.
One annotator judged the more general Wikipedia
page Area_(subnational_entity) as ap-
propriate to annotate the mention, while the sec-
ond annotator deemed the page not suitable and
thus used the ?Missing sense? annotation.
Disagreement of type (2) refers to cases where
at least one of the links proposed by the annota-
tors is the same, but the first (i.e. the one judged
as the most suitable) is different. Given the fol-
lowing context: ?Tom, You know what Liber-
als want?, the two annotation sets provided for
the mention ?Liberal? are: {Liberalism} and
{Liberal_Party, Modern_liberalism_
in_the_United_States, Liberalism}.
The first annotator provided only one link for
the mention ?liberal?, which is different from the
first link provided by second annotator. However,
the second annotator provided also other links,
among which there is the link provided by the first
annotator.
Another example is represented by the annota-
tion of the mention ?killer?. Given the context:
?He?d be the 11th killer put to death in Texas?, the
two annotators provided the following link sets:
{Assassination, Murder} and {Murder}.
Starting from the Wikipedia disambiguation page,
the two annotators agreed on the choice of one of
the links but not on the first one.
Disagreement of type (3) refers to cases where
both annotators agree on the first link, correspond-
ing to the most specific sense, but one of them
also added link(s) considered appropriate to an-
notate the mention. Given the context: ?7th Cav-
alry has just taken three Iraqi prisoners?, the an-
notations provided for the term ?prisoners? are:
{Prisoner_of_war} and {Prisoner_of_
war, Incarceration}. This happens when
more than one Wikipedia pages are appropriate to
describe the mention.
As regards the causes of disagreement, we see
that the cases of disagreement mentioned above
are due to two main reasons:
a. The lack of the appropriate sense in
Wikipedia for the given mention
b. The different interpretation of the context in
which the mention occurs.
In cases of type (a) the annotators adopted differ-
ent strategies to perform their task, that is:
i. they selected a more general sense (i.e.
?area? which has been annotated with
Area_(subnational_entity)),
ii. they selected a more specific sense (see for
example the annotations of the mentions
?river crossing?).
iii. they selected the related senses proposed by
the Wikipedia Disambiguation page (as in
the annotation of ?killer? in the example
above).
25
Disagreement Reas. a Reas. b Tot
type (see above)
1) No match 95 10 105
2) No match on 4 10 14
first link
3) Mismatch on 138 138
additional links
Total 99 158 257
(38,5%) (61,5%)
Table 6: Distribution of disagreements according
to their cause
iv. they used the tag ?Missing sense?.
As Wikipedia is constantly evolving, adding
new pages and consequently new senses, it is
reasonable to think that the considered elements
might find the appropriate specific/general link as
time goes by.
Case (b) happens when the context is ambigu-
ous and the information provided in the text al-
lows different possible readings of the mention
to be annotated, making thus difficult to disam-
biguate its sense. These cases are independent
from Wikipedia sense repository but are related to
the subjectivity of the annotators and to the inher-
ent ambiguity of text.
Table 6 shows the distribution of disagreements
according to their cause. Disagreements of type 1
and 2 can be due to both a and b reasons, while
disagreements of type 3 are only due to b.
The overall number of disagreements shows
that the cases where the two annotators did not
agree are quite limited, amounting only to 8%.
The analyses of the disagreements show some
characteristics of Wikipedia considered as sense
repository. As reported in Table 8, in the 61,5%
of the cases of disagreement, the different anno-
tations are caused by the diverse interpretation
of the context and not by the lack of senses in
Wikipedia. It is clear that Wikipedia has a good
coverage and it proves to be a good sense disam-
biguation tool. In some cases it reveals to be too
fine-grained and in other cases it remains at a more
general level.
5 Conclusion
This paper has presented an annotation work
which connects an existing annotated corpus such
as the English ACE 2005 dataset to a Collabo-
ratively Constructed Semantic Resource such as
Wikipedia. Thanks to this connection Wikipedia
becomes an essential semantic resource for the
task of coreference resolution. On one hand, by
taking advantage of the already existing annota-
tions, with a relatively limited additional effort,
we enriched an existing corpus and made it useful
for a new NLP task which was not planned when
the corpus was created. On the other hand, our
work allowed us to explore and better understand
certain characteristics of the Wikipedia resource.
For example we were able to demonstrate in quan-
titative terms that Wikipedia has a very good cov-
erage, at least as far as the kind of entity men-
tions which are contained in the ACE 2005 dataset
(newswire) is concerned.
Acknowledgments
The research leading to these results has re-
ceived funding from the ITCH project (http://
itch.fbk.eu), sponsored by the Italian Min-
istry of University and Research and by the Au-
tonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint
Research Project under Future Internet - Internet
of Content program of the Information Technol-
ogy Center, Fondazione Bruno Kessler.
We thank Giovanni Moretti from CELCT for
technical assistance.
References
Bryl, Volha, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In
Proceedings of the 19th European Conference on
Artificial Intelligence (ECAI 2010), August.
Csomai, Andras and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Cucerzan, Silviu. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June. Association for Computational Linguistics.
26
Culotta, Aron, Michael L. Wick, and Andrew McCal-
lum. 2007. First-order probabilistic models for
coreference resolution. In Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 81?88.
Fellbaum, Christiane, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Kulkarni, Sayali, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of wikipedia entities in web text. In KDD
?09: Proceedings of the 15th ACM SIGKDD inter-
national conference on Knowledge discovery and
data mining, pages 457?466, New York, NY, USA.
ACM.
Mihalcea, Rada and Dan I. Moldovan. 2001.
Ez.wordnet: Principles for automatic generation of
a coarse grained wordnet. In Russell, Ingrid and
John F. Kolen, editors, FLAIRS Conference, pages
454?458. AAAI Press.
Milne, David and Ian H. Witten. 2008. Learning
to link with wikipedia. In CIKM ?08: Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 509?518, New York,
NY, USA. ACM.
Ng, Vincent and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104?111.
Ng, Vincent. 2007. Semantic class induction and
coreference resolution. In ACL 2007, Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, June 23-30, 2007,
Prague, Czech Republic, pages 536?543.
Ponzetto, S. P. and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 192?
199.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistic, 27(4):521?544.
Versley, Yannick, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
Bart: a modular toolkit for coreference resolution.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies, pages 9?12.
27
