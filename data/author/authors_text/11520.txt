Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 84?92,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Minimally Supervised Model of Early Language Acquisition
Michael Connor
Department of Computer Science
University of Illinois
connor2@uiuc.edu
Yael Gertner
Department of Psychology
University of Illinois
ygertner@cyrus.psych.uiuc.edu
Cynthia Fisher
Department of Psychology
University of Illinois
cfisher@cyrus.psych.uiuc.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@uiuc.edu
Abstract
Theories of human language acquisition as-
sume that learning to understand sentences is
a partially-supervised task (at best). Instead
of using ?gold-standard? feedback, we train
a simplified ?Baby? Semantic Role Labeling
system by combining world knowledge and
simple grammatical constraints to form a po-
tentially noisy training signal. This combina-
tion of knowledge sources is vital for learn-
ing; a training signal derived from a single
component leads the learner astray. When this
largely unsupervised training approach is ap-
plied to a corpus of child directed speech, the
BabySRL learns shallow structural cues that
allow it to mimic striking behaviors found in
experiments with children and begin to cor-
rectly identify agents in a sentence.
1 Introduction
Sentence comprehension involves assigning seman-
tic roles to sentence constituents, determining who
does what to whom. How do young children be-
gin learning to interpret sentences? The structure-
mapping view of early verb and syntax acquisition
proposes that children treat the number of nouns
in the sentence as a cue to its semantic predicate-
argument structure (Fisher, 1996), and represent lan-
guage experience in an abstract format that promotes
generalization to new verbs (Gertner et al, 2006).
Theories of human language acquisition assume
that learning to understand sentences is naturally
a partially-supervised task: the fit of the learner?s
predicted meaning with the referential context and
background knowledge provides corrective feed-
back (e.g., Pinker (1989)). But this feedback must
be noisy; referential scenes provide ambiguous in-
formation about the semantic roles of sentence par-
ticipants. For example, the same participant could
be construed as an agent who ?fled? or as a patient
who is ?chased?.
In this paper, we address this problem by de-
signing a Semantic Role Labeling system (SRL),
equipped with shallow representations of sentence
structure motivated by the structure-mapping ac-
count, that learns with no gold-standard feedback at
all. Instead, the SRL provides its own internally-
generated feedback based on a combination of world
knowledge and linguistic constraints. As a sim-
ple stand-in for world knowledge, we assume that
the learner has animacy information for some set of
nouns, and uses this knowledge to determine their
likely roles. In terms of linguistic constraints, the
learner uses simple knowledge about the possible ar-
guments verbs can appear with.
This approach has two goals. The first is to in-
form theories of language learning by investigating
the utility of the proposed internally-generated feed-
back as one component of the human learner?s tools.
Second, from an NLP and Machine Learning per-
spective we propose to inject information into a su-
pervised learning algorithm through a channel other
than labeled training data. From both perspectives,
our key question is whether the algorithm can use
these internally labeled examples to extract general
patterns that can be applied to new cases.
By building a model that uses shallow representa-
tions of sentences and minimal feedback, but that
84
mimics features of language development in chil-
dren, we can explore the nature of initial representa-
tions of syntactic structure.
1.1 Background
The structure-mapping account of early verb and
syntax acquisition makes strong predictions. First,
it predicts early use of simple structural cues to sen-
tence interpretation. As soon as children can iden-
tify some nouns, they should assign different in-
terpretations to transitive and intransitive sentences,
simply by assuming that each noun in the sentence
bears a distinct semantic role. Similarly, language-
specific syntactic learning should transfer rapidly to
new verbs. Second, however, this account predicts
striking errors. In ?Fred and Ginger danced?, an
intransitive verb occurs with two nouns. If chil-
dren interpret any two-noun sentence as if it were
transitive, they should mistakenly interpret the order
of two nouns in such conjoined-subject intransitive
sentences as agent-patient. Experiments with young
children support these predictions. 21-month-olds
use the number of nouns to understand sentences
containing new verbs (Yuan et al, 2007), generalize
what they have learned about transitive word-order
to new verbs (Gertner et al, 2006), and make the
predicted error, treating intransitive sentences con-
taining two nouns as if they were transitive (Gert-
ner and Fisher, 2006). By 25 months, children have
learned enough about English syntax to interpret
conjoined-subject intransitives differently from tran-
sitives (Naigles, 1990).
Our previous computational experiments with a
system for automatic semantic role labeling (Con-
nor et al, 2008) suggest that it is possible to learn
to assign basic semantic roles based on the simple
representations proposed by the structure-mapping
view. The classifier?s features were limited to lexical
information (nouns and verbs only) and the number
and order of nouns in the sentence, and trained on a
sample of child-directed speech annotated in Prop-
Bank (Kingsbury and Palmer, 2002) style. Given
this training, our classifier learned to label the first
of two nouns as an agent and the second as a patient.
Even amid the variability of casual speech, simply
representing the target word as the first or the second
of two nouns significantly boosts SRL performance
(relative to a lexical baseline) on transitive sentences
containing novel verbs. This result depends on key
assumptions of the structure-mapping view, includ-
ing abstract representations of semantic roles, and
abstract but simple representations of sentence struc-
ture. Another approach was taken by (Alishahi and
Stevenson, 2007). Their model learned to assign se-
mantic roles without prior knowledge of abstract se-
mantic roles. Instead, it relied on built-in syntactic
knowledge and a rich hierarchical representation of
semantic knowledge to learn links between sentence
structure and meaning.
However, our previous experimental design has
a serious drawback that limits its relevance to the
study of how children learn their first language.
In training, our SRL received gold standard feed-
back consisting of correctly labeled sentences. Thus
when the SRL made a mistake in identifying the se-
mantic role of any noun in a sentence, it received
feedback about the ?true? semantic role of this noun.
As noted above, this is an unrealistic assumption for
the input to human learners.
Here we ask whether an SRL could learn to in-
terpret simple sentences even without gold-standard
feedback by relying on world knowledge to gen-
erate its own feedback. This internally-generated
feedback was based on the following assumptions.
First, nouns referring to animate entities are likely
to be agents, and nouns referring to inanimate en-
tities are not. Second, each predicate takes at most
one agent. Such role uniqueness constraints are typ-
ically included in linguistic discussions of thematic
roles (Bresnan, 1982; Carlson, 1998). The animacy
heuristic is not always correct, of course. For ex-
ample, in ?The door hit you?, an inanimate object
is the agent of action, and an animate being is the
patient. Nevertheless, it is useful for two reasons.
First, there is a strong cross-linguistic association
between agency and animacy (Aissen, 1999; Dowty,
1991). Second, from the first year of life, children
have strong expectations about the capacities of an-
imate and inanimate entities (Baillargeon et al, in
press). Given the universal tendency for speakers to
talk about animate action on less animate objects,
many sentences will present useful training data to
the SRL: In ordinary sentences such as ?You broke
it,? feedback generated based on animacy will re-
semble gold-standard feedback.
85
2 Learning Model
Our learning task is similar to the full SRL task (Car-
reras and Ma`rquez, 2004), except that we classify
the roles of individual words rather than full phrases.
A full automatic SRL system (e.g. (Punyakanok et
al., 2005a)) typically involves multiple stages to 1)
parse the input, 2) identify arguments, 3) classify
those arguments, and then 4) run inference to make
sure the final labeling for the full sentence does not
violate any linguistic constraints. Our simplified
BabySRL architecture essentially replaces the first
two steps with developmentally plausible heuris-
tics. Rather than identifying arguments via a learned
classifier with access to a full syntactic parse, the
BabySRL treats each noun in the sentence as a can-
didate argument and assigns a semantic role to it. A
simple heuristic collapsed compound or sequential
nouns to their final noun, an approximation of the
head noun of the noun phrase. For example, ?Mr.
Smith? was treated as the single noun ?Smith?. Other
complex noun phrases were not simplified in this
way. Thus, a phrase such as ?the toy on the floor?
would be treated as two separate nouns, ?toy? and
?floor?. This represents the assumption that young
children know ?Mr. Smith? is a single name, but
they do not know all the predicating terms that may
link multiple nouns into a single noun phrase. The
simplified learning task of the BabySRL implements
a key assumption of the structure-mapping account:
that at the start of multiword sentence comprehen-
sion children can tell which words in a sentence are
nouns (Waxman and Booth, 2001), and treat each
noun as a candidate argument.
We further simplify the SRL task such that clas-
sification is between two macro-roles: A0 (agent)
and A1 (non-agent; all non-A0 arguments). We did
so because we reason that this simplified feedback
scheme can be primarily informative for a first stage
of learning in which learners identify how their lan-
guage identifies agents vs. non-agents in sentences.
In addition, this level of role granularity is more con-
sistent across verbs (Palmer et al, 2005).
For argument classification we use a linear clas-
sifier trained with a regularized perceptron update
rule (Grove and Roth, 2001). This learning algo-
rithm provides a simple and general linear classifier
that works well in other language tasks, and allows
us to inspect the weights of key features to determine
their importance for classification.
For the final predictions, the classifier uses
predicate-level inference to ensure coherent argu-
ment assignments. In our task the only active con-
straints are that all nouns require a tag, and that they
have unique labels, which for this restricted case of
A0 vs. not A0 means there will be only one agent.
2.1 Training and Feedback
The key feature of our BabySRL lies in the way
feedback is provided. Ordinarily, during training,
SRL classifiers predict a semantic label for an argu-
ment and receive gold-standard feedback about its
correct semantic role. Such accurate feedback is not
available for the child learner. Children must rely on
their own error-prone interpretation of events to sup-
ply feedback. This internally-generated feedback
signal is presumably derived from multiple infor-
mation sources, including the plausibility of partic-
ular combinations of argument-roles given the cur-
rent situation (Chapman and Kohn, 1978). Here
we model this process by combining background
knowledge with linguistic constraints to generate
a training signal. The ?unsupervised? feedback is
based on: 1) nouns referring to animate entities are
assumed to be agents, while nouns referring to inan-
imate entities are non-agents and 2) each predicate
can have at most one agent.
This internally-generated feedback bears some
similarities to Inference Based Training (Pun-
yakanok et al, 2005b). In both cases the feedback to
local supervised classifiers depends on global con-
straints. With IBT, feedback for mistakes is only
considered after global inference, but for BabySRL
the global inference is applied to the feedback itself.
Figure 1 gives an overview of the training and test-
ing procedure, making clear the distinction between
training and testing inference.
The training data were samples of parental speech
to one child (?Sarah?; (Brown, 1973), available
via Childes (MacWhinney, 2000)). We trained
on parental utterances in samples 1 through 80,
recorded at child age 2;3-3;10 years. All verb-
containing utterances without symbols indicating
long pauses or unintelligible words were automat-
ically parsed with the Charniak parser (Charniak,
1997) and annotated using an existing SRL sys-
86
tem (Punyakanok et al, 2005a). In this initial
pass, sentences with parsing errors that misidenti-
fied argument boundaries were excluded. Role la-
bels were hand-corrected using the PropBank anno-
tation scheme. The child-directed speech training
set consists of about 8300 tagged arguments over
4700 sentences, of which a majority had a single
verb and two labeled nouns1. The annotator agree-
ment on this data set ranged between 95-97% at the
level of arguments. In the current paper these role-
tagged examples provide a comparison point for the
utility of animacy-based feedback during training.
Our BabySRL did not receive these hand-
corrected semantic roles during training. Instead,
for each training example it generated its own feed-
back based in part on an animacy table. To ob-
tain the animacy table we coded the 100 most fre-
quent nouns in our corpus (which constituted less
than 15% of the total number of nouns, but 65%
of noun occurrences). We considered 84 of these
nouns to be unambiguous in animacy: Personal pro-
nouns and nouns referring to people were coded as
animate (30). Nouns referring to objects, body parts,
locations, and times, were coded as inanimate (54).
The remaining 16 nouns were excluded because they
were ambiguous in animacy (e.g., dolls, actions).
We test 3 levels of feedback representing increas-
ing amounts of linguistic knowledge used to gener-
ate internal interpretations of the sentences. Using
the animacy table, Animacy feedback (Feedback 1)
was generated as follows: for each noun in training,
if it was coded as animate it was labeled A0, if it was
coded as inanimate it was labeled A1, otherwise no
feedback was given. Because of the frequency of an-
imate nouns this gives a skewed distribution of 4091
animate agents and 1337 inanimate non-agents.
(Feedback 2) builds on Feedback 1 by adding an-
other linguistic constraint: if a noun was not found
in the animacy-table and there is another noun in the
sentence that is labeled A0, then the unknown noun
is an A1. In the training set this adds non-agent
training examples, yielding 4091 A0 and 2627 A1
examples.
Feedback 1 and Feedback 2 allow two nouns in
a sentence to be labeled with A0. Feedback 3 pre-
1Corpus available at http://l2r.cs.uiuc.edu/
?
cogcomp
vents this; it implements a unique agent constraint
that incorporates bootstrapping to make an ?intelli-
gent guess? about which noun is the correct agent.
This decision is made based on the current predic-
tions of the classifier. Given a sentence with multi-
ple animate nouns, the classifier predicts a label for
each, and the one with the highest score for A0 is
declared the true agent and the rest are classified as
non-agent. Note that we cannot apply role unique-
ness to the A1 (not A0) role, given that this label en-
compasses multiple non-agent roles. This feedback
scheme, allowing at most one agent per sentence, re-
duces the number of A0 examples and increases the
number of A1 examples to 3019 A0 and 3699 A1.
2.2 Feature Sets
The basic feature we propose is the noun pattern fea-
ture (NPattern). We hypothesize that children use
the number and order of nouns to represent argument
structure. The NPattern feature indicates how many
nouns there are in the sentence and which noun the
target is. For example, in the two-noun sentence
?Did you see it??, ?you? has a feature active indicat-
ing that it is the first noun of two. Likewise, for ?it? a
feature is active indicating that it is the second of two
nouns. This feature is easy to compute once nouns
are identified, and does not require fine-grained part-
of-speech distinctions.
We compare the noun pattern feature to a baseline
lexical feature set (Words): the target noun and the
root form of the predicate. The NPattern feature set
includes lexical features as well as features indicat-
ing the number and order of the noun (first of two,
second of three, etc.). With gold-standard role feed-
back, (Connor et al, 2008) found that the NPattern
feature allowed the BabySRL to generalize to new
verbs: it increased the system?s tendency to predict
that the first of two nouns was A0 and the second of
two nouns A1 for verbs not seen in training.
To the extent that in child-directed speech the first
of two nouns tends to be an agent, and agents tend
to be animate, we anticipate that with the NPat-
tern feature the BabySRL will learn the same thing,
even when provided with internally-generated feed-
back based on animacy. In Connor et al (2008) we
showed that, because this NPattern feature set repre-
sents only the number and order of nouns, with this
feature set the BabySRL reproduced the errors chil-
87
Algorithm BABYSRL TRAINING
INPUT: Unlabeled Training Sentences
OUTPUT: Trained Argument Classifier
For each training sentence
Generate Internal Feedback: Find interpreted meaning
Feedback 1: Apply Animacy Heuristic
For each argument in the sentence (noun)
If noun is animate? mark as agent
If noun is inanimate? mark as non-agent
else leave unknown
end
Feedback 2: Known agent constraint
Beginning with Feedback 1
If an agent was found
Mark all unknown arguments as non-agent
Feedback 3: Unique agent constraint
Beginning with Feedback 2
If multiple agents found
Find argument with highest agent prediction
Leave this argument an agent, mark rest as non-agent
Train Supervised Classifier
Present each argument to classifier
Update if interpreted meaning does not match
classifier prediction
end
(a) Training
Algorithm BABYSRL TESTING
INPUT: Unlabeled Testing Sentences
OUTPUT: Role labels for each argument
For each test sentence
Predict roles for each argument
Test Inference:
Find assignment to whole sentence with highest sum of
predictions that doesn?t violate uniqueness constraint
end
(b) Testing
Figure 1: BabySRL training and testing procedures. In-
ternal feedback is generated using animacy plus optional
constraints. This feedback is fed to a supervised learning
algorithm to create an agent-identification classifier.
dren make as noted in the Introduction, mistakenly
assigning agent- and non-agent roles to the first and
second nouns in intransitive test sentences contain-
ing two nouns. In the present paper, the linguistic
constraints provide an additional cause for this er-
ror. In addition, as a first step in examining recov-
ery from the predicted error, Connor et al (2008)
added a verb position feature (VPosition) specifying
whether the target noun is before or after the verb.
Given these features, the BabySRL?s classification
of transitive and two-noun intransitive test sentences
diverged, because the gold-standard training sup-
ported the generalization that pre-verbal nouns tend
to be agents, and post-verbal nouns tend to be pa-
tients. In the present paper we include the VPosition
feature for comparison to Connor et al (2008).
2.3 Testing
To evaluate the BabySRL we tested it with both a
held-out sample of child-directed speech, and with
constructed sentences containing novel verbs, like
those used in the experiments with children de-
scribed above. These sentences provide a more
stringent test of generalization than the customary
test on a held-out section of the data. Although the
held-out section of data contains unseen sentences,
it may contain few unseen verbs. In a held out sec-
tion of our data, 650 out of 696 test examples contain
a verb that was encountered in training. Therefore,
the customary test cannot tell us whether the system
generalizes what it learned to novel verbs.
All constructed test sentences contained a novel
verb (?gorp?). We constructed two test sentence tem-
plates: ?A gorps B? and ?A and B gorp?, where A and
B were replaced with nouns that appeared more than
twice in training. For each test sentence template we
built a test set of 100 sentences by randomly sam-
pling nouns in two different ways described next.
Full distribution: The first nouns in the test sen-
tences (A) are chosen from the set of all first nouns
in our corpus, taking their frequency into account
when sampling. The second nouns in the sentences
(B) are chosen from the set of nouns appearing as
second nouns in the sentence of our corpus. This
way of sampling the nouns will maximize the SRL?s
test performance based on the baseline feature set
of lexical information alone (Words). This is so be-
cause in our data many sentences have an animate
first noun and an inanimate second noun. Based on
these words alone the SRL could learn to predict an
A0-A1 role sequence for our test sentences. Nev-
ertheless, we expect that when the BabySRL is also
given the NPattern feature it should be able to per-
form better than this high lexical baseline.
Two animate nouns: In these test sentences the
A and B nouns are chosen from our list of animate
nouns. We chose nouns from this list that were
fairly frequent (ranging from 8 to 240 uses in the
88
corpus), and that occurred roughly equally as the
first and second noun. This mimics the sentences
used in the experiments with children (e.g., ?The
girl is kradding the boy!?). The lexical baseline sys-
tem?s tendency to assign an A0-A1 sequence to these
nouns should be much lower for these test sentences.
We therefore expect the contribution of the NPattern
feature to be more apparent in these test sentences.
The test sentences with novel verbs ask whether
the classifier transfers its learning about argument
role assignment to unseen verbs. Does it assume
the first of two nouns in a simple transitive sentence
(?A gorps B?) is the agent (A0) and the second is
not an agent (A1)? In (Connor et al, 2008) we
showed that a system with the same feature and rep-
resentations also over-generalized this rule to two-
noun intransitives (?A and B gorp?), mimicking chil-
dren?s behavior. In the present paper this error is
over-determined, because the classifier learns only
an agent/non-agent contrast, and the linguistic con-
straints forbid duplicate agents in a sentence. How-
ever, for comparison to the earlier paper we test our
system on the ?A and B gorp? sentences as well.
3 Experimental Results
Our experiments use internally-generated feedback
to train simple, abstract structural features: the
NPattern features that proved useful with gold-
standard training in Connor et al (2008). Sec-
tion 3.1 tests the system on agent-identification in
held-out sentences from the corpus, and demon-
strates that the animacy-based feedback is useful,
yielding SRL performance comparable to that of a
system trained with 1000 sentences of gold-standard
feedback. Section 3.2 presents the critical novel-
verb test data, demonstrating that this system repli-
cates key findings of (Connor et al, 2008) with no
gold standard feedback. Using only noisy internally-
generated feedback, the BabySRL learned that the
first of two nouns is an agent, and generalized this
knowledge to sentences with novel verbs.
3.1 Comparing Self Generated Feedback with
Gold Standard Feedback
Table 1 reports for the varying feedback schemes,
the A0 F1 performance for a system with either lex-
ical baseline feature (Words) or structural features
Feedback Words +NPattern
1. Just Animacy 0.72 0.73
2. + non A0 Inference 0.74 0.75
3. + unique A0 bootstrap 0.70 0.74
10 Gold 0.43 0.47
100 Gold 0.61 0.65
1000 Gold 0.75 0.76
Table 1: Agent identification results (A0 F1) on held-
out sections of the Sarah Childes corpus. We compare
a classifier trained with various amounts of gold labeled
data (averaging over 10 different samples at each level
of data). For noun pattern features the internally gener-
ated bootstrap feedback provides comparable accuracy to
training with between 100-1000 fully labeled examples.
(+NPattern) when tested on a held-out section of
the Sarah Childes corpus section 84-90, recorded
at child ages 3;11-4;1 years. Agent identification
based on lexical features is quite accurate given an-
imacy feedback alone (Feedback 1). As expected,
because many agents are animate, the animacy tag-
ging heuristic itself is useful. As linguistic con-
straints are added via non-A0 inference (Feedback
2), performance increases for both the lexical base-
line and NPattern feature-set, because the system ex-
periences more non-A0 training examples.
When the unique A0 constraint is added (Feed-
back 3), the lexical baseline performance decreases,
because for the first time animate nouns are being
tagged as non-agents. With this feedback the NPat-
tern feature set yields a larger improvement over lex-
ical baseline, showing that it extracts more general
patterns. We discuss the source of these feedback
differences in the novel-verb test section below.
We compared the usefulness of the internally-
generated feedback to gold-standard feedback by
training a classifier equipped with the same features
on labeled sentences. We reduced the SRL labeling
for the training sentences to the binary agent/non-
agent set, and trained the classifier with 10, 100,
or 1000 labeled examples. Surprisingly, the simple
feedback derived from 84 nouns labeled with ani-
macy information yields performance equivalent to
between 100 and 1000 hand-labeled examples.
89
Full Distribution Nouns Animate Nouns
Feedback Words NPattern VPosition Words NPattern VPosition
?A gorps B?
1. Animacy 0.86 0.86 0.87 0.76 0.79 0.70
2. + non A0 Inference 0.87 0.92 0.90 0.63 0.86 0.85
3. + unique A0 bootstrap 0.87 0.95 0.89 0.63 0.82 0.66
?A and B gorp?
1. Animacy 0.86 0.86 0.84 0.76 0.79 0.68
2. + non A0 Inference 0.87 0.92 0.85 0.63 0.86 0.66
3. + unique A0 bootstrap 0.87 0.95 0.86 0.63 0.82 0.63
Table 2: Percentage of sentences interpreted as agent first (%A0-A1) by the BabySRL when trained on unlabeled data
with the 3 internally-generated feedback schemes described in the text. Two different two-noun sentence structures
were used (?A gorps B?, ?A and B gorp?), along with two different methods of sampling the nouns (Full Distribution,
Animate Nouns) to create test sets with 100 sentences each.
3.2 Comparing Structural Features with
Lexical Features
The previous section shows that the BabySRL
equipped with simple structural features can use
internally generated feedback to learn a simple
agent/non-agent classification, and apply it to un-
seen sentences. In this section we probe what the
SRL has learned by testing generalization to new
verbs in constructed sentences. Table 2 summarizes
these experiments. The results are broken down both
by what sentence structure is used in test (?A gorps
B?, ?A and B gorp?) and how the nouns ?A? and
?B? are sampled (Full Distribution, Animate Nouns).
The results are presented in terms of %A0A1: the
percentage of test sentences that are assigned an
Agent role for ?A? and a non-Agent role for ?B?.
For the transitive ?A gorps B? sentences, A0A1 is
the correct interpretation; A should be the agent. As
predicted, when A and B are sampled from the full
distribution of nouns, simply basing classification on
the Words feature-set aleady strongly predicts this
A0A1 ordering for the majority of cases. This is be-
cause the data (language in general, child directed
speech in particular here) are naturally distributed
such that particular nouns that refer to animates tend
to be agents, and tend to appear as first nouns, and
those that refer to inanimates tend to be non-agents
and second nouns. Thus, a learner representing sen-
tence information in terms of words only succeeds
with full-distribution ?A gorps B? test sentences even
with the simplest animacy feedback (Feedback 1);
the A and B nouns in these test sentences reproduce
the learned distribution. Also as predicted, given this
simple feedback, the additional higher-level features
(NPattern, VPosition) do not improve much upon
the lexical baseline. This is due to the strictly lexical
nature of the animacy feedback: each lexical item
(e.g., ?you? or ?it?) will always either be animate or
inanimate and therefore either A0 or A1. Therefore,
in this case lexical features are the best predictors.
Also as expected, higher-level features (NPat-
tern, and VPosition) improve performance with a
more sophisticated self-generated feedback scheme.
Adding inferred feedback to label unknown nouns
as A1 when the sentence contains a known animate
noun (Feedback 2) decreases the ratio of A0 to non-
A0 arguments. This feedback is less lexically deter-
mined: for nouns whose animacy is unknown, feed-
back will be provided only if there is another ani-
mate noun in the sentence. This leaves room for the
abstract structural features to play a role.
Next we test a form of the unique-A0 constraint.
In (Feedback 3), in addition to the non-A0 inference
added in (Feedback 2), the BabySRL intelligently
selects one noun as A0 in sentences with multiple
animate nouns. With this feedback we see a striking
increase in test performance based on the noun pat-
tern features over the lexical baseline. In principle,
this feedback mechanism might permit the classifier
to start to learn that animate nouns are not always
agents. Early in training, the noun pattern feature
learns that first nouns tend to be animate (and there-
fore interpreted as agents), and it feeds this informa-
90
tion back into subsequent training examples, gen-
erating new feedback that continues to interpret as
agents those animate nouns that appear first in sen-
tences containing two animates.
For the nouns sampled from the full distribution
we see that structural features improve over the lex-
ical baseline despite the high performance of the
lexical baseline. This finding tells us that simple
representations of sentence structure can be use-
ful in learning to interpret sentences even with no
gold-standard training. Provided only with sim-
ple internally-generated feedback based on animacy
knowledge and linguistic constraints, the BabySRL
learned that the first of two nouns tends to be an
agent, and the second of two does not.
The results for the ?A B gorp? test sentences
demonstrate an important way in which predictions
based on different simple structure representations
can diverge. As expected, the NPattern feature
makes the same overgeneralization error seen by
children and the system in (Connor et al, 2008).
However, when the VPosition feature is added, dif-
ferent results are obtained for the ?A gorp B? and
?A and B gorp? sentences. The SRL predicts fewer
A0A1 for ?A and B gorp? (it cannot predict the ex-
pected A0A0 because of the uniqueness constraint
used in test inference).
Next, we replicate our findings by performing the
same experiments with test sentences in which both
?A? and ?B? are animate. Because lexical features
alone cannot determine if ?A? or ?B? should be the
agent, it is a more sensitive test of generalization.
When we look at the lexical baseline for animate
sentences, the agent-first percentage is lower com-
pared to the full distribution results, because the
word features indicate nearly evenly that both nouns
should be agents, so the Words baseline model must
rely on small, chance differences in its experience
with particular words. This percentage is still well
above chance due to the method used to apply in-
ference during testing. Recall that the classifier uses
predicate-level inference at test to ensure that only
one argument is labeled A0. This inference is imple-
mented using a beam search that looks at arguments
in a fixed order and roles from A0 up. Thus in the
case of ties there is a preference for first seen solu-
tions, meaning A0A1 in this case. This bias has a
large effect on the SRL?s baseline performance with
the test sentences containing two animate nouns.
Despite this high baseline, however, because lexical
features alone cannot determine if ?A? or ?B? should
be the agent, we are able to see more clearly the im-
provement gained by including structural features.
Regardless of our testing scheme, we see that as
the feedback incorporates more information, both
added linguistic constraints and the SRL?s own prior
learning, the noun pattern structural feature is better
used to identify agents beyond the lexical baseline.
The largest improvement over this lexical baseline is
obtained by combining knowledge of animacy with
a single-agent constraint and bootstrapping predic-
tions based on prior learning.
4 Conclusion and Future Work
Conventional approaches to supervised learning re-
quire creating large amounts of hand-labeled data.
This is labor-intensive, and limits the relevance of
the work to the study of how children learn lan-
guages. Children do not receive perfect feedback
about sentence interpretation. Here we found that
our simple SRL classifier can, to a surprising de-
gree, attain performance comparable to training with
1000 sentences of labeled data. This suggests that
fully labeled training data can be supplemented by a
combination of simple world knowledge (animates
make good agents) and linguistic constraints (each
verb has only one agent). The combination of these
sources provides an informative training signal that
allows our BabySRL to learn a high-level seman-
tic task and generalize beyond the training data we
provided to it. The SRL learned, based on the dis-
tribution of animates in sentences of child-directed
speech, that the first of two nouns tends to be an
agent. It did so based on representations of sentence
structure as simple as the ordered set of nouns in
the sentence. This demonstrates that it is possible to
learn how to correctly assign semantic roles based
on these very simple cues. This together with exper-
imental work (e.g. (Fisher, 1996) suggests that such
representations might play a role in children?s early
sentence comprehension.
Acknowledgments
This research is supported by NSF grant BCS-
0620257 and NIH grant R01-HD054448.
91
References
J. Aissen. 1999. Markedness and subject choice in opti-
mality theory. Natural Language and Linguistic The-
ory, 17:673?711.
A. Alishahi and S. Stevenson. 2007. A computational
usage-based model for learning general properties of
semantic roles. In Proceedings of the 2nd European
Cognitive Science Conference.
R. Baillargeon, D. Wu, S. Yuan, J. Li, and Y. Luo.
(in press). Young infants expectations about self-
propelled objects. In B. Hood and L. Santos, editors,
The origins of object knowledge. Oxford University
Press, Oxford.
J. Bresnan. 1982. The mental representation of gram-
matical relations. MIT Press, Cambridge MA.
R. Brown. 1973. A First Language. Harvard University
Press, Cambridge, MA.
G. Carlson. 1998. Thematic roles and the individuation
of events. In S. D. Rothstein, editor, Events and Gram-
mar, pages 35?51. Kluwer, Dordrecht.
X. Carreras and L. Ma`rquez. 2004. Introduction to the
CoNLL-2004 shared tasks: Semantic role labeling. In
Proceedings of CoNLL-2004, pages 89?97. Boston,
MA, USA.
R. S. Chapman and L. L. Kohn. 1978. Comprehension
strategies in two- and three-year-olds: Animate agents
or probable events? Journal of Speech and Hearing
Research, 21:746?761.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. National
Conference on Artificial Intelligence.
M. Connor, Y. Gertner, C. Fisher, and D. Roth. 2008.
Baby srl: Modeling early language acquisition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL), Aug.
D. Dowty. 1991. Thematic proto-roles and argument se-
lection. Language, 67:547?619.
C. Fisher. 1996. Structural limits on verb mapping:
The role of analogy in children?s interpretation of sen-
tences. Cognitive Psychology, 31:41?81.
Y. Gertner and C. Fisher. 2006. Predicted errors in early
verb learning. In 31st Annual Boston University Con-
ference on Language Development.
Y. Gertner, C. Fisher, and J. Eisengart. 2006. Learning
words and rules: Abstract knowledge of word order
in early sentence comprehension. Psychological Sci-
ence, 17:684?691.
A. Grove and D. Roth. 2001. Linear concepts and hidden
variables. Machine Learning, 42(1/2):123?141.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC-2002, Spain.
B. MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Third Edition. Lawrence Elrbaum
Associates, Mahwah, NJ.
L. R. Naigles. 1990. Children use syntax to learn verb
meanings. Journal of Child Language, 17:357?374.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. In Computational Linguistics 31(1).
S. Pinker. 1989. Learnability and Cognition. Cam-
bridge: MIT Press.
V. Punyakanok, D. Roth, and W. Yih. 2005a. The neces-
sity of syntactic parsing for semantic role labeling. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI), pages 1117?1123.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005b.
Learning and inference over constrained output. In
Proc. of the International Joint Conference on Artifi-
cial Intelligence (IJCAI), pages 1124?1129.
S. R. Waxman and A. Booth. 2001. Seeing pink ele-
phants: Fourteen-month-olds?s interpretations of novel
nouns and adjectives. Cognitive Psychology, 43:217?
242.
S. Yuan, C. Fisher, Y. Gertner, and J. Snedeker. 2007.
Participants are more than physical bodies: 21-month-
olds assign relational meaning to novel transitive
verbs. In Biennial Meeting of the Society for Research
in Child Development, Boston, MA.
92
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 767?777,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
The Necessity of Combining Adaptation Methods
Ming-Wei Chang, Michael Connor and Dan Roth
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang21,connor2,danr}@uiuc.edu
Abstract
Problems stemming from domain adaptation
continue to plague the statistical natural lan-
guage processing community. There has been
continuing work trying to find general purpose
algorithms to alleviate this problem. In this
paper we argue that existing general purpose
approaches usually only focus on one of two
issues related to the difficulties faced by adap-
tation: 1) difference in base feature statistics
or 2) task differences that can be detected with
labeled data.
We argue that it is necessary to combine these
two classes of adaptation algorithms, using
evidence collected through theoretical analy-
sis and simulated and real-world data exper-
iments. We find that the combined approach
often outperforms the individual adaptation
approaches. By combining simple approaches
from each class of adaptation algorithm, we
achieve state-of-the-art results for both Named
Entity Recognition adaptation task and the
Preposition Sense Disambiguation adaptation
task. Second, we also show that applying an
adaptation algorithm that finds shared repre-
sentation between domains often impacts the
choice in adaptation algorithm that makes use
of target labeled data.
1 Introduction
While recent advances in statistical modeling for
natural language processing are exciting, the prob-
lem of domain adaptation remains a big challenge.
It is widely known that a classifier trained on one do-
main (e.g. news domain) usually performs poorly on
a different domain (e.g. medical domain) (Jiang and
Zhai, 2007; Daume? III, 2007). The inability of cur-
rent statistical models to handle multiple domains is
one of the key obstacles hindering the progress of
NLP.
Several general purpose algorithms have been
proposed to address the domain adaptation prob-
lem: (Blitzer et al, 2006; Jiang and Zhai, 2007;
Daume? III, 2007; Finkel and Manning, 2009). It
is widely believed that the drop in performance of
statistical models on new domains is due to the
shift of the joint distribution of labels and examples,
P (Y,X), from domain to domain, where X repre-
sents the input space and Y represents the output
space. In general, we can separate existing adap-
tation algorithms into two categories:
Focuses on P (X) This type of adaptation algo-
rithm attempts to resolve the difference between the
feature space statistics of two domains. While many
different techniques have been proposed, the com-
mon goal of these algorithms is to find a better
shared representation that brings the source domain
and the target domain closer. Often these algorithms
do not use labeled examples in the target domain.
The works (Blitzer et al, 2006; Huang and Yates,
2009) all belong to this category.
Focuses on P (Y |X) These adaptation algorithms
assume that there exists a small amount of labeled
data for the target domain. Instead of training two
weight vectors independently (one for source and
the other for the target domain), these algorithms try
to relate the source and target weight vectors. This is
often achieved by using a special designed regular-
ization term. The works (Chelba and Acero, 2004;
Daume? III, 2007; Finkel and Manning, 2009) belong
to this category.
767
It is important to give the definition of an adapta-
tion framework. An adaptation framework is speci-
fied by the data/resources used and a specific learn-
ing algorithm. For example, a framework that used
only source labeled examples and one that used both
source and target labeled examples should be con-
sidered as two different frameworks, even though
they might use exactly the same training algorithm.
Note that the goal of a good adaptation framework is
to perform well on the target domain and quite often
we only need to change the data/resource used to in-
crease the performance without changing the train-
ing algorithm. We refer to frameworks that do not
use target labeled data and focus on P (X) as Unla-
beled Adaptation Frameworks and refer to frame-
works that use algorithms that focus on P (Y |X) as
Labeled Adaptation Frameworks.
The major difference between unlabeled adapta-
tion frameworks and labeled adaptation frameworks
is the use of target labeled examples. Unlabeled
adaptation frameworks do not use target labeled ex-
amples1, while the labeled adaptation frameworks
make use of target labeled examples. Under this
definition, we consider that a model trained on both
source and target labeled examples (later referred as
S+T) is a labeled adaptation framework.
It is important to combine the labeled and unla-
beled adaptation frameworks for two reasons:
? Mutual Benefit: We analyze these two types
of frameworks and find that they address dif-
ferent adaptation issues. Therefore, it is often
beneficial to apply them together.
? Complex Interaction: Another, probably
more important issue, is that these two types
of frameworks are not independent. Different
representations will impact howmuch a labeled
adaptation algorithm can transfer information
between domains. Therefore, in order to have a
clear picture of what is the best labeled adapta-
tion framework, it is necessary to analyze these
two domain adaptation frameworks together.
In this paper, we assume we have both a small
amount of target labeled data and a large amount
1Note that we still use labeled data from source domain in
an unlabeled adaptation framework.
of unlabeled data so that we can perform both unla-
beled and labeled adaptation. The goal of our paper
is to point out the necessity of applying these two
adaptation frameworks together. To the best of our
knowledge, this is the first paper that both theoreti-
cally and empirically analyzes the interdependence
between the impact of labeled and unlabeled adap-
tation frameworks.
The contribution of this paper is as follows:
? Propose a theoretical analysis of the ?Frustrat-
ingly Easy? (FE) framework (Daume? III, 2007)
(Section 3).
The theoretical analysis shows that for FE to be
effective the domains must already be ?close?.
At some threshold of ?closeness? it is better to
switch from FE to just pool all training together
as one domain.
? Demonstrate the complex interaction between
unlabeled and labeled approaches (Section 4)
We construct artificial experiments that demon-
strate how applying unlabeled adaptation may
impact the behavior of two labeled adaptation
approaches.
? Empirically analyze the interaction on real
datasets (Section 5).
We show that in general combining both ap-
proaches on the tasks of preposition sense
disambiguation and named entity recognition
works better than either individual method.
Our approach not only achieves state-of-the-
art results on these two tasks but it also re-
veals something surprising ? finding a bet-
ter shared representation often makes a sim-
ple source+target approach the best adaptation
framework in practice.
2 Two Adaptation Aspects: A Review
Why do we need two types of adaptation frame-
works? First, unlabeled adaptation frameworks are
necessary since many features only exist in one do-
main. Therefore, it is important to develop algo-
rithms that find features which work across domains.
On the other hand, labeled adaptation frameworks
768
are also required because we would like to take ad-
vantages of target labeled data. Even though differ-
ent domains may have different definitions for la-
bels (say in named entity recognition, specific defi-
nition of PER/LOC/ORG may change), labeled data
should still be useful. We summarize these distinc-
tions in Table 1.
While these two aspects of adaptation both saw
significant progress in the past few years, little anal-
ysis has been done on the interaction between these
two types of algorithms2.
In order to have a deep analysis, it is necessary to
choose specific adaptation algorithms for each as-
pect of adaptation framework. While we mainly
conduct analysis on the algorithms we picked, we
would like to point out that the necessity of com-
bining these two types of adaptation algorithms has
been largely ignored in the community.
As our example adaptation algorithms we se-
lected:
Labeled adaptation: FE framework One of the
most popular adaptation frameworks that requires
the use of labeled target data is the ?Frustrat-
ingly Easy? (FE) adaptation framework (Daume? III,
2007). However, why and when this framework
works remains unclear in the NLP community. The
FE framework can be viewed as an framework that
extends the feature space, and it requires source and
target labeled data to work. We denote n as the
total number of features3 and m is the number of
the ?domains?, where one of the domains is the tar-
get domain. The FE framework creates a global
weight vector in Rn(m+1), an extended space for all
domains. The representation x of the t-th domain
is mapped by ?t(x) ? Rn(m+1). In the extended
space, the first n features consist of the ?shared?
block, which is always active across all tasks. The
(t+1)-th block (the (nt+1)-th to the (nt+n)-th fea-
tures) is a ?specific? block, and is only active when
2Among the previously mentioned work, (Jiang and Zhai,
2007) is a special case given that it discusses both aspects of
adaptation algorithms. However, little analysis on the interac-
tion of the two aspects is discussed in that paper
3We assume that the number of features in each domain is
equal.
extracting examples from the task t. More formally,
?t(x) =
2
6
4 x|{z}
shared
(t?1) blocks
z }| {
0 . . .0 x
|{z}
specific
(m?t) blocks
z }| {
0 . . .0
3
7
5 . (1)
A single weight vector w? is obtained by training on
the modified labeled data {yti ,?t(x
t
i)}
m
t=1. Given
that this framework only extends the feature space,
in this paper, we also call it the feature extension
framework (still called FE). We will see in Section 3
that this framework is equivalent to applying a reg-
ularization trick that bridges the source and the tar-
get domains. As it will become clear in Section 3,
in fact, this framework is only effective when there
is target labeled data and hence belongs to labeled
adaptation frameworks.
Although FE framework is quite popular in the
community, there are other even simpler labeled
adaptation frameworks that allow the use of tar-
get labeled data. For example, one of the simplest
frameworks is the S+T framework, which simply
trains a single model on the pooled and unextended
source and target training data.
Unlabeled adaptation: Adding cluster-like fea-
tures Recall that unlabeled adaptation frameworks
find the features that ?work? across domain. In this
paper, we find such features in two steps. First,
we use word clusters generated from unlabeled text
and/or third party resources that spans domains.
Then, for every feature template that contains a
word, we append another feature template that uses
the word?s cluster instead of the word itself. This
technique is used in many recent works including
dependency parsing and NER (Koo et al, 2008;
Ratinov and Roth, 2009). Note that the unlabeled
text need not come from the source or target do-
main. In fact, in this paper, we use clusters gen-
erated with the Reuters 1996 dataset, a superset of
the CoNLL03 NER dataset (Koo et al, 2008; Liang,
2005). We adopt the Brown cluster algorithm to find
the word cluster (Brown et al, 1992; Liang, 2005).
We can use other resources to create clusters as well.
For example, in the NER domain, we also include
gazetteers4 as an unlabeled cluster resource, which
can bring the domains together quite effectively.
4Our gazetteers comes from (Ratinov and Roth, 2009).
769
Framework Labeled Data Unlabeled Data Common Approach
Unlabeled Adaptation
(Focus on P (X))
Source Encompasses Source and Target.
May use other third party resources
(dictionaries, gazetteers, etc.).
Generate features that span domains us-
ing unlabeled data and/or third party re-
sources.
Labeled Adaptation
(Focus on P (Y |X))
Source and Target None Train classifier(s) using both source and
target training data, relating the two.
Table 1: Comparison between two general adaptation frameworks discussed in this paper. Each framework is specified by its setting
(data required) and its learning algorithm. Multiple previous adaptation approaches fit in one of either framework.
While other more complex algorithms (Ando and
Zhang, 2005; Blitzer et al, 2006) for finding bet-
ter shared representation (without using labeled tar-
get data) have been proposed, we find that using
straightforward clustering features is quite effective
in general.
3 Analysis of the FE Framework
In this section, we propose a simple yet informative
analysis of the FE algorithm from the perspective of
multi-task learning. Note that we ignore the effect
of unlabeled adaptation in this section, and focus on
the analysis of the FE framework as a representative
labeled adaptation framework.
3.1 Mistake Bound Analysis
While (Daume? III, 2007) proposed this framework
for adaptation, a very similar idea had been proposed
in (Evgeniou and Pontil, 2004) as a novel regular-
ization term for multitask learning with support vec-
tor machines. Assume that w1,w2, . . . ,wm are the
weight vector for the first domain to the m-th do-
main, respectively. The baseline approach is to as-
sume that each weight vector is independent. As-
sume that we adopt a SVM-like optimization prob-
lem that consider all m tasks, the baseline approach
is equivalent to using the following regularization
term in the objective function:
?m
t=1 ?wt?
2.
In (Evgeniou and Pontil, 2004; Daume? III, 2007),
they assume that wt = u + vt, for t = 1, . . .m,
where vt is the specific weight vector for t-th do-
main and u is a shared weight vector across all do-
mains. The new regularization term then becomes
?u?2 +
m?
t=1
?vt?2. (2)
Note that these two regularization terms are differ-
ent, given that the new regularization term makes
w1,w2, . . . ,wm not independent anymore. It fol-
lows that
wTt x = (u+ vt)
Tx = w?T?t(x),
where
w?T =
[
uT vT1 . . . v
T
m
]
.
and ?w??2 equals to Eq. (2). Therefore, we can think
feature extension framework as a learning frame-
work that adopts Eq. (2) as its regularization term.
The FE framework was in fact originally designed
for the problem of multitask learning so in the fol-
lowing, we propose a simple mistake bound analysis
based on the multitask setting, where we calculate
the mistakes on all domains5. We focus on multi-
task setting for two reasons: 1) the analysis is very
easy and intuitive, and 2) in Section 4.1, we empiri-
cally confirm that the analysis holds for the adapta-
tion setting.
In the following, we assume that the training
algorithm used in the FE framework is the on-
line perceptron learning algorithm (Novikoff, 1963).
This allows us to analyze the mistake bound of the
FE framework with the perceptron algorithm. The
bound can give us an insight on when and why one
should adopt the FE framework. By using the stan-
dard mistake bound theorem (Novikoff, 1963), we
show:
Theorem 1. Let Dt be the labeled data of domain t.
Assume that there exist w1,w2, . . . ,wm such that
ywTt x ? ?,?(x, y) ? Dt,
and assume that max(x,y)?Dt ?x? ? R
2,?t =
1 . . .m. Then, the number of mistakes made with
online perceptron training (Novikoff, 1963) and the
5In the adaptation setting, one generally only cares about the
performance on the target domain.
770
FE framework is bounded by
2R2
?2
(
m?
t=1
?wt?2 ?
?
?m
t=1 wt?
2
m + 1
). (3)
Proof. Define w? as a vector in Rn(m+1). We claim
that there exists a set Sw? such that for all w? ? Sw?,
w?T?t(x) = wTt x for any domain t = 1 . . .m. Note
that?t(x) is defined in Eq. (1). We can construct Sw?
in the following way:
Sw? = {
[
s (w1 ? s) . . . (wm ? s)
]
| s ? Rn},
where s is an arbitrary vector with n elements.
In order to obtain the best possible bound, we
would like to find the most compressed weight vec-
tor in Sw?, w? = minw??Sw? ?w??
2.
The optimization problem has an analytical solu-
tion:
?w??2 =
m?
t=1
?wt?2 ? ?
m?
t=1
wt?2/(m + 1).
The proof is completed by the standard mis-
take bound theorem and the following fact:
maxx ??t(x)?2 = 2maxx ?x?2 ? 2R2.
3.2 Mistake Bound Comparison
In the following, we would like to explore under
what circumstances the FE framework can work bet-
ter than individual models and the S+T framework
using Theorem 1. The analysis is done based on the
assumption that all frameworks use the perceptron
algorithm.
Before showing the bound analysis, note that the
framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization over
these three frameworks (FE, S+T, and the base-
line)6. However, our goal in this paper is different:
we try to provide a deep discussion onwhen and why
one should use a particular framework.
Here, we compare the mistake bounds of the fea-
ture sharing framework to that of the baseline ap-
proach, which learns each task independently7. In
6The framework proposed by (Evgeniou and Pontil, 2004;
Finkel and Manning, 2009) is a generalization of Eq. (1). It
allows the user to weight each block of features. If we put zero
weight on the shared block, it becomes the baseline approach.
On the other hand, if we put zero weight on all task-specific
blocks, the framework becomes the S+T approach.
7Note that mistake bound results can be generalized to gen-
eralization bound results. See (Zhang, 2002).
order to make the comparison easier, we make some
simplifying assumptions. First, we assume that the
problem contains only two tasks, 1 and 2. We also
assume that ?w1? = ?w2? = a. These assump-
tions greatly reduce the complexity of the analysis
and can give us greater insight into the comparisons.
Following the assumptions and Theorem 1, the
mistake bound for the FE frameworks is
4(2? cos(w1,w2))R2a2/(3?2) (4)
This line of analysis leads to interesting bound com-
parisons for two cases. In the first case, we assume
that task 1 and task 2 are essentially the same. In the
second, more common case, we assume that they are
different.
First, when we know a priori that task 1 and task
2 are essentially the same, we can combine the train-
ing data from the two tasks and train them as a sin-
gle task. Therefore, given that we do not need to
expand the feature space, the number of mistakes is
now bounded by R2a2/?2. Note that this bound is
in fact better than (4) with cos(w1,w2) = 1. There-
fore, if we know a priori that these two tasks are the
same, training a single model is better than using the
feature shared approach.
In practice, it is often the case that the two tasks
are not the same. In this case, the number of mis-
takes of an independent approach on both task 1 and
2 will be bounded by the summation of the mistake
bounds of task 1 and task 2. Therefore, using the
independent approach, the number of mistakes for
the perceptron algorithm on both tasks is bounded
by 2R2a2/?2. The following results can be obtained
by directly comparing the two bounds,
Corollary 1. Assume there exists w1 and w2 which
separate D1 and D2 respectively with functional
margin ?, and ?w1? = ?w2? = a. In this case:
(4) will be smaller than the bound of individual ap-
proach, 2R2a2/?2, if and only if cos(w1,w2) =
(wT1 w2)/(?w1??w2?) >
1
2 .
If we assume that there is no difference in
P (X) between domains and hence we can treat
cos(w1,w2) as the similarity between two tasks, the
above argument suggests:
? If the two tasks are very different, the baseline
approach (building two models) is better than
FE and S+T.
771
? If the tasks are similar enough, FE is better than
baseline and S+T.
? If the tasks are almost the same, S+T becomes
better than FE and baseline.
In Section 4.1, we will evaluate whether these claims
can be justified empirically.
4 Artificial Data Experiment Study
In this section we will present artificial experiments.
We have two primary goals: 1) verifying the analysis
proposed in Section 3, and 2) showing that the repre-
sentation shift will impact the behavior of the FE al-
gorithm. The second point will be verified again in
the real world experiments in Section 5.
Data Generation In the following artificial ex-
periments we experiment with domain adaptation
by generating training and test data for two tasks,
source and target, where we can control the differ-
ence between task definitions. The general proce-
dure can be divided into two steps: 1) generating
weight vectors z1 and z2 (for source and target re-
spectively), and 2) randomly generating labeled in-
stances for training and testing using z1 and z2.
The different experiments start with the same ba-
sic z1 and z2, but then may alter these weights to
introduce task dissimilarities or similarities. The ba-
sic z1 and z2 are both generated by a multivariate
Gaussian distribution with mean z and a diagonal
covariance matrix ?I:
z1 ? N (z, ?I), z2 ? N (z, ?I),
where N is the normal distribution and z is random
vector with zero mean. Note that z is only used to
generate z1 and z2. There is one parameter, ?, that
controls the variance of the Gaussian distribution.
Hence we use ? to roughly control the ?angle? of z1
and z2. When ? is close to zero, z1 and z2 will be
very similar. On the other hand, when ? is large, z1
and z2 can be very different. In these experiments,
we vary ? between 0.01 and 5 so that we are exper-
imenting only with tasks where the weight the task
difference is the ?angle? or cosine between z1 and
z2. Once we obtain the z1 and z2, we normalize
them to the unit length.
After selecting z1 and z2, we then generate la-
beled instances (x, y) for the source task in the fol-
lowing way. For each example x, we randomly gen-
erate n binary features, where each feature has 20%
chance to be active. We then label the example by
y = sign(zT1 x),
The data for the target task is generated similarly
with z2. In these experiments, we fix the number of
features n to be 500 and generate 100 source train-
ing examples and 40 target training examples, along
with 1000 target testing examples. This matches the
reasonable case in NLP where there are more fea-
tures than training examples and each feature vector
is sparse. In all of the experiments, we report the
averaged testing error rate on the target testing data.
4.1 Experiment 1, FE algorithm
Goal The goal here is to verify our theoretical
analysis in Section 3. Note that we do not introduce
representation shift in this experiment and assume
that both source and target domains use exactly the
same features.
Result Figure 1(a) shows the performance of the
three training algorithms as variance decreases and
thus cosine between weight vectors (or measure of
task similarity) goes to 1. Note that FE labeled adap-
tation framework beats TGT once the task cosine
passes approximately 0.6. Initially FE slightly out-
performs S+T until the tasks are close enough to-
gether that it is better to treat all the data as coming
from one task. Note that while the experiments are
based on the adaptation setting, the results match our
analysis based on the multitask setting in Section 3.
4.2 Experiment 2, Unseen Features
Goal So far we have not considered the difference
in P (X) between domains. In the previous exper-
iment, we used only cosine as our task similarity
measurement to decide what is the best framework.
However, task similarity should consider the differ-
ence in both P (X) and P (Y |X), and the cosine
measurement is not sufficient for this. Here we con-
struct a simple example to show that even a simple
representation shift can change the behavior of the
labeled adaptation framework. This case shows that
S+T can be better than FE even when the tasks are
not similar according to the cosine measurement.
772
 0.3
 0.32
 0.34
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Cosine
Tgt
S+T
FE
(a) Basic Similarity
 0.32
 0.325
 0.33
 0.335
 0.34
 0.345
 0.35
 0.355
 0.36
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Original Cosine
Tgt
S+T
FE
(b) Shared Features
Figure 1: Artificial Experiment comparing labeled adaptation performance vs. cosine between base weight vectors that defines
two tasks, before and after cross-domain shared features are added. Figure (a) shows results from experiment 1. For FE adaptation
algorithm to work the tasks need to be close (cosine > 0.6), and if the tasks are close enough (cosine ? 1, dividing line) then
it is better to just pool source and target training data together (the S+T algorithm). Figure (b) shows results for experiment 3
when shared features are added to the base weight vectors as used in experiment 1. Here the cosine similarity measure is between
the base task weight vectors before the shared features have been added. Both labeled adaptation algorithms effectively use the
shared features to improve over just training on target. With shared features added the dividing line where S+T improves over
FE decreases so even for tasks that are initially further apart, once clusters are added the S+T algorithm does better than FE. Each
point represents the average of 2000 training runs with random initial z1 and z2 generating data.
Result The second experiment deals with the case
where features may appear in only one domain but
should be treated like known features in the other
domain. An example of this are out of vocabulary
words that may not exist in a small target train-
ing task, but have synonyms in the source train-
ing data. In this case if we had features grouping
words (say by word meanings) then we would re-
cover this cross-domain information. In this experi-
ment we want to explore which adaptation algorithm
performs best before these features are applied.
To simulate this case we start with similar weight
vectors z1 and z2 (sampled with variance = 0.00001,
cos(z1,z2) ? 1), but then shift some set of dimen-
sions so that they represent features that appear only
in one domain.
z1 = (a1,b1) ? z?1 = (0,b1,a1)
z2 = (a2,b2) ? z?2 = (a2,b2,0)
By changing the ratio of the size of the dissimilar
subset a to the similar subset b we can make the
two weight vectors z?1 and z
?
2 more or less similar.
Using these two new weight vectors we can proceed
as above, generating training and testing data.
Figure 2 shows the performance of the three algo-
 0.315
 0.32
 0.325
 0.33
 0.335
 0.34
 0.345
 0.35
 0.355
 0  0.2  0.4  0.6  0.8  1
Ta
rg
et
 %
 E
rro
r
Cosine
Tgt
S+T
FE
Figure 2: Artificial Experiment where unknown features are
included in source or target domains, but not the other. The
simple S+T adaptation framework is best able to exploit the
set of shared features so performs best over the whole space of
similarity in this setting.
rithms on this data as the number of unrelated fea-
tures are decreased. Over the entire range the com-
bined algorithm S+T does better since it more ef-
ficiently exploits the shared similar b subset of the
feature space. When the FE algorithm tries to cre-
ate the shared features, it considers both the similar
subset b and dissimilar subset a. However, since
a should not be shared, FE algorithm becomes less
773
effective than the S+T algorithm. See the bound
comparison in Section 3.2 for more intuitions. With
this experiment we have demonstrated that there is
a need to consider label and unlabeled adaptation
frameworks together.
4.3 Experiment 3, Shared Features
Goal A good unlabeled adaptation framework
should try to find features that ?work? across do-
mains. However, it is not clear how these newly
added features will impact the behavior of the la-
beled adaptation frameworks. In this experiment, we
show that the new shared features will bring the do-
mains together, and hence make S+T a very strong
adaptation framework.
Result For the third experiment we start with the
same setup as in the first experiment, but then aug-
ment the initial weight vector with additional shared
weights. These shared weights correspond to the in-
troduction of features that appear in both domains
and have the same meaning relative to the tasks, the
ideal result of unlabeled adaptation methods.
To generate this case we again start with z1 and
z2 of varying similarity as in section 4.1, then gen-
erate a random weight vector for shared features and
append this to both weight vectors.
zs ? N (0, I), z??1 = (z1, ?zs), z
??
2 = (z2, ?zs),
where ? is used to put increased importance on the
shared weight vectors by increasing the total weight
of that section relative to the base z1 and z2 subsets.
In our experiments we use 100 shared features to the
500 base features and set ? to 2.
Figure 1(b) shows the performance of the labeled
adaptation algorithms once shared features had been
added. Here the x-axis is the cosine between the
original task weight vectors, demonstrating how the
shared features improve performance on potentially
dissimilar tasks. Whereas in the first experiment
FE does not improve over just training on target data
until the cosine is greater than 0.6, once shared fea-
tures have been added then both FE and S+T use
these features to learn with originally dissimilar
tasks. Furthermore the shared features tend to push
the tasks ?closer? so that S+T improves over FE ear-
lier. Comparing to Figure 1(a), there are regions
where before shared features are added it is better
to use FE, and after shared features are added it is
better to use S+T. This shows that labeled adapta-
tion and unlabeled are not independent. Therefore,
it is important to combine these two aspects to see
the real contribution of each adaptation framework.
In these three artificial experiments we have
demonstrated cases where both FE or S+T are
the best algorithm before and after representation
changes like those created with unlabeled adaptation
are imposed. This fact points to the perhaps obvi-
ous conclusion that there is not a single best adapta-
tion algorithm, and the determination of specific best
practices depends on task similarity (in both P (X)
and P (Y |X)), especially after being brought closer
together with other adaptation approaches. If there
is one common trend it is that often once two tasks
have been brought close together using a shared rep-
resentation, then the tasks are now close enough
such that the simple S+T algorithm does well.
5 Real World Experiments
In Section 4, we have shown through artificial data
experiments that labeled and unlabeled adaptation
algorithms are not independent. In this section, we
focus on experiments with real datasets.
For the labeled adaptation algorithms, we have the
following options:
? TGT: Only uses target labeled training dataset.
? FE: Uses both labeled datasets.
? FE+: Uses both labeled datasets. A modifica-
tion of the FE algorithm, equivalent to multi-
plying the ?shared? part of the FE feature vec-
tor (Eq. (1)) by 10 (Finkel andManning, 2009).
? S+T: Uses both source and target labeled
datasets to train a single model with all labeled
data directly.
Throughout all of our experiments, we use SVMs
trained with a modified java implementation8 of
LIBLINEAR as our underlying learning classi-
fier (Hsieh et al, 2008). For the tasks that require
structures, we model each individual decision using
8Our code is modified from the version available on http:
//www.bwaldvogel.de/liblinear-java/
774
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Token F1
(a) MUC7 Dev 58.6 70.5 74.3 73.1
(a) + cluster 77.5 82.5 83.3 83.3
(b) MUC7 Train 73.0 78.2 80.1 78.7
(b) + cluster 85.4 86.4 86.2 86.5
Table 2: NER Experiments. We bold face the best accuracy
in a row and underline the runner up. Both unlabeled adapta-
tion algorithms (adding cluster features) and labeled adaptation
algorithm (using source labeled data) help the performance sig-
nificantly. Moreover, adding cluster-like features also changes
the behavior of the labeled adaptation algorithms. Note that
after adding cluster features, S+T becomes quite competitive
with (or slightly better than) the FE+ approach. The size of
MUC7 develop set is roughly 20% of the size of the MUC7
training set.
a local SVM classifier then make our prediction us-
ing a greedy approach from left to right. While we
could use a more complex model such as Condi-
tional Random Field (Lafferty et al, 2001), as we
will see later, our simple model generates state-of-
the-art results for many tasks. Regarding parameter
selection, we selected the SVM regularization pa-
rameter for the baseline model (TGT) and then fix it
for all algorithms9.
Named Entity Recognition Our first task is
Named Entity Recognition (NER). The source do-
main is from the CoNLL03 shared task (Tjong
Kim Sang and De Meulder, 2003) and the target do-
main is from the MUC7 dataset. The goal of this
adaptation system is to maximize the performance
on the test data of MUC7 dataset with CoNLL train-
ing data and (some) MUC7 labeled data. As an unla-
beled adaptation method to address feature sparsity,
we add cluster-like features based on the gazetteers
and word clustering resources used in (Ratinov and
Roth, 2009) to bridge the source and target domain.
We experiment with both MUC development and
training set as our target labeled sets.
The experimental results are in Table 2. First, no-
tice that addressing the feature sparsity issue helps
the performance significantly. Adding cluster-like
9We use L2-hinge loss for all of the experiments, with
C = 2?4 for NER experiments and C = 2?5 for the PSD
experiments.
features improves the Token-F1 by around 10%. On
the other hand, adding target labeled data also helps
the results significantly. Moreover, using both tar-
get labeled data and cluster-like shared representa-
tion are mutually beneficial in all cases.
Importantly, adding cluster-like features changes
the behavior of the labeled adaptation algorithms.
When the cluster-like features are not added, the
FE+ algorithm is in general the best labeled adap-
tation framework. This result agrees with the re-
sults showed in (Finkel and Manning, 2009), where
the authors show that FE+ is the best labeled adap-
tation framework in their settings. However, after
adding the cluster-like features, the simple S+T ap-
proach becomes very competitive to both FE and
FE+. This matches our analysis in Section 4: re-
solving features sparsity will change the behavior of
labeled adaptation frameworks.
We compare the simple S+T algorithm with
cluster-like features to other published results on
adapting from CoNLL dataset to MUC7 dataset in
table 3. Past works on this setting often only fo-
cus on one class of adaption approach. For example,
(Ratinov and Roth, 2009) only use the cluster-like
features to address the feature sparsity problem, and
(Finkel and Manning, 2009) only use target labeled
data without using gazetteers and word-cluster in-
formation. Notice that because of combining two
classes of adaption algorithms, our approach is sig-
nificantly better than these two systems10.
Preposition Sense Disambiguation We also test
the combination of unlabeled and labeled adaption
on the task of Preposition Sense Disambiguation.
Here the data contains multiple prepositions where
each preposition has many different senses. The
goal is to predict the right sense for a given prepo-
sition in the testing data. The source domain is the
SemEval 2007 preposition WSD Task and the target
domain is from the dataset annotated in (Dahlmeier
et al, 2009). Our feature design mainly comes
from (Tratz and Hovy, 2009) (who do not evalu-
ate their system on our target data). As our un-
10The work (Ratinov and Roth, 2009) also combines their
system with several document-level features. While it is possi-
ble to add these features in our system, we do not include any
global features for the sake of simplicity. Note that our sys-
tem is competitive to (Ratinov and Roth, 2009) even though our
system does not use global features.
775
Systems Cluster? TGT? P.F1 T.F1
Our NER y y 84.1 86.5
FM09 n y 79.98 N/A
RR09 y n N/A 83.2
RR09 + global y n N/A 86.2
Table 3: Comparisons between different NER systems. P.F1
and T.F1 represent the phrase-level and token-level F1 score,
respectively. We use ?Cluster?? to indicate if cluster features
are used and use ?TGT?? to indicate if target labeled data is
used. Previous systems often only use one class of adaptation
algorithms. Using both adaptation aspects makes our system
perform significantly better than FM09 and RR09.
Algorithm TGT FE FE+ S+T
SRC labeled data? no yes
Target labeled data Accuracy
10% Tgt 43.8 48.2 51.3 49.7
10% Tgt + Cluster 44.9 50.5 51.8 52.0
100% Tgt 59.5 60.5 60.3 61.2
100% Tgt + Cluster 61.3 62.0 61.2 62.1
Table 4: Preposition Sense Disambiguation. We mark the best
accuracy in a row using the bold font and underline the runner
up. Note that both adding cluster features and adding source la-
beled data help the performance significantly. Moreover, adding
clusters also changes the behavior of the labeled adaptation al-
gorithms.
labeled adaptation approach we augment all word
based features with cluster information from sepa-
rately generated hierarchical Brown clusters (Brown
et al, 1992).
The experimental results are in Table 4. Note that
we see phenomena similar to what happened in the
NER experiments. First, both labeled and unlabeled
adaptation improves the system. When only 10% of
the target labeled data is used, the inclusion of the
source labeled data helps significantly. When there
is more labeled data, labeled and unlabeled adaption
have similar impact. Again, using unlabeled adap-
tion changes the behavior of the labeled adaption al-
gorithms.
In Table 5, we compare our system to (Dahlmeier
et al, 2009), who do not use the SemEval data but
jointly train their preposition sense disambiguation
system with a semantic role labeling system. With
both labeled and unlabeled adaption, our system is
significantly better.
Systems ACC
Our PSD (S+T and cluster) 62.1
DNS09 56.5
DNS09 + SRL 58.8
Table 5: Comparison between different PSD systems. Note
that after adding cluster features and source labeled data with
S+T approach, our system outperforms the state-of-the-art sys-
tem proposed in (Dahlmeier et al, 2009), even though they
jointly learn a PSD and SRL system together.
6 Conclusion
In this paper, we point out the necessities of com-
bining labeled and unlabeled adaptation algorithms.
We analyzed the FE algorithm both theoretically
and empirically, demonstrating that it requires both
a minimal amount of task similarity to work, and
past a certain level of similarity other, simpler ap-
proaches are better. More importantly, through arti-
ficial data experiments we found that applying unla-
beled adaptation algorithms may change the behav-
ior of labeled adaptation algorithms as representa-
tions change, and hence affect the choice of labeled
adaptation algorithm. Experiments with real-world
datasets confirmed that combinations of both adap-
tation methods provide the best results, often allow-
ing the use of simple labeled adaptation approaches.
In the future, we hope to develop a joint algorithm
which addresses both labeled and unlabeled adapta-
tion at the same time.
Acknowledgment We thank Vivek Srikumar for provid-
ing the baseline implementation of preposition sense disam-
biguation. We also thank anonymous reviewers for their use-
ful comments. University of Illinois gratefully acknowledges
the support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government.
References
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res.
776
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,
and R. L. Mercer. 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Dekang Lin and Dekai Wu, editors, EMNLP.
D. Dahlmeier, H. T. Ng, and T. Schultz. 2009. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In EMNLP.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
T. Evgeniou and M. Pontil. 2004. Regularized multi?
task learning. In KDD.
J. R. Finkel and C. D. Manning. 2009. Hierarchical
bayesian domain adaptation. In NAACL.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear svm. In ICML.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in nlp. In ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
A. Novikoff. 1963. On convergence proofs for percep-
trons. In Proceeding of the Symposium on the Mathe-
matical Theory of Automata.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Walter
Daelemans and Miles Osborne, editors, Proceedings
of CoNLL-2003.
S. Tratz and D. Hovy. 2009. Disambiguation of prepo-
sition sense using linguistically motivated features. In
NAACL.
Tong Zhang. 2002. Covering number bounds of certain
regularized linear function classes. J. Mach. Learn.
Res.
777
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 989?998,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Starting From Scratch in Semantic Role Labeling
Michael Connor
University of Illinois
connor2@uiuc.edu
Yael Gertner
University of Illinois
ygertner@cyrus.psych.uiuc.edu
Cynthia Fisher
University of Illinois
cfisher@cyrus.psych.uiuc.edu
Dan Roth
University of Illinois
danr@illinois.edu
Abstract
A fundamental step in sentence compre-
hension involves assigning semantic roles
to sentence constituents. To accomplish
this, the listener must parse the sentence,
find constituents that are candidate argu-
ments, and assign semantic roles to those
constituents. Each step depends on prior
lexical and syntactic knowledge. Where
do children learning their first languages
begin in solving this problem? In this pa-
per we focus on the parsing and argument-
identification steps that precede Seman-
tic Role Labeling (SRL) training. We
combine a simplified SRL with an un-
supervised HMM part of speech tagger,
and experiment with psycholinguistically-
motivated ways to label clusters resulting
from the HMM so that they can be used
to parse input for the SRL system. The
results show that proposed shallow rep-
resentations of sentence structure are ro-
bust to reductions in parsing accuracy, and
that the contribution of alternative repre-
sentations of sentence structure to suc-
cessful semantic role labeling varies with
the integrity of the parsing and argument-
identification stages.
1 Introduction
In this paper we present experiments with an au-
tomatic system for semantic role labeling (SRL)
that is designed to model aspects of human lan-
guage acquisition. This simplified SRL system is
inspired by the syntactic bootstrapping theory, and
by an account of syntactic bootstrapping known
as ?structure-mapping? (Fisher, 1996; Gillette et
al., 1999; Lidz et al, 2003). Syntactic bootstrap-
ping theory proposes that young children use their
very partial knowledge of syntax to guide sen-
tence comprehension. The structure-mapping ac-
count makes three key assumptions: First, sen-
tence comprehension is grounded by the acquisi-
tion of an initial set of concrete nouns. Nouns are
arguably less dependent on prior linguistic knowl-
edge for their acquisition than are verbs; thus chil-
dren are assumed to be able to identify the refer-
ents of some nouns via cross-situational observa-
tion (Gillette et al, 1999). Second, these nouns,
once identified, yield a skeletal sentence structure.
Children treat each noun as a candidate argument,
and thus interpret the number of nouns in the sen-
tence as a cue to its semantic predicate-argument
structure (Fisher, 1996). Third, children represent
sentences in an abstract format that permits gener-
alization to new verbs (Gertner et al, 2006).
The structure-mapping account of early syn-
tactic bootstrapping makes strong predictions, in-
cluding predictions of tell-tale errors. In the sen-
tence ?Ellen and John laughed?, an intransitive
verb appears with two nouns. If young chil-
dren rely on representations of sentences as sim-
ple as an ordered set of nouns, then they should
have trouble distinguishing such sentences from
transitive sentences. Experimental evidence sug-
gests that they do: 21-month-olds mistakenly in-
terpreted word order in sentences such as ?The girl
and the boy kradded? as conveying agent-patient
roles (Gertner and Fisher, 2006).
Previous computational experiments with a
system for automatic semantic role labeling
(BabySRL: (Connor et al, 2008)) showed that
it is possible to learn to assign basic semantic
roles based on the shallow sentence representa-
tions proposed by the structure-mapping view.
Furthermore, these simple structural features were
robust to drastic reductions in the integrity of
the semantic-role feedback (Connor et al, 2009).
These experiments showed that representations of
sentence structure as simple as ?first of two nouns?
are useful, but the experiments relied on perfect
989
knowledge of arguments and predicates as a start
to classification.
Perfect built-in parsing finesses two problems
facing the human learner. The first problem in-
volves classifying words by part-of-speech. Pro-
posed solutions to this problem in the NLP and
human language acquisition literatures focus on
distributional learning as a key data source (e.g.,
(Mintz, 2003; Johnson, 2007)). Importantly,
infants are good at learning distributional pat-
terns (Gomez and Gerken, 1999; Saffran et al,
1996). Here we use a fairly standard Hidden
Markov Model (HMM) to generate clusters of
words that occur in similar distributional contexts
in a corpus of input sentences.
The second problem facing the learner is
more contentious: Having identified clusters of
distributionally-similar words, how do children
figure out what role these clusters of words should
play in a sentence interpretation system? Some
clusters contain nouns, which are candidate ar-
guments; others contain verbs, which take argu-
ments. How is the child to know which are which?
In order to use the output of the HMM tagger to
process sentences for input to an SRL model, we
must find a way to automatically label the clusters.
Our strategies for automatic argument and pred-
icate identification, spelled out below, reflect core
claims of the structure-mapping theory: (1) The
meanings of some concrete nouns can be learned
without prior linguistic knowledge; these concrete
nouns are assumed based on their meanings to be
possible arguments; (2) verbs are identified, not
primarily by learning their meanings via observa-
tion, but rather by learning about their syntactic
argument-taking behavior in sentences.
By using the HMM part-of-speech tagger in this
way, we can ask how the simple structural fea-
tures that we propose children start with stand up
to reductions in parsing accuracy. In doing so, we
move to a parser derived from a particular theoret-
ical account of how the human learner might clas-
sify words, and link them into a system for sen-
tence comprehension.
2 Model
We model language learning as a Semantic Role
Labeling (SRL) task (Carreras and Ma`rquez,
2004). This allows us to ask whether a learner,
equipped with particular theoretically-motivated
representations of the input, can learn to under-
stand sentences at the level of who did what to
whom. The architecture of our system is similar
to a previous approach to modeling early language
acquisition (Connor et al, 2009), which is itself
based on the standard architecture of a full SRL
system (e.g. (Punyakanok et al, 2008)).
This basic approach follows a multi-stage
pipeline, with each stage feeding in to the next.
The stages are: (1) Parsing the sentence, (2) Iden-
tifying potential predicates and arguments based
on the parse, (3) Classifying role labels for each
potential argument relative to a predicate, (4) Ap-
plying constraints to find the best labeling of ar-
guments for a sentence. In this work we attempt
to limit the knowledge available at each stage to
the automatic output of the previous stage, con-
strained by knowledge that we argue is available
to children in the early stages of language learn-
ing.
In the parsing stage we use an unsupervised
parser based on Hidden Markov Models (HMM),
modeling a simple ?predict the next word? parser.
Next the argument identification stage identifies
HMM states that correspond to possible argu-
ments and predicates. The candidate arguments
and predicates identified in each input sentence are
passed to an SRL classifier that uses simple ab-
stract features based on the number and order of
arguments to learn to assign semantic roles.
As input to our learner we use samples of
natural child directed speech (CDS) from the
CHILDES corpora (MacWhinney, 2000). During
initial unsupervised parsing we experiment with
incorporating knowledge through a combination
of statistical priors favoring a skewed distribution
of words into classes, and an initial hard cluster-
ing of the vocabulary into function and content
words. The argument identifier uses a small set
of frequent nouns to seed argument states, relying
on the assumptions that some concrete nouns can
be learned as a prerequisite to sentence interpreta-
tion, and are interpreted as candidate arguments.
The SRL classifier starts with noisy largely un-
supervised argument identification, and receives
feedback based on annotation in the PropBank
style; in training, each word identified as an argu-
ment receives the true role label of the phrase that
word is part of. This represents the assumption
that learning to interpret sentences is naturally su-
pervised by the fit of the learner?s predicted mean-
ing with the referential context. The provision
990
of perfect ?gold-standard? feedback over-estimates
the real child?s access to this supervision, but al-
lows us to investigate the consequences of noisy
argument identification for SRL performance. We
show that even with imperfect parsing, a learner
can identify useful abstract patterns for sentence
interpretation. Our ultimate goal is to ?close the
loop? of this system, by using learning in the SRL
system to improve the initial unsupervised parse
and argument identification.
The training data were samples of parental
speech to three children (Adam, Eve, and
Sarah; (Brown, 1973)), available via CHILDES.
The SRL training corpus consists of parental utter-
ances in samples Adam 01-20 (child age 2;3 - 3;1),
Eve 01-18 (1;6 - 2;2), and Sarah 01-83 (2;3 - 3;11).
All verb-containing utterances without symbols
indicating disfluencies were automatically parsed
with the Charniak parser (Charniak, 1997), anno-
tated using an existing SRL system (Punyakanok
et al, 2008) and then errors were hand-corrected.
The final annotated sample contains about 16,730
propositions, with 32,205 arguments.
3 Unsupervised Parsing
As a first step of processing, we feed the learner
large amounts of unlabeled text and expect it to
learn some structure over this data that will facil-
itate future processing. The source of this text
is child directed speech collected from various
projects in the CHILDES repository1. We re-
moved sentences with fewer than three words or
markers of disfluency. In the end we used 160
thousand sentences from this set, totaling over 1
million tokens and 10 thousand unique words.
The goal of the parsing stage is to give the
learner a representation permitting it to generalize
over word forms. The exact parse we are after is
a distributional and context-sensitive clustering of
words based on sequential processing. We chose
an HMM based parser for this since, in essence
the HMM yields an unsupervised POS classifier,
but without names for states. An HMM trained
with expectation maximization (EM) is analogous
to a simple process of predicting the next word in a
stream and correcting connections accordingly for
each sentence.
1We used parts of the Bloom (Bloom, 1970; Bloom,
1973), Brent (Brent and Siskind, 2001), Brown (Brown,
1973), Clark (Clark, 1978), Cornell, MacWhin-
ney (MacWhinney, 2000), Post (Demetras et al, 1986)
and Providence (Demuth et al, 2006) collections.
With HMM we can also easily incorporate ad-
ditional knowledge during parameter estimation.
The first (and simplest) parser we used was an
HMM trained using EM with 80 hidden states.
The number of hidden states was made relatively
large to increase the likelihood of clusters corre-
sponding to a single part of speech, while preserv-
ing some degree of generalization.
Johnson (2007) observed that EM tends to cre-
ate word clusters of uniform size, which does
not reflect the way words cluster into parts of
speech in natural languages. The addition of pri-
ors biasing the system toward a skewed alloca-
tion of words to classes can help. The second
parser was an 80 state HMM trained with Varia-
tional Bayes EM (VB) incorporating Dirichlet pri-
ors (Beal, 2003).2
In the third and fourth parsers we experi-
ment with enriching the HMM POS-tagger with
other psycholinguistically plausible knowledge.
Words of different grammatical categories dif-
fer in their phonological as well as in their dis-
tributional properties (e.g., (Kelly, 1992; Mon-
aghan et al, 2005; Shi et al, 1998)); combining
phonological and distributional information im-
proves the clustering of words into grammatical
categories. The phonological difference between
content and function words is particularly strik-
ing (Shi et al, 1998). Even newborns can cate-
gorically distinguish content and function words,
based on the phonological difference between the
two classes (Shi et al, 1999). Human learners may
treat content and function words as distinct classes
from the start.
To implement this division into function and
content words3, we start with a list of function
word POS tags4 and then find words that appear
predominantly with these POS tags, using tagged
WSJ data (Marcus et al, 1993). We allocated a
fixed number of states for these function words,
and left the rest of the states for the rest of the
words. This amounts to initializing the emission
matrix for the HMM with a block structure; words
from one class cannot be emitted by states al-
located to the other class. This trick has been
used before in speech recognition work (Rabiner,
2We tuned the prior using the same set of 8 value pairs
suggested by Gao and Johnson (2008), using a held out set of
POS-tagged CDS to evaluate final performance.
3We also include a small third class for punctuation,
which is discarded.
4TO,IN,EX,POS,WDT,PDT,WRB,MD,CC,DT,RP,UH
991
1989), and requires far fewer resources than the
full tagging dictionary that is often used to intel-
ligently initialize an unsupervised POS classifier
(e.g. (Brill, 1997; Toutanova and Johnson, 2007;
Ravi and Knight, 2009)).
Because the function and content word preclus-
tering preceded parameter estimation, it can be
combined with either EM or VB learning. Al-
though this initial split forces sparsity on the emis-
sion matrix and allows more uniform sized clus-
ters, Dirichlet priors may still help, if word clus-
ters within the function or content word subsets
vary in size and frequency. The third parser was
an 80 state HMM trained with EM estimation,
with 30 states pre-allocated to function words;
the fourth parser was the same except that it was
trained with VB EM.
3.1 Parser Evaluation
 3.2
 3.4
 3.6
 3.8
 4
 4.2
 4.4
 4.6
 4.8
 5
 5.2
 100  1000  10000  100000  1e+06
Va
ria
tio
n 
of
 In
fo
rm
at
io
n
Training Sentences
EM
VB
EM+Funct
VB+Funct
Figure 1: Unsupervised Part of Speech results, match-
ing states to gold POS labels. All systems use 80 states, and
comparison is to gold labeled CDS text, which makes up a
subset of the HMM training data. Variation of Information is
an information-theoretic measure summing mutual informa-
tion between tags and states, proposed by (Meila?, 2002), and
first used for Unsupervised Part of Speech in (Goldwater and
Griffiths, 2007). Smaller numbers are better, indicating less
information lost in moving from the HMM states to the gold
POS tags. Note that incorporating function word precluster-
ing allows both EM and VB algorithms to achieve the same
performance with an order of magnitude fewer sentences.
We first evaluate these parsers (the first stage
of our SRL system) on unsupervised POS tag-
ging. Figure 1 shows the performance of the four
systems using Variation of Information to mea-
sure match between gold states and unsupervised
parsers as we vary the amount of text they receive.
Each point on the graph represents the average re-
sult over 10 runs of the HMM with different sam-
ples of the unlabeled CDS. Another common mea-
sure for unsupervised POS (when there are more
states than tags) is a many to one greedy mapping
of states to tags. It is known that EM gives a better
many to one score than VB trained HMM (John-
son, 2007), and likewise we see that here: with
all data EM gives 0.75 matching, VB gives 0.74,
while both EM+Funct and VB+Funct reach 0.80.
Adding the function/content word split to the
HMM structure improves both EM and VB esti-
mation in terms of both tag matching accuracy and
information. However, these measures look at the
parser only in isolation. What is more important to
us is how useful the provided word clusters are for
future semantic processing. In the next sections
we use the outputs of our four parsers to identify
arguments and predicates.
4 Argument Identification
The unsupervised parser provides a state label for
each word in each sentence; the goal of the ar-
gument identification stage is to use these states
to label words as potential arguments, predicates
or neither. As described in the introduction, core
premises of the structure-mapping account offer
routes whereby we could label some HMM states
as argument or predicate states.
The structure-mapping account holds that sen-
tence comprehension is grounded in the learning
of an initial set of nouns. Children are assumed
to identify the referents of some concrete nouns
via cross-situational learning (Gillette et al, 1999;
Smith and Yu, 2008). Children then assume, by
virtue of the meanings of these nouns, that they are
candidate arguments. This is a simple form of se-
mantic bootstrapping, requiring the use of built-in
links between semantics and syntax to identify the
grammatical type of known words (Pinker, 1984).
We use a small set of known nouns to transform
unlabeled word clusters into candidate arguments
for the SRL: HMM states that are dominated by
known names for animate or inanimate objects are
assumed to be argument states.
Given text parsed by the HMM parser and a
list of known nouns, the argument identifier pro-
ceeds in multiple steps as illustrated in figure 2.
The first stage identifies as argument states those
states that appear at least half the time in the train-
ing data with known nouns. This use of a seed
list and distributional clustering is similar to Proto-
type Driven Learning (Haghighi and Klein, 2006),
except we are only providing information on one
specific class.
992
Algorithm ARGUMENT STATE IDENTIFICATION
INPUT: Parsed Text T = list of (word, state) pairs
Set of concrete nouns N
OUTPUT: Set of argument states A
Argument count likelihood ArgLike(s, c)
Identify Argument States
Let freq(s) = |{(?, s) ? T}|
Let freqN (s) = |{(w, s) ? T |w ? N}|
For each s:
If freqN (s) ? freq(s)/2
Add s to A
Collect Per Sentence Argument Count statistics
For each Sentence S ? T :
Let Arg(S) = |{(w, s) ? S|s ? A}|
For (w, s) ? S s.t. s /? A
Increment ArgCount(s, Arg(S))
For each s /? A, and argument count c:
ArgLike(s, c) = ArgCount(s, c)/freq(s)
(a) Argument Identification
Algorithm PREDICATE STATE IDENTIFICATION
INPUT: Parsed Sentence S = list of (word, state) pairs
Set of argument states A
Sentence Argument Count ArgLike(s, c)
OUTPUT: Most likely predicate (v, sv)
Find Number of arguments in sentence
Let Arg(S) = |{(w, s) ? S|s ? A}|
Find Non-argument state in sentence most likely
to appear with this number of arguments
(v, sv) = argmax(w,s)?SArgLike(s, Arg(S))
(b) Predicate Identification
Figure 2: Argument identification algorithm. This is a two
stage process: argument state identification based on statis-
tics collected over entire text and per sentence predicate iden-
tification.
As a list of known nouns we collected all those
nouns that appear three times or more in the child
directed speech training data and judged to be ei-
ther animate or inanimate nouns. The full set of
365 nouns covers over 93% of noun occurences
in our data. In upcoming sections we experiment
with varying the number of seed nouns used from
this set, selecting the most frequent set of nouns.
Reflecting the spoken nature of the child directed
speech, the most frequent nouns are pronouns,
but beyond the top 10 we see nouns naming peo-
ple (?daddy?, ?ursula?) and object nouns (?chair?,
?lunch?).
What about verbs? A typical SRL model iden-
tifies candidate arguments and tries to assign roles
to them relative to each verb in the sentence. In
principle one might suppose that children learn
the meanings of verbs via cross-situational ob-
servation just as they learn the meanings of con-
crete nouns. But identifying the meanings of
verbs is much more troublesome. Verbs? mean-
ings are abstract, therefore harder to identify based
on scene information alone (Gillette et al, 1999).
As a result, early vocabularies are dominated by
nouns (Gentner, 2006). On the structure-mapping
account, learners identify verbs, and begin to de-
termine their meanings, based on sentence struc-
ture cues. Verbs take noun arguments; thus, learn-
ers could learn which words are verbs by detect-
ing each verb?s syntactic argument-taking behav-
ior. Experimental evidence provides some support
for this procedure: 2-year-olds keep track of the
syntactic structures in which a new verb appears,
even without a concurrent scene that provides cues
to the verb?s semantic content (Yuan and Fisher,
2009).
We implement this behavior by identifying as
predicate states the HMM states that appear com-
monly with a particular number of previously
identified arguments. First, we collect statistics
over the entire HMM training corpus regarding
how many arguments are identified per sentence,
and which states that are not identified as argu-
ment states appear with each number of argu-
ments. Next, for each parsed sentence that serves
as SRL input, the algorithm chooses as the most
likely predicate the word whose state is most likely
to appear with the number of arguments found in
the current input sentence. Note that this algo-
rithm assumes exactly one predicate per sentence.
Implicitly, the argument count likelihood divides
predicate states up into transitive and intransitive
predicates based on appearances in the simple sen-
tences of CDS.
4.1 Argument Identification Evaluation
Figure 3 shows argument and predicate identifi-
cation accuracy for each of the four parsers when
provided with different numbers of known nouns.
The known word list is very skewed with its most
frequent members dominating the total noun oc-
currences in the data. The ten most frequent
words5 account for 60% of the total noun occur-
rences. We achieve the different occurrence cov-
erage numbers of figure 3 by using the most fre-
quent N words from the list that give the specific
coverage6. Pronouns refer to people or objects,
but are abstract in that they can refer to any person
or object. The inclusion of pronouns in our list of
5you, it, I, what, he, me, ya, she, we, her
6N of 5, 10, 30, 83, 227 cover 50%, 60%, 70%, 80%,
90% of all noun occurrences
993
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.45  0.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95
F1
%Noun Occurences Covered
EM
VB
EM+Funct
VB+Funct
Figure 3: Effect of number of concrete nouns for seeding
argument identification with various unsupervised parsers.
Argument identification accuracy is computed against true ar-
gument boundaries from hand labeled data. The upper set of
results show primary argument (A0-4) identification F1, and
bottom lines show predicate identification F1.
known nouns represents the assumption that tod-
dlers have already identified pronouns as referen-
tial terms. Even 19-month-olds assign appropri-
ately different interpretations to novel verbs pre-
sented in simple transitive versus intransitive sen-
tences with pronoun arguments (?He?s kradding
him!? vs. ?He?s kradding!?; (Yuan et al, 2007)).
In ongoing work we experiment with other meth-
ods of identifying seed nouns.
Two groups of curves appear in figure 3: the
upper group shows the primary argument iden-
tification accuracy and the bottom group shows
the predicate identification accuracy. We evaluate
compared to gold tagged data with true argument
and predicate boundaries. The primary argument
(A0-4) identification accuracy is the F1 value, with
precision calculated as the proportion of identified
arguments that appear as part of a true argument,
and recall as the proportion of true arguments that
have some state identified as an argument. F1 is
calculated similarly for predicate identification, as
one state per sentence is identified as the predicate.
As shown in figure 3, argument identification F1
is higher than predicate identification (which is to
be expected, given that predicate identification de-
pends on accurate arguments), and as we add more
seed nouns the argument identification improves.
Surprisingly, despite the clear differences in un-
supervised POS performance seen in figure 1, the
different parsers do not yield very different argu-
ment and predicate identification. As we will see
in the next section, however, when the arguments
identified in this step are used to train SRL clas-
sifier, distinctions between parsers reappear, sug-
gesting that argument identification F1 masks sys-
tematic patterns in the errors.
5 Testing SRL Performance
Finally, we used the results of the previous pars-
ing and argument-identification stages in training
a simplified SRL classifier (BabySRL), equipped
with sets of features derived from the structure-
mapping account. For argument classification we
used a linear classifier trained with a regularized
perceptron update rule (Grove and Roth, 2001).
In the results reported below the BabySRL did
not use sentence-level inference for the final clas-
sification, every identified argument is classified
independently; thus multiple nouns can have the
same role. In what follows, we compare the per-
formance of the BabySRL across the four parsers.
We evaluated SRL performance by testing the
BabySRL with constructed sentences like those
used for the experiments with children described
in the Introduction. All test sentences contained a
novel verb, to test the model?s ability to general-
ize.
We examine the performance of four versions
of the BabySRL, varying in the features used to
represent sentences. All four versions include
lexical features consisting of the target argument
and predicate (as identified in the previous steps).
The baseline model has only these lexical features
(Lexical). Following Connor et al (2008; 2009),
the key feature type we propose is noun pattern
features (NounPat). Noun pattern features indi-
cate how many nouns there are in the sentence and
which noun the target is. For example, in ?You
dropped it!?, ?you? has a feature active indicating
that it is the first of two nouns, while ?it? has a fea-
ture active indicating that it is the second of two
nouns. We compared the behavior of noun pat-
tern features to another simple representation of
word order, position relative to the verb (VerbPos).
In the same example sentence, ?you? has a feature
active indicating that it is pre-verbal; for ?it? a fea-
ture is active indicating that it is post-verbal. A
fourth version of the BabySRL (Combined) used
both NounPat and VerbPos features.
We structured our tests of the BabySRL to test
the predictions of the structure-mapping account.
(1) NounPat features will improve the SRL?s abil-
ity to interpret simple transitive test sentences
containing two nouns and a novel verb, relative
994
to a lexical baseline. Like 21-month-old chil-
dren (Gertner et al, 2006), the SRL should inter-
pret the first noun as an agent and the second as
a patient. (2) Because NounPat features represent
word order solely in terms of a sequence of nouns,
an SRL equipped with these features will make the
errors predicted by the structure-mapping account
and documented in children (Gertner and Fisher,
2006). (3) NounPat features permit the SRL to
assign different roles to the subjects of transitive
and intransitive sentences that differ in their num-
ber of nouns. This effect follows from the nature
of the NounPat features: These features partition
the training data based on the number of nouns,
and therefore learn separately the likely roles of
the ?1st of 1 noun? and the ?1st of 2 nouns?.
These patterns contrast with the behavior of the
VerbPos features: When the BabySRL was trained
with perfect parsing, VerbPos promoted agent-
patient interpretations of transitive test sentences,
and did so even more successfully than Noun-
Pat features did, reflecting the usefulness of po-
sition relative to the verb in understanding English
sentences. In addition, VerbPos features elimi-
nated the errors with two-noun intransitive sen-
tences. Given test sentences such as ?You and
Mommy krad?, VerbPos features represented both
nouns as pre-verbal, and therefore identified both
as likely agents. However, VerbPos features did
not help the SRL assign different roles to the
subjects of simple transitive and intransitive sen-
tences: ?Mommy? in ?Mommy krads you? and
?Mommy krads? are both represented simply as
pre-verbal.
To test the system?s predictions on transitive and
intransitive two noun sentences, we constructed
two test sentence templates: ?A krads B? and ?A
and B krad?, where A and B were replaced with
familiar animate nouns. The animate nouns were
selected from all three children?s data in the train-
ing set and paired together in the templates such
that all pairs are represented.
Figure 4 shows SRL performance on test sen-
tences containing a novel verb and two animate
nouns. Each plot shows the proportion of test sen-
tences that were assigned an agent-patient (A0-
A1) role sequence; this sequence is correct for
transitive sentences but is an error for two-noun
intransitive sentences. Each group of bars shows
the performance of the BabySRL trained using one
of the four parsers, equipped with each of our four
feature sets. The top and bottom panels in Figure 4
differ in the number of nouns provided to seed the
argument identification stage. The top row shows
performance with 10 seed nouns (the 10 most fre-
quent nouns, mostly animate pronouns), and the
bottom row shows performance with 365 concrete
(animate or inanimate) nouns treated as known.
Relative to the lexical baseline, NounPat features
fared well: they promoted the assignment of A0-
A1 interpretations to transitive sentences, across
all parser versions and both sets of known nouns.
Both VB estimation and the content-function word
split increased the ability of NounPat features to
learn that the first of two nouns was an agent, and
the second a patient. The NounPat features also
promote the predicted error with two-noun intran-
sitive sentences (Figures 4(b), 4(d)). Despite the
relatively low accuracy of predicate identification
noted in section 4.1, the VerbPos features did suc-
ceed in promoting an A0A1 interpretation for tran-
sitive sentences containing novel verbs relative to
the lexical baseline. In every case the performance
of the Combined model that includes both Noun-
Pat and VerbPos features exceeds the performance
of either NounPat or VerbPos alone, suggesting
both contribute to correct predictions for transitive
sentences. However, the performance of VerbPos
features did not improve with parsing accuracy as
did the performance of the NounPat features. Most
strikingly, the VerbPos features did not eliminate
the predicted error with two-noun intransitive sen-
tences, as shown in panels 4(b) and 4(d). The
Combined model predicted an A0A1 sequence for
these sentences, showing no reduction in this error
due to the participation of VerbPos features.
Table 1 shows SRL performance on the same
transitive test sentences (?A krads B?), compared
to simple one-noun intransitive sentences (?A
krads?). To permit a direct comparison, the table
reports the proportion of transitive test sentences
for which the first noun was assigned an agent
(A0) interpretation, and the proportion of intran-
sitive test sentences with the agent (A0) role as-
signed to the single noun in the sentence. Here we
report only the results from the best-performing
parser (trained with VB EM, and content/function
word pre-clustering), compared to the same clas-
sifiers trained with gold standard argument iden-
tification. When trained on arguments identified
via the unsupervised POS tagger, noun pattern
features promoted agent interpretations of tran-
995
Two Noun Transitive, % Agent First One Noun Intransitive, % Agent Prediction
Lexical NounPat VerbPos Combine Lexical NounPat VerbPos Combine
VB+Funct 10 seed 0.48 0.61 0.55 0.71 0.48 0.57 0.56 0.59
VB+Funct 365 seed 0.22 0.64 0.41 0.74 0.23 0.33 0.43 0.41
Gold Arguments 0.16 0.41 0.69 0.77 0.17 0.18 0.70 0.58
Table 1: SRL result comparison when trained with best unsupervised argument identifier versus trained with gold arguments.
Comparison is between agent first prediction of two noun transitive sentences vs. one noun intransitive sentences. The unsu-
pervised arguments lead the classifier to rely more on noun pattern features; when the true arguments and predicate are known
the verb position feature leads the classifier to strongly indicate agent first in both settings.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(a) Two Noun Transitive Sentence, 10 seed nouns
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(b) Two Noun Intransitive Sentence, 10 seed nouns
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(c) Two Noun Transitive Sentence, 365 seed nouns
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
EM VB EM+Funct VB+Funct Gold
%
A0
A1
Lexical
NounPat
VerbPos
Combine
(d) Two Noun Intransitive Sentence, 365 seed nouns
Figure 4: SRL classification performance on transitive and intransitive test sentences containing two nouns and a novel
verb. Performance with gold-standard argument identification is included for comparison. Across parses, noun pattern features
promote agent-patient (A0A1) interpretations of both transitive (?You krad Mommy?) and two-noun intransitive sentences
(?You and Mommy krad?); the latter is an error found in young children. Unsupervised parsing is less accurate in identifying
the verb, so verb position features fail to eliminate errors with two-noun intransitive sentences.
sitive subjects, but not for intransitive subjects.
This differentiation between transitive and intran-
sitive sentences was clearer when more known
nouns were provided. Verb position features, in
contrast, promote agent interpretations of subjects
weakly with unsupervised argument identification,
but equally for transitive and intransitive.
Noun pattern features were robust to increases
in parsing noise. The behavior of verb position
features suggests that variations in the identifiabil-
ity of different parts of speech can affect the use-
fulness of alternative representations of sentence
structure. Representations that reflect the posi-
tion of the verb may be powerful guides for un-
derstanding simple English sentences, but repre-
sentations reflecting only the number and order of
nouns can dominate early in acquisition, depend-
ing on the integrity of parsing decisions.
6 Conclusion and Future Work
The key innovation in the present work is the
combination of unsupervised part-of-speech tag-
ging and argument identification to permit learn-
ing in a simplified SRL system. Children do not
996
have the luxury of treating part-of-speech tagging
and semantic role labeling as separable tasks. In-
stead, they must learn to understand sentences
starting from scratch, learning the meanings of
some words, and using those words and their pat-
terns of arrangement into sentences to bootstrap
their way into more mature knowledge.
We have created a first step toward modeling
this incremental process. We combined unsuper-
vised parsing with minimal supervision to begin to
identify arguments and predicates. An SRL clas-
sifier used simple representations built from these
identified arguments to extract useful abstract pat-
terns for classifying semantic roles. Our results
suggest that multiple simple representations of
sentence structure could co-exist in the child?s sys-
tem for sentence comprehension; representations
that will ultimately turn out to be powerful guides
to role identification may be less powerful early in
acquisition because of the noise introduced by the
unsupervised parsing.
The next step is to ?close the loop?, using higher
level semantic feedback to improve the earlier ar-
gument identification and parsing stages. Per-
haps with the help of semantic feedback the sys-
tem can automatically improve predicate identifi-
cation, which in turn allows it to correct the ob-
served intransitive sentence error. This approach
will move us closer to the goal of using initial sim-
ple structural patterns and natural observation of
the world (semantic feedback) to bootstrap more
and more sophisticated representations of linguis-
tic structure.
Acknowledgments
This research is supported by NSF grant BCS-
0620257 and NIH grant R01-HD054448.
References
M.J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience Unit, University Col-
lege London.
L. Bloom. 1970. Language development: Form and
function in emerging grammars. MIT Press, Cam-
bridge, MA.
L. Bloom. 1973. One word at a time: The use of
single-word utterances before syntax. Mouton, The
Hague.
M.R. Brent and J.M. Siskind. 2001. The role of expo-
sure to isolated words in early vocabulary develop-
ment. Cognition, 81:31?44.
E. Brill. 1997. Unsupervised learning of disambigua-
tion rules for part of speech tagging. In Natural
Language Processing Using Very Large Corpora.
Kluwer Academic Press.
R. Brown. 1973. A First Language. Harvard Univer-
sity Press, Cambridge, MA.
X. Carreras and L. Ma`rquez. 2004. Introduction to
the CoNLL-2004 shared tasks: Semantic role label-
ing. In Proceedings of CoNLL-2004, pages 89?97.
Boston, MA, USA.
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. National
Conference on Artificial Intelligence.
E.V. Clark. 1978. Awwareness of language: Some ev-
idence from what children say and do. In R. J. A.
Sinclair and W. Levelt, editors, The child?s concep-
tion of language. Springer Verlag, Berlin.
M. Connor, Y. Gertner, C. Fisher, and D. Roth. 2008.
Baby srl: Modeling early language acquisition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL), pages xx?yy,
Aug.
M. Connor, Y. Gertner, C. Fisher, and D. Roth.
2009. Minimally supervised model of early lan-
guage acquisition. In Proc. of the Annual Confer-
ence on Computational Natural Language Learning
(CoNLL), Jun.
M. Demetras, K. Post, and C. Snow. 1986. Feedback
to first-language learners. Journal of Child Lan-
guage, 13:275?292.
K. Demuth, J. Culbertson, and J. Alter. 2006. Word-
minimality, epenthesis, and coda licensing in the ac-
quisition of english. Language & Speech, 49:137?
174.
C. Fisher. 1996. Structural limits on verb mapping:
The role of analogy in children?s interpretation of
sentences. Cognitive Psychology, 31:41?81.
Jianfeng Gao and Mark Johnson. 2008. A compar-
ison of bayesian estimators for unsupervised hid-
den markov model pos taggers. In Proceedings of
EMNLP-2008, pages 344?352.
D. Gentner. 2006. Why verbs are hard to learn. In
K. Hirsh-Pasek and R. Golinkoff, editors, Action
meets word: How children learn verbs, pages 544?
564. Oxford University Press.
Y. Gertner and C. Fisher. 2006. Predicted errors in
early verb learning. In 31st Annual Boston Univer-
sity Conference on Language Development.
997
Y. Gertner, C. Fisher, and J. Eisengart. 2006. Learning
words and rules: Abstract knowledge of word or-
der in early sentence comprehension. Psychological
Science, 17:684?691.
J. Gillette, H. Gleitman, L. R. Gleitman, and A. Led-
erer. 1999. Human simulations of vocabulary learn-
ing. Cognition, 73:135?176.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of 45th Annual Meeting of
the Association of Computational Linguists, pages
744?751.
R. Gomez and L. Gerken. 1999. Artificial grammar
learning by 1-year-olds leads to specific and abstract
knowledge. Cognition, 70:109?135.
A. Haghighi and D. Klein. 2006. Prototype-drive
learning for sequence models. In Proceedings of
NAACL-2006, pages 320?327.
Mark Johnson. 2007. Why doesnt em find good hmm
pos-taggers? In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 296?305.
M.H. Kelly. 1992. Using sound to solve syntac-
tic problems: The role of phonology in grammat-
ical category assignments. Psychological Review,
99:349?364.
J. Lidz, H. Gleitman, and L. R. Gleitman. 2003. Un-
derstanding how input matters: verb learning and the
footprint of universal grammar. Cognition, 87:151?
178.
B. MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Third Edition. Lawrence Elr-
baum Associates, Mahwah, NJ.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330, June.
Marina Meila?. 2002. Comparing clusterings. Techni-
cal Report 418, University of Washington Statistics
Department.
T. Mintz. 2003. Frequent frames as a cue for grammat-
ical categories in child directed speech. Cognition,
90:91?117.
P. Monaghan, N. Chater, and M.H. Christiansen. 2005.
The differential role of phonological and distribu-
tional cues in grammatical categorisation. Cogni-
tion, 96:143?182.
S. Pinker. 1984. Language learnability and language
development. Harvard University Press, Cambridge,
MA.
V. Punyakanok, D. Roth, and W. Yih. 2008. The im-
portance of syntactic parsing and inference in se-
mantic role labeling. Computational Linguistics,
34(2).
L. R. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?285.
Sujith Ravi and Kevin Knight. 2009. Minimized
models for unsupervised part-of-speech tagging. In
Proceedings of the Joint Conferenceof the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP).
J.R. Saffran, R.N. Aslin, and E.L. Newport. 1996. Sta-
tistical learning by 8-month-old infants. Science,
274:1926?1928.
Rushen Shi, James L. Morgan, and Paul Allopenna.
1998. Phonological and acoustic bases for ear-
liest grammatical category assignment: a cross-
linguistic perspective. Journal of Child Language,
25(01):169?201.
Rushen Shi, Janet F. Werker, and James L. Morgan.
1999. Newborn infants? sensitivity to perceptual
cues to lexical and grammatical words. Cognition,
72(2):B11 ? B21.
L.B. Smith and C. Yu. 2008. Infants rapidly learn
word-referent mappings via cross-situational statis-
tics. Cognition, 106:1558?1568.
Kiristina Toutanova and Mark Johnson. 2007. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In Proceedings of NIPS.
S. Yuan and C. Fisher. 2009. ?really? she blicked
the baby??: Two-year-olds learn combinatorial facts
about verbs by listening. Psychological Science,
20:619?626.
S. Yuan, C. Fisher, Y. Gertner, and J. Snedeker. 2007.
Participants are more than physical bodies: 21-
month-olds assign relational meaning to novel tran-
sitive verbs. In Biennial Meeting of the Society for
Research in Child Development, Boston, MA.
998
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 81?88
Manchester, August 2008
Baby SRL: Modeling Early Language Acquisition.
Michael Connor
Department of Computer Science
University of Illinois
connor2@uiuc.edu
Yael Gertner
Beckman Institute
University of Illinois
ygertner@cyrus.psych.uiuc.edu
Cynthia Fisher
Department of Psychology
University of Illinois
cfisher@cyrus.psych.uiuc.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@uiuc.edu
Abstract
A fundamental task in sentence compre-
hension is to assign semantic roles to sen-
tence constituents. The structure-mapping
account proposes that children start with
a shallow structural analysis of sentences:
children treat the number of nouns in the
sentence as a cue to its semantic predicate-
argument structure, and represent language
experience in an abstract format that per-
mits rapid generalization to new verbs. In
this paper, we tested the consequences of
these representational assumptions via ex-
periments with a system for automatic se-
mantic role labeling (SRL), trained on a
sample of child-directed speech. When
the SRL was presented with representa-
tions of sentence structure consisting sim-
ply of an ordered set of nouns, it mim-
icked experimental findings with toddlers,
including a striking error found in children.
Adding features representing the position
of the verb increased accuracy and elim-
inated the error. We show the SRL sys-
tem can use incremental knowledge gain
to switch from error-prone noun order fea-
tures to a more accurate representation,
demonstrating a possible mechanism for
this process in child development.
1 Introduction
How does the child get started in learning to in-
terpret sentences? The structure-mapping view
of early verb and syntax acquisition proposes that
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
children start with a shallow structural analysis of
sentences: children treat the number of nouns in
the sentence as a cue to its semantic predicate-
argument structure (Fisher, 1996), and represent
language experience in an abstract format that per-
mits rapid generalization to new verbs (Gertner et
al., 2006).
The structure-mapping account makes strong
predictions. First, as soon as children can identify
some nouns, they should interpret transitive and in-
transitive sentences differently, simply by assign-
ing a distinct semantic role to each noun in the sen-
tence. Second, language-specific syntactic learn-
ing should transfer rapidly to new verbs. Third,
some striking errors of interpretation can occur.
In ?Fred and Ginger danced?, an intransitive verb
is presented with two nouns. If children interpret
any two-noun sentence as if it were transitive, they
should be fooled into interpreting the order of two
nouns in such conjoined-subject intransitive sen-
tences as conveying agent-patient role information.
Experiments with young children support these
predictions. First, 21-month-olds use the number
of nouns to understand sentences containing new
verbs (Yuan et al, 2007). Second, 21-month-olds
generalize what they have learned about English
transitive word-order to sentences containing new
verbs: Children who heard ?The girl is gorping the
boy? interpreted the girl as an agent and the boy as
a patient (Gertner et al, 2006). Third, 21-month-
olds make the predicted error, treating intransitive
sentences containing two nouns as if they were
transitive: they interpret the first noun in ?The girl
and the boy are gorping? as an agent and the sec-
ond as a patient (Gertner and Fisher, 2006). This
error is short-lived. By 25 months, children add
new features to their representations of sentences,
and interpret conjoined-subject intransitives differ-
81
ently from transitives (Naigles, 1990).
These experimental results shed light on what
syntactic information children might have avail-
able for early sentence comprehension, but do not
rule out the possibility that children?s early per-
formance is based on a more complex underlying
system. In this paper, we tested the consequences
of our representational assumptions by perform-
ing experiments with a system for automatic se-
mantic role labeling (SRL), whose knowledge of
sentence structure is under our control. Com-
putational models of semantic role labeling learn
to identify, for each verb in a sentence, all con-
stituents that fill a semantic role, and to determine
their roles. We adopt the architecture proposed
by Roth and colleagues (Punyakanok et al, 2005),
limiting the classifier?s features to a set of lexical
features and shallow structural features suggested
by the structure-mapping account. Learning abil-
ity is measured by the level of SRL accuracy and,
more importantly, the types of errors made by the
system on sentences containing novel verbs. Test-
ing these predictions on the automatic SRL pro-
vides us with a demonstration that it is possible to
learn how to correctly assign semantic roles based
only on these very simple cues.
From an NLP perspective this feature study pro-
vides evidence for the efficacy of alternative, sim-
pler syntactic representations in gaining an initial
foothold on sentence interpretation. It is clear that
human learners do not begin interpreting sentences
in possession of full part-of-speech tagging, or full
parse trees. By building a model that uses shal-
low representations of sentences and mimics fea-
tures of language development in children, we can
explore the nature of initial representations of syn-
tactic structure and build more complex features
from there, further mimicking child development.
2 Learning Model
We trained a simplified SRL classifier (Baby SRL)
with sets of features derived from the structure-
mapping account. Our test used novel verbs to
mimic sentences presented in experiments with
children. Our learning task is similar to the full
SRL task (Carreras and M`arquez, 2004), except
that we classify the roles of individual words rather
than full phrases. A full automatic SRL system
(e.g. (Punyakanok et al, 2005)) typically involves
multiple stages to 1) parse the input, 2) identify ar-
guments, 3) classify those arguments, and then 4)
run inference to make sure the final labeling for the
full sentence does not violate any linguistic con-
straints. Our simplified SRL architecture (Baby
SRL) essentially replaces the first two steps with
heuristics. Rather than identifying arguments via
a learned classifier with access to a full syntac-
tic parse, the Baby SRL treats each noun in the
sentence as a candidate argument and assigns a
semantic role to it. A simple heuristic collapsed
compound or sequential nouns to their final noun:
an approximation of the head noun of the noun
phrase. For example, ?Mr. Smith? was treated
as the single noun ?Smith?. Other complex noun
phrases were not simplified in this way. Thus,
a phrase such as ?the toy on the floor? would be
treated as two separate nouns, ?toy? and ?floor?.
This represents the assumption that young children
know ?Mr. Smith? is a single name, but they do not
know all the predicating terms that may link mul-
tiple nouns into a single noun phrase. The simpli-
fied learning task of the Baby SRL implements a
key assumption of the structure-mapping account:
that at the start of multiword sentence comprehen-
sion children can tell which words in a sentence are
nouns (Waxman and Booth, 2001), and treat each
noun as a candidate argument.
Feedback is provided based on annotation in
Propbank style: in training, each noun receives the
role label of the phrase that noun is part of. Feed-
back is given at the level of the macro-role (agent,
patient, etc., labeled A0-A4 for core arguments,
and AM-* adjuncts). We also introduced a NO la-
bel for nouns that are not part of any argument.
For argument classification we use a linear clas-
sifier trained with a regularized perceptron update
rule (Grove and Roth, 2001). This learning algo-
rithm provides a simple and general linear clas-
sifier that has been demonstrated to work well in
other text classification tasks, and allows us to in-
spect the weights of key features to determine their
importance for classification. The Baby SRL does
not use inference for the final classification. In-
stead it classifies every argument independently;
thus multiple nouns can have the same role.
2.1 Training
The training data were samples of parental speech
to one child (?Eve?; (Brown, 1973), available
via Childes (MacWhinney, 2000)). We trained
on parental utterances in samples 9 through 20,
recorded at child age 21-27 months. All verb-
82
containing utterances without symbols indicating
long pauses or unintelligible words were automat-
ically parsed with the Charniak parser (Charniak,
1997) and annotated using an existing SRL sys-
tem (Punyakanok et al, 2005). In this initial pass,
sentences with parsing errors that misidentified ar-
gument boundaries were excluded. Final role la-
bels were hand-corrected using the Propbank an-
notation scheme (Kingsbury and Palmer, 2002).
The child-directed speech (CDS) training set con-
sisted of about 2200 sentences, of which a majority
had a single verb and two nouns to be labeled
1
. We
used the annotated CDS training data to train our
Baby SRL, converting labeled phrases to labeled
nouns in the manner described above.
3 Experimental Results
To evaluate the Baby SRL we tested it with sen-
tences like those used for the experiments with
children described above. All test sentences con-
tained a novel verb (?gorp?). We constructed two
test sentence templates: ?A gorps B? and ?A and B
gorp?, where A and B were replaced with nouns
that appeared more than twice in training. We
filled the A and B slots by sampling nouns that
occurred roughly equally as the first and second
of two nouns in the training data. This procedure
was adopted to avoid ?building in? the predicted er-
ror by choosing A and B nouns biased toward an
agent-patient interpretation. For each test sentence
template we built a test set of 100 sentences by ran-
domly sampling nouns in this fashion.
The test sentences with novel verbs ask whether
the classifier transfers its learning about argument
role assignment to unseen verbs. Does it as-
sume the first of two nouns in a simple transi-
tive sentence (?A gorps B?) is the agent (A0) and
the second is the patient (A1)? Does it over-
generalize this rule to two-noun intransitives (?A
and B gorp?), mimicking children?s behavior? We
used two measures of success, one to assess clas-
sification accuracy, and the other to assess the
predicted error. We used a per argument F1 for
classification accuracy, with F1 based on correct
identification of individual nouns rather than full
phrases. Here precision is defined as the propor-
tion of nouns that were given the correct label
based on the argument they belong to, and recall
is the proportion of complete arguments for which
1
Corpus available at http://L2R.cs.uiuc.edu/
?
cogcomp/data.php
some noun in that argument was correctly labeled.
The desired labeling for ?A gorps B? is A0 for the
first argument and A1 for the second; for ?A and
B gorp? both arguments should be A0. To mea-
sure predicted errors we also report the proportion
of test sentences classified with A0 first and A1
second (%A0A1). This labeling is a correct gener-
alization for the novel ?A gorps B? sentences, but
is an overgeneralization for ?A and B gorp.?
3.1 Noun Pattern
The basic feature we propose is the noun pattern
feature. We hypothesize that children use the num-
ber and order of nouns to represent argument struc-
ture. To encode this we created a feature (NPat-
tern) that indicates how many nouns there are in
the sentence and which noun the target is. For ex-
ample, in our two-noun test sentences noun A has
the feature ? N? active indicating that it is the first
noun of two. Likewise for B the feature ?N ? is ac-
tive, indicating that it is the second of two nouns.
This feature is easy to compute once nouns are
identified, and does not require fine-grained dis-
tinctions between types of nouns or any other part
of speech. Table 1 shows the initial feature pro-
gression that involves this feature. The baseline
system (feature set 1) uses lexical features only:
the target noun and the root form of the predicate.
We first tested the hypothesis that children use
the NPattern features to distinguish different noun
arguments, but only for specific verbs. The NPat-
tern&V features are conjunctions of the target verb
and the noun pattern, and these are added to the
word features to form feature set 2. Now every
example has three features active: target noun, tar-
get predicate, and a NPattern&V feature indicating
?the target is the first of two nouns and the verb
is X.? This feature does not improve results on the
novel ?A gorps B? test set, or generate the predicted
error with the ?A and B gorp? test set, because the
verb-specific NPattern&V features provide no way
to generalize to unseen verbs.
We next tested the NPattern feature alone, with-
out making it verb-specific (feature set 3). The
noun pattern feature was added to the word fea-
tures and again each example had three features ac-
tive: target noun, target predicate, and the target?s
noun-pattern feature (first of two, second of three,
etc.). The abstract NPattern feature allows the
Baby SRL to generalize to new verbs: it increases
the system?s tendency to predict that the first of two
83
CHILDES WSJ
Unbiased Noun Choice Biased Noun Choice Biased Noun Choice
A gorps B A and B gorp A gorps B A and B gorp A gorps B A and B gorp
Features F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1 F1 %A0A1
1. Words 0.59 0.38 0.46 0.38 0.80 0.65 0.53 0.65 0.57 0.31 0.37 0.31
2. NPattern&V 0.53 0.28 0.54 0.28 0.81 0.67 0.53 0.67 0.56 0.31 0.39 0.31
3. NPattern 0.83 0.65 0.33 0.65 0.96 0.92 0.46 0.92 0.67 0.44 0.37 0.44
4. NPattern + NPattern&V 0.83 0.65 0.33 0.65 0.95 0.90 0.45 0.90 0.73 0.53 0.44 0.53
5. + VPosition 0.99 0.96 0.98 0.00 1.00 1.00 0.99 0.01 0.94 0.88 0.69 0.39
Table 1: Experiments showing the efficacy of Noun Pattern features for determining agent/patient roles in
simple two-noun sentences. The novel verb test sets assess whether the Baby SRL generalizes transitive
argument prediction to unseen verbs in the case of ?A gorps B? (increasing %A0A1 and thus F1), and
overgeneralizes in the case of ?A and B gorp? (increasing %A0A1, which is an error). By varying the
sampling method for creating the test sentences we can start with a biased or unbiased lexical baseline,
demonstrating that the noun pattern features still improve over knowledge that can be contained in
typical noun usage. The simple noun pattern features are still effective at learning this pattern when
trained with more complex Wall Street Journal training data.
nouns is A0 and the second of two nouns is A1 for
verbs not seen in training. Feature set 4 includes
both the abstract, non-verb-specific NPattern fea-
ture and the verb-specific version. This feature set
preserves the ability to generalize to unseen verbs;
thus the availability of the verb-specific NPattern
features during training did not prevent the abstract
NPattern features from gathering useful informa-
tion.
3.2 Lexical Cues for Role-Labeling
Thus far, the target nouns? lexical features pro-
vided little help in role labeling, allowing us to
clearly see the contribution of the proposed sim-
ple structural features. Would our structural fea-
tures produce any improvement above a more re-
alistic lexical baseline? We created a new set of
test sentences, sampling the A nouns based on the
distribution of nouns seen as the first of two nouns
in training, and the B nouns based on the distri-
bution of nouns seen as the second of two nouns.
Given this revised sampling of nouns, the words-
only baseline is strongly biased toward A0A1 (bi-
ased results for feature set 1 in table 1). This high
baseline reflects a general property of conversa-
tion: Lexical choices provide considerable infor-
mation about semantic roles. For example, the 6
most common nouns in the Eve corpus are pro-
nouns that are strongly biased in their positions
and in their semantic roles (e.g., ?you?, ?it?). De-
spite this high baseline, however, we see the same
pattern in the unbiased and biased experiments in
table 1. The addition of the NPattern features (fea-
ture set 3) substantially improves performance on
?A gorps B? test sentences, and promotes over-
generalization errors on ?A and B gorp? sentences.
3.3 More Complex Training Data
For comparison purposes we also trained the Baby
SRL on a subset of the Propbank training data
of Wall Street Journal (WSJ) text (Kingsbury and
Palmer, 2002). To approximate the simpler sen-
tences of child-directed speech we selected only
those sentences with 8 or fewer words. This
provided a training set of about 2500 sentences,
most with a single verb and two nouns to be la-
beled. The CDS and WSJ data pose similar prob-
lems for learning abstract and verb-specific knowl-
edge. However, newspaper text differs from ca-
sual speech to children in many ways, including
vocabulary and sentence complexity. One could
argue that the WSJ corpus presents a worst-case
scenario for learning based on shallow representa-
tions of sentence structure: Full passive sentences
are more common in written corpora such as the
WSJ than in samples of conversational speech, for
example (Roland et al, 2007). As a result of such
differences, two-noun sequences are less likely to
display an A0-A1 sequence in the WSJ (0.42 A0-
A1 in 2-noun sentences) than in the CDS training
data (0.67 A0-A1). The WSJ data provides a more
demanding test of the Baby SRL.
We trained the Baby SRL on the WSJ data, and
tested it using the biased lexical choices as de-
scribed above, sampling A and B nouns for novel-
verb test sentences based on the distribution of
nouns seen as the first of two nouns in training, and
as the second of two nouns, respectively. The WSJ
training produced performance strikingly similar
to the performance resulting from CDS training
(last 4 columns of Table 1). Even in this more
complex training set, the addition of the NPattern
84
features (feature set 3) improves performance on
?A gorps B? test sentences, and promotes over-
generalization errors on ?A and B gorp? sentences.
3.4 Tests with Familiar Verbs
Features Total A0 A1 A2 A4
1. Words 0.64 0.83 0.74 0.33 0.00
2. NPattern&V 0.67 0.86 0.77 0.45 0.44
3. NPattern 0.66 0.87 0.76 0.37 0.22
4. NPattern + NPattern&V 0.68 0.87 0.80 0.47 0.44
5. + VPosition 0.70 0.88 0.83 0.50 0.50
Table 2: Testing NPattern features on full SRL task
of heldout section 8 of Eve when trained on sec-
tions 9 through 20. Each result column reflects a
per argument F1.
Learning to interpret sentences depends on bal-
ancing abstract and verb-specific structural knowl-
edge. Natural linguistic corpora, including our
CDS training data, have few verbs of very high fre-
quency and a long tail of rare verbs. Frequent verbs
occur with differing argument patterns. For exam-
ple, ?have? and ?put? are frequent in the CDS data.
?Have? nearly always occurs in simple transitive
sentences that display the canonical word order of
English (e.g., ?I have cookies?). ?Put?, in contrast,
tends to appear in non-canonical sentences that do
not display an agent-patient ordering, including
imperatives (?Put it on the floor?). To probe the
Baby SRL?s ability to learn the argument-structure
preferences of familiar verbs, we tested it on a
held-out sample of CDS from the same source
(Eve sample 8, approximately 234 labeled sen-
tences). Table 2 shows the same feature progres-
sion shown previously, with the full SRL test set.
The words-only baseline (feature set 1 in Table 2)
yields fairly accurate performance, showing that
considerable success in role assignment in these
simple sentences can be achieved based on the
argument-role biases of the target nouns and the
familiar verbs. Despite this high baseline, how-
ever, we still see the benefit of simple structural
features. Adding verb-specific (feature set 2) or
abstract NPattern features (feature set 3) improves
classification performance, and the combination of
both verb-specific and abstract NPattern features
(feature set 4) yields higher performance than ei-
ther alone. The combination of abstract NPattern
features with the verb-specific versions allows the
Baby SRL both to generalize to unseen verbs, as
seen in earlier sections, and to learn the idiosyn-
crasies of known verbs.
3.5 Verb Position
The noun pattern feature results show that the
Baby SRL can learn helpful rules for argument-
role assignment using only information about the
number and order of nouns. It also makes the error
predicted by the structure-mapping account, and
documented in children, because it has no way to
represent the difference between the ?A gorps B?
and ?A and B gorp? test sentences. At some point
the learner must develop more sophisticated syn-
tactic representations that could differentiate these
two. These could include many aspects of the sen-
tence, including noun-phrase and verb-phrase mor-
phological features, and word-order features. As a
first step in examining recovery from the predicted
error, we focused on word-order features. We did
this by adding a verb position feature (VPosition)
that specifies whether the target noun is before or
after the verb. Now simple transitive sentences in
training should support the generalization that pre-
verbal nouns tend to be agents, and post-verbal
nouns tend to be patients. In testing, the Baby
SRL?s classification of the ?A gorps B? and ?A and
B gorp? sentences should diverge.
When we add verb position information (fea-
ture set 5 in table 1 and 2), performance improves
still further for transitive sentences, both with bi-
ased and unbiased test sentences. Also, for the first
time, the A0A1 pattern is predicted less often for
?A and B gorp? sentences. This error diminished
because the classifier was able to use the verb po-
sition features to distinguish these from ?A gorps
B? sentences.
Unbiased Lexical
A gorps B A and B gorp
Features F1 %A0A1 F1 %A0A1
1. Words 0.59 0.38 0.46 0.38
3. NPattern 0.83 0.65 0.33 0.65
6. VPosition 0.99 0.95 0.97 0.00
Table 3: Verb Position vs. Noun Pattern features
alone. Verb position features yield better overall
performance, but do not replicate the error on ?A
and B gorp? sentences seen with children.
Verb position alone provides another simple ab-
stract representation of sentence structure, so it
might be proposed as an equally natural initial
representation for human learners, rather than the
noun pattern features we proposed. The VPo-
sition features should also support learning and
generalization of word-order rules for interpret-
ing transitive sentences, thus reproducing some of
85
the data from children that we reviewed above.
In table 3 we compared the words-only baseline
(set 1), words and NPattern features (set 3), and a
new feature set, words and VPosition (set 6). In
terms of correct performance on novel transitive
verbs (?A gorps B?), the VPosition features out-
perform the NPattern features. This may be partly
because the same VPosition features are used in
all sentences during training, while the NPattern
features partition sentences by number of nouns,
but is also due to the fact that the verb position
features provide a more sophisticated representa-
tion of English sentence structure. Verb position
features can distinguish transitive sentences from
imperatives containing multiple post-verbal nouns,
for example. Although verb position is ultimately
a more powerful representation of word order for
English sentences, it does not accurately reproduce
a 21-month-old?s performance on all aspects of
this task. In particular, the VPosition feature does
not support the overgeneralization of the A0A1
pattern to the ?A and B gorp? test sentences. This
suggests that children?s very early sentence com-
prehension is dominated by less sophisticated rep-
resentations of word order, akin to the NPattern
features we proposed.
3.6 Informativeness vs. Availability
In the preceding sections, we modeled increases
in syntactic knowledge by building in more so-
phisticated features. The Baby SRL escaped the
predicted error on two-noun intransitive sentences
when given access to features reflecting the posi-
tion of the target noun relative to the verb. This
imposed sequence of features is useful as a starting
point, but a more satisfying approach would be to
use the Baby SRL to explore possible reasons why
NPattern features might dominate early in acquisi-
tion, even though VPosition features are ultimately
more useful for English.
In theory, a feature might be unavailable early in
acquisition because of its computational complex-
ity. For example, lexical features are presumably
less complex than relative position features such as
NPattern and VPosition. In practice, features can
also be unavailable at first because of an informa-
tional lack. Here we suggest that NPattern features
might dominate VPosition features early in acqui-
sition because the early lexicon is dominated by
nouns, and it is easier to compute position relative
to a known word than to an unknown word. Many
studies have shown that children?s early vocabu-
lary is dominated by names for objects and peo-
ple (Gentner and Boroditsky, 2001).
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0  5000  10000  15000  20000
 0
 30
 60
 90
 120
 150
 180
A0
-A
1
#V
erb
s
Examples
_N
_V
Known Verbs
(a) Verb threshold = 5
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0  5000  10000  15000  20000
 0
 30
 60
 90
 120
 150
 180
A0
-A
1
#V
erb
s
Examples
_N
_V
Known Verbs
(b) Verb threshold = 20
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0  5000  10000  15000  20000
 0
 30
 60
 90
 120
 150
 180
A0
-A
1
#V
erb
s
Examples
_N
_V
Known Verbs
(c) Verb threshold = 20, +verb-specific features
Figure 1: Testing the consequences of the assump-
tion that Verb Position features are only active for
familiar verbs. The figure plots the bias of the fea-
tures ? N? and ? V? to predict A0 over A1, as the
difference between the weights of these connec-
tions in the learned network. Verb position fea-
tures win out over noun pattern features as the
verb vocabulary grows. Varying the verb familiar-
ity threshold ((a) vs. (b)) and the presence versus
absence of verb-specific versions of the structural
features ((b) vs. (c)) affects how quickly the verb
position features become dominant.
To test the consequences of this proposed infor-
86
mational bottleneck on the relative weighting of
NPattern and VPosition features during training,
we modified the Baby SRL?s training procedure
such that NPattern features were always active, but
VPosition features were active during training only
when the verb in the current example had been en-
countered a critical number of times. This repre-
sents the assumption that the child can recognize
which words in the sentence are nouns, based on
lexical familiarity or morphological context (Wax-
man and Booth, 2001), but is less likely to be able
to represent position relative to the verb without
knowing the verb well.
Figure 1 shows the tendency of the NPattern fea-
ture ? N? (first of two nouns) and the VPosition
feature ? V? (pre-verbal noun) to predict the role
A0 as opposed to A1 as the difference between
the weights of these connections in the learned net-
work. Figure 1(a) shows the results when VPosi-
tion features were active whenever the target verb
had occurred at least 5 times; in Figure 1(b) the
threshold for verb familiarity was 20. In both fig-
ures we see that the VPosition features win out
over the NPattern features as the verb vocabulary
grows. Varying the degree of verb familiarity re-
quired to accurately represent VPosition features
affects how quickly the VPosition features win
out (compare Figures 1(a) and 1(b)). Figure 1(c)
shows the same analysis with a threshold of 20,
but with verb-specific as well as abstract versions
of the NPattern and the VPosition features. In this
procedure, every example started with three fea-
tures: target noun, target predicate, NPattern, and
if the verb was known, added NPattern&V, VPo-
sition, and VPosition&V. Comparing Figures 1(b)
and 1(c), we see that the addition of verb-specific
versions of the structural features also affects the
rate at which the VPosition features come to dom-
inate the NPattern features.
Thus, in training the VPosition features become
dominant as the SRL learns to recognize more
verbs. However, the VPosition features are inac-
tive when the Baby SRL encounters the novel-verb
test sentences. Since the NPattern features are ac-
tive in test, the system generates the predicted error
until the bias of the NPattern features reaches 0.
Note in figure 1(c) that when verb-specific struc-
tural features were added, the Baby SRL never
learned to entirely discount the NPattern features
within the range of training provided. This result
is reminiscent of suggestions in the psycholinguis-
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
A0
A1
Noise
Words
+NPattern+NPattern&V
+VPosition
Figure 2: Testing the ability of simple features
to cope with varying amounts of noisy feedback.
Even with noisy feedback, the noun pattern fea-
tures support learning and generalization to new
verbs of a simple agent-patient template for un-
derstanding transitive sentences. These results are
lower than those found in table 1 due to slightly
different training assumptions.
tics literature that shallow representations of syn-
tax persist in the adult parser, alongside more so-
phisticated representations (e.g., (Ferreira, 2003)).
3.7 Noisy Training
So far, the Baby SRL has only been trained with
perfect feedback. Theories of human language ac-
quisition assume that learning to understand sen-
tences is naturally a partially-supervised task: the
child uses existing knowledge of words and syntax
to assign a meaning to a sentence; the appropriate-
ness of this meaning for the referential context pro-
vides the feedback (e.g., (Pinker, 1989)). But this
feedback must be noisy. Referential scenes pro-
vide useful but often ambiguous information about
the semantic roles of sentence participants. For ex-
ample, a participant could be construed as an agent
of fleeing or as a patient being chased. In a final
set of experiments, we examined the generaliza-
tion abilities of the Baby SRL as a function of the
integrity of semantic feedback.
We provided noisy semantic-role feedback dur-
ing training by giving a randomly-selected argu-
ment label on 0 to 100% of examples. Following
this training, we tested with the ?A gorps B? test
sentences, using the unbiased noun choices.
As shown in Figure 2, feature sets including
NPattern or VPosition features yield reasonable
performance on the novel verb test sentences up to
50% noise, and promote an A0-A1 sequence over
87
the words-only baseline even at higher noise lev-
els. Thus the proposed simple structural features
are robust to noisy feedback.
4 Conclusion
The simplified SRL classifier mimicked experi-
mental results with toddlers. We structured the
learning task to ask whether shallow representa-
tions of sentence structure provided a useful ini-
tial representation for learning to interpret sen-
tences. Given representations of the number and
order of nouns in the sentence (noun pattern fea-
tures), the Baby SRL learned to classify the first
of two nouns as an agent and the second as a pa-
tient. When provided with both verb-general and
verb-specific noun pattern features, the Baby SRL
learned to balance verb-specific and abstract syn-
tactic knowledge. By treating each noun as an
argument, it also reproduced the errors children
make. Crucially, verb-position features improved
performance when added to the noun-pattern fea-
ture, but when presented alone failed to produce
the error found with toddlers. We believe that
our model can be naturally extended to support
the case in which the arguments are noun phrases
rather than single noun words and this extension is
one of the first steps we will explore next.
Acknowledgments
We would like to thank our annotators, espe-
cially Yuancheng Tu. This research is supported
by NSF grant BCS-0620257 and NIH grant R01-
HD054448.
References
Brown, R. 1973. A First Language. Harvard Univer-
sity Press, Cambridge, MA.
Carreras, X. and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared tasks: Semantic role label-
ing. In Proceedings of CoNLL-2004, pages 89?97.
Boston, MA, USA.
Charniak, E. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. National
Conference on Artificial Intelligence.
Ferreira, F. 2003. The misinterpretation of noncanoni-
cal sentences. Cognitive Psychology, 47:164?203.
Fisher, C. 1996. Structural limits on verb mapping:
The role of analogy in children?s interpretation of
sentences. Cognitive Psychology, 31:41?81.
Gentner, D. and L. Boroditsky. 2001. Individuation,
relativity and early word learning. In Bowerman, M.
and S. C. Levinson, editors, Language acquisition
and conceptual development, pages 215?256. Cam-
bridge University Press, New York.
Gertner, Y. and C. Fisher. 2006. Predicted errors in
early verb learning. In 31st Annual Boston Univer-
sity Conference on Language Development.
Gertner, Y., C. Fisher, and J. Eisengart. 2006. Learning
words and rules: Abstract knowledge of word order
in early sentence comprehension. Psychological Sci-
ence, 17:684?691.
Grove, A. and D. Roth. 2001. Linear concepts and
hidden variables. Machine Learning, 42(1/2):123?
141.
Kingsbury, P. and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of LREC-2002, Spain.
MacWhinney, B. 2000. The CHILDES project: Tools
for analyzing talk. Third Edition. Lawrence Elrbaum
Associates, Mahwah, NJ.
Naigles, L. R. 1990. Children use syntax to learn verb
meanings. Journal of Child Language, 17:357?374.
Pinker, S. 1989. Learnability and Cognition. Cam-
bridge: MIT Press.
Punyakanok, V., D. Roth, and W. Yih. 2005. The ne-
cessity of syntactic parsing for semantic role label-
ing. In Proc. of the International Joint Conference
on Artificial Intelligence (IJCAI), pages 1117?1123.
Roland, D., F. Dick, and J. L. Elman. 2007. Fre-
quency of basic english grammatical structures: A
corpus analysis. Journal of Memory and Language,
57:348?379.
Waxman, S. R. and A. Booth. 2001. Seeing pink
elephants: Fourteen-month-olds?s interpretations of
novel nouns and adjectives. Cognitive Psychology,
43:217?242.
Yuan, S., C. Fisher, Y. Gertner, and J. Snedeker. 2007.
Participants are more than physical bodies: 21-
month-olds assign relational meaning to novel tran-
sitive verbs. In Biennial Meeting of the Society for
Research in Child Development, Boston, MA.
88
