Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 11?18,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
POS Tagging for Historical Texts with Sparse Training Data
Marcel Bollmann
Department of Linguistics, Ruhr University Bochum
bollmann@linguistics.rub.de
Abstract
This paper presents a method for part-of-
speech tagging of historical data and eval-
uates it on texts from different corpora
of historical German (15th?18th century).
Spelling normalization is used to prepro-
cess the texts before applying a POS tag-
ger trained on modern German corpora.
Using only 250 manually normalized to-
kens as training data, the tagging accuracy
of a manuscript from the 15th century can
be raised from 28.65% to 74.89%.
1 Introduction1
Part-of-speech (POS) tagging of modern language
data is a well-explored field, commonly achiev-
ing accuracies around 97% (Brants, 2000; Schmid
and Laws, 2008). For historical language varieties,
the situation is worse, as specialized taggers are
typically not available. As an example, a study
by Scheible et al (2011a) reports an average tag-
ging accuracy of 69.6% for Early Modern German
texts. However, with projects to create historical
corpora being on the rise (Sa?nchez-Marco et al,
2010; Scheible et al, 2011b, are recent examples),
the need for more accurate tagging methods on
these types of data increases.
A common approach for historical texts is to
use spelling normalization to map historical word-
forms to modern ones (Baron and Rayson, 2008;
Jurish, 2010). Manually normalized data was
found to improve POS tagging accuracy for a
variety of languages such as German, English,
and Portuguese, with accuracies between 79%
and 91% (Scheible et al, 2011a; Rayson et al,
2007; Hendrickx and Marquilhas, 2011).
1I would like to thank the anonymous reviewers for their
helpful comments. The research reported here was sup-
ported by Deutsche Forschungsgemeinschaft (DFG), Grants
DI 1558/4-1 and DI 1558/5-1.
This paper presents results for POS tagging of
historical German from 1400 to 1770, classified
here as Early New High German (ENHG), us-
ing automatic spelling normalization to prepro-
cess the data for a POS tagger trained on mod-
ern German corpora. To train the normalization
tool, short fragments of a few hundred tokens are
used for each text. This approach allows for a
better adaptation to the individual spelling char-
acteristics of each text while requiring only small
amounts of training data. Additionally, different
ways to deal with typical obstacles for processing
historical texts (e.g., inconsistent use of punctua-
tion) are compared.
The structure of this paper is as follows. Sec. 2
presents the historical texts used for the evalua-
tion. Sec. 3 describes the approach to normal-
ization, while Sec. 4 discusses problems and re-
sults of POS tagging on normalized data. Sec. 5
presents related work, and Sec. 6 concludes.
2 Corpora
This study considers texts from two corpora of his-
torical German: the Anselm corpus (Dipper and
Schultz-Balluff, 2013) and the GerManC-GS cor-
pus (Scheible et al, 2011b).
The Anselm corpus consists of more than
50 different versions of a medieval religious trea-
tise written up in various German dialects. As the
creation of gold-standard annotations for the cor-
pus is still in progress, only two texts are used
here: a manuscript in an Eastern Upper German
dialect kept in Melk, Austria; and an Eastern
Central German manuscript kept in Berlin. Both
manuscripts are dated to the 15th century.
The GerManC-GS corpus aims to be a repre-
sentative subcorpus of GerManC with additional
gold-standard annotations. It contains texts from
Early Modern German categorized by genre, re-
gion, and time period. For this study, the three
texts of the genre ?sermon? are used. They are
11
Corpus Date Name Tokens
Anselm
15c Berlin 5,399
15c Melk 4,783
GerManC-
GS
1677 LeichSermon 2,585
1730 JubelFeste 2,523
1770 Gottesdienst 2,292
Table 1: Texts used for the evaluation
dated from 1677 to 1770, which makes them con-
siderably newer than the Anselm texts. Table 1
gives an overview of all texts used here.
All texts are manually annotated with normal-
izations and POS tags. In the normalization layer,
tokens are mapped to modern German equivalents.
The normalization schemes are not identical, but
roughly comparable for both GerManC-GS and
Anselm (see Scheible et al (2011b) and Bollmann
et al (2012) for details). In both corpora, POS tag-
ging follows the STTS tagset (Schiller et al, 1999)
without morphological information, though some
additional tags were introduced in GerManC-GS.
For our evaluation, they are mapped back to stan-
dard STTS tags; this mapping only affects 80 to-
kens from all three texts.
Additionally, both corpora are annotated with
modern punctuation and sentence boundaries;
however, while modern punctuation is a sep-
arate annotation layer in Anselm, there is al-
ways a 1:1 correspondence between historical and
modern (i.e., normalized) punctuation marks in
GerManC-GS.
Finally, both corpora preserve many spelling
characteristics of the original manuscripts, e.g.,
superposition of characters such as u?, or abbrevi-
ation marks such as the nasal bar (as in v?). Be-
fore any further processing, all wordforms are sim-
plified to plain alphabetic characters; e.g., u? is
mapped to uo. For some abbreviation marks in
the Anselm corpus, there is no clear ?best? sim-
plification: the nasal bar is a prime example here,
which should be simplified most appropriately to
e, (e)n, (e)m, or nothing, either before or after the
letter on which it is placed, depending on context.
In these cases, manually defined heuristics were
used to guess the most appropriate mapping. As
capitalization is not used consistently in the texts,
all letters were additionally lowercased.
3 Normalization
Spelling normalization is performed using the
Norma tool (Bollmann, 2012). It implements a
chain of normalization methods?to the effect that
methods further down the chain are only called if
previous ones failed to produce a result?in the
following order: (1) wordlist mapping; (2) rule-
based normalization; and (3) weighted Leven-
shtein distance.
Wordlist mapping considers simple 1:1 map-
pings of historical wordforms to modern ones
(e.g., vnd? und ?and?), while rule-based normal-
ization applies context-sensitive character rewrite
rules (e.g., transform v to u between a word
boundary and n) to an input string from left to
right. Weighted Levenshtein distance assigns in-
dividual weights to character replacements (e.g.,
v ? u), and performs normalization by retrieving
the wordform from a modern lexicon which can
be derived from the historical input wordformwith
the lowest cost, i.e., using a sequence of edit oper-
ations with the lowest sum of weights.
3.1 Normalization procedure
All normalization algorithms described above re-
quire some kind of parametrization to work (i.e.,
a wordlist; rewrite rules; Levenshtein weights).
These parametrizations are neither hard-coded nor
manually defined, but are derived automatically by
the Norma tool from a set of manually normal-
ized training data. For this purpose, short samples
from the text to be normalized are used; i.e., train-
ing set and evaluation set are always disjoint parts
of the same text. The reasons for choosing this
approach lie in the individual spelling character-
istics of the texts?the following examples show
excerpts from Berlin, Melk, and LeichSermon, re-
spectively, along with their (gold-standard) nor-
malizations:
(1) dyn
dein
lybes
liebes
kynt
kind
?your dear child?
(2) mein
mein
liebs
liebes
chind
kind
?my dear child?
(3) eins
eins
ihrer
ihrer
andern
anderen
kinder
kinder
?one of their other children?
12
Text Baseline Normalizations
100 250 500 1,000
Berlin 23.05% 68.99% 75.02% 79.14% 81.83%
Melk 39.32% 69.10% 74.39% 75.74% 77.98%
LeichSermon 72.71% 77.96% 80.51% 82.85% 87.23%
JubelFeste 79.47% 88.50% 89.98% 91.87% 93.13%
Gottesdienst 83.41% 93.77% 95.24% 95.27% 95.56%
Table 2: Normalization accuracy after training on n tokens and evaluating on 1,000 tokens (average of
10 random training and evaluation sets), compared to the ?baseline? score of the full text without any
normalization
The first two examples, while both dated to the
15th century, show quite different spellings of the
modern Kind ?child?: Ex. (1) shows the frequent
use of y for modern ei or i(e), while Ex. (2) demon-
strates the frequent spelling ch for k. These dif-
ferences are likely a cause of the different dialec-
tal regions from which the manuscripts originate,
but could also be attributed, at least in parts, to in-
dividual preferences by the manuscripts? writers.
The LeichSermon text from 1677 in Ex. (3), on
the other hand, already has the modern German
spelling Kind.
Given this range of spelling variations, it seems
implausible to achieve good normalization results
using the same parametrization for each of the
texts. Furthermore, for the older manuscripts
showing more variation such as in Ex. (1), it is
unclear what other training data could be used.
The full GerManC-GS consists of texts from 1650
to 1800, while Jurish (2010) uses a corpus of
German texts from 1780 to 1880; these texts are
all considerably newer, consequently having less
spelling variation than the Anselm texts. This lack
of appropriate training data applies similarly to all
kinds of less-resourced language varieties.
Therefore, while this approach requires slightly
more effort for manually normalizing parts of the
texts beforehand, it does not depend on the avail-
ability of a large training corpus or a specialized
tool for the language variety to be processed.
3.2 Evaluation
Normalization is evaluated separately for each
text, using a part of that text for training and eval-
uating on a different part of the same text. To
address the question of how much training data
is needed, evaluation is performed with different
sizes of the training set in a range between 100 and
1,000 tokens. The evaluation set is kept at a fixed
size of 1,000 tokens. Normalization accuracy is
calculated by taking the average of 10 trials with
randomly drawn training and evaluation sets. The
results of this evaluation are shown in Table 2.
The baseline score for a text is defined as the
percentage of matching tokens between the un-
modified, historical text and its gold-standard nor-
malization. There is a clear difference between the
Anselm texts, with scores of 23% and 39%, and
the GerManC-GS texts, which range from 72%
to 83%. This shows that spelling variation affects
significantly more wordforms in the Anselm texts.
The age of a text is likely to be the main factor
for this, as even within the group of GerManC-
GS texts, a clear tendency for newer texts to have
higher baseline scores can be observed.
Spelling normalization with the Norma tool
shows rather positive results even for small train-
ing samples: with only 100 tokens used for train-
ing, it achieves a normalization accuracy of 69%
for the Anselm texts, and raises the score for the
GerManC-GS texts by 5?10 percentage points.
Using 250 tokens results in another noticeable in-
crease in accuracy, although the relative gain from
increasing the training size even further attenuates
after this point.
4 Part-of-speech tagging
While spelling normalization can be useful in it-
self (e.g., for search queries in the corpus), our
main focus is on its usefulness for further pro-
cessing of the data such as part-of-speech tagging.
The results presented here were achieved using the
RFTagger (Schmid and Laws, 2008) with an in-
creased context size of 10, which we found to per-
form best on average on our data.
13
Text OrigP ModP NoP
Berlin 85.78% 87.29% 87.07%
Melk 85.21% 87.76% 87.74%
LeichSermon 81.22% 80.59% 81.04%
JubelFeste 90.41% 90.41% 90.03%
Gottesdienst 93.24% 93.24% 92.27%
Table 3: Tagging accuracy on the gold-standard
normalizations (OrigP = original punctuation,
ModP = modern punctuation, NoP = no punctu-
ation)
4.1 Impact of punctuation
Normalization tries to handle the problem of
spelling inconsistencies found in historical lan-
guage data. However, this is not the only challenge
for processing the data with modern POS taggers.
There is often no consistent capitalization, which
can normally be used as a clue to detect nouns in
modern German. This has already led to all word-
forms being lowercased for the normalization pro-
cess. Additionally, punctuation marks are also of-
ten used inconsistently or are missing completely:
e.g., the Melk manuscript mostly uses virgules (vi-
sually resembling a modern slash ?/?) where mod-
ern German would use a full stop, but this is far
from a definite rule, and large parts of the Anselm
texts feature no punctuation marks at all. This
raises the question whether punctuation should be
used for POS tagging at all for these texts.
In order to test the impact of punctuation on
tagging performance, three scenarios are consid-
ered: tagging with original, modern, and no punc-
tuation marks. In order to provide a fairer com-
parison, instead of using the supplied parameter
file for German, we retrain RFTagger on a pre-
pared set of data. For this purpose, the TIGER
corpus (Brants et al, 2002) and version 6 of Tu?ba-
D/Z (Telljohann et al, 2004) are used. First, the
two corpora are combined?with minor modifica-
tions to the POS tags to make them uniform?and
lowercased. The combined corpus has a size of
more than 1.6 million tokens. Additionally, for the
evaluation without punctuation, a separate tagger
model is trained on a version of the TIGER/Tu?ba
corpus where all punctuation marks and sentence
boundaries have been removed.
Using these tagger models, tagging perfor-
Original 96.85%
Lowercased 96.50%
No punctuation and SB 96.22%
Lowercased + no punctuation and SB 95.74%
Table 4: Tagging accuracy on the combined
TIGER/Tu?ba corpus, using 10-fold CV, evaluated
with and without capitalization, punctuation, and
sentence boundaries (SB)
mance is evaluated on the gold-standard normal-
izations with different levels of punctuation. The
results are shown in Table 3. For better compara-
bility, accuracy was evaluated excluding punctua-
tion marks in all scenarios.
Tagging with modern punctuation or no punc-
tuation is shown to be best in all cases, with the
difference between these two scenarios never be-
ing statistically significant (p > 0.05). For the
Anselm texts, using the original punctuation is
worse than using none at all. This is not true
for GerManC-GS, though the differences are mi-
nor; also, original and modern punctuation are
identical for the JubelFeste and Gottesdienst texts,
showing that they already follow modern German
conventions in this regard.
The results show that removing all punctua-
tion marks does not lead to significant losses in
POS tagging accuracy. Indeed, for texts with in-
frequent and/or inconsistent use of punctuation
marks, discarding punctuation is shown to be
preferable. For these reasons, the tagging ap-
proach without punctuation is used for all follow-
ing experiments.
4.2 Tagging ?with handicaps?
So far, the preprocessing of the historical data in-
cludes removing all capitalization and punctua-
tion. Consequently, information about sentence
boundaries should also be removed, as it cannot
easily be derived from texts without (consistent)
punctuation. However, POS tagging with these
?handicaps? potentially increases the difficulty of
the task in general.
To gauge the extent of this effect, an evalua-
tion on modern data was performed using 10-fold
cross-validation on the combined TIGER/Tu?ba
corpus, both with and without these artificial
modifications. Table 4 shows the results of
14
Text Tokens Original Automatically normalized Gold
100 250 500 1,000
Berlin 4,719 28.65% 58.68% 74.89% 75.95% 78.03% 87.07%
Melk 4,550 44.70% 69.63% 74.02% 76.24% 78.66% 87.74%
LeichSermon 2,215 67.95% 72.87% 74.63% 75.85% 78.01% 81.04%
JubelFeste 2,137 82.26% 82.64% 83.62% 86.52% 87.74% 90.03%
Gottesdienst 1,953 88.07% 88.84% 90.27% 91.30% 91.65% 92.27%
Table 5: POS tagging accuracy on texts without punctuation and capitalization, for tagging on the original
data, the gold-standard normalization, and automatic normalizations using the first n tokens as training
data
this experiment; tagging accuracy drops from
96.85% to 95.74% when removing capitalization
and punctuation. While this change is significant
(p < 0.01) considering the corpus size, with re-
gard to the effort involved in manually annotating
whole texts with modern capitalization and punc-
tuation marks, it seems small enough to make tag-
ging without this information a viable approach
for historical data.
4.3 Tagging historical data
POS tagging on the historical texts is evaluated in
three different scenarios: first, tagging on the sim-
plified, but otherwise unmodified, original texts;
second, tagging on the gold-standard normaliza-
tions; and third, tagging on texts which have been
normalized automatically as described in Sec. 3.
For automatic normalization, the first n tokens
of a text were used for training the Norma tool,
with different values for n (cf. Sec. 3.2). Only the
remainder of the text has then been automatically
processed by Norma. This means that, e.g., for a
text with 500 tokens used for training, POS tag-
ging is performed on a version of the text con-
sisting of 500 gold-standard normalizations plus
automatically generated normalizations for the re-
mainder of the text. This evaluation method mod-
els a typical application scenario, where a tradeoff
is made between no manual effort (= tagging on
the original) and full manual preprocessing (= tag-
ging on the gold-standard).
Full evaluation results are shown in Table 5.
Tagging accuracy roughly correlates with nor-
malization accuracy (cf. Table 2); it tends to be
slightly above the normalization score for Anselm
and a few points below that score for GerManC-
GS. Tagging on the original, historical data is
particularly inaccurate for the Anselm texts, with
the Berlin text only achieving an accuracy of
28.7%. This again highlights the need for special-
ized tagging methods on such types of data. The
GerManC-GS texts from the 18th century perform
much better without normalization, with accura-
cies up to 88% for the Gottesdienst text. These
results mainly confirm the observations that the
Anselm texts show much more variety in spelling
than the newer texts from GerManC-GS.
Similar to the results for normalization, us-
ing only 100 tokens for training is enough to in-
crease tagging accuracy for the Melk text from
45% to 70%. For Berlin, this method results in
an even higher relative increase, more than dou-
bling the number of correct POS tags. Results
for these texts can be improved further to about
74% when using 250 tokens for training; after this
figure, POS tagging seems to profit less from in-
creasing the size of the training set, with accura-
cies around 78% for a training set of 1,000 tokens.
The GerManC-GS texts, particularly JubelFeste
and Gottesdienst, do not benefit as much from a
small number of training tokens. With 100 tokens,
POS tagging accuracy only increases by 0.38?0.77
percentage points. However, these texts already
have a comparatively high baseline to start with
(82?88%). As they are already much closer to
modern German spelling, fewer wordforms have
spelling variations at all; consequently, more train-
ing data is required to capture a similar amount of
variant wordforms as in the Anselm texts. Indeed,
when increasing the training portion to 1,000 to-
kens, the benefit of spelling normalization be-
comes more pronounced.
Curiously, for the LeichSermon text, even
the gold-standard normalization only achieves
81% accuracy, which is significantly lower than
for any other text in the evaluation. This is un-
15
expected, considering that the text is much more
recent than Berlin and Melk. The reason for this
discrepancy is the frequent use of bible verse num-
bers in LeichSermon, which are written as numer-
als followed by a dot and annotated as CARD (car-
dinal number) in the gold-standard data. In the
TIGER corpus and Tu?ba-D/Z, such numerals are
treated as ordinal numbers and tagged as ADJA,
leading to a high number of mismatching tags.
4.4 Error analysis
POS tagging results for the historical texts are still
considerably worse than those for modern data,
even when tagging on gold-standard normaliza-
tions (81?92% vs. 95.74%). There are several fac-
tors responsible for this.
It is important to observe that even perfectly
normalized historical data has different character-
istics than modern data, as normalization only af-
fects the spelling of wordforms. One potential
source of errors are semantic changes, as shown
in Ex. (4) from the LeichSermon text: the word-
form so is an adverb in modern German, but is
frequently used as a relative pronoun (PRELS2)
in ENHG, which nevers occurs in the training data
of the TIGER/Tu?ba corpus.
(4) die
die
ART
faelle
fa?lle
NN
so
so
PRELS
aus
aus
APPR
schwacheit
schwachheit
NN
geschehen
geschehen
VVPP
?the cases which occur out of weakness?
Extinct wordforms are a major problem for the
normalization approach. They cannot usually be
normalized to a modern wordform by applying
spelling changes, but would have to be mapped on
a word-by-word basis. However, both GerManC-
GS and the normalization layer of Anselm3 map
extinct wordforms to artificial lemmas, which are
still useful to identify spelling variants, but im-
practical for this POS tagging approach. A com-
mon example in Melk is czuhant ?immediately?,
2Actually, GerManC-GS annotates so in this example
with the new tag PTKREL, which is mapped back to PRELS
for reasons of compatibility. As PTKREL is not found in
TIGER or Tu?ba-D/Z, keeping this tag would not solve the
problem here, though.
3The Anselm corpus provides an additional ?moderniza-
tion? layer which maps extinct forms to actual modern words,
but a first evaluation showed that using this layer has a nega-
tive impact on overall normalization accuracy.
which is mapped to the artificial lemma zehant,
but would rather be expressed as sofort in modern
German:
(5) czuhant
zehant
ADV
chust
ku?sst
VVFIN
iudas
judas
NE
mein
mein
PPOSAT
chint
kind
NN
?Immediately, Judas kisses my child?
Finally, a significant number of errors ap-
pears to result from limitations of the modern
TIGER/Tu?ba corpus used to train the POS tag-
ger. This corpus is created from newspaper texts,
which are typically written in a rather formal style.
The Anselm texts, on the other hand, consist of
question/answer sets which contain a lot of direct
speech. Similarly, the Gottesdienst text is a reli-
gious speech which addresses its audience right
from the beginning. Ex. (6) shows a phrase that
occurs frequently in the Berlin text:
(6) sieh
VVIMP
anselm
NE
?Look, Anselm?
The imperative form sieh ?look? is used
24 times in the Berlin text, but typically mistagged
as a proper noun (NE) despite being correctly nor-
malized. A look at the TIGER/Tu?ba training data
reveals the cause for this: the wordform sieh does
not occur there at all; only the standard form siehe
was learned. Imperative verb forms in general
are very uncommon in TIGER/Tu?ba, only mak-
ing up 397 tokens (0.02%). In comparison, the
gold-standard POS annotation of Berlin already
contains 43 imperative verb forms (0.91%).
Similarly, the religious texts in Anselm and
GerManC-GS often use vocabulary that is rarely
used in newspaper text. Ex. (7) shows the fi-
nite verb form verschma?hten ?despised/spurned?,
which has only one occurrence in the TIGER/Tu?ba
corpus where it was used as an adjective instead,
inevitably leading to a tagging error.
(7) vnd
und
KON
vorsmeten
verschma?hten
VVFIN
yn
ihn
PPER
?and [they] despised him?
These examples show that even if spelling nor-
malization was done perfectly on historical texts,
semantic/syntactic variation and domain adapta-
tion of the POS tagger provide further obstacles
for achieving higher tagging accuracies.
16
5 Related work
For automatic spelling normalization, VARD 2
(Baron and Rayson, 2008) is another tool that has
been developed for Early Modern English. It has
been successfully adapted to other languages, e.g.
Portuguese (Hendrickx and Marquilhas, 2011),
though previous experiments found it to perform
worse than Norma on the Anselm data (Bollmann,
2012). Jurish (2010) presents a normalization
method that includes token context, which seems
to be the logical next step to further improve nor-
malization results.
POS tagging on normalized data has been tried
for the GerManC-GS corpus before with an aver-
age accuracy of 79.7% (Scheible et al, 2011a),
however, only manual normalization was consid-
ered. For English, Rayson et al (2007) report
an accuracy of 89?91% on gold standard normal-
izations and 85?89% on automatically normal-
ized texts. Hendrickx and Marquilhas (2011) per-
form a similar evaluation for Portuguese, achiev-
ing 86.6% and 83.4% on gold standard and auto-
matic normalizations, respectively.
There are some notable differences, however,
between the aforementioned studies and the ap-
proach outlined here. Firstly, those studies using
automatic normalization methods typically utilize
either a much higher amount of training data or
some kind of manually crafted resource. VARD,
for instance, uses a manually compiled list of
spelling variants totalling more than 45,000 en-
tries (Rayson et al, 2005), while Hendrickx and
Marquilhas (2011) use a training set of more than
37,000 tokens. While I certainly expect to improve
the results in the future by using full texts from the
Anselm and/or GerManC-GS corpora as basis for
training, this approach might not always be feasi-
ble. The approach presented here, requiring only a
few hundred tokens for training, seems especially
suited for languages where projects to create his-
torical corpora have only been started, and there-
fore do not have large amounts of previously an-
notated training material to fall back to.
Secondly, the Anselm texts evaluated here show
a much lower baseline than the texts evaluated in
other studies. Without normalization, POS tag-
ging accuracy is 82?88% in Rayson et al (2007),
76.9% in Hendrickx and Marquilhas (2011), and
69.6% for the German data in Scheible et al
(2011a). The texts from Berlin and Melk, on the
other hand, perform much worse without the nor-
malization step (28.7% and 44.7%, respectively).
This suggests a higher amount of variance in the
Anselm data compared to the types of text used in
previous studies, making their automatic process-
ing a potentially more challenging problem. Also,
annotated data from these studies is less likely to
be useful as training data for these texts.
6 Conclusion
I presented an approach to part-of-speech tagging
for historical texts that uses spelling normalization
as a preprocessing step. Evaluation on texts from
Early New High German showed that by manually
normalizing 250 tokens of a text and using them as
training data, automatic normalization of the re-
maining text performs well enough to result in a
notable increase in POS tagging accuracy. Texts
with more spelling variation were shown to ben-
efit more from this approach than texts which are
already closer to the modern target language.
For one German manuscript from the 15th cen-
tury, this method increased tagging accuracy from
28.65% to 74.89%. While this is still far from
the accuracy scores reported for modern language
data, and also quite a bit worse than tagging
on the gold-standard normalization (87.07% for
this text), it offers a way to facilitate the (semi-
automatic) POS annotation of historical texts with
relatively minor effort. Furthermore, as it does not
require a sizeable amount of training data, this ap-
proach is potentially interesting for less-resourced
language varieties in general, assuming some level
of graphematic similarity to a well-resourced tar-
get language.
Future work should likely consider inclusion of
token context for the normalization as proposed by
Jurish (2010). Analysis of the POS tagging errors
also highlighted some of the problems that remain.
Domain-specific differences can negatively impact
tagging performance even on perfectly normalized
data. Furthermore, spelling normalization cannot
account for semantic and syntactic peculiarities of
historical language. For a corpus of Old Spanish,
this led Sa?nchez-Marco et al (2010) to abandon
the normalization approach and use a customized
POS tagger instead. On the other hand, a study by
Dipper (2010) showed that normalization is still
beneficial even when retraining a tagger on a cor-
pus of historical data. Future research could try
to combine a normalization step with a modified
POS tagger to improve the results further.
17
References
Alistair Baron and Paul Rayson. 2008. VARD 2: A
tool for dealing with spelling variation in historical
corpora. In Proceedings of the Postgraduate Con-
ference in Corpus Linguistics.
Marcel Bollmann, Stefanie Dipper, Julia Krasselt, and
Florian Petran. 2012. Manual and semi-automatic
normalization of historical spelling ? Case studies
from Early New High German. In Proceedings
of KONVENS 2012 (LThist 2012 workshop), pages
342?350, Vienna, Austria.
Marcel Bollmann. 2012. (Semi-)automatic normaliza-
tion of historical texts using distance measures and
the Norma tool. In Proceedings of the Second Work-
shop on Annotation of Corpora for Research in the
Humanities (ACRH-2), Lisbon, Portugal.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, ed-
itors, Proceedings of the First Workshop on Tree-
banks and Linguistic Theories (TLT 2002), Sozopol,
Bulgaria.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of ANLP 2000, pages
224?231, Seattle, USA.
Stefanie Dipper and Simone Schultz-Balluff. 2013.
The Anselm corpus: Methods and perspectives of
a parallel aligned corpus. In Proceedings of the
NODALIDA Workshop on Computational Historical
Linguistics.
Stefanie Dipper. 2010. POS-tagging of historical lan-
guage data: First experiments. In Proceedings of
KONVENS 2010, pages 117?121, Saarbru?cken, Ger-
many.
Iris Hendrickx and Rita Marquilhas. 2011. From old
texts to modern spellings: an experiment in auto-
matic normalisation. JLCL, 26(2):65?76.
Bryan Jurish. 2010. More than words: using to-
ken context to improve canonicalization of histori-
cal German. Journal for Language Technology and
Computational Linguistics, 25(1):23?39.
Paul Rayson, Dawn Archer, and Nicholas Smith. 2005.
VARD versus Word. A comparison of the UCREL
variant detector and modern spell checkers on en-
glish historical corpora. In Proceedings of Corpus
Linguistics 2005, Birmingham, UK.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan
Culpeper, and Nicholas Smith. 2007. Tagging the
bard: Evaluating the accuracy of a modern POS tag-
ger on Early Modern English corpora. In Proceed-
ings of Corpus Linguistics 2007, University of Birm-
ingham, UK.
Cristina Sa?nchez-Marco, Gemma Boleda, Josep Maria
Fontana, and Judith Domingo. 2010. Annotation
and representation of a diachronic corpus of Span-
ish. In Proceedings of the Seventh Conference on
International Language Resources and Evaluation,
pages 2713?2718.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011a. Evaluating an ?off-the-shelf?
POS-tagger on Early Modern German text. In Pro-
ceedings of the ACL-HLT 2011 Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities (LaTeCH 2011), pages 19?
23, Portland, Oregon, USA.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011b. A gold standard corpus of
Early Modern German. In Proceedings of the ACL-
HLT 2011 Linguistic Annotation Workshop (LAW V),
pages 124?128, Portland, Oregon, USA.
Anne Schiller, Simone Teufel, Christine Sto?ckert, and
Christine Thielen. 1999. Guidelines fu?r das Tagging
deutscher Textcorpora mit STTS. Technical report.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of COLING ?08, Manchester, Great
Britain.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The Tu?ba-D/Z Treebank: Annotating Ger-
man with a Context-Free Backbone. In Proceed-
ings of the Fourth International Conference on
Language Resources and Evaluation (LREC 2004),
pages 2229?2235, Lisbon, Portugal.
18
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 86?90,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
CorA: A web-based annotation tool for historical
and other non-standard language data
Marcel Bollmann, Florian Petran, Stefanie Dipper, Julia Krasselt
Department of Linguistics
Ruhr-University Bochum, 44780 Bochum, Germany
{bollmann|petran|dipper|krasselt}@linguistics.rub.de
Abstract
We present CorA, a web-based annotation
tool for manual annotation of historical and
other non-standard language data. It allows
for editing the primary data and modify-
ing token boundaries during the annotation
process. Further, it supports immediate re-
training of taggers on newly annotated data.
1 Introduction
1
In recent years, the focus of research in natural
language processing has shifted from highly stan-
dardized text types, such as newspaper texts, to text
types that often infringe orthographic, grammatical
and stylistic norms normally associated with writ-
ten language. Prime examples are language data
produced in the context of computer-mediated com-
munication (CMC), such as Twitter or SMS data,
or contributions in chat rooms. Further examples
are data produced by learners or historical texts.
Tools trained on standardized data perform con-
siderably worse on ?non-standard varieties? such
as internet data (cf. Giesbrecht and Evert (2009)?s
work on tagging the web or Foster et al. (2011)?s
results for parsing Twitter data) or historical lan-
guage data (Rayson et al., 2007; Scheible et al.,
2011). This can mainly be attributed to the facts
that tools are applied out of domain, or only small
amounts of manually-annotated training data are
available.
A more fundamental problem is that common
and established methods and categories for lan-
guage analysis often do not fit the phenomena oc-
curring in non-standard data. For instance, gram-
maticalization is a process of language evolution
where new parts of speech are created or words
switch from one class to another. It is difficult to
draw strict categorial boundaries between words
1
The research reported here was financed by Deutsche
Forschungsgemeinschaft (DFG), Grant DI 1558/5-1.
that take part in a continuous smooth transition of
categories. Factors like these can also affect the
way the data should be tokenized, along with other
problems such as the lack of a fixed orthography.
In the light of the above, we developed a web-
based tool for manual annotation of non-standard
data. It allows for editing the primary data, e.g.
for correcting OCR errors of historical texts, or
for modifying token boundaries during the annota-
tion process. Furthermore, it supports immediate
retraining of taggers on newly annotated data, to
attenuate the problem of sparse training data.
CorA is currently used in several projects that
annotate historical data, and one project that ana-
lyzes chat data. So far, about 200,000 tokens in
84 texts have been annotated in CorA. Once the
annotation process is completed, the transcriptions
and their annotations are imported into the ANNIS
corpus tool (Zeldes et al., 2009) where they can be
searched and visualized.
The paper focuses on the annotation of historical
data. Sec. 2 presents the tool, and Sec. 3 describes
the data model. Sec. 4 concludes.
2 Tool Description
CorA uses a web-based architecture:
2
All data
is stored on a server, while users can access and
edit annotations from anywhere using their web
browser. This approach greatly simplifies collabo-
rative work within a project, as it ensures that all
users are working on the same version of the data
at all times, and requires no software installation
on the user?s side. Users can be assigned to indi-
vidual project groups and are only able to access
documents within their group(s).
2.1 The annotation editor
All annotation in CorA is done on a token level;
the currently supported annotation types are part-
2
It implements a standard AJAX architecture using PHP 5,
MySQL, and JavaScript.
86
Figure 1: Web interface of CorA showing the annotation editor
of-speech tags, morphology tags, lemmatization,
and (spelling) normalization. The tool is designed
to increase productivity for these particular an-
notation tasks, while sacrificing some amount of
flexibility (e.g., using different annotation layers,
or annotating spans of tokens). Note that this is
mainly a restriction of the web interface; the under-
lying database structure is much more flexible (cf.
Sec. 3), facilitating the later addition of other types
of annotation, if desired.
Tokens are displayed vertically, i.e., one token
per line. This way, the annotations also line up
vertically and are always within view. Addition-
ally, a horizontal text preview can be displayed at
the bottom of the screen, which makes it easier
to read a continuous text passage. Fig. 1 shows a
sample screenshot of the editor window.
3
Users
can customize the editor, e.g. by hiding selected
columns.
Parts-of-speech and morphology Within the
editor, both POS and morphology tags can be se-
lected from a dropdown box, which has the ad-
vantage of allowing both mouse-based and faster
keyboard-based input. Tagsets can be defined in-
dividually for each text. If morphology tags are
used, the selection of tags in the dropdown box is
restricted by the chosen POS tag.
3
The user interface is only available in German at the time
of writing, but an English version is planned.
Lemmatization Lemma forms are entered into
a text field, which can optionally be linked to a
pre-defined lexicon from which it retrieves auto-
completion suggestions. Furthermore, if an identi-
cal token has already been annotated with a lemma
form elsewhere within the same project, that lemma
is always displayed as a highlighted suggestion.
Normalization For corpora of non-standard lan-
guage varieties, spelling normalization is often
found as an annotation layer, see, e.g., Scheible
et al. (2011) for historical data and Reznicek et al.
(2013) for learner data.
In addition to normalization, an optional mod-
ernization layer can be used that defaults to the
content of the normalization field. The normaliza-
tion layer can be used for standardizing spelling,
and the modernization layer for standardizing in-
flection and semantics (Bollmann et al., 2012).
Meta information CorA features a progress indi-
cator which can be used to mark annotations as ver-
ified (see the green bar in Fig. 1). Besides serving
as a visual aid for the annotator, it is also used for
the automatic annotation component (cf. Sec. 2.2).
Additionally, tokens can be marked as needing fur-
ther review (indicated with a red checkbox), and
comments can be added.
2.2 Automatic annotation
CorA supports (semi-)automatic annotation by in-
tegrating external annotation software on the server
87
side. Currently, RFTagger (Schmid and Laws,
2008) and the Norma tool for automatic normaliza-
tion (Bollmann, 2012) are supported, but in princi-
ple any other annotation tool can be integrated as
well. The ?retraining? feature collects all verified
annotations from a project and feeds them to the
tools? training functions. The user is then able to
invoke the automatic annotation process using the
newly trained parametrizations, which causes all
tokens not yet marked as verified to be overwritten
with the new annotations.
The retraining module is particularly relevant for
non-standard language varieties where appropriate
language models may not be available. The idea
is that as more data is manually annotated within
a corpus, the performance of automatic annotation
tools increases when retrained on that data. This
in turn makes it desirable to re-apply the automatic
tools during the annotation process.
2.3 Editing primary data
In diplomatic transcriptions of historical
manuscripts, the transcripts reproduce the
manuscripts in the most accurate way, by encoding
all relevant details of special graphemes and
diacritics, and also preserving layout information.
Transcribers often use ASCII-based encodings for
special characters, e.g., the dollar sign $ in place
of a long s (???).
The data model of CorA (cf. Sec. 3) distin-
guishes between different types of token representa-
tions. In the annotation editor, the user can choose
to display either the original transcription layer or
the UTF-8 representation.
If an error in the primary data?e.g., a transcrip-
tion error or wrong tokenization?is noticed during
the annotation, it can be corrected directly within
the editor. CorA provides functionality to edit, add,
or delete existing tokens. Furthermore, external
scripts can be embedded to process any changes,
by checking an edited token for validity (e.g., if
tokens need to conform to a certain transcription
format), or generating the UTF-8 representation
by interpreting special characters (e.g., mapping $
to ?).
2.4 Comparison to related tools
There is a range of annotation tools that can be
used for enriching data with different kinds of an-
notations. Prominent examples are GATE, EX-
MARaLDA, MMAX2, brat, and WebAnno.
4
Many
annotation projects nowadays require distributed
collaborative working of multiple parties. The cur-
rently preferred solution is to use a tool with an
underlying database which is operated through a
standard web-browser. Among the tools above,
only brat and WebAnno are web-based tools. Com-
pared to CorA, these tools are more flexible in
that they support more annotation layers and more
complex (e.g., multi-word) annotations. WebAnno,
in addition, offers facilities for measuring inter-
annotator agreement and data curation. However,
brat and WebAnno do not allow edits to the source
document from within the tool, which is particu-
larly relevant for non-standard language varieties.
Similarly, they do not support retraining on newly
annotated data.
3 Data Model
The requirements described in Sec. 2 present vari-
ous challenges to the data storage, which necessi-
tated the development of our own data model. A
data model in this context is a conceptual model
of the data structure that allows serialization into
various representations such as XML or databases.
Such a model also allows for easy conversion be-
tween serializations and hence facilitates interop-
erability with existing formats and tools. The
complex, multi-layered layout, the differences in
tokenization, and the fine-grained description of
graphematic pecularities in the primary data cannot
be captured well using existing formats. For exam-
ple, tokenization differences as they are handled
by formats such as <tiger2/> (Bosch et al., 2012)
pertain only to the contraction of underlying units
to original forms, and not the other way around.
This means that while a conversion in such formats
is easily possible, some of the data structure that
is captured by our model is necessarily lost in the
process. To come up with a data model that min-
imizes redundancy and allows for flexibility and
extensibility, and accomodates the work flow of
our transcriptors and annotators, we employed nor-
malization techniques from database development.
A slightly simplified version of the data model is
shown in Fig. 2.
4
GATE: http://gate.ac.uk/
EXMARaLDA: http://www.exmaralda.org/
MMAX2: http://mmax2.sourceforge.net/
brat: http://brat.nlplab.org/
WebAnno: https://code.google.com/p/webanno/
88
Figure 2: Data model used for CorA
Token and Text The model is centered around
two units, a text and a token. A token is a virtual
unit that can manifest in two ways, the diplomatic
token and the modern token, each of which has
a one-to-many relation with a token (cf. Fig. 3).
Diplomatic tokens are tokens as they appear in
the original, historical text, while modern tokens
mirror modern conventions for token boundaries,
representing suitable units for further annotations,
e.g. with POS tags. All physical layout information
on the other hand relates to the diplomatic token.
The text is the entirety of a transcribed document
that can be partitioned in various ways. The layout
is captured by its relation to the page, column, and
line, which in turn relate to the diplomatic tokens.
Furthermore, a text can be assigned one or more
tagsets. The tagsets in turn can be open, such as
lemmatization tags, or closed, such as POS tags.
Each text can be assigned different tagsets.
Extensions In addition, the data model also al-
lows for the import of markup annotations with the
texts, which may denote layout-related or linguistic
peculiarities encoded by the transcriptors, as well
as information about its annotation status such as
progress, or dubious annotations. The model is
easily extendable for user management that can tie
in to the text table, e.g., a user can be set as owner
or creator of a text.
As XML serialization is not optimized for data
which is not strictly hierarchically structured, stor-
age and retrieval is rather inefficient, and extensions
are not easily possible. For this reason, we chose
to implement the application with an SQL database
<token>
<!-- diplomatic tokenization -->
<dipl trans="ober"/>
<dipl trans="czugemich"/>
<!-- modern tokenization -->
<mod trans="oberczuge">
<norm tag="?berzeuge"/>
<pos tag="VVIMP.Sg"/>
</mod>
<mod trans="mich">
<norm tag="mich"/>
<pos tag="PPER.1.Sg.
*
.Acc"/>
</mod>
</token>
Figure 3: Example serialization of ober czugemich
(modern ?berzeuge mich ?convince me?) in XML
serialization of the data model.
4 Conclusion
We described CorA, a web-based annotation tool.
Its main features are the integration of automatic
annotation software, the possibility of making edits
to the source document, and the conceptual dis-
tinction between diplomatic and modern tokens in
the data model. We believe that these features are
particularly useful for annotators of non-standard
language data such as historical texts, and set CorA
apart from other existing annotation tools.
We plan to make the tool available under an
open source license eventually. However, we are
currently still working on implementing additional
functionality. In future work, we plan to integrate
features to evaluate annotation quality, such as au-
tomatically calculating inter-annotator agreement.
89
References
Marcel Bollmann, Stefanie Dipper, Julia Krasselt, and
Florian Petran. 2012. Manual and semi-automatic
normalization of historical spelling ? case studies
from Early New High German. In Proceedings of
the First International Workshop on Language Tech-
nology for Historical Text(s) (LThist2012), Vienna,
Austria.
Marcel Bollmann. 2012. (Semi-)automatic normaliza-
tion of historical texts using distance measures and
the Norma tool. In Proceedings of the Second Work-
shop on Annotation of Corpora for Research in the
Humanities (ACRH-2), Lisbon, Portugal.
Sonja Bosch, Key-Sun Choi, ?ric de la Clergerie, Alex
Chengyu Fang, Gertrud Faa?, Kiyong Lee, Antonio
Pareja-Lora, Laurent Romary, Andreas Witt, Amir
Zeldes, and Florian Zipser. 2012. <tiger2/> as a
standardised serialisation for ISO 24615. In Pro-
ceedings of the 11th Workshop on Treebanks and
Linguistic Theory (TLT), Lisbon, Portugal.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS tagging and parsing the twitter-
verse. In Proceedings of AAAI-11 Workshop on
Analysing Microtext, San Francisco, CA.
Eugenie Giesbrecht and Stefan Evert. 2009. Part-of-
speech tagging ? a solved task? An evaluation of
POS taggers for the German Web as Corpus. In
Proceedings of the 5th Web as Corpus Workshop
(WAC5), pages 27?35, San Sebastian, Spain.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan
Culpeper, and Nicholas Smith. 2007. Tagging the
Bard: Evaluating the accuracy of a modern POS tag-
ger on Early Modern English corpora. In Proceed-
ings of Corpus Linguistics 2007, University of Birm-
ingham, UK.
Marc Reznicek, Anke L?deling, and Hagen
Hirschmann. 2013. Competing target hypotheses
in the Falko Corpus: A flexible multi-layer corpus
architecture. In Ana D?az-Negrillo, Nicolas Ballier,
and Paul Thompson, editors, Automatic Treatment
and Analysis of Learner Corpus Data, pages
101?123. Amsterdam: Benjamins.
Silke Scheible, Richard J. Whitt, Martin Durrell, and
Paul Bennett. 2011. Evaluating an ?off-the-shelf?
POS-tagger on Early Modern German text. In Pro-
ceedings of the ACL-HLT 2011 Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities (LaTeCH 2011), pages 19?
23, Portland, Oregon, USA.
Helmut Schmid and Florian Laws. 2008. Estimation of
conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of COLING ?08, Manchester, Great Britain.
Amir Zeldes, Julia Ritz, Anke L?deling, and Christian
Chiarcos. 2009. ANNIS: a search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics, Liverpool, UK.
90
