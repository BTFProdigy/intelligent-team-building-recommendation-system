Proceedings of BioNLP Shared Task 2011 Workshop, pages 65?73,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP Shared Task 2011 ? Bacteria Gene Interactions and Renaming
Julien Jourde1, Alain-Pierre Manine2, Philippe Veber1, Kare?n Fort3, Robert Bossy1,
Erick Alphonse2, Philippe Bessie`res1
1Mathe?matique, Informatique et 2PredictiveDB 3LIPN ? Universite? Paris-Nord/
Ge?nome ? Institut National de la 16, rue Alexandre Parodi CNRS UMR7030 and
Recherche Agronomique F75010 Paris, France INIST CNRS UPS76 ? F54514
MIG INRA UR1077 {apmanine,alphonse} Vand?uvre-le`s-Nancy, France
F78352 Jouy-en-Josas, France @predictivedb.com karen.fort@inist.fr
forename.lastname@jouy.inra.fr
Abstract
We present two related tasks of the BioNLP
Shared Tasks 2011: Bacteria Gene Renam-
ing (Rename) and Bacteria Gene Interactions
(GI). We detail the objectives, the corpus spec-
ification, the evaluation metrics, and we sum-
marize the participants? results. Both issued
from PubMed scientific literature abstracts,
the Rename task aims at extracting gene name
synonyms, and the GI task aims at extracting
genic interaction events, mainly about gene
transcriptional regulations in bacteria.
1 Introduction
The extraction of biological events from scientific
literature is the most popular task in Information Ex-
traction (IE) challenges applied to molecular biol-
ogy, such as in LLL (Ne?dellec, 2005), BioCreative
Protein-Protein Interaction Task (Krallinger et al,
2008), or BioNLP (Demner-Fushman et al, 2008).
Since the BioNLP 2009 shared task (Kim et al,
2009), this field has evolved from the extraction of a
unique binary interaction relation between proteins
and/or genes towards a broader acceptation of bio-
logical events including localization and transforma-
tion (Kim et al, 2008). In the same way, the tasks
Bacteria Gene Interactions and Bacteria Gene Re-
naming deal with the extraction of various molecu-
lar events capturing the mechanisms relevant to gene
regulation in prokaryotes. The study of bacteria has
numerous applications for health, food and indus-
try, and overall, they are considered as organisms
of choice for the recent integrative approaches in
systems biology, because of their relative simplicity.
Compared to eukaryotes, they allow easier and more
in-depth analysis of biological functions and of their
related molecular mechanisms.
Processing literature on bacteria raises linguis-
tic and semantic specificities that impact text anal-
ysis. First of all, gene renaming is a frequent phe-
nomenon, especially for model bacteria. Hence, the
abundance of gene synonyms that are not morpho-
logical variants is high compared to eukaryotes. The
history of bacterial gene naming has led to drastic
amounts of homonyms and synonyms which are of-
ten missing (or worse, erroneous) in gene databases.
In particular, they often omit old gene names that
are no longer used in new publications, but that are
critical for exhaustive bibliography search. Poly-
semy makes the situation even worse, as old names
frequently happen to be reused to denote different
genes. A correct and complete gene synonym table
is crucial to biology studies, for instance when inte-
grating large scale experimental data using distinct
nomenclatures. Indeed this information can save a
lot of bibliographic research time. The Rename Task
is a new task in text-mining for biology that aims at
extracting explicit mentions of renaming relations.
It is a critical step in gene name normalization that
is needed for further extraction of biological events
such as genic interactions.
Regarding stylistics, gene and protein interactions
are not formulated in the same way for eukary-
otes and prokaryotes. Descriptions of interactions
and regulations in bacteria include more knowledge
about their molecular actors and mechanisms, com-
pared to the literature on eukaryotes. Typically in
bacteria literature, the genic regulations are more
65
likely expressed by direct binding of the protein,
while in eukaryote literature, non-genic agents re-
lated to environmental conditions are much more
frequent. The bacteria GI Task is based on (Manine
et al, 2010) which is a semantic re-annotation of the
LLL challenge corpus (Ne?dellec, 2005), where the
description of the GI events in a fine-grained rep-
resentation includes the distinction between expres-
sion, transcription and other action events, as well as
different transcription controls (e.g. regulon mem-
bership, promoter binding). The entities are not only
protein agent and gene target but extend to families,
complexes and DNA sites (binding sites, promoters)
in order to better capture the complexity of the reg-
ulation at a molecular level. The task consists in re-
lating the entities with the relevant relations.
2 Rename Task Description
The goal of the Rename task is illustrated by Figure
1. It consists in predicting renaming relations be-
tween text-bound gene names given as input. The
only type of event is Renaming where both argu-
ments are of type Gene. The event is directed, the
former and the new names are distinguished. Genes
and proteins were not distinguished because of the
high frequency of metonymy in renaming events.
The relation to predict between genes is a Renam-
ing of a former gene name into a new one. In the
example of Figure 1, YtaA, YvdP and YnzH are the
former names of three proteins renamed CotI, CotQ
and CotU, respectively.
Figure 1: Examples of relations to be extracted.
2.1 Rename Task corpus
The Rename Task corpus is a set of 1,836 PubMed
references of bacterial genetic and genomic studies,
including title and abstract. A first set of 23,000 doc-
uments was retrieved, identifying the presence of the
bacterium Bacillus subtilis in the text and/or in the
MeSH terms. B. subtilis documents are particularly
rich in renaming mentions. Many genes were re-
named in the middle of the nineties, so that the new
names matched those of the Escherichia coli homo-
logues. The 1,843 documents the most susceptible
to mention renaming were automatically filtered ac-
cording to two non exclusive criteria:
1. Either the document mentions at least two gene
synonyms as recorded in the fusion of seven B.
subtilis gene nomenclatures. This led to a set
of 703 documents.
2. Or the document contains a renaming expres-
sion from a list that we manually designed and
tested (e.g. rename, also known as). It is an ex-
tension of a previous work by (Weissenbacher,
2004). A total of 1,140 new documents not in-
cluded in the first set match this criteria.
About 70% of the documents (1,146) were kept in
the training data set. The rest was split into the de-
velopment and test sets, containing 246 and 252 doc-
uments respectively. Table 1 gives the distribution
of genes and renaming relations per corpus. Gene
names were automatically annotated in the docu-
ments with the nomenclature of B. subtilis. Gene
names involved in renaming acts were manually cu-
rated. Among the 21,878 gene mentions in the three
corpus, 680 unique names are involved in renaming
relations which represents 891 occurrences of genes.
Training + Dev. Test
Documents (1,146 + 246) 1,392 252 (15%)
Gene names 18,503 3,375 (15%)
Renamings 373 88 (24%)
Table 1: Rename Task corpus content.
2.2 Rename Task annotation and guidelines
Annotation procedure The corpus was annotated
in a joint effort of MIG/INRA and INIST/CNRS.
The reference annotation of the Rename Task cor-
pus was done in two steps, a first annotation step
by science information professionals of INIST with
MIG initial specifications, a second checking step by
people at MIG. Two annotators and a project man-
ager were in charge of the task at INIST. The docu-
ments were annotated using the Cadixe editor1. We
1http://caderige.imag.fr/Articles/
CADIXEXML-Annotation.pdf
66
provided to them detailed annotation guidelines that
were largely modified in the process. A subset of
100 documents from the first set of 703 was anno-
tated as a training session. This step was used to re-
fine the guidelines according to the methodology de-
scribed in (Bonneau-Maynard et al, 2005). Several
inter-annotator agreements coefficients were com-
puted to measure the discrepancy between annota-
tors (Fort et al, 2009). With a kappa and pi scores
(for more details on those, see (Artstein and Poesio,
2008)), the results can be considered satisfactory.
The manual analysis of the 18 discrepancies led to
enrich the annotation guidelines. The first hundreds
of documents of the second set did not mention any
renaming, leading to concentrate the annotation ef-
forts on the first set. These documents actually con-
tained renamings, but nearly exclusively concerning
other kinds of biological entities (protein domains,
molecules, cellular ultrastructures, etc.).
Guidelines In order to simplify the task, only
short names of gene/protein/groups in B. subtilis
were considered. Naming conventions set short
names of four letters long with an upper case let-
ter at the end for all genes (e.g. gerE) and the same
names with the upper case of the initial letter (e.g.
GerE) and long names for the proteins (e.g. Spore
germination protein gerE). But many irregular gene
names exist (e.g. tuf), which are considered as well.
It also happens that gene or protein name lists are
abbreviated by factorization to form a sequence. For
instance queCDEF is the abbreviation of the list of
gene names queC, queD, queE and queF. Such ag-
gregations are acceptable gene names as well. In any
case, these details were not needed by the task par-
ticipants since the corpus was provided with tagged
gene names.
Most renaming relations involve couples of the
same type, genes, proteins or aggregations. Only
18 relations link mixed couples of genes and pro-
teins. In case of ambiguity, annotators would consult
international gene databases and an internal INRA
database to help them determine whether a given
couple of names were actually synonyms.
Multiple occurrences of the same renaming rela-
tion were annotated independently, and had to be
predicted. The renaming pairs are directed, the for-
mer and the new forms have to be distinguished.
When the renaming order was not explicit in the
document, the rule was to annotate by default the
first member of the couple as the new form, and the
second one as the former form. Figure 2 presents the
most common forms of renaming.
Figure 2: Common types of relations to be extracted.
Revised annotations INIST annotations were
systematically checked by two experts in Bioinfor-
matics from INRA. Mainly, encoding relations (e.g.
the gene encoding sigma K (sigK)) that are not re-
naming cases were purged. Given the number of
ambiguous annotations, we designed a detailed ty-
pology in order to justify acceptance or rejection
decisions in seven different sub-cases hereafter pre-
sented. Three positive relations figure in Table 2,
where the underlined names are the former names
and the framed names are the new ones. Explicit re-
naming relations occur in 261 sentences, synonymy-
like relations in 349 sentences, biological proof-
based relations in 76 sentences.
Explicit renaming relation is the easiest positive
case to identify. In the example, the aggregation of
gene names ykvJKLM is clearly renamed by the au-
thors as queCDEF. Although the four genes are con-
Explicit renaming
PMID 15767583 : Genetic analysis of ykvJKLM mu-
tants in Acinetobacter confirmed that each was essen-
tial for queuosine biosynthesis, and the genes were re-
named queCDEF .
Implicit renaming
PMID 8002615 : Analysis of a suppressor mutation
ssb ( kinC ) of sur0B20 (spo0A) mutation in Bacil-
lus subtilis reveals that kinC encodes a histidine pro-
tein kinase.
Biological proof
PMID 1744050 : DNA sequencing established that
spoIIIF and spoVB are a single monocistronic locus
encoding a 518-amino-acid polypeptide with features
of an integral membrane protein.
Table 2: Positive examples of the Rename Task.
67
catenated, there is no evidence mentioned of them
acting as an operon. Furthermore, despite the con-
text involving mutants of Acinetobacter, the aggre-
gation belongs correctly to B. subtilis.
Implicit renaming is an asymmetric relation
since one of the synonyms is intended to replace the
other one in future uses. The example presents two
renaming relations between former names ssb and
spo0A, and new names kinC and sur0B20, respec-
tively. The renaming relation between ssb and kinC
has a different orientation due to additional informa-
tion in the reference. Like in the preceding example,
the renaming is a consequence of a genetic mutation
experiment. Mutation names represent an important
transversal issue that is discussed below.
Biological proof is a renaming relation induced
by an explicit scientific conclusion while the renam-
ing is not, as in the example where experiments re-
veal that two loci spoIIIF and spoVB are in fact the
same one and then become synonyms. Terms such
as ?allelic to? or ?identical to? usually qualify such
conclusions. Predicting biological proof-based rela-
tions requires some biological modeling.
The next three cases are negative (Table 3). Un-
derlined gene and protein names are involved in a
relation which is not a renaming relation.
Protein encoding relation occurs between a gene
and the protein it codes for. Some mentions may
look like renaming relations. The example presents
the gene yeaC coding for MoxR. No member of the
couple is expected to replace the other one.
Homology measures the similarity between gene
or protein sequences. Most of the homology men-
tions involve genes or proteins from different species
Protein encoding
PMID 8969499: The putative products of ORFs yeaB
(Czd protein), yeaC (MoxR), yebA (CNG-channel and
cGMP-channel proteins from eukaryotes),
Genetic homology
PMID 10619015 : Dynamic movement of the ParA-
like Soj protein of B. subtilis and its dual role in nu-
cleoid organization and developmental regulation.
Operon | Regulon | Family
PMID 3127379 : Three promoters direct transcription
of the sigA (rpoD) operon in Bacillus subtilis.
Table 3: Negative examples of the Rename Task.
(orthologues). The others compare known gene or
protein sequences of the same species (paralogues).
This may be misleading since the similarity men-
tion may look like biological proof-based relations,
as between ParA and Soj in Table 3.
Operon, regulon or family renaming involves
objects that may look like genes, proteins or sim-
ple aggregations of gene or protein names but that
are perceptibly different. The objects represent more
than one gene or protein and the renaming does not
necessarily affect all of them. More problematic,
their name may be the same as one of the genes or
proteins they contain, as in the example where sigA
and rpoD are operons but are also known as gene
names. Here, sigA (and so rpoD) represents at least
two different genes. For the sake of clarity, oper-
ons, regulons and families are rejected, even if all
the genes are clearly named, as in an aggregation.
The last point concerns mutation which are fre-
quent in Microbiology for revealing gene pheno-
types. They carry information about the original
gene names (e.g., rvtA11 is a mutant name created
by adding 11 to rvtA). But partial names cannot be
partially annotated, that is to say, the original part
(rvtA) should not be annotated in the mutation name
(rvtA11). Most of these names are local names, and
should not be annotated because of their restricted
scope. It may happen so that the mutation name
is registered as a synonym in several international
databases. To avoid inconsistencies, all renamings
involving a mutation referenced in a database were
accepted, and only biological proof-based and ex-
plicit renamings involving a strict non-null unrefer-
enced mutation (a null mutation corresponds to a to-
tal suppression of a gene) were accepted.
2.3 Rename Task evaluation procedure
The evaluation of the Rename task is given in terms
of recall, precision and F-score of renaming rela-
tions. Two set of scores are given: the first set is
computed by enforcing strict direction of renaming
relations, the second set is computed with relaxed
direction. Since the relaxed score takes into ac-
count renaming relations even if the arguments are
inverted, it will necessarily be greater or equal than
the strict score. The participant score is the relaxed
score, the strict score is given for information. Re-
laxed scores are informative with respect to the ap-
68
plication goal. The motivation of the Rename task
is to keep bacteria gene synonyms tables up to date.
The choice of the canonical name among synonyms
for denoting a gene is done by the bacteriology com-
munity, and it may be independent of the anteriority
or novelty of the name. The annotation of the ref-
erence corpus showed that the direction was not al-
ways decidable, even for a human reader. Thus, it
would have been unfair to evaluate systems on the
basis of unsure information.
2.4 Results of the Rename Task participants
Final submissions were received from three teams,
the University of Turku (Uturku), the University of
Concordia (Concordia) and the Bibliome team from
MIG/INRA. Their results are summarized in Table
4. The ranking order is given by the overall F-score
for relations with relaxed argument order.
Team Prec. Recall F-score
Univ. of Turku 95.9 79.6 87.0
Concordia Univ. 74.4 65.9 69.9
INRA 57.0 73.9 64.4
Table 4: Participant scores at the Rename Task.
Uturku achieved the best F-score with a very high
precision and a high recall. Concordia achieved the
second F-score with balanced precisions and recalls.
Bibliome is five points behind with a better recall
but much lower precision. Both UTurku and Con-
cordia predictions rely on dependencies (Charniak-
Johnson and Stanford respectively, using McClosky
model), whereas Bibliome predictions rely on bag of
words. This demonstrates the high value of depen-
dency parsing for this task, in particular for the pre-
cision of predictions. We notice that UTurku system
uses machine learning (SVM) and Concordia uses
rules based on trigger words. The good results of
UTurku confirms the hypothesis that gene renam-
ing citations are highly regular in scientific litera-
ture. The most frequently missed renamings belong
to the Biological Proof category (see Table 2). This
is expected because the renaming is formulated as a
reasoning where the conclusion is only implicit.
2.5 Discussion
The very high score of Uturku method leads us to
conclude that the task can be considered as solved
by a linguistic-based approach. Whereas Bib-
liome used an extensive nomenclature considered
as exhaustive and sentence filtering using a SVM,
Uturku used only two nomenclatures in synergy but
with more sophisticated linguistic-based methods,
in particular syntactic analyses. Bibliome methods
showed that a too high dependence to nomenclatures
may decrease scores if they contain compromised
data. However, the use of an extensive nomencla-
ture as done by Bibliome may complement Uturku
approach and improve recall. It is also interesting
that both systems do not manage renamings cross-
ing sentence boundaries.
The good results of the renaming task will be ex-
ploited to keep synonym gene lists up to date with
extensive bibliography mining. In particular this
will contribute to enriching SubtiWiki, a collabora-
tive annotation effort on B. subtilis (Flo?rez et al,
2009; Lammers et al, 2010).
3 Gene Interactions Task description
The goal of the Bacteria GI Task is illustrated by
Figure 3. The genes cotB and cotC are related to
their two promoters, not named here, by the rela-
tion PromoterOf. The protein GerE is related to
these promoters by the relation BindTo. As a con-
sequence, GerE is related to cotB and cotC by an In-
teraction relation. According to (Kim et al, 2008),
the need to define specialized relations replacing one
unique and general interaction relation was raised in
(Manine et al, 2009) for extracting genic interac-
tions from text. An ontology describes relations and
entities (Manine et al, 2008) catching a model of
gene transcription to which biologists implicitly re-
fer in their publications. Therefore, the ontology is
mainly oriented towards the description of a struc-
tural model of genes, with molecular mechanisms
of their transcription and associated regulations.
The corpus roughly contains three kinds of genic
Figure 3: Examples of relations to be extracted.
69
interaction mentions, namely regulations, regulon
membership and binding. The first case corresponds
to interactions the mechanism of which is not explic-
itly given in the text. The mention only tells that the
transcription of a given gene is influenced by a given
protein, either positively (activation), negatively (in-
hibition) or in an unspecified way. The second kind
of genic interaction mention (regulon membership)
basically conveys the same information, using the
regulon term/concept. The regulon of a gene is the
set of genes that it controls. In that case, the interac-
tion is expressed by saying that a gene is a member
of some regulon. The third and last kind of mention
provides with more mechanistic details on a regula-
tion, since it describes the binding of a protein near
the promoter of a target gene. This motivates the in-
troduction of Promoter and Site entities, which cor-
respond to DNA regions. It is thus possible to extract
the architecture of a regulatory DNA region, linking
a protein agent to its gene target (see Figure 3).
The set of entity types is divided into two main
groups, namely 10 genic entities and 3 kinds of ac-
tion (Table 5). Genic entities represent biological
objects like a gene, a group of genes or a gene prod-
uct. In particular, a GeneComplex annotation corre-
sponds to an operon, which is a group of genes that
are contiguous in the genome and under the control
of the same promoter. The annotation GeneFamily
is used to denote either genes involved in the same
biological function or genes with sequence homolo-
gies. More importantly, PolymeraseComplex anno-
tations correspond to the protein complex that is re-
sponsible for the transcription of genes. This com-
plex includes several subunits (components), com-
bined with a sigma factor, that recognizes specific
promoters on the DNA sequence.
The second group of entities are phrases express-
ing either molecular processes (e.g. sequestration,
dephosphorylation, etc.) or the molecular state of
the bacteria (e.g. presence, activity or level of a pro-
tein). They represent some kind of action that can
be performed on a genic entity. Note that transcrip-
tion and expression events were tagged as specific
actions, because they play a specific part in certain
relations (see below).
The annotation of entities and actions was pro-
vided to the participants, and the task consisted in
extracting the relations listed in Table 6.
Name Example
Gene cotA
GeneComplex sigX-ypuN
GeneFamily class III heat shock genes
GeneProduct yvyD gene product
Protein CotA
PolymeraseComplex SigK RNA polymerase
ProteinFamily DNA-binding protein
Site upstream site
Promoter promoter regions
Regulon regulon
Action activity | level | presence
Expression expression
Transcription transcription
Table 5: List of molecular entities and actions in GI.
Name Example
ActionTarget expression of yvyD
Interaction ComK negatively regulates
degR expression
RegulonDependence sigmaB regulon
RegulonMember yvyD is member of sigmaB
regulon
BindTo GerE adheres to the pro-
moter
SiteOf -35 sequence of the pro-
moter
PromoterOf the araE promoter
PromoterDependence GerE-controlled promoter
TranscriptionFrom transcription from the up-
stream site
TranscriptionBy transcription of cotD by
sigmaK RNA polymerase
Table 6: List of relations in GI.
The relations are binary and directed, and rely the
entities defined above. The three kinds of interac-
tions are represented with an Interaction annotation,
linking an agent to its target. The other relations
provide additional details on the regulation, like ele-
mentary components involved in the reaction (sites,
promoters) and contextual information (mainly pro-
vided by the ActionTarget relations). A formal def-
inition of relations and relation argument types can
be found on the Bacteria GI Task Web page.
3.1 Bacteria Gene Interactions corpus
The source of the Bacteria GI Task corpus is a set
of PubMed abstracts mainly dealing with the tran-
70
scription of genes in Bacillus subtilis. The semantic
annotation, derived from the ontology of (Manine et
al., 2008), contains 10 molecular entities, 3 different
actions, and 10 specialized relations. This is applied
to 162 sentences from the LLL set (Ne?dellec, 2005),
which are provided with manually checked linguis-
tic annotations (segmentation, lemmatization, syn-
tactic dependencies). The corpus was split into 105
sentences for training, 15 for development and 42
for test. Table 7 gives the distribution of the entities
and actions per corpus and Table 8 gives the distri-
bution of the relations per corpus.
3.2 Annotation procedures and guidelines
The semantic annotation scheme was developed by
two annotators through a series of independent an-
notations of the corpus, followed by reconciliation
steps, which could involve concerted modifications
(Manine et al, 2010). As a third and final stage, the
Entity or action Train. + Dev. Test
Documents (105+15) 120 42
Protein 219 85
Gene 173 56
Transcription 53 21
Promoter 49 10
Action 45 22
PolymeraseComplex 43 14
Expression 29 6
Site 22 8
GeneComplex 19 4
ProteinFamily 12 3
Regulon 11 2
GeneProduct 10 3
GeneFamily 6 5
Table 7: Distribution of entities and actions in GI.
Relation Train. + Dev. Test
Interaction 208 64
ActionTarget 173 47
PromoterOf 44 8
BindTo 39 4
PromoterDependence 36 4
TranscriptionBy 36 8
SiteOf 23 6
RegulonMember 17 2
TranscriptionFrom 14 2
RegulonDependence 12 1
Table 8: Distribution of relations in GI.
corpus was reviewed and the annotation simplified
to make it more appropriate to the contest. The final
annotation contains 748 relations distributed in nine
categories, 146 of them belonging to the test set.
The annotation scheme was generally well suited
to accurately represent the meaning of the sentences
in the corpus, with one notable exception. In the cor-
pus, there is a common phrasing telling that a pro-
tein P regulates the transcription of a gene G by a
given sigma factor S. In that case, the only anno-
tated interactions are between the couples (P, G) and
(S, G). This representation is not completely satis-
factory, and a ternary relation involving P, S and G
would have been more adequate.
Additional specific rules were needed to cope
with linguistic issues. First, when the argument of a
relation had coreferences, the relation was repeated
for each maximally precise coreference of the argu-
ment. Second, in case of a conjunction like ?sig-
maA and sigmaX holoenzymes?, there should ide-
ally be two entities (namely ?sigmaA holoenzyme?
and ?sigmaX holoenzyme?); however, this is not
easy to represent using the BioNLP format. In this
situation, we grouped the two entities into a single
one. These cases were rare and unlikely affected the
feasibility of the task, since entities were provided
in the test set.
3.3 Gene Interactions evaluation procedure
The training and development corpora with the ref-
erence annotations were made available to partici-
pants by December, 1st on the BioNLP shared Task
pages together with evaluation software. The test
corpus with the entity annotations has been made
available by March, 1st. The participants sent the
predicted annotations to the BioNLP shared Task
organizers by March, 10th. The evaluation results
were computed and provided to the participants and
on the Web site the same day. The participants are
evaluated and ranked according to two scores: F-
score for all event types together, and F-score for
the Interaction event type. In order for a predicted
event to count as a hit, both arguments must be the
same as in the reference in the right order and the
event type must be the same as in the reference.
71
3.4 Results of GI Task participants
There was only one participant, whose results are
shown in Tables 9 and 10. Some relations were
not significantly represented in the test set and thus
the corresponding results should be considered with
caution. This is the case for RegulonMember and
TranscriptionFrom, only represented two times each
in the test. The lowest recall, 17%, obtained for the
SiteOf relation is explained by its low representa-
tion in the corpus: most of the test errors come from
a difficult sentence with coreferences.
The recall of 56% for the Interaction relation cer-
tainly illustrates the heterogeneity of this category,
gathering mentions of interactions at large, as well
as precise descriptions of gene regulations. For in-
stance, Figure 4 shows a complex instance where all
of the interactions were missed. Surprisingly, we
also found false negatives in rather trivial examples
(?ykuD was transcribed by SigK RNA polymerase
from T4 of sporulation.?). Uturku used an SVM-
based approach for extraction, and it is thus delicate
to account for the false negatives in a simple and
concise way.
Event U. Turku scores
Global Precision 85
Global Recall 71
Global F-score 77
Interaction Precision 75
Interaction Recall 56
Interaction F-score 64
Table 9: University of Turku global scores.
Event Prec. Rec. F-score
Global 85 71 77
ActionTarget 94 92 93
BindTo 75 75 75
Interaction 75 56 64
PromoterDependence 100 100 100
PromoterOf 100 100 100
RegulonDependence 100 100 100
RegulonMember 100 50 67
SiteOf 100 17 29
TranscriptionBy 67 50 57
TranscriptionFrom 100 100 100
Table 10: University of Turku scores for each relation.
Figure 4: Examples of three missed interactions.
3.5 Discussion
The GI corpus was previously used in a relation
extraction work (Manine et al 2009) based on In-
ductive Logic Programming (Muggleton and Raedt,
1994). However a direct comparison of the results
is not appropriate here since the annotations were
partially revised, and the evaluation setting was dif-
ferent (leave-one-out in Manine?s work, test set in
the challenge).
Nevertheless, we note similar tendencies if we
compare relative results between relations. In partic-
ular, it was also found in Manine?s paper that SiteOf,
TranscriptionBy and Interaction are the most diffi-
cult relations to extract. It is also worth to mention
that both approaches rely on syntactic dependencies,
and use the curated dependencies provided in the
corpus. Interestingly, the approach by the University
of Turku reports a slightly lower F-measure with de-
pendencies calculated by the Charniak parser (about
1%, personal communication). This information is
especially important in order to consider a produc-
tion setting.
4 Conclusion
The quality of results for both challenges suggests
that current methods are mature enough to be used
in semi-automatic strategies for genome annotation,
where they could efficiently assist biological experts
involved in collaborative annotation efforts (Lam-
mers et al, 2010). However, the false positive rate,
notably for the Interaction relation, is still too high
for the extraction results to be used as a reliable
source of information without a curation step.
Acknowlegments
We thank Franc?oise Tisserand and Bernard Talercio
(INIST) for their work on the Rename corpus, and
the QUAERO Programme funded by OSEO (French
agency for innovation) for its support.
72
References
Artstein R., Poesio M. (2008). Inter-coder agreement
for Computational Linguistics. Computational Lin-
guistics, 34(4):555-96.
Bjo?rne J., Heimonen J., Ginter F., Airola A., Pahikkala
T., Salakoski T. (2009). Extracting complex biological
events with rich graph-based feature sets. BioNLP?09
Proc. Workshop Current Trends in Biomedical Natural
Language Processing: Shared Task, pp. 10-18.
Bonneau-Maynard H., Rosset S., Ayache C., Kuhn A.,
Mostefa D. (2005). Semantic annotation of the French
Media Dialog Corpus. Interspeech-2005, pp. 3457-60.
Demner-Fushman D., Ananiadou S., Cohen K.B., Pestian
J., Tsujii J., Webber B. (2008). Themes in biomedical
natural language processing: BioNLP08. BMC Bioin-
formatics, 9(Suppl. 11):S1.
Flo?rez L.A., Roppel S.F., Schmeisky A.G., Lammers
C.R., Stu?lke J. (2009). A community-curated con-
sensual annotation that is continuously updated: The
Bacillus subtilis centred wiki SubtiWiki. Database,
2009:bap012.
Fort K., Franc?ois C., Ghribi M. (2010). ?Evaluer des an-
notations manuelles disperse?es : les coefficients sont-
ils suffisants pour estimer l?accord inter-annotateurs ?
17e Conf. Traitement Automatique des Langues Na-
turelles (TALN 2010).
Kim J.D., Ohta T., Tsujii J. (2008) Corpus annotation for
mining biomedical events from literature. BMC Bioin-
formatics, 9:10.
Kim J.D., Ohta T., Pyysalo S., Kano Y., Tsujii J. (2009).
Overview of BioNLP?09 shared task on event ex-
traction. BioNLP?09 Proc. Workshop Current Trends
in Biomedical Natural Language Processing: Shared
Task, pp. 1-9.
Krallinger M., Leitner F., Rodriguez-Penagos C., Va-
lencia A. (2008). Overview of the protein-protein in-
teraction annotation extraction task of BioCreative II.
Genome Biology, 9(Suppl. 2):S4.
Lammers C.R., Flo?rez L.A., Schmeisky A.G., Roppel
S.F., Ma?der U., Hamoen L., Stu?lke J. (2010). Con-
necting parts with processes: SubtiWiki and Subti-
Pathways integrate gene and pathway annotation for
Bacillus subtilis. Microbiology, 156(3):849-59.
Manine A.P., Alphonse E., Bessie`res P. (2008). Informa-
tion extraction as an ontology population task and its
application to genic interactions. 20th IEEE Int. Conf.
Tools with Artificial Intelligence (ICTAI?08), pp. 74-
81.
Manine A.P., Alphonse E., Bessie`res P. (2009). Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. Int. J. Medical Informat-
ics, 78(12):e31-8.
Manine A.P., Alphonse E., Bessie`res P. (2010). Extrac-
tion of genic interactions with the recursive logical the-
ory of an ontology. Lecture Notes in Computer Sci-
ences, 6008:549-63.
Muggleton S., Raedt L.D. (1994) Inductive Logic Pro-
gramming: Theory and methods. J. Logic Program-
ming, 19-20:629-79.
Ne?dellec C. (2005). Learning Language in Logic ? Genic
Interaction Extraction Challenge. Proc. 4th Learning
Language in Logic Workshop (LLL?05), pp. 31-7.
Weissenbacher, D. (2004). La relation de synonymie en
Ge?nomique. RECITAL 2004 Conference.
73
Proceedings of BioNLP Shared Task 2011 Workshop, pages 102?111,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP 2011 Task Bacteria Biotope ? The Alvis system 
Zorana Ratkovic1,2   Wiktoria Golik1    Pierre Warnier1   Philippe Veber1   Claire N?dellec1 
1 MIG INRA UR1077, Domaine de Vilvert F-850 Jouy-en-Josas, France forename.name@jouy.inra.fr   
2 LaTTiCe UMR 8094 CNRS Univ. Paris 3 1 rue Maurice Arnoux F-92120 MONTROUGE   Abstract 
This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope (BB) task of the Bi-oNLP 2011 shared tasks. Bacteria, geo-graphical locations and host entities were processed by a pattern-based approach and domain lexical resources. For the extraction of environment locations, we propose a framework based on semantic analysis sup-ported by an ontology of the biotope do-main. Domain-specific rules were devel-oped for dealing with Bacteria anaphora. Official results show that our Alvis system achieves the best performance of participat-ing systems. 
1 Introduction Given a set of Web pages, the information extrac-tion goal of the Bacteria Biotope (BB) task is to precisely identify bacteria and their locations and to relate them. The type of the predicted locations has to be selected among eight types. Among them the host and host-part locations have to be related by the part-of relation. Three teams participated in the challenge.  BB task example Ureaplasma parvum is a mycoplasma and a pathogenic 
ureolytic mollicute which colonises 
 the urogenital tracts of humans.  One of the specificities of the BB task is that the bacteria location vocabulary is very large and vari-ous as opposed to protein subcellular locations in 
biology challenges (Kim et al, 2010) and geo-graphical locations (Zhou et al, 2005). Locations include natural environments and hosts as well as food and medical locations. In order to deal with this heterogeneity, we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope (BB) termino-ontology. This mapping derives the type of location terms and filters out non-location terms. Large external dictionaries of host names (i.e. NCBI taxonomy) and geographical names (i.e. Agrovoc thesaurus) complete the lexical resources. The high frequency of bacteria anaphora and ambiguous antecedent candidates in the corpus was also a difficulty. Our Alvis system implements an anaphora resolution algorithm that takes into con-sideration the anaphoric distance and the position of the antecedent in the sentence. Alvis predicts the bacteria names and their relation to the locations with the help of hand-made patterns based on lin-guistic analysis and lexical resources.  The methods for predicting and typing locations (section 2) and bacteria (section 3) are first de-scribed. Section 4 details the method for relating them. Section 5 comments the experimental results. 2 Location  Our system handles separately the recognition of host and geographical names by dictionary map-pings, while the recognition of locations of the en-vironment and host part types is based on linguistic analysis and ontology inference.  Host names and geographical names appeared to be easier to predict by using a named-entity recog-nition strategy than the other types of location. They are less subject to variation than environ-mental locations, which can include any physical feature. For host name extraction, we used the NCBI taxonomy as the major source. Only the eu-karyote subtree was considered for host detection. 
Localization 
Part-of 
102
Our system filters out the ambiguous names such as Indicator (honeyguides) or Dialysis (xylophage insect) by comparing them to a list of common words in English. The host name list was enriched with additional common names including non-taxonomic host groups (e.g. herbivores), progeny names (e.g. calf) and human categories (e.g. pa-tient). The resulting host name list contains more than 1,800,000 scientific names and 60,000 com-mon names. The geographical name recognition component uses a small dictionary of all geo-graphic terms from the Agrovoc thesaurus sub-vocabularies. At first, we considered using the very rich resource GeoNames. However, it contains too many ambiguous names to be directly usable by short-term development. 2.1 Location of Environment type The identification of environment locations is done in two steps. First, the automatic extraction of all candidate terms from the test corpus, then the as-signment of a location type to these terms with the help of the Bacteria Biotope (BB) termino-ontology. The type assigned to a given term is the type of the closest concept label in the ontology. Since the BB termino-ontology was originally not structured according to the eight types, in order to be usable it first had to be enriched by the new concepts and then mapped to this topology.  Corpus term extraction. The corpus terms were automatically extracted by the AlvisNLP/ML pipe-line (Nedellec et al, 2008) with BioYatea (Nedel-lec et al, 2010). BioYatea is the version of Yatea (Hamon & Aubin, 2006) adapted to the biology domain. We modified BioYatea setting according to the training dataset study. We observed that most of the location terms in the training dataset are noun phrases with adjective modifiers (e.g. ro-dent nests) while prepositional phrases are rather rare (e.g. breaks in the skin). We set the term boundaries of BioYatea to include all prepositions except the of preposition. Considering other prepo-sitions such as with may yield syntactic attachment errors, thus we prefer the risk of incomplete terms to incorrect prepositional attachments. Bacteria Biotope ontology. We used the Bacte-ria Biotope (BB) termino-ontology for typing the extracted terms. It is under development for the study of bacteria phenotypes and habitats. The high level of the habitat part is structured in a manner similar to that proposed by the one level classifica-
tion by Floyd (Floyd et al, 2005). It has a fine-grained structure with the same goal as the general-ist EnvO habitat ontology (Field et al, 2008), but it focuses on bacteria phenotype and biotope model-ing. It includes a terminological level that records lexical forms of the concepts including terms, synonyms and variations. For the purpose of the challenge, the initial on-tology was manually completed using location concepts. The training corpus, as well as the habitat and isolation site fields of the GOLD database on sequenced prokaryotes (Liolios et al, 2009) are the main sources of location terms and synonyms. The analysis of the training corpus mainly led to the addition of adjectival forms of host parts (e.g. lym-phatic, intracellular) and human references (e.g. patient, infant, progeny).  The GOLD database isolation site field is a very rich source of bacteria location terms. It is filled by natural language descriptions of matters, natural habitats, hosts and geographical locations. For in-stance, the isolation site of Anoxybacillus flavi-thermus bacterium is waste water drain at the Wairakei geothermal power station in New Zea-land. The term analysis of GOLD isolation site en-tries yielded 3,415 location terms including 1,050 geographical names. Hundreds of these terms were manually added to the BB termino-ontology. The lack of time as well as the full sentence structure of the GOLD resource prevented us from correctly handling them in a fully automatic way. We are currently developing a method for the automatic alignment of the terms extracted from GOLD to the BB termino-ontology. Additionally, the GOLD habitat field provided around a hundred different terms that have been directly integrated into the BB termino-ontology. The current version of the habitat subpart of the BB termino-ontology contains 1,247 concepts and 266 synonyms.  Location types in Bacteria Biotope ontology. The BB termino-ontology has been developed pre-vious to the BB task and the structure of its habitat subpart does not reflect the eight location types of the task. In order to reuse the ontology for the BB task, we assigned types to each location concept. We manually associated the high level nodes of the location hierarchies to the eight BB task types. The types of the lower level concepts were then auto-matically inferred. For instance, the concept aquatic environment is tagged Water in the ontol-
103
ogy and all of its descendants lake, sea, ocean are of type Water as well. Local type exceptions were manually tagged. For instance, the waste tree in-cludes water-carried wastes of type Water and solid industrial residues of type Environment. This way all concepts in the resulting typed ontology were assigned a unique type. The concept types are then propagated to their associated term classes at the terminological level. For instance, underground water and its synonym subterranean water are both typed as Water. The resulting typed BB termino-ontology is then usable for deriving the types of the terms extracted from the test corpus. Derivation of location type. The BB termino-ontology scope is too limited for the correct predic-tion of all candidate term types by Boolean and exact comparison. From the 2,290 candidate terms of the test corpus, only 152 belong as such to the BB termino-ontology. We propose a method based on the head comparison of the candidate and BB terms for the derivation of the candidate term type.  The quality of the ontology-based annotation depends to a large extent on an accurate match be-tween the resource and the terms extracted from the corpus. Our method targets the syntactic structure of terms (candidate and BB terms) in order to gath-er the most of semantically similar terms. This approach differs from the ontology alignment and population methods that also use the information from the ontology structure in order to infer seman-tic relationships (e.g. hyponyms, meronyms) (Eu-zenat, 2007). It also differs from semantic annota-tion supported by context analysis such as distribu-tional semantics (Grefenstette, 1994) or Hearst pat-terns (Hearst, 1992). It belongs to the class of methods that focus on the morphology of the cor-pus terms, which use string-based (Levensthein, 1966, Jaro, 1989) or linguistic-based methods (Jac-quemin & Tzoukermann, 1999).  Even though the context-based approach should produce very good results, we chose a less time-consuming method that is easier and faster to set up, which is based on morphosyntactic analysis.  In our case, string similarity measures turn out to be irrelevant (laboratory rat does not mean rat labo-ratory). We observed that in candidate and BB terms, the head is very often the most informative element. Thus, the linguistic-based analysis of terms, in particular the head-similarity analysis (Hamon & Nazarenko, 2001), represents a promis-ing alternative. Our method is inspired by 
MetaMap (Aronson, 2001). MetaMap tags bio-medical corpora with the UMLS Metathesaurus by syntactic analysis that takes into account lexical heads of terms. The similarity scores computed by linguistically-based metrics are higher for terms whose heads have previously been analyzed.  The MetaMap method includes a variant compu-tation that maps acronyms, abbreviations, syno-nyms as well as derivational, inflectional and spell-ing variants. Our term typing method is less sophis-ticated and uses a few lexical variants due to the lack of a complete resource. Some ontology en-richment applications also use head-supported term matching, as in Desmontils (Desmontils et al, 2003). In Desmontils, new concepts belonging to WordNet (Fellbaum, 1998) are automatically added to the ontology in order to improve the indexing process. However, the analysis of the results shows that a great number of concepts found in the texts are not considered because they do not exist in WordNet. Our typing task uses a similar head-based method, but only for type derivation.  Our system derives the location type of candi-date terms in several steps. First, if there is a term in the BB termino-ontology that is strictly equal to the candidate term, it is assigned the same type. Then, the other candidate terms are assigned types according to the comparison of their heads to the BB term heads. We assume that in most of the cases the term head conveys the information about the type and is non-ambiguous. A given head H is non-ambiguous if all BB terms with head H are of the same type. The location term head set is the set of all habitat term heads found in the BB termino-ontology. The current version contains 693 differ-ent heads. Let Te denote the extracted term to be typed. If the head of Te does not belong to the BB term head set, then the type of Te is simply not Lo-cation (e.g. high metabolic diversity). If Te head does belong to the BB term head set and the head is non-ambiguous, then Te is assigned the associated type. For instance, the head of the extracted term stratified lake is lake. The type of all the BB terms with lake head is Water (e.g. meromictic lake). Stratified lake is therefore typed as Water.  Specific processing is applied to terms with am-biguous heads. The associative set of BB term heads and types exhibits some cases of ambiguous heads with multiple types that we analyzed in de-tail. There are two kinds of ambiguities that were 
104
processed in different ways. In the first, multiple types reflect different roles of the same object. In the second, the head is non-informative with re-spect to the type.  In the latter case the type is con-veyed by the subterm (term after head removal). We qualify non-informative BB term heads as neu-tral. They mainly denote habitats (habitat, envi-ronment, medium, zone) and extracts (sample, sur-face, isolate, material, content). In this case, the type is derived from the subterm. For instance, the head isolate of the extracted term marine isolate is neutral. After head removal, it is assigned the type Water since marine is of type Water. Freshwater has the same type as freshwater medium or fresh-water environment since medium and environment are neutral heads.  Some heads have more than one type although they denote specific locations. Their multiple types reflect different uses or states. For instance, the head bottle has two types: Food and Medical. The type Food is derived from the BB concept water bottle and the type Medical is derived from bedside water bottles in a hospital environment. The correct type for the extracted terms is then selected by a set of patterns based on the context of the term in the document. For instance, many vegetables and meats could be either of type Host or Food. The type is Host by default. One pattern states that if a term includes or is preceded by a food processing-related word (e.g. cooked, grilled, fermented), then the term is reassigned the type Food. Another pat-tern states that if a host is preceded by a death-related adjective (dead, decaying), then its type should be revised as Environment.  Our system currently includes nine disambigua-tion/retyping patterns. The first version of the type derivation method was automatically applied to the 1,263 GOLD terms after head analysis. Manual examination of the results yielded an extension of the two lists of neutral heads and heads with am-biguous types. There are 20 neutral heads and 21 ambiguous heads in the current version of the BB termino-ontology. The head-matching algorithm appears to be quite productive for the biotope terms. The procedure applied to the test corpus yielded the following figures: BioYatea extracted 2,290 terms. 416 terms matching the post-processing filters were discarded. This includes terms which are too general (i.e. approach, diver-sity), terms containing irrelevant or non desirable adjectives (i.e. numerous deficiencies, known spe-
cies) and terms containing forbidden words accord-ing to the annotation location rules (i.e. bacteria, pathogen, contaminated, parasite). Finally, 1,873 candidate terms were kept. Among these figures:  - 152  terms belong to the BB termino-ontology  - 90  terms were typed using the ontology heads - 6 terms with several types were handled by disambiguation patterns. We plan to extend the list of neutral heads and dis-criminate adjectives for type disambiguation by machine learning classification applied to the BB termino-ontology modifiers. Location entity boundary. The analysis of term extraction result from the training corpus shows that the predicted boundaries of locations were not fully consistent with the task annotation guidelines. Post-processing adjusts incorrect boundaries by filtering irrelevant words, packing and merging terms. Irrelevant words (e.g. contaminated, in-fected, host species, disease, inflammation) were removed from the location candidate terms inde-pendently of their types (e.g. contaminated Bach-man Road site vs. Bachman Road ; host plant vs. plant). Note that BioYatea extracts not only the maximum terms (e.g. contaminated Bachman Road site), but also their constituents (Bachman Road site, Bachman Road and site). Boundary adjust-ment often consists in selecting the relevant alter-native among the subterms.  Other boundary issues are handled by several patterns, which are applied after the typing stage. These patterns are type-dependent: each pattern only applies to one type or a subset of location types. When necessary, they shift the boundaries in order to include relevant modifiers.  They also split location terms or join adjacent location terms. BioYatea may have missed relevant modifiers be-cause of POS-tagging errors. For instance, if a na-tionality name precedes a location, then it is in-cluded (e.g. German oil field). Also, it frequently happens that hosts are modifiers of host parts (e.g. insect gut). BioYatea extracts the whole term and its constituents. The term is correctly typed as Host-part and the host modifier as Host. In order to avoid embedded locations, a specific pattern is de-voted to the splitting of these terms. In this way insect gut (Host-part) becomes insect (Host) and gut (Host-part). Most of these patterns involve several specific lexicons, including cardinal directions, relevant and 
105
irrelevant modifiers for each type of location, as well as types, which can be merged and split. The current resources were manually built by examin-ing the location terms of the training set and GOLD isolation fields. The acquisition of relevant and irrelevant modifiers could be automated by ma-chine learning. Some linguistic phenomena could be better handled by the customization of BioYa-tea. For instance BioYatea considers the preposi-tion with as a term boundary so it cannot extract terms containing with, like areas with high sulfur and salt concentrations.  3 Extraction of Bacteria names  We observed in the training corpus that not only were bacteria names tagged, but also higher level taxa (families) and lower level taxa (strains). We used the NCBI taxonomy as the main bacteria tax-on resource since it includes all organism levels and is kept up-to-date. This bacteria dictionary was enriched by taxa from the training corpus, in par-ticular by non standard abbreviations (e.g. Chl. = Chlorobium, ssp. = subsp) and plurals, (Vibrios as the plural for Vibrio) that were hopefully rather rare. Determining the boundaries of the bacteria names was one of the main issues because corpus strain names do not always follow conventional nomenclature rules.  Also, the recognition of bacte-ria name is evaluated using a strict exact match. Patterns were developed to account for such cases. They handle inversion (LB400 of Burkholderia xe-novorans instead of Burkholderia xenovorans LB400) and parenthesis (Tropheryma whipplei (the Twist strain) instead of Tropheryma whipplei strain Twist).  The corpus also mentions names of bacte-ria that contain modifiers not found in the NCBI dictionary, such as antimicrobial-resistant C. coli or L. pneumophila serogroup 1. Such cases, as well as abbreviations (e.g. GSB for green sulfur bacte-ria) and partial strain names (e.g. strain DSMZ 245 T for Chlorobium limicola strain DSMZ 245 T) were also specifically handled. The main source of error in bacteria name pre-diction is due to the mixture of family names and strain name abbreviations in the same text. It fre-quently happens that the strain name is abbreviated into the first word of the name. For instance Bar-tonella henselae is abbreviated as Bartonella. Un-fortunately, Bartonella is a genus mentioned in the 
same text, thus yielding ambiguities between the anaphora and the family name, which are identical. 3.1 Bacteria anaphora resolution Anaphors are frequent in the text, especially for bacteria reference and to a smaller extent for host reference. Our effort focused on bacteria anaphora resolution ignoring host anaphora. The extraction method of location relations (section 4) assumes that the relation arguments, location and bacterium (or anaphora of the bacterium) occur in the same sentence. From a total of 2,296 sentences in the training corpus, only 363 sentences contain both the location and the explicit bacterium, while 574 mention only the location. Two thirds of the loca-tions do not co-occur with bacteria. This demon-strates the importance of recovering the bacteria for these cases, which is potentially referred to by an explicit anaphora.  The manual examination of the training corpus showed that the most frequent anaphora of bacteria are not pronouns but higher level taxa, often pre-ceded by a demonstrative determinant, (i.e. This bacteria, This Clostridium) and sortal anaphora (i.e. genus, organism, species and strain), both of which are commonly found in biological texts (Torri & Vijay-Shanker, 2007). The style of some of the documents is rather relaxed and the antece-dent may be ambiguous even for a human reader. We observed three types of anaphora in the corpus. First, the standard anaphora which includes both pronouns and sortal anaphora, which requires a unique bacterial antecedent. Second, bi-anaphora or an anaphora that requires two bacteria antece-dents. This happens when the properties of two strains are compared in the document. Finally, the case of a higher taxon being used to refer to a lower taxon, which we named name taxon anaph-ora.  Anaphora with a unique antecedent C. coli is pathogenic in animals and humans. Peo-ple usually get infected by eating poultry that con-tained the bacteria, eating raw food, drinking raw milk, and drinking bottle water [?].  Anaphora with two antecedents C. coli is usually found hand in hand with its bac-teria relative, C. jejuni. These two organisms are recognized as the two most leading causes of acute inflammation of intestine in the United States and other nations. 
106
 Name taxon anaphora Ticks become infected with Borrelia duttonii while feeding on an infected rodent. Borrelia then multi-plies rapidly, causing a generalized infection throughout the tick.  For anaphora detection and resolution a pattern-based approach was preferred to machine learning because the constraints for relating anaphora to antecedent candidates of the same taxonomy level were mainly semantic and domain-dependent and the annotation of anaphora was not provided in the training corpus.  Anaphora detection consists of identifying po-tential anaphora in the corpus, given a list of pro-nouns, sortal anaphora and taxa and then filtering out irrelevant cases (Segura-Bedmar et al, 2010, Lin & Lian, 2004) before anaphora resolution. Not all the pronouns, sortal anaphora terms and higher taxon bacteria are anaphoric. For example, if a higher taxon is preceded or followed by the word genus, this signals that it is not anaphoric but that the text is actually about the higher taxon.   Non-anaphoric higher taxon Burkholderia cenocepacia HI2424[?] The genus Burkholderia consists of some 35 bacte-rial species, most of which are soil saprophytes and phytopathogens that occupy a wide range of environmental niches.  The anaphora resolution algorithm takes into ac-count two features: the distance to the antecedent candidate and its position in the sentence. The an-tecedent is usually found in proximity to the ana-phora, in order to maintain the coherence of the text. Therefore, our method ranks the antecedent candidates according to the anaphoric distance counted in sentences.  If more than one bacterium is found in a given sentence, their position is discriminate. Centering theory states that in a sentence the most prominent entities and therefore the most probable antecedent candidates are in the order: subject > object > other position (Grosz et al, 1995). In English, due to the SVO order of the language the subject is most of-ten found at the beginning of the sentence, fol-lowed by the object and the others. Therefore, the method retains the leftmost bacterium in the sen-tence when searching for the best antecedent can-didate. 
More precisely, the method selects the first ante-cedent that it finds according to the following pre-cedence list: - First bacterium in the current sentence (s) - First bacterium in the previous sentence    (s-1) - First bacterium in sentence s-2 - First bacterium in sentence s-3 - First bacterium in the current paragraph - Last bacterium in the previous paragraph - First bacterium in the first sentence of the document - The first bacterium ever mentioned. -  The method only relates anaphora to antecedents that are found before. It does not handle cataphors since they are rarely found in the corpus. For ana-phors that require two antecedents we use the same criteria but search for two bacteria in each sentence or paragraph, instead of one. For taxon anaphora we look for the presence of a lower taxon in the document found before the anaphora that is com-patible according to the species taxonomy. The counts of anaphora detected by the patterns are given in Table 1.   Corpus Single ante Bi ante Taxon ante Train 933 4 129 Dev 204 3 22 Test 240 0 18 Total 1,377 7 169  Table 1. The count of the types of anaphora per corpus.  The anaphora resolution algorithm allowed us to retrieve more sentences that contain both a bacte-rium and a location.  Out of the 574 sentences that contain only a location, 436 were found to contain an anaphora related to at least one bacterium. The remaining 138 sentences are cases where there is no bacterial anaphora or the bacterium name is im-plicit. It frequently happens that the bacterium is referred to through its action. For example in the sentence below, the bacterium name could be de-rived from the name of the disease that it causes.   In the 1600s anthrax was known as the "Black bane" and killed over 60,000 cows.  One of the questions we had about the resolution of anaphora is whether anaphora that are found in the same sentence together with a bacterium (there-fore potentially its antecedent) should be consid-
107
ered or not.  We tested this on the development set. We found that removing such anaphora from con-sideration improved the overall score. It yielded an F-score of 53.22% (precision: 46.17%, recall: 62.81%), compared to the original F-score of 50.15% (precision: 41.06%, recall: 64.44%). This improvement in F-score is solely due to an increase in precision, which shows that while resolving anaphora is important and required, the incorrect recognition of terms as anaphora and incorrect anaphora resolution can introduce noise. 4 Relation extraction In this work we concentrated most of our effort on the prediction of entities. For the prediction of events we used a strategy based on the co-occurrence of  arguments and trigger words within a sentence: - If a bacteria name, a location and a trigger word are present in a sentence, then the system pre-dicts a Localization event between the bacte-rium and the location. - If a bacteria anaphora, a location and a trigger word are present in a sentence, then the system predicts a Localization event between each ana-phora antecedent and the location. - If a host, a host part, a bacterium and at least one trigger word are present in a sentence, then the system predicts a PartOf event between the host and the host part.  The list of trigger words contains 20 verbs (e.g. inhabit, colonize, but also discover, isolate), 16 disease markers (e.g. chronic, pathogen) and 19 other relevant words (e.g. ingest, environment, niche). This list was designed by ranking words in the sentences of the training corpus containing both a bacteria name and a location. The ranking crite-rion used was the information gain with respect to whether the sentence contained an event or not. The ranked list was adjusted by removing spurious words and adding domain knowledge words. By removing the constraint of the occurrence of a trigger word in the sentence, we can determine that the maximum recall the method can achieve with this strategy is 47% (precision: 41%, F-score: 44%). The selected trigger word list yielded a re-call close to the maximum, thus it seems that the trigger words do not affect the recall and are suit-able for the task.  
5 Results Table 2 summarizes the official scores that the Bib-liome Alvis system achieved for the Bacteria Biotope Task. It ranked first among three partici-pants. The first column gives the recall of entity prediction. The prediction of hosts and bacteria named-entities achieved a good recall of 84 and 82, respectively.    Entity recall Event recall Event Precis. F-score Bacteria 84 - - - Host 82 61 48 53 Host part 72 53 42 47 Env. 53 29 24 26 Geo. 29 13 38 19 Food - - 29 41 Medical 100 50 33 40 Water 83 60 55 57 Soil 86 69 59 63 Total  45 45 45  Table 2. Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011.  However, geographical locations based on a similar strategy were poorly predicted (29%). Our system predicted only 15 countries. A more appropriate resource of geographical names than the Agrovoc thesaurus would certainly increase the recall of geographical locations.  The host parts, medical, water and soil locations predicted with the same ontology-based method were surprisingly good with a recall of 72, 100, 83 and 86, respectively. The small size of the ontology and the small number of different term heads (i.e. 51 different heads) initially appeared as a limitation factor for reuse on new corpora. The good recall shows that the location vocabulary of the test set has similarities with the training set compared to potential space of location names.  The potential space is reflected by the richness of the GOLD iso-lation site field. This demonstrates the robustness of the type derivation approach based on term heads. The correctness of the derivation type can-not be calculated without a corpus where all the locations and not only bacteria ones are annotated. The recall of the environment location prediction is a little bit lower, 53%. The environment type in-
108
cludes many different types that cannot all be an-ticipated. Therefore the coverage of the BB ter-mino-ontology environment part is limited except for water and soil, which are more focused topics.  The localization event recall (column 2) is on average 20% lower for all types than the location entity recall. The regularity of the difference may suggest that once the argument is identified, the localization relation is equally harder to find by our method independently of the type. The localization event precision (column 3) is more difficult to ana-lyze because many sources of error may be in-volved, such as an incorrect arguments, incorrect anaphora resolution, relation to the wrong bacte-rium among several or the absence of a relation.  The prediction precision of localization events involving soil, water and host is better than envi-ronment and food. The manual analysis of the test corpus shows that in some cases environmental locations were mentioned as potential sources of industrial applications without actually being bac-teria isolation places. For instance, in Other fields of application for thermostable enzymes are starch-processing, organic synthesis, diagnostics, waste treatment, pulp and paper manufacture, and ani-mal feed and human food, the Alvis system errone-ously predicted waste treatment, paper manufac-ture, animal feed and human food.  This is due to the fact that the system does not handle modalities. Such hypotheses are specific to the BB task text genre, i.e. Bacteria sequencing projects. Such pro-jects contain details for potential industrial applica-tions, which are absent from academic literature. Ambiguous types are also a source of error. De-spite the host dictionary cleaning, some ambigui-ties remained. For example, the head canal in tooth root canal is erroneously typed as water and should be disambiguated with its tooth host-part modifier.  After test publication we measured the gain of anaphora resolution by using the on-line service. The anaphora resolution algorithm was found to have a strong impact on the final result.  Running the test set using all of the modules except for the anaphora resolution algorithm yielded a decrease in the F-score by almost 13% (F-score: 32.5%, preci-sion: 48.5%, 24.4%).  This shows that the addition of an anaphora resolution algorithm significantly increases the precision and that a resolution algo-rithm adapted to the Bacteria domain is necessary for the Biotope corpus. 
The part-of event prediction relies on the strict co-occurrence of a bacterium, trigger word, host and host part within a sentence. An additional run with the more relaxed constraint where the bacte-rium can be denoted by an anaphora as well yielded a gain of 6 recall points, a loss of 5 preci-sion points with a net benefit of 1 F-measure point. 6 Discussion The use of trigger words for the selection of sen-tences for relation extraction does not take into ac-count the structure or syntax of the sentence for the prediction of relation arguments. The system pre-dicts all combinations of bacteria and locations as localization events and all combination of host and host parts as part-of event. This has a negative ef-fect on the precision measure since some pairs are irrelevant as in the sentence below.  Baumannia cicadellinicola. This newly discovered or-ganism is an obligate endosymbiont of the leafhopper insect Homalodisca coagulata (Say), also known as the Glassy-Winged Sharpshooter, which feeds on the xylem of plants.  It has been shown that the use of syntactic de-pendencies to extract biological events (such as protein-protein interactions) improves the results of such systems (Erkan et al, 2007, Manine et al, 2008, Airola et al 2008). The use of syntactic de-pendencies could offer a more in depth examina-tion of the syntax and the semantics and therefore allow for a more refined extraction of bacteria-localization and host-host part relations.   Term extraction appears to be a good method for predicting locations including unseen terms, but it is limited by the typing strategy that filters out all terms with unknown heads (with respect to the BB termino-ontology). In the future, we will study the effect of linguistic markers such as enumeration and exemplification structures for recovering addi-tional location terms. For instance, in heated or-ganic materials such as compost heaps, rotting hay, manure piles or mushroom growth medium, our system has correctly typed heated organic ma-terials as environment but not the other examples because of their unknown heads. The promising performance of the Alvis system on the BB task shows that a combination of semantic analysis and domain-adapted resources is a good strategy for information extraction in the biology domain. 
109
References  Agrovoc: http://aims.fao.org/website/AGROVOC-Thesaurus Antti Airola, Sampo Pyysalo, Jari Bj?rne, Tapio Pah-nikkala, Filip Ginter, and Tapio Salakoski. 2008. A Graph Kernel for Protein-Protein Interaction Extrac-tion. BioNLP2008: Current Trends in Biomedical Natural Language Processing, pages 1-9. Alan R. Aronson. 2001. Effective mapping of biomedi-cal  text to the UMLS Metathesaurus: The MetaMap program. Proceedings of AMIA Symposium 2001, pages 17-21. Emmanuel Desmontils, Christine Jacquin and  Laurent Simon. 2003. Ontology enrichment and indexing process. Research report RR-IRIN-03.05, Institut de Recherche en Informatique de Nantes, Nantes, Fran-ce. J?r?me Euzenat and Pavel Shvaiko. 2007. Ontology matching, Springer Verlag, Heidelberg (DE),page 333. Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metage-nomes: the Minimum Information about a Genome Sequence (MIGS) specification. Nature Biotechnol-ogy 26, pages 541-547. GeoNames: http://www.geonames.org/  Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery.  Natural Language Processing and Machine Translation. London: Kluwer Academic Publishers. Barbara J. Grosz, Araving K. Joshi and Scott Weinstein. 1995. Centering: A Framework for Modelling the Lo-cal Coherence of Discourse.  University of Pennsyl-vania Institute for Research in Cognitive Science Technical Reports Series. G?ne? Erkan, Arzucan ?zg?r and Dragomir R. Radev. 2007. Semi-Supervised Classification for Extracting Protein Interaction Sentences using Dependency Parsing. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pag-es 228-237. Thierry Hamon and Sophie Aubin. 2006. Improving term extraction with terminological resources. In Salakoski, T. et al, editors, Advances in Natural Lan-guage Processing 5th International Conference on NLP (Fin- TAL?06), pages 380?387. Springer. Thierry Hamon and Adeline Nazarenko. 2001. Detection of synonymy links between terms: experiment and re-
sults, Recent Advances in Computational Terminol-ogy. Pages 185-208. John Benjamins. Marti A. Hearst. 1992. Automatic acquisition of hypo-nyms from large text corpora. In Zampolli, A.(ed.), Proceedings of the 14 th COLING, pages 539?545, Nantes, France. Christian Jacquemin and Evelyne Tzoukermann. 1999. NLP for term variant extraction: A synergy of mor-phology, lexicon, and syntax. In Strzalkowski, T. (ed.), Natural language information retrieval, volume 7 of Text, speech and language technology, chapter 2, pages  25?74. Dordrecht & Boston: Kluwer Aca-demic Publishers. Matthew A. Jaro. 1989. Advances in record linkage me-thodology as applied to matching the 1985 census of Tampa, Florida. Journal of the American Statistical Association 84(406), pages 414-20. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano and Jun?ichi Tsujii. (to appear). Extract-ing bio-molecular events from literature - the Bi-oNLP?09 shared task. Special issue of the Interna-tional Journal of Computational Intelligence. Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Dok-lady akademii nauk SSSR, 163(4):845-848, 1965. In Russian. English translation in Soviet Physics Dok-lady, 10(8), pages 707-710. Yu-Hsiang Lin and Tyne Liang. 2004. Pronomial and Sortal Anaphora Resolution for Biomedical Litera-ture. In Proceedings of ROCLING XVI: Conference on Computational Linguistics and Speech Processing.  Konstantinos Liolios, I-Min A. Chen., Konstantinos Mavromatis, Nektarios Tavernarakis, Philip Hugen-holtz, Victor M. Markowitz and Nikos C. Kyrpides. 2009. The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. NAR Epub. Alain-Pierre Manine, Erick Alphonse and Philippe Bes-si?res. 2008. Information extraction as an ontology population task and its application to genic interac-tions, 20th IEEE Intl. Conf. Tools with Artificial In-telligence, ICTAI'08., vol. II, pp. 74-81. NCBI taxonomy: http://www.ncbi.nlm.nih.gov/Taxonomy/  Claire N?dellec, Wiktoria Golik, Sophie Aubin and Robert Bossy. 2010. Building Large Lexicalized On-tologies from Text: a Use Case in Indexing Biotech-nology Patents, International Conference on Knowl-edge Engineering and Knowledge Management (EKAW 2010), Lisbon, Portugal. 
110
Isabel Segura-Bedmar, Mario Crespo, C?sar de De Pa-blo-S?nchez and Paloma Mart?nez. 2010. Resolving anaphoras for the extraction of drug-drug interactions in pharmacological documents. BMC Bioinformatics 11(Supl 2):S1. Manabu Torii and K. Vijay-Shanker. 2007. Sortal Anaphora Resolution in Medline Abstracts. Computa-tional Intelligence 23, pages 15-27. Zhou GuoDong, Su Jian, Zhang Jie and Zhang Min. 2005. Exploring Various Knowledge in Relation Ex-traction. In Proceedings of the 43rd Annual Meeting of the ACL, pages 427-434, Ann Arbor. Association for Computational Linguistics.  
111
